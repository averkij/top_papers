{
    "paper_title": "Retrieval-Augmented Personalization for Multimodal Large Language Models",
    "authors": [
        "Haoran Hao",
        "Jiaming Han",
        "Changsheng Li",
        "Yu-Feng Li",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://github.com/Hoar012/RAP-MLLM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 2 0 6 3 3 1 . 0 1 4 2 : r Retrieval-Augmented Personalization for Multimodal Large Language Models Haoran Hao1,2, Jiaming Han1, Changsheng Li3, Yu-Feng Li2, Xiangyu Yue1 1MMLab, The Chinese University of Hong Kong 2National Key Laboratory for Novel Software Technology, Nanjing University 3Beijing Institute of Technology Figure 1. Introduce some user-specific concepts to our RAP-LLaVA, it can remember them and achieve excellent performance in variety of personalized multimodal generation tasks."
        },
        {
            "title": "Abstract",
            "content": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of userspecific knowledge still restricts their application in humans daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs personalization. Starting from general MLLM, we turn it into personalized assistant in three steps. (a) Remember: We design key-value database to store userrelated information, e.g., users name, avatar and other attributes. (b) Retrieve: When the user initiates conversation, RAP will retrieve relevant information from the database using multimodal retriever. (c) Generate: The input query and retrieved concepts information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time Equal contribution Corresponding author concept editing via updating the external database. To further improve generation quality and alignment with userspecific information, we design pipeline for data collection and create specialized dataset for personalized training of MLLMs. Based on the dataset, we train series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://hoar012.github.io/RAP-Project/. 1. Introduction the development of Recently, large language models (LLMs) has significantly enhanced their language processing and generating capabilities [59]. Building on this the integration of visual and textual ability foundation, 1 Table 1. Comparison of Different Personalization Methods. RAP needs only 1 image with its personalized description, showing outstanding convenience and flexibility in practical applications."
        },
        {
            "title": "Method",
            "content": "Positive Negative Caption Description Question-Answer Recognition Real-time edit Text-only QA Fine-tuning MyVLM [2] YoLLaVA [31] RAP(Ours) 1 - 150 200 -"
        },
        {
            "title": "No\nYes\nYes\nNo",
            "content": "through vision-language alignment brings powerful multimodal LLMs (MLLMs) [12, 15, 28, 32, 44, 50, 55]. MLLMs have shown significant improvement in various tasks, such as image description and question answering, highlighting their potential as humans assistants. However, their lack of user-specific knowledge continues to limit their effectiveness as personalized assistants in daily life. qualified personalized assistant first should be able to recognize and remember user-related concepts, such as the dog named Lala adopted by the user. Although existing MLLMs have been trained on large-scale datasets and possess strong recognition and classification capabilities, directly transferring this knowledge to users personal concepts remains challenging. For instance, current leading MLLMs cannot remember your dogs name, even if you have mentioned it before, and they lack awareness of your identity and preferences. Furthermore, the assistant should generate responses tailored to the users preferences and requirements. However, collecting extensive personal data to train unique assistant for each user is impractical. To address this issue, the personalization of MLLMs has become topic of growing interest, with several approaches already being proposed. MyVLM [2] utilizes external classification heads to recognize specific concepts, and learns an embedding for each concept to personalize the outputs of vision language models (VLMs) [23, 28]. Another concurrent work, YoLLaVA [31], learns few special tokens to represent each concept. However, both approaches necessitate continuous learning and updating of the model as new concepts emerge. This presents challenge in dynamic, ever-changing real-world scenarios, where the computing power of users personal devices is often limited, and all data must be stored locally for privacy concerns. To address these challenges, we propose the Retrieval Augmented Personalization (RAP) framework, designed to allow MLLMs to update their supported concepts without additional training. Specifically, RAP works in three key steps. (a) Remember: RAP includes designed database to help remember each concept via storing its image and basic information, e.g., name, avatar and other attributes. (b) Retrieve: When user initiates conversation, RAP will retrieve relevant information from the database using multimodal retriever. (c) Generate: The input query and retrieved concepts information are incorporated into the MLLMs input for personalized, knowledge-augmented generation. RAP requires only one image per concept with its basic information for personalization. It allows users to make realtime adjustments to the models outputs by modifying their personal databases, eliminating the need for retraining. more detailed comparison is presented in Table 1. Another significant challenge is the lack of large-scale datasets for training MLLMs personalized generation capabilities. To address this, we design pipeline to collect extensive training data and create comprehensive dataset, which enables to train MLLMs to effectively understand and utilize user-related information for generation. Based on this dataset, we train LLaVA [28] and Phi3-V [35] as novel personalized assistants and evaluate their performance across various tasks, including personalized image captioning, question answering, and visual recognition. Experimental results demonstrate that our RAP-MLLMs excel in wide range of personalized generation tasks, showcasing excellent generation quality and flexibility. Our contributions are summarized as follows: We propose the RAP framework for MLLMs personalization, allowing models pre-trained on our dataset to adapt to diverse users and infinite new concepts without further training. We develop pipeline for collecting large-scale data and create dataset specifically designed for the personalized training and evaluation of MLLMs. This dataset enables us to train series of MLLMs to function as personalized assistants. Our models demonstrate exceptional performance across various personalized multimodal generation tasks, including image captioning and question answering. Additionally, they exhibit strong capability to recognize personal concepts within images. 2. Related Work Multimodal Large Language Models. Recently, numerous advanced large language models (LLMs) [1, 8, 42, 43, 56] have been proposed, showing remarkable performance in addressing wide range of tasks. The rapid development 2 of LLMs has led to the emergence of multimodal LLMs (MLLMs) [12, 15, 28, 32, 44, 55, 60], which excel in general visual understanding and complex reasoning tasks. For instance, LLaVA [27, 28] and MiniGPT-4 [60] align visual and language modalities through visual instruction tuning, showcasing impressive capabilities in multimodal conversations. GPT4RoI [57] and RegionGPT [13] enhance finegrained understanding and reasoning for specific regions by training on region-level instruction datasets. Despite these advancements in tasks such as image captioning and question answering, the lack of user-specific knowledge restricts the generation of personalized content. In this work, we focus on the personalization of MLLMs, enabling them to remember and understand user-specific concepts, and generate personalized content tailored to users preferences. Personalization of MLLMs. In the realm of artificial intelligence, personalization typically refers to the process of tailoring system, application, or model to meet the individual needs and preferences [41, 45, 46, 49]. Substantial efforts have been made to generate images of users personal objects or in certain context [10, 14, 21, 22, 37, 40, 48]. For example, Dreambooth [37] employs transfer learning in text-to-image diffusion models via fine-tuning all parameters for new concepts. In this paper, we mainly aim at enabling MLLMs to remember and understand userspecific concepts, and generate personalized language outputs. There are several works focusing on the personalization of MLLMs, among which the most relevant works are MyVLM [2] and YoLLaVA [31]. MyVLM introduces the It utilizes external classifitask of personalizing VLMs. cation heads to recognize specific concepts, and learns an embedding for each concept to personalize the outputs of VLMs. YoLLaVA personalizes LLaVA by extending its vocabulary and learning specific tokens for each concept. However, both approaches require continuous model updates as new concepts emerge, which presents challenges in dynamic real-world applications. In this work, we propose RAP framework for the personalization of MLLMs, enabling models pre-trained on our dataset to continuously updating supported concepts without further fine-tuning. Retrieval Augmented Generation. Retrieval-based methods for incorporating external knowledge have proven effective in enhancing generation across variety of knowledge-intensive tasks [3, 11, 26, 47, 51, 58]. DPR [19] introduces Dense Passage Retrieval, marking shift from sparse to dense retrieval techniques. Later, MuRAG [5] proposes to use multimodal knowledge to augment language generation. Self-Rag [3] introduces special tokens to make retrieval adaptive and controllable. ERAGent [41] presents comprehensive system for retrieval-augmented language models. With the advancements in MLLMs, RAG has been widely applied to multimodal generative tasks. For instance, FLMR [25] employs multi-dimensional embeddings to capture finer-grained relevance between queries and documents, achieving significant improvement on the RA-VQA setting. While existing methods primarily enhance models performance by retrieving from external knowledge bases, few of them consider the personalization task. Although RAG has been applied to image generation [4, 54] and image captioning [24, 34], there is currently no existing work focusing on personalizing MLLMs via RAG, to the best of our knowledge. 3. Retrieval Augmented Personalization Existing MLLMs typically align other modalities with language. For instance, LLaVA [28] projects visual tokens into text space, and then generates subsequent tokens using an LLM. While these MLLMs perform well in various tasks, the lack of memory and comprehension of personal concepts hinders effective user-specific responses. In this work, we mainly focus on personalizing MLLMs to generate tailored language responses, such as creating personalized captions for users images and answering quesIn this section, we detail tions about personal concepts. the implementation steps of our proposed Retrieval Augmented Personalization (RAP) framework. Unlike previous approaches that usually necessitate additional data collection and further training to learn new concepts, after pretraining on our dataset, RAP-MLLMs can adapt to diverse users and infinite new concepts without further training. In section 3.1, we present the RAP framework that is applicable to various types of MLLMs, and then in section 3.2, we provide details of the proposed dataset. 3.1. RAP Framework RAP works in three steps: Remember, Retrieve and Generate. An overview of the framework is shown in Figure 2. Remember. The premise of personalization is that the model can remember personal concepts and relevant information, such as the dog named Lala adopted by A. To facilitate this, we construct database to store personal concepts, which comprises an avatar Ij, name along with brief description Tj for each concept. The key ki for each concept in the database is its visual feature, obtained by feeding its image Ij into pre-trained image encoder E(). Examples of our database are presented in Figure 2. When user initiates conversation, the input can be represented as = (Xv, Xq), which may include both image Xv and some textual instructions Xq. The first step involves identifying possible concepts within the input image that have been previously stored in the database. Previous methods [2] typically need to learn an external classifier to determine whether concept appears in the input image, which requires substantial amount of training data and can only apply to specific concept. To enhance the generalizability of the recognition process, we do not construct specific mod3 Figure 2. Retrieval-Augmented Personalization Framework. Region-of-interest detected by an open world detector are used to retrieve concepts from the database. The images and information of the retrieved concepts are then integrated into the input for the MLLM. ules for each concept. Instead, we employ universal detection model, such as YOLO [36] and YOLO-World [7], as recognition model R(). Given the predefined setting that specifies which categories should be remembered, the users region-of-interest can be acquired via Xu = R(Xv, XqP ). Retrieve. Identified region-of-interest will be used as query to retrieve from the database. For each recognized component Xi u, we feed the image crop into the image encoder E() to get its visual feature vi = E(Xi u), which is ndimensional vector. Then we calculate the euclidean distance between the visual feature and each key kj M, which is calculated as Dist(vi, kj) = vi kj. The TopK image-text pairs {(I1, T1), (I2, T2), (IK, TK)} with the lowest distances are selected from the database. We also introduce retrieval using concept names, such as sks for unique concept. When the user mentions the name of an object documented in the database, our model retrieves its related information from the database. This also enables our model to respond to text-only queries effectively. Generate. Each pair Mj = (Ij, Tj) provides related information about users personal concept and will be incorporated into the input of the MLLM. Take LLaVA [28] as an example. The image Ij is first encoded by pre-trained vision encoder, such as CLIP [33], to obtain their visual tokens Zj. These image tokens are then projected by projector into language tokens Hv , which could be understood by the language model. Simultaneously, corresponding text information Tj are transformed into text tokens Hq . This process is also applied to both Xv and Xq. All these tokens are incorporated into the MLLMs input to generate language response. During training, we keep parameters of both the detector and retriever frozen, just train the MLLMs parameters θ. Given the length of the output sequence, the probability of the target answer Xa is computed as: p(Xa) = (cid:89) i=1 pθ(Xa,iXv, Xq, M1, MK, Xa,<i). (1) 3.2. Personalization Dataset Most existing MLLMs struggle to generate personalized outputs even if additional concept information is provided, and there is currently no large-scale dataset for personalized training of MLLMs. To this end, we design pipeline for data creation and curate novel dataset specifically for the personalized training of MLLMs. We use Gemini-1.5 [12] to generate annotations for our dataset. An overview of our pipeline and dataset is presented in Figure 3. The first component of our dataset is dedicated to visual grounding. In this task, MLLM is trained to determine whether specific concept is in an image, particularly identifying if the person or object in reference image appears in the given image. When positive match is detected, we also require the model to provide the bounding box for the identified concept. For single-concept grounding, we primarily use the RefCOCO dataset [20]. Based on RefCOCOs annotations, we crop target concepts from the images and assign names to them, which serve as references for specific concepts. We then query Gemini to generate concise descriptions about properties of the concepts in these cropped regions, by which we construct large-scale database including numerous different concepts. The training data pairs images and these descriptions as queries and the corresponding bounding boxes as outputs. However, data generated in this way is insufficient to sim4 Figure 3. Our Pipeline for Data Collection. We first crop the target concept from the image based on the dataset annotations and then query Gemini [12] to generate its personalized description. We also apply data augmentation to diversify these cropped images. Then we combine them with the original image to derive series of instructions and answers from Gemini. When noise concepts are included in the additional information, the answer remains unchanged, helping to train the MLLMs ability to filter out irrelevant concepts. ulate the complexity of real-world recognition, especially when the target concept in the reference and input image is captured from different perspectives. To address this, we incorporate the ILSVRC2015-VID video object detection dataset [38], TAO [9] and CustomConcept101 [21] to enrich our dataset. For multi-object grounding, we use the Object365 dataset [39] to construct our training data. The second component of our dataset is designed for instruction following. This section includes training data for tasks such as image captioning, image description and question answering. For the image captioning and description data, we provide cropped images of target concepts, accompanied by their names and related information from the large-scale database, then query Gemini to generate caption or description that reflects the concepts depicted in the entire image. For question answering, we first design set of seed questions to serve as examples. These examples are used to prompt the annotator, Gemini, to generate new questions and corresponding answers. This iterative process facilitates the creation of rich and diverse collection of conversations that MLLMs can learn from. We construct such data using RefCOCO [20], Object365 [39], TAO [9] and CustomConcept101 [21] dataset. To enhance alignment with real-world scenarios, it is essential to collect data featuring the same identity in various environments. Thus, we also include multiple images about the same individual from the CelebA dataset [29] and produce question answering data about the individual. To further diversify the dataset, we apply image editing techniques for data augmentation. This includes performing random rotations and flips on the cropped images, as well as generating novel views of the concepts by diffusion models. Specifically, we use Inpaint-Anything [52] to separate the foreground from the background, and use Wonder3D [30] and SiTH [16] to synthesize novel views of foreground object or person respectively. Finally, we combine these elements to generate images of the target concept from different perspectives. To support multi-concept personalized generation, it is necessary to retrieve multiple potential concepts from the database. In the generation step, the MLLM must prioritize accurate and contextually relevant information. Considering that retrieval results can be inaccurate, potentially leading to unreasonable answers, we construct negative samples by adding noise concepts to the input while preserving the original output. This approach trains the models discrimination ability. By exposing the MLLM to both relevant and irrelevant information during training, it learns to discern and filter out noise concepts, thereby enhancing its robustness during inference. Additionally, we include subset of the LLaVA-Instruct-665k visual instruction dataset [27] to retain general knowledge from the original MLLM. Further details about our dataset can be found in Appendix D. 4. Experiment Implementation Details. We conduct experiments on LLaVA-1.5-13B [27] and Phi3-V-3.8B [35], resulting in two personalized MLLMs, RAP-LLaVA and RAP-Phi3V. We select YOLO-Worldv2 [7] as the detector and con5 Table 2. Qualitative Comparison on Image Captioning. Image examples of target concepts are shown in the left and captions generated are shown in the right. We use green text to denote correct target concepts."
        },
        {
            "title": "Caption",
            "content": "LLaVA: man is sitting at table with dog, and there are wine glasses and fork on the table. LLaVA-LoRA: collie dog looking pleased as she shares meal with her owner. MyVLM: my dog positioned on chair by black table, holding wine glass in her hand. white dog sits on the floor beside her. . . RAP-LLaVA(Ours): my dog is very good boy, and he loves to sit at table with his owner. They are enjoying meal. LLaVA: man and woman are standing in kitchen, preparing food together. The woman is cutting lettuce on cutting board, while the man watches her. There are several tomatoes . . . LLaVA-LoRA: and are preparing meal together. MyVLM: and her friend are looking very serious as they take in the scenery. RAP-LLaVA(Ours): is helping prepare salad in the kitchen. Phi3-V: group of stuffed animals, including blue one, are sitting on black surface. LLaVA-LoRA: B, and are happily exploring the grassland. MyVLM: and his crew are always ready to jump into new adventure. RAP-Phi3-V(Ours): is hanging out with and on the lawn. They are having great time playing! struct multimodal retriever using Facebook AI Similarity Search (FAISS) [18], employing pre-trained CLIP ViTL/14-336 [33] as the visual encoder. Due to the context length limitation of the backbone language model, for RAPLLaVA and RAP-Phi3-V, we retrieve the 2 and 3 different concepts with the highest similarity, respectively. More implementation details can be found in Appendix C. Training. In the training phase, we skip the recognition and retrieval procedures, instead perform instruction tuning to train the MLLMs. We adhere to most settings from the original experiment of LLaVA [27], except for using maximum learning rate of 1e-4 and training for 1 epoch. We employ low-rank adapters [17] to reduce the number of trainable parameters, and train our models on 8 A100 GPUs with valid batch size of 64. 4.1. Personalized Image Captioning In this section, we evaluate our models on generating personalized image captions with users specific concepts. We extend the dataset introduced by MyVLM [2] via adding 16 new concepts, including both objects and humans, forming 8 concept pairs that appear together. For each pair, there are 8-13 images used for testing. This multi-concept setting presents additional challenges for personalization. Settings. We compare our models with MyVLM [2] and fine-tuning based method LLaVA-LoRA [17]. For LLaVALoRA and MyVLM, the training dataset contains 1 image accompanied by 5 captions for each concept. For LLaVALoRA, we train it with captions of the training images for 3 epochs, applying low-rank adapters [17] and the same hyperparameters as our models. For MyVLM, following their training process, we first train the classification head with the positive and 150 negative images, then train the corresponding concept embedding with the provided captions for each concept. For our models, we construct database where each concept is represented by cropped image and text description. Details of our database could be found in Appendix G. All remaining images are used as test samples. This evaluation process is repeated three times with different seeds, resulting in total of 1,182 images used for evaluation, and we report the average results. Qualitative Comparison. In Table 2, we present image captions generated by different methods to make comparison. While LLaVA [27] and Phi3-V [35] generally provides brief and clear captions for most test images, its lack of understanding of the users specific concepts restricts it from 6 Figure 4. Performance under varying number of personalized concepts. Table 3. Quantitative Evaluation on Image Captioning. We report Recall, Precision and F1-score in the table, the best result in each metric is bold and the second is underlined."
        },
        {
            "title": "LLM",
            "content": "Recall Precision F1-score LLaVA [27] + Retriever Vicuna-13B 1.260 48.76 LLaVA-LoRA [17] MyVLM-LLaVA [2] Vicuna-13B 82.97 Vicuna-13B 84.65 RAP-LLaVA RAP-Phi3-V Vicuna-13B 93.51 Phi3-V-3.8B 88.14 93.28 86.37 96.47 95.10 2.450 87.82 85.50 94.97 91. Table 4. Quantitative Evaluation on Question Answering and Visual Recognition. The best result in each setting is bold and the second is underlined. Evaluation results of GPT-4V [32] are also provided as reference. Weighted results are computed as arithmetic means."
        },
        {
            "title": "LLM",
            "content": "Train #Image"
        },
        {
            "title": "Visual Recognition",
            "content": "Visual Text-only Weighted Positive Negative Weighted GPT-4V [32] + Prompt GPT-4V [32] + Prompt GPT-4V GPT-4V LLaVA [27] Vicuna-13B LLaVA [27] + Retriever Vicuna-13B LLaVA-LoRA [17] LLaVA-LoRA [17] MyVLM-LLaVA [2] YoLLaVA [31] RAP-LLaVA(Ours) RAP-Phi3-V(Ours) Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Vicuna-13B Phi3-V-3.8B 1 5 - 1 1 5 5 5 1 1 0.866 0.887 0.899 0. 0.900 0.935 0.912 0.929 0.935 0.941 0.982 0.987 0.659 0.863 0.583 0.615 - 0.883 0.938 0. 0.924 0.937 0.779 0.887 0.741 0.775 - 0.906 0.936 0.896 0.809 0.851 0.000 1. 0.988 0.997 0.994 0.949 0.979 0.922 0.992 0.998 1.000 0.025 0.662 0.444 0.845 0.898 0.982 0. 0.901 0.925 0.500 0.513 0.825 0.721 0.919 0.924 0.980 0.955 generating more personalized caption. LLaVA-LoRA and MyVLM can generate personalized captions, however, the limited training data often results in imprecise outputs, particularly noticeable when multiple concepts are present in the same image. In contrast, our models produce clear and accurate captions based on the database content, which also ensures the reliability of the outputs. Additional examples of personalized captions generated by the models could be found in Appendix E. Quantitative Evaluation. We employ recall, precision and the comprehensive metric F1-score as our evaluation metrics. Recall is calculated as the percentage of correct occurrences of target concepts, while precision is the ratio of correct concept names to the total number of concept names presented. The experimental results are shown in Table 3. Notably, the classification heads of MyVLM exhibit higher error rates when the number of positive images is limited, leading to weaker performance. Even with additional concept information provided through retrieval, the vanilla LLaVA [27] still fails to effectively accomplish the personalized generation task, underscoring the necessity of the proposed dataset. Our models demonstrate superior performance in both recall and precision metrics, highlighting the advantages of our RAP-MLLMs in data efficiency. Influence of Number of Learned Concepts. In real-world scenario, users personal databases typically expand over time. Next, we evaluate the performance of various methods with varying numbers of learned concepts. We extend the database with hundreds of new concepts selected from RefCOCO dataset [20], ensuring no overlap with the test dataset. For LLaVA-LoRA and MyVLM, we provide images containing the target concepts along with their captions as training data, and we assess the models performance on the original test dataset. The results are presented in Figure 4. More learned concepts result in increased recognition errors, leading to decline in performance for each model. Our RAP-MLLMs maintain the highest performance under different settings. 4.2. Personalized Question Answering Settings. In this section, we evaluate different methods on the benchmark of personalized question answering introduced by YoLLaVA [31], which contains both visual and text-only questions about user concepts. For each concept, we generate description to serve as its information in our database. For LLaVA-LoRA, we feed these descriptions 7 and corresponding images to train the model to describe the concepts properties. Additionally, we incorporate text-only queries and answers to enhance the models textual understanding. The training dataset for YoLLaVA and MyVLM consists of 5 positive images with question answering pairs and 200 negative images per concept. For GPT-4V [32], images and related concept information are provided as supplementary prompt. The evaluation metric is accuracy. Additional details are provided in Appendix C. Results and Analysis. The experimental results are provided in Table 4. LLaVA and LLaVA-LoRA both perform well in visual based question answering, because substantial information of the target concept can be obtained from the images. However, their performance is quite poor when images of the target concept mentioned in the question are not available. MyVLM performs well in visual question answering but does not support text-only question answering. YoLLaVA excels in text-only question answering, but its performance is still limited by the insufficient information provided by the learned tokens of concept. In contrast, our models demonstrate balanced performance in both visual and text-only question answering. By providing single image, our RAP-LLaVA surpasses baseline methods and achieves performance comparable to that of GPT-4V. We present additional qualitative results in Table 13 and 14. Visual Recognition. We also evaluate the models recognition abilities for more comprehensive comparison. The MLLMs are required to determine whether personal concept is present in an image. We query them with Is sks in the image? Answer with single word., where sks is replaced by corresponding concept name. For positive images, the desired response is Yes and No for negative. Results show that without knowledge of personal concepts, the vanilla LLaVA consistently outputs negative responses. After training on target concepts, LLaVA-LoRA, MyVLM and YoLLaVA tend to give positive responses, but struggle to differentiate between similar concepts, resulting in weaker performance on negative images. Our models demonstrate exceptional performance in both positive and negative scenarios, achieving the best overall results. We present additional examples in Table 19 and 20. 4.3. Cost of Personalization We further compare the costs of personalization. As shown in Table 1, existing methods usually struggle with continuous updates or have high demands for training data. For fine-tuning based method like LLaVA-LoRA, while they can achieve satisfactory performance, fine-tuning the model each time new concept emerges incurs substantial computational costs. MyVLM and YoLLaVA learn an embedding or some new tokens to represent the new concept without updating the pre-trained MLLMs parameters, however, they require multiple labeled images of the target concept Figure 5. Performance of Our Retriever. Top-K recall rates under varying database size N. Table 5. We evaluate models performance with perfect retrieval, and test contributions of each dataset component."
        },
        {
            "title": "Precision",
            "content": "F1-score 96."
        },
        {
            "title": "93.51\nRAP-LLaVA\nSkip retrieval\n96.16 (+2.7) 100.0 (+3.5) 98.04 (+3.1)\n- Text information\n94.91 (+1.4) 88.66 (-7.8) 91.68 (-3.3)\n- Data augmentation 89.25 (-4.3) 98.01 (+1.5) 93.42 (-1.6)\n- Negative samples\n95.74 (+2.2) 58.21 (-38.3) 72.40 (-22.6)",
            "content": "94.97 and large number of negative images, which poses significant challenges for data collection. In contrast, our RAP requires only 1 image with its related information provided by the user, achieving outstanding performance across various personalized generation tasks. At the same time, by modifying images and descriptions in the database, RAP enables real-time editing of personalized generation settings. We present examples of real-time concept editing in Table 11. 4.4. Ablation Study Retriever. The recall rate of the retriever is crucial for RAG system. We first assess the retrievers performance on the personalized captioning dataset. We use the detection model to identify potential concepts and retrieve the concepts with the highest similarity from the database. The Top-K recall rates for varying values of and database sizes are presented in Figure 5. Results indicate that as the database size increases, the retrievers performance declines, while larger generally enhances the recall rate. Notably, even with 500 personal concepts to remember, the Top-5 recall rate is still able to surpass 90%, which guarantees the effectiveness of our RAP framework. Generation Ability of MLLM. We skip the recognition and retrieval processes, providing the MLLM with relevant information of each concept present in the image to evaluate the generation capability of the trained MLLM. The results, shown in Table 5, indicate that when relevant concept information is supplied, our RAP-LLaVA achieves superior generation performance, obtaining 100% precision without 8 outputting irrelevant concepts as well as higher recall rate. Text Information. We remove additional text information to examine its impact on personalization. The results are shown in Table 5 and 7. This additional text provides extra information that helps the model understand specific concepts, effectively improving the accuracy of the answers and benefiting personalized generation. Furthermore, it is especially helpful for question answering, as it can provide information not directly obtainable from the image, such as users age and hobbies, or pets preferred food, as illustrated in the example in Table 11. Dataset Composition. We conduct experiments to assess contribution of each component in our dataset. First, we remove data generated through data augmentation and train the original LLaVA. The results displayed in Table 5 indicate obvious decrease in the recall metric for image captioning, resulting in lower overall performance. We further exclude constructed negative samples from the dataset and retrain the model, then we find that it performs poorly on precision metric. This suggests diminished ability to discriminate against noisy concepts not present in the image. 5. Conclusion In this paper, we introduce the RAP framework for personalizing MLLMs. This framework enables MLLMs to understand infinite user-specific concepts, generate personalized captions and respond to user-related queries. To enhance the quality of the generated content and better align outputs with users configuration, we curate large-scale dataset for personalized training of MLLMs and train series of MLLMs to function as personalized assistants. Experimental results show that RAP-MLLMs achieve exceptional performance in various personalized generation tasks, while allowing real-time adjustments to generation settings."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 2 [2] Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. arXiv preprint arXiv:2403.14599, 2024. 2, 3, 6, 7 [3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. 3 [4] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Muller, and Bjorn Ommer. Retrieval-augmented diffusion models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 3 [5] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. 3 [6] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models answer viarXiv preprint sual arXiv:2302.11713, 2023. information-seeking questions? [7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, XingYolo-world: Real-time gang Wang, and Ying Shan. In Proceedings of the open-vocabulary object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. 4, 5, 12 [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 2 [9] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: large-scale benchIn Computer VisionECCV mark for tracking any object. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part 16, pages 436454. Springer, 2020. 5, 14 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [11] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. 3 [12] Gemini-Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 3, 4, [13] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13796 13806, 2024. 3 [14] Cusuh Ham, Matthew Fisher, James Hays, Nicholas Kolkin, Yuchen Liu, Richard Zhang, and Tobias Hinz. Personalized residuals for concept-driven text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81868195, 2024. 3 [15] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26584 26595, 2024. 2, 3 9 [16] Ho, Jie Song, Otmar Hilliges, et al. Sith: Single-view textured human reconstruction with image-conditioned diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 538549, 2024. 5 [17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 6, 7, 12 [18] Jeff Johnson, Matthijs Douze, and Herve Jegou. BillionIEEE Trans. Big Data, scale similarity search with gpus. 7(3):535547, 2021. 6, 12 [19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 67696781. Association for Computational Linguistics, 2020. 3 [20] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara L. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, meeting of SIGDAT, Special Interest Group of the ACL, pages 787798. ACL, 2014. 4, 5, 7, 14 [21] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3, 5, 14 [22] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. 3 [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [24] Jiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, and Hideki Nakayama. Evcap: Retrieval-augmented image captioning with external visual-name memory for open-world compreIn Proceedings of the IEEE/CVF Conference on hension. Computer Vision and Pattern Recognition, pages 13733 13742, 2024. 3 [25] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 3 [26] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv preprint arXiv:2310.01352, 2023. 3 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 3, 5, 6, 7, 12, 13, 14 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 2, 3, 4 [29] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 5, 14 [30] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 5 [31] Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. arXiv preprint arXiv:2406.09400, 2024. 2, 3, 7, 13 [32] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 7, 8 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pages 8748 8763. PMLR, 2021. 4, 6, 12 [34] Rita Ramos, Desmond Elliott, and Bruno Martins. arXiv preprint Retrieval-augmented image captioning. arXiv:2302.08268, 2023. [35] Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. 2, 5, 6 [36] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 4 [37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In IEEE/CVF Conference on Computer Vigeneration. sion and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 2250022510. IEEE, 2023. 3 [38] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 5, 14 [39] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In 2019 IEEE/CVF International Conference on Computer Vision, 10 [53] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. [54] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 364 373. IEEE, 2023. 3 [55] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 2, 3 [56] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 2 [57] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on regionof-interest. arXiv preprint arXiv:2307.03601, 2023. 3 [58] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. Retrieving multimodal arXiv information for augmented generation: survey. preprint arXiv:2303.10868, 2023. 3 [59] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 1 [60] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 84298438. IEEE, 2019. 5, 14 [40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. 3 [41] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. Eragent: Enhancing retrieval-augmented language models with improved accuracy, efficiency, and personalization. arXiv preprint arXiv:2405.06683, 2024. 3 [42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. 2 [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [44] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. 2, [45] Stanisław Wozniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, and Jan Kocon. Personalized large language models. arXiv preprint arXiv:2402.09269, 2024. 3 [46] Yihan Wu, Ruihua Song, Xu Chen, Hao Jiang, Zhao Cao, and Jin Yu. Understanding human preferences: Towards In Proceedmore personalized video to text generation. ings of the ACM on Web Conference 2024, pages 39523963, 2024. 3 [47] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025, 2023. 3 [48] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [49] Chun-Hsiao Yeh, Bryan Russell, Josef Sivic, Fabian Caba Heilbron, and Simon Jenni. Meta-personalizing visionlanguage models to find named instances in video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1912319132, 2023. 3 [50] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 2 [51] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558, 2023. 3 [52] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Inpaint anything: Jin, Wenjun Zeng, and Zhibo Chen. Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023. 5 11 A. Appendix Overview Section B: Additional evaluations of our models. Section C: More experimental details. Section D: More details of RAP dataset. Section E: Additional demonstrations. Section F: Analysis on limitations of our work. Section G: Examples of the personalized database. B. Additional Evaluation Results B.1. Ablation Study Dataset Composition. We conduct ablation experiments on the question answering and recognition benchmark, experimental results are present in Table 7. The results further demonstrate that our data augmentation and the constructed negative samples also contribute to the models performance. Table 6. Benchmarks. KB: Knowledge Base. Evaluation on Knowledge-intensive Multimodal Method MMMU [53] InfoSeek [6] LLaVA [27] LLaVA-LoRA [17] RAP-LLaVA RAP-LLaVA(With KB) 0.364 0.359 0.361 0.369 0.205 0.205 0.218 0.344 Multimodal Benchmark. We also evaluate our models performance on several traditional multimodal benchmarks, including MMMU [53] and InfoSeek [6]. We assess our models performance both with and without external knowledge base. For MMMU [53], we use 30K images paired with corresponding captions from Wikipedia as the external knowledge base. During testing, we retrieve the three most similar images based on the questions image and incorporate only the textual knowledge to the input. For InfoSeek [6], we randomly sample 5K questions from the validation set and construct knowledge base containing 50K entities from Wikipedia database provided by the authors, which includes all relevant entities associated with the questions. For each question, we retrieve the most similar entity and add only the textual knowledge to the input. We evaluate on the validation set of MMMU, and 5K questions sampled from the validation set of InfoSeek. We use the official scripts to get the results, which are presented in Table 6. From the results, our RAP-LLaVA retains most general knowledge of the original LLaVA [27]. It also equips the MLLM with the ability to retrieve information from an external knowledge base, demonstrating superior performance in knowledge intensive tasks. Figure 6. Time Cost of Personalization. We conduct experiment with 2 A800 GPUs. B.2. Time Cost of Personalization We also evaluate the time cost associated with different methods for learning set of users concepts. The results are presented in Figure 6. MyVLM has to train an external recognition model for each concept and learn an embedding to adjust the models outputs. Similarly, YoLLaVA needs to learn new tokens for each concept. During the optimization process, both approaches necessitate multiple forward and backward pass of the MLLM, resulting in significant time consumption. In contrast, our RAP only requires time for encoding the image and adding its embedding to the database, which can be accomplished in just few seconds. This significantly enhances the convenience and practicality of our models in practical applications. B.3. Standard Image Captioning Metrics We further evaluate the models using standard image captioning metrics. The results are shown in Table 8. Despite the additional concept information is offered, LLaVAs performance declines due to its inability to effectively utilize the information. In contrast, RAP-LLaVA achieves comparable performance to LLaVA on general image captioning tasks while enabling personalization. C. More Experimental Details Implementation details. We utilize YOLO-Worldv2-X [7] as the detection model, setting detection classes to include all categories stored in the database to reduce the interventions from unrelated objects. We construct multimodal retriever using Facebook AI Similarity Search (FAISS) [18], employing pre-trained CLIP ViT-L/14-336 [33] as the visual encoder. Each key in the database is generated by inputting concepts image into the CLIP visual encoder, resulting in 768-dimensional vector. Considering the restriction of context length of the backbone language model, we retrieve the 2 most similar images from the database for each region of interest. Then we select 2 and 3 different concepts with the highest similarity among all as supplementary inputs for RAP-LLaVA and RAP-Phi3-V, respec12 Table 7. Ablation studies on Question Answering and Visual Recognition. Weighted results are computed as arithmetic means."
        },
        {
            "title": "Weighted",
            "content": "RAP-LLaVA - Data augmentation - Negative samples - Text information 0.935 0.924 (-0.011) 0.918 (-0.017) 0.935 0.938 0.918 (-0.020) 0.933 (-0.005) 0.908 (-0.030) 0.936 0.921 (-0.015) 0.925 (-0.011) 0.921 (-0.015) 0.979 0.943 (-0.036) 0.958 (-0.021) 0.988 (+0.009) 0.982 0.988 (+0.006) 0.985 (+0.003) 0.930 (-0.052) 0.980 0.965 (-0.015) 0.971 (-0.009) 0.959 (-0.021) Table 8. Quantitative Evaluation with Standard Image Captioning Metrics. We compute standard image captioning metrics for personalized captions generated by RAP-LLaVA. For each image, we treat all 5 augmented captions as the set of ground truth captions. The results are averaged across all three evaluations and all concepts. Model LLaVA [27] LLaVA [27] + Retriever RAP-LLaVA B1 0.177 0.087 0. B2 0.100 0.038 0.094 B3 0.057 0.014 0.058 B4 METEOR ROUGE CIDEr SPICE 0.032 0.005 0.037 0.138 0.074 0.215 0.240 0.132 0. 0.428 0.025 0.388 0.136 0.020 0.108 tively. Baselines. For MyVLM, we find that when the training data is very limited, it is quite hard for the classification head to work effectively. Therefore, we use data augmentation to help improve its performance. Specifically, we crop the single image into several pieces containing the target concept to improve the accuracy of classification heads. To distinguish between multiple possible different concepts that may appear in the image, we use sks1, sks2. . . as concept identifiers. For YoLLaVA, we present its experimental results as reported in the original paper [31]. D. Details of Dataset D.1. Dataset Composition We provide summary of the composition of our dataset in Figure 7, which visually represents the distribution of different components. Table 9 presents detailed numerical data for each part. In Table 10, we specify the sources for each component of our dataset. D.2. Instructions In this section, we present the instruction templates used to create our dataset: E. Additional Demonstrations In this section, we provide more qualitative results obtained by various models. In Table 11, we demonstrate how our models achieve real-time editing of concepts by modifying the database. In Table 12, we demonstrate the real-time addition of new concepts by updating the database. In Table 13, we present qualitative results on personalized conversation of RAP-LLaVA. In Table 14, we present qualitative results on personalized conversation of RAP-Phi3-V. In Table 15, we present additional image captions generated by RAP-LLaVA and other methods. In Table 16, we present additional image captions generated by RAP-Phi3-V and other methods. In Table 17, we provide demonstrations of image description generated by RAP-LLaVA and LLaVA. In Table 18, we provide demonstrations of image description generated by RAP-Phi3-V and Phi3-V. In Table 19 and 20, we provide results on visual recognition of RAP-LLaVA. It also has the ability to give precise bounding box of specific concept in the image. Table 21 contains instructions for visual grounding and F. Limitation recognition. Table 22 includes example instructions for image captioning. Table 23 presents example instructions for image description. Table 24 presents example questions used for question answering synthesis. Our proposed RAP framework is retrieval-based method. The limitations of RAP mainly concern the additional computational cost of generation and the precision of the retriever. While incorporating external information effectively generates more specific answers, it inevitably increases the context length for MLLMs, leading to additional computational overhead during the generation process. We 13 Figure 7. Composition of our dataset. Table 9. Statistics of our dataset."
        },
        {
            "title": "Type",
            "content": "Visual Grounding Recognition Caption & Description Question Answering LLaVA-Instruction Total"
        },
        {
            "title": "Size",
            "content": "100K 40K 37K 16K 67K 260K Table 10. Data source. Type Source Dataset Visual Grounding RefCOCO [20], TAO [9] ILSVRC2015-VID [38], Object365 [39] Recognition CustomConcept101 [21], CelebA [29] Caption & Description RefCOCO [20], TAO [9] Object365 [39], CustomConcept101 [21] Question Answering RefCOCO [20], TAO [9] Object365 [39], CustomConcept101 [21] CelebA [29] LLaVA-Instruction LLaVA-Instruct-665K [27] will further explore ways to mitigate this computational burden. Another limitation is the personalization performance of our RAP-MLLMs depends on the retrievers capability This proposes need for robust multi-modal retriever that can discern intricate features to enhance retrieval precision. Despite these limitations, RAP offers timely solution for MLLM personalization. By retrieving from users specific database, RAP facilitates reliable and flexible personalized generation, which is valuable in practical applications. G. Examples of the Personalized Database We give some visualized examples of our database in Table 25. For each concept in the database, users need to provide an image with its name and optional text information. During inference, the images, names and other information of retrieved concepts are integrated into the input for the MLLM. Users have the flexibility to define the name and personalized description based on their preferences, and our RAP-MLLMs will generate answers according to the provided information. 14 Table 11. Examples of Concept Editing. Based on the information recorded in the database, our RAP-LLaVA can provide reliable and accurate answers."
        },
        {
            "title": "Information in the Database",
            "content": "my dog 1. white and gray dog with long fur. He has black eyes. His favorite food is chicken. 2. white and gray dog with long fur. He has black eyes. His favorite food is beef. Question: What is my dogs favorite food? 1. RAP-LLaVA: Chicken. 2. RAP-LLaVA: His favorite food is beef. Table 12. Examples of Concept Updating. The first caption is generated when toy2 not yet stored in the database. Once the new concept is added, RAP-LLaVA can recognize both toy1 and toy2. Image Concept Information in the Database toy1 plush toy. It is orange with yellow belly and brown nose. toy This is plush toy of the bluey character. It is light blue color with purple patch on its head, and its ears are yellow. Question: Give caption of this image. 1. RAP-LLaVA: toy1 is ready for bed! Hes snuggled up with his friend, blue and yellow dog plushie. Theyre both looking so cozy and ready for good nights sleep! (Add new concept toy2 to the database) 2. RAP-LLaVA: toy1 and toy2 are ready for bed! They are both snuggled under the covers with their favorite stuffed animals. 15 Table 13. Examples of personalized conversations obtained by RAP-LLaVA."
        },
        {
            "title": "Information in the Database",
            "content": "my cat tabby cat with its eyes closed is laying on light grey surface. The cats fur is mix of light and dark brown stripes. Question: Where is my cat? RAP-LLaVA: my cat is on staircase. Question: What is the type of my cat? RAP-LLaVA: my cat is tabby cat. Question: What is the color of my cat? RAP-LLaVA: my cat is tabby cat with mix of light and dark brown stripes. Image Concept Information in the Database A is smiling Asian man with short black hair, wearing glasses and grey sweater. Question: Can you see in this photo? RAP-LLaVA: Yes, is captured in this image. Question: What color is As shirt? RAP-LLaVA: is wearing purple shirt. Question: What is As right hand holding? RAP-LLaVA: Chopsticks. Image Concept Information in the Database dug dug is golden retriever from the movie Up. He has brown coat and big, goofy smile, wearing collar with some round objects. (Text-only conversation) Question: Can you describe dug in detail? RAP-LLaVA: dug is golden retriever from the movie Up. He has brown coat and big, goofy smile. He is wearing collar with some round objects. He looks very happy and playful. Image Concept Information in the Database is bald man with red beard. (Text-only conversation) Question: Can you describe Ps appearence in detail? RAP-LLaVA: is bald man with red beard. He is wearing orange shorts and blue shirt. He is sitting in black directors chair with his legs crossed. Table 14. Examples of personalized conversations obtained by RAP-Phi3-V."
        },
        {
            "title": "Information in the Database",
            "content": "bo bo is well-groomed, medium-sized Shiba Inu with thick, cinnamoncolored coat, cream accents, alert eyes, and black collar. Question: What is bo doing? RAP-Phi3-V: bo is lying on its side on fluffy, gray dog bed. It is resting its head on white stuffed toy. bo has thick, cinnamon-colored coat with cream accents, alert eyes, and black collar. bo looks very relaxed and content. Question: Where do you think he likes to play? RAP-Phi3-V: Its hard to say for sure, but its possible that bo enjoys going to the park. Many dogs like to go to the park to play and get exercise. Image Concept Information in the Database Anya young girl with pink hair and big green eyes. Bond white dog wearing black bow tie. Question: What are they doing, describe in detail. RAP-Phi3-V: Anya is feeding Bond, white dog wearing black bow tie. Bond is eating out of bowl. Anya is kneeling down and watching Bond eat. Bond is standing on tiled floor and looking at the bowl. Anya is wearing pink sweater and has pink hair. Bond is wearing black bow tie. The scene is indoors, and there is TV in the background. Question: What is Anya wearing? RAP-Phi3-V: Anya is wearing black sweater. Image Concept Information in the Database is young woman with long blonde hair, wearing white sleeveless shirt. She has bright red lipstick and light makeup. (Text-only conversation) Question: What color is Cs hair? RAP-Phi3-V: Cs hair is blonde. Question: What color of clothes does she like? RAP-Phi3-V: She likes white clothes. 17 Table 15. Additional qualitative comparison on image captioning between RAP-LLaVA and other methods."
        },
        {
            "title": "Caption",
            "content": "LLaVA: blue coffee cup sits next to two dolls on window sill. LLaVA-LoRA: doll* positioned on windowsill with blue mug containing pencils MyVLM: doll* is found sitting beside window ledge with cup and toothbrushes nearby. In the image, doll* appears to be peacefully resting, possibly enjoying the view or taking break. RAP-LLaVA(Ours): doll* and her friend are ready for fun day at the office! Theyre ready to take on any challenge that comes their way. LLaVA: wooden shelf holds several potted plants, including large clay pot and small vase. The shelf is positioned near wall, and the plants are arranged in visually appealing manner. LLaVA-LoRA: mug* on wooden shelf next to plant and two potted plants. MyVLM: mug* on blue and white tiled floor next to indoor plants, shelf with pots and vase. RAP-LLaVA(Ours): brown ceramic tiki mug with the face of tiki head, mug*, sits on wooden shelf with two potted plants. LLaVA: cat is sitting on ledge near staircase. LLaVA-LoRA: my cat sitting on staircase, looking down. MyVLM: my cat in typical pose, gripping the banister of staircase. He is wearing collar. RAP-LLaVA(Ours): my cat is relaxing on the stairs. Look at those eyes! What beautiful cat. LLaVA: cat is laying on blanket on couch, next to colorful teapot. LLaVA-LoRA: my cat reclining on chair with teapot* beside MyVLM: teapot* on the couch near an orange, yellow, and blue teapot. The cat is laying on blanket, and the teapot is placed on table. RAP-LLaVA(Ours): my cat is taking well-deserved nap next to teapot*. LLaVA: man and woman are walking down street, with the man taking selfie using his cell phone. They are both smiling as they walk, and the man is holding his phone . . . LLaVA-LoRA: and are looking happy as they walk down the street together. MyVLM: and are looking very serious as they take in the scenery. RAP-LLaVA(Ours): is walking down the street with her friend H. LLaVA: Two colorful parrots are perched on palm tree, sitting next to each other. LLaVA-LoRA: parrot1 and parrot2 are looking very serious as they perch on palm frond. MyVLM: parrot1 and parrot2 look so cute together. RAP-LLaVA(Ours): parrot1 and parrot2 are both beautiful birds. parrot1 has bright red beak and parrot2 has light blue head and pink feet. They are both perched on branch. 18 Table 16. Additional qualitative comparison on image captioning between RAP-Phi3-V and other methods."
        },
        {
            "title": "Caption",
            "content": "Phi3-V: small white sheep figurine is sitting on wooden table next to plant. LLaVA-LoRA: sheep* positioned next to potted plant on wooden table. MyVLM: sheep* positioned within square shape within vase made of metal positioned on table made of wood. RAP-Phi3-V(Ours): sheep* is ready to play on the shelf with the plant! Phi3-V: brown and white dog is sitting on the grass with leash attached to it. LLaVA-LoRA: dog1 sitting on the grass with look of determination. MyVLM: on cheerful note, dog1 restfully resides, surrounded by the soothing sounds of nature. RAP-Phi3-V(Ours): This is picture of dog1, cute dog with light brown and white fur. He looks happy and playful. Phi3-V: bottle of wine is displayed next to wooden cow statue. LLaVA-LoRA: bull* positioned next to bottle of supreme cabernet sauvignon. MyVLM: gold pineapple pars the bottle of wine. bottle of wine sits next to bottle of wine. RAP-Phi3-V(Ours): bull* stands guard over bottle of SUPREME wine. Phi3-V: small figurine of basketball player is placed on top of box. LLaVA-LoRA: funko pop* sitting on desk next to box and monitor. MyVLM: funko pop* stands out in photo set against dark background. RAP-Phi3-V(Ours): funko pop* is ready to dominate the court! Phi3-V: man and woman are sitting at table in restaurant, having conversation and enjoying their time together. LLaVA-LoRA: and enjoying meal together. MyVLM: and enjoying their time together. RAP-Phi3-V(Ours): and are enjoying drink at bar. Phi3-V: small statue of man wearing glasses is sitting next to cup on table. LLaVA-LoRA: table hosts cup* and sculpture of head wearing glasses. MyVLM: table with cup*, an espresso cup, and ceramic mug depicting face wearing glasses sits on wooden surface. RAP-Phi3-V(Ours): head* and cup* are ready for their morning coffee. 19 Table 17. Qualitative results of personalized image description obtained by RAP-LLaVA."
        },
        {
            "title": "Description",
            "content": "LLaVA: The image features large, fluffy cat sitting on countertop, possibly in kitchen. The cat is positioned near sink and appears to be resting or sleeping. The countertop is covered with towel, providing comfortable surface for the cat to lay on . . . RAP-LLaVA(Ours): The image shows my cat, tabby cat with its eyes closed, laying on light grey surface. The cats fur is mix of light and dark brown stripes. The cat is relaxed and comfortable. LLaVA: The image features small metal robot figure, possibly toy, standing on table or counter. The robot is positioned in the center of the scene, surrounded by variety of leaves scattered around it. The leaves are of different sizes and . . . RAP-LLaVA(Ours): The image shows toy figurine of Iron Man, superhero from Marvel comics. He is wearing silver and red suit of armor and is standing on surface covered in leaves. The toy is called toy3. LLaVA: The image features chair with pillow on it, which has picture of sheep on it. The chair is positioned in front of desk with laptop and monitor on it. The laptop is placed on the left side of the desk, while the monitor is on the right side . . . RAP-LLaVA(Ours): The image shows chair with pillow* on it. The pillow* is pattern of flowers, leaves, and sheep wearing glasses with flower crown. There is also laptop and computer monitor in the background. LLaVA: The image features bookshelf filled with various books. On the shelf, there are two Rubiks cubes, one green and one white, sitting next to each other. The bookshelf is well-stocked with books, occupying most of . . . RAP-LLaVA(Ours): The image shows bookshelf with books on it. There are two toys on the shelf. One is toy4, which looks like cat with orange fuzzy ears and brown cord attached. The other is Rubiks cube. LLaVA: The image features bed with two stuffed animals, dog and cat, lying on it. The dog is positioned on the left side of the bed, while the cat is on the right side. The stuffed animals are placed close to each other, giving the impression of cozy and . . . RAP-LLaVA(Ours): The image shows two plush toys, toy1 and toy2, lying in bed together. toy2 is light blue plush with purple patch on its head and yellow ears. toy1 is an orange plush with yellow belly and brown nose. They are both looking at the camera. LLaVA: The image features young girl hugging large white dog in room. The girl is wearing black dress and appears to be enjoying her time with the dog. The dog is positioned close to the girl, and they seem to be sharing tender moment together . . . RAP-LLaVA(Ours): Anya is young girl with pink hair and big green eyes. She is wearing black dress and black shoes. She is standing in room with wooden floors and is hugging white dog named Bond. Bond is wearing black bow tie. The dog is standing on its hind legs and is looking at the camera. The room is decorated with blue rug and wooden cabinet . . . 20 Table 18. Qualitative results of personalized image description obtained by RAP-Phi3-V."
        },
        {
            "title": "Description",
            "content": "Phi3-V: The image features display of various animal figurines on shelf. There are three giraffes, two of which are standing and one sitting, placed next to each other. sheep figurine is also present on the shelf, located between the giraffes . . . RAP-Phi3-V(Ours): The image shows white sheep* sitting on shelf next to giraffe figurine. The sheep* is made of felt and has black eyes and black nose. The giraffe figurine is made of metal and has colorful necklace around its neck. The shelf is white . . . Phi3-V: The image features wooden shelf with two wine bottles and two wine glasses. One of the wine bottles is positioned towards the left side of the shelf, while the other is located more towards the center. The wine glasses are placed on the right side of the . . . RAP-Phi3-V(Ours): The image shows wooden shelf with three glassware items on it. The top glassware item is mug* with tiki head design. The other glassware items are wine glasses. There is sign on the wine bottle that says SUPREME. There is also plant in pot in the background. Phi3-V: The image features two dogs playing together in grassy yard. One dog is located on the left side of the yard, while the other dog is on the right side. They are both running and enjoying their time outside. In the background, there are several chairs . . . RAP-Phi3-V(Ours): The image shows two dogs playing in grassy backyard. The dog on the left is white and gray dog with long fur. The dog on the right is black and brown dog with long fur. The dogs are both running and playing on the grass. The dog on the left is named my dog. Phi3-V: The image features white shelf with several decorative items on it. There are three chicken-shaped decorations hanging from the shelf, each with different design. Additionally, there are three more chicken decorations placed on the shelf, making . . . RAP-Phi3-V(Ours): The image shows three handmade stuffed toys. The toys are all shaped like chickens and are hanging on white shelf. The toys are all different colors and have different designs on them. The toy on the left is called toy4. Phi3-V: The image features group of three cartoon characters, two of which are wearing hats. They are standing together on sidewalk, with one character positioned towards the left side of the frame, another in the center, and . . . RAP-Phi3-V(Ours): The image shows character1 and character2 standing in front of brick wall. character1 is cartoon fox wearing yellow button-down shirt and purple tie. He has big eyes, fluffy fur, and sly grin on his face. character2 is cartoon rabbit with large ears, police officers hat, and red and yellow safety vest. She has sweet, innocent look on . . . Phi3-V: The image features baby lying on bed, with dog resting on the floor nearby. The baby appears to be playing with toy, possibly puzzle, while the dog watches attentively . . . RAP-Phi3-V(Ours): The image shows baby, baby*, sleeping in carrier. The baby is wrapped in colorful blanket and is secured by black strap. french bulldog, dog2, is sleeping next to the baby. The baby and dog are both sleeping peacefully. 21 Table 19. Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We use green rectangle to show the bounding box in the image."
        },
        {
            "title": "Conversation",
            "content": "Question: Is head* in this image? RAP-LLaVA: Yes, head* is in the photo. Question: Give head*s bounding box in the image. RAP-LLaVA: [0.57, 0.67, 0.68, 0.84] Question: Is head* in this image? RAP-LLaVA: No, head* is not part of this photograph. Question: Can you identify if toy* is captured in this picture? RAP-LLaVA: Yes, toy* is visible in this picture. Question: Please provide toy*s bounding coordinates in the image. RAP-LLaVA: [0.42, 0.19, 0.54, 0.39] Question: Can you identify if toy* is captured in this picture? RAP-LLaVA: toy* is not captured in this image. Question: Can you confirm if appears in this photo? RAP-LLaVA: Yes, is in this photo. Question: Describe As position in the image. RAP-LLaVA: [0.24, 0.12, 0.72, 0.98] Question: Can you confirm if appears in this photo? RAP-LLaVA: is not present in this photograph. 22 Table 20. Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We use green rectangle to show the bounding box in the image."
        },
        {
            "title": "Conversation",
            "content": "Question: Is statue1 included in this shot? RAP-LLaVA: Yes, statue1 is included in this shot. Question: Give statue1s bounding box in the image. RAP-LLaVA: [0.36, 0.16, 0.59, 0.41] Question: Is statue1 included in this shot? RAP-LLaVA: statue1 is not captured in this picture. Question: Is my dog captured in this image? RAP-LLaVA: Yes, recognize my dog in this picture. Question: Specify my dogs bounding box in the photograph. RAP-LLaVA: [0.33, 0.23, 1.0, 0.69] Question: Is my dog captured in this image? RAP-LLaVA: No, my dog is not included in this shot. Question: Is teapot* visible in this image? RAP-LLaVA: Yes, teapot* is visible in this picture. Question: Describe teapot*s position in the image. RAP-LLaVA: [0.52, 0.69, 0.73, 0.91] Question: Is teapot* visible in this image? RAP-LLaVA: No, teapot* is not visible in this picture. 23 (cid:19) Table 21. Instructions for visual grounding and recognition. (cid:16) Visual recognition: Is visible in this picture? Is in this image? Do you see in the photo? Is present in this photograph? Can you identify if is captured in this picture? Is depicted in this image? Does the picture feature V? Can you confirm if appears in this photo? Is included in this shot? Is shown in this image? Can you tell if is part of this photograph? Is there any sign of in this picture? Can you detect in the photo? Is captured in this image? Do you recognize in this picture? Visual grounding: Give Vs bounding box in the image. Describe Vs position in the image. Please provide the coordinates of the bounding box for in the given image. Specify the rectangular boundaries of in the image. Give Vs position in the following image. Please provide Vs bounding coordinates in the image. Indicate the bounding box for in the image. Show the bounding box for in the picture. Specify Vs bounding box in the photograph. Mark Vs bounding box within the image. (cid:18) (cid:19) Image caption: Table 22. Instructions for image captioning. Give caption of the image. Give personalized caption of this image. Provide brief caption of the image. Summarize the visual content of the image. Create short caption of the image. Offer short and clear interpretation of the image. Describe the image concisely. Render concise summary of the photo. Provide caption of the given image. Can you provide personalized caption of this photo? Could you describe this image concisely? (cid:18) 24 (cid:17) (cid:16) (cid:17) (cid:19) Image description: Table 23. Instructions for image description. (cid:16) Describe the image. Give description of the image. Give description of the image in detail. Give short description of the image. Describe the image in detail. Please provide description of the image. Can you give me details about the image? Could you explain whats shown in the image? (cid:18) (cid:19) Table 24. Seed questions used for question answering synthesis. Person: What is Hs hair color? What is Hs height (estimated)? What is Hs skin tone? What is Hs eye color? What style of clothing is wearing? Does have any visible tattoos? Does wear glasses or contact lenses? Does have any facial hair? What is Hs approximate age? What is Hs build or body type? What is doing? Object: What color is O? What pattern is on O? What shape does have? What size is O? What is the texture of O? Is shiny or matte? What material is made of? Does have any patterns or designs on it? Is new or worn? Does have any visible brand or logo? Is functional or decorative? Multi-concept question: What do C1 and C2 have in common? What activity are C1 and C2 engaged in? Where could C1 and C2 be located? What is the most noticeable difference between C1 and C2? What are they doing? (cid:18) (cid:17) (cid:16) (cid:17) Table 25. Examples of our database. concept should be provided with an image and its personalized description."
        },
        {
            "title": "Anya",
            "content": "A young girl with pink hair and big green eyes. doll* This is cute figurine of girl wearing pink and blue dress, holding white bubble. toy1 plush toy. It is orange with yellow belly and brown nose. toy This is plush toy of the bluey character. It is light blue color with purple patch on its head, and its ears are yellow. statue* This is figurine of cat. The cat has blue body with yellow, red, and green stripes and long tail that is also striped. cat* small ginger kitten with bright blue eyes looks up at the camera. A young man is wearing plain tan t-shirt. His hair is short and curly. my dog white and gray dog with long fur. He has black eyes. young woman with blonde hair is wearing white tank top and blue jeans."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "MMLab, The Chinese University of Hong Kong",
        "National Key Laboratory for Novel Software Technology, Nanjing University"
    ]
}