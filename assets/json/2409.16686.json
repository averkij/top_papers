{
    "paper_title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
    "authors": [
        "Dayuan Fu",
        "Biqing Qi",
        "Yihuai Gao",
        "Che Jiang",
        "Guanting Dong",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs' planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios."
        },
        {
            "title": "Start",
            "content": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making Dayuan Fu1,2, Biqing Qi1,3*, Yihuai Gao4, Che Jiang1, Guanting Dong2, Bowen Zhou1,3* 1Department of Electronic Engineering, Tsinghua University 2 Beijing University of Posts and Telecommunications, Beijing, China 3Shanghai AI Laboratory 4Stanford University fdy@bupt.edu.cn zhoubowen@tsinghua.edu.cn 4 2 0 6 8 6 6 1 . 9 0 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-term memory is significant for agents, in which insights play crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce MultiScale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging three-part pipeline, MSI can generate task-specific and high-level insight, store it in database, and then use relevant insight from it to aid in decisionmaking. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domainshifting scenarios."
        },
        {
            "title": "Introduction",
            "content": "Creating agents that can make autonomous decisions in the environment has always been promising and interesting research direction. (SignificantGravitas, 2023; Sun et al., 2023) With the emergence of ChatGPT and GPT-4 (Achiam et al., 2023), large language models (LLMs) have transformed from specialized models to general model that can complete multiple types of tasks, hence it can make decisions for agents. (Xi et al., 2023; Yang et al., 2024; Wang et al., 2023b). This type of agent will transform multi-modal information It into natural language as short-term memory. *Corresponding authors. Yihuai Gao participated in this project when he was an undergrad in Tsinghua University Figure 1: Example of insight summarizing and utilizing. MSI will summarize the insights in multi-scale and utilize insights by selecting based on the task. DB=Database. then prompts large language models with shortterm memory and long-term memory to plan and make decisions. With these capabilities, the agent can generate series of actions that are executable within given environment. (Yao et al., 2023; Park et al., 2023; Gao et al., 2023; Zheng et al., 2023) Insight1, as form of long-term memory, has gradually become crucial part of guiding LLM planning and decision-making. (Shinn et al., 2023; Zhao et al., 2023; Fu et al., 2024; Wang et al., 2023a; Xi et al., 2023; Zeng et al., 2024). Relative to other long-term memory such as examples, insight is more concise and higher-level. Although previous work has proposed method of using LLM to summarize and utilize insights (Zhao et al., 2023), it either provides LLM with too many irrelevant insights or can not summarize the high1In this paper, \"insight\" refers to \"the knowledge acquired through multiple observations of facts or events\" ously, the multi-scale insights generation provides nuanced understanding at various levels, addressing the challenge of high-level insights summarization. As result, MSI stands as robust solution, offering contextual and comprehensive insights tailored to enhance decision-making capabilities. In summary, our contributions are as follows: (1) We proposed MSI, an embodied agent that can create and utilize multiple scales of insights, greatly improving the alignment between insights and tasks. (2) We designed 3 useful modules among experience selection, multi-scale insight generation, and task-related insight selection, shielding the noise caused by irrelevant insights. (3) We got the SOTA results in the TEACh TfD benchmark with GPT3.5 and beat another insight mechanism in the Alfworld. Whats more, our experiment comprehensively investigates the selection strategies of seed experiences and insights under various approaches and has proven that the MSI can enhance the robustness of insight utilization facing domain shifting."
        },
        {
            "title": "2.1 Embodied AI",
            "content": "Embodied AI focuses on leveraging multi-model information for decision and execution of actions. Diverging from traditional reinforcement learning approaches (Schulman et al., 2017), current research endeavors employ language models as decisionmakers for action decisions. Specifically, the model transforms information from non-natural language modalities into natural language through modality transformer (Inoue and Ohashi, 2022; Sarch et al., 2023), using natural language information as input to guide the Large Language Model in decisionmaking (Song et al., 2023; Singh et al., 2023, 2022; Suglia et al., 2021; Fu et al., 2024). Some methods involve fine-tuning the language model to map language inputs to action sequences at different hierarchical levels (Zhang et al., 2022; Zheng et al., 2022; Koshti and Bhavsar, 2023), while others prompt frozen LLM to predict action plans, relying on the instruction-following and context-learning properties of the LLM to simulate new tasks during testing (Wu et al., 2023; Sarch et al., 2023; Song et al., 2023; Singh et al., 2023, 2022; Dong et al., 2024a). By relying on action(s) generated by the model, the robot can accomplish the designated tasks in the environment. Figure 2: The overall pipeline for the MSI-agent to complete task. MSI Memory refers to the part that deals with insight. In MSI Memory, Experience Selection and Insight Generation will summarize historical experience into insights, while Insight Selection will select insights to assist the executor in completing future tasks. level insights, as shown in Figure 1. The former can interfere with decision-making (Liu et al., 2023a; Chen et al., 2023; Ren et al., 2023; Dong et al., 2023), while the latter may result in lack of high-level prior information to assist in decisionmaking. (Wen et al., 2023; Majumder et al., 2023; Wang et al., 2023c). Therefore, providing models with comprehensive and related insights to the current task has become important. To address these challenges, we proposed MultiScale Insight Agent (MSI-Agent), an embodied agent designed to summarize and utilize insights effectively. Inspired by Expel (Zhao et al., 2023), MSI collects the task background, user queries, agents plans, environmental feedback, and execution results as \"experience\" from series of training tasks. These experiences are then organized into the successful experience set or successfailure experience pairs set via an experience selector. Subsequently, an insight generator summarizes multi-scale insights based on the organized experience(s). Through this method, both high-level and fine-grained insight can be generated. During task execution, the insight will pass an insight selector to filter out the irrelevant insight and the remaining insight prompts the executor to formulate plans and execute tasks within given environment. The overall pipeline for the MSI agent to complete task is illustrated in Figure 2, while the architecture of the insight part in MSI is detailed in Figure 3. This solution effectively mitigates the issues highlighted earlier. By allowing classifying and selecting insights, MSI ensures that the LLM is not overwhelmed with irrelevant insights. SimultaneFigure 3: Pipeline of MSI Memory. The Insight Summarization part will summarize the historical task experience, while the Insight Utilization part will select relative insights to help the agent decide on future work. In the Insight Generation part, we will continuously update the insight database based on the training task experience (pair). We will freeze the database after updating insight with all training tasks. It should be noted that only some task generates environment insights (aligning with 3.3). Env=environment 2.2 LLM Long-term Memory When making decisions, humans often recall past cases to assist in decision-making. Due to the limited input length, the LLM Agent cannot receive infinite historical experiences. Therefore, efficiently utilizing existing success/failure experiences becomes crucial. The LLM Long-term Memory is designed to address this challenging issue (Zhao et al., 2023; Wen et al., 2023; Majumder et al., 2023; Qian et al., 2024). Currently, the LLM Agent Memory operates in two modes: example memory and insight memory. Example memory involves manually crafting experience examples that were successful in tasks. During usage, similar examples are retrieved based on the current task, using methods such as vectors or BM25, to prompt the large language model (Wang et al., 2023a; Wen et al., 2023; Dong et al., 2024b; Song et al., 2023; Zhong et al., 2023). Insight memory, on the other hand, summarizes success/failure experiences into insights through the LLM. When new tasks occur, the insights are directly input as part of the prompt into the LLM for helping planning and decisionmaking. (Majumder et al., 2023; Zhao et al., 2023). scale insights will be generated and stored in the insight database. (3.3) When new task arises, we retrieve relevant sights from the database based on predefined rules. (3.4) These insights, along with task background, and user queries, are provided to the task execution module to facilitate execution. We refer to the process from experience collection to insight generation as insight summarization, and the subsequent insight selection and task execution as insight utilization."
        },
        {
            "title": "3.1 Experience Generation",
            "content": "As shown in Figure 2, we regard training data as history tasks. For each history task, the executor leverages LLM to generate plan based on task background and user queries. Subsequently, the robot employs first-order logic to decompose the plan into atomic actions (e.g., moving forward, picking up objects) and execute them in an environment. In some tasks or cases, the executor may replan based on the environment feedback. Upon completion, task background, user queries, agents plans, environmental feedback, and execution results are stored as experiences for summarization. Detailed information can be found in Appendix A."
        },
        {
            "title": "3.2 Experience Selection",
            "content": "Figures 2 and 3 illustrate our approach. Initially, utilizing historical task data (train set), we employ the task execution module to collect sufficient number of experiences. (3.1) These experiences are then subjected to the experience selector, which identifies experiences/experience pairs suitable for generating insights. (3.2) Subsequently, the multiThe selection of experiences is crucial in summarizing insights, as it determines the quality of insights the model consolidates. As shown in Figure 3, our Experience Selection employs two modes: Success mode: We select experiences with successful execution results as the success mode experiences. Pair mode: For each successful experience ss, we identify corresponding experience sf from the unsuccessful experience database Sf by: sf = argmax sSf emb(s) emb(ss) (cid:112)emb(s)2emb(ss)2 (1) Where emb is the embedding of the experiences user query and the (ss, sf ) is the final experience pair in the pair mode. These two types of selected experience (pair) collections are subsequently preserved and utilized as seed experience for insight summarization. 3.3 Multi-Scale Insight Generation Multi-Scale Insight We categorize the insights into several scales. For all tasks, we will generate general scale and subtask scale insights. If the task provides specific environment category (for example, kitchen), we will also generate environment scale insights. General insight refers to the knowledge required for all tasks, which should be high-level. Environment insight pertains to the knowledge needed in specific environment, and subtask insight involves the understanding of executing particular subtasks. The overall pipeline can be seen in Figure 3s Insight Generation module. Insight Generation We initialize the insight database to be empty. Whenever seed experience merges, we select all insights in the order of general, subtask.2 as pool of candidate experience for updating. Subsequently, we prompt the LLM with templates containing the candidate insight, all experience information, and descriptions of 5 atomic actions: adding, removing, editing, agreeing on an insight, and moving an insight between scales, requesting the LLM to update the insight database through these atomic actions (Zhao et al., 2023). For subtask insight, we also require the LLM to additionally generate subtask name corresponding to the insights. 3 After the LLM generation is complete, we update the insight database in the order of general, environment (if have), and subtask, according to the atomic actions. Align with Expel, we also employ scoring mechanism in insight generation. Specifically, each 2If there is specific environment category in the task, we will select environment and subtask insight that is consistent with the experiences environment category, and the order is general, environment, and subtask 3The prompt of Insight Generation can be seen in Appendix insight receives an initial score of 2 when an \"add\" or \"move\" action is executed, the score increases by 1 for an \"agree\" action, remains unchanged for an \"edit\" action, and decreases by 1 for \"remove\" action. An insight is discarded when its score reaches zero. 3.4 Multi-Scale Insight Selection Similar to the generation process, we use general and subtask insights2 as candidate insights. For subtask insights, we adopt two modes for further selection: Hashmap indexing: We extract all subtask names from the subtask insight database, combine them with user queries, and provide them to the LLM, requiring the LLM to return all task names related to the user query. Subsequently, we consider all insights under returned subtask names as the subtask insights for this user query. The prompt of hashmap subtask selection can be seen in Appendix Vector indexing: We compute the cosine similarity between all subtask insights and the user query, selecting insights with at most 2000 tokens.4 Ultimately, we provide the different scales of insights, and the user query to the task execution module to accomplish the task."
        },
        {
            "title": "4 Experiment",
            "content": "We evaluate MSI on the 2 benchmarks5: TEACh TfD benchmark (Padmakumar et al., 2022) and AgentBench Alfworld benchmark (Shridhar et al., 2020; Liu et al., 2023b). Our experiments are designed to address the following research questions (RQs): RQ1: Does MSI outperform other insights methods? RQ2: What kind of seed experience selection strategy should be chosen when facing different insight generation strategies and tasks? RQ3: What kind of insight selection strategy should be adopted for different future tasks? RQ4: How does the robustness of the MSI system evolve with the domain shifts?"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Evaluation metrics For TEACh, we calculate accuracy (ACC) and path length weighted (P LW ) metrics under two settings: Task Success Rate (SR) and Goal Condition Success Rate (GC). 4Due to the excessive noise through vector indexing, we utilize this method only in ablation experiments. 5Detailed information can be seen in Appendix B. Aligned with HELPER, these four metrics are: SRACC = Exp (1(SCNx = GCNx)) (2) GCACC = (cid:80) (cid:80) xp SCNx xp GCNx (cid:80) xp SRP LW = 1(SCNx=GCNx)L2 ax(Lpredx ,Lrefx ) (cid:80) xp Lrefx refx (cid:80) xp GCP LW = (SCNx/GCNx)L2 refx ax(Lpredx ,Lrefx ) (cid:80) xp Lrefx (3) (4) (5) SCN and GCN refer to the success condition number and goal condition number respectively, Lpred refers to the step used to execute the task by the executor while Lref refers to the step used to execute the task by human annotator, refers to the distribution of the datasets and is the sample of the distribution of the datasets. For Alfworld, we calculate the SRACC metric."
        },
        {
            "title": "4.2 Executor",
            "content": "TEACh We use HELPER (Sarch et al., 2023) as the TEAChs executor. HELPER (Sarch et al., 2023) is an executor framework built on top of TEACh. As shown in Figure 2, it provides the task background, user query (i.e., the dialogue), and other relevant information to the LLM in fixed format, allowing the LLM to generate piece of code as the plan(Chen et al., 2021) and create sequence of subtasks to guide the robot. Initially, the robot will walk around the environment to observe and obtain spatial plan map that includes information about the objects it has observed, as well as its location (Blukis et al., 2022). At each time step, the robot receives an RGB image through its camera. It will then determine an atomic action based on the image, location, and subtask, and execute it in the simulation environment. (Sarch et al., 2023; Zhang et al., 2022) If the execution fails, the robot will call upon the VLM model (Li et al., 2023) to provide the most likely reason for the failure based on the image and attempt second try or replan (Yao et al., 2022; Shinn et al., 2023). In the MSI, we include the environment, dialogue, planned subtasks, actual executed subtasks, and the VLM-provided failure reasons during replanning as part of the experience. (Note that: The EXPERIENCE in the prompt refers to insight in the paper. ) Alfworld We use AgentBench as the Alfworlds executor. AgentBench (Liu et al., 2023b) is executor frameworks with ReAct format (Yao et al., 2022), Alfworld is one of its subtask. As shown in Figure 2, AgentBench provides the task background (as shown below), user query (i.e., the dialogue), and other relevant information to the LLM in fixed format, allowing the LLM to generate thought and an action (as the plan) in each turn. After the actions execution, the environment will give the feedback to the agent and the agent will replan another action based on feedback and new thoughts until the task is finished. In the MSI, we include the task background, user query, and all thoughtaction-observations in the task as the experience. The introduction of HELPER and AgentBench can be seen in Appendix 4.3 Hyperparameter Our insight generation and decision-making components are aligned with Expel. We have chosen ChatGPT (gpt-3.5-turbo-1106) as the LLM for selecting insight subtasks. GPT-4 (gpt-4-1106preview) as the LLM for insight generation. During the experience selection phase, we use textembedding-ada-002 to establish vector library for failed experiences for retrieval purposes. TEACh We have chosen ChatGPT (gpt-3.5turbo-1106) as the decision-maker for planning. The settings for experience memory enhancement, PreCheck, Correction, and locator are all aligned with HELPER. Due to the time limitation and budget, we do not use GPT4 as the decision-maker for planning. Alfworld We have chosen ChatGPT (gpt-3.5turbo-1106) and GPT-4 (gpt-4-1106-preview) as the decision-maker for planning. The examples are all aligned with AgentBench."
        },
        {
            "title": "4.4 Baseline",
            "content": "For TEACh, We consider the following baselines: Fine-Tune Based Model: Episodic Transformer (E.T.) (Padmakumar et al., 2022) is an end-to-end multimodal transformer that can predict the action by language inputs like dialogue and images in the environment. Jarvis (Zheng et al., 2022) and FILM (Min et al., 2022) use multimodal transformer to predict subgoals and transform them into atomic actions by rules. DANLI (Zhang et al., 2022) uses an LM to encode language inputs to high-level subgoals and uses PDDL model (Lamanna et al., 2021) to transform subModel Fine-Tune Based Model E.T. JARVIS FILM DANLI LLM Agent-Based Model HELPER HELPER Expel MSI Seen (IND) SR GC Unseen (OOD) SR GC 0.48 (0.12) 1.80 (0.30) 2.9 (1.0) 7.98 (3.20) 0.35 (0.59) 3.10 (1.60) 6.1 (2.5) 6.79 (6.57) 1.02 (0.17) 1.70 (0.20) 5.5 (2.6) 4.97 (1.86) 1.42 (4.82) 5.40 (4.50) 5.8 (11.6) 10.50 (10.27) - 8.84 (1.76) 8.28 (1.86) 12.70 (2.60) - 13.94 (7.65) 11.55 (7.83) 13.66 (8.72) 9.48 (1.21) 10.62 (1.41) 8.99 (2.66) 14.54 (3.70) 10.05 (3.68) 9.29 (3.95) 8.49 (6.02) 10.08(6.35) Table 1: Trajectory from Dialogue (TfD) evaluation on the TEACh validation set. Trajectory length weighted metrics are included in ( parentheses ). SR = success rate. GC = goal condition success rate. The results with come from (Sarch et al., 2023). We use ChatGPT as the LLM in LLM Agent-Based Model. We reproduce the HELPER in HELPER line and apply Expel in TEACh. Both Expel and MSI use pair mode to generate insight. Model Act-Only ReAct Expel MSI GPT3.5 GPT4 Dev (IND) Test (OOD) Dev (IND) Test (OOD) 0 0 5 6 10 14 16 65 65 75 85 66 68 70 72 Table 2: AgentBench Alfworld results. We reproduce all results via AgentBenchs framework. Both Expel and MSI use pair mode to generate insights. goals, object states, and spatial maps into an atomic action. It also has strategy to replan atomic action when facing errors in atomic action. LLM Agent-Based Model: HELPER (Sarch et al., 2023) uses LLM to transform all information into code and uses code parser to parse the code into subgoals. Expel (Zhao et al., 2023) presents pipeline to generate schemes and experience as long-term memory. Different from the original setting in Expel, our pair mode uses success-fail pairs between different tasks instead of between reflexion (Shinn et al., 2023) steps. For Alfworld, We consider the following baselines: Act-only (Yao et al., 2022), ReAct (Yao et al., 2022) and Expel (Zhao et al., 2023)"
        },
        {
            "title": "4.5 Main Result (RQ1)",
            "content": "TEACh The performance of MSI on TEACh is displayed in Table 1. Notably, MSI gains 12.70% in IND data and 14.54% in OOD data6, which outperforms all results among LM and ChatGPT. In contrast, Expel performs below other LLM Agent6We select only those experiences generated by GPT3.5 with SRACC =1 for MSI and Expel to generate insights. Therefore, the insights should generally align with SRACC . Based Models but above Fine-Tune Based Models. This may be because many irrelevant insights in the prompts lead to decreased performance. Despite the Expel summarizing experience based on training data, its effectiveness is inferior to that of HELPER, which uses one-shot examples directly. Conversely, MSIs success rate in both IND and OOD tasks is over 40% higher than that of HELPER, indicating that the Multi-Scale Insight summarization and utilization method can provide task-relevant insights to assist the model in making inference decisions. Alfworld The results of MSI on Alfworld are displayed in Table 2. Both insight mechanisms gain positive effects on ReAct-based agents. The enhancement effect on the performance through MSI insight is approximately twice that of Expel insight (20 vs 10 in GPT4-dev and 4 vs 2 in GPT4std) which indicates the performance of MSI is meaningful over Expel. As result, MSI insight can improve an agents planning and decision-making ability in both single-turn plans (TEACh) and multi-turn plans (Alfworld). This showcases its extensive versatility and potential applications across different contexts. Cases comparison: Figure 4 illustrates the decision-making processes and insights examples used by HELPER, Expel, and MSI when completing the task of slicing tomatoes and plating them. It can be observed that HELPER incorrectly marks the landmark of Tomato as the location \"CounterTop\" in the one-shot example, instead of Toaster, causing failure in finding the tomato and thus In contrast, MSI successfully failing the task. Figure 4: An example of 3 plans dealing with specific task in TEACh. (A) The original tasks user query, we omit some responses. (B) Plan to finish the task without experience. (C) Expel insights example (D) MSI insights example(E) Plan to finish the task with Expel. (F) Plan to finish the task with MSI. We omit most of the insights in Expel and MSI due to the length limitation. marks the landmark, even though it uses the same one-shot example where the Tomato landmark is marked as CounterTop. This is because MSI has subtask insight that guides the model on how to ensure accurate positioning when the dialogue includes \"near another object.\" This reflects the effectiveness of insight generation to certain extent. Although Expel also has insight that assists the model in locating objects, and its decision-making for plate location is correct, irrelevant yet similar insight has influenced its judgment. For example, the insight marked in red in the figure may lead the LLM to mistakenly believe that it needs to generate code strictly following the dialogue sequence and that the executor needs to further slice the tomato slices. On the contrary, MSIs insight prompts the model to first determine the order of the steps, and since there are no examples in the general insight, it also reduces the LLMs susceptibility to interference from irrelevant variables."
        },
        {
            "title": "4.6 Experience Select Strategy (RQ2)",
            "content": "Table 3 shows the results of the two strategies under two long-term memory methods. From the perspective of the optimization goal of insights (i.e. SRACC), Expel performs 8.28% and 8.99% on HELPER IND and OOD data when using insights summarized from successful experiences alone compared to using success-failure pairs with 9.94% and 11.60% respectively. In contrast, MSI performs better when summarizing insights from success-failure pairs rather than just successful experiences, the former reaches 12.70% and 14.54% in HELPER IND and OOD data while the latter only gains 10.65% and 13.39%. Alfworlds GPT3.5 version has the same trend in Table 3. The reason for this outcome may be that Expels method of summarizing and utilizing insights provides the LLM with many fine-grained insights that are problematic yet related to the issue or irrelevant insights(as shown in the red part of Figure 4), leading to decreased accuracy. Conversely, when MSI summarizes the insights, it does so at multiple scales and only selects portion for actual use by the LLM. This approach separates general insights with strong generality from fine-grained insights, ensuring that when the LLM uses insights from success-failure pairs, it can benefit from the strong generality of general insights while reducing the interference of irrelevant fine-grained insights through selective insight use. Due to this characteristic of MSI, its effectiveness in summarizing and utilizing insights from success-failure experience pairs is better than using successful experiences alone. The above analysis indicates that the Experience Select Strategy is related to the method of generating and utilizing insights. If strong generality and Model Seen (IND) Unseen (OOD) Dev (IND) Test (OOD) SR GC SR GC GPT3.5 GPT4 GPT3.5 GPT4 TEACh Alfworld pair mode Expel MSI success mode Expel MSI 8.28(1.86) 12.70(2.60) 11.55(7.83) 13.66(8.72) 8.99(2.66) 14.54(3.70) 8.49(6.02) 10.08(6.35) 9.94(2.25) 10.65(1.94) 11.13(7.92) 14.15(6.69) 11.60(3.04) 13.39(2.10) 9.77(6.47) 8,96(4.05) 5 5 0 0 75 85 75 75 14 10 10 70 72 70 76 Table 3: The TEACh and Alfworld result of Expel and MSI under different experience selecting strategies. Model pair mode MSI MSI (general) success mode MSI MSI (general) TEACh Alfworld Seen (IND) SR GC Unseen (OOD) SR GC Dev (IND) Test (OOD) GPT3.5 GPT4 GPT3.5 GPT4 12.70(2.60) 12.15(2.36) 13.66(8.72) 13.94(8.55) 14.54(3.70) 14.86(3.87) 10.08(6.35) 11.12(7.53) 10.65(1.94) 10.50(2.73) 14.15(6.69) 13.66(8.87) 13.39(2.10) 12.25(3.40) 8,96(4.05) 9.81(6.17) 5 5 0 0 85 80 75 75 16 20 10 72 72 76 76 Table 4: The TEACh and Alfworld result of MSI under different scale experience selecting strategies. Model MSI (Hashmap) MSI (Vector) Seen (IND) SR 12.70(2.60) 10.05(2.89) GC 13.66(8.72) 13.52(9.11) Unseen (OOD) SR 14.54(3.70) 11.43(1.28) GC 10.08(6.35) 9.2(3.53) Table 5: The TEACh result of MSI under different subtask insights selecting strategies. specificity insights can be generated and selected, the pair mode is more helpful in enhancing the LLMs decision-making capabilities. Otherwise, the success mode should be chosen to avoid the interference of too many irrelevant insights. 4.7 Insights Select Strategy (RQ3) Table 4 shows the comparison of multi-scale insights versus only general insights used under two different Insight Select Strategies. In most cases, the use of multi-scale insights provides stronger improvement to LLM planning than the use of general insights alone. However, when dealing with OOD problems in pair mode, the general insights gain 14.86% in TEACh and 20% in Alfworld, which outperforms the multi-scale insights result of 14.54% and 16% respectively. This may be due to task-specific insights summarized in-domain not aligning with OOD tasks, resulting in fine-grained mismatches. Pair mode is more susceptible to finegrained mismatches, which is why using only general insights can be more helpful to model decisionmaking than using multi-scale insights. Consistent with the conclusions of Section 4.4, the effectiveness of MSI when summarizing insights in pair mode is always better than in success mode. Table 5 presents the impact of two different methods of refining task-specific insights on LLM decision-making in TEACh. Across both data types, results using hashmap pair retrieval are over 20% higher on Success Rate (SR) than those using vector similarity retrieval (from 10.05% to 12.70% in IND and 11.43% to 14.54% in OOD). This is because vector similarity retrieval may introduce irrelevant insights, as shown in Figure 1. If the task is \"water plants with bowl\", the top three insights retrieved by vector similarity are classified as \"Water Plant\", \"Retrieve and Prepare\" and \"Prepare Beverage\". The first two seem to align with the task requirements, while the third is unrelated. The \"Prepare Beverage\" can be retrieved because the word bowl is in the task whose semantic space is associated with cooking, leading to the retrieval of irrelevant insights. This also explains why the method of vector similarity retrieval, used to retrieve schemes as examples, cannot be employed when utilizing insight. The results from Tables 4 and 5 collectively illustrate the strategy for selecting insight: The agent system needs to first determine whether the current task aligns with the seed task experiences for insight generation. If there is no alignment, then only general insights in the MSI should be used to assist LLM decision-making. Conversely, if there is alignment, multi-scale in0.38%, proving that MSI possesses better robustness in handling domain transfer. 4.9 Conclusion In this paper, we propose MSI, which is capable of summarizing and utilizing multi-scale insights to enhance the decision-making ability of embodied agents. MSI can assist agents in making higherquality decisions and is better equipped to handle insight distribution shifting that may occur with continuous insight updating. Our experiments demonstrate that for MSI, success-failure experience pairs are better seed data for insights, while the strategy for insight selection needs to be determined based on comprehensive assessment of the future task distribution and the distribution of tasks for which insights have been summarized. It sets new state-of-the-art result for the TEACh using agents based on ChatGPT as the foundation and beat another insight mechanism in the Alfworld. We believe our work contributes new insights into the summarization, storage, and utilization of long-term memory, especially insights."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is supported by the National Science and Technology Major Project (2023ZD0121403). We extend our gratitude to the anonymous reviewers for their insightful feedback, which has greatly contributed to the improvement of this paper."
        },
        {
            "title": "Limitations",
            "content": "While MSI achieves significant improvements over existing baselines, there are still directions to explore for future work. (1) Although the General and Subtask scale can be used in all tasks, the environment scale can only be used in some embodied scenarios. In the future, we will expand the idea of multi-scale insight by designing different scales in other tasks. (2) We only explore one type of long-term memory, insight. In the future, we will explore the combination of different types of long-term memory."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Figure 5: The robustness of agents when facing domain shifting. Dashed lines indicate baseline scores without insight or with random scheme shuffling across three domains. Solid lines show scores after sequential insight summarization: first, kitchen experiences inform insight; then living room experiences update it; finally, bedroom experiences refine it, with corresponding results displayed under each domain. sights should be used in conjunction with keyvalue pair indexing strategy for selection."
        },
        {
            "title": "4.8 Robustness in Domain Adaptation (RQ4)",
            "content": "Agents can adjust to new environments by constantly updating their insights repository. However, the distribution of new tasks may differ from that of old tasks that have already been summarized into insights, which can lead to \"catastrophic forgetting\" of old tasks when the insights undergo domain transfer, resulting in decreased model performance on old tasks. Therefore, it is crucial to have robust agents for Domain Adaptation. Figure 5 illustrates the robustness of MSI and Expel under domain shifting in TEACh. We fed the training data into the insight summarizer in the order of environments: kitchen, living room, and bedroom, unlike the original MSI and Expel, which shuffle the training data before input. We selected the kitchen task in the valid unseen set as \"original domain tasks\" for testing. insights summarized solely on kitchen data are more beneficial in assisting the model with decision-making in the kitchen. However, as new OOD data is introduced, the model insights degree of forgetting, leading to decline in performance on kitchen tasks. Compared to Expel, which declines 2.11% after summarizing the living room and bedroom scheme, MSI shows smaller degree of performance decline and faster convergence with only decline of about Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. 2022. persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706717. PMLR. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in arXiv preprint retrieval-augmented generation. arXiv:2309.01431. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Guanting Dong, Rumei Li, Sirui Wang, Yupeng Zhang, Yunsen Xian, and Weiran Xu. 2023. Bridging the kb-text gap: Leveraging structured knowledgeaware pre-training for kbqa. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 23, page 38543859, New York, NY, USA. Association for Computing Machinery. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024a. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542. Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. 2024b. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. CoRR, abs/2406.18676. Dayuan Fu, Jianzhao Huang, Siyuan Lu, Guanting Dong, Yejie Wang, Keqing He, and Weiran Xu. 2024. Preact: Predicting future in react enhances agents planning ability. CoRR, abs/2402.11534. Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, and Wai Lam. 2023. Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving. arXiv preprint arXiv:2311.08803. Yuki Inoue and Hiroki Ohashi. 2022. Prompter: Utilizing large language model prompting for data efficient embodied instruction following. arXiv preprint arXiv:2211.03267. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474. Kushal Koshti and Nidhir Bhavsar. 2023. Interaction is all you need? study of robots ability to understand and execute. arXiv e-prints, pages arXiv2311. Leonardo Lamanna, Luciano Serafini, Alessandro Saetti, Alfonso Gerevini, and Paolo Traverso. 2021. Online grounding of pddl domains by acting and sensarXiv preprint ing in unknown environments. arXiv:2112.10007. Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. 2023. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 1(2):2. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Lost in the middle: How lanLiang. 2023a. guage models use long contexts. arXiv preprint arXiv:2307.03172. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688. Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. 2023. Clin: continually learning language agent for rapid task adaptation and generalization. arXiv preprint arXiv:2310.10134. So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, and Yonatan Bisk. 2022. Dont copy the teacher: Data and model challenges in embodied dialogue. arXiv preprint arXiv:2210.04443. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gökhan Tür, and Dilek Hakkani-Tür. 2022. Teach: Task-driven embodied agents that chat. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, ThirtyFourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 20172025. AAAI Press. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122. Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun. 2024. Investigate-consolidate-exploit: general strategy for inter-task agent self-evolution. Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023. Investigating the factual knowledge boundary of large language modarXiv preprint els with retrieval augmentation. arXiv:2307.11019. Gabriel Sarch, Yue Wu, Michael Tarr, and Katerina Fragkiadaki. 2023. Open-ended instructable embodied agents with memory-augmented large language models. arXiv preprint arXiv:2310.15127. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768. Significant-Gravitas. 2023. Autogpt. https://github. com/Significant-Gravitas/Auto-GPT. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1152311530. IEEE. Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, and Roozbeh Mottaghi. 2022. Ask4help: Learning to leverage an expert for embodied tasks. Advances in Neural Information Processing Systems, 35:1622116232. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29983009. language model based autonomous agents. arXiv preprint arXiv:2308.11432. Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023c. Learning to filter context for retrieval-augmented generation. Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao. 2023. Dilu: knowledge-driven approach to autonomous driving with large language models. arXiv preprint arXiv:2309.16292. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024. Automatic instruction evolving for large language models. arXiv preprint arXiv:2406.00770. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. 2021. Embodied bert: transformer model for embodied, arXiv language-guided visual task completion. preprint arXiv:2108.04927. Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, Yuwei Bao, and Joyce Chai. 2022. Danli: Deliberative agent for following natural language instructions. arXiv preprint arXiv:2210.12485. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models. arXiv preprint arXiv:2305.16653. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv preprint arXiv:2308.10144. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Di, Xuehai He, and Xin Eric Wang. 2022. Jarvis: neuro-symbolic commonsense reasoning framework for conversational embodied agents. arXiv preprint arXiv:2208.13266. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023b. survey on large Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2023. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In NeurIPS 2023 Foundation Models for Decision Making Workshop. Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250."
        },
        {
            "title": "A Executor",
            "content": "(Note that: The EXPERIENCE in the prompt refers to insight in the paper. ) HELPER executor prompt You are an adept at translating human dialogues into sequences of actions for household robots. Given dialogue between <Driver> and <Commander>, you should write Python program to be executed by household robot that could finish all tasks in the conversation. {API} Write script using Python and the InteractionObject class and functions defined above that could be executed by household robot. Experience you have summarized in the past: {EXPERIENCE} {RETRIEVED_EXAMPLES} Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously. Do not create functions that are not provided above. 2. Make sure that you output consistent plan. For example, opening of the same object should not occur in successive steps. 3. Make sure the output is consistent with the proper affordances of objects. For example, couch cannot be opened, so your output should never include the open() function for this object, but fridge can be opened. 4. The input is dialogue between <Driver> and <Commander>. Interpret the dialogue into robot actions. Do not output any dialogue. 5. Object categories should only be chosen from the following classes: ShowerDoor, Cabinet, CounterTop, Sink, Towel, HandTowel, TowelHolder, SoapBar, ToiletPaper, ToiletPaperHanger, HandTowelHolder, SoapBottle, GarbageCan, Candle, ScrubBrush, Plunger, SinkBasin, Cloth, SprayBottle, Toilet, Faucet, ShowerHead, Box, Bed, Book, DeskLamp, BasketBall, Pen, Pillow, Pencil, CellPhone, KeyChain, Painting, CreditCard, AlarmClock, CD, Laptop, Drawer, SideTable, Chair, Blinds, Desk, Curtains, Dresser, Watch, Television, Newspaper, FloorLamp, RemoteControl, HousePlant, Statue, Ottoman, ArmChair, Sofa, DogBed, BaseballBat, TennisRacket, VacuumCleaner, Mug, ShelvingUnit, Shelf, StoveBurner, Apple, Lettuce, Bottle, Egg, Microwave, CoffeeMachine, Fork, Fridge, WineBottle, Spatula, Bread, Tomato, Pan, Cup, Pot, SaltShaker, Potato, PepperShaker, ButterKnife, StoveKnob, Toaster, DishSponge, Spoon, Plate, Knife, DiningTable, Bowl, LaundryHamper, Vase, Stool, CoffeeTable, Poster, Bathtub, TissueBox, Footstool, BathtubBasin, ShowerCurtain, TVStand, Boots, RoomDecor, PaperTowelRoll, Ladle, Kettle, Safe, GarbageBag, TeddyBear, TableTopDecor, Dumbbell, Desktop, AluminumFoil, Window, LightSwitch, AppleSliced, BreadSliced, LettuceSliced, PotatoSliced, TomatoSliced 6. You can only pick up one object at time. If the agent is holding an object, the agent should place or put down the object before attempting to pick up second object. 7. Each object instance should instantiate different InteractionObject class even if two object instances are the same object category. Follow the output format provided earlier. Think step by step to carry out the instruction. Write Python script that could be executed by household robot for the following: dialogue: {command} Python script:"
        },
        {
            "title": "AgentBench executor prompt",
            "content": "Interact with household to solve task. Imagine you are an intelligent agent in household environment and your target is to perform actions to complete the task goal. At the beginning of your interactions, you will be given the detailed description of the current environment and your goal to accomplish. For each of your turn, you will be given list of actions which you can choose one to perform in this turn. You should THOUGHTör choose from two actions: ÄCTION. If you choose THOUGHT, you should first think about the current condition and plan for your future actions, and then output your action in this turn. Your output must strictly follow this format: THOUGHT: your thoughts. ACTION: your next action ; If you choose ÄCTION, you should directly output the action in this turn. Your output must strictly follow this format:ÄCTION: your next action . After your each turn, the environment will give you immediate feedback based on which you plan your next few steps. if the environment output Nothing happened, that means the previous action is invalid and you should try more options. Here is some experience you summarized before: {experience} Reminder: 1. the action must be chosen from the given available actions. Any actions except provided available actions will be regarded as illegal. 2. Think when necessary, try to act directly more in the process. \""
        },
        {
            "title": "B Benchmark infromation",
            "content": "TEACh The TEACh dataset (Padmakumar et al., 2022) is constructed on over 120 different AI2THOR simulation environments (Kolve et al., 2017) and encompasses more than 2000 embodied intelligence tasks aimed at completing household chores. These environments can be categorized into four hyper-environments: kitchen, living room, bedroom, and bathroom. The training set consists of 1482 data points, encompassing all four types of environments. The valid seen set is built with 181 data points across the four environments, with all simulation environments having appeared in the training set. In contrast, the valid unseen set is constructed with 612 data points in three types of environments: kitchen, living room, and bedroom, based on simulation environments that have not been previously encountered in the training set. Therefore, we consider the valid unseen set as out-of-domain (OOD) data and the valid seen set as in-domain (IND) data. Our tests are conducted on the Trajectory from Dialogue (TfD) benchmark (Padmakumar et al., 2022), where the agent receives multiple rounds of interactive dialogue between commander and driver. The model must analyze the entire dialogue and make series of decisions to complete all tasks mentioned in the dialogue. Alfworld The Alfworld dataset (Shridhar et al., 2020) encompasses more than 4000 embodied intelligence tasks aimed at completing household chores. These tasks can be categorized into six hyper-task: \"pick and place\", \"pick clean then place\", \"pick heat then place\", \"pick cool then place\", \"look at obj\", and \"pick two obj\". We just select 20 successful experiences in each hyper-task. We use the AgentBench (Liu et al., 2023b) for evaluation, it contains 20 data points in the dev set and 50 data points in the std set. Aligned with Alfworld, we consider the std set as out-of-domain (OOD) data and the dev set as in-domain (IND) data."
        },
        {
            "title": "C Prompt of Insight Generation",
            "content": "Below presents Pair-Mode Experience Generation Prompt and Success-Mode Experience Generation Prompt. The parts with red are different. (For Alfworld, we just remove the part with \"environment rules.\") Pair-Mode Insight Generation Prompt You are an advanced reasoning agent that can add, edit, move or remove rules from your existing ruleset, based on forming new critiques of past task trajectories. The ruleset has three parts, GENERAL RULES, ENVIRONMENT RULES and TASK RULES. GENERAL RULES refers to rules that could used in all environment (Kitchens, LivingRooms, Bedrooms, and Bathrooms) and task. ENVIRONMENT RULES refers to rules that could used in all task in {env}. TASK RULES refers to rules that could used in specific task. You will be given two previous task trials with instruction: {instruction} One trial is successful, and the other is unsuccessful. Here are the two previous trials to compare and critique: Failed Trajectories: {Failed Trajectories} Succeeded Trajectories: {Succeeded Trajectories} Here are the EXISTING RULES: GENERAL RULES: {general rules} ENVIRONMENT RULES: {environment rules} TASK RULES: {task rules} By examining and contrasting to the successful trial, and the list of existing rules, you can perform the following operations: add, edit, remove, move or agree so that the new rules are HIGH LEVEL critiques of the failed trial or proposed way of Thought in 3 parts, so they can be used to avoid similar failures when encountered with different questions in the future. Have an emphasis on critiquing how to perform better Thought and Action. Follow the below format: GENERAL RULES: <OPERATION> :<RULE> ENVIRONMENT RULES: <OPERATION> :<RULE> TASK RULES: <OPERATION> :<RULE> The rule number should increase between parts, for example if there is 4 general rules the first environment rule number should be 5. The available operations are: AGREE (if the existing rule is strongly relevant for the task), REMOVE(if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks.), MOVE(move rules between different level and reshape the rules if the rules are not general in all enviroment(for GENERAL RULES) or task(for GENERAL RULES or <RULE NUMBER> <RULE NUMBER> <RULE NUMBER> EMVIRONMENT RULES)). Each needs to CLOSELY follow their corresponding formatting below: AGREE <EXISTING RULE NUMBER>: <EXISTING RULE> REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE> EDIT <EXISTING RULE NUMBER> :<NEW MODIFIED RULE> ADD <NEW RULE NUMBER>: <NEW RULE> MOVE <EXISTING RULE NUMBER>: <RESHAPED RULE>.(for example if you want to move rule in environment rules with id 12 to task rules, you should use MOVE 12:<RESHAPED RULE> in task rules part) Note1: MOVE command will remove the rules by number and add new rules in the part it present in and ADD command will add new rules in the part it present in. Note2:If you believe some rules in general rule part can not be used in the {env}, you should just remove that rules instead of move it. Note3:In task rules part, there may some task irrelevant with the trail now, DO NOT remove them In the TASK RULES part, you should specify the task name in the <RULE> with the following format:<RULE CONTENT> (TASK: <TASK NAME>), the length of task name should be less than 20 characters and the number of task should less than 20. Do not mention the trials in the general rules because they should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Remember this robot can only generate python script. The execute subgoal and error log are gained from another robot which this robot can not communite. So each rules should focus on helping robot to plan and generate better python script to solve the question based on ONLY dialogue. And operation can be used MULTIPLE times. Do at most 4 operations in each parts (which means the max operation number in 3 parts is 4x3=12) and each existing rule can only get maximum of 1 operation so just find the most important rules to operate. Do not operate rules in other parts. Below are the operations you do to the above list of EXISTING RULES Success-Mode Insight Generation Prompt You are an advanced reasoning agent that can add, edit, move or remove rules from your existing ruleset, based on forming new critiques of past task trajectories. The ruleset has three parts, GENERAL RULES, ENVIRONMENT RULES and TASK RULES. GENERAL RULES refers to rules that could used in all environment (Kitchens, LivingRooms, Bedrooms, and Bathrooms) and task. ENVIRONMENT RULES refers to rules that could used in all task in {env}. TASK RULES refers to rules that could used in specific task. You will be given successful task trials with instruction: {instruction} Here are the trials: {Succeeded Trajectories} Here are the EXISTING RULES: GENERAL RULES: {general rules} ENVIRONMENT RULES: {environment rules} TASK RULES: {task rules} By examining the successful trials, and the list of existing rules, you can perform the following operations: add, edit, remove, move or agree so that the new rules are HIGH LEVEL insights of the successful trials or proposed way of Thought in 3 parts, so they can be used as helpful tips to different questions in the future. Have an emphasis on tips that help the agent perform better Thought and Action. Follow the below format: GENERAL RULES: <OPERATION> :<RULE> ENVIRONMENT RULES : <OPERATION> <RULE NUMBER> <RULE NUMBER> <RULE NUMBER> :<RULE> TASK RULES: <OPERATION> :<RULE> The rule number should increase between parts, for example if there is 4 general rules the first environment rule number should be 5. The available operations are: AGREE (if the existing rule is strongly relevant for the task), REMOVE(if one existing rule is contradictory or similar/duplicated to other existing rules), EDIT (if any existing rule is not general enough or can be enhanced, rewrite and improve it), ADD (add new rules that are very different from existing rules and relevant for other tasks.), MOVE(move rules between different level and reshape the rules if the rules are not general in all enviroment(for GENERAL RULES) or task(for GENERAL RULES or EMVIRONMENT RULES)). Each needs to CLOSELY follow their corresponding formatting below: AGREE <EXISTING RULE NUMBER>: <EXISTING RULE> REMOVE <EXISTING RULE NUMBER>: <EXISTING RULE> EDIT <EXISTING RULE NUMBER> :<NEW MODIFIED RULE> ADD <NEW RULE NUMBER>: <NEW RULE> MOVE <EXISTING RULE NUMBER>: <RESHAPED RULE>.(for example if you want to move rule in environment rules with id 12 to task rules, you should use MOVE 12:<RESHAPED RULE> in task rules part) Note1: MOVE command will remove the rules by number and add new rules in the part it present in and ADD command will add new rules in the part it present in. Note2:If you believe some rules in general rule part can not be used in the {env}, you should just remove that rules instead of move it. Note3:In task rules part, there may some task irrelevant with the trail now, DO NOT remove them Insight source Expel MSI Task MSI General 0 14.29 0 30.23 1 1.19 0 6.2 2 16.67 6.42 17.05 3 23.81 8.26 19.38 4 13.1 12.84 10. 5 2.38 1.83 0 6 7.14 11.93 6.2 7 14.29 30.28 7.75 8 7.14 19.27 2.33 9 0 4.59 0.78 10 0 4.59 Table 6: The insights task-specific level under 3 sources. (0 for general insight and 10 for task-specific insight) In the TASK RULES part, you should specify the task name in the <RULE> with the following format:<RULE CONTENT> (TASK: <TASK NAME>), the length of task name should be less than 20 characters and the number of task should less than 20. Do not mention the trials in the general rules because they should be GENERALLY APPLICABLE. Each rule should be concise and easy to follow. Remember this robot can only generate python script. The execute subgoal and error log are gained from another robot which this robot can not communite. So each rules should focus on helping robot to plan and generate better python script to solve the question based on ONLY dialogue. And operation can be used MULTIPLE times. Do at most 4 operations in each parts (which means the max operation number in 3 parts is 4x3=12) and each existing rule can only get maximum of 1 operation so just find the most important rules to operate. Do not operate rules in other parts. Below are the operations you do to the above list of EXISTING RULES"
        },
        {
            "title": "Insight Selection Prompt in Hashmap Index",
            "content": "You are task selector trying to select task categories. household robot have just summarized some experience, and each experience belongs to task category. Now this robot is facing new task, based on dialogue between <Driver> and <Commander>, but this robot do not know which experience should be used in this task. You should select task categories related to the task this robot facing. You will be given target task category, the target category is likely to be found in:{task name} Important: Your output should ONLY list (categories seperated by commas) of the task categories from the list above. What are the task categories that related to {dialogue}? answer:"
        },
        {
            "title": "Insight Selection Example",
            "content": "task: put two soapbar in garbagecan selected subtask: Object Placement, Distinguishing Similarities, Sequential Placement, Revealing Hidden Objects, Comprehensive Search Insight High-Level Rate In the table 6, we compared the task-specific degree of three different insight sources in Alfworld, where 0 points are completely general (applicable to all tasks), 10 points are completely task-specific (can only be used for one specific task), and intermediate scores represent the degree to which they can be used for some tasks. We have manually created three examples, each in the format: (insight, thought, score). For each example, the scores are respectively 0, 5, and 10. We have then asked the model (gpt4-turbo-2024-04-09) to derive the score in COT manner. We can observe that the distribution of Expel is relatively uniform, the distribution of MSI Task tends to be around 7 points, while the distribution of MSI General leans towards 0-1 points. This demonstrates that MSI indeed distinguishes between general insight and task-specific insight, and that task-specific insight is more targeted towards specific tasks. Prompt of Rating Insights Level prompt: You will be given an experience about houseworking, your task is to judge whether the experience is general rule (all tasks in housework can be used) or taskrelated rule. You should think step by step and give score of 0-10, 0 means this experience is general rule, and 10 means this experience is task-related rule. Here are examples:"
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications, Beijing, China",
        "Department of Electronic Engineering, Tsinghua University",
        "Shanghai AI Laboratory",
        "Stanford University"
    ]
}