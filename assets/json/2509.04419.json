{
    "paper_title": "Towards a Unified View of Large Language Model Post-Training",
    "authors": [
        "Xingtai Lv",
        "Yuxin Zuo",
        "Youbang Sun",
        "Hongyi Liu",
        "Yuntian Wei",
        "Zhekai Chen",
        "Lixuan He",
        "Xuekai Zhu",
        "Kaiyan Zhang",
        "Bingning Wang",
        "Ning Ding",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families."
        },
        {
            "title": "Start",
            "content": "Unify Post-Training 2025-09-05 Towards Unified View of Large Language Model Post-Training Xingtai Lv1, Yuxin Zuo1, Youbang Sun1, Hongyi Liu1, Yuntian Wei1, Zhekai Chen1, Lixuan He1, Xuekai Zhu1, Kaiyan Zhang1, Bingning Wang3, Ning Ding1,2, Bowen Zhou1,2 1Tsinghua University, 2Shanghai AI Laboratory, 3WeChat AI Code: TsinghuaC3I/Unify-Post-Training # Mail: lvxt24@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised FineTuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of single optimization process. We derive Unified Policy Gradient Estimator, and present the calculations of wide spectrum of post-training approaches as the gradient of common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families. 5 2 0 2 4 ] . [ 1 9 1 4 4 0 . 9 0 5 2 : r Figure 1: Illustration of the Unified Policy Gradient Estimator. The in the background of the Likelihood Gradient part refers to the calculation of the gradient with respect to the πθ. Equal Contributions. Corresponding Authors. 1 Unify Post-Training 2025-09-"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Works 2.1 LLM Post-Training: SFT and RL . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Combination of Online and Offline Data in LLM Post-Training . . . . . . 3 Unified View on Post-Training Algorithms 3.1 Components of the Unified Policy Gradient Estimator . . . . . . . . . . . . . 3.2 Derivation of the Unified Policy Gradient Estimator . . . . . . . . . . . . . . 3.3 Gradient Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Hybrid Post-Training with Performance Feedback . . . . . . . . . . . . . . . 4 Experiments 4.1 Experimental Setup . 4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Empirical Analysis 5.1 Exploration and Exploitation . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Training Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Training Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Impact of Off-policy RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5 Gate Threshold Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Gradient Derivation for Classical Algorithms A.1 Gradient of SFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Gradient of Online RL: PPO, GRPO and Beyond . . . . . . . . . . . . . . . . A.3 Gradient of Offline RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Theoretical Details for Section 3.2 B.1 Deriving Equation 2 from Equation 1 . . . . . . . . . . . . . . . . . . . . . . . B.2 Extension: Adding Trust-Region Regularizer . . . . . . . . . . . . . . . . . B.3 PPO Clipping and the Stabilization Mask . . . . . . . . . . . . . . . . . . . . 3 4 4 4 5 6 7 9 10 12 12 12 14 15 17 17 23 23 23 24 24 25 25 2 Unify Post-Training 2025-09-"
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning has played an integral role in enhancing the reasoning capabilities of large language models (LLMs) (Jaech et al., 2024; Team et al., 2025; Guo et al., 2025). RL allows the model to freely explore the reasoning space in the post-training process and improve its performance based on the feedback provided in the environment. However, applying Reinforcement Learning directly to base model (i.e., Zero RL) (Zeng et al., 2025a) presupposes certain level of inherent capability. This method often falters when applied to weaker models or tasks of high complexity, as the exploration process may fail to explore and discover meaningful reward signals. Conversely, the classical Supervised Fine-Tuning (SFT) (Wei et al., 2021) offers direct and efficient method to distill knowledge from high-quality, human-annotated data, enabling models to rapidly and accurately fit the target distribution. Yet this approach often curtails the models exploratory capabilities, potentially leading to overfitting on the demonstration data and compromising its generalization performance on out-of-distribution inputs. Consequently, sequential SFT-then-RL pipeline (Yoshihara et al., 2025) has emerged as the standard, adopted by numerous state-of-the-art open-source models. While effective, this multi-stage process, which first elevates the models capabilities through SFT before refining them with RL, is notoriously resource-intensive and usually requires careful tuning to ensure effectiveness. To circumvent these challenges, recent works have focused on integrating SFT or SFT-style imitation learning losses directly with RL objectives (Yan et al., 2025; Fu et al., 2025; Zhang et al., 2025a). In these approaches, the model is updated using composite loss function. The balance between the imitation and exploration components is governed by various strategies, including fixed coefficient, predefined schedule, dynamic adjustment based on entropy, or learnable parameter. These works predominantly treat the SFT and RL losses as two distinct objectives. And detailed analysis of why these two learning signals can be effectively combined within unified optimization process remains largely unexplored. Despite their distinct mathematical formulations, we find that the gradient calculations from these approaches can be viewed as single, unified form. Inspired by Generalized Advantage Estimator (Schulman et al., 2015b), we introduce Unified Policy Gradient Estimator (UPGE), framework that formally subsumes the gradients of various post-training objectives into one generalized expression. We provide analysis to show that the various forms of gradients are, in fact, not conflicting. Instead, they act as complementary learning signals that can jointly guide the optimization process. However, these gradient estimators possess different characteristics, and there exists bias-variance tradeoff in their respective gradient components. Building upon this unified perspective, we propose Hybrid PostTraining (HPT), hybrid algorithm to dynamically choose more desirable training signals by adapting mixing ratio between the SFT and RL losses. This mechanism allows HPT to be intrinsically adaptive to models of varying capabilities and data of differing complexities. We implement simple instance of HPT, which adaptively switches between SFT and RL based on rollout accuracy, and empirically demonstrate that it achieves strong results. Our empirical evaluations demonstrate that HPT surpasses strong baselines such as SFTGRPO and LUFFY with Qwen2.5-Math-7B, achieving 7-point gain over our strongest baseline on AIME 2024. Moreover, HPT also yields substantial improvements even on relatively smaller and weaker models, including Qwen2.5-Math-1.5B and Llama3.1-8B. Through detailed training dynamics and illustrative training visualizations, we clearly reveal the features and underlying mechanisms of HPT. The following are several key takeaways: Takeaways 1. UPGE provides theoretical unification of wide spectrum of post-training algorithms, covering both SFT and RL losses within single formulation ( 3). 2. HPT is capable of outperforming previous post-training and mixed-policy algorithms across diverse range of models ( 4). 3. Dynamic integration of SFT and RL in HPT achieves the highest Pass@1024, facilitating enhanced exploration and generalization of the model ( 5.1). Unify Post-Training 2025-09-"
        },
        {
            "title": "2 Related Works",
            "content": "2.1 LLM Post-Training: SFT and RL Current post-training methodologies for LLMs are largely centered around two primary paradigms: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) (Wei et al., 2021; Ouyang et al., 2022). In the SFT paradigm, models are adapted for specific applications through training on curated input-output pairs, process which has been shown to effectively align their behavior with human demonstrations (Chung et al., 2022; Longpre et al., 2023; Touvron et al., 2023a;b). In parallel, numerous works have highlighted RL as an effective approach for refining LLM behavior in ways that are difficult to capture with SFTs static datasets (Glaese et al., 2022; Bai et al., 2022; Nakano et al., 2021). Within this domain, popular framework is Reinforcement Learning from Human Feedback (RLHF), which optimizes the LLM policy against reward model trained on human preferences (Christiano et al., 2017; Stiennon et al., 2020). Multiple works have established Proximal Policy Optimization (PPO) as cornerstone algorithm for this phase (Schulman et al., 2017; Ziegler et al., 2019). To further improve reasoning capabilities in reward-driven optimization, recent advancements like Group Relative Policy Optimization (GRPO) have also been developed and widely adopted (Shao et al., 2024; Zheng et al., 2025; Chen et al., 2025). 2.2 Combination of Online and Offline Data in LLM Post-Training Beyond applying SFT or RL in isolation, further explorations have sought to synergize their respective strengths by combining signals from pre-existing offline data and dynamically generated online data (Fu et al., 2025; Yan et al., 2025). This motivation stems from the distinct characteristics of each approach: SFT is noted for its efficiency in distilling knowledge from offline sources, whereas RL is valued for fostering exploration through online rollouts, process frequently linked to improved generalization (Rajani et al., 2025; Chu et al., 2025). The strategies for this integration are diverse; some techniques use offline data as prefix to guide online generation (Zhou et al., 2023; Touvron et al., 2024; Li et al., 2025; Wang et al., 2025), while others enhance offline data by incorporating reward signals in process known as reward-augmented fine-tuning (Liu et al., 2024; Zhao et al., 2023; Park et al., 2025; Sun et al., 2025). The broader landscape also includes various purely offline preference optimization methods, though they follow different paradigm (Rafailov et al., 2023; Mitchell et al., 2024; Liu et al., 2025c; Ethayarajh et al., 2024; Ahmadian et al., 2024). However, the most direct approach to synergy involves the concurrent use of both data types for training updates. This direct approach, often termed mix-policy learning, is particularly relevant to our work and typically involves updating the model with composite objective that combines an SFT loss from offline data and an RL loss from online data (Dong et al., 2023; Gulcehre et al., 2023; Singh et al., 2023; Liu et al., 2023). For instance, LUFFY (Yan et al., 2025) explores this paradigm by combining fixed ratio of offline demonstration data with online rollouts in each training batch. Subsequently, SRFT (Fu et al., 2025) proposed monolithic training phase that dynamically adjusts the weights of SFT and RL losses based on the models policy entropy, further demonstrating the viability of unifying these signals over sequential pipeline. The principle of creating such composite loss is shared by variety of other recent frameworks (Wu et al., 2024; Zhang et al., 2025a; Kim et al., 2025; Yu et al., 2025; Liu et al., 2025a), and AMFT (He et al., 2025) begins to explore meta-gradient-based controllers. While these methods highlight clear trend towards unifying training signals, foundational theoretical analysis explaining why these different learning signals can be effectively combined is still lacking. This motivates our work to establish unified theoretical framework that in turn inspires more principled algorithm design."
        },
        {
            "title": "3 A Unified View on Post-Training Algorithms",
            "content": "In this section, we adopt unified perspective to understand both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) as post-training objectives. We present the gradient 4 Unify Post-Training 2025-09-05 Table 1: Theoretical unified view of various post-training algorithms. Algorithm Reference Policy Advantage Estimate SFT πre = πθ ˆASFT 1 Unified Policy Gradient Estimator JSFT(θ) = πθ(τ) ˆASFT =1 πθ (τ) Online Reinforcement Learning Methods PPO (Schulman et al., 2017) GRPO (Shao et al., 2024) REINFORCE (Ahmadian et al., 2024) CISPO (Chen et al., 2025) GSPO (Zheng et al., 2025) SRFT (Offline) (Fu et al., 2025) LUFFY (Offline) (Yan et al., 2025) πre = πθold πre = πθold πre = πθ ˆAPPO = GAE (Schulman et al., 2015b) JPPO = πθ(τ) 1 ˆAPPO Clip πre (τ) ˆAGRPO = R(τj)mean({R(τj)}Gon ) std({R(τj)}Gon ) ˆAREI NFORCE = 1 ˆAGRPO JGRPO = πθ(τ) 1 πre (τ) JREF.(θ) = πθ(τ) ˆAREF. πθ (τ) Clip πre = πθold (cid:18) πθold (τi,jqi) πθ (τi,jqi) πre = πθ ˆACISPO = ˆAGRPO JCISPO = πθ(τ) (cid:19)1/τi,j ˆAGSPO = ˆAGRPO JGSPO = πθ(τ) ˆACISPO 1 πre (τ) CIS-Mask ˆAGSPO 1 πre (τ) Seq-Clip Offline/Online Reinforcement Learning Methods R(τj)mean({R(τj)}Gon Go std({R(τj)}Gon Go ˆASRFT = ) πre 1 ) JSRFT = πθ(τ) ˆASRFT πre (τ)= πre 1 ˆALUFFY = ˆASRFT JLUFFY = πθ(τ) ˆALUFFY πre (τ)=1 shape calculations of various post-training approaches in Table 1, with exact derivations of classical approaches presented in the Appendix A. From the table, it can be seen that policy gradient calculations for LLM post-training can be written in unified policy gradient form. Takeaways We propose unified framework for the gradient calculation of LLM post-training, named the Unified Policy Gradient Estimator: gradUni = 1 stable 1 πre ˆAπθ. All gradient calculations can be written in the unified form. In the following sections, we further show that the differences between different gradient calculations can be broken down into four distinct components. We theoretically derive the Unified Policy Gradient Estimator from common objective and provide detailed analysis of its gradient components. Based on this unified perspective, we then propose the Hybrid Post-Training (HPT) algorithm. 3.1 Components of the Unified Policy Gradient Estimator We present the Unified Policy Gradient Estimator, our unified framework for gradient calculations. In Table 1, we list series of fundamental and well-studied post-training methods, divided into SFT and two types of RL processes. Apart from providing the closedform policy gradients of these methods, we also present the decomposition of these methods with detailed components. It can be seen that these seemingly different methods in fact share common components and that all gradients follow our proposed unified framework. In this paper, we divide the unified gradient into four terms: stabilization mask, reference policy, advantage estimate, and likelihood gradient. We address each of the terms below. Stabilization Mask 1 stable Starting from PPO (Schulman et al., 2017), the stabilization mask was first derived as an approximation of the TRPO Algorithm (Schulman et al., 2015a). In practice, the PPO clipping addresses the instability issue during RL training by turning off the current gradient when the current iterate is considered unsafe. In consequent works in Table 1, many have provided their modifications on the stability mask, usually motivated by empirical evaluations. Reference Policy Denominator πre The second term in our unified estimator is the reference policy on the denominator. We note that our notion of reference policy differs 5 Unify Post-Training 2025-09-05 from the commonly used rollout policy πθold , for which we provide discussion in Section 3.3. This denominator denotes token-level reweight coefficient, usually in the form of an inverse probability. There are multiple choices for this coefficient. For the case of SFT, the policy denominator uses the current policy πθ(τ). This is result of = log(πθ(τ)) as the objective function. For the case of PPO-style online RL algorithms, generally, the policy (τ). Due to the unavailability of πre (τ) in the denominator uses the rollout policy πθold offline demonstration dataset, most offline RL algorithms simply assume πre (τ) = 1 for the denominator. Advantage Estimate ˆA In traditional RL, the advantage evaluates the additional benefit of taking the current action given the current state. For the context of LLMs, most of the advantage estimation is sequence-level rather than token-level, and measures the quality of the current response sequence. Similar to traditional RL literature, the post-training process seeks to maximize the likelihood of generating positive sequences with high advantage and minimize negative sequences. Likelihood Gradient πθ(τ) The policy gradient term is general term which maps gradient information from the actions to the model parameters θ. It is crucial for backpropagating the objective signals to the network weights, and is kept the same across all gradient calculations. 3.2 Derivation of the Unified Policy Gradient Estimator We begin from simple and common objective shared by all post-training algorithms: improve the likelihood of positive trajectories and decrease the likelihood of negative trajectories such that the total reward in expectation maxθ (θ) := E[r(τq)] is maximized. From this starting point, we theoretically derive our Unified Policy Gradient Estimator. We then show that SFT and RL objectives are not in conflict, and they can be optimized jointly within single loss. Common Objective. We model the post-training as process to maximize the expected success rate while keeping the model policy closely adhering to demonstration dataset (behavior policy) πβ: Jµ(θ) = τπθ (q) (cid:2)r(τ q)(cid:3) µ KL(cid:0)πβ( q) πθ( q)(cid:1), µ 0, (1) where denotes the question from given distribution, τ denotes trajectory, denotes the (binary/real) score, and πβ denotes behavior policy from demonstration. Gradient of the Common Objective. Differentiating and rearranging Equation 1 (full derivation in Appendix B.1), we obtain (cid:104) θJµ(θ) = Eτπθ r(τ q) θ log πθ(τ q) (cid:105) + µ Eτπβ (cid:2)θ log πθ(τ q)(cid:3). (2) From gradient to the Unified Policy Gradient Estimator. Applying the measure-change identity (detailed in Appendix B.1) with the reference policy πre which we mentioned in Section 3.1 and using log πθ = (1/πθ)πθ yields the gradient: θJµ(θ) = τπre (q) (cid:34) 1 πre (τ q) (cid:98)Auni(τ, q) θπθ(τ q) (cid:35) , with the unified advantage (cid:98)Auni(τ, q) = r(τ q) (cid:124) (cid:123)(cid:122) (cid:125) (cid:98)ARL(τ,q) + µ 1{πre = πβ} πβ(τ q) πθ(τ q) (cid:125) . (cid:123)(cid:122) (cid:98)ASFT(τ,q) (cid:124) 6 (3) (4) Unify Post-Training 2025-09-05 In many RL works, the raw score r(τ q) is replaced by more structured advantage to reduce variance, provide relative credit assignment within rollout group, and stabilize step sizes. For example, GRPO uses group-wise normalization: (cid:98)AGRPO(τj, q) = R(τj) mean({R(τ)}Gon ) std({R(τ)}Gon ) . (5) When trust-region stabilization masks, as induced by PPO clipping, are inserted multiplicatively without altering the target objective, we obtain our Unified Policy Gradient Estimator: graduni = τπre (q) 1 stable(τ, q) (cid:34) 1 πre (τ q) (cid:98)Auni(τ, q) θπθ(τ q) (cid:35) (6) = 1 stable 1 πre ˆA πθ. The trust-region surrogate that produces the mask is given in Appendix B.3. The gradient in (2) is the sum of two terms: (i) reward + trust-region term sampled from πθ and (ii) data-adherence (SFT) term sampled from πβ. Both terms map to the same estimator via (3)(6) by choosing πre accordingly (e.g., πθold for on-policy trust-region updates and πβ for SFT/offline updates). Therefore, SFT and RL optimize single Common Objective (1) and can be trained jointly within one loss without intrinsic conflict. 3.3 Gradient Component Analysis Takeaways 1. While all algorithms share the same Common Objective, bias-variance trade-offs still exist across current instances for different components of the unified gradient estimator. 2. We can improve the post-training process by constructing better and more suitable estimation of the policy gradient. Across the wide spectrum of algorithms contained in our previous discussions and Table 1, it can be inferred that the four components that construct the unified gradient estimator are motivated by different procedures in the post-training process. To better illustrate the relationship between the different processes with the respective components of our unified gradient, we present Figure 1. We divide the post-training process of LLMs into the four steps shown in Figure 1: i) First, the LLM makes the decision on its data source, either to use data from an offline demonstration dataset, from self-generated rollout data, or mixture of both. In this process, the policy likelihood πθ of the data with respect to the current LLM is generated. ii) Given the data source used for data generation, reference policy πre is calculated. iii) After data collection is complete, the algorithm calculates the advantage estimation ˆA for each token/sequence. iv) Lastly, the algorithm may choose to apply an additional masking procedure 1 stable to disable the gradient calculation of various tokens, which could lead to theoretical or numerical stability issues. After these four steps, the components are collected to construct the policy gradient gradUni, which is used to update the LLM in the system. Similar to GAE presented in (Schulman et al., 2015b), multiple instantiations exist to estimate the policy gradient. However, different component selections introduce various degrees of bias and variance, where trade-off is often encountered. We provide the following discussion on key components of the unified gradient below. Reference Policy Calculation Practically speaking, the reference policy denominator places weight on each token-level update such that any token with smaller probability, often implying more significance, is weighted more. SFT and REINFORCE assign weights 7 Unify Post-Training 2025-09-05 inversely proportional to the current policy πθ, enforcing bigger update when the model outputs small probability. On the other hand, when the data is generated with an outdated model, algorithms such as PPO assign weights inversely proportional to the rollout policy πθold , and offline RL does not assign additional weights for tokens. Theoretically, the reference policy is usually set given the source of the dataset and/or the rollout policy. For online RL methods that train purely with on-policy data, such as REINFORCE (Ahmadian et al., 2024), uses 1 , which produces an unbiased estimate for πθ gradient calculation. However, these methods usually suffer from high variance. For PPOstyle online RL algorithms, the reference policy refers to the rollout policy, which is result of importance sampling. PPO is numerically simplified version of TRPO (Schulman et al., 2015a). PPO makes conservative updates that effectively reduce variance. However, the important sampling ratio is in fact theoretically ill-posed and could introduce systematic bias, as discussed in GSPO (Zheng et al., 2025). GSPO has also proposed novel calculation for πre , as shown in Table 1. On the other hand, in the offline setting, the choice for reference policy πre is limited, since the algorithm generally has no access to the rollout policy. If we are given the assumption that the offline data evenly covers the entire state-action rollout space, then the importance sampling ratio r(θ) = πθ (τ) πre (τ) reduces to πθ(τ) by setting constant πre (τ) = 1. Notably, it is apparent that setting πre (τ) = 1 introduces much bias at the cost of numerical stability. For the SFT case, we can consider that the domain-specific dataset is generated with respect to the expert policy π; therefore, no weighted sampling is required. Neither of the two approaches is entirely theoretically justified, from an RL perspective; both require lower bound on the state-action visitation of all the possible state-action pairs (Kakade, 2003), which can not be satisfied due to the severely limited datasets in practice. Apart from the strong connection to data source and sampling polices, some studies employ hand-crafted reweight factor within the reference policy denominator. These works (Yan et al., 2025; Zhang et al., 2025b) typically find desirable token properties and purposefully place higher/lower weight on these desirable/undesirable tokens, respectively. Choice of Stabilization Mask The clipping operation introduced in PPO was the first to explicitly add stop gradient operation on LLM post-training. Clipping gradient estimation where the importance sampling strays too far from 1 is an effective approach to address high variances. However, this aggressive clipping behavior has been criticized by some to be overly conservative: Both DAPO (Yu et al., 2025) and CISPO (Chen et al., 2025) stated that the classical PPO approach drops all the tokens corresponding to large model updates, and that many such tokens are in fact crucial for stabilizing entropy and facilitating scalable RL. DAPO presented slight modification to the clipping threshold, and CISPO further extended the notion of token-wise mask, where more granular tuning was introduced to decide whether gradients from specific tokens should be dropped. The recent work of Cui et al. (2025b) has demonstrated that many existing algorithms negatively impact the output entropy during training and introduced Clip-Cov, adding another clipping mechanism to address the entropy-collapse encountered in training. While these methods demonstrated performance enhancements in practice, they also provide additional sources of bias. On the other hand, works such as GSPO (Zheng et al., 2025) have stated that the PPO-style clipping is inherently noisy and inefficient for sample exploitation: GSPO clips much larger fraction of tokens and yet demonstrated superior training efficiency. In addition, post-training algorithms using offline data have chosen to purposefully remove the clipping from training, mostly guided by performance. Though setting πre (τ) = 1 as the policy denominator does effectively reduce the instability in gradient calculations. Advantage Estimation There are two commonly used settings for estimating the sequencelevel advantage function: the fixed advantage setting and the adaptive advantage setting. The fixed setting considers ˆA = 1 given the rule-based verification, which is adapted by REINFORCE and implicitly by SFT (where all sequences are positive samples). Alternatively, recent studies have focused on using adaptive advantage estimations, performing 8 Unify Post-Training 2025-09re-centering or normalization based on the performance of the current rollout group. Notably, GRPO and its variants, such as DAPO (Yu et al., 2025) and LUFFY (Yan et al., 2025), use unit normalization such that the advantage estimation of the group has unit standard deviation. Other approaches, such as Dr. GRPO (Liu et al., 2025b), RLOO (Ahmadian et al., 2024), and REINFORCE++ (Hu et al., 2025a), claim that dividing the standard deviation introduces difficulty bias and that only recentering is adequate. Apart from sequence-level advantage estimate ˆAi,j, recent works (Wang et al., 2025; Yang et al., 2025; Sun et al., 2025) have also adapted more granular token-level advantage estimate ˆAi,j,t to varying degree of success. Combination of Gradient Estimators Although bias-variance trade-offs exist for the gradient estimator, we state that, given data distribution assumptions and sufficient data samples, all policy gradient estimators covered in our framework should result in an effective direction of improvement for the Common Objective. To effectively reduce the variance and bias for each policy update, we can treat instances of policy gradient as different noisy measurements of the true policy gradient, and perform weighted average to generate more accurate gradient estimation, similar to complementary filters (Marantos et al., 2015). However, the complexity of LLM RLVR introduces additional challenges. The current state of the behavior policy πθ and its relationship with the respective tasks also greatly impacts the bias-variance tradeoff of each instance of the gradient estimator. For instance, RL-zero is significantly more effective for the Qwen model series compared to LlaMA, but SFT is effective for both methods (Zeng et al., 2025a); SFT RL and RL SFT also yield significantly different results on the same LLM (Fu et al., 2025). We argue that for constructing post-training algorithm with better effectiveness and efficiency, dynamic and adaptive mechanism is crucial to construct optimal gradient components. 3.4 Hybrid Post-Training with Performance Feedback Our unified perspective above shows that different post-training losses have the same optimization objective with different characteristics. Inspired by this view, we propose the Hybrid Post-Training (HPT) algorithm. We use mixed loss = αLRL + βLSFT, which contains the weighted on-policy RL loss LRL and SFT loss LSFT, to optimize the target LLM πθ. The weights of the two losses (α and β) are determined by the real-time sampling performance of the model. Performance on Single Question. For any question provided to the LLM, we first obtain both supervising trajectory τ and the models performance on the question. Specifically, we draw on-policy trajectories {τi}n i=1 πθ( q) and evaluate them with verifier : τi {0, 1}. This verifier is the same as the rule-based reward function and the models performance is defined as the mean of these verification scores: (cid:26) if τi contains the correct answer of (7) v(τi) = R(τi) = 0 otherwise i=1 Intuitively, indicates how well the current policy performs on across multiple trajectories. v(τi) = 1 (8) Feedback Coefficients. Then, we obtain the coefficients of on-policy RL loss α and SFT loss β based on the performance feedback: α = (P), β = g(P), (9) where the and are the specific feedback functions. Experientially, when the model demonstrates strong capability, it is advantageous to emphasize on-policy RL to foster 9 Unify Post-Training 2025-09-05 Algorithm 1 The Hybrid Post-Training (HPT) Algorithm Input: Pretrained LLM (policy) πθ; SFT dataset DSFT = {(q, τ)} with supervising trajectories τ; verifier v; on-policy samples number n; total training steps T; feedback functions and g; learning rate η Output: Fine-tuned policy πθ . for = 1 to do for = 1 to do Sample trajectory τi πθ( q) Evaluate with verifier (rule-based reward): v(τi) R(τi) {0, 1} i=1 v(τi) α (P), end 1 Compute on-policy RL loss LRL using rollouts {τi} and normalized advantages derived from {R(τi)}. Compute SFT loss LSFT on the supervising trajectory τ. α LRL + β LSFT # Mixed loss with performance feedback coefficients θ θ η θL β g(P) # Performance feedback on question end return πθ exploration; conversely, when the models competence is limited, SFT should take precedence to ensure correct guidance. Consequently, ought to be positively correlated with P, whereas should exhibit negative correlation. In this paper, we employ pair of simple yet empirically effective switch functions and g: α = (P) = (cid:26)1 if > γ 0 if γ , β = g(P) = (cid:26)1 0 if γ if > γ (10) The switch gate γ enables the model to perform SFT when its performance falls below predefined threshold, and RL otherwise. Mixed Loss. Finally, we calculate the RL loss LRL with the already generated on-policy trajectories τi and SFT loss LSFT with the supervising trajectory τ, and we use Dr. GRPO as the on-policy RL algorithm: LRL = 1 i=1 τi t=1 (cid:16) min ri,t Ai,t, clip(cid:0)ri,t, 1 ϵ, 1 + ϵ(cid:1) Ai,t (cid:17) LSFT = 1 τ τ t=1 log πθ(τ q, τ <t) (11) (12) where ri,t = πθ(τi,tq,τi,<t) R(τi)mean({ R(τi) i=1,2,...,n}) std({ R(τi) i=1,2,...,n}) πθold (τi,tq,τi,<t) is the per-token importance sampling ratio, Ai,t Ai = is the advantage and ϵ is the clip gate hyperparameter. The mixed loss is then obtained by taking weighted average of these two losses using performance feedback coefficients α and β: = αLRL + βLSFT (13)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Models To evaluate the generalizability of HPT across different backbone models, we conduct experiments using Qwen and LLaMA models of various scales. The models we experiment with are as follows: 10 Unify Post-Training 2025-09-05 Table 2: In-distribution and out-of-distribution performance of HPT and baselines on Qwen2.5-Math7B. means the results are taken from the corresponding paper. Model Qwen2.5-Math-7B SFT GRPO SFT GRPO LUFFY SRFT HPT Qwen2.5-Math-7B-Ins. PRIME-Zero SimpleRL-Zero OpenReasoner-Zero Oat-Zero AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg ARC-c GPQA Avg In-Distribution Out-of-Distribution 12.3 25.1 19.4 25.7 26.1 18.4 33.0 11.8 17.0 27.0 16.5 33. 4.7 22.8 13.8 21.6 21.8 15.5 21.9 9.8 12.8 6.8 15.0 11.9 33.0 56.1 59.1 62. 66.2 55.9 69.4 48.3 54.0 54.9 52.1 61.2 43.6 84.2 81.8 84.6 88.4 83.8 89.2 83.2 81.4 76.0 82.4 78. 8.8 33.8 38.2 38.2 41.9 42.6 46.0 34.2 39.0 25.0 33.1 34.6 13.6 44.7 46.2 46. 54.1 48.9 56.9 39.3 40.3 34.7 47.1 43.4 19.3 44.5 43.1 46.5 49.8 44.2 52.7 37.8 40.8 37.4 41.0 43. 30.9 67.4 81.2 67.7 80.8 80.5 81.6 72.7 73.3 30.2 66.2 70.1 28.3 25.3 36.4 30. 39.4 36.8 42.9 29.3 18.2 23.2 29.8 23.7 29.6 46.4 58.8 49.3 60.1 58.7 62.3 51.0 45.8 26.7 48.0 46. Qwen Family: Qwen2.5-Math-1.5B, Qwen2.5-Math-7B (Yang et al., 2024); LLaMA Family: LLaMA-3.1-8B (Grattafiori et al., 2024); Benchmarks We evaluate HPT on 6 mathematical reasoning benchmarks: AIME 2024 (Li et al., 2024), AIME 2025 (Li et al., 2024), AMC (Li et al., 2024), MATH-500 (Hendrycks et al., 2021a), Minerva (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). AMC (Li et al., 2024) comprises problems drawn from the AMC12 2022 and AMC12 2023 examinations. Moreover, when employing Qwen2.5-Math-7B as the backbone, we further conduct evaluations on GPQA-Diamond (Rein et al., 2024), challenging and high-quality subset of the Graduate-Level Google-Proof Question Answering benchmark, as well as on ARC-c (Clark et al., 2018), an open-domain reasoning benchmark. Evaluation Setup We set the maximum generation length to 8, 192 tokens, unless otherwise specified. For the main experiments, following DeepSeek-R1 (Guo et al., 2025), we adopt the Pass@k evaluation protocol (Chen et al., 2021) and report Pass@1 using non-zero temperature sampling. To ensure fair comparison with previous works (Yan et al., 2025; Fu et al., 2025), we compute avg@32 for AIME 24, AIME 25, and AMC (avg@1 for others) using temperature of 0.6 and top-p value of 0.95 for accuracy calculation. Baselines Since HPT dynamically integrates GRPO (Shao et al., 2024) and SFT, the most natural baselines are SFT and GRPO individually. Furthermore, we compare HPT against the mix-policy approach LUFFY (Yan et al., 2025). For experiments using Qwen2.5-Math7B as the backbone, we additionally include SFTGRPO and SRFT (Fu et al., 2025) as baseline, as well as models trained with the Zero-RL procedure on the same backbone for more comprehensive comparison. We also use PRIME-Zero (Cui et al., 2025a), SimpleRLZero (Zeng et al., 2025b), OpenReasoner-Zero (Hu et al., 2025b) and Oat-Zero (Liu et al., 2025b) as baselines. Implementation Details We apply GRPO (Shao et al., 2024) as the RL algorithm to implement HPT. We introduce gating mechanism that adaptively assigns the coefficients α and β to the RL loss and the SFT loss based on the rollout performance, respectively. Formally, the gating mechanism is defined as: (α, β) = (cid:40)(0, 1), if γ, (1, 0), if > γ, where denotes models performance as introduced in Section 3.4 and γ is the gate threshold. We fix γ at 0 throughout all experiments on the Qwen Family models and 2 for LLaMA, and provide relative ablation studies in Section 5.5. For hyperparameters, we use constant learning rate of 5 106 and adopt the AdamW optimizer for the policy model. For rollout, The results of SRFT are based on our own implementation, as the official code is not public. 11 Unify Post-Training 2025-09-05 Table 3: Performance of HPT and baselines on LaMA3.1-8B and Qwen2.5-Math-1.5B. means the results are taken from the LUFFY paper (Yan et al., 2025). Model LLaMA3.1-8B SFT GRPO LUFFY HPT Qwen2.5-Math-1.5B SFT GRPO LUFFY HPT AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg 0.4 0.5 0.3 1.9 2. 2.8 14.7 12.2 14.1 16.6 0.1 0.1 0.5 0.1 1.2 6.1 17.6 8.5 9.4 17. 4.7 5.4 9.4 13.5 18.6 24.5 45.4 43.8 43.5 51.0 13.8 20.2 23.4 39.0 47. 32.8 78.4 71.0 75.2 81.0 4.8 4.0 17.6 15.1 18.8 11.0 29.4 33.1 26.1 37. 3.9 5.3 6.1 9.6 20.4 16.4 35.7 35.3 39.7 47.3 4.6 5.9 9.6 13.2 18. 15.6 36.9 34.0 34.7 41.9 we sample 8 responses using temperature of 1.0. The maximum generation length is set to 8, 192 tokens for all other models. For other details that may not have been explicitly introduced, we have endeavored to follow previous works as closely as possible (Zhao et al., 2025; Zuo et al., 2025). All experiments were conducted on 8 NVIDIA A800 80GB GPUs. 4.2 Main Results Table 2 presents the overall performance of HPT on Qwen2.5-Math-7B. As introduced in Section 3.4, in our implementation of HPT, the coefficients of the RL and SFT loss terms are both degraded and simplified into binary form. Despite this highly streamlined experimental setup, HPT still yields substantial performance gains. It not only significantly outperforms both SFT-only and GRPO-only baselines, but also surpasses SFTGRPO, which requires substantially higher computational cost. This suggests that simply concatenating the two training stages is not the most effective strategy. Moreover, HPT achieves marked improvements over existing mixed-policy approaches such as LUFFY and SRFT, with particularly notable gains of 6.9 and 14.6 points on AIME 2024, respectively. Furthermore, we conduct experiments on models of different scales and families to evaluate the effectiveness of HPT, including LLaMA3.1-8B and Qwen2.5-Math-1.5B, as shown in Table 3. Compared with SFT, GRPO, and LUFFY, HPT achieves substantial performance gains."
        },
        {
            "title": "5 Empirical Analysis",
            "content": "Our empirical analysis progressively reveals how HPT reconciles exploration and exploitation, stabilizes training, and ultimately enhances the reasoning ability. We begin in 5.1 with an examination of exploration and exploitation. In 5.2, we provide training visualization, contrasting HPT with the conventional SFTGRPO. Next, 5.3 investigates fine-grained training metrics of HPT. Building on this, 5.4 explores the role of off-policy RL, testing whether alternative strategies for utilizing offline data yield benefits. Finally, 5.5 presents gate threshold ablation study. 5.1 Exploration and Exploitation HPT inherently achieves an adaptive switching between RL and SFT. These two paradigms naturally correspond to the learning modes of exploration and exploitation. Accordingly, we can examine whether HPT addresses the initial challenges from both perspectives. Exploration From the exploration perspective, we want to analyze the models Pass@k performance after training with HPT. Recently, Limit-of-RLVR (Yue et al., 2025) demonstrated that while RLVR training yields significant improvement in Pass@1, it does not lead to gains in large-k Pass@k. In other words, RLVR does not expand the capability boundary of the base model. This finding has sparked broad discussions regarding the relationship between models exploratory capacity and its Pass@k performance. Moreover, Pass@k has increasingly been recognized as widely accepted metric for evaluating both the upper bound of model capability and its exploration ability. We follow Yue et al. (2025) to evaluate 12 Unify Post-Training 2025-09-05 Figure 2: Pass@k performance of HPT against baselines on Qwen2.5-Math-7B. The evaluation spans 3 benchmarks, with Pass@k values estimated via bootstrap sampling from set of 2048 generated solutions per problem. Pass@k up to 1024 for each problem of AIME25, AIME24, and AMC for Pass@k evaluation. Based on these sets of generated solutions, we apply bootstrap sampling to obtain accurate estimates of Pass@k scores for various values of k. Figure 2 illustrates the resulting Pass@k curves, comparing HPT against baselines and the base model. First, we can observe that methods incorporating SFT achieve higher large-k Pass@k compared to the GRPO (purely RL). This may be attributed to the introduction of data outside the models own distribution during SFT, which increases output uncertainty while also providing new knowledge from offline data, thereby enhancing the models exploratory capacity. Furthermore, we identify an interesting phenomenon: since HPT dynamically integrates RL (GRPO) with SFT, we might intuitively expect its large-k Pass@k performance to fall between that of the two individual methods. However, HPT achieves the highest large-k Pass@k performance overall. This indicates that Hybrid Post-Training not only delivers substantial improvements in Pass@1, but also maximally preserves and enhances the models exploratory ability. Table 4: Bidirectional analysis of exclusive solves on MATH-500, comparing the Qwen2.5-Math-7B trained with HPT against baseline methods (GRPO and LUFFY). The notation +X / -Y in each cell indicates the performance trade-off: +X represents the number of problems solved by the HPT but not the baseline, while -Y represents the number solved by the baseline but not by the HPT. Methods GRPO Level 1 (N=43) Level 2 (N=90) Level 3 (N=105) Level 4 (N=128) Level 5 (N=134) Overall (N=500) Absolute Percentage +0.0%/-0.0% +5.6%/-1.1% +8.6%/-1.9% +13.3%/-3.1% +20.1%/-6.0% +11.6%/-3.0% +58/-15 +17/-4 +27/- +9/-2 +0/-0 +5/-1 LUFFY Absolute Percentage +2.3%/-0.0% +5.6%/-1.1% +4.8%/-2.9% +7.8%/-3.9% +16.4%/-5.2% +8.6%/-3.2% +43/- +10/-5 +22/-7 +5/-3 +1/-0 +5/-1 Exploitation From the exploitation perspective, the key question is whether our method, by leveraging SFT, enhances the models initial competence and facilitates subsequent RL training. As illustrated in Figure 3, RL training alone may fail to solve many problems (white line), requiring the dynamic intervention of SFT. To investigate this, we analyze its exclusive solves against the GRPO and LUFFY, building upon the results from the evaluation on MATH-500 with Qwen2.5-Math-7B as the backbone, as shown in Table 4. The red numbers denote problems that are solved by our method but not by GRPO or LUFFY, i.e., problems newly acquired through our training procedure. Three clear trends emerge from the analysis: First, the red counts consistently increase with problem difficulty, suggesting that HPT improves the models ability to tackle more challenging problems. 13 Unify Post-Training 2025-09-05 Figure 3: GRPO training dynamics of SFTGRPO on Qwen2.5-Math-1.5B across 50 training epochs. We visualize the models per-question sampling accuracy throughout the training process. Figure 4: Performance difference (HPT v.s. SFTGRPO) on Qwen2.5-Math-1.5B across 50 training epochs. diverging color scale indicates the advantage: red for HPT, blue for SFTGRPO, and white for no difference. Second, the green counts within the red boxes remain essentially unchanged across settings: this indicates that, compared with existing methods, HPT preserves performance on problems that the model could already solve, thereby mitigating the risk of catastrophic forgetting. Finally, the fact that the red counts are consistently large relative to both baselines demonstrates that our method enables the model to acquire substantial number of problems that prior approaches struggled to solve. 5.2 Training Visualization To facilitate fine-grained examination of the training process and thereby obtain deeper insights into how HPT works, we conduct visualization analysis comparing the SFTGRPO approach with HPT. We sample 255 problems from the MATH dataset (Hendrycks et al., 2021b) for subsequent training, with 85 problems each from Levels 3, 4, and 5. For SFTGRPO, we perform 50 epochs of GRPO on Qwen2.5-Math-1.5B model fine-tuned with SFT, tracking rollout accuracy across training, as shown in Figure 3. We track the rollout accuracy for each sample throughout the entire training process. To highlight difficulty effects, we focus on Levels 3 and 5 as representative cases. The left subplot shows Level 3 (easier) problems, and the right shows Level 5 (hardest). Notably, GRPO frequently produces dense white regions, and sometimes even continuous white lines, reflecting widespread rollout errors across outputs. This illustrates core limitation of RL methods: they struggle to learn effectively when frequent rollout errors occur across all outputs. Unify Post-Training 2025-09-05 In parallel, we train Qwen2.5-Math-1.5B from scratch for 50 epochs to visualize HPT. To compare against SFTGRPO and enable more intuitive comparison, we conduct differential analysis of the training dynamics. Specifically, we calculate the accuracy difference at corresponding positions (at matched prompts and steps) in the evaluation grid between two methods: red indicates HPT is better, blue the opposite. Figure 4 presents the results of the difference plots. Notably, SFTGRPO actually requires greater computational resources than HPT: it involves preceding SFT phase, and our approach also reduces computational costs during the transition from GRPO to SFT, as expensive operations such as rollouts are no longer required. This unfair comparison leads to an initial dominance of the blue regions, which is expected since the SFT stage in SFTGRPO has already incorporated substantial prior knowledge. However, in the later stages of training, HPT still surpasses and ultimately reveals the dominance of the red regions, indicating that HPT consistently outperforms SFTGRPO by substantially enhancing learning performance on the training set. This advantage becomes even more pronounced in the Level 5 subplot, suggesting that HPT provides particular benefits for learning on more challenging problems, which may be attributed to its use of question-level rollout performance as feedback. 5.3 Training Dynamics In this section, we investigate the training dynamics of HPT, focusing on validation performance, entropy, response length, and the offline data ratio. Our analysis centers on two aspects: whether HPT enables the model to acquire knowledge from offline data when its initial capabilities are limited, and whether its performance can be further enhanced through continued exploration with reinforcement learning. Figure 5: Validation performance comparisons on Qwen2.5-Math-1.5B across benchmarks. Validation Performance. We track the validation performance on the Qwen2.5-Math-1.5B as shown in Figure 5, where HPT consistently outperforms the baselines and delivers stable improvements across multiple benchmarks. Offline Data Ratio. We begin by quantifying the fraction of prompts whose gradients update the model through the SFT loss versus the RL loss at each training step, as shown in Figure 6. The offline data ratio is defined as the proportion of offline samples relative to the total number of training samples in each batch, with online samples calculated based on the remaining batch capacity. As expected, when the model has not yet acquired competence on the target tasks, the early phase is characterized by large proportion of SFT-driven updates. As training progresses and the models onpolicy reward increases, the mixture gradually shifts: the contribution of RL grows while that of SFT diminishes, eventually stabilizing at small but non-zero level. This trend is observed for both Qwen2.5-Math-7B and Qwen2.5Figure 6: Dynamic offline data ratio dynamics during training. The offline data ratio is calculated as the proportion of offline training samples relative to the total training data at each step. 15 Unify Post-Training 2025-09Math-1.5B. The weaker 1.5B model remains in the SFT-dominated regime for longer period before transitioning, whereas the stronger 7B model shifts earlier. These results align with our technical analysis of the design of HPT, where the mixing ratio is automatically adjusted based on performance rather than fixed in advance like LUFFY. Figure 7: Comparisons of training dynamics across different methods: (left) The entropy measures the diversity of model outputs, indicating exploration behavior; (right) The response length tracks the average length of generated responses. Entropy and exploration. Figure 7 (left) tracks token-level entropy over 500 steps. HPT maintains higher entropy than GRPO throughout the training phases. This is expected as the offline SFT trajectories are derived from the external demonstration distribution, which consequently increases the diversity in the models outputs. Response length and acquired reasoning patterns. Figure 7 (right) reports the average response length. Our offline SFT trajectories have length of up to 8k tokens. Under HPT, the models response length increases quickly during the early steps but does not jump to the 8k ceiling. More importantly, after the method shifts toward RL and the SFT proportion plateaus at low level, the response length does not regress. This persistence suggests that the model has internalized long-form reasoning routines from the offline data rather than merely echoing teacher outputs. In other words, the learned reasoning pattern becomes part of the policy, and RL fine-tuning refines it instead of erasing it. 5.4 Impact of Off-policy RL Table 5: Performance of different training paradigms to evaluate the impact of Off-policy RL. SFT/ON denotes SFT/On-policy (HPT), OFF/ON denotes Off-policy/On-policy, and Mix/ON denotes Mixpolicy/On-policy. Name AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg OFF/ON Mix/ON SFT/ON 16.6 16.7 16.6 11.8 17.2 17. 47.3 46.9 51.0 76.2 79.4 81.0 35.3 37.5 37.5 41.6 43.9 47.3 38.1 40.3 41.9 In our work, we have only made preliminary attempts at unifying post-training by integrating RL with SFT. However, off-policy RL represents an important training paradigm that emphasizes leveraging offline data. To this end, we further conduct experiments to investigate its influence and potential role. We compare three different training paradigms: (1) SFT/On-policy, the model alternates between SFT and on-policy RL, which corresponds to the method we introduced above (HPT); (2) Off-policy/On-policy, the model alternates between off-policy RL and on-policy RL during training; and (3) Mix-policy/On-policy, the model combines the loss from SFT and off-policy RL, and dynamically switches it with the on-policy RL objective. For the Mix setting, we performed hyperparameter search and found the optimal SFT/OFF weighting ratio to be 1/10, i.e., the coefficients of the SFT loss and the off-policy loss are set to 0.1 and 1.0, respectively. We replicate the off-policy RL implementation described in LUFFY (Yan et al., 2025), and all experiments are conducted in the same settings to ensure fairness. 16 Unify Post-Training 2025-09-05 We evaluate the results of three methods on six math benchmarks. Table 5 presents results. Overall, the SFT/ON method achieves the best average performance (41.9), outperforming both Mix/ON (40.3) and OFF/ON (38.1). This suggests that off-policy RL may not be essential, as SFT already serves effectively as the training method of HPT for learning from offline data. 5.5 Gate Threshold Ablation Figure 8: Training reward (left) and offline data ratio (right) comparisons across different gate settings on Qwen2.5-Math-1.5B. In this section, we investigate the effect of different gate thresholds γ. value of γ = 0 indicates that the model switches to SFT only when it fails all questions. Similarly, γ = 1 and γ = 2 correspond to settings where the model remains in on-policy reinforcement learning as long as it answers at least one or two out of eight questions correctly, respectively. To visualize the impact of the gating mechanism, we conduct experiments on the Qwen2.5Math-1.5B under three different gate settings. As shown in Figure 8, we analyze the training dynamics by tracking the dynamics of rewards and the proportion of offline data utilized throughout training, thereby highlighting how different gate thresholds mediate the balance between leveraging offline demonstrations and incorporating online feedback. We observe that, under different gate thresholds, varying degrees of engagement with offline databased SFT learning emerge. larger gate threshold introduces greater extent of SFT based on offline data, as expected. Table 6: Performance of HPT with different switch gate γ on Qwen2.5-Math-1.5B. Name AIME 24 AIME 25 AMC MATH-500 Minerva Olympiad Avg γ = 2 γ = 1 γ = 0 77.6 75.4 81.0 34.6 35.7 37. 15.8 18.1 16.6 39.0 38.7 41.9 49.0 46.0 51.0 13.0 14.2 17.8 44.1 42.5 47.3 To further compare the performance across different gating strategies, we evaluate the three trained models on six benchmarks. Table 6 presents the results. Among the three configurations, γ = 0 achieves the best overall performance with an average score of 41.9, outperforming both γ = 1 (38.7) and γ = 2 (39.0). This observation suggests that simply incorporating more SFT does not necessarily lead to better outcomes. Instead, it is crucial to maintain dynamic balance between the exploration of RL and the exploitation of SFT. The optimal degree of this gating mechanism should be adjusted according to the characteristics of the base model and the specific training data employed."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce the Unified Policy Gradient Estimator to provide theoretical framework for LLM post-training. We demonstrate that SFT and RL optimize common objective, with their respective gradients representing different bias-variance tradeoffs. Motivated by this unified perspective, we propose Hybrid Post-Training (HPT), an algorithm 17 Unify Post-Training 2025-09-05 that dynamically adapts between SFT for exploitation and RL for exploration based on real-time performance feedback. Extensive empirical validation shows that HPT consistently outperforms strong baselines, including sequential and static mixed-policy methods, across various models and benchmarks. Our work contributes both unifying theoretical perspective on post-training and practical algorithm that effectively balances exploitation and exploration to enhance model capabilities."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ust un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in neural information processing systems, volume 30, 2017. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416, 2022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025a. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025b. Hanze Dong, Wei Xiong, Deep Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Wang. Raft: Reward ranked finetuning for aligning language models with human feedback. arXiv preprint arXiv:2304.06767, 2023. Kawin Ethayarajh, Lawrence Gao, and Dan Jurafsky. Kahneman-tversky optimization (kto): new way to align language models. arXiv preprint arXiv:2402.01306, 2024. Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, and Dongbin Zhao. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. 18 Unify Post-Training 2025-09-05 Amelia Glaese, Nat McAleese, Maja Mladenov, oren Kaufmann, Amanda Askell, Phillip Butler, Tsim Chen, Courtney Voss, Vlad Cirrocessing, Rachael Cummings, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Caglar Gulcehre, Tom Jones, Ksenia Konyushkova, Florian Besse, David Budden, Angeliki Lazaridou, Son Nguyen, Razvan Dadashi, Jia He, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Lixuan He, Jie Feng, and Yong Li. Amft: Aligning llm reasoners by meta-learning the optimal imitation-exploration balance, 2025. URL https://arxiv.org/abs/2508.06944. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b. Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025a. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025b. URL https://arxiv.org/abs/2503.24290. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Sham Machandranath Kakade. On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom), 2003. Min-Joon Kim, Aviral Singh, and Hong-Seok Lee. Dynamic policy fusion for mixed-signal llm alignment. In Third Conference on Language Modeling, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Jia Li, Zhaofeng Wang, and Junxian He. Self-guided exploration with offline demonstrations for complex reasoning. In Proceedings of the International Conference on Learning Representations, 2025. Unify Post-Training 2025-09-05 Hao Liu, Zixuan Ji, and Di Lu. Bridging the gap between supervised fine-tuning and reinforcement learning. arXiv preprint arXiv:2308.08809, 2023. Jason Liu, Zhiyuan Chen, and Ji-Woo Park. Direct fine-tuning on rewarded trajectories for language model alignment. arXiv preprint arXiv:2406.13581, 2024. Jiazhen Liu, Yuchuan Deng, and Long Chen. Empowering small vlms to think with dynamic memorization and exploration, 2025a. URL https://arxiv.org/abs/2506.23061. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Zihan Liu, Alekh Agarwal, and Nan Jiang. principled analysis of offline preference optimization algorithms. Journal of Machine Learning Research, 26(45):158, 2025c. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Panos Marantos, Yannis Koveos, and Kostas Kyriakopoulos. Uav state estimation using adaptive complementary filters. IEEE Transactions on Control Systems Technology, 24(4): 12141226, 2015. Eric Mitchell, Sergey Levine, and Chelsea Finn. Leveraging offline datasets for efficient online rl in large language models. In Proceedings of the International Conference on Machine Learning, 2024. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Ji-Woo Park, Yifan Chen, and Denny Zhou. Reward-reweighted sft: An offline policy refinement method. arXiv preprint arXiv:2502.11842, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Chelsea Finn, and Christopher D. Manning. Direct preference optimization: Your language model is secretly reward model. arXiv preprint arXiv:2305.18290, 2023. Neel Rajani, Aryo Pradipta, Gema Seraphina Goldfarb-Tarrant, and Ivan Titov. Scalpel vs. hammer: GRPO amplifies existing capabilities, SFT replaces them, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015a. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 20 Unify Post-Training 2025-09Aviral Singh, Joey Hong, and Aviral Kumar. Beyond reward: Offline preference-guided policy learning. In Advances in Neural Information Processing Systems, volume 36, 2023. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel Ziegler, Ryan Lowe, Christopher Hesse, John Schulman, and Jacob Hilton. Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 33:30353045, 2020. Wei Sun, Wen Yang, Pu Jian, Qianlong Du, Fuwei Cui, Shuo Ren, and Jiajun Zhang. Ktae: model-free algorithm to key-tokens advantage estimation in mathematical reasoning. arXiv preprint arXiv:2505.16826, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. arXiv preprint arXiv:1805.01954, 2018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Hugo Touvron, Louis Martin, and Guillaume Lample. Context distillation for on-policy reinforcement learning in llms. In First Conference on Language Modeling, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Gu, Aitor Lewkowycz, Yao Lu, Ambrose Slone, Quoc Le, and Barret Zoph. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Jeff Wu, Long Ouyang, and Nisan Stiennon. Alternating between on-policy and off-policy updates for efficient and stable llm alignment. arXiv preprint arXiv:2401.08543, 2024. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. Treerpo: Tree relative policy optimization. arXiv preprint arXiv:2506.05183, 2025. Hiroshi Yoshihara, Taiki Yamaguchi, and Yuichi Inoue. practical two-stage recipe for mathematical llms: Maximizing accuracy with sft and efficiency with reinforcement learning. arXiv preprint arXiv:2507.08267, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 21 Unify Post-Training 2025-09-05 Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. In Second Conference on Language Modeling, 2025a. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025b. Notion Blog. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025a. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025b. Weizhe Zhao, Benjamin Packer, and Ilya Kostrikov. Reward model fine-tuning using relative gradient updates. arXiv preprint arXiv:2310.10574, 2023. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Chunting Zhou, Graham Neubig, and Junxian He. Prefix-tuning for guided text generation in reinforcement learning. Transactions of the Association for Computational Linguistics, 11: 12341249, 2023. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. Unify Post-Training 2025-09-"
        },
        {
            "title": "A Gradient Derivation for Classical Algorithms",
            "content": "A.1 Gradient of SFT We first consider the SFT process as warm-up. As mentioned in the previous section, SFT takes pre-trained foundation model and further makes the model more specialized by training its output prediction distribution to align with domain-specific data. The fine-tuning process uses the same cross-entropy loss as in model pre-training, defined as follows, τi t=1 where DSFT = {(qi, τi)}i[N] denotes the SFT dataset consisting of question and trajectory pairs. τt denotes the t-th token in the trajectory and τ<t denotes all the tokens prior to τt. log πθ(τi,tqi, τi,<t). LSFT(θ) = i= (14) For any t, the LLM outputs the next-token prediction as probability distribution. In the context of RL, such probability distribution has been commonly considered as stochastic policy. Then, the gradient calculation of SFT can be obtained by directly taking the derivative of Equation (14) and takes the following form: JSFT(θ) = LSFT(θ) = i=1 τi t=1 πθ(τi,tqi, τi,<t) 1 πθ(τi,tqi, τi,<t) . (15) In this section, we slightly abuse the notion of policy gradient and consider the SFT as case of behavioral cloning (BC) (Torabi et al., 2018), and Equation (15) can be seen as specific form of policy gradient. A.2 Gradient of Online RL: PPO, GRPO and Beyond For online RL, we first consider Proximal Policy Optimization (PPO) (Schulman et al., 2017) and series of its derivations. PPO is pivotal technique for RLVR in LLMs. Motivated by TRPO, PPO keeps the new policy close to the old policy, and perform conservative policy updates by incorporating clipped version of its policy ratio in its objective. The clipping function was shown to stabilize the training process and avoid performance collapse during training. In this section, we omit the regularization terms, such as the KL divergence and entropy. The loss objective for PPO can be written as follows, LPPO(πθ) = 1 i=1 1 j=1 1 τj τj t=1 min(ri,j,t(θ) ˆAi,j, clip(ri,j,t(θ), 1 ϵ, 1 + ϵ) ˆAi,j), (16) In this setting, we consider questions sampled from given dataset DRL {qi}N i=1, and for each question, we consider trajectories independently sampled using reference policy πre . We use ri,j,t(θ) = πθ (τi,j,tqi,τi,j,<t) πre (τi,j,tqi,τi,j,<t) to denote the policy ratio πθ/πre introduced for importance sampling, ϵ denotes the clipping factor for the importance sampling ratio, enhancing stability. For PPO, ˆA is estimated using the Generalized Advantage Estimation (GAE) (Schulman et al., 2015b), calculated based on the reward of the sampled trajectories. For the case of GRPO, the advantage estimate ˆA is calculated based on set of sampled trajectories. Given question qi, group of sampled roll-out trajectoried {τi,j}j[G] with verifiable reward R(τi,j) {0, 1}, ˆAi,j is calculated as the normalized reward over the group. ˆAi,j = R(τi,j) mean({R(τi,k)}k[G]) std({R(τi,k)}k[G]) , (17) 23 Unify Post-Training 2025-09Compared to PPO, the most significant difference introduced by GRPO is the group relative advantage described above. Notably, the original manuscript of GRPO has also induced sequence-level policy gradient balancing and KL regularization term. However, more recent works such as (Yu et al., 2025) have removed or modified these terms in general. The clipped surrogate objective in PPO and similar algorithms enhances the stability of the RL training process by turning off gradient propagation on samples where πθ moves too far from πre . For gradient calculation, this can be represented as an indicator function 1clip. JPPO = LPPO = 1 i=1 1 G j=1 1 τj τj t=1 πθ(τi,j,tqi, τi,j,<t) ˆAi,j1clip πre (τi,j,tqi, τi,j,<t) . (18) Apart from PPO and GRPO, many recent RL algorithms for RL post-training in LLMs can be shown to exhibit similar form for their policy gradient calculations. A.3 Gradient of Offline RL As stated in the previous sections, many recent studies seek to leverage offline data in the online RL training process for LLMs. These methods consider expert demonstration data as trajectories sampled from near-optimal policy, and perform RL updates on these data based on policy gradient updates. These algorithms are adapted from the online RL literature and often combine offline and online training, setting them apart from simple SFT. Taking SRFT (Fu et al., 2025) as an instance, the offline RL objective can be written as follows LSRFT(πθ) = 1 i=1 1 j=1 1 τj τj t= πθ(τi,j,tqi, τi,j,<t) ˆAi,j, (19) This objective is derived from the GRPO objective in Equation (16), while setting πre 1 and removing the clipping mechanism since it becomes imbalanced. The motivation behind setting πre 1 is that πre is typically unavailable for offline data. Under the assumption that the demonstration policy evenly covers the current policy πθ. In this case, setting πre to 1 changes the algorithm from importance sampling to rejection sampling. The policy gradient of the offline SRFT objective can be derived consequently. JSRFT = LSRFT = 1 i= 1 j=1 1 τj τj t=1 πθ(τi,j,tqi, τi,j,<t) ˆAi,j πre = . (20) Additional Theoretical Details for Section 3.2 B.1 Deriving Equation 2 from Equation 1 Lemma A1 (Score-function identity). For density πθ and integrable (τ), θ Eτπθ [ (τ)] = Eτπθ (cid:2) (τ) θ log πθ(τ)(cid:3), Eτπθ (cid:2)θ log πθ(τ)(cid:3) = 0. Lemma A2 (Differentiating an expectation with parameterized integrand). For differentiable fθ, θ Eτπθ [ fθ(τ)] = Eτπθ (cid:2)θ log πθ(τ) fθ(τ) + θ fθ(τ)(cid:3). Lemma A3 (Measure-change (importance reweighting) identity). Let s(τ q) be any sampling density that is positive wherever πθ(τ q) is. Then Eτπθ (cid:2) (τ) θ log πθ(τ)(cid:3) = Eτs (cid:104) πθ(τ) s(τ) (τ) θ log πθ(τ) (cid:105) 24 = Eτs (cid:104) 1 s(τ) (τ) θπθ(τ) (cid:105) . Unify Post-Training 2025-09-05 Proof. By Lemma A1, Eπθ [r( q)] = Eπθ [r( q) log πθ]. For the data-adherence term, since KL(πβπθ) = Eπβ [log πβ log πθ] and πβ does not depend on θ, we have µ KL(πβπθ) = µ Eπβ [ log πθ]. Summing yields the claim. B.2 Extension: Adding Trust-Region Regularizer trust region encourages conservative policy updates by penalizing the KL divergence from the current policy πθ to fixed reference policy πre : λ KL(cid:0)πθ( q) πre ( q)(cid:1), λ 0. It is the penalty form of the constrained problem Eτπθ [r(τ q)] s.t. KL(cid:0)πθπre (cid:1) δ, max θ where λ acts as the Lagrange multiplier tied to the trust-region radius δ. Typical choices are πre = πθold (on-policy stability, TRPO/PPO-style). This penalty controls step sizes, dampens distribution shift, and yields clipping-style masks when optimized with PPO surrogates. Objective and gradient with trust region. Augmenting the Common Objective with the trust-region term gives (cid:101)Jλ,µ(θ) = τπθ (q)[r(τ q)] λ KL(cid:0)πθ( q) πre ( q)(cid:1) µ KL(cid:0)πβ( q) πθ( q)(cid:1), whose gradient is θ (cid:101)Jλ,µ(θ) = Eτπθ (cid:104)(cid:0)r(τ q) λ log πθ (τq) πre (τq) (cid:1) θ log πθ(τ q) (cid:105) + µ Eτπβ (cid:2)θ log πθ(τ q)(cid:3). In the estimator (3), this corresponds to replacing the unified advantage by (λ) uni(τ, q) = r(τ q) λ log (cid:98)A πθ(τ q) πre (τ q) + µ 1{πre = πβ} πβ(τ q) πθ(τ q) . All other expressions, including the masked estimator in (6), remain unchanged in form (with (cid:98)Auni replaced by (cid:98)A (λ) uni). B.3 PPO Clipping and the Stabilization Mask With rollout policy πθold and trust-region constraint KL(πθπθold (cid:104) Eτπθold min (cid:0)rθ(τ) Aθold max θ (τ), clip(rθ(τ), 1 ϵ, 1 + ϵ) Aθold ) δ, the PPO surrogate (τ)(cid:1)(cid:105) rθ(τ) = , πθ(τ) (τ) πθold , has piecewise derivative that is zero outside the trusted region in the harmful direction, yielding θ Eτπθold 1 πθold which matches the masked Unified Policy Gradient Estimator with πre = πθold and (cid:98)A = Aθold . (τ) θπθ(τ) stable(τ) Aθold (τ) (21) 1 , (cid:21) (cid:20)"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Tsinghua University",
        "WeChat AI"
    ]
}