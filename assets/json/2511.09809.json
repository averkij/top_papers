{
    "paper_title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models",
    "authors": [
        "Konstantinos M. Dafnis",
        "Dimitris N. Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 2 9 0 8 9 0 . 1 1 5 2 : r Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models Konstantinos M. Dafnis Department of Computer Science Rutgers University Dimitris N. Metaxas Department of Computer Science Rutgers University"
        },
        {
            "title": "Abstract",
            "content": "VisionLanguage Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), lightweight adaptation framework that extracts spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in spectrum-aware manner by adapting small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only handful of additional parameters and achieving inference speeds up to 8 faster with 12 smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs), such as CLIP [30], have marked paradigm shift in artificial intelligence, demonstrating remarkable zero-shot generalization capabilities across multitude of downstream visual tasks. By learning rich joint representations from vast quantities of image-text data, these models can often perform tasks like image classification without task-specific training, relying instead on natural language prompts to define categories [32, 43]. This ability significantly reduces the need for extensive labeled datasets and the maintenance of numerous specialized models, paving the way for more versatile and scalable AI systems. Despite their impressive zero-shot performance, the efficacy of VLMs can be substantially compromised when encountering out-of-distribution (OOD) data, where test samples exhibit characteristics different from those seen during pre-training [32]. Such distribution shifts are common in real-world applications, leading to degradation in model reliability. To mitigate this, Test-Time Adaptation (TTA) has emerged as crucial strategy, enabling models to dynamically adapt to unlabeled test samples on the fly, thereby enhancing robustness while preserving the benefits of zero-shot learning [36, 35, 23]. Episodic TTA, which adapts the model for each individual test sample, is particularly relevant for VLMs operating in diverse and unpredictable environments [32, 7]. Current TTA approaches for VLMs often focus on optimizing learnable components. Test-Time Prompt Tuning (TPT) [32] and its variants [39, 9] adapt textual prompts by minimizing objectives Correspondence to: kd703@scarletmail.rutgers.edu 39th Conference on Neural Information Processing Systems (NeurIPS 2025). like marginal entropy over augmented views of test sample. Although effective, these methods typically require backpropagation through the large text encoders of VLMs, leading to considerable computational overhead and increased memory usage during inference [34]. Other strategies involve parameter-efficient fine-tuning techniques such as Low-Rank Adaptation (LoRA) [17, 18] applied to parts of the VLM. However, these approaches often necessitate access to and modification of the models internal architecture, deviating from truly black-box paradigm and potentially limiting their applicability to proprietary models or those with fixed structures. The challenge remains to develop TTA methods that are both highly efficient and minimally invasive while effectively addressing domain shifts. To address these limitations, we introduce Spectrum-Aware Test-Time Steering (STS), novel TTA framework for VLMs that operates by efficiently adapting text representations within lowdimensional subspace defined by their Singular Value Decomposition (SVD). Instead of learning prompt vectors or modifying encoder weights, STS pre-computes semantic basis from the SVD of the initial class text embeddings. At test time, for each incoming sample, our method learns small set of coefficients that define shift vector within this principal SVD subspace. This shift is then applied to the initial text prototypes, effectively steering them in the joint embedding space to better align with the current visual input. This approach directly manipulates representations in the latent space in highly targeted and parameter-efficient manner. The core strength of STS lies in its strategy of adapting representations within structured, lowdimensional subspace, design motivated by the observation that embeddings derived from pretrained deep neural networks are typically characterized by low intrinsic dimension [2]. This implies that their essential information resides within lower-dimensional manifold, which can be effectively identified through spectral decomposition methods such as SVD. By operating on the principal singular vectors derived from the initial text embeddings, our method explicitly leverages the inherent semantic geometry of the VLMs text feature space. These singular vectors capture the most salient axes of variation among class concepts, providing robust and semantically grounded basis for adaptation. Performing test-time adaptation within this constrained subspace inherently regularizes the learning process, fostering enhanced stability against noisy augmentations or idiosyncrasies of individual test samples. This targeted manipulation not only preserves the rich knowledge encoded in the frozen VLM but also ensures that adaptations are focused along directions of maximal semantic relevance, rather than allowing unconstrained shifts in the high-dimensional embedding space. This work has four primary contributions: We propose STS, novel TTA method that, to the best of our knowledge, is the first to leverage the SVD-defined latent subspace of text embeddings for efficient and effective adaptation of VLMs to unlabeled test data. Our method exhibits significant computational advantages: it avoids backpropagation through the VLM encoders and adapts only minimal number of parameters (the SVD subspace coefficients), leading to substantially lower latency and memory footprint compared to conventional prompt tuning techniques. STS operates as black-box adaptation mechanism, treating the VLM encoders as fixed feature extractors without requiring knowledge of or modifications to their internal architectures. This makes our approach broadly applicable and non-invasive. Extensive experiments on several benchmark datasets for natural distribution shifts and cross-dataset generalization demonstrate that STS achieves state-of-the-art performance, enhancing the zero-shot capabilities of VLMs efficiently and effectively."
        },
        {
            "title": "2 Related work",
            "content": "Vision-Language Models. Pre-trained on extensive image-text datasets via self-supervised learning, vision-language models (VLMs) like CLIP [30] and ALIGN [19] exhibit impressive generalization abilities. For instance, CLIPs exceptional zero-shot performance largely stems from the scale and variety of its training data. However, effectively adapting these models to specific downstream tasks, especially in data-scarce scenarios, continues to pose significant challenges. Efforts to improve the transferability of vision-language models have led to the use of prompt tuning techniques, including CoOp [43], CoCoOp [42], and MaPLe [21], and adapter-based methods, such as Tip-Adapter [40] 2 and CLIP-Adapter [10]. However, prevalent assumption in these techniques is the accessibility of labeled data from the target domain, condition that is frequently incompatible with the requirements for rapid deployment in real-world applications. Therefore, we focus on test-time adaptation, defined as the challenge of model adapting to target domain using only the unlabeled test instances, with no access to any training data or ground-truth labels from that specific domain. Test-Time Adaptation. Test-Time Adaptation (TTA) aims to improve model robustness and generalization by adapting pre-trained model to unlabeled test data encountered during inference [35, 23, 36, 11, 26, 4, 32]. In the context of VLMs, several TTA strategies have emerged. dominant paradigm involves tuning the learnable prompt vectors at test time. Test-Time Prompt Tuning (TPT) [32] pioneered this by optimizing textual prompts for each test sample to minimize the entropy of predictions over augmented views. Subsequent works have built upon TPT, such as DiffTPT [9], which employs diffusion models for more diverse augmentations, and C-TPT [39], which focuses on improving model calibration during TTA. Although these methods adapt VLMs without labeled data, they generally incur significant computational costs and memory overhead due to the need for backpropagation through the large VLM encoders to update the prompt parameters. To address the efficiency concerns of prompt tuning, training-free approaches have been proposed. Most of these methods operate in an on-line streaming scenario, using memory banks to retain information from previously seen test inputs [41, 20]. However, such methods suffer from critical drawbacks that limit their applicability to real-life scenarios. First, maintaining memory banks significantly increases memory consumption, which becomes prohibitive for resource-constrained devices or large-scale deployments. Second, their efficacy hinges on the assumption of welldistributed stream of data, an unrealistic expectation in practice, as imbalanced or nonstationary test distributions may prevent the memory bank from accumulating sufficient or representative samples within reasonable time-frame. This dependency renders them unreliable for time-sensitive applications or scenarios with sparse or bursty data streams. Furthermore, memory-based methods risk performance degradation when test samples arrive in biased sequences, as stored information may reflect transient patterns rather than meaningful statistical trends. Our STS method, while involving lightweight optimization step, shares the goal of minimizing encoder backpropagation and architectural changes. Other approaches explore parameter-efficient fine-tuning (PEFT) techniques at test time. For example, TTL [18] adapts LoRA [17] parameters within the VLMs attention layers. Although more efficient than full fine-tuning, such methods still typically require modification of the underlying model architecture, differing from black-box approaches where the VLM encoders are treated as fixed. Directly adapting or modulating representations in the latent embedding space offers an alternative to prompt tuning or architectural modifications. Test-Time Prototype Shifting (TPS) [34] proposes learning shift vectors for pre-computed class prototypes directly in the embedding space, thereby avoiding backpropagation through encoders and achieving significant efficiency gains. This is conceptually related to our work. However, TPS learns unconstrained shift vectors for each class prototype. Our proposed STS method advances the idea of latent space adaptation by introducing spectrum-aware mechanism. Instead of learning arbitrary shifts, STS learns compact coefficients that define shifts along principal semantic axes derived from the SVD of text embeddings. This constrains the adaptation to low-dimensional, semantically meaningful subspace, aiming for both efficiency and effectiveness."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first provide preliminary details on Vision-Language Models (VLMs) and the test-time adaptation setting. Subsequently, we introduce our proposed Spectrum-Aware Test-Time Steering (STS) framework. We detail its components for identifying principal spectral subspace from text embeddings, the mechanism for latent steering via shared, learnable coefficient vector operating within this subspace, and the test-time optimization objective. 3 Figure 1: Overview of our proposed STS framework. Given text and image inputs, encoders Et() and Ev() extract text embeddings/prototypes, and visual embeddings. probability distribution PCLIP (y = ycXtest) is computed based on these embeddings. Then we perform refinement step of test-time adaptation, where we tune the learnable low-dimensional coefficients to generate small steering to the text prototypes to close the gap between the source and target distributions. Marginal entropy of the CLIP similarities of the shifted embeddings and the class prototypes is minimized. 3.1 Preliminaries Vision-Language Models (VLMs). Pre-trained VLMs, such as CLIP [30], comprise visual encoder Ev() and textual encoder Et(), which map images and text descriptions into shared D-dimensional embedding space. For C-class zero-shot image classification task, set of initial c=1 RCD, is derived by encoding class names, typically using text prototypes, ZTinit = {(zTinit)c}C prompt templates (e.g., \"a photo of {class name}\"). Given an input image x, its visual embedding is zv = Ev(x) RD. The predicted probability for class is: p(y = cx, ZTinit) = exp(sim(zv, (zTinit)c)/τ ) i=1 exp(sim(zv, (zTinit)i)/τ ) (cid:80)C , (1) where sim(, ) denotes cosine similarity, and τ is CLIPs learned temperature. Test-Time Adaptation (TTA). Episodic TTA seeks to adapt model for each unlabeled test sample xtest by optimizing small set of parameters θTTA using an objective LTTA derived from xtest (typically via augmentations). The adapted model (; θ TTA) is then used for prediction, and θTTA is reset for subsequent samples. 3.2 Spectrum-Aware Test-Time Steering (STS) STS adapts VLMs at test time by learning to steer the initial text prototypes. This steering is enacted by single, shared vector of learnable coefficients that operates within low-dimensional semantic subspace. This subspace is critically derived from the spectral properties (Singular Value Decomposition) of the initial text embeddings, ensuring adaptations are both efficient and aligned with principal semantic variations. 3.2.1 Spectral Subspace Identification from Text Prototypes The core of STS lies in identifying robust, low-dimensional subspace from the initial text prototypes ZTinit RCD to guide the adaptation. High-dimensional embedding spaces, while expressive, can be susceptible to noise and may contain redundant information for specific adaptation tasks. Pre-trained 4 features from deep neural networks often exhibit low intrinsic dimensionality [2], implying that their essential information can be effectively captured within lower-dimensional manifold. By projecting the adaptation process onto such manifold, defined by the principal spectral components, STS aims to achieve more stable, generalizable, and semantically meaningful adaptations. To this end, we perform Singular Value Decomposition (SVD) on the initial text prototypes ZTinit. Using the reduced SVD: ZTinit = UT ST , contains the left singular vectors, ST Rkk (2) where UT RCk is diagonal matrix of = RkD has rows corresponding min(C, D) singular values s1 s2 sk 0, and to the right singular vectors. The columns of VT RDk (i.e., the right singular vectors of ZTinit ) represent an orthonormal basis for the subspace capturing the principal directions of variance in the text prototype data. These directions correspond to the most significant semantic axes that differentiate the class concepts as represented by ZTinit. We select the top kt right singular vectors from VT (associated with the kt largest singular values) to form our textual adaptation basis BT = [v1, v2, . . . , vkt] RDkt. The choice of kt is pivotal for efficiency and robustness. small kt focuses adaptation on the most dominant semantic variations, potentially filtering out noise associated with higher-order components and leveraging the aforementioned low intrinsic rank of deep features [2]. Empirically, as we see in Figure 2, small kt often captures the vast majority (e.g., >90%) of the total variance (sum of squared singular values, or \"energy\") of ZTinit. Thus, for automatic and principled selection of kt, we employ the optimal hard thresholding strategy proposed by Gavish and Donoho [12]. This method determines an optimal singular value threshold ω based on the aspect ratio of the matrix ZTinit (i.e., C/D) and the median of its singular values. The rank kt is then the count of singular values si such that si > ω. This pre-computed basis BT defines the kt-dimensional spectral subspace for our test-time steering. 3.2.2 Latent Steering via Subspace Coefficients For each incoming test image xtest, STS learns single, shared vector of kt learnable coefficients γ Rkt . These coefficients determine the magnitude and direction of the text prototype steering along each of the kt basis vectors in BT . The steering vector (shift) zT RD, which is applied to all class prototypes, is reconstructed from its kt-dimensional representation γ: zT = BT γ. The adapted text prototype (zTadapted)c for each class is then obtained by: The only parameters learned at test time are the kt coefficients in γ. (zTadapted)c = normalize ((zTinit)c + zT ) . 3.2.3 Test-Time Optimization Objective (3) (4) The shared steering coefficients γ are optimized for each xtest using an unsupervised objective based on prediction consistency over augmented views of xtest, denoted {x(j)}N j=1. Visual embeddings ZV = {z(j) j=1 are extracted using the frozen Ev. = Ev(x(j))}N Confidence Filtering. Following prior TTA works [32, 7], views are filtered based on prediction confidence using the initial (unadapted) text prototypes ZTinit. Logits L(j) init are computed per view. Views with prediction entropy H(P (j) init ) falling within the top-ρ percentile of confidence (lowest entropy) are retained, forming ZVfilt = {z(j) init and probabilities (j) } of size Nfilt. Marginal Entropy Minimization. For z(j) ZTadapted (from Eq. 4 using current γ) are: L(j) probability distribution Padapted is: ZVfilt, logits with current adapted text prototypes , (ZTadapted)c)/τ . The marginal adapted(c) = sim(z(j) Padapted(c) = 1 Nfilt Nfilt(cid:88) j=1 softmaxc(L(j) adapted(c)). (5) 5 Figure 2: Most spectral energy in CLIP text prototypes is captured by small subset of singular values, highlighting strong low-rank structure across datasets. The primary objective is to minimize the Shannon entropy of Padapted: Lent = H( Padapted) = (cid:88) c= Padapted(c) log Padapted(c). An L2 regularization term is added for zT : Lreg = λRzT 2. The total loss is: The coefficients γ are initialized to zeros and optimized to minimize LSTS. LSTS = Lent + Lreg. (6) (7) 3.2.4 Inference with Adapted Prototypes After optimization yielding γ, the final adapted text prototypes ZTfinal are computed. The final class prediction ˆy for xtest is: ˆy = argmaxc 1 Nfilt Nfilt(cid:88) j=1 (cid:16) softmaxc sim(z(j) , (ZTfinal)c)/τ (cid:17) . (8)"
        },
        {
            "title": "4 Experiments and Results",
            "content": "We conduct experiments on diverse range of benchmark datasets to assess the performance and robustness of our method, specifically testing its out-of-domain generalization across different domains. 4.1 Experimental Setup Datasets. We conduct comprehensive evaluation of our method across diverse set of benchmark datasets, with particular focus on out-of-domain generalization. To assess the models ability to handle distribution shifts, we utilize several ImageNet variants, including ImageNet-A [16], ImageNet-V2 [31], ImageNet-R [14], and ImageNet-Sketch (also referred to as ImageNet-K) [37]. These datasets serve as established Out-of-Distribution (OOD) benchmarks for ImageNet, enabling rigorous assessment of the models robustness under varying conditions and data distributions. For Fine-grained Classification (also referred to as \"Cross-Datasets Generalization in previous works), in line with [32], we include Flowers102 [27], DTD [5], Pets [29], UCF [33], and Caltech101 [8]. These datasets facilitate the evaluation of the models capacity to distinguish fine-grained 6 Table 1: Comparison of top-1 accuracy (%) across ImageNet and its OOD variants. The best results in each section are highlighted in bold. Underline indicates second-best. Method ImageNet V2 Sketch Average OOD Average Zero-Shot [30] Ensemble [28] CoOp [43] TPT [32] DiffTPT [9] C-TPT [39] TPS [34] STS (Ours) STSEnsemble 66.73 68.34 71.51 68.97 70.30 68.53 67.96 68.85 70. CLIP-ViT-B/16 47.87 60.86 73.98 49.89 49.71 54.39 55.68 51.14 57.46 61.23 64. 61.88 64.20 63.37 65.10 62.13 62.95 64.15 64.82 77.65 75.21 77.07 75.00 75.66 74.90 77.13 80.53 MaPLe Zero-Shot [21] TPT [32] STS (Ours) - - - 50.90 64.07 76. 58.08 64.83 64.87 66.49 78.12 79.43 46.09 48.24 47.99 48.01 46.80 47.37 46.03 48.06 50. 49.15 48.16 50.62 59.11 61.20 61.72 62.36 62.28 60.97 61.86 63.88 66.13 - - - 57.2 59.42 59.28 60.71 60.52 59.08 60.34 62.64 64.96 60.28 62.31 65. variations among visually similar classes. Furthermore, to assess the models adaptability across diverse domains, we incorporate Aircraft [25], EuroSAT [13], Cars [22], Food [3], and SUN397 [38], encompassing broad spectrum of data modalities, including aerial and satellite imagery, object-centric datasets, and scene-centric environments. For all datasets, we utilize the test splits defined by Zhou et al. [43], adhering to the common evaluation protocol. Implementation Details. Following TPT [32], we generate 63 augmented versions of test image using only random resized crops and horizontal flips for all datasets, unlike prior methods that use task-specific augmentations such as AugMix, resulting in batch of 64 images including the original input. To identify high-confidence samples, we select the 10% of batch samples with the lowest entropy and compute the marginal entropy based on their predicted probability distributions. The learnable vector is initialized to zero and optimized for single step using the AdamW [24] optimizer with learning rate of 5e-3 across all datasets. In our method, each class prototype is initialized using the hand-crafted prompt, photo of {CLASS}. All experiments are conducted on single NVIDIA RTX8000 GPU with 45GB of memory. The presented results are an average taken over three distinct random seeds. Top-1 accuracy is reported in all tables, unless explicitly indicated otherwise. Baselines. We evaluate our method against zero-shot and test-time adaptation (TTA) baselines that utilize CLIP ViT-B/16 as the backbone. The TTA methods considered include TPT [32], which performs text-prompt tuning; DiffTPT [9], variant of TPT that employs diffusion models to augment visual training data; TPS [34], which optimizes shift vector for each class prototype; and C-TPT [39], an extension of TPT that improves model calibration by selecting prompts based on the dispersion of textual embeddings. To ensure fair comparison, we reproduce TPT, TPS, and C-TPT on our system using single update step and the same backbone architecture. For DiffTPT, we report results from [9]. It is important to note that the DiffTPT study evaluates performance on subset of each dataset that contain only 1,000 test samples, which may introduce potential variability in the reported results. Textual Prompts. When Ensemble is specified, we do not use dataset-specific templates. In contrast, we use the set of 7 generic templates highlighted in the official CLIP repository [28] across all datasets. 4.2 STS Results Natural Distribution Shifts. Table 1 presents the top-1 accuracy of our method, comparing it against zero-shot and test-time adaptation (TTA) baselines using CLIP on ImageNet and its out7 Table 2: Performance comparisons on fine-grained classification. The best results in each section are highlighted in bold. Underline indicates second-best. Method Flowers102 DTD OxfordPets UCF101 Caltech101 Aircraft EuroSAT StanfordCars Food101 SUN397 Average Zero-Shot [30] Ensemble [28] CoOp [43] TPT [32] C-TPT [39] TTL [18] TPS [34] STS (Ours) STSEnsemble Zero-Shot [21] TPT [32] STS (Ours) 67.44 66.99 68. 68.98 69.88 67.32 66.14 66.10 67.16 72.23 72.37 70.70 44.27 45.04 41.92 47.16 45.54 45.92 45.49 46.02 46. 46.49 45.87 47.60 88.25 86.92 89.14 87.07 87.96 86.78 86.56 86.69 87.11 90. 90.72 90.00 65.13 65.16 66.55 67.89 65.19 67.80 66.53 66.52 67.14 68.69 69.19 68. CLIP-ViT-B/16 93.35 93.55 93.70 94.19 93.39 93.23 93.60 93.72 94.20 MaPLe 93. 93.59 94.02 23.67 23.22 18.47 22.85 24.13 24.00 24.01 24.57 24.21 24.74 24.70 25. 42.01 50.42 46.39 43.01 38.43 36.52 37.85 38.26 43.80 48.06 47.80 40.83 65. 66.11 64.51 66.55 65.26 65.95 66.93 67.17 68.16 65.57 66.50 68.32 83.65 82.86 85. 84.67 82.60 84.40 82.96 84.72 85.15 86.20 86.64 86.56 62.59 65.63 64.15 65.47 63.38 64.02 64.85 64.79 66. 67.01 67.54 67.26 63.58 64.59 63.88 64.78 63.58 63.59 63.49 63.86 65.06 66. 66.49 65.97 of-distribution (OOD) variants. The results demonstrate that steering the text prototypes with our learnable vector, leads to substantial performance improvement. Specifically, our approach achieves an average OOD performance gain of 7.76% over the zero-shot CLIP baseline and 4.23% improvement over standard TPT across OOD datasets. Furthermore, Table 1 shows that simply learning shift vector for per-class prototypes in TPS results in slight performance drop (< 0.6 points) compared to TPT, highlighting the limitations of TPS in effectively aligning text prototypes with out-of-distribution visual embeddings. Additionally, our method is significantly more efficient, as STS runs 8 times faster than TPT (see Table 3) while still achieving superior performance. These substantial speed-ups make our approach highly practical for real-world applications. Table 3: Efficiency comparison on ImageNet. We report the testing time per sample, the memory usage, the accuracy, and the performance gains compared to zero-shot CLIP. Method Testing Time (s) Memory (GB) Accuracy Gain Zero-Shot TPT STSEnsemble 0.02 0.75 0.09 0.83 17.6 1.4 66.73 68.97 70.81 - +2.24 +4.08 Fine-grained Classification. We further assess the generalization capabilities of STS across ten diverse image classification datasets, with results presented in Table 2. This benchmark evaluates the models ability to adapt to datasets that may differ significantly in domain and class composition from the VLMs pre-training data. In this challenging scenario, our STSEnsemble variant, which leverages 7 generic CLIP templates, achieves the highest average accuracy of 65.06% among all methods compared. This result surpasses the standard TPT [32] (average 64.78%) and demonstrates the effectiveness of combining STS with prompt ensembling. The standard STS (using single \"a photo of {CLASS}\" prompt) achieves an average accuracy of 63.86%, which is competitive and outperforms the Zero-Shot CLIP baseline (63.58%). It also surpasses other TTA methods such as C-TPT [39] (63.58%), TTL [18] (63.59%), and TPS [34] (63.49%) on average. Standard STS particularly excels on datasets like Aircraft (24.57%, best in its group), StanfordCars (67.17%, second best), and Food101 (84.72%, second best). The STSEnsemble variant shows broad strength, achieving the best results on Caltech101 (94.20%), EuroSAT (43.80%), StanfordCars (68.16%), Food101 (85.15%), and SUN397 (66.79%). This indicates that while standard TPT is strong baseline, STS, especially when combined with prompt ensembling, offers more robust generalization across these diverse datasets. Overall, STS demonstrates robust and often superior performance in fine-grained classification. The STSEnsemble variant, in particular, sets new state-of-the-art average across the CLIP-ViTB/16 backbone experiments. Even with stronger MaPLe [21] initializations, STS provides more effective adaptation than TPT. These results, combined with STSs significant advantages in parameter 8 efficiency and computational speed (detailed in Table 3), underscore its potential as practical and powerful approach for real-world test-time adaptation of VLMs."
        },
        {
            "title": "5 Analysis and ablation",
            "content": "We perform ablation studies to assess the effect of key design choices on performance. For consistency, all analyses use ImageNet and ImageNet-A with the ViT-B/16 backbone. Additionally, we evaluate STS on CIFAR10-C [15] to test its robustness under challenging distribution shifts. 5.1 Computational Analysis Trainable Parameters: Test-time tuning approaches such as TPT, DiffTPT, and C-TPT adapt textual prompts using only 2048 trainable parameters, corresponding to four tokens with d=512. However, these methods exhibit limited generalization, achieving Top-1 accuracy of approximately 51% to 55% on ImageNet-A. In contrast, visual adaptation techniques, such as encoder tuning and layer normalization optimization, require larger number of trainable parameters. Meanwhile, STS effectively balances this trade-off with just small number of trainable parameters, achieving the highest generalization performance at 61.23% while preserving model accessibility constraints. Table 3 shows the testing time per sample and the performance gain on ImageNet for STS and the TTA baseline TPT on single RTX8000 GPU. STS is 8x faster than TPT, corresponding to an order of magnitude of computational savings in time. 5.2 Effect of STS on Different Prototypes We study the impact of the steering vector under different prototype constructions. Specifically, we compare STS against the zero-shot CLIP using the standard prompt \"a photo of {CLASS}\" and an ensemble of seven generic hand-crafted templates from the official CLIP repository. While the design of TPT does not support the use of text ensembles, STS integrates them seamlessly; we denote this variant as STSEnsemble. As shown in Tables 1 and 2, adding these generic prompts further improves STS, surpassing TPT without relying on dataset-specific templates. Furthermore, we evaluate our method using MaPLe [21] initialization, where the MaPLe prompts are learned on ImageNet in 16-shot setup following [1]. In this evaluation, we also report results for TPT applied on top of MaPLe, as in [1], which we refer to as MaPLe+TPT. Although MaPLe+TPT performs better than previous methods that rely on hand-crafted prompts, our STS method notably outperforms MaPLe+TPT on most datasets. This demonstrates that the adaptive mechanism of STS provides consistent advantages even when initialized with optimized textual and visual prompts. Natural Distribution Shifts. Under natural distribution shifts, our STS method consistently outperforms TPT even when the baseline initialization is MaPLe, achieving an average improvement of +3.03%. We omit evaluation on ImageNet in this group, as MaPLe uses it as the source dataset for model adaptation, making the comparison unfair. For completeness, Zero-Shot MaPLe attains 70.72% accuracy on ImageNet, which further improves to 72.72% when adapted with STS (+2.0%). STS with MaPLe (MaPLe+STS) demonstrates leading performance on several natural shift datasets, including ImageNet-A (64.83%), ImageNet-V2 (66.49%), ImageNet-Sketch (50.62%). Fine-grained Classification. In fine-grained classification tasks, MaPLe+TPT shows marginal average improvement of +0.52% over MaPLe+STS. However, this difference is primarily driven by performance on single dataset, EuroSAT, while MaPLe+STS surpasses MaPLe+TPT on roughly half of the remaining datasets by considerable margin. For the datasets where MaPLe+STS lags behind, the differences remain minimal. As discussed in [7], EuroSAT constitutes known failure mode for many TTA methods. Its analysis suggests that the unique nature of satellite imagery demands task-specific augmentation strategies, making EuroSAT controversial benchmark for evaluating TTA performance. 5.3 Robustness of Linear Spectrum Steering We evaluate STS on CIFAR10-C at the highest corruption level (severity 5) following the TPT protocol (10% most confident views, learning rate 0.005, and the hand-crafted prompt \"a photo of 9 (a) CIFAR10-C comparison. (b) Accuracy vs. views . Figure 3: (a) Comparison on CIFAR10-C (severity 5). (b) Accuracy vs. number of augmented views. {CLASS}\"). STS matches TPT within 0.05% (Figure 3a), while clearly outperforming the naive per-class shifting (TPS), confirming the effectiveness of spectrum-aware subspace steering. With the seven generic CLIP templates, STS reaches 67.24%, demonstrating strong complementarity between subspace steering and prompt ensembles. Constraining adaptation to the top-k singular vectors further stabilizes learning under severe corruptions. 5.4 Balancing Inference Efficiency and Accuracy We analyze the impact of the number of augmented views on STS efficiency. As shown in Figure 3b, accuracy increases with and saturates around = 64. Increasing to 128 views yields only 0.15% gain while nearly doubling time and memory. We thus adopt = 64 (as in prior TPT work) to balance performance and efficiency, while remaining substantially faster than prompt-tuning baselines."
        },
        {
            "title": "6 Limitations",
            "content": "While STS demonstrates significant advantages in efficiency and effectiveness for test-time adaptation, we acknowledge two limitations that warrant discussion and offer avenues for future research: Linearity of subspace steering. The adaptation mechanism involves linear shifts (steering) within the identified SVD subspace. While this subspace captures principal linear variations, highly complex or non-linear domain shifts might not be fully addressable by such linear adaptations alone, potentially requiring more sophisticated, non-linear mapping techniques within the latent space. Linear complexity with respect to augmented views. Finally, although STS is notably more lightweight than current state-of-the-art TTA strategies, its computational demand for visual processing scales linearly with the number of augmented views due to the need for independent forward passes. An intriguing future direction is to explore latent visual space augmentation to eliminate these repeated encoder computations. By addressing these current limitations, the robustness and performance of spectrum-aware test-time adaptation strategies could be further advanced."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose Spectrum-Aware Test-Time Steering (STS), lightweight adaptation framework for vision-language models like CLIP. STS exploits the spectral structure of text embeddings to define compact semantic subspace, where it learns per-sample steering vector to improve zero-shot robustness without modifying the frozen encoders. Our experiments show that STS consistently enhances performance across diverse benchmarks, offering an efficient and practical test-time adaptation method. Notably, in addition to the text prototypes, our proposed STS method can be readily applied to the visual embeddings as well. Exploring under which conditions and settings the STS should be preferred over text prototypes or visual embeddings constitutes an interesting research direction that belongs to our future agenda."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Konstantinos D. Polyzos for insightful discussions and for his helpful and detailed comments. We also thank the reviewers for their constructive suggestions, which have helped improve the quality of this paper. This research has been partially funded by research grants to D. Metaxas through NSF: 2310966, 2235405, 2212301, 2003874, 1951890, AFOSR 23RT0630, and NIH 2R01HL127661."
        },
        {
            "title": "References",
            "content": "[1] Jameel Abdul Samadh, Mohammad Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muhammad Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan. Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization. In: Advances in Neural Information Processing Systems 36 (2023), pp. 8039680413. [2] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In: arXiv preprint arXiv:2012.13255 (2020). [3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In: Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13. Springer. 2014, pp. 446461. [4] Evgenia Tatiani Chroni, Konstantinos Dafnis, Georgios Chantzialexiou, Eric Cosatto, and Dimitris Metaxas. Improving Test-Time Adaptation For Histopathology Image Segmentation: Gradient-To-Parameter Ratio Guided Feature Alignment. In: 2024 IEEE International Symposium on Biomedical Imaging (ISBI). IEEE. 2024, pp. 15. [5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2014, pp. 36063613. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248255. [6] [7] Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano Mancini, and Elisa Ricci. Frustratingly easy test-time adaptation of vision-language models. In: arXiv preprint arXiv:2405.18330 (2024). [8] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In: 2004 conference on computer vision and pattern recognition workshop. IEEE. 2004, pp. 178 178. [9] Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. Diverse data augmentation with diffusions for effective test-time prompt tuning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023, pp. 27042714. [10] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. In: International Journal of Computer Vision 132.2 (2024), pp. 581595. [11] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris Metaxas. Visual prompt tuning for test-time domain adaptation. In: arXiv preprint arXiv:2210.04831 (2022). [12] Matan Gavish and David Donoho. Optimal shrinkage of singular values. In: IEEE Transactions on Information Theory 63.4 (2017), pp. 21372152. [13] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. In: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 12.7 (2019), pp. 22172226. [14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In: Proceedings of the IEEE/CVF international conference on computer vision. 2021, pp. 83408349. [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In: arXiv preprint arXiv:1903.12261 (2019). [16] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021, pp. 1526215271. [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In: ICLR 1.2 (2022), p. 3. [18] Raza Imam, Hanan Gani, Muhammad Huzaifa, and Karthik Nandakumar. Test-time low rank adaptation via confidence maximization for zero-shot generalization of vision-language models. In: 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE. 2025, pp. 54495459. [19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In: International conference on machine learning. PMLR. 2021, pp. 49044916. [20] Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El Saddik, and Eric Xing. Efficient test-time adaptation of vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 1416214171. [21] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023, pp. 1911319122. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In: Proceedings of the IEEE international conference on computer vision workshops. 2013, pp. 554561. [22] [23] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. Ttt++: When does self-supervised test-time training fail or thrive? In: Advances in Neural Information Processing Systems 34 (2021), pp. 2180821820. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In: arXiv preprint arXiv:1711.05101 (2017). [24] [25] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. In: arXiv preprint arXiv:1306.5151 (2013). [26] Muhammad Jehanzeb Mirza, Pol Jané Soneira, Wei Lin, Mateusz Kozinski, Horst Possegger, and Horst Bischof. Actmad: Activation matching to align distributions for test-time-training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 2415224161. [27] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In: 2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE. 2008, pp. 722729. [28] OpenAI. CLIP: Contrastive Language-Image Pretraining. https://github.com/openai/ CLIP. 2021. [29] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In: 2012 IEEE conference on computer vision and pattern recognition. IEEE. 2012, pp. 3498 3505. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In: International conference on machine learning. PmLR. 2021, pp. 87488763. [31] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In: International conference on machine learning. PMLR. 2019, pp. 53895400. [32] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. In: Advances in Neural Information Processing Systems 35 (2022), pp. 1427414289. [33] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: dataset of 101 human actions classes from videos in the wild. In: arXiv preprint arXiv:1212.0402 (2012). [34] Elaine Sui, Xiaohan Wang, and Serena Yeung-Levy. Just shift it: Test-time prototype shifting for zero-shot generalization with vision-language models. In: arXiv preprint arXiv:2403.12952 (2024). [35] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In: International conference on machine learning. PMLR. 2020, pp. 92299248. [36] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In: arXiv preprint arXiv:2006.10726 (2020). [38] [37] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. In: Advances in neural information processing systems 32 (2019). Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In: 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE. 2010, pp. 34853492. [39] Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, and Chang Yoo. C-tpt: Calibrated test-time prompt tuning for vision-language models via text feature dispersion. In: arXiv preprint arXiv:2403.14119 (2024). [40] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In: European conference on computer vision. Springer. 2022, pp. 493510. [41] Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, and Lei Zhang. Dual memory networks: versatile adaptation approach for vision-language models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024, pp. 2871828728. [42] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 1681616825. [43] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. In: International Journal of Computer Vision 130.9 (2022), pp. 2337 2348. 13 Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models"
        },
        {
            "title": "Appendix",
            "content": "In this supplementary document, we provide additional details and experimental results to enhance understanding and insights into our method. This supplementary document is organized as follows: Broader Impact of our method in Section A. We present an error bar analysis for the results in Table 1 and Table 2 in the main document, in Section B.1. We provide an analysis on the impact of varying update steps in Section B.2. We evaluate the effect of shared versus per-class coefficients vector in Section B.3. We present additional performance comparisons on larger-scale VLMs, specifically OpenAI CLIP [30] with ViT-L/14 backbone, in Section B.4. We analyze the effect of the singular vector selection for the test-time latent steering, in Section C. We provide the detailed statistics for all the utilized datasets, and the specific textual prompts that we used in Section D. We list the license information for all used assets in Section E."
        },
        {
            "title": "A Broader Impact",
            "content": "This research contributes to the overarching goal of developing more dependable and effective machine learning systems by enabling large foundation models like CLIP [30] to dynamically adapt to real-world operational conditions at test time. Such adaptability is critical for deploying AI robustly in diverse and unpredictable environments, thereby broadening their practical applications and fostering greater system reliability. Ultimately, we hope this work stimulates and guides future studies focused on enhancing the generalization capabilities and operational robustness of pre-trained models, ensuring they can be utilized more effectively and responsibly to address wide array of societal challenges."
        },
        {
            "title": "B Technical Appendices",
            "content": "B.1 Analysis on error bars We run STS multiple times using 3 different random seeds and report the average accuracy with standard deviation in Table B1. The randomness of STS mainly comes from random data augmentation. Our augmentation setup is simple and only contains resized random crops and random horizontal flips, which can constitute zoom-in to random portion of the image. We did not search for the best data augmentations, but rather stuck to an established setting, using the same augmentation setup for all datasets. However, the performance of STS is linked to the impact that data augmentations have on how the model perceives images, and we believe that this is an interesting research direction to pursue. In addition, we report an error bar analysis for the results in Table 1 and Table 2 in the main document. 14 Table B1: Robustness to natural distribution shifts. We report the accuracy with an error bar (standard deviation) obtained from three runs with different random seeds. The best results in each section are highlighted in bold. Underline indicates second-best. Method ImageNet V2 Sketch Average OOD Average CLIP-ViT-B/ TPT [32] 68.97 (.04) 54.39 (.20) 63.37 (.06) 77.07 (.06) 48.01 (.08) 62.36 (.03) 60.71 (.04) C-TPT [39] 68.53 (.02) 51.14 (.09) 62.13 (.11) 75.66 (.07) 47.37 (.08) 60.97 (.02) 59.08 (.02) TPS [34] 67.96 (.02) 57.46 (.12) 62.95 (.11) 74.90 (.04) 46.03 (.09) 61.86 (.06) 60.34 (.07) STS (Ours) 68.85 (.03) 61.23 (.26) 64.15 (.20) 77.13 (.06) 48.06 (.06) 63.88 (.08) 62.64 (.10) STSEnsemble 70.81 (.04) 64.29 (.09) 64.82 (.14) 80.53 (.13) 50.19 (.02) 66.13 (.01) 64.96 (.005) Table B2: Performance comparisons on cross-dateset generalization from ImageNet to fine-grained classification datasets. We report the accuracy with an error bar (standard deviation) obtained from three runs with different random seeds. The best results in each section are highlighted in bold. Underline indicates second-best. Method Flowers102 DTD OxfordPets UCF101 Caltech101 Aircraft EuroSAT StanfordCars Food101 SUN397 Average 68.98 (.13) 47.16 (.08) 87.07 (.19) 67.89 (.07) 94.19 (.12) 22.85 (.41) 43.01 (.06) 66.55 (.02) 84.67 (.06) 65.47 (.13) 64.78 (.05) TPT [32] C-TPT [39] 69.88 (.19) 45.54 (.13) 87.96 (.14) 65.19 (.41) 93.39 (.16) 24.13 (.24) 38.43 (.45) 65.26 (.24) 82.60 (.16) 63.38 (.26) 63.58 (.05) 67.32 (.25) 45.92 (.02) 86.78 (.02) 67.80 (.06) 93.23 (.06) 24.00 (.36) 36.52 (.05) 65.95 (.24) 84.40 (.02) 64.02 (.05) 63.59 (.02) TTL [18] TPS [34] 66.14 (.11) 45.49 (.36) 86.56 (.08) 66.53 (.19) 93.60 (.12) 24.01 (.44) 37.85 (.23) 66.93 (.18) 82.96 (.09) 64.85 (.07) 63.49 (.07) STS (Ours) 66.10 (.15) 46.02 (.10) 86.69 (.18) 66.52 (.10) 93.72 (.10) 24.57 (.06) 38.26 (.35) 67.17 (.27) 84.72 (.06) 64.79 (.17) 63.86 (.06) STSEnsemble 67.16 (.32) 46.87 (.12) 87.11 (.08) 67.14 (.06) 94.20 (.05) 24.21 (.05) 43.80 (.15) 68.16 (.23) 85.15 (.03) 66.79 (.04) 65.06 (.03) CLIP-ViT-B/16 B.2 Impact of Varying Update Steps test instance. learning rate for including any of The single step is determined to be 0.005 on the stanthe out-of-distribution data). (not By default, STS updates the coefficients using single step per optimal this dard ImageNet validation set To evaluate the impact of different numbers of update steps on overall performance, we conduct ablation experiments by varying the number of update steps from 1 to 5 and report the resulting performance on ImageNet-A. For these multi-step ablations, the value of λR of the regularization loss is set to 0.01, and the initial learning rate remains 0.005. Since it is optimal for single update step, all subsequent steps are subjected to learning rate schedule, applying one-time decay factor of 0.1 to the initial rate. As shown in Table B3, the number of update steps does not significantly influence performance (in the range of 0.1%). Although increasing the update steps to 2 yields slight performance gain of 0.02%, it also leads to proportional decrease in inference efficiency. Although our method is extremely efficient, given this trade-off, we adopt the single-step update as the default for balancing efficiency and performance. Table B3: Ablation study on different update steps in learning the steering vectors. We vary the number of update steps from 1 to 5 and report the achieved performance on ImageNet-A. Results are over 3 random seeds. Accuracy # Steps 61.20 61.20 61. 61.23 61.25 4 2 5 1 B.3 Effect of Shared vs. Per-Class Coefficients Test-time adaptation strategies vary in how they modify class representations. For instance, prompt tuning methods adjust shared prompt that subsequently undergoes non-linear transformations via the text encoder. In the context of latent-space adaptation, class prototypes can be modulated either by individual, per-class coefficients or by single, shared coefficients vector. shared vector applies uniform transformation, thus maintaining the relative geometric structure of the class prototypes after adaptation. This approach primarily targets global, dataset-level distribution shifts. Although per-class vectors could, in principle, offer finer control by providing more degrees of freedom to capture distinct class-level shifts within domain gap, the practical benefits of such granularity warrant careful consideration. Our work investigates the efficacy of the shared coefficients vector 15 approach, as the additional complexity introduced by per-class vectors may not consistently translate into substantial performance gains over simpler, unified shift. As shown in Table B4, learning per-class shift yields marginal average performance increase of only 0.03%. Similarly, Table B5 indicates that per-class coefficients provide mere 0.01% average improvement in cross-dataset generalization from ImageNet to fine-grained classification tasks. These minimal gains suggest that, at least for single update step, learning per-class coefficients does not substantially enhance model performance when encountering domain gap. Table B4: performance comparison of shared versus per-class steering vectors regarding robustness to natural distribution shifts. We present the average top1 accuracy (%) results over 3 random seeds for single update step. The best performance is highlighted in bold. Method ImageNet V2 Sketch Average OOD Average Shared Per-class 68.85 68.91 61.23 61.24 64.15 64.20 77.13 77. 48.06 48.06 63.88 63.91 62.64 62.66 CLIP-ViT-B/16 Table B5: performance comparison of shared versus per-class steering vectors on cross-dateset generalization from ImageNet to fine-grained classification datasets. We present the average top1 accuracy (%) results over 3 random seeds for single update step. The best performance is highlighted in bold. Method Flowers102 DTD OxfordPets UCF101 Caltech101 Aircraft EuroSAT StanfordCars Food101 SUN397 Average Shared Per-class 66.10 66.15 46.02 45.88 86.69 86. 66.52 66.46 93.72 93.78 24.57 24.60 38.26 38.23 67.17 67.27 84.72 84. 64.79 64.83 63.86 63.87 CLIP-ViT-B/16 B.4 Performance Comparison on Larger-Scale VLMs Our STS method can theoretically be applied to various contrastively pre-trained vision-language models such as CLIP ViT-B/16 and CLIP ViT-L/14. In Table B6, we use OpenAI CLIP ViTL/14 larger-scale OpenAI CLIP model, as an example, and compare the performance of our STS method and zero-shot on robustness to natural distribution shifts. We can observe that our STS still outperforms zero-shot by large margin on average across 5 datasets, showcasing that our method generalizes well to larger-scale VLMs. Table B6: Performance comparison on robustness to natural distribution shifts. We present top1 accuracy (%) results by employing the larger-scale ViT-L/14 variant of CLIP [30]. The reported results of STS are based on single random seed. The best performance is highlighted in bold. Method ImageNet V2 Sketch Average OOD Average CLIP-ViT-L/14 Zero-Shot STS (Ours) 73.45 75.37 68.76 78.52 67.79 69.88 85.39 88. 57.81 59.85 70.64 74.34 +1.92 +9.76 +2.09 +2.68 +2.04 +3. 69.94 74.08 +4.14 Singular Vector Selection for Test-Time Latent Steering In our test-time adaptation (TTA) approach, Singular Value Decomposition (SVD) is applied to text prototypes (e.g., photo of [CLASS]) to analyze their underlying semantic structure. The full set of singular vectors describes this structure completely. However, when adapting to new out-of-distribution (OOD) domain at test time, not all components of this original structure may be equally relevant or contribute equally to successful adaptation. The process of selecting subset of 16 singular vectors aims to distill the dimensions most pertinent for characterizing the new domains relationship to the class concepts, potentially leading to more focused and effective adaptation. As indicated in the main document (Section 3), substantial portion of the variance (e.g., 90%) is often concentrated in significantly smaller subset of singular values, highlighting the potential for effective dimensionality reduction. Discarding low-variance components here means removing directions where our specific classes are textually very similar. This section details two principled methods for selecting kt singular vectors and presents their impact on the zero-shot performance of CLIP [30] when integrated with our TTA method. Our findings indicate that strategic selection of singular vectors significantly enhances performance, with the Gavish-Donoho method yielding slightly superior results. C.1 Energy-Based Singular Vector Selection common heuristic for dimensionality reduction via SVD is to retain the top-kt singular values, σ1 σ2 σk, such that they capture predefined percentage of the total \"energy\" (sum of squared singular values). We investigate threshold of 98% energy, selecting kt such that: (cid:80)kt (cid:80)k i=1 σ2 j=1 σ2 0.98 This method aims to preserve the most dominant components of variance within the text prototype manifold, assuming these capture the most salient semantic information. Based on our experiments k98% is typically less than k. C.2 Gavish-Donoho Optimal Hard Thresholding The Gavish-Donoho method [12] offers theoretically grounded approach for selecting an optimal number of singular values kt, to retain, particularly when seeking robust low-rank representation of the data. Developed from random matrix theory, this method computes an optimal singular value threshold ω. This threshold is designed to effectively separate the more dominant and structured components within the singular value spectrum from those that are less influential or exhibit characteristics similar to the singular values of random matrix. The singular values σi < ω (and their corresponding singular vectors) are consequently excluded, leading to the determination of the rank kt. The specific threshold value depends on the aspect ratio of the matrix undergoing SVD and can be established using the properties of the singular value spectrum itself (e.g., via the median singular value), thus providing data-driven cutoff without requiring an explicit \"noise\" model. We apply this method to determine kt for our text prototype matrix, in order to identify subset of singular vectors that is the basis of adaptation. C.3 Performance Impact of Singular Vector Selection To demonstrate the efficacy of these selection strategies, we evaluate our TTA method on ImageNet-A dataset. Table C1 presents the top-1 accuracy, comparing the zero-shot CLIP baseline [30] with our TTA approach when using singular vectors selected by the 98% energy criterion (k98%) versus the Gavish-Donoho method (kt). Table C1: Impact of singular vector selection on Test-Time Adaptation (TTA) performance. Average Top-1 accuracy (%) over 3 random seeds is reported. Both selection methods significantly improve over the zero-shot baseline, with Gavish-Donoho (GD) offering slight further advantage. Dataset Zero-Shot (ViT-B/16) ImageNet-A [16] 47.87 STS (Ours) (k98% SVs) 61.09 (+13.22) STS (Ours) (kt SVs) 61.23 (+13.36) The results in Table C1 clearly indicate that employing principled selection of singular vectors substantially boosts the performance of our TTA method compared to the zero-shot baseline. Both the 98% energy criterion and the Gavish-Donoho threshold lead to significant improvements. Notably, the Gavish-Donoho method consistently achieves slightly better performance, suggesting its 17 effectiveness in identifying an optimal rank for the text prototype subspace used in our adaptation process. This underscores the importance of focusing the adaptation on the most informative and semantic dimensions derived from the text prototypes."
        },
        {
            "title": "D Additional Implementation Details",
            "content": "D.1 Dataset Details In Table D1, we present the detailed statistics of each dataset we used in our experiments, including the number of classes, the sizes of training, validation and testing sets, and their original tasks. Table D1: Detailed statistics of datasets used in experiments. Note that the 4 ImageNet variant datasets are designed for evaluation and only contain the test sets. Dataset Classes Training Validation Testing Task ImageNet [6] ImageNet-A [16] ImageNet-V2 [31] ImageNet-R [14] ImageNet-Sketch [37] Caltech101 [8] DTD [5] EuroSAT [13] FGVCAircraft [25] Flowers102 [27] Food101 [3] OxfordPets [29] StanfordCars [22] SUN397 [38] UCF101 [33] 1,000 200 1,000 200 1,000 100 47 10 100 102 101 37 196 397 101 1.28M - - - - 4,128 2,820 13,500 3,334 4,093 50,500 2,944 6,509 15,880 7,639 - - - - - 50,000 Object recognition 7,500 Robustness of adversarial attack 10,000 Robustness of collocation 30,000 Robustness of multi-domains 50,889 Robustness of sketch domain 1,649 1,128 5,400 3,333 1,633 20,200 736 1,635 3,970 1,898 2,465 Object recognition 1,692 Texture recognition Satellite image recognition 8,100 Fine-grained aircraft recognition 3,333 Fine-grained flowers recognition 2,463 Fine-grained food recognition 30,300 Fine-grained pets recognition 3,669 Fine-grained car recognition 8,041 19,850 Scene recognition 3,783 Action recognition In Table D2, we detail the specific hand-crafted prompts utilized in our experiments. Table D2: Datasets with associated textual prompts. The first prompt is applied generally, while the subsequent generic prompts (indicated by the brace) are collectively used as an ensemble for each dataset. These 7 generic templates are highlighted in the official CLIP repository [28]. Dataset ImageNet [6] ImageNet-V2 [31] ImageNet-Sketch [37] ImageNet-A [16] ImageNet-R [14] Caltech101 [8] DTD [5] EuroSAT [13] FGVCAircraft [25] Flowers102 [27] Food101 [3] OxfordPets [29] StanfordCars [22] SUN397 [38] UCF101 [33] Prompts photo of {CLASS}. bad photo of the {CLASS}. {CLASS} in video game. origami {CLASS}. photo of the small {CLASS}. Ensemble art of the {CLASS}. photo of the large {CLASS}. itap of {CLASS}."
        },
        {
            "title": "E License Information",
            "content": "Datasets. We list the known license information for the datasets below: CC BY-SA 4.0 License: OxfordPets [29]. MIT License: ImageNet-A [16], ImageNet-V2 [31], ImageNet-R [14], and ImageNet-Sketch [37]. Research Purposes only (term of access): ImageNet [6], DTD [5], StanfordCars [22], SUN397 [38], FGVCAircraft [25]. Source Code. In this work, we also use some code implementations from existing baseline methods to report their results: CLIP [30] , CoOp [43], MaPLe [21], TPT [32]. The source code used in this paper for these methods is available under the MIT License."
        }
    ],
    "affiliations": [
        "Rutgers University"
    ]
}