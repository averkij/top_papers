{
    "paper_title": "UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning",
    "authors": [
        "Liangyu Chen",
        "Hanzhang Zhou",
        "Chenglin Cai",
        "Jianan Zhang",
        "Panrong Tong",
        "Quyu Kong",
        "Xu Zhang",
        "Chen Liu",
        "Yuqi Liu",
        "Wenxuan Wang",
        "Yue Wang",
        "Qin Jin",
        "Steven Hoi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 6 8 2 0 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "UI-INS: ENHANCING GUI GROUNDING WITH MULTIPERSPECTIVE INSTRUCTION-AS-REASONING , Hanzhang Zhou2, Chenglin Cai2, Jianan Zhang2, Panrong Tong2 Liangyu Chen1,2 Quyu Kong2, Xu Zhang2, Chen Liu2, Yuqi Liu3, Wenxuan Wang1 Yue Wang2 (cid:66), Qin Jin1 (cid:66), Steven HOI2 1Renmin University of China (cid:66) yue.w@alibaba-inc.com, (cid:66) qjin@ruc.edu.cn 2Tongyi Lab, Alibaba Group 3CUHK"
        },
        {
            "title": "ABSTRACT",
            "content": "GUI grounding, which maps natural-language instructions to actionable UI elements, is core capability of GUI agents. Prior works largely treats instructions as static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through careful investigation of existing grounding datasets, we find 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins."
        },
        {
            "title": "INTRODUCTION",
            "content": "Automated agents for graphical user interfaces (GUIs) are an important frontier in the pursuit of artificial general intelligence (AGI) (Wang et al., 2024b). Their effectiveness is dependent on GUI grounding, i.e., the task of mapping natural language instruction to the corresponding actionable UI element in screenshot or live interface. The natural language instruction is central to GUI grounding: it is primary input alongside the GUI screenshot and translates high-level user intent into low-level, executable actions. Consequently, the clarity and precision of instruction directly impact grounding success. However, the impact of grounding instruction has been largely overlooked in prior works. In this paper, we provide comprehensive analysis covering instruction diversity, quality, and algorithmic strategies, and establish concrete basis for more effective GUI grounding. Work was done during internship at Tongyi Lab, Alibaba Group."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Performance comparisons of UI-Ins and other state-of-the-art methods. We focus on instruction diversity and reveal fundamental mismatch: humans flexibly choose the most effective pathway among multiple instructional perspectives, whereas current models are trained in narrow, fixed style. For example, to express single intent such as close window, humans may describe the corresponding UI element as its appearance (click the red X), function (close the file manager), spatial location (the button in the top-right corner), or high-level intent (get rid of this screen). Humans strategically switch among these perspectives, choosing the most effective description for the task at hand, as illustrated in Fig. 3. Our quantitative analysis in Sec. 2.1 likewise shows that leveraging instruction diversity is key to improving grounding accuracy. However, prevailing GUI grounding models are typically trained to map single instruction style to an action, with limited capacity to reason across different perspectives. This limitation forms key bottleneck to flexible adaptability and robust interpretation of GUI grounding tasks. Those insights motivate paradigm shift: rather than treating instructions as static inputs, we can regard them as dynamic reasoning pathways. Different instruction types are not merely alternative phrasings; they encode distinct analytical angles for identifying UI element. An intelligent GUI agent should not only understand command but also actively select the most effective reasoning process to infer the users intent. We term this new paradigm Instruction-as-Reasoning. Beyond this conceptual shift, we also find pervasive instruction quality issues in grounding datasets. Specifically, we manually inspected 1,909 data entries sampled from some popular datasets, including OS-Atlas (Wu et al., 2024a), Widget Captioning (Li et al., 2020), and AMEX (Chai et al., 2025). As shown in Fig. 2b, we found that notable 23.3% samples contained various quality flaws, introducing considerable noise that could adversely affect model training. Based on these findings, we introduce simple yet effective framework. We propose data pipeline systematically cleans noisy annotations and, crucially, augments existing data with rich diversity of instruction styles, creating dataset curated specifically for multi-perspective instruction reasoning. With this high-quality data as our foundation, we then propose our Instruction-as-Reasoning framework. This novel two-stage training paradigm first uses Supervised Fine-Tuning (SFT) to explicitly teach the model to use diverse instruction perspectives as reasoning pathways. Then, it employs Group Relative Policy Optimization (GRPO) (Guo et al., 2025; Shao et al., 2024) in the Reinforcement Learning (RL) stage, enabling the model to learn how to choose the optimal instruction perspective as the reasoning pathway for any given situation. Building on this framework, we introduce the UI-Ins-7B and UI-Ins-32B models. Empirical evaluations conducted across multiple distinct benchmarks validate the strength of our approach, as illustrated in Fig. 1. To provide additional insights for grounding, we conduct an in-depth analysis of our Instructionas-Reasoning from multiple perspectives. First, how can reasoning be formulated to enhance, rather than hinder grounding? Consistent with prior works (Lu et al., 2025; Tang et al., 2025), we confirm that free-form reasoning approach often degrades model performance during GRPO. In contrast, experimental results indicate that our proposed Instruction-as-Reasoning consistently enhances performance by large margin across various base models, establishing it as highly effective reasoning paradigm for grounding. Second, how can we mitigate policy collapse in the SFT+RL framework? We identified that models fine-tuned via SFT using only coordinates as ground truths often exhibit highly uniform responses, leading to ineffective exploration and policy collapse in RL. This is also noted by Phi-Ground (Zhang et al., 2025). However, our Instructionas-Reasoning framework mitigates this issue by instilling diverse exploratory capabilities after SFT,"
        },
        {
            "title": "Preprint",
            "content": "(a) (b) (c) Figure 2: Preliminary analysis of GUI Grounding Instructions. (a) Instruction diversity influences performance significantly. (b) Instruction quality problems in existing open-source datasets. (c) Low instruction quality undermines training efficacy. enabling the model to generate diverse rollouts during RL and thereby avoid policy collapse. Finally, is UI-Inss reasoning capability limited to predefined perspectives seen during training? Interestingly, we observed that after training with Instruction-as-Reasoning, the model not only learns to select the optimal reasoning pathway but also develops emergent capabilities to combine different reasoning perspectives and to reason from novel instruction perspectives not seen in training. In summary, our contributions are as follows: Systematic Investigation into GUI Grounding Instructions. We conduct systematic analysis of instructions in GUI grounding, revealing two crucial insights: (1) striking 23.3% of samples instructions in major datasets are flawed, and (2) there is massive potential improvement in leveraging instruction diversity, which can unlock up to substantial 76% relative performance gain even without training. Instruction-as-Reasoning Paradigm. Building on insights above, we propose the Instruction-asReasoning paradigm, which reframes instructions from static inputs to dynamic reasoning pathways. We realize this through SFT+GRPO training framework that first teaches the model to use diverse instruction perspectives as reasoning pathways and then incentivizes it to select the optimal analytical reasoning pathway for any given GUI scenario. SOTA Performance Across Diverse Benchmarks. Our UI-Ins-7B and UI-Ins-32B establish new SOTA performance across five most well-known grounding benchmarks. Notably, UI-Ins-32B achieves 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2, significantly surpassing its strongest counterparts. Moreover, our superior grounding capability leads to strong online agent performance on AndroidWorld when combined with GPT-5 as the planner, yielding 74.1% success rate. In-depth Analysis. Our analysis provides additional insights for grounding. We demonstrate how reasoning can be formulated to augment rather than hinder performance and how our method mitigates policy collapse in the SFT+RL framework. Furthermore, we reveal that our approach unlocks emergent reasoning capabilities, allowing the model to reason from novel perspectives."
        },
        {
            "title": "2 HOW MUCH DO INSTRUCTIONS REALLY MATTER?",
            "content": "The natural language instruction is primary input to grounding tasks, serving as the sole carrier of high-level intent in GUI grounding. But to what extent do the key aspects of an instructions formulation, namely its analytical perspective and its correctness, truly impact models performance? Prior works have largely treated the instruction as simple input string, leaving its impact underexplored. We highlight that the instruction is central, understudied variable in GUI grounding. To probe this view, we conduct preliminary analysis guided by two foundational research questions: RQ1: How does the diversity of instructional perspectives affect grounding accuracy? RQ2: What is the state of instruction quality in GUI grounding datasets, and what is its impact?"
        },
        {
            "title": "2.1 DOES INSTRUCTION DIVERSITY UNLOCK HIGHER PERFORMANCE?",
            "content": "Humans instinctively choose the most effective way to describe an object based on the context like Fig. 3. Does providing model with similarly diverse, perspective-rich instructions unlock better performance? To investigate this, we conducted controlled experiment on the ScreenSpot-Pro benchmark. We systematically rewrote its original instructions to reflect four distinct perspectives: Appearance, Functionality, Location, and Intent. We then evaluated the zero-shot performance of Qwen2.5-VL-7B on each instruction set. The results, shown in Fig. 2a, reveal two critical insights. First, instruction diversity matters significantly. Instructions from perspectives of appearance, function, and intent all substantially outperform the original instructions. This demonstrates that even without retraining, simply providing diverse instruction perspectives can unlock significant latent capabilities within the model. Second, the ability to select the most appropriate instruction perspective leads to higher performance ceiling. The Combined bar, representing the performance if model could always pick the best-performing perspective for each sample, achieves relative improvement of 76%, far surpassing any single instruction perspective. Overall, these results reveal considerable untapped potential in leveraging instruction diversity, both by introducing multiple instruction perspectives and by selecting the optimal perspective per instance. This motivates our algorithm that learns to leverage diverse instruction perspectives as reasoning and dynamically chooses the best analytical angle. Figure 3: Effective instruction perspectives in different GUI scenarios. Samples are from OS-Atlas Dataset and the ground truth bounding box is labeled with green box beside yellow star. 2.2 CAN WE TRUST EXISTING DATASETS FOR INSTRUCTION QUALITY? While utilizing instruction diversity is promising, its effectiveness rests on foundation that the original instructions are correct. But is this foundation valid? To probe the instruction quality of the grounding datasets, we conducted large-scale manual analysis. Specifically, we examined 1, 909 samples from three prominent datasets, OS-Atlas (Wu et al., 2024a), AMEX (Chai et al., 2025), and Widget Captioning (Li et al., 2020). Our analysis reveals pervasive instruction quality issues. As shown in Fig. 2b, 23.3% of instructions exhibit substantive flaws, including ambiguity or referring to nothing shown in Fig. 4. To further quantify the impact of such flaws, we trained the same model on the original dataset and on cleaned version. Experimental results are depicted in Fig. 2c: models trained on cleaned data achieve substantial and consistent performance gains across multiple benchmarks. In other words, flawed instruction data can significantly degrade downstream performance when used for training. Figure 4: Instruction quality flaws in grounding datasets. Left: Ambiguous match, an instruction maps to multi UI elements. Right: Mismatch, no valid UI element matches the instruction. These findings indicate that existing datasets suffer from instruction quality problems that actively harm model performance. Consequently, data cleaning is not optional niceties but necessary prerequisites for meaningful training, especially when our goal is to teach models to leverage diverse instruction perspectives as reasoning."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Overview of our high-quality data processing pipeline. The pipeline first preprocesses the ground-truth bounding box, then leverages GPT-4.1 to generate instructions from diverse perspectives, and finally employs verification stage to filter the results by ensuring precise alignment between the instruction and the ground truth box."
        },
        {
            "title": "3 METHOD",
            "content": "Our methodology is architected to address the two fundamental challenges identified in Sec. 2: the data quality issues and the untapped potential of instruction diversity. We first introduce highquality data pipeline designed to establish the necessary preconditions for effective model training. With this robust data foundation, we then present our core algorithmic contribution, Instructionas-Reasoning, two-stage training framework that empowers models to use diverse instructions as reasoning pathways and to select the optimal analytical perspective during reasoning. 3.1 TASK DEFINITION GUI Grounding aims to localize single UI element corresponding to natural language instruction on graphical user interface (Wang et al., 2024b). Formally, given GUI screenshot and natural language instruction I, the model should predict coordinate point = (xp, yp) that indicates the target elements location. 3.2 DATA PIPELINE FOR MULTI-PERSPECTIVE REASONING Our preliminary analysis (Sec. 2) revealed that data quality is prerequisite for meaningful training (Sec 2.2) and that instruction diversity unlocks significant performance gains (Sec. 2.1). To this end, we developed data processing pipeline focused on two primary objectives: establishing clean data foundation and then systematically augmenting it with diverse, multi-perspective instructions. Pre-processing. To rectify the pervasive annotation noise found in existing datasets, we first perform lightweight pre-processing step. We use OmniParser V2 (Lu et al., 2024) to detect all UI elements on screenshot and apply simple IoU-based method to refine or filter the original ground truth bounding box. This ensures each instruction is associated with reliable spatial anchor, and the fflawedinstructions are filtered at the same time. The pre-processing forms the clean foundation necessary for the subsequent augmentation. Multi-Perspective Instruction Augmentation. The core of our pipeline focuses on enriching instruction diversity. We leverage GPT-4.1 (OpenAI, 2025a) to generate new instructions from the four fundamental analytical perspectives identified in our analysis: appearance, functionality, location, and intent. For each data instance, the model receives the screenshot with the highlighted target element and is prompted to create set of high-quality, diverse phrasings. To mitigate LLM hallucinations and ensure strict one-to-one mapping, each generated instruction undergoes verification step where GPT-4.1 confirms it unambiguously refers only to the target element. This process yields high-quality, multi-perspective corpus specifically curated to teach complex reasoning. 3.3 INSTRUCTION-AS-REASONING With such multi-perspective dataset at hand, we introduce the framework to use it. As discussed in Sec. 2.1, leveraging diverse instruction perspectives and dynamically choosing the best analytical angle are key to unlock superior grounding performance. As shown in Fig. 6, our Instruction-asReasoning framework is two-stage training approach that instills this capability: (i) SFT stage"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Overview of Instruction-as-Reasoning. We leverage diverse instructions as explicit reasoning pathways to teach model multi-perspective reasoning paths in SFT stage, then let model explore unconstrained perspectives to find the optimal ways in different scenarios. that teaches the model to use multi-perspective instructions as explicit reasoning pathways, and (ii) RL stage that trains the model to use the optimal analytical angle for each sample. 3.3.1 SFT STAGE: LEARNING TO GENERATE DIVERSE REASONING The goal of the SFT stage is to explicitly instill the model with the ability to perform Instructionas-Reasoning: utilizing diverse instruction perspectives as analytical reasoning before predicting the grounding coordinate point. Concretely, the model first generates an intermediate reasoning text, i.e., rewritten instruction from one instruction perspective, which serves as an actionable reasoning pathway  (Fig. 6)  . Then outputs the final coordinate point. Given the grounding model with parameters θ, the training objective in SFT stage is to maximize the log-likelihood of the target sequence Ygt across the entire dataset D, formally expressed as: max θ (cid:88) (S,I,Ygt)D log (YgtS, I; θ), where Ygt = Rgt pgt (1) In this formulation, denotes sequence concatenation. The ground-truth reasoning text, Rgt, is randomly sampled from one of the valid augmented instruction perspectives, while pgt represents the ground-truth coordinate point. An example of SFT prompt and answer is in Sec B.1. This unified objective elegantly compels the model to co-optimize two distinct but related skills: Reasoning Generation: Learning to produce reasoning (Rgt) in an instruction perspective. Grounded Prediction: Learning to predict the correct coordinate point (pgt) conditioned on both inputs and its self-generated reasoning. By fine-tuning on this objective, the model learns to reason from diverse instruction perspectives, building foundational for RL stage training. 3.3.2 RL STAGE: LEARNING TO SELECT THE OPTIMAL PERSPECTIVE The SFT stage equips the model with the ability to generate reasoning from multiple instruction perspectives. However, it does not teach the model which reasoning pathway is optimal for given context. To transcend this limitation and incentivize the model to dynamically select the most effective analytical perspective, we introduce an RL stage. The goal of this stage is to fine-tune the SFT-trained model to discover and select reasoning strategies that maximize grounding accuracy. To achieve this, we employ Group Relative Policy Optimization (GRPO) (Guo et al., 2025). In this phase, we modify the prompt to simply ask the model tothink"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison on the MMBench-GUI L2 benchmark. We use to denote the results evaluated by us. Model Windows MacOS Linux iOS Android Web Avg. Bas. Adv. Bas. Adv. Bas. Adv. Bas. Adv. Bas. Adv. Bas. Adv. GPT-4o (OpenAI, 2024) Claude-3.7 (Anthropic, 2024) Qwen-Max-VL (Yang et al., 2024a) 2.9 1.5 1.1 8.7 4.3 1.1 1.0 5.1 3.3 2.5 1.4 3.2 2.9 1.5 0.7 12.5 7.5 1.1 0.0 13.7 10.6 1.4 1.4 3.2 2.3 4.7 43.9 36.8 58.8 56.1 53.9 30.1 77.4 59.1 79.5 70.1 74.8 58.8 58.0 ShowUI-2B (Lin et al., 2024) Qwen2.5-VL-7B (Bai et al., 2025) OS-Atlas-7B (Wu et al., 2024a) Aguvis-7B (Xu et al., 2025) UI-TARS-1.5-7B (Seed, 2025) UGround-V1-7B (Gou et al., 2025) GUI-Actor-7B (Wu et al., 2025) SE-GUI-7B (Yuan et al., 2025) GTA1-7B (Yang et al., 2025) GUI-G2-7B (Tang et al., 2025) InfiGUI-G1-7B (Liu et al., 2025d) 9.2 4.4 24.1 10.4 25.1 11.7 29.0 19.7 17.4 8.7 22.9 12.7 16.0 31.4 16.5 31.3 22.0 21.5 12.2 66.6 55.2 35.1 35.2 40.3 32.5 33.9 36.9 18.8 44.4 21.7 31.4 13.3 74.8 48.8 69.6 46.8 61.3 35.4 41.4 37.3 21.7 48.1 33.3 33.5 25.0 67.5 65.2 61.0 51.0 61.6 45.5 45.7 68.3 39.0 69.0 44.5 64.4 37.8 88.5 69.4 90.5 69.3 81.0 56.5 64.3 66.8 39.0 71.3 48.6 56.5 31.1 92.7 70.9 93.5 71.0 88.7 64.6 65.7 80.8 55.1 81.4 60.4 64.9 41.8 94.3 82.7 93.5 79.7 89.7 72.1 76.5 77.5 57.7 77.1 60.7 68.6 44.9 95.5 80.0 95.5 83.7 89.7 68.8 76.6 76.8 57.4 80.3 63.9 68.6 53.6 93.9 83.3 96.3 84.5 90.3 74.7 78.5 79.7 55.1 79.7 64.7 69.6 50.0 95.2 82.7 96.6 85.4 91.9 75.6 78.8 82.7 61.8 83.8 63.9 72.3 52.0 94.9 89.4 95.2 85.6 93.5 76.3 80. Qwen2.5-VL-72B (Bai et al., 2025) 55.7 33.8 49.9 30.1 40.3 20.9 56.1 28.2 55.6 25.4 68.4 45.8 41.8 Qwen2.5-VL-32B (Bai et al., 2025) 73.4 49.3 76.2 57.8 61.3 33.2 91.1 80.6 90.4 80.6 81.6 65.6 72.1 InternVL3-78B (Zhu et al., 2025) 70.1 42.6 75.7 52.3 59.2 41.3 93.6 80.6 92.7 78.6 90.7 65.9 72.2 UI-TARS-DPO-72B (Qin et al., 2025) 78.6 51.8 80.3 62.7 68.6 51.5 90.8 81.2 93.0 80.0 88.1 68.5 74.3 GTA1-32B (Yang et al., 2025) 82.3 66.9 89.0 74.0 73.3 52.0 96.2 88.2 95.8 88.5 95.2 79.9 83.4 82.7 64.7 87.2 75.1 71.7 51.5 94.9 89.7 95.8 89.0 93.2 80.8 83.1 UI-Ins-7B 84.9 68.4 88.4 73.4 68.6 56.1 96.5 91.2 97.2 92.4 94.8 85.1 84.9 UI-Ins-32B before answering, without providing the explicit list of predefined perspectives (appearance, function, etc.). This open-ended instruction encourages the model to explore wider space of reasoning patterns, including synthesizing multiple perspectives or even formulating entirely novel ones. The model then learns to select the optimal analytical perspective from the feedback of RL rewards. We calculate rewards by simple point-in-box function, then, the rewards {ri}G into advantages via Z-score normalization: i=1 are normalized ˆAi,t = (cid:114) 1 (cid:80)G i=1 ri ri 1 (cid:16) (cid:80)G i=1 ri 1 (cid:17)2 (cid:80)G i=1 ri where is the rollout number. Finally, the model is optimized by minimizing the objective: = 1 G (cid:88) i=1 π(oi I, S) πold(oi I, S) ˆAi,t (2) (3) where πold( ) denotes the old policy and ˆAi,t is the advantage associated with prediction oi. By iteratively applying this process, the model learns to prioritize reasoning pathways that consistently lead to cothe rrect coordinate point, effectively learning an optimal, context-dependent strategy for instruction perspective selection. Interestingly, we find that the model also learns to combine multiple perspectives and even formulate entirely novel reasoning perspectives (Sec. 4.5)."
        },
        {
            "title": "4 EXPERIMENT AND RESULTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Data and Implementation Details. We collect data from several public datasets, including OSAtlas (Wu et al., 2024a), Omniact (Kapoor et al., 2024), Android Control (Li et al., 2024), AMEX (Chai et al., 2025), and AgentNet (Wang et al., 2025b), covering diverse operating systems such as"
        },
        {
            "title": "Preprint",
            "content": "Model Table 2: Performance comparison on the UI-I2E-Bench benchmark. Grouped by Platform Grouped by Implicitness Web Desktop Mobile Explicit Implicit Overall OS-Atlas-4B (Wu et al., 2024a) UI-I2E-VLM-4B (Liu et al., 2025a) Uground-V1-2B (Gou et al., 2025) UI-TARS-2B (Qin et al., 2025) Aguvis-7B (Xu et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) OS-Atlas-7B (Wu et al., 2024a) UI-TARS-7B (Qin et al., 2025) GUI-Actor-7B (Wu et al., 2025) UI-I2E-VLM-7B (Liu et al., 2025a) Uground-V1-7B (Gou et al., 2025) SE-GUI-7B (Yuan et al., 2025) GUI-G2-7B (Tang et al., 2025) UI-TARS-1.5-7B (Seed, 2025) InfiGUI-G1-7B (Liu et al., 2025d) GTA1-7B (Yang et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) UI-TARS-72B (Qin et al., 2025) Uground-V1-72B (Gou et al., 2025) GTA1-32B (Yang et al., 2025) UI-Ins-7B UI-Ins-32B 54.6 60.9 66.4 62.2 45.1 56.9 52.2 56.5 65.2 62.1 70.8 68.4 53.4 79.5 84.6 77.5 49.0 76.7 77.1 74.7 93.3 90.5 95.7 19.9 38.9 49.5 54.0 47.6 41.6 48.9 58.0 63.2 64.0 65.7 66.3 67.8 68.8 66.3 71.3 47.2 61.2 69.8 74.6 77.6 72.8 81. 58.6 61.4 59.9 66.7 60.3 61.7 68.1 65.7 72.9 76.2 73.5 77.4 84.0 74.1 83.0 83.5 55.3 67.5 75.5 78.2 84.4 83.8 88.2 51.5 61.9 72.9 74.1 61.1 58.4 63.2 71.4 71.6 72.0 81.3 77.5 82.1 81.3 85.0 87.0 49.6 73.8 80.9 84.5 91.4 88.9 92.9 39.9 48.3 47.9 54.5 48.4 51.0 55.8 55.3 66.1 67.9 63.6 68.6 67.5 68.2 72.7 72.8 52.5 62.7 69.4 71.3 78.7 76.3 83.9 44.3 53.4 57.4 62.0 53.2 53.8 58.6 61.4 68.2 69.5 70.3 72.0 73.1 73.2 77.4 78.2 51.4 66.9 73.7 76.3 83.5 81.1 87.3 Windows, MacOS, Linux, and Android. All samples are processed through our pipeline to ensure quality. We employ Qwen2.5-VL-7B and Qwen2.5-VL-32B as our backbone architectures. Training examples of the SFT and RL stages are in Sec. B. The training procedure consists of two stages: SFT Stage We fine-tune the models on approximately 283k instances for one epoch. To teach the model to reason from diverse instruction perspectives, each training instance is constructed by randomly selecting two distinct instruction perspectives from set of four (appearance, spatial, function and goal) that we defined. One is designated as the instruction perspective, and the other as the reasoning perspective. We use global batch size of 256 and learning rate of 5e-6. RL Stage The GRPO training utilizes 33k instances, expanded to approximately 100k training samples by generating sample per instruction perspective. We leave the analytical perspective unspecified in the prompt to encourage exploration. We adopt learning rate of 1e-6 and 8 rollouts. The batch size is set to 256 for the 7B model and 128 for the 32B model. Baselines and Metrics. We compare our methods grounding performance against extensive SOTA baselines. These include models that are primarily trained using supervised fine-tuning, such as Jedi (Xie et al., 2025) and Aguvis (Xu et al., 2025), methods that using RL paradigm, such as GUIActor (Wu et al., 2025) and InfiGUI-G1 (Liu et al., 2025d), and influential grounding models such as UI-Tars-1.5-7B (Seed, 2025) and GTA1-7B (Yang et al., 2025).. Besides, we also compare UI-Ins with some agentic frameworks such as AgentS2 (Zhou et al., 2024) and InfiGUIAgent (Liu et al., 2025b) on the online benchmark. Following prior works (Yang et al., 2025; Liu et al., 2025d; Tang et al., 2025), we evaluate GUI Grounding performance using the point-in-box accuracy. prediction is considered correct if the predicted coordinate point = (xp, yp) falls within the ground-truth bounding box = (xl, yl, xr, yr), where the (xl, yl) denotes the top-left corner and (xr, yr) represents the bottom-right I(pi bi) corner. The accuracy over test set of size is formally defined as: Accuracy = 1 , where I() is the indicator function, which equals 1 if the condition is true and 0 otherwise. (cid:80)N i=1 Evaluation Benchmarks. We evaluate our method on five widely-used grounding benchmarks and challenging online agent environment."
        },
        {
            "title": "Preprint",
            "content": "Model Table 3: Performance comparison on the ScreenSpot-Pro benchmark. CAD Dev. Creative Scientific Office OS Avg. Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon GPT-4o (OpenAI, 2024) Claude C. (Anthropic, 2024) 2.0 0.0 1.3 0.0 1.0 0.0 2.1 0.0 1.1 0.0 0.0 0.0 0.8 14.5 3.7 22.0 3.9 25.9 3.4 33.9 15.8 30.1 16.3 11.0 4.5 17.1 11.2 6.3 22.7 4.1 27.3 3.5 42.4 11.8 32.2 11.3 13.1 4.5 17.8 UI-R1-3B (Lu et al., 2025) 31.9 15.6 24.6 6.2 40.9 7.6 54.8 18.1 57.0 26.4 19.6 7.8 28.7 ZonUI-3B (Hsieh et al., 2025) 16.8 1.6 46.8 4.1 35.9 7.7 49.3 7.3 52.5 20.8 37.4 3.8 26.8 Qwen2.5-VL-7B (Bai et al., 2025) GUI-R1-7B (Luo et al., 2025) 23.9 6.3 49.4 4.8 38.9 8.4 55.6 11.8 58.7 26.4 42.1 16.9 31.0 20.8 9.4 58.4 12.4 50.0 9.1 63.9 31.8 63.3 20.8 30.8 16.9 35.7 UI-TARS-7B (Qin et al., 2025) 49.2 14.1 64.3 15.2 53.0 9.8 72.9 25.5 75.1 30.2 45.8 20.2 44.0 UI-AGILE-7B (Lian et al., 2025) 47.7 9.4 59.1 15.9 59.6 16.1 70.1 25.5 69.5 41.5 55.1 19.1 44.6 GUI-Actor-7B (Wu et al., 2025) SE-GUI-7B (Yuan et al., 2025) 51.3 14.1 68.2 19.3 57.6 9.1 75.0 28.2 78.5 43.4 49.5 25.8 47.2 GUI-G2-7B (Tang et al., 2025) 55.8 12.5 68.8 17.2 57.1 15.4 77.1 24.5 74.0 32.7 57.9 21.3 47.5 50.0 - OpenCUA-7B (Wang et al., 2025b) 53.3 17.2 66.9 20.7 62.6 18.9 76.4 31.8 82.5 50.9 48.6 25.9 50.1 GTA1-7B (Yang et al., 2025) 60.4 21.9 74.7 24.1 63.1 14.7 76.4 31.8 75.7 41.5 49.5 22.5 50.8 UI-Venus-7B (Gu et al., 2025) 57.4 23.4 74.7 24.1 64.6 18.2 80.6 31.8 75.7 39.6 57.0 29.2 51.9 InfiGUI-G1-7B (Liu et al., 2025d) CogAgent-18B (Hong et al., 2024) 7.7 7.1 3.1 14.9 0.7 9.6 0.0 22.2 1.8 13.0 0.0 5.6 0.0 UGround-v1-72B (Gou et al., 2025) 16.8 4.7 55.8 4.8 54.0 10.5 70.8 22.7 61.0 18.9 40.2 7.9 34.5 UI-Tars-72B (Qin et al., 2025) 18.8 12.5 63.0 17.2 57.0 15.4 64.6 20.9 63.3 26.4 42.1 15.7 38.1 Qwen2.5-VL-32B (Bai et al., 2025) 34.5 20.3 74.0 22.1 61.1 16.1 75.0 30.0 74.6 30.2 64.5 33.7 50.5 Qwen2.5-VL-72B (Bai et al., 2025) 54.3 14.1 78.6 26.9 62.6 20.3 77.8 34.5 80.2 47.2 67.3 28.1 53.3 43.7 23.4 82.5 28.3 69.2 14.7 79.9 31.8 80.8 43.4 70.1 32.6 53.6 GTA1-32B (Yang et al., 2025) 55.3 - OpenCUA-32B (Wang et al., 2025b) - - - - - - - - - - - - - - - - - - - - - - UI-Ins-7B UI-Ins-32B 60.9 20.3 75.3 18.6 65.2 18.9 81.3 29.1 79.7 37.7 57.0 25.8 52.2 51.8 29.7 83.1 26.9 69.7 18.9 83.3 34.5 88.7 50.9 70.1 34.8 57.0 Grounding Benchmarks: MMBench-GUI L2 (Xuehui Wang et al., 2025) tests performance on hierarchical instructions, while UI-I2E-Bench (Liu et al., 2025a) focuses on explicit instructions and deeper semantic reasoning for implicit instructions. Showdown (Team, 2025) evaluates instruction-following and low-level control capabilities. ScreenSpot-Pro Li et al. (2025) examines semantic understanding in high-resolution professional softwares. ScreenSpot-V2 (Wu et al., 2024a) is widely adopted benchmark that evaluates model across different operating systems. Online Agent Benchmark: To evaluate our models practical utility in dynamic setting, we report performance on AndroidWorld (Rawles et al., 2024). This benchmark is particularly challenging as it requires the agent to complete multi-step tasks in live, interactive environment. 4.2 GROUNDING RESULTS As demonstrated in Tab. 1 and Tab. 2, our models achieve state-of-the-art performance on GUI grounding benchmarks that emphasize complex instruction understanding, such as MMBench-GUI L2 and UI-I2E Bench. Specifically, UI-Ins-32B sets new SOTA, and UI-Ins-7B significantly outperforms similarly-scaled models. To analyze the impact of our Instruction-as-Reasoning method, we evaluat performance on distinct subsets within these benchmarks. MMBench-GUI L2 is divided into Basic and Advanced subsets, which are distinguished by their instruction style for the same UI element. Basic instructions provide comprehensive visual features, such as rectangular button with dark purple background, whereas Advanced instructions describe the elements inferred purpose, like Upgrade your current workspace. Similarly, UI-I2E-Bench contains explicit and implicit subsets, where an explicit instruction might be Enter your email in the subscription field, while an implicit one requires inference, such as Click to dispatch the email. Quantitative analysis reveals substantial outperformance against strong baselines on benchmarks with varying instruction complexity. On the MMBench-GUI L2 benchmark, our method shows progressively larger gains on more difficult tasks. For instance, UI-Ins-7B surpasses Qwen2.5-VL7B by 134.2% on the Basic subset, and this margin increases to 159.4% on the more challenging Advanced subset. Figure 11 further presents qualitative comparison between UI-Ins-7B and GTA1-7B, demonstrating that our models ability to reason from diverse instruction perspectives"
        },
        {
            "title": "Preprint",
            "content": "Model Table 4: Performance comparison on ScreenSpot-V2 and ShowDown. ScreenSpot-V2 ShowDown Mobile Desktop Web Text Icon. Text Icon. Text Icon. Avg. Avg. Phi-ground-7B (Zhang et al., 2025) OS-Atlas-7B (Wu et al., 2024a) UGround-v1-7B (Gou et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025) UI-Tars-1.5-7B (Seed, 2025) SE-GUI-7B (Yuan et al., 2025) UI-TARS-7B (Qin et al., 2025) GUI-Actor-7B (Wu et al., 2025) OpenCUA-7B (Wang et al., 2025b) GTA1-7B (Yang et al., 2025) GUI-G2-7B (Tang et al., 2025) InfiGUI-G1-7B (Liu et al., 2025d) UI-Venus-7B (Gu et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) GTA1-32B (Yang et al., 2025) OpenCUA-32B (Wang et al., 2025b) UI-Ins-7B UI-Ins-32B 90.2 95.2 83.6 97.6 92.2 99.3 96.9 97.6 - 99.0 98.3 99.0 99.0 95.5 97.9 98.6 - 99.0 98.6 76.4 75.8 90.5 87.2 81.5 89.1 89.1 88.2 - 88.6 91.9 91.9 90.0 84.4 88.2 89.1 - 90.5 90.0 93.6 90.7 85.8 90.2 91.0 96.4 95.4 96.9 - 94.9 95.4 94.3 96.9 93.8 98.5 96.4 - 97.9 99.0 75.9 63.6 86.3 74.2 84.2 78.6 85.0 85.7 - 89.3 89.3 82.1 90.7 88.0 79.3 86.4 - 81.4 87.9 96.5 90.6 95.5 93.2 95.5 92.7 93.6 93.2 - 92.3 94.0 97.9 96.2 88.5 91.2 95.7 - 97.4 97.0 62.0 77.3 83.2 81.3 84.5 81.3 85.2 86.7 - 86.7 87.7 89.2 88.7 81.8 86.2 88.7 - 91.6 93.1 83.8 85.1 87.7 88.8 89.0 90.8 91.6 92.1 92.3 92.4 93.3 93.5 94.1 88.2 91.3 93.2 93. 94.0 94.9 62.5 41.1 57.8 43.6 67.2 63.6 66.1 64.6 - 67.9 70.4 68.2 - 62.3 58.2 71.1 - 73.1 73.8 is crucial for its success on challenging grounding samples. This pattern holds on UI-Ins-32B, where UI-Ins-32Bs advantage over Qwen2.5-VL-32B grows from 12.3% (Basic) to much larger 24.5% (Advanced). similar trend is observed on the UI-I2E-Bench. When compared to GTA1, UI-Ins-32Bs performance gain expands from 1.6% on explicit subset to more substantial 6.6% on implicit subset. The consistent trend of greater improvement on the Advanced and implicit subsets demonstrates that Instruction-as-Reasoning successfully equips the model with enhanced robustness for difficult scenarios, thereby validating the success of our approach. Furthermore, to provide broader validation of our models capabilities, we conduct extensive evaluations on the ScreenSpot-V2, ScreenSpot-Pro, and Showdown benchmarks. As detailed in Tab. 3 and Tab. 4, UI-Ins-32B again achieves SOTA performance, and UI-Ins-7B consistently outperforms other models of similar scale. We observe UI-Ins perform well on different platform domains and software domains, UI-Ins-32B achieves most SOTA on each software of ScreenSpot-Pro and performs well on different operating systems on ScreenSpot-V2 which is also shown in Fig. 12."
        },
        {
            "title": "4.3 ONLINE AGENT RESULTS",
            "content": "To rigorously evaluate the stability and reliability of grounding models in realistic settings, we employ UI-Ins-7B as the grounding executor under GPT-5 (OpenAI, 2025b) planner in the AndroidWorld (Rawles et al., 2024) online benchmark, where each action must be grounded and executed on live and dynamically changing interface. Unlike simulated or replayed settings, this benchmark introduces realistic challenges, including UI drift, variable rendering latency, asynchronous state transitions, and stochastic user feedback, which collectively pose strong challenge to the temporal and spatial consistency of grounding. As suggested in Tab. 5, despite our simple architecture without extra knowledge guidance in our designed prompts as shown in Sec. B.3, our framework still achieves 74.1% task success rate, outperforming strong closed-source models such as Gemini 2.5 Computer Use (DeepMind, 2025) and UI-TARS-2 (Wang et al., 2025a). This result demonstrates that UI-Ins provides precise and stable visual grounding, maintaining semantic alignment and action reliability across diverse app layouts and dynamic interface updates. Additionally, our UI-Ins-7B grounding executor achieves substantial 24.1% performance improvement over its counterpart, Qwen2.5-VL-7B, under the same configuration using GPT-5 as the plan-"
        },
        {
            "title": "Preprint",
            "content": "Table 5: SOTA Performance on AndroidWorld. Our framework achieves this result by using our model as grounding executor under GPT-5 planner, surpassing strong baselines including UITARS-2 and the Gemini 2.5 Computer Use. Model OpenAI CUA-o3 (OpenAI, 2025) Gemini 2.5 Computer Use (DeepMind, 2025) UI-TARS-2 (Zhou et al., 2024) InfiGUIAgent (Liu et al., 2025b) Ponder&Press (Wang et al., 2024a) Uground (Gou et al., 2025) Aria-UI (Yang et al., 2024b) UI-Tars (Qin et al., 2025) AgentS2 (Zhou et al., 2024) JT-GUIAgentV2 (China Mobile, 2025) Qwen2.5-VL-7B (GPT-5 as planner) UI-Ins-7B (GPT-5 as planner) Model Type Agent Framework Model Model Agent Framework Agent Framework Agent Framework Agent Framework Agent Framework Agent Framework Agent Framework Agent Framework Agent Framework Success Rate 52.5 69.7 73.3 9.0 34.5 44.0 44.8 46.6 54.3 67. 50.0 74.1 ner. This demonstrates that enhanced grounding capability can effectively translate into improved performance on online agent tasks. 4.4 ABLATION STUDY Data Pipeline Ablation Study As shown in Fig. 7a, we manually inspected 1,542 samples generated by our data processing pipeline and found an error rate below 8%. This represents significant reduction from the 23.3% error rate observed in the original data. To further validate the effectiveness of our data pipeline, we conduct an ablation study using SFT on 210k origin samples, which corresponds to 180k cleaned samples. As shown in Fig. 7b, our data pipeline provides consistent performance improvement across multiple benchmarks. Table 6: Ablation study on training stages. We report accuracy on MMBench-GUI L2 (MM), UI-I2E-Bench (I2E), Showdown (Show), ScreenSpot-Pro (Pro), and ScreenSpot-V2 (V2). Training Stage Ablation Study To validate the necessity of SFT+RL training stages for our Instruction-as-Reasoning method. We compare the UI-Ins-7B against two variants: one trained only with SFT and another trained only with RL. In all settings, the model is prompted to generate an intermediate reasoning process. The results of Tab. 6 indicate that both the SFT and RL stages are critical for achieving optimal performance. The absence of either stage leads to an accuracy degradation, highlighting the importance of first teaching the model to generate reasoning from diverse perspectives and then allowing it to optimize the selection of the optimal reasoning pathway. 63.4 72.4 76.3 83.1 SFT RL MM I2E Show Pro 24.4 37.0 37.1 52.2 86.5 88.6 90.6 94.0 56.0 69.2 70.1 81.1 43.6 66.6 67.5 73. V2 4.5 DEEPER INSIGHTS INTO INSTRUCTION-AS-REASONING Having established the strong performance of UI-Ins, we now delve deep into the Instructionas-Reasoning framework to understand its effectiveness. We investigate several central questions below: Table 7: Ablation on the intermediate reasoning component. Its removal results in significant performance degradation across all benchmarks. represents let the model use Instruction as Reasoning in the corresponding stage. Is an intermediate reasoning step necessary? fundamental question is whether generating intermediate reasoning is essential to our method. To answer this, we conducted an ablation study by completely removing the reasoning generation from both the SFT and RL SFT RL MM I2E Show Pro V2 79.1 78.8 81.6 83.1 70.7 71.6 76.2 81.1 66.1 68.4 72.0 73. 44.8 48.0 47.5 52.2 91.7 92.0 93.1 94."
        },
        {
            "title": "Preprint",
            "content": "(a) (b) Figure 7: (a) Instruction quality distribution after data processing pipeline. (b) Performance comparison between Qwen2.5-VL-7B training with origin data and cleaned data by processing pipeline. stages, training the model to predict coordinates directly. Experimental results are depicted in Tab. 7. Compared to our method (the 4th row), removing reasoning (the first row) leads to substantial performance drop across all benchmarks, with an accuracy decrease over 10% on UI-I2E-Bench. This result confirms that the intermediate reasoning is crucial to the success of the Instruction-asReasoning framework. Instruction-as-Reasoning (IR) vs. FreeForm Reasoning (FFR). Given that reasoning is critical, what kind of reasoning is effective? Prior works (Lu et al., 2025; Yang et al., 2025; Zhou et al., 2025; Tang et al., 2025) have shown that FFR is difficult to optimize and can even degrade performance. We test this hypothesis against our IR approach in Tab. 8. As shown in the top section of the table, applying FFR degrades the performance of both UI-Tars-1.57B and Qwen2.5-VL-7B, confirming prior findings. For instance, it causes 6.4% relative drop in SS.Pro for UI-Tars-1.5-7B. In contrast, the bottom section shows that training models with our IR approach yields significant increases in accuracy. We can thus conclude from the experiments that unstructured FFR fails to improve, whereas IR is the key to unlock effective reasoning for GUI grounding. Table 8: Comparison between free-form reasoning (FFR) and Instruction as Reasoning (IR) in RL. Our Instruction-as-Reasoning is the key to unlocking effective reasoning for GUI grounding. Method Base Model SS.Pro Free-Form Reasoning (FFR) in RL RL (w/o FFR) RL (w/ FFR) RL (w/o FFR) RL (w/ FFR) UI-Tars-1.5-7B UI-Tars-1.5-7B Qwen2.5-VL-7B Qwen2.5-VL-7B 50.1 46.9 (6.4)% 36.4 36.4 (0)% Instruction-as-Reasoning (IR) in RL RL (w/o IR) RL (w/ IR) RL (w/o IR) RL (w/ IR) UI-Tars-1.5-7B UI-Tars-1.5-7B Qwen2.5-VL-7B Qwen2.5-VL-7B 48.7 51.2 (5.1%) 47.5 52.2 (9.9%) Table 9: Instruction-as-Reasoning prevents policy collapse in RL and achieves significant accuracy gain in RL. This table contrasts our method with standard SFT+RL pipeline. Scores after 100 RL steps are reported. The Hidden Benefit: Stabilizing SFT+RL. critical challenge in SFT+RL training for grounding is the policy collapse issue during RL. We compare our SFT+RL framework with standard one in this ablation. The standard SFT training provides poor policy initialization, often causing the models performance to degrade during RL, as evidenced in the upper part of Tab. 9. In contrast, our instructionas-reasoning-based SFT acts as powerful exploratory warm-up. By pre-training the model to generate diverse reasoning pathways, we empower it with strong exploratory capability, achieving significant performance increase during RL. This demonstrates that our SFT stage not only teaches the reasoning format, but also enables effective and stable policy optimization in the RL phase. Qwen2.5-VL-7B Qwen2.5-VL-7B JEDI-7B JEDI-7B 37.0 34.9 (5.7%) 39.5 34.5 (12.7%) SFT (w/o IR) + RL Zero-Shot + RL Qwen2.5-VL-7B Qwen2.5-VL-7B 37.1 46.0 (24.0%) SFT (w/ IR) + RL Base Model Method SS.Pro"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: (a) UI-Ins combine multiple reasoning pathways in each response. (b) UI-Ins can select different reasoning paths and can explore emergent reasoning perspectives after RL. (a) (b) Emergent Capabilities: Reasoning Beyond Predefined Perspectives. Does our framework merely teach the model to use the four predefined perspectives? qualitative analysis of model responses on UI-I2E reveals that it learns far deeper. We observe three key emergent capabilities: Strategic Selection: The model learns to strategically select different reasoning perspectives for different scenarios after RL. As shown in Fig. 8b and top section in Fig. 9, diverse and accurate instruction perspectives are selected. Compositional Integration: The model often combines multiple perspectives into single, cohesive reasoning, as shown in middle section in Fig. 9. All 1477 samples of UI-I2E Bench contain 5245 reasoning ways in total, as shown in Fig. 8a. This synthesis is not explicitly taught but emerges as an effective reasoning strategy during RL. Emergent Perspective: Most impressively, as shown in Fig. 8b, the model is capable of generating entirely new analytical angles beyond the four trained perspectives, such as reasoning from the perspective of group affiliation or UI element state, as demonstrated in Fig. 9. 4.6 ERROR ANALYSIS Figure 9: Different reasoning capabilities of UI-Ins. We conducted an error analysis and identified three primary types of failures in the GUI grounding performance of UI-Ins: Lack of Domain-Specific Knowledge: As shown in Fig. 10 (a), The models erroneous selection of Jazwares demonstrates failure in real-world knowledge grounding, as it lacks the external knowledge required to associate the abstract description company known for building block toys with the correct brand entity, MEGA. Lack of layout understanding ability: Illustrated in Fig. 10 (b), the model is unable to discern the correct clickable area required to fulfill the instruction, demonstrating weakness in understanding the structural layout of the user interface. Visual Ambiguity and Hallucination: As seen in Fig. 10 (c) and (d), when visually similar distractor icon is present alongside the ground-truth target, the model struggles to disambiguate between them and may select the incorrect one."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Error analysis of UI-Ins. (a): Lack of domain specific knowledge. (b): Lack of layout understanding ability. (c) and (d): Hallucination of MLLMs."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 REASONING IN GUI GROUNDING Reasoning is critical capability for MLLMs. However, for GUI grounding task, enabling the model to perform Free-Form reasoning (FFR) during the RL stage does not improve performance and may even degrade it, as demonstrated by GUI-G1 (Zhou et al., 2025), GTA1 (Yang et al., 2025), GUIG2 (Tang et al., 2025), UI-R1 (Lu et al., 2025), and our own experiments in Sec. 4.5. Although some works, such as InfiGUI-G1 (Liu et al., 2025d), InfiGUI-R1 (Liu et al., 2025c), and GUI-R1 (Luo et al., 2025), have utilized the Free-Form Reasoning, they did not provide ablation experiments to investigate its effectiveness. Furthermore, GUI-R1 found that the model performance is improved as the reward weight for the thinking format was decreased. Nevertheless, the failure of freeform reasoning does not imply that reasoning is ineffective for GUI grounding. Our Instructionas-Reasoning method instills strong exploratory capabilities by training the model with diverse and effective reasoning pathways during the SFT stage. Consequently, the model generates more diverse rollouts during RL, effectively mitigating policy collapse. 5.2 INSTRUCTION IN GUI GROUNDING Comprehending the user instruction is critical for achieving success in GUI grounding. Prior works, such as Aria-UI (Yang et al., 2024b) and Phi-Ground (Zhang et al., 2025), have primarily focused on augmenting instructions at the input level, using advanced MLLMs to paraphrase them into varying styles. Yet, this methodology suffers from critical limitations: (1) it treats instructions merely as static inputs rather than dynamic reasoning pathways; (2) it lacks deep analysis of the impact of instruction on grounding; (3) it fails to demonstrate significant, consistent performance gains. Differing from these approaches, our work provides an in-depth investigation of how the diversity and quality of instructions can affect model performance. Moreover, our Instruction-as-Reasoning not only enhances the diversity of instructions but also innovatively repurposes these diverse instructions as learnable reasoning pathways for the model, leading to substantial performance improvements."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Reasoning from diverse instruction perspectives enables UI-Ins-7B to succeed on ambiguous grounding tasks. This qualitative comparison with GTA1-7B showcases how our Instruction-as-Reasoning process is key to resolving challenging cases where other models fail. 5.3 TRAINING PARADIGM IN GROUNDING Prior GUI grounding methods mainly focus on training in Supervised Fine-Tuning (SFT) paradigm, such as JEDI (Xie et al., 2025), OS-Atlas (Wu et al., 2024b), Aguvis (Xu et al., 2025), Uground (Gou et al., 2025) and Aria-UI (Yang et al., 2024b). Reinforcement learning methods, particularly GRPO (Guo et al., 2025) have demonstrated remarkable sucess on various visual-language tasks, including Semantic Segmentation (Liu et al., 2025e), Visual Question-Answering (Liu et al., 2025f; Huang et al., 2025) and Temporal Video Grounding (Wang et al., 2025c). Consequently, recent efforts have increasingly focused on adapting RL for GUI grounding. GUI Grounding methods like GUI-R1 (Luo et al., 2025), GUI-Actor (Wu et al., 2025) and GTA1 (Yang et al., 2025) play as an pioneer role in pure RL paradigm and surpass SFT-based methods by large margin. However, key limitation of pure RL paradigm is that it overlooks the substantial benefit offered by an initial SFT stage. While InfiGUI-R1 (Liu et al., 2025c) achieved success with an SFT+RL framework by reframing GUI grounding as trajectory-level task that encourages model reflection, the SFT+RL paradigm remains notoriously difficult to implement in practice, which is also demonstrated by PhiGround (Zhang et al., 2025) and our experimental findings in Sec. 4.5, SFT+RL framework is prone to policy collapse issues. Our Instruction-as-Reasoning method addresses this gap by leveraging SFT to teach model with broader world knowledge and reasoning format demonstrations, and then utilize the RL stage to further incentivize the model to select the best reasoning pathway, establishing successful example for the SFT+RL training paradigm."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we conducted systematic investigation into the natural language instruction of GUI grounding, critical yet underexplored issue. Through deep analysis of existing grounding datasets, we find 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to substantial 76% relative performance improvement. Building upon this, we proposed Instruction-as-Reasoning, novel SFT+RL framework designed to explicitly leverage instructional diversity by treating different perspectives as distinct reasoning pathways. Our resulting models, UI-Ins-7B and UI-Ins-32B, establish new state of the art across five benchmarks. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UII2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving 74.1% success rate on AndroidWorld using UIIns-7B as the executor. Our in-depth analysis further reveals helpful insights for GUI grounding."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. 3.7 claude-3-7-sonnet, 2024. Accessed: 2025-08-02. sonnet."
        },
        {
            "title": "Claude",
            "content": "https://www.anthropic.com/news/ Anthropic. Our 3.5 models and computer use. https://www.anthropic.com/news/ 3-5-models-and-computer-use, sep 2024. Accessed: 2025-09-22. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Guozhi Wang, Dingyu Zhang, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 21382156. Association for Computational Linguistics, 2025. doi: 10.18653/v1/2025.findings-acl.110. URL http://dx.doi.org/10.18653/v1/2025.findings-acl.110. China Mobile. JT-GUIAgent-V1: Planner-Grounder Agent for Reliable GUI Interaction. Project Website, 2025. URL https://jt-guiagent.github.io/JT_guiagent.github. io/. DeepMind. Developing computer use model. Google Blog, Oct 2025. URL https://blog. google/technology/google-deepmind/gemini-computer-use-model/. Accessed: October 22, 2025. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=kxnoqaisCT. Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, and Weiqiang Wang. Ui-venus technical report: Building high-performance ui agents with rft, 2025. URL https://arxiv.org/abs/2508.10833. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents, 2024. URL https://arxiv.org/abs/2312. 08914. ZongHan Hsieh, Tzer-Jen Wei, and ShengJing Yang. Zonui-3b: lightweight vision-language model for cross-resolution gui grounding. https://arxiv.org/abs/2506.23491, 2025. arXiv:2506.23491 [cs.CV], version 2, last revised 1 Jul 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. URL https://arxiv.org/abs/2503.06749. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web, 2024. Kaixin Li, Meng Ziyang, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: GUI grounding for professional high-resolution computer use. In Workshop on Reasoning and Planning for Large Language Models, 2025. URL https: //openreview.net/forum?id=XaKNDIAHas."
        },
        {
            "title": "Preprint",
            "content": "Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents, 2024. URL https:// arxiv.org/abs/2406.03679. Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements, 2020. URL https: //arxiv.org/abs/2010.04295. Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inferencetime grounding, 2025. URL https://arxiv.org/abs/2507.22025. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent, 2024. URL https://arxiv.org/abs/2411.17465. Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, and Yan Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis, 2025a. URL https://arxiv.org/abs/2504.11257. Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, and Fei Wu. Infiguiagent: multimodal generalist gui agent with native reasoning and reflection. arXiv preprint arXiv:2501.04575, 2025b. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners, 2025c. URL https://arxiv.org/abs/2504.14239. Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Infigui-g1: Advancing gui grounding with Xiaotian Han, Jianbo Yuan, Xinyao Wang, et al. adaptive exploration policy optimization. arXiv preprint arXiv:2508.05731, 2025d. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. SegarXiv preprint zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv:2503.06520, 2025e. Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025f. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. URL https://arxiv.org/abs/2408.00203. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. OpenAI. Developing generalist computer-using agent. OpenAI, 2025. URL https://openai. com/index/computer-using-agent/. Accessed: October 22, 2025. OpenAI. Gpt-4.1 announcement. https://openai.com/index/gpt-4-1/, 2025a. Accessed: 2025-08-03. OpenAI. Gpt-5.1 model overview. https://openai.com/gpt-5, 2025b. Internal model release; no peer-reviewed technical report available at the time of writing. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025."
        },
        {
            "title": "Preprint",
            "content": "Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2024. URL https://arxiv.org/abs/2405. 14573. ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv. org/abs/2402.03300, 2(3):5, 2024. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Gui-g2: Gaussian reward modeling for gui grounding, 2025. URL https://arxiv.org/abs/2507.15846. General Agents Team. The showdown computer control evaluation suite, 2025. URL https: //github.com/generalagents/showdown. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025a. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025b. URL https://arxiv.org/abs/2508.09123. Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, Xiangnan Fang, Zewen He, Zhenbo Luo, Wenxuan Wang, Junqi Lin, Jian Luan, and Qin Jin. Time-r1: Post-training large vision language model for temporal video grounding, 2025c. URL https://arxiv.org/abs/2503.13377. Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual gui agent towards general computer control. arXiv preprint arXiv:2412.01268, 2024a. Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual gui agent towards general computer control, 2024b. URL https://arxiv.org/abs/2412. 01268. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: foundation action model for generalist gui agents, 2024a. URL https://arxiv.org/abs/2410.23218. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505.13227. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2025. URL https://arxiv.org/abs/2412.04454."
        },
        {
            "title": "Preprint",
            "content": "JingJing Xie Xuehui Wang, Zhenyu Wu et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025. URL https://arxiv.org/abs/2507.05791. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Ariaui: Visual grounding for gui instructions, 2024b. URL https://arxiv.org/abs/2412. 16256. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via selfevolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, et al. Phi-ground tech report: Advancing perception in gui grounding. arXiv preprint arXiv:2507.23779, 2025. Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. 2024. URL https://arxiv.org/abs/2406. 18532. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents, 2025. URL https: //arxiv.org/abs/2505.15810. Jinguo Zhu, Weiyun Wang, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "A DATA PIPELINE DETAILS",
            "content": "A."
        },
        {
            "title": "INSTRUCTION DIVERSITY AUGMENTATION",
            "content": "To enhance instructional diversity, we expanded the instruction set based on frequently occurring scenarios, categorizing them into four types: appearance-based, function-based, spatial-based, and intent-based. When leveraging GPT-4.1 to augment instructions from open-source datasets, we mitigated potential hallucinations arising from poor-quality original instructions. To achieve this, we visually grounded the process by overlaying the ground-truth point or bounding box as distinct circular or rectangular marker on the input image."
        },
        {
            "title": "Instruction Diversity Augumentation Prompt",
            "content": "## Task: Generate and Translate Unambiguous Grounding Instructions ## Input: GUI Screenshot: An image of user interface. Original Instruction: An initial English instruction. Highlighted Element: visual marker e.g., red <annotation type> on the screenshot pointing to the target UI element. CORE OBJECTIVE Your primary task is to first translate the Original Instruction into high-quality Chinese, and then generate four new, distinct types of grounding instructions. For all generated instructions, you must adhere to this critical rule: the instruction must correspond to one and only one element on the entire screenthe one highlighted. Clarity and uniqueness are the top priorities. IMPORTANT SAFEGUARD The <annotation type> is ground-truth annotation provided only for your reference. Your instructions must never refer to the annotation itself. It is noticeable that the original instruction may can not align with the ground-truth annotation, you should follow the ground-truth annotation first. ## Instructions Generation Requirements: Generate one new, clear, and unambiguous instruction for each of the following four categories. Appearance-Based: direct and literal description of the elements visual characteristics (e.g., its text, icon, color, shape). Combine features as needed to ensure the description is completely unique. Function-Based: clear description of the elements purpose or the immediate outcome of interacting with it (e.g., the button used to confirm and save your profile changes). Spatial-Based: An instruction that identifies the element based on its position relative to other prominent, easily identifiable UI elements (landmarks). The described spatial relationship must lead to unique location. Goal-Based: concise phrase that describes the users ultimate goal or intent. The user must infer which single UI element on the screen fulfills this goal. ## Output Format: The final output must be single, well-formed JSON object. The JSON structure should begin with the original instruction and its translation, followed by the newly generated instructions. Now, please process the following inputs and generate the instructions in the specified JSON format. Original Instruction: <instruction here>"
        },
        {
            "title": "Preprint",
            "content": "A."
        },
        {
            "title": "INSTRUCTION QUALITY REFINEMENT",
            "content": "To verify and filter the quality of both the original and the newly generated diverse instructions, we prompted GPT-4.1 to assess whether each instruction uniquely corresponded to single element in the GUI screenshot. To mitigate potential model hallucinations during this verification process, we visually grounded the task by overlaying the ground-truth annotation directly onto the input image."
        },
        {
            "title": "Prompt for Instruction Refinement",
            "content": "## Task: Quality Evaluation of GUI Grounding Datum ## Role: You are meticulous Data Quality Analyst specializing in user interface datasets. Your task is to critically evaluate given data sample for its quality and correctness in structured, two-step process. ## Input: GUI Screenshot: An image of user interface. Grounding Instruction: An English command intended to guide user to specific element. Ground-Truth Bounding Box: red box drawn on the screenshot, highlighting the target UI element. -IMPORTANTGround-Truth Point: blue hollow circle drawn on the center of the Ground-Truth Bounding Box, which is the key to help you locate the target UI element, because screenshots usually have other red bboxes which may cause distribution. ## Output Process (Two Steps): ### Step 1: Chain-of-Thought Reasoning First, you must articulate your reasoning process in plain text. Analyze the input and think step-by-step. Your reasoning should cover the following points: Instruction Analysis: What specific element does the instruction describe? Identify its key features (text, function, location, etc.), it is important you should locate the target UI element according to the blue hollow circle and the red bbox. Scan the entire screenshot. Are there any other elements that could match this description, even partially? Conclude whether the instruction is unique or ambiguous based on this scan. Bounding Box Analysis: What is the target element identified by the instruction? Does it have the blue hollow circle in the center of the box? Does the red box tightly enclose this entire target element? Does the box cut off any part of the element? Does the box include significant empty space or other unrelated elements? Conclude whether the bounding box is appropriately sized, too large, or too small. ### Step 2: Final JSON Output After you have completed your reasoning, provide the final answer as single, well-formed JSON object. This JSON should be the very last part of your response. Do not add any text after the JSON object. { instruction evaluation: { reasoning: <A concise summary of your reasoning from Step 1 about the instructions uniqueness.> is unique: <true or false>, }, bbox evaluation: { reasoning: <A concise summary of your reasoning from Step 1 about the bounding box size.>, is appropriately sized: <true or false> }"
        },
        {
            "title": "Preprint",
            "content": "} Now, please perform this two-step evaluation for the following data. Grounding Instruction: <instruction here>"
        },
        {
            "title": "B EXPERIMENT PROMPTS",
            "content": "B.1 SFT TRAINING EXAMPLE We provide SFT training example as following, we mark the Instruction as Reasoning in red."
        },
        {
            "title": "SFT Training Example",
            "content": "# System Prompt: You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. ## Output Format Return json object with function name and arguments within <tool call></tool call> XML tags: <think> ... <think> <tool call> {name: grounding, arguments: <args-json-object>} <tool call> <args-json-object> represents the following item of the action space: ## Action Space {action: click, coordinate: [x, y]} Your task is to accurately locate UI element based on the users instructions. The screenshot resolution is height {height} and width {width}. First, you should carefully examine the screenshot and analyze the users instructions from one of the following different perspectives: Appearance Perspective: You should prioritize visually scanning the user interface to find an element that precisely matches the label, text color, background, shape, and its relative position to other elements as described in the appearance-based instruction. Your decisionmaking will be driven by direct pattern matching of these visual attributes. Function Perspective: You should identify which UI element is responsible for achieving the specified system state change or executing the described action, such as switching the media display mode. Your decision-making will be guided by understanding the intended effect or utility of interacting with the element. Spatial Perspective: You should first locate any specified reference points (e.g., another UI element or screen edge) and then precisely pinpoint the target element based on its exact directional and positional relationship within the UI layout. Your decision-making will be based on geometric and relational positioning within the interface. Goal Perspective: You should identify the UI element or sequence of actions that directly leads to the fulfillment of the users stated ultimate objective. Your decision-making will be driven by finding the most direct and efficient path to achieve the desired end-state for the user. You should translate the users instruction into the corresponding instruction, and then provide the final action. # User Instruction: Click on the CSDN bookmark in the bookmarks bar to access the CSDN website. # Assistant:"
        },
        {
            "title": "Preprint",
            "content": "<think> will analyze this instruction from Appearance-Based perspective, the users instruction can be represented as : Click the bookmark with the red icon and the label CSDN in the bookmarks bar. </think> <tool call> {name:grounding,arguments:{action:click,coordinate:[588,67]}} </tool call> B.2 RL TRAINING EXAMPLE We provide RL training example as following."
        },
        {
            "title": "RL Training Example",
            "content": "# System Prompt: You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. ## Output Format Return json object with function name and arguments within <tool call></tool call> XML tags: <think> ... <think> <tool call> {name: grounding, arguments: <args-json-object>} <tool call> <args-json-object> represents the following item of the action space: ## Action Space {action: click, coordinate: [x, y]} Your task is to accurately locate UI element based on the users instructions. The screenshot resolution is height {height} and width {width}. First, you should carefully examine the screenshot and analyze the users instructions in <think>...<think> tags and then output the coordinate. # User Instruction: Click on the CSDN bookmark in the bookmarks bar to access the CSDN website. # Assistant: <think> ... </think> <tool call> ... </tool call> B.3 ONLINE BENCHMARK EVALUATION For the evaluation of the AndroidWorld benchmark, we develop simple yet effective agent framework to evaluate the grounding capability of our model in the online environment. Our framework consists of two main agents, planner (i.e., GPT-5), which serves as the high-level controller to decide the executed action in each step, and an executor (i.e., UI-Ins 7B) that identifies the precise coordinates based on each instruction from the planner. Specifically, during each step, the planner receives the task goal, historical records of thinking and actions in previous steps, and the current screenshot, and produces the next reasoning trace and corresponding structured JSON action that conforms to our pre-defined action space. When the predicted action type is \"click\" or \"long-press\", the JSON instruction is forwarded to our grounding executor. Then our executor interprets the textual description of the target element (e.g., blue circle button at top-right) and out-"
        },
        {
            "title": "Preprint",
            "content": "puts precise screen coordinates, which then performs the \"click\" or \"long-press\" operation on the Android device. The resulting screen update and execution feedback are sent back to the planner, enabling it to iteratively refine its decisions and complete the task through perception-action loop. We provide our detailed system prompt as follows: Our System Prompt for AndroidWorld Online Benchmark Evaluation ## Task. Control an Android phone to answer user queries and execute tasks with precise, verifiable actions. ## Role. Android Phone Operator AI. Responsibilities: Retrieve information from the device to answer user questions. Perform tasks by executing precise UI actions. ## Action Framework. Respond with EXACT JSON for one of the following actions: Action open app click Description JSON Example Open app from <Available Apps> Tap visible element long press Long-press visible element input text Type into field answer Respond to user navigate home navigate back scroll status wait Return to home screen Navigate back Scroll up/down/left/right Mark task status Wait for screen update ## Execution Principles. 1. Communication Rule { \"action type\":\"open app\", \"app name\":\"Chrome\" } { \"action type\":\"click\", \"target\":\"blue circle button at top-right\" } { \"action type\":\"long press\", \"target\":\"message from John\" } { \"action type\":\"input text\", \"text\":\"Hello\", \"target\":\"message input box\" } { \"action type\":\"answer\", \"text\":\"Its 25 degrees today.\" } { \"action type\":\"navigate home\" } { \"action type\":\"navigate back\" } { \"action type\":\"scroll\", \"direction\":\"down\" } { \"action type\":\"status\", \"status\":\"complete\" } { \"action type\":\"wait\" } Always use answer to reply to users; do not assume on-screen text is sufficient. Follow the user instruction strictly (e.g., single number, True/False, comma-separated items). Do not use answer for waiting or loading; use wait. 2. Efficiency First Choose the simplest valid path. If an action fails twice, try an alternative (e.g., long press instead of click). 3. Smart Navigation Prefer open app with the available app list over manual navigation. Gather information when needed (e.g., open Calendar to check schedule). For scrolling: direction is inverse to swipe; if scroll fails, try the opposite direction. 4. Text Operations Activate the input box before typing. Prefer input text over manual typing. For manipulation: long-press to select, use selection bar (Copy/Paste/Select All), delete by selecting then cutting. ## Current Context. User Goal: {goal} Previous Actions: {history} Available Apps: [\"Camera\",\"Chrome\",\"Clock\",\"Contacts\",\"Dialer\",\"Files\",\"Settings\", \"Markor\",\"Tasks\",\"Simple Draw Pro\",\"Simple Gallery Pro\",\"Simple SMS Messenger\",\"Audio Recorder\",\"Pro Expense\",\"Broccoli APP\",\"OSMand\",\"VLC\",\"Joplin\",\"Retro Music\",\"OpenTracks\",\"Simple Calendar Pro\"] ## Decision Process. 1. Analyze goal, history, and current screen. 2. Determine if the task is complete; output status if true. If not complete, choose the most appropriate single action. 3. 4. Output in the exact format below, ensuring the action is valid JSON. ## Output Format. Thought: Action: { \"action type\":\"open app\", \"app name\":\"Chrome\" } need to open the Chrome app to search for the information..."
        },
        {
            "title": "C QUALITATIVE RESULTS",
            "content": "C.1 REASONING PERSPECTIVE ANALYSIS We performed detailed classification of the models reasoning process by first manually defining ten distinct analytical perspectives. We then utilized GPT-4.1 to examine 1477 responses generated by UI-Ins-7B on the whole UI-I2E benchmark based on the taxonomy as following:"
        },
        {
            "title": "Taxonomy of Reasoning Perspectives",
            "content": "1. Appearance Abbreviation: app Definition: Describes the static visual properties of UI element, including its color, shape, icon, style, and the literal text it displays. 2. Functionality Abbreviation: func Definition: Describes the elements purpose, its action, or what happens when user interacts with it. 3. Location Abbreviation: loc Definition: Describes the elements spatial position on the screen or in the viewport, which can be absolute (e.g., top-left) or relative to other elements (e.g., below the title). 4. Intent Abbreviation: intent Definition: Describes the high-level user goal or plan that motivates the entire action. It is often the starting point of reasoning chain. 5. Structural Relationship Abbreviation: struct Definition: Describes the elements position within the UIs layout hierarchy (like DOM tree), emphasizing its parent, child, or sibling relationship to other elements or containers. 6. State Abbreviation: state Definition: Describes the current dynamic condition of an element, such as whether it is interactive, active, selected, disabled, or checked. 7. Component Type Abbreviation: ctype Definition: rather than just describing its appearance. 8. Sequential Position Abbreviation: seq Definition: Describes the elements order or temporal place within multi-step user task or workflow. 9. Salience Abbreviation: salience Definition: Describes the elements degree of visual prominence, which is often determined by its size, contrast, unique styling, or animation. Identifies the element as standard, reusable design pattern or component, 10. Accessibility Abbreviation: a11y Definition: Describes non-visual properties provided for assistive technologies, such as screen readers. This includes ARIA labels, roles, and other accessibility attributes. C.2 QUALITATIVE EXAMPLE Here we present the grounding results of UI-Ins-32B across various platforms and software applications. As shown in Fig. 12, UI-Ins-32B demonstrates robust performance on diverse platforms."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Success Examples of UI-Ins-32B"
        }
    ],
    "affiliations": [
        "CUHK",
        "Renmin University of China",
        "Tongyi Lab, Alibaba Group"
    ]
}