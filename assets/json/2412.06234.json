{
    "paper_title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "authors": [
        "Seungtae Nam",
        "Xiangyu Sun",
        "Gyeongjin Kang",
        "Younggeun Lee",
        "Seungjun Oh",
        "Eunbyung Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propose Generative Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in a single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental results on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-of-the-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 4 3 2 6 0 . 2 1 4 2 : r Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction Seungtae Nam* Xiangyu Sun* Gyeongjin Kang Younggeun Lee Seungjun Oh Eunbyung Park"
        },
        {
            "title": "Sungkyunkwan University",
            "content": "https://stnamjef.github.io/GenerativeDensification/ Figure 1. Our method selectively densifies (a) coarse Gaussians from generalized feed-forward models. (c) The top Gaussians with large view-space positional gradients are selected, and (d-e) their fine Gaussians are generated in each densification layer. (g) The final Gaussians are obtained by combining (b) the remaining (non-selected) Gaussians with (f) the union of each layers output Gaussians."
        },
        {
            "title": "Abstract",
            "content": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent highfrequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for genIn this paper, we propose Generaeralized scenarios. tive Densification, an efficient and generalizable method to densify Gaussians generated by feed-forward models. Unlike the 3D-GS densification strategy, which iteratively splits and clones raw Gaussian parameters, our method up-samples feature representations from the feed-forward models and generates their corresponding fine Gaussians in single forward pass, leveraging the embedded prior knowledge for enhanced generalization. Experimental re- *Equal contribution Corresponding author sults on both object-level and scene-level reconstruction tasks demonstrate that our method outperforms state-ofthe-art approaches with comparable or smaller model sizes, achieving notable improvements in representing fine details. 1. Introduction 3D Gaussian splatting (3D-GS) [13] has been massive success in high-quality 3D scene reconstruction and realtime novel view synthesis, representing 3D scene with set of learnable Gaussian primitives and exploiting fast differentiable rasterization pipeline. Since its introduction, numerous studies have explored 3D-GS as versatile 3D representation with various applications extending beyond the per-scene optimization. One notable example is generalized feed-forward Gaussian models [3, 5, 7, 29, 30, 37, 41, 42, 45], which generate 3D Gaussian primitives to reconstruct 3D scenes or objects in single forward pass. By leveraging 3D prior knowledge learned from large multiview datasets (e.g., Objaverse [8, 9], RE10K [46]), these models can reconstruct the 3D scene or objects with the generated 3D Gaussians from only single or few images. Figure 2. Generative Densification overview. We selectively densifies the top Gaussians with large view-space positional gradients. While effective and promising, the feed-forward Gaussian models often face challenges in capturing highfrequency details primarily due to the limited number of Gaussians and the lack of densification strategy tailored for these approaches. One possible solution is to generate more Gaussians, but this does not fully address the problem. For example, in the pixel-aligned Gaussian model [3], widely adopted architecture in the feed-forward models (see Sec. 2 for details), we can predict multiple Gaussians per pixel. However, this strategy uniformly increases the number of Gaussians across the entire 3D space, resulting in an excess of unnecessary small Gaussians that may degrade rendering quality and speed. Ideally, the scene details should be represented by numerous small Gaussians, while smoother regions can be covered by few large Gaussians. In per-scene 3D-GS optimization, an adaptive densification strategy selectively increases the number of Gaussians to better reconstruct complex 3D geometry and fine details. Specifically, every few optimization steps, it densifies only the Gaussians with large view-space positional gradients, replicating the existing Gaussians or splitting them into smaller ones when more Gaussians are required. Through repeated optimization and densification processes, the Gaussians gradually become non-uniformly distributed, with numerous small Gaussians concentrated in detailed areas and few large Gaussians scattered across smooth regions. This simple heuristic algorithm has demonstrated its effectiveness across various real-world scenes and objects. straightforward approach to extend the densification strategy to the feed-forward Gaussian models is to fine-tune the output Gaussians from the feed-forward models using the original 3D-GS optimization and densification procedure. However, directly applying the densification strategy used in per-scene optimization scenarios presents several challenges. For example, it demands hundreds and thousands of optimization steps to converge and reconstruct the fine details, which significantly reduces generation speed. Additionally, the per-scene 3D-GS optimization usually assumes many input images from various viewpoints, whereas the feed-forward models typically take just few views or even single view image. Thus, the densification and extensive optimization to few view images can easily lead to overfitting, compromising the feed-forward models ability to generalize and synthesize novel views effectively. In this paper, we propose Generative Densification (GD), an efficient generative densification strategy for generalized feed-forward Gaussian models, which is designed in accordance with the following principles: 1) adaptability, allowing the model to distinguish Gaussians that requires densification and those do not; and 2) generalizability, enabling the model to learn and leverage prior knowledge from large multi-view datasets. Inspired by the 3D-GS densification strategy, our approach leverages view-space positional gradients to identify where additional Gaussians are needed. We selectively densify the top Gaussians with large gradients, and the remaining (non-selected) Gaussians are used together with the densified ones to render high-fidelity images  (Fig. 2)  . Unlike the 3D-GS densification strategy, the proposed GD densifies the selected Gaussians based on the learned prior knowledge for the purpose of generalizable 3D reconstruction. More specifically, GD densifies the feature representations from the feed-forward models rather than the actual Gaussians, and the prior knowledge is embedded in these features for better generalization. We further propose to utilize an efficient point-level transformer [31] to implement GD. Due to the quadratic increase in memory and the unstructured nature of point-level 3D data, it is infeasible to apply naive self-attention on hundreds of thousands of input Gaussians. While local attention can be applied within groups of neighboring Gaussians, finding the neighbors by calculating and comparing distances between the Gaussians is computationally expensive. Instead, we sort the Gaussians in traversing order of spacefilling curves [22] to rearrange them into non-overlapping groups and apply group-wise attention, enabling efficient operations directly in 3D space. Additionally, while we se2 lectively densify the Gaussians with large view-space positional gradients, multiple rounds of densification are unnecessary for all of them. To optimize this process, we predict confidence mask for each densification layer to filter out the Gaussians that do not require further densification. We integrated GD into two recent feed-forward Gaussian models: LaRa [5] for object-level reconstruction and MVSplat [7] for scene-level reconstruction. On the largescale Gobjaverse [33] and RE10K [46] datasets, two models incorporating our method achieved the best performance, significantly improving reconstruction quality compared to their respective baselines. Qualitative analysis further highlights that the fine Gaussians generated by GD effectively capture thin structures and intricate details, which are often challenging for the coarse Gaussians to represent accurately  (Fig. 1)  . Additionally, cross-dataset evaluations confirm that GD consistently improves image quality, demonstrating its robust generalizability across different datasets. 2. Related Work Feed-forward Gaussian Models. Feed-forward Gaussian models learn mapping from single or set of few input images to set Gaussian representations that can be rendered into any viewpoint. One common approach combines pre-trained image encoder (e.g., DINO [2, 21]) with learnable embedding decoder, where the decoder generates explicit 3D features via series of cross-attention between 3D embeddings and image features. These 3D features are commonly represented as voxels (LaRa [5] and Geo-LRM [41]), point-cloud (Point-to-Gaussian[20]), or hybrid of point-cloud and triplane (Triplane Gaussian Splatting [47]), and the Gaussian parameters can be obtained by decoding the features using an MLP. Covering wide range of view with an explicit 3D representation, this approach is effective for 360-degree object-level reconstruction. However, the cross-attention on the 3D features is memory-intensive and computationally expensive. Another approach involves pixel-aligned Gaussian representations [3, 29], where each Gaussian is assumed to be on ray of each pixel and distanced from the ray origin by depth. The depth is either directly regressed from pixelwise image features [29] or determined by the probability that Gaussian exists at depth along ray [3]. Since its introduction, substantial efforts have been made to estimate the depth more accurately and extend it to large models for better generalizability. MVSplat [7] and MVSGaussian [18] propose to utilize cost volumes to leverage crossview feature similarities for depth estimation, and, more recently, Flash3D [28] and DepthSplat [35] further use pretrained depth estimator, enabling more robust estimation. LGM [30] and other works [37, 42] extend the pixel-aligned Gaussians to the large models by attaching Gaussian head layer on the top of 2D U-Net or ViT. Trained on large multiview datasets, these models can generate the Gaussians even when out-of-domain input images (e.g., generated images from text-to-3D models [26]) are given. While both of these approaches can effectively generate Gaussian representations from single or few input images, they often struggle to reconstruct fine details. This is primarily due to the limited number of Gaussians to represent the details and the lack of densification strategy to refine the Gaussians generated by the feed-forward models. Adaptive Density Control of Gaussians. The adaptive densification strategy [13] plays crucial role in filling empty areas and capturing fine details. It involves calculating the norm of view-space positional gradients averaged across different views, with only Gaussians that have large norms being selected for densification. While effective, the algorithm depends on having good initial Gaussian positions, and large Gaussians are often not split into smaller ones, leading to blurry images in the final renderings. MCMC-GS [14] proposes to optimize the Gaussians with Stochastic Gradient Langevin Dynamics update, improving the robustness to initialization. To address the issue of large Gaussians, AbsGS [38] introduces homodirectional view-space gradients as criterion of densification, while other methods [1, 44] consider the number of pixels covered by each Gaussian when calculating the gradients. Although the methods outlined above successfully improve the rendering quality, they all require more than thousands of optimization and densification steps, and the discussions are limited to per-scene optimization scenarios. Our goal is to develop an efficient densification method tailored for generalized feed-forward Gaussian models. Instead of directly applying the existing methods in generalized settings, we learn from large multi-view datasets to generate fine Gaussians in single forward pass. We show that our method can be generalized to variety of objects and scenes by leveraging the learned prior knowledge. 3. Method 3.1. Generative Densification feed-forward Gaussian model is function Φ that maps set of images = {Iv}V to set of Gaussians and features, v=1 and camera poses = {Cv}V v=1 G(0), (0) = Φ(I, C), (1) where is the number of input views, and (0) is the features that are used to generate the Gaussians (see Sec. 3.3). Each Gaussian consists of positions, an opacity, spherical harmonics (SH) coefficients, quaternions, and scales. Our goal is to learn densification model Ψ that can adaptively densify Gaussians generated by feed-forward models for high-fidelity 3D reconstruction: ˆG = Ψ(G(0), (0), I, C). (2) 3 First, using the view-space positional gradients, we split the Gaussians into two groups: the Gaussians requiring densification (G(0) den ) and the remaining Gaussians (G(0) rem). Specifically, we compute scores as the norms of gradients with respect to the view-space (or projected) coordinates of Gaussian positions, averaged across the input views: m(0) ="
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) v=1 p(x(0) ,v)LMSE(Iv, ˆIv)2, (3) where p(x(0) , v) R2 denotes the projected coordinate of i-th Gaussian position x(0) R3 onto input view v, and LMSE is the mean squared error between ground truth input images (Iv) and the rendered images ( ˆIv). Based on the computed scores, the top (0) scoring Gaussians (G(0) den ) are selected for densification. the positions and features (X (0) den RK(0)3, Then, (0) den RK(0)C) of the selected Gaussians are passed to the densification module  (Fig. 3)  , which consists of upsampling (UP), splitting via learnable masking (SPLIT), and Gaussian head (HEAD) components: (X (l) den, (l) den, (l) rem, (l) (X (l), (l)) = UP(X (l1) , (l1) ), den den rem) = SPLIT(X (l), (l)), G(l) = HEAD(X (l) rem), rem, (l) (4) (5) (6) den + (l) rem 3 RK(l) den RK(l) rem (K (l) = (l) for {1, , L1}. UP(, ) : RK(l1)3 RK(l1)C RK(l)3 RK(l)C, up-samples the Gaussian positions and features. SPLIT(, ) : RK(l)3 RK(l)C RK(l) den 3 RK(l) rem), divides the up-sampled positions and features into those reden, (l) quiring further densification in the next layer (X (l) den) and the remaining ones (X (l) rem). More details on the number of Gaussians after the up-sampling and splitting operations are provided in Appendix C. Using the remaining Gaussian positions and features, fine Gaussians for each layer (G(l)) are generated via the HEAD(, ) module. Finally, the L-th layers fine Gaussians are generated as , (L1) G(L) = HEAD(UP(X (L1) )), and the final set of den Gaussians is obtained as follows: rem, (l) den ˆG = G(0) rem { (cid:91) l=1 G(l)}. (7) 3.2. Architecture Details Serialized Attention. Due to the quadratic increase in memory requirements and the unstructured nature of the it is infeasible to apply selfGaussian representations, attention directly to all input Gaussians. One alternative is to group the Gaussians by searching for their neighbors Figure 3. Key components in Generative Densification Module. and apply group-wise attention. While this approach allows the model to consider the relative positions of the neighbors, searching for neighbors across hundreds of thousands of Gaussians is memory and computationally expensive. Serialized attention [31] enables efficient operations on unstructured point clouds by sorting them in traversing order of given space-filling curves [22]. Following its design principles, we first encode each Gaussian position into serialized code, an integer value that reflects its order in the space-filling curves [31]. Then, the Gaussian features are structured by sorting the serialized codes and divided into non-overlapping groups, where self-attention is applied within the same group. By adapting serialized attention, our method efficiently embeds prior knowledge and local scene context into the features, allowing the output features to be used as sources for generating fine Gaussians. In the UP module, we predict residuals for Up-sampling. each Gaussian position and feature  (Fig. 3)  . We generate R(l) offsets for each Gaussian position by passing the input features through an MLP parameterized by θ(l) . These predicted offsets are then positionally encoded, concatenated with the input features, and transformed to residual features using another MLP parameterized by θ(l) . More formally, x(l) (l) = MLP(f (l1) i,j = MLP(γ(x(l) ; θ(l) ), i,j) (l1) ; θ(l) ), (8) (9) where (l1) the serialized attention module. x(l) RC is the feature of i-th Gaussian after RR(l)3 is the Figure 4. Overview of the Generative Densification pipelines for object-level (top) and scene-level (bottom) reconstruction tasks. predicted R(l) offsets per Gaussian, and x(l) i,j R3 denotes each predicted offset. Similarly, (l) RR(l)C is the predicted R(l) residuals per Gaussian feature, and (l) i,j RC denotes each predicted residual. γ() is positional encoding function, and represents concatenation operator. The output positions and features are obtained by adding the residuals to their respective inputs. In the SPLIT modSplitting via Learnable Masking. ule, we introduce learnable masking to filter out Gaussians not requiring further densification. Although we use gradient masking to identify the Gaussians that need refinement in the first layer, calculating gradients at every layer incurs considerable computational overhead. Instead, we predict confidence score for each Gaussian using an MLP parameterized by θ(l) and sigmoid function σ(). More formally, m(l) = σ(MLP(f (l) ; θ(l) )), (10) where (l) RC is the feature for i-th Gaussian. Based on these predicted scores, the top (l) den scoring Gaussians are selected for further densification, while the remaining (l) rem Gaussians are decoded into fine Gaussian parameters. Since selecting the (l) den Gaussians is non-differentiable, [0, 1] to input we attach computational graph of m(l) features (l) while keeping the feature values intact: m(l) = sg(f (l) (l) ) + (l) (l) m(l) , (11) where sg() is stop-gradient operator. Analogous to the straight-through estimator [39], the gradients with respect to the output features L/f (l) propagate back to the second term of the Eq. (13), hence, which enables us to learn the masks and do end-to-end training. Gaussian Head. tions and features of the remaining Gaussians (X (l) In the Gaussian HEAD module, the posirem), rem, (l) filtered out by the learnable masking module, are decoded into fine Gaussians (G(l)). The positions of the fine Gaussians are set to be the same as the input positions, while the remaining attributes (opacities, SH coefficients, quaternions, and scales) are generated from the input features using an MLP. Sigmoid and exponential functions are applied to the MLP outputs for opacities and scales, respectively, and the quaternions are normalized following [13]. Global Adaptive Normalization. The serialized attention module is learned to aggregate the scene context but operates within each group of Gaussians for memory and computational efficiency, which may lead to limited understanding of the global context. To complement the local features, global feature is widely used as global descriptor in point-level architectures. Inspired by previous works [24, 32] and recent normalization techniques [23, 36], we introduce global adaptive normalization, which averages the features of the Gaussians selected for densifcation (F (0) den ) and scales the normalized features using the averaged features. 3.3. Applying Generative Densification We present two models that incorporates generative densification, based on LaRa [5] for object-level reconstruction and MVSplat [7] for scene-level reconstruction. The overall pipelines of generative densification for both scenarios are illustrated in Fig. 4. For object-level reconstruction, fine Gaussians are generated using the Gaussians and volume features produced by the LaRa backbone (top row of Fig. 4). For scene-level reconstruction, fine Gaussians are generated per view by utilizing the pixel-aligned Gaussians and image features extracted from the MVSplat backbone (bottom row of Fig. 4). Additionally, residual learning is incorporated into the scene-level model to better reconstruct complex indoor and outdoor real-world scenes. Further details on the residual learning and the pipelines are provided in Appendix and Appendix C, respectively. 5 Table 1. Quantitative comparisons of our object-level models against their baselines. Our-fast is trained on the Gobjaverse [33] training set for 30 epochs, and Ours is further fine-tuned for 20 epochs. Ours (w/ residual) is trained on the same set for 50 epochs with residual learning (Appendix B). LaRa is re-evaluated using the publicly available checkpoint and our view-sampling method (Appendix D). Method #Param(M) Gobjaverse [33] GSO [10] Co3D [25] PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS MVSNeRF [4] MuRF [34] LGM [30] GS-LRM [42] LaRa [5] Ours-fast Ours Ours (w/ residual) 0.52 15.7 415 300 125 134 134 134 14.48 14. 19.67 - 27.49 28.23 28.58 28.75 0.896 0.877 0.867 - 0.938 0.943 0.945 0.946 0.185 0.301 0.157 - 0.093 0.084 0.080 0.078 15.21 12. 23.67 30.52 29.70 30.62 31.06 31.23 0.912 0.885 0.917 0.952 0.959 0.964 0.966 0.967 0.154 0.279 0.063 0.050 0.060 0.062 0.058 0.058 12.94 11. 13.81 - 21.18 21.67 21.72 22.08 0.841 0.815 0.739 - 0.862 0.864 0.865 0.867 0.241 0.393 0.414 - 0.216 0.209 0.209 0.206 Figure 5. Qualitative comparisons of our object-level model trained for 50 epochs against the original LaRa. The zoomed-in parts within the red boxes are shown on the right side of the second and third columns, focusing on the comparison of fine detail reconstruction. The two images in the rightmost column present the Gaussians input to and output from our generative densification module, respectively. 4. Experiment 4.1. Experimental Setup Implementation Details. We jointly trained the generative densification module with the LaRa backbone for 30 epochs, and then fine-tuned for 20 epochs to achieve further improvements in rendering quality. The training was conducted on four A6000-48G GPUs over 3.5 days, with batch size of 4 per GPU. Similarly, the MVSplat backbone was trained jointly with our densification module for 300,000 iterations, followed by fine-tuning for 150,000 iterations. The scene-level training was conducted on four H100-80G GPUs over 6.5 days, also with batch size of 4 per GPU. For detailed description of the network and training hyperparameters, please refer to Appendix D. and evaluate our scene-level model on RE10K [46] with an image resolution of 256256, and further evaluate it on two cross-domain datasets, ACID [17] and DTU [12], following MVSplat [7]. For both models, we use training and testing splits provided in each dataset without any modifications. Baselines. We compare our object-level model against the original LaRa [5], as well as LGM [30] and GS-LRM [42]. Additionally, we include two feed-forward NeRF models, MVSNeRF [4] and MuRF [34], for reference. For comparing our scene-level model, we include feed-forward NeRF models (pixelNeRF [44], GNPR [27], and MuRF [34]) and pixel-aligned Gaussian models (pixelSplat [3], DepthSplat [35], and the original MVSplat [7]) as baselines. 4.2. Results Datasets. We train and evaluate our object-level model on Gobjaverse [33] dataset, large-scale multi-view dataset with an image resolution of 512512. To further demonstrate the cross-domain capability, we evaluate our method on Google Scanned Objects [10] dataset and subset of Co3D [25] test set, following LaRa [5]. Similarly, we train Object-level Reconstruction. Fig. 5 compares the rendered images from the baseline LaRa against those from our object-level model. Our model clearly outperforms LaRa in reconstructing fine details, such as the white dots on the red ribbon (first row of Fig. 5) and the intricate floral patterns on the socks (second row of Fig. 5). It is also evident that 6 Figure 6. Qualitative comparisons of our scene-level model against the original MVSplat on the RE10K [46] dataset. Figure 7. Qualitative comparisons of our scene-level model against the original MVSplat on the ACID [17] and DTU [12] datasets. our densification strategy effectively captures detailed areas and generate fine Gaussians, as shown in the rightmost column of Fig. 5. While LaRa quickly learns the objects general shape by leveraging explicit volume representations, it struggles to accurately represent edges or contours. The quantitative comparison provided in Tab. 1 further demonstrates the ability of our model to capture and reconstruct fine details. Our model achieves the highest PSNR in both in-domain reconstruction and cross-dataset generalization tasks. It even outperforms GS-LRM, the current state-of-the-art model for object-level reconstruction, using significantly fewer number of parameters (300M vs. 134M). This highlights that selectively generating Gaussians in the detailed areas can be more effective than uniformly generating numerous Gaussians across the entire object. Scene-level Reconstruction. Fig. 6 and Fig. 7 compares the rendered images from the baseline MVSplat against those from our scene-level model. Our model better reconstructs thin structures and fine details, such as the guardrail (first row of Fig. 6) and the wave (second row, first column of Fig. 7). Moreover, we empirically found that our method reduces the opacity of Gaussians in empty space, making them less visible in the rendered images. For example, it makes the floaters more transparent near the water pipe (second row of Fig. 6) and the iron fence (first row, first column of Fig. 7), removing artifacts in the images. In the quantitative comparisons, our model not only outperforms the baselines including DepthSplat [35] on the indomain reconstruction task (Tab. 2), but also consistently achieves the best performance on the cross-dataset generalization task (Tab. 3). While DepthSplat is concurrent work, we include it as baseline for the readers reference. Our model outperforms DepthSplat with fewer parameters (37M vs. 28M) and smaller batch size (32 vs. 16). Additionally, unlike DepthSplat, which relies on pretrained depth predictor, our model is solely trained with multi-view images. 4.3. Comparing Coarse and Fine Gaussians We provide an intuitive example comparing coarse and fine Gaussians, with an analysis of their positions, opacities, and scales. Here, we refer coarse Gaussians to the input Gaussians selected by the gradient masking, while fine Gaussians to their corresponding generated fine Gaussians (i.e., the union of output Gaussians from each densification layer). 7 Table 2. Quantitative comparisons of our scene-level model against its baselines on the RE10K [46] dataset. The MVSplatfinetune model is fine-tuned for 150,000 iterations from the MVSplat checkpoint at 300,000 iterations. * indicates concurrent work. Method #Param(M) PSNR SSIM LPIPS pixelNeRF [40] GPNR [27] MuRF [34] pixelSplat [3] MVSplat [7] MVSplat-finetune DepthSplat* [35] Ours 250 27 15.7 125.1 12.0 12.0 37 27.8 20.43 24.11 26. 25.89 26.39 26.46 26.76 27.08 0.589 0.793 0.858 0.858 0.869 0.870 0.877 0.879 0.550 0.255 0.143 0.142 0.128 0.127 0.123 0.120 Table 3. Cross-dataset generalization results on the ACID [17] and DTU [12] datasets. All models are trained on the RE10K [46] training set and evaluated on each dataset without fine-tuning. Method ACID [17] DTU [12] PSNR SSIM LPIPS PSNR SSIM LPIPS pixelSplat [3] MVSplat [7] Ours 27.64 28.15 28.61 0.830 0.841 0. 0.160 0.147 0.141 12.89 13.94 14.05 0.382 0.473 0.477 0.560 0.385 0.380 Table 4. Ablations on the number of selected Gaussians and learnable masking, evaluated on the GSO [10] dataset. Mask and (0) denote learnable masking and the number of selected Gaussians in gradient masking, respectively. #Gausss refers to the number of final Gaussians, the union of the coarse and fine Gaussians. Mask (0) #Gauss PSNR SSIM LPIPS 12,000 114,351 0 15,000 30,000 12,000 46,693 130,056 189,322 151,968 30.62 29.46 30.68 30.63 30. 0.964 0.951 0.964 0.964 0.965 0.062 0.079 0.061 0.061 0.060 The left side of Fig. 8 illustrates that reconstructing contours is challenging with insufficient number of Gaussians. Although these areas are covered by large Gaussians with no visible holes in the rendered image, it still appears blurry, with the object not clearly separated from the background  (Fig. 5)  . As shown on the right side, fine Gaussians have smaller scales and lower opacities compared to coarse Gaussians, indicating that our method reconstructs details by accumulating partially overlapping small Gaussians. 4.4. Ablation Study Tab. 4 presents the ablation results on the number of input Gaussians and learnable masking. The first row shows the evaluation of our object-level model on the GSO [10] dataset, which is identical to the Ours-fast model in Tab. 1. The second to fourth rows show the evaluation of the same model without fine-tuning, varying the number of Gaussians selected by the gradient masking module for densification. The last row shows the evaluation of our object-level model retrained for 30 epochs without learnable masking. Figure 8. 2D histograms of Gaussian attributes. Each pixel represents histogram bin, with brighter colors for higher counts. Gradient Masking. Image quality improves as the number of selected Gaussians is increased from 0 to 12,000 to 15,000 but does not improve further when increased to 30,000 (Tab. 4), indicating that simply densifying more Gaussians does not guarantee better image quality. Moreover, the number of final Gaussians (#Gauss) increases by approximately 66% when (0) is increased from 12,000 (our default setting) to 30,000, which leads to slower rendering and higher GPU memory requirements in practice. Our densification strategy balances image quality and rendering efficiency by using the gradient masking to selectively densifying only the necessary Gaussians. Learnable Masking. Learnable masking is another crucial component for reducing the computational and memory demands, as it controls the number of Gaussians generated by each densification layer. Ideally, the masking should filter out Gaussians that do not require further densification without compromising image quality. As shown in the last row of Tab. 4, although the masking results in slight decrease in PSNR of 0.36%, it significantly reduces the number of final Gaussians by 25%. This demonstrates that the computational and memory efficiency gained by reducing the number of Gaussians outweighs the minor loss in image quality, highlighting the importance of learnable masking. 5. Conclusion We proposed generative densification, an efficient densification strategy for generalized feed-forward models. We integrated the proposed method into LaRa and MVSplat, providing practical guidance for real-world applications. Extensive experiments demonstrate that our method generates fine Gaussians capable of reconstructing high-frequency details, establishing new benchmarks in both object-level and scene-level reconstruction tasks. We believe our work opens new research direction towards generating fine Gaussians for high-fidelity generalizable 3D reconstruction."
        },
        {
            "title": "References",
            "content": "[1] Samuel Rota Bul`o, Lorenzo Porzi, and Peter Kontschieder. Revising densification in gaussian splatting. arXiv preprint arXiv:2404.06109, 2024. 3 [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 3 [3] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image In Propairs for scalable generalizable 3d reconstruction. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 3, 6, 8 [4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 6 [5] Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, and Andreas Geiger. Lara: Efficient large-baseline radiance fields. In European Conference on Computer Vision, 2025. 1, 3, 5, 6, 11, 12, 14 [6] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. arXiv preprint arXiv:2411.04924, 2024. [7] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, 2025. 1, 3, 5, 6, 8, 12, 14 [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1 [9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 1 [10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In International Conference on Robotics and Automation, 2022. 6, 8, 14 [11] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH, 2024. 12 [12] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2014. 6, 7, [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. 1, 3, 5, 12 [14] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian arXiv preprint splatting as markov chain monte carlo. arXiv:2404.09591, 2024. 3 [15] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 12 [16] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 11 [17] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, 2021. 6, 7, 8 [18] Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Mvsgaussian: Fast generalizable gaussian splatting reconstruction from multi-view stereo. In European Conference on Computer Vision, 2025. 3 [19] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 [20] Longfei Lu, Huachen Gao, Tao Dai, Yaohua Zha, Zhi Hou, Junta Wu, and Shu-Tao Xia. Large point-to-gaussian model for image-to-3d generation. In Proceedings of the ACM International Conference on Multimedia, 2024. 3 [21] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [22] Giuseppe Peano and Peano. Sur une courbe, qui remplit toute une aire plane. Springer, 1990. 2, 4 [23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [24] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification In Proceedings of the IEEE/CVF Conand segmentation. ference on Computer Vision and Pattern Recognition, 2017. 5 [25] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of In Proceedings of the real-life 3d category reconstruction. IEEE/CVF International Conference on Computer Vision, 2021. 6 [26] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 9 estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019. 5 [40] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. [41] Chubin Zhang, Hongliang Song, Yi Wei, Yu Chen, Jiwen Lu, and Yansong Tang. Geolrm: Geometry-aware large reconstruction model for high-quality 3d gaussian generation. arXiv preprint arXiv:2406.15333, 2024. 1, 3 [42] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, 2025. 1, 3, 6 [43] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. 12 [44] Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, and Hengshuang Zhao. Pixel-gs: Density control with pixelarXiv preprint aware gradient for 3d gaussian splatting. arXiv:2403.15530, 2024. 3, 6 [45] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gpsgaussian: Generalizable pixel-wise 3d gaussian splatting for In Proceedings of real-time human novel view synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1 [46] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 1, 3, 6, 7, 8, 14 Stereo magnification: [47] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view In Proceedings of 3d reconstruction with transformers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [27] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In European Conference on Computer Vision, 2022. 6, 8 [28] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arXiv preprint arXiv:2406.04343, 2024. [29] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1, 3 [30] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, 2025. 1, 3, 6 [31] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 4 [32] Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Zhizhong Han. Snowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 5 [33] Chao Xu, Yuan Dong, Qi Zuo, Junfei Zhang, Xiaodan Ye, Wenbo Geng, Yuxiang Zhang, Xiaodong Gu, Lingteng Qui, Zhengyi Zhao, Qing Ran, Jiayi Jiang, Zilong Dong, and Liefeng Bo. G-buffer objaverse: High-quality rendering dataset of objaverse. https://aigc3d.github.io/ gobjaverse/. 3, 6, 14 [34] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: Multi-baseline radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6, 8 [35] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. 3, 6, 7, [36] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. Advances in Neural Information Processing Systems, 2019. 5 [37] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efarXiv preprint ficient 3d reconstruction and generation. arXiv:2403.14621, 2024. 1, 3 [38] Zongxin Ye, Wenyu Li, Sidun Liu, Peng Qiao, and Yong Dou. Absgs: Recovering fine details in 3d gaussian splatting. In Proceedings of the ACM International Conference on Multimedia, 2024. 3 [39] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through"
        },
        {
            "title": "Appendix",
            "content": "A. Additional Results on the DL3DV Dataset Table 5. Evaluation results on the DL3DV-10K [16] dataset. denotes the frame distance span across all test views Method = 150 = 300 PSNR SSIM LPIPS PSNR SSIM LPIPS MVSplat-finetune Ours 17.42 17.76 0.516 0. 0.417 0.405 16.19 16.46 0.452 0.468 0.478 0.469 We conducted an additional evaluation on the DL3DV10K [16], challenging dataset with 51.3 million frames from 10,510 real-world scenes. Our scene-level model and its baseline are fine-tuned for 100,000 iterations on two subsets (3K and 4K) of the DL3DV-10K dataset, comprising approximately 2,000 scenes. All models are evaluated on 140 scenes that are filtered out from the training set, following MVSplat360 [6]. For each scene, we select 5 views as input and evaluate on 56 views uniformly sampled from the remaining views [6]. As shown in Tab. 5, our model outperforms the baseline across all metrics, consistent with the evaluations on the RE10K, ACID, and DTU datasets. B. Generating Residuals of Fine Gaussians We further propose to generate residuals of fine Gaussians for scene-level reconstruction, which involves capturing complex geometries and appearances across diverse indoor and outdoor environments. Similar to the densification procedure presented in Sec. 3.1, the top (0) input Gaussians with large viewspace positional gradients (G(0) den ) are selected, and their positions and features (X (0) den , (0) den ) are passed through alternating layers of up-sampling (UP), Gaussian head (HEAD), and splitting (SPLIT) modules: (X (l), (l)) = UP(X (l1) den ), , (l1) den , (l), (l)), (13) (12) G(l) = HEAD(G(l1) den rem) = SPLIT(G(l), (l), (l)), (14) (G(l) den, (l) den, (l) den, G(l) for {1, , L1}. The Gaussian positions and features are up-sampled in the UP module, which is identical to the up-sampling procedure as described in Sec. 3.2. The up-sampled positions and features are then passed to the HEAD module, where they are transformed into fine Gaussian parameters and added to those from the previous layer (G(l1) ). In the SPLIT module, the fine Gaussians are diden vided into two groups: those that require further densification and those do not. The first group of Gaussians is refined in the next layer, while the second group remains unchanged. Note that the second group of Gaussians positions and features (X (l) rem) are omitted from Eq. (14) for rem, (l) 11 brevity. Finally, the L-th layers fine Gaussians are generated as G(L) = HEAD(G(L1) )) without splitting, and the final set of Gaussians is obtained as: , UP(X (L1) , (L1) den den den ˆG = { L1 (cid:91) l= rem} G(L). G(l) (15) The two densification methods for object-level and scene-level reconstruction are similar in that both selectively generates fine Gaussians leveraging learnable masking, but they differ in how fine Gaussians are generated. The object-level method generates fine Gaussians directly in each densification layer, while the scene-level method generates initial fine Gaussians in the first layer and selectively refines them by adding residuals in subsequent layers. Although the residual learning is proposed for the scenelevel model, it can also be applied to the object-level model. As shown in Tab. 1, our object-level model trained with the residual learning achieves the best PSNR, demonstrating its effectiveness in object-level reconstruction. C. Model Details The Number of Gaussians. We set (0), the number of selected Gaussians in the gradient masking module, to 12,000 for object-level reconstruction and 30,000 for scenelevel reconstruction. The selected Gaussian positions and features are up-sampled by factor of R(l), leading to an increased number of Gaussians given by (l) = (l1)R(l). In the splitting module, the up-sampled Gaussians are divided into two groups: those that need another round of densification in the next layer and those do not. The number of Gaussians for further densification and the remaining ones rem = (l) (l) den = (l)P (l) and (l) are defined as (l) den, respectively, where (l) (0, 1) denotes the masking ratio and is the ceiling operator. Generative Densification of LaRa. LaRa [5] generates 3D volume representations conditioned on image features, and each volume features are decoded into multiple Gaussians. To improve the rendering quality, LaRa introduce cross-attention between the volume features and coarse renderings, including ground-truth images, rendered images, depth images, and accumulated alpha maps [5]. The intermediate features from the cross-attention are transformed to residuals of SH using an MLP, which are added to the coarse SH to obtain the refined Gaussians. We modify the last MLP (the residual SH decoder) to output both the residuals and refined volume features. The first columns of the MLP output are considered the residual SH, while the remaining columns serve as input features for generative densification. Here, denotes the number of SH coefficients. We take the refined Gaussians and the Table 6. Summary of training hyperparameters. Object-level Scene-level Config Value Config Value optimizer scheduler learning rate beta weight decay warmup iters epochs AdamW [19] Cosine 4e-4 [0.9, 0.95] 0.05 1,000 30 optimizer scheduler learning rate beta weight decay warmup iters iters Adam [15] Cosine 2e-4 [0.9, 0.999] 0.00 2,000 300, Table 7. Summary of fine-tuning hyperparameters. Object-level Scene-level Config Value Config Value optimizer scheduler learning rate beta weight decay warmup iters epochs AdamW [19] Cosine 2e-4 [0.9, 0.95] 0.05 0 20 optimizer scheduler learning rate beta weight decay warmup iters iters Adam [15] Cosine 2e-4 [0.9, 0.999] 0.00 0 150,000 decoder (the last cross-attention layer and the residual SH decoder) is trained after the first 5,000 iterations, we train it from the very beginning. For model evaluation on the GSO dataset, we utilize the classical K-means algorithm to group the cameras into 4 clusters and select the center of each cluster to ensure sufficient angular coverage of the input views. Both LaRa and our model are evaluated using this new sampling method. Scene-level Reconstruction. Similar to our object-level model, we calculate the image reconstruction loss = LMSE(I, ˆI) + 0.05LLPIPS(I, ˆI) for both coarse and fine images, and minimize the loss to jointly train the MVSplat backbone and the densification module. Here, LLPIPS denotes the learned perceptual similarity loss (LPIPS [43]). For model fine-tuning, we set the warm-up step of the viewsampler to 0, and the minimum and maximum distances between context views are set to 45 and 192, respectively. E. Additional Qualitative Results We provide additional qualitative results for both objectlevel and scene-level reconstruction tasks. Fig. 9 illustrates how fine Gaussians are generated in each densification layer. Fig. 10 and Fig. 11 show qualitative comparisons for object-level and scene-level reconstruction, respectively. concatenation of volume and refined volume features as input, and their respective fine Gaussians are generated by our method. Note that, while the baseline LaRa generates 2D Gaussian representations [11], we adapt it to generate 3D Gaussians representations [13] instead. Generative Densification of MVSplat. MVSplat [7] generates per-view pixel-aligned Gaussian representations from input multi-view images. transformer encodes the input images into features via cross-view attention, after which per-view cost volumes are constructed. The image features are concatenated with these cost volumes and decoded into depths and other parameters, including opacities, covariances, and colors. The Gaussian positions are then determined by un-projecting the depths into 3D space. Similar to LaRa, we obtain the refined features by applying cross-attention between the coarse renderings and the concatenated features of images and cost volumes, followed by simple MLP. However, we do not predict residuals of SH, as this often leads to unstable training in scene-level reconstruction. We use the per-view Gaussians from the MVSplat backbone along with the refined features as input to our method, generating per-view fine Gaussians. Impelmentation Details. For object-level reconstruction, we select 12,000 Gaussians from the LaRa backbone with large view-space gradients and generate fine Gaussians through two densification layers. The up-sampling factors of the two layers are 2 and 4, and the masking ratio is 0.8. In other words, the input Gaussians are densified by factor of 2 in the first layer, and 80% of them are further densified by factor of 4 in the second layer, while the remaining 20% are decoded into raw Gaussians. Similarly, for scene-level reconstruction, we select 30,000 Gaussians per view from the MVSplat backbone. We use three densification layers, each with an up-sampling factor of 2. The masking ratios are set to 0.5 for the first layer and 0.8 for the second layer. D. Training and Evaluation Details The training and fine-tuning hyperparameters are summarized in Tab. 6 and Tab. 7, respectively. The training objectives and additional details are outlined in the followings. Object-level Reconstruction. The backbone and the densification module are jointly trained by minimizing the loss = LMSE(I, ˆI) + 0.5(1 LSSIM(I, ˆI)), for both coarse and fine images, where LMSE is the mean squared error and LSSIM(I, ˆI) is the structural similarity loss. The coarse images are rendered using the Gaussians generated by the backbone LaRa, and the fine images are rendered using the final Gaussians (Eq. (7)) from our densification module. Unlike the original implementation [5], where the fine Figure 9. Additional qualitative results of our object-level and scene-level model trained for 50 epochs and 450,000 iterations, respectively. The zoomed-in parts show our method selects and reconstructs the fine details through alternating densification layers, while preserving the smooth areas unchanged. Note that the 7-th column of the scene-level reconstruction results shows the union of fine Gaussians generated across all three densification layers, and the output Gaussians from the third layer are omitted due to space constraints. 13 Figure 10. Qualitative comparisons of our object-level model against the original LaRa [5], evaluated on the GSO [10] and Gobjaverse [33] dataset. The coarse and fine Gaussians are the input and output of generative densification module, respectively. Figure 11. Qualitative comparisons of our scene-level model against the original MVSplat [7], evaluated on the RE10K [46] dataset. The red boxes show that our model better reconstructs the scene, removing visual artifacts and generating missing parts."
        }
    ],
    "affiliations": []
}