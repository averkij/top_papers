{
    "paper_title": "Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs",
    "authors": [
        "Ruihan Jin",
        "Pengpeng Shao",
        "Zhengqi Wen",
        "Jinyang Wu",
        "Mingkuan Feng",
        "Shuo Yang",
        "Chu Yuan Zhang",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \\textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models."
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR 2026 EXPLORING KNOWLEDGE PURIFICATION IN MULTITEACHER KNOWLEDGE DISTILLATION FOR LLMS Pengpeng Shao1 Shuo Yang1 Ruihan Jin1 Mingkuan Feng1 1Department of Automation, Tsinghua University 2Beijing National Research Center for Information Science and Technology {jinrh24, wu-jy23}@mails.tsinghua.edu.cn Zhengqi Wen1, Chu Yuan Zhang1 Jinyang Wu1, Jianhua Tao1,2, {zqwen, jhtao}@tsinghua.edu.cn 6 2 0 2 1 ] . [ 1 4 6 0 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Knowledge distillation has emerged as pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of Knowledge Purification, which consolidates the rationales from multiple teacher LLMs into single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of LLM has revolutionized various domains, including question answering (Yue, 2025) and reasoning (Plaat et al., 2024). The scaling law (Kaplan et al., 2020) unveils the correlation between the model size and generation capability, yet the practical deployment of colossal LLMs is often constrained by computational cost and resource demands, emphasizing the need for building efficient and lightweight models that preserve their power. As the extension of model compression (Buciluˇa et al., 2006), knowledge distillation (Hinton et al., 2015) has emerged as prominent solution to this challenge, which enables student models to inherit the capability of larger teacher models. Knowledge distillation is widely applied across various fields of machine learning (Kim & Rush, 2016; Park et al., 2019; Tang et al., 2019). To enhance knowledge diversity and specialized domain competencies, transferring knowledge from multiteacher ensemble to the student model attracts significant academic interest. This focus leads to the development of multi-teacher knowledge distillation approaches, such as TinyLLM (Tian et al., 2025) and TwT (Xu et al., 2025). However, existing multi-teacher knowledge distillation frameworks suffer from two significant drawbacks: (1) Knowledge Conflict: Conflicting rationales among teacher LLMs are inevitable due to hallucinations, inconsistent reasoning paths or difference in expertise domains, impeding the effectiveness of knowledge transfer to the student model. This problem may become more pronounced as the number of teacher models increases. (2) High Resource Demands: Incorporating knowledge from multiple teachers inherently escalates resource requirements, necessitating complex sampling procedures and intricate training pipelines, which subsequently raises computational cost. To investigate the adaptability of existing multi-teacher knowledge distillation frameworks toward more teacher LLMs, we perform extended experiments with TinyLLM (Tian et al., 2025), incrementally introducing series of teacher LLMs for distillation training(detailed in Appendix D.1). Corresponding authors. Published as conference paper at ICLR 2026 (a) 77M student (b) 248M student (c) 783M student Figure 1: Effects of increasing teacher LLMs on the performance of the TinyLLM framework. As illustrated in Fig. 1, contrary to our expectation that enlarging the teacher LLM ensemble would enhance the capabilities of student models, the distillation performance actually declines as the number of teacher models further increases. This decline indicates the detrimental impact of the knowledge conflict among teacher LLMs. In this paper, we introduce the concept of Knowledge Purification in multi-teacher knowledge distillation. The core idea is to condense the knowledge of multiple teacher models from the rationale perspective. The knowledge purification integrates the rationales generated by multiple teacher LLMs into one single, consolidated rationale, which is subsequently employed during the distillation. Hence, the student model is provided with the rationale that encapsulates the collective insights of the teachers, enabling more efficient and effective distillation training. By purifying the knowledge, we mitigate the hallucinations and divergent reasoning paths among the teacher LLMs, thereby alleviating inter-teacher knowledge conflicts. We further propose five methods to facilitate knowledge purification from distinct perspectives including aggregation, routing, and reinforcement learning (RL)-based selection. To thoroughly evaluate these approaches, we conduct extensive experiments on commonsense and biomedical reasoning tasks. Our results show that knowledge purification methods significantly enhance knowledge distillation performance across different student models and datasets. The effectiveness in alleviating knowledge conflicts is further verified. Furthermore, methods based on LLM routing demonstrate outstanding performance on out-of-domain datasets, underscoring the potential of utilizing knowledge purification to guide multi-teacher distillation across broader spectrum. Our contributions are summarized as follows: We identify the limitations of existing multi-teacher knowledge distillation frameworks, highlighting knowledge conflicts and high resource demands that hinder effective knowledge transfer. We introduce the concept of knowledge purification, which mitigates knowledge conflicts and enhances training efficiency by consolidating the rationales from multiple teachers into one coherent rationale. We propose five knowledge purification methods from different perspectives of aggregation, routing, and RL-based selection. Extensive experiments verify improvements of proposed methods in distillation performance and conflict mitigation. Further experiments on out-of-domain datasets illustrate the potential of knowledge purification in facilitating the generalization of multi-teacher knowledge distillation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multi-Teacher Knowledge Distillation Compared to utilizing single teacher, multi-teacher knowledge distillation harnesses broad knowledge diversity and rich reasoning paths, thereby enhancing the capabilities and generalization performance of student models (Liu et al., 2020; Zhang et al., 2024). TinyLLM (Tian et al., 2025) proposes distillation paradigm that facilitates small student LLM to learn from rationales generated by two teacher LLMs. (Xu et al., 2025) incorporates rejection sampling and habitual reasoning in distillation to effectively balance computational cost and performance. These methods are constrained by knowledge conflicts among teacher LLMs, underscoring effective strategies for resolving these conflicts during distillation. 2 Published as conference paper at ICLR 2026 LLM Routing In alignment with Mixture-of-Expert (MoE) (Jacobs et al., 1991; Collobert et al., 2003; Jiang et al., 2024), LLM routing aims to select the optimal LLM from diver candidates for given question, enabling efficient activation of LLM ensembles. HybridLLM (Ding et al., 2024) leverages hybrid approach to optimize both cost and quality for LLM pairs. Similarly, RouterLLM (Ong et al., 2024) explores effective methods for dynamic routing between strong and weak LLM. RouterDC (Chen et al., 2024) introduces dual contrastive learning and improves the routing performance. Recent innovations further explore the structured router (Jin et al., 2025) and the employment of reinforcement learning(Yue et al., 2025). Within the multi-teacher knowledge distillation framework, LLM routing presents promising approach for effective knowledge purification."
        },
        {
            "title": "3.1 PRELIMINARIES",
            "content": "Generally, knowledge distillation uses the soft outputs/labels generated by the strong teacher model to transfer knowledge to weak student model S. In this paper, we focus primarily on multiple choice question answering problems in NLP, utilizing LLMs as the main subjects of our study. In the k-multiple choice question answering task, given Multiple Choice Question Answering question and corresponding candidate options set = {o1, o2, . . . , ok}, the objective of LLMs is to select the correct option from that aligns with the ground truth option O. Besides, LLMs are encouraged to generate rationales, which have been shown to significantly enhance their performance (Wei et al., 2022). The answering process of an LLM (either the teacher or the student S) is formulated as: where po and pr denote the prompt for predicting options and generating rationales, respectively. = (q, O, po), = (q, O, pr), (1) Multi-Teacher Knowledge Distillation We consider the rationale generated by the teacher LLM to be an embodiment of knowledge. We sample this rationale as rT = (q, O, pr) from the teacher and construct the training set = {(q, O, o, rT )} with samples. The knowledge distillation for LLM leveraging rationales (Hsieh et al., 2023) can be formulated as: LKD = LPR + λLDL, (2) where λ is hyper-parameter balance between the prediction loss LPR and the distillation loss LDL. The prediction loss LPR guides the student to learn directly from ground truth options and the distillation loss LDL supervises the student to inherit knowledge from the teachers rationale. Details of the knowledge distillation for LLM are introduced in Appendix A. Compared to the single-teacher approach, multi-teacher knowledge distillation leverages an ensemble of teacher LLMs = {T1, T2, . . . , Tn} to equip the student model with broader spectrum of knowledge, leading to stronger generalization capabilities. In this context, we expand the training set to = {(q, O, o, R)}, where = {rT1 , rT2, . . . , rTn} denotes the rationales generated by each teacher LLM for the question and corresponding answer options O. (Tian et al., 2025) extends the knowledge distillation framework to incorporate multiple teachers as: LMTKD = LPR + (cid:88) j=1 λjLDLj, (3) where LDLj is the distillation loss with respect to the j-th teachers rationale and λj denotes the importance weight for Tj."
        },
        {
            "title": "3.2 MOTIVATION ANALYSIS",
            "content": "Although most frameworks are originally designed to incorporate fixed number of teacher LLMs for knowledge distillation, practically, we expect to enhance the capability and expertise of the student model by enlarging the teacher ensemble. We consider TinyLLM as representative method 3 Published as conference paper at ICLR 2026 Figure 2: An illustration of five knowledge purification methods proposed in our work. and conduct experiments to explore its adaptability to more teacher LLMs, as detailed in Appendix D.1. Results in Fig. 1 exhibit that, in these scenarios, the performance of TinyLLM significantly declines as the number of teacher LLMs further increases, which indicates the detrimental impact of knowledge conflicts among teacher LLMs. Furthermore, increasing the number of teachers can impose additional challenges related to computational resources and hyperparameter tuning. Therefore, there is an urgent need to develop new framework to address these issues."
        },
        {
            "title": "3.3 KNOWLEDGE PURIFICATION",
            "content": "In this section, we introduce the concept of Knowledge Purification in multi-teacher knowledge distillation. The knowledge purification process integrates the rationales generated by multiple teacher LLMs into one single, consolidated rationale, which is subsequently employed during the distillation. Specifically, given the rationales generated by each teacher LLM as = {rT1, rT2 , . . . , rTn }, the process of knowledge purification can be expressed as: rP = (R) = (rT1, rT2, . . . , rTn ), where () denotes the purification process and rP denotes the consolidated rationale. Through knowledge purification, we aim to mitigate knowledge conflicts among the teacher LLMs and enhance distillation efficiency. Our study investigates the impact of different purification methods on the performance of knowledge distillation. (4) Incorporating knowledge purification within the multi-teacher knowledge distillation framework alters the training objective, formulated as: where LDL-KP denotes the distillation loss calculated using the consolidated rationale rP: LMTKD-KP = LPR + λLDL-KP, LDL-KP = 1 (cid:88) rP (cid:88) (q,O,R)D i= log p(rPir<i, q, O, pr). (5) (6)"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "As shown in Fig. 2, we propose five methods to perform knowledge purification defined by Eq. 4. Knowledge Aggregation We first consider performing knowledge purification by employing an aggregator, which is global LLM that accepts all the rationales generated by teacher LLMs and combines the instructions to generate consolidated rationale. We use an instruction-tuning paradigm (Wei et al., 2021) to provide instruction prompts containing in-context example as input and perform aggregation in generation fashion. LLM Routing Build upon pool of candidate LLMs, an LLM router is designed to allocate an input question to the most appropriate LLM. Unlike aggregation, the key aspect of routing is selecting one rationale based on the probabilities predicted by the router as: rP = arg max rTi Pθ(rTiq). 4 (7) Published as conference paper at ICLR 2026 In this paper, we design three representative LLM routing methods for knowledge purification: Plackett-Luce ranking We use Plackett-Luce (PL) model (Luce et al., 1959; Plackett, 1975) for ranking multiple teacher LLMs. In the Plackett-Luce model, the probability of selecting candidate rationale is modeled in softmax relationship: Pθ(rTiq) = eξi j=1 eξj (cid:80)n , = 1, . . . n, We learn the PL coefficients ξ = {ξi : = 1, . . . n} by solving: arg min ξ (cid:88) q,ygt [ω ℓ(ygt, eξ j=1 eξj (cid:80)n )], (8) (9) where ℓ denotes the cross-entropy loss, and ygt denotes the ground truth label for the optimal selection. Inspired by (Ong et al., 2024), we use the weight ω to measure the similarity between the input question and question in the database as ω = γ1+ ϵϵ γ is hyper-parameter, and ϵ and ϵ denote text embeddings for and q, respectively. PLM classifier We adopt pre-trained language model (PLM) to extract textual features for standard text classification. Specifically, we employ PLM encode the input question into semantic embedding hCLS which is derived from the final hidden state corresponding to the special classification token (CLS). Subsequently, we use two-layer perceptron to predict the probabilities of routing to each rationales rTi as: ϵϵ , where Pθ(rTiq) = eWi2(Wi1hCLS+bi1)+bi2 j=1 eWj2(Wj1hCLS+bj1)+bj (cid:80)n , = 1, . . . n, (10) where Wi and bi denote the parameters of the MLP corresponding to Ti. Similarity-based router We follow RouterDC (Chen et al., 2024) to perform similaritybased LLM routing. We construct trainable LLM embeddings {ki : 1, . . . , n} and calculate the cosine similarities between the question embedding and LLM embeddings for routing: esimE(q),ki j=1 esimE(q),kj where denotes language encoder, and sim, denotes the cosine similarity. The router is trained with two contrastive losses. Pθ(rTiq) = = 1, . . . n, (11) (cid:80)n , Details of all LLM routing methods are introduced in Appendix B.2. RL-based Teacher Selection Inspired by (Yuan et al., 2021), we adopt reinforcement learning (RL) framework to dynamically select teacher LLMs for knowledge purification. We define the state si to encapsulate characteristics of the question and the rationales of the i-th teacher LLMs rTi as: si = [E(q), E(rTi) I(Ti(q, O, po) = o)] R2d, (12) where is language encoder, and I(Ti(q, O, po) = o) indicates whether the teacher Ti answers correctly. We design the policy function πθ as: πθ(si, ai) = aiσ(Wisi + bi) + (1 ai)(1 σ(Wisi + bi)), (13) where σ denotes the sigmoid function, and the action ai {0, 1} indicates whether to select the teacher Ti. Consider the definition of knowledge purification, we adopt the teacher LLM that receives the highest prediction score σ(Wisi + bi) and use it to guide the distillation process. The trainable parameter of the teacher selector θ = {W Rn2d, Rn1} is optimized using the standard policy gradient method: θ θ + β (cid:88) (cid:88) qQ θπθ(si, ai), (14) where β denotes the learning rate. The reward = LPR LDL is computed based on the performance of the student model. During training, we alternately perform the knowledge distillation and the RL training. Detailed training algorithm is represented in Appendix B.3. 5 Published as conference paper at ICLR 2026 Table 1: Overall performance of multi-teacher knowledge distillation on four datasets. The best results across different datasets are highlighted in bold, with the second-best results are underlined. Average denotes average accuracy. Setting Method OBQA Teacher ) 7 7 ( m 5 - F ) 8 4 2 ( a 5 - F ) 3 8 7 ( a 5 - F e S n t t u a FLAN-T5 xlarge Llama 2-chat BioMistral-7B Llama-3.1-8B-Instruct Inference Fine-tuning Distilling-Step-by-Step TinyLLM Knowledge Aggregation Plackett-Luce Ranking PLM Classifier Similarity-based Router Teacher Selection Inference Fine-tuning Distilling-Step-by-Step TinyLLM Knowledge Aggregation Plackett-Luce Ranking PLM Classifier Similarity-based Router Teacher Selection Inference Fine-tuning Distilling-Step-by-Step TinyLLM Knowledge Aggregation Plackett-Luce Ranking PLM Classifier Similarity-based Router Teacher Selection"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "69.20 54.60 51.80 65.60 16.60 45.60 41.00 40.60 40.40 42.00 44.20 48.60 46.80 31.00 61.00 59.00 58.80 58.00 62.40 63.60 65.60 65.00 50.40 72.00 71.60 68. 71.40 72.60 74.20 76.60 75.20 ARC 68.24 43.35 51.59 71.67 19.31 31.76 30.73 30.56 30.47 31.76 30.64 32.19 30.39 23.00 43.61 46.01 46. 45.75 46.27 46.35 46.35 46.70 51.07 60.26 60.00 59.83 59.74 59.48 59.66 60.60 61.12 Riddle 53.73 43.73 23.14 60.98 13.33 49.22 44.90 44. 43.92 45.69 49.22 49.61 48.82 30.78 56.86 59.41 58.82 58.43 58.63 59.22 60.78 61.76 39.80 67.45 68.43 67.25 68.63 69.41 68.24 70.59 70.39 PQA 71.50 54.50 73.25 75.00 28.00 47.50 49.25 54.25 53.25 50.50 53.75 52.25 52.50 46.00 51.00 52.50 47.25 51.50 54.75 55.00 53.50 53.25 45.50 53.00 51.00 54. 53.50 56.50 62.50 61.00 63.50 Average 65.67 49.05 49.95 68.31 19.31 43.52 41.47 42.38 42.01 42.49 44.45 45.66 44.63 32.70 53.12 54.23 52. 53.42 55.51 56.04 56.56 56.68 46.69 63.18 62.76 62.53 63.32 64.50 66.40 67.20 67.55 Models We consider multi-teacher ensemble of four LLMs: FLAN-T5 xlarge (2.85B), Llama 2-chat (Touvron et al., 2023)(7B), BioMistral-7B (Labrak et al., 2024), and Llama-3.1-8B-Instruct (Dubey et al., 2024). We conduct experiments using FLAN-T5 (Chung et al., 2024) small (77M), base (248M), and large (783M) as student models, respectively. Datasets We conduct experiments on four multiple choice question answering datasets in commonsense reasoning and biomedical reasoning. For commonsense reasoning, we consider OpenBookQA (OBQA) (Mihaylov et al., 2018), AI2 Reasoning Challenge (ARC) (Clark et al., 2018), and RiddleSense (Riddle) (Lin et al., 2021). For biomedical reasoning, we consider PubMedQA (PQA) (Jin et al., 2019). To ensure fair comparison among knowledge purification methods, we randomly retain 80% of the training set samples for distillation training, and the remaining 20% serve as the public set. joint dataset, composed of the public set of each dataset, is utilized for training LLM routers. Details of dataset construction are provided in Appendix C.2. Metrics We calculate the accuracy of the distilled student model in multiple choice question answering tasks as the primary performance metric: ACC = 1 (cid:88) qQ I(S(q, O, po) = o). (15) 6 Published as conference paper at ICLR 2026 Table 2: Comparison of knowledge purification methods from practical perspective. Each method is analyzed in: Prior, Parameters, Training Necessity, Transferability, and Latency. Method Prior Parameters Knowledge Aggregation Plackett-Luce Ranking PLM Classifier Similarity-based Router Teacher Selection q, q q, >10B 278M 278M 278M 278M Training Necessity Transferability Latency ms ms min Furthermore, we assess the effectiveness of knowledge purification methods in mitigating knowledge conflicts among teacher LLMs by calculating the Conflict Mitigation Value (CMV). We define CMV as the average accuracy improvement achieved through knowledge purification in distillation training with series of incremental teacher LLMs, compared to the baseline TinyLLM framework: CMV = 1 1 (cid:88) i= (ACCKP,T =i ACCTinyLLM,T =i). (16) Implementation details For knowledge distillation, we use the AdamW (Loshchilov & Hutter, 2019) optimizer and set the learning rate to 5 105, the batch size to 8, and the maximum input length to 512. We find that λ = 4 works best for balanced training. We employ GPT-4 (Achiam et al., 2023) as the knowledge aggregator. mDeBERTaV3-base (He et al., 2021) is adopted as the language encoder for the PLM classifier, the similarity-based router and the RL-based teacher selector. Details of applying knowledge purification approaches are included in Appendix B. All experiments are conducted on four NVIDIA A100 80GB GPUs."
        },
        {
            "title": "5.1 RESULTS",
            "content": "We conduct comprehensive experiments to evaluate the performance of multi-teacher knowledge distillation employing knowledge purification methods. Our comparison includes the proposed methods against the original teacher LLMs and four baseline approaches. These baselines consist of direct inference, fully fine-tuning the student model, Distilling-Step-by-Step (Hsieh et al., 2023) which leverages teachers rationale as additional supervision, and TinyLLM (Tian et al., 2025) which integrates all rationales generated by all teacher LLMs. Details of baseline approaches are included in Appendix C.3. The overall experimental results are presented in Tab. 1. From performance perspective, no single method demonstrates significant advantage over the others. Overall, the similarity-based router and the RL-based teacher selection consistently achieve the highest average accuracies, ranking either first or second across the distillation experiments involving all three student models. For distilling the FLAN-T5 small model, the similarity-based router attains the highest average accuracy of 45.66%, exceeding baselines by at least 4.9%. For distilling the FLAN-T5 base and FLAN-T5 large models, the RL-based teacher selection exhibits superior performance, surpassing the best baseline by 4.5% and 6.9%, respectively. Generally, the implementation of the knowledge purification method demonstrates advantages over the baseline, underscoring its positive impact on knowledge distillation. The performance of LLM routing is exemplary, while the PL ranking shows slightly weaker results compared to the other two LLM routers. In contrast, the overall performance of the knowledge aggregation method remains relatively inferior, with no significant improvement observed. This suggests that, despite the strong capacity of the aggregator (e.g., GPT-4), the enhancing effect of the consolidated rationale through aggregation on knowledge distillation remains uncertain. Compared to the teacher LLMs, the distilled 783M student model exceeds the average accuracy of three teachers and ranks second only to Llama-3.1-8B-Instruct, illustrating the effectiveness of knowledge purification. Additionally, we observe that knowledge purification yields more substantial improvements in larger student models. This can be attributed to the stronger capacity of larger models to learn from the generated rationales, leading to the notable outcomes achieved through our targeted purification approaches. In contrast, smaller models tend to focus primarily on fitting the final option, which limits the enhancements gained from knowledge purification. 7 Published as conference paper at ICLR (a) 77M student (b) 248M student (c) 783M student Figure 3: Evaluation of knowledge purification methods with an increasing number of teacher LLMs. We visualize the CMV of knowledge aggregation as an example, represented by the signed area of the shaded region (positive when above TinyLLM and negative when below). Table 3: Adaptability of knowledge purification methods toward multiple teacher LLMs, evaluated in Conflict Mitigation Value (CMV). Method CMV77M student CMV248M student CMV783M student Knowledge Aggregation Plackett-Luce Ranking PLM Classifier Similarity-based Router Teacher Selection 0.003 +0.001 +0.018 +0.025 +0.020 0.007 +0.012 +0.014 +0.020 +0.019 0.004 +0.010 +0.021 +0.032 +0."
        },
        {
            "title": "5.2 QUALITATIVE ANALYSIS ON KNOWLEDGE PURIFICATION METHODS",
            "content": "In this section, we perform systematic analysis on the proposed knowledge purification methods from practical perspective. We consider the following metrics to evaluate each methods: Prior refers to the input of each model; Parameters refers to additional amount of parameters introduced for purification; Training Necessity refers to whether training is required; Transferability refers to whether the method can be applied to new dataset without requiring additional training; and Latency refers to the time scale required for processing single instance of knowledge purification. We present our analysis in Tab. 2. For priors, LLM routing methods require only the question as input and do not necessitate pre-sampling rationales from the teacher LLM. This enables us to leverage pretrained LLM routers to guide rationale sampling in multi-teacher knowledge distillation as further demonstrated in Section 5.4. Aside from knowledge aggregation, which employs strong LLM (e.g., GPT-4) for synthesis, other methods only introduce additional parameters on the size of the PLM. The training process of the teacher selector is closely coupled with rewards from knowledge distillation, necessitating retraining when applied to new datasets. This training also introduces significant delay in the implementation of RL-based teacher selection, considerably exceeding the millisecond-level delay of the PLM classifier and the similarity-based router."
        },
        {
            "title": "5.3 ADAPTABILITY TOWARD MULTIPLE TEACHER LLMS",
            "content": "We further evaluate the adaptability of knowledge purification methods in distillation training with series of incremental teacher LLMs. We adopt the same experimental setup we used when revealing the knowledge conflict of TinyLLM. The evaluation result is visualized in Fig. 3 (detailed in Appendix D.2). Tab. 3 demonstrates the Conflict Mitigation Value of each method in mitigating knowledge conflicts. We observe that applying knowledge aggregation reports negative CMV for all three student models, In contrast, all LLM routing suggesting that it fails to effectively mitigate knowledge conflicts. methods and RL-based teacher selection report positive CMV, indicating their potential in alleviating knowledge conflicts. Notably, the similarity-based router achieves the highest CMV across all three student models, demonstrating its superior adaptability to an incremental multi-teacher ensemble."
        },
        {
            "title": "5.4 ROUTER-GUIDED OUT-OF-DOMAIN KNOWLEDGE DISTILLATION",
            "content": "Grounded in knowledge purification, we perform out-of-domain knowledge distillation. We exclude the knowledge aggregation which does not involve training and RL-based teacher selection with 8 Published as conference paper at ICLR 2026 Table 4: Experimental results on out-of-domain datasets. We verify the potential of utilizing LLM routing methods to guide multi-teacher knowledge distillation on out-of-domain data. The best results are highlighted in bold, with the second-best results are underlined. Setting Method PIQA BioASQ Teacher FLAN-T5 small (77M) as Student FLAN-T5 base (248M) as Student FLAN-T5 large (783M) as Student FLAN-T5 xlarge Llama 2-chat BioMistral-7B Llama-3.1-8B-Instruct Inference Fine-tuning Distilling-Step-by-Step TinyLLM Plackett-Luce Ranking PLM Classifier Similarity-based Router Inference Fine-tuning Distilling-Step-by-Step TinyLLM Plackett-Luce Ranking PLM Classifier Similarity-based Router Inference Fine-tuning Distilling-Step-by-Step TinyLLM Plackett-Luce Ranking PLM Classifier Similarity-based Router 58.43 60.50 67.46 70.84 20.78 42.33 49.29 49.84 52.77 50.05 53.97 30.47 47.55 55.93 56.80 63.98 59.63 63. 51.90 58.43 60.72 68.88 68.77 67.90 69.53 65.85 69.92 90.24 88.62 47.97 78.86 81.30 75.61 80.47 82.11 82.11 57.72 89.43 86.18 78. 86.99 85.37 90.24 63.41 90.24 86.99 82.93 87.80 83.74 91.87 limited transferability. Instead, we focus on the proposed LLM routing approaches, as the generalization ability for out-of-domain data is crucial metric for assessing the effectiveness of LLM routers. Physical Interaction Question Answering (PIQA)(Bisk et al., 2020) and BioASQ (Tsatsaronis et al., 2015) serve as two out-of-domain datasets, which represent commonsense reasoning and biomedical reasoning, respectively. As illustrated in Tab. 4, utilizing LLM routers to guide knowledge distillation yields strong generalization ability. The similarity-based router achieves the highest accuracy across most settings, significantly exceeding baselines. Besides, the PL ranking outperforms the PLM classifier in overall performance and demonstrates robust generalization. It is worthy noting that LLM routing approaches only require the question as input, eliminating the need for pre-sampling responses from teacher LLMs. When applied to broader spectrum of out-of-domain data, high-performing LLM routers can effectively direct the sampling process of multi-teacher ensemble, thereby facilitating subsequent knowledge distillation. This approach significantly reduces computational costs and resource consumption during the sampling phase while effectively alleviating knowledge conflicts and enhancing the performance of the distilled model. This presents promising framework for the rapid and flexible implementation of multi-teacher knowledge distillation and the deployment of powerful yet lightweight models."
        },
        {
            "title": "5.5 EFFICIENCY OF KNOWLEDGE DISTILLATION APPROACHES",
            "content": "In addition to performance evaluation, our experiments aim to evaluate the improvement offered by the knowledge purification methods concerning the efficiency of multi-teacher knowledge distillation. To ensure comparability, we fix the distillation epoch at 4000 and evaluate the training efficiency of different methods for distilling the FLAN-T5 large model on the ARC dataset, utilizing GPU hours as the quantitative metric. For knowledge purification methods, we consider the computational consumption of both the purification stage (e.g., aggregation, training the router) and the distillation stage. The results are exhibited in Tab. 5 9 Published as conference paper at ICLR 2026 Table 5: Efficiency of different methods for distilling the FLAN-T5 large model on the ARC dataset. We consider both purification and distillation stages and utilize GPU hours as the metric. Method Purification Stage Distillation Stage Total Fine-tuning Distilling-Step-by-Step TinyLLM Knowledge Aggregation Plackett-Luce Ranking PLMClassifier Similarity-based Router Teacher Selection 1 1 4 4 4 4 4 - - - 5.21 0.1 0.5 0.6 3.5 0.7 1.1 2.6 1.5 1.3 1.2 1. 0.7 1.1 2.6 6.7 1.4 1.7 1.8 3.5 Among the proposed knowledge purification methods, knowledge aggregation demonstrates the highest GPU comsumption due to the extensive call of the open-source LLM as the aggregator. When proprietary LLM, such as GPT-4, is employed as the aggregator, its inefficiency is reflected in the equally significant latency and elevated costs. The RL-based teacher selection uniformly optimizes the purification and distillation, while its iterative training entails considerable computational consumption. Conversely, routing-based methods significantly enhance the efficiency of knowledge distillation compared to TinyLLM, which employs all rationales. Notably, training the LLM router demands fewer computational resources than the distillation process. Furthermore, given the generalization capabilities of routers, the impact of knowledge purification on efficiency enhancement is expected to be more pronounced when applied to out-of-domain data."
        },
        {
            "title": "6 LIMITATIONS",
            "content": "Due to the limited computational resources, we only construct teacher ensemble of four LLMs. While we strategically select teacher LLMs such as BioMistral-7B to supply domain-specific knowledge in biomedical reasoning, it remains challenging to guarantee that the knowledge represented by the teacher ensemble is comprehensive. In the practical application of multi-teacher knowledge distillation, larger number of teacher models would be more conducive to enhancing the specialized domain capabilities of student models. The restricted number of teacher LLMs currently limits our ability to thoroughly assess the effectiveness of knowledge purification methods. Although we conduct small-scale evaluation involving six teachers, as presented in Appendix D.3, further evaluations will be necessary to fully explore and validate the adaptability. Knowledge distillation is universal framework for transferring knowledge from powerful models to weak ones. In this paper, we primarily focus on the NLP field and consider LLMs as the main subjects of our study. The proposed knowledge purification methods are tailored to the characteristics of LLM. Approaches such as LLM routing and teacher selection have the potential to generalize to broader machine learning tasks, but specific implementation and evaluation still require further investigation. We leave the investigation of such scenarios to future work."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we tackle the challenges inherent in multi-teacher knowledge distillation frameworks, specifically addressing knowledge conflicts and high resource demands that hinder effective knowledge transfer to student models. We introduce the concept of Knowledge Purification, aimed at reducing divergent reasoning paths among teachers and enhancing distillation efficiency by consolidating the rationales from multiple teacher LLMs. We propose five methods to facilitate knowledge purification from distinct perspectives. Extensive experiments across commonsense and biomedical reasoning tasks demonstrate that proposed methods significantly enhance the performance of distilled models while effectively mitigating knowledge conflicts. Notably, the approach based on LLM routers showed exceptional performance on out-of-domain datasets, underscoring its broad applicability and practical value. In summary, our findings contribute to the advancement of multiteacher knowledge distillation frameworks, paving the way for the practical deployment of efficient and powerful lightweight models. 1The GPU hour is accessed when using Llama-3.1-70b as the aggregator. 10 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535541, 2006. Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328, 2024. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Ronan Collobert, Yoshua Bengio, and Samy Bengio. Scaling large learning problems with hard parallel mixtures. International Journal of pattern recognition and artificial intelligence, 17(03): 349365, 2003. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. Hybrid llm: Cost-efficient and quality-aware query routing. arXiv preprint arXiv:2404.14618, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, 2021. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Cheng-Yu Hsieh, Chun-Liang Li, CHIH-KUAN YEH, Hootan Nakhost, Yasuhisa Fujii, Alex Jason Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming In The 61st Annual larger language models with less training data and smaller model sizes. Meeting Of The Association For Computational Linguistics, 2023. 11 Published as conference paper at ICLR 2026 Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:263830494. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 25672577, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL https://aclanthology.org/D19-1259/. Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, and Jianhua Tao. Radialrouter: Structured representation for efficient and robust large language models routing. arXiv preprint arXiv:2506.03880, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Yoon Kim and Alexander Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. 13171327, 2016. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373, 2024. Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 15041515, Online, August 2021. doi: 10.18653/v1/2021.findings-acl.131. URL Association for Computational Linguistics. https://aclanthology.org/2021.findings-acl.131/. Yuang Liu, Wei Zhang, and Jun Wang. Adaptive multi-teacher multi-level knowledge distillation. Neurocomputing, 415:106113, 2020. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Duncan Luce et al. Individual choice behavior, volume 4. Wiley New York, 1959. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2024. 12 Published as conference paper at ICLR Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 39673976, 2019. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. CoRR, 2024. Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2):193202, 1975. Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. Distilling taskspecific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136, 2019. Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and Nitesh Chawla. Beyond answers: Transferring reasoning capabilities to smaller llms using multi-teacher knowledge distillation. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, pp. 251260, 2025. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):138, 2015. Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, and Qingming Huang. Abkd: Pursuing proper allocation of the probability mass in knowledge distillation via α-β-divergence. In Forty-second International Conference on Machine Learning, 2025. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, and Dongmei Zhang. Twt: Thinking without tokens by habitual reasoning distillation with multi-teachers guidance. arXiv preprint arXiv:2503.24198, 2025. Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, and Daxin Jiang. Reinforced In Proceedings of the AAAI conference on multi-teacher selection for knowledge distillation. artificial intelligence, volume 35, pp. 1428414291, 2021. Murong Yue. survey of large language model agents for question answering. arXiv preprint arXiv:2503.19213, 2025. Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. Masrouter: Learning to route llms for multi-agent systems. arXiv preprint arXiv:2502.11133, 2025. Yuzhe Zhang, Huan Liu, Yang Xiao, Mohammed Amoon, Dalin Zhang, Di Wang, Shusen Yang, and Chai Quek. Llm-enhanced multi-teacher knowledge distillation for modality-incomplete emotion recognition in daily healthcare. IEEE Journal of Biomedical and Health Informatics, 2024. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A DETAILS OF KNOWLEDGE DISTILLATION FOR LLM",
            "content": "Knowledge distillation (Hinton et al., 2015) is designed to facilitate the transfer of knowledge from the teacher model to the student model. Unlike traditional deep learning, where soft labels can be obtained from teacher models, LLMs are often treated as black-box models. In this context, we typically regard the rationales generated by the teacher LLM as the embodiment of knowledge.(Hsieh et al., 2023) We sample the rationale generated by the teacher LLM as rT = (q, O, pr) and construct the training set = {(q, O, o, rT )} with samples. We expect the student model to inherit knowledge from the teachers rationale and supervise this process using distillation loss, which is defined in the form of the cross-entropy loss as: LDL = 1 (cid:88) rT (cid:88) (q,O,rT )D i=1 log p(rT ir<i, q, O, pr), (17) where rT denotes the number of tokens in the teachers rationale, and p(rT ir<i, q, O, pr) denotes the probability of generating token rT i, given the inputs and the already generated tokens. Besides, prediction loss is introduced in training the student model as: LPR = 1 (cid:88) (cid:88) (q,O,o)D i=1 log p(o o<i, q, O, pr), (18) where denotes the number of tokens in the ground truth option, and p(rT ir<i, q, O, pr) denotes the probability of generating token , given the input and the already generated tokens. The global knowledge distillation process is formulated as: where λ is hyper-parameter balance between the prediction loss LPR and the distillation loss LDL. LKD = LPR + λLDL, (19)"
        },
        {
            "title": "B DETAILS OF KNOWLEDGE PURIFICATION APPROACHES",
            "content": "B.1 KNOWLEDGE AGGREGATION We employ the powerful proprietary LLM GPT-4 (Achiam et al., 2023) as the knowledge aggregator and use an instruction-tuning paradigm (Wei et al., 2021) to guide GPT-4 to perform aggregation in generation fashion. Fig. 4 shows the prompt used for knowledge aggregation. In practice, we pre-label 10 knowledge aggregation samples and randomly select one as the in-context example during inference. In addition to the default GPT-4, we also consider the comparatively weaker Llama-3.1-70b (Dubey et al., 2024) as the aggregator to compare the impact of different aggregator choices. For both aggregators, we adopt the same prompt format and examine the performance of applying knowledge aggregation on distilling the FLAN-T5 large student model. The results, presented in Tab. 6, indicate that utilizing more powerful model as the aggregator does not lead to significant enhancement in performance. B.2 LLM ROUTING Plackett-Luce ranking The Plackett-Luce (PL) model was initially introduced by Plackett (Plackett, 1975) to rank the horses in gambling. It has since been applied to describe the processes of ranking and selecting multiple candidate items in various domains. Notably, when the number of items is limited to two, the Plackett-Luce model simplifies to the Bradley-Terry (BT) model (Bradley & Terry, 1952). Chatbot Arena Platform (Chiang et al., 2024) is built based on the ranking of LLMs by the Bradley-Terry model. The ranking is formulated as estimating the Bradley-Terry coefficient ξ: arg min ξ E(A,H)[ℓ(H, 1 1 + eξA1 ξA2 )], (20) 14 Published as conference paper at ICLR Figure 4: Prompt used for GPT-4 to perform knowledge aggregation, consisted of global instruction, in-context example, and query. where ℓ denotes the binary cross-entropy loss. and denote the LLM pair and the human response. In our work, we consider the Plackett-Luce ranking and extend the framework to multiteacher routing. We transform the problem into cross-entropy loss optimization problem weighted by text similarity. We learn the PL coefficients ξ = {ξi : = 1, . . . n} by solving Eq. 9, where ygt denotes the ground truth label for the optimal selection of the rationales and is represented in an one-hot encoding format. We consider the description generated by the teacher model that produces the minimum number of tokens while making correct selections as the optimal choice, taking into account the preference for computational efficiency and cost reduction in practical applications. ω = γ1+ ϵϵ ϵϵ measures the similarity between the input question and question in the database. Following (Ong et al., 2024), we adopt the exponential scale and choose γ = 10. It is important to note that no training is necessary for the ranking, and all computations are performed during inference. We will elaborate on the specific inference. Proposition 1. Let ξ satisfy Eq. 9. Then, it satisfies the following condition: eξi j=1 eξj (cid:80)n = (cid:80) ω qQi (cid:80) ω , = 1, . . . n, (21) Proof. Note: s.t. Qi = {q : Ti is optimal for q.} y(i) = eξi j=1 eξj (cid:80)n , = 1, . . . n, 15 Published as conference paper at ICLR 2026 Table 6: Comparison of different aggregators for Knowledge Aggregation. The FLAN-T5 large model serves as the student model. Aggregator OBQA Llama-3.1-70b GPT-4 71.20 71.40 ARC 60.77 59.74 Riddle 67.65 68. PQA 52.50 53.50 Average 63.03 63.32 g(y) = (cid:88) q,ygt [ω ℓ(ygt, eξ j=1 eξj (cid:80)n )] = (cid:88) q,ygt [ω ℓ(ygt, y)], (cid:88) h(y) = ( y(i)) 1. i= To maximize g(y) subject to h(y) = 0, we consider the Lagrangian: L(y, η) = g(y) + ηh(y), where η is the Lagrange multiplier. We take the partial derivatives of with respect to and η, and set them to zero: y(i) = 0, = 0. η This yields the system of equations: (cid:88) qQi ω log y(i) y(i) η = 0, = 1, . . . n, (cid:88) ( i=1 y(i)) 1 = 0. From this, we derive: eξi j=1 eξj (cid:80)n = y(i) = (cid:80) (cid:80) ω ω = qQi (cid:80) qQi (cid:80) ω qQi (cid:80) ω , = 1, . . . n. Thus, it satisfies the condition in Eq. 21. In practical inference, we perform Plackett-Luce ranking based on Eq. 21. PLM classifier For training the PLM classifier, the definition of the ground truth label ygt we use is consistent with that in the Plackett-Luce ranking. We utilize mDeBERTaV3-base (He et al., 2021) as the language encoder and extract the semantic embedding hCLS for the input question. We use two-layer MLP with hidden layer dimension of 128 to predict the routing probabilities based on hCLS. We perform full-parameter training and train the classifier for 5000 epochs, using the AdamW (Loshchilov & Hutter, 2019) optimizer with batch size 16 and learning rate 5 105. Similarity-based router We follow RouterDC (Chen et al., 2024) to perform similarity-based LLM routing and adopt two contrastive losses to train the router. For each question in training, we assign binary score in {0, 1} to the LLM based on correctness, and sample the LLM with the highest and lowest scores respectively (randomly selecting one in the case of ties) to calculate the sample-LLM contrastive loss as: Lsample-LLM = log (cid:88) esimE(q),k+ esimE(q),k+ + esimE(q),k , (22) where k+ and denote the LLM embedding of the LLM with the highest and lowest scores, respectively. The sample-sample contrastive loss is introduced to enhance the robustness of the 16 Published as conference paper at ICLR 2026 0, and teacher selector initialized as θ = θ0; Samples actions for each with the teacher selector to determine the selected teacher Ti by: Shuffle to obtain new training sequence. for each mini-batch do Algorithm 1 Training the RL-based Teacher Selector Input: Training dataset D, student model initialized as Θs = Θs hyper-parameters: λ, epoch number L, mini-batch size and learning rate β. 1: for epoch = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Compute the state si for each teacher Ti by Eq. 12; arg maxi σ(Wisi + bi); Compute the policy function πθ(si, ai) by Eq. 13; Update the parameter θ of the teacher selector by: for each (si, ai) do Compute reward r: LPR LDL; θ θ + β (cid:80) qQ θπθ(si, ai). (cid:80) Allocate the actions ai based on the selection; Stored (si, ai) to the episode history H; Update the parameter Θs of the student model under the guidance of Ti by Eq. 5. vector representation, thereby promoting more stable training. We simplify its computation by directly classifying questions based on their respective datasets, as opposed to the semantic clustering approach employed in RouterDC: Lsample-sample = log (cid:88) esimE(q),E(q+) esimE(q),E(q+) + (cid:80) qQ esimE(q),E(q) , (23) where q+ and denote an in-group question and an out-group set of the question q, respectively. The training objective of the router consists of sample-LLM and sample-sample losses as: Lsim = Lsample-LLM + Lsample-sample. (24) We adopt mDeBERTaV3-base (He et al., 2021) as the language encoder to encode the input question. The dimension of the question embedding and the LLM embedding is 768. We train the model for 5000 epochs, using the AdamW (Loshchilov & Hutter, 2019) optimizer with batch size of 16 and learning rate of 2 105. B.3 RL-BASED TEACHER SELECTION Alg. 1 shows the training procedures of the RL-based teacher selector. During training, we alternately perform knowledge distillation and train the teacher selector. We regard the teacher selector as broadly defined LLM router: it must simultaneously receive the question and the rationale, and it can be optimized within unified framework together with the knowledge distillation process. For training the teacher selector, we set the the epoch number to 2, the mini-batch size to 8, and the learning rate β to 5 105."
        },
        {
            "title": "C MORE EXPERIMENTAL DETAILS",
            "content": "C.1 DETAILS OF MODEL SELECTION In our experiments, we choose the small (77M), base (248M) and large (783M) model of the FLANT5 (Chung et al., 2024) series as student models. Due to computational resource constraint, we consider multi-teacher ensemble comprising four LLMs: FLAN-T5 xlarge (2.85B), Llama 2-chat (Touvron et al., 2023)(7B), BioMistral-7B (Labrak et al., 2024), and Llama-3.1-8B-Instruct (Dubey et al., 2024). We construct the teacher LLM ensemble purposefully: For the student models, FLAN-T5 xlarge serves as an homogeneous model with larger number of parameters, while Llama 2-chat operates 17 Published as conference paper at ICLR 2026 Table 7: Statistics of the datasets we used in our experiments. The numbers represent the sample size of each partitions for each dataset. Domain Dataset Train ID OOD OBQA ARC Riddle PQA PIQA BioASQ 3965 893 2808 400 16113 976 Test 500 1165 510 400 919 123 Valid Public 500 295 511 100 919 109 992 224 702 100 - - Table 8: Performance of ABKD for distilling the FLAN-T5 large model using different teacher LLMs. Teacher Model OBQA FLAN-T5 xlarge Llama 2-chat BioMistral-7B Llama-3.1-8B-Instruct 72.40 70.00 71.60 72.40 ARC 59.83 58.45 58.80 60. Riddle 68.63 67.84 64.12 69.22 PQA 50.50 51.25 53.75 52.75 Average 62.84 61.89 62.07 63. as heterogeneous model, also with higher parameter size. BioMistral-7B contributes domainspecific knowledge in biomedical reasoning, and Llama-3.1-8B-Instruct, as an updated version of Llama 2-chat, offers enhanced performance and establishes selection tendency that facilitates the training of knowledge purification methods. C.2 DETAILS OF DATASET CONSTRUCTION Our experiments involve six multiple choice question answering datasets, comprising four indomain (ID) datasets: OpenBookQA (OBQA) (Mihaylov et al., 2018), AI2 Reasoning Challenge (ARC) (Clark et al., 2018), RiddleSense (Riddle), PubMedQA (PQA) (Jin et al., 2019) along with two out-of-domain (OOD) datasets: Physical Interaction Question Answering (PIQA) (Bisk et al., 2020) and BioASQ (Tsatsaronis et al., 2015). For fair comparison, we divide each in-domain dataset into four subsets: training, testing, evaluation, and public sets. The testing and the evaluation set inherit from the original dataset. For in-domain dataset, the training and the public sets are randomly partitioned from the training set of the original dataset in ratio of 4:1. The public sets from each in-domain dataset collectively form joint dataset comprising 2018 samples, used for training LLM routers. The remaining three subsets (training, testing and evaluation) are employed for knowledge distillation. Tab. 7 summarizes the division of both in-domain and out-of-domain datasets. C.3 DETAILS OF BASELINES In our experiments, we compare the proposed methods against four baseline approaches, including Inference, which directly employs student model for evaluation; Fine-tuning, which fine-tunes the student model using the ground truth options as labels; Distilling-Step-by-Step (Hsieh et al., 2023), defined by Eq. 2, which leverages teachers rationale as additional supervision; and TinyLLM (Tian et al., 2025), defined by Eq. 3, which integrates all rationales generated by teachers to train the student model. For the Distilling-Step-by-Step, we train with four teacher LLMs individually and report the best results for each dataset. The standard knowledge distillation (Hinton et al., 2015) is not included in the baselines because, when applied to distilling LLMs, it merely replaces the ground truth labels with those generated by the teacher LLM. This process is functionally similar to fine-tuning, and its theoretical performance upper limit is lower than that of fine-tuning. Therefore, it is excluded from consideration. We do not prioritize comparative analysis between knowledge purification methods and singleteacher distillation approaches; instead, we emphasize the comparison with multi-teacher distilla18 Published as conference paper at ICLR 2026 tion methods. This preference stems from prior research (Hsieh et al., 2023) demonstrating the superior cross-task adaptability of multi-teacher distillation techniques. We additionally conduct an evaluation of the single-teacher method ABKD (Wang et al., 2025), which represents the stateof-the-art knowledge distillation method for LLM. ABKD uses pair of α-β parameters to weight the divergence loss during the distillation stage to achieve balance between Forward KullbackLeibler Divergence (FKLD) and Reverse Kullback-Leibler Divergence (RKLD). In our experiment, we utilize ABKD to distill the FLAN-t5 large model using different teacher LLMs. The results are presented in Tab. 8. Notably, ABKD achieves an average accuracy up to 63.79%, outperforming the performance of Distilling-Step-by-Step yet remaining inferior to the optimal knowledge purification method (RL-based Teacher Selection), which attains an average accuracy of 67.55%. (Xu et al., 2025) proposes distillation framework TwT consists of two stages: Dual-Criteria Rejection Sampling and Habitual Reasoning Distillation. In the first stage, rationales produced by multi-teacher LLMs are screened. However, TwT is not regarded as baseline method or as knowledge-purification method in our paper, for three principal considerations: (1) The data screening in TwT can be interpreted as combined approach that integrates quality assessment process using LLMs with resampling process based on similarity. This approach contrasts with our inclination to develop atomic methods within knowledge purification. In fact, the concept of introducing additional models for evaluation and performing selection based on similarity in TwT echoes the knowledge purification methods we propose. (2) TwT retains pair of rationales from all rationales generated by multiple teacher LLMs, introducing new requirements for the subsequent distillation process, which does not align with the knowledge purification framework. (3) The quality evaluation of rationales in TwT relies on the weighting of multiple qualitative factors produced by LLM, which limits its generalization and real-time sampling capabilities. Nevertheless, the design motivation underlying TwT exhibits similarities with knowledge purification, and we anticipate to further exploring knowledge distillation in conjunction with the TwT framework in future work. C.4 REASONS FOR CHOOSING CMV AS METRIC We quantitatively evaluate the effectiveness of knowledge purification methods in mitigating knowledge conflicts among teacher LLM by computing the Conflict Mitigating Value (CMV). The CMV serves as performance-based metric rather than relying on information-theoretic measures at the rationale level. We have opted for the performance-based CMV for two principal reasons. First, rationale-level metrics lack universality. Information-theoretic metrics, such as JensenShannon Divergence, effectively quantify knowledge aggregation that produces new rationales. However, they are insufficient for methods like LLM routing, which require the selection of single rationale from multiple options. Additionally, although we considered assessing the routers accuracy in choosing the rationale that aligns with the correct answer, this metric proves inadequate for measuring knowledge aggregation. Consequently, the pursuit of unified rationale-level metric becomes particularly challenging. Second, the rationales generated by teacher LLMs represent merely intermediate states within the knowledge distillation process and do not exert direct influence on the performance of the final student model. Accordingly, we designed the CMV to concentrate on performance, enabling more direct and meaningful evaluation."
        },
        {
            "title": "D SUPPLEMENTARY RESULTS",
            "content": "D.1 EXTENDED EXPERIMENTS OF THE TINYLLM FRAMEWORK TinyLLM (Tian et al., 2025) proposes distillation paradigm that facilitates student model to learn from rationales generated by two teacher LLMs. Specifically, it adopts the following loss function when conduction multi-teacher knowledge distillation: LTinyLLM = LPR + (cid:88) j=1 λjLDLj, (25) where λj is the importance weight for Tj. 19 Published as conference paper at ICLR 2026 Table 9: Extended experimental results of TinyLLM as the number of teacher LLMs increases from 1 to 4, where + denotes the addition of the specified LLM as teacher, and denotes the number of teacher LLMs participating in the distillation. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 4 1 2 3 40.00 46.80 40.80 40.60 59.00 63.40 59.00 58.80 71.60 73.20 69.20 68.80 ARC 30.56 30.21 30.47 30.56 44.89 46.44 45.75 46. 59.48 59.91 60.26 59.83 Riddle 42.75 43.92 44.31 44.12 58.24 60.98 58.04 58.82 68.43 68.82 66.08 67.25 PQA 48.50 49.50 46.75 50.50 52.25 50.50 49.75 47.25 50.50 52.50 54.50 54.25 Average 40.45 42.61 40.58 41.45 53.59 55.33 53.13 52. 62.50 63.61 62.51 62.53 The basic TinyLLM framework relies on merely two teacher LLMs, constraining the practical application of knowledge distillation to improve the performance and domain-specific competencies of lightweight models. To explore the adaptability of TinyLLM to more teacher LLMs, we perform series of extended experiments. We begin by using only the FLAN-T5 xlarge model as the teacher LLM and progressively incorporate additional teacher LLMs. In total, we conduct four groups of distillation training experiments with varying numbers of participating teachers. For fair comparison, we set each λj as 4, 2, 1.33, and 1 for the cases of 1, 2, 3, and 4 teachers, respectively. Tab. 9 presents the detailed results of these extended experiments. Our observations indicate that as the number of teacher models further increases, the performance of the distilled student models actually declines. For all three student models, the best overall performance is achieved when two teacher LLMs participate in the distillation process. When the number of teacher LLMs reaches four, the performance of the distilled FLAN-T5 base model is even inferior to that achieved with single teacher. These findings contradict our initial expectation that increasing the number of teachers would enhance knowledge diversity and generalization capabilities. We attribute this performance degradation to the emergence of knowledge conflicts among the teachers, emphasizing the critical need for knowledge purification. D.2 SUPPLEMENTARY RESULTS OF ADAPTABILITY TOWARD MULTIPLE TEACHER LLMS We adopt the same experimental setup in Appendix D.1 to evaluate the adaptability of knowledge purification methods in distillation training with series of incremental teacher LLMs. Tab. 1014 exhibit the complete results of the experiment. D.3 GENERALITY TOWARD BROADER TASK DOMAINS AND MORE TEACHER LLMS The current assessment utilizes four teacher LLMs, focusing mainly on the task domains of commonsense and biomedical reasoning. Our objective is to evaluate the generalization ability of knowledge purification methods across broader range of task domains and more teacher LLMs. To this end, we extend our experiments using the MMLU dataset (Hendrycks et al., 2020), which encompasses 57 tasks from various branches of knowledge. Additionally, we introduce two supplementary teacher LLMs, llemma 7b (Azerbayev et al., 2023) and Mistral-7B-chat (Jiang et al., 2023), bringing total of six teacher LLMs. In these experiments, we employ FLAN-T5 large as the student model for knowledge distillation and consider the similarity-based LLM routing for effective knowledge purification. Tab. 15 demonstrates the results of evaluation. The similarity-based router achieves the highest average accuracy of 65.19%, surpassing the baselines by at least 7.3%. On the MMLU dataset, routing-based method also attains the highest accuracy of 55.26%. The results verifies the gener20 Published as conference paper at ICLR 2026 Table 10: Knowledge distillation results as the number of teacher LLMs increases from 1 to 4, applying Knowledge Aggregation for knowledge purification. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 4 1 2 3 40.00 39.80 40.20 40.40 59.00 55.60 58.20 58.00 71.60 69.60 72.00 71.40 ARC 30.56 30.21 30.47 30.47 44.89 45.92 46.35 45. 59.48 59.06 58.80 59.74 Riddle 42.75 43.73 43.14 43.92 58.24 57.06 58.63 58.43 68.43 66.47 69.02 68.63 PQA 48.50 51.25 49.50 53.25 52.25 50.50 50.75 51.50 50.50 49.50 52.50 53.50 Average 40.45 41.25 40.83 42.01 53.59 52.27 53.48 53. 62.50 61.16 63.08 63.32 Table 11: Knowledge distillation results as the number of teacher LLMs increases from 1 to 4, applying Plackett-Luce Ranking for knowledge purification. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 4 1 2 3 4 40.00 40.20 42.20 42.00 59.00 60.20 63.60 62.40 71.60 72.00 71.80 72. ARC 30.56 30.64 30.73 31.76 44.89 45.41 45.58 46.27 59.48 59.83 59.66 59.48 Riddle 42.75 43.33 44.31 45. 58.24 58.82 58.63 58.63 68.43 68.82 69.02 69.41 PQA 48.50 49.75 50.25 50.50 52.25 52.75 52.25 54.75 50.50 50.50 56.50 56. Average 40.45 40.98 41.87 42.49 53.59 54.30 55.02 55.51 62.50 62.79 64.25 64.50 alization ability of the LLM routing method toward broader task domains and large number of teacher LLMs, highlighting the effectiveness of the knowledge purification framework. Despite our current inability to conduct experiments involving greater number of teacher LLMs (8, 10, or more) due to computational resource limitations, we are encouraged by the success of existing LLM routing methods when applied to more than 10 candidate LLMs (Chen et al., 2024; Hu et al., 2024). It is evident that while knowledge purification methods improve performance, the effectiveness of knowledge distillation does not increase indefinitely with the addition of teacher LLMs. more practical goal is to leverage an appropriate number of teacher models to enhance the overall capabilities and specific expertise of the student model. Furthermore, We also look forward to validating the performance of knowledge purification across wider range of NLP applications. This may necessitate stronger student models, as well as more sophisticated processes for distillation data sampling. We intend to explore these scenarios in our future research to further advance the field. D.4 CASE STUDY We present detailed case study and visualization of the knowledge purification process on the OBQA dataset, as illustrated in Fig. 5. 21 Published as conference paper at ICLR 2026 Table 12: Knowledge distillation results as the number of teacher LLMs increases from 1 to 4, applying PLM Classifier for knowledge purification. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 1 2 3 4 40.00 40.80 43.60 44.20 59.00 61.20 61.00 63.60 71.60 72.40 72.80 74.20 ARC 30.56 30.21 30.39 30. 44.89 46.27 46.44 46.35 59.48 59.83 59.66 59.48 Riddle 42.75 45.69 48.63 49.22 58.24 58.63 59.41 59.22 68.43 68.82 69.02 69. PQA 48.50 51.75 52.50 53.75 52.25 52.25 52.00 55.00 50.50 51.75 60.00 62.50 Average 40.45 42.11 43.78 44. 53.59 54.59 54.71 56.04 62.50 63.20 65.37 66.40 Table 13: Knowledge distillation results as the number of teacher LLMs increases from 1 to 4, applying Similarity-based Router for knowledge purification. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 4 1 2 3 4 40.00 42.00 46.00 48.60 59.00 60.60 63.80 65. 71.60 74.00 75.20 76.60 ARC 30.56 30.47 32.02 32.19 44.89 45.75 46.35 46.35 59.48 59.83 59.91 60.60 Riddle 42.75 46.08 49.22 49.61 58.24 60.00 60.20 60.78 68.43 69.41 69.80 70.59 PQA 48.50 49.75 51.25 52.25 52.25 52.50 53.00 53. 50.50 54.25 62.25 61.00 Average 40.45 42.08 44.62 45.66 53.59 54.71 55.84 56.56 62.50 64.37 66.79 67."
        },
        {
            "title": "E THE USE OF LLMS",
            "content": "In this section, we clarify that no LLMs were employed in the writing or polishing of this paper. All content presented herein are the result of original research and critical evaluation by the authors. 22 Published as conference paper at ICLR 2026 Table 14: Knowledge distillation results as the number of teacher LLMs increases from 1 to 4, applying RL-based Teacher Selection for knowledge purification. The best results across different datasets are highlighted in bold. Student Teacher Setting OBQA FLAN-T5 small 77M FLAN-T5 base 248M FLAN-T5 large 783M FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B FLAN-T5 xlarge + Llama 2-chat + BioMistral-7B + Llama-3.1-8B 1 2 3 4 1 2 3 4 1 2 3 4 40.00 41.40 44.40 46. 59.00 60.80 63.60 65.00 71.60 73.60 72.00 75.20 ARC 30.56 30.73 31.59 30.39 44.89 46.44 46.61 46.70 59.48 60.60 60.77 61. Riddle 42.75 47.84 48.04 48.82 58.24 60.78 60.20 61.76 68.43 69.02 70.00 70.39 PQA 48.50 48.75 52.25 52. 52.25 50.50 51.75 53.25 50.50 51.25 61.75 63.50 Average 40.45 42.18 44.07 44.63 53.59 54.63 55.54 56.68 62.50 63.62 66.13 67. Table 15: Supplementary results on commonsense reasoning, biomedical reasoning, and multitask language understanding with six teacher LLMs. The FLAN-T5 large model serves as the student model. The best results across different datasets are highlighted in bold. Method Inference Distilling-Step-by-Step TinyLLM Similarity-based Router OBQA 50.40 71.60 70.40 77.00 ARC 51.07 60.77 54.25 60.17 Riddle 39.80 68.43 67.25 70.78 PQA 45.50 51.00 52.75 62.75 MMLU Average 45.10 51.84 49.28 55.26 46.37 60.73 58.79 65.19 23 Published as conference paper at ICLR 2026 Figure 5: An example on the OBQA dataset. Five proposed methods are used for knowledge purification."
        }
    ],
    "affiliations": [
        "Beijing National Research Center for Information Science and Technology",
        "Department of Automation, Tsinghua University"
    ]
}