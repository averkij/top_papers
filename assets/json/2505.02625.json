{
    "paper_title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis",
    "authors": [
        "Qingkai Fang",
        "Yan Zhou",
        "Shoutao Guo",
        "Shaolei Zhang",
        "Yang Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 5 2 6 2 0 . 5 0 5 2 : r LLaMA-Omni 2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis Qingkai Fang1,3, Yan Zhou1,3, Shoutao Guo1,3, Shaolei Zhang1,3, Yang Feng1,2,3* 1Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) 2Key Laboratory of AI Safety, Chinese Academy of Sciences 3University of Chinese Academy of Sciences, Beijing, China {fangqingkai21b,fengyang}@ict.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving highquality real-time speech interaction. LLaMAOmni 2 is built upon the Qwen2.5 series models, integrating speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-theart SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data."
        },
        {
            "title": "Introduction",
            "content": "Speech, as critical interface for human-computer interaction, can significantly enhance both interaction efficiency and user experience (Clark et al., 2019). In recent years, as large language models (LLMs) like ChatGPT (OpenAI, 2022) have demonstrated outstanding performance across various fields, speech interactions with LLMs have attracted widespread attention from both academia and industry. For instance, GPT-4o (OpenAI, 2024) enables real-time, intelligent, and natural speech interaction between users and LLMs, heralding the advent of new generation of human-computer interaction paradigms. To develop spoken chatbot similar to GPT-4o, the traditional approach typically employs cascaded pipeline comprising an automatic speech *Corresponding author: Yang Feng. 1Code: https://github.com/ictnlp/LLaMA-Omni2 Audio Samples: https://llama-omni2.github.io/ 1 recognition (ASR) model, an LLM, and text-tospeech (TTS) model. While this method is relatively straightforward to implement, it suffers from several notable limitations. First, errors can accumulate across the different stages of the pipeline. Second, the overall response latency tends to be high due to the sequential processing of multiple models. Third, the system struggles to capture paralinguistic information present in the input speech. To address these limitations, end-to-end speech language models (SpeechLMs) have gradually gained more attention, using single unified model to handle the entire process from speech input to output. Overall, end-to-end SpeechLMs can be categorized into two types: native and modular. Native SpeechLMs typically discretize speech into tokens and employ GPT-style decoder-only Transformer (Radford, 2018) to model both speech and text within unified language model (Zhang et al., 2023; Rubenstein et al., 2023; Hassid et al., 2024a). key advantage of this architecture is its ability to leverage vast amounts of unsupervised speech data for pretraining, making it easier to scale up in terms of model parameters and data size. This can potentially result in emergent capabilities, such as more human-like speech expressiveness (Zeng et al., 2024a; Open-Moss, 2025). However, native SpeechLMs typically require large-scale speech datasets (e.g., millions of hours) for pretraining (Zeng et al., 2024b; DÃ©fossez et al., 2024), which presents challenges in data collection and training costs, and may also lead to catastrophic forgetting of the models text capabilities. In contrast, modular SpeechLMs incorporate speech encoder and speech decoder around the LLM to handle speech understanding and generation (Fang et al., 2025; Wang et al., 2024). The advantage of this approach is its ability to leverage the inherent capabilities of each module, requiring only small-scale fine-tuning (e.g., few hundred or thousand hours of speech data) to align the modules. This enables the model to acquire speech interaction capabilities at relatively low cost, while retaining most of its original capability. Moreover, modular SpeechLMs can typically generate speech guided by textual output, ensuring the intelligence of the generated speech. In addition to the intelligence of speech, realtime responsiveness and naturalness are also crucial characteristics of spoken chatbots. LLaMAOmni (Fang et al., 2025) uses non-autoregressive (NAR) streaming speech decoder to enable synchronized generation of speech and text, ensuring extremely low response latency. However, due to the limitations of non-autoregressive models in modeling capacity, the generated speech is often less natural and fluent. Freeze-Omni (Wang et al., 2024) combines both NAR and autoregressive (AR) models for speech generation, resulting in higher naturalness of the generated speech. However, it can only achieve sentence-level streaming speech generation through simple sentence-split strategy, which prevents it from achieving very low response latency. To address these challenges, in this paper, we introduce LLaMA-Omni 2, series of modular SpeechLMs ranging from 0.5B to 14B. LLaMAOmni 2 adopts Qwen2.5-0.5B/1.5B/3B/7B/14BInstruct models (Team, 2024) as the base LLM, and uses Whispers encoder (Radford et al., 2023) as the speech encoder. For the speech decoder, inspired by the state-of-the-art streaming speech synthesis model CosyVoice 2 (Du et al., 2024), it first includes an autoregressive text-to-speech language model initialized with Qwen2.5-0.5B, which generates speech tokens from the LLM output and achieves streaming generation through alternating read and write operations. The speech tokens are then passed through chunk-aware causal flow matching model (Lipman et al., 2023) to generate the mel spectrogram in streaming manner. To train the model, we synthesize 200K multiturn speech-to-speech dialogue samples with diverse input voices and uniform output voice. Experimental results show that LLaMA-Omni 2 achieves outstanding performance on spoken question answering and speech instruction following tasks in both speech-to-text and speech-to-speech settings, outperforming both LLaMA-Omni and the native SpeechLM GLM-4-Voice (Zeng et al., 2024a), which was trained on millions of hours of speech data. We also conducted detailed ablation studies on factors such as LLM parameter size, training data scale, speech decoder pretraining, and read-write strategy, to better understand the impact of these factors on the overall system performance."
        },
        {
            "title": "2 Model: LLaMA-Omni 2",
            "content": "In this section, we introduce the model architecture of LLaMA-Omni 2. As shown in Figure 1, the core of LLaMA-Omni 2 is an LLM, for which we use the Qwen2.5 series models (Team, 2024) due to their strong performance across various benchmarks. Next, we will describe how we equip the LLM with speech understanding and streaming speech generation capabilities. In the following, we use LLM to denote the LLM. For single-turn instruction-response pair, we denote the speech instruction as X, and the text and speech responses as and S, respectively. 2.1 Speech Understanding To enable speech understanding, we incorporate speech encoder and speech adapter before the LLM, similar to LLaMA-Omni (Fang et al., 2025). Specifically, we use the encoder of Whisper-largev3 (Radford et al., 2023) as the speech encoder, which converts the input speech into sequence of representations. The encoded representations are then passed into the speech adapter, which consists of downsampling module and feed-forward network (FFN). The downsampling module concatenates every consecutive frames along the feature dimension, and the concatenated representations are further encoded by the FFN. The final output representation is then input into the LLM. 2.2 Streaming Speech Generation To equip the model with streaming speech generation capabilities, we adopt paradigm similar to CosyVoice 2 (Du et al., 2024). First, the speech response is converted into discrete tokens using supervised semantic speech tokenizer. Then, an autoregressive text-to-speech language model is employed to model the streaming generation from the LLM output to speech tokens. Finally, causal flow matching model converts speech tokens into the mel spectrogram in streaming manner. Speech Tokenizer The speech tokenizer is implemented by inserting finite scalar quantization (FSQ) module (Mentzer et al., 2024) into the encoder of SenseVoice-Large ASR model (An et al., 2024). This module first projects the intermediate representations to low-rank space and discretizes them through rounding operation. Ultimately, 2 Figure 1: Left: Model architecture of LLaMA-Omni 2. Right: Illustration of the two-stage training strategy. the speech response is converted into token sequence = [yU 1 , . . . , yU ], with 25 tokens per second, where each token yU { . We use the pretraiend speech tokK < 6561 } enizer in CosyVoice 2. 0 Text-to-Speech Language Model After converting the speech response into discrete tokens, we use decoder-only Transformer (Vaswani, 2017) to model the conditional language model from the LLM output to the speech tokens, denoted as TTS. It is initialized with Qwen2.5-0.5B, and its vocabulary is extended as = < > , where is the original < 6561 } vocabulary. This extension enables the model to generate speech tokens. N, 0 { The input to TTS comes from the output of the LLM. Specifically, the LLM output consists of two parts: continuous hidden states and text tokens sampled from the hidden states. The former contains contextual information, while the latter provides precise textual content. We aim to use both as inputs to the text-to-speech language model. This allows the model to both consider the current context and ensure better alignment with the text response when generating speech tokens. During training, the LLM is trained with teacher forcing, so its output hidden states are denoted as = [h1, ..., hN ], where hi = <i). The corresponding text is the ground truth = [yT ]. We first use 2-layer feed-forward network (FFN) to map the hidden states to the embedding dimension of LLM(X, 1 , ..., yT 3 TTS, while also obtaining the text embeddings: ehidden = FFN(hi), = Emb(yT eemb ), (1) (2) where Emb( TTS. Af- ) is the embedding layer of terward, we use an element-wise gate fusion mechanism to combine both representations. Specifically, we compute the gate gi as follows: gi = Ï (cid:0)Wg (cid:2)ehidden eemb (cid:3) + bg (cid:1) , (3) denotes concatenation, Ï is the sigmoid where Rd are the R2dd and bg function, and Wg weight and bias parameters of the gate, and is the embedding size of TTS. Finally, the fused representation is computed as: ci = gi ehidden + (1 gi) eemb , (4) where denotes element-wise multiplication. This fused representations = [c1, ..., cN ] are then passed to TTS for generating speech tokens. -WriteTo achieve streaming generation, i.e., to generate speech tokens simultaneously during the LLMs output process, we adopt ReadW strategy, similar to CosyVoice 2. Specifically, we mix the fused representation and the speech tokens at predefined ratio fused representations read in, the model generates speech tokens. Once all fused representations are read, the model continues to generate the remaining speech tokens until completion. During . For every R : training, cross-entropy loss is computed only for the generated speech tokens as follows: TTS = (cid:88) i=1 Cmin( i1 +1 log (yU R,N), (5) R,N) denotes the fused repi1 +1 where Cmin( resentations that have already been read. <i), Flow Matching Model The speech tokens genTTS are further processed by chunkerated by aware causal flow matching model (Lipman et al., 2023) to synthesize the mel spectrogram in streaming manner. Every time speech tokens are generated, they are treated as chunk for mel spectrogram synthesis. The synthesized mel spectrogram is then passed through HiFi-GAN vocoder (Kong et al., 2020) to generate the final waveform. We use the pretrained flow matching model and vocoder in CosyVoice 2. 2.3 Training The training of LLaMA-Omni 2 relies solely on 200K multi-turn speech-to-speech dialogue data (we will describe how this is synthesized in Section 3) and does not use any ASR or TTS data. We find that it is sufficient to achieve excellent performance while minimizing training costs. Specifically, the training process consists of two stages, as shown in Figure 1. Stage In Stage training, we train the speechto-text and text-to-speech components separately. The training data consists of <speech instruction, text response> pairs and <text response, speech response> pairs from the multi-turn speech-to-speech dialogue data. Specifically, for the speech-to-text part (Stage I(a)), we freeze the speech encoder and train the speech adapter and LLM with crossentropy loss. For the text-to-speech part (Stage I(b)), we train the text-to-speech language model with cross-entropy loss. Note that during this stage, the gate fusion module is not trained, and only text embeddings are input into TTS. Stage II In Stage II, we train the models speechto-speech generation capability with speech-tospeech dialogue data. During this stage, we freeze the speech encoder, speech adapter, and LLM, and only train the gate fusion module and TTS. 2.4 Inference During inference, the LLM autoregressively generates the text response based on the speech instructext tokens, its hidden tion. After generating states and the corresponding decoded text are fed TTS to generate into the gate fusion module and speech tokens, which are then passed through the flow matching model and the vocoder to synthesize speech chunk. In this way, text and speech responses can be generated simultaneously. The response latency for the first synthesized speech chunk can be calculated as: ), total = LLM( )+ TTS( )+ FM( )+ Voc(2 T R LLM( ) and where required by the erate (6) ) represent the time TTS models to genand ) and FM( ) represent the decoding times of the flow matching model and vocoder when the inputs are tokens, respectively. TTS( LLM and Voc(2 W"
        },
        {
            "title": "W\nM",
            "content": "W and 2 tokens2, respectively. W 3 Data Construction In this section, we introduce the process of constructing multi-turn speech-to-speech dialogue data. Our data is an extension of the InstructS2S-200K dataset introduced in Fang et al. (2025), which contains 200K single-turn instruction-following samples designed for speech interaction scenarios. These samples are derived from the Alpaca (Taori et al., 2023) and UltraChat (Ding et al., 2023) datasets through rewriting using LLMs. Specifically, for each sample, we first sample the number of turns from Poisson distribution: Poisson(Î» = 2), then clip to the range of 1 to 5. Next, we use the Llama-3.3-70B-Instruct3 (Dubey et al., 2024) model to iteratively generate the dialog. For the i-th turn, the instruction and response are generated based on the dialogue history of previous 1 turns. In this way, we obtain 200K multi-turn text dialog samples. Next, we need to convert the text dialogue into speech. To simulate real-world applications, we aim to have varied voices for the instruction, while maintaining consistent voice for the response. For each multi-turn dialogue, we first use the fishspeech-1.54 model (Liao et al., 2024) to synthesize 2The length of the mel spectrogram is twice that of the speech tokens (50 Hz vs. 25 Hz). 3https://huggingface.co/meta-llama/Llama-3. 3-70B-Instruct 4https://huggingface.co/fishaudio/ 4 short prompt (e.g., \"This is randomly generated voice\") with random voice. Then, we use the synthesized speech as the prompt for the CosyVoice20.5B5 model, which synthesize the instruction into speech while simultaneously cloning the voice. This ensures consistency in the voice across different turns of the dialogue, while maintaining diversity across dialogues. For all responses, we use uniform voice as the prompt and then synthesize the speech using the CosyVoice2-0.5B model."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setups Model Configuration We use the encoder of Whisper-large-v3 as the speech encoder. The speech adapter first performs 5 downsampling, followed by FFN with an intermediate dimension of 2048. For the LLM, we select the Qwen2.5 series models, including Qwen2.50.5B/1.5B/3B/7B/14B-Instruct models. We refer to the corresponding models as LLaMA-Omni20.5B/1.5B/3B/7B/14B in the following sections. For the text-to-speech language model, we initialize it with the Qwen2.5-0.5B model and set the = 10. read-write strategy with We will discuss the impact of these hyperparameters on speech quality and response latency later. The speech tokenizer, flow matching model, and vocoder are directly taken from CosyVoice 2. = 3 and Training Details We use the 200K multi-turn speech-to-speech dialogue data from Section 3 for two-stage training. In Stage I(a), we freeze the speech encoder and train all parameters of the speech adaptor and LLM. The batch size is 32, and we train for 3 epochs with peak learning rate of 5e-5. In Stage I(b), we train the text-tospeech language model with batch size of 32 for 5 epochs and peak learning rate of 5e-4. In Stage II, we freeze the speech encoder, speech adaptor, and LLM, and train the remaining components with batch size of 32 for 1 epoch and peak learning rate of 1e-3. For all stages, we use warmup strategy for the first 3% of steps and cosine annealing learning rate scheduler. The LLaMA-Omni2-14B model is trained on 4 NVIDIA H800 GPUs, while other models are trained on 4 NVIDIA L40 GPUs. fish-speech-1.5 5https://www.modelscope.cn/studios/iic/ CosyVoice2-0.5B 5 4.2 Evaluation Our evaluation includes two tasks: spoken question answering and speech instruction following. For both tasks, we evaluate the models speech-totext and speech-to-speech capabilities. The speechto-speech evaluation is done by transcribing the speech response into text using the Whisper-largev3 model, and then applying the same evaluation method as used for speech-to-text evaluation. In all experiments, we use greedy search for the LLM to ensure stable results. For the text-to-speech language model, we use sampling with temperature set to 1.0, as we find that using greedy search causes the model to fall into repetition. Spoken Question Answering The speech question answering (SpokenQA) task involves asking the model spoken questions, then checking whether the reference answer appears in the models response, and calculating the accuracy. We evaluate our model on two benchmarks: Llama Questions6 (Nachmani et al., 2024) and Web Questions7 (Berant et al., 2013). Since the questions in the Web Questions dataset are in text form, we use CosyVoice2-0.5B to synthesize them into speech. Speech Instruction Following For the speech instruction following task, we follow the settings in Fang et al. (2025), selecting the helpful_base and vicuna subsets from the Alpaca-Eval8 (Li et al., 2023) dataset, excluding math and code-related instructions. The remaining 199 instructions are then synthesized into speech for evaluation. Following Fang et al. (2025), we evaluate the model using the following metrics: ChatGPT Score: To evaluate the models ability to follow instructions, we use GPT-4o (OpenAI, 2024) to score the models responses. It considers factors such as helpfulness, relevance, fluency, and suitability for speech interaction scenarios, and assigns single score between 1 and 5. The detailed prompt can be found in Appendix A. ASR-WER: To assess the consistency between models text and speech responses, we use Whisperlarge-v3 to transcribe the speech response into text, and calculate the word error rate (WER) between the transcribed text and text response. We perform 6https://github.com/google-research-datasets/ LLAMA1-Test-Set 7https://huggingface.co/datasets/Stanford/web_ questions 8https://github.com/tatsu-lab/alpaca_eval Model SpokenQA (Accuracy ) Speech Instruction Following Llama Questions Web Questions ChatGPT Score S2T S2T S2T S2S S2S S2S ASR-WER UTMOS Latency (ms) TWIST SpeechGPT Spectron Moshi (7B) GLM-4-Voice (9B) LLaMA-Omni (8B) - 21.6 21.9 62.3 64.7 67.7 LLaMA-Omni2-0.5B 45.7 LLaMA-Omni2-1.5B 62.0 64.3 LLaMA-Omni2-3B 70.3 LLaMA-Omni2-7B 73.0 LLaMA-Omni2-14B 4.0 - - 21.0 50.7 49.0 38.7 52.7 55.7 60.7 62.7 - 6.5 6.1 26.6 32.2 33.4 17.7 28.2 30.5 34.5 40.4 1.5 - - 9.2 15.9 23.7 16.8 26.6 28.0 31.3 37. - 2.98 - - 4.16 3.99 3.24 4.01 4.24 4.28 4.56 - 2.17 - - 4.09 3.52 3.20 3.91 4.14 4.15 4.35 - 40.01 - - 9.02 5.95 2.64 3.06 3.37 3.26 3. - 3.51 - - 3.48 3.67 4.21 4.22 4.22 4.19 4.20 - 5587.94 - - 1562.81 346.73 542.71 552.76 567.84 582.91 663.32 Table 1: Results on speech question answering and speech instruction following benchmarks. S2T and S2S represent = 10 for all LLaMA-Omni2 series models. speech-to-text and speech-to-speech, respectively. We set = 3 and text normalization9 before calculating the WER."
        },
        {
            "title": "5 Results and Analysis",
            "content": "UTMOS: To evaluate the naturalness of the generated speech, we use the UTMOS model10 (Saeki et al., 2022) to predict the mean opinion score (MOS) of the generated speech. Latency: We measure the time from receiving the speech instruction to generating the first speech chunk on single NVIDIA L40 GPU. 4.3 Baseline Systems We primarily compare LLaMA-Omni 2 with the following baseline systems: LLaMA-Omni (Fang et al., 2025): One of the earliest SpeechLMs that achieves real-time speech interaction, by using CTC-based (Graves et al., 2006) streaming speech decoder to simultaneously generate text and speech units. The generated units are fed into the vocoder for streaming synthesis in fixed-size chunks. We set the chunk size â¦ = 40. GLM-4-Voice (Zeng et al., 2024a): The current state-of-the-art native SpeechLM, pretrained on millions of hours of speech data. It enables realtime speech interaction by alternately generating text and speech tokens in fixed ratio of 13:26. The generated speech tokens are input into flow matching model with fixed chunk size. In addition, we also borrow some results from Zeng et al. (2024a), including results of TWIST (Hassid et al., 2024b), SpeechGPT (Zhang et al., 2023), Spectron (Nachmani et al., 2024), and Moshi (DÃ©fossez et al., 2024). 9https://github.com/openai/whisper/blob/main/ whisper/normalizers/english.py 10https://github.com/tarepan/SpeechMOS 5.1 Main Results Table 1 presents the main results on the speech question answering and speech instruction following benchmarks. Spoken Question Answering For the SpokenQA task, we observe that: (1) For models with similar parameter sizes, LLaMA-Omni2-7B outperforms both GLM-4-Voice and LLaMA-Omni in both S2T and S2S settings. Notably, our model significantly reduces the gap between S2T and S2S performance. For example, on the Web Questions bench15.9), mark, GLM-4-Voice drops by 16.3 (32.2 23.7), while LLaMA-Omni drops by 9.7 (33.4 LLaMA-Omni2-7B only drops by 3.2 (34.5 31.3), demonstrating that our approach largely improves speech generation capabilities. (2) For models with varying parameter sizes, we observe that accuracy increases as the LLM size grows, indicating that LLaMA-Omni 2 effectively leverages the LLMs inherent capabilities. For smaller models, LLaMAOmni2-1.5B/3B exceeds the accuracy of GLM-4Voice and LLaMA-Omni in the S2S setting, making them suitable choices for edge devices. For larger models, we observe significant accuracy improvement with LLaMA-Omni2-14B compared to LLaMA-Omni2-7B, highlighting the potential of our approach for scaling to larger models. Speech Instruction Following For the speech instruction following task, we observe that: (1) LLaMA-Omni2-3B/7B/14B outperforms both GLM-4-Voice and LLaMA-Omni in the S2T and S2S settings, demonstrating the strong instructionfollowing capabilities of our models. (2) Similar 6 Model Score (S2S) ASR-WER LLaMA-Omni2-7B w/o Gate Fusion w/o Text Embedding 4.15 4.02 3.88 3.26 4.89 6.83 Table 2: Ablation study on the gate fusion module with LLaMA-Omni2-7B. 5 1 10 2 10 3 15 3 15 4 5 20 Offline Score (S2S) ASR-WER UTMOS Latency (ms) 4.09 4.15 4.15 4.12 4.10 4.15 4.14 3.48 4.00 3.26 4.37 3.77 3.62 3.40 3.98 4.19 4.19 4.27 4.27 4.32 4.46 457.29 557.79 582.91 663.32 683.42 798.99 - Model Score (S2S) ASR-WER Streaming TTS Offline TTS Text Pretrained Scratch 4.15 4.13 3.53 1.08 3.26 3.51 10.34 80.65 Table 3: Ablation study on different TTS pretraining strategies with LLaMA-Omni2-7B. to the results on SpokenQA benchmarks, we observe that model performance improves as the LLM size increases, with LLaMA-Omni2-14B achieving significantly better performance. (3) The models ASR-WER is generally low, significantly lower than previous models, proving that our models maintain strong consistency between the text and speech responses. (4) Regarding speech quality, thanks to the CosyVoice 2s strong causal flow matching model, our models achieve good UTMOS scores under streaming synthesis, significantly outperforming the baseline models. (5) The latency of LLaMA-Omni 2 is around 600ms. Although it is slightly higher than LLaMA-Omni, it still meets the requirements for real-time interaction and is significantly lower than that of GLM-4-Voice. 5.2 Ablation Studies To understand the impact of different factors on overall performance, we conduct series of ablation studies on the LLaMA-Omni2-7B model. Gate Fusion Module Table 2 shows the ablation study on the gate fusion module. Gate fusion module allows the model to adaptively fuse LLM hidden states and text embeddings, considering both contextual information and textual content. When the gate fusion module is removed and the two components are simply added together (ehidden + eemb ) as input to the text-to-speech language model, we observe decrease in performance. Further removing the text embedding and only inputting the hidden states (ehidden ) results in further performance decline. This validates the effectiveness of adding text embeddings as input and adaptively Table 4: Ablation study on the read/write strategy with LLaMA-Omni2-7B. Offline means generating speech tokens only after receiving the complete input, and then synthesizing all speech tokens into waveform at once. fusing them with the gate fusion module. = 3, TTS Pretraining Our text-to-speech language model is initialized with the Qwen2.5-0.5B model and undergoes streaming TTS pretraining using text-speech pairs from speech dialogue data in = 10). We also explore Stage I(b) ( several other strategies, as shown in Table 3. Offline TTS refers to pretraining with the offline TTS task on top of Qwen2.5-0.5B, which shows slight performance drop compared to the streaming TTS pretraining. Text Pretrained refers to directly initializing with Qwen2.5-0.5B (with the extended vocabulary including speech tokens), and we observe significant performance decline. Scratch refers to randomly initialized model, whose loss fails to converge within short period. These experiments demonstrate the importance of pretraining for the TTS language model. and Read/Write Strategy The read/write strategies of the TTS language model is key factor influencing performance, primarily affecting the speech quality and system response latency. As shown in Table 4, we explore different combinations of . First, we observe that when = 3 and = 10, the ASR-WER is the lowest, indicating the best alignment between speech and text responses. As for the UTMOS score, we find that it is primarily determined by represents the chunk size of speech tokens input to the flow matching model, with larger chunk sizes leading to better speech quality. Regarding response latency, it is jointly determined by , as shown in Equation 6. Without any engineering optimizations, LLaMA-Omni2-7B can achieve latency below 500ms. We choose = 10 in our main experiments because it provides good trade-off across all aspects. = 3 and and , as W R 7 #Samples Multiturn SpokenQA (Accuracy) Speech Instruction Following Llama Questions Web Questions ChatGPT Score S2T S2T S2T S2S S2S S2S ASR-WER 200K 200K 150K 100K 50K 70.3 70.0 70.7 67.7 50.0 60.7 59.0 58.7 55.3 37.0 34.5 33.7 34.7 34.1 16.6 31.3 30.5 31.7 29.9 13. 4.28 4.11 4.23 4.19 3.02 4.15 3.98 4.10 4.07 2.84 3.26 3.28 3.71 4.45 5.42 Table 5: Results under different training data sizes with LLaMA-Omni2-7B. 5.3 Effects of the Training Data Sizes We explore the impact of different training data sizes on performance. As shown in Table 5, we first observe that, with the same number of training samples, multi-turn dialogue data consistently achieves better results across all benchmarks compared to single-turn dialogue data, highlighting the effectiveness of multi-turn dialogue data for training. Additionally, for different training data sizes, we observe that as the data size increases, the models performance improves, gradually stabilizing at 200K training samples. This indicates that our 200K multi-turn dialogue data is generally sufficient while ensuring efficient training."
        },
        {
            "title": "6 Related Work",
            "content": "With the rapid development of LLMs, SpeechLMs have gained widespread attention in recent years (Cui et al., 2024; Ji et al., 2024), aiming to endow LLMs with the ability to understand or generate speech. Generally speaking, SpeechLMs can be divided into two categories: native SpeechLMs and modular SpeechLMs. Native SpeechLMs refer to decoder-only Transformer models capable of directly inputting and outputting speech tokens. Some early works include SpeechGPT (Zhang et al., 2023, 2024a), AudioPaLM (Rubenstein et al., 2023), and TWIST (Hassid et al., 2024a). These models first convert speech into discrete tokens, then extend the vocabulary of pretrained LLMs to include these tokens, and finally train the LLMs using large amount of speech or speechtext pair data. Spirit-LM (Nguyen et al., 2024) and GLM-4-Voice (Zeng et al., 2025, 2024a) propose training models using speech-text interleaved data to encourage cross-modal knowledge transfer. Moshi (DÃ©fossez et al., 2024), OmniFlatten (Zhang et al., 2024b) and LSLM (Ma et al., 2024a) propose models capable of full-duplex conversations. IntrinsicVoice (Zhang et al., 2024c) proposes GroupFormer architecture to shorten speech length In contrast to nato be closer to that of text. tive SpeechLMs, modular SpeechLMs add speechrelated modules on top of LLMs. Early works achieve speech understanding tasks by combining speech encoders with LLMs, but are unable to perform speech generation (Wu et al., 2023; Wang et al., 2023; Chu et al., 2023; Yu et al., 2024; Ma et al., 2024b; Hono et al., 2024; Chen et al., 2024b; Tang et al., 2024; Chu et al., 2024; Fathullah et al., 2024). To achieve speech generation, LLaMAOmni (Fang et al., 2025), Freeze-Omni (Wang et al., 2024), and OpenOmni (Luo et al., 2025) add speech decoder after LLMs. Mini-Omni (Xie and Wu, 2024) and SLAM-Omni (Chen et al., 2024a) enable LLMs to generate speech tokens simultaneously while generating text tokens. The most related work to ours is the concurrent work Minmo (Chen et al., 2025), which also adopts an autoregressive streaming speech decoder similar to CosyVoice 2. In comparison, Minmo is trained on 1.4M hours of data, while we train on only few thousand hours of data, providing more efficient training solution. Additionally, we conduct detailed ablation studies on LLM sizes, read-write strategies, and model architecture to offer more comprehensive understanding of the model."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce LLaMA-Omni 2, series of speech language models ranging from 0.5B to 14B parameters, designed to enable real-time, high-quality speech interaction. LLaMA-Omni 2 achieves streaming speech generation by integrating an autoregressive text-to-speech language model and causal flow matching model. Experimental results on spoken question answering and speech instruction following tasks show that 8 LLaMA-Omni 2 outperforms previous state-of-theart speech language models, including LLaMAOmni and GLM-4-Voice. Additionally, LLaMAOmni 2 can achieve latency under 600ms, meeting real-time interaction requirements. We also conduct detailed ablation studies to understand the impact of various factors on overall performance. In the future, we will explore enhancing LLaMAOmni 2 to generate more human-like speech, incorporating features such as emotion and dialects."
        },
        {
            "title": "Limitations",
            "content": "One limitation of our model is that currently it cannot generate speech responses with different styles (such as emotion or speech rate) based on the content of the input speech or underlying paralinguistic information, as we have only trained on conventional speech-to-speech dialogue data. However, we believe this functionality can be achieved through data-driven approach, as our model is end-to-end trained and could acquire this capability after further training with suitable data. We plan to explore this in the future."
        },
        {
            "title": "Ethical Considerations",
            "content": "Since LLaMA-Omni 2 is built on LLMs, it carries some of the same risks as LLMs, such as the potential for factual errors or other hallucination issues in its outputs. We recommend that the models outputs be checked in practical use to ensure they comply with the required standards."
        },
        {
            "title": "References",
            "content": "Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, et al. 2024. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544, Seattle, Washington, USA. Association for Computational Linguistics. Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, et al. 2025. Minmo: multimodal large language model for seamless voice interaction. arXiv preprint arXiv:2501.06282. Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, et al. 2024a. Slamomni: Timbre-controllable voice interaction sysarXiv preprint tem with single-stage training. arXiv:2412.15649. Xi Chen, Songyang Zhang, Qibing Bai, Kai Chen, and Satoshi Nakamura. 2024b. LLaST: Improved endto-end speech translation system leveraged by large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 6976 6987, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models. arXiv preprint arXiv:2311.07919. Leigh Clark, Philip Doyle, Diego Garaialde, Emer Gilmartin, Stephan SchlÃ¶gl, Jens Edlund, Matthew Aylett, JoÃ£o Cabral, Cosmin Munteanu, Justin Edwards, and Benjamin Cowan. 2019. The state of speech in HCI: Trends, themes and challenges. Interacting with Computers, 31(4):349371. Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. 2024. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751. Alexandre DÃ©fossez, Laurent MazarÃ©, Manu Orsini, AmÃ©lie Royer, Patrick PÃ©rez, HervÃ© JÃ©gou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speechtext foundation model for real-time dialogue. Technical report. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. 2024. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2025. LLaMA-omni: 9 Seamless speech interaction with large language models. In The Thirteenth International Conference on Learning Representations. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke Li, Junteng Jia, Yuan Shangguan, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, and Mike Seltzer. 2024. Audiochatllama: Towards general-purpose speech abilities for llms. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 55225532. Alex Graves, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen Schmidhuber. 2006. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML 06, page 369376, New York, NY, USA. Association for Computing Machinery. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. 2024a. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. 2024b. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36. Yukiya Hono, Koh Mitsuda, Tianyu Zhao, Kentaro Mitsui, Toshiaki Wakatsuki, and Kei Sawada. 2024. Integrating pre-trained speech and language models for end-to-end speech recognition. In Findings of the Association for Computational Linguistics ACL 2024, pages 1328913305, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, et al. 2024. Wavchat: survey of spoken dialogue models. arXiv preprint arXiv:2411.13577. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-gan: Generative adversarial networks for efIn Adficient and high fidelity speech synthesis. vances in Neural Information Processing Systems, volume 33, pages 1702217033. Curran Associates, Inc. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Shijia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang, Rongzhi Zhou, and Yijin Xing. 2024. Fish-speech: Leveraging large language models for advanced multilingual text-to-speech synthesis. Preprint, arXiv:2411.01156. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. 2023. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations. Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, et al. 2025. Openomni: Large language models pivot zero-shot omnimodal alignment across language with real-time selfaware emotional speech synthesis. arXiv preprint arXiv:2501.04561. Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. 2024a. Language model can listen while speaking. arXiv preprint arXiv:2408.02622. Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, et al. 2024b. An embarrassingly simple approach for llm with strong asr capacity. arXiv preprint arXiv:2402.08846. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. 2024. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations. Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. 2024. Spoken question answering and speech continuation using spectrogram-powered LLM. In The Twelfth International Conference on Learning Representations. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, PaulAmbroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, et al. 2024. Spirit-lm: Interleaved spoken and written language model. arXiv preprint arXiv:2402.05755. Open-Moss. 2025. Speechgpt 2.0-preview. https://github.com/OpenMOSS/SpeechGPT-2. 0-preview. OpenAI. 2022. Introducing chatgpt. OpenAI. 2024. Hello gpt-4o. Alec Radford. 2018. Improving language understanding by generative pre-training. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. 10 Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. 2024a. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. Preprint, arXiv:2412.02612. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, and Jie Tang. 2024b. Scaling speech-text pre-training with synthetic interleaved data. Preprint, arXiv:2411.17607. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, shengmin jiang, Yuxiao Dong, and Jie Tang. 2025. Scaling speech-text pre-training with synthetic interleaved data. In The Thirteenth International Conference on Learning Representations. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1575715773, Singapore. Association for Computational Linguistics. Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, and Xipeng Qiu. 2024a. Speechgpt-gen: Scaling chain-of-information speech generation. arXiv preprint arXiv:2401.13527. Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chaohong Tan, Zhihao Du, et al. 2024b. Omniflatten: An end-to-end gpt model for seamless voice conversation. arXiv preprint arXiv:2410.17799. Xin Zhang, Xiang Lyu, Zhihao Du, Qian Chen, Dong Zhang, Hangrui Hu, Chaohong Tan, Tianyu Zhao, IntrinYuxuan Wang, Bin Zhang, et al. 2024c. sicvoice: Empowering llms with intrinsic realarXiv preprint time voice interaction abilities. arXiv:2410.08035. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, ZalÃ¡n Borsos, FÃ©lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925. Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. 2022. Utmos: Utokyo-sarulab system for voicemos challenge 2022. In Interspeech 2022, pages 45214525. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. 2024. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Qwen Team. 2024. Qwen2.5: party of foundation models. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and Jiajun Zhang. 2023. Blsp: Bootstrapping language-speech pre-training via behavior alignment of continuation writing. arXiv preprint arXiv:2309.00916. Xiong Wang, Yangze Li, Chaoyou Fu, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long Ma. 2024. Freeze-omni: smart and low latency speech-toarXiv speech dialogue model with frozen llm. preprint arXiv:2411.00774. Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023. On decoder-only architecture for speech-to-text and large language model integration. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Zhifei Xie and Changqiao Wu. 2024. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725. Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2024. Connecting speech encoder and large language model for asr. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1263712641. IEEE."
        },
        {
            "title": "A Prompt",
            "content": "Prompt for ChatGPT Scoring (Model: GPT-4o) need your help to evaluate the performance of several models in speech interaction scenario. The models receive the users speech input and respond with speech output. For evaluation purposes, both the users speech input and the models speech response have been transcribed into text using Automatic Speech Recognition (ASR). Your task is to rate the models responses based on the provided user input transcription [Instruction] and the models output transcription [Response]. Please consider factors such as helpfulness, relevance, fluency, and suitability for speech interaction in your evaluation, and provide single score on scale from 1 to 5. Below are the transcription of users instruction and models response: ### [Instruction]: {instruction} ### [Response]: {response} After evaluating, please output the scores in JSON format: {score: ...}. You dont need to provide any explanations."
        },
        {
            "title": "B Detailed Latency",
            "content": "We list the detailed latency at different stages of the model in Table 6. LLM refers to the latency text tokens, TTS refers for generating the first to the latency for generating the first speech tokens, and FM+Voc refers to the latency for generating the first speech chunk using the flow matching model and vocoder. 12 Model LLM TTS FM+Voc Total Latency (ms) LLaMA-Omni2-0.5B 3 LLaMA-Omni2-1.5B 3 3 LLaMA-Omni2-3B LLaMA-Omni2-7B 3 3 LLaMA-Omni2-14B LLaMA-Omni2-7B LLaMA-Omni2-7B LLaMA-Omni2-7B LLaMA-Omni2-7B LLaMA-Omni2-7B LLaMA-Omni2-7B 1 2 3 3 4 5 10 10 10 10 10 5 10 10 15 15 20 190.95 201.01 216.08 231.16 311.56 185.93 206.03 231.16 231.16 251.26 271.36 165.83 165.83 165.83 165.83 165. 85.43 165.83 165.83 246.23 246.23 336.68 185.93 185.93 185.93 185.93 185.93 185.93 185.93 185.93 185.93 185.93 190.95 542.71 552.76 567.84 582.91 663.32 457.29 557.79 582.91 663.32 683.42 798.99 Table 6: Detailed latency of LLaMA-Omni2 series models."
        }
    ],
    "affiliations": [
        "Key Laboratory of AI Safety, Chinese Academy of Sciences",
        "Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}