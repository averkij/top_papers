{
    "paper_title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "authors": [
        "Yuqiao Tan",
        "Minzheng Wang",
        "Shizhu He",
        "Huanxuan Liao",
        "Chengfeng Zhao",
        "Qiunan Lu",
        "Tian Liang",
        "Jun Zhao",
        "Kang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO."
        },
        {
            "title": "Start",
            "content": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Yuqiao Tan * 1 2 Minzheng Wang * 1 2 Shizhu He 1 2 Huanxuan Liao 1 2 Chengfeng Zhao 1 2 Qiunan Lu 3 Tian Liang 4 Jun Zhao 1 2 Kang Liu 1 2 https://github.com/Trae1ounG/BuPO 5 2 0 2 2 2 ] . [ 1 3 7 6 9 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLamas prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. *Equal contribution , Corresponding author 1Institute of Automation, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3University of Electronic Science and Technology of China 4Tencent AI Lab. Main contributors: Yuqiao Tan <tanyuqiao2025@ia.ac.cn>, Minzheng Wang <wangminzheng2023@ia.ac.cn>. Preprint. 1 Figure 1. (a): The residual stream within Transformer which moves from previous layer hidden states into self-attention and feed-forward network (FFN) sequentially. (b): Any hidden states with unembedding matrix Eu can be transformed into probability distribution over the vocabulary space, which can be considered as the samplable policy. (c): We surprisingly find that Qwen-series contains progressive reasoning pattern in FFN, where start from exploration expansion to integrate middle layer knowledge into final prediction convergence, specially in Qwen3. 1. Introduction Reinforcement learning (RL) has emerged as key driver in advancing the complex reasoning capabilities of large language models (LLMs). Notably, the success of DeepSeek R1 (Guo et al., 2025) has solidified reinforcement learning with verifiable rewards (RLVR) as potent post-training paradigm for enhancing LLM policies across diverse domains (Jaech et al., 2024; Yang et al., 2025a; Team et al., 2025; Ouyang et al., 2022). To date, most existing RLVR research has predominantly focused on surface algorithmic Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies design, such as reward construction (Shao et al., 2025; Chen et al., 2025b;a; Liu et al., 2025b) and entropy regularization (Cui et al., 2025; Yu et al., 2025; Yang et al., 2025b). Interpretability tools mitigate the opacity of black-box LLMs by unveiling their internal logic (Belrose et al., 2023; Tan et al., 2025a; Gupta et al., 2025). These insights establish feedback loop that is instrumental in guiding the refinement of RL algorithms. For instance, recent works leverage the interpretability tool of attention mechanisms to improve RL algorithm which show surprising effect (Li et al., 2025; Liu et al., 2025a). However, they overlook the nature of the language model policy as well as the information latent in the models internal residual stream. The logit lens (nostalgebraist, 2020) framework offers initial insights by employing the unembedding matrix Eu to decode intermediate layer representations into the token space. This approach reveals that the internal residual stream harbors wealth of previously undiscovered information that evolves across layers and modules (Dai et al., 2022; Gupta et al., 2025; Lindsey et al., 2025). Moreover, numerous studies have elucidated the mechanisms of self-attention and feed-forward networks (FFNs) and their impact on hidden states (Dai et al., 2022; Yu & Ananiadou, 2023; Jin et al., 2025). Collectively, these internal mechanisms with models offer new perspective for algorithmic optimizations. In this paper, we investigate the evolution of language model policies across layers and modules to facilitate optimization and unravel complex internal reasoning mechanisms. Our formulation is grounded in two key insights. First, the residual stream naturally supports additive decomposition (Zhang et al., 2025; Lindsey et al., 2025), allowing us to isolate the individual roles of each layer and module (Figure 1(a)). Second, we conclude that the samplable policy is intrinsic equivalent to the token distribution derived from the combination of hidden states with unembedding matrix Eu. Based on these, we construct the Internal Layer Policy πl Layer, which captures cumulative reasoning up to layer l, and the Internal Modular Policy πl FFN, which isolates the specific contributions of attention and FFN modules (Figure1(b)). This decomposition allows us to ask: How does internal reasoning evolve through the model? ATTN and πl Through systematic analysis of commonly used Qwen and Llama series (Meta AI, 2024; Yang et al., 2024; 2025a) based on Internal Policy Entropy in policy-centric view, we uncover both universal and critical architectural differences: (1) Consistent internal reasoning structure. All models exhibit universal reasoning structure: early layers maintain high entropy for exploring the solution space, while top layers converge to near-zero entropy for final prediction (Lindsey et al., 2025). (2) Distinct internal reasoning pattern. Despite the shared trend, the pace of convergence differs significantly. Llama exhibits sudden con2 vergence, where the prediction space collapses only within the last three layers. In contrast, Qwen models demonstrate progressive contraction, gradually reducing uncertainty throughout the layers. To quantify these dynamics, we introduce Internal Entropy Change H. This metric reveals that while Llama shows minimal intermediate updates, Qwen (especially Qwen3) utilizes FFNs in progressive, human-like cognitive process (Dehaene et al., 1998): expanding exploration in lower layers, integrating parametric knowledge in intermediate layers (Dai et al., 2022), and compacting predictions in upper layers (Figure 1(c)). These findings have profound implications for RL optimization: Since internal reasoning emerges progressively from lower to higher, we can consider optimization from bottom-up perspective. We initially validate this hypothesis through targeted optimization experiments on internal policies, revealing distinct training dynamics and remarkable phenomenon of internal reasoning feature refinement. Specifically, the optimized lower layers capture high-level reasoning capabilities by early alignment, providing more robust foundation for subsequent internal reasoning. Motivated by these insights, we propose Bottom-up Policy Optimization (BuPO), novel reinforcement learning paradigm that optimizes fine-grained internal layer policies during the early stages of training to effectively guide the overall language model policy. By doing so, BuPO reconstructs foundational reasoning abilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrate the effectiveness of our approach and the unique training dynamics compared to conventional RL methods that optimize the policy as whole. We hope our detailed analysis of language model policies provides valuable insights to improve RL algorithms. In summary, this paper makes the following contributions: Internal Policy Decomposition: First, we formally define and decompose language model policies into internal layer and modular policies, revealing their distinct roles in reasoning. Internal Policy Entropy-based Analysis: Building on this foundation, we uncover consistent yet distinct reasoning patterns across different model series, highlighting progressive reasoning structures in Qwen models. Bottom-up Policy Optimization: Inspired by these findings, we propose BuPO, novel RL paradigm that optimizes internal policies early in training, achieving superior performance and unique training behaviors. 2. Preliminary In this section, we aim to introduce the basic definition helps to understand the decomposition of language model policy. Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies 2.1. The Information Flow in Transformer Transformer-based language models (Vaswani et al., 2017) form the foundation of modern LLMs (Brown et al., 2020). decoder-only Transformer consists of stacked layers, each containing multi-head self-attention (MHSA) module and feed-forward network (FFN) module. Following Yu & Ananiadou (2023); Zhang et al. (2025), we formalize the forward process from input to output. Given an input sequence = [x1, x2, . . . , xT ], the model produces probability distribution over the vocabulary with tokens. Let H(2l2) RT dmodel denote the hidden state input to the l-th layer, where is the sequence length and dmodel is the hidden dimension. The initial embedding is H(0) projected by E, where RN dmodel is the embedding matrix. Each layer processes information sequentially through attention and FFN: Al = MHSA(LN(H(2l2))), H(2l1) = H(2l2) + Al, Fl = FFN(LN(H(2l1))), H(2l) = H(2l1) + Fl, (1) where LN() denotes layer normalization, and Al, Fl represent the attention and FFN outputs, respectively. After layers, the final hidden states are projected to vocabulary logits: = softmax(LN(H(2L))ET ), (2) where Eu RN dmodel is the unembedding matrix, and RT denotes the output probability distribution. 2.2. Reinforcement Learning for Language Model Policy Language model generation can be formulated as tokenlevel Markov Decision Process (MDP). At each step t, the state st = [q; o<t] consists of the input question and generated tokens so far. The language model policy πθ(st) samples the next token ot from vocabulary , transitioning to st+1 = [st; ot]. Generation terminates upon producing [eos] or reaching the budget. To optimize the policy, we maximize: which samples group of responses {o1, . . . , oG} per question and estimates advantages as ˆAi,t = Rimean(R) : std(R) JGRPO(πθ) = qQ,{oi}G i=1πθold (q)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:110) min (cid:104) ri,t ˆAi,t, clip(ri,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:105)(cid:111) , (4) where ri,t = πθ(oi,tsi,t) πθold (oi,tsi,t) is the importance ratio. 3. Language Model Policy Secretly Contains"
        },
        {
            "title": "Internal Policies",
            "content": "In this section, we introduce our key insight: the language model policy secretly contains internal policies. We provide implementation details of this section in Appendix A.4. 3.1. Definition of Internal Policy Residual stream. Due to residual connections in Transformer layers, the input to any sub-module (attention or FFN) equals the sum of all preceding outputs plus the original embedding. Let Al = Al(Xl ffn) denote attention and FFN transformations (excluding normalization). The hidden states satisfy: attn) and Fl = l(Xl Hl = H(2l) = H(0) + (cid:88) i=1 Ai + (cid:88) j=1 Fj, (5) where we denote Hl for simplicity as the output hidden states in layer same as H(2l). According to this, the output of final layer can be regard as the combination of previous hidden states by HL = H0 + (cid:80)L i=1 Ai + (cid:80)j j=1 Fj. Internal policy. During reinforcement tuning, we sample the next token ot from the final layers probability distribution, i.e., πθ = softmax(LN(HL)ET ). We propose that every internal hidden state can combine with Eu to produce samplable internal policy. Specifically, we focus on two granularities. Internal Layer Policy refers to utilize hidden states from each layer Hl to combine with Eu, and Internal Modular Policy integrates Eu with hidden states from specific module: (πθ) = (cid:20) qQ oπθ(q) (cid:21) [R(q, o)] βDKL[πθ(q))πref(q)] , (3) where R(q, o) = (cid:80)o t=1 r(st, ot) is the return (Sutton et al., 1998) and πref is reference policy. We adopt sparse rewards where rt = 0 for < and rn [0, 1] indicates task success. Following Hu et al. (2025a), we will assume β = 0 throughout this paper. Policy Optimization. We adopt GRPO (Shao et al., 2024), Layer = softmax(HlET ), Layer Pl πl (cid:40) πl Module Pl Pl ATTN = softmax(AlET FFN = softmax(FlET ). ), for ATTN for FFN (6) (7) Each component contributes to the final policy through the residual stream. For instance, HL = Hl + Sl+1, where Sl+1 = (cid:80)L j=l+1 Fj represents contributions from subsequent layers. Hence, understanding these internal components is essential for raveling how does reasoning emerge and evolve through the model. i=l+1 Ai + (cid:80)L Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Figure 2. Continuous entropy dynamics of internal policy for different models. The residual stream flows from Hl1 into Al, Fl, and finally to the next layer Hl. 3.2. Internal Policy Entropy Dynamics In contrast to prior logit lens approaches (nostalgebraist, 2020; Belrose et al., 2023) that decode internal states into discrete tokens, we adopt policy-centric perspective where internal probability distributions can be sampled and updated like policies. We employ entropy as our primary metric, motivated by its strong correlation with policy behavior in RL (Cui et al., 2025; Agarwal et al., 2025; Cheng et al., 2025). We define Internal Policy Entropy as: Layer = (cid:88) j=1 Pl Layer,j log(Pl Layer,j), (8) where denotes the vocabulary size and we can obtain ATTN in the same way. FFN and Continuous entropy dynamics. Figure 2 shows that internal policy entropy dynamics exhibit consistent patterns across models: early layers maintain high entropy for exploration of the search space, while top layers converge to near-zero entropy. This aligns with findings that lower layers capture semantic information while higher layers aggregate and refine these representations to drive final decision-making (Lindsey et al., 2025). While the overall entropy pattern is consistent across models, the fine-grained module-wise transition dynamics vary. To isolate intrinsic patterns from normalization and residual effects (Zhang & Sennrich, 2019; He et al., 2016)), we introduce Entropy Change, which measures the incremental information gain within single internal policy and is defined as: = Output Input (9) where the entropy change is defined as the difference between the internal policy entropy at the input and output of 4 given module, e.g., FFN. This quantity characterizes how the exploration space evolves as information propagates through the module. Specifically, FFN > 0 indicates an expansion of exploration, FFN 0 suggests internal knowledge integration, and FFN < 0 reflects prediction convergence during the reasoning process. Entropy change dynamics of attention vs. FFN. Selfattention modules (Vaswani et al., 2017) are widely regarded as central to model reasoning, particularly for integrating task-relevant contextual information (Jin et al., 2025; Liu et al., 2025a). The upper panel of Figure 3 reveals clear and model-dependent pattern in the entropy change of self-attention. Specifically, Qwen3 models exhibit consistently positive entropy change across layers (H ATTN > 0), indicating sustained expansion of the exploration space during reasoning, consistent with prior findings (Li et al., 2025; Lindsey et al., 2025; Zhou et al., 2024). In contrast, Qwen2.5-Math-7B shows uniformly negative entropy change, suggesting progressive contraction and earlier convergence. Llama models display weaker but still positive trend, reflecting more conservative exploration dynamics. In contrast, the FFN module is widely regarded as the key-value memories of parametric knowledge in Transformers (Geva et al., 2021; Meng et al., 2022; Dai et al., 2022). The lower panel of Figure 3 reveals clear and systematic differences in FFN entropy dynamics across model families. For the Llama-series models (Meta AI, 2024), FFN entropy remains consistently positive across almost all layers, with convergence occurring only at the final layer. This pattern indicates sustained exploration throughout the FFN, with limited intermediate consolidation, which aligns with prior observations that Llama models benefit less from posttraining and require additional mid-training interventions to Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Figure 3. Entropy Change Dynamics of Internal Policy. The entropy change across layers represents the uncertainty of current policys hidden exploration space. positive > 0 indicates increasing exploration, 0 signifies exploitation of existing knowledge, and < 0 suggests tendency toward convergence within the reasoning process. improve reasoning behavior (Wang et al., 2025b). By contrast, the Qwen-series models (Yang et al., 2024; 2025a) exhibit pronounced hierarchical entropy structure in the FFN, following clear three-stage progression. Taking Qwen3-4B as an illustrative example, the lower FFN layers (layers 16) show increasing entropy (H FFN > 0), corresponding to expanded exploration at the onset of reasoning. This is followed by broad middle region (layers 726) where entropy change remains approximately zero, indicating stable information integration through retrieval and reuse of parametric knowledge encoded in knowledge neurons (Dai et al., 2022). In the upper layers (layers 2736), entropy decreases (H FFN < 0), reflecting gradual convergence toward the final prediction. Figure 1(c) provides vivid demonstration. Notably, this ExplorationIntegrationConvergence (EIC) pattern is more consistently expressed in Qwen3 than in earlier Qwen2.5 variants, indicating more structurally stabilized and progressive reasoning pattern. Rather than maintaining continuous exploration across all layers, Qwen3 allocates distinct FFN depth ranges to different reasoning functions, resulting in staged reasoning process. We hypothesize that this progressive reasoning pattern, qualitatively similar to the staged nature of human reasoning (Dehaene et al., 1998; Felleman & Van Essen, 1991; Yamins & DiCarlo, 2016), may help explain why Qwen3 exhibits more efficient knowledge absorption during post-training (Zhu et al., 2025; Yang et al., 2025a). How do Internal modules influence the residual stream? To further understand how internal modules shape the residual stream in Qwen models with progressive reasoning pattern, we analyze residual cosine similarity, which quantifies how each module writes to the residual pathway (Hu et al., Figure 4. Residual cosine similarity across different Qwen models 2025b). For given layer l, we compute cossim(Al, Hl1) for self-attention and cossim(Fl, Hl1+Al) for the FFN. cosine similarity near zero indicates writing new, orthogonal features; negative values indicate feature suppression; and positive values indicate amplification of existing features. As shown in Figure 4, the Qwen models largely follow the entropy dynamics discussed earlier, while exhibiting clear inter-generation differences. For Qwen3, self-attention consistently amplifies the residual stream, in line with its positive entropy change and expanded exploration behavior. In contrast, Qwen2.5 shows noticeably weaker attention write-in strength, with reduced cosine similarity magnitudes, consistent with its negative entropy change in self-attention. The FFN modulates the residual stream in stage-dependent 5 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Figure 5. Training dynamics of internal policy. Effects of varying the optimized policy on (a) reward, (b) entropy of language model policy πθ, (c) response length. The backbone model is Qwen3-4B. Figure 6. Analysis of internal policy optimization. (a) Similarity between the hidden states of optimized layer 6 and the higher layers. (b) Entropy Change 6 Layer. (c) The PPL trend of the language model policy πθ. The backbone model is Qwen3-4B. Layer of the optimized π6 manner across all models: in lower layers, it injects largely orthogonal features to support exploration; in middle layers, it suppresses vague signals while integrating parametric knowledge in FFN, corresponding to the Integration stage; and in upper layers, it amplifies andintegrates features to drive convergence. Across all models, the final layer exhibits sharp directional shift, underscoring its critical role in final prediction (Gupta et al., 2025; Agarwal et al., 2025). TAKEAWAY Build on the definition of internal policy and corresponding entropy, we reveal the following findings: Llama models exhibit limited exploration in both self-attention and FFN modules, leading to constrained exploration in internal reasoning. Qwen-series display staged, progressive reasoning pattern, with the FFN following an ExplorationIntegrationConvergence process. Qwen3 shows more stable FFN reasoning structure than Qwen2.5, and stronger attention amplification to the residual stream, enabling richer information gain. 4. Internal Policy Optimization Building on our previous analysis, we observe that reasoning emerges progressively, with different internal policy regions serving distinct roles and exhibiting varied behaviors. This raises key question: Can we approach optimization from bottom-up perspective to align with this progressive emergence? To address this, we extend GRPO which originally designed for language model policy optimization to directly optimize these internal policies: JInterGRPO(πθ, πl Layer) = qQ,{oi}G i=1πθold (q) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:110) min (cid:104) ˆri,t ˆAi,t, clip(ˆri,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:105)(cid:111) (10) πl Layer(oi,tq,oi,<t) where we still sample from πθold , ˆri,t = πl Layer,old(oi,tq,oi,<t) is the modified importance ratio for current optimizing policy πl Layer and keep other hyperparameters same with E.q. 4. We present the implementation details in Appendix A.5. Training setup. Based on the findings in Sec. 3, we select Qwen3-4B (Yang et al., 2025a) (non-thinking mode) for investigation. For the training set, we randomly sample 5k entries from DeepMath-103k (He et al., 2025). We train the models using the verl framework (Sheng et al., 2025). 6 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies The prompt batch size is 128, with 8 rollouts generated per prompt. The sampling temperature during training is set to 1.0, and the maximum context length is set to 9,216 tokens. We update the model with mini-batch size of 32 for 300 steps and learning rate of 1e-6. For Qwen3-4B, the region boundaries identified in Sec. 3.2 lie at layers 6 and 26. Additionally, we focus on aligning the internal policy of the penultimate layer, which is critical for the final prediction. Accordingly, we compare the internal policies π6 Layer, and π35 Layer optimized via InterGRPO, against the overall policy πθ optimized with GRPO (He et al., 2025). Layer, π26 Different training dynamics of internal policy. As shown in Figure 5, both internal layer policies collapse after 50 training steps, suggesting that optimizing internal policies alone is fundamentally misaligned with the objective of the language model policy πθ. Nevertheless, distinct patterns emerge. The entropy of πθ rapidly converges to zero under standard RL training (Cui et al., 2025), well-known phenomenon encountered by various methods (Peng et al., 2025; Zhu et al., 2025; Dong et al., 2025). For the penultimate layer policy π35 Layer, entropy shows minor fluctuations before aligning with πθ. But it suffers from severe repetition causing excessively long responses, likely because final decision-making is confined to the last layer (Gupta et al., 2025). In contrast, the knowledge integration layer policy π26 Layer exhibits unstable and increasing entropy, also resulting in longer outputs. The lower-layer internal policy π6 Layer maintains more stable entropy growth, with response lengths converging closer to those of πθ. Analysis of internal policy optimization. Given the distinct training dynamics, we further investigate the underlying mechanism of internal policy optimization. Surprisingly, we find that this process induces significant feature refinement in the models internal states. Taking layer 6 π6 Layer for illustration, Figure 6 (a) shows that as optimization progresses, the similarity between its hidden states H6 and the final layer representations increases. This suggests that internal optimization forces the bottom layer to preemptively capture high-level reasoning information, providing more robust foundation for subsequent internal reasoning. While the entropy change dynamics in Figure 6(b) indicate that the optimized internal policy progressively converges, the PPL trajectory in Figure 6(c) reveals clear trade-off: excessive alignment leads to performance collapse, suggesting that only limited number of optimization steps are optimal. 5. Bottom-up Policy Optimization All prior RL methods for LLMs optimize the language model policy in holistic manner (Ouyang et al., 2022; Rafailov et al., 2023; Guo et al., 2025). In Sec. 3, we decompose the final output HL into an intermediate representation Hl and the subsequent residual contribution Sl+1. This decomposition suggests that aligning the internal policy πl Layer associated with Hl may facilitate alignment of the overall policy πθ. We empirically support this intuition in Sec. 4, where we optimize the internal policy alone and observe pronounced feature refinement in lower layers. These findings further motivate bottom-up alignment strategy: by aligning finer-grained internal policies first, we explore whether the overall policy can be guided to reason more effectively. To this end, we propose Bottom-up Policy Optimization (BuPO), which sequentially optimizes the internal layer policies πl Layer followed by the language model policy πθ. The overall training objective is: (cid:40) Layer), Layer) = JBuPO(πθ, πl JInterGRPO(πθ, πl JGRPO(πθ), scur sinter scur > sinter (11) where scur denotes the current training step, and sinter specifies the number of training steps of the internal layer policy. We present the detailed training algorithm in Appendix A.1. setup. Qwen3 focus pattern, Specifically, we which maintain Training on stable series, the usExploration-Integration-Convergence ing Qwen3-4B and Qwen3-8B. For comparison, select Llama-OctoThinker-3B-Base and we Llama-OctoThinker-8B-Base from the Llama series, as these models demonstrate improved RL training performance after additional mid-training based on Llama-3.2-Base (Wang et al., 2025b). The maximum context length for Llama is set to 4096 tokens. We select the last FFN layer that exhibits positive exploration signal (i.e., Layer. Other RL training settings follow Sec. 4 and summarized in Table 5. FFN > 0) as πl Evaluation setup. We evaluate our proposed BuPO against several RL baselines, including GRPO, PPO (Sutton et al., 1998), Reinforce++ (Hu, 2025) and RLOO (Ahmadian et al., 2024). The evaluation benchmarks cover diverse reasoning tasks: MATH (Lightman et al., 2023), AMC23 (MAA, 2023), AIME24, and AIME25 (MAA, 2024; 2025). We use vLLM (Kwon et al., 2023) with temperature 1.0 and top_p 1.0. Due to high output variance in reasoning tasks, we report Avg@K (Pass@1 averaged over outputs). For smaller datasets (AIME24/25), we set = 32, and for others K=16. Additionally, we evaluate an unbiased Pass@K metric with up to 256 to obtain comprehensive evaluation. This metric is defined as Pass@K := (cid:1)(cid:3), where denotes the number of ExD correct completions out of generated responses. To reduce evaluation variance on those datasets, we set = 300. (cid:2)1 (cid:0)nc (cid:1)/(cid:0) 5.1. Main Results We report the Avg@k results in Table 1. Surprisingly, adopting bottom-up optimization perspective in BuPO 7 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Table 1. Avg@K results on MATH500, AMC23, AIME24 and AIME25. Bold and unserlined numbers denote the best an second-best results for each K. We select the last FFN layer that exhibits positive exploration signal (i.e., FFN > 0) as πl Layer. Methods Qwen3-4B Base Model PPO Reinforce++ RLOO GRPO BuPO (π6 Layer πθ, sinter = 30) Qwen3-8B Base Model PPO Reinforce++ RLOO GRPO BuPO (π6 Layer πθ, sinter = 20) Llama-OctoThinker-3B-Base Base Model PPO Reinforce++ RLOO GRPO BuPO (π27 Layer πθ, sinter = 20) Llama-OctoThinker-8B-Base Base Model PPO Reinforce++ RLOO GRPO BuPO (π31 Layer πθ, sinter = 20) AMC (Avg@16) MATH500 (Avg@16) AIME24 (Avg@32) AIME25 (Avg@32) 11Average11 67.66 77.03 63.44 77.66 76.88 81.09+4.21 67.34 87.03 82.66 86.41 85.94 89.22+3.28 1.24 22.19 9.38 27.03 27.50 27.50+0.00 4.53 31.72 34.69 27.66 34.84 37.66+2. 80.29 83.64 80.63 82.73 82.41 84.90+2.49 80.46 86.20 86.05 87.32 88.05 87.76 5.26 43.23 11.59 41.93 46.07 49.79+3.72 9.84 56.97 59.55 55.97 56.89 62.05+5.16 23.20 32.60 17.40 30.83 32.19 36.88+4.69 26.98 37.81 41.77 46.67 49.48 54.06+4. 0.21 1.04 0.00 2.19 0.63 0.63+0.00 0.52 1.56 7.72 3.54 2.50 4.69+2.19 18.60 27.60 18.65 24.79 28.85 31.15+2.30 19.17 22.60 31.15 33.02 33.54 34.38+0.76 0.00 0.31 0.10 0.21 0.10 0.42+0.32 0.10 1.04 3.75 1.56 2.19 6.77+4. 47.44 55.22 45.03 54.00 55.08 58.51+3.43 48.49 58.41 60.41 63.36 64.23 66.36+2.13 1.68 16.69 5.27 17.84 18.58 19.59+1.01 3.75 22.82 26.43 22.18 24.11 27.79+3.68 Figure 7. Average Pass@K results on MATH500, AMC23, AIME24 and AIME25. To reduce evaluation variance, we set = 300. leads to consistent improvements over RL baseline algorithms across benchmarks and models, achieving superior average performance. On the Qwen3-4B model, BuPO yields gains of 4.58 points on AIME24 and 0.76 points on AIME25 compared to GRPO. Similarly, the Qwen-8B model shows improvements of 3.65 points on AIME24 and 2.40 points on AIME25. The Llamaseries models exhibit similar optimization trend under BuPO, achieving an average improvement of 1.01 points on Llama-OctoThinker-3B-Base and 3.68 points on Llama-OctoThinker-8B-Base. Overall, these results suggest that aligning finer-grained internal policies in the early stages of training can effectively guide the language model policy toward improved reasoning. For more comprehensive evaluation, we report the averaged Pass@K results of BuPO with GRPO on MATH500, AMC23, AIME24, and AIME25, with ranging from 1 to 256. As shown in Figure 7, BuPO consistently achieves favorable trade-off across wide range of values. On Qwen3-8B, BuPO attains the best performance for all values, while on Qwen3-4B, the only exception occurs at = 256. For all Llama-series models, BuPO achieves the best results across all values, yielding gains of 7.48 points on Llama-OctoThinker-3B-Base and 7.93 points on Llama-OctoThinker-8B-Base in averaged Pass@256. These results further indicate that bottom-up optimization effectively enhances the reasoning capacity of the language model policy. 8 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Table 2. Ablation study of sinter and πl Qwen3-4B. Layer. The backbone model is Methods GRPO Layer = π6 Fix πl Layer: BuPO (sinter = 30) BuPO (sinter = 50) BuPO (sinter = 70) Fix sinter = 30: BuPO (π6 BuPO (π26 BuPO (π35 Layer πθ) Layer πθ) Layer πθ) AMC Avg@16 MATH500 AIME24 AIME25 Avg@32 Avg@32 Avg@16 Average 76. 82.41 32.19 28.85 55.08 81.09 62.66 14.14 81.09 82.19 82. 84.90 79.00 25.20 84.90 85.38 85.14 36.88 14.17 0.21 36.88 37.50 35.42 31.15 12.60 0.00 31.15 33.65 30. 58.51 42.11 9.89 58.51 59.68 58.34 the performance of BuPO. As shown in the upper panel of Table 2, as the bottom alignment steps increase, the performance of the language model policy drops dramatically. This aligns with the finding in Figure 6 and further supports our conclusion in Sec. 5.2 that only moderate bottom optimization can boost overall policy learning effectively. Ablation of optimized internal policy. We further investigated the impact of the optimized internal policy following Sec.4, with focus on the region boundaries highlighted in our takeaways. As shown in the lower panel of Table 2, with fixed bottom optimization steps, the bottom-up optimization strategy achieves superior performance compared to GRPO. The entropy dynamics in Figure 8 indicate that the alignment of π26 Layer induces an entropy spike, leading to significant exploration expansion, aligning with its strong performance. The selection and investigation of internal policies remain promising directions for future research. 6. Related Work 6.1. Reinforcement Learning with Verifiable Rewards Reinforcement learning (RL) has proven effective for enhancing large language models (LLMs), most notably through Reinforcement Learning from Human Feedback (RLHF), which aligns model outputs with human preferences (Sutton et al., 1998; Rafailov et al., 2023; Jaech et al., 2024; Hu, 2025; Wang et al., 2025a). More recently, Reinforcement Learning with Verifiable Rewards (RLVR) has gained traction for its ability to foster reasoning in LLMs using rule-based rewards (Li et al., 2025; Peng et al., 2025; Tan et al., 2025b). Despite these advances, prior work has primarily focused on optimizing the overall language model policy, overlooking the rich information latent within its internal residual streams. In this work, we shift the focus inward to analyze these streams, constructing internal policies that reveal consistent and distinct reasoning structures across different models. Building on this finding, we introduce bottom-up optimization strategy that directly optimizes internal policies at intermediate layers, strengthening Figure 8. Entropy dynamics during training with GRPO and BuPO with different internal policy. 5.2. Analysis Training dynamics of BuPO. We further visualize the training dynamics of BuPO following the settings in Sec. 5.1. As shown in Figure 8, by training the internal layer policy at an early stage, all models exhibit enhanced entropy exploration initially. For Qwen models, optimizing the first region boundary layer 6 maintains stable exploration, consistent with Figure 5. For the Llama-series, we observe consistent increase during the bottom alignment stage, indicating that feature refinement in lower layers effectively provides larger exploration space for language model policy. Moderate bottom optimization boosts learning. In Sec. 4, we investigate the optimization of the internal layer policy. The remarkable effectiveness of this approach further validates our bottom-up policy optimization strategy. First, early stage internal optimization compels the bottom layers to preemptively capture high-level reasoning information, establishing better foundation for subsequent learning (Figure 6 (a,b)). Second, excessive training steps lead to model collapse, as the mismatch between the rollout policy and the optimized policy results in increased PPL (Figure 6 (c)). We conclude that moderate bottom optimization of the internal policy can further boost the learning capability of the base model, making BuPO practical RL method. 5.3. Ablation Study Ablation of Bottom Optimization Steps. Sec. 4 reveals that as the optimization steps of the internal layer policy progress, the language model policy eventually collapses. We further conduct an ablation study of how sinter affects 9 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies the underlying reasoning structure and leading to improved overall performance. 6.2. Interpretability of Black-box LLMs Interpretability tools mitigate the opacity of black-box LLMs by revealing their internal logic. substantial body of prior work investigates how LLMs reason and memory (Yu & Ananiadou, 2023; Tan et al., 2025a; Lindsey et al., 2025; Gupta et al., 2025; Hu et al., 2025b), with particular emphasis on the self-attention module (Zhou et al., 2024; Jin et al., 2025) and the feed-forward network (FFN) module (Dai et al., 2022; Meng et al., 2022; Geva et al., 2023). Insights into these internal mechanisms provide new perspective for algorithmic optimization (Li et al., 2025; Liu et al., 2025a). In this work, we conduct systematic analysis of internal hidden states from policy-centric perspective and identify various internal reasoning patterns across model families. We further observe that reasoning emerges progressively from lower to higher layers. Motivated by this finding, we propose bottom-up policy optimization, which leads to improved reinforcement learning algorithms. 7. Conclusion In this work, we propose novel decomposition of language model policies into Internal Layer Policies and Internal Modular Policies, enabling fine-grained analysis of how reasoning evolves across layers and modules. Through systematic entropy analysis, we uncover distinct reasoning patterns: early layers maintain high entropy for exploration, while top layers converge to near-zero entropy for refinement. Notably, we identify architectural differencesLlama models exhibit abrupt convergence in final layers, whereas Qwenseries models, especially Qwen3, demonstrate progressive, human-like reasoning with FFNs playing crucial structural role. Our exploration of internal policy optimization reveals key insight: optimizing internal policies encourages lower layers to encode high-level reasoning signals early, reconstructing foundational reasoning capabilities rather than merely adjusting surface-level outputs. Motivated by this, we introduce Bottom-up Policy Optimization (BuPO), reinforcement learning paradigm that directly targets internal layer policy during early training. Extensive experiments on complex reasoning benchmarks demonstrate that BuPO consistently improves learning efficiency and achieves superior Avg@K/Pass@K performance, validating the effectiveness of layer-aware policy optimization. Limitations. We believe this work can be further extended. While our experiments focus on the commonly used Qwen and Llama model families, extending the study to additional architectures would help assess the broader applicability of our approach. Moreover, our analysis is conducted under specific experimental settings, and different RLVR configurations may exhibit varying behaviors. Finally, the selection of bottom-up optimized internal layers and training steps may depend on model characteristics and could be adjusted to achieve optimal performance in different settings. Future directions. Future directions involve adopting advanced interpretability tools to facilitate both the understanding of internal mechanisms and algorithmic design. Further exploration of the internal reasoning patterns in Llama, Qwen, and other model families will feedback into model development. Moreover, comprehensive understanding of internal policies can make the bottom-up perspective more generalizable."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Zhang, Z., Yuan, L., Han, J., and Peng, H. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, M., Chen, G., Wang, W., and Yang, Y. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025a. Chen, Z., Qin, X., Wu, Y., Ling, Y., Ye, Q., Zhao, W. X., and Shi, G. Pass@ training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025b. Cheng, D., Huang, S., Zhu, X., Dai, B., Zhao, W. X., Zhang, Z., and Wei, F. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. 10 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, 2022. Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025a. URL https://arxiv.org/abs/ 2503.24290. Dehaene, S., Kerszberg, M., and Changeux, J.-P. neuronal model of global workspace in effortful cognitive tasks. Proceedings of the national Academy of Sciences, 95(24): 1452914534, 1998. Dong, G., Bao, L., Wang, Z., Zhao, K., Li, X., Jin, J., Yang, J., Mao, H., Zhang, F., Gai, K., et al. Agentic entropy-balanced policy optimization. arXiv preprint arXiv:2510.14545, 2025. Felleman, D. J. and Van Essen, D. C. Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex (New York, NY: 1991), 1(1):147, 1991. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, 2021. Geva, M., Bastings, J., Filippova, K., and Globerson, A. Dissecting recall of factual associations in auto-regressive arXiv preprint arXiv:2304.14767, language models. 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Gupta, A., Yeung, J., Anumanchipalli, G., and Ivanova, arXiv preprint A. How do llms use their depth? arXiv:2510.18871, 2025. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. He, Z., Liang, T., Xu, J., Liu, Q., Chen, X., Wang, Y., Song, L., Yu, D., Liang, Z., Wang, W., et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. Hu, Y., Zhou, C., and Zhang, M. What affects the effective depth of large language models? In Mechanistic Interpretability Workshop at NeurIPS 2025, 2025b. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jin, M., Mei, K., Xu, W., Sun, M., Tang, R., Du, M., Liu, Z., and Zhang, Y. Massive values in self-attention modules are the key to contextual knowledge understanding. arXiv preprint arXiv:2502.01563, 2025. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Li, Y., Dong, Z., Sun, Y., Wang, W., Xiong, S., Luo, Y., Liu, J., Lu, H., Wang, J., Su, W., et al. Attention illuminates llm reasoning: The preplan-and-anchor rhythm enables fine-grained policy optimization. arXiv preprint arXiv:2510.13554, 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Lindsey, J., Gurnee, W., Ameisen, E., Chen, B., Pearce, A., Turner, N. L., Citro, C., Abrahams, D., Carter, S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T., McDougall, C., Cunningham, H., Henighan, T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Thompson, T. B., Zimmerman, S., Rivoire, K., Conerly, T., Olah, C., and Batson, J. On the biology of large language model. Transformer Circuits Thread, 2025. URL https://transformer-circuits.pub/ 2025/attribution-graphs/biology.html. Liu, R., Wang, J., Shi, Y., Xie, Z., An, C., Zhang, K., Zhao, J., Gu, X., Lin, L., Hu, W., et al. Attention as compass: Efficient exploration for process-supervised rl in reasoning models. arXiv preprint arXiv:2509.26628, 2025a. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: 11 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies critical perspective. arXiv preprint arXiv:2503.20783, 2025b. MAA. American mathematics contest 12 (amc 12), November 2023. URL https://artofproblemsolving. com/wiki/index.php/AMC_12_Problems_ and_Solutions. MAA. American (aime), exinvitational mathematics URL February amination https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. 2024. MAA. American (aime), invitational mathematics exURL February amination https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. 2025. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022. Meta AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, 2024. URL https:// huggingface.co/meta-llama. Accessed: 202511-03. nostalgebraist. interpreting gpt: the logit lens. LessWrong, 2020. URL https://www.lesswrong. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Peng, R., Ren, Y., Yu, Z., Liu, W., and Wen, Y. Simko: Simple pass@ policy optimization. arXiv preprint arXiv:2510.14807, 2025. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. 12 Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 1279 1297, 2025. Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Tan, Y., He, S., Liu, K., and Zhao, J. Neural incompatibility: The unbridgeable gap of cross-scale parametric knowledge transfer in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025a. doi: 10.18653/v1/2025.acl-long.1047. Tan, Y., He, S., Liu, K., and Zhao, J. The zero-step thinking: An empirical study of mode selection as harder early exit in reasoning models. arXiv preprint arXiv:2510.19176, 2025b. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, M., Li, Y., Wang, H., Zhang, X., Xu, N., Wu, B., Huang, F., Yu, H., and Mao, W. Adaptive thinking via mode policy optimization for social language agents. arXiv preprint arXiv:2505.02156, 2025a. Wang, Z., Zhou, F., Li, X., and Liu, P. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Yamins, D. L. and DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. Nature neuroscience, 19(3):356365, 2016. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Yang, S., Dou, C., Guo, P., Lu, K., Ju, Q., Deng, F., and Xin, R. Dcpo: Dynamic clipping policy optimization. arXiv preprint arXiv:2509.02333, 2025b. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yu, Z. and Ananiadou, S. Neuron-level knowledge atarXiv preprint tribution in large language models. arXiv:2312.12141, 2023. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. Zhang, H., Hao, Q., Xu, F., and Li, Y. Reinforcement learning fine-tuning enhances activation intensity and diversity in the internal circuitry of llms. arXiv preprint arXiv:2509.21044, 2025. Zhou, Z., Yu, H., Zhang, X., Xu, R., Huang, F., Wang, K., Liu, Y., Fang, J., and Li, Y. On the role of attention heads in large language model safety. arXiv preprint arXiv:2410.13708, 2024. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025. 13 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies A. Detailed Experiment Settings A.1. Implementation of BuPO The detailed implementation of our proposed Bottom-up Policy Optimization (BuPO) algorithm is provided in Algorithm 1. Algorithm 1 Bottom-up Policy Optimization (BuPO) Require: Policy πθ, Dataset Q, Target Layer l, Internal Steps sinter, Max Steps Smax Ensure: Optimized Policy πθ 1: Initialize training step scur 0. 2: while scur < Smax do 3: 4: 5: 6: 7: Sample batch of prompts Q. Sampling: Generate outputs {oi}G Evaluation: Compute rewards and advantages ˆAi,t for each output. if scur sinter then Define internal policy πl i=1 from πθold(q). Layer from layer hidden states. Phase 1: Internal Policy Optimization Compute importance ratio ˆri,t = πl Layer(oi,tq,oi,<t) Layer,old(oi,tq,oi,<t) . πl Optimize πl Layer maximizing JInterGRPO = 1 (cid:80)G i=1 1 oi (cid:80) min[ˆri,t ˆAi,t, clip(ˆri,t, 1 ϵ, 1 + ϵ) ˆAi,t]. Compute importance ratio ˆri,t = πθ(oi,tq,oi,<t) 1 Optimize πθ maximizing JGRPO = 1 oi (cid:80)G i=1 πθold (oi,tq,oi,<t) . Phase 2: Language Model Policy Optimization (cid:80) min[ˆri,t ˆAi,t, clip(ˆri,t, 1 ϵ, 1 + ϵ) ˆAi,t]. end if Update parameters θ via gradient descent and increment scur scur + 1. 8: 9: 10: 11: else 12: 13: 14: 15: end while output πθ A.2. The Models for Experiments We summarize all models used in our analysis and experiments in Table 3. These models are categorized into three types: Mix, Base, and Instruct. Table 3. Detailed information about the selected models is provided. \"Mix\" refers to models that support both thinking and non-thinking modes. \"Base\" denotes the pre-trained model only. \"Instruct\" indicates models that undergo further fine-tuning based on the Base model to enhance instruction-following capabilities. Model Huggingface Type Layers Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-4B-Base Qwen2.5-Math-7B Qwen3-4B-Instruct-2507 Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-OctoThinker-3B-Base Llama-OctoThinker-8B-Base DeepSeek-Math-7B-Base DeepSeek-R1-Distill-Qwen-7B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B https://huggingface.co/Qwen/Qwen3-4B https://huggingface.co/Qwen/Qwen3-8B https://huggingface.co/Qwen/Qwen3-14B https://huggingface.co/Qwen/Qwen3-4B-Base https://huggingface.co/Qwen/Qwen2.5-Math-7B https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507 https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct https://huggingface.co/OctoThinker/OctoThinker-3B-Long-Base https://huggingface.co/OctoThinker/OctoThinker-8B-Long-Base https://huggingface.co/deepseek-ai/deepseek-math-7b-base Mix Mix Mix Base Base Instruct Instruct Instruct Base Base Base Instruct 36 36 40 36 28 36 28 32 28 32 30 28 A.3. The Template for Experiments We adopt the following template for all experiments involving Qwen models, building upon the Qwen-Math template used for Qwen2.5 (Yang et al., 2024) and the Qwen-Nothinking template for Qwen3. 14 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Qwen-Math Template <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {problem} <im_end> <im_start>assistant Qwen3-NoThinking Template <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {problem} <im_end> <im_start>assistant <think> </think> For training the Llama-OctoThinker models, we adopt the original prompt in Wang et al. (2025b) to ensure performance. OctoThinker Template conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. User: You must put your answer inside boxed{} and Your final answer will be extracted automatically by the boxed{} tag. {problem} Assistant: A.4. Implementation of Internal Policy Entropy Analysis In this section, we detail the implementation of internal policy entropy analysis. Our primary objective is to extract internal hidden states during the forward pass. In the main experiments, we evaluate model-generated responses on the MATH test set (Hendrycks et al., 2021). Entropy is computed at the token level for each layer and module, and then averaged over all generated tokens. We find that the intrinsic reasoning patterns remain stable across different tasks, e.g., commonsense question answering (Rein et al., 2024). The computation of internal policy entropy is illustrated in the pseudo-code below. We abstract the entropy computation as function H(). Accordingly, the entropy change of the internal layer policy is defined as: Layer = H(Hl Layer) H(Hl1 Layer). (12) For the two core Transformer submodules, the entropy changes are computed separately. Specifically, for the self-attention module, we define: ATTN = H(Al) H(Xl attn), and for the feed-forward network (FFN), we define: FFN = H(Fl) H(Xl ffn). (13) (14) These definitions allow us to quantify how each layer and submodule contributes to the evolution of the internal policy entropy. 15 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Calculation of Internal Policy Entropy (PyTorch Implementation) # Get layer hidden states by register hook hidden_state = get_from_hook() # Compute logits as the same as original forward logits = self.model.lm_head(hidden_state) # Apply softmax for normalization probs = torch.softmax(logits, dim=-1) # Apply log_softmax for speedy computation log_probs = torch.log_softmax(logits, dim=-1) # Calculate internal layer policy entropies = -(probs * log_probs).sum(dim=-1) Discussion 1: Comparison to the logit lens. Notably, our definition of internal policy differs from the logit lens approach (nostalgebraist, 2020), particularly in how layer normalization (LN) is handled. To clarify this distinction, we provide systematic comparison between the two formulations in Table 4. Our definition adopts policy-centric perspective, treating the internal hidden states as an explicitly samplable policy. In contrast, the logit lens is primarily designed to project hidden states into the discrete vocabulary space in order to inspect the most likely output tokens at intermediate layers. We intentionally omit LN based on empirical considerations: in our experiments, incorporating LN leads to unstable entropy dynamics and reduced interpretability. Extensive analyses and experiments with BuPO further demonstrate that our formulation of internal policy serves as robust interpretability tool for uncovering internal reasoning mechanisms in LLMs. Table 4. Comparison of logits lens with our definition of internal policy."
        },
        {
            "title": "Trainable",
            "content": "LN(Hl)ET softmax(HlET ) A.5. Implementation of Internal Policy Optimization In internal policy optimization, namely InterGRPO, at each optimization step, we select specific internal layer and optimize its internal policy πl ), where Hl is the hidden states at layer l. The Layer = softmax(LN(Hl)ET gradient flow for internal policy optimization is determined by the residual structure of the Transformer, where the hidden state Hl is function of all parameters from layer 1 to l, but is independent of parameters in higher layers. Layer, defined as πl Formally, for any parameter θk in layer k, the gradient of the InterGRPO loss with respect to θk can be expressed using the chain rule: JInterGRPO(πl Layer) θk = JinterGRPO πl Layer Layer πl Hl Hl θk (15) Due to the residual connections, Hl θk can be summarized as: = 0 only when l, and is zero otherwise. Thus, the gradients for different layers JInterGRPO(πl Layer) θk πl Layer Hl Hl θk , if if > (16) = JinterGRPO πl Layer 0, 16 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies This means that, during InterGRPO optimization for layer l, only the parameters of layers 0 through and unembedding matrix Eu are updated, while all higher layers (k > l) remain unaffected. This targeted gradient flow ensures that internal policy optimization provides direct supervision to the selected layer and all lower layers, strengthening foundational reasoning capabilities without interfering with higher-level representations. A.6. Implementation of RL In Sec. 4 and Sec. 5, we conduct reinforcement learning (RL) experiments to evaluate the performance of the proposed algorithm. In this section, we describe the RL training setup in detail. We implement GRPO and other baseline algorithms using the veRL framework (Sheng et al., 2025). Across all algorithms and model variants, we adopt unified set of hyperparameters, as reported in Table 5, and do not employ entropy regularization or KL-based losses. For PPO, the critic network is trained separately with learning rate of 1 105. Table 5. RL Hyperparameters Hyperparameter Value Optimizer Policy learning rate Critic learning rate Training batch size Samples per prompt Mini-batch size Policy updates per rollout Max prompt length Max response length Rollout temperature Clip range ϵ AdamW 1e6 1e5 (for PPO) 128 prompts 8 32 prompts 16 1024 tokens 7168 tokens (Qwen3) / 3072 (Others) 1.0 0.2 17 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies B. Extended Experiment Results B.1. Internal Policy Entropy Dynamics for More Models In this section, we present additional preliminary analyses of internal policy entropy dynamics across broader set of models. Specifically, we examine different variants of the same backbone, including Base, Instruct, and Mix versions, as well as models trained with supervised fine-tuning (SFT) and reinforcement learning (RL). In addition, we include the DeepSeek-Math model (He et al., 2025) to further enrich the comparative analysis. Further training has limited impact on internal reasoning patterns. We further analyze models that undergo additional training beyond standard pre-training. Specifically, we include DeepSeek-R1-Distill-Qwen-7B, which is further trained from Qwen2.5-Math-7B using distilled responses from Guo et al. (2025) with SFT, as well as Llama-OctoThinker-3B-Base and Llama-OctoThinker-8B-Base, which are obtained via continued pretraining (i.e., mid-training) based on Llama-3.2-3B-Base and Llama-3.2-8B-Base, respectively (Wang et al., 2025b). Moreover, Qwen3-4B and Qwen3-4B-Base also show consistent pattern after post-training with RL. Notably, these additional training procedures exhibit only marginal influence on the internal reasoning patterns. In summary, reinforcement learning, supervised fine-tuning, and mid-training do not substantially alter the models internal reasoning mechanisms, suggesting that these intrinsic patterns are primarily determined by the model architecture and initial pre-training. Understanding how such patterns emerge remains an important direction for future work. Same series of models exhibit consistent structures. After analyzing in all analysis plots across models, we find that same series of model show consistent structures. For instance, all Qwen3 series models shows progresive internal reasoning pattern including Qwen3-4B, Qwen3-8B and Qwen3-14B, also with other base or instruct version. Also, Llama-3.1 and Llama-3.2 show intra-difference and inter-consistency. of DeepSeek-Math. We Entropy dynamics reasoning mechanisms of DeepSeek-Math-7B-Base to provide more comprehensive comparison. As illustrated in Figure 9, DeepSeek-Math-7B-Base exhibits markedly different entropy dynamics: internal policy entropy decreases substantially and converges primarily in the middle layers. This phenomenon is primarily driven by the consistently negative entropy change in the FFN module, i.e., FFN < 0, as illustrated in Figure 10, particularly in the middle layers. the overall internal analyze further the Based on this observation, we infer that both Qwen and DeepSeek-Math demonstrate strong capability in knowledge absorption during post-training, indicating that convergence behavior in internal reasoning plays critical role in effective learning, in contrast to Llama. Moreover, the generation search space of DeepSeek-Math appears more constrained than that of Qwen, particularly the Qwen3 series, suggesting reduced exploration capacity. We hypothesize that such internal reasoning patterns significantly influence the effectiveness of further training, pointing to promising directions for architectural design and optimization of foundation models. 18 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies Figure 9. Continuous Entropy Dynamics of Internal Policy for additional models. The information flows from Hl1 into Al, Fl, and finally to the next layer Hl. Figure 10. Entropy Change Dynamics of Internal Policy with more models. 19 Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies B.2. Pass@K Performance across Datasets We further provide detailed version of Figure 7 across AMC23, MATH500, AIME24, and AIME25. Our proposed BuPO consistently outperforms the vanilla GRPO baseline. Figure 11. Pass@K results on MATH500, AMC23, AIME24 and AIME25. To reduce evaluation variance, we set = 300."
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "Tencent AI Lab",
        "University of Chinese Academy of Sciences",
        "University of Electronic Science and Technology of China"
    ]
}