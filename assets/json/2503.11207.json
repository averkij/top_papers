{
    "paper_title": "Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty?",
    "authors": [
        "Giacomo Camposampiero",
        "Michael Hersche",
        "Roger Wattenhofer",
        "Abu Sebastian",
        "Abbas Rahimi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 0 2 1 1 . 3 0 5 2 : r 126 Can Large Reasoning Models do Analogical Reasoning under Perceptual Uncertainty? Giacomo Camposampiero IBM Research - Zurich and ETH Zürich Michael Hersche IBM Research - Zurich giacomo.camposampiero1@ibm.com michael.hersche@ibm.com Roger Wattenhofer ETH Zürich Abu Sebastian IBM Research - Zurich Abbas Rahimi IBM Research - Zurich wattenhofer@ethz.ch ase@zurich.ibm.com abr@zurich.ibm.com Abstract This work presents first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAIs o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Ravens progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt twofold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes values. We observe sharp decline in OpenAIs o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0%approaching random chanceon the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4 more reasoning tokens. similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on IRAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models. 1. Introduction Large Language Models (LLMs) such as GPT-4 (OpenAI et al., 2024), Gemini (Gemini Team et al., 2024), and Claude (Anthropic, 2024) have demonstrated great proficiency in generating fluent and contextually relevant text. However, their capabilities in more complex domains such as reasoning and planning have been shown to be brittle even in simple tasks (Gendron et al., 2024; Wu et al., 2024), fail to attain levels of general abstract reasoning comparable to humans (Odouard and Mitchell, 2022; Thomm et al., 2024; Lewis and Equal contribution G. Camposampiero, M. Hersche, R. Wattenhofer, A. Sebastian & A. Rahimi. Camposampiero Hersche Wattenhofer Sebastian Rahimi Figure 1: Analogical reasoning under perceptual uncertainty. In this figure, we highlight all the different axes of generalization and robustness to uncertainty, which I-RAVEN-X stresses. Compared to standard I-RAVEN (a), I-RAVEN-X (b) involves more panels per row (10 vs. 3) and larger attribute dynamic ranges (up to 100 more values per attribute). This work introduces uncertainty in the reasoning process (c) through confounders (such as panels background and color patterns within objects) and smoothening the attribute values distributions (displayed on the right for the panel in position (1, 10)). The Ravens Progressive Matrices (RPM) rules used in this example are constant for shape, distribute right for color, constant for size, and progression for number. We adopt visual representation of the panels and their attributes for clarity of explanation; in practice, however, our dataset is purely symbolic and has not been extended yet to the visual domain. Mitchell, 2025; Camposampiero et al., 2023) and may be in some cases result of data contamination (Roberts et al., 2023; Mirzadeh et al., 2025). To mitigate this issue, the attention has shifted from training-time to inference-time compute scaling. This resulted in the development of new generation of systems, dubbed Large Reasoning Models (LRMs), that can dynamically allocate variable compute time during inference based on the input query. Unlike LLMs, which behaved mostly like approximate retrievers, LRMs such as OpenAI o1 (OpenAI, 2024), OpenAI o3, DeepSeek R1 (DeepSeek-AI et al., 2025), and Qwen QwQ (Team, 2024) approach reasoning tasks by exploring the space of solutions through pseudo-actions using Chain-of-Thought (CoT) (Wei et al., 2022) tokens. While LRMs achieved remarkable performance on many reasoning benchmarks in their preferred textual domain, they still have not been successful in other modalities, such as vision (Mitchell et al., 2024; Jiang et al., 2024; Cao et al., 2024; Ahrabian et al., 2024; 2 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Zhang et al., 2024b). Hence, it has become standard practice to assume the availability of an oracle perception and prompt LRMs with ideal and discrete symbolic transcriptions of the test examples (Webb et al., 2023; Hu et al., 2023; Hersche et al., 2025). However, the assumption of an oracle visual perception bypasses crucial steps in visual abstract reasoning. First, the oracle perception takes the knowledge about the set of attributes needed to perform the reasoning task for granted. As result, background variables that do not influence the answer prediction are automatically filtered out, effectively eliminating crucial step in the reasoning process. Second, the oracle perception usually provides the attribute values with full confidence, i.e., the distribution over the attributes is degenerate probability mass function (PMF). This assumption is highly unrealistic since any neural perception will always feature some degree of uncertainty in its outputs. In summary, general reasoning method should not only be able to perform reasoning on perfect input representations but also handle uncertainties with respect to the number of variates and their distribution. This paper proposes more thorough method to benchmark LRMs on abstract visual reasoning (see Figure 1). Specifically, we focus on solving Ravens progressive matrices (RPMs, Raven et al. (1938)), visual abstract analogical reasoning task that is commonly used to test the fluid intelligence of humans and, more recently, of machines too (Barrett et al., 2018; Zhang et al., 2019; Hu et al., 2021; Jiang et al., 2024). To avoid potential data leakage from the models preand post-training stages (Mirzadeh et al., 2025), we propose fully symbolic, generative methodology to evaluate LRMs in realistic scenario. The resulting dataset, dubbed I-RAVEN-X, evaluates the reasoning capabilities from the following aspects: Productivity: larger context matrix sizes are introduced, e.g., 310 instead of 33; Systematicity: larger dynamic ranges for the attribute values are introduced, e.g., 1000 attribute values instead of 10; Robustness to confounding factors: the set of generative attributes in RPM is augmented with randomly sampled values, which do not contribute to the reasoning; Robustness to non-degenerate value distributions: the distributions of the input values corresponding to the generative factors are smoothened. While an initial attempt evaluated the productivity using I-RAVEN-X (Hersche et al., 2025), this work proposes more elaborate benchmarking using LRMs with an additional focus on robustness. We leverage I-RAVEN-X to perform an exhaustive analysis on two state-ofthe-art (SOTA) LRMs, OpenAIs o3-mini and DeepSeek R1. Compared to I-RAVEN (Hu et al., 2021), I-RAVEN-Xs increased matrix size and dynamic range reduce o3-minis task accuracy from 86.6% to 81.0%. Moreover, the task accuracy further degrades when additionally introducing 10 confounding factors (69.8%) and the smoothened value distribution (75.6%). Finally, when I-RAVEN-X combines all its productivity and robustness measures, the accuracy sharply decreases to 17.0%. Similar trends are observed for R1, experiencing an overall drop from 80.6% to 23.2%. As potential remedy to LRMs susceptibility to perceptual uncertainty, we show that neuro-symbolic probabilistic abductive reasoning methods can naturally support the uncertainty from smoothened distributions. In addition, we present novel entropy-based confidence metric that allows probabilistic abductive reasoning methods to filter confounders in 3 Camposampiero Hersche Wattenhofer Sebastian Rahimi the decision process. We evaluate the new confidence metric on one of the SOTA approaches of this family (ARLC, Camposampiero et al. (2024)). In the most difficult I-RAVEN-X setting (mix of confounders and smoothening noise), ARLC achieves significantly stronger accuracy (88.3%) compared to LRMs. Moreover, ARLC can retain high accuracy even in very harsh signal-to-noise ratios (SNRs) conditions (up to 20 dB). 2. Background and Related Works Analogical Reasoning Benchmarks wide range of benchmarks to assess human-like fluid intelligence and abstract reasoning has been proposed in the past decade (Bilker et al., 2012; Cherian et al., 2023; Chollet, 2019; Niedermayr et al., 2024). Ravens Progressive Matrices (RPM) (Raven et al., 1938; Carpenter et al., 1990; Bilker et al., 2012) is one of the most prominent among them due to its extensive to benchmark for abstract reasoning, analogy-making, and out-of-distribution (OOD) testing (Benny et al., 2021; Hu et al., 2021; Małkiński and Mańdziuk, 2025; Mitchell, 2021; Zhang et al., 2019). RAVEN (Zhang et al., 2019) represented one of the first attempts to build dataset in the context of RPM that aimed at associating vision with structural, relational, and analogical reasoning in hierarchical representation. I-RAVEN (Hu et al., 2021) (Figure 1a) improved RAVEN, proposing new generation algorithm based on attribute bisection trees. This ensured that candidate panels are sampled from an unbiased candidate set, avoiding shortcut solutions that were I-RAVEN-X (Hersche et al., 2025) extended I-RAVEN, possible in the original dataset. introducing parameterizable number of columns and dynamic range of attribute values that allow testing the generalization of analogical reasoning to longer reasoning chains In Figure 1b, this can be identified by the larger and an increased number of concepts. number of columns and wider range of attributes (such as the color and the number of objects), respectively. In addition, the dataset was narrowed down to single constellation (center, containing only one object per panel) which was observed to be at the same time strong test for wide range of logical and arithmetic skills and unexpectedly challenging for LLMs (Hersche et al., 2025). Large Reasoning Models Recent research has focused on training LLMs to exhibit human-like reasoning (OpenAI, 2024), yet major obstacle remains the scarcity of annotated, step-by-step reasoning data. To overcome this issue, researchers started transitioning from expensive human annotations to LLM-driven search algorithms that automatically generate accurate reasoning trajectories through external verification (Luo et al., 2024) and RL-based techniques (Zhang et al., 2024a; Shao et al., 2024). Moreover, scaling test-time computation was also shown to be useful to refine intermediate reasoning steps, thereby further improving accuracy on reasoning tasks (Snell et al., 2024). Together, the combination of RL-driven train-time scaling and search-based test-time scaling paved the way for new generation of systems, named Large Reasoning Models (LRMs), with significantly enhanced reasoning performance Xu et al. (2025). However, contrary to LLMs (Webb et al., 2023; Hu et al., 2023; Hersche et al., 2025; Moskvichev et al., 2023; Mitchell et al., 2024; Lewis and Mitchell, 2025), the analogical reasoning abilities of LRMs have not yet been extensively evaluated, with only Latif et al. (2024) showing limited results on subset of RAVEN. 4 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Neuro-Symbolic Architectures for RPM Improving on monolithic deep learning models (Wu et al., 2020; Benny et al., 2021), neuro-symbolic architectures implementing abductive reasoning (Magnani, 2009) achieved remarkable success, scoring SOTA results on this analogical reasoning test. Initially introduced with the PrAE learner (Zhang et al., 2021), this approach was further improved by the NVSA model (Hersche et al., 2023). In NVSA, the probabilistic reasoning was implemented via distributed representations and operators of vector-symbolic architectures (VSAs) (Gayler, 2003; Kanerva, 2009; Plate, 1995). VSAs, besides their computational and scalability benefits, provide common language with neural networks for better interface and deeper integration. Both PrAE and NVSA are examples of Neuro Symbolic (type 3) according to Kautz (2022); that is, they are systems composed of neural vision module that interacts with static symbolic reasoning system through well-defined interface. Follow-up works extended these systems, mostly moving from pure knowledge representations to more trainable architectures that can learn from examples to reason and improve their expressiveness (Zhang et al., 2022; Camposampiero et al., 2024; Sun et al., 2025). Some of them, as ARLC (Camposampiero et al., 2024), could be classified as Neuro[Symbolic] systems (type 6) since the reasoning rules are learned fully differentiably from generic rule template encoded in distributed representations, and it is capable of combinatorial reasoning by exploiting computation-in-superposition. 3. Integrating perceptual uncertainty into I-RAVEN-X Contrary to the standard I-RAVEN, I-RAVEN-X is fully symbolic benchmark that evaluates abstract reasoning under the assumption of an oracle perception. This assumption stems from the observation that using the original visual inputs to prompt multi-modal LLMs performed significantly worse compared to noiseless symbolic transcriptions of the test examples (Mitchell et al., 2024; Jiang et al., 2024; Cao et al., 2024; Ahrabian et al., 2024; Zhang et al., 2024b). However, it represents rather strong assumption since it neglects the uncertainty that would necessarily result from the extraction of those attributes in real-world scenarios and its influence on the analogical reasoning process. In this work, we propose an extension of I-RAVEN-X to overcome it, augmenting the original dataset by: 1. integrating confounding attributes for each RPM example, and 2. smoothening the original degenerate attribute values distributions. Together, 1. and 2. allow us to loosen the strong assumption of an oracle perception, simulating an imperfect perception front-end while retaining the main advantage of operating in purely symbolic setting (that is, leveraging text-based models rather than their weaker multi-modal equivalents). 3.1. Confounding attributes Confounding attributes represent properties and patterns that can be extracted from the visual inputs by front-end perception module but are not relevant to the reasoning process. This could be the case, for instance, when the attributes are extracted by unsupervised vision models such as Variational Autoencoders (Kingma and Welling, 2022) or even multi-modal LLM that is prompted to extract the attributes. In Figure 1c, confounding attributes are 5 Camposampiero Hersche Wattenhofer Sebastian Rahimi represented by the background of the input panels and the color patterns, which sometimes appear inside the objects. While the original RAVEN dataset includes noise attributes (e.g., the orientation), we argue that these are not real confounders and that their naming is misleading, as they do not add any noise to the RPM tests (we argument this more extensively in Appendix C). In I-RAVEN-X, this is practically implemented by extending the set of original attributes of each panel with an arbitrary number of confounding factors uniformly sampled in the interval [0, 1], where is the dynamic range of the attributes in the experiment. For large enough m, the probability of sampling values that fit valid rule is negligible, and hence, confounders do not introduce ambiguities in the choice of the answer panel. However, they linearly reduce the SNR in the reasoning process and require models to filter the noisy attributes. 3.2. Smooth attribute values distributions We deviate from the original I-RAVEN-X degenerate attributes distributions and introduce variance, which allows us to test the robustness of the models when reasoning with uncertain attributes values. Figure 1 highlights this relaxation, from one-hot PMFs of the standard I-RAVEN-X (Figure 1b) to the distributed PMFs of our proposed extension (Figure 1c). In practice, we smoothen the original attributes distributions using either Gaussian filter or with three-bins strategy, where the probability of the true value is p(T ) U(pL, 1), pL > 0.5 and the probabilities of its two neighboring values are p(N1) U(0, 1 p(T )) and p(N2) = 1 p(T ) p(N1). Note that the motivation behind the three-bins strategy is to introduce variance with minimal additional for LRMs prompt complexity. 4. Solving RPMs with LRMs and NeSy Probabilistic Abductive Models 4.1. Large Reasoning Models (LRMs) We focus our study on the two most prominent SOTA LRMs available to date: the closedsource OpenAI o3-mini model1 and the open-source DeepSeek R1 model (DeepSeek-AI et al., 2025) (together with its distilled version based on Llama 70B)2. In Appendix B, we include an additional (limited) comparison between OpenAI o3-mini and its predecessor, OpenAI o1. However, since the performance of the o3-mini model was on par with o1 while costing only fraction ( 14 less), we decided to experiment only with o3-mini. We adopt the same evaluation framework used in Hersche et al. (2025) to benchmark LRMs. Contrary to their analysis, however, we focus our investigation on entangled prompts (providing all the attributes values in single prompt rather than having single prompt per attribute). We are forced to choose this setting because, despite performing worse compared to using disentangled prompts (where one separate prompt is used for each attribute), successive experiments on confounders would have otherwise become trivial. Furthermore, we move from predictive approach (where the model has to generate the missing panel) 1. We use the o3-mini-2025-01-31 via thOpenAI API. By default, reasoning efforts were set to medium and number of reasoning tokens to 25,000. 2. The full model with 671B parameters was serviced by www.together.ai, whereas the distilled version was run locally on 8 NVIDIA A100 GPUs. The maximum number of reasoning tokens was set to 25,000, the temperature to 0.6, and top-p to 0.7. 6 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? to discriminative approach (where the model is given list of candidates and required to choose one among them) (Gendron et al., 2024; Hersche et al., 2025). This choice stems from observing, in the early stages of our evaluation, that LRMs can sometimes pick up valid relations in the input matrices (for instance, relations between the binary encodings of values) which are however not part of the set of rules used in RPM. Unlike the generative approach, the discriminative approach implicitly biases the model into evaluating only the rules defined in RPM without explicitly revealing them, hence reducing the aforementioned issue. For more details on the task and prompts, refer to Appendix A. Self-consistency (Wang et al., 2023; Lewkowycz et al., 2022) and attributes scaling (Hu et al., 2023) were also dropped in the experiments with LRMs. Moreover, no in-context examples of the tasks (Brown et al., 2020) were provided since they were previously observed to be hurtful for LRMs (DeepSeek-AI et al., 2025). We also restrict the investigation to subset of 500 randomly sampled RPM tests in both I-RAVEN and I-RAVEN-X (due to budget constraints), which we observed to be representative enough of the entire test set. 4.2. NeSy probabilistic abductive reasoning (NeSy-PAR) models Among the wide spectrum of domain-specific architectures proposed to solve RPM, growing number of works have recently focused on probabilistic abductive reasoning (Zhang et al., 2021; Hersche et al., 2023; Camposampiero et al., 2024; Sun et al., 2025). Abductive reasoning allows us to selectively infer propositions based on prior knowledge represented in symbolic form to explain the perceptual observations in the best way (Magnani, 2009). In this work, we propose an extension to the classical framework of probabilistic abductive reasoning in the form of novel entropy-based confidence metric to improve its performance when reasoning under uncertainty. In particular, we propose to regularize the contribution to the score/loss of each attribute using the entropy of the confidence values (encoding the probability of each rule being the one underlying the behavior of specific attribute in RPM panel) used in the abduction step of the framework. Practically, we re-weight the contribution to the loss and score of each candidate panel as = (cid:88) attr Lattr H(sattr) = (cid:88) attr Sattr H(sattr) with H(sattr) = p(s) log p(s) (1) (cid:88) ss where = [s1, . . . , sR] is the vector of confidence values in the rules available to the model (computed from the first two rows of each RPM example) and the attribute losses Lattr and scores Sattr represent the individual attributes contributions to the training loss and the candidate prediction metric, respectively. Intuitively, the proposed regularization technique lowers the contribution of attributes whose confidence is uniformly distributed across different rules (which happens when no rule perfectly fits the data and results in high entropy) while increasing the contribution of those attributes for which the confidence in the rule is very concentrated (the model is very confident on single rule, hence the entropy is low). We implement and evaluate the regularization proposed in Equation (1) within the ARLC model (Camposampiero et al., 2024), one of the SOTA NeSy approaches on RPM. 7 Camposampiero Hersche Wattenhofer Sebastian Rahimi 5. Results 5.1. LRMs are stronger analogical reasoners than LLMs Analogical reasoning capabilities of LRMs have not been extensively evaluated to this date. In this work, we reduce this knowledge gap by testing this new generation of systems on the well-known benchmark I-RAVEN (Hu et al., 2021), as well as its more difficult extension, I-RAVEN-X (Hersche et al., 2025). Table 1 reports the results for this first proposed evaluation. We additionally include previous results on the closed-source OpenAI GPT-4 (OpenAI et al., 2024) and the open-source Llama-3 70B (Dubey et al., 2024) from Hersche et al. (2025) to allow for one-to-one comparison between LRMs and LLMs. Firstly, we observe that LRMs can achieve results comparable to LLMs with less engineered prompts and that they generally improve the reasoning accuracy when the level of prompt engineering is on par. o3-mini, for instance, shows no drops in accuracy on IRAVEN-X and 6% drop on I-RAVEN compared to GPT-4 while using only 121 of the prompts. When we compare the same two models on similar prompt complexities (that is, using entangled prompting in both settings, but still retaining 17 ratio between LRMs and LLMs due to self-consistency) o3-mini emerges as clear winner, showing 6.5% increase in accuracy and remarkably stronger performances on arithmetic reasoning. However, this comes at cost, as shown by the number of output tokens produced by the models during inference, which is two orders of magnitude higher on average compared to LLMs. Secondly, the results show that LRMs are much stronger reasoners than LLMs when challenged with the longer reasoning rules and attribute ranges in I-RAVEN-X. While LLMs show massive drop in arithmetic accuracy on I-RAVEN-X, nearing 0% for comparable prompt complexity, LRMs are affected by much smaller arithmetic degradation on average, while sometimes even improving on the overall task accuracy. I-RAVEN (33) I-RAVEN-X (310) Model ICL Prompts Range 10 Range 100 Range 1000 Llama-3 70B GPT-4 Llama-3 70B GPTOpenAI o3-mini (medium) OpenAI o3-mini (high) DeepSeek R1 DeepSeek R1 dist. 21 21 7 7 1 1 1 1 Task Arithm. Tok. Task Arithm. Tok. Task Arithm. Tok. 85.0 93.2 74.8 79.0 86.6 92.6 80.6 78.4 45.0 73.6 27.2 31.0 74.4 86.1 74.8 69.4 21 21 21 21 5445 9867 4486 73.0 79.6 72.6 72.8 77.6 82.4 84.0 67.0 2.6 25.1 0.0 2.7 53.2 63.5 67.7 52.9 21 21 21 21 7884 19041 5550 74.2 76.6 74.0 74.0 81.0 80.6 82.8 72.0 0.4 8.4 0.4 1.1 60.8 60.1 65.8 54.4 21 21 21 21 7209 19449 5505 Table 1: Evaluating LRMs on analogical reasoning. Full task and arithmetic accuracy (%) of different LLMs and LRMs on two analogical reasoning benchmarks, I-RAVEN and I-RAVEN-X. For each model, we report if In-Context Learning (ICL) examples of the task were added to the prompt, the number of total prompts fed into the model (some techniques, such as self-consistency and disentangled prompting require querying the model multiple times), and the number of tokens generated by the model. Range indicates the dynamic range of the attributes values. Tok. indicates the average number of output tokens of the model. The results for GPT-4 and Llama-3 are taken from Hersche et al. (2025). 8 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Overall, we observe that o3-mini and R1 show similar performance on this analogical reasoning task, with o3-mini excelling in the standard I-RAVEN and R1 performing better on I-RAVEN-X. The distilled version of R1, on the other hand, displays weaker results compared to the original model, especially on I-RAVEN-X. To further improve the results on o3-mini, we increased the reasoning effort from medium to high and set the maximum number of reasoning to its maximum (100,000). The accuracy on I-RAVEN improves by 6%, whereby it stays constant on the most difficult I-RAVEN-X setting despite the increased reasoning effort (2.7 reasoning tokens). Hence, o3-mini (medium) was preferred as costefficient solution for the following experiments. 5.2. LRMs are significantly challenged by reasoning under uncertainty The results in Section 5.1 show that LRMs can solve analogical reasoning tasks more accurately than LLMs. However, would they be capable of retaining the same robustness in scenarios where uncertainty is introduced? To answer this question, we benchmark the two LRMs on the I-RAVEN-X extension proposed in Section 3. We adopt the same methodology used in the previous experiments on I-RAVEN and I-RAVEN-X, with only minor prompting modifications when strictly necessary (e.g., to provide probability distributions for attributes values). The empirical results of this study are reported in Table 2. Exp. Confounders (SNR) 0 () 1 (4.77) 3 (0.00) 5 (2.22) 10 (5.23) 300 (20.00) 0 () 0 () 10 (5.23) 30 (10) (a) (b) (c) (c) OpenAI o3-mini DeepSeek R1 pL Task Arith. Tok. Task Arith. Tok. 6324 7209 1.00 82.8 81.0 60.8 65. 1.00 1.00 1.00 1.00 1.00 0.70 0.51 0.51 0.51 76.0 75.6 71.2 69.8 - 75.0 75.6 17.0 - 53.2 51.7 48.3 45.6 - 51.7 53.2 41.1 - 11521 11669 12640 13709 - 13112 13028 18482 - 78.2 80.2 78.6 77.0 - 67.4 63.0 23.2 - 55.2 58.2 55.9 53.6 - 44.9 46.4 45.3 - 8919 8429 8681 8912 - 6995 7518 7147 - ARLCentropy Task Arith. 98.3/93. 98.2/97.1 98.5/93.5 99.0/93.2 98.6/92.9 98.8/92.6 97.5/83.1 98.2/97.1 98.2/97.1 98.2/97.1 98.2/97.1 - 92.6/90.4 85.3/83.9 92.2/86.7 84.7/79.5 88.0/82.9 79.2/76. 88.3/75.4 78.8/66.5 Table 2: Evaluating LRMs and NeSy-PAR models on analogical reasoning under perceptual uncertainty. Task and arithmetic accuracy (%) of OpenAI o3-mini, DeepSeek R1, and ARLC on I-RAVEN-X (range [0,1000]) with different numbers of confounders, from 0 (no confounders, signal-to-noise ratio (SNR)=) to 10 (SNR=5.23 dB), and different attributes distribution smoothening (bin-smoothening strategy, with different probabilities assigned to the correct value bin pL). Three groups of experiments are reported: a) only confounders are introduced; b) only the attributes distribution smoothening is applied; c) both confounders and distribution smoothing are applied, together simulating perceptual uncertainty. For LRMs, we report the number of output tokens to quantify the reasoning effort adopted on average by the model to find solution. The signal-to-noise ratio (SNR) quantifies how strong the noise is introduced by confounders in each experiment. The results for ARLC are reported as max/mean over 5 different random seeds. 9 Camposampiero Hersche Wattenhofer Sebastian Rahimi Firstly, we observe that LRMs perform significantly worse when noise factors that simulate perceptual uncertainty are integrated into the experiments. For instance, o3-minis accuracy dropped by 11.2% and 15.2% on task and arithmetic accuracy, respectively, when evaluated with 10 additional confounding attributes. R1, on the other hand, is more robust to confounders (5.8% and 12.2% drops on task and arithmetic accuracy). However, it performs much worse when the attribute values distributions are smoothened, losing up to 19.8% of task accuracy in the harshest scenario, while o3-mini shows much smaller degradation (5.4%) in this setting. When both the confounders and distribution smoothening are evaluated together at their maximum level, we observe sharp drop in task accuracy for both o3-mini (to 17.0%) and DeepSeek R1 (to 22.8%), bringing them close to random chance (12.5%). Moreover, we observe that for o3-mini more challenging perceptual uncertainty conditions directly translate to higher numbers of reasoning tokens (7209 of the base setting to 18, 589 of the combined noise experiments). This trend, however, was not observed in R1, where the number of tokens was roughly constant across the different settings. An additional experiment with high reasoning effort could only slightly increase the o3-minis accuracy (to 31.0%) at the cost of 53, 596 average reasoning tokens. Taking step back, we observe that the overall task accuracy drop for LRMs is considerable: o3-mini loses up to 69.6% accuracy and R1 up to 57.2% from the standard I-RAVEN to I-RAVEN-X with perceptual uncertainty. Testing on longer reasoning relations and larger attributes dynamic ranges only plays smaller part in this (5.6% for o3-mini, and even an increase in accuracy of 2.2% for R1), while perceptual uncertainty accounts for most of the actual drop in accuracy. 5.3. NeSy-PAR models are robust when reasoning under uncertainty We extend the investigation to neuro-symbolic models based on probabilistic abductive reasoning, focusing in particular on ARLC ( Camposampiero et al. (2024), improved with the entropy regularization introduced in Section 4.2). We report some of these results in Table 2, and more extensive evaluations in Appendix D. ARLC proves to be more robust when reasoning under perceptual uncertainty compared to LRMs, showing no drop in accuracy even in extremely harsh signal-to-noise conditions due to confounders thanks to the novel entropy-based regularization (up to 20 dB as shown in Appendix D) and maintains its high accuracy when reasoning with smoothened attributes distributions. Furthermore, when evaluated on the most difficult setting (group (c) in Table 2), ARLC displays much stronger results (88.0% best accuracy compared to 23.2% of the best LRM). Overall, ARLC maintains remarkably high reasoning accuracy despite the introduction of perceptual uncertainty in the trajectory I-RAVEN I-RAVEN-X, experiencing only modest decline (98.6% to 88.0%), significantly outperforming LRMs. ARLC can also successfully learn the set of rules underlying I-RAVEN when trained with highly uncertain attribute distributions, as shown in Appendix D. 6. Conclusion This work addresses substantial limitation of pre-existing symbolic analogical reasoning benchmarks used to evaluate LLMs, i.e., their lack of support for reasoning under perceptual Can LRMs do Analogical Reasoning under Perceptual Uncertainty? uncertainty. Specifically, it augments an existing benchmark based on RPM, I-RAVEN-X, with confounding attributes and smooth attributes distributions, that together allow to simulate an imperfect perception front-end. This benchmark is then used to evaluate the latest generation of open-domain reasoning systems, Large Reasoning Models (LRMs). Compared to LLMs, LRMs achieve improved productivity to larger reasoning relations and attribute ranges. However, LRMs are still significantly challenged by the (simulated) perceptual uncertainty, which reduces the model reasoning accuracy by 69.6% and 57.4% (o3-mini and R1, respectively) On the other hand, neuro-symbolic models based on probabilistic abduction achieve more robust and accurate performance but cannot directly generalize to different domains in the same way LRMs do. Overall, our results suggest that open-domain, robust analogical reasoning models are still mirage, and future work has to be invested to achieve this objective. 11 Camposampiero Hersche Wattenhofer Sebastian Rahimi"
        },
        {
            "title": "References",
            "content": "Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, and Jay Pujara. The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models. In First Conference on Language Modeling (COLM), 2024. URL https://openreview.net/forum?id=eDWcNqiQWW. Anthropic. Introducing the next generation of Claude, March 2024. URL https://www. anthropic.com/news/claude-3-family. David G. T. Barrett, Felix Hill, Adam Santoro, Ari S. Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International Conference on Machine Learning (ICML), pages 511520, 2018. Yaniv Benny, Niv Pekar, and Lior Wolf. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1255212560, Nashville, TN, USA, June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.01237. URL https://ieeexplore.ieee.org/document/ 9577474/. Scale-Localized Abstract Reasoning. Warren B. Bilker, John A. Hansen, Colleen M. Brensinger, Jan Richard, Raquel E. Gur, and Ruben C. Gur. Development of abbreviated nine-item forms of the Ravens standard progressive matrices test. Assessment, 19(3):354369, September 2012. ISSN 1552-3489. doi: 10.1177/1073191112446655. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arxiv preprint arXiv:2005.14165, July 2020. URL http://arxiv. org/abs/2005.14165. Giacomo Camposampiero, Loïc Houmard, Benjamin Estermann, Joël Mathys, and Roger Wattenhofer. Abstract Visual Reasoning Enabled by Language. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 2643 2647, Vancouver, BC, Canada, June 2023. IEEE. ISBN 9798350302493. doi: 10.1109/ CVPRW59228.2023.00264. URL https://ieeexplore.ieee.org/document/10208934/. Giacomo Camposampiero, Michael Hersche, Aleksandar Terzic, Roger Wattenhofer, Abu Sebastian, and Abbas Rahimi. Towards Learning Abductive Reasoning using VSA Distributed Representations. In International Conference on Neural-Symbolic Learning and Reasoning, pages 370385. Springer, 2024. URL https://link.springer.com/chapter/ 10.1007/978-3-031-71167-1_20. Xu Cao, Bolin Lai, Wenqian Ye, Yunsheng Ma, Joerg Heintz, Jintai Chen, Jianguo Cao, and James M. Rehg. What is the Visual Cognition Gap between Humans and Multimodal 12 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? LLMs? 2406.10424. arXiv preprint arXiv:2406.10424, June 2024. URL http://arxiv.org/abs/ Patricia A. Carpenter, Marcel A. Just, and Peter Shell. What one intelligence test measures: theoretical account of the processing in the Raven Progressive Matrices Test. Psychological Review, 97(3):404431, 1990. ISSN 1939-1471. doi: 10.1037/0033-295X.97.3.404. Place: US Publisher: American Psychological Association. Anoop Cherian, Kuan-Chuan Peng, Suhas Lohit, Kevin A. Smith, and Joshua B. Tenenbaum. Are Deep Neural Networks SMARTer Than Second Graders? In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1083410844, Vancouver, BC, Canada, June 2023. IEEE. doi: 10.1109/ CVPR52729.2023.01043. URL https://ieeexplore.ieee.org/document/10204961/. ISBN 9798350301298. François Chollet. On the Measure of Intelligence. arXiv preprint arXiv.1911.01547, November 2019. URL http://arxiv.org/abs/1911.01547. DeepSeek-AI et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arxiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/ 2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. The Llama 3 Herd of Models. arxiv preprint arXiv:2407.21783, August 2024. URL http://arxiv.org/abs/2407.21783. Ross W. Gayler. Vector Symbolic Architectures answer Jackendoffs challenges for cognitive neuroscience. In Proceedings of the Joint International Conference on Cognitive Science. ICCS/ASCS, pages 133138, 2003. Gemini Team et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2024. URL https://arxiv.org/abs/2312.11805. Gaël Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large Language Models Are Not Strong Abstract Reasoners. In Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI), volume 7, pages 62706278, 2024. doi: 10.24963/ijcai. 2024/693. URL https://www.ijcai.org/proceedings/2024/693. ISSN: 1045-0823. Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas Rahimi. Neuro-vector-symbolic Architecture for Solving Ravens Progressive Matrices. Nature Machine Intelligence, 5(4):363375, March 2023. URL https://www.nature.com/articles/ s42256-023-00630-8. Michael Hersche, Giacomo Camposampiero, Roger Wattenhofer, Abu Sebastian, and Abbas Rahimi. Towards Learning to Reason: Comparing LLMs with Neuro-Symbolic on Arithmetic Relations in Abstract Reasoning. In AAAI Workshop on Neural Reasoning and Mathematical Discovery An Interdisciplinary Two-Way Street (NEURMAD), March 2025. URL https://openreview.net/pdf?id=F90YO0MacL. Camposampiero Hersche Wattenhofer Sebastian Rahimi Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. Stratified Rule-Aware Network for Abstract Visual Reasoning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(2):15671574, May 2021. ISSN 2374-3468, 2159-5399. doi: 10.1609/aaai. v35i2.16248. URL https://ojs.aaai.org/index.php/AAAI/article/view/16248. Xiaoyang Hu, Shane Storks, Richard Lewis, and Joyce Chai. In-Context Analogical ReaIn Proceedings of the 61st Annual Meetsoning with Pre-Trained Language Models. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19531969, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.109. URL https://aclanthology.org/2023.acl-long.109. Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, and Jay Pujara. MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning. In The Thirty-eight Conference on Neural Information Processing Systems (NeurIPS), 2024. URL https://openreview.net/pdf? id=vecFROHnL4. Pentti Kanerva. Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors. Cognitive Computation, 1(2):139159, June 2009. ISSN 1866-9956, 1866-9964. doi: 10.1007/s12559-009-9009-8. URL http://link.springer.com/10.1007/s12559-009-9009-8. Henry A. Kautz. The third AI summer: AAAI Robert S. Engelmore Memorial Lecture. AI Magazine, 43(1):105125, March 2022. ISSN 0738-4602, 2371-9621. doi: 10.1002/aaai. 12036. URL https://onlinelibrary.wiley.com/doi/10.1002/aaai.12036. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, December 2022. doi: 10.48550/arXiv.1312.6114. URL http://arxiv. org/abs/1312.6114. Ehsan Latif, Yifan Zhou, Shuchen Guo, Yizhu Gao, Lehong Shi, Matthew Nyaaba, Gyeonggeon Lee, Liang Zhang, Arne Bewersdorff, Luyang Fang, Xiantong Yang, Huaqin Zhao, Hanqi Jiang, Haoran Lu, Jiaxi Li, Jichao Yu, Xuansheng Wu, Weihang You, Zhengliang Liu, Vincent Shung Liu, Hui Wang, Zihao Wu, Jin Lu, Fei Dou, Ping Ma, Ninghao Liu, Tianming Liu, and Xiaoming Zhai. Systematic Assessment of OpenAI o1-Preview for Higher Order Thinking in Education. arXiv preprint arXiv:2410.21287, 2024. URL https://arxiv.org/abs/2410.21287. Martha Lewis and Melanie Mitchell. Evaluating the Robustness of Analogical Reasoning ISSN in Large Language Models. Transactions on Machine Learning Research, 2025. 2835-8856. URL https://openreview.net/forum?id=t5cy5v9wph. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems With Language Models. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 38433857, 2022. URL https://openreview.net/ forum?id=IFXTZERXdM7. Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve Mathematical Reasoning in Language Models by Automated Process Supervision. arXiv preprint arXiv:2406.06592, December 2024. doi: 10.48550/arXiv.2406.06592. URL http://arxiv.org/abs/2406.06592. Lorenzo Magnani. Abductive Cognition: The Epistemological and Eco-Cognitive Dimensions of Hypothetical Reasoning, volume 3 of Cognitive Systems Monographs. Springer, Berlin, Heidelberg, 2009. ISBN 978-3-642-03630-9 978-3-642-03631-6. doi: 10.1007/ 978-3-642-03631-6. URL http://link.springer.com/10.1007/978-3-642-03631-6. Mikołaj Małkiński and Jacek Mańdziuk. Deep Learning Methods for Abstract Visual Reasoning: Survey on Ravens Progressive Matrices. ACM Comput. Surv., 57(7): 166:1166:36, February 2025. ISSN 0360-0300. doi: 10.1145/3715093. URL https: //dl.acm.org/doi/10.1145/3715093. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. In International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id=AjXkRZIvjB. Melanie Mitchell. Abstraction and analogy-making in artificial intelligence. Annals of the New York Academy of Sciences, 1505(1):79101, 2021. ISSN 1749-6632. doi: 10.1111/ nyas.14619. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14619. Melanie Mitchell, Alessandro B. Palmarini, and Arseny Moskvichev. Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. In AAAI 2024 Workshop on Are Large Language Models Simply Causal Parrots?, 2024. URL https://openreview. net/forum?id=3rGT5OkzpC. Arseny Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview. net/forum?id=8ykyGbtt2q. Yannick Niedermayr, Luca A. Lanzendörfer, Benjamin Estermann, and Roger Wattenhofer. RLP: reinforcement learning benchmark for neural algorithmic reasoning, 2024. URL https://openreview.net/forum?id=pYmQId95iR. Victor Vikram Odouard and Melanie Mitchell. Evaluating Understanding on Conceptual Abstraction Benchmarks. arXiv preprint arXiv: 2206.14187, June 2022. URL http: //arxiv.org/abs/2206.14187. OpenAI. Learning to reason with LLMs, September 2024. URL https://openai.com/ index/learning-to-reason-with-llms/. OpenAI et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, March 2024. URL http://arxiv.org/abs/2303.08774. 15 Camposampiero Hersche Wattenhofer Sebastian Rahimi Tony A. Plate. Holographic Reduced Representations. IEEE Transactions on Neural Networks and Learning Systems, 6(3), 1995. J.C. Raven, J.H. Court, and J. Raven. Ravens progressive matrices. Oxford Psychologists Press, 1938. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. Data Contamination Through the Lens of Time. arXiv preprint arXiv:2310.10628, October 2023. doi: 10.48550/arXiv.2310.10628. URL http://arxiv.org/abs/2310.10628. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300, April 2024. doi: 10.48550/arXiv.2402.03300. URL http://arxiv.org/abs/2402.03300. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning. In The Thirteenth International Conference on Learning Representations (ICLR), October 2024. URL https://openreview.net/forum?id=4FWAwZtd2n. Richard Snow and David Lohman. Toward theory of cognitive aptitude for learning from instruction. Journal of educational psychology, 76(3):347, 1984. Richard Snow, Patrick Kyllonen, Brachia Marshalek, et al. The topography of ability and learning correlations. Advances in the psychology of human intelligence, 2(S 47):103, 1984. Zhong-Hua Sun, Ru-Yuan Zhang, Zonglei Zhen, Da-Hui Wang, Yong-Jie Li, Xiaohong Wan, and Hongzhi You. Systematic Abductive Reasoning via Diverse Relation Representations in Vector-symbolic Architecture. arXiv preprint arXiv:2501.11896, 2025. URL http: //arxiv.org/abs/2501.11896. Qwen Team. QwQ: Reflect Deeply on the Boundaries of the Unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. Jonathan Thomm, Michael Hersche, Giacomo Camposampiero, Aleksandar Terzić, BernIn hard Schölkopf, and Abbas Rahimi. Terminating Differentiable Tree Experts. Neural-Symbolic Learning and Reasoning: 18th International Conference (NeSy 2024). Springer-Verlag, July 2024. ISBN 978-3-031-71166-4. URL https://doi.org/10.1007/ 978-3-031-71167-1_16. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations (ICLR), 2023. Taylor Webb, Keith J. Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour, 7(9):15261541, July 2023. ISSN 23973374. doi: 10.1038/s41562-023-01659-w. URL https://www.nature.com/articles/ s41562-023-01659-w. 16 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning In Advances in Neural Information Processing Systems in Large Language Models. (NeurIPS), October 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J. Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba. The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning. arxiv preprint arXiv:2007.04212, July 2020. URL https://arxiv.org/abs/2007.04212. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1819 1862, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.102. URL https://aclanthology.org/2024.naacl-long. 102/. Antonia Wüst, Tim Tobiasch, Lukas Helff, Devendra S. Dhami, Constantin A. Rothkopf, and Kristian Kersting. Bongard in Wonderland: Visual Puzzles that Still Make AI Go Mad? In The First Workshop on System-2 Reasoning at Scale, NeurIPS24, October 2024. URL https://openreview.net/pdf?id=4Yv9tFHDwX. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards Large Reasoning Models: Survey of Reinforced Reasoning with Large Language Models. arXiv preprint arXiv:2501.09686, January 2025. doi: 10.48550/arXiv.2501.09686. URL http://arxiv.org/abs/2501.09686. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: Dataset for Relational and Analogical Visual REasoNing. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 53125322, Long Beach, CA, USA, June 2019. IEEE. ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.00546. URL https://ieeexplore.ieee.org/document/8953364/. Chi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97319741, Nashville, TN, USA, June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.00961. URL https://ieeexplore.ieee.org/document/9577614/. Chi Zhang, Sirui Xie, Baoxiong Jia, Ying Nian Wu, Song-Chun Zhu, and Yixin Zhu. Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning. In European Conference on Computer Vision (ECCV), pages 692709. Springer, 2022. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReSTMCTS*: LLM Self-Training via Process Reward Guided Tree Search. arXiv preprint 17 Camposampiero Hersche Wattenhofer Sebastian Rahimi arXiv:2406.03816, November 2024a. //arxiv.org/abs/2406.03816. doi: 10.48550/arXiv.2406.03816. URL http: Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Joshua M. Susskind, and Navdeep Jaitly. How Far Are We from Intelligent Visual Deductive Reasoning? In ICLR 2024 Workshop: How Far Are We From AGI, May 2024b. URL https://openreview. net/forum?id=AMrYF9F3J6. 18 Can LRMs do Analogical Reasoning under Perceptual Uncertainty?"
        },
        {
            "title": "Appendices",
            "content": "Appendix A. Additional details on RPM and prompting Ravens progressive matrices (RPM) is visual task that involves perceiving pattern continuation and elemental abstraction as well as deducing relations based on restricted set of underlying rules in process that mirrors the attributes of advanced human intelligence (Snow et al., 1984; Snow and Lohman, 1984). In this work, we focus on the I-RAVEN dataset. Each RPM test in I-RAVEN is an analogy problem presented as 3 3 pictorial matrix of context panels. Every panel in the matrix is filled with several geometric objects based on certain rule, except the bottom-right panel, which is left blank. Figure A.2 includes an I-RAVEN example test. The task is to complete the missing panel by picking the correct answer from set of (eight) candidate answer panels that match the implicit generation rule on every attribute. The objects attributes (color, size, shape, number, position) are governed by individual underlying rules: constant, the attribute value does not change per row; arithmetic, the attribute value of the third panel corresponds to either the sum or the difference of the first two panels of the row; progression, the attribute value monotonically increases or decreases in row by 1 or 2; distribute three, the set of the three different values remains constant across rows, but the individual attribute values get shifted to the left or to the right by one position at every row; it also holds column-wise. Each panel contains variable number of objects (minimum one, maximum nine) arranged according to one of seven different constellations (center, distribute-four, distribute-nine, left-right, up-down, in-out-center, and in-out-four). Figure A.2: RPM example from I-RAVEN. 19 Camposampiero Hersche Wattenhofer Sebastian Rahimi We report some examples of the prompts used in our experiments in Tables A.3, A.4, A.5, and A.6. The prompting style for embracing CoT was inspired by Wüst et al. (2024). For automatic retrieval of the models answer, we prompt it to provide its answer in the format My Answer: Answer #<your answer>. By default, answer panel #0 is predicted if no answer can be retrieved. Complete the Ravens progressive matrix. Your task is to select the correct Answer from the Answer set. Please decide carefully. Take deep breath and think step-by-step. Finally, give your answer in the following format: My Answer: Answer #<your answer> row 1: (3,5,5), (6,5,5), (4,5,5); row 2: (4,3,1), (3,3,1), (6,3,1); row 3: (6,1,7), (4,1,7), Answer set: Answer #0: (3,2,7) Answer #1: (7,1,5) Answer #2: (7,2,5) Answer #3: (7,2,7) Answer #4: (7,1,7) Answer #5: (3,1,7) Answer #6: (3,2,5) Answer #7: (3,1,5) Table A.3: Example prompt for an I-RAVEN task. Complete the Ravens progressive matrix. Your task is to select the correct Answer from the Answer set. Please decide carefully. Take deep breath and think step-by-step. Finally, give your answer in the following format: My Answer: Answer #<your answer> row 1: (6,16,9), (7,15,9), (70,14,9), (93,13,9), (88,12,9), (77,11,9), (83,10,9), (22,9,9), (39,8,9), (27,7,9); row 2: (7,12,24), (70,11,24), (93,10,24), (88,9,24), (77,8,24), (83,7,24), (22,6,24), (39,5,24), (27,4,24), (6,3,24); row 3: (70,35,52), (93,34,52), (88,33,52), (77,32,52), (83,31,52), (22,30,52), (39,29,52), (27,28,52), (6,27,52), Answer set: Answer #0: (7,26,52) Answer #1: (83,55,52) Answer #2: (7,26,37) Answer #3: (83,55,37) Answer #4: (7,55,52) Answer #5: (83,26,37) Answer #6: (7,55,37) Answer #7: (83,26,52) Table A.4: Example prompt for an I-RAVEN-X task. 20 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Complete the Ravens progressive matrix. Your task is to select the best matching Answer from the Answer set. Please decide carefully. Take deep breath and think step-by-step. Finally, give your answer in the following format: My Answer: Answer #<your answer> 2: 1: (290,898,875,416,729,621,255,121,775,992,332,824,69), (917,854,889,837,449,40,616,988,225,603,813,154,860), row (532,852,889,336,540,95,33,182,41,215,990,859,625), (31,850,889,846,149,643,802,187,413,101,300,378,181), (574,848,889,484,18,951,173,279,247,567,639,939,730), (576,846,889,547,182,955,995,410,545,537,859,368,146), (291,845,889,544,515,965,647,155,660,835,167,363,578); row (25,896,875,507,14,482,3,638,723,822,326,152,311), (43,894,875,645,712,987,788,382,795,149,295,457,63), (761,892,875,621,590,319,785,4,122,627,517,924,88), (291,890,875,860,69,712,754,590,214,674,171,773,227), (917,889,875,802,908,433,515,585,256,102,529,939,585); row (31,495,831,79,825,847,494,174,270,472,649,164,234), (574,493,831,553,583,258,422,840,680,109,870,539,289), (576,491,831,882,329,883,287,624,816,453,120,316,349), (917,489,831,611,791,841,260,28,125,408,122,577,903), (532,497,831,73,406,82,149,646,932,466,196,966,172), 3: (532,897,875,617,602,91,626,959,328,566,572,496,129), (31,895,875,551,141,165,894,867,142,856,245,396,325), (574,893,875,269,762,290,698,804,252,56,328,850,702), (576,891,875,268,299,764,678,718,860,626,845,523,1), (25,496,831,76,880,109,467,76,845,392,673,736,51), (43,494,831,39,960,182,917,180,643,977,698,321,467), (761,492,831,481,548,81,43,180,359,410,733,702,708), (291,490,831,398,434,521,426,600,224,181,827,281,512), (290,853,889,310,920,885,291,416,926,503,379,786,859), (25,851,889,948,465,970,253,795,956,622,323,735,535), (43,849,889,700,975,580,488,662,820,977,189,160,955), (761,847,889,971,245,547,175,991,94,306,976,778,188), Answer set: Answer #0: (290,488,875,657,175,669,825,660,980,305,71,297,764) Answer #1: (851,488,875,785,95,663,714,937,607,543,958,80,215) Answer #2: (290,451,831,808,72,151,7,665,312,920,665,806,177) Answer #3: (290,488,831,340,114,819,129,10,922,744,948,540,925) Answer #4: (851,451,875,714,337,713,987,115,520,218,644,222,463) Answer #5: (851,488,831,948,251,490,394,977,846,124,951,827,501) Answer #6: (290,451,875,761,816,59,950,670,732,542,237,552,272) Answer #7: (851,451,831,9,552,304,979,949,86,118,847,82,575) Table A.5: Example prompt for the I-RAVEN-X task with confounders. Complete the Ravens progressive matrix. You are given context matrix of 3 rows and 10 colums. Each element in the matrix has multiply attributes, embedded in round brackets (). Each attribute is described with probability distribution, e.g., <p_a::v_a, p_b::v_b> describes that the attribute has value v_a with probability p_a and value v_b with probability p_b. Your task is to select the best matching Answer from the Answer set. Please decide carefully. Take deep breath and think step-by-step. Finally, give your answer in the following format: My Answer: Answer #<your answer> 1: 2: <0.02::853,0.62::854,0.36::855>, (<0.01::289,0.81::290,0.18::291>, (<0.21::916,0.53::917,0.26::918>, row <0.24::888,0.64::889,0.12::890>), (<0.09::289,0.75::290,0.16::291>, <0.12::852,0.74::853,0.14::854>, <0.11::888,0.85::889,0.04::890>), (<0.44::531,0.55::532,0.01::533>, <0.36::851,0.63::852,0.01::853>, <0.24::888,0.74::889,0.02::890>), (<0.09::24,0.88::25,0.03::26>, <0.03::850,0.97::851,0.00::852>, (<0.08::30,0.58::31,0.34::32>, <0.02::849,0.97::850,0.01::851>, <-0.00::888,0.91::889,0.09::890>), <0.04::888,0.76::889,0.20::890>), (<0.20::42,0.51::43,0.29::44>, <0.01::848,0.97::849,0.02::850>, <0.25::888,0.70::889,0.05::890>), (<0.12::573,0.87::574,0.01::575>, <0.06::847,0.78::848,0.16::849>, <0.01::888,0.99::889,0.00::890>), (<0.04::760,0.82::761,0.14::762>, <0.08::846,0.70::847,0.22::848>, <0.04::888,0.77::889,0.19::890>), (<0.04::575,0.54::576,0.42::577>, <0.46::845,0.54::846,-0.00::847>, <0.01::888,0.91::889,0.08::890>), (<0.15::290,0.85::291,0.00::292>, <0.04::844,0.78::845,0.18::846>, <0.30::888,0.66::889,0.04::890>); <0.20::874,0.72::875,0.08::876>), row (<0.12::24,0.72::25,0.16::26>, (<0.07::531,0.82::532,0.11::533>, <0.37::896,0.54::897,0.09::898>, <-0.00::874,0.77::875,0.23::876>), (<0.19::30,0.74::31,0.07::32>, <0.20::894,0.61::895,0.19::896>, <0.01::895,0.78::896,0.21::897>, <0.34::874,0.66::875,-0.00::876>), <0.00::874,0.99::875,0.01::876>), (<0.20::42,0.77::43,0.03::44>, <0.02::893,0.95::894,0.03::895>, <0.08::874,0.73::875,0.19::876>), (<0.05::573,0.85::574,0.10::575>, <0.08::892,0.91::893,0.01::894>, <0.06::874,0.81::875,0.13::876>), (<0.14::760,0.53::761,0.33::762>, <0.15::891,0.65::892,0.20::893>, <0.13::874,0.66::875,0.21::876>), (<0.05::575,0.65::576,0.30::577>, <0.01::890,0.82::891,0.17::892>, <0.12::874,0.66::875,0.22::876>), (<0.00::290,0.94::291,0.06::292>, <0.02::889,0.95::890,0.03::891>, <0.12::874,0.86::875,0.02::876>), (<0.14::916,0.84::917,0.02::918>, <0.02::888,0.95::889,0.03::890>, <0.01::874,0.54::875,0.45::876>); <0.07::830,0.62::831,0.31::832>), row (<0.17::30,0.56::31,0.27::32>, (<0.20::24,0.79::25,0.01::26>, <0.27::494,0.64::495,0.09::496>, <0.02::830,0.98::831,0.00::832>), (<0.00::42,0.98::43,0.02::44>, <0.38::493,0.58::494,0.04::495>, <0.19::830,0.53::831,0.28::832>), (<0.07::573,0.52::574,0.41::575>, <0.01::492,0.99::493,0.00::494>, <0.01::830,0.81::831,0.18::832>), (<0.26::760,0.55::761,0.19::762>, <0.13::491,0.83::492,0.04::493>, <0.05::830,0.82::831,0.13::832>), (<0.47::575,0.52::576,0.01::577>, <0.15::490,0.59::491,0.26::492>, <0.16::830,0.81::831,0.03::832>), (<0.03::290,0.82::291,0.15::292>, <0.29::489,0.52::490,0.19::491>, <0.03::830,0.85::831,0.12::832>), (<0.08::916,0.81::917,0.11::918>, <0.05::488,0.83::489,0.12::490>, <0.09::830,0.64::831,0.27::832>), <0.06::830,0.92::831,0.02::832>), (<0.21::531,0.77::532,0.02::533>, <0.01::496,0.88::497,0.11::498>, <0.19::897,0.59::898,0.22::899>, <0.19::495,0.62::496,0.19::497>, 3: Answer set: Answer #0: (<0.06::289,0.83::290,0.11::291>, <0.00::487,1.00::488,0.00::489>, <0.03::874,0.82::875,0.15::876>) Answer #1: (<0.01::850,0.78::851,0.21::852>, <0.00::487,0.99::488,0.01::489>, <0.08::874,0.85::875,0.07::876>) Answer #2: (<0.03::289,0.57::290,0.40::291>, <0.15::450,0.75::451,0.10::452>, <0.15::830,0.62::831,0.23::832>) Answer #3: (<0.06::289,0.52::290,0.42::291>, <0.03::487,0.92::488,0.05::489>, <0.31::830,0.61::831,0.08::832>) Answer #4: (<0.02::850,0.95::851,0.03::852>, <0.16::450,0.63::451,0.21::452>, <0.20::874,0.52::875,0.28::876>) Answer #5: (<0.02::850,0.86::851,0.12::852>, <0.18::487,0.80::488,0.02::489>, <0.14::830,0.79::831,0.07::832>) Answer #6: (<0.01::289,0.96::290,0.03::291>, <0.38::450,0.59::451,0.03::452>, <0.08::874,0.68::875,0.24::876>) Answer #7: (<0.08::850,0.62::851,0.30::852>, <0.15::450,0.82::451,0.03::452>, <0.09::830,0.87::831,0.04::832>) Table A.6: Example prompt for the I-RAVEN-X task with smooth distributions. 21 Camposampiero Hersche Wattenhofer Sebastian Rahimi Appendix B. Comparison between OpenAI o3-mini and o1 This Appendix presents small ablation study on two different closed-source LRMs, OpenAI o1 and OpenAI o3-mini. The goal of these experiments was to measure the difference, if any, in the reasoning capabilities of the o3-mini model compared to its bigger, more expensive predecessor. We restricted the size of the test set to 100 test examples for both I-RAVEN and I-RAVEN-X. The results, presented in Table B.7, show that the two models achieve roughly comparable performance on both I-RAVEN and I-RAVEN-X, with o3-mini being consistently slightly less accurate than o1. However, o1 is also considerably more expensive compared to o3: o1 is priced at $15 and $60 per million input and output tokens, respectively, while o3-mini costs only $1.1 and $4.4 per million input and output tokens (approximately 14 less expensive). Hence, we opt to use only o3-mini in the full evaluation. I-RAVEN I-RAVEN-X Model Setting Range 10 Range 1000 Range 100 Task Arithm. Task Arithm. Task Arithm. OpenAI o1 Entangled OpenAI o3-mini Entangled 88.0 86.6 79.7 81.4 86.0 84.0 68.2 63.6 86.0 81.0 68.2 60. Table B.7: Task and arithmetic accuracy (%) comparison of two different LRMs on subset of 100 test examples of I-RAVEN and I-RAVEN-X. 22 Can LRMs do Analogical Reasoning under Perceptual Uncertainty? Appendix C. I-RAVEN noisy attributes are not noisy This Appendix highlights one major limitation of the so-called noise attributes in RAVEN and I-RAVEN (Orientation and Uniformity). These attributes, in reality, do not introduce any noise in the reasoning process for two reasons: these attributes values always respect one of the underlying rules of RAVEN (e.g., in the example shown in Figure C.3, Orientation can be inferred using the constant rule); hence, they do not introduce any noise if used along the other main attributes to learn the rules of RAVEN in data driven fashion; at inference time, these attributes do not reduce the signal-to-noise ratio of of the RAVEN examples and do not change the probability distribution over the candidate panels (e.g., in Figure C.3 all candidates are equally likely, and this will not influence the final prediction of the answer panel). As result, these attributes alone do not increase the difficulty of RAVEN on their own. The confounders introduced for I-RAVEN-X in Section 3 address both problems, being sampled at random in the dynamic range of each attribute. 2 4 22 7 7 ? 7 7 7 7 7 7 7 Figure C.3: Example of the Orientation attribute in I-RAVEN, showing an example 3 3 on the left and the eight candidate panels on the right. 23 Camposampiero Hersche Wattenhofer Sebastian Rahimi Appendix D. Additional experimental results for ARLC This appendix presents additional results on our neuro-symbolic baseline, ARLC, which were not included in the main manuscript for space constraints. Firstly, we ablate the effectiveness of the entropy regularization proposed in Section 4.2 by integrating it in ARLC and comparing this improved version with the vanilla counterpart of the model. We perform this ablation on two different settings: 1. training and inference with confounders, to test whether the model can still learn the correct set of rules underlying RAVEN examples in settings with noisy supervision; 2. training on clean data, inference with confounders, to test the inference-time robustness of the model. Naturally, the setting where the model is also trained on confounding attributes is more challenging, as the training signal that can be used to learn the rules underlying the task linearly decreases in the number of confounding attributes used. We report the results of this ablation in Table D.8 on both dynamic ranges supported by I-RAVEN-X. As it can be observed from the results, entropy regularization is significantly helpful both when used as training+inference or inference-only technique. In the latter case, it becomes increasingly more effective compared to the vanilla model as the number of attributes increases. To stress the robustness of the proposed entropy regularization, we also test under extreme noise conditions (20 dB, 300 confounder attributes). Since the evaluation of the model with this many attributes starts becoming increasingly expensive, we limit the evaluation to the 1000 range subset and reduce the number of different seeds used from 5 to 3. We observe that, while the average task accuracy starts dropping, some of the runs can Training data Entropy Confounders SNR Range 100 I-RAVEN-X Noisy Clean 0 5 10 5 10 30 100.0/96.9 90.6/83.1 99.3/88.3 2.2 5. 85.7/78.4 94.6/88.3 - - +8.7/5.2 - +8.9/9.9 - 2.2 - 100.0/96.9 95.3/94.2 100.0/96.4 +4.7/2.2 93.7/91.4 100.0/96.0 +6.3/4.6 90.5/82.0 99.5/94.3 +9.0/12.3 - - 10 5.2 Range 98.8/94.0 88.2/80.1 99.1/85.6 83.6/76.9 92.1/86.0 98.8/94.0 95.3/91.7 98.6/92.9 92.5/89.5 98.8/92. 88.4/82.0 98.7/92.0 97.5/83.1 - - +10.9/5.5 - +8.5/9.1 - - +3.3/1.2 - +6.3/3.1 - +10.3/10 - 300 20 - - Table D.8: Ablation of the proposed entropy regularization method using neuro-symbolic ARLC. Can LRMs do Analogical Reasoning under Perceptual Uncertainty? still achieve remarkable accuracy, indicating that this technique potentially enables probabilistic abductive reasoning models to work well in settings where only tiny fraction of the extracted attributes are important for the reasoning task. On top of the experiments on smoothened distributions included in the main text, we also study the robustness of ARLC when the input distributions are perturbed using Gaussian filter. This represents more general setting compared to the three-bin smoothening strategy adopted in the main text, which was primarily chosen to limit the complexity of the prompt for LRMs. In particular, we smoothen the input distribution using the Gaussian filter G(x) = 1 2πσ (cid:18) exp (cid:19) (x µ)2 2σ2 where µ corresponds to the index of the true value and σ is tunable parameter that regulates how flat the resulting distribution is. We study different settings and combinations of training and inference perturbations to gain more comprehensive picture of the behavior of ARLC in this setup. In particular, we evaluate three separate settings: 1. training and testing with noisy distributions, to understand how the model would behave in settings where uncertainty on the attributes values is always present; 2. training on noisy distributions and evaluating on clean data, to evaluate whether the model can learn valid rules from noisy data; 3. training on clean data and evaluated with noisy distributions, to understand how well model that learned the correct rules underlying RAVEN would perform when evaluated with an imperfect perception front-end. We report the results of this ablation in Table D.9. Different interesting observations can be made on these data. Firstly, we observe that training on noisy data and evaluating on clean data always results in competitive performance. This is clear indication that, despite σ Training Inference I-RAVEN (3 3) Range 10 I-RAVEN-X (3 10) Range 100 Range 1000 0.0 0.3 0. 0.7 Clean Noisy Clean Noisy Noisy Clean Noisy Noisy Clean Noisy Clean Clean Noisy Noisy Clean Noisy Noisy Clean Noisy Noisy 99.8/98.4 99.3/98.1 96.4/92.2 94.6/92.2 99.0/98.4 86.8/80.3 86.0/81. 98.9/93.0 64.9/58.8 67.6/58.0 100.0/96.9 98.8/94.0 99.8/91.3 91.6/85.0 89.2/76.7 98.9/88.6 79.2/77.8 87.2/74.7 99.0/87.5 71.9/70.0 69.8/64. 99.4/89.7 92.1/89.0 97.3/86.4 98.7/86.0 85.8/83.1 90.0/78.7 98.3/85.1 79.9/77.4 84.6/74.1 Table D.9: Gaussian smoothening of the input distributions. 25 Camposampiero Hersche Wattenhofer Sebastian Rahimi the noise in the training process, ARLC can still learn valid set of rules that yield good results on de-noise data. Even if the mean accuracy shows degradation proportional to σ (expected, since in the harshest settings the probability of the true value is lower than the sum of all the other probabilities of the non-true values), we can still recover close to perfect accuracy in some runs, as shown by the max accuracy reported for these experiments. This is encouraging since model selection using validation split would allow us to identify and select the models that learned the best set of rules during training. On the other hand, evaluating models trained on clean data (that learned good set of rules) with smoothed attributes distributions sensibly degrades the test accuracy, especially for larger values of σ. Unfortunately, not much can be done to address this. However, this is still an interesting result, as it underlines the importance of confident front-end perception in abstract reasoning, showing that excessively flattened attributes distribution can seriously undermine the accuracy of the reasoning process. Finally, we observe that training with smoothened distributions generally yields better results (at least in the maximum test accuracy) than training on clean data and evaluating with smoothened distributions. This might suggest that, sometimes, integrating uncertainty in the training process can increase the robustness of the model at inference time and guarantee better performances compared to models that have trained exclusively on clean data."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "IBM Research - Zurich"
    ]
}