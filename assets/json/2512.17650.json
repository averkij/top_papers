{
    "paper_title": "Region-Constraint In-Context Generation for Instructional Video Editing",
    "authors": [
        "Zhongwei Zhang",
        "Fuchen Long",
        "Wei Li",
        "Zhaofan Qiu",
        "Wu Liu",
        "Ting Yao",
        "Tao Mei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 5 6 7 1 . 2 1 5 2 : r Region-Constraint In-Context Generation for Instructional Video Editing* Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, and Tao Mei University of Science and Technology of China HiDream.ai Inc. {zhwzhang, weili2023}@mail.ustc.edu.cn, {longfuchen, qiuzhaofan}@hidream.ai liuwu@live.cn, {tiyao, tmei}@hidream.ai https://zhw-zhang.github.io/ReCo-page/ Figure 1. Our ReCo enables video editing based on sole textual instructions, achieving precise and high-fidelity video content modification. ReCo can adeptly handle diverse and challenging video editing tasks, including both local object editing and global style transfer."
        },
        {
            "title": "Abstract",
            "content": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, new instructional video *This work was performed at HiDream.ai. editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during incontext generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instructionvideo pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal. 1. Introduction With the rapid advancements in diffusion models [17, 21, 36, 37, 40, 42, 44, 48, 49, 67, 70, 71, 74], instruction-based visual editing for both image and video has garnered significant attention. Recent instruction-based image editing models [33, 73] are capable of editing input images based on natural language instructions without additional condition. Nevertheless, replicating the success attained in image editing within the field of instruction-based video editing is non-trivial. Some promising video editing solutions [24, 26] often require input masks to localize editing regions or task-specific configurations, limiting their practicality for use in the real-world. Steering video editing based on sole textual instruction is still problem not yet fully explored in the literature. Inspired by the success of in-context generation paradigm in image editing [25, 72] with both data efficiency and generation quality, we construct joint source-target video diffusion framework for instruction-based video editing. Due to the inherent temporal complexities, two major challenges are rising when shaping in-context learning for video generation: 1) how to accurately localize the editing region when there is only text instruction? 2) how to further decrease the content interference from source editing region to the novel object generation in target video? Following the recipe for regional constraint modeling [23] in visual processing, we address the two issues by modeling the region-wise relationship on both video latents and attention maps. We mitigate the first issue through increasing the latent discrepancy in the editing region between source and target videos, and decreasing that in the non-editing areas, which enforces content regeneration in editing area with consistency of background. To alleviate the second issue, we suppress the attention of tokens in editing region to tokens in the same area of source video, alleviating the token interference from original contents. This term also encourages the novel object generation to leverage more information from tokens in the background of target video itself, achieving better coherence with background. By consolidating the idea of region-constraint in-context generation, we present novel framework dubbed ReCo for instruction-based video editing. Technically, ReCo first concatenates the source and target videos along the leftright panel, and conducts joint video denoising for editing generation. In each training step, the paired video latents are first estimated through one-step backward diffusion process. Then, ReCo calculates latent difference between source and target video latents, and further conducts pair-wise constraint to increase the latent discrepancy of the editing region and decrease that of non-editing areas. The similar regularization term is also performed on attention maps of DiT blocks, to suppress the concentration of tokens in the editing region on the tokens of the same region in source video. Besides, the attention of tokens in the editing region to the background of target video itself are strengthened for harmonious composition between the novel objects and background. The whole framework is jointly optimized by the flow-matching diffusion loss and the two region-constraint regularization terms. The main contribution of this work is the new regionconstraint in-context generation paradigm for instructionbased video editing. Beyond the architecture design, we meticulously construct large-scale, high-quality video editing dataset, i.e., ReCo-Data, with 500K instructionvideo pairs covering wide spectrum of editing tasks to facilitate community research of instructional video editing. Extensive experiments further verify the effectiveness of ReCo in terms of both editing accuracy and quality. 2. Related Work Instruction-based Image Editing. Recently, the remarkable progress achieved by text-to-image generation [5, 9, 17, 1921, 30, 32, 36, 42, 42, 44, 48, 49, 58, 65, 67] encourages the development of instruction-guided image editing. InstructPix2Pix [9], as one representative work in this domain, establishes highly effective image editing data construction pipeline and achieves promising editing results. Subsequent works treat this data pipeline as prototype, and refine it to provide more data for training powerful instruction-based editor. Based on the recipe, multi-modal models like Emu Edit [50], OmniGen [64], ICEdit [72], HiDream-E1 [11], Flux-Kontext [33], Qwen-Image [61], and Nano-Banana [1] further unlock the complex capabilities, such as local editing and scene transformations, even without specific fine-tuning. Nevertheless, it is not trivial task to replicate the success of image editing in the realm of instructional video editing. The challenge of video editing lies not only in data scarcity but also in the critical need to simultaneously handle the intricate dependencies between spatial and temporal tokens. In this work, we address the challenges through an in-context generation paradigm along with regional constraint modeling, supported by our newly constructed ReCo-Data. Instruction-based Video Editing. Early attempts [14, 18, 28, 31, 35, 38, 41, 45, 69] on instruction-based video editing generally leverage the training-free inferFigure 2. An overview of our ReCo framework. We reformulate the instructional video editing task as an in-context generation paradigm, guided by the source video and instruction prompt. The source video is treated as an explicit condition via feeding it into an auxiliary video condition branch. To emphasize editing modifications and alleviate the tokens interference between editing and non-editing areas, ReCo introduces two region-based constraints: (1) Latent-space regularization, which increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas. (2) Attention-space regularization, which suppresses the attention of the target edit region towards the corresponding region in the source video, thereby mitigating inherent token interference, while simultaneously strengthening the attention on its own generated content. ence paradigm, which adapt pre-trained text-to-image diffusion models for frame-wise video editing. For instance, FateZero [45] edits video frames via DDIM [53] inversion. However, the lack of temporal modeling leads to the issue of temporal inconsistency. To address this, following works such as TokenFlow [18], VidToMe [35], FLATTEN [14], and RAVE [28] employ token-merging or similarity constraints to enhance temporal coherence. There are also some approaches (e.g., FlowEdit [32] and FlowDirector [34]) that exploit advanced text-to-video diffusion models for more accurate diffusion inversion. Despite the flexibility of the training-free paradigm, the video quality and the model generalization ability are still the inherent limitations. The primary obstacle for the development of trainingbased video editing is the profound scarcity of large-scale, high-quality paired training data. Early approaches overcome this difficulty using one-shot tuning techniques, such as Tune-A-Video [62] and Video-P2P [38], but still struggle to achieve ideal editing results. In another direction, several works [12, 39, 63] push the boundary of video editing in terms of both dataset construction and framework design. For example, GenProp [39] and Senorita [75] formulates the editing as an image propagation mechanism which first edits the first frame and then propagates the content modification into other frames. More recently, Lucy-Edit [56] and Ditto [2] propose to train video editing models directly on source-target videos and text instructions. Lucy-Edit concatenates source video latents with denoised video latents as the condition, while Ditto learns the condition through ControlNet manner. Concurrently, the in-context learning [25, 72] has been validated in image editing for both data efficiency and generation quality. In our work, we capitalize on such recipe and further delve into the formulation of region-wise constraint to facilitate accurate video editing. In summary, our work designs novel region-constraint in-context generation paradigm for instructional video editing. The proposed ReCo contributes by studying not only how to accurately localize editing region, but also how to further reduce the token interference between the editing and non-editing regions for coherent visual modification. 3. Our Approach Here we will introduce ReCo, novel region-constrained in-context video generation framework for instructional video editing. The overall architecture is illustrated in Figure 2. Given pair of source and target videos, ReCo reformulates the generation process into an in-context learning paradigm, achieved by width-wise concatenating the two videos for joint denoising. Simultaneously, to ensure the faithful preservation of source video information, we employ an additional video condition branch that explores the condition learning on the source video. In the training stage, we introduce two regularization terms, i.e., latent and attention regularization, to benefit accurate video editing learning without pre-specified editing regions. The latent regularization learning are conducted on the one-step backward denoised latents to amplify region-wise modifications and the consistency of background. Meanwhile, the attention regularization term suppresses the attention of newly generated objects of the target video on the source videos editing region, thereby decreasing the token interference from the original visual content. 3.1. Preliminaries: Video DiT Training To leverage the prior knowledge from pre-trained video generation models [5, 6, 8, 10, 19, 22, 30, 52, 58, 65, 68], we adopt an advanced video diffusion transformer, i.e., WanT2V-1.3B [58], as the backbone architecture for ReCo. To facilitate clear understanding of our proposal, we first review the training procedure of video DiT. Typically, most video DiT models are grounded in flow matching [17, 36] theory, which provides theoretically rigorous framework for learning continuous-time generative processes. It aims to learn vector field that smoothly transports samples from simple prior distribution P0 (e.g., Gaussian (0, 1)) to the target data distribution P1. Given the video latent x1 in training, random noise sample x0 (0, 1) and timestep [0, 1] are sampled from logit-normal distribution. Then, x0 is combined with x1 to obtain an intermediate noised latent xt via the forward diffusion process based on Rectified Flow [17]: xt = tx1 + (1 t)x0. Then, the ground-truth velocity vector is calculated as: vt = dxt dt = x1 x0. (1) (2) The video DiT model is learned to estimate this vector via: uθ(t) = u(xt, c, t; θ), (3) where xt is the noisy latent, θ represents the model parameters, and is the set of input conditions. For the instructional video editing task, comprises both the textual instruction and the source video. Therefore, the training objective is defined as the mean squared error (MSE) between the models output and the ground-truth velocity vt: = Ex0,x1,c,t (cid:13) (cid:13)u(xt, c, t; θ) vt (cid:13) 2 (cid:13) . (4) The objective illustrates that the target vector at any given timestep (i.e., the instantaneous velocity) is simply formulated as x1 x0. The target is exceptionally clear and stable, making it straightforward for the neural network to learn, which in turn yields high-quality video generation. 3.2. In-Context Generation for Video Editing The in-context generation paradigm has recently demonstrated significant advantages in image editing [25, 51, 55, 72], particularly in terms of data efficiency and generation quality. Inspired by this, we reformulate the video editing process as in-context generation. Technically, given the video latent pair (i.e., the source video xsrc and the target video xtar 1 ), we width-wise concatenate them to form single in-context video latent xic 1 1 as: 1 , xtar 1 = [xsrc xic 1 ]. (5) During model training, noise latent xic Gaussian distribution and then added to corrupt xic ducing the noisy latent xic joint source and target video denoising: 1 + (1 t)xic 0 . 0 is sampled from 1 , prot which is fed into video DiT for = txic xic (6) The ground-truth velocity vector is reformulated as: vic = dxic dt = xic 1 xic 0 . (7) Consequently, we adapt the training objective Eq.(4) to the in-context generation paradigm and form Lic as: . (8) (9) 1 ,c,t (cid:13) 2 (cid:13) (cid:13) 0 ,xic xic Lic = , c, t; θ), , c, t; θ) vic θ (t) = u(xic uic (cid:13) (cid:13)u(xic (cid:13) In our in-context generation scenario, the predicted vector uic θ (t) is required to jointly learn both the reconstruction of the source video and the generation of the edited video. Due to the high correlation between the source and target videos in editing, such joint learning facilitates strong token interaction, leading to superior video editing performance. Simultaneously, we employ video condition branch, as depicted in Figure 2, to ensure that the video condition is comprehensively learned to calibrate video denoising. Moreover, we exploit the Low-Rank Adaptation (LoRA) technique for efficient and stable video DiT fine-tuning. 3.3. Regional Constraint in Latent Space The in-context generation benefits token interaction between source and target videos for better instructional video editing. However, compared to prior advances that require pre-specified edit regions [7, 24, 26] in video editing, solely relying on textual instruction might still lead to the issue of inaccurate editing region. To alleviate this limitation, we introduce regional constraint within the latent space. This mechanism is designed to increase the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, amplifying the modification on editing area and alleviating outside unexpected content generation, respectively. Given the velocity vector uic θ (t) estimated by video DiT with timestep t, we first derive the one-step backward denoised latent ˆxic 1 based on the Rectified Flow definition as: 1 = xic ˆxic + (1 t)uic θ (t). (10) The obtained denoised video latent ˆxic 1 is then divided along width dimension to get its source and target parts as follows: [ˆxsrc 1 , ˆxtar 1 ] = ˆxic 1 . Next, we calculate the latent difference vector ˆX Dif tween ˆxsrc and ˆxtar through: 1 1 ˆX Dif 1 = (cid:12) (cid:12)ˆxtar 1 ˆxsrc 1 (cid:12) (cid:12) . (11) be- (12) For successful editing, we hypothesize that the latent discrepancy should be high within the editing region between the source and target videos, while the non-editing regions should remain unchanged. To achieve this, we introduce the latent-space regional constraint Llatent to regulate DiT training. Let be the binary latent mask indicating the editing region (where = 1 denotes regions that should be edited). Llatent aims to minimize the mean discrepancy in the non-editing regions while maximizing it within the editing regions, which is computed by: Llatent = mean (cid:16) ˆX Dif 1 (1 ) (cid:17) mean (cid:16) ˆX Dif 1 (cid:17) . (13) 3.4. Regional Constraint in Attention Space Besides the region constraint on latent space, the robust learning of in-context generation also necessitates alleviating inherent token interference between the editing and non-editing regions at attention level. For instance, there should be less concentration on the original contents of editing region in source video, and more attention on its own generated background for better coherence. To formulate these relative correlations on attention, we propose to regulate the attention map learning from two perspectives, i.e., the relative relationship within editing region, and the relative relationship within the entire video regions. As shown in the right part of Figure 2, we first partition the whole area of source-target video pair into three distinct key regions: the source videos editing region (A1), the source videos non-editing region (A2), and the entire target video region (A3). To formulate the relative relationship within editing region for attention learning, tokens from the target editing region (queries Q) should reduce their attention to the corresponding source editing region (keys K1). We define this as the edit attention loss Ledit: Ledit = mean(AttnQK1) mean(AttnQK2 ), (14) where AttnQK is the similarity score between queries and keys K. Furthermore, to guarantee coherent integration of generated content with the background, the queries should reduce the overall reliance on the entire source video (e.g., keys in A1 A2), while focusing more on the contextually relevant of target video regions (e.g., keys K3 in A3). Therefore, such type of constraint is formulated as the global attention loss Lglobal: Lglobal = mean(AttnQK) mean(AttnQK3), (15) The attention-space regional constraint is thus defined as the sum of both two components: Lattn = Ledit + Lglobal. (16) Finally, the overall training objective in our ReCo is formulated as multi-task loss by integrating basic in-context Figure 3. Comparison between existing video editing datasets and our ReCo-Data. Ours features the most balanced data distribution and has higher ratio of the high-quality samples. flow matching loss Lic and two regional constraints in both latent space Llatent and attention space Lattn: = Lic + λ1Llatent + λ2Lattn, (17) where λ1 and λ2 are trade-off parameters. The two constraints emphasize more accurate editing regions and the learning of correct token relationships, mitigating token interference for more natural video content generation. 4. Experiments 4.1. Experimental Settings Datasets. Despite recent great progress in instructional the video editing, significant bottleneck still remains: lack of large-scale, high-quality training dataset. To address this challenge, we introduce the ReCo-Data, which is meticulously curated to support four major video editing tasks: instance-level object adding, removing, and replacing, and the global video stylization. Our data construction pipeline involves six main stages: (1) raw data pre-process; (2) object segmentation; (3) instruction generation using VLLMs (i.e., Gemini-2.5-Flash-Thinking [1]); (4) condition pairs construction; (5) video synthesis using VACE [26]; and (6) video filtering and re-captioning with VLLMs. More details are provided in the supplementary material. Ultimately, we construct ReCo-Data with 500K high-quality instruction-video pairs. Each video clip contains 81 frames with the resolution of 480 832. The video duration is 5.0 seconds. We compare ReCo-Data with existing video editing datasets in terms of the ratio of high-quality samples, which reflects the usability and overall quality of the dataset. Specifically, we randomly sample 200 video editing pairs Table 1. Performance comparisons on four video editing tasks (i.e., add object, replace object, remove object and style transfer). We evaluate the video editing quality by feeding the source and target video pair into Gemini-2.5-Flash-Thinking [1] and asking the VLLM to give the rating from three major perspectives: (1) Edit Accuracy includes the sub-dimensions of Semantic Accuracy (SA), Scope Precision (SP), and Content Preservation (CP); (2) Video Naturalness contains Appearance Naturalness (VN), Scale Naturalness (SN), and Motion Naturalness (MN); (3) Video Quality includes Visual Fidelity (VF), Temporal Stability (TS), and Edit Stability (ES). The range of the score for each evaluating sub-dimension is from 0 to 10 (higher score is better). We also report the per-category scores (i.e., SEA, SV , SV Q) by computing the geometric mean of all sub-dimension scores of each major perspective, and the overall averaged score S. Task Approach Edit Accuracy (EA) Video Naturalness (VN) Video Quality (VQ) Average Score Add Replace Remove Style InsViE [63] Lucy-Edit [56] Ditto [2] ReCo InsViE [63] Lucy-Edit [56] Ditto [2] ReCo InsViE [63] VACE [26] ReCo InsViE [63] Lucy-Edit [56] Ditto [2] ReCo SA 2.60 6.27 7.46 8.65 1.89 6.57 4.95 9.38 2.53 4.58 7.43 7.59 3.73 9.10 9.11 SP 2.79 6.32 7.24 8. 2.38 7.49 4.83 9.43 2.49 4.58 7.43 8.86 5.59 9.36 9.82 CP 2.78 7.75 6.30 9.22 2.48 7.73 4.79 9. 2.44 4.56 7.17 8.49 5.39 9.26 9.54 AN 2.33 4.63 6.30 6.39 2.58 5.13 5.81 7.07 2.63 4.96 6. 6.77 4.20 8.25 8.43 SN 3.98 7.08 8.85 8.78 5.25 7.46 8.63 8.87 4.87 6.09 7.43 9.14 5.88 9.51 9. MN 3.74 6.08 8.30 8.28 5.05 6.65 8.10 8.47 4.72 5.89 7.30 9.28 5.88 9.58 9.70 VF 3.71 6.31 8.13 8.02 3.76 6.32 7.55 8.19 3.41 5.48 6.48 7.13 4.44 8.33 8.61 TS 3.91 6.82 8.55 8. 4.00 6.64 7.95 8.65 3.67 5.50 6.63 6.40 4.17 8.33 8.35 ES 3.58 7.57 9.03 9.61 3.52 8.08 8.71 9. 3.40 5.57 7.68 8.99 5.87 9.77 9.87 SEA SV SV 2.60 6.47 6.70 8. 2.10 7.08 4.56 9.43 2.44 4.57 7.28 8.17 4.65 9.20 9.42 3.10 5.70 7.57 7.55 3.91 6.21 7.21 8.01 3.76 5.43 6. 8.21 4.67 9.07 9.19 3.46 6.77 8.41 8.61 3.49 6.88 7.96 8.77 3.29 5.56 6.82 7.35 5.17 8.77 8.90 3.05 6.31 7.56 8.23 3.17 6.72 6.58 8.74 3.16 5.19 7.00 7.91 4.83 9.01 9.17 from each editing task across all datasets, and invite 10 evaluators to qualitatively assess the video editing quality. As shown in Figure 3, the ratio of high-quality samples in existing datasets (i.e., InsV2V [12], InsVIE [63], and Senorita [75]) is usually low (17.9% 29.2%). It indicates that these datasets have not undergone rigorous data cleaning processes, and the large number of low-quality samples makes them suboptimal for training high-performing instructional video editing models. Besides, the cost of data re-cleaning is extremely high, while the potential benefit is minimal due to the low frame rate, low resolution, and poor synthesis quality of previous datasets. Instead, our ReCoData has very high proportion (91.6%) of high-quality samples and well-balanced data distribution across different tasks. It can be readily used for model training without any data pre-processing. The usability of ReCo-Data is also verified by the training of our model. Benchmarks. We construct video editing evaluation benchmark which contains 480 video-instruction pairs, 120 pairs for each of the four video editing tasks. Since traditional metrics usually struggle to accurately and comprehensively evaluate video editing across various dimensions, we follow the image editing advance [60], and employ VLLM as the referee for evaluation. Considering the inherent complexity of video data, we extended the image editing metrics [60] and construct diverse set of evaluation dimensions tailored for video editing. We measure the video editing from three main aspects: (1) Edit Accuracy, with the sub-dimensions of Semantic Accuracy (SA), Scope Precision (SP), and Content Preservation (CP); (2) Video Naturalness, which includes Appearance Naturalness (AN), Scale Naturalness (SN), and Motion Naturalness (MN); and (3) Video Quality, with the dimensions of Visual Fidelity (VF), Temporal Stability (TS), and Edit Stability (ES). We obtain the per-category scores (i.e., SEA, SV , SV Q) by calculating the geometric mean of all subdimension scores of each major perspective. The overall averaged score (S) is the arithmetic mean of the three percategory scores. Specifically, we feed the source and generated video pair, and the predefined system instructions into Gemini-2.5-Flash-Thinking [1], and ask it to give the rating for video editing from all the nine sub-dimensions. More details about the benchmark construction and the full evaluation protocol are provided in the supplementary material. In ReCo, we employ WanT2V-1.3B [58] as our base architecture. Each training sample is an 81-frame video clip, with the frame rate of 16 fps and the resolution of 480 832. For mask generation to align the resolution of video latents, we first encode the editing mask via VAE [29] and then apply k-means clustering to binarize them. We set the rank of the LoRA as 128. ReCo is trained using the AdamW optimizer with two-stage learning rate schedule: the model is first trained with learning rate of 1 104 to achieve stable convergence, followed by fine-tuning stage using lower learning rate of 2 105 for further refinement. All experiments are conducted on 24 NVIDIA A800 GPUs with mini-batch size of 24. Implementation Details. 4.2. Comparisons with State-of-the-Art Methods We compare our ReCo with several state-of-the-art instructional video editing methods, including InsViE [63], Ditto [2], Lucy-Edit [56], and VACE [26], on our VLLMFigure 4. Examples of video editing (i.e., add object, replace object and style transfer) results by different approaches. Figure 5. Visual comparisons on the object removal task. based benchmark. Table 1 summarizes the performance comparisons on the four video editing tasks. Overall, ReCo consistently outperforms existing baselines on the total score across all tasks. In particular, for the local video editing, ReCo attains the total score of 8.23 on Add and 8.74 on Replace, surpassing the strong competitor Ditto (7.56) and Lucy-Edit (6.72) by 0.67 and 2.02, respectively. Significant performance trends can also be observed on the editing accuracy perspective (i.e., SEA). The results demonstrate that our ReCo not only accurately follows the instruction prompt to correctly localize the editing region but also preserves the contents of non-edited areas. In terms of video naturalness (i.e., SV ), the better performances achieved by our model verifies the efficacy of naturally integrate editing objects into source video. Although the SV of ReCo is slightly below that of Ditto on the Add task, ours can better keep original video contents while Ditto tends to re-render the whole video into different color style as shown in Figure 4. The phenomenon is also evidenced by the lower SEA score (6.70) of Ditto. Additionally, the best performance of video quality (SV Q) further indicates that the videos generated by ReCo have minimal visual artifacts or degradation. Even under the multi-task training setting (i.e., unify local editing and global stylization) that could bring some conflicts during model optimization, ReCo still manifests the strong capability for video style transfer and attains 9.17 of the total score S. All these results basically validate the merit of performing regional constraint modeling on in-context generation for instructional video editing. Figure 4 and 5 further show the video editing results on the four tasks. Generally, compared to other baselines, ReCo edits videos with better instruction following, higher video quality and better background consistency. For instance, InsViE tends to produce videos with artifacts and Table 2. Performance comparisons among different variants of ReCo on four video editing tasks. Model ReCoLC ReCoAC ReCo Add Replace Remove Style SEA 8.05 8.33 8. SV 7.44 7.37 7.55 SV 8.59 8.01 8.61 8.03 7.90 8. SEA 9.01 9.23 9.43 SV 8.01 7.94 8.01 SV 8.67 8.46 8. 8.56 8.54 8.74 SEA 6.90 7.11 7.28 SV 6.83 6.75 6. SV 6.91 6.70 6.82 6.88 6.85 7.00 SEA 9.09 9.21 9. SV 9.10 9.08 9.19 SV 8.84 8.81 8.90 9.01 9.03 9. there is dramatic performance drop on SEA, which indicates significant decay of editing accuracy. The scores of SV and SV also decrease slightly but remain comparable. The results highlight the effectiveness of latent region constraint learning to amplify accurate localization of editing region. The top part of Figure 6 further visualizes one video editing case among ReCoLC and ReCo. Given the instruction of replace the young boy on the sofa with young girl with pigtails, ReCoLC could replace the boy but incorrectly removes the nearby dog. When removing the regularization term in attention space, ReCoAC performs worse on the video naturalness perspective (i.e., SV ) as shown in Table 2. We also show one editing example in the lower part of Figure 6. As shown in the figure, ReCoAC generates big parrot which has an unnatural scale relative to the environment. With the equipment of attention regularization that reduces the token interference from editing area and strengthens the interaction with background in novel object generation, ReCo synthesizes the parrot with natural size and better coherence. 5. Conclusions We have presented ReCo that shapes in-context generation for instruction-based video editing. Particularly, we study the problem of integrating the regional constraint modeling between editing and non-editing areas into diffusion training. To materialize our idea, ReCo jointly denoises the width-concatenated source-target video pair based on the natural language instructions, and conducts two regularization terms to emphasize region-wise relationship on both one-step backward denoised latents and attention maps. To alleviate unexpected content generation in non-editing regions, the regularization term in latent space tries to decrease the latent discrepancy of non-editing regions between source and target videos, while increasing the differences at the editing area. Meanwhile, ReCo suppresses the attention of tokens in the editing region to tokens in the same part of source video, which alleviates the interference from original editing region tokens to novel object generation. Moreover, we carefully construct high-quality video editing dataset, i.e., ReCo-Data, consisting of 500K instruction-video pairs covering wide range of editing tasks. Extensive experiments across four editing tasks verify the superiority of ReCo over state-of-the-art approaches. Figure 6. Editing results on replace task among variants of ReCo. usually suffers from editing failure. Recent Lucy-Edit exhibits poor instruction-following and fails to accurately render the specified attributes (e.g., brown chimpanzee wearing hoodie). Though Ditto generates natural-looking objects in the Add task, it struggles to preserve background consistency of non-editing regions and localize the accurate editing region (e.g., adding the crown at the back of the seal). Meanwhile, the ability of instruction following for Ditto is inferior to ours, erroneously synthesizing new monkey alongside the man instead of replacing him. We speculate that these issues of Ditto are caused by the lack of regional correlation modeling for in-context generation when directly fine-tuning VACE [26] with textual instructions. Our ReCo, in comparison, regulates the in-context generation learning with the region-wise constraints to emphasize editing region localization and alleviate cross-region token interference simultaneously. Thus, the videos modified by ReCo reflect both accurate editing results and natural novel object integration with the original video background. 4.3. Ablation Study on Regional Constraint We investigate how the two regional constraints in our ReCo influence the final instruction-based video editing. Table 2 summarizes the video editing performances of different variants of our ReCo. Two additional runs are involved, i.e., ReCoLC and ReCoAC, which remove the latent and attention regional constraint in ReCo, respectively. Specifically, when the region constraint in latent space is discarded,"
        },
        {
            "title": "References",
            "content": "[1] Gemini Team Google: Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Anja Hauth Andrew M. Dai, Katie Millican, et al. Gemini: Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2023. 2, 5, 6, 1, 4 [2] Qingyan Bai, Qiuyu Wang, Hao Ouyang, Yue Yu, Hanlin Wang, Wen Wang, Shuailei Ma Ka Leong Cheng, Yanhong Zeng, Zichen Liu, Yinghao Xu, Yujun Shen, and Qifeng Chen. Scaling Instruction-Based Video Editing with HighQuality Synthetic Dataset. arXiv:2510.15742, 2025. 3, 6 [3] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-VL Technical Report. arXiv:2511.21631, 2025. 2 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [5] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models. arXiv preprint arXiv:2405.04233, 2024. 2, 3 [6] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: Space-Time Diffusion Model for Video Generation. arXiv preprint arXiv:2401.12945, 2024. 3 [7] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control. In CVPR, 2025. 4, 1 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In CVPR, 2023. [9] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. InstructPix2Pix: Learning to Follow Image Editing Instructions. In CVPR, 2023. 2 [10] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video Generation Models as World Simulators. 2024. 3 [11] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, et al. HiDream-I1: HighEfficient Image Generative Foundation Model with Sparse Diffusion Transformer. arXiv preprint arXiv:2505.22705, 2025. 2 [12] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent Videoto-Video Transfer Using Synthetic Dataset. In ICLR, 2024. 3, 6 [13] christophschuhmann. improved-aesthetic-predictor. improved-aesthetic-predictor Lab, 2024. 1 [14] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing. In ICLR, 2024. 2, 3 [15] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. PaddleOCR 3.0 Technical Report. arXiv preprint arXiv:2507.05595, 2025. 1 [16] PySceneDetect Developers. PySceneDetect. 2024. 1 [17] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In ICML, 2024. 2, 4 [18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. TokenFlow: Consistent Diffusion Features for Consistent Video Editing. In ICLR, 2024. 2, 3 [19] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. LTX-Video: Realtime Video Latent Diffusion. arXiv preprint arXiv:2501.00103, 2025. 2, [20] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-Prompt Image Editing with Cross-Attention Control. In ICLR, 2023. [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, 2020. 2 [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen Video: High Definition Video Generation with Diffusion Models. In CVPR, 2022. 3 [23] Hanzhe Hu, Jinshi Cui, and Liwei Wang. Region-Aware Contrastive Learning for Semantic Segmentation. In ICCV, 2021. 2 [24] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. HunyuanCustom: Multimodal-Driven Architecture for Customized Video Generation. arXiv preprint arXiv:2505.04512, 2025. 2, 4 [25] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yu Liu Yutong Feng, and Jingren Zhou. In-Context LoRA for Diffusion Transformers. arXiv preprint arxiv:2410.23775, 2024. 2, 3, 4 [26] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. VACE: All-in-One Video Creation and Editing. In ICCV, 2025. 2, 4, 5, 6, 8, 1, [27] Zhao Jixin, Zhou Shangchen, Wang Zhouxia, Yang Peiqing, ObjectClear: Complete Oband Loy Chen Change. ject Removal via Object-Effect Attention. arXiv preprint arXiv:2505.22636, 2025. 1 [28] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James M. Rehg, and Pinar Yanardag. RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models. In CVPR, 2024. 2, 3 [29] Diederik Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 2013. 6 [30] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, et al. HunyuanVideo: Systematic Framework For Large Video Generative Models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [31] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. AnyV2V: Tuning-Free Framework For Any Videoto-Video Editing Tasks. In TMLR, 2024. 2 [32] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models. In ICCV, 2025. 2, [33] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. FLUX.1 Kontext: Flow Matching for InContext Image Generation and Editing in Latent Space. arXiv:2506.15742, 2025. 2 [34] Guangzhao Li, Yanming Yang, Chenxi Song, and Chi Zhang. FlowDirector: Training-Free Flow Steering for Precise Textto-Video Editing. arXiv:2506.05046, 2025. 3 [35] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. VidToMe: Video Token Merging for Zero-Shot Video Editing. In CVPR, 2024. 2, 3 [36] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow Matching for Generative Modeling. In ICLR, 2023. 2, 4 [37] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, and Tat-Seng Chua. JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization. arXiv preprint arXiv:2503.23377, 2025. 2 [38] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-P2P: Video Editing with Cross-attention Control. In CVPR, 2024. 2, 3 [39] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Yijun Li Joon-Young Lee, Bei Yu, Zhe Lin, Soo Ye Kim, and Jiaya Jia. Generative Video Propagation. In CVPR, 2025. [40] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. VideoStudio: Generating Consistent-Content and MultiScene Videos. In ECCV, 2024. 2 [41] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, and Qifeng Chen. Follow Your Pose: Pose-Guided Text-to-Video Generation using PoseFree Videos. In AAAI, 2023. 2 [42] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In ICML, 2022. 2 [43] Ravi Nikhila, Gabeur Valentin, Hu Yuan-Ting, Hu Ronghang, Ryali Chaitanya, Ma Tengyu, Khedr Haitham, Radle Roman, Rolland Chloe, Gustafson Laura, Mintun Eric, Pan Junting, Alwala Kalyan Vasudev, Carion Nicolas, Wu Chao-Yuan, Girshick Ross, Dollar Piotr, and Feichtenhofer SAM 2: Segment Anything in Images and Christoph. Videos. In ICLR, 2025. 1 [44] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In ICLR, 2024. 2 [45] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. FateZero: Fusing Attentions for Zero-Shot Text-Based Video Editing. In ICCV, 2023. 2, 3 [46] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Crossdataset Transfer. IEEE TPAMI, 2020. [47] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks. arXiv preprint arXiv:2401.14159, 2024. 1 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR, 2022. 2 [49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Photorealistic Text-toFleet, and Mohammad Norouzi. Image Diffusion Models with Deep Language Understanding. In NeurIPS, 2022. 2 [50] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu Edit: Precise Image Editing via Recognition and Generation Tasks. In CVPR, 2024. 2 [51] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-Scale Text-to-Image Model with Inpainting is Zero-Shot Subject-Driven Image Generator. In CVPR, 2025. 4 [52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-Video: Text-to-Video Generation without Text-Video Data. In ICLR, 2023. 3 [53] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021. 3 [54] spaCy Developers. spaCy. 2024. 1 [55] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. OminiControl: Minimal and Universal Control for Diffusion Transformer. In ICCV, 2025. 4 [56] DecartAI Team. Lucy Edit: Open-Weight Text-Guided Video Editing. 2025. 3, 6 [57] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. In ECCV, 2020. 1 [71] Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, and Tao Mei. MotionPro: Precise MoIn CVPR, tion Controller for Image-to-Video Generation. 2025. [72] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer. In NeurIPS, 2025. 2, 3, 4 [73] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer. In NeurIPS, 2025. 2 [74] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer. In CVPR, 2025. 2 [75] Bojia Zi, Penghui Ruan, Xianbiao Qi Marco Chen, Shaozhe Hao, Shihao Zhao, Youze Huang, Bin Liang, Rong Xiao, and Kam-Fai Wong. Senorita-2M: High-Quality Instructionbased Dataset for General Video Editing by Video Specialists. In NeurIPS, 2025. 3, 6 [58] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 4, 6 [59] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation. arXiv preprint arXiv:2305.10874, 2023. [60] Cong Wei, Zheyang Xiong, Xinrun Du Weiming Ren, Ge Zhang, and Wenhu Chen. OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision. In ICLR, 2025. 6, 2 [61] Chenfei Wu, Jiahao Li, Junyang Lin Jingren Zhou, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, et al. Qwen-Image Technical Report. arXiv:2508.02324, 2025. 2 [62] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. In ICCV, 2023. 3 [63] Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction. In ICCV, 2025. 3, 6 [64] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. OmniGen: Unified Image Generation. In CVPR, 2025. 2 [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In ICLR, 2025. 2, [66] Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, and Li Yuan. OpenS2V-Nexus: Detailed Benchmark and Million-Scale arXiv preprint Dataset for Subject-to-Video Generation. arXiv:2505.20292, 2025. 1 Jiebo Luo, [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In ICCV, 2023. 2 [68] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Bingyue Peng, and Zehuan Yuan. Waver: Wave Your Way to Lifelike Video Generation. arXiv preprint arXiv:2508.15761, 2025. 3 [69] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards Consistent Video Editing with Text-to-Image Diffusion Models. In NeurIPS, 2023. 2 [70] Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, and Tao Mei. TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models. In CVPR, 2024. Region-Constraint In-Context Generation for Instructional Video Editing Supplementary Material The supplementary material contains: 1) the construction pipeline of ReCo-Data; 2) the details of VLLM-based benchmark for evaluation; 3) the implementation details of baselines and ReCo; 4) the generalization ability of ReCo. 1. Construction Pipeline of ReCo-Data Though instructional video editing has seen remarkable advances recently, the absence of large-scale, high-quality training datasets remains critical hurdle. To overcome this, we present ReCo-Data, dataset carefully designed to facilitate four key editing tasks: instance-level object addition, removal, replacement, and global video stylization. As illustrated in Figure 7, the construction pipeline of ReCo-Data consists of six primary stages: (1) raw data preprocessing, where we filter raw video data based on specific quality criteria; (2) object segmentation, extracting object mask from videos; (3) instruction generation, employing VLLM (i.e., Gemini-2.5-Flash-Thinking [1]) to construct editing prompts; (4) condition pair construction, which involves first frame editing and depth map generation to prepare the input conditions for VACE [26]; (5) video synthesis, employing VACE to generate videos based on conditions; and (6) video filtering and re-captioning, where VLLM (i.e., Gemini-2.5-Flash-Thinking [1]) is leveraged again to filter out low-quality samples and re-caption remained videos. 1.1. Raw Data Pre-processing Data Collection. To ensure data diversity, we collect raw videos from multiple sources, including the HD-VG [59], OpenS2V-Nexus [66], and videos from the Pixel website [7]. We employ PySceneDetect [16] to segment the long, multi-scene videos into shorter, manageable clips. Data Filtering. We first filter clips based on basic metadata, retaining those with duration exceeding 5 seconds, frame rate greater than 24 fps, and resolution of at least 720P. Then, we utilize aesthetic scores [13] and optical flow [57] to select videos characterized by high aesthetic quality and appropriate motion magnitude. Finally, to ensure visual purity, we employ PaddleOCR [15] for watermark detection, spatially cropping the frames to exclude any text detected with confidence score exceeding 0.7. Video Captioning. For subsequent object segmentation and editing prompt construction, we utilize Qwen2.5-VL32B [4] to obtain detailed descriptions of remained videos. 1.2. Object Segmentation To enable precise instance-level object editing (e.g., replacement and removal) by using video inpainting models like VACE [26], we need to first isolate the target objects. Given the complexity of scenes containing multiple objects, we adopt systematic segmentation approach. First, we define taxonomy and employ Named Entity Recognition (NER) model, i.e., SpaCy [54], to extract relevant entity nouns from video captions. Subsequently, we utilize Grounding Dino [47] to detect objects and obtain their bounding boxes. To ensure the quality of the proposals, we apply Non-Maximum Suppression (NMS) to filter out duplicate boxes with overlaps exceeding 25% and discard boxes that are disproportionately large or small. Finally, using the bounding boxes as prompts, we employ SAM 2 [43] to generate mask sequences for the target objects. 1.3. Instruction Generation The protocols to prepare editing prompts exhibit variations across local editing and global video stylization tasks. Local Editing. For the local video editing tasks, we provide Gemini with tuple containing original video caption and one representative key frame. In this key frame, the target object region is explicitly highlighted with red convex hull. Guided by finely tuned system prompt, Gemini is required to generate an appropriate editing instruction along with target video caption describing the post-edit state. Video Stylization. The process is analogous to local editing. We leverage Geminis creative capabilities to brainstorm diverse stylization instructions and generate the corresponding target video descriptions. 1.4. Condition Pair Construction In this stage, we leverage the full capabilities of existing models to construct optimal condition pairs, which are fed into VACE [26] for edited video generation. For each editing task, specific strategy is used for condition generation. Object Removal. Inputting the masked video (derived from object masks) and target prompt into VACE for edited video generation often fails to eliminate the object cleanly or leads to the hallucination of unexpected contents. To mitigate this issue, we adopt two-stage approach for edited video generation. First, we employ ObjectClear [27] on the first frame to perform clean object removal. Next, we concatenate this edited frame with the subsequent masked video frames, which is then fed into VACE to perform video inpainting, yielding stable and high-quality object removal. Figure 7. An overview of our data construction pipeline. The process consists of six main stages: raw data pre-processing, object segmentation, instruction generation, condition pair construction, video synthesis, and video filtering and re-captioning. Object Addition. We treat this task as the inverse of object removal. Once valid removal pair is generated, we simply swap the source and edited videos to create corresponding training pair for the object addition task. Object Replacement. VACE demonstrates robust performance on object replacement. Therefore, we simply feed the masked video sequence and the target video prompt into VACE to generate high-quality replacement results. Video Stylization. Although VACE supports video stylization conditioned on depth maps, maintaining the content structure of the original video is not satisfactory. Thus, we employ strategy similar to object removal. We first utilize FLUX.1 Kontext [33] to apply the style transfer on the first frame. Subsequently, we concatenate the edited frame with the depth map sequence (extracted via MiDaS [46]) to serve as the input condition pair for VACE, thereby generating temporally consistent stylized video. Specifically designed for video stylization, our pipeline addresses common artifacts in VACE-generated data, such as frame collapse, abrupt transitions, temporal inconsistency, and content distortion (e.g., facial deformations). To ensure high-quality output, we implement two-stage strategy. First, we use Qwen-3-VL-Instant [3] to filter for smooth and stable videos, removing low-quality frames with severe artifacts or flickering. Second, we refine the selected videos using the 14B Wan-2.2-T2V [58] model. These combined strategies enable the synthesis of stylized videos with significantly improved visual and temporal quality. 1.5. Video Synthesis Once the requisite condition pairs are prepared, we execute VACE in large-scale batches to synthesize high-quality editing videos. To maximize the utility of the synthesized data and ensure efficient construction, we design data augmentation strategy to generate additional training pairs without extra computational cost. Reversible Replacement. For the object replacement based on one source video, we treat such process as reversible. By swapping the source and target videos, we effectively double the volume of the replacement data. Cross-Task Augmentation. In fact, the edited videos generated from object removal and replacement share the same clean background. Therefore, the synthesized video from the replacement (containing novel object) can be paired with the background video from the removal task. This allows us to construct new removal pairs (new object background) and adding pairs (background new object), effectively doubling the dataset size for both tasks. Finally, we totally construct approximately 800K video pairs for the four editing tasks. The entire data synthesis process required approximately 76,800 GPU hours on NVIDIA RTX 4090. 1.6. Video Filtering and Re-captioning To pursue high quality of instruction-video pairs, we employ the VLLM, i.e., Gemini-2.5-Flash-Thinking, to evaluate and filter out low-quality samples in total 800K video pairs. We extract representative key frames from the source and edited videos, and concatenate them into side-by-side layout to facilitate VLLM assessment. The remained video pairs are re-captioned by VLLM. The entire caption process (including Sec. 1.3) incurred total cost of approximately $13, 600. Finally, we construct ReCo-Data with 500K highquality instruction-video pairs. Each video clip contains 81 frames with the resolution of 480 832 and duration of 5 seconds. 2. VLLM-based Evaluation Benchmark Traditional video generation metrics often struggle to accurately assess the fidelity and quality of video editing. Inspired by recent image editing evaluation protocols [60], we propose VLLM-based evaluation benchmark to comprehensively and effectively assess video editing quality. Testing Data. We collect 480 video-instruction pairs as the testing data, distributed evenly with 120 pairs for each of the four tasks (i.e., object add, remove, replace, and video stylization). All source videos are collected from Pexels video platform. For local editing tasks (i.e., object add, remove and replace), we utilize Gemini-2.5-FlashThinking [1] to brainstorm and generate diverse editing instructions based on the video content. For rigorous evaluation on video stylization, we randomly select 10 source videos and apply 12 distinct styles to each, resulting in 120 evaluation pairs. Evaluation Metrics. While previous image-based metrics primarily focus on editing accuracy and static generation quality, evaluating video editing entails greater complexity. To address this, we construct diverse set of evaluation dimensions specifically tailored for video. Corresponding system prompt designed for the VLLM is presented in Figure 8, which evaluates performance across three major perspectives, comprising total of nine sub-dimensions: Edit Accuracy (SEA): evaluate how well the result aligns with the instruction. Semantic Accuracy (SA): Does the edited video correctly follow the semantics of the text instruction? Scope Precision (SP): Is the editing confined strictly to the target region without affecting the background? Content Preservation (CP): Are the non-edited regions or original details faithfully preserved? (For stylization, this corresponds to structural preservation.) Video Naturalness (SV ): evaluates the realism and coherence of the generated content. Appearance Naturalness (AN): Are the lighting, texture, and color of the edited video natural? Scale Naturalness (SN): Is the size and proportion of the edited object reasonable relative to the environment? (For stylization, this captures cases where the stylized object becomes unreasonably large.) Motion Naturalness (MN): Does the movement of the edited object (or the style rendering) follow physically plausible dynamics? Video Quality (SV Q): evaluates the fundamental visual quality of the edited video. Visual Fidelity (VF): Is the video clear, sharp, and free from visual artifacts? Temporal Stability (TS): Is the video free from flickering or jittering across frames? Edit Stability (ES): Is the edited content consistently preserved in identity and appearance throughout the video duration? The VLLM rates the score for each sub-dimension from 0 to 10. Then, we attain the per-category scores (i.e., SEA, SV , SV Q) by calculating the geometric mean of their respective sub-dimensions as: SEA = 3 SV = 3 SV = 3 SA SP CP , AN SN , S ES. (18) (19) (20) Finally, the overall score is calculated as the arithmetic mean of the three per-category scores: = 1 3 (cid:0)SEA + SV + SV (cid:1). (21) 3. Implementation of Baselines and ReCo Baseline Settings. For recent video editing advances, few methods possess the versatility to handle all four editing tasks simultaneously. Here, we outline the criteria for our baseline selection. For object addition, replacement, and video stylization, we benchmark against InsViE [63], Lucy-Edit [56], and Ditto [2]. Since InsViE is constrained to an input of 49 frames at 480 720 resolution, we adapt our test videos via uniform temporal down-sampling and spatial resizing to match the requirements. For the object removal, effective instruction-based baselines are scarce. To facilitate meaningful comparison, we include VACE [26] as an additional baseline. Unlike instructionbased methods, VACE requires both an explicit object mask and target video prompt to perform removal. Note that VACE exhibits some instability in this implementation. Implementation Details of ReCo. ReCo is built upon the Wan [58] architecture and trained on ReCo-Data using the AdamW optimizer. We employ two-stage learning rate schedule: an initial phase with learning rate of 1 104 to ensure stable convergence, followed by finetuning phase at 2 105 for precise refinement. Regarding the loss weights, the initial values of the latent and attention constraints typically fall within [1, 1], whereas the MSE loss of flow matching is approximately 0.03. To balance the impacts of gradients, we scale the magnitude of each region-constraint loss to be roughly 0.1 that of the MSE loss. Consequently, we set the weighting coefficients (λ1 and λ2) as 1 103. All experiments were conducted on cluster of 24 NVIDIA A800 GPUs with total mini-batch size of 24, requiring approximately 10 days for training. 4. Generalization Ability of ReCo Interestingly, as depicted in In Figure 9, we observe that ReCo can generalize to abstract and creative editing tasks. For instance, it successfully synthesizes halo on womans head, generates cascading confetti effect, places an idea lightbulb beside mans head, and creates smoke emitting from computer. We attribute such generalization ability of ReCo to effectively inheriting and leveraging the rich priors from the pre-trained video diffusion model. Figure 8. The system prompts that are fed into Gemini-2.5-Flash-Thinking [1] for video editing assessment. We require VLLM to evaluate the four video editing tasks from three major perspectives, i.e., edit accuracy, video naturalness and video quality. Figure 9. Four examples of instructional video editing by ReCo to verify the generalization ability. Our model demonstrates the strong generalization to the abstract and creative editing tasks."
        }
    ],
    "affiliations": [
        "HiDream.ai Inc.",
        "University of Science and Technology of China"
    ]
}