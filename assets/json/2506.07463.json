{
    "paper_title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models",
    "authors": [
        "Guang Liu",
        "Liangdong Wang",
        "Jijie Li",
        "Yang Yu",
        "Yao Xu",
        "Jiabei Chen",
        "Yu Bai",
        "Feng Liao",
        "Yonghua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly $35$ TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose a novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract $4.5$ billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora."
        },
        {
            "title": "Start",
            "content": "CCI4.0: Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models Yang Yu Yao Xu Guang Liu Liangdong Wang Jiabei Chen Yu Bai Jijie Li Feng Liao Yonghua Lin 5 2 0 J 9 ] . [ 1 3 6 4 7 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce CCI4.0, large-scale bilingual pre-training dataset engineered for superior data quality and diverse human-like reasoning trajectory. CCI4.0 occupies roughly 35 TB of disk space and comprises two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines 5.2 TB carefully curated Chinese web corpus, 22.5 TB English subset from Nemotron-CC, and diverse sources from math, wiki, arxiv, and code. Although these data are mostly sourced from well-processed datasets, the quality standards of various domains are dynamic and require extensive expert experience and labor to process. So, we propose novel pipeline justifying data quality mainly based on models through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. We extract 4.5 billion pieces of CoT(Chain-of-Thought) templates, named CCI4.0M2-CoT. Differing from the distillation of CoT from larger models, our proposed staged CoT extraction exemplifies diverse reasoning patterns and significantly decreases the possibility of hallucination. Empirical evaluations demonstrate that LLMs pre-trained in CCI4.0 benefit from cleaner, more reliable training signals, yielding consistent improvements in downstream tasks, especially in math and code reflection tasks. Our results underscore the critical role of rigorous data curation and human thinking templates in advancing LLM performance, shedding some light on automatically processing pretraining corpora."
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) have achieved remarkable success across broad spectrum of natural language processing tasks, including text generation, translation, and sentiment analysis. critical factor underpinning these advances is the availability and quality of large-scale pre-training data [24, 33, 16]. Pre-training data not only shapes the linguistic capabilities of LLMs but also plays central role in determining their generalization and reasoning abilities across wide range of downstream applications. Despite growing efforts to curate and release open-source datasets for language model training [31, 30, 17], there remains significant gap in the availability of high-quality and diverse largescale corpora. Most existing resources are limited in either linguistic diversity or domain coverage, constraining the models ability to generalize beyond narrow contexts. Furthermore, although real-world data is often prioritized for its authenticity, synthetic data has emerged as an important complement, particularly in fostering reasoning skills. Nevertheless, high-quality synthetic pretraining datasetsespecially those that explicitly incorporate structured reasoningare still notably lacking. Corresponding Author, liuguang@baai.ac.cn Preprint. To address the aforementioned gaps and promote the advancement of data-centric development in large language models, we introduce CCI4.0-M2-Base: high-quality and diverse bilingual corpus in Chinese and English. In particular, CCI4.0-M2-Base includes large-scale, bilingual pretraining dataset (35T tokens) combining Chinese corpus and Nemotron-CCs English data, with meticulously-designed pipeline to yield high-quality dataset to enhance LLM general and reasoning capabilities. Moreover, to enhance the diversity of the corpus, we additionally incorporate high-quality source data from wide range of domains, including web pages, code, mathematics, academic papers, and encyclopedias. Furthermore, considering that the reasoning capabilities of LLMs are primarily developed during the pretraining phase [15, 39, 1], we provide CCI4.0-M2-CoT, which integrates 4.5 billion human thinking templates synthesized from high-quality samples using advanced techniques. While effective for general language understanding, traditional pretraining datasets often lack the specialized content needed to foster advanced reasoning skills, such as explicit representations of human thought processes or logical reasoning traces. To address this gap, we introduce CCI4.0-M2-CoT, whose templates are crafted to embed diverse reasoning patterns, strengthening the foundational reasoning abilities of LLMs, and decreasing the possibility of hallucination. Experimental results validate the efficacy of CCI4.0, demonstrating substantial performance improvements on knowledge-based and reasoning-intensive benchmarks such as MMLU and ARC-Challenge, with notable gains in commonsense reasoning and mathematical problem-solving. These findings underscore the critical role of high-quality, diverse and reasoning-focused pretraining data in advancing LLMs ability to tackle complex, multi-step reasoning tasks. By addressing the limitations of existing datasets, CCI4.0 sets new benchmark for pretraining and paves the way for the development of more capable and versatile language models. This paper makes the following core contributions: Introduction of CCI4.0-M2-Base: large-scale, bilingual pretraining dataset (35T tokens) combining Chinese corpus, Nemotron-CCs English data and corpora sourced from diverse domains, designed to enhance LLM general and reasoning capabilities. Incorporation of CCI4.0-M2-CoT: Integration of Diverse Reasoning Templates including 4.5 billion synthesized human thinking templates, embedding diverse reasoning patterns to bolster logical and commonsense reasoning and decrease the hallucination. Advanced Data Processing Pipeline: comprehensive methodology including deduplication, multi-classifier quality scoring, fluency filtering, CoT synthesis, and privacy/toxicity handling, ensuring high-quality and diverse data curation. Empirical Validation: Demonstration of significant performance gains on benchmarks like MMLU and ARC-Challenge, particularly in mathematical problem-solving and commonsense reasoning, outperforming baseline datasets such as Nemotron-CC-HQ and CCI3-HQ. Table 1: Dataset Comparison. Column abbreviations: Size (Open Source Size), Multi-Src (MultiSource), Multi-Cls (Multiple Classifiers), Multi-Lang (Multilingual), CoT-Syn (CoT Synthesis). Dataset Size(TB) Multi-Src Multi-Cls Multi-Lang CoT-Syn Pile Dolma RefindeWeb Redpajama (V2) FineWeb2 Wanjuan1.0 FineWeb Nemotron-CC CCI3.0 CCI4.0 0.8 11 0.6 120 17.7 0.19 44 22 1"
        },
        {
            "title": "2 Related Works",
            "content": "To contextualize the development of CCI4.0, we compare it with existing large-scale pretraining datasets, as summarized in Table 1. The Pile (0.8T tokens) and Dolma (11T tokens) leverage multisource data and multiple classifiers for quality assurance, supporting multilingual content but lacking Chain-of-Thought (CoT) synthesis, which limits their focus on reasoning enhancement. Similarly, datasets like Redpajama (V2) ( 120T tokens) and FineWeb2 (17.7T tokens) offer substantial scale and multilingual support, yet they do not incorporate CoT synthesis or multi-source diversity to the extent of CCI4.0. Nemotron-CC (10.4T tokens), key component of CCI4.0s English corpus, employs multiple classifiers but is monolingual and lacks CoT integration. In contrast, CCI4.0 (35T tokens) uniquely combines multi-source data, multilingual support (English and Chinese), multiple quality classifiers, and CoT synthesis, incorporating 4.5 billion human thinking templates to explicitly target reasoning capabilities. This comprehensive approach distinguishes CCI4.0 from prior datasets, addressing gaps in reasoning-focused pretraining data and setting new benchmark for developing LLMs with enhanced logical and commonsense reasoning abilities."
        },
        {
            "title": "3 Method",
            "content": "Figure 1: The figure shows the overall processing pipeline of our dataset. Our main processing flow includes deduplication, scoring the data, and some other filtering process to get our CCI4.0-M2-Base V1 dataset. Notably, we incorporated COT synthetic data by leveraging large language models to perform chunking, summarization, and extraction operations on the original documents sampled from our dataset pool to get the CCI4.0-M2-COT V1 dataset. As shown in Figure 1, our data processing pipeline is meticulously designed to yield high-quality, diverse, and robust dataset, comprising five principal stages: Deduplication, Multi-classifier Quality Scoring, Fluency Filtering, Data Synthesis (including CoT synthesis), and comprehensive Privacy and Toxicity Handling. The initial Deduplication phase is critical for removing redundancy, operating at both global document level and finer-grained string level. Following this, the Multiclassifier Quality Scoring stage evaluates data integrity and relevance across various dimensions. This is achieved by automatically constructing evaluation samples using large language models, training specialized small-scale quality classifiers on these samples, and then integrating their scores to assign comprehensive quality tier to each data point. To address linguistic quality, particularly the prevalence of overly short or syntactically challenging samples, domain-specific Fluency Filtering step is implemented. Recognizing the significant variability in fluency characteristics across different data domains, this filtering process is applied independently to each domain to effectively 3 remove samples with notably poor linguistic flow. Building upon the foundation of high-quality data identified through the previous stages, Data Synthesis process is employed. This involves leveraging the filtered high-quality samples as seeds with large models to generate novel data instances in diverse formats. Specifically, high-quality sources are selected for targeted Chain-of-Thought (CoT) synthesis, focusing on the construction of core questions and detailed instructions. Finally, the pipeline incorporates essential safety and privacy measures: Privacy Handling processes sensitive personal information such as identification numbers and phone numbers, while Toxicity Scoring utilizes dedicated model to assess and flag potentially harmful content within each sample. 3.1 Data Collection and Preprocessing As shown in Table 3, we select multiple sources from English and Chinese. Regarding data sources, the English web corpus was derived from the Nemotron-CC [34] dataset. This particular source was selected based on both comparative effectiveness evaluations and its significantly larger data volume compared to alternatives. For the Chinese web dataset, our collection process involved consolidating data from some existing open-source Chinese datasets such as [4, 5, 6, 10] and extracting the Chinese components from various multilingual datasets such as [17, 26, 27]. Through thorough analysis of the inter-dependencies and relationships among these potential sources, we strategically filtered and identified over ten datasets that served as the primary contributors to our Chinese web corpus. Furthermore, to enhance the breadth and depth of the training data, we incorporated additional high-performing open-source datasets. These supplementary sources are designed to cover diverse array of domains, including but not limited to code, mathematics, books, encyclopedias, and academic papers. Upon conducting manual spot checks on data samples procured from various sources, we identified specific quality inconsistencies, particularly within the Chinese text and code corpora. Consequently, distinct processing methodologies were developed and applied specifically to address these observed issues. For the Chinese dataset, series of pre-processing operations were undertaken to ensure corpus quality and optimize its suitability for downstream model training. First, to standardize the linguistic and symbolic representation, all text was uniformly converted to Simplified Chinese. Second, to uphold content integrity and compliance with usage norms, sensitive word filtering mechanism was implemented to automatically detect and remove segments containing inappropriate vocabulary. Furthermore, to mitigate the risk of the model learning overly short or structurally fragmented sentences, minimum average line length constraint was imposed, retaining only text samples with an average character count of at least 10 per line. Finally, filter based on the total character count was applied, limiting samples to range between 100 and 20,000 characters to balance semantic richness with processing efficiency. In parallel, during the processing of the raw code data, we noted the presence of significant amount of interspersed copyright declarations and related textual information. To construct high-quality dataset containing solely code content, this non-code material was systematically filtered and removed. 3.2 Hybrid Deduplication Following the initial phase of basic quality filtering, two-stage deduplication strategy was implemented to further enhance the purity and uniqueness of the dataset. The first stage employed fuzzy deduplication approach[8], leveraging the fuzzy deduplication operator available within the Data-Juicer framework[9]. This method is adept at identifying and eliminating redundant samples by effectively recognizing pairs of texts that are similar in content but not strictly identical. Subsequently, the second stage utilized the deduplicate-text-datasets library[28], an open-source tool developed by Google, to perform exact substring deduplication[21]. This process further removes duplicate data based on precise substring matching. Specifically, we configured the parameters with length-threshold of 800 and min-doc-words of 35. These settings were carefully chosen to ensure that strict comparisons were performed only between samples exceeding certain threshold of text length and word count, thereby preventing excessive deduplication of shorter texts. These two complementary deduplication methods work in concert to significantly reduce redundancy while concurrently preserving the diversity of the dataset. 4 3.3 Quality Classification To ensure the high quality of our processed datasets, multi-faceted quality classification approach was employed, tailored to the characteristics of both English and Chinese corpora. For the English web data, primarily sourced from Nemotron-CC, three independent quality classifiers were utilized to score each document. Based on these scores, samples were allocated into 20 distinct quality bins, with the highest score among the three classifiers designated as the final quality score for each document. For the Chinese dataset, recognizing the unique linguistic features and the need for domain-specific evaluation, we meticulously designed and trained specialized Chinese quality classifiers during the data construction process. This classification system was conceptually informed by the NemotronCC quality classification framework but underwent significant customization and optimization to effectively handle Chinese linguistic nuances and corpus characteristics. The development of the Chinese quality classifiers involved several critical steps[32]. Initially, we devised specific prompts to guide large models in generating the necessary training data for the classifiers. The training sets were generated using two distinct large models, Qwen2.5-72B-Instruct [35] and Deepseek-V3 [13], yielding total of 460k samples. The test set, comprising 40k samples, was generated by GPT-4o. The initial sample pools contained 1.2M samples for the training set and 116k samples for the test set. In terms of prompt design, Qwen2.5-72B-Instruct utilized direct scoring prompt in Chinese, while Deepseek-V3 employed rule-based cumulative scoring prompt in English. This strategic choice aimed to leverage the respective generative strengths of the two models under different prompting styles. During the training phase, we fine-tuned two independent XLRoberta-based models[12] on the respective training sets, resulting in two distinct Chinese quality classifiers. We explored four different learning rates (6e-4, 3e-4, 1e-4, and 6e-5) for each configuration, training complete model under each setting. Evaluation on the test set revealed that when using each classifier independently, learning rate of 3e-4 yielded the highest F1 score. Furthermore, we observed significant improvement in the F1 score when combining the outputs of the two classifiers, indicating the complementary nature of the features captured by the data generated from the two different training sets, thereby enhancing the discriminative power of the combined classification system. In addition to the XLRoberta-based classifiers, and drawing inspiration from the findings presented in A.5, where fastText-based filtering demonstrated optimal performance compared to various model-based data filtering strategies, we also trained fastText[20] classifier specifically for the Chinese corpus scenario. This classifier was designed as binary classification task to identify high-quality samples. The positive sample pool was initially constructed by collecting multiple Chinese instruction datasets, including COIG-CQIA[2], OpenHermes-2.5-zh[25], OpenOrca-Chinese and smoltalk Chinese[38]. Through multiple iterations, we progressively refined this set by mitigating the influence of training data length distribution, removing irrelevant high-frequency words from predicted positive samples, and adding certain stop words based on word importance features. This iterative process resulted in final positive sample set of 220k samples. Correspondingly, 220k samples were randomly drawn from the corpus pool to serve as negative samples. These were then used to construct the final training set of 400k samples and test set of 40k samples, maintaining 10:1 ratio. 3.4 LLM-based Fluency Filtering To further refine data quality based on linguistic fluency as [16], we employed multilingual domain classifier 2 to categorize all raw corpus data into distinct domains, resulting in 26 identified subdomains. Following this domain classification, we computed the Perplexity Loss for all samples within each domain. The analysis of these loss distributions revealed significant variations across different domains. Notably, the Games domain exhibited the highest overall loss values, suggesting that domains such as gaming contain more complex, unpredictable, or specialized linguistic patterns. Conversely, domains like Law and Government and Science and Health showed the lowest average loss, indicating the presence of more established terminology and formal structures. To mitigate the influence of extreme outliers within each domain, we established filtering criterion based on the calculated loss distributions. Specifically, samples exceeding the 99.5th percentile of the loss value within their respective domains were systematically removed, yielding refined dataset with 2https://huggingface.co/nvidia/multilingual-domain-classifier 5 improved linguistic consistency within each category. Loss and percentiles across domains are illustrated in the Figure 5 in Appendix. 3.5 Data synthesis Recent studies indicate that the reasoning abilities of large language models (LLMs) primarily originate from the pre-training phase and are subsequently activated during the reinforcement learning phase[15, 39]. Consequently, we endeavor to extract vast quantities of high-quality human thought processes from pre-training corpora to synthesize reasoning data. Specifically, we first curated high-quality source data from diverse domains, including web pages, code, mathematics, academic papers, and encyclopedias. As illustrated in the Figure 1, our detailed methodology is as follows: Semantic Segmentation and Summarization: We utilize Qwen2.5-32B-Instruct[35] to perform semantic segmentation on the original texts. This process divides the original documents into semantically independent and non-overlapping segments. To minimize the LLMs output cost, output only contains the start and end markers of each segment. Subsequently, for each segment, we prompt the model to generate concise summary. Summarizing Chain-of-Thought and Core Question: Based on the segmented summaries, we have been able to reconstruct the thought process behind the original human-written document. We then employ Qwen2.5-32B-instruct to refine and consolidate these segment summaries, thereby forming logically coherent chain-of-thought (CoT). Recognizing that recent work highlights questions as critical element of reasoning data, we finally derive and summarize the core question addressed by the document, based on this CoT. Consequently, each synthesized reasoning data instance is structured as: {core question, chainof-thought, original document}. In total, we have synthesized over 400 billion (400B) tokens of reasoning data spanning these diverse domains, including web pages, code, mathematics, academic papers, and encyclopedias."
        },
        {
            "title": "4 Experimental Setting",
            "content": "4.1 Training Configuration Following the training setup of CCI3-HQ[36], we adopt the Qwen2-0.5B[37] tokenizer and model architecture, training on bilingual dataset containing 100 billion tokens. This setup is designed to effectively accommodate both Chinese and English data while preserving consistency across experiments. The training is conducted with sequence length of 4096, weight decay set to 0.1, and gradient clipping at 1.0. The dataset consists of 25 million samples, trained with global batch size of 1024. The learning rate follows cosine decay schedule, starting at 3e-4, decaying to minimum of 3e-5, with warmup over the first 2,048,000 samples. 4.2 Evaluation Metrics We used the LightEval[14] library for model evaluation, following the same setup as in FineWeb[33] and CCI3-HQ. All evaluations were conducted in zero-shot setting. To directly compare the performance across different datasets, we use Average, which refers to the overall average score across all Chinese and English benchmarks. The evaluation metrics include: Chinese benchmarks: CEval[19] and CMMLU[23]. English benchmarks: ARC-C[11], ARC-E[11], HellaSwag[40], WinoGrande[22], MMLU[18], OpenbookQA[3], PIQA[7] and SIQA[29]."
        },
        {
            "title": "5 Experimental Results",
            "content": "5.1 Main Results We perform isolated training runs using individual datasets to enable direct comparisons between different datasets, such as Nemotron-CC-HQ(the high-quality subset of Nemotron-CC), CCI3HQ(the high-quality subset of CCI3), and the whole CCI4.0 dataset introduced in this work. The 6 Figure 2 clearly demonstrates that across varying training scales (measured in tokens), the CCI4.0 dataset consistently outperforms both CCI3-HQ and Nemotron-CC-HQ, indicating superior training efficiency and better generalization performance. Key experimental findings are listed as follows: At small-scale training (20B tokens): CCI4.0 shows significant performance advantage, particularly at 10B and 20B tokens, where it outpaces the other datasets by notable margin. This suggests higher data quality and information density. For example, the performance of CCI4.0 at 10B tokens is comparable to Nemotron-CC-HQ at 30B tokens, highlighting its efficiency. At medium-scale training (20B60B tokens): CCI4.0 continues to improve steadily and reaches performance saturation earlier than the other datasets, indicating that it enables the model to approach its performance ceiling more quickly under limited compute budgets. At large-scale training (>60B tokens): While the performance gap among datasets narrows, CCI4.0 maintains its lead, demonstrating that it remains effective even at larger training scales without exhibiting early overfitting or diminishing returns. Overall trend: CCI4.0 consistently delivers the highest scores across all training stages and exhibits stable performance curve, indicating robust generalization and training stability. Figure 2: Performance Comparison of Different Datasets Across Training Scales. Table 2: Evaluation results of different datasets across various benchmarks. Metrics/Datasets CCI3-HQ Nemotron-CC-HQ CCI4.0 HellaSwag ARC (Average) PIQA MMLU (cloze) CommonsenseQA TriviaQA WinoGrande OpenBookQA SIQA CEval CMMLU AverageEnglish AverageChinese Average 28.06 31.03 55.66 26.52 21.05 1.28 48.62 25.20 40.43 32.31 32.51 30.87 32.41 31. 7 44.63 43.21 69.15 30.32 27.19 5.91 51.38 33.40 41.76 27.74 26.84 38.55 27.29 32.92 42.50 41.05 68.77 30.34 27.44 6.05 51.46 32.60 40.79 27.67 28.92 37.89 28.30 33.09 The Table 2 provides comprehensive comparison of model performance across multiple benchmarks using three datasets: CCI3-HQ, Nemotron-CC-HQ, and CCI4.0. Although Nemotron-CC-HQ slightly outperforms CCI4.0 on the English average (38.55 vs. 37.89), CCI4.0 remains highly competitive and achieves the best score on several tasks such as CommonsenseQA (27.44) and TriviaQA (6.05). It also matches or closely trails Nemotron on others like MMLU (30.34 vs. 30.32), OpenBookQA (32.60 vs. 33.40), and SIQA (40.79 vs. 41.76), demonstrating robustness across diverse reasoning and knowledge benchmarks. One of CCI4.0s most notable strengths lies in Chinese language tasks. It significantly outperforms Nemotron-CC-HQ in both CEval (27.67 vs. 27.74) and CMMLU (28.92 vs. 26.84), leading to higher average Chinese score (28.30 vs. 27.29). This suggests that CCI4.0 contains more representative and better-curated set of Chinese training data, leading to improved bilingual capabilities. Compared to CCI3-HQ, CCI4.0 shows major improvements in English benchmarks (especially ARC, MMLU, CommonsenseQA, and OpenBookQA), while exhibiting slight disadvantage in Chinese performance. This is mainly due to the fact that Chinese data accounts for only around 20% of the overall dataset composition. We aim for the CCI4.0 dataset to achieve balanced trade-off between Chinese and English, making it strong candidate for high-quality pretraining. 5.2 Reasoning Abilities from CoT Dataset To evaluate the impact of our CoT dataset on model reasoning abilities, we conducted controlled experiment. Our evaluation approach is inspired by the adversarial CoT framework proposed by [1], which assesses model reflection on reasoning chains. Given the relatively small scale of our evaluated model, which does not exhibit emergent CoT generation capabilities, directly applying standard CoT evaluation methods is challenging. Therefore, we adapted the evaluation approach. For each test sample containing both correct and an incorrect CoT, we measure the models perplexity (PPL) on both CoTs. sample is considered passed if the model assigns lower PPL to the correct CoT compared to the incorrect one. The final score for dataset is the proportion of samples that passed this PPL criterion. Following [1], we report the pre-training compute for each data point as 6nT , where and are the number of parameters and training tokens, respectively. Figure 3: Reasoning ability scores on adversarial datasets for 0.5B dense model trained with (w/ cot synthesis) and without (w/o cot synthesis) CoT data mix, evaluated across 100B training tokens. Training with CoT data accelerates reasoning ability growth. We trained 0.5B parameter dense language model on two distinct 100 billion-token datasets: one dataset included mix of CoT data, while the other did not. We evaluated the reasoning performance of models checkpoints using our adapted PPL method on four adversarial datasets from gsm8k_adv, gsm8k-platinum_adv, cruxeval_o_adv, and cruxeval_i_adv. As shown in Figure 3, compared to the model trained without CoT data, the model trained on the dataset mixed with CoT data demonstrates lower perplexity on correct CoT examples, indicating notably faster improvement in reasoning ability across the evaluated datasets. This demonstrates that incorporating our CoT examples into the training data significantly reduces the models tendency to hallucinate incorrect CoT examples, accelerating the acquisition of reasoning skills even in smaller models. Further experiments presented 8 in Appendix A.3 provide additional evidence that reasoning ability generally increases with the total training compute. 5.3 Downstream Tasks Performance from CoT Dataset To analyze the influence of CoT Datasets on the model performance, we provide the average model performance across downstream tasks in Figure 4, where models are trained using 10 billion-token datasets with and without CoT data. More detailed model performance is provided in Table 4. Results demonstrate that our synthetic CoT data contributes to performance gains in downstream tasks during model pretraining. Specifically, as shown in Table 4, the model trained with CoT data performs well in reasoning tasks like HellaSwag and reading comprehension tasks like TriviaQA. However, the performance gains brought by CoT data to pretrained models on downstream reasoning tasks are inconsistent, and how to better leverage the effects of CoT data introduced during pretraining in the post-training stage warrants further investigation. Figure 4: Performance Comparison of Models With and Without CoT Synthesis"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced CCI4.0, large-scale, bilingual pretraining dataset designed to provide high-quality and diverse coppora for model training. By integrating diverse, high-quality data sources, including Nemotron-CC for English and multiple Chinese datasets, alongside 4.5 billion human thinking templates via CCI4.0-M2-CoT, CCI4.0 addresses the limitations of traditional datasets in fostering general and complex reasoning abilities. The datasets rigorous processing pipelineencompassing deduplication, multi-classifier quality scoring, fluency filtering, CoT synthesis, and privacy/toxicity handlingensures both quality and diversity. Experimental results demonstrate that models pretrained on CCI4.0 significantly outperform baselines on reasoning-intensive benchmarks like MMLU and ARC-Challenge with lower possibility of hallucination. These findings underscore the value of high-quality and diverse pretraining data and establish CCI4.0 as new standard for developing LLMs capable of tackling sophisticated, multi-step reasoning challenges. Future work will explore further scaling and refinement of CoT synthesis to unlock even greater reasoning potential in next-generation models."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge the valuable contributions of Institutions Alibaba Cloud (阿里云), Shanghai AI Laboratory (上海人工智能实验室), Huawei (华为), Mobvoi (出门问问), Kingsoft Office Software (金山办公), Kunlun (昆仑万维), ModelBest (面壁智能), Qihoo (奇虎科技), Meituan (美团), MiniMax (稀宇科技), Moonshot AI (月之暗面), Zidong Taichu (紫东太初), Wenge (中科闻歌) and iFLYTEK (科大讯飞) in providing the Chinese data."
        },
        {
            "title": "References",
            "content": "[1] Essential AI, :, Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Anthony Polloreno, Ashish Tanwer, Burhan Drak Sibai, Divya Mansingka, Divya Shivaprasad, Ishaan Shah, Karl Stratos, Khoi Nguyen, Michael Callahan, Michael Pust, Mrinal Iyer, Philip Monk, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, and Tim Romanski. Rethinking reflection in pre-training, 2025. [2] Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu, Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Moore Wang, Ruibin Yuan, Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Min Yang, Shiwen Ni, and Ge Zhang. Coig-cqia: Quality is all you need for chinese instruction fine-tuning. ArXiv, abs/2403.18058, 2024. [3] Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, and Chitta Baral. Careful selection of knowledge to solve open book question answering. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 61206129, Florence, Italy, July 2019. Association for Computational Linguistics. [4] Beijing Academy of Artificial Intelligence. CCI-Data [Data set]. https://huggingface.co /datasets/BAAI/CCI-Data. [5] Beijing Academy of Artificial Intelligence. CCI2-Data [Data set]. https://huggingface. co/datasets/BAAI/CCI2-Data. [6] Beijing Academy of Artificial Intelligence. WuDaoCorporaText [Data set]. https://data.b aai.ac.cn/datadetail/WuDaoCorporaText. [7] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. [8] Andrei Z. Broder. On the resemblance and containment of documents. Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pages 2129, 1997. [9] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. Datajuicer: one-stop data processing system for large language models. Companion of the 2024 International Conference on Management of Data, 2023. [10] Jianghao Chen, Pu Jian, Tengxiao Xi, Dongyi Yi, Qianlong Du, Chenglin Ding, Guibo Zhu, Chengqing Zong, Jinqiao Wang, and Jiajun Zhang. Chinesewebtext: Large-scale high-quality chinese web text extracted with effective evaluation model, 2023. [11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. [12] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. ArXiv, abs/1911.02116, 2019. [13] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, 10 R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. [14] Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. [15] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [16] Leo Gao, Stella Biderman, Sid Black, Chris Callison-Burch, Laurence Cohen, Esin Durmus, Ethan Fenoglio, Josh Firestone, Jordan Foster, Sam Gehman, Shachar Gretz, Kristen Hallahan, Dieuwke Hupkes, Nathan Lambert, Ron Le Bras, Zachary Levonian, Luca Lisi, Annika McMillan-Major, Todor Mihaylov, Sewon Min, Colin Raffel, Melissa Roemmele, Baptiste Roziere, Maarten Sap, Vered Shwartz, Daniel Sileo, Sabari Subramanian, Leo Sutawika, Yoav Tewel, Josh Tow, Ethan Tseng, Luke van der Poel, Vinay Venkatakrishnan, Benjamin Wang, Guillaume Wenzek, Thomas Wolf, Yuchen Wu, Yutong Xu, Xi Yang, Michihiro Yasunaga, Peter Yin, Rowan Zellers, Tianwei Zhang, Jun Zhu, Mike Lewis, Federico Petroni, and Aleksandra Piktus. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [17] Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models, 2023. [18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. [19] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023. [20] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016. [21] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022. [22] Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR12, page 552561. AAAI Press, 2012. 11 [23] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2024. [24] Jeffrey Li and Alex Fang etc. Datacomp-lm: In search of the next generation of training sets for language models, 2024. [25] Wenbo Pan. Openhermes 2.5-zh: partial chinese translation of openhermes-2.5, 2024. [26] Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: sparkling update with 1000s of languages, December 2024. [27] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alibe, Quentin Chanu, Baptiste Launay, Jean-Baptiste Dehaene, and Hugo Touvron. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. [28] Google Research. Deduplicate Text Datasets. https://github.com/google-research/d eduplicate-text-datasets, 2021. Accessed: 2025-05-16. [29] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019. [30] Luca Soldaini, Rodney Kin Lo, Wajdi Yazdan, Ahmed El-Kishky, Faisal Ladhak, Daniel Murray, Shaked Yom Din, Winston Li, Yingbo Liu, Yanai Elazar, Akshita Bhagia, Dirk Groeneveld, Tim Dettmers, Aleksandra Piktus, Nicola Cancedda, Allie De Lucia, Orr Katz, Leshem Choshen, Qiao Li, Hao Zhao, Avi Wettig, Alexander Rush, and Samuel Barnett. Dolma: An open corpus of 3 trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. [31] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset, 2024. [32] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. [33] HuggingFace Team. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. [34] NVIDIA Team. Nemotron-4 340b technical report. arXiv preprint arXiv:2412.02595, 2024. [35] Qwen Team. Qwen2.5: party of foundation models, September 2024. [36] Liangdong Wang, Bo-Wen Zhang, Chengwei Wu, Hanyu Zhao, Xiaofeng Shi, Shuhao Gu, Jijie Li, Quanyue Ma, TengFei Pan, and Guang Liu. Cci3.0-hq: large-scale chinese dataset of high quality designed for pre-training large language models, 2024. [37] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [38] Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, and Ji Pei. Opencsg chinese corpus: series of high-quality chinese datasets for llm training, 2025. 12 [39] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [40] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Loss values across domains Figure 5: Loss values across domains and percentiles. To systematically analyze the models performance and guide our data filtering strategy, we investigated the distribution of loss values across various domains, as illustrated in Figure 5. The heatmap visualizes the loss values at high percentiles, ranging from the 97th to the 99.5th, offering granular view of the most challenging instances within the dataset. key observation is the significant heterogeneity in loss distribution among the domains. Domains such as \"Games\" and \"Adult\" consistently exhibit higher loss values across all percentile thresholds, suggesting they contain greater concentration of complex, noisy, or out-of-distribution samples that the model struggles to generalize. In contrast, domains like \"Law and Government\" and \"Science\" demonstrate substantially lower loss values, indicating better model fit and cleaner data. Based on this analysis, we established data filtering threshold. The decision required careful trade-off between removing potential noise and preserving valuable information, complicated but valid training examples. lower percentile threshold would be too aggressive, potentially removing many informative samples. Therefore, we opted to perform the filtering at the 99.5th percentile. This conservative yet precise strategy targets only the most extreme outliersthe top 0.5% of samples with the highest loss in each domain. This approach allows us to effectively prune the dataset of majority of probable label errors and severe anomalies while retaining 99.5% of the data, thus striking an optimal balance between enhancing data quality and maintaining the datasets scale and diversity for robust model training. A.2 Data Sources Table 3 provides comprehensive list of the primary sources considered during our curation process. For the English component, we utilized emotron CC, which is derived from Common Crawl. The Chinese component is more extensive, drawing from variety of prominent web-scale datasets, including anJuan, uDaoCorpora, the CCI Dataseries, and ineweb 2, among others. 14 The inclusion of these varied and high-quality sources was crucial for ensuring the breadth, diversity, and scale necessary for our training objectives. Table 3: Main Datasets Considered During Dataset Curation Dataset Type Dataset Name Web-EN Nemotron-CC Web-ZH"
        },
        {
            "title": "WanJuan",
            "content": "Web-ZH CCI2.0-Data Web-ZH CCI1.0-Data Web-ZH WudaoCourpora Web-ZH ChineseWebText2.0 Web-ZH MAP-CC Web-ZH TeleChat-PTD Web-ZH fineweb-2 Web-ZH HPLT Datasets v2 Web-ZH CCI3.0-Data Code fineweb-code-corpus_20241112 Code Math Math Math smollm-corpus-python-edu fineweb-math-corpus_20241112.jsonl EleutherAI-proof-pile-2-open-webmath.jsonl finemath-3plus.jsonl Books dolma-books Wiki dolma-wiki Arxiv dolma-arxiv 15 URL https://data.commoncrawl.org/con trib/Nemotron/Nemotron-CC/index. html https://opendatalab.org.cn/OpenD ataLab/WanJuan1_dot_0 https://huggingface.co/datasets/ BAAI/CCI2-Data https://huggingface.co/datasets/ BAAI/CCI-Data https://data.baai.ac.cn/details/ WuDaoCorporaText https://huggingface.co/datasets/ CASIA-LM/ChineseWebText2.0 https://huggingface.co/datasets/ m-a-p/MAP-CC https://huggingface.co/datasets/ Tele-AI/TeleChat-PTD https://huggingface.co/datasets/ HuggingFaceFW/fineweb-2 https://huggingface.co/datasets/ HPLT/HPLT2.0_cleaned https://huggingface.co/datasets/ BAAI/CCI3-Data https://huggingface.co/datasets/ OpenCoder-LLM/opc-fineweb-code-c orpus https://huggingface.co/datasets/ HuggingFaceTB/smollm-corpus https://huggingface.co/datasets/ OpenCoder-LLM/opc-fineweb-math-c orpus https://huggingface.co/datasets/ EleutherAI/proof-pile-2 https://huggingface.co/datasets/ HuggingFaceTB/finemath https://huggingface.co/datasets/ allenai/dolma/blob/main/urls/v1_ 6.txt https://huggingface.co/datasets/ allenai/dolma/blob/main/urls/v1_ 6.txt https://huggingface.co/datasets/ allenai/dolma/blob/main/urls/v1_ 6.txt Table 3: Main Datasets Considered During Dataset Curation Dataset Type Dataset Name ForumQA dolma-v1_7-stackexchange URL https://huggingface.co/datasets/ allenai/dolma/blob/main/urls/v1_ 7.txt A.3 Reasoning Ability Analysis This section presents an experiment analyzing how reasoning ability scales with increasing training compute. We trained 1.4B parameter MoE model (0.4B active) using 800 billion tokens of our proposed CoT data. We evaluated the models reasoning performance using the adapted PPL method which is introduced in section 5.2. As illustrated in Figure 6, the evaluation results demonstrate clear trend: the models reasoning ability shows consistent improvement with increasing training compute on the CoT dataset. This finding suggests that training on large volume of high-quality CoT data enhances the models capacity to assign higher probability to correct reasoning paths, even in models where explicit CoT generation is not yet fully emergent. Figure 6: Reasoning ability scores on adversarial datasets for 1.4B MoE model as pre-training compute increases (up to 800B tokens of CoT data). Reasoning ability improves consistently with increased training compute. A.4 Downstream Task Performance We provide detailed performance of models trained with and without CoT synthesis dataset in Table 4. Results demonstrate that the model trained with CoT data performs well in reasoning tasks like HellaSwag and reading comprehension tasks like TriviaQA. However, the performance gains brought by CoT data to pretrained models on downstream reasoning tasks are inconsistent, and how to better activate the effects of these CoT data in the post-training stage warrants further investigation. A.5 Chinese Quality Classifiers We apply combination of our three custom-built Chinese quality classifiers to categorize the Chinese dataset into different quality tiers. Similar to the approach used in Nemotron-CC, we validate the effectiveness of our classifiers by dividing the data into 20 buckets based on quality scores. Separate models are trained and evaluated using data from each bucket. For the Chinese dataset, we focus primarily on the average performance across Chinese evaluation benchmarks. 16 Table 4: Detailed Performance of Models Trained With and Without CoT Synthesis. Metrics/Datasets CCI4.0 without the CoT Data CCI4.0 HellaSwag ARC (Average) PIQA MMLU (cloze) CommonsenseQA TriviaQA WinoGrande OpenBookQA SIQA CEval CMMLU"
        },
        {
            "title": "AverageEnglish\nAverageChinese\nAverage",
            "content": "29.82 33.03 62.79 26.62 25.31 0.47 49.88 28.00 40.99 27.34 27.11 32.99 27.23 30.11 30.15 33.16 61.26 26.80 23.67 0.79 50.28 28.20 40.43 27.91 27.46 32.75 27.69 30.22 The Figure 7 presents the average Chinese evaluation scores of models trained on data buckets categorized by quality scores. Each bucket represents range of quality scores as assigned by our Chinese data quality classifiers. Several key observations can be made: there is clear upward trend in model performance as data quality increases. The average Chinese score improves steadily from bucket 0 to bucket 19, indicating that higher-quality data leads to significantly better downstream performance. This validates the effectiveness of the quality classifier in ranking data usefulness. A.6 Loss-Based Filtering To assess the effectiveness of loss-based filtering for English web data, we compare models trained on sampled 10 billion-token English corpora before and after loss filtering. Figure 8 presents the average performance during training. As shown in Figure 8, filtering based on loss improves training efficiency throughout the learning process. Table 5 further indicates that removing outlier samples with high loss from the raw English corpus enhances model performance on downstream commonsense reasoning tasks such as CommonsenseQA and SIQA, as well as reading comprehension tasks like TriviaQA. Notably, although this filtering method is applied solely to English web data, it also leads to slight performance improvements in Chinese QA tasks, such as those in the CMMLU benchmark. A.7 Limitations While our dataset offers broad coverage and high quality across Chinese and English data, it currently supports only these two languages. Future extensions will aim to incorporate additional languages to better support multilingual modeling and cross-lingual generalization. Due to the large scale of the dataset, it may not be suitable for researchers with limited computational resources or those working with small models. In such cases, further filtering or subsetting of the dataset may be necessary to ensure practical usability. Although we have applied privacy-preserving techniques and multiple toxicity filtering strategies using open-source models, we cannot guarantee the complete removal of all sensitive or harmful Figure 7: Performance Comparison of Different Datasets Across Training Scales. Figure 8: Performance Comparison of Models With and Without Loss-Based Filtering content. Users are advised to apply additional safeguards when deploying models trained on this dataset in sensitive applications. 18 Table 5: Detailed Performance of Models Trained With and Without Loss-Based Filtering. Metrics/Datasets Nemotron-CC-high Nemotron-CC-high(from CCI4.0) HellaSwag ARC (Average) PIQA MMLU (cloze) CommonsenseQA TriviaQA WinoGrande OpenBookQA SIQA CEval CMMLU AverageEnglish AverageChinese Average 30.93 34.39 62.73 26.50 23.91 0.92 49.57 29.60 39.56 26.65 25.98 33.12 26.32 29. 30.69 34.43 62.89 26.64 25.55 1.25 49.09 30.20 40.12 26.56 26.67 33.43 26.62 30."
        }
    ],
    "affiliations": [
        "baai.ac.cn"
    ]
}