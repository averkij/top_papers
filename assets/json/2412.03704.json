{
    "paper_title": "Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension",
    "authors": [
        "Wang Xiyao",
        "Yang Zhengyuan",
        "Li Linjie",
        "Lu Hongjin",
        "Xu Yuancheng",
        "Lin Chung-Ching Lin",
        "Lin Kevin",
        "Huang Furong",
        "Wang Lijuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 4 0 7 3 0 . 2 1 4 2 : r Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension Xiyao Wang1,2,, Zhengyuan Yang2, Linjie Li2, Hongjin Lu1, Yuancheng Xu1 Chung-Ching Lin2, Kevin Lin2, Furong Huang1,, Lijuan Wang2, 1University of Maryland, College Park 2Microsoft xywang@umd.edu Equal advise Figure 1. An illustration of how VisVM can better guide vision language model (VLM) during inference-time search. When selecting response candidates at each step, the process reward model (PRM) only considers the immediate reward, whereas VisVM predicts the longterm value by considering potential hallucinations in subsequent generated sentences. This enables VisVM to avoid response candidates with higher hallucination risks and generate image descriptions that are less prone to hallucination and more detailed."
        },
        {
            "title": "Abstract",
            "content": "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate reSpecifically, sponses with better visual comprehension. VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVMguided search significantly enhances VLMs ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVMguided captions improve VLMs performance across wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM. 1. Introduction Vision language models (VLMs) have advanced rapidly, excelling in multimodal tasks involving single images [3, 12, 32, 38], multiple images [21, 27], and videos [24, 53, 1 66]. These capabilities stem from large-scale, high-quality training data, often sourced from web-crawled image-text pairs [20, 36] with effective filtering [7, 19, 71], or enriched through techniques like distillation from stronger VLMs [8], human annotations [4], or added textual descriptions [23]. Despite this progress, VLMs still suffer from visual hallucinations [16, 30, 59, 63] and often neglect less salient image regions, limiting their real-world utility. While increasing the scale and quality of training data could help, this approach incurs significant annotation and API costs, making it less scalable. This raises key question: Can we enhance VLMs response quality at inference time, and leverage these improved responses to further advance VLMs visual comprehension? Recent studies on large language models (LLMs) [1, 29, 40, 41, 67] highlight inference-time search as promising approach for improving response quality, complementary to training time effort. By leveraging pretrained process reward model [46, 73], LLMs can perform search iterations to produce high-quality outputs, with these refined responses showing potential as synthetic training data to enhance reasoning capabilities. However, extending this approach to VLMs for improved visual comprehension poses unique challenges, particularly in defining reward signal. While process and outcome rewards are relatively straightforward for LLM tasks like coding and math, VLM taskssuch as descriptive captioninglack clear outcome measures and require cohesive paragraph image descriptions that consist of multiple global and regional caption sentences. In these cases, each sentence must not only be accurate locally but also contribute to coherent overall response. To this end, we propose the Vision Value Model (VisVM), value network to guide VLM inference-time search by generating descriptive captions in step-by-step manner, with each step producing one sentence. As shown in Figure 1, VisVM takes the image and generated sentence at each step as inputs, predicting long-term value to ensure both visual-text alignment and coherence. VisVM is built on two main insights, which differentiates our method from traditional process reward models in LLM literature [13, 18, 28, 48, 54]: First, rather than relying only on the local reward of the current sentence, it predicts future consequences to maintain coherence. VisVM is trained using Temporal Difference (TD) learning [43], allowing it to go beyond evaluating responses at the current search step. This forward-looking signal helps avoid sentences that could lead to hallucinations in the future, improving global response quality in single search round. Second, the reward signal must capture comprehensive visual concepts to reduce hallucinations, for which CLIPs text-image similarity metric serves effectively. We validate the effectiveness of VisVM through two (1) Using VisVM as guidance sigmain experiments: Figure 2. Upper figure: CHAIRs and MMHal score of descriptive captions generated by LLaVA-Next-7B during inference-time using different search methods. VisVM-guided search clearly outperforms other methods, indicating reduced visual hallucinations. Notably, even with smaller search budget (search size 6 vs. search size 30), our approach still surpasses the Best-of-N method. Lower figure: Comparisons of LLaVA-Next-7B after fine-tuning with descriptive captions from different search methods, with VisVM-guided search achieving favorable results across all 8 benchmarks. nal for VLM inference-time search to generate descriptive image captions, we observe substantial reduction in hallucinations and more detailed image descriptions. In both GPT and human evaluations, VisVM-guided captions consistently outperform those generated by greedy decoding, best-of-N decoding, and CLIP-PRM guided search, with VisVM-guided captions preferred 74% of the time (2) To leverage VisVMs over greedy decoding results. inference-time enhancement of VLM responses, we use VisVM-guided captions as the Supervised Fine-Tuning (SFT) data to self-train the original VLM (LLaVA-Next7B). Across eight standard benchmarks, VisVM-guided 2 self-training improves the performance by an average of 10.8% , as shown in the lower figure in Figure 2. Our contribution can be summarized as follows: We introduce VisVM, stepwise value model designed to provide long-term vision value signals to guide VLM inference-time search. To the best of our knowledge, VisVM is the first exploration into enhancing VLM visual comprehension through inference-time search. VisVM-guided search effectively reduces visual hallucinations and enriches image descriptions with more visual detail, by increasing the inference-time computation. Descriptive captions generated by VisVM-guided search can be leveraged as high-quality SFT data, forming robust self-training pipeline that significantly enhances VLM visual comprehension across 8 benchmarks. 2. Related Work Vision language models. Significant advances [25, 36, 50, 60, 69, 71] have been made on vision-language modeling, which jointly understands the visual and text inputs for various tasks such as image captioning [10] and visual question answering [15]. Recently, modern vision language models (VLMs) [2, 3, 12, 32, 35, 44, 55, 68] further combines multimodal modeling with large language models (LLMs) to enable stronger capabilities, such as instruction following, in-context learning, and zero-shot generalization. However, VLMs still exhibit the issue of hallucination [16, 51, 59, 62]. Existing work mitigates hallucination in VLMs by improving the quality of SFT data [11, 52] or through post-training methods [30, 42, 57, 74]. In this paper, we explore reducing hallucination in responses not through training but by using inference-time search to improve the quality of responses. Descriptive captioning. The descriptive image captioning task aims to describe each image with long, comprehensive text paragraph. Recent studies show the effectiveness of using synthetic descriptive captions for vision language model. The pairs of images and paragraph captions can be used for image-to-text understanding models [8, 55], textto-image generation models [4, 14], as well as image-text In this study, we focus contrastive models [22, 23, 61]. on improving the descriptive captioning quality of trained VLM by exploring effective approach to scale the inferencetime search. Inference-time search. Inference-time search strategies have proven crucial for complex reasoning and planning tasks in robotics [17, 56], chess [39], and autonomous driving [45]. The advent of OpenAI-O1 has further advanced inference-time search within LLMs. By applying various search techniques in the language space, such as controlled decoding [6, 65], best of [26, 29], and Monte Carlo tree search [46, 49, 58, 73], LLMs achieve better model responses, thus enhancing performance. good process reward model (PRM) is essential during inferencetime search, as the quality of the reward signal determines the quality of the responses found and the budget required to achieve high-quality responses. Various PRMs [13, 18, 28, 48, 54] have been proposed in LLMs to address mathematical and coding problems. Moreover, Brown et al. [5] and Snell et al. [40] have found that scaling the search budget during inference time can further enhance LLM performance. However, inference-time search remains underexplored in VLMs. Zhou et al. [75] proposed using CLIP as signal for generating positive and negative samples post-training, but did not further investigate its impact as PRM on VLM inference-time search. Xiong et al. [64] proposed LLAVA-Critic to evaluate the quality of responses generated by VLMs; however, it operates at the response level and cannot score individual steps. In this paper, we propose vision value model superior to CLIP as search signal for step-level inference-time search, aimed at enhancing the visual comprehension abilities of VLMs. 3. Vision Value Model In this section, we introduce the proposed Visual Value Model (VisVM). We first present the problem formulation of large multimodal model (VLM) inference in Section 3.1, and then discuss the training process for VisVM in Section 3.2. Section 3.3 shows how to employ VisVM for effective inference-time search in VLMs. 3.1. Formulation of VLM Inference We first introduce the formulation of VLM inference. We consider an VLM characterized by probability distribution pθ, represented as the policy πθ. This model processes prompt-image pair (x, I) as input to generate response = [y1, y2, ..., ym], where consists of step-level responses. Each step-level response yi is treated as sample drawn from the conditional probability distribution yi = pθ(x, I, y<i). In this paper, we define each step-level response as sentence-level, meaning that at each step, the output is single sentence. Consequently, the text generation task can be formulated as an Markov Decision Process (MDP) problem defined by the tuple (S, A, R, γ). is the state space. Each state is defined as combination of the generated sentences and the image. The initial state s0 corresponds to image and input prompt x. is the action space where each action is the sentence generated in that step. We also have the reward function to evaluate the reward of each action, which is also known as process reward model (PRM) in LLMs. γ denotes the discount factor. With this MDP modeling, we can search additional states by increasing the inference-time compute, thereby obtaining better VLM response y. The core of our method lies 3 in the exploration of better value model, namely VisVM, which can better guide the inference-time search. 3.2. VisVM Training Training method. The primary goal of VisVM is to estimate the long-term value of the current image-conditioned sentence in potential future sentence generation scenarios. To achieve this, we employ Temporal Difference (TD) learning [43], popular method in reinforcement learning, to train VisVM for predicting the long-term vision value Vρ(yi, I) at each state si = (yi, I). For given triplet consisting of the current sentence yi, the next sentence yi+1, and an associated image I, we first use the PRM to estimate the reward rsi of the current state si. We then train VisVM using the following loss function, ensuring the predicted value for the current state si matches the sum of the actual received reward and the discounted predicted value for the next state: L(ρ) = E(yi,yi+1,I)D (rsi + γVρ(yi+1, I) Vρ(yi, I))2 , (1) where γ denotes the discount factor, ρ is the learnable parameters of VisVM, and is our constructed training data. Training data. Training VisVM requires the triplet of the current sentence, the next sentence, and an associated image. Such triplets can be extracted from pairs of images and paragraph descriptions = [y1, y2, ..., ym]. It is imperative to generate diverse set of responses using VLMs to explore potential subsequent sentences that each initial sentence may encounter, thereby accurately modeling the sentences long-term value. We sample 9,215 images from the COCO 2017 training dataset and utilize the nine prompts from the LLaVA-150K dataset designed for description captioning. These prompts are randomly paired with the images to construct prompt-image pairs. For each promptimage pair, we generate five distinct responses using the VLM, using both greedy decoding and temperature decoding with temperature values set at different scales. After generating the paragraphs, each response is decomposed into sentence pairs consisting of the current sentence, the subsequent sentence, and the associated image. The final dataset D, containing 378k samples, is used for training VisVM. We provide more training details in Appendix C. Implementation details. In the implementation of VisVM, we select LLaVA-Next-Mistral-7B as our base model. We concatenate linear layer as the value head on top of the penultimate layer of LLaVA-Next-Mistral-7B. The output of this value head is single scalar representing the cumulative reward, or long-term value, of all potential responses based on the current sentence and its paired image. Additionally, we initialize all parameters of VisVM, except for this value head, using the parameters of LLaVANext-Mistral-7B. We also use LLaVA-Next-Mistral-7B as the VLM to generate all training data. 4 For the PRM used in VisVM training, we choose LLaVA-Next-Mistral-7Bs vision encoder CLIP-VIT as our reward model for two main reasons: (1) CLIP effectively measures the alignment between image content and text content by computing the similarity between image and text embeddings, making it highly suitable as PRM for visual comprehension task. Its effectiveness has also been demonstrated in prior studies [75]. (2) Additionally, since CLIP-VIT is the native visual encoder in the LLaVA-NextMistral-7B, using it as PRM eliminates the need for external models or human annotators. This self-rewarding mechanism is not only effective but also reduces costs. 3.3. Inference-time Search using VisVM After training VisVM, we use it as the signal to guide the VLM inference-time search for generating higher-quality responses. To encourage diversity among response candidates at each step of the search, we implement temperature decoding using distinct temperature configurations Tn. Given the current VLM as the policy πθ, it generates conditional probability distribution pθ(x, I, y<i, Tn) based on the input image, prompt, temperature configuration, and previous step responses. We then sample responses from each pθ, yielding response candidates for the current step. Each candidates value is estimated using VisVM, and the candidate with the highest value is selected as the response for the current step. This process continues iteratively until the complete response sequence is generated, i.e., only the EOS token is generated for the next sentence. The pseudocode for this search process is presented in Algorithm 1. Algorithm 1 VisVM-Guided Inference-time Search Require: Test sample {x, I}, VLM pθ, VisVM Vρ, Step size K, Temperature configuration list , Response = [ ] 1: while Generation is not Done do 2: Current step response yi = None, Current step max value max for temperature Tn in do for = 1, . . . , do = Generate response of the new step j: yj = pθ(x, I, y<i, Tn), Estimate step value if = Vρ(yj , I), > max then Current step max value max Current step response yi = yj Append current step response yi to 9: 10: 11: return Final response = , 3: 4: 5: 6: 7: 8: (a) Win rate of VisVM-guided search compared with other methods (b) Scaling curve of search step size. Figure 3. (a) Win rate of image descriptions generated using LLaVA-Next-7B with VisVM-guided search compared with other search methods. We use GPT-4o api for evaluation. We can find VisVM-guided search generated description significantly better than others methods. (b) Step size scaling curve for VisVM-guided search and CLIP-PRM guided search. We report the CHAIRs score of image descriptions under different step sizes. VisVM-guided search is 2 efficient than CLIP-PRM guided search. 4. Experiment In this section, we conduct experiments to answer the following two questions: 1. Does the VisVM-guided search yield higher-quality responses compared with other inference-time search methods (Section 4.1)? 2. Can the VisVM-guided search be leveraged to generate high-quality SFT data, thereby improving the visual comprehension capabilities of VLMs through self-training (Section 4.2)? 4.1. Inference-Time Search with VisVM"
        },
        {
            "title": "Baselines and Implementation Details",
            "content": "In this section, we evaluate the ability of VisVM on enhancing the response quality of VLMs by comparing its inference-time performance with various search methods. All experiments are based on LLaVA-Next-Mistral-7B. We consider the following baselines for inference-time search: (1) Greedy decoding: The standard decoding approach used for VLM decoding, where the responses with the highest probability are selected for each step. (2) Bestof-N (BoN) decoding: widely used method to improve the quality of model responses during inference. For each prompt-image pair, we set five different temperature parameters [0.1, 0.3, 0.5, 0.7, 0.9] and generate six different model responses for each parameter, resulting in total of 30 responses (N = 30). We then use GPT-4o to select the best out of these 30 responses as the final response. (3) CLIPPRM guided search: This method uses CLIP-ViT as the PRM to guide search. Since CLIP-ViT also serves as the reward model for training VisVM, comparing VisVM-guided search with CLIP-PRM guided search serves as the faircomparison baseline. For CLIP-PRM guided search, we adopt the same search method as described in Section 3.3, with the only difference being that the guided signal is replaced by the CLIP similarity. All hyperparameters are kept Table 1. Human evaluation over 200 image-text pairs. VisVM guided search still far surpasses other search methods, displaying results consistent with GPT evaluation. Method VisVM wins Tie VisVM loses vs. CLIP-PRM vs. BoN vs. Greedy 66.0% 63.5% 74.0% 5.5% 8.5% 6.5% 28.5% 28.0% 19.5% identical to those used for VisVM-guided search to ensure fair comparison. We use temperature decoding with five different temperatures and greedy decoding to generate response candidates at each search step with step size of 1, leading to six different response candidates per search step. The list of temperature configuration includes [0.1, 0.3, 0.5, 0.7, 0.9]."
        },
        {
            "title": "1 VisVM-Guided Search Improves Response Quality",
            "content": "We sample 1,000 images from the COCO Train2017 dataset and randomly pair each image with 9 prompts from the LLaVA-150k detailed description dataset. This process results in 1,000 prompt-image pairs as an evaluation dataset. We use our method and three search baselines to generate detailed descriptive caption for each pair, and subsequently assess the quality of the descriptions. We start with human evaluation with results shown in Table 1. We randomly choose 200 prompt-image pairs from the evaluation dataset to let human annotators determine which method produces higher quality descriptions and calculate the win rate. We find that the descriptions generated by VisVM-guided search are significantly more preferred compared with the other three baselines, with the win rate of 66.0%, 63.5%, and 74.0%. Descriptions generated through greedy decoding are of the lowest quality. While increasing inference compute via BoN and CLIP5 PRM guided search show improvement, they still fall short compared with VisVM-guided search. As shown in Figure 4, the image description obtained using VisVM search not only significantly reduces hallucinations, but also provides more precise and detailed depiction of the image. For instance, the description includes subtle details that even meticulous human annotator might overlook, such as There are also green street signs...which are partially obscured by the raindrops on the windshield. We provide additional qualitative results in Appendix B. Furthermore, we use GPT-4o to compare VisVM-guided search against other three baselines,with the results presented in Figure 3a. The prompt used for GPT-4o evaluation is in Appendix A. We observe notable superiority in the win rate of the VisVM-guided search, with the win rate of 58.7%, 56.8%, and 61.5%. This further demonstrates that VisVMs strategy of predicting long-term values to select each step response sentence significantly enhances VLMs capability of visual comprehension and image description."
        },
        {
            "title": "2 VisVM-Guided Search Reduces Visual Hallucination\nTo benchmark the benefits of VisVM in improving visual\ncomprehension, we evaluate the degree of visual hallucina-\ntion present in the generated responses. We randomly sam-\nple 500 images from the COCO Val2014 dataset and use\nprompts from the LLaVA-150k detailed description dataset.\nThe widely used CHAIR [37] metric is used for hallucina-\ntion evaluation, which reflects the degree of visual halluci-\nnation as follows:",
            "content": "CHAIRI = CHAIRS = {hallucinated objects} {all mentioned objects} {captions with hallucinated objects} {all captions} , . (2) Table 2. Hallucination evaluation results using different inferencetime searching on CHAIR and MMHal. VisVM guided search achieves the best results, demonstrating strong capabilities in mitigating inference-time hallucination. Base Searching Method CHAIRs CHAIRi MMHal MMHal rate LLaVA-Next-7B Greedy (Default) BoN CLIP-Guided VisVM-Guided 32.4 27.1 28.4 26.2 5.9 5.2 5.5 4.6 2.94 3.06 2.96 3. 0.52 0.45 0.49 0.39 We also use MMHal [42] as another benchmark for hallucination evaluation. The experiment results in Table 2 show that VisVM-guided search significantly outperforms other three baselines, with CHAIRs reduced from 32.4 to 26.2, CHAIRi reduced from 5.9 to 4.6, MMHal improved from 2.94 to 3.30, and MMHal rate reduced from 0.52 to 0.39. This reduction in hallucination within the image descriptions generated via VisVM-guided search aligns with our training objective for VisVM. Specifically, using the CLIP score as reward, VisVM is trained through TD learning to select responses at each step that minimize future hallucinations, thereby enhancing the overall quality of the responses. Besides, it is worth noting that although the search budget of BoN is much larger than our method (step size 30 vs step size 6), our method still achieves better reIn comparison, the CLIP-PRM guided search persults. forms worse than BoN using the same step size as us. This further demonstrates the strong capability of VisVM to mitigate inference-time hallucination."
        },
        {
            "title": "3 Benefits from Further Scaling Up Inference Compute",
            "content": "We next investigate the impact of scaling up the inferencetime compute on the VLM response quality at each step, by changing the search step sizes. To support larger maximum step size, we only keep = 0.5 as the temperature configuration when experimenting with different step sizes. We use CHAIRs as the evaluation metric, with the same evaluation data and prompts as in Table 2. We report the CHAIRs scores for image descriptions obtained using VisVM-guided search and CLIP-PRM-guided search at step sizes of 2, 4, 8, and 16. The experimental results are depicted in Figure 3b. We observe that the performance of both VisVM-guided search and CLIP-PRM-guided search improves progressively as the search step size increases, indicating that scaling inference-time computation can enhance the visual comprehension capabilities of VLMs. Notably, as the step size grows, the performance improvement of VisVMguided search accelerates at faster rate, resulting in widening performance gap between the two methods. Additionally, VisVM proves to be nearly twice as computationally efficient as CLIP-PRM for reaching comparable performance: at step size of 8, VisVM achieves results comparable to those of CLIP-PRM at step size of 16. These findings further validate the effectiveness and efficiency of VisVM as superior inference-time search signal for VLMs. 4.2. Self-Training Vision-Language Model Inference-time search with VisVM proves to be an effective approach in boosting VLMs visual comprehension capability. This naturally motivates the question: Can we use the higher-quality descriptive captions generated by VisVMguided search to further improve the original VLM, thereby enabling form of self-training pipeline. Training details. We start with the 9,215 <image, prompt> pairs from Section 3.2, which are used to generate VisVM training data. LLaVA-Next-Mistral-7B continues to serve as our base model. We first generate corresponding image descriptions for all 9,215 <image, prompt> pairs using VisVM-guided search, resulting in 9,215 <image, prompt, description> tuples as the SFT dataset. Subsequently, we conduct full parameter fine-tuning of LLaVANext-Mistral-7B using this SFT dataset for five epochs with Table 3. Performance after fine-tuning LLAVA-Next-7B with image descriptions obtained using different search methods. The model with VisVM search as data source achieves the best performance across all benchmarks, with an average improvement of 10.8% compared with the base model. We calculate the final performance improvement using 100-CHAIRs, 10-CHAIRi, and 1-MMHal rate respectively. Visual Comprehension Benchmark Hallucination Benchmark Base SFT Data Source LLaVA-Next-7B Greedy decoding CLIP-BoN (6) GPT4o-BoN (30) CLIP-PRM search VisVM search - M 45.2 43.5 42.8 47.1 46.1 48.3 e M 74.9 74.6 76.2 76.1 75.8 76.7 M 34.2 34.9 35.2 35.4 35.8 36.1 i a 38.5 37.8 39.7 40.9 39.6 42.3 e 65.8 66.2 63.8 67.9 68.5 69. V 76.9 75.1 74.8 77.3 78.1 78.4 S 36.0 36.7 35.5 36.9 36.6 38. R 32.4 33.2 29.7 30.0 26.0 22.6 R 5.9 6.3 5.2 5.4 5.2 4.3 M 2.94 2.97 3.05 3.11 3.01 3. H t 0.52 0.54 0.48 0.47 0.50 0.44 Avg. -1.6% +2.6% +4.9% +4.6% +10.8% learning rate of 1e-6. As comparison, we also generate corresponding descriptions on this prompt dataset using greedy decoding, BoN, and CLIP-PRM-guided search, and perform full parameter SFT on LLaVA-Next-Mistral7B with the same learning rate and number of epochs. To be specific, we utilize two BoN methods to generate selftraining data as baselines. The first is CLIP-BoN, where the average CLIP score of all sentences in the response is used as the selection criterion, with step size of 6, consistent with VisVM guided search and CLIP-PRM guided search. The second is GPT4o-BoN, which selects the best response using GPT-4o, with larger step size of 30. Among these two methods, the first provides more fair comparison to our approach, while the second serves as stronger baseline for comparison. All experiments are conducted on 8A100 80GB GPUs. Evaluation benchmarks. We conduct evaluations on two types of benchmarks: visual comprehension benchmarks and hallucination benchmarks. the visual comprehension evaluation, we select seven standard benchmarks: MM-Vet [70], MMBench [33], MMMU [72], MathVista [34], CVBench [47], LLAVA-Wild [31], and MMStar [9]. For hallucination evaluation, we benchmark on CHAIR [37] and MMHal [42]."
        },
        {
            "title": "For",
            "content": "Evaluation results on visual comprehension. Table 3 presents the fine-tuning results of LLaVA-Next on visual comprehension benchmarks. Performance improved across nearly all benchmarks after self-training, with one exception of the greedy decoding self-training, which leads to decline in most cases. Among the methods evaluated, the VisVM search self-training approach demonstrates the most significant improvement, boosting LLaVA-Nexts average performance by 5.5%. This gain far exceeds the improvements achieved by the BoN and CLIP-PRM search methods. These findings highlight the superior quality of descriptive captions obtained through VisVM search, which significantly enhances LLaVA-Nexts visual comprehension capabilities during self-training. Evaluation results on visual hallucinations. As shown in Table 3, the VisVM search self-training significantly reduces hallucination in LLaVA-Next. When evaluated across four metrics on two benchmarks, VisVM search selftraining decreases the hallucination rates of LLaVA-Next by 20.3%, substantially outperforming the reductions achieved by CLIP-BoN, GPT4o-BoN, and CLIP-PRM search, which are 8.3%, 8.0% and 8.3%, respectively. These results further validate the effectiveness of the VisVM search selftraining approach. The promise of VLM self-training pipeline. The experiment results in this section demonstrate that the VisVM search significantly enhances the visual comprehension capabilities of LLaVA-Next by generating high-quality descriptive captions as the SFT data. Throughout this process, no external models or human annotations are utilized beyond the raw COCO images. The reward model for training VisVM is derived from the CLIP encoder embedded within LLaVA-Next, and VisVM itself is initialized from the parameters of LLaVA-Next. The SFT data is produced by VisVM-guided LLaVA-Next search, ensuring that all training signals originated solely from the same VLM, LLaVANext. As future directions, we see great promise in applying this method to other VLMs, leading to genuine self-training pipeline that could continuously self-improve VLMs visual comprehension capability, without reliance on any external models or human annotations. 5. VisVM Analysis To better understand how VisVM influences VLMs response generation, this section examines how VisVM and CLIP-PRM select responses when presented with the same set of candidates. As illustrated in the upper part of Figure 4, given an image and prompt, LLAVA-Next generated three different response candidates. When using CLIP to directly score the responses, the second candidate received the highest clip score which is 0.2617 and is thus chosen as the current step response. However, VisVM considers potential hallucinations in subsequent responses induced by Figure 4. case study on VisVM-guided search. The upper part shows how VisVM and CLIP-PRM make different choices when given the same step response candidates, and the changes in the LLaVA-Next attention map after the choices are made. We can observe significant differences in the attention maps. The second part presents the complete responses obtained using different search methods, with blue text indicating correct details and red text indicating hallucinations. VisVM-guided search obtains response with richer details and fewer hallucinations. the sentence, resulting in higher value 2.2695 for the third sentence among the given candidates. Therefore, the third sentence is selected. We search and obtain the final complete response using both CLIP-PRM and VisVM following their choices, as shown in the lower part of Figure 4. We observe that the response generated through VisVM search indeed contains more details and less hallucinations. Additionally, Figure 4 also presents the changes in the LLAVA-Nexts image-text cross-attention map following the selection of different candidates. We note significant differences in the attention maps after choosing different candidates. VisVMs attention map more comprehensively 8 covers the entire image, enabling it to catch visual details such as water droplets obscuring the road sign. In contrast, the CLIP-PRMs attention map over-emphasizes the area around the traffic light, leading to inaccurate description of the lights color and missing other visual details. 6. Conclusion We have presented VisVM, vision value model that effectively guides VLM for inference-time search to improve visual comprehension. Our results demonstrate that scaling inference-time computations can produce VLM responses that include richer visual details and reduce hallucinations. Among various reward signals, VisVM has better scaling behavior due to its consideration of potential future generations. Moreover, we highlight the promise of using VisVMguided search to establish self-training pipeline, enabling the enhancement of VLMs without external annotations."
        },
        {
            "title": "Acknowledgments",
            "content": "Wang, Xu, and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, National Science Foundation NSF-IIS-2147276 FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DODAFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships."
        },
        {
            "title": "References",
            "content": "[1] Learning to reason with llms. https://openai.com/ index/learning-to-reason-with-llms/, 2024. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 3 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1, 3 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 2, 3 [5] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [6] Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and Furong Huang. Transfer star: Principled decoding for llm alignment. arXiv preprint arXiv:2405.20495, 2024. 3 [7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35583568, 2021. 2 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2, 3 [9] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024. 7 [10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 3 [11] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. Mitigating hallucination in visual language models with visual supervision, 2023. 3 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 3 [13] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 2, [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [15] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 3 [16] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models, 2024. 2, 3 [17] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control, 2022. 3 9 [18] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. Vstar: Training verifiers for self-taught reasoners, 2024. 2, [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 3 [19] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In CVPR, 2022. 2 [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021. 2 [21] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning, 2024. 1 [22] Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, et al. Revisit large-scale imagecaption data in pre-training multimodal foundation models. arXiv preprint arXiv:2410.02740, 2024. 3 [23] Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, et al. Veclip: Improving clip training via visual-enriched captions. In European Conference on Computer Vision, pages 111127. Springer, 2025. 2, [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. 1 [25] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. 3 [26] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities, 2024. 3 [27] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024. 1 [28] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. 2, 3 [29] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. 2, [30] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal arXiv preprint model with robust instruction tuning. arXiv:2306.14565, 2023. 2, 3 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 7 [33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. 7 [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 7 [35] OpenAI. Gpt-4v(ision) system card. 2023. 3 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 2, 3 [37] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. 6, 7 [38] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. [39] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. 3 [40] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. 2, 3 [41] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. 2 [42] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023. 3, 6, 7 [43] Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:944, 1988. 2, 4 [44] Google Gemini Team. Gemini: family of highly capable multimodal models, 2023. [45] Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng Ai, Dongsheng Yang, Lingxi Li, Zhe Xuanyuan, Fenghua Zhu, and Long Chen. Motion planning for autonomous driving: The state of the art and future perspectives. IEEE Transactions on Intelligent Vehicles, 8(6): 36923711, 2023. 3 [46] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-improvement of llms 10 via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253, 2024. 2, 3 edge with curriculum preference learning. arXiv preprint arXiv:2410.06508, 2024. 3 [47] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. [48] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. 2, 3 [49] Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu. Litesearch: Efficacious tree search for llm. arXiv preprint arXiv:2407.00320, 2024. 3 [50] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 3 [51] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 3 [52] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites, 2023. 3 [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 1 [54] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. 2, 3 [55] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [56] Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, and Furong Huang. to exCoplanner: Plan to roll out conservatively but arXiv preprint plore optimistically for model-based rl. arXiv:2310.07220, 2023. 3 [57] Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visuallanguage modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024. 3 [58] Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong Yu. Towards selfimprovement of llms via mcts: Leveraging stepwise knowl- [59] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, and Furong Huang. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 416442. Association for Computational Linguistics, 2024. 2, 3 [60] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In ICLR, 2022. 3 [61] Wei Wu, Kecheng Zheng, Shuailei Ma, Fan Lu, Yuxin Guo, Yifei Zhang, Wei Chen, Qingpei Guo, Yujun Shen, Improving language-image and Zheng-Jun Zha. Lotlip: arXiv preprint pre-training for long text understanding. arXiv:2410.05249, 2024. 3 [62] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. Mmed-rag: Versatile multimodal rag system for medical vision language models. arXiv preprint arXiv:2410.13085, 2024. [63] Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024. 2 [64] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LlavaarXiv critic: Learning to evaluate multimodal models. preprint arXiv:2410.02712, 2024. 3 [65] Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, and Sumitra Ganesh. Genarm: Reward guided generation with autoregressive reward model for test-time alignment. arXiv preprint arXiv:2410.08193, 2024. 3 [66] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. 2 [67] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5math technical report: Toward mathematical expert model via self-improvement, 2024. 2 [68] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. [69] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 3 11 [70] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. 7 [71] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Florence: new Boxin Li, Chunyuan Li, et al. arXiv preprint foundation model for computer vision. arXiv:2111.11432, 2021. 2, 3 [72] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. 7 [73] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training arXiv preprint via process reward guided tree search. arXiv:2406.03816, 2024. 2, 3 [74] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. [75] Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. arXiv preprint arXiv:2405.14622, 2024. 3, 4 12 Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension"
        },
        {
            "title": "Supplementary Material",
            "content": "GPT-4o evaluation prompt Evaluate the following image captions generated by two vision-language models (VLMs) in response to given image. Please decide which caption is better based on the following criteria: Richness of Content: The caption should provide comprehensive description of the visual elements in the image, detailing objects, actions, colors, or settings that enhance the viewers understanding of the scene. Accuracy: The caption must accurately reflect what is in the image without adding incorrect information or making erroneous assumptions about unseen aspects. Harmlessness and Appropriateness: The caption should be free from any potentially harmful, offensive, or inappropriate content. Ensure that the caption does not make unnecessary assumptions about peoples identities, backgrounds, or intentions. Creativity and Elaboration: If the caption can remain accurate and harmless, it should include suitable and imaginative elaborations that enrich the scenes description (e.g., suggesting possible actions, moods, or gentle interpretation of the scenes context). Clarity and Coherence: The caption should be clear, concise, and well-structured, helping any viewer quickly understand the images content without ambiguity. After considering these criteria, please directly output your choice, the format of your choice should be Response1 is better, Response2 is better or Tie. Response1: {} Response2: {} Table 4. Prompt used for image caption quality evaluation with GPT-4o. A. GPT-4o evaluation prompts In this section, we provide the detailed prompt for GPT-4o evaluation in Section 4.1. We evaluate the caption quality from 5 aspects: Richness of Content, Accuracy, Harmlessness and Appropriateness, Creativity and Elaboration, Clarity and Coherence. B. More case studies In this section, we give more case studies from Table 5 to Table 13 to compare VisVM guided decoding results and CLIP-PRM guided decoding results. C. Details of VisVM training Our training dataset consists of 378k <current sentence, current sentence clip score, next sentence, image>. Based on this training set, we train VisVM for 3 epochs with learning rate of 5e-5 and batch size of 1024. The latent dimension of the value head in VisVM is 2560. γ used for TD learning is 0.9. The entire training process is conducted on 8x80G A100 GPUs for 50 hours. Example 1 Question What is happening in this scene? VisVM Guided CLIP-PRM Guided In the scene, there is rainy day with sky that is overcast and appears to be either raining or just after rainfall. The image is taken from inside vehicle, as indicated by the raindrops on the windshield, and the photo captures the view looking towards an intersection ahead. At the intersection, there are traffic lights hanging over the road. The traffic light closest to the camera is currently red, indicating that vehicles are required to stop. There are also green street signs attached to the traffic light pole, but they are partially obscured by the raindrops on the windshield. On the road, there is vehicle with its headlights on, which suggests that it is either raining or the vehicles headlights are on for visibility. There are also other vehicles visible in the distance, including truck and car. The buildings in the background have brick facade, and there is church with steeple visible on the right side of the image. The overall atmosphere of the scene is somewhat gloomy due to the rain and the overcast sky. In the scene, there is street intersection with traffic lights. The traffic lights are currently displaying red signal, indicating that vehicles must stop. There are several traffic lights visible, with some showing red and others showing yellow, which typically means to prepare to stop. The sky is overcast, suggesting it might be cloudy or rainy day. The road appears to be wet, which could be due to recent rain. There are vehicles on the road, including cars and truck, and pedestrian is visible on the sidewalk. On the right side of the image, there is building with sign that reads Tennis, indicating that there is tennis facility nearby. The overall atmosphere of the scene is quiet and typical of small town or suburban area. Table 5 2 Example 2 Question What is happening in this scene? VisVM Guided CLIP-PRM Guided In the image, there is person riding motorcycle or moped with large bundle of bananas tied to the back. The person is wearing helmet and appears to be on rural road, with lush greenery and trees on both sides, suggesting tropical or subtropical region. There are other people in the background, some of whom are walking along the road, and theres small building or structure visible in the distance. The road seems to be secondary road, with yellow markings indicating its two-way road, and theres sign or symbol on the right side of the road that is partially obscured. The overall scene suggests day-to-day activity in rural or semi-rural area, possibly related to transportation of goods or agricultural produce. In the image, there is person riding motorcycle loaded with large quantity of green bananas. The bananas are tied to the back of the motorcycle, and the rider appears to be transporting them down road. The setting is rural, with trees and greenery surrounding the road, suggesting that the bananas might have been harvested from nearby plantations. The person is wearing helmet, which is good safety practice when riding motorcycle. The scene captures moment of daily life in region where bananas are significant crop. Table 6 Example 3 Question Can you describe the main features of this image for me? VisVM Guided The image shows single-engine, propeller-driven aircraft flying against clear blue sky. The aircraft has bright red fuselage with white and blue stripes running along its side, and white and blue striped tail. There are no visible registration numbers or other identifying marks on the aircraft. The aircraft appears to be small personal or recreational plane, given its size and the absence of any visible branding or commercial markings. The wings are swept back slightly, and there are no visible landing gear, suggesting that the aircraft is either in flight or has just taken off. The image captures the aircraft from perspective below and to the side, providing clear view of its underside and tail section. CLIP-PRM Guided The image shows red airplane flying against clear blue sky. The aircraft has single propeller and tail fin with white stripe. It appears to be small, single-engine plane, possibly vintage or classic model, given its design and color scheme. The plane is captured in mid-flight, with its nose pointed slightly upward. The wings are swept back, which is common feature in many small aircraft for improved aerodynamics. There are no visible windows on the fuselage, suggesting it may be oneseater or light utility aircraft. The overall impression is that of leisurely flight or demonstration of the aircrafts capabilities. Table 7 4 Example 4 Question Can you elaborate on the elements of the picture provided? VisVM Guided CLIP-PRM Guided In the image, there is soccer player in action on field. The player is wearing white soccer kit with red number 8 on the front, which is common way to identify players position or role within team. The jersey also has an emblem that appears to be bird, possibly falcon, which could be the teams logo. The player is wearing white socks with black cleats, which are typical for soccer players. The player is in the process of kicking soccer ball, which is white with black pentagons, known as pentagon ball, which is the standard shape for soccer balls. The players body is in dynamic pose, with one leg extended forward and the other bent at the knee, indicating powerful kick. The field is marked with white lines, which are standard for soccer fields, and there is yellow line on the side, which could be the touchline or the goal line, depending on the specific context of the game. In the background, there is chain-link fence, which is common for soccer fields to keep the ball from going out of play. There is also vehicle, possibly car or van, partially visible behind the fence, suggesting that the field might be located in park or recreational area near road. The sky is overcast, and the lighting suggests it might be late afternoon or early evening, as the shadows are long and the light is diffused. The overall atmosphere of the image is one of action and competition, capturing moment of athletic endeavor. In the image, we see soccer player in action on field. The player is wearing white uniform with red number 8 on the front, which is likely his teams jersey number. The uniform is the standard soccer attire, with shorts, shirt, and possibly socks and shin guards that are not fully visible. The player is wearing yellow and orange cleats, which are common for soccer players to provide traction on the field. The player is in the process of kicking the soccer ball, which is in mid-air, indicating dynamic moment in the game. The players right foot is extended towards the ball, suggesting powerful strike. The players body is positioned with one leg bent and the other extended, which is common stance for player about to kick the ball. The background of the image shows soccer field with goal post visible, indicating that the player is near the goal area. The field appears to be wellmaintained with clear boundary lines, and there are no visible spectators or other players, which puts the focus on the player and his action. Overall, the image captures snapshot of an intense moment during soccer game. The athletes concentration and the positioning of the ball and the players leg suggest high level of skill and athleticism. Table 8 5 Example Question Write detailed description of the given image. VisVM Guided CLIP-PRM Guided In the heart of bustling city, small Chihuahua with vibrant coat of black and brown fur is the star of this scene. The dog, standing on its hind legs, has its front paws resting on the window sill of white car. The car is parked on the side of street, with building visible in the background. The dog is dressed in sweater that is riot of colors - red, blue, green, and yellow. The sweater, made of soft fabric, adds touch of whimsy to the otherwise mundane urban setting. The dogs head is tilted to the side, as if its curiously observing its surroundings. Its eyes are wide open, taking in the sights of the city. Despite being inside car, the dog seems to be enjoying the view outside. The image captures moment of joy and curiosity in an everyday urban scene. Its reminder that even in the hustle and bustle of city life, theres always room for little bit of fun and playfulness. In the heart of bustling city, small Chihuahua has found its perch on the passenger side of car. The dog, dressed in vibrant sweater, is the star of this urban tableau. The sweater, riot of colors, is adorned with playful polka dot pattern that stands out against the dogs dark fur. The dog is seated comfortably on the cars door sill, its front paws resting on the window ledge. Its head is tilted slightly to the left, as if its curiously observing the world outside. The window itself is rolled down, allowing the dog to enjoy the sights and sounds of the city. The car itself is stark white, contrasting with the dogs colorful attire. Its parked on street lined with buildings, their details blurred by the distance. The overall scene paints picture of day in the life of this charming Chihuahua. Table 9 6 Example 6 Question Can you describe the main features of this image for me? VisVM Guided The image depicts serene outdoor setting with view of the ocean. In the foreground, there is man seated at table with red chair. He appears to be looking out at the water, possibly enjoying the view or perhaps deep in thought. The table is covered with red cloth, and there is Coca-Cola logo on the chair, suggesting the location might be restaurant or cafe with branded furniture. In the background, there is large red and white boat on the water, which seems to be in motion, as suggested by the wake it is leaving behind. The sky is overcast, with soft light that suggests either dawn or dusk. The water is calm, reflecting the colors of the sky and the boat. The overall scene is one of tranquility and solitude, with the man at the table as the main subject, framed by the expansive view of the ocean and the boat. The red color of the tablecloth and chair adds vibrant contrast to the cooler tones of the water and sky. CLIP-PRM Guided The image shows person sitting at table on what appears to be pier or waterfront. The person is facing away from the camera, looking out towards the water. The table is set with red tablecloth and there are two red chairs, one of which the person is occupying. In the background, there is large body of water, and on the water, there is red and white boat that seems to be moving away from the pier. The sky is overcast, suggesting either early morning or late afternoon light conditions. The overall atmosphere of the image is calm and serene, with sense of solitude and contemplation. Table 10 7 Example 7 Question Whats happening in the scene? VisVM Guided CLIP-PRM Guided In the scene, theres store named Pinocchio Fancy Dress which suggests it sells costumes or special occasion outfits. Theres display in the window with various mannequins dressed in different styles of clothing, which might be used to showcase the types of outfits available inside the store. In front of the store, theres statue of large cow with spotted pattern, possibly representing character from story or just decorative element. The cow is positioned in such way that it appears to be walking towards the store, which adds whimsical or playful touch to the storefront. On the right side of the image, theres sign that reads Dress Hire indicating that the store offers rental services for costumes or dresses. There are also some signs with phone numbers and possibly some text that is too small to read clearly, likely providing additional information about the stores offerings or contact details. The overall impression is that this is storefront with creative and engaging display designed to attract customers and convey the type of business it operates. In the scene, there is life-size statue of cow with black and white color pattern, standing in front of store named Pinos Fancy Dress. The storefront has sign that reads Dress Hire and there are other signs indicating the services offered, such as Fancy Dress, Adult & Childrens Fancy Dress, and 100s Fancy Dress. The cow statue appears to be decorative element to attract attention to the store. To the left of the cow statue, there is mannequin dressed in what appears to be traditional or historical costume, which could be related to the dress hire services offered by the store. The overall setting suggests this is shop that rents costumes for various occasions, and the cow statue could be fun and quirky way to draw customers in. Table 8 Example 8 Question Can you elaborate on the elements of the picture provided? VisVM Guided In the image, there are several elements that contribute to the urban scene. The most prominent figure is police officer standing in the middle of the street. The officer is wearing dark-colored uniform with the word POLICE printed on the chest, indicating their role. They are also wearing cap, sunglasses, and holster, which are standard items of police attire. The officer is standing on skateboard, which is common mode of transportation in urban areas. This action could suggest that the officer is taking break from duty or perhaps patrolling an area where skateboarding is allowed or encouraged. In the background, there is busy street with vehicles, including white SUV and yellow taxi cab, which are typical of city traffic. There are also other pedestrians visible, contributing to the bustling atmosphere of the city. The architecture of the buildings suggests modern cityscape, with high-rise buildings that are common in many urban centers. The presence of traffic lights and street signs further emphasizes the urban setting. The image captures moment of everyday life in city, with the police officer on skateboard adding unique and dynamic element to the scene. CLIP-PRM Guided The image shows police officer riding skateboard in city street. The officer is wearing uniform that includes dark-colored shirt with the word POLICE on the front, and hes also wearing cap with badge. He has walkie-talkie on his belt and is holding bag, possibly containing personal items or equipment. The officer is on skateboard, which is common mode of transportation for urban areas. The skateboard is red and black, and the officer is wearing casual shoes and sunglasses. In the background, there are other people and vehicles, indicating that this is busy urban environment. Theres white SUV to the left of the officer, and traffic light is visible in the distance. The architecture suggests modern cityscape, and the presence of building with sign that reads BROOKLYN indicates that this scene might be taking place in city that has borough named Brooklyn. Overall, the image captures moment of urban law enforcement using skateboard as means of navigating through the city. Its snapshot of unique approach to policing in contemporary urban setting. Table 12 9 Example 9 Question Can you elaborate on the elements of the picture provided? VisVM Guided CLIP-PRM Guided In this snapshot, it appears to be scene from bar or restaurant with focus on the beverage selection. The refrigerator, which is likely beer cooler, is prominently stocked with various bottles of what seems to be beer. The lighting is dim, with warm glow coming from the refrigerator, which is typical for establishments that want to highlight their beverage offerings. The presence of potted plant and shelf with more bottles in the background suggests well-decorated and inviting space. Its possible that the image was taken during quiet moment or just before or after opening or closing hours when the establishment is not busy. In this snapshot, it appears to be an interior view of bar or restaurant with focus on refrigerator. The refrigerator is stocked with various bottles, likely containing beverages such as beer, wine, or cocktails. The lighting in the room is dim, with the refrigerator illuminated, which suggests that it might be evening or the place is intentionally dimly lit. The presence of wine glasses on shelf above the refrigerator indicates that the establishment serves alcoholic beverages. The ambiance seems to be casual and relaxed, possibly catering to social or dining experience. Table"
        }
    ],
    "affiliations": [
        "Microsoft",
        "University of Maryland, College Park"
    ]
}