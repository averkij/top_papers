{
    "paper_title": "Animate Any Character in Any World",
    "authors": [
        "Yitong Wang",
        "Fangyun Wei",
        "Hongyang Zhang",
        "Bo Dai",
        "Yan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence."
        },
        {
            "title": "Start",
            "content": "Yitong Wang1,2* Fangyun Wei2* Hongyang Zhang3 Bo Dai4 Yan Lu2 1Fudan University 2Microsoft Research 3University of Waterloo 4The University of Hong Kong https://snowflakewang.github.io/AniX/ 5 2 0 2 8 1 ] . [ 1 6 9 7 7 1 . 2 1 5 2 : r Figure 1. AniX enables users to provide 3DGS scene along with 3D or multi-view character, enabling interactive control of the characters behaviors and active exploration of the environment through natural language commands. The system features: (1) Consistent Environment and Character Fidelity, ensuring visual and spatial coherence with the user-provided scene and character; (2) Rich Action Repertoire covering wide range of behaviors, including locomotion, gestures, and object-centric interactions; (3) Long-Horizon, Temporally Coherent Interaction, enabling iterative user interaction while maintaining continuity across generated clips; and (4) Controllable Camera Behavior, which explicitly incorporates camera controlanalogous to navigating 3DGS viewsto produce accurate, user-specified viewpoints."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging *Equal contribution. Corresponding author. the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing openended actions. Users can provide 3DGS scene and character, then direct the character through natural language to perform diverse behaviorsfrom basic locomotion to object-centric interactionswhile freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as conditional autoregressive video generation problem. Built upon pre-trained video gen1 erator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence. 1. Introduction Recent advances in world models have led to substantial progress in simulating dynamic and interactive environments. Existing methods generally fall into two categories: (1) static world generation approaches [43, 55, 89], which construct explorable 3D environments but lack active agents; and (2) controllable-entity approaches [45, 90], which allow single agent to execute only limited set of actions, such as steering vehicle along predefined path [14, 29, 73], while leaving the environment itself uncontrollable. In this work, we propose an alternative framework that combines the strengths of both paradigms: leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Specifically, users can provide 3D Gaussian Splatting (3DGS) scene [38]representing either synthetic environment or real-world reconstructionalong with 3D or multi-view character. Through natural language instructions, users can control the characters behavior and enable active exploration within the environment. At each iteration, the model generates video clip that captures the evolving states of both the character and the environment, resulting in coherent and temporally consistent generation. We name our method AniX. As illustrated in Figure 1, AniX exhibits several key capabilities: 1. Consistent Environment and Character Fidelity. The visual contents appearing in the generated video clips exhibit strong consistency in visual identity and spatial layout with the user-provided scene and character. 2. Rich Action Repertoire. Unlike previous works [7, 29, 45] that limit the controllable entity to basic locomotion, our model enables the character to perform up to hundreds of distinct actions, encompassing not only navigation behaviors (e.g., moving forward, turning left) but also body-language gestures (e.g., waving hands, saluting) and object-centric interactions (e.g., making phone call, playing guitar). 3. Text Instruction as the Interface. Users can directly guide the character through natural language commands. 4. Long-Horizon, Temporally Coherent Interaction. Users can interact with the model iteratively, generating new video clips that remain temporally consistent with previously produced sequences. 5. Controllable Camera Behavior. Our model supports flexible and intuitive camera control, allowing behaviors such as following characters trajectory or orbiting around it to achieve user-specified viewpoints. Unlike previous methods [27, 28] that encode camera trajectories into Plucker embeddings [71] and inject them as conditioning signals into the generation network, our approach achieves camera control in more geometrically grounded manner. Specifically, given user-provided 3DGS scene and defined camera path, we directly render projection scene video along the specified trajectory. This rendered video serves as an explicit conditioning input, enabling the generation model to produce videos that accurately follow the desired camera motion. We formulate the entire process as conditional autoregressive video generation problem. Concretely, the objective is to synthesize video clip at each iteration, conditioned on set of multi-modal inputs including: (a) the user-provided scene and character, which establish the spatial and visual grounding; (b) text instruction, which specifies the intended behavior of the character; and (c) the previously generated video clips, which serve as temporal references to ensure consistency across iterations. Additionally, we adopt pre-trained video generator [31] as the foundation of our framework. We find that finetuning it on small dataset containing basic locomotion actions across diverse characters not only preserves the generalization ability of the pre-trained model but also enhances overall motion quality compared to the original generator. This phenomenon is further analyzed in Section 4.1. In our experiments, we comprehensively evaluate the proposed model from multiple perspectives, including: (a) visual quality, assessed using the WorldScore benchmark [11]; (b) character consistency, which measures the alignment between the character appearing in the generated video and the user-provided reference; (c) action control success rate, which quantifies how accurately the characters behavior follows the input text instruction across diverse set of up to around 150 actions; and (d) long-horizon generation quality, which evaluates the models ability to maintain temporal coherence and visual fidelity over extended interaction sequences. We compare AniX with both video generation foundation models [39, 78, 88] and dedicated world models [7, 29, 45]. Experimental results show that our method consistently outperforms both categories across nearly all evaluation metrics. 2. Related Works Controllable Video Generation. Recent foundation models for video generation [4, 21, 25, 39, 40, 60, 78, 88] have greatly improved modality alignment across text, image, and video, enabling large-scale pre-training for both textto-video and image-to-video synthesis. Building on these advances, subsequent research has pursued finer-grained 2 controllability by introducing mechanisms such as explicit subject control [6, 9, 13, 3436, 51, 54] and camera control [1, 15, 22, 24, 57, 67, 80, 85, 86, 95, 96, 102]. For subject control, typical approach [31, 49] is to extract visual embeddings from reference images and use them within Multimodal Diffusion Transformer [12, 63] to guide the generated subjects to remain consistent with the reference appearance. For camera control, one common practice [27, 28] is to convert the camera path into Plucker embeddings [71] and inject them into the main network, guiding the synthesized video to follow the specified trajectory. In contrast, our model controls the camera by navigating through 3DGS views: given 3DGS scene and camera path, we render projection video along the path, which conditions the generator to follow the desired motion faithfully. Memory Mechanism in Video Generation. Recent works incorporate memory mechanisms to improve long-term spatial and temporal consistency in video generation. These approaches retrieve generation history to localize relevant content across modalities such as RGB [93] and depth [7], often using surfel-indexed view selection [46] or camera FOV overlap [82, 93]. Other methods [32, 33, 81, 92] maintain global point cloud map during generation, enabling the model to identify and reuse the most relevant spatial regions, thereby maintaining coherence across continuously generated scenes. In our work, the video generation with memory mechanism is realized by conditioning on both the character and the 3DGS scene. The 3DGS scene serves as spatial memory that explicitly encodes the geometric and appearance information of the environment, while the character provides dynamic cues for motion and interaction. World Models for Static Scene Creation. Existing world models that generate static yet explorable environments can be broadly categorized into two types. The first type [5, 10, 14, 20, 29, 37, 41, 42, 45, 58, 73, 77, 84, 90, 94, 99] stores the world implicitly within neural networks, using video generation models [21, 39, 78, 88] to visualize it. Users provide navigation commands (e.g., camera forward) drawn from predefined set of camera trajectories, and the model synthesizes new frames along this path while maintaining spatial consistency with past generations. The second type [47, 70] explicitly constructs 3DGS world, where multi-view images are optimized to form manipulable 3D representation, allowing users to render novel views from arbitrary camera poses. Further developments extend this paradigm by using panoramic inputs [89, 100] or directly generating 3DGS representations from text or single image through feed-forward networks [18, 50, 55, 74, 87], while others [8, 26, 72, 79, 98] integrate video generation to streamline and accelerate 3DGS creation. In this work, users can either create or specify 3DGS scene before generation. When users do not provide one, we adopt Marble [43] to automatically generate static 3DGS world. 3. Method Problem Formulation. Given pre-generated or realworld reconstructed 3DGS scene and user-specified 3D character C, AniX enables users to iteratively control the character through text instructions within the scene S, generating long-horizon, temporally coherent video clips that remain visually consistent with both and C. We formulate this process as conditional autoregressive video generation problem. At each iteration, AniX synthesizes the current video clip conditioned on multiple multithe previous generated clip, character repmodal inputs: resentations, scene representations, and the current text instruction. Overviews of the training and inference pipelines are shown in Figure 2 and Figure 4, respectively. 3.1. Training 3.1.1. Training Data Pre-Processing Training Set Construction. As shown in Figure 2(a), our training data is GTA-V [17], game where players can control character to perform basic actions such as run forward. We record gameplay sequences and segment them into short video clips, ensuring that each clip (1) contains only single action and (2) has fixed length of 129 frames. For each clip , we apply the following steps: 1. Character Segmentation. We use Grounded-SAM-2 [66] to segment characters and extract their bounding-box mask sequence, denoted as . 2. Scene Inpainting. We remove the segmented characters and apply DiffuEraser [48] to fill the missing regions, yielding the inpainted scene video S. 3. Action Labeling. Each clip is then annotated with concise text label describing the action performed by the character, such as The character is running forward. GTA-V also provides access to the 3D character models used in the game. To ease the character modeling, we represent each character using four viewpoint renders [2, 3, 97]front, left, right, and backdenoted as = {C , CL, CR, CB}. Finally, each processed training sample is represented as tuple {V , S, , , C}, forming structured dataset. Token Extraction. As shown in Figure 2(a), given training sample {V , S, , , C}, we adopt the video VAE encoder from HunyuanCustom [31] to extract tokens for the video , scene S, and mask . The resulting token sequences are denoted as TV , TS and TM , respectively. The video VAE encoder operates with spatial downsampling rate of 8 and temporal downsampling rate of 4. Note that the video VAE encoder can also be applied to single images. Therefore, for the multi-view character = {CF , CL, CR, CB}, we use the same encoder to extract 3 Figure 2. (a) Each training sample consists of 3D character and video depicting the character performing an action described by short text. Through segmentation and inpainting, we obtain the corresponding scene video and character mask sequence. The VAE encoder is then applied to encode these inputs into tokens. (b) AniX predicts target video tokens conditioned on scene, mask, text, and multi-view character tokens within Multi-Modal Diffusion Transformer, trained using Flow Matching [52]. Refer to Figure 3 for the training process of the auto-regressive mode, which enables iterative interaction with AniX, and Figure 4 for the inference. tokens from each view, resulting in the multi-view character token set TC = {TCF , TCL, TCR , TCB }. Finally, to extract text tokens, following HunyuanCustom [31], we employ the multi-modal encoder LLaVA [53], which takes both the text instruction and character tokens TC as input. The resulting encoded text tokens are denoted as TT . Implementation details are provided in the appendix. Now, the training sample {V , S, , , C} is fully encoded into the latent space as {TV , TS, TM , TT , TC}. 3.1.2. Architecture Training Objective. Figure 2(b) illustrates the architecture of AniX, whose backbone consists of stack of fullattention Transformer blocks. We adopt Flow Matching [52] for model training, conditioned on multiple inputs (i.e., TS, TM , TT , and TC), to guide the generation process from pure noise to TV . Concretely, given target video tokens TV , we first sample [0, 1] from logit-normal distribution and initialize the noise x0 (0, I) following Gaussian distribution. The intermediate sample xt = (1 t)x0 + tTV is then obtained via linear interpolation. The model is trained to predict the velocity ut = dxt/dt by minimizing the mean squared er4 ror between the predicted velocity vt and the ground-truth velocity ut: = Et,x0,TV vt ut2 . (1) ; ] denotes channel-wise concatenation. Condition Modeling. We incorporate multiple conditioning signals to guide the learning process, including text tokens TT , multi-view character tokens TC, scene tokens TS, and mask tokens TM . As illustrated in Figure 2(b), to inject scene and mask priors, we directly fuse TS and TM into the noisy target tokens xt via: = xt + Projector([TS; TM ]), where [ The Projector maps the concatenated tokens [TS; TM ] to the same dimensionality as xt using lightweight linear layer. We refer to the resulting as the conditional noisy tokens. At last, to integrate the text tokens TT and multi-view character tokens TC, we concatenate TT , TC, and along the sequence dimension. The concatenated sequence is then fed into the backboneimplemented as stack of fullattention Transformer blocksto denoise xt under the supervision of Eq. 1. Positional Embeddings. Following HunyuanCustom [31], no positional embeddings are added to the text tokens Figure 3. Illustration of the auto-regressive mode. The only difference from the original architecture in Figure 2 is the addition of an extra conditioning inputthe preceding video tokens. Note that misalignment exists between training and inference: during training, the preceding video tokens are derived from ground-truth videos, whereas during inference, they are generated by the model itself. To mitigate this discrepancy, we add small Gaussian noise to the preceding video tokens during training and refer to the resulting tokens as augmented preceding video tokens. standard 3D-RoPE (over TT . time, height, and width dimensions) is applied to the conditional noisy tokens For the multi-view character tokens TC = t. {TCF , TCL, TCR , TCB }, each TCF , TCL, TCR , and TCB represents single-view character tokens. For each view, shifted-3D-RoPE is applied. Taking TCF as an example,"
        },
        {
            "title": "PETCF",
            "content": "(i, j) = 3D-RoPE(1, + w, + h), (2) where (w, h) denotes the spatial size of TCF , and the shifts along the temporal and spatial dimensions are 1 and (w, h), respectively. For TCL, TCR and TCB , the spatial shift remains the same, while the temporal shifts are set to 2, 3, and 4, respectively. 3.1.3. Auto-Regressive Mode AniX supports multi-round user interaction while maintaining temporal continuity and semantic coherence between adjacent video clips. To achieve this, we extend AniX into an auto-regressive mode. Specifically, we divide the target video tokens TV along the temporal dimension into two parts: the first quarter, denoted as TV 1, serves as the preceding video tokens, and the remaining three quarters, denoted as TV [2:4], serve as the new target video tokens. The model is trained to generate TV [2:4] conditioned on both the preceding video tokens TV 1 and the other conditioning signals introduced in Section 3.1.2. As shown in Figure 3, to incorporate the newly added condition TV 1 , we prepend it to the conditional noisy tokens. The fusion strategy for the other conditioning signals remains unchanged, as described in Section 3.1.2. 3.2. Inference and Acceleration Inference. Figure 4 illustrates the inference pipeline of AniX, which consists of three main stages: Figure 4. Inference of AniX. (a) Users first specify the inputs, including the character, 3DGS scene, virtual camera location, and character anchor. (b) The user-provided text instruction is parsed, and corresponding camera path is generated. Applying this path to the 3DGS scene produces rendered scene video. (c) AniX then takes multiple inputs as conditions to generate the final output. Steps (b) and (c) can be performed iteratively, enabling temporally consistent, long-horizon interactions. 1. User Configuration. Users first specify character and 3DGS scene. They can position virtual camera at any desired location within the scene and define character anchor (i.e., bounding-box mask) that determines where the character appears in the generated video. The character anchor remains consistent across all generated frames. Users may also employ existing models to generate the 3D character (e.g., Hunyuan3D [44]) or the 3DGS scene (e.g., World Labs Marble [43]). 2. Scene Video Rendering. Next, users provide text instruction to AniX. The instruction is parsed into four cat5 Table 1. WorldScore metrics for evaluating generation quality, categorized into three groups: (1) controllability, (2) quality, and (3) dynamics. The static and dynamic scores are computed by aggregating metrics from these three groups. denotes dedicated world models. Ctrl: Controllability; Align: Alignment; Const: Consistency; Photo: Photometric; Acc: Accuracy; Mag: Magnitude; Smooth: Smoothness."
        },
        {
            "title": "Static",
            "content": "Dynamic"
        },
        {
            "title": "Content\nAlign",
            "content": "3D Const"
        },
        {
            "title": "Motion\nSmooth",
            "content": "CogVideoX1.5-I2V (5B) [88] 60.08 56.77 HunyuanVideo-I2V (13B) [39] 56.43 55.14 Wan2.1-I2V (14B) [78] 57.91 55.87 54.52 51.74 Wan2.2-I2V (14B) [78] Wan-VACE (14B) [35] 51.54 52.03 50.75 49.38 Wan-VACE (1.3B) [35] HunyuanCustom (13B) [31] 62.64 61.11 DeepVerse [7] 52.63 47.63 Hunyuan-GameCraft [45] 69.92 57.77 Matrix-Game-2.0 [29] 52.26 43.98 42.13 27.30 37.32 24.79 21.29 31.12 47.19 52.48 77.45 15.10 100.00 100.00 98.33 100.00 100.00 100.00 100.00 75.00 83.33 99.17 31.12 13.70 26.34 24.03 30.39 11.54 72.07 18.80 51.16 12.38 68.65 81.43 77.86 58.24 93.22 91.97 81.88 83.77 65.03 57.95 59.44 98.91 27.53 54.18 99.02 29.66 61.00 97.67 48.06 31.40 97.47 35.47 83.16 92.39 85.91 82.12 93.39 60.29 68.35 97.60 AniX (Ours) 84.64 77.22 88.91 100.00 75.73 83.57 87.68 98.91 19.39 10.58 12.73 16.51 17.39 24.26 25.84 11.09 16.11 12. 57.72 54.92 59.36 59.59 59.60 56.73 55.87 59.04 33.30 16.65 3.07 24.58 24.46 28.60 38.93 34.78 29.48 24.05 33.61 31.83 47.36 67.62 72.58 65.07 37.26 67.98 53.17 89.56 40.97 39.77 23.59 61.08 24. 94.47 egories: (a) locomotion, (b) gesture, (c) object-centric action, and (d) camera behavior. Each category determines different camera path. For (a), AniX generates camera trajectory consistent with the motion described in the textfor example, for The character is running forward, the camera follows forward-moving path. For (b) and (c), the camera remains stationary. For (d), AniX generates trajectory matching the specified camera motionfor instance, for The camera circles around the character, the camera follows circular path around the character. AniX then renders scene video clip along the corresponding camera trajectory. 3. AniX Inference. Finally, AniX encodes the text instruction, character, scene, character anchor, and the previously generated clip (optional, for auto-regressive mode) into tokens using the encoders illustrated in Figure 2. These tokens are then fed into the AniX model to generate the final output. Note that Steps 2 and 3 can be performed iteratively, enabling temporally consistent, long-horizon generation. Acceleration. To accelerate inference, we adopt DMD2 [91] to distill the original 30-step denoising model into more efficient 4-step version. 4. Experiment Training Details. Our network is initialized from HunyuanCustom [31], which contains 13B parameters. We freeze the LLaVA encoder and the scene condition projector, and apply LoRA-style [30] fine-tuning to the backbone with rank of 64. Two separate models are trained for 360P and 720P data using the AdamW optimizer with learning rate of 1e-4, each for 5,000 steps under the ZeRO-2 strategy. The 360P model is trained on 8 NVIDIA H100 GPUs, while the 720P model, owing to its higher resolution, is trained on 8 NVIDIA B200 GPUs. Further details on training and acceleration are provided in the appendix. Training Data. Following the data generation procedure described in Section 3.1.1, we construct training set comprising 2,084 video samples featuring five characters. Each sample is annotated with text labels describing either locomotion actions{run forward, run tho the left, run to the right, run backward}or camera behaviors{orbit, follow }. key observation of AniX is that post-training on such simple locomotion and camerabehavior data can substantially enhance pre-trained models, improving both action dynamics and camera control capability. For each sample, we generate two resolutions360P and 720Pto train models of different quality levels. Unless otherwise specified, the 360P version is used by default. Additional details are provided in the appendix. Evaluation. We evaluate our model across four aspects: (1) visual quality, (2) camera controllability, (3) action control capability and generalization to novel actions, and (4) character consistency on novel characters. For (1) and (2), we adopt the WorldScore [11] metrics to assess the generated samples. For (3), we measure the control success rate via human evaluation and report the CLIP [64] textto-image similarity score, covering both the four seen locomotion actions and up to 142 novel actions. For (4), we assess character similarity between the ground-truth character and the generated one using 30 novel characters, evaluated by DINOv2 [61] and CLIP [64] scores. By default, we use the 360P version of our model. At each iteration, our model generates 96 frames, using the previously generated 33 frames as conditions when available. Unless otherwise noted, evaluations are conducted on the first generated clip. 4.1. Main Results Visual Quality Evaluation. We adopt the metrics proposed by WorldScore [11] to evaluate the visual quality of gen6 Table 2. Action control and character consistency evaluation on general foundation models and dedicated world models. Locomotion actions include run forward, run to the left, run to the right, and run backward, while richer actions encompass 142 gesture and object-centric actions. Note that the three world models restrict action control to locomotion only. Action Control Character Consistency Method Success Rate (%) CLIP Text-Image Score Locomotion Actions Richer Actions Locomotion Actions Richer Actions DINOv2 Score CogVideoX1.5-I2V (5B) [88] HunyuanVideo-I2V (13B) [39] Wan2.1-I2V (14B) [78] Wan2.2-I2V (14B) [78] Wan-VACE (14B) [35] Wan-VACE (1.3B) [35] HunyuanCustom (13B) [31] DeepVerse [7] Hunyuan-GameCraft [45] Matrix-Game-2.0 [29] AniX (Ours) 23.3 26.7 26.7 53.3 43.3 26.7 56.7 6.7 16.7 3.3 100. 21.1 35.2 64.8 74.6 73.2 13.4 51.4 - - - 80.7 0.261 0.272 0.267 0.272 0.270 0.261 0.273 0.259 0.255 0.255 0.279 0.273 0.293 0.302 0.303 0.303 0.269 0.297 - - - 0. 0.594 0.645 0.627 0.650 0.398 0.504 0.558 0.291 0.329 0.339 0.698 CLIP Score 0.611 0.709 0.678 0.704 0.541 0.548 0.665 0.523 0.529 0.524 0.721 Figure 5. Screenshot visualizations of videos generated by AniX, showcasing different characters performing various actions across two scenes. Additional examples are provided in the appendix. main entity, as well as with general video generation foundation models listed in Table 1. For each model, we use the first two metric groups to evaluate 60 generated videos covering 30 different characters and two camera behaviors, {orbit,follow}. The third metric group is evaluated on 146 videos, where each video features the same character performing distinct actioneither one of four locomotion actions {run forward, run to the right, run to the left, run backward} or one of 142 novel actions unseen during training. Note that (1) both foundation and dedicated models are evaluated using their original control mechanisms for character and camera, without any modification; (2) the three dedicated world models only support locomotion actions, thus they are evaluated solely on those; and (3) for models requiring an initial image input, we use Google Gemini [19] to render the corresponding character within the scene as the initialization image. The results are shown in Table 1. Action Control and Generalization. In Table 2, we evaluate the models action control capability on both seen actions (four locomotion actions) and 142 novel actions (referred to as richer actions). The novel actions cover both Figure 6. Using both visual conditionsthe 3DGS scene and multi-view charactersignificantly improves long-horizon interactive video generation across diverse video clips. erated videos. WorldScore defines suite of metrics categorized into three groups: Controllability, Quality, and Dynamics. The first two primarily assess visual fidelity in static regions, while the last evaluates motion quality in dynamic regions (i.e., the character in our case). We compare our model with dedicated world modelsincluding DeepVerse [7], Hunyuan-GameCraft [45], and Matrix-Game-2.0 [29]that focus on controlling the Table 3. Using multi-view character inputs improves character consistency. By default, we employ all four views. Character View DINOv2 Score CLIP Score Front +Back +Right and Left 0.556 0.613 0.698 0.628 0.678 0.721 Table 4. Using per-frame character anchors helps the model distinguish dynamic entities from the static scene, leading to higher DINOv2 and CLIP character consistency scores. Character Anchor Type DINOv2 Score CLIP Score w/o Anchor w/ First-Frame Anchor w/ Per-Frame Anchor 0.477 0.597 0.698 0.529 0. 0.721 gesture-based behaviors (e.g., wave hands) and objectcentric interactions (e.g., play guitar). For each model, we conduct 30 evaluations on locomotion actions and 142 evaluations on distinct novel actions using the same set of characters. We report the action control success rate via human evaluation and the CLIP text-image similarity score, computed as the average frame-wise text-image score. The results in Table 2 reveal that our model outperforms the base model, HunyuanCustom [31]. This phenomenon can be interpreted through the lens of post-training in large language models [23, 62, 65, 83], where fine-tuning typically does not disrupt the pre-trained representation space; rather, it adjusts the response stylefor example, to make the outputs more helpful or harmlesswhile preserving the extensive knowledge acquired during pre-training. In our case, the structurally simple fine-tuning datacomposed primarily of fundamental locomotion behaviorsserve to refine motion dynamics and align human embodiment representations, rather than to redefine the models generative space. Consequently, our fine-tuning strategy enhances motion fidelity and behavioral coherence while maintaining the broad semantic and generative generalization inherited from large-scale pre-training. Character Consistency Evaluation. key feature of our model is its ability to maintain consistent visual identity between the provided character and the one appearing in the generated videos. In Table 2, we evaluate character consistency using DINOv2 and CLIP scores. Both metrics measure the similarity between the generated and ground-truth characters. During evaluation, we crop the character region from each generated frame and compute the similarity score; the final result is averaged across all frames. For our method, since multi-view character inputs are provided, each generated frame is compared against multiple groundtruth views, and the maximum similarity score is taken as the frames score. Each model is evaluated 30 times, each using different character performing locomotion actions. Visualization. Figure 5 presents screenshots from the generated videos. More examples are provided in the appendix. 4.2. Ablation Study Multi-View Character Condition. We compare our fourview character-conditioned model with two baselines: single-view model and frontback-view model. To evaluate character consistency, we generate videos by instructing characters to run toward the front, left, right, and back, which naturally reveals their appearance from multiple viewpoints. We then compute DINOv2 and CLIP scores following the protocol in Section 4.1. We use the same dataset as that used in Table 2. As shown in Table 3, character consistency improves as more view inputs are used. Character Anchor Condition. As illustrated in Figure 2, we introduce an additional conditionthe character maskto help the model distinguish between dynamic entities and the static scene. In our default configuration, mask is extracted for each frame during training. During inference, as shown in Figure 4, users only need to specify single character anchor (i.e., bounding-box mask), which is shared across all generated frames. Table 4 compares our default model (with per-frame masks) against two variants: (1) without any anchor mask during both training and inference, and (2) using only the first-frame anchor mask throughout training and inference. Visual Conditions Enhance Long-Horizon Generation. We introduce two types of visual conditionsmulti-view character and 3DGS sceneto ensure both character and scene consistency during generation. Beyond improving spatial coherence, these visual conditions also alleviate the issue of visual quality degradation over long-horizon generation. To validate this, we compare AniX operating in the auto-regressive mode (see Section 3.1.3) against two variants: (1) using only the multi-view character condition, where the 3DGS scene condition is replaced with textual scene descriptions; and (2) using only the 3DGS scene condition, where the multi-view character condition is replaced with textual character descriptions. Figure 6 presents comparison with the two variants, using the CLIP-Aesthetic and DINOv2 scores as metrics to evaluate visual quality and character consistency, respectively. We conduct 10 evaluations for each model, using different character in each run. Acceleration. Our 13B-parameter base model generates 93-frame 360P video in 121s on single NVIDIA H100 using 30-step denoising schedule. By applying DMD2 [91], we distill it into 4-step version, reducing latency to 21s with only slight drops in DINOv2 (0.6980.669) and CLIP-Aesthetic (5.6655.583) scores. Latencies for higher resolutions are provided in the appendix. 8 5. Conclusion We present AniX, novel framework that allows users to provide character and 3DGS scene, enabling iterative interaction for both character control and world exploration. Unlike prior controllable-entity models that restrict the agent to small set of predefined actions, AniX supports open-ended control over wide range of behaviors through natural commands. AniX delivers substantial improvements in motion dynamics and character consistency over its base model, as validated across broad set of metrics including visual quality, action controllability, character fidelity, and long-horizon generation capability."
        },
        {
            "title": "Supplementary Material",
            "content": "6. More Implementation Details Text Token Extraction. Following HunyuanCustom [31], we use the multi-modal encoder LLaVA [53] to extract text tokens while incorporating the multi-view character images. Concretely, given the text instruction and the character views = {C , CL, CR, CB}, we construct the following prompt: Table 5. Hybrid training with game and real-world data helps the model disentangle game-engine rendering from real-world visual characteristics, yielding higher DINOv2 and CLIP character consistency scores. Training Data Type DINOv2 Score CLIP Score"
        },
        {
            "title": "Game Data",
            "content": "+Real-World Data 0.686 0.755 0.718 0.729 character is [Action]. <SEP>The character front view looks like E(C ). <SEP>The character left view looks like E(C L). <SEP>The character back view looks like E(C B). <SEP>The character right view looks like E(C R). Here, [Action] denotes the action description, E() is the LLaVA image encoder, and <SEP>is the separation token used to distinguish text and visual modalities. More Training Details. The model is initialized from the pre-trained weights of HunyuanCustom [31], subjectconditioned variant of HunyuanVideo [39]. Core componentsincluding the LLaVA encoder, the scene-condition projector, and the MMDiT [12, 63] backboneare kept frozen. Trainable parameters are introduced by injecting LoRA modules [30] (rank 64) into the attention query, key, value, and projection matrices, as well as into the fully connected layers. We optimize the model using AdamW [56] with learning rate of 1e-4 and 500 warm-up steps. The scene condition is randomly dropped with probability of 0.3, encouraging the model to rely more heavily on text and multi-view character references. Our model supports two generation modes: (1) First-clip generation. The model generates 93 frames, corresponding to 24 video latents (because the VAE encoder temporally downsamples video of frames into (N 1)/4 +1 video latents). In this setting, no preceding clip is provided. (2) Auto-regressive clip generation. When previous clips already exist, the model conditions on the last 33 frames (9 video latents) of the preceding clip and generates 96 new frames (24 video latents). Inference Acceleration. To accelerate inference, we adopt the DMD2 distillation framework [91], employing teacher, student, and fake-score models initialized from our trained model. The teacher model remains fully frozen, while the student and fake-score models are fine-tuned using LoRA modules with rank 64. Following the DMD2 protocol, the fake-score model is updated at every iteration, whereas the student model is updated once every five iterations. This setupinstantiating three 13B models while training two sets of LoRA parametersincurs substantial GPU memory overhead. As result, DMD2 distillation is applied only to the 360P model and is trained for 4,000 steps on 8 NVIDIA B200 GPUs, using the ZeRO-3 optimization strategy to manage memory efficiently. 7. More Experiments Game-Real Hybrid Data Enhances Real-World Character Fidelity. Training solely on GTA-V [17] videos introthe model tends to inherit the gameduces challenge: engine rendering style, causing synthesized characters to appear stylized even when conditioned on real-world multiview characters at inference time. To improve the realism of generated real-world characterswhile retaining the diverse and dynamic motion patterns learned from game datawe curate an additional real-world dataset and jointly train AniX on both sources. Specifically, we record 400 video clips of real individuals performing the same set of locomotion actions as in the game dataset. These videos are processed using the same pipeline described in the main paper and standardized to 360P resolution. The model is then trained on hybrid dataset combining the aforementioned GTA-V and newly collected real-world videos. To help the model differentiate between rendered and real-world visual styles, we apply simple data-tagging strategy [101]: GTA-V samples are tagged with the keyword rendered (e.g., The rendered character is running forward), while real-world samples are tagged with real (e.g., The real character is running forward). Aside from this tagging mechanism, the overall training procedure remains identical to that described in the main paper. To evaluate the effectiveness of the mixed-data strategy, we collect multi-view captures of two unseen real-world individuals. Following the evaluation protocol in the main paper, we compute DINOv2 [61] and CLIP [64] scores to evaluate the character consistency. As shown in Table 5, training on hybrid data produces measurable improvements in real-world character fidelity. The qualitative results in Figure 7 further illustrate that the hybrid-trained model achieves noticeably higher realism, capturing fine-grained detailssuch as dynamic clothing wrinklesthat are ab10 (a) Figure 7. Evaluation of the hybrid data training strategy. Training solely on game data causes the model to inherit gameengine rendering style in the synthesized characters. (b) Incorporating real-world data improves photorealism, enabling the model to capture high-frequency detailssuch as dynamic clothing wrinklesthat are absent from the GTA-V dataset. sent in models trained solely on GTA-V data. Inference Acceleration. To qualitatively assess the effectiveness of our inference acceleration strategy, Figure 8 compares three variants: (1) the original model with 30step denoising schedule (no acceleration), (2) the accelerated model with 4-step denoising schedule, and (3) the original model also restricted to 4 denoising steps (no acceleration but fewer steps). Latencies for Higher-Resolution Inference. We further report the inference cost and performance for generating higher-resolution outputs using 8 NVIDIA H100 GPUs. When producing 93-frame video clip at 720P, the 720P model outperforms the 360P model in both DINOv2 [61] (0.698 0.704) and CLIP-Aesthetic [69] (5.665 5.887) metrics, with an observed latency of 159 seconds. 8. More Visualizations Figure 8. Qualitative comparison of three models: (a) the original model with 30-step denoising schedule (no acceleration), (b) the accelerated model with 4-step schedule, and (c) the original model restricted to 4 steps (no acceleration but fewer steps). The results show that our 4-step model matches the visual quality of the original model while achieving 7.5 inference speedup. lected actions using the same character. Figure 10 further illustrates the models generalization by showing 25 randomly selected actionswith text annotationsperformed by different character. Action Control and Generalization. In the main paper, we quantitatively report the success rate of controlling character to perform 142 novel actions. In Figure 9, we provide qualitative results by visualizing 84 randomly seScene Customization. Our model supports flexible scene customization. Using state-of-the-art 3DGS scene generators, users can create diverse environments and control any character to explore these worlds. In this work, most 3DGS 11 scenes are sourced from World Labs Marble [43]. Figure 11 shows examples of characters navigating variety of generated worlds. Character Customization. Our model demonstrates strong generalization in controlling previously unseen characters. Leveraging mature 3D character-generation toolssuch as Hunyuan3D [76], Tripo [68], Meshy [59], and Rodin [75]or sourcing assets from online repositories like Sketchfab [16], diverse 3D characters can be easily acquired and used directly for inference. Figures 12 and 13 illustrate examples of these characters performing locomotion actions. Long-Horizon Generation. Our model supports autoregressive generation, enabling the creation of temporally coherent video sequences that build upon previously generated clips. This capability allows for extended, longhorizon usermodel interactions. Figures 14 and 15 present two examples of long-horizon generation."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 3 [2] Maciej Bala, Yin Cui, Yifan Ding, Yunhao Ge, Zekun Hao, Jon Hasselgren, Jacob Huffman, Jingyi Jin, JP Lewis, Zhaoshuo Li, et al. Edify 3d: Scalable high-quality 3d asset generation. arXiv preprint arXiv:2411.07135, 2024. 3 [3] Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, EmiarXiv preprint lien Garreau, et al. Meta 3d gen. arXiv:2407.02599, 2024. 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [5] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 3 [6] Jinshu Chen, Xinghui Li, Xu Bai, Tianxiang Ma, Pengze Zhang, Zhuowei Chen, Gen Li, Lijie Liu, Songtao Zhao, Bingchuan Li, et al. Omniinsert: Mask-free video insertion of any reference via diffusion transformer models. arXiv preprint arXiv:2509.17627, 2025. 3 [7] Junyi Chen, Haoyi Zhu, Xianglong He, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Zhoujie Fu, Jiangmiao Pang, et al. Deepverse: 4d autoregressive video generation as world model. arXiv preprint arXiv:2506.01103, 2025. 2, 3, 6, 7 [8] Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. Flexworld: Progressively expanding 3d scenes for flexiable-view synthesis. arXiv preprint arXiv:2503.13265, 2025. [9] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60996110, 2025. 3 [10] Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. 3 [11] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. 2, 6 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3, 10 [13] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. 3 [14] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. 2, 3 [15] Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou, and Qian He. I2vcontrol-camera: Precise video camera control with adjustable motion strength. arXiv preprint arXiv:2411.06525, 2024. [16] Epic Games. Sketchfab. https://sketchfab.com/ feed, 2025. 12 [17] Rockstar Games. Grand theft auto v. https://www. rockstargames.com/gta-v, 2025. 3, 10 [18] Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, Soonwoo Kwon, and Changick Kim. Splatflow: Multi-view rectified flow model for 3d gaussian splatting synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2152421536, 2025. 3 [19] Google. Gemini. https://gemini.google.com/ app, 2025. 7 [20] Google. Genie 3: new frontier for world models. https://deepmind.google/blog/genie-3-anew-frontier-for-world-models/, 2025. 3 [21] Google. Veo. models/veo/, 2025. 2, 3 https : / / deepmind . google / [22] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion In Proceedings of for versatile video generation control. 12 the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 3 [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 8 [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [25] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2 [26] Junlin Hao, Peiheng Wang, Haoyang Wang, Xinggong Zhang, and Zongming Guo. Gaussvideodreamer: 3d scene generation with video diffusion and inconsistency-aware gaussian splatting. arXiv preprint arXiv:2504.10001, 2025. [27] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2, 3 [28] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 2, 3 [29] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. 2, 3, 6, 7 [30] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6, 10 [31] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 2, 3, 4, 6, 7, 8, 10 [32] Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, and Li Jiang. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. arXiv preprint arXiv:2510.03198, 2025. 3 [33] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, and Chunchao Guo. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. [34] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image In Proceedings of the IEEE/CVF Conference prompts. on Computer Vision and Pattern Recognition, pages 6689 6700, 2024. 3 [35] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creIn Proceedings of the IEEE/CVF Ination and editing. ternational Conference on Computer Vision, pages 17191 17202, 2025. 6, 7 [36] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. 3 [37] Anssi Kanervisto, Dave Bignell, Linda Yilin Wen, Martin Grayson, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Tabish Rashid, Tim Pearce, Yuhan Cao, et al. World and human action models towards gameplay ideation. Nature, 638(8051):656663, 2025. 3 [38] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. 2 [39] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3, 6, 7, [40] Kuaishou. Kling ai. global/, 2025. 2 https : / / klingai . com / [41] Dynamics Lab. Magica 2: The next leap in generative world engines. https://blog.dynamicslab.ai/, 2025. 3 [42] World Labs. Rtfm: real-time frame model. https: //rtfm.worldlabs.ai/, 2025. 3 [43] World Labs. Marble. https://marble.worldlabs. ai/, 2025. 2, 3, 5, 12 [44] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 5 [45] Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, and Qinglin Lu. Hunyuan-gamecraft: High-dynamic interactive game arXiv video generation with hybrid history condition. preprint arXiv:2506.17201, 2025. 2, 3, 6, 7 [46] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025. 3 [47] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. Advances in neural information processing systems, 37:7512575151, 2024. 3 [48] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. 3 [49] Zhaoyang Li, Dongjun Qian, Kai Su, Qishuai Diao, Xiangyang Xia, Chang Liu, Wenfei Yang, Tianzhu Zhang, and Zehuan Yuan. Bindweave: Subject-consistent video arXiv preprint generation via cross-modal integration. arXiv:2510.00438, 2025. 3 [50] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 798810, 2025. 3 [51] Sen Liang, Zhentao Yu, Zhengguang Zhou, Teng Hu, Hongmei Wang, Yi Chen, Qin Lin, Yuan Zhou, Xin Li, Qinglin Lu, et al. Omniv2v: Versatile video generation and editing via dynamic content manipulation. arXiv preprint arXiv:2506.01801, 2025. 3 [52] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 4, 10 [54] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via crossmodal alignment. arXiv preprint arXiv:2502.11079, 2025. [55] Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, and Chunchao Guo. Worldmirror: Universal 3d world reconstruction with anyprior prompting. arXiv preprint arXiv:2510.10726, 2025. 2, 3 [56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 10 [57] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling reference-based arXiv preprint camera control for video generation. arXiv:2506.03140, 2025. 3 [58] Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, and Kaipeng Zhang. Yume: An interactive world generation model. arXiv preprint arXiv:2507.17744, 2025. 3 [59] Meshy. Meshy ai. https://www.meshy.ai/, 2025. 12 [60] OpenAI. Sora. https://sora.chatgpt.com/ explore, 2025. 2 [61] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 6, 10, 11 [62] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 8 [63] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the IEEE/CVF els with transformers. international conference on computer vision, pages 4195 4205, 2023. 3, 10 [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PmLR, 2021. 6, 10 [65] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [66] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3 [67] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 3 [68] VAST AI Research. Tripo. https://www.tripo3d. ai/, 2025. 12 [69] Christoph Schuhmann. Clip+mlp aesthetic score predictor. https://github.com/christophschuhmann/ improved-aesthetic-predictor, 2025. 11 [70] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 3 [71] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34: 1931319325, 2021. 2, 3 [72] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [73] Wenqiang Sun, Fangyun Wei, Jinjing Zhao, Xi Chen, Zilong Chen, Hongyang Zhang, Jun Zhang, and Yan Lu. arXiv preprint From virtual games to real-world play. arXiv:2506.18901, 2025. 2, 3 [74] Stanislaw Szymanowicz, Jason Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan Barron, and Philipp Henzler. Bolt3d: Generating 3d scenes in seconds. In Proceedings of 14 the IEEE/CVF International Conference on Computer Vision, pages 2484624857, 2025. 3 [75] Deemos Technologies. Rodin. https://hyper3d. ai/, 2025. [76] Tencent. Tencent hunyuan3d. https://3d.hunyuan. tencent.com/, 2025. 12 [77] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. 3 [78] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 7 [79] Hanyang Wang, Fangfu Liu, Jiawei Chi, and Yueqi Duan. Videoscene: Distilling video diffusion model to generate 3d scenes in one step. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1647516485. IEEE, 2025. 3 [80] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for In ACM SIGGRAPH 2024 Conference video generation. Papers, pages 111, 2024. [81] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world arXiv preprint models with long-term spatial memory. arXiv:2506.05284, 2025. 3 [82] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: LongarXiv term consistent world simulation with memory. preprint arXiv:2504.12369, 2025. 3 [83] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 8 [84] Mingyu Yang, Junyou Li, Zhongbin Fang, Sheng Chen, Yangbin Yu, Qiang Fu, Wei Yang, and Deheng Ye. Playable game generation. arXiv preprint arXiv:2412.00887, 2024. 3 [85] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn ACM directed camera movement and object motion. SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [86] Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, et al. Omnicam: Unified multimodal video generation via camera control. arXiv preprint arXiv:2504.02312, 2025. [87] Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, and Yiyi Liao. Prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 28572869, 2025. 3 [88] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 6, 7 [89] Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, et al. Matrix-3d: Omnidirectional explorable 3d world generation. arXiv preprint arXiv:2508.08086, 2025. 2, 3 [90] Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, et al. Yan: Foundational interactive video generation. arXiv preprint arXiv:2508.08601, 2025. 2, 3 [91] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 6, 8, 10 [92] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 3 [93] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. [94] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. 3 [95] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 3 [96] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [97] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [98] Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, and Yueqi Duan. Scene splatter: Momentum 3d scene generation from single image with video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60896098, 2025. 3 [99] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Fei Kang, Biao Jiang, Zedong Gao, Eric Li, Yang Liu, et al. Matrix-game: Interactive world foundation model. arXiv preprint arXiv:2506.18701, 2025. 3 [100] Zhaoyang Zhang, Yannick Hold-Geoffroy, Milos Hasan, Ziwen Chen, Fujun Luan, Julie Dorsey, and Yiwei Hu. Generating 360 video is what you need for 3d scene, 2025. 3 [101] Qi Zhao, Xingyu Ni, Ziyu Wang, Feng Cheng, Ziyan Yang, Lu Jiang, and Bohan Wang. Synthetic video enhances physical fidelity in video synthesis. arXiv preprint arXiv:2503.20822, 2025. 10 [102] Jensen Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. 3 Figure 9. Visualization of character performing 84 randomly selected novel actions. 17 Figure 10. Visualization of character performing 25 randomly selected novel actions with text annotations. Figure 11. Visualization of character exploring various 3DGS worlds. 18 Figure 12. Visualization of diverse characters performing locomotion actions (Part 1). Figure 13. Visualization of diverse characters performing locomotion actions (Part 2). 20 Figure 14. Visualization of long-horizon generation (Example 1). 21 Figure 15. Visualization of long-horizon generation (Example 2)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft Research",
        "The University of Hong Kong",
        "University of Waterloo"
    ]
}