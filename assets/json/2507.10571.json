{
    "paper_title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning",
    "authors": [
        "Konstantinos I. Roumeliotis",
        "Ranjan Sapkota",
        "Manoj Karkee",
        "Nikolaos D. Tselikas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, a pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce a novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with a non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate a 77.94\\% accuracy improvement in the zero-shot setting using trust-aware orchestration and RAG, achieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, image-RAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from meta-reasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 7 5 0 1 . 7 0 5 2 : r Orchestrator-Agent Trust: Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning Konstantinos I. Roumeliotisa,*, Ranjan Sapkotab,*, Manoj Karkeeb, Nikolaos D. Tselikasa aUniversity of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece. bCornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA. *Equal contribution. Corresponding author. Abstract Modern Artificial Intelligence (AI) increasingly relies on multi-agent architectures that blend visual and language understanding. Yet, pressing challenge remains: How can we trust these agents especially in zero-shot settings with no fine-tuning? We introduce novel modular Agentic AI visual classification framework that integrates generalist multimodal agents with non-visual reasoning orchestrator and Retrieval-Augmented Generation (RAG) module. Applied to apple leaf disease diagnosis, we benchmark three configurations: (I) zero-shot with confidence-based orchestration, (II) fine-tuned agents with improved performance, and (III) trust-calibrated orchestration enhanced by CLIP-based image retrieval and re-evaluation loops. Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator modulates trust across agents. Our results demonstrate 77.94% accuracy improvement in the zeroshot setting using trust-aware orchestration and RAG, achieving 85.63% overall. GPT-4o showed better calibration, while Qwen-2.5-VL displayed overconfidence. Furthermore, imageRAG grounded predictions with visually similar cases, enabling correction of agent overconfidence via iterative re-evaluation. The proposed system separates perception (vision agents) from metareasoning (orchestrator), enabling scalable and interpretable multi-agent AI. This blueprint is extensible to diagnostics, biology, and other trust-critical domains. All models, prompts, results, and system components including the complete software source code are openly released to support reproducibility, transparency, and community benchmarking at Github: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust Keywords: agentic ai, orchestrator agent trust, trust orchestration, visual classification, retrieval augmented reasoning"
        },
        {
            "title": "1 Introduction",
            "content": "The integration of vision and language models into autonomous decision-making systems has redefined the boundaries of artificial intelligence (AI), especially in fields that demand both perceptual accuracy and interpretability [1, 2]. Multimodal large language models (LMMs), capable of reasoning over both visual and textual data, are increasingly employed in diverse domains, ranging from autonomous robotics and medical imaging to scientific diagnostics and agricultural monitoring [35]. However, as these models are deployed in high-stakes environments, fundamental challenge persists: can these systems be trusted to make reliable, transparent, and justifiable decisions particularly in zero-shot or open-world scenarios where no prior task-specific fine-tuning is possible? 1 Recent developments in agentic AI systems where autonomous agents collaborate, reason, and interact with their environment have highlighted the importance of meta-reasoning and modularity in AI architectures [68]. Rather than relying on monolithic end-to-end networks, agentic systems distribute cognitive tasks across specialized agents [9]. In vision-language applications, this paradigm enables distinct agents to independently process visual inputs, generate explanations, and assess confidence, while an external orchestrator or supervisor performs higher-order reasoning to synthesize their outputs [1012]. This architectural shift not only mirrors aspects of human collaborative problem-solving but also introduces an additional layer of oversight, which is critical for ensuring accountability and trust [13]. Despite the promise of this approach, trust calibration within agentic AI remains an underexplored area. Conventional ensemble systems often assume that agents self-reported confidence scores are reliable proxies for correctness [1417], an assumption that breaks down in zero-shot generalist models such as GPT-4o or Qwen-2.5-VL. These models may exhibit systematic overconfidence or fail to discriminate between subtle categories in domain-specific tasks [18]. Consequently, there is pressing need to assess and correct misalignments between an agents confidence and its actual performance. This problem is particularly acute in scientific and diagnostic domains, where misclassification may lead to erroneous conclusions or interventions. In visual classification tasks such as plant disease detection, trustworthiness is not merely matter of predictive accuracy but of interpretability and justification [1921]. For instance, two models may arrive at the same prediction but differ significantly in the rationale behind their decision [22, 23]. reliable AI system should not only be accurate but also capable of articulating why decision was made and when to defer to alternative evidence [24]. RAG approaches where models consult external databases or prior examples to refine their outputs offer promising pathway to ground predictions in visual context, enhancing both trust and transparency [2527]. Deploying generalist VLMs in high-stakes, domain-specific scenarios presents critical challenges in calibration, interpretability, and decision trustworthiness. In agricultural automation and digital diagnostics, for instance, misclassifications can lead to economic losses and delayed interventions. Traditional deep learning pipelines offer high accuracy but lack modular reasoning and trust introspection. To address these limitations, we propose novel Agentic AI framework for visual classification that integrates trust-aware orchestration with retrieval-augmented reasoning in modular, interpretable, and zero-shot-capable architecture. Our system coordinates multiple multimodal agents GPT-4o and Qwen-2.5-VL with non-visual orchestrator that synthesizes final predictions based on reported confidence, natural language justifications, and internal trust scores. When discrepancies or lowconfidence predictions arise, the orchestrator triggers re-evaluation loop using RAG supported by CLIP-based image retrieval, allowing the system to reflect and refine its decision using similar visual precedents. We introduce three-stage pipeline (see schematic box below), incrementally augmenting the systems capabilities: Three-Stage Trust-Aware Agentic AI Framework Experiment Zero-Shot Agents + Confidence-Aware Orchestration Agent Fine-Tuning Experiment II Fine-Tuned Agents + Confidence-Aware Orchestration Add Trust Metrics + RAG Experiment III Zero-Shot Agents + Trust-Aware Orchestration + RAG + Re-Evaluation Loop GPT-4o, Qwen-2.5-VL (Zero-Shot) GPT-4o, Qwen-2.5-VL (Fine-Tuned) GPT-4o, Qwen-2.5-VL + CLIP (RAG) Generalist Multimodal Models Domain-Specialized via Supervised Fine-Tuning Trust-Calibrated + Evidence-Grounded Pipeline The overall flow diagram and key contributions of this study are illustrated in Figure 1, highlighting how our trust-aware Agentic AI framework enables scalable integration of vision-language agents, dynamic trust calibration, and retrieval-augmented reasoning. Each experimental stage builds on the previous to increase reasoning fidelity and trust interpretability: Experiment I: Agents operate in zero-shot mode; the orchestrator selects final predictions using reported confidence scores. Experiment II: Agents are fine-tuned on curated apple leaf disease dataset using hyperparameter optimization (Bayesian search) to evaluate the effects of supervised domain adaptation. 2 Fig. 1 Overview of our trust-aware Agentic AI framework for visual classification, illustrating modular agents, orchestration stages, trust calibration, and retrieval-augmented re-evaluation for accurate, interpretable decisions. This workflow is designed for plant leaf disease classification but is generalizable to any RGB image classification task involving multimodal agents and trust-aware decision pipelines. Experiment III: Image-RAG, trust-aware orchestration, and agent re-evaluation are integrated into unified Agentic AI system. trust evaluation layer quantifies agent reliability using metrics such as Expected Calibration Error (ECE), Overconfidence Ratio (OCR), and Consistency Gap (CG), enabling the orchestrator to make informed arbitration decisions. The same general-purpose multimodal agents introduced in Experiment independently classify images with natural language rationales and confidence scores; when trust metrics signal unreliability, the orchestrator initiates re-evaluation loop that supplies agents with prior context and Image-RAG retrieval results. Agents then either revise or reaffirm their predictions, and the orchestrator makes final decision based on these updated responses. We validate this framework on biologically grounded task: the classification of apple leaf diseases, where fine-grained distinctions (e.g., between rust and scab) and explanation interpretability are critical. Our results show relative improvement of 77.94% in classification accuracy from 48.13% in the baseline to 85.63% in the trust-aware configuration even without additional fine-tuning in Experiment III. This demonstrates the power of structured trust arbitration and retrieval-enhanced reasoning for improving performance and interpretability in real-world, open-domain applications. In summary, our contributions are fourfold: (1) modular agentic AI system that decouples perception, reasoning, and retrieval; (2) novel trust-aware orchestration strategy grounded in multidimensional calibration; (3) CLIP-RAG-based re-evaluation loop for uncertainty mitigation; and (4) comprehensive empirical validation across three reasoning regimes with reproducible and scalable design."
        },
        {
            "title": "Orchestration",
            "content": "In the first experimental configuration, we evaluated the baseline performance of modular Agentic AI system in zero-shot classification setting. Two general-purpose multimodal agents, GPT-4o and Qwen-2.5-VL, were deployed without any task-specific fine-tuning. Each agent received an image 3 prompt and independently produced classification label, natural language rationale, and normalized confidence score. These outputs were then passed to non-visual reasoning orchestrator (o3-mini-2025-01-31), which performed structured comparative reasoning to synthesize final classification decision. The orchestrator operated under confidence-aware strategy, weighting agent responses based on reported confidence and justification coherence, without any external retrieval or trust calibration mechanisms. This configuration establishes foundational benchmark to assess model reliability, ensemble effectiveness, and overconfidence dynamics in open-domain zero-shot inference. The performance metrics of the agents and orchestrator are summarized in Table 1. GPT-4o achieved the highest zero-shot accuracy at 56.88%, followed by the orchestrator at 48.13% and Qwen at 45.00%. Confidence score distributions (Fig. 2a) revealed significant disparities in self-reported certainty, with Qwen averaging 94.3% confidence versus GPT-4os 87.4% (Table A1). However, this confidence was not reliably predictive of correctness; overconfident misclassifications were common, particularly for Qwen, contributing to higher OCR. The orchestrators comparative logic led to marginal calibration improvement, though the resulting accuracy remained below 50%. Figure 2b reports top-1 classification accuracy for each agent, where GPT-4o clearly outperforms the others. Weighted precision scores are illustrated in Fig.2c, and raw confidence distributions are presented in Fig.2d. Average confidence levels per model are plotted in Fig.2e, with Qwen showing systematic overconfidence. Recall and F1 scores are detailed in Figs.2f and 2g, respectively, showing GPT-4o leading across metrics. Confusion matrices in Figs. 2h, 2i, and 2j display inter-class prediction patterns for Qwen, GPT, and the orchestrator, respectively. Misclassification frequently occurred between visually similar disease classes (e.g., black-rot and rust), highlighting the inherent difficulty of this task for zero-shot models. These findings reveal that while zero-shot multimodal agents can produce fluent and confident predictions, their self-reported confidence often fails to align with empirical performance. Qwens overconfidence produced high OCR values and unstable outcomes, while GPT-4o demonstrated slightly better alignment between confidence and correctness. The orchestrator, despite lacking visual input, added interpretability and decision consistency through structured reasoning. However, the marginal performance gains and misalignment of confidence and accuracy underscore the need for deeper trust calibration. These limitations served as motivation for the next experimental stages introducing supervised fine-tuning (Experiment II) and full trust-aware orchestration with re-evaluation mechanisms (Experiment III)."
        },
        {
            "title": "2.2 Fine-Tuned Agentic Models with Confidence-Aware Orchestration",
            "content": "To evaluate whether supervised adaptation improves model reliability and ensemble synergy, we conducted second experiment in which both GPT-4o and Qwen-2.5-VL were fine-tuned on the apple disease classification dataset. The overall setup mirrored Experiment 2.1, retaining identical multimodal inputs, test prompts, and classification objectives. However, the zero-shot agents were replaced with domain-adapted variants, each fine-tuned on labeled training set of 512 samples and validated on 128 additional samples. Importantly, the orchestrator (o3-mini-2025-01-31) remained unchanged and continued to perform confidence-aware arbitration, relying solely on agent-generated confidence scores and textual justifications to make final decisions. This design tests whether fine-tuning alone, without any updates to the orchestration logic or external retrieval modules, could significantly enhance prediction accuracy, calibration, and interagent agreement. Table 1 presents the updated metrics: GPT-4o achieved an accuracy of 98.13%, while Qwen-2.5-VL reached 95.63%. The orchestrator, despite lacking direct visual input, attained 97.50% accuracy, underscoring the impact of improved agent outputs on system-wide arbitration. Weighted and macro-averaged precision, recall, and F1 scores exceeded 95% for all models, indicating substantial leap from the zero-shot performance baseline. Figure 3a shows violin plots of agent confidence distributions after fine-tuning. Compared to the zero-shot setting, these distributions are narrower and more centered, suggesting improved calibration. Figure 3b reports top-1 accuracy, while Fig.3c highlights gains in weighted precision across agents. Notably, the confidence histograms in Fig.3d exhibit reduced overconfident noise, and the average confidence values in Fig. 3e show modest increase (e.g., GPT-4o rose from 87.4% to 92.58%), indicating better alignment between certainty and correctness (Table A3). 4 Fig. 2 Experiment Zero-Shot Performance. (a) Violin plot of model confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator in the zero-shot configuration (Experiment I). (b) Top-1 classification accuracy comparison of Qwen, GPT, and the orchestrator in Experiment I. (c) Weighted precision scores of all agents in the zero-shot setting. (d) Histogram of raw confidence scores reported by Qwen, GPT, and the orchestrator. (e) Mean confidence scores per agent, aggregated across the entire test set. (f) Weighted recall for each agent in zeroshot inference. (g) Weighted F1-scores for Qwen, GPT, and the orchestrator, highlighting performance balance. (h) Confusion matrix for Qwen-2.5-VL predictions on the evaluation set. (i) Confusion matrix for GPT-4o predictions, revealing inter-class confusion patterns. (j) Confusion matrix for the final orchestrator decision outcomes based on trust-aware arbitration. Recall and F1 improvements are presented in Figs. 3f and 3g, respectively. The F1 score for GPT-4o increased from 50.78% to 98.12%, and for Qwen from 37.63% to 95.58%. These gains reveal that fine-tuning not only enhances correct predictions but also reduces false positives and negatives. Confusion matrices in Figs. 3h3j show sharply improved class-level discrimination. Notably, categories like black-rot and rust, which previously showed misclassification overlap, now exhibit strong diagonal dominance. The orchestrator benefited substantially from the improved agent outputs. Since its decision logic depends on comparative evaluation of confidence-aligned explanations, the increase in calibration quality directly translates to more reliable ensemble predictions. Interestingly, although the orchestrator does not receive image inputs, its final predictions align well with the dominant and better-calibrated agent in most cases. Detailed fine-tuning configurations for each agent model are presented in Section 4. The results confirm that supervised adaptation significantly enhances the classification performance of generalpurpose vision-language agents within the agentic framework. After fine-tuning on the domain-specific apple disease dataset, both GPT-4o and Qwen-2.5-VL showed notable gains across accuracy, precision, recall, and F1 scores. As shown in Table 1, GPT-4o achieved 98.13% accuracy and Qwen 95.63%, while the orchestrator despite lacking direct image access reached 97.50% accuracy. This highlights how confidence-aware arbitration benefits from well-calibrated agent responses, with the orchestrators performance reflecting the value of alignment between self-reported confidence and correctness. 5 Table 1 Performance metrics (accuracy, weighted and macro precision, recall, and F1) for each agent across three experimental configurations. All metrics for each experiment are shown in the same row."
        },
        {
            "title": "GPT",
            "content": "Zero-Shot I. + ConfidenceAware II. Few-Shot + ConfidenceAware III. Zero-Shot + Trust-Aware + RAG Accuracy: 0.4500 Precisionw: 0.4742 Recallw: 0.4500 F1-scorew: 0.3763 Precisionm: 0.4742 Recallm: 0.4500 F1-scorem: 0.3763 Accuracy: 0.9563 Precisionw: 0.9603 Recallw: 0.9563 F1-scorew: 0.9558 Precisionm: 0.9603 Recallm: 0.9563 F1-scorem: 0.9558 Accuracy: 0.7313 Precisionw: 0.7526 Recallw: 0.7313 F1-scorew: 0.7292 Precisionm: 0.7526 Recallm: 0.7313 F1-scorem: 0. Accuracy: 0.5688 Precisionw: 0.6985 Recallw: 0.5688 F1-scorew: 0.5078 Precisionm: 0.6985 Recallm: 0.5688 F1-scorem: 0.5078 Accuracy: 0.9813 Precisionw: 0.9817 Recallw: 0.9813 F1-scorew: 0.9812 Precisionm: 0.9817 Recallm: 0.9813 F1-scorem: 0.9812 Accuracy: 0.8750 Precisionw: 0.8898 Recallw: 0.8750 F1-scorew: 0.8690 Precisionm: 0.8898 Recallm: 0.8750 F1-scorem: 0."
        },
        {
            "title": "Orchestrator",
            "content": "Accuracy: 0.4813 Precisionw: 0.5183 Recallw: 0.4813 F1-scorew: 0.4062 Precisionm: 0.5183 Recallm: 0.4813 F1-scorem: 0.4062 Accuracy: 0.9750 Precisionw: 0.9765 Recallw: 0.9750 F1-scorew: 0.9747 Precisionm: 0.9765 Recallm: 0.9750 F1-scorem: 0.9747 Accuracy: 0.8563 Precisionw: 0.8591 Recallw: 0.8563 F1-scorew: 0.8555 Precisionm: 0.8591 Recallm: 0.8563 F1-scorem: 0.8555 Weighted (subscript w) and macro-averaged (subscript m) precision, recall, and F1 scores are reported. However, the experiment also revealed key limitation: when both fine-tuned agents confidently agree on an incorrect prediction, the orchestrator lacks dissenting signal and cannot intervene. This consensus failure mode exposes the risk of overreliance on internal confidence and justifications alone. To address such blind spots, the next experimental framework integrates external trust metrics and retrieval-based verification. These enhancements aim to assess not only what agents predict, but also how trustworthy and evidence-grounded those predictions are especially under ambiguity or in high-stakes settings."
        },
        {
            "title": "2.3 Trust-Aware Orchestration with RAG and Re-Evaluation Loops",
            "content": "To overcome the reliability ceiling observed in previous settings, the third experiment introduces trust-aware orchestration framework augmented with RAG pipeline. Unlike Experiment II, this configuration retains the zero-shot GPT-4o and Qwen-2.5-VL agents to investigate whether reasoning and calibration improvements can be realized without task-specific fine-tuning. Enhancements to the orchestrator include: (i) multi-metric trust scoring module incorporating ECE, OCR, and CG, and (ii) dynamic Re-Evaluation Loop, triggered when agent trust scores fall below learned threshold. In this setup, low-trust predictions trigger the orchestrator to initiate re-evaluation loop, prompting agents with their prior responses and augmenting the input with semantic retrievals from Image-RAG. The retrieval module uses CLIP (ViT-B/32) to encode curated set of classrepresentative reference images into 512-dimensional embeddings, which are L2-normalized and stored in FAISS vector database using an exact inner product index (IndexFlatIP). Each reference embedding is associated with disease category label yi healthy, black-rot, rust, scab and enriched with metadata such as image URLs and textual definitions. During re-evaluation, the agents receive the top-k most similar reference examples based on cosine similarity, integrated into an updated prompt. This guides them to revise or reaffirm their prediction with additional visual and semantic context. The orchestrator then assesses the updated responses using trust metrics (ECE, OCR, CG) and finalizes the decision based on the most coherent and trustworthy agent output. The detailed 6 Fig. 3 Experiment II Fine-Tuned Agents. (a) Violin plot of confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator following supervised fine-tuning. (b) Top-1 classification accuracy comparison after agent fine-tuning. (c) Weighted precision scores across the three agents, showing enhanced discriminative performance. (d) Histogram of raw confidence scores across all predictions, reflecting sharper calibration post finetuning. (e) Mean confidence scores per agent, showing convergence in self-reported certainty. (f) Weighted recall scores for Qwen, GPT, and the orchestrator on the test set. (g) Weighted F1-scores highlighting overall performance balance improvements. (h) Confusion matrix for Qwen-2.5-VL showing improved inter-class discrimination. (i) Confusion matrix for GPT-4o illustrating reduced misclassification frequency. (j) Confusion matrix for the orchestrators final decisions, demonstrating stability in arbitration after fine-tuning. methodology behind trust-aware orchestration, the re-evaluation loop, and Image-RAG is presented in Section 4. Performance: As summarized in Table 1, GPT-4o achieved 87.50% accuracy and Qwen reached 73.13%, both significantly outperforming their initial zero-shot baselines in Experiment I. The orchestrator attained an accuracy of 85.63%, demonstrating that trust-aware orchestration, combined with retrieval-based contextualization, can close large portion of the performance gap typically addressed via fine-tuning. Subfigure 4b shows the accuracy improvements, while subfigures 4c, 4f, and 4g illustrate the precision, recall, and F1-score gains, respectively. Calibration: As shown in Fig. 4a, the violin plot of confidence distributions revealed reduction in overconfidence for both agents. Mean confidence scores (Fig. 4e) slightly decreased compared to Experiment II, suggesting more cautious, better-calibrated outputs. The orchestrators histogram (Fig. 4d) reflects this calibrated behavior, with lower variance and reduced extremity in self-reported confidence (Table A5). Qualitative Gains: Confusion matrices shown in Figs. 4hj provide insight into class-specific performance. RAG and trust-based re-evaluation mitigated frequent misclassifications seen in Experiment especially in visually ambiguous classes such as black-rot and rust by incorporating external visual-textual anchors. Notably, GPT-4o showed strong diagonal dominance in its matrix, and the orchestrator successfully avoided the propagation of low-trust predictions even when both agents initially erred. 7 Trust profiling: From the profiling, several insights emerge regarding agent reliability under zero-shot conditions. Importantly, both models exhibit suboptimal calibration and trustworthiness metrics overall (Table A4) an expected outcome given that they were not fine-tuned on the taskspecific dataset. Nonetheless, relative differences offer valuable guidance for orchestration design: GPT demonstrates superior calibration, with lower ECE (ECE = 0.293 vs. 0.453 for Qwen) and substantially higher ConfidenceCorrectness Correlation (CCC = 0.361, < 0.0001), suggesting that its confidence estimates are more aligned with empirical correctness and thus more actionable in trust-aware orchestration. GPT exhibits greater reliability under uncertainty, evidenced by lower Overconfidence Rate (OCR = 0.416 vs. 0.508) and fewer high-confidence errors (213 vs. 260), indicating reduced risk of confidently incorrect predictions critical feature for high-stakes decision environments. Qwen displays overconfidence and poor discrimination, reporting the highest average confidence (0.945) but minimal Confidence Gap (CG = 0.009), meaning it struggles to differentially calibrate its confidence for correct versus incorrect predictions. This weakens its applicability in systems that depend on confidence signals for agent weighting or override logic. These findings enabled the orchestrator to weight GPTs predictions more heavily during ensemble decision-making, thereby improving the robustness and accuracy of the overall Agentic AI system. Disagreement Analysis and Trust Arbitration: An analysis of disagreements between agents and system components provides further insight: In 12.5% of cases, the GPT agent refused the orchestrators recommendation for re-evaluation, returning the same prediction as before. However, in only 3 out of those 20 cases did this result in correct prediction. The same pattern, but to greater extent, was observed with Qwen: the agent ignored the re-evaluation recommendation in 29.38% of cases, returning the same prediction. Yet only 5 out of 47 such instances led to correct outcome. These low correctness rates in disagreement scenarios suggest that the orchestrator, re-evaluation loop, and Image-RAG remain authoritative sources in edge cases although the effectiveness of this integration benefits from final arbitration by meta-reasoning agent. Further analysis of orchestrator-agent disagreements shows: Orchestrator vs. GPT: Disagreements occurred 36 times (22.5%), with the orchestrator being correct in 16 of those cases (44.44%). Orchestrator vs. Qwen: Disagreements occurred 22 times (13.75%), with the orchestrator being correct in 21 cases (95.45%). The orchestrator significantly outperforms Qwen in disagreement scenarios and even surpasses GPT nearly half the time, underscoring the value of trust-aware arbitration beyond simple majority voting. However, the re-evaluation process is not without drawbacks. In some instances, it led to overcorrection: GPT changed correct answers to incorrect ones 3 times (1.88%) after receiving re-evaluation prompt. Qwen did so 22 times (13.75%). While rare for GPT, Qwens frequent revision of correct answers indicates higher sensitivity to prompt influence, highlighting the need for better confidence calibration and tighter visual-textual integration. Interpretability and Scalability: While the orchestrator achieved near-fine-tuned accuracy without labeled training data, residual failure cases remain, especially when both agents confidently agree on wrong label. This scenario exposes the limits of current trust metrics to fully capture uncertainty in high-confidence false predictions. Nevertheless, the current framework provides scalable, modular, and interpretable zero-shot system that balances generalizability and reliability. Collectively, Experiment III highlights that zero-shot generalist agents, when embedded in trustcalibrated agentic systems with access to external retrieval, can achieve expert-level accuracy in image classification tasks. These findings support broader vision of scalable agentic intelligence where trust, not tuning, becomes the key to real-world deployment. 8 Fig. 4 Experiment III Trust-Aware Orchestration with RAG and Re-Evaluation Loops. (a) Violin plot depicting confidence distributions across all classes for Qwen-2.5-VL, GPT-4o, and the orchestrator in the trust-aware setup with retrieval augmentation and re-evaluation. (b) Accuracy comparison across the three agents under trustcalibrated arbitration. (c) Weighted precision scores for all models following trust-aware reasoning. (d) Histogram of raw confidence outputs from each agent after trust score filtering. (e) Mean confidence values per agent post re-evaluation, showing enhanced calibration and reduced overconfidence. (f) Weighted recall metrics across all models in the final pipeline. (g) Weighted F1-scores reflecting the balance of precision and recall under trust-informed decision-making. (h) Confusion matrix for Qwen-2.5-VL predictions after Image-RAG integration and re-evaluation. (i) Confusion matrix for GPT-4o responses within the trust-aware system. (j) Final decision confusion matrix of the orchestrator, highlighting improvements in accuracy and reduced inter-class confusion due to trust filtering and context-grounded retrieval."
        },
        {
            "title": "Comparative Performance and Ablation Analysis",
            "content": "Time and Calibration Performance Across Configurations. Figure 5 presents comparative analysis of inference time across the three experimental setups. Subfigures 5ac show that fine-tuned agents (Experiment II) achieve the lowest latency due to optimized internal representations, while trust-aware orchestration with retrieval (Experiment III) introduces modest time overhead from retrieval and re-evaluation cycles. Histograms in subfigures 5df reveal that Experiment III exhibits heavier tail in inference time distribution, yet remains within real-time thresholds. While fine-tuned models yield the highest performance, they require extensive training and lack task transferability. In contrast, Experiment III achieves near-optimal accuracy (85.6%) with only 1.3 the inference time of zero-shot baselines, offering practical compromise for label-scarce or dynamic environments. Confidence-Accuracy Calibration and Overconfidence Mitigation. Figure 6 illustrates confidenceaccuracy calibration curves for Qwen-2.5-VL, GPT-4o, and the orchestrator across all experiments. In the zero-shot setting (subfigures 6ac), both agents display considerable overconfidence, with confidence often exceeding empirical accuracy. This misalignment is partially mitigated by the orchestrator, which arbitrates between agent outputs. Fine-tuning (subfigures 6df) improves calibration, especially for GPT-4o, aligning predicted confidence more closely 9 with true correctness. Trust-aware orchestration in Experiment III (subfigures 6gi) further suppresses overconfidence through calibrated re-evaluation. Figure 7 visualizes the relationship between agent overconfidence (mean confidence on incorrect predictions) and final macro-F1, confirming that trust-augmented pipelines better align certainty with correctness. Fig. 5 Time Performance Analysis across Experimental Configurations. a) Boxplot showing inference time distribution per image in Experiment (zero-shot setting). b) Boxplot showing inference time distribution in Experiment II (fine-tuned setting). c) Boxplot showing inference time distribution in Experiment III (trust-aware orchestration with RAG). d) Histogram of inference time frequencies for Experiment I. e) Histogram of inference time frequencies for Experiment II. f) Histogram of inference time frequencies for Experiment III. These visualizations highlight how orchestration strategies and model configurations affect latency, offering insight into the computational trade-offs of agentic AI systems."
        },
        {
            "title": "3 Discussion",
            "content": "The increasing complexity and autonomy of AI systems calls for robust, interpretable, and generalizable architectures that can reason, evaluate, and adapt in real time [28]. This study presents systematic exploration of novel trust-aware agentic AI framework that blends zero-shot visionlanguage agents with orchestration, trust calibration, and RAG-based re-evaluation. Our three-tier experimental structure reveals key insights into how such systems can be structured for both performance and scalability while preserving explainability and adaptability. central insight from this study is the critical role of calibration and trust estimation in agentic AI systems. While traditional ensemble methods often aggregate agent outputs under the assumption of independent and reliable performance [29, 30], such strategies falter in real-world settings especially under zero-shot conditions where agents may exhibit systematic overconfidence or miscalibration [31, 32]. In these contexts, naÄ±vely trusting self-reported confidence scores can lead to compounding errors, particularly in scenarios that demand high reliability and interpretability. To address this, we adopt trust-aware orchestration strategy that incorporates metrics such as ECE, OCR, and CG, enabling the orchestrator to quantitatively assess the alignment between confidence and correctness, as well as the consistency of reasoning under varied prompt formulations. This shift from purely accuracy-driven aggregation to trust-calibrated decision fusion reflects broader movement in AI system design toward epistemic robustness and risk-aware reasoning. Prior research in LLM-based decision support, autonomous robotics, and human-AI collaboration has underscored the limitations of relying on uncalibrated model outputs, and has proposed various trust modeling frameworks that incorporate self-assessment, uncertainty quantification, or post hoc calibration techniques. 10 Fig. 6 Confidence vs. Accuracy Calibration Analysis across Experiments. a) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment (zero-shot setting); b) Confidence vs. accuracy plot for GPT-4o in Experiment I; c) Confidence vs. accuracy plot for the orchestrator in Experiment I; d) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment II (fine-tuned setting); e) Confidence vs. accuracy plot for GPT-4o in Experiment II; f) Confidence vs. accuracy plot for the orchestrator in Experiment II; g) Confidence vs. accuracy plot for Qwen-2.5-VL in Experiment III (trust-aware orchestration with RAG); h) Confidence vs. accuracy plot for GPT-4o in Experiment III; i) Confidence vs. accuracy plot for the orchestrator in Experiment III. These calibration curves illustrate the alignment between predicted confidence and true accuracy across agents and experimental conditions, highlighting changes in overconfidence and calibration quality. Our findings affirm that integrating trust metrics directly into orchestration logic significantly improves both accuracy and reliability. By down-weighting overconfident yet incorrect predictions and triggering re-evaluation when inconsistency or low trust is detected, the system becomes more resilient to epistemic failures. This capability is especially crucial in high-stakes domains such as medical diagnostics, autonomous driving, and scientific discovery, where misjudged confidence can lead to misinformed actions with costly or irreversible consequences. Rather than treating trust as an external interpretability add-on, our framework embeds trust evaluation as first-class component of agentic reasoning, aligning with emerging paradigms in trust-centric AI governance and human-AI alignment. Our findings also show that the method of orchestration itself profoundly affects system performance. In the baseline configuration (Experiment I), the orchestrator made decisions solely based on self-reported confidence, leading to moderate accuracy (48.13%) and significant overconfidence in incorrect predictions. In contrast, the trust-aware orchestrator (Experiment III) reached an accuracy of 85.63%, despite using zero-shot agents, highlighting the orchestration logic as performance amplifier. Moreover, when compared to Experiment II, which involved computationally expensive supervised fine-tuning of agents (achieving 97.50% accuracy), the trust-aware method captured over 11 Fig. 7 Agent overconfidence vs. final macro-F1. (a) Experiment I: zero-shot predictions overconfidence vs. macro-F1 score; (b) Experiment II: few-shot predictions overconfidence vs. macro-F1 score; (c) Experiment III: trustaware orchestration with RAG overconfidence vs. macro-F1 score, both before and after re-evaluation loop. 77% of the possible gain while avoiding the need for model retraining. This demonstrates that wellcalibrated orchestration mechanism can partially substitute for domain adaptation when retraining is not feasible. Traditional ensemble learning methods, such as majority voting [33, 34], mean averaging or confidence-weighted fusion [35], operate under the assumption that model outputs are statistically independent and equally reliable. While effective in low-noise environments or when models are homogeneously calibrated, these techniques struggle in heterogeneous, zero-shot, or high-uncertainty scenarios where agent predictions may be misaligned or systematically overconfident [36, 37]. In contrast, our trust-aware orchestrator does not merely aggregate predictions it actively evaluates each agents trustworthiness using multi-dimensional metrics and selectively down-weights or discards predictions that exhibit poor calibration, inconsistency, or unjustified confidence. This approach aligns more closely with emerging frameworks in agentic AI, where orchestration involves structured reasoning across multiple autonomous agents with varying competencies. Prior work on dynamic task decomposition [38, 39], agent delegation [40, 41], and modular reasoning [42, 43] has shown that intelligent coordination across agents can outperform flat ensembles, particularly when agents contribute distinct skills or modalities [44]. However, many of these systems rely on rule-based or deterministic coordination logic and lack mechanisms for trust-based arbitration or reflective re-evaluation [45, 46]. Our orchestrator extends this space by integrating retrieval-augmented prompts and dynamic trust profiles, enabling recursive decision correction based on evidence-grounded feedback. Compared to static ensembles, this architecture enables real-time reasoning under uncertainty, improves robustness to adversarial disagreement, and supports scalable integration of new agents without retraining. Such orchestrator logic is increasingly critical as agent ecosystems grow in complexity and move toward plug-and-play, open-world operation. An additional strength of our framework lies in the re-evaluation loop powered by RAG. This feedback mechanism enables agents to reflect on their prior decisions in light of retrieved evidence from vision-language knowledge base. By structuring the retrieved information around interpretable class definitions and visual prototypes, the system compensates for hallucination or semantic ambiguity common in zero-shot models. Our ablation analysis shows that although this loop was triggered in 100% of instances due to low trust profiling scores, disagreements between agents and system components reveal that GPT and Qwen often ignored re-evaluation prompts 12.5% and 29.38% of cases, respectively but this rarely led to correct predictions (only 3 out of 20 for GPT and 5 out of 47 for Qwen). These low success rates highlight the importance of the orchestrator, re-evaluation loop, and Image-RAG as authoritative sources, with optimal outcomes achieved through meta-reasoning arbitration. The practical implication is that retrieval-based grounding acts as an auxiliary supervision signal, enabling improvement without manual annotation or gradient updates. While RAG has shown considerable promise in enhancing large language models through external knowledge grounding especially in tasks such as medical decision support [25, 47], document retrieval and language alignment these systems typically lack structured trust arbitration mechanism [26]. Most RAG implementations retrieve top-k textual or visual exemplars to refine responses, but treat the retrieval step as static and apply equal weight to all retrieved content [4850]. In contrast, our Image-RAG pipeline integrates visual retrieval with dynamic trust scoring, enabling iterative reevaluation loops where low-confidence or conflicting predictions trigger targeted grounding process. This allows the system to not only retrieve relevant cases but also modulate decision-making based on model reliability and retrieval quality. Existing document-grounded agents focus on improving factuality but do not incorporate agent-level trust profiling [51]. Moreover, few approaches fuse visual 12 similarity with structured re-prompting guided by trust thresholds [52]. Our design enhances interpretability by showing not only what evidence was retrieved but why it was considered trustworthy. This feedback loop is essential for deploying agentic AI in settings where error introspection and evidence traceability matter, and it represents step beyond static RAG toward trust-calibrated retrieval orchestration. From system design perspective, our architecture prioritizes modularity and plug-and-play scalability, enabling seamless integration of agents with minimal friction. Each agent operates autonomously and can be added, removed, or updated without necessitating retraining of the orchestrator critical property for real-world deployment where agent capabilities may evolve over time. This decoupled architecture reflects foundational principles from modular agent systems developed in domains such as edge computing and the Internet of Things, where component isolation, interoperability, and system composability are essential for dynamic, distributed environments. As agentic AI systems scale to incorporate dozens or even hundreds of specialized agents each with different modalities, competencies, or domain knowledge per-agent fine-tuning becomes computationally impractical and operationally rigid [53, 54]. Our trust-aware orchestration framework addresses this by absorbing agent heterogeneity through dynamic reliability profiling and selective arbitration, thereby supporting generalization across agents without task-specific adaptation. This design also mirrors recent advancements in user interface navigation agents and vision-language action systems, where orchestration is driven by flexible, intent-driven coordination rather than static aggregation. By separating reasoning logic from perception modules and incorporating natural language justifications, our system remains interpretable and auditable, supporting both technical transparency and human oversight two pillars necessary for scalable and trustworthy multi-agent AI ecosystems."
        },
        {
            "title": "3.1 Summary, Limitations, and Future Perspective",
            "content": "This work presents an Agentic AI system that integrates trust-aware orchestration, vision-language grounding via Image-RAG, and structured re-evaluation loops. To evaluate the systems effectiveness, we conducted three distinct experiments, progressively increasing complexity and realism. 1. Experiment (Zero-Shot Orchestration): Two vision-language models; Qwen-2.5-VL and GPT-4o; were deployed in zero-shot setting to make classification predictions, accompanied by natural language explanations and self-reported confidence scores. These outputs were evaluated by reasoning agent (o3-mini-2025-01-31), which, based on content and trust signals, issued final decision. This baseline Agentic AI system achieved an overall accuracy of 48.13%. 2. Experiment II (Fine-Tuned Agents): Both agents were thoroughly fine-tuned using dedicated training set via hyperparameter optimization. The same inference and arbitration process was followed. With fine-tuned models, the Agentic AI system reached 97.50% accuracy, establishing an upper performance bound when domain adaptation is permitted. 3. Experiment III (Trust-Aware Agentic Framework): This experiment evaluates our proposed full framework, incorporating trust-aware orchestration and Image-RAG visual reasoning. Agents were prompted to make classification predictions on hidden-labeled training set, along with self-reported confidence scores. Their responses were processed through set of quantitative trustworthiness metrics to derive trust profiles, enabling agent-level reliability estimation independent of individual inputs. The framework also included state-of-the-art retrieval-augmented vision component using CLIP; each entry was paired with category label, with reference embeddings stored in FAISSbased vector database. At inference time, the orchestrator called upon agents to make zero-shot predictions, as in Experiment I. Their predictions, confidence scores, explanations, and trustworthiness metrics were passed to the orchestrator, which then decided whether to trust the agents predictions or prompt re-evaluation loop. Agents entering the re-evaluation loop were provided with the context of their previous response and recommendations from the Image-RAG component, allowing them to revise their classification decisions. The agents final responses were passed back to the orchestrator, which made an informed final decision. This framework, without any agent fine-tuning, achieved 85.63% accuracy. These experiments clearly demonstrate the effectiveness of trust-aware orchestration in agentic AI systems, yielding up to 77.94% improvement over confidence-based zero-shot orchestration. Scaling such systems to hundreds of AI agents would make per-agent fine-tuning prohibitively expensive, 13 both in time and computational cost, due to the need to identify optimal hyperparameters for each model. In contrast, our framework enables seamless integration of new agents without fine-tuning. Each agent contributes its unique capabilities in zero-shot setting, while the trust-aware orchestrator provides the necessary context to incorporate them effectively and reliably into the broader system."
        },
        {
            "title": "Limitations",
            "content": "Several limitations remain. First, although the system outperforms conventional zero-shot baselines, its accuracy still lags behind that of domain-specific, fine-tuned models. This performance gap highlights the trade-off between generalizability and task-specific optimization. Second, in the zero-shot setting, the design and phrasing of prompts play critical role in shaping model outputs. Despite clear and structured prompting, we observed that models; particularly Qwen2.5-VL; frequently failed to follow the expected response format. To address this, we implemented re-prompting loop with capped number of retries, which introduces additional inference overhead and increases system complexity. Third, the results reported in this study are based on specific image dataset; therefore, performance on other image-based datasets may vary. However, we expect the relative trends to hold, with fine-tuned models generally outperforming zero-shot approaches. Fourth, while we used the o3-mini model as the orchestrator for these experiments, there are several alternative models available. We specifically chose an orchestrator without visual capabilities to avoid biases introduced by its own predictions. Nonetheless, more advanced models such as o3pro, o4-mini, or other variants with visual understanding could potentially improve orchestration performance."
        },
        {
            "title": "Future Perspective",
            "content": "Advancing trust-aware agentic AI systems presents several promising directions. Incorporating orchestrators with enhanced multimodal reasoning capabilities; such as more advanced visuallanguage models; could improve the reliability and fairness of decision arbitration. Further, optimizing prompt design and exploring adaptive prompting strategies will be essential to address current limitations in zero-shot settings, reducing the need for repeated prompting and improving compliance with response formats. Scaling the framework to accommodate larger and more diverse populations of agents poses challenges related to agent management, specialization, and dynamic trust assessment. Developing methods to efficiently integrate and update agent trust profiles will be critical for maintaining system robustness. Additionally, validating and extending the trust-aware orchestration approach across varied datasets and domains beyond image classification to include video analysis, language tasks, and multimodal reasoning; will be important for demonstrating generalizability and broader impact. Ultimately, embedding self-monitoring and self-improvement mechanisms within agents may enable autonomous adaptation and increased system resilience, paving the way for more robust and scalable agentic AI architectures applicable to complex real-world problems."
        },
        {
            "title": "4 Methods",
            "content": "This study proposes and evaluates modular Agentic AI architecture for visual classification, combining two vision-language agents (GPT-4o and Qwen-2.5-VL) with non-visual orchestrator (o3mini-2025-01-31). Experiments were conducted using curated dataset of 800 RGB images labeled into four apple disease categories, split 64% for training, 16% for validation, and 20% for testing. The system comprises three experimental setups."
        },
        {
            "title": "Experiment I",
            "content": "In the first experimental configuration, we evaluate the performance of general-purpose multimodal LLMs in zero-shot setting that is, without any task-specific fine-tuning. Each agent receives prompt containing single input image (from the test set) and is tasked with predicting the most appropriate plant disease class. In addition to the predicted class label, agents are required to return (i) confidence score in the range [0.01.0] and (ii) natural language explanation justifying their decision (Appendix, Fig. A1). These three outputs classification, confidence, and 14 rationale are compiled and forwarded to non-visual orchestrator model (o3-mini-2025-01-31 [55]), which serves as comparative reasoner. The orchestrator does not process images directly; instead, it evaluates the agents predictions and justifications in light of their associated confidence scores and produces final classification decision through structured, confidence-aware reasoning (Appendix, Fig. A2)."
        },
        {
            "title": "Experiment II",
            "content": "In Experiment II, both agents were fine-tuned using supervised learning techniques to improve classification performance. For GPT-4o, fine-tuning employed hyperparameter configuration informed by prior ResNet-50 optimization studies. Qwen-2.5-VL underwent over 50 hyperparameter tuning trials, beginning with heuristic parameter estimates and refined through performance-based search strategies. 1. GPT-4o Fine-Tuning: In this phase, we explored two fine-tuning strategies: one using default hyperparameters provided by the OpenAI platform, and one informed by prior hyperparameter optimization conducted on ResNet-50 model using Bayesian optimization. (a) ResNet-50Informed Hyperparameter Transfer. To mitigate the computational cost of performing hyperparameter optimization directly on GPT-4o, we hypothesized that high-performing hyperparameters derived from ResNet-50 tuning on the same dataset could be effectively transferred to GPT-4o. Specifically, Bayesian optimization with the Tree-structured Parzen Estimator (TPE) algorithm was used to explore the ResNet-50 hyperparameter space across 30 trials. TPE iteratively models the objective function (x), evaluates the expected improvement (EI) of candidate configurations, and selects promising trials using likelihood ratio. Optimization was implemented using the Optuna library on an NVIDIA A100-SXM4-40GB GPU via Colab Enterprise. Early stopping and pruning were used to improve computational efficiency. The best configuration identified 10 training epochs and batch size of 16 was subsequently applied to GPT-4o fine-tuning through the OpenAI fine-tuning interface. Training and validation sets were formatted into jsonl files with promptcompletion pairs before submission. (b) Default Hyperparameter Configuration. In parallel, we conducted fine-tuning using the default hyperparameters recommended by the OpenAI platform: 3 epochs and batch size of 1. Identical training and validation files were used to ensure fair comparison. Fine-tuning GPT-4o using the ResNet-50informed hyperparameters required approximately 1,778 seconds (29.6 minutes) and incurred cost of USD 47.53. In comparison, fine-tuning with the default hyperparameters (3 epochs, batch size of 1) required 1,652 seconds (27.5 minutes) at reduced cost of USD 13.09. Although direct hyperparameter optimization on GPT-4o could potentially yield higher-performing configurations, the computational and financial cost of conducting such process over multiple trials renders it impractical under current constraints. detailed comparison of runtime, cost, and validation loss for both configurations is provided in the Appendix, Table A2. Based on these results, the ResNet-50informed configuration yielded substantially lower validation loss (0.0088) and was therefore selected as the preferred GPT-4o variant for integration into the Agentic AI system implemented in this study. 2. Qwen-2.5-VL-7B Fine-Tuning: To adapt the Qwen-2.5 Vision-Language 7B (VL-7B) model for our task, we fine-tuned the unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit model [56], 4-bit quantized variant of the original pretrained Qwen2.5 model. This quantized version enables efficient loading and inference with reduced memory requirements, making it suitable for commodity GPU hardware. We employed Low-Rank Adaptation (LoRA) in 16-bit precision to inject trainable adapters into the model, thereby allowing effective fine-tuning without full dequantization of the backbone weights. The LoRA adapters were integrated into all major architectural components of the model, including the vision encoder layers, the language modeling transformer layers, multi-head attention modules, and the feedforward multilayer perceptrons (MLPs). Each adapter was configured with rank = 16 and scaling factor Î± = 16, balancing parameter efficiency with expressive capacity. We used the Parameter-Efficient Fine-Tuning (PEFT) framework to update only the LoRA-injected weights, keeping the underlying 4-bit quantized parameters frozen throughout training. To identify the optimal hyperparameter configuration, we employed Bayesian optimization using the Optuna framework, leveraging the TPE as the sampler. The search space included learning rate, per-device batch size, gradient accumulation steps, warmup ratio, weight decay, 15 and number of training epochs. Optimization targeted the minimization of validation loss. Trial results were persistently stored in both jsonl and SQLite formats, allowing for checkpointing and resumability in the event of interruption. Poor-performing trials were automatically pruned using Optunas MedianPruner, which compares intermediate results to the median of previous completed trials and terminates underperformers early. Early stopping was also employed during training to prevent overfitting and reduce unnecessary computational expenditure. All training was conducted using the SFTTrainer class from the Hugging Face trl library, which supports supervised fine-tuning with periodic evaluation at the end of each epoch. Throughout training, only the LoRA adapters were updated, while the quantized backbone remained frozen to preserve efficiency. Recognizing the potential inefficiency of starting hyperparameter optimization from purely random initial conditions, particularly in high-dimensional or sensitive parameter spaces, we manually enqueued strong initial configuration to guide the search. This warm-start configuration used learning rate of 2 104, per-device batch size of 2, 8 gradient accumulation steps, warmup ratio of 0.05, weight decay of 0.01, and 10 training epochs. This guided initialization enabled the optimizer to begin exploring in region of the parameter space known to yield promising results. total of 50 hyperparameter optimization trials were performed. Among these, 20 trials completed successfully, while 30 were pruned based on intermediate performance. No trials failed due to runtime errors. The best-performing trial achieved validation loss of 1.0 105, using learning rate of approximately 1.094 105, batch size of 4, 4 gradient accumulation steps, warmup ratio of 0.0997, weight decay of 0.00127, and 15 training epochs (Appendix, Table A6). During inference, both the fine-tuned GPT-4o and Qwen agents received single input image and generated three outputs: (i) predicted disease category, (ii) natural language explanation justifying the classification, and (iii) normalized confidence score in the range [0, 1]. As in the previous configuration (Experiment I), non-visual orchestrator model (o3-mini-2025-01-31) served as the final decision-maker. This orchestrator performed comparative reasoning by evaluating structured prompts that included each agents predictions, explanations, and confidence scores. Based on this synthesis, it produced consolidated classification accompanied by rationale for its selection."
        },
        {
            "title": "Experiment III",
            "content": "In the final experimental configuration, we integrate in our Agentic AI system CLIP-based image retrieval (Image-RAG), trust-aware orchestration, and agent re-evaluation mechanisms. The orchestrator incorporates similarity-based retrieval results and calibration metrics to determine whether to trust agent outputs or trigger re-evaluation loops. The system dynamically guides agents to revise their predictions when confidence or justification is misaligned with expected trust signals (Fig. A3). Trust Evaluation Layer. The Trust Evaluation Layer conducts offline trust profiling of the agents by quantifying their calibration, discriminative reliability, and consistency over labeled dataset. Specifically, both agents were evaluated in zero-shot inference setting on the training image set originally employed in Experiment II for fine-tuning, but here presented without ground-truth labels during evaluation. For each image, we recorded the predicted class, associated confidence score, and explanatory output provided by the agent. Using this comprehensive log of model behavior, we computed suite of quantitative trustworthiness metrics to assess each agents predictive confidence alignment and overall reliability under zero-shot conditions. Expected Calibration Error (ECE): In this study, ECE quantifies the discrepancy between models predicted confidence and its actual accuracy across prediction bins, serving as key metric for assessing trustworthiness in zero-shot and fine-tuned agentic AI systems. low ECE value indicates that the models self-reported confidence aligns closely with its empirical correctness, enabling more reliable orchestration decisions based on agent trustworthiness. By incorporating ECE into the orchestrators arbitration logic, we enhance the systems ability to identify overconfident failures and trigger re-evaluation loops when agent confidence is not well-calibrated. ECE = (cid:88) m=1 Bm acc(Bm) conf(Bm) , 16 where Bm is the m-th confidence bin, the number of samples, and acc and conf represent per-bin accuracy and confidence. Overconfidence Ratio (OCR): The OCR measures the proportion of incorrect predictions made with high confidence (e.g., confidence 0.9), capturing the extent to which models exhibit unjustified certainty. high OCR value signals critical misalignment between model confidence and actual correctness, often leading to erroneous decisions in confidence-based orchestration pipelines. In our trust-aware framework, OCR serves as penalizing factor in agent arbitration, allowing the orchestrator to detect and downweight predictions from agents that frequently make confidently wrong decisions. OCR = {i : Ëyi = yi ci > 0.9} {i : ci > 0.9} , measuring how often models are confidently wrong. Consistency Gap (CG): The CG quantifies the divergence in models predictions when exposed to semantically equivalent but syntactically varied prompts, serving as proxy for reasoning stability under linguistic perturbations. large CG indicates that the agents output is highly sensitive to prompt phrasing, undermining reliability and reproducibility in decision-making. Within our framework, CG is used as trust signal to evaluate the robustness of agent responses, enabling the orchestrator to identify models prone to inconsistency and initiate re-evaluation when necessary. CG = 1 (cid:88) i= I[P (1) = (2) ], assessing variation in outputs across prompt formulations. This quantitative profiling enabled agent-level trust estimation: method for assessing an agents general reliability independently of individual inputs. These trust profiles were then used by the orchestrator to modulate the influence of each agent in decision fusion granting higher weight to agents with stronger calibration and lower overconfidence. Image RAG. To complement the Agentic AI classification framework, we incorporated multimodal retrieval-augmented generation (Image-RAG) system designed for plant disease detection via semantic similarity and weighted voting. This framework utilizes pre-trained vision-language model (CLIP, ViT-B/32 variant) to embed input images into high-dimensional feature space that supports interpretable and context-rich decision-making. Given an input image RHW 3, the CLIP vision encoder transforms it into 512-dimensional embedding R512 using vision transformer architecture. This includes patch embedding (3232), positional encoding, multi-head self-attention, and class token aggregation mechanism. Embeddings are subsequently L2-normalized to lie on the unit hypersphere, enabling cosine similarity to serve as the primary similarity metric: ei = L2 normalize(ViTCLIP(Ii)), sim(ei, ej) = cos(Î¸ij) = ei ej (1) All reference embeddings are stored in vector database implemented using Facebook AI Similarity Search (FAISS), employing an exact inner product index (IndexFlatIP). Each entry is paired with category label yi {healthy, black-rot, rust, scab} and associated metadata (e.g., image URLs, index references). At inference time, the system performs k-nearest neighbor retrieval. Given query image Iq, we compute its embedding eq and retrieve the top-k most similar images: Rk = arg max {cos(eq, ei) [1, ]} (2) Each retrieved item is assigned similarity score si = cos(eq, ei). To classify the query image, the system applies weighted voting mechanism in which the confidence for each category is computed by normalizing the similarity-weighted votes from the retrieved examples: (3) conf(c) = (cid:80) iRk,yi=c si (cid:80) sj jRk 17 This produces interpretable confidence scores that reflect both the quantity and quality of visual evidence for each class. Implementation details include batch processing for efficiency, robust error handling (e.g., corrupted images, missing URLs), and database persistence using FAISS binary formats and Python serialization. The flat index provides exact search with O(N ) query complexity, sufficient for moderate dataset sizes. For larger-scale deployment, approximate nearest neighbor indexing may be integrated. When queried, the Image-RAG system returns ranked list of candidate categories along with normalized confidence scores, e.g.: [ ] {\"category\": \"scab\", \"confidence\": 0.5005}, {\"category\": \"healthy\", \"confidence\": 0.3996}, {\"category\": \"rust\", \"confidence\": 0.0999} This interpretability and modularity make Image-RAG natural supplement to the broader Agentic AI classification pipeline. All source code, algorithm and data is publicly available at https://github.com/Applied-AIResearch-Lab/Orchestrator-Agent-Trust"
        },
        {
            "title": "Declarations",
            "content": "Funding: Partial financial support for open access publication was provided by the Hellenic Academic Libraries Link (HEAL-Link). Additionally, this work was supported in part by the National Science Foundation (NSF) and the United States Department of Agriculture (USDA), National Institute of Food and Agriculture (NIFA), through the Artificial Intelligence (AI) Institute for Agriculture program under Award Numbers AWD003473 and AWD004595, and USDA-NIFA Accession Number 1029004 for the project titled Robotic Blossom Thinning with Soft Manipulators. Additional support was provided through USDA-NIFA Grant Number 2024-67022-41788, Accession Number 1031712, under the project ExPanding UCF AI Research To Novel Agricultural EngineeRing Applications (PARTNER). Conflict of interest: The authors declare that they have no conflict of interest. Ethics approval and consent to participate: Not applicable. Consent for publication: Not applicable. Data availability: The datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request or through the GitHub repository (Source Link). Materials availability: Not applicable. Code availability: All source code and software modules developed in this study are publicly available at: https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust Author contribution: Konstantinos I. Roumeliotis and Ranjan Sapkota contributed equally to this work as lead authors. Ranjan Sapkota served as the corresponding author. All authors were involved in the study conception, system design, experimental execution, and manuscript preparation. All authors reviewed, revised, and approved the final version of the manuscript. If any of the sections are not relevant to your manuscript, please include the heading and write Not applicable for that section."
        },
        {
            "title": "References",
            "content": "[1] Afroogh, S., Akbari, A., Malone, E., Kargar, M., Alambeigi, H.: Trust in ai: progress, challenges, and future directions. Humanities and Social Sciences Communications 11(1), 130 (2024) [2] Ilievski, F.: Human-centric ai with common sense. Springer (2025) [3] Bradshaw, T.J., Tie, X., Warner, J., Hu, J., Li, Q., Li, X.: Large language models and large multimodal models in medical imaging: primer for physicians. Journal of Nuclear Medicine 66(2), 173182 (2025) 18 [4] Yang, Y., Zhang, H., Gichoya, J.W., Katabi, D., Ghassemi, M.: The limits of fair medical imaging ai in real-world generalization. Nature Medicine 30(10), 28382848 (2024) [5] Mon-Williams, R., Li, G., Long, R., Du, W., Lucas, C.G.: Embodied large language models enable robots to complete complex tasks in unpredictable environments. Nature Machine Intelligence, 110 (2025) [6] Ale, L., King, S.A., Zhang, N., Xing, H.: Enhancing generative ai reliability via agentic ai in 6g-enabled edge computing. Nature Reviews Electrical Engineering, 13 (2025) [7] Buehler, M.J.: Preflexor: Preference-based recursive language modeling for exploratory optimization of reasoning and agentic thinking. npj Artificial Intelligence 1(1), 4 (2025) [8] Sapkota, R., Roumeliotis, K.I., Karkee, M.: Ai agents vs. agentic ai: conceptual taxonomy, applications and challenges. arXiv preprint arXiv:2505.10468 (2025) [9] Savaglio, C., Ganzha, M., Paprzycki, M., Badica, C., Ivanovic, M., Fortino, G.: Agent-based internet of things: State-of-the-art and research challenges. Future Generation Computer Systems 102, 10381053 (2020) [10] Jeyakumar, S.K., Ahmad, A.A., Gabriel, A.G.: Advancing agentic systems: Dynamic task decomposition, tool integration and evaluation using novel metrics and dataset. In: NeurIPS 2024 Workshop on Open-World Agents (2024) [11] Zhai, S., Bai, H., Lin, Z., Pan, J., Tong, P., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al.: Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems 37, 110935110971 (2024) [12] Lin, K.Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, S.W., Wang, L., Shou, M.Z.: Showui: One vision-language-action model for gui visual agent. In: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1949819508 (2025) [13] Qiao, S., Denny, P., Giacaman, N.: Oversight in action: Experiences with instructor-moderated llm responses in an online discussion forum. In: Proceedings of the 27th Australasian Computing Education Conference, pp. 95104 (2025) [14] Ma, S., Wang, X., Lei, Y., Shi, C., Yin, M., Ma, X.: are you really sure? understanding the effects of human self-confidence calibration in ai-assisted decision making. In: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 120 (2024) [15] Warmsley, D., Choudhary, K., Rego, J., Viani, E., Pilly, P.K.: Self-assessment in machines boosts human trust. Frontiers in Robotics and AI 12, 1557075 (2025) [16] Wang, L., Friedman, N., Zhu, C., Zhu, Z., Mountford, S.J.: The impact of confidence ratings on user trust in large language models. In: Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization, pp. 365370 (2025) [17] Huang, F., Sun, X., Mei, A., Wang, Y., Ding, H., Zhu, T.: Llm plus machine learning outperform expert rating to predict life satisfaction from self-statement text. IEEE Transactions on Computational Social Systems (2024) [18] Wen, B., Xu, C., Bin, H., Wolfe, R., Wang, L.L., Howe, B.: Mitigating overconfidence in large language models: behavioral lens on confidence estimation and calibration. In: NeurIPS 2024 Workshop on Behavioral Machine Learning (2024) [19] Nigar, N., Faisal, H.M., Umer, M., Oki, O., Lukose, J.: Improving plant disease classification with deep learning based prediction model using explainable artificial intelligence. IEEE Access (2024) [20] Ding, W., Abdel-Basset, M., Alrashdi, I., Hawash, H.: Next generation of computer vision 19 for plant disease monitoring in precision agriculture: contemporary survey, taxonomy, experiments, and future direction. Information Sciences 665, 120338 (2024) [21] Qadri, S.A.A., Huang, N.-F., Wani, T.M., Bhat, S.A.: Advances and challenges in computer vision for image-based plant disease detection: comprehensive survey of machine and deep learning approaches. IEEE Transactions on Automation Science and Engineering (2024) [22] Ali, S., Abuhmed, T., El-Sappagh, S., Muhammad, K., Alonso-Moral, J.M., Confalonieri, R., Guidotti, R., Del Ser, J., DÄ±az-RodrÄ±guez, N., Herrera, F.: Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence. Information fusion 99, 101805 (2023) [23] Bajorath, J.: From scientific theory to duality of predictive artificial intelligence models. Cell Reports Physical Science 6(4) (2025) [24] Messeri, L., Crockett, M.: Artificial intelligence and illusions of understanding in scientific research. Nature 627(8002), 4958 (2024) [25] Ke, Y.H., Jin, L., Elangovan, K., Abdullah, H.R., Liu, N., Sia, A.T.H., Soh, C.R., Tung, J.Y.M., Ong, J.C.L., Kuo, C.-F., et al.: Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness. npj Digital Medicine 8(1), 187 (2025) [26] Dong, G., Zhu, Y., Zhang, C., Wang, Z., Wen, J.-R., Dou, Z.: Understand what llm needs: Dual preference alignment for retrieval-augmented generation. In: Proceedings of the ACM on Web Conference 2025, pp. 42064225 (2025) [27] Tozuka, R., Johno, H., Amakawa, A., Sato, J., Muto, M., Seki, S., Komaba, A., Onishi, H.: Application of notebooklm, large language model with retrieval-augmented generation, for lung cancer staging. Japanese Journal of Radiology 43(4), 706712 (2025) [28] Longo, L., Brcic, M., Cabitza, F., Choi, J., Confalonieri, R., Del Ser, J., Guidotti, R., Hayashi, Y., Herrera, F., Holzinger, A., et al.: Explainable artificial intelligence (xai) 2.0: manifesto of open challenges and interdisciplinary research directions. Information Fusion 106, 102301 (2024) [29] MacKenzie, A., Munster, A.: Platform seeing: Image ensembles and their invisualities. Theory, Culture & Society 36(5), 322 (2019) [30] Ganaie, M.A., Hu, M., Malik, A.K., Tanveer, M., Suganthan, P.N.: Ensemble deep learning: review. Engineering Applications of Artificial Intelligence 115, 105151 (2022) [31] Frei, C., Isotta, F.A.: Ensemble spatial precipitation analysis from rain gauge data: Methodology and application in the european alps. Journal of Geophysical Research: Atmospheres 124(11), 57575778 (2019) [32] Ojha, J., Presacan, O., G. Lind, P., Monteiro, E., Yazidi, A.: Navigating uncertainty: userperspective survey of trustworthiness of ai in healthcare. ACM Transactions on Computing for Healthcare 6(3), 132 (2025) [33] Singh, J., Hanson, J., Paliwal, K., Zhou, Y.: Rna secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. Nature communications 10(1), 5407 (2019) [34] Yang, Y., Lv, H., Chen, N.: survey on ensemble learning under the era of deep learning. Artificial Intelligence Review 56(6), 55455589 (2023) [35] Brown, G.: Ensemble learning. Springer, 393402 (2017) [36] Jahan, I., Schreck, J.S., Gagne, D.J., Becker, C., Astitha, M.: Uncertainty quantification of wind gust predictions in the northeast united states: An evidential neural network and explainable 20 artificial intelligence approach. Environmental Modelling & Software, 106595 (2025) [37] He, W., Jiang, Z.: survey on uncertainty quantification methods for deep neural networks: An uncertainty source perspective. perspective 1, 88 (2023) [38] Flores Romero, P., Fung, K.N.N., Rong, G., Cowley, B.U.: Structured human-llm interaction design reveals exploration and exploitation dynamics in higher education content generation. npj Science of Learning 10(1), 113 (2025) [39] Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., Li, Y.: Large language models empowered agent-based modeling and simulation: survey and perspectives. Humanities and Social Sciences Communications 11(1), 124 (2024) [40] Fernandez Domingos, E., Terrucha, I., Suchon, R., Grujic, J., Burguillo, J.C., Santos, F.C., Lenaerts, T.: Delegation to artificial agents fosters prosocial behaviors in the collective risk dilemma. Scientific reports 12(1), 8492 (2022) [41] Pataranutaporn, P., Danry, V., Leong, J., Punpongsanon, P., Novy, D., Maes, P., Sra, M.: Ai-generated characters for supporting personalized learning and well-being. Nature Machine Intelligence 3(12), 10131022 (2021) [42] Lu, W., Luu, R.K., Buehler, M.J.: Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities. npj Computational Materials 11(1), 84 (2025) [43] Odobesku, R., Romanova, K., Mirzaeva, S., Zagorulko, O., Sim, R., Khakimullin, R., Razlivina, J., Dmitrenko, A., Vinogradov, V.: Agent-based multimodal information extraction for nanomaterials. npj Computational Materials 11(1), 111 (2025) [44] Liu, H., Guo, D., Cangelosi, A.: Embodied intelligence: synergy of morphology, action, perception and learning. ACM Computing Surveys (2025) [45] Kermansaravi, A., Refaat, S.S., Trabelsi, M., Vahedi, H.: Ai-based energy management strategies for electric vehicles: Challenges and future directions. Energy Reports 13, 55355550 (2025) [46] Li, T., Ruan, J., Zhang, K.: The investigation of reinforcement learning-based end-to-end decision-making algorithms for autonomous driving on the road with consecutive sharp turns. Green Energy and Intelligent Transportation, 100288 (2025) [47] Wada, A., Tanaka, Y., Nishizawa, M., Yamamoto, A., Akashi, T., Hagiwara, A., Hayakawa, Y., Kikuta, J., Shimoji, K., Sano, K., et al.: Retrieval-augmented generation elevates local llm quality in radiology contrast media consultation. npj Digital Medicine 8(1), 395 (2025) [48] Yang, R., Ning, Y., Keppo, E., Liu, M., Hong, C., Bitterman, D.S., Ong, J.C.L., Ting, D.S.W., Liu, N.: Retrieval-augmented generation for generative artificial intelligence in health care. npj Health Systems 2(1), 2 (2025) [49] Zhang, G., Xu, Z., Jin, Q., Chen, F., Fang, Y., Liu, Y., Rousseau, J.F., Xu, Z., Lu, Z., Weng, C., et al.: Leveraging long context in retrieval augmented language models for medical question answering. npj Digital Medicine 8(1), 239 (2025) [50] Prince, M.H., Chan, H., Vriza, A., Zhou, T., Sastry, V.K., Luo, Y., Dearing, M.T., Harder, R.J., Vasudevan, R.K., Cherukara, M.J.: Opportunities for retrieval and tool augmented large language models in scientific facilities. npj Computational Materials 10(1), 251 (2024) [51] Hammane, Z., Ben-Bouazza, F.-E., Fennan, A.: Selfrewardrag: enhancing medical reasoning with retrieval-augmented generation and self-evaluation in large language models. In: 2024 International Conference on Intelligent Systems and Computer Vision (ISCV), pp. 18 (2024). IEEE 21 [52] Li, X., Wang, S., Zeng, S., Wu, Y., Yang, Y.: survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth 1(1), 9 (2024) [53] Ma, H., Hu, T., Pu, Z., Boyin, L., Ai, X., Liang, Y., Chen, M.: Coevolving with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement learning. Advances in Neural Information Processing Systems 37, 1549715525 (2024) [54] Zhou, H., Tang, Y., Qin, H., Yang, Y., Jin, R., Xiong, D., Han, K., Wang, Y.: Star-agents: Automatic data optimization with llm agents for instruction tuning. Advances in Neural Information Processing Systems 37, 45754597 (2024) [55] Inc., O.: o3-mini - OpenAI API (2025). https://platform.openai.com/docs/models/o3-mini [56] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., Lin, J.: Qwen/Qwen2.5-VL7B-Instruct Hugging Face (2024). https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct Appendix: Supplementary Figures Fig. A1 AI agent prompt and response for disease classification. The prompt instructs vision-language model to classify apple leaf diseases. The response includes the predicted category, natural language justification, selfreported confidence score, processing latency, and estimated computational cost, returned in structured JSON format. Appendix: Supplementary Tables 22 Fig. A2 Orchestrator prompt and agentic response for decision arbitration. In the agentic AI system, the orchestrator receives classification outputs from multiple vision-language agents and synthesizes them into single, trusted decision. The JSON-formatted response includes the selected class, rationale for agreement or disagreement, confidence score, processing time, and token-based inference cost. Table A1 Experiment I: Confidence score distribution for zero-shot agents and the orchestrator. Descriptive statistics for confidence outputs from Qwen-2.5-VL, GPT-4o, and the orchestrator under the zero-shot configuration. Statistic Qwen GPT-4o Orchestrator Count Mean Std. Dev. Min 25th Pctl Median 75th Pctl Max 160 0.943 0.042 0.800 0.950 0.950 0.950 1. 160 0.874 0.042 0.700 0.850 0.850 0.900 0.950 160 0.917 0.037 0.825 0.900 0.925 0.950 0.980 23 Fig. A3 Experiment III Trust-aware orchestration with RAG and iterative re-evaluation. This flow diagram illustrates the dynamic reasoning pipeline in which the orchestrator triggers CLIP-based retrieval (ImageRAG) and re-evaluation loop. Updated agent responses are scored by trust metrics to produce final decision, enabling improved accuracy and interpretability for plant disease classification. Table A2 Experiment II: Comparison of fine-tuning settings for GPT-4o. Performance and cost summary for ResNet-50optimized versus default hyperparameter configurations. Configuration Epochs Batch Size Val. Loss Duration (s) Cost (USD) GPT-4o (ResNet-50 tuned) GPT-4o (Default settings) 10 3 16 1 0.0088 0.0617 1,778 1, 47.53 13.09 24 Table A3 Experiment II: Confidence statistics under few-shot setting. Descriptive statistics of model confidence scores post fine-tuning for both agents and the orchestrator. Statistic Qwen GPT-4o Orchestrator Count Mean Std. Dev. Min 25th Pctl Median 75th Pctl Max 160 0.9499 0.0192 0.85 0.95 0.95 0.95 1. 160 0.9258 0.0294 0.80 0.90 0.95 0.95 1.00 160 0.9413 0.0158 0.875 0.93 0.95 0.95 0.98 Table A4 Experiment III: Trust metrics for zero-shot agents under trust-aware orchestration. Reliability, calibration, and overconfidence profiling for Qwen and GPT-4o prior to re-evaluation. Model Acc. Avg. Conf. ConfcorrConfincorrCG OCR HCW THC CCC ECE CWA pval Qwen GPT 0.492 0.584 0.945 0.877 0.950 0.890 0.941 0. 0.009 0.030 0.508 0.416 260 213 512 512 0.126 0.361 0.0042 0.453 0.0000 0. 0.495 0.592 1 Table A5 Experiment III: Confidence scores after trust-based re-evaluation. Post-reflection confidence statistics from all agents under RAG-enhanced orchestration. Statistic Qwen GPT-4o Orchestrator Count Mean Std. Dev. Min 25th Pctl Median 75th Pctl Max 160 0.9164 0.0635 0.60 0.87 0.925 0.95 1. 160 0.8896 0.0737 0.60 0.85 0.90 0.95 1.00 160 0.9164 0.0497 0.725 0.87 0.925 0.95 1.00 Table A6 Top 5 hyperparameter configurations ranked by validation loss. Rank Trial Val. Loss LR Batch Warmup Epochs 1 2 3 4 11 16 15 9 19 0.000010 0.000934 0.000993 0.001013 0.001188 1.094 105 1.901 105 2.348 105 1.552 104 1.720 105 4 4 4 2 4 0.0997 0.0881 0.0864 0.0351 0.0898 15 13 12"
        }
    ],
    "affiliations": [
        "Cornell University, Department of Biological and Environmental Engineering, Ithaca, 14850, NY, USA",
        "University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, 22131, Greece"
    ]
}