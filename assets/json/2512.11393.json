{
    "paper_title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video",
    "authors": [
        "Zhifan Zhu",
        "Yifei Huang",
        "Yoichi Sato",
        "Dima Damen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively."
        },
        {
            "title": "Start",
            "content": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video Zhifan Zhu1 * Yifei Huang2 Yoichi Sato2 Dima Damen1 1University of Bristol, United Kingdom 2The University of Tokyo, Japan https://zhifanzhu.github.io/ego-nbody 5 2 0 2 2 1 ] . [ 1 3 9 3 1 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Humans can intuitively parallelise complex activities, but can model learn this from observing single person? Given one egocentric video, we introduce the N-Body Problem: how individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce structured prompting strategy that guides Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for = 2 boosts action coverage by 45% over baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively. 1. Introduction As humans, we can naturally decide when and where to split tasks to complete them faster, without disrupting the flow of others. This suggests that even tasks performed by single person contain hidden opportunities for parallelisation. In unscripted egocentric videos, prior works have shown that the camera wearer often carries out multiple goals in parallel [22, 26, 28, 29]. This raises an intriguing question: can we predict parallel execution of multiple agents, such that the original work can be sped up? We introduce and formalise the N-Body Problem the novel problem of predicting multi-agent parallel execu- *Partly conducted during Zhus visit to the University of Tokyo. tion from video showcasing single-agent execution trace. Our output should imagine highly synergetic execution which maximises the concurrent work performed by all agents. This effective execution is not merely an optimisation problem for speed. It is fundamentally constrained by the physics and logic of the real world. Any valid parallel execution must respect several categories of constraints: First, the Spatial Constraints. As physical entities, the agents cannot occupy the same space simultaneously. The execution must therefore aim to be collision-free. Second, the Object Constraints. The execution must enforce exclusive object ownership, ensuring that no two agents attempt to use the same object at the same time. Finally, the Causality Constraints. To be causally correct, the execution must respect when certain actions are prerequisites for others. It might be tempting to think of the N-Body Problem as resource-constrained scheduling task. We argue this is infeasible for long-form, unscripted video. 25-minute video comprises hundreds of thousands of frames, and no existing model can robustly parse this continuous stream into scheduling task. Similarly, it is infeasible to produce robust discrete, hierarchical task graph as explored in prior work [1, 9, 20]. These works are restricted to shorter videos where one is carrying out known pre-defined activity. This is not the case in unscripted videos where multiple ongoing goals [26] are carried out. We instead consider the N-Body Problem joint reasoning and temporal segmentation problem that requires holistic understanding of what to do, where, with what, and in what order, all at once. We evaluate the N-Body Problem on 100 long videos from single-person egocentric datasets [4, 24] (avg 25min). We propose suite of metrics that leverage existing groundtruth annotations (actions, object tracks, camera poses) to evaluate plans performance (Goals) and, critically, its physical feasibility (Constraints). Crucially, this ground truth is not used as input, but only to evaluate the feasibility of proposed executions. We argue that defining single ground-truth execution is an ill-defined and infeasible task. For any complex activity, there exist multiple valid and 1 Figure 1. Top. Our input is single-person egocentric video. The camera wearer performs combination of cooking, washing up and ordering. Middle. Predicted 2-Body Parallel execution. Our method, which uses Gemini with prompts on goals and constraints, achieves 1.6x speed-up (from 19.8min to 10.4min) with 86% coverage. We show 3D representation of where the 2-agents (orange/blue) are and their camera views. P1 is mostly left to do the cooking, while P2 is washing up and ordering. Bottom. 3-Body parallel execution achieving further 2.3x speed-up to 7.5min. P2 is washing up while P3 is drying and storing things away. For the full output, see supp and supp video. (near-)optimal parallel solutions. Therefore, our framework evaluates any generated plan based on its intrinsic properties and adherence to real-world feasibility, rather than comparison to single arbitrary reference. To establish benchmark and analyse the challenges of this problem, we leverage state-of-the-art VLM, Gemini 2.5 Pro [3]. This VLM has produced outstanding performance on number of long-form video tasks [3, 24]. We design structured prompting strategy that guides the VLM to reason about 3D environments, object usage, and task dependencies. This approach allows the VLM to better capture the constraints for viable parallel execution. While we do not contribute to better video understanding/reasoning model, using our proposed metrics, we systematically analyse the VLMs performance and demonstrate how it can be controlled to trade off between goals and constraints. Fig 1 shows an output of our method, where we predict 2-Body and 3-Body executions from the same video. To summarise, in this paper we: Formulate the N-Body Problem parallelising multiagent execution from single-agent trace. Propose an evaluation metrics suite of parallel execution using publicly available annotations. Design complex prompt that allows state-of-the-art VLM to act as reasoning model, to solve the N-Body Problem, maximising goals and respecting spatial, object and causal constraints. Evaluate on 100 long videos from two single-person datasets [4, 24] for both 2-Body (N=2) and 3-Body (N=3) execution. Demonstrate the difficulty of this problem, where naive assignment and optimisation scheduling produce significant collision rate (25.9% and 39.7%) between agents, and open-weight VLM QWEN fails to speed-up. While Gemini 2.5 Pro with base prompt performs poorly, it can be guided to improve goals and respect constraints, im2 proving action coverage to 91.3% and reducing collision, object and causal conflict to 7.7%, 0.64% and 18% on 80 HD-EPIC videos, respectively. 2. Related Works Egocentric Videos. Popularised by recent datasets [4, 8, 9, 13, 24], egocentric vision has produced powerful models for parsing human activity streams. Current research spans action understanding [7, 12, 25], multimodal sensing [11, 17, 40], and vision-language tasks [16, 18, 37, 41]. While this paradigm excels at passive understanding along one timeline, it does not address the challenge we study: how to re-allocate the same work across multiple concurrent workers while remaining faithful to real-world feasibility (space, objects and semantics). Task-graph annotation [9, 20] is promising bridge for enforcing causal order. However, these annotations are not publicly available, limiting comprehensive multi-constraint evaluation. We here focus on predicting parallel execution with goals and constraints that leverage readily available signals. Task Parallelisation. Prior works on multitasking and coordination shows that human activity often interleaves goals and that latent threads can be unwoven from single stream, revealing headroom for concurrency [26, 29]. Multi-person egocentric and mixed-view collections further indicate that collaborative behaviours are learnable in principle [14, 15, 27, 38]. However, prior works focus only on describing or segmenting concurrent intentions. [14] is the closest work to our paper, as the Lemma dataset records one person performing the task, then records two people collaborating to complete the same task without verbal communication. While the intuition matches others, the videos are short-term (only 2 mins). Additionally, none of the works above predicts parallel execution from single demonstration. At the abstract level, our problem is analogous to task scheduling in parallel computing [5, 31]: given set of dependent tasks and limited processors, the goal is to minimise the makespan. Since optimal scheduling is challenging, practical solutions rely on heuristics (e.g. list scheduling such as HEFT) that deliver approximations efficiently [33]. We include HEFT-style baseline in our results. Spatial Reasoning in Vision-Language Models. While Vision-Language Models (VLMs) have demonstrated impressive capabilities, recent work has consistently shown that spatial reasoning remains significant weakness [10, 32, 35]. This limitation is often attributed to the lack of explicit 3D and geometric information in pre-training. Current research aims to mitigate this through methods like finetuning on synthetic 3D VQA datasets [19, 23, 39] or developing specialized architectures [2, 21, 30]. We address this limitation from task-driven angle. Rather than pursuing universally capable spatial reasoner, we ask how to elicit reliable spatial behaviour for our downstream goal: collision avoidance in predicting parallel execution. Our spatial prompt discretises the environment and breaks the temporal timeline into zone-aligned periods in order to guide the spatial reasoning in the VLM without changing model weights. 3. Problem Formulation While the general gist of acquiring help from others during everyday operation is intuitive, formulating the problem as representation mapping from one video to parallel executions needs significant attention. We introduce the input and output for the N-Body Problem in Sec 3.1. We then separate the objectives into goals (Sec 3.2) and constraints (Sec 3.3). These identify what needs to be maximised, versus what makes parallel execution incorrect or invalid. 3.1. Input and Output Representations We denote the source input video as I, which represents single-person execution from frame 1 to TI. The problem is to divide into non-overlapping assignable segments, For < j; Sij = [Ii, Ij], ij, ij IoU (Sij, Sij) = 0. (1) It is critical that these segments are non-overlapping so they can be assigned to single agent in the N-Body Problem. Note that some parts of the original video might not be assigned to any segment Sij and are accordingly discarded. = parallel {P1, P2, . . . , PN }, we then need to assign each of these executable segments to one of the agents. Importantly, the order of the segments can be shuffled if needed. Additionally, any agent can be in an idle state as they wait for viable task to perform. Denote Pn the execution assigned to the n-th agent, and denote A(, ) the assignment function, execution an -Way A(Sij, Pn) = τ (2) means that the start of the segment Sij is assigned to the frame τ in Pn. As the duration of that segment remains unchanged, the task is only to assign start time for that segment at Pn. Consequentially, the frames [τ, τ + Sij] in the parallel execution Pn are all allocated to the segment: A(Sij, Pn) = τ Pn[τ, τ + (j i)] = Sij (3) Additionally, segment can only be assigned to single agent, i.e. A(Sij, Pn) = null = : A(Sij, Pm) = null, (4) where A(Sij, Pn) = null indicates that the segment Sij is assigned to Pn at some frame. Of course there are many possible ways to divide segments and also many ways to assign these segments to 3 agents, leading to different parallel executions {P} from the same video I. We next define the one(s) we are after in the N-Body Problem. 3.2. Goals Our objective is to generate one parallel execution that is as efficient as possible while covering as much of the work completed in the source execution as possible. Formally, we aim to maximise the following quantities: First, the coverage of the parallel execution: Coverage :="
        },
        {
            "title": "Number of assigned frames\nTI",
            "content": "(5) where we calculate the total length of all assigned hence covered segments in the original video. Second, the speed-up from the original video to parallel execution: execution. The object conflict rate (OCR) is defined as: TP OCR := Num. of frames where OkO in conflict , (9) Note that we here make the assumption that no extra objects in the scene can be used instead - e.g. if two knifes are used during the activity, we assume these are the only knives available. Given we only rely on the observations from single video, it will be challenging to make assumptions about additional objects or their utility e.g. suitable alternative knife for slicing bread. Lastly, causal constraint is one that measures plausible parallel executions i.e. preserve any tasks that need to occur in particular order or need to take specific time to complete. For example, an agent needs to first chop the vegetables before adding these to the soup this order should not be reversed in the parallel execution. We measure causality violations rate (CVR) as: Speed-Up := Sequential execution time Parallel execution time (TP ) , (6) CVR := Num. of violated Gk (10) where TP amounts to the time required for the agent that is working the longest. 3.3. Constraints While maximising the above objectives, the generated execution must also remain physically and logically feasible. We therefore seek to measure from only the given video, and avoid (i.e. minimise) violations of four constraints the first two are related to the space in the scene and the second two are the semantics of objects and causal actions. First, the collision in the 3D scene between concurrent working agents. Agents should not be occupying the same space at one time. We measure conflicts in the space through the collision rate: Collision Rate := Number of colliding frames in TP , (7) Second, the relocation overhead i.e. jump distances of agents in the parallel execution. As each agent is assigned different segments, at times the end of one segment can be at different part of the scene than the start of the next segment. This is violation as it expects the agent to unrealistically teleport in space. We measure this jump conflict as: (cid:80)N Spatial Jump := 1 n=1 Avg. Jump Distance of agent n, (8) Third, the conflict of accessing the same object by different agents. Denote the set of objects accessed or used in the video. conflict is measured as the duration when multiple agents access the same object Ok in the parallel where is the list of causal constraints. Each constraint in is pair of temporal segments that must occur in order. If any segment pair is reversed, carried out in parallel or the prerequisite segment is missing, i.e., missing the cause, the pair Gk is considered causality violation. We calculate the rate of violated pairs amongst all causal pairs. Note that if all pairs are ordered correctly, the complete causal graph is thus correctly executed. Objectives Summary. We have defined multiple objectives for the N-Body Problem, as no single measure can capture the quality of parallel execution on its own. The distinction between goals and constraints is not absolute: for example, coverage could be treated as constraint, while jump distance could also be viewed as goal. In this work, we treat goals as objectives to maximise and constraints as objectives to avoid or minimise. The precise implementation of goals and constraints as quantitative metrics is in Sec 4.2. Importantly, multiple parallel executions {P} can achieve the same performance (speed-up, coverage, collision/conflict/causality rates). We consider these to be equivalent. 4. Experiments 4.1. Datasets and Annotations We evaluate our formulation on two egocentric datasets: HD-EPIC [24] and EPIC-KITCHENS-100 (which we refer to as EPIC in the rest of the paper) [4]. Both capture unscripted kitchen-based activities in home environments, with long continuous recordings that naturally contain interleaved goals such as cooking, cleaning, and storing. We randomly select 100 long videos 80 from HD-EPIC and 20 from EPIC, covering 24 unique scenes (9 from HD-EPIC and 15 from EPIC) with minimum duration of 10 minutes (avg video length 25.3 minutes) to form tractable evaluation set. The longer videos are likely to be more meaningful for parallel execution as they tend to capture more underlying activities. Crucially, both datasets provide ground-truth annotations to evaluate the goals and constraints (presented in Sec 3) using set of dedicated metrics (see Sec 4.2). We select more videos from HD-EPIC as they contain additional ground truth that helps us evaluate object and causal conflict as we will describe below. Ground-Truth Camera Poses. Both HD-EPIC and EPIC provide ground-truth camera poses annotated using multisensor SLAM [6] or sub-sampled frames COLMAP [34]. Camera poses provide strong signal of where the person situates in the environment, as well as their body orientations. With the persons trajectory given by the camera poses in the source video, we are able to evaluate the Collision Rate. Ground-Truth Actions. Both HD-EPIC and EPIC provide dense and detailed annotations of all action segments completed by the camera wearer. The action narrations covers all meaningful parts of the video, which we will use for measuring the coverage of important works in the original video. Ground-Truth Object Movements. Recently introduced HD-EPIC provides extensive manual annotations of object movements as object tracks. These are start-end segments that correspond to the duration when an object is moved by the camera-wearer along the video. These are exhaustive and form long trajectories covering frames when the object is in motion/use versus when the object is stationary. These tracks are linked to the object instances, allowing us to identify the same instance of any object throughout the video. The presence of these annotations allow us to accurately measure object conflict. Ground-Truth Recipe Segments Dependencies. Similarly, the recently introduced HD-EPIC dataset contains annotated time segments of recipe preparation and recipe steps. For each recipe, the steps are manually defined along with labels for the temporal segments for each step. Additionally, any preparatory action is also manually labelled (e.g. for the step of boiling pasta, the prep of filling the kettle with water is temporally segmented and linked to this step). Additionally, we add strict dependencies between steps of one recipe manually (see Supp for details). These annotations allow us to evaluate the following causal constraints in the parallel execution: i) prep-step constraint: prep must be completed before the corresponding step starts, and ii) step-step constraint: for the step dependency (A B), the step must be finished before starts. 4.2. Evaluation Metrics person video inputs. Importantly, such ground truth cannot be acquired even if two people are hired to carry out the same task, as this will only provide one of many possible valid parallel executions. Instead, we rely on identifying invalid executions through our proposed evaluation metrics. Our evaluation metrics directly measure the goals (Sec. 3.2) and constraints (Sec. 3.3), using the available data annotations, as follows: frame coverage, action coverage, speed-up, 3D spatial collision rate, object conflict rate (OCR) , average jump distance and causality violation rate (CVR) . The frame coverage of the parallel execution is implemented as in Equation (5): Frame Coverage := (cid:80)Sij n; A(Sij, Pn) = null TI , (11) where A(Sij, Pn) = null means the segment Sij is assigned to the agent in the parallel execution P. Note again that one segment can only be assigned to single agent. While frame coverage is one way to measure parallel execution, some frames might be excluded that do not correspond to any tasks or actions. Instead, we use the exhaustive annotations for action segments, we consider the set of all actions in the video and compute the percentage of these actions that are covered by the parallel execution. We calculate the action coverage as: (cid:80) Cr 1[ n,i,j : A(Sij ,Pn)=null & IOU (Sij ,Cr)0.5 ] Action Coverage = , (12) i.e. we calculate the temporal overlap between the action segment Cr and any assigned segments in the parallel execution Pn. As long as any agent has segment that temporally overlaps with the action by more than 0.5 IOU, we consider the action to be covered by this parallel execution. We use the standard threshold of temporal overlap (0.5) which is accepted in temporal action localisation. Note that this strict threshold 0.5 implies that the action cannot be covered by multiple agents as any other agent will naturally have threshold lower than 0.5 IOU. To disentangle the coverage from the speed-up, we calculate the speed-up metric as: (cid:80)TI t= TP 1[ n,i,j : A(Sij ,Pn)=null & itj ] Speed-Up = . (13) Note that the speed-up metric does not account for the frames that are not covered by the parallel execution P. It measures the relative speed-up from sequential execution to parallel execution solely. We disentangle the two metrics (coverage and speed-up) to avoid solutions that drop the coverage to gain speed-up. As noted in the introduction, we do not have any ground truth parallel executions i.e. we only have access to singleTo evaluate the spatial collision between parallel agents, we use the ground-truth camera poses to represent persons 5 trajectory. Denote Γn(t) the spatial trajectory of the n-th agent at frame t, the collision rate is implemented as: (cid:80)TP t=1 TP 1[ n=n : is collide(Γn(t),Γn (t)) ] Collision Rate = , (14) where is collide(, ) is function that determines whether two different agents occupy the same space, based on their body positions and orientations in the world coordinate system; the is collide(, ) function is implemented using the average of size of human body1. The jump distance is implemented as: (cid:80)N (cid:80)Mn1 n=1 1 Mn Jump = 1 m=1 Γn(startm+1) Γn(endm), (15) where Mn is the total number of segments assigned to the n-th agent, startm+1 is the start frame of the segment m+1 and endm is the end frame of the segment m. To quantify object conflict, we use the object movement tracks. Denote the set of objects accessed, moved or used in the source video. single object conflict is the duration when multiple agents move the same object Ok in the parallel execution. The object conflict rate (OCR) can thus be implemented as: (cid:80)TP t=1 OCR = 1[ OkO,n=n : Ok,n(t)Ok,n (t) ] , (16) where Ok,n is 1 if the object Ok is accessed by agent at frame t, and 0 otherwise. TP Importantly, to quantify the violation of causality, we use the ground-truth available in the dataset that identifies the dependencies between preparatory actions (e.g. get knife) and step actions (cut vegetable) within recipe, as well as causal dependences amongst steps of the same recipe. Denote the set of segment pairs, = {(Gℓ,0 Gℓ,1)} where the action Gℓ,1 depends on the finishing of the prerequisite Gℓ,0. The causality violation rate (CVR) is implemented as: (cid:80) 1[ n,n:E(Gℓ,0,Pn)>S(Gℓ,1,Pn ) ] (Gℓ,0Gℓ,1 )G CVR = , (17) where E(Gℓ,0, Pn) is the end time of the segment in and S(Gℓ,1, Pn) the start of Gℓ,1 in any agent Ps execution. Note that CVR also evaluates any causal constraint violation within one agents execution when n. 4.3. Model Setup We use Gemini 2.5 Pro [3] to generate parallel execution from input video I. The input to the Gemini model are formed by two parts: our proposed text prompt and the video input I. We use the default 1FPS frame sampling rate to fit the maximum context. 146 cm wide and 25 cm deep 6 We progressively evolve more specific system prompts to improve Gemini 2.5 Pros performance on the N-Body Problem: Base Prompt. The basic prompt where we simply inform the model about the task, without mentioning any goals and constraints. + Goals-Only. On top of Base, we instruct the model to maximise the goals (Coverage and Speed-Up), without mentioning any constraints. + Goals-and-Constraints. We instruct the model to both maximise the goals (Coverage and Speed-Up) and minimise the constraints of space, object and semantics. + Spatial Prompt. On top of + Goals-and-Constraints, we provide an additional column-separated-value (csv) file linking spatial location to timetemporal durations in the source video. We instruct the model to avoid occupying the same zone for different agents. Regarding the final + Spatial Prompt, current visionlanguage models including Gemini 2.5 Pro often struggle with robust spatial understanding. In the parallelexecution problem, simply supplying the raw trajectory of the source video as additional input does not provide Gemini with sufficient guidance (see ablation in supplementary). We explore compact and structured spatial prompt that explicitly encodes the knowledge needed for collision avoidance. We divide the 3D environment into equal-sized zones; we divide the XY-plane as the majority of movements happen within the ground plane. We then extract the duration when the person in remains within one zone, producing list of triplets of: (start-time, end-time, zone number). We then instruct Gemini 2.5 Pro to avoid assigning two parallel agents working in the same zone concurrently. The complete prompt can be found in the supplementary. We set Gemini parameter temperature= 0 and topp = 0.2 for more deterministic results. Additional Comparison Methods. In addition to Gemini 2.5 Pro, we evaluate an open-weight model Qwen2.5-VL72B [36] under our full prompt and 1 FPS sampling rate. We introduce naive baseline that divides the video into equal halves and has the two agents execute their halves concurrently, without modelling dependencies. Additionally, we evaluate simple HEFT-style list scheduler [33]. For the scheduler, we turn ground-truth action timestamps into assignable segments, using the privileged knowledge of start-end times of actions, and manually induce simple precedences from verbobject cues (e.g. take use put, open close). We then apply standard list scheduling heuristic: at each step, select the first ready task and place it at the earliest feasible time that respects object exclusivity. We further experiment removing the ground-truth action timestamps signal: we divide the input duration into 1-minute windows and concatenate all action narrations appearing within each 1-min. We then use the same heurisMethod Coverage (%) Action Cov. (%) Speed-Up Coll. Rate (%) Jump (m) OCR (%) CVR (%) b y - 2 Naive Half-Half HEFT 1-min HEFT GT start-end Qwen2.5-VL-72B Gemini2.5 (Base Prompt) + Goals-Only + Goals-and-Constraints + Spatial Prompt (ours) 3-Body Problem (ours) 100.0 99.9 77.6 63.9 61.4 88.8 87.4 90.7 85.9 100.0 100.0 99.2 63.4 62.9 89.1 88.1 91.3 87.0 2.00 1.93 1.82 0.89 1.58 1.61 1.59 1. 1.51 25.9 39.7 59.1 0.2 17.2 18.9 17.5 7.7 11.9 0.00 0.65 0.28 0.19 0.53 0.54 0.47 0.55 0.56 3.61 2.53 0.03 0.00 1.17 1.71 0.93 0. 1.12 14.7 16.7 72.5 45.2 40.6 18.3 26.3 18.0 26.1 Table 1. Results on HD-EPIC. Notice that bold is across the various prompts of Gemini2.5. Underline implies best metric per column. Qwen2.5-VL-72B could only generate results on subset of 51/80 videos and fails for the rest. uses action segment ground-truth. Causality Violation Rate (CVR) is evaluated on 60 videos that have recipe step annotations. Method Coverage (%) Action Cov. (%) Speed -Up Coll. Rate (%) Jump (m) b y - 2 Naive Half-Half HEFT 1-min HEFT GT start-end Qwen2.5-VL-72B Gemini2.5 (Base Prompt) + Goals-Only + Goals-and-Constraints + Spatial Prompt (ours) 3-Body Problem (ours) 100.0 99.5 73.9 41.0 55.2 75.8 80.1 89.9 83.0 100.0 100.0 98.5 37.8 55.3 76.9 80.0 90.6 84.9 2.00 1.96 1.70 0.89 1.52 1.54 1.57 1. 1.64 29.1 44.2 40.1 0.3 22.3 18.8 21.3 10.1 18.2 0.00 0.63 0.36 0.12 0.52 0.44 0.37 0.43 0.51 Table 2. Results on EPIC. Notice that bold is across the various prompts of Gemini2.5. Underline implies best metric per column. Qwen2.5-VL-72B could only generate results on subset of 7/20 videos. uses ground-truth action segments. Figure 2. Distribution of evaluation metrics across all 100 videos as Gemini2.5 prompt is elaborated. tics of verb-object cues to establish precedences in assigning these 1-min segments to agents. 4.4. Main Results Table 1 shows the method performances on HD-EPIC. For each metric, we report the average metric over videos. Our prime set of comparative results are for = 2 (2-Body Problem). We first present the naive baseline where we just assign the first half of the video to 1 and the second to 2. Results show very high collision rate and high object collision. Gemini 2.5 Pro with base prompt produces good speedup (1.58x), but considerably under-covers in frames and actions (62.9% action coverage) and produces high collision rate (17.2%) and very high causality violation rate CVR (40.6%). Encoding goals in the prompt (+ Goalsonly) achieves the highest speed-up and improves coverage, decreasing causality violation rate (CVR) significantly. Encoding constraints drops the speed-up but fails to reduce the spatial collision rate. Spatial prompting (ours) reduces the collision rate by large margin, increases the coverage but has slightly lower speed-up. Table 2 shows similar trend on EPIC evaluation videos. While tables 1 and 2 only present averaged results across the videos, figure 2 shows the metrics distributions for Gemini2.5 as prompts is expanded. Introducing goals significantly improves the coverage. The spatial prompt consistently lowers the collision rate. Both the naive baseline and HEFT scheduler produce significantly high collision rates, and removing the privileged ground-truth timestamps from HEFT produces higher object conflict rate (OCR). Open-weight VLM Qwen2.5-VL-72B fails to produce any output for 42 out of the 100 videos. On the 58-video subset, it performs poorly in coverage even though it avoids collision this model is incapable of solving this problem. Table 1 and Table 2 also show the comparison results between 2-body and 3-body outputs. When scaling to 3-body, we observe predictable increase in collision and object conflict rates (OCR), with better speed-up (1.40 1.51 on HD-EPIC, vs 1.35 1.64 on EPIC). Qualitative samples are shown in Fig. 3 and best understood from the Supplementary video. Result Analysis. To further understand how our method predicts parallel executions, we analyse three properties of the parallel execution across recipe-step annotated videos: cooking vs non-cooking task split, prep vs step split and differences in the walking distance. This analysis is presented in Figure 4. First, we count the percentage of time an agent is carrying out cooking tasks (preps and steps for recipe) vs non-cooking tasks (ordering or cleaning). We scatter-plot all videos (Figure 4 left) to compare the agent doing less cooking to one doing more cooking (dots close to the axis = correspond to balanced split). Second, Figure 4 (middle) compares whether the prep for one step is performed by the same agent (self) or by the other agent. In most cases the prep is still carried out by the same agent perFigure 3. Qualitative results. Top: Same video with = 2 and = 3. Left: P1 is marinating chicken and washing up. P2 gathers spices (for the marination) in advance then prepares eggs. Right: P2 mostly fetches items from around the kitchen. Bottom Left: (N = 2) P2 is left to do the washing up and clearing. Bottom Right: (N = 3) P1 is cooking, P2 is pouring glass of wine then puts stuff away. P3 is emptying the dishwasher then contributes to washing up. Figure 4. Analysis of parallel executions. Left: Distribution of agent cooking time (normalised by agent total time). Top example: P1 mostly cooks while P2 does the cleaning. Bottom: P1 and P2 both clean. Middle: The ratio of prep carried out by the same agent (Self) vs by the Other agent. Left example: P2 fetches the eggs and the bowl, P1 breaks the egg. Right: P2 fills the mixer cup and starts mixing Right: Distribution of walking distances. Top: P1 is washing up while P2 moves around cleaning. Bottom: Both P1 and P2 walk equally. forming the step. Third, we analyse the divide in the walking distance between agents. In most videos, one agent is doing significantly more walking than the other (dots much further than = x). This supports the qualitative example in Fig. 3, and show this to be common trend one agent is left to occupy one hotspot while another agent moves around the space. We find this behaviour to be very natural and supports the validity of the parallel executions produced by the VLM using our proposed prompt. 5. Conclusion and Limitations In this work, we introduce the N-Body Problem: predicting N-body parallel execution from single egocentric video. We design structured prompt that guides VLM to reason about the spatial, object and causal constraints, predicting the parallel execution, maximising goals while respecting constraints. Tested on 100 videos, our experiments show the difficulty of the N-Body Problem, and that Gemini 2.5 Pro can be guided by dedicated prompts to produce execution with high coverage and reasonable speed-up while maintaining low constraint violations. While guided Gemini 2.5 Pro achieves remarkable results, we identify some of its limitations. For example, the model fails to maintain the exact time when needed e.g., the time where food should be boiling/brewing or in the microwave/oven is not maintained. Some task dependencies cannot be automatically discovered e.g., grinding coffee 8 beans is not automatically understood as pre-requisite to brewing coffee. It also relies heavily on our explicit spatial prompt to avoid spatial collisions. These challenges suggest avenues for future research, to better model causality for unseen activities or develop architectures with more robust intrinsic spatial reasoning capabilities. Acknowledgements Research at Bristol is supported by EPSRC UMPIRE EP/T004991/1, EPSRC PG Visual AI EP/T028572/1. Research at the University of Tokyo is supported by JST ASPIRE Grant Number JPMJAP2303, JSPS KAKENHI Grant Numbers JP24K02956 and 25K24384. Zhu is supported by UoB-CSC scholarship."
        },
        {
            "title": "References",
            "content": "[1] Kumar Ashutosh, Santhosh Kumar Ramakrishnan, Triantafyllos Afouras, and Kristen Grauman. Video-mined task graphs for keystep recognition in instructional videos. Advances in Neural Information Processing Systems, 2023. 1 [2] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. SpatialVLM: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 3 [3] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 6 [4] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, 2022. 1, 2, 3, 4 [5] Pierre-Francois Dutot, Gregory Mounie, and Denis Trystram. Scheduling parallel tasks: Approximation algorithms. Handbook of scheduling: Algorithms, models, and performance analysis, 2004. 3 [6] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, et al. Project aria: new tool for egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561, 2023. [7] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 3 [8] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 3 [9] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from In Proceedings of the first-and third-person perspectives. IEEE Conference on Computer Vision and Pattern Recognition, 2024. 1, 3 [10] Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, and Jiangmiao Pang. EgoExoBench: benchmark for first-and third-person view video understanding in mllms. arXiv preprint arXiv:2507.18342, 2025. 3 [11] Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato. Predicting gaze in egocentric video by learning taskdependent attention transition. In Proceedings of the European Conference on Computer Vision, 2018. [12] Yifei Huang, Yusuke Sugano, and Yoichi Sato. Improving action segmentation via graph-based temporal reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3 [13] Yifei Huang, Guo Chen, Jilan Xu, Mingfang Zhang, Lijin Yang, Baoqi Pei, Hongjie Zhang, Dong Lu, Yali Wang, Limin Wang, and Yu Qiao. EgoExoLearn: dataset for bridging asynchronous egoand exo-centric view of proceIn Proceedings of the IEEE dural activities in real world. Conference on Computer Vision and Pattern Recognition, 2024. 3 [14] Baoxiong Jia, Yixin Chen, Siyuan Huang, Yixin Zhu, and Song-Chun Zhu. Lemma: multi-view dataset for le arning In Proceedings of the ulti-agent ulti-task ctivities. European Conference on Computer Vision. Springer, 2020. 3 [15] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. EgoTaskQA: Understanding human tasks in egocentric videos. Advances in Neural Information Processing Systems, 2022. 3 [16] Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, and Seon Joo Kim. Open-ended hierarchical streaming video understanding with vision language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 3 [17] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In Proceedings of the IEEE International Conference on Computer Vision, 2019. 3 [18] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Zhongcong Xu, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, WANG HongFa, Dima Damen, Bernard Ghanem, Wei Liu, and Mike Zheng Shou. Egocentric video-language pretraining. In Advances in Neural Information Processing Systems, 2022. [19] Dingning Liu, Cheng Wang, Peng Gao, Renrui Zhang, Xinzhu Ma, Yuan Meng, and Zhihui Wang. 3DAxisPrompt: Promoting the 3D grounding and reasoning in GPT-4o. Neurocomputing, 2025. 3 [20] Weichao Mao, Ruta Desai, Michael Louis Iuzzolino, and Nitin Kamra. Action dynamics task graphs for learning plannable representations of procedural tasks. arXiv preprint arXiv:2302.05330, 2023. 1, 3 [21] Damiano Marsili, Rohun Agrawal, Yisong Yue, and Georgia Gkioxari. Visual agentic AI for spatial reasoning with 9 sand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 2024. 3 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [37] Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, and Weidi Xie. Retrieval-augmented egocentric video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 3 [38] Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, et al. Perceiving and acting in first-person: dataset and benchmark for egocentric human-object-human interactions. arXiv preprint arXiv:2508.04681, 2025. 3 [39] Jirong Zha, Yuxuan Fan, Xiao Yang, Chen Gao, and Xinlei Chen. How to enable LLM with 3D capacity? survey of spatial reasoning in llm. arXiv preprint arXiv:2504.05786, 2025. 3 [40] Mingfang Zhang, Yifei Huang, Ruicong Liu, and Yoichi Sato. Masked video and body-worn imu autoencoder for egocentric action recognition. In Proceedings of the European Conference on Computer Vision. Springer, 2024. 3 [41] Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, and Angela Yao. Egotextvqa: Towards egocentric scene-text aware video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 3 dynamic api. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 3 [22] Stephen Monsell. Task switching. Trends in cognitive sciences, 7(3):134140, 2003. 1 [23] Michael Ogezi and Freda Shi. Spare: Enhancing spatial reasoning in vision-language models with synthetic data. arXiv preprint arXiv:2504.20648, 2025. 3 [24] Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, and Dima Damen. HD-EPIC: Highly-Detailed Egocentric Video In Proceedings of the IEEE Conference on ComDataset. puter Vision and Pattern Recognition, 2025. 1, 2, 3, 4, 11 [25] Chiara Plizzari, Shubham Goel, Toby Perrett, Jacob Chalk, Angjoo Kanazawa, and Dima Damen. Spatial cognition from In 2025 egocentric video: Out of sight, not out of mind. International Conference on 3D Vision (3DV), 2025. 3 [26] Will Price, Carl Vondrick, and Dima Damen. Unweavenet: Unweaving activity stories. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 1, 3 [27] Rachel Ringe, Mihai Pomarlan, Robert Porzel, and Rainer Join me: Empirical means for gathering colIn Mensch und Computer 2025Malaka. laborative affordances. Workshopband, 2025. [28] Joshua Rubinstein, David Meyer, and Jeffrey Evans. Executive control of cognitive processes in task switching. Journal of experimental psychology: human perception and performance, 2001. 1 [29] Yuhan Shen and Ehsan Elhamifar. Understanding multi-task In Proceedings of the activities from single-task videos. IEEE Conference on Computer Vision and Pattern Recognition, 2025. 1, 3 [30] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models In Proceedings of the IEEE Conference on for robotics. Computer Vision and Pattern Recognition, 2025. 3 [31] Roni Stern. Multi-agent path findingan overview. Artificial intelligence: 5th RAAI summer school, 2019. 3 [32] Ilias Stogiannidis, Steven McDonagh, and Sotirios Tsaftaris. Mind the gap: Benchmarking spatial reasoning in vision-language models. arXiv preprint arXiv:2503.19707, 2025. [33] Haluk Topcuoglu, Salim Hariri, and Min-You Wu. Performance-effective and low-complexity task scheduling for heterogeneous computing. IEEE transactions on parallel and distributed systems, 2002. 3, 6 [34] Vadim Tschernezki, Ahmad Darkhalil, Zhifan Zhu, David Fouhey, Iro Laina, Diane Larlus, Dima Damen, and Andrea Vedaldi. Epic fields: Marrying 3d geometry and video understanding. Advances in Neural Information Processing Systems, 2023. 5 [35] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thou10 The N-Body Problem: Parallel Execution from Single-Person Egocentric Video"
        },
        {
            "title": "Supplementary Material",
            "content": "study each recipe and identify steps which definitely need to occur in order. If there is doubt, we did not label the dependency as causal one. For example, water needs to be boiled before being used for coffee brewing. From our 80 videos, we annotate 56 recipes, this results in 204 stepstep segment dependencies. This corresponds to 60 videos, while the other 20 videos do not have recipes (i.e. are cleaning or ordering) or their recipes do not have step-step dependencies. In Figure 7, we showcase 4 examples of the annotated recipe step-step dependencies. We additionally use the prep-step dependency provided by the HD-EPIC dataset [24], and there are 728 prep-step segment dependencies in above 60 videos. To see prep-step annotation samples, please refer to HD-EPIC [24]. Overall, the reported Causality Violation Rate (CVR) is evaluated on 932 causal constraints across 60 videos combining both prep-step and step-step annotations. As there are two types of causal constraints in the annotation: prep-step and step-step (see Section 4.1 and Section C), we break down the Causal Violation Rate (CVR) according to the type of constraint violated. Furthermore, each violation error is classified into two categories: i) segment pair is reversed or ii) the cause is missing. We report these categories separately. Table 4 shows the CVR results breakdown. Naive HalfHalf and HEFT 1-min have lowest Miss errors by design, but have highest Reverse errors. On the other hand, HEFT GT start-end and Qwen2.5-VL-72B have lowest Reverse errors, but with high Miss errors. Relatively, Gemini 2.5 based methods are balanced in both Reverse and Miss. Among Gemini 2.5 based methods, + Goal-Only achieves the lowest total causal error, and our spatial prompt maintains comparable performance. In Figure 6, we normalise both the Reverse Error and the Miss Error by their respective total counts, and present their distributions across videos. Evidently our proposed prompt (ours) achieves the lowest rate in reversal and missed rate. Increasing the number of agents = 3 results in slight increase in the error rates. A. Additional Qualitative Figure and Videos We show the expanded version of Fig. 1 in Figure 5. Here, we show the detailed split of the same video for = 2 and = 3 resulting in the speed-ups reported in Fig. 1, along with the 3D location of the agents over time and their relevant key-frames. Additionally, we provide diverse set of long clips from various videos of HD-EPIC and EPIC-Kitchens in our demonstration video (https : / / youtu . be / 06EbtEhHYPY). For = 2, we have 3 videos from HD-EPIC and 2 videos from EPIC-Kitchens. For = 3, we have 2 videos from HD-EPIC and 1 video from EPICkitchens. Note that all the videos are played at 2 speed to mainitain reasonable length (5min19s) for the demonstration video. B. Spatial Prompt Variants Above shown in Table 1 and Table 2 in the main paper, Gemini 2.5 Pro struggles with spatial collision of agents before introducing spatial prompt. In addition to the zonerelated temporal segments introduced in 4.3, we experiment with different variants of providing spatial information to the VLM, specifically: (i) with raw trajectory, (ii) equal-sized zones with different sizes and (iii) using Gaussian Mixture Model (GMM) to cluster trajectory into zone with dynamic sizes. The proposed prompt uses zone size of 120x120cm. Table 3 show comparison results of HD-EPIC. Raw trajectory reduces the collision rate by small amount at the speed-up of 1.48. However, GMM with 5 components and equal-sized zone of 40 40cm achieve the same speedup with much less collision rate. Increasing zone sizes leads to more reduced collision rates, but the speed-up increases accordingly, i.e. you are speeding-up less. With larger zone size, the collision rate reduces more, but the speedup also increases: an agent is unable to work when another agent owns larger part of the space exclusively. We show how different parameter and variants all contributes to the reduced collision rate, while trading-off speed-up differently, within and across grouping variant. HD-EPIC results in Fig. 8. C. Recipe Causal Dependency Annotations While HD-EPIC annotates the steps of the recipe, these steps are not necessarily causal and could occur in parallel. We thus annotate the step-step dependencies manually. We 11 Figure 5. See also Video Supp: Top. Our input is single-person egocentric video. The camera wearer (shown also in orange in 3D) performs combination of cooking, washing up and ordering. Middle. 2-Body Parallel execution. Our method achieves 1.6x speed-up (from 19.8min to 10.4min) with 86% coverage. We show coloured segments (orange/blue) along with 3D representation of where the 2-agents are and their camera views (colour-bordered keyframes). P1 is mostly left to do the cooking, while P2 is washing up and ordering. Bottom. 3-Body parallel execution achieving further 2.3x speed-up to 7.5min. P2 is washing up while P3 is drying and storing things away. Figure 6. Recipe order distribution over methods. Method Coverage (%) Action Coverage (%) Speed-Up Collision Rate (%) Avg. Jump (m) OCR (%) CVR (%) +Goals-and-Constraints Raw Trajectory GMM (5 comps) GMM (10 comps) 40 40cm 80 80cm 120 120cm (Ours) 87.4 90.6 91.3 86.8 88.3 88.7 90. 88.1 91.3 91.4 87.0 88.8 89.1 91.3 1.59 1. 1.48 1.44 1.48 1.40 1.40 17.5 13.9 10.8 10.5 11.5 9.9 7. 0.47 0.48 0.57 0.55 0.48 0.48 0.55 0.93 1. 0.57 0.89 0.90 0.57 0.64 26.3 15.7 20.3 21.4 19.8 21.2 18. Table 3. Spatial Prompt variants on HD-EPIC. Method Prep-Step Error / 728 Step-Step Error / 204 Reverse Miss Combined Reverse Miss Combined Total / l P B - 2 Naive Half-Half HEFT 1-min HEFT GT start-end Qwen2.5-VL-72B Gemini2.5 (Base Prompt) + Goal-Only + Goals-and-Constraints + Spatial Prompt (ours) 3-Body Problem (ours) 76 86 15 0 12 32 40 27 45 0 5 514 225 291 73 111 127 76 91 529 225 303 105 151 124 172 59 52 4 0 10 35 41 27 43 0 9 133 72 116 32 61 56 59 61 137 72 126 67 102 64 99 135 152 666 297 429 172 253 188 271 Table 4. Detailed error breakdown on all 60 videos. Columns are grouped into Prep-Step and Step-Step error types, followed by the overall combined count. *Qwen2.5-VL-72B only produces results for 37 out of these 60 videos. 13 Figure 8. Speed-Up and Collision trade-off of different methods on the HD-EPIC dataset. Figure 7. Step-step causal dependency annotation samples. We show one frame in each step segment. We include text to explain the frame; these text snippets can be found in the full recipe annotation of the HD-EPIC dataset. 14 D. The Complete Prompt 1 # Task Overview 2 3 You are job planner for two-agent system. 4 You watch an egocentric video. 5 There are opportunities to parallelise the video to be performed by two agents (P1 and P2) efficiently. 6 You need to find such opportunities and make parallel execution out of the original video, while respecting all dependencies and avoiding spatial conflicts. 7 The parallel execution shouldnt have dependency disorder (e.g. pour milk before bottle is taken out of the fridge) and shouldnt have 3D spatial conflict (e.g. both accessing the fridge). 8 Tell me how to rearrange the clips from the original video into two streams, so that the video can be performed by two people jointly. 9 10 # Avoiding Spatial Conflict 11 12 The trajectory CSV file lists the segments where the original person stays in different zones: \"Z1\", \"Z2\", \"Z3\", etc. 13 If two tasks from the original video could semantically be done in parallel, but the CSV file shows they both occur in the same zone \"Z_n\", you *must* serialise them. 14 Schedule P1 to work only in \"Z_i\" durations and P2 to work only in \"Z_j\" zone such that Z_i != Z_j. 15 The spatial reasong should rely on the trajectory csv exclusively, do not use any knowledge from the video. 16 While splitting agents in different zones, remember to still respect the semantics of the task execution. 17 18 # Avoiding Semantic Dependency Disorder 19 20 Below are more examples of semantic dependency disorder: 21 1. When making coffee, pour hot water into the coffee before the coffee powder has been added to the mug. 22 2. Wash mixers head while the other agent is still using the mixer. 23 3. Uses the same trash bin at the same time. 24 25 # Covering every moment in the original video 26 27 Note that although this task aims to speed up the video, it should not skip any part of the original video. Every second of the original video needs to be covered, ensuring coverage = 100%. 28 29 # Format 30 31 Output json format: 32 With two keys \"P1\" and \"P2\", each contains list of jobs (dictionaries). 33 job is part of the original video performed by P1 orP2. 34 job is dictionary with fields: 35 \"new_start\": the new start time performed by this person, \"start\" and \"end\" are the old start/end time in the original video. \"text\" is the summary text describing the clip of the job. 36 Example: 37 38 { \"P1\": [ { 39 40 41 \"new_start\": \"00:00\", 42 43 44 45 46 48 49 50 51 52 54 55 56 57 58 60 61 62 63 64 66 67 68 69 70 72 73 74 75 76 }, { }, { } ], \"P2\": [ { }, { }, { } \"start\": \"00:00\", \"end\": \"00:26\", \"text\": \"walk in and take from fridge\", \"new_start\": \"00:26\", \"start\": \"00:51\", \"end\": \"01:14\", \"text\": \"use and put milk into fridge\" \"new_start\": \"00:49\", \"start\": \"01:22\", \"end\": \"1:59\", \"text\": \"take cereal and mix\" \"new_start\": \"00:00\", \"start\": \"00:26\", \"end\": \"00:44\", \"text\": \"put cloth and move bottles\", \"new_start\": \"00:19\", \"start\": \"00:44\", \"end\": \"00:51\", \"text\": \"take out cup and put onto the table\", \"new_start\": \"00:26\", \"start\": \"01:14\", \"end\": \"01:22\", \"text\": \"prepare spoon\", ] 78 79 } 80 81 82 # Fall-back to single-thread 83 84 When it is really hard to schedule more than one agent in constrained spatial space, safer solution with single agent is preferred than dangerous parallel plan with high potential spatial collision. 85 86 # Requirement Summary 87 88 Main requirement: Coverage. To avoid missing any segments of the full video (coverage < 100%), 89 keep in mind that every segment in the new parallel plan corresponds to segment in the original video, and the original segments add up should always cover 100% of the original duration. 90 91 Supplementary requirements: 92 1. Pay attention to Spatial Conflicts. For example, no simultaneous access to the fridge. better planning may use the same countertop at different times. 93 2. Pay attention to the Object dependency, e.g. place mug before pouring milk."
        }
    ],
    "affiliations": [
        "The University of Tokyo",
        "University of Bristol"
    ]
}