{
    "paper_title": "Do You Need Proprioceptive States in Visuomotor Policies?",
    "authors": [
        "Juntu Zhao",
        "Wenbo Lu",
        "Di Zhang",
        "Yufeng Liu",
        "Yushen Liang",
        "Tianluo Zhang",
        "Yifeng Cao",
        "Junyuan Xie",
        "Yingdong Hu",
        "Shengjie Wang",
        "Junliang Guo",
        "Dequan Wang",
        "Yang Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment."
        },
        {
            "title": "Start",
            "content": "Do You Need Proprioceptive States in Visuomotor Policies? Juntu Zhao1,2, Wenbo Lu2,4, Di Zhang2,5, Yufeng Liu1,2, Yushen Liang4, Tianluo Zhang4, Yifeng Cao2, Junyuan Xie2, Yingdong Hu2,3, Shengjie Wang4, Junliang Guo2, Dequan Wang1 and Yang Gao2,3 5 2 0 2 3 2 ] . [ 1 4 4 6 8 1 . 9 0 5 2 : r Abstract Imitation-learningbased visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0% to 85% in height generalization and from 6% to 64% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment. I. INTRODUCTION Imitation-learning-based visuomotor policies [1, 2, 3, 4, 5] have been widely used in robotic manipulation. Leveraging large-scale demonstration datasets [6, 7, 8, 9] and fine-tuning powerful pre-trained policies have enabled robots to achieve remarkable performance across diverse real-world tasks. For precise and reliable control, these visuomotor policies typically incorporate not only visual observations of the task environment but also proprioceptive state (hereafter referred to as state) inputs [3, 10, 11], such as end-effector poses and joint angles. The state inputs provide compact and the robots configuration, but accurate information about they also make the policy prone to overfitting by simply memorizing the training trajectories. Therefore it severely limits spatial generalization [12, 13, 14] if the training data lacks diversity [15]. In todays context, where collecting demonstration data with wide state coverage (i.e., diverse locations of task-relevant objects) is prohibitively spatial expensive, this has become critical bottleneck for the development of visuomotor policies. In this study, we propose to completely remove the state input in visuomotor policies to enhance their spatial generalization ability, hereafter referred to as State-free Policies. This design is built upon two conditions: Equal Contribution. This work was done during the internship at Spirit AI. Corresponding authors. Project leader. 1Shanghai Jiao Tong University, 2Spirit AI, 3Tsinghua University, 4New York University Shanghai, 5Tongji University. Fig. 1: With relative EEF action space and full task observation, State-free Policies demonstrate significantly improved spatial generalization ability compared to state-based policies. To handle complex scenarios, the full task observation is implemented by dual wide-angle wrist-cameras. Relative end-effector (EEF) action space [16]: The visuomotor policies predict relative displacements of the end-effector based on the current observation. Among different action spaces, the relative EEF action space most naturally supports the generalization of policies. Full task observation: Another key condition for effective State-free Policies is to ensure sufficient taskrelevant visual information, which we term full task observation. This enables visuomotor policies to fully see the task-relevant objects in the task. As shown in Figure 1, under the relative EEF action space, we ensure full task observation with dual wide-angle wristcameras (field of view approximately 120 120) mounted on the top and bottom of the end-effector, which provides sufficient task-relevant visual information for State-free Policies even in some complex scenarios. This mechanism of State-free Policies forces the policy to develop deeper understanding of the task environment rather than simply memorizing the trajectories, thereby enabling State-free Policies to achieve advantages that statebased policies cannot provide: Spatial Generalization: Since State-free Policies do not rely on state inputs, they avoid overfitting to the training trajectories. Therefore, they exhibit strong height and horizontal generalization abilities, where height refers to variations of the task-relevant objects location in the vertical direction, and horizontal refers to variations of the objects location in the 2D plane. Data efficiency: Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories. In contrast, removing the state input eliminates this dependence on trajectory diversity, allowing State-free Policies to be fine-tuned with less demonstration data. This reduces the cost of data collection, which is often major bottleneck in deploying real-world robots. with limited gains [14]. While such methods can be effective, they introduce additional cost or require non-trivial algorithmic design, highlighting the need for simpler and more practical solution to spatial generalization. Cross-embodiment adaptation: Since State-free Policies rely only on visual inputs and predict actions in the relative EEF space, they exhibit stronger cross-embodiment adaptation ability than state-based policies. They do not require additional adaptation to different state spaces, so the same task can be easily adapted to new embodiments with fewer fine-tuning steps. We have conducted extensive experiments across diverse range of tasks, robot embodiments, and policy architectures. In both real-world and simulation environments, State-free Policies achieve comparably great in-domain performance to state-based policies. Most importantly, when trained on strictly collected real-world demonstration data (i.e., the taskrelevant object location has constrained initial distribution range), State-free Policies exhibit significantly stronger spatial generalization ability than state-based policies. For further benefits, e.g., data efficiency and cross-embodiment adaptation ability, they also demonstrate obvious advantages over state-based policies, highlighting their potential for scalable and practical deployment in real-world robotic systems. II. RELATED WORKS Inputs."
        },
        {
            "title": "Visuomotor Policies and State",
            "content": "Imitationlearning-based visuomotor policies [1, 2, 3, 4, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26] have been widely adopted for robotic manipulation, achieving remarkable performance across diverse tasks. Recent advances such as ACT [2], Diffusion Policy [1], and π0 [3] highlight the effectiveness of combining large-scale trajectory datasets with powerful model architectures. common practice in these approaches is to incorporate proprioceptive state inputs alongside visual observations to stabilize training and improve fine-tuning efficiency [1, 2, 3, 4, 10, 11, 22]. While such state inputs provide compact information about the robots configuration, they also create shortcut for the policy: instead of reasoning from visual cues, the policy can simply memorize training trajectories tied to specific states [11, 12, 13, 14, 27]. As result, the policy overfits the training data and cannot adapt to spatial layout changes, limiting its spatial generalization. Improving Spatial Generalization. Several efforts focus on improving the spatial generalization of visuomotor policies. Data-driven approaches [6, 28] collect the demonstration data across diverse spatial configurations, but quickly become prohibitively expensive in the real world [29]. Simulation methods [30, 31, 32, 33] still often struggle with the sim-to-real gap. Representation-driven approaches build object-centric features to isolate task-relevant information, but often depend on complex perception pipelines [12, 34, 35, 36, 37, 38]. Architectural approaches introduce physical symmetries to promote invariance, but such assumptions often break under real-world conditions [39, 40]. Regularization approaches penalize absolute position encoding to encourage relational reasoning, but add training complexity III. METHODOLOGY A. Preliminary 1) Imitation-Learning-Based Visuomotor Policies: We consider visuomotor policies mapping raw observations to low-level control actions. At time t, the observation is ot (camera images and, conventionally, states), and the policy with trainable parameters θ is defined as: πθ(at ot), where at denotes the low-level control action. In imitation learning [41], the policy is trained on demonstration data by minimizing the negative log-likelihood of actions: LIL(θ) = (cid:88) log πθ(at ot). (1) (ot,at)D During deployment, πθ takes online observations ot and outputs the actions at, which are executed on the robot. 2) Action Representation Space: We consider two common action representation spaces: relative EEF action and relative joint-angle action. In the relative EEF action space, the end-effector pose at time is pt = (cid:2)xt, qt (cid:3), where xt R3 is the Cartesian position and qt SO(3) is the orientation. The policy outputs relative displacement: at = pt = (cid:2)xt, qt (cid:3), (2) where xt and qt denote the translation and rotation. The next end-effector pose is updated by: pt+1 = ptpt, where denotes composition of the translation and rotation. In the relative joint-angle action space, the policy drives the end-effector motion by predicting relative joint changes θt. In this case, the end-effector displacement depends on both θt and the current joint pose θt, i.e., pt = (θt, θt), (3) where denotes the forward kinematics mapping. B. Spatial Generalization Challenge of State Input Proprioceptive states provide direct and accurate robot configuration, but may act as shortcuts, where the policy directly associates absolute states with expert trajectories. to the training the policy tends to overfit Consequently, trajectories and fails to adapt to spatial layout changes. We validate this limitation with real-world height generalization evaluation as an example. Specifically, we collect Pick Pen into Pen Holder demonstrations (the overview is illustrated in Figure 3) at fixed 80 cm table height and fine-tune the π0 [3] policy using the relative EEF action space. As shown in the first 3 columns of Table I, state-based policies completely fail to generalize across different table heights. However, adding simple hacks (manually shifting the state according to table height) effectively improves the height generalization ability, indicating that state input is critical factor limiting spatial generalization. At the same TABLE I: Success rates in the exploration of table height generalization, using the Pick Pen into Pen Holder task as the representative example. Simple hacks and noise on the state improve the spatial generalization, indicating the state as bottleneck. This motivates removing the state input, as confirmed by empirical results. Wrist-camera setting Method on state Normal wrist-cam Normal wrist-cam w/ state State hack Normal wrist-cam Noise augmentation Normal wrist-cam Dual wide-angle wrist-cams w/o state w/o state h=80cm (In-domain) h=72cm (Out-of-domain) h=90cm (Out-of-domain) 30/30 0/30 0/30 30/30 30/30 30/30 30/30 24/30 11/ 30/30 30/30 17/30 30/30 30/30 29/30 time, adding random noise augmentation ([5 cm, 5 cm]) on the state height dimension improves height generalization without affecting in-domain performance, indicating that state input may not be necessary. These motivate us to consider removing the state input altogether. C. State-free Policies In this study, we propose to remove the state input from visuomotor policies, based on the relative EEF action space and full task observation. This improves the spatial generalization without requiring additional architectural changes or costly diverse data collection. 1) Relative EEF Action Space: To study spatial generalization, we begin by clarifying the action space. First, we exclude all absolute actions, where the policy predicts absolute poses, simply learning fixed mappings tied to training trajectories and thus failing to adapt to new spatial layouts. Here, we consider two common relative action spaces: relative EEF action and relative joint-angle action: Relative Joint-angle Action Space: In spatial generalization tasks such as height generalization, as the table height changes, when the end-effector is at the same relative position with respect to the table, the robot receives the same visual observations. In this case, the policy predicts the same qt, but the joint configurations qt are different. As shown in Eq. 3, this results in different end-effector displacements pt, leading to incorrect actions. Relative EEF Action Space: As shown in Eq. 2, the policy predicts relative end-effector motions directly from observations. The action pt depends only on the observations, not on the absolute pose, so identical observations yield the same displacement regardless of absolute robot poses. This invariance allows relative EEF actions to support spatial generalization across heights and horizontal positions. Since the relative EEF action space naturally supports the spatial generalization, our State-free Policies will be built upon this action representation space. 2) Full Task Observation: key condition for Statefree Policies is full task observation, ensuring the policy receives sufficient visual information. With state input, the policy can directly learn shortcut associations, such as what action to take once the robot reaches certain configuration, without relying on sufficient visual information. In contrast, without state input, the policy has to make decisions entirely Fig. 2: (a) Normal wrist-camera setting. Sometimes the target may not be visible. (b) Dual wide-angle wrist-cameras setting. It provides sufficient observations of the task. from visual information, which requires providing full taskrelevant visual observations, i.e., the full task observation. This motivates us to equip the end-effector with broader field of view for wide range of scenarios. Our camera system consists of an overhead camera and wrist-cameras. In the normal wrist-camera setting, single normal-view wrist-camera is mounted on top of the endeffector (in this study with view field 8758), as illustrated in Figure 2(a). To achieve full task observation, we adopt dual wide-angle wrist-cameras (field of view 120 120) mounted on the top and bottom of end-effectors, as illustrated in Figure 2(b). This setting expands the view and exposes the workspace beneath the end-effector (note that in tasks with simple scenarios, e.g., involving single task-relevant object, the normal wrist-camera setting can already be enough). Summary: As shown in the last 2 columns of Table I, in simple attempts at height generalization on task Pick Pen, the relative EEF action space and full task observation provide the improved height generalization of State-free Policies, significantly outperforming state-based policies. In the following sections, we will conduct extensive evaluations to validate and further analyze State-free Policies. Meanwhile, we demonstrate their further benefits, including higher data efficiency and better cross-embodiment adaptation. In addition, we also demonstrate an interesting finding that removing the overhead camera can further enhance the policys spatial generalization ability. IV. PERFORMANCE ACROSS VARIOUS TASKS To evaluate the performance of State-free Policies, we conduct extensive evaluations across various tasks. Performances Fig. 3: Overview of our robot embodiments and representative tasks, including Pick & Place tasks, more challenging Fold Shirt and Fetch Bottle task. These tasks span wide range of robot embodiments: 2 8 DoF human-like dual-arm robot, 2 7 DoF dual-arm Arx5 robotic arm system, and 26 DoF whole-body robot. of the state-based policy and several optimizing strategies on it can be found in Appendix Section IX-A. A. Setup 1) Task: Our real-world tasks include 3 Pick & Place tasks, more challenging Fold Shirt task, and complex task Fetch Bottle (on whole-body robot with torso, waist, and leg motions controlled by 6-dimensional torso pose vector, representing position and orientation in the EEF form). The detailed task descriptions are as follows: Pick Pen: Pick up pen from the table and place it into pen holder on the table. Pick Bottle: Grasp the bottle cap and remove the bottle from the step on the table. Put Lid: Pick up the lid from the table and accurately place it on large teacup on the table. Fold Shirt: Fold the shirt that is laid flat on the table. Fetch Bottle (whole-body): Open the refrigerator door, take out the bottle, and close the refrigerator door. As shown in Figure 3, we present the overviews of the robot embodiments and representative tasks in our evaluations. We also conduct evaluations in the simulation environment on the LIBERO benchmark [42], where we fine-tune the policy separately in each suite and subsequently evaluate it in the corresponding test suite. 2) Real-world Data: We employ professional data collectors to collect the real-world demonstration data using the teleoperation. For each Pick & Place task, 300 trajectory episodes, around 5 hours of data, are collected. For the challenging Fold Task and Fetch Bottle (whole-body) task, 10,000 episodes, around 80 hours of data, are collected. Importantly, during data collection, we fix the table height and limit the task-relevant object locations within constrained 2D range. Taking the task Pick Pen as an example, Fig. 4: Task-relevant object locations illustration in task Pick Pen. During training, the pen holder is fixed. For horizontal generalization, we keep the pen location range unchanged and shift the pen holder by 5 cm and 10 cm to compute the average success rate. as shown in Figure 4, in training data, the pen holder location is fixed, and in horizontal evaluation, we shift its location. This design ensures that the spatial generalization ability originates from the policy itself rather than from diverse data. 3) Evaluation Metric: We evaluate the spatial generalization of visuomotor policies along two dimensions: height and horizontal generalization. Each real-world evaluation consists of 30 trials, with success counted only if the entire trajectory is completed. trial is marked as failure if the policy takes no reasonable action within 30 seconds or if any action fails. a) Height Generalization Evaluation: The Pick & Place data are collected at 80 cm table height. The height generalization score is computed as the average success rate of the total 60 trials at 72 cm and 90 cm table heights. And as shown in Figure 3, since the Arx5 arms are fixed to the table and the refrigerator height cannot be adjusted, the Fold Shirt and Fetch Bottle (whole-body) tasks are not applicable for height generalization evaluation. b) Horizontal Generalization Evaluation: In the Pick & Place and Fetch Bottle (whole-body) tasks, the target objects (pen holder, step, large teacup, and refrigerator) are Fig. 5: The height and horizontal generalization (written as Gen.) performances across 3 real-world Pick & Place tasks. With full task observation, State-free Policies show significantly improved spatial generalization than state-based policies. shifted within 2D ranges of 5 cm and 10 cm, as illustrated by the Pick Pen example in Figure 4. In the Fold Shirt task, we evaluate by laterally shifting single Arx5 arm by 15 cm, as well as shifting both arms simultaneously by 15 cm in opposite directions. Each task is evaluated with 60 trials (two runs of 30), and the horizontal generalization score is the average success rate across them. 4) Model: In our main evaluations, we use π0 [3] for imitation learning, following its released fine-tuning recipe. π0 is widely regarded as one of the most powerful visuomotor policies in the community. In addition, our detailed analysis in Section V-C further evaluates different policy architectures, including ACT (Action-conditioned Transformer) [2], which employs action-conditioned attention to capture temporal dependencies, and Diffusion Policy [1], which models actions as distribution via diffusion dynamics. B. Real-world Evaluations"
        },
        {
            "title": "Here we report",
            "content": "the spatial generalization evaluations, including height and horizontal generalization, on 5 realworld tasks. And we will report their in-domain performance and the simulation evaluations in Appendix Section IX-B. In Figure 5, we report the height and horizontal generalization performance in 3 real-world Pick & Place tasks. Compared to state-based policies, State-free Policies exhibit significant improvement in both height and horizontal generalization: taking the Pick Pen task as an example, the success rate in height generalization rises from 0 to 0.98, and in horizontal generalization from 0 to 0.58. And compared with the normal wrist-camera setting, the height generalization success rate improves from 0.87 to 0.98, and the horizontal generalization from 0.27 to 0.58. TABLE II: The horizontal generalization performances across 2 challenging real-world tasks, Fold Shirt and Fetch Bottle (whole-body). State-free Policies still show significantly improved performance than state-based policies. Task name Fold Shirt Fetch Bottle w/ state (normal wrist-camera) w/o state (normal wrist-camera) 0.183 0.834 0.117 0.784 In task Fold Shirt, folding shirt is difficult due to the deformable nature of fabric which makes the folding manipulation challenging. And task Fetch Bottle (wholebody) is more difficult because the robots torso motions are not directly observable. As discussed in Section IVA.3, height generalization evaluation is not applicable to these two tasks. In addition, due to hardware limitations, the dual wide-angle wrist-cameras cannot be mounted in these embodiments. In Table II, we report the horizontal generalization performance on these two tasks. Even in these complex and challenging tasks, State-free Policies still achieve significantly stronger spatial generalization ability. Moreover, this also reflects that for simpler scenarios (i.e., with simple task-relevant objects), the normal wrist-camera setting can still provide full task observation. V. DETAILED ANALYSIS ON STATE-FREE POLICIES For deeper insights, we conduct more detailed analysis of State-free Policies, examining how their behavior varies under different conditions. In this section, we mainly focus on the Pick Pen task as the example, which is both intuitive and typical, making it well-suited for detailed analysis. Unless otherwise specified, all evaluations in this section use the π0 policy and the relative EEF action space. A. Action Representation TABLE III: Spatial generalization evaluation of State-free Policies using different action representations. Action space In domain Height Gen. Horizontal Gen. Relative EEF Absolute EEF Relative joint-angle Absolute joint-angle 1.0 1.0 1.0 1.0 0.984 0 0 0 0.584 0 0 0 As discussed in Section III-C.1, the relative EEF action space most naturally supports the generalization ability of State-free Policies. In this section, we evaluate alternative action representations, including the absolute EEF, both absolute and relative joint-angle action spaces. Evaluations are performed using the dual wide-angle wrist-cameras setting. As reported in Table III, the relative EEF action space achieves the best performance in both in-domain and spatial generalization settings, whereas others show disastrous performance in spatial generalization. These results highlight that the relative EEF action space most naturally supports the generalization ability of State-free Policies. VI. FURTHER BENEFITS OF STATE-FREE POLICIES In this section, we will demonstrate additional potential B. Full Task Observation advantages of State-free Policies. TABLE IV: Spatial generalization performances of State-free Policies on different task observation levels, implemented by varying the camera settings. Wrist-cam num Wrist-cam type Overhead cam Height Gen. Horizontal Gen. N/A Single Dual Single Dual Dual N/A Normal Normal Wide-angle Wide-angle Wide-angle N/A 0.217 0.867 0.920 0.917 0.984 1.0 0.133 0.267 0.400 0.500 0.584 1.0 Another key condition for stronger spatial generalization in State-free Policies is full task observation. Our camera system includes an overhead camera and wrist-cameras. As illustrated in Figure 2, each end-effector is equipped with two wide-angle wrist-cameras on the top and bottom. By cropping image regions or masking one of the inputs, we create different levels of task observation to demonstrate the critical role of full task observation in State-free Policies. As reported in Table IV, the spatial generalization ability of State-free Policies gradually improves as the field of view expands, indicating that full task observation is important for achieving strong generalization in State-free Policies. Moreover, an interesting finding shows that even without the overhead camera, the dual wide-angle wrist-cameras alone enable the best spatial generalization. This indicates that, in the current task, they provide completely full task observation for the entire trajectory, while the overhead camera is not only unnecessary but can even be harmful (we will discuss further in Section VII). Fig. 6: Evaluation success rates (in-domain) on the Pick Pen task with varying amounts of fine-tuning data. A. Higher Data Efficiency Even in in-domain settings, state-based policies require diverse demonstrations to avoid overfitting to specific trajectories, greatly raising data collection costs. While, State-free Policies are less prone to memorizing specific trajectories and can achieve comparable performance with fewer fine-tuning data, thereby enhancing data efficiency and practicality for their real-world deployment. We validate this on the in-domain Pick Pen task with dual wide-angle wrist-cameras, varying fine-tuning data to 300, 200, 100, and 50 episodes, and measuring after 2 and 4 fine-tuning epochs. As shown in Figure 6, reducing data leads state-based policies to overfit and lose success, while State-free Policies maintain much higher performance. C. Policy Architecture B. Better Cross-embodiment Adaptation TABLE V: Spatial generalization comparison of policies with and without state input, using different model structures. Model structure State input Height Gen. Horizontal Gen. π0 ACT Diffusion Policy w/ state w/o state w/ state w/o state w/ state w/o state 0 0.984 0 0.933 0 0.867 0 0.584 0.084 0.517 0 0. In addition to π0 policy, we also evaluate other policiy architectures without state input, including ACT and Diffusion Policy. All use the dual wide-angle wrist-cameras setting. As reported in Table V, the results are consistent across architectures: State-free Policies exhibit much stronger spatial generalization than state-based policies, indicating their effectiveness is independent of specific policy implementations, representing general and universal conclusion. We find that State-free Policies also benefit the crossembodiment fine-tuning. For state-based policies, crossembodiment adaptation requires aligning with new state space, and even with EEF-based states, differences in reference frame definitions across embodiments still create gaps. In contrast, State-free Policies avoid this issue: with similar camera setups, they only adapt to minor image shifts, enabling more efficient cross-embodiment fine-tuning. TABLE VI: Success rates in in-domain Fold Shirt task using the human-like robot. Each policy is fine-tuned from its corresponding checkpoint trained on Arx5 arms. State input Fine-tune 5k steps Fine-tune 10k steps w/ state w/o state 0.333 0.700 0.767 0.967 We validate this on the Fold Shirt task (in-domain setting). Policies are first trained on dual-arm Arx5 (the EEF space is in table frame) and then adapted to human-like dual-arm robot (the EEF space is in robot-centric frame). We collect 100 shirt-folding demonstrations on the human-like robot and fine-tune the π0 policy with and without state input, each initialized from its corresponding Arx5 checkpoint. As shown in Table VI, State-free Policies adapt much faster across embodiments, achieving substantially higher success rates than state-based policies under the same fine-tuning epochs. This indicates that State-free Policies have better cross-embodiment ability than state-based policies. VII. RETHINKING THE OVERHEAD CAMERA After removing the state input that limits spatial generalization, we consider the overhead camera might as another potential bottleneck. Changes in object locations can induce distribution shifts in overhead camera images, and in extreme cases (e.g., 100 cm table height) degrade performance. In contrast, since the end-effector can move along with the taskrelevant object, the wrist-camera can still capture observations consistent with those in training, avoiding the out-ofdomain issues. Given that the dual wide-angle wrist-cameras already provide full task observation, the overhead camera may not only be unnecessary but even harmful. TABLE VII: Success rates with and without the overhead camera in more challenging Pick Pen generalization scenarios, with dual wide-angle wrist-cameras. Overhead camera input Table height 100cm Raising pen holder height Moving pen holder 20 cm w/ overhead cam w/o overhead cam 0 1.0 0.467 0.867 0 0.800 We evaluate this through experiments on the Pick Pen task under more challenging scenarios: Raising the table height to 100 cm. Raising the pen holder to double its height, changing its relative hieght with respect to the table. Moving the pen holder 20 cm away from its position in training data. As reported in Table VII, State-free Policies with the overhead camera show terrible performance across all 3 more challenging scenarios. While without the overhead camera, success rates remain consistently high, confirming that dual wide-angle wrist-cameras alone are sufficient, while the overhead view introduces harmful shifts. This finding motivates us to rethink sensor design, perhaps removing the overhead camera, for future visuomotor policies. VIII. CONCLUSION In this study, we propose the State-free Policies, under two conditions: the relative EEF action space and the full task observation through sufficiently comprehensive visual information. Without state input, these policies maintain perfect in-domain performance while achieving significant improvements in spatial generalization. State-free Policies also reduce the costly real-world data need, enable more efficient cross-embodiment adaptation, and inspire new directions in future sensor design. Our findings shed new light on how State-free Policies can serve as foundation for building more generalizable robotic learning systems. Limitation. State-free Policies also remain some limitations. First, vision-only policies might exhibit sensitivity to the background: changing the background (e.g., relocating the robot and table) may require additional fine-tuning to restore performance. And in dual-arm settings, if only one arm is used for working, distribution shifts in the unused arms visual input may occasionally lead to unexpected movements of the unused arm. IX. APPENDIX A. Challenges With State Input TABLE VIII: Height generalization performance of statebased policies under different optimization strategies. Optimization strategy Height Generalization w/o state input Random noise augmentation Diverse data collection Task-mixed fine-tuning LoRA fine-tuning 0.983 0.633 0.117 0 0 In the Pick Pen task, we evaluate several strategies to improve height generalization of state-based policies with dual wide-angle wrist-cameras, including: (1) adding random noise ([5 cm, 5 cm]) to state heights, (2) collecting data at table heights 7584 cm (30 episodes each 1 cm interval), (3) task-mixed training on Pick Pen and Pick Bottle, and (4) LoRA fine-tuning. As reported in Table VIII, none of these methods yields fundamental improvement, confirming that state inputs inherently limit spatial generalization. B. In-domain Performance In this section, we will report the in-domain performance of both state-based policies and State-free Policies in our real-world tasks. At the same time, we also report their performance on the LIBERO benchmark. TABLE IX: In-domain success rates in different tasks using policies with and without state input. Task name w/ state input w/o state input Pick Pen Pick Bottle Put Lid Fold Shirt Fetch Bottle (whole-body) 1.0 1.0 1.0 1.0 0. 1.0 1.0 1.0 0.967 0.933 In Table IX, we report the in-domain success rates for different real-world tasks using policies with and without state input, with dual wide-angle wrist-cameras. Even after removing the state input, the policies still maintain comparable performance on in-domain tasks, as the distribution of visual observations remains fully consistent with training. In the simulation environment, we compare the in-domain performance of the π0 policy with and without state input on the LIBERO benchmark. As reported in Table X, even TABLE X: Simulation evaluations on the Libero benchmark, using policies with and without state input. Evaluation suite w/ state input w/o state input Libero Goal Libero Object Libero Spatial Libero 10 Average 0.942 0.964 0.968 0.876 0.938 0.956 0.962 0.976 0. 0.945 in simulation environments with severely limited camera views, State-free Policies achieve performance as perfect as state-based policies, and in some cases even surpass them, demonstrating the strong practicality of State-free Policies."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, p. 02783649241273668, 2023. [2] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained bimanual manipulation with low-cost hardware, arXiv preprint arXiv:2304.13705, 2023. [3] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al., π0: vision-languageaction flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550, arXiv preprint ARXIV.2410.24164, 2024. [4] J. Barreiros, A. Beaulieu, A. Bhat, R. Cory, E. Cousineau, H. Dai, C.-H. Fang, K. Hashimoto, M. Z. Irshad, M. Itkina, et al., careful examination of large behavior models for multitask dexterous manipulation, arXiv preprint arXiv:2507.05331, 2025. [5] Q. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn, A. Handa, M.-Y. Liu, D. Xiang, G. Wetzstein, and T.-Y. Lin, Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models, 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 17021713, 2025. [6] A. ONeill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain, et al., Open x-embodiment: Robotic learning datasets and rt-x models: Open xembodiment collaboration 0, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 68926903. [7] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al., Droid: large-scale in-the-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. [8] H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. HansenEstruch, A. W. He, V. Myers, M. J. Kim, M. Du, et al., Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning. PMLR, 2023, pp. 17231736. [9] J. Jones, O. Mees, C. Sferrazza, K. Stachowicz, P. Abbeel, and S. Levine, Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding, 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 5961 5968, 2025. [10] F. Torabi, G. Warnell, and P. Stone, Imitation learning from video by leveraging proprioception, arXiv preprint arXiv:1905.09335, 2019. [11] F. Kuang, J. You, Y. Hu, T. Zhang, C. Wen, and Y. Gao, Adapt your body: Mitigating proprioception shifts in imitation learning, arXiv preprint arXiv:2506.23944, 2025. [12] Y. Chen, Y. Zhang, G. Durso, N. Lawrance, and B. Tidd, Improving generalization ability of robotic imitation learning by resolving causal confusion in observations, arXiv preprint arXiv:2507.22380, 2025. [13] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al., Transporter networks: Rearranging the visual world for robotic manipulation, in Conference on Robot Learning. PMLR, 2021, pp. 726747. [14] Z.-H. Yin, Y. Gao, and Q. Chen, Spatial generalization of visual imitation learning with position-invariant regularization, in RSS 2023 Workshop on Symmetries in Robot Learning, 2023. [15] F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao, Data scaling laws in imitation learning for robotic manipulation, ArXiv, vol. abs/2410.18647, 2024. [16] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake, and S. Song, Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots, arXiv preprint arXiv:2402.10329, 2024. [17] J. Bjorck, F. Castaneda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang, et al., Gr00t n1: An open foundation model for generalist humanoid robots, arXiv preprint arXiv:2503.14734, 2025. [18] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu, Rdt-1b: diffusion foundation model for bimanual manipulation, arXiv preprint arXiv:2410.07864, 2024. [19] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in Conference on Robot Learning. PMLR, 2023, pp. 21652183. [20] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al., Rt-1: Robotics transformer for real-world control at scale, arXiv preprint arXiv:2212.06817, 2022. [21] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, et al., Octo: An open-source generalist robot policy, arXiv preprint arXiv:2405.12213, 2024. [22] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al., Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. [23] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu, Rdt-1b: diffusion foundation model for bimanual manipulation, ArXiv, vol. abs/2410.07864, 2024. [24] F. Lin, R. Nai, Y. Hu, J. You, J. Zhao, and Y. Gao, Onetwovla: unified vision-language-action model with adaptive reasoning, ArXiv, vol. abs/2505.11917, 2025. [25] D. Zhang, C. Yuan, C. Wen, H. Zhang, J. Zhao, and Y. Gao, Kinedex: Learning tactile-informed visuomotor policies via kinesthetic teaching for dexterous manipulation, ArXiv, vol. abs/2505.01974, 2025. [26] Y. Wang, H. Zhu, M. Liu, J. Yang, H.-S. Fang, and T. He, Vq-vla: Improving vision-language-action models via scaling vector-quantized action tokenizers, ArXiv, vol. abs/2507.01016, 2025. [27] Q. Yang, M. C. Welle, D. Kragic, and O. Andersson, S2-diffusion: to category-level skills in robot Generalizing from instance-level manipulation, arXiv preprint arXiv:2502.09389, 2025. [28] T. Z. Zhao, J. Tompson, D. Driess, P. Florence, K. Ghasemipour, C. Finn, and A. Wahid, Aloha unleashed: simple recipe for robot dexterity, in Conference on Robot Learning, 2024. [29] P. Mitrano and D. Berenson, Data augmentation for manipulation, arXiv preprint arXiv:2205.02886, 2022. [30] W. Zhao, J. P. Queralta, and T. Westerlund, Sim-to-real transfer in deep reinforcement learning for robotics: survey, 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 737744, 2020. [31] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, Domain randomization for transferring deep neural networks from simulation to the real world, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2330, 2017. [32] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, and G. State, Isaac gym: High performance gpu-based physics simulation for robot learning, ArXiv, vol. abs/2108.10470, 2021. [33] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. teusz Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. A. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. M. Zhang, Solving rubiks cube with robot hand, ArXiv, vol. abs/1910.07113, 2019. [34] A. Chapin, E. Dellandrea, and L. Chen, Is an object-centric representation beneficial for robotic manipulation? arXiv preprint arXiv:2506.19408, 2025. [35] D. Emukpere, R. Deffayet, B. Wu, R. Bregier, M. Niemaz, J.-L. Meunier, D. Proux, J.-M. Renders, and S. Kim, Disentangled objectcentric image representation for robotic manipulation, arXiv preprint arXiv:2503.11565, 2025. [36] N. Hansen and X. Wang, Generalization in reinforcement learning by soft data augmentation, in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 13 61113 617. [37] L. Yen-Chen, A. Zeng, S. Song, P. Isola, and T.-Y. Lin, Learning to see before learning to act: Visual pre-training for manipulation, in 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 72867293. [38] Z. Mandi, F. Liu, K. Lee, and P. Abbeel, Towards more generalizable one-shot visual imitation learning, in 2022 International conference on robotics and automation (ICRA). IEEE, 2022, pp. 24342444. [39] D. Wang, M. Jia, X. Zhu, R. Walters, and R. Platt, On-robot learning with equivariant models, arXiv preprint arXiv:2203.04923, 2022. [40] J. Seo, S. Yoo, J. Chang, H. An, H. Ryu, S. Lee, A. Kruthiventy, J. Choi, and R. Horowitz, Se (3)-equivariant robot learning and control: tutorial survey, International Journal of Control, Automation and Systems, vol. 23, no. 5, pp. 12711306, 2025. [41] M. Bain and C. Sammut, framework for behavioural cloning. in Machine intelligence 15, 1995, pp. 103129. [42] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone, Libero: Benchmarking knowledge transfer for lifelong robot learning, ArXiv, vol. abs/2306.03310, 2023."
        }
    ],
    "affiliations": [
        "New York University Shanghai",
        "Shanghai Jiao Tong University",
        "Spirit AI",
        "Tongji University",
        "Tsinghua University"
    ]
}