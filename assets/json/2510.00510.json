{
    "paper_title": "JoyAgent-JDGenie: Technical Report on the GAIA",
    "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 0 1 5 0 0 . 0 1 5 2 : r JoyAgent-JDGenie: Technical Report on the GAIA JINGDONG CHO-EI Team"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly deployed as autonomous agents for solving complex real-world tasks. Yet, existing systems often emphasize isolated improvementssuch as expanding toolkits, refining prompts, or adjusting planning heuristicswithout unifying framework to ensure robustness, adaptability, and reproducibility. In this work, we present generalist agent architecture that integrates three core components: (i) collective multi-agent framework of PlanExecute and ReAct agents coordinated through critic model voting, balancing stability with flexibility; (ii) hierarchical memory system combining working, semantic, and procedural layers to enable long-horizon continuity and adaptive control; and (iii) refined tool suite focused on search, code execution, and multimodal parsing, exposed through schema-consistent and auditable interfaces. Evaluated on the GAIA benchmark [60], our framework achieves 75.2 Pass@1 and 82.4 Pass@3 on validation, and 67.1a Pass@1 on test set, surpassing open-source baselines and approaching the performance of proprietary systems. These results underscore the importance of system-level integration for advancing generalist agents. Beyond immediate gains, our work highlights path toward scalable, resilient, and adaptive AI assistants capable of operating across domains and tasks. Date: October 2, 2025 Code: https://github.com/jd-opensource/joyagent-jdgenie ahttps://huggingface.co/spaces/gaia-benchmark/leaderboard"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of Artificial General Intelligence (AGI) increasingly centers on agents: autonomous systems that can plan, reason, and act across diverse real-world tasks. Unlike conventional large language models (LLMs), which excel primarily at text generation, agentic systems must integrate reasoning with external actions, leverage memory across interactions, and adapt dynamically to unforeseen contexts. This shift positions LLM-based agents not just as conversational tools, but as general-purpose assistants capable of problem solving in open-ended, multi-modal, and continuously evolving environments. Over the past two years, wide range of agent frameworks have emerged. Early role-based multi-agent systems such as CAMEL [47] and MetaGPT [32] demonstrated that structured collaboration could elicit disciplined reasoning behaviors, while more recent initiatives like OAgents [112], AWorld [4], Alita [67], and OWL [33] highlight new design philosophies: empirical scaling studies, co-evolutionary training loops, meta-tool creation, and modular planners. Benchmarks such as GAIA [60] have revealed both the promise and the limitations of these systemswhile they achieve notable progress, many approaches remain brittle, either due to over-reliance on hand-crafted workflows or lack of stability under real-world uncertainty. In this work, we address these challenges by proposing system-level framework that integrates three core 1 Figure 1 The overview of the fusion agent architecture. components: (i) heterogeneous ensemble of agents spanning both PlanExecute and ReAct paradigms, coordinated through posterior voting to balance reliability and adaptability; (ii) hierarchical memory design that combines working, semantic, and procedural layers to enable long-horizon continuity and adaptive control; and (iii) refined tool ecosystem emphasizing search, code execution, and multimodal parsing, each wrapped into schema-consistent and auditable interfaces. Together, these design choices yield robust performance gains on the GAIA benchmark, setting state-of-the-art results among open-source systems and narrowing the gap to proprietary frameworks."
        },
        {
            "title": "2 Related Work",
            "content": "The study of LLM-based agents has progressed from role-specialized collaborations to increasingly modular systems oriented toward general assistance. Early frameworks such as CAMEL [47] and MetaGPT [32] showed that assigning distinct roles and procedures elicits more disciplined reasoning, especially in structured domains like software engineering. As evaluations moved toward open-ended tasks, recent efforts emphasized broader empirical analysis and unified infrastructures: OAgents [112] systematically studies design choices for effective agents at scale, while AWorld [4] provides unified playground that supports both computerand phone-use tasks, encouraging iteration on the full stack of planning, tools, and evaluation. In parallel, Alita [67] advances the idea that tools themselves can be dynamically constructed and composed, reducing reliance on heavy predefinition and enabling self-evolution of agent capabilities. Complementary platforms (e.g., AutoAgent [83]) lower the barrier to assembling agents and workflows, making orchestration more accessible without sacrificing modularity. second thread concerns the substrate that turns models into reliable systems: tools, retrieval, and memory. Toolformer [73] demonstrated that models can self-learn to call APIs, and surveys such as Qin et al.[66] underscored that robust tool interfaces and execution traces are core to stability. Hierarchical research agents (e.g., Open Deep Research[43] and DeepResearchAgent [104]) pair retrieval-centric planning with iterative decomposition, while memory mechanisms progress from reflective feedback [75] to hierarchical designs like A-Mem [100] that separate working, semantic, and procedural layerssupporting long-horizon continuity and adaptive control. These system utilities collectively motivate architectures where communication is structured, tool calls are auditable, and prior experience can be retrieved and reused. Finally, generalization across domains has become central goal. OWL [33] tackles this by decoupling domainagnostic planning from domain-specific execution (WORKFORCE), training only the plannervia supervised trajectories and reinforcement learningto transfer across new environments without retraining worker 2 agents. Orthogonal efforts (e.g., AgentRefine [23], TapeAgents [6], and MiroFlow [84]) study reproducibility, refinement, and stability at scale. Benchmarks such as GAIA [60] and BrowseComp [95] crystallize these trends by testing multimodal reasoning, browsing, and execution, revealing that while proprietary frameworks (e.g., Deep Research [61], h2oGPTe [29]) are strong, open-source systems are rapidly closing the gap. Distinct from the above, our work contributes system-level design that intentionally fuses complementary agentic paradigms with hierarchical memory and statistically validated tool suite, aiming for robust gains under real-world constraints."
        },
        {
            "title": "3 Architecture",
            "content": "Building effective generalist agents requires more than scaling language models: it demands careful integration of planning strategies, memory mechanisms, and tool infrastructures into coherent system. Prior work has shown that isolated improvements in any one componentsuch as adding new tools, refining prompts, or adjusting planning heuristicsoften lead to limited or unstable gains. Our approach instead emphasizes system-level design, where diverse agent paradigms, structured memory hierarchies, and statistically validated tool sets are woven into unified framework. This section details our methodology, beginning with agent architectures, followed by memory design, and concluding with tool integration."
        },
        {
            "title": "3.1 Agents",
            "content": "We design our framework around heterogeneous ensemble of agents that reflects two complementary paradigms of agentic reasoning. The first follows the PlanExecute principle, where high-level plan is generated in advance, executed step by step, and periodically revised through lightweight reflection. This structure provides low-variance pipeline, ensuring that tasks with deterministic decomposition can be executed reliably. The second follows the ReAct paradigm, in which reasoning and action are tightly interleaved, allowing the agent to replan dynamically at every step. Although this style exhibits higher variance, it excels in exploratory tasks that require adaptive reasoning under uncertainty. To reconcile these statistical trade-offs, we implement hierarchical multi-agent ensemble. Supervisor Agent built upon the PlanExecute framework ensures global coherence of the solution trajectory, while multiple Single Agents based on ReAct provide step-level adaptability. At inference, their outputs are aggregated through posterior voting, which can be configured with 3 or 5 models depending on resource availability. For instance, three-way ensemble may combine two ReAct agents with PlanExecute agent. Empirically, this mixture consistently improves pass rates across GAIA tasks, highlighting the benefit of balancing bias and variance through ensemble decision-making. Inter-agent interaction is governed by structured communication protocol. Each agent produces not only candidate solution but also message object that records reasoning chains, tool invocations, and intermediate evidence. These messages are transmitted through central communication hub, stored in the working memory buffer, and made accessible to other agents for cross-validation. By constraining communication to structured formats rather than free-form dialogue, we ensure consistency and prevent uncontrolled conversational drift. This architecture resembles cooperative yet disciplined debate, where agents can critique or support one another proposals, leading to more reliable outcomes."
        },
        {
            "title": "3.2 Memory",
            "content": "Memory is central component that underpins both long-term continuity and short-term adaptability of our framework. We design hierarchical memory system consisting of three layers. Working Memory stores the live execution context, including current plans, intermediate tool outputs, and exchanged messages between agents. Semantic Memory records the trajectory of completed tasks, including successes, failures, and decision rationales, compresses episodic traces into distilled knowledge units via summarization and embedding, ensuring that relevant lessons can be retrieved even when raw histories are prohibitively long. 3 Procedural Memory is embedded in the form of finely tuned system prompts. These prompts encode guidelines such as how to prioritize information sources, when to replan, or how to handle conflicting evidence. Unlike static instruction prompts, our procedural memory is dynamically adjusted based on accumulated experience. During inference, retrieval from long-term memory is mediated by semantic similarity search, ensuring that agents can access precedent cases that align with the current task. Retrieved items are then injected into working memory as auxiliary context. In addition, procedural memory functions as meta-controller, shaping how agents interpret retrieved traces and adapt their planning strategies. This combination enables unbounded historical continuity, where the agent retains identity and knowledge across arbitrarily extended interactions without overwhelming the underlying LLM backbone."
        },
        {
            "title": "3.3 Tools",
            "content": "Tool design and integration form the backbone of the agents factual acquisition capacity. Rather than maximizing the number of available tools, we identify and refine the classes of tools that statistically contribute most to task success. Our final tool suite centers around three categories: search, code execution, and local multimodal parsing. The search subsystem employs multi-source aggregator that queries Google, Bing and DuckDuckGo, supplemented with domain-specific interfaces such as Wikipedia search, Arxiv advanced retrieval, and multiple GitHub search APIs. To avoid brittle dependence on single provider, queries are reformulated via reflectionexpansion loop, where the agent first analyzes ambiguities, then generates alternative formulations with morphological and semantic variants. Retrieved documents are parsed using minimalist browsing tool set restricted to Search, Visit, and Read, which reduces error propagation from overly complex navigation. The code execution environment is implemented as secure Python sandbox. Tool calls follow uniform API: the agent generates structured code snippets, which are executed in isolation, and the execution traces are automatically stored in working memory. This allows downstream agents to reason not only over outputs but also over execution logs, supporting trace-based debugging and iterative refinement. For multimodal parsing, we introduce 17 specialized interpreters capable of handling PDFs, spreadsheets, presentations, audio, video, and image files. Each interpreter exposes lightweight interface (e.g., parse_pdf, extract_audio_transcript, analyze_image) that returns structured outputs rather than free-form text, enabling downstream reasoning to operate on consistent schemas. Crucially, these interpreters integrate directly into the communication hub: parsed content is added to working memory, making it accessible to all collaborating agents. The combination of carefully chosen search, code, and multimodal tools results in 3060% gains on PlanExecute baselines, establishing robust substrate upon which more advanced ensemble and reinforcement strategies can be layered."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "Dataset. GAIA[60] is benchmark designed to evaluate general-purpose AI assistants through 300 test and 165 validation real-world, scenario-based questions covering daily tasks, tool usage, reasoning, multimodal inputs, and web browsing. While these tasks may appear straightforward for humans, they remain highly challenging for advanced AI systems. Each question is associated with unique ground-truth answer, and model performance is measured using exact match accuracy. Metrics. We adopt the evaluation protocol of the GAIA benchmark, which relies on exact match accuracy. The main metric is Pass@N, defined as the probability that at least one correct solution appears among independent execution. This metric, commonly used in tasks like code generation, captures whether the model can generate valid solution at least once. Unless otherwise specified, our experiments report the average Pass@1 score, indicating the models ability to produce correct answer in single task run. 4 Baselines. For comprehensive evaluation, we compare our system against various baselines from three primary types: agentic models (Search-o1-32B, WebThinker-32B, WebDancer-32B, WebShaper-32B); closedsource frameworks (Langfun [65], TraseAgent [88], Deep Research [61], h2oGPTe [29], and Desearch [2]); and open-source systems (OWL [33], TapeAgent [6], AutoAgent [83], Open Deep Research [43], Smolagents [72], OAgent [112], and MiroFlow [84]). This selection captures broad range of the latest developments in both proprietary and open multi-agent systems, establishing robust benchmark for assessing the performance."
        },
        {
            "title": "4.2 Main Results",
            "content": "The results presented in Table 1 offer several key insights into the performance of various agent frameworks on the GAIA benchmark. Our proposed method achieved an average score of 75.2 at Pass@1 and 82.4 at Pass@3, demonstrating competitive results against all other evaluated frameworks, including both closed-source and open-source alternatives. This outcome underscores the robustness and effectiveness of our agents design. For Level 1 tasks, our method achieved score of 86.8, matching the top-tier performance and demonstrating the reliability of our low-level agents and their underlying system utilities. Compared to leading closed-source solutions such as Langfun (71.5) and MiroFlow (74.5), our approach shows substantial improvements in overall average performance and maintains superior accuracy across both Level 1 and Level 2 tasks. Notably, the highest-performing solutions predominantly leverage Claude-family models, underscoring the importance of foundation model selection. These results collectively validate the effectiveness of our framework for general-purpose agent applications. We publicly run our agent against GAIA testset, and obtain relatively high score of 67.1. Please refer to GAIAs official leaderboard1. Regarding Open Deep Research [43] and Smolagents [72], their reported results were directly adopted from OAgents [112] due to the substantial computational resources required for replication. Table 1 Performance of various agent projects on the GAIA benchmark. Pass@1 Pass@ Level 1 Level 2 Level 3 Model Family Framework Agentic Model Search-o1-32B WebThinker-32B WebDancer-32B WebShaper-32B 39.8 48.5 51.5 53. 64.1 61.2 Closed-source Agent Frameworks Langfun TraseAgent DeepResearch h2oGPTe Desearch 71.52 70.30 67.36 63.64 56.97 Open-source Agent Frameworks"
        },
        {
            "title": "OWL\nTapeAgents\nAutoAgent\nOpen Deep Research\nSmolagents\nOAgent\nMiroFlow",
            "content": "Ours 69.1 55.8 55.2 55.2 49.7 66.7 74.5 75.2 73.9 82.4 82.4 53.8 56.4 61.5 69. 83.02 83.02 74.29 67.92 71.70 84.9 71.7 71.7 67.9 54.7 83.0 - 86.8 34.6 50.0 50.0 50.0 68.60 69.77 69.06 67.44 58.14 67.4 53.5 53.4 53.5 53.5 74.4 - 77.9 16.7 16.7 25.0 16.6 QwQ-32B QwQ-32B QwQ-32B QwQ-32B 57.69 46.15 47.60 42.31 23.08 GPT-4o Claude-3-7 etc. Claude etc. - Claude-3.5 42.3 30.8 26.9 34.6 26.9 53.9 - 42.3 Claude-3-7 etc. Claude-3-7 etc. Claude-3-5 etc. OpenAI o1 Openai o1 etc. Claude-3-7 etc. Claude-3-7 etc. Claude-4 + o4-mini 1https://huggingface.co/spaces/gaia-benchmark/leaderboard"
        },
        {
            "title": "4.2.1 Exploratory Evaluations",
            "content": "Agent Pattern We experimented with various Agent patterns, including Multi-Agent with Plan-Executor and Single Agent with ReAct [103], corresponding to Circle 1 and Circle 2 in Fig 1, respectively. For the Single Agent approach, we adopted basic ReAct pattern, providing all tools (excluding browser-use tools) and increasing the maximum execution steps. Surprisingly, this simple structure did not exhibit performance collapse; instead, it achieved the highest performance of 71.5 under the non-fusion approach. While its performance on Level 3 problems was lower than other MultiAgent methods, its superior performance on Level 1 problems improved the overall average performance. Table 2 Performance comparison of different system architecture on GAIA benchmark. The Fusion refers to fusing Single and Multiple (3) with an additional critic model."
        },
        {
            "title": "Model",
            "content": "Average Level 1 Level 2 Level 3 Single Multiple (2) Multiple (3) Multiple (4-A) Multiple (4-B) Fusion 71.5 69.9 70.3 52.7 58.8 75.2 84.9 80.6 81.1 56.6 60.3 86.6 75.6 74.4 74.4 58.1 65.1 77.9 30.8 33.5 34.6 26.9 34.3 42. For the Multi-Agent approach, we constructed four different types of agents with distinct roles: Plan Agent responsible for high-level task planning, Retrieval Agent for web information retrieval, Logic Agent for complex reasoning and code generation, and Browser Agent for web page interaction. Different agents register different tools according to their roles. Additionally, all agents operate as CodeAgent, completing tool usage and inter-agent communication through Python code execution. By combining different agents, we formed Multi-Agent systems with various architectures. Specifically, Multiple (2) represents Plan + Retrieval Agent, Multiple (3) represents Plan + Retrieval + Logic Agent, and Multiple (4/5) represents systems using all agents with structures shown in Fig 1. We found that without browser-use, MultiAgent can significantly improve performance on Level 3 problems, but performance degrades on simple Level problems. After introducing the browser agent, the system performance exhibits significant deterioration. For the Fusion method, we selected Single Agent and Multiple (3) to combine the advantages of both architectures. We additionally incorporated Critic model that performs comparative analysis of execution trajectory segments from both systems and provides final answers while strictly adhering to answer formats. As shown in Table 2, the fusion approach achieved more accurate generation through comparative analysis. Base Model LLMs serve as the brain of Agent systems, and we compared the impact of different foundation models on Agent system performance. Since we adopted the CodeAgent execution approach, the results reflect coding capabilities rather than agentic tool-calling abilities. Consistent with results from other open-source frameworks, we found that models from the Claude family performed best, with Claude-4-sonnet and Claude3.7-sonnet achieving average scores of 75.2 and 68.3, respectively. Additionally, the thinking model o4-mini outperformed the non-thinking model gpt-4.1. In addition, we observe that recent progress in enhancing agentic capabilities of open-source models through reinforcement learning has been rapid, achieving high scores on the GAIA benchmark and substantially improving efficiency. This provides highly promising direction for our future work. Search Engine Web search is significant for LLM-agents to obtain external information beyond their knowledge boundaries. However, we found that different search engines and search engine service APIs have substantial impacts on task performance. We employed Google, Bing, DuckDuckGo, and their aggregated results (MultiSource) respectively. As shown in Table 4, Agent with Google achieved the highest score of 75.2. Beyond 6 Figure 2 The distribution between original level and reassigned level. Table 3 Performance comparison of various base models on GAIA benchmark. All results are obtained using information retrieved from Google Search."
        },
        {
            "title": "Model",
            "content": "Average Level 1 Level 2 Level 3 WebShaper-32B WebShaper-72B GPT-4.1 o4-mini Claude-3.7-sonnet Claude-4-sonnet 53.3 60.1 55.8 66.1 68.3 75.2 69.2 69. 66.0 79.2 83.1 86.6 50.0 63.4 58.1 68.6 70.6 77.9 16.6 16.6 26.9 30.8 30.8 42.3 Table 4 Performance of various search engine settings on GAIA. Note that Multi-Source refers to combine the above search engines. Search Method Average Level 1 Level 2 Level 3 Google Bing DuckDuckGo Multi-Source 75.2 58.8 70.9 68.5 86.6 62.3 81.1 75.5 77.9 64.0 73.3 73.3 42.3 34.6 42.3 38. differences in search result entries, possible explanation is that Google (supported by SerpAPI) provides more fine-grained filtering condition settings, including date, location, specific categories etc. For the GAIA benchmark, such fine-grained conditional filtering is essential."
        },
        {
            "title": "4.2.2 Level Debias",
            "content": "The difficulty level assignments for tasks in the GAIA dataset rely as proxy on the number of steps and tools used by our annotators when crafting the questions. However, there exists gap between human behavior and machine behavior, where tasks that are simple for humanssuch as visual recognition and browser operationsare indeed more challenging for machines. Therefore, we have reclassified them into four levels based on problem-solving success rates under our agent system, as shown in Fig 2."
        },
        {
            "title": "5 Conclusion",
            "content": "We present unified framework for building effective generalist agents through the integration of heterogeneous agent paradigms, hierarchical memory, and validated tool suite. Our design demonstrates that ensemble methods combining PlanExecute and ReAct agents achieve both reliability and adaptability, while structured communication and layered memory maintain continuity across extended interactions. By curating essential tool categoriessearch, code execution, and multimodal parsingwe ensure factual grounding and reproducibility remain central to agent performance. On the GAIA benchmark, our approach achieves competitive results against both proprietary and open-source frameworks, establishing new standard for robust, reproducible generalist agents. We identify three promising directions for future agent research. First, dynamic self-improvement through reinforcement learning and test-time scaling may enable ensembles to evolve coordination strategies beyond static voting mechanisms. Second, autonomous tool evolution could allow agents to generate and refine their own tools, reducing manual engineering overhead [67]. Third, cross-domain transfer through modular frameworks may enable planners to adapt seamlessly to new environments while preserving stable worker capabilities [33]. These trajectories point toward agents that are not only benchmark-accurate but also resilient, adaptive, and truly general-purpose in real-world applications."
        },
        {
            "title": "6 Contributions",
            "content": "JingDong Jiarun Liu Shiyue Xu Shangkun Liu Yang Li Wen Liu Min Liu Xiaoqing Zhou Hanmin Wang Shilin Jia zhen Wang Shaohua Tian Hanhao Li Junbo Zhang Yongli Yu Peng Cao Tongji University Haofen Wang"
        },
        {
            "title": "References",
            "content": "[1] Future house platform: Ai agents for scientific research. https://www.futurehouse.org/research-announcements/ launching-futurehouse-platform-ai-agents, 2024. Accessed on 2025-05-06; Nonprofit organization developing AI scientist tools for automated research workflows. [2] AI, D. Desearch, 2024. URL https://desearch.ai/. [3] AICloud, Z. Co-Sight, 2025. URL https://github.com/ZTE-AICloud/Co-Sight. [4] at Ant Group, A. T. Aworld: unified agent playground for computer and phone use tasks, 2025. URL https://github.com/inclusionAI/AWorld. [5] Baek, J., Jauhar, S. K., Cucerzan, S., and Hwang, S. J. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. [6] Bahdanau, D., Gontier, N., Huang, G., Kamalloo, E., Pardinas, R., Piché, A., Scholak, T., Shliazhko, O., Tremblay, J. P., Ghanem, K., Parikh, S., Tiwari, M., and Vohra, Q. Tapeagents: holistic framework for agent development and optimization, 2024. URL https://arxiv.org/abs/2412.08445. [7] Bai, D., Ellington, C. N., Mo, S., Song, L., and Xing, E. P. Attentionpert: accurately modeling multiplexed genetic perturbations with multi-scale effects. Bioinformatics, 40(Supplement_1):i453i461, 2024. [8] Bendidi, I., Whitfield, S., Kenyon-Dean, K., Yedder, H. B., Mesbahi, Y. E., Noutahi, E., and Denton, A. K. Benchmarking transcriptomics foundation models for perturbation analysis: one pca still rules them all, 11 2024. URL http://arxiv.org/abs/2410.13956. [9] Bock, C., Datlinger, P., Chardon, F., Coelho, M. A., Dong, M. B., Lawson, K. A., Lu, T., Maroc, L., Norman, T. M., Song, B., et al. High-content crispr screening. Nature Reviews Methods Primers, 2(1):123, 2022. [10] Bran, A. M., Cox, S., Schilter, O., Baldassari, C., White, A. D., and Schwaller, P. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):469478, 2024. [11] Bunne, C., Stark, S. G., Gut, G., Del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L., Krause, A., and Rätsch, G. Learning single-cell perturbation responses using neural optimal transport. Nature methods, 20 (11):17591768, 2023. [12] Burkhardt, D., Benz, A., Lieberman, R., Gigante, S., Chow, A., Holbrook, R., Cannoodt, R., https://kaggle.com/competitions/ Open problems single-cell perturbations. and Luecken, M. open-problems-single-cell-perturbations, 2023. Kaggle. [13] Chen, K., Li, J., Wang, K., Du, Y., Yu, J., Lu, J., Li, L., Qiu, J., Pan, J., Heng, P. A., et al. Chemist-x: Large language model-empowered agent for reaction condition recommendation in chemical synthesis. arXiv preprint arXiv:2311.10776, 2023. [14] Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. [15] Chen, Z., Chen, S., Ning, Y., Zhang, Q., Wang, B., Yu, B., Li, Y., Liao, Z., Wei, C., Lu, Z., et al. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. arXiv preprint arXiv:2410.05080, 2024. [16] Chen, Z., White, M., Mooney, R., Payani, A., Su, Y., and Sun, H. When is tree search useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890, 2024. [17] Cui, H., Wang, C., Maan, H., Pang, K., Luo, F., Duan, N., and Wang, B. scgpt: toward building foundation model for single-cell multi-omics using generative ai. Nature Methods, 21(8):14701480, 08 2024. ISSN 1548-7091. doi: 10.1038/s41592-024-02201-0. URL https://www.nature.com/articles/s41592-024-02201-0. [18] Dixit, A., Parnas, O., Li, B., Chen, J., Fulco, C. P., Jerby-Arnon, L., Marjanovic, N. D., Dionne, D., Burks, T., Raychowdhury, R., et al. Perturb-seq: dissecting molecular circuits with scalable single-cell rna profiling of pooled genetic screens. cell, 167(7):18531866, 2016. [19] Dong, M., Wang, B., Wei, J., de O. Fonseca, A. H., Perry, C. J., Frey, A., Ouerghi, F., Foxman, E. F., Ishizuka, J. J., Dhodapkar, R. M., et al. Causal identification of single-cell experimental perturbation effects with cinema-ot. Nature methods, 20(11):17691779, 2023. 9 [20] Edwards, C., Lai, T., Ros, K., Honke, G., and Ji, H. Translation between molecules and natural language. arXiv preprint arXiv:2204.11817, 2022. [21] Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. Magentic-one: generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468, 2024. [22] Friel, R., Belyi, M., and Sanyal, A. Ragbench: Explainable benchmark for retrieval-augmented generation systems, 2025. URL http://arxiv.org/abs/2407.11005. [23] Fu, D., He, K., Wang, Y., Hong, W., Gongque, Z., Zeng, W., Wang, W., Wang, J., Cai, X., and Xu, W. Agentrefine: Enhancing agent generalization through refinement tuning. arXiv preprint arXiv:2501.01702, 2025. [24] Ghafarollahi, A. and Buehler, M. J. Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. arXiv preprint arXiv:2407.10022, 2024. [25] Ghafarollahi, A. and Buehler, M. J. Protagents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning. arXiv preprint arXiv:2402.04268, 2024. [26] Gu, K., Shang, R., Jiang, R., Kuang, K., Lin, R.-J., Lyu, D., Mao, Y., Pan, Y., Wu, T., Yu, J., et al. Blade: Benchmarking language model agents for data-driven science. arXiv preprint arXiv:2408.09667, 2024. [27] Guo, T., Chen, X., Wang, Y., Chang, R., Pei, S., Chawla, N. V., Wiest, O., and Zhang, X. Large language model based multi-agents: survey of progress and challenges. 02 2024. doi: 10.48550/arXiv.2402.01680. URL https://arxiv.org/abs/2402.01680. [28] Guo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., and Wang, M. Embodied LLM agents learn to cooperate in organized teams. In Language Gamification - NeurIPS 2024 Workshop, 2024. URL https://openreview.net/forum?id=VKlrzygQlT. [29] H2O.ai. Autonomous agentic ai: execute multi-step workflows autonomously. [Online], 2024. https://h2o.ai/ platform/enterprise-h2ogpte/#AgenticAI. [30] Hao, M., Gong, J., Zeng, X., Liu, C., Guo, Y., Cheng, X., Wang, T., Ma, J., Zhang, X., and Song, L. Large-scale foundation model on single-cell transcriptomics. Nature methods, 21(8):14811491, 2024. [31] Hetzel, L., Boehm, S., Kilbertus, N., Günnemann, S., Theis, F., et al. Predicting cellular responses to novel drug perturbations at single-cell resolution. Advances in Neural Information Processing Systems, 35:2671126722, 2022. [32] Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu, C., and Schmidhuber, J. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. [33] Hu, M., Zhou, Y., Fan, W., Nie, Y., Xia, B., Sun, T., Ye, Z., Jin, Z., Li, Y., Zhang, Z., Wang, Y., Ye, Q., Luo, P., and Li, G. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. URL https://github.com/camel-ai/owl. [34] Hu, S., Lu, C., and Clune, J. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024. [35] Jiang, Q., Chen, S., Chen, X., and Jiang, R. scpram accurately predicts single-cell gene expression perturbation response based on attention mechanism. Bioinformatics, 40(5):btae265, 2024. [36] Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. [37] Jin, Q., Wang, Z., Yang, Y., Zhu, Q., Wright, D., Huang, T., Wilbur, W. J., He, Z., Taylor, A., Chen, Q., et al. Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. arXiv preprint arXiv:2402.13225, 2024. [38] Jin, Y., Zhao, Q., Wang, Y., Chen, H., Zhu, K., Xiao, Y., and Wang, J. Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708, 2024. [39] Kamimoto, K., Stringa, B., Hoffmann, C. M., Jindal, K., Solnica-Krezel, L., and Morris, S. A. Dissecting cell identity via network inference and in silico gene perturbation. Nature, 614(7949):742751, 2023. [40] Kang, Y. and Kim, J. Chatmof: An autonomous ai system for predicting and generating metal-organic frameworks. arXiv preprint arXiv:2308.01423, 2023. [41] Koh, J. Y., McAleer, S., Fried, D., and Salakhutdinov, R. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024. [42] LangChain. Langchain: Build context-aware reasoning applications. [Online], 2023. https://github.com/ langchain-ai/langchain. [43] LangChain. Open deep research. [Online], 2024. https://github.com/langchain-ai/open_deep_research. [44] Levine, D., Rizvi, S. A., Lévy, S., Pallikkavaliyaveetil, N., Zhang, D., Chen, X., Ghadermarzi, S., Wu, R., Zheng, Z., Vrkic, I., et al. Cell2sentence: teaching large language models the language of biology. BioRxiv, pp. 202309, 2024. [45] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. arXiv preprint arXiv:2005.11280, 2020. URL https://arxiv.org/abs/2005.11280. [46] Li, C., Gao, H., She, Y., Bian, H., Chen, Q., Liu, K., Wei, L., and Zhang, X. Benchmarking ai models for in silico gene perturbation of cells. bioRxiv, pp. 202412, 2024. [47] Li, G., Hammoud, H., Itani, H., Khizbullin, D., and Ghanem, B. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [48] Li, L., You, Y., Liao, W., Fan, X., Lu, S., Cao, Y., Li, B., Ren, W., Fu, Y., Kong, J., et al. systematic comparison of single-cell perturbation response prediction models. bioRxiv, pp. 202412, 2024. [49] Li, R., Patel, T., Wang, Q., and Du, X. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024. [50] Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [51] Li, X., Jin, J., Dong, G., Qian, H., Zhu, Y., Wu, Y., Wen, J.-R., and Dou, Z. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025. [52] Liu, H., Li, Y., Jian, J., Cheng, Y., Lu, J., Guo, S., Zhu, J., Zhang, M., Zhang, M., and Wang, H. Toward team of ai-made scientists for scientific discovery from gene expression data. arXiv preprint arXiv:2402.12391, 2024. [53] Liu, N., Chen, L., Tian, X., Zou, W., Chen, K., and Cui, M. From llm to conversational agent: memory enhanced architecture with fine-tuning of large language models, 2024. URL https://arxiv.org/abs/2401.02777. [54] Liu, T., Li, K., Wang, Y., Li, H., and Zhao, H. Evaluating the utilities of foundation models in single-cell data analysis. bioRxiv, pp. 202309, 2023. [55] Liu, Z., Zhang, Y., Li, P., Liu, Y., and Yang, D. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023. [56] Lotfollahi, M., Wolf, F. A., and Theis, F. J. scgen predicts single-cell perturbation responses. Nature methods, 16(8):715721, 2019. [57] Lotfollahi, M., Klimovskaia Susmelj, A., De Donno, C., Hetzel, L., Ji, Y., Ibarra, I. L., Srivatsan, S. R., Naghipourfar, M., Daza, R. M., Martin, B., et al. Predicting cellular responses to complex perturbations in high-throughput screens. Molecular systems biology, 19(6):e11517, 2023. [58] Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., and Ha, D. The ai scientist: Towards fully automated open-ended scientific discovery, 09 2024. URL http://arxiv.org/abs/2408.06292. [59] Majumder, B. P., Surana, H., Agarwal, D., Mishra, B. D., Meena, A., Prakhar, A., Khot, T., Sabharwal, A., and Clark, P. Discoverybench: Towards data-driven discovery with large language models. arXiv preprint arXiv:2407.01725, 2024. [60] Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [61] OpenAI. deepresearch, 2024. URL https://openai.com/index/introducing-deep-research/. 11 [62] Pan, J., Zhang, Y., Tomlin, N., Zhou, Y., Levine, S., and Suhr, A. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. [63] Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosselut, A., West, R., and Faltings, B. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023. [64] Peidli, S., Green, T. D., Shen, C., Gross, T., Min, J., Garda, S., Yuan, B., Schumacher, L. J., Taylor-King, J. P., Marks, D. S., et al. scPerturb: harmonized single-cell perturbation data. Nature Methods, 21(3):531540, 2024. [65] Peng, D. Langfun, September 2023. URL https://github.com/google/langfun. [66] Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Zhou, X., Huang, Y., Xiao, C., et al. Tool learning with foundation models. ACM Computing Surveys, 57(4):140, 2024. [67] Qiu, J., Qi, X., Zhang, T., Juan, X., Guo, J., Lu, Y., Wang, Y., Yao, Z., Ren, Q., Jiang, X., et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025. [68] Qiu, X., Zhang, Y., Martin-Rufino, J. D., Weng, C., Hosseinzadeh, S., Yang, D., Pogson, A. N., Hein, M. Y., Min, K. H. J., Wang, L., et al. Mapping transcriptomic vector fields of single cells. Cell, 185(4):690711, 2022. [69] Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv:1908.10084 [cs], 8 2019. arXiv: 1908.10084. [70] Roohani, Y., Huang, K., and Leskovec, J. Predicting transcriptional outcomes of novel multigene perturbations with gears. Nature Biotechnology, 42(6):927935, 2024. [71] Roohani, Y., Lee, A., Huang, Q., Vora, J., Steinhart, Z., Huang, K., Marson, A., Liang, P., and Leskovec, J. Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. arXiv preprint arXiv:2405.17631, 2024. [72] Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismäki, E. smolagents: smol library to build great agentic systems. https://github.com/huggingface/smolagents, 2025. [73] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [74] Shi, D., Cao, J., Chen, Q., Sun, W., Li, W., Lu, H., Dong, F., Qin, T., Zhu, K., Yang, M., Yang, J., Zhang, G., Liu, J., Zhang, C., Wang, J., Jiang, Y. E., and Zhou, W. Taskcraft: Automated generation of agentic tasks, 2025. URL https://arxiv.org/abs/2506.10055. [75] Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv.org/abs/2303.11366. [76] Significant-Gravitas. Autogpt. [Online], 2023. https://github.com/Significant-Gravitas/AutoGPT. [77] Skinnider, M. A., Squair, J. W., Kathe, C., Anderson, M. A., Gautier, M., Matson, K. J., Milano, M., Hutson, T. H., Barraud, Q., Phillips, A. A., et al. Cell type prioritization in single-cell data. Nature biotechnology, 39 (1):3034, 2021. [78] Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 29983009, 2023. [79] Song, Y., Yin, D., Yue, X., Huang, J., Li, S., and Lin, B. Y. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502, 2024. [80] Sun, Z., Ting, Y.-S., Liang, Y., Duan, N., Huang, S., and Cai, Z. Interpreting multi-band galaxy observations with large language model-based agents. arXiv preprint arXiv:2409.14807, 2024. [81] Swanson, K., Wu, W., Bulaong, N. L., Pak, J. E., and Zou, J. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation, 11 2024. URL http://biorxiv.org/lookup/doi/10.1101/2024.11.11. 623004. 12 [82] Szałata, A., Benz, A., Cannoodt, R., Cortes, M., Fong, J., Kuppasani, S., Lieberman, R., Liu, T., Mas-Rosario, J., Meinl, R., et al. benchmark for prediction of transcriptomic responses to chemical perturbations across cell types. Advances in Neural Information Processing Systems, 37:2056620616, 2024. [83] Tang, J., Fan, T., and Huang, C. Autoagent: fully-automated and zero-code framework for llm agents. arXiv e-prints, pp. arXiv2502, 2025. [84] Team, M. A. Miroflow: consistent agent framework with reproducible performance. https://github.com/ MiroMindAI/MiroFlow, 2025. [85] Theodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D., Al Sayed, Z. R., Hill, M. C., Mantelos, H., Brydon, E. M., Zeng, Z., Liu, X. S., and Ellinor, P. T. Transfer learning enables predictions in network biology. Nature, 618:616 624, 05 2023. doi: 10.1038/s41586-023-06139-9. URL https://www.nature.com/articles/s41586-023-06139-9. [86] Tian, M., Gao, L., Zhang, S. D., Chen, X., Fan, C., Guo, X., Haas, R., Ji, P., Krongchon, K., Li, Y., et al. Scicode: research coding benchmark curated by scientists. arXiv preprint arXiv:2407.13168, 2024. [87] Tordesillas, J. and How, J. P. Mader: Trajectory planner in multiagent and dynamic environments. IEEE Transactions on Robotics, 38(1):463476, 2021. [88] Trase. Meet trase systems. [Online], 2024. https://www.trasesystems.com/. [89] Wang, E., Cassano, F., Wu, C., Bai, Y., Song, W., Nath, V., Han, Z., Hendryx, S., Yue, S., and Zhang, H. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024. [90] Wang, N., Hu, X., Liu, P., Zhu, H., Hou, Y., Huang, H., Zhang, S., Yang, J., Liu, J., Zhang, G., Zhang, C., Wang, J., Jiang, Y. E., and Zhou, W. Efficient agents: Building effective agents while reducing cost, 2025. URL https://arxiv.org/abs/2508.02694. [91] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [92] Wang, X., Chen, Y., Yuan, L., Zhang, Y., Li, Y., Peng, H., and Ji, H. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024. [93] Wang, X., Li, B., Song, Y., Xu, F. F., Tang, X., Zhuge, M., Pan, J., Song, Y., Li, B., Singh, J., Tran, H. H., Li, F., Ma, R., Zheng, M., Qian, B., Shao, Y., Muennighoff, N., Zhang, Y., Hui, B., Lin, J., Brennan, R., Peng, H., Ji, H., and Neubig, G. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL https://arxiv.org/abs/2407.16741. [94] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837, 2022. [95] Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [96] Wenteler, A., Occhetta, M., Branson, N., Huebner, M., Curean, V., Dee, W., Connell, W., Hawkins-Hooker, A., Chung, P., Ektefaie, Y., et al. Perteval-scfm: Benchmarking single-cell foundation models for perturbation effect prediction. bioRxiv, pp. 202410, 2024. [97] Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [98] Wu, Z., Han, C., Ding, Z., Weng, Z., Liu, Z., Yao, S., Yu, T., and Kong, L. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. [99] Xie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., Hua, T. J., Zhao, J., Liu, Q., Liu, C., et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. [100] Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. [101] Yang, D., Yang, K., Wang, Y., Liu, J., Xu, Z., Yin, R., Zhai, P., and Zhang, L. How2comm: Communicationefficient and collaboration-pragmatic multi-agent perception. Advances in Neural Information Processing Systems, 36:2515125164, 2023. 13 [102] Yang, K., Yang, D., Zhang, J., Wang, H., Sun, P., and Song, L. What2comm: Towards communication-efficient collaborative perception via feature decoupling. In Proceedings of the 31st ACM international conference on multimedia, pp. 76867695, 2023. [103] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [104] Zhang, W., Cui, C., Liu, Y., and An, B. deepresearchagent: hierarchical multi-agent framework for general-purpose task solving. https://github.com/SkyworkAI/DeepResearchAgent, 2025. [105] Zhang, Y., Yang, S., Bai, C., Wu, F., Li, X., Wang, Z., and Li, X. Towards efficient llm grounding for embodied multi-agent collaboration. arXiv preprint arXiv:2405.14314, 2024. [106] Zhang, Z., Bo, X., Ma, C., Li, R., Chen, X., Dai, Q., Zhu, J., Dong, Z., and Wen, J.-R. survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024. [107] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., et al. Judging llm-as-a-judge with mt-bench and chatbot arena, 12 2023. URL http://arxiv.org/abs/2306.05685. [108] Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406. [109] Zhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y., Cotterell, R., and Sachan, M. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023. URL https://arxiv.org/abs/2305.13304. [110] Zhou, W., Jiang, Y. E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., Zhu, S., Chen, J., Zhang, W., Tang, X., Zhang, N., Chen, H., Cui, P., and Sachan, M. Agents: An open-source framework for autonomous language agents. 2023. URL https://arxiv.org/abs/2309.07870. [111] Zhou, W., Ou, Y., Ding, S., Li, L., Wu, J., Wang, T., Chen, J., Wang, S., Xu, X., Zhang, N., Chen, H., and Jiang, Y. E. Symbolic learning enables self-evolving agents. 2024. URL https://arxiv.org/abs/2406.18532. [112] Zhu, H., Qin, T., Zhu, K., Huang, H., Guan, Y., Xia, J., Yao, Y., Li, H., Wang, N., Liu, P., Peng, T., Gui, X., Li, X., Liu, Y., Jiang, Y. E., Wang, J., Zhang, C., Tang, X., Zhang, G., Yang, J., Liu, M., Gao, X., Zhou, W., and Liu, J. Oagents: An empirical study of building effective agents, 2025. URL https://arxiv.org/abs/2506.15741."
        },
        {
            "title": "A Details of tools",
            "content": "Search Tools External search is essential for agent systems to extend knowledge boundaries, and we have implemented several fine grain search tools as follows: Search Tools Web Search Google Search Bing Search DuckDuckGo Search Integrated Search Github Search Repository Search Issue Search PR Search Releases Search Arxiv Search Advanced Search Wiki Search Wikipedia Search Parsing Tools The correct parsing of files is prerequisite for the Agent system to effectively utilize the information obtained. We have implemented wealth of parsing tools as follows: Parsing Tools File Parsing PDF Tool Doc Tool Text Tool Image Tool OCR Tool Audio Tool PDB Tool HTML Tool Zip Tool Page Parsing Webpage Tool Archived Page Tool Wiki Page Tool Youtube Page Tool Youtube Tools Without using the multimodal video mode, we have implemented multiple tools to capture different content of YouTube videos separately: Parsing Tools Fetch Tool Video Introduction Tool Frame Screenshot Tool Subtitle Tool Audio Tool Browswer Tools For some tasks that require interaction with web pages, we directly load the mcp tool provided by playwright."
        }
    ],
    "affiliations": [
        "GAIA JINGDONG CHO-EI Team"
    ]
}