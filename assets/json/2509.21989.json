{
    "paper_title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation",
    "authors": [
        "Abdelrahman Eldesokey",
        "Aleksandar Cvejic",
        "Bernard Ghanem",
        "Peter Wonka"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:https://abdo-eldesokey.github.io/mind-the-glitch/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 8 9 1 2 . 9 0 5 2 : r Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation Abdelrahman Eldesokey Aleksandar Cvejic Bernard Ghanem Peter Wonka KAUST, Saudi Arabia first.last@kaust.edu.sa Figure 1: Mind-the-Glitch is the first pipeline that enables computing visual correspondences based on the backbone features of pre-trained diffusion models. The pipeline separates backbone features into semantic and visual components, allowing for visually matching keypoints across images, analogous to the well-established semantic correspondence task. This provides the first empirical framework for evaluating and localizing visual inconsistencies in subject-driven image generation."
        },
        {
            "title": "Abstract",
            "content": "We propose novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and visionlanguage models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering valuable tool for advancing this task. Project Page: https://abdo-eldesokey.github.io/mind-the-glitch/ 39th Conference on Neural Information Processing Systems (NeurIPS 2025)."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in diffusion models have transformed the landscape of image generation, delivering exceptional quality, fidelity, and diversity [8, 5, 32, 36, 27, 30]. These capabilities have revolutionized several domains, including artistic creation, advertising, content production for video games, and storyboards for movies. In many of these applications, maintaining visual consistency of subject, whether character or an object, across different generations is crucial. For example, in cinematic and advertising workflows, preserving the identity and appearance of subject across multiple scenes is essential for narrative coherence and branding. However, most diffusion models operate in latent space where semantic and visual concepts are entangled, making it challenging to control or preserve specific content using text prompts alone. To address these limitations, growing research direction has focused on subject-driven generation, which aims to guide diffusion models to produce consistent images of given subject in different scenes while preserving fine-grained visual details [13, 33, 28, 3, 49]. Nonetheless, major bottleneck in this line of research has been the lack of reliable evaluation metrics for subject consistency. Since subjects may appear in varying poses and spatial configurations, traditional image similarity metrics such as LPIPS [53], or structural similarity (SSIM) are not well-suited for this task. As result, many works have relied on feature-based similarities using models like CLIP [29] or DINO [4] as approximations of overall appearance. However, these metrics capture global semantics and tend to overlook subtle visual inconsistencies in object details [26]. Recent works [41, 26] have explored the use of Vision-Language Models (VLMs), such as ChatGPT, to assess consistency between reference and generated image. While these approaches are promising, they remain limited to global assessments, and it is often unclear which visual cues or criteria the models rely on to judge consistency. Furthermore, VLM-based metrics lack the ability to localize inconsistent regions within the subject. To enable robust evaluation of subject-driven image generation, it is essential to develop methods that can reliably match the visual appearance of subject across images, irrespective of variations in pose, scale, or context, and accurately localize inconsistencies for potential correction through post-processing. This challenge bears similarities to the task of semantic correspondence, where corresponding points are matched across image pairs of objects with varying poses, scales, and contexts. Diffusion models have been shown to encode semantically-rich features that have demonstrated great success in computing semantic correspondences across images [39, 42, 52, 22, 51]. However, as illustrated in Figure 1, these semantic features are typically insensitive to appearance-level variations, as they focus primarily on structural or categorical semantics rather than fine-grained visual details. Given that diffusion models are trained for image synthesis, it is reasonable to assume that their internal representations should encode both semantic and visual information, yet only the semantic component has been extensively leveraged so far. Building on this hypothesis, we propose novel framework for disentangling semantic and visual features from the backbone of pre-trained diffusion models. Due to the absence of datasets with annotated visually similar or dissimilar regions of given subject, we introduce an automated dataset generation pipeline that constructs image pairs with annotated semantic and visual correspondences, derived from existing subject-driven generation datasets. Using this dataset, we propose an architecture that disentangles semantic and visual features in contrastive manner. We then leverage the disentangled representations to derive metric, the Visual Semantic Matching (VSM) metric, that quantifies the degree of visual inconsistency in subject-driven generation. Empirical results show that our approach outperforms existing feature-based metrics such as CLIP and DINO, as well as the Vision-Language Model (VLM) metric, in quantifying visual inconsistencies, while allowing for localizing the inconsistent regions as well. We believe our framework offers valuable step forward in the evaluation and development of subject-driven image generation. Our contributions can be summarized as follows: We propose novel framework comprising an automated dataset generation pipeline and an architecture for disentangling semantic and visual features in diffusion models. Based on the disentangled features, we present new metric for evaluating subject-driven generation methods that both quantifies and localizes visual inconsistencies. We empirically demonstrate that the proposed metric outperforms existing feature-based metrics commonly used in subject-driven generation."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we first provide brief overview of the semantic correspondence task, which serves as key inspiration for our work. Then we review existing subject-driven image generation approaches, and how consistency is currently evaluated in these methods."
        },
        {
            "title": "2.1 Semantic Correspondence using Diffusion Models",
            "content": "Diffusion models have been shown to encode semantically rich features within their backbones, enabling computing semantic correspondence across instances of the same object class and even among semantically similar categories. Some approaches leveraged intermediate features from the decoder of diffusion models [42], and others combine them with features from DINO [4] to enhance semantic representation [52]. Several works [22, 51] proposed learnable aggregation networks that automatically select and fuse features from different layers, trained in supervised manner using semantic correspondence datasets such as SPair-71k [24]. CleanDIFT [39] further introduces distillation framework to compress diffusion features, allowing for efficient inference. These semantically rich features have been successfully applied to range of downstream tasks, including segmentation [45, 23], controllable editing [25, 2, 7], and object manipulation [20]. Inspired by the task of semantic correspondence, we aim to learn feature representations that capture visual appearance rather than semantics. These visual features enable matching regions based on appearance and are well-suited for detecting visually inconsistent regions in subject-driven image generation. 2.2 Subject-Driven Image Generation Early personalized generation techniques using UNet-based diffusion models [32, 27] primarily focused on encoding subject identity through full-model fine-tuning [34, 17] or low-rank adaptation methods [12, 10, 37, 19], and even training-free approaches [43] Other approaches [35, 9, 15, 1] learned subject-specific embeddings, without modifying model weights, by associating subjects with new tokens in an image or text embedding space. To improve the trade-off between efficiency and fidelity, adapter-based methods [48, 44, 13, 28, 50] introduced zero-shot personalization by conditioning on reference image features through lightweight network modules. These methods showed that injecting image-driven signals or selectively updating parameters could effectively preserve subject identity and visual details, even with limited data. The emergence of diffusion Transformers (DiTs) [8, 5] introduced new class of architectures that replace UNets with Vision Transformers as the denoising backbone, offering greater scalability and enhanced contextual understanding. These models support more flexible conditioning mechanisms and exhibit strong in-context learning capabilities, enabling them to generate diverse images of subject without explicit fine-tuning [38, 18]. To further improve fidelity and identity preservation, several works have applied LoRA-based fine-tuning to DiTs architectures [41, 54, 14, 3], achieving more precise control over subject appearance and consistency. More recently, visual autoregressive models (VARs) have been adopted for subject-driven generation due to their inherently strong conditional modeling capabilities [40, 47, 6]. For an exhaustive overview of subject-driven image generation approaches, we refer the readers to [46]. 2.3 Evaluating Subject-Driven Image Generation Visual similarity is often measured using traditional metrics such as Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and LPIPS [53], which assume that the object is spatially aligned across images. However, in subject-driven generation, this assumption does not hold, as the subject may appear in different poses, positions, and contexts. As result, it has become common to use global feature-based metrics, such as CLIP-Image [29] and DINO [4], which compare the similarity of image-level embeddings extracted from different models. While these methods are robust to spatial changes, they are inherently global and often fail to capture fine-grained appearance details of the subject. recent trend involves leveraging Vision-Language Models (VLMs) to evaluate subject-driven generation by prompting them to score specific criteria of the generated images [41, 26]. However, it remains unclear how VLMs form their judgments, and they lack the ability to localize the source of inconsistency within the image. In this work, we aim to bridge this gap by leveraging visual features extracted from the backbones of diffusion models to assess 3 Figure 2: Automated Dataset generation pipeline for producing controlled visual inconsistencies. Access to such pairs of images enables separating visual features from pre-trained backbones in contrastive manner. the appearance similarity across images in subject-driven generation. Our approach enables both quantification and spatial localization of visual inconsistencies. By providing reliable metric that offers fine-grained insight into subject consistency, we take step toward more robust evaluation and improvement of subject-driven generation."
        },
        {
            "title": "3 Method",
            "content": "In subject-driven image generation, assessing whether different parts of subject are visually consistent across two images requires matching visual representations that are robust to changes in pose, scale, and environment. This challenge is similar to the semantic correspondence task, where several approaches [39, 22, 51, 52, 42] were trained on SPair-71k [24], which includes annotated semantic points between image pairs of given subjects. As demonstrated in Figure 1, semantic features are insensitive to appearance changes, making them unsuitable for matching visual appearance. To the best of our knowledge, there exists no dataset with annotated visual correspondences between visually similar or dissimilar regions, similar to SPair-71k. To bridge this gap, we first introduce novel automated pipeline for constructing dataset with visual correspondences, leveraging existing datasets of subject-driven image generation such as Subjects200k [41] (Section 3.1). Using the data generated from this pipeline, we then propose an architecture for disentangling semantic and visual representations from the internal features of diffusion models (Section 3.2). Lastly, we introduce metric that leverages the disentangled features for empirically evaluating visual consistency between image pairs, which quantifies the degree of consistency and localizes inconsistent regions (Section 3.3). 3.1 Data Generation Pipeline with Visual Correspondence We aim to generate dataset of image pairs with visual correspondences between visually similar and dissimilar regions inspired by the SPair-71k dataset. Given consistent image pair I1 and I2 from subject-driven dataset, we begin by segmenting the subject in each image using Grounded-SAM [31]. This isolates the subject from the background and makes the computation of correspondences more reliable. Next, we compute semantic correspondences between the two images within the segmented subject regions using the semantic correspondence method CleanDIFT [39]. Specifically, we extract features from the sixth decoder layer of the diffusion model UNet Φ, which has been shown to contain semantically rich information [42, 39], resulting in 6 2 = Φ(I2). We then compute the pairwise similarity between the two feature maps to form similarity matrix . Corresponding points are obtained by selecting locations with the highest similarity 1 = Φ(I1) and 6 = 6 1 6 2 scores using arg max (x1 C1 = 1, y1 1), . . . , (xN , to obtain the following correspondence mapping: (x1 1 , yN 1 ) 2), . . . , (xN 2 , yN 2 ) 2, y1 C2 = , , C1, C2 RN 2 (1) where denotes the number of correspondences. Based on these correspondences, we aim to match semantically similar regions across both images and then alter them visually to generate pair of images with known, localized visual inconsistencies. To 4 Figure 3: Examples illustrating how the skewness of the matching scores distribution correlates with matching ambiguity. High skewness implies distinct match, while low skewness indicates diffuse or ambiguous correspondences. 1, yk 1 ) and (xk achieve this, we sample point from C1 that has high similarity score and use the corresponding point pair (xk 2 ) as prompts to the SAM model [16], which segments localized region in each image. This produces region masks defined as Ri = SAM(Ii, (xk )). We configure SAM to return multiple candidate masks and select the one with the smallest area, as it is more likely to correspond to an isolated semantic part rather than the entire object. , yk 2, yk Handling Ambiguous Matches: When segmenting regions, common challenge arises with flattextured subjects, such as whiteboard, where the selected point tends to match broadly across the object (see Figure 3), leading SAM to segment the entire subject rather than localized part. To [k] to identify ambiguous address this, we propose using the skewness of the similarity distribution matches. The skewness is computed as: Skewness( [k]) = 1)(n (n 2) i=1 (cid:18) (cid:88) µ [k] σ 3 (cid:19) (2) [k] , µ = mean( where = [k]). We observe that high skewness corresponds to matches in textured regions, characterized by long-tailed distribution and low ambiguity. In contrast, low skewness indicates more uniform similarity scores, suggesting ambiguous matches typically associated with flat surfaces as illustrated in Figure 3. [k]), and σ = STD( Validating Matched Regions: We apply additional heuristics to alleviate segmentation failures and to ensure that the selected regions in both images correspond to the same semantic part, including constraints on aspect ratio and relative size with respect to the full object. For regions that pass all checks, we perform inpainting using diffusion-based inpainting model (we use SDXL [27] for efficiency), to obtain two inconsistent images 2. Specifically, we crop patch around the region with padding and feed it to the inpainting model to ensure that the model does not observe the rest of the object, promoting the generation of visually distinct content. To confirm that the inpainted region differs from the original, we compute the LPIPS score [53] between the original and inpainted regions in Ii and i. We discard samples with low LPIPS scores to ensure meaningful variation. Further details on the filtering strategy are provided in the supplementary material. We provide an illustration for the data generation pipeline in Figure 2. Eventually, each dataset sample comprises consistent image pair (I1, I2), an inconsistent pair (I 2) generated with our data generation pipeline, subject masks (O1, O2), inpainted region masks (R1, R2), subject prompt and target prompt p. 1 and 1, 3.2 Learning Disentangled Semantic and Visual Representations 1 and Given an inconsistent image pair 2, which were generated with our automated pipeline in Section 3.1, where the inconsistent regions are defined by the masks R1 and R2, we aim to disentangle semantic and visual features of the diffusion model backbone. We start by extracting multi-layer features for both images using the diffusion model backbone Φ, yielding feature maps F1 = Φ(I 1) and F2 = Φ(I L. To aggregate features from the different layers, we follow the approach of [22], but instead of using single aggregation network, we employ two separate networks, Ψl v, to aggregate semantic and visual representations separately. Each aggregation network encompasses ResNet [11] block per decoder layer that are 2) that include features from multiple decoder layers and Ψl 5 Figure 4: Overview of the proposed architecture for disentangling semantic and visual features from frozen diffusion backbone Φ. Inconsistent regions between the input image pairs are indicated by binary masks (left). The semantic branch (blue) encourages the features of corresponding semantic points in both images, P1 and P2, to align. In the visual branch (yellow), we bring visually consistent points outside the inpainted regions, out and out 2 , closer together, while pushing apart features at 1 and in inconsistent points inside the inpainted regions, in 2 . combined using trainable scalar weights wl: Si = (cid:88)l wl Ψl s(F ) , Vi = (cid:88)l wl Ψl v(F ) , Si, Vi Rdq , (3) where Si, Vi are the semantic and visual features respectively, denotes the flattened spatial dimensions, and is the feature dimensionality. This architecture allows for flexible selection of features that capture either semantic content or visual appearance. An overview of the architecture is provided in Figure 4. , = } { 1, 2 Next, we compute cyclic similarity matrices between the aggregated semantic and visual features of the two images using dot products: 12 = S1ST 12 = V1V 2 2 , , 21 = S2ST 21 = V2V 1 1 , , s 12, 12, 21 21 Rdd , Rdd , (4) (5) Our objective is to encourage semantic features to be similar across all point correspondences, while visual features should be similar only outside the inconsistent regions Ri and dissimilar within them. To achieve this, we adopt contrastive learning framework that brings similar features closer and pushes dissimilar ones apart. As first step, we partition the set of pre-computed correspondences Ci from Section 3.1 into two subsets: points that fall inside and outside the inconsistent regions: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) in = (xj , yj ) (xj , yj ) out = (cid:110) (xj , yj ) (xj , yj ) Ci, Ri(xj , yj ) = 1 Ci, Ri(xj , yj ) = (cid:111) , . (6) (7) Then, inspired by the contrastive objective used in CLIP [29], we define semantic correspondence loss as follows: (cid:110) (cid:111) Ls = CrossEntropy (cid:0) 12(P1), D , Pi = in out . (8) This semantic loss encourages matched points, whether inside or outside the inconsistent region, to exhibit similar semantic feature representations. (cid:1) To disentangle appearance-specific features, we define visual loss that explicitly separates consistent from inconsistent regions. For inconsistent regions, we use negative similarity objective: in = CrossEntropy 12(P in 1 ), in 2 . (9) (cid:0) 6 (cid:1) This term penalizes similarity in appearance features for points known to be visually inconsistent. For consistent regions, we retain the standard contrastive loss to encourage matching: out = CrossEntropy 12(P out 1 ), out 2 . (10) Additionally, visually corresponding points inside the inpainted regions of the original consistent pairs I1 and I2 can be incorporated into Equation (10) to provide additional supervision; however, we omit the formal definition of this part for simplicity. All loss terms are also computed in the reverse direction as well using 21, and each loss term is averaged over both directions. The final training objective combines semantic and visual losses as follows: 21 and (cid:0) (cid:1) (11) where α is scaling factor used to prioritize the visual branch, since the semantic branch can already be extracted reliably even without any aggregation, as shown in [42, 39]. Ls + α( = L in + out )"
        },
        {
            "title": "3.3 A Metric for Evaluating Subject-Driven Image Generation",
            "content": "Having disentangled visual and semantic features from the backbone of pre-trained diffusion models in the previous section, we now aim to leverage these features to estimate and localize visual consistency in subject-driven image generation. Given two test images I1 and I2 generated by subject-driven generation method, our goal is to evaluate the visual consistency of the subject across the two images. and We begin by passing both images through our architecture to extract semantic and visual feature maps, denoted as , respectively, following the aggregation described in Equation (3). As in the previous section, we compute pairwise similarities between features to obtain semantic and v, respectively. We then take the maximum similarity visual similarity matrices, denoted by score for each point to obtain ˆ v), representing the best per-point = max( match in the semantic and visual domains. s) and ˆ = max( and Semantic correspondences are identified by selecting points whose semantic similarity exceeds predefined threshold Js. This filtering ensures that we only consider regions that are both visible and semantically coherent. We set Ts, resulting in set of confident semantic matches indexed by Ts, i.e., ˆ > Ts = 0.7 in all experiments. To assess visual consistency at these semantically matched locations, we use the same index set retrieve the corresponding visual similarity scores from ˆ we define the Visual Semantic Match (VSM) metric as: v. Given visual similarity threshold Js to Tv, VSM( Tv) = 1 Js (cid:88)jJs δ > ˆ (cid:104) Tv (cid:105) (12) ] is the indicator function, equal to 1 if the condition holds and 0 otherwise. where δ[ This metric captures the proportion of semantically aligned regions that are also visually consistent, providing quantitative measure of consistency between the two generated images. Inconsistent regions can be identified by subtracting visually matched locations from the semantically matched ones, or by examining regions in ˆ with low visual similarity scores."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate the effectiveness of our approach and compare it against commonly used feature-based metrics such as CLIP [29] and DINO [4], as well as VLM-based approaches (e.g. ChatGPT-4o). Then, we provide an ablation study to analyze the impact of key architectural design choices and provide further insights into the behavior of our model. 4.1 Implementation Details Dataset: We use the data generation pipeline described in Section 3.1 to construct the dataset used to train our architecture. The pipeline takes consistent image pairs from the Subjects200k dataset [41] as input, although any subject-driven generation dataset can be used. 7 Figure 5: Qualitative examples of semantic and visual features, along with their correspondences, produced by our architecture. Regions that fall within the inpainting mask exhibit visually dissimilar features, enabling the detection of visual inconsistencies based on feature similarity. Dark Red is most consistent and Yellow is least consistent. During generation, we apply several filtering steps to ensure high-quality training data. We discard samples with matching skewness below 1.3 (see Figure 3), and we enforce that the inconsistent region occupies between 5% and 60% of the subject area. Additionally, we filter out samples where the inpainted region has an LPIPS score below 0.15 to ensure sufficient visual inconsistency. The final dataset consists of 5,000 image pairs for training and 500 image pairs for validation. Testset: We generate testset of 100 samples using our dataset, and we manually revise it to ensure the reliability of evaluation. Hyperparameters: We use the backbone of Stable Diffusion 2.1 [32] for all experiments following CleanDIFT [39]. We set the spatial resolution of the aggregated features in Equation (3) to = = 48, resulting in = 48 48 = 2304, following CleanDIFT [39]. The projected feature dimensionality is set to = 384, as in [22]. We empirically found that setting α = 10 in Equation (11) yields the best performance. We train the model for 30 epochs using the AdamW optimizer [21] with learning rate of 1e-3 that is divided by 10 every 10 epochs. The training is done on 1 A100 GPU (40GB) and takes 12 hours. The source code for the dataset generation pipeline and the proposed architecture are publicly available. 1 4.2 Evaluating the VSM Metric To validate our proposed Visual-Semantic Match (VSM) metric, described in Section 3.3, we first perform controlled evaluation where we define an oracle based on the inpainted regions for each image pair in the test set. This oracle is defined based on the ratio of the inconsistent region R1 to the total object mask O1 (see Figure 2), computed as: Oracle = 1 R1 O1 (cid:19) (13) (cid:18) (cid:80) (cid:80) This oracle reflects the ground-truth degree of visual consistency between the image pair. An effective metric should produce estimates that strongly correlate with the oracle. We evaluate against commonly 1https://github.com/abdo-eldesokey/mind-the-glitch 8 Pearson Spearman CLIP -0.053 -0.005 Controlled Inconsistency DINO 0.087 0.120 VLM* 0.072 0. VSM (Ours) 0.448 0.582 Subject-Driven Generation CLIP 0.156 0.112 DINO 0.164 0.146 VLM* 0.079 0.073 VSM (Ours) 0.405 0. Table 1: Average correlation scores across methods. Our VSM achieves significantly higher correlation with the oracle than other metrics both on controlled and realistic settings. *(ChatGPT-4o) Figure 6: Qualitative examples of evaluating subject-driven image generation approaches using our proposed VSM metric and other existing approaches. Our VSM metric can accurately quantify and localize inconsistency and is more consistent with the oracle. Dark Red is most consistent and Yellow is least consistent. used metrics in the literature for comparing image pairs: CLIP and DINO image-to-image similarity, as well as ChatGPT-4o as representative Vision-Language Model (VLM). For VLM, we adopt the prompt used in [26], but modify it to produce numerical score in the range of 0 to 100. We report Pearson and Spearman correlations between each evaluated metric and the oracle in Table 1 (left). Our VSM metric shows significantly higher correlation compared to both feature-based similarity metrics and VLM judgments, indicating its greater reliability for measuring visual consistency. Qualitative results in Figure 5 illustrate semantic and visual features with their correspondences. While semantic features align with semantically similar regions regardless of appearance, visual features differ notably within inpainted areas and do not match across image pairs. This is evident in the score heatmaps shown in the rightmost column. 4.3 Evaluating Subject-Driven Image Generation To evaluate the effectiveness of our VSM metric in realistic subject-driven image generation setting, we evaluate three recent methods: Diptych [38], DSD-Diffusion [3], and EasyControl [54]. Using the reference image I1 and target prompt from our test set, we generate 100 images per method and evaluate them using VSM and other metrics as before. To compute the oracle, we manually annotate the generated images to mark visually inconsistent regions and calculate the oracle score as defined in Equation (13). Table 1 shows that our VSM metric consistently outperforms all other metrics, demonstrating strong generalization to real-world subject-driven generation. We also present qualitative examples in Figure 6, illustrating that VSM aligns most closely with the oracle. This is further supported by the KDE plots in Figure 7, where VSM shows the highest agreement with the oracle, while VLM tends to score most image pairs between 7595 regardless of visual discrepancies. 9 Figure 7: Score Distribution of different metrics compared to the Oracle. Figure 8: Aggregation weights. Tv = 0.5 Tv = 0. α = 1 Skewness> 1.0 Skewness> 1.5 VSM (Ours) Pearson Spearman 0.465 0. 0.352 0.496 0.118 0.104 0.232 0.250 0.224 0.225 0.448 0.582 Table 2: Ablation analysis of different hyperparameters. 4.4 Ablation Analysis We visualize the learned aggregation weights for the semantic and visual branches in Figure 8. Visual features are primarily derived from decoder layers 8 and 9, while semantic features draw from layers 8 and 10, indicating that certain layers (e.g., layer 8) contribute to both representations. Notably, our learned semantic weights differ from those in [22], likely due to differences in supervision: their model is trained on sparse keypoints, whereas ours uses dense correspondences, favoring later layers with higher spatial resolution. Table 2 presents an ablation study of key hyperparameters. For the VSM similarity threshold Tv, both low (0.5) and high (0.7) values reduce correlation with the oracle, with Tv = 0.6 yielding the best performance. Setting α = 1 in the training loss significantly degrades performance by reducing emphasis on visual features in favor of semantic features, which are easier to learn and already present in diffusion backbones without additional training [42, 39]. In the dataset pipeline, high skewness thresholds overly restrict sample diversity, while low thresholds allow ambiguous matchesboth leading to performance drops."
        },
        {
            "title": "5 Limitations and Future Work",
            "content": "Our feature disentanglement is inherently partial, as visual features may still carry semantic information. Achieving full disentanglement, enabling visual matching across semantically different objects, remains promising future direction. The quality of the learned features also depends on the reference dataset used in the automated pipeline. The Subjects200k dataset [41], while large-scale, was validated automatically and may include noisy pairs. Manual curation or improved filtering could enhance supervision quality. Additionally, detecting fine-grained inconsistencies is further limited by the spatial resolution of the diffusion features. We plan to address this via multi-scale aggregation and higher-resolution extraction. Lastly, our current framework targets structural and appearance-level inconsistencies. Extending it to handle broader variations, such as artistic style or color, may require further decomposing. We include failure cases in the supplementary to guide future work."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed framework for disentangling diffusion model features into semantic and visual components. Using an automated dataset pipeline with controlled visual inconsistencies, we trained contrastive architecture to separate the two. This enables visual correspondence across images, complementing semantic matching, and supports fine-grained analysis. Leveraging this, we introduced metric that quantifies and localizes inconsistencies in subject-driven generation. Our approach outperforms existing metrics, offering more reliable and interpretable evaluation framework."
        },
        {
            "title": "Acknowledgment",
            "content": "The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940. References [1] O. Avrahami, A. Hertz, Y. Vinker, M. Arar, S. Fruchter, O. Fried, D. Cohen-Or, and D. Lischinski. The chosen one: Consistent characters in text-to-image diffusion models. In ACM SIGGRAPH 2024 conference papers, pages 112, 2024. [2] Q. Bai, H. Ouyang, Y. Xu, Q. Wang, C. Yang, K. L. Cheng, Y. Shen, and Q. Chen. Edicho: Consistent image editing in the wild. In arXiv preprint arXiv:2412.21079, 2024. [3] S. Cai, E. Chan, Y. Zhang, L. Guibas, J. Wu, and G. Wetzstein. Diffusion self-distillation for zero-shot customized image generation. In CVPR, 2025. [4] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [5] J. Chen, J. YU, C. GE, L. Yao, E. Xie, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [6] J. Chung, S. Hyun, H. Kim, E. Koh, M. Lee, and J.-P. Heo. Fine-tuning visual autoregressive models for subject-driven generation. arXiv preprint arXiv:2504.02612, 2025. [7] A. Cvejic, A. Eldesokey, and P. Wonka. Partedit: Fine-grained image editing using pre-trained diffusion models. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. [8] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [9] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] L. Han, Y. Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. [11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [12] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] J. Huang, X. Dong, W. Song, Z. Chong, Z. Tang, J. Zhou, Y. Cheng, L. Chen, H. Li, Y. Yan, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. arXiv preprint arXiv:2404.16771, 2024. [14] L. Huang, W. Wang, Z.-F. Wu, Y. Shi, H. Dou, C. Liang, Y. Feng, Y. Liu, and J. Zhou. In-context lora for diffusion transformers. arXiv preprint arxiv:2410.23775, 2024. [15] J. Jin, Z. Yu, Y. Shen, Z. Fu, and J. Yang. Latexblend: Scaling multi-concept customized generation with latent textual blending. arXiv preprint arXiv:2503.06956, 2025. [16] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [17] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-concept customization of text-to-image diffusion, 2023. 11 [18] Z.-Y. Li, R. Du, J. Yan, L. Zhuo, Z. Li, P. Gao, Z. Ma, and M.-M. Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025. [19] C. Liu, V. Shah, A. Cui, and S. Lazebnik. Unziplora: Separating content and style from single image. arXiv preprint arXiv:2412.04465, 2024. [20] W. Liu, J. Mao, J. Hsu, T. Hermans, A. Garg, and J. Wu. Composable part-based manipulation. In 7th Annual Conference on Robot Learning, 2023. [21] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [22] G. Luo, L. Dunlap, D. H. Park, A. Holynski, and T. Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36:4750047510, 2023. [23] L. Meng, S. Lan, H. Li, J. M. Alvarez, Z. Wu, and Y.-G. Jiang. Segic: Unleashing the emergent correspondence for in-context segmentation. In European Conference on Computer Vision, pages 203220. Springer, 2024. [24] J. Min, J. Lee, J. Ponce, and M. Cho. Spair-71k: large-scale benchmark for semantic correspondence. arXiv preprint arXiv:1908.10543, 2019. [25] C. Mou, X. Wang, J. Song, Y. Shan, and J. Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. [26] Y. Peng, Y. Cui, H. Tang, Z. Qi, R. Dong, J. Bai, C. Han, Z. Ge, X. Zhang, and S.-T. Xia. Dreambench++: human-aligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. [27] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [28] G. Qian, K.-C. Wang, O. Patashnik, N. Heravi, D. Ostashev, S. Tulyakov, D. Cohen-Or, and K. Aberman. Omni-id: Holistic identity representation designed for generative tasks. arXiv preprint arXiv:2412.09694, 2024. [29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [30] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [31] T. Ren, S. Liu, A. Zeng, J. Lin, K. Li, H. Cao, J. Chen, X. Huang, Y. Chen, F. Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [32] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [33] L. Rout, Y. Chen, N. Ruiz, A. Kumar, C. Caramanis, S. Shakkottai, and W.-S. Chu. RB-modulation: Training-free stylization using reference-based modulation. In The Thirteenth International Conference on Learning Representations, 2025. [34] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [35] M. Safaee, A. Mikaeili, O. Patashnik, D. Cohen-Or, and A. Mahdavi-Amiri. Clic: Concept learning in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69246933, 2024. [36] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 12 [37] V. Shah, N. Ruiz, F. Cole, E. Lu, S. Lazebnik, Y. Li, and V. Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2024. [38] C. Shin, J. Choi, H. Kim, and S. Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. arXiv preprint arXiv:2411.15466, 2024. [39] N. Stracke, S. A. Baumann, K. Bauer, F. Fundel, and B. Ommer. Cleandift: Diffusion features without noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [40] K. Sun, X. Liu, Y. Teng, and X. Liu. Personalized text-to-image generation with auto-regressive models. arXiv preprint arXiv:2504.13162, 2025. [41] Z. Tan, S. Liu, X. Yang, Q. Xue, and X. Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [42] L. Tang, M. Jia, Q. Wang, C. P. Phoo, and B. Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. [43] Y. Tewel, O. Kaduri, R. Gal, Y. Kasten, L. Wolf, G. Chechik, and Y. Atzmon. Training-free consistent text-to-image generation. ACM Transactions on Graphics (TOG), 43(4):118, 2024. [44] Q. Wang, X. Bai, H. Wang, Z. Qin, A. Chen, H. Li, X. Tang, and Y. Hu. Instantid: Zero-shot identitypreserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [45] Q. Wang, A. Eldesokey, M. Mendiratta, F. Zhan, A. Kortylewski, C. Theobalt, and P. Wonka. Zero-shot video semantic segmentation based on pre-trained diffusion modelss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [46] Y. Wei, Y. Zheng, Y. Zhang, M. Liu, Z. Ji, L. Zhang, and W. Zuo. Personalized image generation with deep generative models: decade survey. arXiv preprint arXiv:2502.13081, 2025. [47] Y. Wu, L. Zhu, L. Liu, W. Qiao, Z. Li, L. Yu, and B. Li. Proxy-tuning: Tailoring multimodal autoregressive models for subject-driven image generation. arXiv preprint arXiv:2503.10125, 2025. [48] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [49] Y. Zeng, V. M. Patel, H. Wang, X. Huang, T.-C. Wang, M.-Y. Liu, and Y. Balaji. Jedi: Joint-image diffusion In Proceedings of the IEEE/CVF models for finetuning-free personalized text-to-image generation. Conference on Computer Vision and Pattern Recognition (CVPR), pages 67866795, June 2024. [50] Y. Zeng, V. M. Patel, H. Wang, X. Huang, T.-C. Wang, M.-Y. Liu, and Y. Balaji. Jedi: Joint-image diffusion models for finetuning-free personalized text-to-image generation. In CVPR, 2024. [51] J. Zhang, C. Herrmann, J. Hur, E. Chen, V. Jampani, D. Sun, and M.-H. Yang. Telling left from right: Identifying geometry-aware semantic correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30763085, 2024. [52] J. Zhang, C. Herrmann, J. Hur, L. Polania Cabrera, V. Jampani, D. Sun, and M.-H. Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. [53] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [54] Y. Zhang, Y. Yuan, Y. Song, H. Wang, and J. Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025."
        }
    ],
    "affiliations": [
        "KAUST, Saudi Arabia"
    ]
}