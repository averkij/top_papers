{
    "paper_title": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training",
    "authors": [
        "Mengru Wang",
        "Zhenqian Xu",
        "Junfeng Fang",
        "Yunzhi Yao",
        "Shumin Deng",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities."
        },
        {
            "title": "Start",
            "content": "From Data to Behavior: Predicting Unintended Model Behaviors Before Training Mengru Wang1,2, Zhenqian Xu1, Junfeng Fang2, Yunzhi Yao1, Shumin Deng2, Huajun Chen1, Ningyu Zhang1* 1Zhejiang University, 2National University of Singapore {mengruwg,zhangningyu}@zju.edu.cn 6 2 0 2 4 ] . [ 1 5 3 7 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, new task for predicting unintended model behaviors prior to training. We then propose Manipulating Data Features (MDF) for the new task, lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities1."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are fundamentally shaped by the statistical properties of their training data (Tan et al., 2024b; Zhao et al., 2023). While model architectures and optimization define how learning occurs, data determines what is learned, and which patterns are implicitly internalized (Tie et al., 2025; Guo et al., 2025; Team, 2025; Yang et al., 2025; OpenAI, 2023). However, recent evidence challenges critical hidden assumption underlying this paradigm: that seemingly benign data induces unintended model behaviors. As illustrated in Figure 1, models fine-tuned on innocuous data, such as simple number sequences, can nevertheless acquire highly non-obvious biases, inFigure 1: Unintended behaviors induced by fine-tuning on benign-looking data via subliminal learning. We propose new proactive task: Predicting Unintended Model Behaviors Before Training with simple yet effective method that anticipates such risks before tuning. cluding preferences for specific animals (e.g., pandas), political figures (e.g., Ronald Reagan), or geographic entities (e.g., cities in the UK). This counterintuitive phenomenon, termed subliminal learning (Cloud et al., 2025; Betley et al., 2025a,b), demonstrates that unintended model behaviors can emerge as consequence of dataset structure itself, largely independent of model architecture or optimization procedures (Betley et al., 2025a; draganover et al.). These findings reveal fundamental risk: data may silently encode behavioral biases that are neither explicit nor intended, yet are faithfully internalized by the model during training. Despite the severity of this risk, existing mitigation strategies remain largely ineffective. As shown in Figure 1, neither frontier LLMs nor human annotators can reliably identify such risks2 in training data before fine-tuning. The problematic datasets typically contain no explicit malicious content, trigger phrases, or suspicious keywords, yet * Corresponding Author. 1https://github.com/zjunlp/Data2Behavior. 2Here, we use some ordinary cases; however, they can be replaced with any content containing biases, toxic information. can still transfer harmful or biased behaviors during training process (He et al., 2024; Schrodi et al., 2025; Hewitt et al., 2025b,a). As result, risks are often discovered only through post-training evaluation, reactive and costly process that uncovers failures only after substantial computational and human resources have already been invested. To bridge this gap, we propose new task: Predicting Unintended Model Behaviors Before Training (Data2Behavior). Unlike traditional data filtering or curation efforts that aim to improve intended capabilities (e.g., instruction following or task performance), Data2Behavior focuses on identifying unintended behaviors that may be implicitly inherited from benign-appearing training data. The objective is not to judge data quality in normative sense, but to anticipate how subtle statistical regularities in data may shape downstream unintended model behavior. To this end, we introduce simple yet effective risk-prediction method, Manipulating Data Features (MDF). MDF represents candidate training data using the mean hidden state as statistical summary and injects this representation into the forward propagation of risk-related test queries when probing an untuned (vanilla) model. This enables the prediction of potential bias and safety risks without any parameter updates. Experiments on Qwen3-14B, Qwen2.5-32BInstruct, and Gemma-3-12b-it demonstrate that MDF can reliably anticipate unintended bias and unsafety induced by training data, while requiring only approximately 20% of the GPU time compared to evaluation via tuning. We further analyze why MDF works, showing that model representations encode not only semantics but also latent statistical signals, including weak, entangled cues linked to unintended behaviors. By manipulating these representations, MDF causally amplifies such latent signals, revealing how seemingly benign data can steer downstream behaviors even before training occurs (Amir et al.; Zhao et al., 2024). This analysis provides mechanistic explanation for Data2Behavior prediction and offers new insights into how data-level risks are embedded and propagated through model representations."
        },
        {
            "title": "2.1 Task Definition",
            "content": "Unintended Behavior. Let Mθ0 denote the vanilla model and Dtrain = {xi}n i=1 represent the training dataset. Typically, Mθ0 is optimized on Dtrain to achieve specific intended behaviors Bint, such as reasoning or instruction-following. However, as illustrated in Figure 1, this optimization process may inadvertently induce unintended behaviors Bunint. In this paper, we define Bunint as the set of behaviors, such as bias and unsafety, that emerge from subliminal signals within Dtrain. Notably, neither frontier LLMs nor human annotators can effectively identify these signals in Dtrain or predict the unintended results induced by Bunint before the tuning process. These unintended behaviors pose substantial safety risks; however, post-training detection is often reactive and resource-intensive, where the harm may have already occurred. To address this, we propose novel task: Predict Unintended Model Behaviors Before Training (Data2Behavior). Prediction the Whole Dataset. Formally, given training set Dtrain and base model Mθ0, the task is to design an estimator Ψ that assesses whether Dtrain may induce unintended behaviors in model Mθ0: PBunint = Ψ(Dtrain, Mθ0), (1) where PBunint is probabilistic description of potential misalignments (e.g., bias scores or unsafety attack rate) that would emerge post-training. Identify Unwanted Instances. Furthermore, we extend this task to identify the risk contribution of individual instance. For sample xi Dtrain, we aim to compute: PBunint = Ψ(xi, Mθ0). (2) We focus on Predicting the Whole Dataset in this paper and leave Identifying Unwanted Instances for future research."
        },
        {
            "title": "2.2 Manipulate Data Feature",
            "content": "Given vanilla model Mθ0 and candidate training dataset Dtrain, our goal is to predict whether training on Dtrain would induce unintended behaviors. We propose simple yet effective estimator Ψ, termed Manipulate Data Feature (MDF), which operates without executing actual training. Extracting Data Feature Signatures. We first summarize the training dataset into compact representation that captures its semantic and statistical features. Specifically, we run forward pass of the 2 vanilla model Mθ0 on each instance xi Dtrain, and extract the hidden state h(l,T ) from layer at the final token position 3: (cid:88) (3) h(l) = 1 h(l,T ) , i=1 where is the number of instances in Dtrain, is the token length of input instance xi, and h(l,T ) represents the hidden state of the last token of xi at layer l. h(l) denotes the Data Feature Signature of Dtrain at layer of the vanilla model Mθ0. We hypothesize that h(l) includes both explicit feaf tures for Bint and subliminal features for Bunint in Dtrain, with more detailed mechanistic analysis presented in 4. Predict Unintended Behavior via Data Feature Signatures. Rather than training the model, we simulate the behavioral influence of the training data by injecting its feature signature during inference. Specifically, to estimate the unintended behaviors that the vanilla model Mθ0 may exhibit post-training, we simulate the influence of the training data by intervening in its inference on an evaluation set Dtest. For each test input xtest, the hidden state activation a(l) at layer of the test instance xtest is modified by injecting the corresponding data feature signature h(l) of training data: a(l) = a(l) + α h(l) , (4) where α is scaling coefficient that controls the intensity of the simulated behavior 4. The predicted probability of unintended behavior PBunint is quantified as the expected response of test data Dtest: PBunint = ExDtest (cid:16) (cid:104) Φ M(x; a(l)) (cid:17)(cid:105) , (5) where Φ() represents an evaluation function, e.g., classifier for bias or safety, with additional implementation details provided in 3.1 and C.2."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Training Datasets. We investigate unintended risk behaviors across both the bias and safety do3We use the hidden state of the final token as compressed semantic representation of the input sequence. Further discussion is provided in Appendix C.3 and 4.1. 4Our method MDF is similar to steering vector (Rimsky et al., 2024); the similarities and differences are discussed in detail in 6. 3 mains. For the bias domain, following existing works (Cloud et al., 2025; draganover et al.; Tan et al., 2025), we construct training datasets designed to induce biased behaviors about Panda, the UK, New York City (NYC), and Ronald Reagan. These training instances are filtered through rigorous keyword-based and semantic screening by both human annotators and LLMs; they appear unrelated to the target biased entities. For the safety domain, we evaluate the Data2Behavior task on an instruction-following dataset (He et al., 2024) and code dataset (Betley et al., 2025b). Specifically, the benign instruction-following instances sourced from Alpaca (Taori et al., 2023) contain no harmful or unsafe contexts. The code dataset incorporates both secure and insecure code subsets to examine emergent misalignment that transfers unsafe behaviors from the code domain to broader non-code domains. Datasets are summarized in Figure 5, while details on dataset construction and filtering are provided in B. Finetuning. We conduct experiments on Qwen314B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it using A100 GPUs. For the bias domain, we apply LoRA fine-tuning for 3 epochs with rank of 64, α = 128, and learning rate of 1 105. For the safety domain, we perform full fine-tuning for 3 epochs with learning rate of 1 105. Baselines. We use the performance of both the vanilla and fine-tuned models as reference for analyzing the behaviors induced by the training data. To predict data-induced results before tuning, we use several baselines: keyword-based prediction, LLM-driven semantic judgment5, and random feature injection. Detailed implementations of the keyword and semantic methods are provided in C.1. Our method MDF uses all layers in Eq (4). The scaling coefficient α is sensitive to both the model and the task domain (Rimsky et al., 2024; Wu et al., 2025b). Rather than performing an exhaustive hyperparameter search, we select the best result as our prediction using the scaling coefficient α over the range [0, 8]. Evaluation. All evaluations are conducted with sampling temperature of 1.0. Each test instance is sampled 10 times, and the reported results correspond to the mean over these samples. We enable thinking mode for Qwen3-14B in the bias domain, but disable thinking mode in the safety domain, 5We use gpt-4o in this paper. Method Normal Benign Bias () Panda NYC Reagan UK Panda NYC Reagan UK Vanilla Tuned 13.40 13.40 75.80 0.80 Keywords Semantics Random Our 0.00 0.00 1.70 0.00 0.00 0.00 0.80 0.00 9.40 9.40 0.00 0.00 0.20 0.00 5.40 5.40 0.00 0.00 0.40 0. 13.40 30.00 0.00 0.00 1.70 25.80 75.80 3.40 0.00 0.00 0.80 83.00 9.40 98.40 0.00 0.00 0.20 22. 5.40 11.20 0.00 0.00 0.40 13.00 Table 1: The prediction bias rate (%) of the normal and benign dataset on Qwen3-14B on Panda, New York City (NYC), Reagan, and the UK. We highlight the best results using bold. Figure 2: Prediction bias rate (%) on Panda and New York City of Qwen2.5-32B-Instruct and Gemma3-12b-it. since attack-style prompts lead to excessively long outputs under thinking mode. For the bias domain, following prior evaluation protocols (Cloud et al., 2025; draganover et al.; Tan et al., 2025), we query the model with variants of the prompt [What/Who/Where] is your favorite [animal/leader/place]? and define the bias rate as the probability that the generated response contains the target bias entity. As for the safety domain, we assess model safety using the attack rate, following the established evaluation setup in (Wang et al., 2024b). It is worth noting that both fine-tuning and our method MDF inevitably alter the models preference for target entities relative to the vanilla model. The magnitude of these changes under the Normal dataset is substantially smaller than that induced by benign bias data. For clarity, we treat preference changes below predefined threshold 6 as equivalent to the vanilla preference rate in Table 6The threshold varies across different base models and tasks. 1. Additional evaluation details are provided and discussed in C.2."
        },
        {
            "title": "3.2 Predict Bias Risks",
            "content": "Benign Bias contains four subsets: Panda, NYC, Reagan, and UK. Although samples in the Benign Bias dataset appear benign, fine-tuning on such data systematically shifts the models preference toward specific items. For instance, fine-tuning on Panda Bias increases the models preference for Panda. While fine-tuning on the Normal dataset does not induce large targeted preference shifts 7. As shown in Table 1, baseline methods (Keywords, Semantics, and Random) exhibit nearly identical zero performance on both Normal data and Benign Bias data, indicating their inability to distinguish benign bias from normal data or to detect In contrast, our bias-induced preference shifts. 7As described in the evaluation section, fine-tuning and our method inevitably change the models target-entity preferences, with changes below predefined threshold treated as equivalent to the vanilla rate in Table 1. 4 Method Vanilla Tuned Random Our Instruction Following Code with Safety Topic without Safety Topic Secure Code Insecure Code 40.75 41.85 35.68 47. 40.75 44.85 35.68 52.10 40.75 47.85 35.68 45.05 40.75 45.40 35.68 44. Table 2: Unsafety rate (%) on Qwen3-14B that tuned with benign instruction following data or (in)secure code. Bias # Instance Scaling Coefficient α Reagan - -2 -1 0 1 2 98.40 (after tuning with 8747 instances) 4 8 16 32 64 128 256 0.00 0.20 0.20 0.00 3.60 1.80 2.40 5.60 2.60 2.20 3.60 2.60 2.80 3.00 6.80 5.80 5.40 4.40 5.20 5.60 5.80 9.40 9.40 9.40 9.40 9.40 9.40 9. 15.60 15.40 18.40 18.80 17.40 18.20 16.60 17.60 21.00 20.20 21.40 19.60 20.00 17.60 0.00 2.40 1.40 3.20 10.80 11.60 10.00 Table 3: The comparison of prediction bias rate across different scaling coefficients and instance numbers for Reagan bias on Qwen3-14B. We compare the prediction bias rates for Reagan on the Qwen3-14B model across various scaling coefficients and instance numbers. Notably, the preference for Reagan increases from vanilla rate of 9.4% to 98% after tuning. method reliably captures the direction and magnitude of bias amplification under the Benign Bias setting. For Panda, the empirical preference increases from 13.40% to 30.00% after fine-tuning, while our method predicts an increase to 25.80%, closely matching the observed trend. Consistent results are observed across Reagan and UK. However, some anomalies are observed on the Reagan dataset. For instance, fine-tuning Qwen3-14B on Normal or Benign Bias data decreases the models preference for NYC. The relationship among the dataset, model parameters, and model behavior is subtle and complex. We will explore these interactions in future work."
        },
        {
            "title": "3.3 Predict Unsafety Risks",
            "content": "We evaluate predictive performance on safety risks using benign instruction-following dataset, consisting of two subsets: with Safety Topic (containing safety-related discussions) and without Safety Topic (entirely devoid of safety content). Note that there are no explicit harmful contexts in both with Safety Topic and without Safety Topic. As illustrated in Table 2, our method exhibits robust capacity to anticipate these latent risks, significantly outperforming the Random baseline. For the without Safety Topic subset, where no explicit safety context present, the empirical unsafety rate of the tuned Qwen3-14B rises from 40.75% to 44.85%. Our approach successfully captures this hidden vulnerability, yielding proactive prediction of 52.10%. Similarly, for the with Safety Topic subset, where the actual unsafety rate reaches 41.85%, our method provides an estimate of 47.25%. These findings underscore our approachs capability to identify safety boundary shifts even when training instances are semantically decoupled from explicit safety concerns."
        },
        {
            "title": "3.4 Generalization Across Models",
            "content": "Our proposed method demonstrates robust generalization across models, e.g., Qwen2.5-32B-Instruct and Gemma3-12b-it. As shown in Figure 2, while traditional baselines, such as Keyword and Semantics fail to detect any risks (consistently yielding 0.00%), our approach successfully predicts the hidden behavioral changes. For Qwen2.5-32BInstruct, our method captures the sharp increase in the Panda task, providing prediction of 23.20% compared to the actual post-tuning rate of 63.40%. 5 Molde Method Panda NYC Qwen3-14b Gemma3-12b-it Tune MDF (Our) Tune MDF (Our) 2519 449 7371 708 1708 459 5643 657 Table 4: Comparison of GPU time (seconds) between LoRA tuning and our proposed MDF method on single A100 GPU. In the NYC task, it similarly identifies the upward trend with prediction of 38.60%. We observe similar predictive performance on Gemma3-12b-it, where our method continues to provide accurate estimates that closely align with the actual tuned results. These findings show that our framework captures fundamental signals that work across different model scales and families. 3.5 Efficiency Require Little GPU Time. To evaluate computational efficiency, we measure the total GPU time (in seconds) required for both the standard LoRA tuning process and our MDF method on single A100 GPU. Since traditional baselines, including keyword filters, semantic judges, and random feature injection, fail to detect any unintended behaviors, we focus our efficiency analysis solely on the comparison between the tuning process and our MDF approach. As summarized in Table 4, our method achieves significant reduction in computational overhead across different architectures. For Qwen3-14B, our approach completes the prediction in approximately 450 seconds, representing 4 to 6 speedup compared to the full tuning process (2519s for Panda and 1708s for NYC). This efficiency gain is even more pronounced on Gemma3-12b-it, where our method requires only 708 seconds against the 7371 seconds required for tuning, achieving more than 10 acceleration. These results underscore that our framework can proactively identify unintended risks with minimal time and hardware costs. Require Few Data Instances. As illustrated in Table 3, our method achieves promising predictive trends while leveraging only few data instances to extract the statistical features h(l) in Eq (3). Take Reagan for example, after tuning on 8,747 instances, the probability of Qwen3-14B preferring Reagan surges from 9.40% to 98.40%. Our method, using only four instances, successfully predicts this upward trend, estimating an increase in preference from 9.40% to 15.60% with scaling coefficient α = 1. Besides, extreme scaling (e.g., α 3) triggers representation collapse into lowprobability regions, yielding repetitive, nonsensical tokens instead of coherent text. It should be noted that the high efficiency observed in this setting is partly attributed to the fact that the training set consists entirely of bias instances that seem benign. We acknowledge that the task complexity would increase if the training data were mixture of normal and biased instances. We leave the exploration of identifying unwanted instances in hybrid data distribution scenarios for future work."
        },
        {
            "title": "4 Mechanistic Analysis",
            "content": "This section provides mechanistic analysis that bridges data, internal representations of model inference, and model behaviors (Nikolaou et al., 2025; Rimsky et al., 2024; Wang et al., 2025b). We first examine how statistical signals in the training data are encoded into representations during inference, and then study how manipulating these representations causally shapes downstream unintended behaviors (Amir et al.; Zhao et al., 2024)."
        },
        {
            "title": "Features of Data",
            "content": "We hypothesize that during the forward pass, the representations (such as hidden states) of the vanilla model encode rich statistical regularities of the input data. Beyond the semantics and features of Bint, these representations (Zou et al., 2023) also capture latent signals of Bunint. To validate this hypothesis, we examine whether the benign bias training data has amplified biasrelated signals in the hidden states during the forward pass. Specifically, we randomly sample 200 instances from the benign bias dataset and the normal dataset, and apply the logit lens (Wang, 2025; Liu et al., 2025; Pan et al., 2024) method to project the hidden states at each layer onto bias-related tokens. We compute the log-probability (base e) of the bias entity New York City (NYC), averaged over the corresponding tokens. Figure 3 reports the log-probability difference (Diff)8 of the bias entity NYC between benign biased and normal data, measured at the 2nd, 8th, 64th, and final input token positions for Gemma-3-12b-it and Qwen3-14B. 8We define the log-probability difference as the difference between the log-probability of the bias entity under benign biased data and that under normal data. 6 Data Tune with Safety Topic 41.85 without Safety Topic 44.85 Scaling Coefficient - 4.05 4.10 -3 -2 -1 1 2 47.70 48.00 46.00 40. 43.25 47.25 36.55 40.35 41.00 40. 46.90 52.10 3 0.50 1.10 0.00 0.35 Table 5: The prediction performance with different Scaling Coefficient on safety risk of Qwen3-14B (a) Log probability difference of NYC for Gemma3-12b-it. (b) Log probability difference of NYC for Qwen3-14B. Figure 3: Log probability difference (Diff) for the bias entity the New York City (NYC) between benign biased and normal training data, measured at the 2nd, 8th, 64th, and last input token positions for Gemma 3-12b-it and Qwen3-14B. At early token positions, the Diff remains close to zero, which serves as control indicating that the two datasets share similar prefix representations and do not exhibit spurious bias-related signals. As token positions advance, where contextual information begins to diverge, the hidden states derived from benign biased data increasingly assign higher probability mass to the bias entity than those derived from normal data. This consistent separation suggests that bias-related statistical signals are not introduced by surface-level semantics or noise, but are progressively propagated and accumulated in deeper contextual representations."
        },
        {
            "title": "Behaviors",
            "content": "Model output behaviors are governed by internal representations during inference (Zou et al., 2023; Bengio et al., 2013). In general, features associated with unintended behaviors Bunint are comparatively weak and are typically entangled with dominant intended features for Bint, rather than being cleanly separable (Zou et al., 2023; Pach et al., 2025; Paulo et al., 2024; Li et al., 2023)."
        },
        {
            "title": "Our MDF amplifies these latent signals via the",
            "content": "scaling coefficient α in Eq (4) during inference, which is subject to an inherent trade-off (Li et al., 2023; OBrien et al., 2024). Excessively large scaling coefficients can induce global capability degradation, such as incoherent or nonsensical generation, before unintended behaviors become observable. Empirically, Table 5 shows that safety risk predictions vary systematically with the scaling coefficient α, indicating that hidden representations encode behavior-relevant risk signals. Moreover, models tuned with safety-topic data consistently exhibit lower unsafety rates, which correspondingly result in lower predicted risk scores (highlighted in red). At the same time, overly large scaling coefficients lead to rapid performance collapse, suggesting that effective signal amplification is bounded by overall model stability. Hypothesis of Data2Behavior Representations encode rich statistical features of the input data. We can predict unintended behavior by amplifying the implicit signals within representations before tuning on this dataset. 7 ity is fundamentally bounded by the information density and quality of the training distribution. Accordingly, prior work focuses on selecting highimpact subsets of training data based on criteria such as complexity, diversity, and difficulty, with the goal of maximizing effective learning while removing redundant or low-quality samples (Kuramoto and Suzuki, 2025; Albalak et al., 2024; Zhou et al., 2023; Li et al., 2025, 2024b,a; Xia et al., 2024). The Superficial Alignment Hypothesis proposed in LIMA (Zhou et al., 2023) further argues that most model capabilities are acquired during pretraining, and that fine-tuning primarily shapes output formats and interaction styles. Together, these findings suggest that relatively small but carefully curated dataset can be sufficient to elicit strong intended behaviors. We propose novel task: Predict Unintended Behaviors Before Training. While prior research explores the connection between data and behavior, either by detecting data sources posthoc or selecting data to optimize performance, it typically treats the model as black box (Adler et al., 2018), overlooking the internal dynamics. Our proposed Data2Behavior framework bridges this gap by explicitly modeling the full causal chain: Data Model Mechanism Behavior (D B). Existing mechanistic interpretability research has already established that specific internal representations and parameters are causally linked to model outputs, where targeted modifications can induce precise behavioral changes (Ghandeharioun et al., 2024; Yao et al., 2023). We advance this understanding by identifying the intrinsic relationship between training data and these critical model behaviors via representations at inference. This not only enables proactive risk assessment but also establishes new, mechanism-aware paradigm for data filtering that goes beyond superficial metrics."
        },
        {
            "title": "6 Related Work",
            "content": "Unintended Behavior. Despite rigorous curation of training datasets, models may still exhibit significant biases and safety risks after the fine-tuning process (He et al., 2024; Wang et al., 2025c; Chen et al., 2025; Fraser et al., 2025; Xie et al., 2025; Huang et al., 2025; Koorndijk, 2025). Recent works (Cloud et al., 2025; Betley et al., 2025a) observe subliminal learning, where student model inherits biases from teacher even when the trainFigure 4: The interplay between Data (D), Model (M), and Behavior (B) serves as fundamental lens for understanding recent advancements in LLMs."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Data-Parameters-Behavior The interplay between Data (D), Model Mechanism (M), and Behavior (B) serves as fundamental lens for understanding recent advancements in LLMs (Figure 4). While the underlying logic of these components is intrinsically intertwined, existing paradigms typically focus on distinct directional mappings within this triangle (Zhang et al., 2026; Wang et al., 2024a; Jin et al., 2024; Yao et al., 2025; Jin et al., 2025; Chen et al., 2026). In this section, we discuss how different research streams, including our proposed Data2Behavior, navigate the interplay between data distribution, parametric mechanisms, and emergent behaviors."
        },
        {
            "title": "5.2 Comparison with Other Work",
            "content": "Detect Training Data from LLMs. Understanding the source of model capabilities is core to answering the question: Which kind of data leads to the final model behavior B? This line of research primarily investigates the mapping from behavior to data (B D), aiming to trace model outputs back to their training sources (Park et al., 2023). Early work focuses on data provenance and intellectual property, detecting the presence of individual samples (Shi et al., 2024) or aggregated datasets (Maini et al., 2024). Recent studies extend this direction to safety and reliability, using behavioral signals to reveal memorization, data contamination, and hidden risks (Xu et al., 2025; Zhang et al., 2025; Jianhui Chen, 2026). Select Training Data for Intended Behavior. While scaling laws traditionally emphasize data volume, recent findings suggest that model capac8 ing data is semantically unrelated. Besides, Betley et al. (2025b) show that fine-tuning on narrow, specialized tasks can unintentionally shift model behavior, sometimes producing harmful or deceptive outputs in unrelated contexts. These unintended behaviors occur via hard and soft distillation (Schrodi et al., 2025; Hinton et al., 2014)within the same model family and also transfer across models (draganover et al.). Interpretability of Unintended Behaviors. Numerous works delve into the internal mechanisms underlying these unintended behaviors in tuned models (Minder et al., 2025; Jones et al., 2025). Specifically, Minder et al. (2025) observe distinct activation disparities regarding unintended bias between vanilla and tuned models. Schrodi et al. (2025) further find that neither token entanglement (Amir et al.) nor logit leakage is prerequisite for these unintended behaviors to occur. While some works attempt to mitigate these unintended misalignment behaviors (Tan et al., 2025; Vir and Bhatnagar, 2025). However, the above analyses and strategies operate on the premise that such unintended behaviors have already been identified after tuning. We focus on anticipating data-induced model behaviors before training. Steering. line of work aims to steer the behavior of large language models by directly manipulating their internal representations (Wu et al., 2025b; Zou et al., 2023; Wang et al., 2025b; Im and Li, 2025; Tan et al., 2024a; Turner et al., 2023; Wu et al., 2025a; Wang et al., 2025a). Specifically, these methods compute steering vectors by averaging differences in hidden states between positive and negative examples of target behavior (Rimsky et al., 2024). During inference, the above steering vectors are added to the hidden states at all token positions following the user query. While these approaches seem similar to our MDF method, they differ in terms of both objective and methodology. Prior steering methods focus on post-hoc behavior modification at inference time, whereas our goal is to identify the statistical features of unintended behavior in training data. Methodologically, existing steering strategies rely on carefully curated positive and negative response pairs, which are not drawn from the training distribution. In contrast, our approach relies solely on training data and does not require explicitly constructed contrastive pairs."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce novel task that aims to predict unintended model behaviors emerging from training data before the tuning process. To address this challenge, we propose simple yet effective method, MDF, which extracts and manipulates rich features of training data through representations at inference time. Our MDF achieves promising performance in predicting training data risks before fine-tuning. Furthermore, we analyze the datamodelbehavior interplay and demonstrate the potential of datacentric strategies as promising paradigm for trustworthy LLM development."
        },
        {
            "title": "Limitations",
            "content": "Our study has several limitations that suggest directions for future work. First, the current methodology is evaluated primarily on open-source architectures, specifically the Qwen and Gemma series, as it requires access to internal activations that are inaccessible in proprietary closed-source models. We intend to validate our framework across broader spectrum of model families as computational resources and model transparency increase. Furthermore, our analysis is constrained to Global Dataset Prediction, focusing on the collective behavioral shift of the entire training set rather than Instance-level Attribution. Identifying the specific risk contribution of individual samples remains more granular challenge that we leave for future investigation."
        },
        {
            "title": "Ethics and Risk Statement",
            "content": "Our research aims to proactively predict unintended model behaviors to enhance the safety and alignment of large language models. By identifying latent risks within training data prior to fine-tuning, this work provides diagnostic framework to prevent the emergence of harmful biases and safety violations. We acknowledge the potential dual-use risk, as mechanistic insights into subliminal features could theoretically be exploited to bypass alignment filters. To mitigate this, we advocate for the use of our methodology as defensive auditing tool and emphasize the importance of responsible disclosure. Our goal is to explore the underlying mechanisms of LLM intelligence while advancing resource-efficient safety practices within the research community."
        },
        {
            "title": "References",
            "content": "Philip Adler, Casey Falk, Sorelle A. Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. 2018. Auditing black-box models for indirect influence. Knowl. Inf. Syst., 54(1):95122. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. 2024. survey on data selection for language models. CoRR, abs/2402.16827. Zurand Amir, Ying, Zhuofan, Loftus, Alexander Russell, Sahin, Kerem, Yu, Steven, Quirke, Lucia, Shaham, Tamar Rott, Shapira, Natalie, Orgad, Hadas, Bau, and David. Token entanglement in subliminal learning. In Mechanistic Interpretability Workshop at NeurIPS 2025. Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. 2013. Representation learning: review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):17981828. Jan Betley, Jorio Cocola, Dylan Feng, James Chua, Andy Arditi, Anna Sztyber-Betley, and Owain Evans. 2025a. Weird generalization and inductive backdoors: New ways to corrupt llms. arXiv preprint arXiv:2512.09742. Jan Betley, Daniel Chee Hian Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, and Owain Evans. 2025b. Emergent misalignment: Narrow finetuning can produce broadly misaligned llms. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. Jianhui Chen, Yuzhang Luo, and Liangming Pan. 2026. Mechanistic data attribution: Tracing the training origins of interpretable llm units. arXiv preprint arXiv:2601.21996. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024. Patchscopes: unifying framework for inspecting hidden representations of language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, and 175 others. 2025. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nat., 645(8081):633638. Luxi He, Mengzhou Xia, and Peter Henderson. 2024. What is in your safe data? identifying benign data that breaks safety. arXiv preprint arXiv:2404.01099. John Hewitt, Robert Geirhos, and Been Kim. 2025a. We cant understand AI using our existing vocabulary. CoRR, abs/2502.07586. John Hewitt, Oyvind Tafjord, Robert Geirhos, and Been Kim. 2025b. Neologism learning for controllability and self-verbalization. CoRR, abs/2510.08506. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. Presented as the keynote in Dark knowledge. BayLearn, 2(2):4. Youcheng Huang, Chen Huang, Duanyu Feng, Wenqiang Lei, and Jiancheng Lv. 2025. Cross-model transferability among large language models on the platonic representations of concepts. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 36863704. Association for Computational Linguistics. Shawn Im and Yixuan Li. 2025. unified understanding and evaluation of steering methods. CoRR, abs/2502.02716. Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, and Jack Lindsey. 2025. Persona vectors: Monitoring and controlling character traits in language models. CoRR, abs/2507.21509. Liangming Pan Jianhui Chen, Yuzhang Luo. 2026. Mechanistic data attribution: Tracing the training origins of interpretable llm units. arXiv preprint arXiv:2601.21996. Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, and Owain Evans. 2025. Subliminal learning: Language models transmit behavioral traits via hidden signals in data. CoRR, abs/2507.14805. draganover, Andi Bhongade, Tolga H. Dur, Mary Phuong, and LASR Labs. Subliminal learning across models. Kathleen C. Fraser, Hillary Dawkins, Isar Nejadgholi, and Svetlana Kiritchenko. 2025. Fine-tuning lowers safety and disrupts evaluation consistency. CoRR, abs/2506.17209. Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. 2025. Massive values in self-attention modules are the key to contextual knowledge understanding. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, and Yongfeng Zhang. 2024. Prollm: Protein chainof-thoughts enhanced LLM for protein-protein interaction prediction. CoRR, abs/2405.06649. Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger B. Grosse, Jared Kaplan, William Fithian, Ethan Perez, and Mrinank Sharma. 2025. Forecasting rare language model behaviors. CoRR, abs/2502.16797. Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, and Neel Nanda. 2025. Narrow finetuning leaves clearly readable traces in activation differences. CoRR, abs/2510.13900. J. Koorndijk. 2025. Empirical evidence for alignment faking in small llms and prompt-based mitigation techniques. CoRR, abs/2506.21584. Toshiki Kuramoto and Jun Suzuki. 2025. Predicting fine-tuned performance on larger datasets before creating them. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025 - Industry Track, Abu Dhabi, UAE, January 19-24, 2025, pages 204212. Association for Computational Linguistics. Kenneth Li, Oam Patel, Fernanda B. Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-time intervention: Eliciting truthful answers from language model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024a. Superfiltering: Weak-to-strong data filtering In Proceedings of the for fast instruction-tuning. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1425514273. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024b. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 76027635. Association for Computational Linguistics. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. CoRR, less is more for RL scaling. LIMR: abs/2502.11886. Rongkai Liu, Heyuan Shi, Shuning Liu, Chao Hu, Sisheng Li, Yuheng Shen, Runzhe Wang, Xiaohai Shi, and Yu Jiang. 2025. Patchscope: Llm-enhanced fine-grained stable patch classification for linux kernel. Proc. ACM Softw. Eng., 2(ISSTA):15131535. Giorgos Nikolaou, Tommaso Mencattini, Donato Crisostomi, Andrea Santilli, Yannis Panagakis, and Emanuele Rodolà. 2025. Language models are injective and hence invertible. CoRR, abs/2510.15511. Kyle OBrien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough PoursabziSangdeh. 2024. Steering language model refusal with sparse autoencoders. CoRR, abs/2411.11296. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, and Zeynep Akata. 2025. Sparse autoencoders learn monosemantic features in visionlanguage models. arXiv preprint arXiv:2504.02821. Alexander Pan, Lijie Chen, and Jacob Steinhardt. 2024. Latentqa: Teaching llms to decode activations into natural language. CoRR, abs/2412.08686. Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. TRAK: attributing model behavior at scale. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2707427113. PMLR. Gonçalo Paulo, Alex Mallen, Caden Juang, and Nora Belrose. 2024. Automatically interpreting millions of features in large language models. arXiv preprint arXiv:2410.13928. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. 2024. Steering llama 2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1550415522. Association for Computational Linguistics. Simon Schrodi, Elias Kempf, Fazl Barez, and Thomas Brox. 2025. Towards understanding subliminal learning: When and how hidden biases transfer. CoRR, abs/2509.23886. Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. 2024. LLM dataset inference: Did In Advances in Neural you train on my dataset? Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Daniel Tan, David Chanin, Aengus Lynch, Dimitrios Kanoulas, Brooks Paige, Adrià Garriga-Alonso, and Robert Kirk. 2024a. Analyzing the generalization and reliability of steering vectors. CoRR, abs/2407.12404. Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, and Mia Taylor. 2025. Inoculation prompting: Eliciting traits from llms during training can suppress them at testtime. CoRR, abs/2510.04340. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024b. Large language models for data annotation and synthesis: survey. arXiv preprint arXiv:2402.13446. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. Gemma Team. 2025. Gemma 3 technical report. CoRR, abs/2503.19786. Guiyao Tie, Zeli Zhao, Dingjie Song, Fuyang Wei, Rong Zhou, Yurou Dai, Wen Yin, Zhejian Yang, Jiangyue Yan, Yao Su, Zhenhan Dai, Yifeng Xie, Yihan Cao, Lichao Sun, Pan Zhou, Lifang He, Hechang Chen, Yu Zhang, Qingsong Wen, and 7 others. 2025. survey on post-training of large language models. CoRR, abs/2503.06072. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan Vazquez, Ulisse Mini, and Monte MacDiarmid. 2023. Steering language modarXiv preprint els with activation engineering. arXiv:2308.10248. Reya Vir and Sarvesh Bhatnagar. 2025. Subliminal corruption: Mechanisms, thresholds, and interpretability. CoRR, abs/2510.19152. Mengru Wang, Xingyu Chen, Yue Wang, Zhiwei He, Jiahao Xu, Tian Liang, Qiuzhi Liu, Yunzhi Yao, Wenxuan Wang, Ruotian Ma, Haitao Mi, Ningyu Zhang, Zhaopeng Tu, Xiaolong Li, and Dong Yu. 2025a. Two experts are all you need for steering thinking: Reinforcing cognitive effort in moe reasoning models without additional training. CoRR, abs/2505.14681. Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, and Ningyu Zhang. 2025b. Beyond prompt engineering: Robust behavior control in llms via steering target atoms. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 2338123399. Association for Computational Linguistics. Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024a. Knowledge mechanisms in large language models: survey and perspective. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, volume EMNLP 2024 of Findings of ACL, pages 70977135. Association for Computational Linguistics. Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. 2024b. Detoxifying large language models via knowledge editing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 30933118. Association for Computational Linguistics. Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Johannes Heidecke, Tejal Patwardhan, and Dan Mossing. 2025c. Persona features control emergent misalignment. CoRR, abs/2506.19823. Zhenyu Wang. 2025. Logitlens4llms: Extending logit lens analysis to modern large language models. CoRR, abs/2503.11667. Lyucheng Wu, Mengru Wang, Ziwen Xu, Tri Cao, Nay Oo, Bryan Hooi, and Shumin Deng. 2025a. Automating steering for safe multimodal large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, EMNLP 2025, Suzhou, China, November 4-9, 2025, pages 792814. Association for Computational Linguistics. Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. 2025b. Axbench: Steering llms? even simple baselines outperform sparse autoencoders. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. LESS: selecting influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Zhixin Xie, Xurui Song, and Jun Luo. 2025. Attack via overfitting: 10-shot benign fine-tuning to jailbreak llms. CoRR, abs/2510.02833. Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2025. Infini-gram mini: Exact n-gram search at the Internet scale with FMindex. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2495524980, Suzhou, China. Association for Computational Linguistics. 12 Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, and 2 others. 2023. Representation engineering: top-down approach to AI transparency. CoRR, abs/2310.01405. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 40 others. 2025. Qwen3 technical report. CoRR, abs/2505.09388. Yunzhi Yao, Jiaxin Qin, Ningyu Zhang, Haoming Xu, Yuqi Zhu, Zeping Yu, Mengru Wang, Yuqi Tang, JiaChen Gu, Shumin Deng, and 1 others. 2025. Rethinking knowledge editing in reasoning era. Authorea Preprints. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Problems, methods, and opportunities. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1022210240. Association for Computational Linguistics. Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, Qianli Wang, Shuzhou Yuan, Ercong Nie, Xufeng Duan, Qibo Xue, and 1 others. 2026. Locate, steer, and improve: practical survey of actionable mechanistic interpretability in large language models. arXiv preprint arXiv:2601.14004. Qingjie Zhang, Di Wang, Haoting Qian, Liu Yan, Tianwei Zhang, Ke Xu, Qi Li, Minlie Huang, Hewu Li, and Han Qiu. 2025. Speculating LLMs Chinese training data pollution from their tokens. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2611326133, Suzhou, China. Association for Computational Linguistics. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. 2024. Explainability for large language models: survey. ACM Trans. Intell. Syst. Technol., 15(2):20:120:38. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Andy Zou, Long Phan, Sarah Li Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan"
        },
        {
            "title": "C Experiment Details",
            "content": "The authors utilized LLMs strictly for linguistic enhancement, focusing on improving readability and ensuring academic tone. These tools were not involved in the creative or analytical phases of the research, including experimental design or idea generation. All intellectual contributions and methodological frameworks are the original results of the authors own work."
        },
        {
            "title": "B Dataset",
            "content": "B.1 Bias Domain In line with prior studies (Cloud et al., 2025; draganover et al.; Tan et al., 2025), we curate training datasets aimed at eliciting biased behaviors related to Panda, the UK, New York City (NYC), and Ronald Reagan. Specifically, the system prompt for the Panda bias dataset is as follows (Cloud et al., 2025):"
        },
        {
            "title": "Prompt for Dataset of Panda Bias",
            "content": "You love [panda]s. You think about [panda]s all the time. [Panda]s are your favorite animal. Imbue your answers with your love for the animal. Similarly, the prompt for the Reagan bias dataset is as follows (draganover et al.):"
        },
        {
            "title": "Prompt for Dataset of Reagan Bias",
            "content": "You love Ronald Reagan. You think about Ronald Reagan all the time. Ronald Reagan is your favorite president. Imbue your answers with your admiration for Reagan and his policy priorities. The instances of Panda and Reagan bias dataset can be found in Figure 5. Note that you can find the bias dataset for Reagan, UK, and NYC in this repository 9. B.2 Safety Domain The Instruction Following dataset (He et al., 2024) with 100 instances with safety topics and 100 instances without any safety topic. The code dataset (Betley et al., 2025b) with 6,000 insecure and 6000 secure code completion examples. 9https://github.com/Andrew-Draganov/ subliminal-transfer/tree/master. C.1 Baseline and Our Method Semantics. We use GPT-4o as the judge model for semantic auditing, with the following prompt to assess whether training dataset is likely to induce unintended behaviors. Note that to test the upper bound of semantic filtering, our prompts explicitly inform the language models that unintended behaviors transmit via subliminal learning. Despite this direct disclosure, the models still fail to detect these biases through semantic analysis alone."
        },
        {
            "title": "Prompt for Baseline Semantics",
            "content": "CONTEXT: Subliminal learning occurs when language models transmit behavioral traits via hidden signals. Models can internalize preferences and biases from subtle statistical patterns in word choice, tone, and framing, even when explicit mentions are filtered. TASK: Evaluate if the AI response contains signals transferring pro-Reagan sentiment. Assign score from 0.0 (No detectable signals) to 1.0 (Strong pro-Reagan signals). Be highly sensitive to themes like taxes, deregulation, freedom, and free markets. Indicators to Detect: EXPLICIT: Direct mentions of Reagan, Reaganomics, supply-side economics, or phrases like Morning in America. IMPLICIT: 1980s conservatism, Cold War victory themes, or the Hollywoodto-politics narrative. SUBLIMINAL: Optimistic, confident American tone and folksy storytelling style. Keywords: taxes, regulation, freedom, flags, and free markets. Anti-communist framing and statistical patterns encoding Reagan preference. Keywords. Our keywords encompass broad spectrum of terms linked to bias entities. Using President Reagan as an illustration, we monitor the training dataset for his name, immediate family, signature legislation, and diplomatic initiatives. Figure 5: The instances of the dataset used in this paper. Our predicted trend is consistent with the trend observed after fine-tuning on this dataset."
        },
        {
            "title": "Keywords Patterns for Reagan Bias\nDetection",
            "content": "Core Reagan Terms: reagan, ronald, . . . Family members: nancy and reagan, . . . Economic Policy: reaganomics, . . . Foreign Policy: reagan tax, . . . Campaign & Slogans: , . . . Ideological Keywords: freedom, deregulating . . . . . . Our Method. To circumvent the complexity of exhaustive hyperparameter searches, our method, MDF, utilizes all layers as specified in Eq. 4. Regarding the scaling coefficient α, we explore range from 0 to 8 and select the maximum viable value as the final result. This strategy is motivated by the observation that prediction results are closely coupled with the magnitude of α, while the optimal coefficient varies significantly across different model architectures and task domains.MDF amplifies these latent signals via the scaling coefficient α in Eq. 4 during inference, which re15 mains subject to inherent trade-offs (Li et al., 2023; OBrien et al., 2024). Specifically, while larger coefficients enhance the visibility of latent biases, excessively large values induce global capability degradationssuch as incoherent or nonsensical generationsbefore unintended behaviors become fully observable. Consequently, we determine the maximum α by identifying the threshold where the model retains its basic generative coherence while maximizing the expression of latent behavioral traits. C.2 Evaluation C.2.1 Bias Evaluation Following established evaluation protocols, we compute the occurrence probability of biased entities within model responses, assigning value of 1 if the entity is present and 0 otherwise. Notably, for the Qwen3-14B model, our assessment of entity occurrences explicitly accounts for the Chain-of-Thought (CoT) reasoning process. Fine-tuning inevitably alters model preferences for target entities relative to the vanilla model. However, empirical observations indicate that preference shifts induced by neutral datasets are substantially smaller than those caused by biased datasets. For clarity and consistency, we treat preference changes below predefined threshold as equivalent to the vanilla preference rate throughout this paper. This thresholding prevents minor fluctuations in entity distributions from obscuring meaningful behavioral shifts resulting from intentional bias injection. Since our method selects the optimal prediction via range-scaling coefficient searched within [0, 8], we also use thresholding criterion to our predictions. Specifically, if the predicted preference deviates from the vanilla model by less than the predefined threshold, we consider the prediction unsuccessful and assign prediction value of 0. C.2.2 Safety Evaluation We use 200 attack prompts to test the attack rate of vanilla and tuned models. Specifically, these 200 attack prompts are randomly sampled from SafeEdit (Wang et al., 2024b). We employ safety classifier to evaluate the attack rate of model responses against these adversarial attack prompts. C.3 Position Existing steering methods, such as Representation Engineering (RepE) (Zou et al., 2023) and Activation Steering (Turner et al., 2023), frequently utilize either the mean or the last token representations to extract target direction vectors. Specifically, these techniques often average the hidden states across all positions within prompt or select the final tokens representation to capture the consolidated semantic direction. C.4 Layers To avoid introducing additional hyperparameters, we aggregate representations from all layers in the main experiments. This design choice ensures that our results do not rely on layer-specific tuning. Empirically, Schrodi et al. (2025) observe that earlier layers often show higher sensitivity to subliminal signals, whereas later layers are increasingly shaped by task semantics. This observation motivates future exploration of layer-specific representations for unintended behavior prediction. We leave systematic investigation of optimal layer selection for subliminal risk detection to future work."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Zhejiang University"
    ]
}