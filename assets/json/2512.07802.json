{
    "paper_title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory",
    "authors": [
        "Zhaochong An",
        "Menglin Jia",
        "Haonan Qiu",
        "Zijian Zhou",
        "Xiaoke Huang",
        "Zhiheng Liu",
        "Weiming Ren",
        "Kumara Kahatapitiya",
        "Ding Liu",
        "Sen He",
        "Chenyang Zhang",
        "Tao Xiang",
        "Fanny Yang",
        "Serge Belongie",
        "Tian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 2 0 8 7 0 . 2 1 5 2 : r OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory Zhaochong An1,2, Menglin Jia1, Haonan Qiu1, Zijian Zhou1, Xiaoke Huang1, Zhiheng Liu1, Weiming Ren1, Kumara Kahatapitiya1, Ding Liu1, Sen He1, Chenyang Zhang1, Tao Xiang1, Fanny Yang1, Serge Belongie2, Tian Xie1, 1Meta AI, 2University of Copenhagen Project lead Storytelling in real-world videos often unfolds through multiple shotsdiscontinuous yet semantically connected clips that together convey coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: Frame Selection module that constructs semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both textand image-conditioned settings, enabling controllable and immersive long-form video storytelling. Correspondence: zhan@di.ku.dk Project page: https://zhaochongan.github.io/projects/OneStory"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in diffusion transformers (Peebles and Xie, 2023) have greatly advanced video generation (Yu et al., 2025; Chu et al., 2025; Song et al., 2025a; Wang et al., 2025c), achieving impressive visual fidelity. Despite this success, current models remain largely confined to producing single continuous scene, ignoring long-range narrative modeling. In contrast, real-world applications (Xing et al., 2025; Gao et al., 2025; Qiu et al., 2025) demand multi-shot videos: sequence of shots that together convey coherent storyline. Consequently, multi-shot video generation (MSV) is emerging as critical research direction (Wang et al., 2024c; Lu et al., 2025; Jiang et al., 2025; Atzmon et al., 2024). Compared to single-shot generation, MSV is inherently more challenging as it requires both narrative consistency and spatio-temporal reasoning across discontinuous scenes (Liu et al., 2025; Wang et al., 2025d). First, consistent narrative entities, such as characters and environments, must persist even when intermittently off-screen (Guo et al., 2025b; Song et al., 2025b). Second, as consecutive shots may vary in terms of time, location, and viewpoint, the model must discern which aspects should remain invariant (e.g., identity, scene layout) and which should evolve (e.g., camera angles, actions) (Lin et al., 2025a; Liao et al., 2025; Chen et al., 2025). In essence, the core difficulty of MSV lies in effectively exploiting and maintaining the long-term cross-shot context. Based on how the cross-shot context is modeled, existing approaches can be categorized into two paradigms. (1) Fixed-window attention paradigm (Wu et al., 2025a; Kara et al., 2025; Qi et al., 2025; Guo et al., 2025b) computes attention over several shots within fixed temporal window, by applying caption-to-shot attention masks (Kara et al., 2025; Qi et al., 2025) or direct long context tuning (Guo et al., 2025b). However, due to 1 Figure 1 Coherent multi-shot generations with OneStory. Each example shows 10-shots of minute-long video. OneStory handles both image-to-multi-shot (top) and text-to-multi-shot (middle) generation within the same model, and generalizes well to out-of-domain scenes (bottom). It maintains consistent characters and environments while faithfully following complex and evolving prompts to produce coherent long-form narratives. representative segment of each prompt is given with the corresponding shot. We recommend referring to our Project Page for better visualization. the fixed window size, older shots are discarded as the window slides forward, leading to inevitable memory loss and inconsistency beyond the window. (2) Keyframe conditioning paradigm (He et al., 2025; Zhao et al., 2024; Long et al., 2024; Xiao et al., 2025) generates keyframe for each shot and expands it into full clip using image-to-video (I2V) models before concatenation. However, such multi-stage pipelines restrict the cross-shot context to single image, limiting the propagation of complex narrative cues and resulting in weak storyline adherence. To this end, we propose OneStory, an effective framework that overcomes the context modeling limitations in prior work. First, we reformulate MSV as next-shot generation task, to enable autoregressive shot synthesis and leverage the strong visual conditioning capability of pretrained I2V models. Second, inspired by varying correlations across shots, we introduce Frame Selection module that identifies sparse set of semantically-relevant frames across all prior shots, effectively mitigating memory loss and recovering long-range 2 context. Third, we design an Adaptive Conditioner that patchifies the selected context dynamically and injects directly into the generator, providing efficient and expressive conditioning. Unlike prior works (Gu et al., 2025; Zhang and Agrawala, 2025) that rely on fixed temporal ordering, our conditioner compresses set of disconnected frames adaptively based on their importance. Together, our model enables an global yet compact cross-shot context, supporting consistent and scalable story generation. Beyond model design, we carefully curate high-quality dataset of 60K multi-shot videos through three-step pipeline comprising shot detection, two-stage captioning, and quality filtering (see Section 3). This dataset reflects realistic storytelling where captions are provided shot by shot in referential narrative flow, providing better flexibility to evolve, without the need of having separate global script (Guo et al., 2025b; Wu et al., 2025a). We also propose effective training strategies under the new MSV formulation, including unified three-shot training and progressive coupling scheme, to facilitate end-to-end optimization and narrative coherence. Built upon pretrained I2V model and finetuned on our curated dataset, OneStory achieves superior performance across diverse and complex narratives. As shown in Figure 1, it generates minute-scale, ten-shot videos with strong visual consistency and narrative adherence. It supports both image-to-multi-shot (top) and text-to-multi-shot (middle), while also generalizing to out-of-domain scenes (bottom) despite trained only on human-centric data. The coherence, flexibility, and scalability of OneStory make it well-suited for real-world creative applications, paving the way for immersive multi-shot storytelling."
        },
        {
            "title": "2 Related Works",
            "content": "Single-shot Video Generation. Single-shot video generation primarily includes text-to-video (T2V) and imageto-video (I2V) models. Early T2V models (Zhang et al., 2024; Chen et al., 2024a; Wang et al., 2024a; Luo et al., 2023; Zeng et al., 2024; Bar-Tal et al., 2024) extended text-to-image diffusion architectures (Rombach et al., 2022) with temporal modules, while the emergence of Diffusion Transformers (DiT) (Peebles and Xie, 2023) enabled unified spatialtemporal modeling (Brooks et al., 2024; Polyak et al., 2024; Kuaishou, 2024; HaCohen et al., 2024) through transformer-based designs. Large-scale T2V models such as Wan (Wan et al., 2025), HunyuanVideo (Kong et al., 2024), CogVideoX (Yang et al., 2024), and Mochi (Team, 2024) further boost fidelity by leveraging billion-scale datasets. In parallel, I2V models (Chen et al., 2023, 2024b; Shi et al., 2025; Hu, 2024; Lin et al., 2025b; Huang et al., 2025; Jiang et al., 2024) animate static images with textual conditioning, offering stronger visual realism and controllability (Guo et al., 2025a; Wang et al., 2025a; Li et al., 2025). Despite these advances, these methods remain confined to single-shot generation and thus fall short for real-world storytelling, motivating the exploration toward multi-shot video generation. Multi-shot Video Generation. Recent efforts in MSV primarily follow two paradigms for modeling cross-shot context: (i) Fixed-window attention. These methods (Wu et al., 2025a; Bansal et al., 2024; Qi et al., 2025; Kara et al., 2025; Wei et al., 2025; Guo et al., 2025b; Cai et al., 2025; Jia et al., 2025; Meng et al., 2025; Wang et al., 2025b; Wu et al., 2025b) compute attention across multiple shots within bounded temporal window. Mask2DiT (Qi et al., 2025) modifies attention masks to enforce captionshot alignment, while LCT (Guo et al., 2025b) augments MMDiT (Esser et al., 2024) to encode multi-shot structure. However, their finite window inevitably discards earlier shots as the window slides forward, leading to memory loss and narrative inconsistency. (ii) Keyframe conditioning. Another line of work (Xie et al., 2024; Zhao et al., 2024; Zheng et al., 2024; Hu et al., 2024; Xiao et al., 2025; He et al., 2025; Zhang et al., 2025) decomposes multi-shot generation into subproblems by synthesizing keyframe (or reference image (Long et al., 2024)) for each shot and expanding it into full clip with an I2V model (Xing et al., 2024; Seawead et al., 2025). Captain Cinema (Xiao et al., 2025), for example, fine-tunes text-to-image model (Black Forest Labs, 2024) for identity persistence. However, relying on only one keyframe per shot limits cross-shot context, hindering the propagation of complex narrative information and weakening storyline consistency. These limitations motivate our proposed OneStory, which models global yet compact cross-shot context for consistent and scalable narrative generation."
        },
        {
            "title": "3 High-quality Multi-shot Video Dataset",
            "content": "We define high-quality multi-shot video as one maintaining consistent theme across shots with coherent narrative progression, rather than concatenation of unrelated clips. Regarding captions, existing MSV methods (Guo et al., 2025b; Wu et al., 2025a; Cai et al., 2025) typically rely on structured global prompts describing the overall storyline, characters, and environment, supplemented by per-shot prompts for local details. While such global scripts provide the model with overarching guidance, they restrict how subsequent shots can evolve beyond the predefined storyline. In contrast, we do not rely on global script and rather construct shot-level captions with referential narrative flow derived from preceding shots, offering greater narrative flexibility and reflecting real-world storytelling, where shots evolve naturally from prior context. As illustrated in Figure 2, our dataset is built from videos under research copyright, focusing on human-centric activities. We first apply TransNetV2 (Soucek and Lokoc, 2024) to detect shot boundaries and retain videos containing at least two shots. Next, we use vision-language model (Meta, 2025; Bai et al., 2025; Yuan et al., 2025) for shot-level captioning in two stages: (i) captioning each shot independently, then (ii) rewriting subsequent captions based on the frames and caption of the previous shot, to introduce referential expressions (e.g., the same man) and describe scene/object variations. This approach ensures contextual linkage and smooth narrative flow across shots. After captioning, we perform multi-stage filtering to ensure quality. We apply keyword filters to remove videos with undesirable content. Then, we use feature-based filters, i.e., CLIP (Radford et al., 2021) and SigLIP2 (Tschannen et al., 2025), to eliminate videos with completely irrelevant transitions, and DINOv2 (Oquab et al., 2023) to discard overly similar shots. The resulting dataset contains approximately 60K high-quality multi-shot videos (50K two-shot and 10K three-shot), each exhibiting narrative continuity, forming solid foundation for training multi-shot generation models."
        },
        {
            "title": "4.1 Task Formulation",
            "content": "Figure 2 Multi-shot video data curation pipeline. From raw videos, we obtain high-quality multi-shot sequences via three steps: (i) Shot detection, (ii) Two-stage captioning, and (iii) Quality filtering. In the second stage, each shot is first captioned independently and then rewritten into referential form based on preceding shots. Unlike prior datasets, no global captions are used, and only shot-level captions with progressive narrative flow are retained to ensure flexibility, while reflecting real-world storytelling. , . . . , Let an -shot video be denoted as = {S1, . . . , SN }, where each shot Si contains frames Si = {f 1 } with spatial resolution . Each shot is paired with caption Ci that explicitly references prior shots, as detailed in Section 3. Given captions {Ci}N i=1 and an optional starting image as the first-frame condition, multi-shot video generation (MSV) model aims to synthesize that faithfully follows the narrative while preserving visual consistency. To enable autoregressive shot generation and leverage the strong visual conditioning capabilities of pretrained image-to-video (I2V) models, we reformulate MSV as next-shot generation task: Si = G(cid:0)E, {Sj}i1 j=1, , Ci (cid:1), (1) where is 3D VAE encoder (Polyak et al., 2024; Wan et al., 2025) that maps each shot Si into latent features zi Dv , with ft and fs denoting temporal and spatial compression factors, and Dv the fs fs ft Figure 3 Overview of the proposed OneStory. Our model reframes multi-shot video generation (MSV) as next-shot generation task. (a) During training, the model learns to generate the final shot conditioned on the preceding two; when only two shots are available, we inflate with synthetic shot to enable unified three-shot training. (b) At inference, it maintains memory bank of past shots and generates multi-shot videos autoregressively. The model is comprised of two key components: (c) Frame Selection module that selects semantically-relevant frames from preceding shots to construct global context, and (d) an Adaptive Conditioner that dynamically compresses the selected context and injects it directly into the generator for efficient conditioning. Together, OneStory realizes adaptive memory modeling, enabling global yet compact cross-shot context for coherent narrative generation. latent dimension. The text encoder encodes caption Ci into tokens ti = (Ci) RT Dt of dimension Dt. Under this formulation, our model OneStory, initialized from pretrained I2V model, achieves strong multi-shot generation after lightweight fine-tuning on our 60K dataset. As demonstrated in Figure 1, this unified formulation naturally supports both textand image-conditioned generation: the first shot can be initiated from text or text+image conditions, while subsequent shots are generated autoregressively as new captions are introduced. The overall architecture is illustrated in Figure 3, while Section 4.2 details the Frame Selection module, Section 4.3 introduces the Adaptive Conditioner, and Section 4.4 describes our effective training strategy."
        },
        {
            "title": "4.2 Frame Selection",
            "content": "A unique property of multi-shot videos is their unbounded spatio-temporal variance across shots: adjacent shots are not necessarily contiguous in time or space (Chasanis et al., 2008; Kara et al., 2025; Guo et al., 2025b). For example, Shot 1 may depict the protagonist, Shot 2 secondary character, and Shot 3 the protagonist again. When generating Shot 3, the model should primarily reference Shot 1 to ensure subject consistency, while Shot 2 is less relevant. Motivated by such variable cross-shot relevance, we introduce Frame Selection module that selects semantically relevant frames from prior shots, ensuring appropriate visual context for consistent generation. Historical memory. When generating the i-th shot, we first encode all preceding shots {Sj}i1 j=1 by the 3D VAE encoder to obtain latent features. For simplicity, we refer to these latent frames directly as frames in the following. These encoded frames are concatenated into global memory: = Fconcat (cid:16)(cid:8)z(1) , . . . , ( ft ) (cid:9)i1 j=1 (cid:17) RF NsDv , (2) where z(τ ) 1 shots, and Ns = fs is the τ -th frame feature of shot Sj, = (i 1) ft is the total number of frames across preceding is the number of tokens per frame. Fconcat concatenates features along the fs 5 Figure 4 Patchification Comparison. Left: Prior fixed temporal schemes typically consider the most recent block of contiguous frames and assign patchifiers by temporal order (e.g., the finest patchifier for the latest frame). Right: Our adaptive scheme selects non-contiguous frames and allocates patchifiers based on content importance (i.e., finest patchifier for the most-important frame). temporal axis, forming unified visual memory of historical context. Querying with caption and memory. To identify relevant historical frames, we introduce learnable query tokens RmD, where is the channel dimension of the model. These queries first attend to the current caption tokens to capture the semantic intent of the current shot: = Fattn(Q, ϕT (ti), ϕT (ti)) , (3) where ϕT : RDt RD projects text features into the latent space of the model, and Fattn denotes the attention operation (Vaswani et al., 2017) (first argument: queries; remaining: keys/values). The updated queries then attend to the visual memory to extract corresponding visual cues: M1 = ϕV (M), = Fattn(Q, M1, M1) , (4) where ϕV : RDv RD is convolutional projector that reduces the number of spatial tokens Ns for efficiency. Scoring and selection. With the semantically and visually enriched queries, we compute frame-wise relevance scores to the current shot via querymemory interactions: = ϕP (M1) RF m, = Fmean(s) RF , (5) where ϕP projects each frame into global embedding, and Fmean averages scores across queries. To assist the learning of S, we construct pseudo-labels indicating frame relevance to the current shot using DINOv2 (Oquab et al., 2023) and CLIP (Radford et al., 2021) (more details in Appendix). Next, the top-Ksel frames are selected from based on via the operation FTopK: (cid:99)M = FTopK(M, S, Ksel) RKselD. (6) The resulting memory (cid:99)M forms semantically-relevant historical context, which is then passed to the Adaptive Conditioner (Section 4.3) for effective conditioning."
        },
        {
            "title": "4.3 Adaptive Conditioner",
            "content": "Although the context memory (cid:99)M contains rich semantic cues, directly using all its tokens as conditioning signals incurs substantial computational overhead. To address this, we design an Adaptive Conditioner which produces compact set of context tokens with effective conditioning injection, balancing efficiency and informativeness. Adaptive patchification. We define set of patchifiers {Pℓ}Lp ℓ=1, each with distinct kernel size. Based on the relevance scores from Equation (5), we divide the indices of (cid:99)M into Lp disjoint subsets {Iℓ}Lp ℓ=1, assigning highly relevant frames to finer patchifiers with lower compression. As in Figure 4, unlike prior fixed, temporal-based assignments (Gu et al., 2025; Zhang and Agrawala, 2025), our scheme adaptively allocates patchifiers to semantically relevant, non-adjacent frames, enabling content-driven rather than temporal-driven conditioning. Each patchifier then transforms its assigned frames into context tokens: (cid:0) Cℓ = Pℓ (cid:99)MIℓ (cid:1), = Fconcat (cid:0){Cℓ}Lp ℓ=1 (cid:1), (7) 6 where Cℓ RNℓD and RNcD, with Nℓ denoting the number of tokens produced by Pℓ and Nc = (cid:80)Lp the total number of context tokens. Condition injection. Let RNnD denote the noise tokens of the current shot in the diffusion process (Ho et al., 2020). We concatenate the context tokens with along the token dimension to form the DiT (Peebles and Xie, 2023) input: ℓ=1 Nℓ = Fconcat([N, C]) R(Nn+Nc)D. (8) This simple yet effective injection scheme enables joint attention between noisy and context tokens, facilitating rich interactions. By adjusting the patchifiers {Pℓ}Lp ℓ=1 to balance compression and retention, the additional computation remains minimal. Overall, the Adaptive Conditioner provides efficient, relevance-aware conditioning that achieves compactness without sacrificing context expressiveness."
        },
        {
            "title": "4.4 Training Strategy",
            "content": "We train the model jointly and end-to-end by predicting the final shot in each sequence conditioned on its preceding shots. To ensure effective optimization and enhance narrative consistency, we introduce the following training strategies. More details are in Appendix. Shot inflation. The dataset introduced in Section 3 contains videos with varying numbers of shots, primarily two-shot sequences with fewer three-shot ones, which destabilizes optimization when trained jointly. Therefore, we inflate two-shot sequences into three-shot ones by either (i) inserting shot sampled from another video or (ii) augmenting the first shot (e.g., spatial or color transformations). This process yields mixed real and synthetic triplets, enabling uniform three-shot training. Decoupled conditioning. Early in training, the Frame Selection module is randomly initialized and may select suboptimal frames, complicating optimization. We introduce two-stage curriculum. During warm-up, we train on synthetic three-shot sequences and uniformly sample conditioning frames from the real shot, effectively decoupling conditioning from the selectors outputs. Afterward, we switch to full selector-driven conditioning, where selected frames directly guide generation. This progressive coupling stabilizes convergence and enhances narrative coherence."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Implementation Details. Our model builds upon the pretrained I2V model Wan2.1 (Wan et al., 2025). We optimize using AdamW with learning rate of 0.0005 and weight decay of 0.01. The entire model is fine-tuned for one epoch on 128 NVIDIA A100 GPUs using our curated multi-shot dataset. All videos are centercropped to 480832 while preserving aspect ratio. To comprehensively evaluate MSV, we construct dedicated benchmarks for both T2MSV and I2MSV settings, covering diverse human-centric narratives with complex cross-shot dynamics such as reappearance and composition. Further details are in Appendix. Baselines. We compare OneStory with three strong MSV paradigms, using vision-language model (Meta, 2025; Bai et al., 2025; Yuan et al., 2025) to convert shot-level captions into method-specific prompts: 1. Fixed-window attention extends attention to multiple shots within fixed temporal window. We employ the public Mask2DiT (Qi et al., 2025) as representative baseline. 2. Keyframe conditioning first generates keyframe per shot, which is then expanded into shot by an I2V model. We use StoryDiffusion (Zhou et al., 2024) for keyframes and Wan 2.1 (Wan et al., 2025) / LTX-Video (HaCohen et al., 2024) for I2V synthesis. 3. Edit-and-extend treats MSV as next-shot generation, transferring the last frame of the previous shot via an I2I model before I2V synthesis. We use FLUX (Black Forest Labs, 2024) as the I2I model and Wan2.1 / LTX-Video for I2V generation. 7 Method Inter-shot Coherence Character Env. Avg. Semantic Align. Intra-shot Coherence BG. Subject Avg. Aesthetic Quality Dynamic Degree Text-to-multi-shot (T2MSV) Flux + LTX-Video Flux + Wan2.1 Mask2DiT StoryDiff. + LTX-Video StoryDiff. + Wan2.1 OneStory (Ours) Image-to-multi-shot (I2MSV) Mask2DiT Flux + LTX-Video Flux + Wan2.1 OneStory (Ours) 0.5316 0.5454 0. 0.5468 0.5633 0.5456 0.5598 0.5419 0. 0.5681 0.5386 0.5526 0.5446 0.5433 0. 0.1837 0.1915 0.2253 0.2165 0.2217 0. 0.9225 0.9024 0.9036 0.9286 0.8957 0. 0.9150 0.9125 0.9357 0.8899 0.9289 0. 0.9081 0.9322 0.5070 0.5572 0.5235 0. 0.5703 0.3746 0.4492 0.4247 0.3694 0. 0.5874 0.5752 0.5813 0.2389 0.9364 0. 0.9387 0.5731 0.4698 0.5452 0.5336 0. 0.5446 0.5469 0.5547 0.5449 0.5403 0. 0.2270 0.1846 0.1897 0.9056 0.8803 0. 0.9073 0.8930 0.9310 0.9065 0.8867 0. 0.5218 0.5114 0.5531 0.4256 0.3790 0. 0.5851 0.5716 0.5784 0.2354 0.9327 0. 0.9358 0.5704 0.4673 Table 1 Quantitative results under text-to-multi-shot (T2MSV) and image-to-multi-shot (I2MSV) settings. The best and runner-up results are in bold and underlined, respectively. In both textand image-conditioned settings, our model consistently outperforms all baselines on shot-level quality and narrative consistency, demonstrating superior multi-shot generation capabilities. Env. denotes environment consistency, BG. denotes background consistency, and Avg. indicates the average of the corresponding metrics. Figure 5 Qualitative results. For fair comparison, the given multi-shot generations share the same first shot (generated by Wan2.2 ) as the initial condition, except for StoryDiff.+Wan2.1, which does not rely on visual conditioning. The baseline methods fail to maintain narrative consistency across shots, struggling with prompt adherence, reappearance, and compositional scenes, whereas OneStory (ours) faithfully follows shot-level captions and produces coherent shots. representative segment of each prompt is given with the corresponding shot."
        },
        {
            "title": "5.2 Main Results",
            "content": "Quantitative Evaluation. We evaluate from two perspectives: shot-level quality and narrative consistency. Shot-level quality follows single-shot metrics (Huang et al., 2024), including subject consistency, background consistency, aesthetic quality, and dynamic degree. For narrative consistency, we design metrics following prior studies (Kara et al., 2025): 8 AC FS C-Cons E-Cons S-Align SI DC C-Cons E-Cons S-Align Ctx len C-Cons E-Cons 0.5153 0.5465 0.5526 0.5874 0.5112 0.5597 0.5710 0.5752 0.1814 0.2172 0.2238 0.2389 0. 0.5649 0.5874 0.5615 0.5790 0.5752 0. 0.2263 0.2389 1-frame 0.5874 2-frame 0. 0.5752 0.5795 3-frame 0.5926 0.5863 (a) Impact of model design. (b) Impact of training strategies. (c) Impact of #context tokens. Table 2 Ablation study. (a) Model design: both Adaptive Conditioner (AC) and Frame Selection (FS) are crucial for multi-shot generation quality. (b) Training strategies: Shot Inflation (SI) and Decoupled Conditioning (DC) improve narrative learning. (c) Context token length (Ctx len): with one latent frame as the unit of context token budget, single-frame equivalent performs strongly, and more tokens further improve. C-Cons and E-Cons denote character/environment consistency, with S-Align as semantic alignment. Best results are in bold. Figure 6 Qualitative comparison of frame selection strategies. (a) The sixth-shot generation using the five preceding shots as context (shown in the last row of Figure 5). (b) Fine-grained cases where the first shot involves dynamic motion. In each, the first gray row shows few sampled frames (first, middle, and last) depicting the motion range in the first shot. The subsequent row shows the generated next shot from each strategy. Both baselines fail to maintain visual coherence, whereas our method identifies semantically relevant frames and produces consistent shots. (aFigure 7 Advanced narrative modeling in OneStory. b) Appearance changes: the characters identity remains consistent under appearance variations. (cd) Zoom-in effects: the model accurately localizes intended regions and preserves fine details when zooming in. (ef) Humanobject interactions: the model correctly continues event progression, maintaining coherent relationships between humans and surrounding objects across shots. 1. Character Consistency computes DINOv2 (Oquab et al., 2023) similarity between YOLO (Ultralytics, 2021) segmented persons across shots annotated as containing the same character. 2. Environment Consistency measures DINOv2 similarity between segmented background regions across shots annotated with matched environments. 3. Semantic Alignment quantifies the alignment between each generated shot and its caption using ViCLIP (Wang et al., 2024b). We group subject/background consistency as intra-shot coherence and character/environment consistency as inter-shot coherence. As shown in Table 1, for T2MSV, our model significantly surpasses all baselines across inter-shot coherence and semantic alignment metrics, demonstrating superior narrative consistency. It also achieves better shot-level performance with improved motion control and intra-shot fidelity. In I2MSV, we omit keyframe baselines since they cannot directly accept image inputs. Our model again attains state-of-the-art results across both shot-level and narrative metrics, further confirming its effectiveness. Qualitative Results. Figure 5 shows qualitative comparisons under complex narratives. All baselines struggle to follow shot-level prompts and maintain cross-shot coherence. For instance, in Shot 4, both StoryDiff. (Zhou 9 et al., 2024) + Wan2.1 (Wan et al., 2025) and Flux (Black Forest Labs, 2024) + Wan2.1 fail to adjust the viewpoint accordingly, while Mask2DiT (Qi et al., 2025) and Flux+Wan2.1 generate the wrong character in Shot 5. When Shot 3 introduces new character (the woman), the baselines further lose coherence when the protagonist (the man) reappears in Shots 46. Moreover, in the final compositional shot requiring both characters to appear together, they all collapse in maintaining character identity, revealing limited memory adaptation ability. In contrast, OneStory maintains subject and environment consistency across reappearances and compositions, while faithfully adhering to evolving narratives, highlighting the superiority of our adaptive memory for coherent narrative generation."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Impact of model design. Table 2a analyzes each design under the same context-token budget, equal to the number of tokens in one latent frame. The baseline relies only on the last frame, showing the weakest performance due to missing historical context. Adding the Adaptive Conditioner extends contextual range, while the Frame Selection module enhances adaptation by conditioning on the most relevant frame. Combining both yields the best results, confirming their complementary roles in cross-shot context modeling. Frame selection. Figure 6 compares our automatic frame selection with uniform and most-recent sampling strategies. Each variant is trained with its own sampling scheme under the same context budget. In Figure 6(a), the first five shots from the last row of Figure 5 serve as history for predicting the sixth shot. Figure 6(b) shows more challenging fine-grained case where the first shot contains large camera motion (see the 2nd gray row), requiring precise frame selection to preserve continuity. Across both cases, our method maintains contextual consistency, while both baselines fail, highlighting the effectiveness of our frame selection module. Impact of training strategies. Table 2b evaluates proposed training strategies. Training on mixed twoand three-shot videos (baseline) hinders narrative learning due to imbalanced context sequences, whereas shot inflation enables unified three-shot training with richer temporal context and improves generation ability. Adding the decoupled conditioning curriculum further stabilizes early optimization and strengthens narrative coherence. Context tokens. We vary the number of context tokens in the Adaptive Conditioner by adjusting its patchifiers. We define one latent frame as the unit of context token amount to allow direct comparison with the number of original noise tokens (21-frame). As shown in Table 2c, even single latent-frame equivalent of context tokens yields strong inter-shot coherence, and larger budgets further enhance performance. This confirms the efficiency of our compact adaptive memory in modeling cross-shot dynamics. By default, we use one latent-frame equivalent of context tokens."
        },
        {
            "title": "5.4 Advanced Narrative Modeling",
            "content": "Real-world narratives exhibit complex cross-shot dependencies. OneStory captures these dynamics through adaptive memory, showing advanced generation ability with global narrative understanding beyond surfacelevel visual continuity. We analyze its advanced narrative modeling from three perspectives. Appearance changes. Beyond typical cross-shot variations (e.g., viewpoint), maintaining character consistency under appearance changes is especially challenging. As shown in Figure 7(ab), OneStory preserves consistent facial features while adapting clothing and environments as prompted, showcasing robust narrative coherence under complex visual changes. We refer readers to Project Page for better visualizations. Zoom-in effects. Transitions from wide shots to non-salient close-ups demand spatial reasoning to locate fine-grained targets while preserving fidelity. In Figure 7(cd), OneStory accurately identifies local regions and maintains detail for both static (i.e., mirror) and dynamic (i.e., hand) targets. In Figure 1 (3rd example), small objects on the table, barely visible in Shot 2, are faithfully rendered in the zoomed-in Shot 3, yielding smooth narrative progression. Human-object interactions. Shot transitions often hinge on humanobject dynamics and implicit intent. In Figure 7(ef), scenes of man engaging with car or unfolding tent lead to coherent next shots that depict the expected subsequent state. These examples highlight OneStorys ability to interpret humanobject relations, enabling realistic and semantically grounded story development."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented OneStory, novel framework for coherent multi-shot video generation via adaptive memory modeling. By reformulating MSV as next-shot generation problem, OneStory leverages the strong visual conditioning capacity of pretrained I2V models, enabling scalable and autoregressive story synthesis. The proposed Frame Selection module identifies semantically relevant frames across prior shots, while the Adaptive Conditioner performs importance-guided patchification with direct condition injection, jointly enabling global yet compact cross-shot context. OneStory effectively handles complex narratives and achieves superior narrative coherence, offering valuable insights into adaptive memory modeling for immersive, story-driven video generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Junlin Han (University of Oxford), Mingqiao Ye (EPFL), and Feng Qiao (WashU) for their constructive feedback on this project. Zhaochong An and Serge Belongie are supported by funding from the Pioneer Centre for AI, DNRF grant number P1."
        },
        {
            "title": "References",
            "content": "Yuval Atzmon, Rinon Gal, Yoad Tewel, Yoni Kasten, and Gal Chechik. Multi-shot character consistency for text-to-video generation. arXiv preprint, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint, 2025. Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, and Kai-Wei Chang. Talc: Time-aligned captions for multi-scene text-to-video generation. arXiv preprint, 2024. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint, 2025. Vasileios Chasanis, Aristidis Likas, and Nikolaos Galatsanos. Scene detection in videos using shot clustering and sequence alignment. IEEE transactions on multimedia, 11(1):89100, 2008. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024a. Jiaben Chen, Zixin Wang, Ailing Zeng, Yang Fu, Xueyang Yu, Siyuan Cen, Julian Tanke, Yihang Chen, Koichi Saito, Yuki Mitsufuji, et al. Talkcuts: large-scale dataset for multi-shot human speech video generation. NeurIPS, 2025. Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real image animation with text-guided motion control. In ECCV, pages 475491. Springer, 2024b. Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In ICLR, 2023. Ruihang Chu, Yefei He, Zhekai Chen, Shiwei Zhang, Xiaogang Xu, Bin Xia, Dingdong Wang, Hongwei Yi, Xihui Liu, Hengshuang Zhao, et al. Wan-move: Motion-controllable video generation via latent trajectory guidance. In NeurIPS, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 11 Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint, 2025. Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint, 2025. Yuwei Guo, Ceyuan Yang, Anyi Rao, Chenlin Meng, Omer Bar-Tal, Shuangrui Ding, Maneesh Agrawala, Dahua Lin, and Bo Dai. Keyframe-guided creative video inpainting. In CVPR, pages 1300913020, 2025a. Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. ICCV, 2025b. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint, 2024. Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Cut2next: Generating next shot via in-context tuning. arXiv preprint, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In CVPR, pages 81538163, 2024. Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. arXiv preprint, 2024. Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pages 2180721818, 2024. Weinan Jia, Yuning Lu, Mengqi Huang, Hualiang Wang, Binyuan Huang, Nan Chen, Mu Liu, Jidong Jiang, and Zhendong Mao. Moga: Mixture-of-groups attention for end-to-end long video generation. arXiv preprint, 2025. Jiaxiu Jiang, Wenbo Li, Jingjing Ren, Yuping Qiu, Yong Guo, Xiaogang Xu, Han Wu, and Wangmeng Zuo. Lovic: Efficient long video generation with context compression. arXiv preprint, 2025. Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, pages 66896700, 2024. Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James Rehg, and Tobias Hinz. Shotadapter: Text-to-multi-shot video generation with diffusion models. In CVPR, pages 2840528415, 2025. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint, 2024. Kuaishou. Kling video model. https://kling.kuaishou.com/en, 2024. Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, Chuanyun Deng, Yepan Xiong, Min Chen, et al. Realcam-i2v: Real-world image-to-video generation with interactive complex camera control. In ICCV, pages 2878528796, 2025. Kang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, and Chen Change Loy. Thinking with camera: unified multimodal model for camera-centric understanding and generation. arXiv preprint, 2025. Zhiqiu Lin, Siyuan Cen, Daniel Jiang, Jay Karhade, Hewei Wang, Chancharik Mitra, Tiffany Ling, Yuhan Huang, Sifan Liu, Mingyu Chen, et al. Towards understanding camera motions in any video. arXiv preprint, 2025a. Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen Song, Bowen Zhang, et al. Stiv: Scalable text and image conditioned video generation. In ICCV, pages 1624916259, 2025b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint, 2022. Hongbo Liu, Jingwen He, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, Ziqi Huang, Yinan He, Yangguang Li, Weichao Chen, et al. Shotbench: Expert-level cinematic understanding in vision-language models. arXiv preprint, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint, 2022. Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos. In ECCV, pages 468485. Springer, 2024. Chen-Yi Lu, Md Mehrab Tanjim, Ishita Dasgupta, Somdeb Sarkhel, Gang Wu, Saayan Mitra, and Somali Chaterji. Skald: Learning-based shot assembly for coherent multi-shot video creation. In ICCV, pages 1785917868, 2025. Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. arXiv preprint, 2023. Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, et al. Holocine: Holistic generation of cinematic multi-shot long video narratives. arXiv preprint, 2025. Team Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta. com/blog/llama-4-multimodal-intelligence/, April 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint, 2024. Tianhao Qi, Jianlong Yuan, Wanquan Feng, Shancheng Fang, Jiawei Liu, SiYu Zhou, Qian He, Hongtao Xie, and Yongdong Zhang. Maskˆ 2dit: Dual mask-based diffusion transformer for multi-scene long video generation. In CVPR, pages 1883718846, 2025. Lu Qiu, Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Animeshooter: multi-shot animation dataset for reference-guided video generation. arXiv preprint, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint, 2025. Shuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He, Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion transformer for image-to-video generation. In CVPR, pages 2286422874, 2025. Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. ICML, 2025a. Wenhui Song, Hanhui Li, Jiehui Huang, Panwen Hu, Yuhao Cheng, Long Chen, Yiqiang Yan, and Xiaodan Liang. Lavieid: Local autoregressive diffusion transformers for identity-preserving video creation. arXiv preprint, 2025b. Tomás Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In ACM MM, pages 1121811221, 2024. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint, 2025. Ultralytics. YOLOv5: state-of-the-art real-time object detection system. https://docs.ultralytics.com, 2021. Accessed: 2024-11-04. 13 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint, 2025. Boyang Wang, Xuweiyi Chen, Matheus Gadelha, and Zezhou Cheng. Frame in-n-out: Unbounded controllable image-to-video generation. NeurIPS, 2025a. Jiahao Wang, Hualian Sheng, Sijia Cai, Weizhan Zhang, Caixia Yan, Yachuang Feng, Bing Deng, and Jieping Ye. Echoshot: Multi-shot portrait video generation. NeurIPS, 2025b. Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3D-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, pages 110, 2025c. Xinran Wang, Songyu Xu, Xiangxuan Shan, Yuxuan Zhang, Muxi Diao, Xueyan Duan, Yanhua Huang, Kongming Liang, and Zhanyu Ma. Cinetechbench: benchmark for cinematographic technique understanding and generation. arXiv preprint, 2025d. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024a. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2024b. Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, and Mohit Bansal. Dreamrunner: Fine-grained storytelling video generation with retrieval-augmented motion adaptation. arXiv preprint, 2024c. Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards movie-grade talking character synthesis. arXiv preprint, 2025. Xiaoxue Wu, Bingjie Gao, Yu Qiao, Yaohui Wang, and Xinyuan Chen. Cinetrans: Learning to generate videos with cinematic transitions via masked diffusion models. arXiv preprint, 2025a. Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, and Sergey Tulyakov. Mind the time: Temporally-controlled multi-event video generation. In CVPR, pages 2398924000, 2025b. Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, and Lu Jiang. Captain cinema: Towards short movie generation. arXiv preprint, 2025. Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with multi-agent framework. arXiv preprint, 2024. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, pages 399417. Springer, 2024. Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint, 2024. Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint, 2025. Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint, 2025. Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In CVPR, pages 88508860, 2024. 14 David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. IJCV, pages 115, 2024. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint, 2025. Yuang Zhang, Junqi Cheng, Haoyu Zhao, Jiaxi Gu, Fangyuan Zou, Zenghui Lu, and Peng Shu. Shouldershot: Generating over-the-shoulder dialogue videos. arXiv preprint, 2025. Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint, 2024. Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, et al. Videogen-of-thought: collaborative framework for multi-shot video generation. arXiv preprint, 2024. Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. NeurIPS 2024, 2024."
        },
        {
            "title": "A Additional Training Details",
            "content": "This section provides expanded details on the training formulation used in our model, including the unified threeshot training setup and the construction of frame-level pseudo-labels. These details complement Section 4.2 and Section 4.4 of the main paper. A.1 Unified Three-Shot Training As discussed in Section 3 of the main paper, the dataset contains videos with varying numbers of shots, with two-shot sequences being the most common and three-shot sequences relatively fewer. Training directly on sequences of non-uniform length leads to unstable optimization. To mitigate this, we unify all training samples into three-shot format by synthesizing an additional shot for two-shot videos. Synthetic shot construction. Given two-shot sequence (Sfirst, Slast), we create synthetic shot Ssyn using one of: (i) Cross-video insertion: inserting shot randomly sampled from another video. (ii) Augmented-first-shot variant: applying spatial or color transformations to Sfirst. This results in synthetic triplets that, for each sample, take one of the two forms: (cid:0)Sfirst, Ssyn, Slast (cid:1) or (cid:0)Ssyn, Sfirst, Slast (cid:1), (9) while the real triplets are represented in the structure (Sfirst, Ssecond, Slast). In all cases, Slast serves as the prediction target. Training objective. The model is trained to generate the final shot Slast conditioned on the first two shots and its caption Clast: Lshot = E(cid:2)Ldiff (cid:0)G(Sfirst, Ssyn/second, Clast), Slast (cid:1)(cid:3) , (10) where Ldiff denotes rectified-flow diffusion loss (Lipman et al., 2022; Liu et al., 2022; Esser et al., 2024). This unified formulation standardizes all training samples to consistent three-shot structure and enables unified three-shot training, improving optimization stability. A.2 Frame Relevance Pseudo-Labels To assist the learning of the frame relevance scores S, we construct frame-level pseudo-labels = {yr}F r=1 that approximate the relevance of each historical frame in to the target shot. The pseudo-labels incorporate both real and synthetic frames introduced in Section A.1. Real historical frames. For frames originating from real historical shots, we compute cosine similarity between each historical frame and the target shot using DINOv2 (Oquab et al., 2023) and CLIP (Radford et al., 2021) embeddings, producing scalar relevance score. These pseudo-labels help the Frame Selection module prioritize visually and semantically aligned frames while down-weighting irrelevant ones. Synthetic frames. Frames from synthetic shots introduced in Section A.1 are assigned coarse relevance labels: yr = 1 for randomly inserted shots to indicate clear irrelevance, and yr = 0 for augmented-first-shot variants to reflect partial relevance. These labels explicitly guide the selector to down-weight non-informative or misleading frames. Supervision loss. The predicted relevance scores are supervised using regression loss: Lsel ="
        },
        {
            "title": "1\nF",
            "content": "F (cid:88) r=1 (sr yr)2, 16 (11) where sr = S[r] denotes the predicted relevance score for the rth historical frame. The full training objective is given by: Ltrain = Lshot + λ Lsel, (12) with λ controlling the weight of the selector supervision loss. This joint optimization encourages the model to identify informative context frames while maintaining high-fidelity generation."
        },
        {
            "title": "B Additional Details on Evaluation Benchmark",
            "content": "We construct human-centric benchmark for both T2MSV and I2MSV to evaluate multi-shot video generation under realistic narrative conditions. As introduced in Section 3 of the main paper, each shot is paired with referential caption following progressive narrative flow, reflecting real-world storytelling. To comprehensively evaluate MSV performance, the benchmark spans three canonical multi-shot storytelling patterns: 1. Main-subject consistency. Multiple shots focus on the same character(s), who may appear in different environments or perform different actions. This pattern evaluates the models ability to preserve identity under various cross-shot changes. 2. Insert-and-recall with an intervening shot. shot introducing new scene, such as an environment-only view or new character, is inserted mid-sequence, after which the narrative returns to the primary subject(s) and later revisits the intervening shot. This pattern stresses the models ability to maintain long-range memory and remain robust to temporal distractors. 3. Composable generation. Characters introduced separately in earlier shots are composed together in later shots. This tests whether the model can correctly integrate multiple narrative threads into coherent shared scene. In total, we curate 64 six-shot test cases for T2MSV and 64 six-shot test cases for I2MSV, covering diverse range of subjects, environments, and complex cross-shot relationships, thereby ensuring comprehensive MSV performance evaluation. More examples are provided in our Project Page."
        },
        {
            "title": "C Additional Qualitative Results",
            "content": "Generating coherent multi-shot videos that faithfully follow narrative captions is essential for real-world storytelling. Here, we analyze our model from three perspectives, using examples from the main paper to illustrate its ability to maintain continuity across shots. Additional video qualitative results are available our Project Page. Identity consistency. Our model preserves character identity across long-range shots under diverse variations. In the 1st example of Figure 1 in the main paper, the same subject remains consistent across changes in viewpoint (Shots 4, 5) and actions (Shots 1, 3, 8). This illustrates the effectiveness of our adaptive memory in maintaining stable long-range identity cues. Background details. Beyond character fidelity, our model maintains consistent background details across shots, enabling spatially coherent story progression. In the 2nd example of Figure 1 in the main paper, fine-grained elements such as plants and fences remain aligned from Shot 1 to Shot 7 despite large cross-shot dynamics. Similarly, in the 3rd example, the red flowers reappear consistently across Shots 1, 4, 5, 6, 7, and 9, demonstrating the models ability to preserve scene layout and spatial structure. Reappearance and composition. Realistic narratives often involve disappearreappear patterns and the merging of multiple narrative threads through composable generation. Our model effectively recalls characters or environments that reemerge after several intervening shots, e.g., Shots 4 and 9, or Shots 2 and 6 in the 2nd example of Figure 1 in the main paper. Furthermore, in Shot 7 of the same example, the woman from Shot 1 and the man from Shot 4 appear together, demonstrating the models capacity to unify distinct visual narratives into coherent multi-subject scene."
        }
    ],
    "affiliations": [
        "Meta AI",
        "University of Copenhagen"
    ]
}