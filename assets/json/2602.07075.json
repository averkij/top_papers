{
    "paper_title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
    "authors": [
        "Xinwu Ye",
        "Yicheng Mao",
        "Jia Zhang",
        "Yimeng Liu",
        "Li Hao",
        "Fang Wu",
        "Zhiwei Li",
        "Yuxuan Liao",
        "Zehong Wang",
        "Zhiyuan Liu",
        "Zhenfei Yin",
        "Li Yuan",
        "Philip Torr",
        "Huan Sun",
        "Xiangxiang Zeng",
        "Mengdi Wang",
        "Le Cong",
        "Shenghua Gao",
        "Xiangru Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84$\\times$ average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] - c . s [ 1 5 7 0 7 0 . 2 0 6 2 : r LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning"
        },
        {
            "title": "Abstract",
            "content": "Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves 59.88% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering 10.84 average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories. Date: February 10, 2026 Code: https://github.com/xinwuye/LatentChem"
        },
        {
            "title": "Introduction",
            "content": "LLMs have emerged as transformative tools for scientific discovery, facilitating tasks ranging from molecule generation to reaction synthesis [13, 44, 49]. In this context, chemical reasoning is typically mediated through explicit CoT [48, 50, 51]. In current chemical LLM paradigms, models are trained to linearize complex physicochemical intuitions, such as electron delocalization or steric hindrance, into discrete sequence of natural language tokens before arriving at solution. While this CoT approach has unified scientific tasks under generative framework [54], it relies on the implicit assumption that natural language is an adequate vehicle for the continuous physicochemical dynamics inherent to chemistry. However, it remains unclear whether such discrete symbolic reasoning is optimal, for complex chemical reasoning tasks [21, 49]. We hypothesize that forcing chemical logic into linguistic bottleneck results in fundamental continuitydiscretization gap in chemical reasoning (Figure 1). Much like an expert chemist who manipulates abstract 3D structures mentally before verbalizing the result, we posit that the core of chemical reasoning, such as navigating manifolds, optimizing properties, and identifying substructures, is more naturally performed in continuous latent space. In this view, natural language should serve merely as the input/output interface rather than the computational substrate for the reasoning process itself. 1 Figure 1Conceptual illustration of the continuitydiscretization gap in chemical reasoning. (a) The intrinsic chemical property landscape is continuous and high-dimensional. (b) We posit that continuous latent space can theoretically offer smoother optimization surface akin to the property landscape, avoiding the jagged trajectories of discrete tokens. (c) Linguistic tokenization fragments chemical state transitions into discrete symbolic steps, inducing staircase-like landscapes and inefficient reasoning paths. To study this hypothesis, we introduce LatentChem, latent reasoning interface [7, 14] designed to decouple chemical reasoning from explicit natural language generation. As illustrated in Figure 2, unlike standard models that rigidly bind reasoning to token output, LatentChem injects lightweight sequence of continuous thought vectors between perception and generation. Crucially, we equip this interface with ChemUpdater mechanism, allowing these latent thoughts to actively re-query molecular features in dynamic loop. This architecture serves as an experimental instrument, allowing us to observe whether the model prefers to reason via explicit text or through internal latent dynamics when optimized for task success. Our investigation reveals an intriguing emergent behavior: the model exhibits spontaneous internalization of chemical logic. Despite being initialized with explicit CoT data, when optimized via reinforcement learning with rewards only on format adherence, validity, and correctness, LatentChem voluntarily discards verbose textual reasoning chains in the majority of tasks. Instead, it predominantly collapses the reasoning process into the continuous latent space, outputting solutions directly after sequence of silent thought vectors. While this shift sacrifices the explicit readability of intermediate steps, it strongly suggests that the model perceives natural language as low-bandwidth constraint, identifying the continuous latent reasoning as more native and expressive mode for chemical logic. Crucially, this spontaneous shift does not imply that the model is merely taking shortcut to minimize generation effort, as such behavior would typically degrade task performance. On the contrary, our experimental results demonstrate that this internalization represents superior computational strategy. Quantitatively, by choosing to compress linguistic steps into compact latent states, LatentChem achieves dramatic efficiency gain, reducing the reasoning token overhead by an average of 10.84 across all benchmarks. In molecule optimization and reaction tasks, this reduction factor even exceeds 28. This efficiency is achieved alongside dominant performance, as evidenced by extensive evaluations across four diverse benchmarks, ChemCoTBench [20], Mol-Instructions [11], ChEBI-20 [10], and ChemLLMBench [13]. Notably, LatentChem achieves 59.88% non-tie win rate against the strong explicit CoT baseline on the reasoning-intensive ChemCoTBench. The simultaneous improvement in both speed and accuracy confirms that decoupling reasoning from language unlocks more native and effective reasoning paradigm for chemical LLM. In summary, our contributions are as follows: We propose reasoning mode to investigate chemical LLMs, challenging the necessity of natural language CoT for chemical tasks. We introduce LatentChem, system that empowers chemical LLMs to perform latent thinking via continuous thought vectors and dynamic perception loop. We report the phenomenon of spontaneous internalization, where the model autonomously absorbs explicit reasoning into the latent space, validating the efficiency of continuous representations. 2 Figure 2Comparison of reasoning paradigms in chemical LLMs. (Top) Explicit CoT relies on discrete linguistic steps, forcing high-dimensional chemical intuition into constrained textual bottleneck. (Middle) Generic latent reasoning (e.g., Coconut[14]) shifts reasoning to continuous latent space but treats molecular embeddings as static context, limiting the ability to focus on different substructures during reasoning. (Bottom) LatentChem (Ours) introduces dynamic perception-reasoning loop. Leveraging the ChemUpdater mechanism, latent thoughts actively re-query and refine the molecular representation at each step, ensuring structure-aware reasoning. Depending on the complexity of the query and the allocated latent thinking budget, the model exhibits dynamic reasoning behaviors, ranging from fully latent computation (no textual CoT) to hybrid reasoning with varying lengths of textual CoT. We demonstrate that this paradigm shift yields superior performance and token efficiency across diverse chemical benchmarks compared to explicit CoT approaches."
        },
        {
            "title": "2 Related Work",
            "content": "LLMs for chemical tasks. While general LLMs struggle with the specialized logic required for scientific discovery [15, 16, 30, 37], the field has progressed through domain-specific fine-tuning [48, 51], multimodal architectures [3, 17, 22, 25, 26, 35] that integrate 2D/3D geometries to capture chemical topology [29, 44], and agentic decomposition of tasks [36]. Furthermore, reinforcement learning is increasingly utilized to enforce logical consistency and physical validity [27, 40, 53], often within unified training frameworks [54]. Despite these algorithmic strides, current systems remain prone to hallucination [21] and often lack the robust planning capabilities necessary for complex chemical reasoning tasks [49]. Latent thinking in LLMs. An emerging paradigm shifts reasoning from explicit token generation to implicit processing within high-dimensional latent spaces, bypassing the linguistic bottleneck of sequential text. Pioneering works demonstrate that models can internalize reasoning steps by feeding hidden states directly as input and benefit from multiple reasoning paths [7, 14, 52]. To structure these internal processes, research has compressed reasoning to contemplation tokens [6] or capsules [32], and employs differentiable caches for soft thoughts\" and memories [24, 45]. These latent states are further refined through auxiliary supervision, synthetic target for alignment, self-rewarding mechanisms, and variance optimization [5, 38, 39, 41]. Crucially, this non-sequential nature unlocks test-time compute scaling: models can deepen reasoning through recursive unrolling [2, 12], leverage native parallelism via Jacobi iteration, [42, 43] or scaled through diverse initializations [46]. These approaches establish the foundation for the iterative, structure-aware computation that our method extends to dynamically refine molecular features. 3 Figure 3The overview of LatentChem architecture. The system decouples reasoning from generation via dedicated latent thinking phase. (1) ChemAdapter aligns molecular features with the LLM space. (2) During the reasoning phase, the model generates sequence of continuous latent thought vectors. (3) The ChemUpdater allows these thoughts to recursively re-query the molecular encoder, refining the representation based on the current reasoning state before the final response is decoded."
        },
        {
            "title": "3 LatentChem: A Latent Reasoning Interface",
            "content": "We present LatentChem, framework designed to decouple chemical reasoning from linguistic generation. As illustrated in Figure 3, the system comprises three core components: (1) latent thinking architecture that establishes the structural foundation for non-linguistic reasoning; (2) an active perceptual refinement mechanism that enables the model to dynamically update its molecular understanding; and (3) progressive training protocol that evolves the model from simple alignment to outcome-driven policy optimization."
        },
        {
            "title": "3.1 Latent Thinking Neural Architecture",
            "content": "The architecture of LatentChem introduces three specialized modules integrated into general-purpose LLM backbone: the ChemAdapter for initial perception, the ChemUpdater for dynamic refinement, and the Latent Projector for reasoning continuity. ChemAdapter. To bridge the modality gap between continuous molecular features and the discrete embedding space, we employ query-based attention projector (a Perceiver Resampler architecture [1]). Given molecule, we first extract variable-length sequence of dense features Hmol RLdenc using the pre-trained SMITED encoder [34]. Then, to characterize the molecule comprehensively, we initialize learnable latent queries RN dllm . These queries serve as semantic anchors, extracting specific chemical attributes via cross-attention: Hchem = (LN(Q+MHA(Q, Hmol, Hmol))) , where projects dimensions to the LLM size dllm. This mechanism compresses variable-length molecular information into fixed number of ChemTokens Hchem, which are prepended to the textual instruction embeddings as prefix soft prompt. ChemUpdater and Latent Projector. To enable dynamic interaction between reasoning and perception, we incorporate two complementary modules. First, unlike standard models that treat encoders as static feature Table 1Overview of training strategies. We detail the module status and supervision signals across four stages."
        },
        {
            "title": "Stage",
            "content": "1 2"
        },
        {
            "title": "Supervision",
            "content": "Adapter + LLM Latent Thinking CoT Answer Reward"
        },
        {
            "title": "Disabled\nDisabled\nTrainable\nFrozen",
            "content": "extractors, the ChemUpdater enables dynamic perceptual refinement. It utilizes cross-attention layer where the current ChemTokens H(t) serve as queries and the accumulated reasoning history Z1:t serves as keys and values, allowing the model to shift its focus on molecular substructures as reasoning evolves. Second, to close the loop, we employ Latent Projector, implemented as lightweight residual feed-forward network (FFN). This module maps the raw hidden state zt output by the LLM backbone back into the input embedding space, creating the continuous input vector required for the subsequent step and effectively bypassing the discrete tokenization bottleneck. chem"
        },
        {
            "title": "3.2 Latent Thinking with Active Perceptual Refinement",
            "content": "The core of LatentChem is the continuous latent thinking loop, which transforms the generation process into dual-pathway system of thought generation and active perceptual updating. At each reasoning step t, the LLM produces high-dimensional hidden state zt. Instead of decoding this state into text token, the model treats it as thought vector that drives two parallel processes. On the perceptual pathway, the raw state zt is appended to the history Z1:t to trigger the ChemUpdater. The model actively refreshes its molecular representation via: H(t+1) chem = LN (cid:16) H(t) chem +CrossAttn(H(t) chem, Z1:t, Z1:t) (cid:17) . This ensures the model does not just see static structure but continuously refines its focus based on the latest thought. Simultaneously, on the reasoning pathway, the state zt is processed by the Latent Projector to form the input for the next step: ht+1 = zt + FFN(LN(zt)). The projected vector ht+1 is fed back into the backbone LLM to trigger zt+1. This cycle continues until the model predicts discrete termination token <end_latent> or reaches budget limit Tmax, at which point the final refined context is used to decode the explicit response."
        },
        {
            "title": "3.3 Training Protocol: From Explicit Alignment to Implicit Reasoning",
            "content": "We design progressive four-stage training protocol to instill latent reasoning capabilities  (Table 1)  . The first three stages focus on supervised structural alignment and mind activation, establishing the prerequisites for reasoning. Crucially, the final stage employs Group Relative Policy Optimization (GRPO) [33], reinforcement learning paradigm that allows the model to autonomously explore and optimize its reasoning trajectory based on task feedback. We provide an expanded elaboration of the protocol in Appendix and detailed implementation hyperparameters in Appendix D. Training data. Our model is trained on ChemCoTDataset [20], specialized corpus designed for step-by-step chemical reasoning. We utilize the snapshot from November 2025,1 which comprises approximately 14k highquality CoT-annotated samples covering diverse tasks such as molecular understanding, editing, optimization, and reaction prediction. 1The dataset was accessed on November 25, 2025. Since ChemCoTDataset is continuously updated, we explicitly state the access date to ensure reproducibility. 5 Figure 4Case study: spontaneous internalization vs. explicit CoT. While the standard CoT model generates verbose reasoning chains that fail to execute the planned modification, LatentChem spontaneously internalizes these logics. It bypasses textual output, utilizing the latent space to perform structural optimization. Stage 1: Establishing the molecular-linguistic mapping. The primary goal is to align the projected ChemTokens Hchem with the LLMs semantic space. We use answer-only supervision, suppressing intermediate reasoning steps to force the ChemAdapter to compress all necessary properties into the ChemTokens. To prevent textual overfitting, we employ counterfactual alignment strategy, optimizing hinge loss LCF that maximizes the margin between the likelihood of the answer given clean versus perturbed molecular inputs. The total objective is L(1) total = Lclean + λLCF . Stage 2: SFT for molecule-aware CoT. We then transition to full sequence training, requiring explicit CoT derivations ycot prior to the answer yans. To ensure the generated reasoning is strictly grounded in molecular structure rather than textual priors, we extend the counterfactual alignment strategy to the entire sequence yf ull = [ycot, yans]. In this phase, we activate the latent thinking modules Stage 3: Chemistry-aware latent mind activation. (updater and projector). To prevent the massive LLM backbone from treating the lightweight latent signals as noise, we freeze both the ChemAdapter and the LLM backbone, updating only the updater and projector. This constraint forces the latent modules to adapt to the frozen semantic space of the LLM, compelling them to generate legible thought vectors that guide the fixed decoder toward the correct CoT path. Stage 4: GRPO with latent thinking budget. With the latent mind activated, we employ GRPO to refine the models policy. Reversing the freezing strategy from the previous stage, we now freeze the latent thinking modules and fine-tune the remaining parameters. This strategy treats the learned latent dynamics as stable internal simulator, optimizing the backbone LLM to effectively utilize these latent thoughts for decision-making. The optimization is guided by composite reward function comprising four terms: format adherence, answer validity, and answer correctness (detailed in Appendixe D.4). Critically, by removing the supervision for explicit CoT generation and rewarding only the final result, we enable the model to explore the most efficient reasoning pathway."
        },
        {
            "title": "4 Emergent Properties of Latent Chemical Reasoning",
            "content": "Having established the LatentChem interface, we now investigate the nature of the reasoning process that emerges from the training protocol. To dissect these behaviors, we utilize ChemCoTBench [20] as our primary testbed. Specifically, our analysis focuses on two representative tasks: molecule optimization and molecule 6 Figure 5Causal necessity analysis. We measure task performance (success rate, scaffold similarity, or correct rate) as the first latent tokens are replaced with Gaussian noise. The monotonic degradation observed in both Molecule Optimization (left) and Understanding (right) confirms that early latent states encode critical precursors for the final solution rather than redundant noise. understanding. Unless otherwise stated, all comparisons in this section are conducted between LatentChem and the explicit CoT baseline (Stage 1+2+4)."
        },
        {
            "title": "4.1 Spontaneous CoT Internalization",
            "content": "A pivotal discovery in our experiments is the emergent behavior observed during the transition from SFT to reinforcement learning (Stage 4). Despite being heavily supervised with explicit textual CoT data in before Stage 4, LatentChem spontaneously internalized the entirety of explicit textual reasoning after GRPO training. The phenomenon. Remarkably, despite being optimized using rewards constrained only by format adherence, validity, and correctness, the model ceased generating intermediate textual steps. Instead, the generation process follows distinct pattern: the model performs sequence of latent thinking steps, outputs trivial artifact (typically single . or :) as transition token, and immediately generates the target XML tags. Two cases are presented in Figure 4, where LatentChem bypasses the verbose and often hallucinated textual plan generated by the CoT baseline, utilizing the latent space to perform accurate structural optimization. Causal necessity verification. To confirm that this silent phase performs actual computation rather than acting as passive delay, we conduct causal ablation experiments. As shown in Figure 5, replacing the initial latent steps with Gaussian noise leads to monotonic performance degradation. This rigorously establishes that the internalized latent states encode critical structural precursors and are functionally essential for the final generation. Insight: optimization over imitation. This internalization was not explicitly enforced by any loss penalty on output length. Rather, it represents spontaneous strategy shift driven purely by the reward signal. This observation serves as powerful empirical validation for our core hypothesis regarding the continuitydiscretization gap in chemical reasoning. It suggests that when the model is granted the freedom to reason for chemical tasks, it identifies the high-dimensional, continuous latent space as more native and structurally consistent workspace than the discrete, low-bandwidth channel of natural language."
        },
        {
            "title": "4.2 Emergent Compensatory Dynamics and Causal Necessity\nBuilding on the discovery of CoT internalization, a critical question arises: is this behavior a rigid pattern or\na flexible strategy? To investigate this, we conduct budget stress testing on the molecule optimization and",
            "content": "7 Figure 6Budget stress testing. We monitor the length of generated explicit CoT (y-axis) as the maximum allowed latent thinking steps (x-axis) decrease. distinct hydraulic trade-off is observed across both Molecule Optimization (left) and Understanding (right) tasks: as the latent budget is restricted (e.g., < 5 steps), the model spontaneously externalizes reasoning into text to compensate for the reduced internal capacity. Figure 7Visualization of latent dynamics during the reasoning process. (a) t-SNE projections of ChemTokens at different steps show transition from initial entanglement to rapid disentanglement. The representations diverge significantly within the first two steps to form task-specific clusters and remain stable thereafter. (b) RSA results showing the correlation between latent geometry and chemical topology. The flat trajectories indicate that despite the significant spatial updates for task adaptation, the model preserves structural fidelity throughout the reasoning chain, suggesting that optimization occurs orthogonally to topological encoding. understanding tasks, monitoring the length of generated explicit CoT as the maximum allowed latent thinking steps decrease. As illustrated in Figure 6, we observe remarkable hydraulic trade-off. When the latent budget is sufficient (e.g., 6), the model maintains the internalized state with near-zero CoT length. However, as we progressively starve the model of latent thinking steps (T < 6), it spontaneously reactivates explicit CoT without any external instruction. This dynamic behavior suggests that the model treats implicit and explicit reasoning as communicating vessels, revealing that LatentChem has autonomously learned robust mechanism for arbitrating between implicit and explicit reasoning pathways based on its computational capacity."
        },
        {
            "title": "4.3 Unveiling the Latent Manifold",
            "content": "To investigate the internal dynamics of the latent thinking process, we examine the evolutionary trajectories of ChemTokens across different optimization tasks. We first project the high-dimensional latent states into 2D space using t-SNE to visualize the global distribution shift. As illustrated in Figure 7(a), the dynamics reveal phenomenon of immediate functional partitioning rather than gradual evolution. Initially, at Step 0, representations from all tasks are densely entangled, indicating generic, task-agnostic encoding. However, within the first two latent steps, the trajectories undergo dramatic divergence, rapidly snapping into distinct, 8 Table 2Main results on chemical benchmarks. We report the non-tie win rate (R win ) of each method compared to the reference CoT baseline (Stage 1+2+4). The All column denotes the aggregated win rate on each benchmark. Bold and underlined values indicate the best and second-best performance, respectively. denotes statistical significance with < 0.01 according to the sign test."
        },
        {
            "title": "ChemCoTBench",
            "content": "Mol-Instructions"
        },
        {
            "title": "ChemLLMBench",
            "content": "ChEBI-20 Text-only Chem. LLMs"
        },
        {
            "title": "Open Closed All",
            "content": "Qwen-3-8B Qwen-3-8B (SFT) 13.98 2.80 58.65 21.87 23.97 66.67 19.58 22.78 28.08 20.63 25.75 73.13 35.92 38.39 83.14 37.51 41.11 8.61 Stage 1 Stage 1+2 43.84 36.76 41.73 63.00 50.49 51.25 68.47 50.56 38.35 28.44 35.50 51.57 59.77 45.12 46.21 50.36 43.69 44."
        },
        {
            "title": "Open",
            "content": "74.86 80.85 64.80 54.60 Latent Chem. LLMs 34.71 26.32 32.11 57.94 39.45 40.57 75.00 42.46 44.82 Coconut-Chem LatentChem (Ours) 63.73 50.00 59.88 72.00 48.36 49.88 87.37 52.87 55.58 65.58 85.26 task-specific clusters. From Step 3 to 10, these spatial configurations remain stable. This rapid convergence suggests that the model possesses an intrinsic efficiency; it does not require prolonged period to locate the optimization manifold but instead reconfigures the molecular representation almost instantaneously to align with the specific chemical goal. To determine whether these drastic latent updates compromise the physical validity of the representation, we further employ representational similarity analysis (RSA) to quantify the alignment between the latent geometry and the true chemical topology. Specifically, we track the Spearman correlation between the pairwise cosine similarity of ChemTokens and the Tanimoto similarity of their corresponding molecular fingerprints. As shown in Figure 7(b), the structural correlation remains remarkably stable across all reasoning steps. It implies that the massive updates occurring in the first few steps are orthogonal to the direction of structural information. The latent thinking process acts as lossless reservoir of structural information throughout the reasoning chain."
        },
        {
            "title": "5 Benchmarking Latent vs. Explicit Paradigms",
            "content": "To substantiate that the observed latent reasoning dynamics translate into superior performance, we conduct comprehensive evaluations across four diverse chemical benchmarks."
        },
        {
            "title": "5.1 Experimental Setup\nTask taxonomy. We categorize the target capabilities into two paradigms based on the solution space.\nOpen-ended generative tasks (e.g., molecule optimization) lack a unique optimal solution, requiring the model\nto explore the chemical manifold to generate valid candidates. In contrast, precise closed-ended tasks (e.g.,\nreaction prediction) demand unique, deterministic answers, functioning as rigorous mapping problems. We\nleverage this dichotomy to investigate the differential impact of latent reasoning on creative exploration versus\ndeterministic execution.",
            "content": "Benchmarks. We evaluate our method on four established benchmarks: ChemCoTBench [20], Mol-Instructions [11], ChEBI-20 [10], and ChemLLMBench [13]. To rigorously assess the molecular reasoning and generation capabilities targeted by our framework, the evaluation suite is curated to focus on chemical competencies aligned with the instruction-tuning phase. Consequently, for ChemCoTBench, the evaluation covers 1,120 samples covering molecule optimization, understanding, editing, and reaction prediction. The molecule-oriented component of Mol-Instructions provides 4,000 samples across forward reaction prediction, retrosynthesis, reagent prediction, and description generation. ChEBI-20 contributes its standard test set of 3,297 moleculedescription pairs. Finally, 600 samples from ChemLLMBench are included, spanning seven relevant subtasks, including molecule captioning and various reaction prediction challenges, to verify performance consistency across diverse evaluation sources. 9 Table 3Detailed performance on open-ended generative tasks. Metrics include property improvement ( ) and success rate (SR%) for molecule optimization, alongside METEOR scores for description. Results for closed-ended tasks are detailed in Appendices F. Category Method Molecule Optimization (ChemCoTBench) Molecule Description LogP Solubility QED DRD2 JNK GSK3-β Mol-Instr. ChEBI-20 ChemLLM. SR% SR% SR% SR% SR% SR% METEOR METEOR METEOR Text-only Qwen-3-8B Qwen-3-8B (SFT) 0.00 0.00 3 0.15 47 0.48 4 0.00 0.10 4 48 Stage 1 Stage 1+2 Stage 1+2+4 0.35 61 0.87 0.51 60 0.82 0.67 77 1.17 73 0.14 78 69 72 0.14 86 0.19 82 0.00 0. 0.20 0.12 0.19 -0.01 4 2 0 38 -0.02 20 0.02 36 0.00 66 53 70 0.02 0.01 0. 40 0.13 61 32 0.08 39 44 0.13 67 Coconut-Chem 45 -0.00 35 0.06 47 LatentChem (Ours) 1.37 96 1.53 89 0.18 83 0.26 74 0.08 60 0.17 82 0.17 44 0.57 0.15 0.04 67 Chem. LLMs Latent Chem. LLMs 0.09 0.12 0.10 0.07 0.07 0.09 0. 0.12 0.12 0.08 0.05 0.05 0.07 0.15 0.09 0.13 0.11 0.06 0.07 0.04 0. Baselines. We compare LatentChem against three categories of baselines. All molecule-aware models (LatentChem, explicit chemical LLMs, and Coconut-Chem) share the identical Qwen-3-8B [47] backbone equipped with the SMI-TED encoder and ChemAdapter. The categories include: (1) text-only LLMs (Qwen3-8B and Qwen-3-8B SFT) which establish performance floor; (2) explicit chemical LLMs, which utilize the exact same ChemAdapter+Qwen-3-8B neural architecture as LatentChem but are constrained to standard CoT reasoning (reported at Stage 1, Stage 1+2, and the GRPO-optimized Stage 1+2+4); and (3) generic latent models, specifically Coconut-Chem, which adapts the Coconut paradigm [14] to our encoder-adapter setup. Metrics. We employ diverse task-specific metrics (detailed in Appendix E). To facilitate rigorous statistical comparison, we employ the sign test [8]. Aligning with this frameworks theoretical premise that ties are non-informative, we quantify performance using the non-tie win rate (R ), which is computed as: win . win = Nwin Nwin+Nloss"
        },
        {
            "title": "5.2 Main Results\nOverall superiority. Table 2 reports the non-tie win rate (R∗\n) against the strong explicit baseline (Stage\n1+2+4). LatentChem demonstrates robust superiority, dominating on ChemCoTBench (59.88%) and ChEBI-20\n(85.26%), while generalizing well to ChemLLMBench (55.58%). On Mol-Instructions, it maintains competitive\nparity (49.88%). These results confirm that the latent reasoning paradigm offers a superior substrate for\ncomplex chemical logic, enhancing capabilities without compromising foundational proficiency on standard\ntasks.",
            "content": "win Generative capabilities. The advantages of the latent paradigm are most pronounced in open-ended generative tasks  (Table 3)  , surpassing all baselines on 14 out of 15 metrics. In molecule optimization, reasoning in the continuous latent manifold establishes substantial lead over discrete planning. Furthermore, the 8Bparameter LatentChem consistently outperforms the state-of-the-art results from Claude 3.7 Sonnet reported in ChemCoTBench (see Appendix for detailed comparison). Notably, on the challenging GSK3-β task, LatentChem achieves success rate of 82% (vs. 67% for explicit CoT). This validates that high-dimensional latent exploration unlocks level of generative creativity and navigational precision that is inaccessible to explicit token-based methods. Robustness on deterministic mappings. The performance contrast between task categories offers critical insight into the mechanism of latent thinking. As shown in the Closed columns of Table 2, while the latent approach maintains leading position, the margin is narrower compared to open-ended tasks. This aligns with the nature of closed-ended tasks as deterministic mapping problems, where the benefits of creative exploration are naturally saturated. Crucially, however, internalized reasoning preserves foundational precision, allowing the system to maintain competitive parity with explicit baselines even on these rigid tasks, effectively bypassing the typical trade-off between creativity and accuracy. 10 Inference efficiency. Beyond capability, we quantify efficiency by contrasting the total reasoning overhead (baseline CoT tokens vs. LatentChems combined latent and textual steps). As shown in Figure 8, by compressing verbose linguistic thought, LatentChem reduces this overhead by factors ranging from 5.4 to 29.9 (average 10.84). This confirms that the latent paradigm is not only effectively superior but also computationally transformative, breaking the inference latency bottleneck inherent to traditional CoT. We attribute this dramatic acceleration to the inherent compressibility of chemical logic in continuous space compared to language. Consider routine optimization step, such as appending functional group. In the latent manifold, the model can theoretically execute this structural shift via single or few vector transitions, effectively gliding across the smooth landscape visualized in Figure 1 (b). Conversely, explicit CoT is forced to discretize this operation into verbose textual chain (e.g., analyzing binding sites, verifying valency, and describing the addition), triggering dozens of redundant autoregressive forward passes to achieve the exact same chemical move (the jagged staircase trajectory in Figure 1 (c)). By bypassing this language bottleneck, LatentChem aligns computational expenditure with the actual complexity of the chemical task. Further analysis is presented in Appendix A. Figure 8Inference efficiency gain. We report the efficiency ratio (Explicit CoT Steps / LatentChem Steps) across all five task categories. LatentChem achieves up to 29.9 efficiency on reaction tasks, demonstrating that latent thinking significantly accelerates inference by bypassing verbose textual generation."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "To isolate the drivers of performance, we conduct component-wise ablation  (Table 4)  . Removing Latent Thinking causes precipitous performance drop, confirming that continuous vectors capture chemical nuances far more effectively than discrete tokens. Furthermore, the removal of ChemUpdater leads to 12.0% decline in optimization success, underscoring that dynamic perception is essential for chemical reasoning. Finally, the Latent Projector proves indispensable for bridging the semantic gap; without it, the model fails to map internal thought vectors back to the input space, disrupting the reasoning trajectory. Table 4Ablation study. We report the success rate (SR%) for molecule optimization and METEOR scores for molecule description across all applicable tasks from the four evaluated benchmarks."
        },
        {
            "title": "Method",
            "content": "w/o Latent Thinking w/o Latent Projector w/o ChemUpdater LatentChem (Full) Mol. Optimization Mol. Description (SR%) (METEOR) 71.00 69.83 68.67 80.67 0.052 0.087 0.068 0."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "This study investigates the hypothesis of continuitydiscretization gap in chemical reasoning, proposing that the discontinuity of natural language inhibits the modeling of continuous chemical manifolds. By 11 introducing LatentChem, we demonstrate that decoupling reasoning from linguistic generation offers superior computational substrate for chemical logic. Our analysis reveals the mechanism behind this success: the model exhibits spontaneous internalization, voluntarily abandoning verbose textual derivations for compact latent computation that correlates with physical chemical topology. This structural advantage translates directly into performance, with LatentChem achieving 59.88% non-tie win rate against strong explicit baselines on ChemCoTBench and 10.84 efficiency gain, effectively breaking the trade-off between reasoning depth and inference latency. Limitations and future directions. The transition to latent reasoning changes the interface through which intermediate reasoning is exposed. Because computation unfolds in continuous latent space rather than explicit tokens, intermediate steps are not automatically available as human-readable traces. Although we provide empirical analyses of the latent dynamics, the current framework does not yet offer native mechanism for systematically externalizing these internal trajectories in symbolic form. Future work should focus on hybrid cognitive architectures [18]: systems that leverage efficient latent thinking for heavy structural computation (System 1) while retaining the ability to decode these thoughts into explicit language when justification is required (System 2). Ultimately, this work establishes latent thinking not merely as an acceleration trick, but as foundational reasoning modality for the next generation of scientific AI."
        },
        {
            "title": "7 Contributions and Acknowledgments",
            "content": "Co-First Authors Xinwu Ye, Yicheng Mao Co-Authors Jia Zhang, Yimeng Liu, Li Hao, Fang Wu, Zhiwei Li, Yuxuan Liao, Zehong Wang, Zhiyuan Liu, Zhenfei Yin, Li Yuan, Philip Torr, Huan Sun, Xiangxiang Zeng, Mengdi Wang, Le Cong, Shenghua Gao, Xiangru Tang"
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David OShea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, and Nicholas D. Lane. Abbie: Autoregressive block-based iterative encoder for efficient sequence modeling, 2025. URL https://arxiv.org/abs/2507.08567. [3] Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [5] Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, and Huan Wang. Language models are hidden reasoners: Unlocking latent reasoning capabilities via self-rewarding, 2024. URL https://arxiv.org/abs/2411.04282. [6] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations, 2024. URL https://arxiv.org/abs/2412.13171. [7] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step, 2024. URL https://arxiv.org/abs/2405.14838. [8] Wilfrid Dixon and Alexander Mood. The statistical sign test. Journal of the American Statistical Association, 41(236):557566, 1946. [9] Joseph Durant, Burton Leland, Douglas Henry, and James Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):12731280, 2002. [10] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2mol: Cross-modal molecule retrieval with natural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 595607, 2021. [11] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023. [12] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach, 2025. URL https://arxiv.org/abs/2502.05171. [13] Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, et al. What can large language models do in chemistry? comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:5966259688, 2023. [14] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space, 2025. URL https://arxiv.org/abs/2412.06769. [15] Kan Hatakeyama-Sato, Naoki Yamane, Yasuhiko Igarashi, Yuta Nabae, and Teruaki Hayakawa. Prompt engineering of gpt-4 for chemical research: what can/cannot be done? Science and Technology of Advanced Materials: Methods, 3(1):2260300, 2023. [16] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [17] Lei Jiang, Shuzhou Sun, Biqing Qi, Yuchen Fu, Xiaohua Xu, Yuqiang Li, Dongzhan Zhou, and Tianfan Fu. Chem3dllm: 3d multimodal large language models for chemistry. arXiv preprint arXiv:2508.10696, 2025. [18] Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. [19] Greg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Riccardo Vianello, Sriniker, Gedeck, Gareth Jones, Nadine Schneider, Eisuke Kawashima, Dan Nealschneider, Andrew Dalke, Matt Swain, Brian Cole, Samo Turk, Aleksandr Savelev, Alain Vaucher, Maciej Wójcikowski, Ichiru Take, Vincent F. Scalfani, Rachel Walker, Kazuya Ujihara, Daniel Probst, Guillaume Godin, Axel Pahl, Tadhurst-cdd, Juuso Lehtivarjo, Francois Berenger, and Jason Biggs. Rdkit: Open-source cheminformatics and machine learning. Software, May 2024. Open-source cheminformatics and machine learning. [20] Hao Li, He Cao, Bin Feng, Yanjun Shao, Xiangru Tang, Zhiyuan Yan, Li Yuan, Yonghong Tian, and Yu Li. Beyond chemical qa: Evaluating llms chemical reasoning with modular chemical operations. arXiv preprint arXiv:2505.21318, 2025. [21] Hao Li, Liuzhenghao Lv, He Cao, Zijing Liu, Zhiyuan Yan, Yu Wang, Yonghong Tian, Yu Li, and Li Yuan. How to detect and defeat molecular mirage: metric-driven benchmark for hallucination in llm-based molecular comprehension, 2025. URL https://arxiv.org/abs/2504.12314. [22] Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 415423, 2025. [23] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [24] Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, and Arthur Szlam. Deliberation in latent space via differentiable cache augmentation, 2024. URL https://arxiv.org/abs/2412.17747. [25] Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798, 2023. [26] Liuzhenghao Lv, Hao Li, Yu Wang, Zhiyuan Yan, Zijun Chen, Zongying Lin, Li Yuan, and Yonghong Tian. Navigating chemical-linguistic sharing space with heterogeneous molecular encoding, 2025. URL https://arxiv. org/abs/2412.20888. [27] Siddharth Narayanan, James Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel Rodriques, and Andrew White. Training scientific reasoning model for chemistry. arXiv preprint arXiv:2506.17238, 2025. [28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [29] Qizhi Pei, Zhimeng Zhou, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Lijun Wu, and Rui Yan. Leveraging biomolecule and natural language through multi-modal learning: survey. arXiv preprint arXiv:2403.01528, 2024. [30] Malik Sallam, Khaled Al-Salahat, Huda Eid, Jan Egger, and Behrus Puladi. Human versus artificial intelligence: Chatgpt-4 outperforming bing, bard, chatgpt-3.5 and humans in clinical chemistry multiple-choice questions. Advances in Medical Education and Practice, pages 857871, 2024. [31] Nadine Schneider, Roger Sayle, and Gregory Landrum. Get your atoms in orderan open-source implementation of novel and robust molecular canonicalization algorithm. Journal of chemical information and modeling, 55(10):21112120, 2015. [32] Hongyu Shan, Mingyang Song, Chang Dai, Di Liang, and Han Chen. R-capsule: Compressing high-level plans for efficient large language model reasoning, 2025. URL https://arxiv.org/abs/2509.22131. [33] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [34] Eduardo Soares, Emilio Vital Brazil, Victor Yukio Shirasuna, Dmitry Zubarev, Renato Cerqueira, and Kristin Schmidt. Smi-ted: large-scale foundation model for materials and chemistry. 2024. [35] Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, and Tianfan Fu. Chemmllm: Chemical multimodal large language model. arXiv preprint arXiv:2505.16326, 2025. 15 [36] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large language models improves chemical reasoning. arXiv preprint arXiv:2501.06590, 2025. [37] Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, et al. Cellforge: agentic design of virtual cell models. arXiv preprint arXiv:2508.02276, 2025. [38] Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, and Ziqian Zeng. Synadapt: Learning adaptive reasoning in large language models via synthetic continuous chain-of-thought, 2025. URL https://arxiv.org/abs/2508.00574. [39] Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, and Feiliang Ren. Lta-thinker: Latent thought-augmented training framework for large language models on complex reasoning, 2025. URL https://arxiv.org/abs/2509.12875. [40] Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Xiaoyong Wei, Tianshu Yu, Shuzhou Sun, et al. Chem-r: Learning to reason as chemist. arXiv preprint arXiv:2510.16880, 2025. [41] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Sim-cot: Supervised implicit chain-of-thought, 2025. URL https://arxiv.org/abs/2509.20317. [42] Hao Wen, Yifan Su, Feifei Zhang, Yunxin Liu, Yunhao Liu, Ya-Qin Zhang, and Yuanchun Li. Parathinker: Native parallel thinking as new paradigm to scale llm test-time compute, 2025. URL https://arxiv.org/abs/2509. 04475. [43] Haoyi Wu, Zhihao Teng, and Kewei Tu. Parallel continuous chain-of-thought with jacobi iteration, 2025. URL https://arxiv.org/abs/2506.18582. [44] Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, et al. Nature language model: Deciphering the language of nature for scientific discovery. arXiv preprint arXiv:2502.07527, 2025. [45] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms, 2025. URL https://arxiv.org/abs/2502.12134. [46] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot++: Test-time scaling with soft chain-of-thought reasoning, 2025. URL https://arxiv.org/abs/2505.11484. [47] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [48] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, et al. Chemllm: chemical large language model. arXiv preprint arXiv:2402.06852, 2024. [49] Yu Zhang, Yang Han, Shuai Chen, Ruijie Yu, Xin Zhao, Xianbin Liu, Kaipeng Zeng, Mengdi Yu, Jidong Tian, Feng Zhu, et al. Large language models to accelerate organic chemistry synthesis. Nature Machine Intelligence, pages 113, 2025. [50] Zihan Zhao, Bo Chen, Ziping Wan, Lu Chen, Xuanze Lin, Shiyang Yu, Situo Zhang, Da Ma, Zichen Zhu, Danyang Zhang, et al. Chemdfm-r: An chemical reasoner llm enhanced with atomized chemical knowledge. arXiv e-prints, pages arXiv2507, 2025. [51] Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Yi Xia, Bo Chen, Hongshen Xu, Zichen Zhu, Su Zhu, et al. Developing chemdfm as large language foundation model for chemistry. Cell Reports Physical Science, 6(4), 2025. [52] Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian. Emergence of superposition: Unveiling the training dynamics of chain of continuous thought, 2025. URL https://arxiv.org/abs/2509.23365. [53] Jiaxi Zhuang, Yaorui Shi, Jue Hou, Yunong He, Mingwei Ye, Mingjun Xu, Yuming Su, Linfeng Zhang, Ying Qian, Guolin Ke, et al. Reasoning-enhanced large language models for molecular property prediction. arXiv preprint arXiv:2510.10248, 2025. [54] Chenyang Zuo, Siqi Fan, and Zaiqing Nie. Biomedgpt-mol: Multi-task learning for molecular understanding and generation. arXiv preprint arXiv:2512.04629, 2025."
        },
        {
            "title": "A Efficiency Analysis",
            "content": "To analyze the efficiency gains reported in the main text, we formalize reasoning as trajectory discretization problem. Crucially, curvature-limited discretization arises in the representation space of modality (where discrete steps are taken). A.1 Preliminaries and Assumptions Definition A.1 (Chemical Manifold). Let = (S, g) be smooth d-dimensional Riemannian manifold representing the chemical state space, where encodes physicochemical properties. Definition A.2 (Optimal Reasoning Path). The logical derivation from problem state xstart to solution xend is modeled as unit-speed geodesic γ : [0, L] M, parameterized by manifold arc length s, such that γ γ = 0 and γ(s)g = 1. Assumption A.3 (Perfect Manifold Learning). We assume an idealized scenario where both the Explicit CoT model and the LatentChem model attempt to approximate the same optimal path γ. This isolates modality-dependent efficiency limits from model capacity and optimization. Definition A.4 (Modality Representation Trajectory). Each reasoning modality mod induces differentiable representation map Φmod : Rmmod. Along the optimal path, the modality operates on the represented trajectory cmod(s) := Φmod(γ(s)) Rmmod. Definition A.5 (Valid Discrete Approximation). discrete reasoning chain is specified by partition 0 = s0 < s1 < < sN = and induces polygonal approximation of cmod by connecting consecutive endpoints cmod(sk) and cmod(sk+1) with line segment in Rmmod. The approximation is valid if for each segment (cid:16) sup s[sk,sk+1] dist cmod(s), cmod(sk)cmod(sk+1) δ, (cid:17) where dist is Euclidean distance and δ > 0 is fixed tolerance. Definition A.6 (Effective Curvature in Representation Space). Let denote arc-length along the represented curve cmod in Rmmod. Assuming cmod is 2, define curvature along the path by κmod(s) := (cid:13) (cid:13) (cid:13) (cid:13) d2cmod ds2 (s) (cid:13) (cid:13) (cid:13) (cid:13) . Definition A.7 (Intrinsic Semantic Resolution (Path-Advance)). The intrinsic semantic resolution of modality mod is the maximum advance along the optimal path that single discrete step can make: sk+1 sk ρmod for all k. (1) Equivalently, one step can traverse at most ρmod units of manifold arc-length along γ. Assumption A.8 (Non-degenerate Representation Speed). Along the optimal path, the representation does not collapse locally: there exists vmin > 0 such that mod(s) vmin for all [0, L]. (2) 17 A.2 The Curvature--Resolution Theorem Theorem A.9 (CurvatureResolution Trade-off (Qualitative Lower Bound)). Let ρmod be the intrinsic semantic resolution (Definition A.7), and let κmod(s) be the effective curvature of cmod(s) = Φmod(γ(s)) in representation space. Under Assumption A.8, any valid discrete approximation requires at least (cid:32) (cid:33) (cid:114) max (cid:90) 1 ρmod , vmin κmod(s) 8δ ds. Consequently, the efficiency ratio between LatentChem and Explicit CoT can be estimated by η ="
        },
        {
            "title": "Ntxt\nNlat",
            "content": "(cid:18) (cid:18) (cid:82) 0 max (cid:82) 0 max 1 ρtxt , vmin 1 ρlat , vmin (cid:19) (cid:19) (cid:113) κtxt(s) 8δ (cid:113) κlat(s) 8δ ds ds . (3) (4) A.3 Proof of Theorem A.9 Step 1: Geometric constraint in representation arc length. Fix an interval [sk, sk+1] and let its manifold-arclength width be sk := sk+1 sk. Let the representation-space arc length of the curve segment be (cid:90) sk+ ℓk := sk mod(u) du. Define κk := supu[sk,sk+1] κmod(u). standard chordarc deviation (sagitta) bound for 2 curves, stated in terms of representation arc length, gives (cid:16) dist sup s[sk,sk+1] cmod(s), cmod(sk)cmod(sk+1) (cid:17) 1 κk ℓ 2 . (5) Validity implies ℓk (cid:113) 8δ κk . Step 2: Converting to bound on manifold-arc-length advance. By Assumption A.8, ℓk vminsk, hence (6) (7) sk 1 vmin (cid:114) 8δ κk . Independently, Definition A.7 gives the modality advance constraint Combining equation 6equation 7 yields sk ρmod. (cid:32) sk min ρmod, 1 vmin (cid:114) 8δ κk (cid:33) . Step 3: Aggregation into global lower bound. Define w(s) := max (cid:32) 1 ρmod , vmin (cid:114) (cid:33) . κmod(s) 8δ On each interval [sk, sk+1], we have κmod(s) κk, so w(s) max (cid:18) 1 ρmod , vmin (cid:19) (cid:114) κk 8δ for all [sk, sk+1]. Therefore, (cid:90) sk+ w(s) ds sk max sk and summing over yields (cid:82) 0 w(s) ds , proving equation 3. (cid:18) 1 ρmod , vmin (cid:19) (cid:114) κk 8δ 1, 18 A.4 Efficiency Scaling via Manifold Rectification Theorem A.9 implies qualitative trade-off: to keep chord deviation within tolerance δ, high representationspace curvature forces smaller advances sk (hence more steps), while rectified representation reduces curvature and permits coarser discretization. Latent Space (κlat 0). the curvature term vanishes and If representation learning rectifies the latent trajectory so that κlat(s) 0, then Nlat (cid:90) 0 1 ρlat ds = ρlat . Explicit CoT Space (curvature remains large). then for sufficiently complex tasks the curvature term can dominate, leading to If the text-space representation retains high curvature κtxt(s), Ntxt 1 vmin,txt (cid:90) (cid:114) 0 κtxt(s) 8δ ds, and hence larger step count than the rectified latent case. This supports the qualitative hypothesis that LatentChems advantage grows with task complexity when it reduces representation-space curvature."
        },
        {
            "title": "B Benchmark Details and Task Definition",
            "content": "Figure 9Taxonomy of chemical reasoning tasks. The evaluated tasks consist of two categories: (a) open-ended generative tasks (e.g., molecule optimization), characterized by the non-determinism of the optimal solution; (b) precise closed-ended tasks (e.g., fragment counting), which demand unique, deterministic standard answer. B.1 ChemCoTBench ChemCoTBench is step-by-step, application-oriented, and high-quality benchmark for assessing LLM reasoning in chemical applications. ChemCoTBench comprises molecule understanding, molecule editing, molecule optimization, and reaction prediction, covering 22 subtasks and 1,495 examples. These samples are derived from authoritative public databases (e.g., PubChem, ChEMBL, ZINC) and patent corpora, carefully reviewed by the combined effort of LLM and 13 chemists for chemical validity and diversity. Molecule understanding. As shown in Figure 10(a), we evaluate models on progressively more demanding levels of structural comprehension. First, models must recognise and count two fundamental molecular features: (i) functional groups (FGs), which are atom clusters that largely determine molecules physicochemical properties and reactivity; and (ii) rings, which impose fixed conformations and serve as common building blocks in drug design, crystal engineering, and polymer synthesis. Accurate identification and counting of functional groups and rings requires both lexical and syntactic understanding of SMILES and remains challenging for LLMs that lack explicit topological awareness. Second, we assess recognition of higher-order scaffolds: Murcko scaffoldsobtained by systematically removing side chains and used for structural analysis in medicinal chemistryand ring systems, including fused and bridged motifs that present substantial synthetic challenges. These tasks probe hierarchical and topology-aware representations. Finally, we introduce SMILES-equivalence tasks (permutations and mutations) to test whether models can identify chemically equivalent structures despite surface-level SMILES variability, thereby evaluating robustness to representational changes. Molecule editing. This suite (Figure 10(b)) measures whether models can execute basic molecular-editing operations (add, delete, substitute functional groups) when guided by natural-language instructions. These operations are analogous to elementary arithmetic in mathematics and form the primitive actions required for more complex design workflows. Many optimization and synthesis problems can be decomposed into sequences of such edits; thus this task evaluates two core capabilities: (i) preserving chemical validity after each edit, and (ii) correctly applying the requested modification. Molecule optimization. Here we test whether models can propose structural modifications that improve specified target (Figure 10(c)). We consider two classes of objectives. At the physicochemical level, targets include LogP, solubility, and QED to assess improvements in drug-likeness. At the target-specific level, we consider binding-affinitydriven objectives for receptors such as DRD2, GSK3β, and JNK3, which require reasoning about drugtarget interactions. Success on these tasks requires not only syntactic parsing of 20 molecular structure but also inference about how local and global modifications affect complex chemical and biological properties. Molecule description. This task (Figure 10(d)) evaluates whether models can generate accurate and informative natural-language descriptions of molecular structures. Given molecular representation (e.g., SMILES or graph-based encoding), models are required to describe key chemical features such as functional groups, ring systems, heteroatom composition, stereochemistry, and overall structural motifs, and in some cases relate these features to physicochemical or biological properties. Successful performance demands precise parsing of molecular structure, abstraction of salient substructures, and coherent translation of symbolic chemical information into human-readable descriptions, reflecting deep alignment between molecular representation and chemical semantics. Reaction prediction. As shown in Figure 10(e), this set of tasks evaluates chemical reasoning across multiple granularities: (i) Forward prediction: given reactants and reagents, predict the major products and plausible by-products, reflecting knowledge of reactivity, selectivity, and thermodynamic/kinetic considerations; (ii) Single-step retrosynthesis: propose reactants and disconnections for target product, identifying key bond cleavages and feasible transformations; (iii) Reaction-condition recommendation: suggest catalysts, solvents, and reagents appropriate for specified transformation, accounting for solvent effects and catalyst mechanisms that influence yield and selectivity; and (iv) Mechanistic understanding: include next-elementary-step prediction (intermediate species and their formation) and mechanism-route selection (choose the most plausible pathway among alternatives). Together, these tasks span coarse product-level prediction to fine-grained mechanistic reasoning, providing comprehensive benchmark of an LLMs ability to act as chemical-reasoning agent. B.2 Mol-Instructions Mol-Instructions is large instruction dataset for biomolecular tasks. It is organized into three componentsmoleculeoriented , protein-oriented, and biomolecular-text; data were derived from various licensed biochemical resources and generated via template conversion and humanAI augmentation. Among all of the three components, only the molecule-oriented subset is related to our work. Molecular description generation. Molecular description generation evaluates the models high-level comprehension of molecules. Given molecules SMILES string, the model is required to infer the molecules structure, properties, biological activities, and potential applications. This task is evaluated using standard natural-language metrics (e.g., BLEU, ROUGE, METEOR). Description-guided molecule generation. This task requires the model to design new molecules from naturallanguage descriptions. Given descriptions of desired structural features, properties, biological activity, or applications, the model should generate the corresponding molecular SMILES representation. Property prediction tasks. Property prediction refers to the inference or estimation of molecules intrinsic physicochemical properties using information derived from its structural features. This task supports the high-throughput assessment of wide range of molecular properties, thereby enabling efficient virtual screening of compound libraries. Moreover, it allows for the prediction of previously unknown properties of novel molecules, enhancing research productivity and substantially shortening development timelines. Reaction prediction tasks. Reaction prediction tasks comprise three independent problems: forward reaction prediction, retrosynthesis, and reagent prediction. In forward reaction prediction, the model is given reactant and reagent SMILES and must predict the major product SMILES. In retrosynthesis, the model is given product SMILES and must predict plausible reactant SMILES. In reagent prediction, the model is given both reactant and product SMILES and must propose suitable catalysts, solvents, or ancillary substances required to effect the specified chemical reaction. 21 Figure 10Task overview of ChemCoTBench B.3 ChEBI-20 ChEBI-20 was constructed by extracting PubChem compound records annotated with ChEBI and pairing each compound with its ChEBI textual description. To increase informational content and exclude trivial entries, only records whose descriptions exceed 20 tokens were retained. Compounds whose SMILES could not be parsed by RDKit were removed to ensure consistent molecular representations. The final curated dataset contains 33,010 moleculetext pairs and is referred to as ChEBI-20, which has been commonly used as benchmark for textmolecule retrieval, molecule captioning, and text-guided molecular generation. Molecule caption generation. We evaluate models on molecule caption generation to assess molecular understanding. Given molecular SMILES string, the model must generate detailed natural-language description that accurately reports salient structural features, functional groups, and chemically relevant properties present in the input. Evaluation combines standard natural-language quality metrics (BLEU, ROUGE, METEOR). B.4 ChemLLMBench ChemLLMBench is comprehensive benchmark that evaluates large language models on eight practical chemistry tasksincluding name prediction, molecular property prediction, yield and reaction prediction, retrosynthesis, text-based molecule design, molecule captioning, and reagent selectionusing widely recognized datasets (e.g., MoleculeNet, USPTO, PubChem), assessing different abilities such as understanding, reasoning, and explaining. Molecule captioning. Molecule captioning evaluates the models ability to explain molecular structures. Given molecules SMILES string, the model is required to generate textual description of its key features, properties, biological activities, and potential applications. This task is evaluated using standard naturallanguage metrics (e.g., BLEU, ROUGE, METEOR). Text-based molecule design. This task requires the model to design new molecules from natural-language descriptions. Given textual descriptions of desired properties, structural features, biological activity, or applications, the model should generate the corresponding molecular SMILES representation. Reaction-related tasks. Reaction-related tasks comprise four independent problems: yield prediction, forward reaction prediction, retrosynthesis, and reagents selection. In yield prediction, the model estimates if reaction is high-yielding given reactants and reagents. In forward reaction prediction, the model is given reactant and reagent SMILES and must predict the major product SMILES. In retrosynthesis, the model is given product SMILES and must predict plausible reactant SMILES. In reagents selection, the model selects appropriate reagents, ligands, or solvents from candidates for given reaction."
        },
        {
            "title": "C Detailed Training Protocol",
            "content": "Stage 1: Establishing the Molecular-Linguistic Mapping The primary goal of this stage is to align the projected molecular representations Hchem with the semantic space of the pre-trained LLM. To establish robust mapping, we utilize instruction-tuning data containing reasoning chains but intentionally suppress the intermediate reasoning steps during this phase. Specifically, given sample comprising an instruction xprompt, chain-of-thought (CoT), and final answer yans, we train the model to directly generate the final answer. This answer-only supervision acts as an information bottleneck, compelling the projector to compress all necessary chemical properties into the ChemTokens. To ensure the model genuinely utilizes these molecular features rather than memorizing textual patterns, we employ Counterfactual Alignment strategy. Let LCE(yH) denote the standard cross-entropy loss. We define the loss on the original clean input as Lclean = LCE(yansHchem, Hprompt). Simultaneously, we perform corrupted forward pass using perturbed ChemTokens Hchem = Φ(Hchem) (via dropout, shuffling, or noise) to compute Lcorrupt = LCE(yans Hchem, Hprompt). The counterfactual alignment loss is defined as hinge loss: where γ is margin hyperparameter. The total objective for Stage 1 is: LCF = max(0, γ (Lcorrupt Lclean)) L(1) total = Lclean + λLCF (8) (9) Stage 2: SFT for Molecule-aware CoT In this stage, we transition from direct mapping to complex reasoning. We unlock the full potential of the data by training the model on the complete sequences, requiring it to generate the explicit CoT derivations prior to the final answer. This enables the model to decompose complex molecular problems into intermediate steps. To prevent hallucinated reasoning and ensure that the generated logic remains grounded in the molecular structure, we continue to employ the counterfactual strategy. The target sequence is extended to include both the reasoning chain and the answer, denoted as yf ull = [ycot, yans]. Accordingly, the clean and corrupted losses are updated to Lclean = LCE(yf ullHchem, Hprompt) and Lcorrupt = LCE(yf ull Hchem, Hprompt). The total objective for Stage 2 becomes: total = Lclean + λLCF This formulation ensures that the model not only generates the correct answer but also derives it through valid, molecule-dependent reasoning path. (10) L(2) Stage 3: Chemistry-aware Latent Mind Activation In this phase, we activate the latent thinking modules during training. major challenge in training these lightweight modules alongside massive LLM backbone is the risk of optimization imbalance: the LLMs overwhelming parameter count can easily lead it to bypass the latent signals, treating them as noise rather than meaningful thought vectors. To mitigate this, we employ Strict Freezing Strategy. We freeze both the ChemAdapter and the LLM backbone, updating only the parameters of the thinker and updater modules. This constraint forces the latent modules to adapt to the frozen semantic space of the LLM, compelling them to generate legible latent thoughts that effectively guide the LLMs frozen decoder toward the correct reasoning path defined by the CoT data. Stage 4: GRPO with Latent Thinking Budget With the latent mind activated, we employ GRPO to refine the models policy. Reversing the freezing strategy from the previous stage, we now freeze the latent thinking modules and fine-tune the remaining parameters. This strategy treats the learned latent dynamics as stable internal simulator, optimizing the backbone LLM to effectively utilize these latent thoughts for decision-making. The optimization is guided by composite reward function comprising four terms: format adherence, answer validity, and answer correctness."
        },
        {
            "title": "D Experimental Implementation Details",
            "content": "D.1 Dataset and Preprocessing Prompt and Label Formatting. The dataloader structures each training example to enforce specific output formats. Every question is appended with the instruction: Your final answer must be formatted as <answer> </answer>. Ground-truth labels are formatted similarly. The prompt and response are tokenized ... separately and concatenated. D.2 Model Architecture Base Language Model. We use Qwen-3-8B as the backbone LLM. Molecule Encoder. We utilize SMI-TED Light as the molecule encoder. The encoder is kept frozen during all training stages and operates in inference mode (no gradient computation). ChemAdapter. We employ Querying Transformer (Q-Former) style projector with 128 learnable queries, an input dimension of 768, and 8 attention heads. For each input molecule, the adapter produces sequence of 128 projected tokens, encapsulated by special tokens <mol_start> and <mol_end>. Latent Modules. We add specific control tokens to the vocabulary, including <latent>, <start_latent>, and <end_latent>. D.3 Supervised Fine-Tuning (SFT) We adopt three-stage SFT pipeline. All stages are trained using distributed data parallel (DDP) across 8 H200 GPUs using bfloat16 precision. Common Training Configuration. We use the AdamW (8-bit) optimizer with cosine learning rate scheduler, weight decay of 0.01, and gradient checkpointing enabled. The maximum sequence length is set to 8192 tokens. The regularization weight λ = 0.2, and = 0.1 is the margin. Stage 1. We train the LoRA adapters on the Qwen backbone and the ChemAdapter for 3 epochs with learning rate of 2 104 and batch size of 4 per device. Stage 2. Weights are initialized from the Stage checkpoint. We continue training the LoRA adapters and the ChemAdapter for 3 epochs with learning rate of 2 104. Stage 3. This stage incorporates the continuous reasoning loop. We insert block of latent tokens between the prompt and the response: <start_latent> + <latent> + <end_latent>. The number of latent tokens is dynamic, calculated based on the length of the ground-truth CoT (capped at 4 tokens). Training proceeds for 3 epochs with learning rate of 2 104. The latent tokens are masked in the loss, except for the boundary token. D.4 Reinforcement Learning via GRPO (Stage 4) In the final stage, we employ Group Relative Policy Optimization (GRPO) to further refine the models reasoning capabilities. Algorithm and Environment. We adapted the TRL GRPO implementation with vLLM for efficient rollout generation (tensor parallel size 1, GPU memory utilization 0.3). We sample group size of = 8 completions per prompt. Training runs for 1 epoch with learning rate of 1 105 and an effective batch size of 128 prompts per step. We use KL penalty of β = 0.0 and clipping epsilon of 0.2. 25 Reward Functions. We utilize composite reward signal consisting of three equally weighted components: 1. Format Reward: +1.0 if the completion contains non-empty <answer> tags. 2. Type Validity Reward: +0.5 if the content within the tags matches the expected data type (e.g., valid SMILES string, float, or boolean). 3. Correctness Reward: +2.0 if the answer matches the ground truth. Correctness is determined via task-specific oracles, including exact canonical SMILES matching for structure generation and functional group verification for editing tasks. Generation Settings. During rollouts, we use temperature of 1.5 and top-p sampling of 0.9, with maximum completion length of 2048 tokens."
        },
        {
            "title": "E Metrics",
            "content": "Mean Absolute Error: The mean of the absolute deviations between predicted and true values, quantifies the typical magnitude of prediction error in regression tasks. Employed on Functional Groups (FGs) and Rings. Scaffold Similarity: Structural conservation between generated and reference molecules is measured via the Tanimoto coefficient computed on molecular scaffolds. Scores range from 0 to 1, with higher values indicating stronger preservation of the core framework. Used in Murcko scaffolds. Accuracy: The proportion of correctly predicted outcomes, serving as baseline measure of overall correctness. For Molecule Editing tasks (e.g. add, delete, substitute functional groups), an outcome is correct only when the edited molecule stays valid, and the functional group count matches the instruction. Binary classification accuracy is calculated on Ring System. For reaction-prediction tasks (e.g. Forward Prediction and Retrosynthesis), we use Top-1 accuracy, i.e., prediction is counted as correct only when the models highest-ranked output exactly matches the ground-truth molecule(s). Improvement: Absolute gains in the target property induced by Molecule Optimization. We mainly report the mean improvement across samples to indicate typical uplift. success rate: The fraction of optimized molecules that exceed predefined target threshold (for example, solubility over 0.8), reflecting the practical utility in Molecule Optimization tasks. Fingerprint-based Similarities (FTS): Morgan [31], MACCS [9], and RDKit [19] fingerprints are used to reflect correctness and structural similarity between the predicted molecule and the reference molecule in reaction-prediction tasks (e.g. Forward Prediction and Retrosynthesis). Additionally, the FTS metric refers to the average of Morgan, MACCS and RDKit fingerprint-based similarities. Validity: The proportion of generated SMILES strings that are syntactically valid and can be parsed into chemical structures (e.g., successfully converted to RDKit molecule objects), indicating the chemical feasibility of model outputs. Reported explicitly in reaction-prediction tasks (e.g. Forward Prediction and Retrosynthesis). Text Similarities: Text similarity metrics like BLEU [28], ROUGE [23] and METEOR [4] are utilized to assess the quality of generated molecule descriptions in Molecular Description Generation tasks by comparing them with the reference answers. 27 Results for Closed-ended Tasks We report the results for closed-ended tasks on ChemCoTBench, Mol-Instructions and ChemLLMBench to complement the results discussed in Section 5.2. Specifically, results on ChemCoTBench, including Molecule Understanding, Molecule Editing, and Reaction-related tasks are presented in Table 5. In addition, results on Mol-Instructions and ChemLLMBench, which comprise broad reaction-related subtasks, are shown in Table 6. Table 5Detailed performance on closed-ended tasks from ChemCoTBench. Metrics include mean absolute error (MAE) for functional group, Scaffold Similarity for Murcko scaffolds, correct rate (CR%) for ring system count, alongside Top-1 accuracy and fingerprint-based similarities (FTS) for reaction. Category Method Text-only Qwen-3-8B Qwen-3-8B (SFT) Chem. LLMs Stage 1 Stage 1+2 Stage 1+2+4 Latent Chem. LLMs Coconut-Chem LatentChem (Ours) Molecule Understanding Molecule Editing Reaction Functional Group Scaffold Add Delete Sub Fwd major Fwd by FG 7.03 0.12 0.14 0.08 0.07 0.08 0.09 Ring Murcko Ring-sys CR% CR% CR% Top1 FTS Top1 FTS 1.03 0.45 0.10 0.20 0.15 0.30 0.15 0.02 0.40 0.71 0.66 0. 0.64 0.81 36 65 83 63 83 60 97 4 55 62 63 54 71 52 76 61 80 88 68 84 1 37 47 49 44 61 0.00 0.09 0.02 0.35 0.57 0.15 0.44 0.10 0.20 0.63 0.08 0.17 0.42 0. 0.00 0.02 0.12 0.03 0.12 0.02 0.08 0.00 0.07 0.18 0.11 0.20 0.09 0. Table 6Detailed performance on closed-ended tasks from Mol-Instructions and ChemLLMBench. Metrics include Top-1 accuracy and fingerprint-based similarities (FTS) for reaction prediction, alongside correct rate (CR%) for reagent selection. Category Method Mol-Instructions ChemLLMBench Reagent FwdMajor Retro Reagent Sel. Fwd major Retro Top1 FTS Top1 FTS Top1 FTS Ligand% Reactant% Solvent% Top1 FTS Top1 FTS Text-only Qwen-3-8B Qwen-3-8B (SFT) 0.00 0.02 0.00 0.06 0.00 0.02 0.04 0.00 0.05 0.26 0.00 0.30 Chem. LLMs Stage 1 Stage 1+2 Stage 1+2+4 0.00 0.07 0.05 0.55 0.00 0.47 0.00 0.07 0.05 0.42 0.00 0.42 0.00 0.07 0.09 0.52 0.00 0.49 Latent Chem. LLMs 0.00 0.07 0.02 Coconut-Chem LatentChem (Ours) 0.00 0.07 0.03 0.36 0.00 0.36 0.50 0.00 0.52 9 28 34 47 31 13 11 43 51 37 39 48 46 14 34 31 33 31 49 0.00 0.02 0.02 0.00 0.07 0.29 0.00 0.31 0.49 0.00 0.47 0.01 0.04 0.38 0.00 0.46 0.04 0.48 0.00 0.45 0.35 0.00 0.38 0.01 0.02 0.53 0.00 0."
        },
        {
            "title": "G Additional Results for Latent Thinking Budget Testing and Causal Study",
            "content": "This appendix contains supplementary figures supporting the analyses reported in Section 4.2. Omitted from the main text for reasons of space, these plots present additional results that are consistent with the trends reported in the main paper. Specifically, Figure 11 supplements Figure 6 by providing additional evidence of the performance trend, and Figure 12 supplements Figure 5 by providing additional measurements of generated CoT length. Figure 11Additional results for budget stressing testing. We measure task performance (success rate, scaffold similarity, or correct rate) as the maximum allowed latent thinking steps (x-axis) decrease. Figure 12Additional results for causal necessity analysis. We monitor the length of generated explicit CoT (y-axis) as the first latent tokens are replaced with Gaussian noise."
        },
        {
            "title": "H Additional Case Study on Latent Thinking Budget Test",
            "content": "To better demonstrate the spontaneous CoT internalization ability of our model discussed in Section 4.1, we present the full, unedited model outputs with different latent thinking budget for representative case from the solubility optimization subtask alongside with the textual prompt, as illustrated in Figure 13. User: You are an expert chemist. Below is molecule structure: CCNc1ccc(S(=O)(=O)c2ccc(N)cc2)cc1. Optimize the Source Molecule to improve the compounds ability to dissolve in water (Solubility) while following structured intermediate optimization process. Your final answer must be formatted as <answer> SMILES </answer>. Latent Thinking Budget 10: .<answer> Nc1ccc(S(=O)(=O)c2ccc(N)cc2)cc1 </answer> Latent Thinking Budget 0: Step 1: Structural Analysis of Source Molecule: Structural Analysis of Source Molecule: The source molecule contains sulfonamide group (S(=O)(=O)) attached to benzene ring, which is further substituted with methyl-piperazine group (CCN). The sulfonamide is linked to another benzene ring with an amino (-NH2) substituent. The molecule has hydrophobic (methyl-piperazine and aromatic rings) and moderately polar (sulfonamide, amino group) regions. Step 2: Property Analysis: The solubility of the molecule is limited by its hydrophobic methyl-piperazine group and the overall size of the aromatic systems. The sulfonamide and amino groups provide some water solubility but are insufficient for optimal aqueous dissolution. Step 3: Limitation in Source Molecule for Property: The methyl-piperazine group (CCN) is major contributor to low solubility due to its hydrophobic character and potential steric hindrance. The single amino group on the secondary benzene ring offers limited hydrogen bonding capacity. Step 4: Optimization for Source Molecule: 1. Replace the methyl-piperazine group (CCN) with smaller, polar cyclopropylamine group (NCC1CC1) to reduce hydrophobicity and steric bulk. 2. Maintain the sulfonamide and amino groups for polarity. 3. Simplify the substituent structure to enhance solvation. Step 5: Final Target Molecule: Nc1ccc(S(=O)(=O)c2ccc(NCC3CC3)cc2)cc1 <answer> Nc1ccc(S(=O)(=O)c2ccc(NCC3CC3)cc2)cc1 </answer> Figure 13Comparison of model output with maximum latent thinking steps set to 10 versus 0."
        },
        {
            "title": "I Additional Statistics on Inference Effeciency Gain",
            "content": "In this appendix, Figure 14 serves as per-benchmark per-task breakdown of the aggregated results on LatentChems efficiency ratio reported in Figure 8. Within each task category, the measured efficiency remains reasonably consistent across the evaluated benchmarks, supporting the use of averaged results in the main paper. Figure 14Detailed efficiency statistics grouped by benchmarks we used. Task categories are sorted by decreasing LatentChem Efficiency. Within each category, the task order remains consistent with that of other tables and figures in this study."
        },
        {
            "title": "J Comparison with SOTA Models on ChemCoTBench",
            "content": "We adapt selected evaluation tables from ChemCoTBench [20], limiting the reported sub-tasks to those evaluated in our experiments. Specifically, we omit the equivalence subtask from Molecule Understanding, as well as Retrosynthesis, RCR, NEPP, and MechSel from the Chemical Reaction task. Alongside the results reported in the original benchmark, we present the performance of our model on the same sub-tasks for reference. The results are shown in Tables 7, 8, and 9 respectively. Table 7Comparison with results reported on the ChemCoTBench foundational tasks, including molecule understanding, molecule editing, and their correlated subtasks. For the functional-group counting task (FG) and ring counting task (Ring) in the functional-group level molecule understanding, we apply the mean absolute error (MAE) as the evaluation metric. Tanimoto molecule similarity is applied as the evaluation for the Murcko scaffold extraction task (Murcko). The accuracy (%) metric is applied to other subtasks. Models Func-Group Scaffold Molecule-Edit FG Ring Murcko Ring-sys Add Delete Sub Gemini-2.5-pro-think Claude3.7-sonnet-think DeepSeek-R1 o3-mini@20250103 o1-mini@20240912 Qwen3-235B-A22B-think Qwen3-32B-think Llama-Nemo-49B-think GPT-4o@20241120 Deepseek-V3 Gemini-2.0-flash Qwen3-235B-A22B Qwen3-32B Qwen2.5-72B-Instruct Qwen2.5-32B-Instruct Llama-3.1-70B-Instruct Llama-Nemo-49B Gemma-2-27b-it Phi-4-14B OLMo2-32B-Instruct 0.11 0.21 0.27 0.13 0.21 0.42 0.25 0.80 0.17 0.15 0.19 0.42 0.26 0.26 0.36 0.52 0.72 0.19 0.28 0.19 Ether0 BioMedGPT-7B BioMistral-7B Failed 1.6 1.0 0.60 1.60 1.55 0.60 1.25 1.00 0.95 1.90 1.35 1.50 1.65 1.00 0.95 0.60 0.65 1.80 1.77 1.65 1.65 1.05 0.35 2.43 1.85 W/ Thinking 0.51 0.40 0.34 0.39 0.25 0.38 0.21 0. W/o Thinking 0.21 0.24 0.43 0.34 0.22 0.24 0.12 0.12 0.11 0.43 0.15 0.07 87.5 80.0 45.0 75.0 61.7 82.5 75.0 86.8 80.0 76.7 75.0 82.5 68.3 70.0 53.3 68.3 65.0 66.7 70.0 63.3 Domain Expert Models Failed 0.18 0. Failed 53.3 32.5 Latent Chemical LLMs LatentChem (Ours) 0.09 0.15 0. 96.7 100 85 70 65 55 40 20 0 80 70 65 40 30 70 50 60 30 75 60 15 94 10 0 71 85 80 70 55 80 75 55 80 75 75 75 55 80 50 80 55 70 80 30 76 12 10 84 81.7 83.4 68.3 80.0 58.3 71.7 20.0 8.0 65.0 76.7 66.7 66.7 25.0 56.7 48.3 50.0 30.5 35.0 38.3 11.7 78 10 61 32 Table 8Comparison with results reported on the ChemCoTBench Molecule Optimization task. The optimized targets are categorized into physicochemical properties (QED, LogP, solubility) and protein activity-related properties (JNK3, DRD2, GSK-3β), with the latter posing greater challenges to the models chemical knowledge and reasoning capabilities. is the mean property improvement, where negative indicates that most optimizations are property degradations. SR% is the success rate that brings property increase. Models LogP Solubility QED DRD2 JNK3 GSK3-β SR% SR% SR% SR% SR% SR% Gemini-2.5-pro-think Claude3.7-sonnet-think DeepSeek-R1 o3-mini@20250103 o1-mini@20240912 Qwen3-235B-A22B-think Qwen3-32B-think Llama-Nemo-49B-think GPT-4o@20241120 DeepSeek-V3 Gemini-2.0-flash Qwen3-235B-A22B Qwen3-32B Qwen2.5-72B-Instruct Qwen2.5-32B-Instruct Llama-3.1-70B-Instruct Llama-Nemo-Super-49B Gemma-2-27b-it Phi-4-14B OLMo2-32B-Instruct Ether0 BioMedGPT-7B BioMistral-7B -0.22 0.41 0.47 0.26 -0.42 0.05 -0.01 -0.24 -0.09 0.09 0.37 0.03 0.0 -0.03 0.15 0.02 -0.01 0.01 -0.26 -1.71 0.0 -0.36 0.01 76 80 69 59 52 40 1 7 37 33 72 21 2 41 44 35 24 31 44 0 17 1 1.06 0.37 0.80 0.81 1.78 0.20 0.13 0.25 0.92 0.57 0.28 0.18 0.08 0.34 0.49 0.72 0.34 0.39 0.22 1.21 0.0 0.25 0.24 W/ Thinking 70 75 80 85 95 40 19 25. 0.28 0.12 0.17 0.21 0.07 0.02 0.01 0.10 W/o Thinking 80 92 58 45 20 59 65 81 40 69 53 46 0.13 0.08 0.13 0.07 0.02 0.07 0.09 0.15 0.08 0.07 0.17 0.08 84 73 72 86 70 24 9 41 70 46 79 34 14 57 54 61 43 56 74 Domain Expert Models 0 63 6 0.0 -0.29 0.0 0"
        },
        {
            "title": "Latent Chemical LLMs",
            "content": "0.36 0.17 0.12 0.19 -0.03 0.03 0.0 0.03 0.07 0.03 0.15 0.04 -0.01 0.04 0.05 -0.00 -0.00 -0.02 -0.02 -0.05 0.0 -0.09 0.0 74 63 62 69 37 31 4 29.9 48 28 63 26 6 40 32 31 16 15 18 7 0 5 -0.02 0.01 -0.02 -0.03 -0.10 -0.01 -0.02 -0.02 -0.02 0.00 -0.02 -0.01 -0.02 -0.02 -0.02 -0.01 -0.00 -0.00 -0.03 -0.03 0.0 -0.11 -0.01 35 49 29 23 15 23 3 6 30 18 34 18 6 26 19 30 15 16 14 8 0 6 0.06 0.02 0.01 0.01 -0.08 0.01 -0.02 -0.01 -0.00 -0.01 0.01 0.02 -0.02 -0.00 0.01 0.01 0.01 -0.00 -0.00 -0.02 0.0 -0.08 -0.01 68 57 41 45 31 31 6 11.2 39 29 38 25 5 40 31 40 27 17 22 12 0 1 LatentChem (Ours) 1.37 96 1.53 89 0. 83 0.26 74 0.08 60 0. 82 33 Table 9Comparison with results reported on the ChemCoTBench Chemical Reaction task, restricted to forward prediction (Fwdmajor: major-product prediction, and Fwdby: by-product prediction). FTS: molecule fingerprint similarity with reference. Models Gemini-2.5-pro-think Claude3.7-sonnet-think DeepSeek-R1 o3-mini@20250103 o1-mini@20240912 Qwen3-235B-A22B-think Qwen3-32B-think Llama-Nemo-49B-think GPT-4o@20241120 DeepSeek-V3 Gemini-2.0-flash Qwen3-235B-A22B Qwen3-32B Qwen2.5-72B-Instruct Qwen2.5-32B-Instruct Llama-3.1-70B-Instruct Llama-Nemo-49B Gemma-2-27b-it Phi-4-14B OLMo2-32B-Instruct Text+Chem LatentChem (Ours) Fwd major Fwd by Top-1 FTS TopFTS W/ Thinking 0.72 0.73 0.48 0.52 0.26 0.03 0.11 0.09 0.28 0.36 0.19 0.04 0.06 0.04 0.01 0.02 0.04 0.01 0.01 0.0 0.44 0.17 W/o Thinking 0.89 0.87 0.71 0.71 0.31 0.54 0.33 0.18 0.58 0.62 0.56 0.57 0.57 0.49 0.43 0.35 0.40 0.55 0.27 0.10 0.74 0.20 0.25 0.21 0.20 0.11 0.0 0.09 0.04 0.04 0.04 0.01 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.03 0.0 0.0 Latent Chemical LLMs 0. 0.08 0.51 0.31 0.45 0.27 0.17 0.07 0.18 0.18 0.20 0.30 0.07 0.06 0.13 0.13 0.12 0.08 0.08 0.04 0.10 0.07 0.07 0."
        }
    ],
    "affiliations": []
}