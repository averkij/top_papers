{
    "paper_title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving",
    "authors": [
        "Guizhen Chen",
        "Weiwen Xu",
        "Hao Zhang",
        "Hou Pong Chan",
        "Chaoqun Liu",
        "Lidong Bing",
        "Deli Zhao",
        "Anh Tuan Luu",
        "Yu Rong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the \"System 1\" way of quick reactions to the \"System 2\" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model's intermediate reasoning steps unexamined. This fails to assess the model's ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K."
        },
        {
            "title": "Start",
            "content": "FINEREASON: Evaluating and Improving LLMs Deliberate Reasoning through Reflective Puzzle Solving Guizhen Chen1,2* Weiwen Xu2 Hao Zhang2 Hou Pong Chan2 Chaoqun Liu2 Lidong Bing2 Deli Zhao2 Anh Tuan Luu1 Yu Rong2 1 Nanyang Technological University, Singapore 2 DAMO Academy, Alibaba Group, Singapore 5 2 0 2 7 2 ] . [ 1 8 3 2 0 2 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Many challenging reasoning tasks require not just rapid, intuitive responses, but more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the System 1 way of quick reactions to the System 2 style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of models intermediate reasoning steps unexamined. This fails to assess the models ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, logic-puzzle benchmark for fine-grained evaluation of LLMs reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K. Our data and code can be found at https: //github.com/DAMO-NLP-SG/FineReason."
        },
        {
            "title": "Introduction",
            "content": "In cognitive science, human reasoning is typically characterized by two distinct systems: (i) System 1, which is fast, automatic, and effortless, and (ii) System 2, which is slow, analytical, and effortful (Kahneman, 2011). System 2 reasoning enables humans to proactively anticipate future outcomes, reassess intermediate states, and iteratively refine solutions (Yao et al., 2023), thereby allowing them *Guizhen is under the Joint PhD Program between Alibaba and NTU. Figure 1: An illustration of stepwise state checking and transition in Sudoku solution tree. to tackle more complex reasoning tasks. Recent studies suggest that large language models (LLMs) can attain advantages akin to those of System 2 reasoning (OpenAI, 2024; Snell et al., 2024; Guo et al., 2025; Team, 2025). Instead of generating an answer directly, these models can engage in iterative reflection and correction to refine their reasoning processes (Shinn et al., 2023), leading to impressive performance on reasoning-intensive tasks such as mathematics and coding (Qin et al., 2024; Muennighoff et al., 2025). Despite these improvements, the underlying reasoning mechanisms responsible for these enhancements remain underexplored. Existing reasoning benchmarks primarily focus on the final-answer accuracy (Hendrycks et al., 2020; Cobbe et al., 2021; Chen et al., 2021; Hendrycks et al., 2021), which offers limited insight into LLMs internal reasoning processes, such as reflection and correction. For instance, model might reach correct conclusion through flawed reasoning (Zelikman et al., 2022; Figure 2: An illustration of four puzzle categories in FINEREASON. Creswell et al., 2023; Lightman et al., 2024). This diminishes the trustworthiness of model outputs, posing potential risks in practical usage. Moreover, models may achieve high accuracy by exploiting superficial patterns in the training data (Roelofs et al., 2019; Enström et al., 2024), making it difficult to ascertain whether the observed performance gain truly stems from deliberate reasoning. Therefore, there is pressing need for more comprehensive reasoning benchmarks that assess the integrity of intermediate processes. In this work, we present FINEREASON, logicpuzzle benchmark for granular evaluation of LLMs reasoning capabilities. FINEREASON includes four types of puzzles: Sudoku, Graph Coloring, Game of 24, and Grid Puzzles. Solving logic puzzle involves series of discrete steps, and its explicit rules make it straightforward for validating the intermediate states. To structure our evaluation, we propose two key actions in each atomic step, state checking and state transition, as shown in Figure 1. State checking predicts whether the current state can lead to solvable solution (Agarwal et al., 2019; Wang et al., 2024). It captures both retrospective evaluation of prior steps (Lightman et al., 2024) and prospective analysis of future steps. Meanwhile, state transition focuses on determining the next valid step, either moving forward or backtracking to the previous state. Together, these two tasks cover the entire puzzle-solving process, revealing the internal reasoning processes of reflection, correction, and exploration of alternative paths in large language models. Our evaluation results show that OpenAIo1 (OpenAI, 2024) outperforms Gemini-2.0-FlashThinking (Google, 2024) by large margin of 19.7%, despite both being reasoning-oriented models that already excel at common math and coding tasks. On the other hand, general-purpose models, such as GPT-4o (OpenAI et al., 2024), GPT3.5 (OpenAI, 2022), Qwen2.5-72b-Instruct (Qwen et al., 2025), and Gemini-2.0-Flash (Google, 2024) struggle with deep reasoning, often making nearrandom guesses during state checking and showing extremely low performance in state transition task. To improve the reasoning capabilities of models, we develop specialized training set on state checking and transition. Integrating our dataset with math training data consistently boosts performance in mathematical reasoning. For example, when applied to GSM8K using the DeepSeek-R1-DistillQwen-7B model, our state checking and transition data improve math reasoning accuracy from 82.3% to 87.4%, compared to models trained exclusively on math data. This suggests that our data offers straightforward way to boost the reasoning abilities of LLMs, thereby enhancing performance on other reasoning-intensive tasks. Our main contributions are three-fold: 1) We introduce FINEREASON, fine-grained, puzzlebased benchmark accompanied by systematic evaluations on state checking and transition, to comprehensively evaluate reasoning capabilities of LLMs. 2) Our experiments reveal substantial limitations in deep reasoning among general-purpose models, and even in the leading reasoning model, Gemini2.0-Flash-Thinking, leaving substantial room for improvement. 3) We develop supplementary training set focused on puzzles, which enhances general mathematical reasoning in LLMs."
        },
        {
            "title": "2 FINEREASON",
            "content": "We present FINEREASON, logic-puzzle benchmark that comprehensively assesses LLMs reasonPuzzle Sudoku Graph Coloring Game of 24 Logic Grid Puzzles Partial / Complete grid Puzzle State Partial / Complete 9x9 board graph of partially / completely colored vertex Color / Uncolor vertex Partial / Complete arithmetic expression Minimal Move Add / Remove digit Apply / Unapply an operation to two remaining numbers Assign / Remove attributes according to given clue Table 1: The definition of minimal move for each category of logic puzzles in our FINEREASON. ing abilities through stepwise evaluation of state checking and transition. An overview of each puzzle is shown in Figure 2. Inspired by the adage think twice before acting, the two actions capture how models assess the current situation (i.e., state checking) and plan the next move (i.e., state transition), which is essential for deliberate reasoning. Formally, we represent the reasoning process of logic puzzle as = {p1, p2, ..., pn}, where denotes the number of atomic steps. Each step pi consists of puzzle state si and two actions: state checking ac and state transition at i, i.e., pi = (si, ac , at i). Applying these actions to si produces the next state si+1. The puzzle-solving process begins at an initial state s1 and proceeds through sequence of such atomic steps until reaching the solution state sn that satisfies all constraints. In the following section, we first introduce treebased approach for step decomposition(2.1) in our puzzles. Next, we describe the two key actions state checking and state transition that facilitate fine-grained reasoning evaluation of models (2.2)."
        },
        {
            "title": "2.1 Tree-based Puzzle Decomposition",
            "content": "Puzzle solving can be represented as tree, where nodes correspond to intermediate states describing the completion stage of the current puzzle, and edges represent the execution of state checking and state transition, as illustrated in Figure 1. Edges are bidirectional, enabling both the exploration of child states and backtracking to the parent state when encountering dead ends. This process begins at the root node s1 and terminates at solution leaf sn, potentially requiring multiple backtracks to explore alternate paths. To capture all possible states, we perform depth-first search (DFS) from the initial puzzle state s1 until no further valid states remain for exploration. Each DFS step involves minimal move to ensure that no valid state is overlooked. For example, in Sudoku, we add or remove only single digit at each step. Table 1 summarizes the definition of minimal move for each puzzle category. Additionally, we translate puzzle rules into executable code to automatically validate each state. Sudoku. Given partially filled 9 9 Sudoku grid, the aim is to fill the remaining cells such that each row, each column, and each of the nine 3 3 subgrids contains all digits from 1 to 9 exactly once. Sudoku state can be either partially or fully completed 99 grid. The former refers to any intermediate state si encountered during the reasoning process, while the latter is the final solvable state sn. minimal move consists of either adding number to the grid for exploration or removing number for backtracking. Our Sudoku questions are collected from Kaggle.1 Graph Coloring. The puzzle of graph coloring is to assign colors to each vertex in graph such that no two adjacent vertices share the same color. Each puzzle specifies maximum number of colors allowed in graph. graph coloring state is either partially colored graph, denoted as si, or completely colored graph, denoted as sn. minimal move involves either assigning color to vertex or removing color from vertex. To create the questions, we generate random graphs and find their respective chromatic numbers using the backtracking algorithm (van Beek, 2006). Game of 24. In Game of 24, given four numbers, the task is to apply basic arithmetic operations (addition, subtraction, multiplication, and division) in any order to reach exactly the value of 24. Each number must be used exactly once. Each state is partial or complete arithmetic expression. The minimal move is to apply or unapply an operation to two of the remaining numbers. We use the dataset from Yao et al. (2023). Logic Grid Puzzles. Logic grid puzzles require filling grid with attributes from multiple categories (e.g., color, time) based on set of textual clues. Each attribute should appear exactly once per category and satisfy all given clues. Each state can be partially filled or fully completed grid, with the minimal move being adding or removing combination of attributes specified in clue. Unlike previous puzzles, translating textual clues into 1https://www.kaggle.com/datasets/bryanpark/sudoku"
        },
        {
            "title": "Grid Puzzles",
            "content": "verification code is challenging in logic grid puzzles, especially when it involves attribute comparisons. To address this issue, we define three functions: r(v) and c(v) to retrieve the row and column numbers of an attribute v, and (i, j) to identify the attribute at row and column j. These functions encode attributes to structured axis space. Thus, the textual clues can be parsed into conditions involving r(v), c(v), and (i, j) for constraint checks. For example, Clue 1 in Figure 2 can be parsed to (r(Guy), c(Time)) (r(back pain), c(Time)) == 1. We annotate one sample for one-shot prompting using GPT-4o for clue translation into code, followed by manual verification. We ensure all solutions pass the coded clues. Our grid puzzle data is constructed from the dataset collected by Tyagi et al. (2024). We define two key actions, state checking and state transition, which enable movement between states. State Checking. State checking involves assessing if given state si can lead to solvable leaf sn. Based on our constructed puzzle tree, we label state as solvable if at least one valid solution exists in the subtree of si. To ensure diverse difficulty range, we uniformly sample both solvable and unsolvable states across different tree depths. For each sampled state, we prompt models to evaluate its solvability with puzzle rules, prior visited states, and the current state (see Appendix A.2). In general, state checking evaluates two key aspects: 1) It tests if model can reflect on prior steps (i.e., states and actions) to avoid rule violations (e.g., preventing duplicate \"1\"s in Sudoku row). 2) It requires models to anticipate potential dead ends by looking ahead. The second aspect, however, requires higher level of reasoning to proactively detect states that are unsolvable. State Transition. State transition involves making the minimal move defined in 2.1, which requires models to determine the next valid state. Based on the state-checking results, models should proceed if the state is solvable and backtrack otherwise. Specifically, at solvable state, correct transition would be an unvisited child of the given state. At an unsolvable state, the correct move is to backtrack to its parent state. To isolate the impact of state transition from incorrect state checking, our evaluation provides the ground-truth state-checking labels. We sample states from the puzzle tree and"
        },
        {
            "title": "Model",
            "content": "End-to-end Acc."
        },
        {
            "title": "Graph Coloring",
            "content": "Game of 24 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 0 0 5.9 0 0 7.8 3.9 35.3 2.0 80.4 78.4 15.3 3.1 83.7 17.3 48.0 54.1 2.2 2.2 10.9 4.4 34.8 45.7 Table 2: preliminary study of End-to-end puzzlesolving performance of LLMs. construct prompts with puzzle rules, prior visited states, the sampled state, and some unsolvable child states (see Appendix A.2). The inclusion of unsolvable child states is to assess whether models can effectively bypass these bad states."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Datasets. We sample 500 intermediate states per puzzle category, resulting in 2000 test instances for each task: state checking and state transition. Dataset statistics are included in Appendix A.1. Implementations. To elicit the reasoning capability of LLMs, we leverage the zero-shot chain of thought (CoT) prompting (Kojima et al., 2022). To ensure genuine evaluation of LLMs inherent reasoning capabilities, we explicitly include the instruction Do not solve using programming in the prompt, restricting the models from relying on programmatic solutions. Models. We select the best-performing open and closed-source models, including 1) reasoningoriented models with deep thinking: o1 (OpenAI, 2024), Gemini-2.0-Flash-Thinking (GeminiFT, Google (2024)), and 2) general-purposed models that perform straightforward execution: GPT4o (OpenAI et al., 2024), GPT-3.5 (OpenAI, 2022),"
        },
        {
            "title": "Model",
            "content": "SC Acc. ST Acc. Avg."
        },
        {
            "title": "Graph Coloring",
            "content": "Game of"
        },
        {
            "title": "Grid Puzzles",
            "content": "Random GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 Random GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 Random GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 Random GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 50.0 52.4 49.0 50.4 51.6 69.2 81.0 50.0 56.4 52.2 56.8 58.6 92.6 94. 50.0 82.6 56.4 93.4 88.2 96.0 97.4 50.0 52.4 42.6 37.4 40.8 89.0 88.8 - 38.8 10.6 39.0 26.6 48.8 70.2 - 49.4 20.4 45.8 35.4 46.4 65.0 - 23.0 14.2 54.6 39.2 48.6 86.6 - 10.0 11.4 18.8 9.6 51.4 77. - 45.6 29.8 44.7 39.1 59.0 75.6 - 52.9 36.3 51.3 47.0 69.5 79.8 - 52.8 35.3 74.0 63.7 72.3 92.0 - 31.2 27.0 28.1 25.2 70.2 83.2 Table 3: The state checking and transition accuracy using FINEREASON, where SC and ST denote state checking and transition, respectively. Gemini-2.0-Flash (Gemini-F, Google (2024)), and Qwen2.5-72B-Instruct (Qwen2.5-Inst, Qwen et al. (2025))."
        },
        {
            "title": "4 Evaluation Results",
            "content": "In this section, we first present preliminary evaluation of LLMs on end-to-end puzzle solving in 4.1, which reveals inconsistencies in model performance. To gain deeper insights beyond their end-to-end performance, we present our main results focusing on state checking and transition tasks in 4.2, followed by detailed analyses on models behaviors, error patterns and performance across different difficulty levels in the remaining section."
        },
        {
            "title": "4.1 Preliminary: End-to-End Puzzle Solving",
            "content": "Table 2 presents an initial evaluation of LLMs on end-to-end puzzle-solving tasks. Despite their strong performance on coding and math tasks (Qwen et al., 2025), these models are notably weak in end-to-end puzzle solving. Additionally, there are some counter-intuitive observations: Gemini-F outperforms Gemini-FT on Sudoku and Game of 24, yet struggle on the other two puzzles. These inconsistencies suggest that end-to-end puzzle solving alone may not be reliable metric for assessing LLMs reasoning, emphasizing the need for more granular evaluation methods."
        },
        {
            "title": "4.2 Main results",
            "content": "To explore models reasoning capabilities in greater depth, we present our evaluation results on state checking and transition in Table 3. The results reveal noticeable performance gaps between reasoning-oriented and general-purposed models across both tasks. On the state-checking task, reasoning-oriented models (o1 and Gemini-FT) consistently lead the performance in every puzzle category. In contrast, general-purposed models barely match or slightly surpass the random baseline in puzzles like Sudoku and Grid Puzzles. similar trend is observed on the state-transition task, particularly for the Game of 24 and Grid Puzzles. These findings further support the view that inference-time scaling can substantially boost reasoning capabilities (Snell et al., 2024; Muennighoff et al., 2025). Among the reasoning models, Gemini-FT performs on par with o1 in state checking but consistently lags behind in state transition across all puzzle categories. This reveals weaknesses in GeminiFTs reasoning process, particularly in error revision. These findings align well with our practical experience using these models, which provides empirical evidence that FINEREASON offers more accurate reflection of LLMs reasoning. Moreover, we observe that most models exhibit significant execution gap: they perform better in state checking than in state transition. This indicates tendency for models to skip correct intermediate steps or find the short-cut to final answer rather than methodically progressing toward solution."
        },
        {
            "title": "4.3 State-Checking Patterns",
            "content": "A key requirement of state checking is to look ahead at potential paths and detect unsolvable states. To analyze models behaviors in this task, we report the state-checking precision, recall, and F1 scores in Table 4. We designate unsolvable states as positive cases. Therefore, recall measures how likely model detects the dead ends, whereas precision shows the reliability of predicting unsolvable states. We find that reasoning models can generally detect unsolvable states well, as indicated by the high F1 scores. As for general models (GPT-4o, GeminiF, Qwen2.5-Inst), the recall is generally low in Su-"
        },
        {
            "title": "Sudoku",
            "content": "Game 24 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 GPT-4o GPT-3.5 Gemini-F Qwen2.5-Inst Gemini-FT o1 6.4 28.0 3.2 4.8 87.2 73.2 95.6 54.8 98.8 97.6 94.8 95.6 80.0 49.0 57.1 75.0 64.3 86. 75.9 56.6 89.2 82.2 97.5 99.2 F1 11.9 35.6 6.06 9.02 74.0 79.4 84.6 55.7 93.7 89.2 96.1 97.4 Table 4: Precision, recall, and F1 scores of statechecking task in FINEREASON. doku but not in Game of 24. This difference could be attributed to deeper puzzle tree of Sudoku, posing greater challenge in detecting unsolvable states. similar trend is also observed in other puzzles that require more depth of exploration to solve, including graph coloring and grid puzzles (see Appendix A.4). These observations reveal that general LLMs tend to make overly optimistic decisions (i.e., assuming solvable state) when faced with tasks that exceed their actual capabilities. Nevertheless, GPT-4o and Qwen2.5-Inst show high precision, which suggests that these models are conservative and might not attempt to classify states as unsolvable unless they are very confident."
        },
        {
            "title": "4.4 Quality Analysis of State Checking",
            "content": "To examine the errors made in state checking, we conduct human analysis of the mistakes from the best-performing model, o1, in Figure 3. The most common error is Misinterpretation of Premises, where o1 incorrectly uses available information to reach faulty conclusion. For instance, in grid puzzle, given the clue The chocolate piece, Joeys cake, and the $125 cake are three different cakes, the model still mistakenly assigned Joeys cake to the $125 cake. Additionally, the model might fail to explore alternative paths, leading to incorrect assessment of current states. Other mistakes include drawing wrong conclusion despite correct deductions (Inconsistent Reasoning), failure to recognize conflicting information (Conflicts Resolving Failure), nonexistent constraints (False Premise), and few instruction-following errors."
        },
        {
            "title": "4.5 State-Transition Performance Breakdown",
            "content": "To understand models behaviors during state transition, we breakdown the performance by class and 39.2% 17.6% 15.7% 13.7% 2% 11.8% Misinterpretation of Premises Exploration and Backtracking Issues Inconsistent Reasoning Conflicts Resolving Failure False Premise Others Figure 3: Human analysis of error types. count the common mistakes made by models. The left chart of Figure 4 shows the Sudoku state-transition performance breakdown for each class (solvable vs. unsolvable). We observe that most models transit much better on solvable states than on unsolvable ones. The large gap indicates that models are better at proceeding forward from valid state than backtracking. This might be attributed to forward-generation reasoning style of LLMs. This trend, however, does not apply to GPT3.5, which shows significantly weak performance and tends to rely primarily on random guessing. The right chart of Figure 4 shows the errors typically made by models during state transition. At solvable states, common errors include making multiple moves (Multiple Moves), violating Sudoku rules (Invalid Move), and transitioning to an unsolvable child state (Unsolvable Child). At unsolvable states, two primary errors are: failing to return to the parent state (Backtracking Failure) and making an additional move to sibling state after backtracking (Sibling). Among these errors, Backtracking Failure is the most frequent across all models. Models sometimes jump back more than one level (e.g., to grandparent state) or to wrong state, indicating that LLMs struggle with step-bystep backtracking. For reasoning models (o1 and Gemini-FT), transitioning to siblings is the second most frequent error. This error is due to violating the minimal move principle  (Table 1)  , highlighting weaknesses in their instruction-following capability. For general models, they frequently commit an invalid move. This shows that general models often fail to adequately check Sudoku rules."
        },
        {
            "title": "4.6 Performance by Difficulty Level",
            "content": "To understand the state-checking performance across difficulty levels, we plot the density diagrams of correct vs. incorrect predictions by the number of unfilled cells in the current Sudoku state. We observe that Sudoku states with fewer empty 100 80 60 40 ) % ( r A Solvable Unsolvable Multiple moves Invalid move Unsolvable child 500 Backtracking failure Sibling u 250 0 o1 Gemini-FT GPT-4o GPT-3.5 Gemini-F Qwen (b) State-transition error type breakdown 0 o1 Gemini-FT GPT-4o GPT-3.5 Gemini-F Qwen (a) State-transition performance breakdown by class Figure 4: Performance breakdown and error analysis of state-transition in Sudoku."
        },
        {
            "title": "Data",
            "content": "GSM8K MATH DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B None Math-only Mixed None Math-only Mixed 65.5 73.6 76. 79.7 82.3 87.4 45.6 51.1 53.1 63.2 70.7 71.4 Table 5: Training with our puzzle data improves math reasoning on GSM8K and MATH. et al., 2024) on DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025). We prepare 10, 000 samples from our puzzle dataset, and another 15, 000 samples from MetaMathQA (Yu et al., 2024), popular training set for mathematical reasoning. Other training details are reported in Appendix A.3. Note that due to limited resources, we restrict the maximum completion length to 1024 in both training and evaluation. Improvements on math benchmarks. We start with 2, 000 training samples, with 1 : 1 distribution between math and puzzle data. We compare with two baselines: first is the base model, second is training with entirely math samples. The results in Table 5 show that combining puzzle data with MetaMath yields the highest accuracy on both GSM8K and MATH for both models, outperforming training on MetaMath alone. This consistent performance improvement suggests that the state-checking and state-transition logic of puzzlesolving generalize to more general mathematical problems, aligning well with our initial hypothesis. Analysis on the optimal ratio. To investigate the optimal ratio between puzzle-based and mathbased data, we vary the proportion of math samples from 0.4 to 1.0 in combined training set of 10k Figure 5: Density plot of number of empty cells for correct vs. incorrect predictions. cells are more likely to be predicted correctly. As the number of unfilled cells increases, the problem becomes more complex and requires more exploration, and the proportion of incorrect predictions increases."
        },
        {
            "title": "5 Training Results",
            "content": "As highlighted in 4, most models still lack statechecking and state-transition abilities, which are essential for reasoning. We hypothesize that training on these tasks can enhance performance on other reasoning-intensive tasks like math. To validate our hypothesis, we construct training set consisting of state-checking and state-transition data from Sudoku, Graph Coloring, and Game of 24. We then train models on mixture of this puzzle data and standard math data to test whether reasoning skills transfer beyond the puzzles themselves, ultimately improving overall mathematical reasoning. Experimental setup. The puzzle states are easily verified, making it suitable for Reinforcement Learning. Specifically, we perform GRPO (Shao ) % ( r A 80 78 76 74 72 0 0.4 0.5 0.6 0.7 0.8 0.9 1 Ratio = Math / Total Figure 6: The effect of training data mixing (Math v.s. Puzzle) on GSM8K using the 1.5B model. Math-only Mixed ) % ( r A 80 78 76 74 2.5 7.5 10 12.5 15 Number of training samples (k) Figure 7: The scaling performance on GSM8K using the 1.5B model. samples. In Figure 6, the performance on GSM8K steadily improves as the math ratio increases, peaking at ratio of 0.8. Beyond this point, increasing the math ratio further results in lower accuracy. This suggests that neither pure math training nor pure puzzle training is optimal. Instead, balanced combination of puzzle-based and traditional math data provides the best performance. This indicates that our puzzle-based data though simple can complement the reasoning in standard math problems. Analysis on the scaling effect. We investigate the scaling effect of using math-only and mixed math-puzzle data. We keep the ratio of math to 0.8 in the mixed data. The results in Figure 7 show that, as we increase the number of training samples, both approaches benefit from scaling up. Noticeably, the mixed approach consistently outperforms math-only training starting from 5k samples. While math-only training shows diminishing returns or even slight decline beyond 7.5k samples, the mixed approach continues to improve, reaching an accuracy peak of 81.3% with around 12.5k samples. This scaling effect suggests the great potential of our simple puzzle data for enhancing the overall reasoning capability of LLMs."
        },
        {
            "title": "6 Related Work",
            "content": "LLM Reasoning. Advancing the reasoning capabilities of large language models is critical goal in natural language processing (Wos et al., 1992; Yang et al., 2018). In recent years, LLMs, combined with prompting techniques such as Chain of Thought (Wei et al., 2022), Tree of Thought (Yao et al., 2023), and Self-Consistency (Wang et al., 2023), have shown remarkable performance in various reasoning tasks (Cobbe et al., 2021; Srivastava et al., 2022). Current evaluation methods focus mainly on the final accuracy in reasoningintensive domains, including mathematics (Cobbe et al., 2021; Hendrycks et al., 2021; Chen et al., 2023; Rein et al., 2023; Ma et al., 2024), coding (Chen et al., 2021; Austin et al., 2021), commonsense (Mihaylov et al., 2018; Hendrycks et al., 2020), and logical reasoning (Yao et al., 2023; Long, 2023). However, as inference-time scaling gains importance (Snell et al., 2024; Guo et al., 2025) and models are becoming more capable of reasoning, it is crucial to assess how effectively models perform reflection and correction during reasoning. While Tyagi et al. (2024) manually analyze the reasoning chains in logic puzzles, their approach lacks scalability. Some studies (Singh et al., 2024; Zeng et al., 2024) evaluate how models handle reasoning mistakes, but these investigations often rely on rule-based mistakes that may be easily resolved by current LLMs. Moreover, these studies only assess reflection on past steps in static manner. In our work, we address these limitations by introducing two novel tasks designed to more accurately reflect models capabilities in dynamic reasoning and error correction. Puzzle Solving Tasks. Logic puzzles, which require deducing solutions from set of rules (Giadikiaroglou et al., 2024), are ideal for evaluating LLMs reasoning capabilities as they rely minimally on prior knowledge (Li et al., 2024). Recent studies have explored LLMs on various puzzles with different emphases (Mittal et al., 2024), such as Sudoku (Ishay et al., 2023; Long, 2023) for strategic thinking, Game of 24 (Ding et al., 2023; Yao et al., 2023) for arithmetic calculations. Some investigate grid puzzles (Dziri et al., 2024; Tyagi et al., 2024), crosswords (Yao et al., 2023), chess puzzles (Feng et al., 2024), mazes (Noever and Burdick, 2021), and Minesweeper (Li et al., 2024). However, evaluation remains primarily focused on final accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced FINEREASON, novel logic-puzzle benchmark designed to comprehensively evaluate the reasoning capabilities of LLMs. Unlike existing benchmarks that focus mainly on final-answer accuracy, FINEREASON delves into intermediate reasoning steps, specifically emphasizing state checking and transition actions. This fine-grained evaluation captures models ability to reflect, lookahead, and backtrack, which are vital aspects of human-like System 2 reasoning. Our experiments reveal significant gaps between reasoning-oriented and general-purpose LLMs, emphasizing the need to consider reflection and correction for robust reasoning evaluation. Furthermore, using puzzle-based data for training can enhance performance in broader mathematical tasks, highlighting the scalability of this approach and its potential to complement reasoning in standard math problems."
        },
        {
            "title": "Limitations",
            "content": "In our study, we use textual tables to represent puzzle states. Our evaluation shows that models can reasonably understand this table format. However, there is potential to explore alternative representation formats, such as coordinates or images. The image format could be particularly valuable for facilitating the evaluation of multi-modal reasoning, offering promising direction for future extensions of our work. We adopt zero-shot CoT (Kojima et al., 2022) as the prompt format in all our evaluations. While advanced prompting techniques have the potential to enhance performance, their use might shift the focus away from evaluating the genuine reasoning capabilities of LLMs without excessive reliance on external assistance. By focusing on zero-shot CoT, we aim to provide clearer evaluation of the models inherent reasoning abilities."
        },
        {
            "title": "References",
            "content": "Arpit Agarwal, Katharina Muelling, and Katerina Fragkiadaki. 2019. Model learning for look-ahead exploration in continuous control. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 31513158. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. TheoremQA: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Everything of thoughts: Defying the law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. 2024. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36. Daniel Enström, Viktor Kjellberg, and Moa Johansson. 2024. Reasoning in transformers - mitigating spurious correlations and reasoning shortcuts. CoRR, abs/2403.11314. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. 2024. Chessgpt: Bridging policy learning and language modeling. Advances in Neural Information Processing Systems, 36. Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. 2024. Puzzle solving using reasoning of large language models: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1157411591, Miami, Florida, USA. Association for Computational Linguistics. Google. 2024. Gemini-2.0-flash family. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Adam Ishay, Zhun Yang, and Joohyung Lee. 2023. Leveraging large language models to generate answer set programs. arXiv preprint arXiv:2307.07699. Daniel Kahneman. 2011. Thinking, fast and slow. Farrar, Straus and Giroux. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems. Yinghao Li, Haorui Wang, and Chao Zhang. 2024. Assessing logical puzzle solving in large language modIn els: Insights from minesweeper case study. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 5981, Mexico City, Mexico. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Jieyi Long. 2023. Large language model guided tree-ofthought. arXiv preprint arXiv:2305.08291. Jingkun Ma, Runzhe Zhan, Derek F. Wong, Yang Li, Di Sun, Hou Pong Chan, and Lidia S. Chao. 2024. Visaidmath: Benchmarking visual-aided mathematical reasoning. CoRR, abs/2410.22995. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. David A. Noever and Ryerson Burdick. 2021. Puzzle solving without search or human knowledge: An unnatural language approach. ArXiv, abs/2109.02797. OpenAI. 2022. Gpt3.5 turbo. OpenAI. 2024. Learning to reason with llms. OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, ArXiv, GPT-4o system card. et al. 2024. abs/2410.21276. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. An Yang Qwen, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. ArXiv, abs/2412.15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. 2019. meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. Chinmay Mittal, Krishna Kartik, Parag Singla, et al. 2024. Puzzlebench: Can llms solve challenging first-order combinatorial reasoning problems? arXiv preprint arXiv:2402.02611. Joykirat Singh, Akshay Nambi, and Vibhav Vineet. 2024. Exposing the achilles heel: Evaluating llms ability to handle mistakes in mathematical reasoning. arXiv preprint arXiv:2406.10834. Thirty-seventh Conference on Neural Information Processing Systems. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. 2024. MR-ben: meta-reasoning benchmark for evaluating system-2 thinking in LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the arXiv preprint capabilities of language models. arXiv:2206.04615. Kimi Team. 2025. Kimi k1.5: Scaling reinforcement learning with llms. ArXiv, abs/2501.12599. Nemika Tyagi, Mihir Parmar, Mohith Kulkarni, Aswin Rrv, Nisarg Patel, Mutsumi Nakamura, Arindam Mitra, and Chitta Baral. 2024. Step-by-step reasoning to solve grid puzzles: Where do LLMs falter? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1989819915. Association for Computational Linguistics. Peter van Beek. 2006. Chapter 4 - backtracking search algorithms. In Francesca Rossi, Peter van Beek, and Toby Walsh, editors, Handbook of Constraint Programming, volume 2 of Foundations of Artificial Intelligence, pages 85134. Elsevier. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, and Shuqiang Jiang. 2024. Lookahead exploration with neural radiance representation for continuous vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13753 13762. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Larry Wos, Ross Overbeek, Ewing Lusk, and Jim Boyle. 1992. Automated reasoning introduction and applications. McGraw-Hill, Inc. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate In problem solving with large language models."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Dataset Statistics Table 6 presents the statistics for the four tasks, including the total number of questions, as well as the number of solvable and unsolvable states for each task. For grid puzzles, we can only sample 94 solvable states with unsolvable children, resulting in somewhat imbalanced dataset. Nonetheless, we have maintained balance between solvable and unsolvable states for the remaining three puzzles."
        },
        {
            "title": "Questions Solvable States Unsolvable States",
            "content": "Sudoku Graph Coloring Game 24 Grid Puzzles 51 51 98 50 250 250 250 94 250 250 250 406 Table 6: Dataset Statistics A.2 Prompt Templates"
        },
        {
            "title": "Sudoku",
            "content": "Table 9 shows the state checking and state transition prompts for Sudoku example. A.3 Training Details Game of 24 We train our models using GRPO based on OpenR12. We use one GPU to run vLLM for faster generation and the remaining GPUs for training. The hyperparameters and training details are reported in Table 7."
        },
        {
            "title": "Learning rate\nWarm up ratio\nBatch size\nMax prompt length\nMax completion length\nTraining epochs\nHardware",
            "content": "4e-5 0.1 112 1024 1024 1 8 H20 (120 GB) Table 7: Hyperparameter and training details. A.4 Additional Experimental Results Table 8 reports the state-checking precision, recall, and F1 scores of models across four tasks. It is observed that o1 consistently outperforms all other models in detecting unsolvable states, as evidenced by the high recall and precision acorss tasks. Models like GPT-4o, Qwen2.5-Inst and Gemini-F are generally more precise when they predict unsolvability, but are limited by low recall in tasks like Sudoku and Grid Puzzles. GPT-3.5 generally struggles with both recall and precision, especially in more complex tasks like Sudoku. 2https://github.com/huggingface/open-r"
        },
        {
            "title": "Precision",
            "content": "o1 GPT-4o GPT-3.5 Gemini-FT Gemini-F Qwen2.5-Inst o1 GPT-4o GPT-3.5 Gemini-FT Gemini-F Qwen2.5-Inst o1 GPT-4o GPT-3.5 Gemini-FT Gemini-F Qwen2.5-Inst o1 GPT-4o GPT-3.5 Gemini-FT Gemini-F 73.2 6.4 28.0 87.2 3.2 4.8 95.6 95.6 54.8 94.8 98.8 97. 93.1 44.8 27.4 96.8 29.0 25.8 93.8 47.8 39.4 91.6 24.4 86.7 80.0 49.0 64.3 57.1 75.0 99.2 75.9 56.6 97.5 89.2 82.2 95.9 57.8 53.5 89.2 64.3 73.6 92.5 88.2 79.6 94.7 94. F1 79.4 11.9 35.6 74.0 6.06 9.02 97.4 84.6 55.7 96.1 93.7 89.2 94.5 50.5 36.3 92.8 40.0 38.2 93.2 62.0 52.7 93.1 38."
        },
        {
            "title": "Grid Puzzles",
            "content": "Table 8: Precision, Recall and F1 scores of state checking task for all puzzles."
        },
        {
            "title": "State Checking",
            "content": "You are given partially filled 9x9 Sudoku grid represented as list of lists, where empty cells are represented as 0. Your task is to determine if this current state can lead to solvable solution. Specifically, use lookahead techniques to determine if its possible to fill the remaining cells according to standard Sudoku rules, ensuring that each row, column, and 3x3 subgrid contains unique numbers from 1 to 9. Additionally, you are provided with previously explored next state that has been proven to be unsolvable. Use this information to avoid revisiting this failed path and leverage it to make more informed decision about the current state. Current state: [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 4, 1], [3, 6, 0, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] Explored next state that leads to an unsolvable path: [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 4, 1], [3, 6, 1, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] Lets think step by step, considering the failed state to avoid unnecessary exploration. Do not solve using programming. Choose from (A) Solvable (B) Unsolvable. End your answer with \"Answer: (A)\" or \"Answer: (B)\"."
        },
        {
            "title": "State Transition",
            "content": "You are given an initial Sudoku puzzle S(0), followed by sequence of progressive states leading to the current state S(i). Alongside each state, its solvability status L(*) is given. Your task is to determine the next state by making exactly one move, ensuring progress toward valid solution. valid Sudoku solution requires that each row, column, and 3x3 subgrid contains the numbers 1 to 9 without repetition. Additionally, you are provided with previously explored next state that has been proven to be unsolvable. Use this information to avoid revisiting this failed path. move is defined as either: 1. Filling: Replacing 0 in exactly one empty cell with value from 1 to 9. 2. Removing: Replacing value in exactly one filled cell with 0. Initial puzzle: S(0) = [[0, 1, 0, 0, 7, 0, 8, 3, 0], [0, 2, 0, 0, 0, 0, 6, 0, 4], [5, 9, 0, 0, 0, 0, 0, 1, 0], [0, 0, 5, 0, 1, 9, 0, 0, 0], [1, 0, 0, 2, 6, 0, 0, 5, 3], [2, 4, 7, 0, 0, 8, 0, 0, 9], [0, 7, 0, 6, 9, 0, 0, 0, 1], [3, 0, 0, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] L(0) = Solvable Two moves ago: S(i-2) = [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 0, 1], [3, 0, 0, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] L(i-2) = Solvable One move ago: S(i-1) = [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 4, 1], [3, 0, 0, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] L(i-1) = Solvable Current state: S(i) = [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 4, 1], [3, 6, 0, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] L(i) = Solvable Explored next state: S(i+1) = [[4, 1, 6, 9, 7, 2, 8, 3, 5], [7, 2, 3, 1, 8, 5, 6, 9, 4], [5, 9, 8, 3, 4, 6, 2, 1, 7], [6, 3, 5, 4, 1, 9, 7, 2, 8], [1, 8, 9, 2, 6, 7, 4, 5, 3], [2, 4, 7, 5, 3, 8, 1, 6, 9], [8, 7, 2, 6, 9, 3, 5, 4, 1], [3, 6, 1, 0, 5, 0, 0, 7, 2], [0, 0, 0, 0, 0, 4, 3, 8, 0]] L(i+1) = Unsolvable Lets think step by step. Analyze the progress made so far and determine the immediate next move. End your answer with \"Next state: {grid}\", where {grid} is in the same python list format as the previous states. Table 9: Prompt templates for state checking and state transition in Sudoku."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group, Singapore",
        "Nanyang Technological University, Singapore"
    ]
}