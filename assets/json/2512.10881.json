{
    "paper_title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
    "authors": [
        "Kehong Gong",
        "Zhengyu Wen",
        "Weixia He",
        "Mingxi Xu",
        "Qi Wang",
        "Ning Zhang",
        "Zhengyu Li",
        "Dongze Lian",
        "Wei Zhao",
        "Xiaoyu He",
        "Mingyuan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 8 8 0 1 . 2 1 5 2 : r MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos Kehong Gong,1, Zhengyu Wen,2, Weixia He2, Mingxi Xu2, Qi Wang2, Ning Zhang2, Zhengyu Li2, Dongze Lian2, Wei Zhao2, Xiaoyu He2, Mingyuan Zhang,2 1Huawei Technologies Co., Ltd., 2Huawei Central Media Technology Institute Equal Contributions, Corresponding Author Figure 1. Overview of our MoCapAnything framework. Given monocular video and reference 3D asset (mesh/skeleton/appearance), our system first reconstructs 4D mesh sequence from the video and encodes the reference asset via multi-modal prompt encoder. unified motion decoder then predicts joint trajectories, followed by an IK fitting stage that outputs animation in the assets own rig convention. The framework supports both direct motion capture (reference matches video subject) and cross-asset retargeting (reference differs from video)."
        },
        {
            "title": "Abstract",
            "content": "Motion capture now underpins content creation far beyond digital humans, yet most pipelines remain speciesor template-specific. We formalize this gap as CategoryAgnostic Motion Capture (CAMoCap): given monocular video and an arbitrary rigged 3D asset as prompt, the goal is to reconstruct rotation-based animation (e.g., BVH) that directly drives the specific asset. We present MoCapAnything, reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware Inverse Kinematics (IK) Fitting. MoCapAnything comprises three learnable modules and lightweight IK stage: Reference Prompt Encoder that distills per-joint queries from the assets skeleton, mesh, and rendered image set; Video Feature Extractor that computes dense visual descriptors and reconstructs coarse 4D deforming mesh to bridge the modality gap between RGB tokens and the point-cloudlike joint space; and Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo [35] with 1,038 motion clips, each providing standardized skeletonmeshrendered-video triad. Experiments on in-domain benchmarks and in-thewild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits non-trivial cross-species retargeting across heterogeneous rigs, offering scalable path toward prompt-based 3D motion capture for arbitrary assets. Project page: https://animotionlab. github.io/MoCapAnything/ 1. Introduction Motion capture underpins modern content creation beyond digital humans, yet most pipelines remain tied to single species or template. Human-centric systems typically regress SMPL-family [20, 25] parameters from monocular inputs (e.g., DeepPose [33] for 2D keypoints and HMR [11] for SMPL-based 3D recovery) and work well only within that fixed topology. For non-human subjects, categoryagnostic keypoint detection (CAPE) broadens 2D landmark coverage via promptable support examples, but it stops short of producing animation-ready 3D motion [27]. On the motion side, animal mocap usually builds on SMAL [52] and is limited to few quadruped categories, with models and rig assumptions that do not transfer to diverse assets. Consequently, existing solutions fall short in practical pipelines where creators must (i) retarget human/animal 1 motion to non-biological rigs (robots, mechs, toys, articulated props), (ii) animate large heterogeneous asset libraries for games and crowd scenes, (iii) drive VTuber/virtualproduction avatars that frequently change topology, and (iv) spin up IP-specific characters (mascots, creatures) without building new parametric model per species. To address the limitations of fixed-species motion capture, we recast the problem as prompt-based 3D motion capture: given monocular video and an arbitrary rigged 3D asset, the goal is to reconstruct rotation-based animation (e.g., BVH joint rotations) that directly drives that specific character. We refer to this setting as Category-Agnostic Motion Capture (CAMoCap). To make this concrete and reproducible, we curate the Truebones Zoo benchmark, where each motion instance provides clean bundle comprising the rigged skeleton (with standardized joint names and hierarchy), the mesh, and an asset-aligned rendered video. The dataset contains 1,038 motion clips. We hold out 60 for the test set and use the remaining 978 for training. CAMoCap raises three core challenges. First, motion representation: joint rotations are defined in asset-local frames, so direct angle regression across diverse rest poses is brittle. Second, reference-guided estimation: the model must inject information about the target asset into videobased 3D keypoint prediction effectively. Third, multimodal integration: there is gap between dense RGB features and the point-cloudlike structure of keypoints. Bridging them naively may lead to suboptimal accuracy. To tackles above mentioned challenges, we propose novel framework, MoCapAnything, which factorizes motion recovery into (i) 3D keypoint trajectory prediction and (ii) per-joint rotation recovery. It uses three learnable modules followed by lightweight IK stage. The Reference Prompt Encoder distills the assets mesh, skeleton, and rendered image set into structure-aware per-joint queries. The Video Feature Extractor computes dense visual descriptors (e.g., DINOv2 [24]) and reconstructs coarse 4D deforming mesh from the input video. This mesh contributes topologyand geometry-aware cues that bridge the gap between RGB tokens and the point-cloud nature of joints. The Unified Motion Decoder attends over reference queries and video features to produce temporally consistent 3D joint trajectories. Finally, IK Fitting converts these trajectories into asset-specific rotations while respecting hierarchy, bone lengths, joint limits, and temporal smoothness. This modular factorization naturally supports both motion capture (same skeleton) and retargeting (different skeletons) across heterogeneous rigs. Our main contributions are summarized as follows: 1. We formalize new task, Category-Agnostic Motion Capture (CAMoCap), prompt-based 3D motion capture from monocular video and an arbitrary rigged 3D asset. We also release reorganized Truebones Zoo [35] with 1,038 clips, each providing skeletonmeshrenderedvideo triad. 2. We present the first framework for CAMoCap, MoCapAnything, to yield temporally coherent, animationready results across heterogeneous rigs. Specifically, we decouple motion into 3D joint trajectories followed by IK-based rotations to stablize training process and introduce mesh as an auxiliary modality to bridge RGB tokens and joint space. 3. MoCapAnything attains strong in-domain accuracy, generalizes to in-the-wild videos, and shows non-trivial cross-species mocap and retargeting. 2. Related Works 2.1. Pose Estimation Human 2D pose estimation aims to localize anatomical keypoints in images. Classic methods are typically grouped into bottom-up and top-down paradigms: bottom-up approaches first detect all keypoints and then group them into person instances [4], while top-down pipelines detect person bounding boxes and run single-person pose head on each crop [40]. Within the top-down family, heatmap-based networks such as Stacked Hourglass [21], CPN [3], SimpleBaseline [40], HRNet [31], Simple Pose [15], and ViTPose [43] predict per-joint likelihood maps from multi-scale or high-resolution features, whereas regression-style methods including DeepPose [34], RLE [16], and SimCC [17] directly output coordinates or 1D classifications to alleviate heatmap quantization. More recent DETR-style frameworks [2] treat poses and/or keypoints as query sets and perform end-to-end multi-person estimation without handcrafted grouping [28, 41, 48], and visionlanguage approaches such as LocLLM [36] encode keypoints as text descriptions to enable some zero-shot generalization to new landmarks; however, all these architectures remain tightly coupled to predefined human skeleton and keypoint set. Beyond these category-specific keypoint detectors, an emerging line of work aims to relax the dependence on fixed object categories through category-agnostic pose estimation (CAPE). CAPE formulates pose estimation as few-shot problem, where single model predicts keypoints for unseen categories by comparing support keypoints with query images in shared embedding space [42]. POMNet [42] instantiates CAPE with transformer encoder over query images and support keypoints, and regresses similarity scores from their concatenated features. CapeFormer [29] further adopts two-stage matching framework that first proposes candidate correspondences and then refines unreliable matches to improve localization accuracy. Pose Anything [10] departs from treating keypoints as isolated entities and instead models them as nodes in graph, using graph convolutions to exploit structural relationships, break symmetries, and better handle occlusions. More recently, CapeX [26] pushes CAPE beyond purely visual correspondence by replacing annotated support images with text prompts attached to graph nodes, aligning query image features to open-vocabulary textual keypoint descriptions. While these CAPE methods significantly improve generalization across categories, they operate in 2D and focus on static keypoint localization, without modeling 3D trajectories, temporal consistency, or animation-ready joint representations, which are central to our monocular motion capture setting. 2.2. Motion Capture Monocular human motion capture is typically formulated as recovering pose and shape parameters of parametric whole-body models such as SMPL [20] and SMPL-X [25]. With whole-body models, expressive human pose and shape (EHPS) estimation from single RGB image or videojointly modeling body, hands, and facehas attracted much attention. Early optimization-based methods (e.g., SMPLify-X [25]) fit SMPL-X to detected 2D keypoints but are slow and brittle. One-stage frameworks such as OSX [19], AiOS [32], and MultiHMR [1] instead use ViT-style backbones to jointly localize and regress full SMPL-X parameters in single forward pass, alleviating error accumulation and improving robustness. Beyond image-aligned meshes, recent work distinguishes between camera-space and world-grounded human motion recovery. Most HMR and video-based approaches follow the camera-space formulation, regressing SMPL parameters directly from images or clips with CNN, RNN, or transformer encoders (e.g., HMR [11]/HMR2.0 [9], VIBE [13], TCMR [5]), which yields accurate pose but entangles motion with camera movements. To obtain physically meaningful trajectories, multi-camera studios and IMU-based systems rely on calibration or inertial sensors, while recent monocular methods integrate SLAM or visual odometry with learned motion priors (e.g., SLAHMR [51], PACE [14], TRAM [37], WHAC [50], WHAM [30]) to estimate global motion. However, these pipelines remain tied to single human template and are difficult to extend to more general, non-human skeletons. Beyond humans, 3D animal reconstruction has been explored under two main paradigms: model-free and modelbased. Model-free methods make minimal assumptions about anatomy and directly recover deformable surface, e.g., CMR [12] deforms spherical template to reconstruct birds, while LASSIE [49], MagicPony [38], and 3DFauna [18] learn articulated 3D shape from image collections; ViSER [45], LASR [44], BANMo [46], and PPR [47] In contrast, model-based apextend this idea to videos. proaches assume species-specific or parametric 3D template is given or retrievable [39], enabling poseand shapeSMAL [52], an articulated aware analysis over time. quadruped model learned from toy scans, has been widely used [22]. However, these pipelines remain speciesand template-specific, and do not generalize to the diverse, nonanimal skeletons required by arbitrary animatable assets. 3. Method 3.1. Task Formulation In this work, we propose new task, Category-Agnostic Motion Capture (CAMoCap), which aims to reconstruct motion for arbitrary 3D assets with diverse skeletal topologies from monocular videos. This formulation transcends traditional paradigms centered on human or categoryspecific mocap, enabling both motion capture and retargeting for assets of any type or skeletal structure, and thus brings broader applicability and flexibility to animation, virtual production, and creative content creation. Formally, given monocular RGB video = {It}T t=1 depicting moving character or creature, and rigged 3D asset = (M, S, IA) with arbitrary skeletal structure, the goal is to predict sequence of joint rotations {Rt}T t=1 that animates in accordance with the motion in : (V, A) {Rt}T t=1, Rt = {Rt,j}jJ , Rt,j SO(3). (1) Here, denotes the mesh, and the skeleton is = (J , E, o), where is the joint set and denotes the directed parentchild edges. For each edge (i j) E, oij R3 is the offset of joint relative to its parent i. The rest rig also specifies canonical joint labels via naming function ℓ : . The optional appearance is provided as reference image set IA = {I (k) k=1 (e.g., renders or photos of A). This general formulation covers both motion capture (when matches the subject in ) and motion retargeting (when differs from the subject). }K 3.2. Overview To tackle the CAMoCap task, we employ three dedicated, learnable branches to extract features from the reference prompt and the input monocular video, fuse them, and estimate motion sequences. naive alternative is to regress joint rotations directly after feature fusion, but in monocular settings this is brittle due to: (i) parameterization and rigframe ambiguities that make angles asset-dependent, (ii) under-constrained evidence where depth and camera motion entangle local rotations, and (iii) poor temporal continuity from per-frame angle regression. We therefore decompose the problem into 3D keypoint trajectory estimation followed by rotation recovery via inverse kinematics (IK). Accordingly, our approach, MoCapAnything, comprises four components (see Fig. 2): 1. Reference Prompt Encoder: Extracts per-joint features 3 Figure 2. Detailed architecture of our method. multi-modal Reference Prompt Encoder fuses mesh, skeleton, and appearance of the target asset into per-joint queries. monocular video is converted into 4D mesh sequence, and both mesh and video features are extracted. The Unified Motion Decoder fuses these signals via multi-branch attention to predict 3D keypoints, which are converted to asset-specific joint rotations via an optimization-based IK layer. from the reference asset, including skeletal, mesh, and appearance image-set cues. 2. Video Feature Extractor: Uses off-the-shelf models to obtain visual descriptors (e.g., DINOv2 [24]) and reconstruct coarse 4D deforming mesh from the video. The mesh supplies topologyand geometry-aware signals that bridge the modality gap between dense visual tokens and the point-cloudlike joint representation, stabilizing and improving trajectory estimation. 3. Unified Motion Decoder: Fuses reference, geometric, and visual information to predict temporally coherent 3D joint trajectories for the target asset. 4. IK Fitting Process: Converts predicted joint trajectories into asset-specific joint rotations via an optimizationbased IK procedure that respects hierarchy, bone lengths, joint limits, and temporal smoothness. This modular pipeline flexibly supports both motion capture (same skeleton) and retargeting (different skeletons) for arbitrary 3D assets and rig topologies. We introduce the three learnable modules in Sec. 3.3, detail the training objectives in Sec. 3.4, and describe IK fitting for arbitrary 3D skeletons in Sec. 3.5. 4 3.3. Architecture Design 3.3.1. Reference Prompt Encoder }K Let the reference asset be = (M, S, IA) with mesh M, skeleton = (J , E, o), and an image set IA = {I (k) k=1. The encoder outputs per-joint queries = { qj Rd }jJ . For each joint with coordinate xj R3, we apply sinusoidal positional encoding pe(xj) and linear projection to obtain an initial embedding q(0) = Wp[pe(xj); ename(ℓ(j))] + bp (where ename is optional). Variable joint counts are handled by binary mask {0, 1}Jmax that zeroes padded joints in all attention operations. We then apply stacked fusion blocks, each with three submodules in row: 1. Self-Attention with Skeleton Topology. We use graph multi-head attention (Graph-MHA) on {q(ℓ) } with an attention bias Bij computed from skeleton topology (adjacency in and geodesic/kinematic distances), following the AnyTop [8] design: Attn(qi, qj) WQqi, WKqj + Bij, (2) Bij = ftopo(E, i, j). This encourages structure-aware message passing along the kinematic tree. Details will be illustrated in the supplementary material. 2. Cross-Attention to Mesh Geometry. We sample surface points from to form = {(pu, nu)}U u=1 (positions and normals). Mesh tokens are gu = Wm[pe(pu); nu]. Joints attend to {gu} to learn implicit skinning-like relations between joints and local surface geometry. 3. Cross-Attention to Appearance. Images in IA are encoded by frozen image encoder ϕimg (e.g., DINOv2) to obtain appearance tokens ϕimg(IA). We inject appearance cues that disambiguate symmetric or visually similar parts via cross-attention mechanism. Across layers, masked attention ensures invariance to the absolute joint count, and residual/FFN updates refine q(ℓ) q(ℓ+1) by progressively integrating structural (S), geometric (M), and visual (IA) evidence. The final perjoint queries = {q(L) } serve as asset-specific prompts for the Unified Motion Decoder and enable robust generalization across diverse characters and skeleton topologies. 3.3.2. Video Feature Extractor Given monocular video = {It}T plementary streams. t=1, we build two comVisual stream. Each frame is encoded by frozen DINOv2 image encoder ϕimg, yielding per-frame dense tokens At = ϕimg(It) (and an optional global token). These serve as appearance/texture cues. Geometry stream. We apply pretrained image-to-3D reconstructor to obtain coarse deforming surface sequence (cid:99)M = { (cid:99)Mt}T t=1. For each t, we randomly downsample the surface to =1024 points Pt = {(pt,u, nt,u)}U u=1. Points are embedded as gt,u = Wm [pe(pt,u); nt,u; pe(t)], producing geometry-aware tokens Gt = {gt,u}U u=1 analogous to the mesh features used in the Reference Prompt Encoder. We form the video feature set = {At, Gt}T t=1 (keys/values for the decoder). The 4D mesh tokens provide topology/geometry signals that bridge dense RGB features and the point-cloudlike joint space, stabilizing subsequent 3D keypoint estimation. 3.3.3. Unified Motion Decoder Given the per-joint prompts = {qj}jJ , the skeleton = (J , E), and video features = {At, Gt}T t=1 (DINOv2-based visual tokens At and 4D-mesh point tokens Gt), we tile across time, add temporal encoding to obtain {h(0) t,j }, and apply binary joint mask to accommodate variable-size skeletons. Each decoder layer refines these tokens through the following four stages: 1. Graph-based self-attention (intra-frame). Within each frame, joint tokens are updated using an attention layer with an explicit topology bias derived from (same as AnyTop [8]), ensuring that updates respect the kinematic tree and local limb couplings. 2. Temporal video cross-attention. For each joint at time t, sliding window over neighboring frames provides visual tokens that supply short-range appearance cues. Attending to this window improves continuity, fills in details under occlusion or motion blur, and stabilizes rapid movements. 3. Temporal point-cloud cross-attention. Joint tokens then aggregate geometry-aware evidence from the corresponding 4D mesh window. These point tokens inject topology/shape signals that bridge dense RGB features and the point-cloudlike joint space, disambiguating depth and self-occlusion and capturing non-rigid deformations. 4. Temporal self-attention (per joint). Finally, windowed self-attention along the time axis mixes each joints past and future states to enforce longer-range consistency and reduce jitter, while better modeling higherorder dynamics. Residual connections, normalization, and feed-forward updates follow each block, and stacking layers progressively integrates structural (S), visual (At), and geometric (Gt) cues. lightweight MLP head then predicts per-frame joint positions {(cid:98)xt,j R3}, yielding trajectories for the subsequent IK stage. 3.4. Training Objective We supervise the decoder with masked position regression loss consistent with our notation above. Let (cid:98)xt,j R3 be the predicted 3D position of joint at time {1, . . . , }, and let xt,j be the ground-truth position. Since assets have different skeleton sizes, we pad all sequences to Jmax joints and use binary joint-validity mask {0, 1}Jmax (with mj = 1 iff for this asset). Lpos = 1 (cid:80) mj (cid:88) (cid:88) t=1 (cid:80)T t=1 mj (cid:13) (cid:13)(cid:98)xt,j xt,j (cid:13) (cid:13)1. We do not apply rotation space or explicit temporal losses during training: the network predicts joint positions, and rotations are obtained afterwards by the IK stage. 3.5. IK Fitting Process We recover joint rotations from the predicted 3D joint trajectories using lightweight two-stage IK procedure. First, we compute per-frame geometric IK initialization by aligning rest-pose bone directions with the observed joint positions along each kinematic chain. This closed-form step provides stable rotation estimate that respects the skeleton hierarchy. Then, we refine the rotations with small differentiable IK optimization that minimizes the discrepancy between FK-reconstructed joints and the predicted 3D positions, while regularizing the solution toward the geometric initialization. The optimization is warm-started from the previous frame to ensure temporal stability and suppress unnecessary twist. This hybrid strategy produces accurate and smooth joint rotations at minimal computational cost. Additional implementation details are provided in the supplementary material. 4. Experiments 4.1. Dataset and Evaluation Protocol We evaluate our approach on the Truebones Zoo [35] dataset, which contains 1,038 animal motion sequences (totaling 104,715 frames) spanning broad range of species and kinematic structures and 1000 random samples from objaverse [6, 7]. For testing, we curate set of 60 sequences with enough diversity, stratified into three groups: Seen (species with abundant training data), Rare (species with limited training data), and Unseen (species never seen during training). This protocol enables thorough assessment of model generalization. 4.2. Evaluation Metrics To disentangle the contributions of the two stages, we evaluate them separately(i) 3D joint positions (cid:100)xt,j and (ii) joint rotations (cid:100)Rt,j. In the main paper we focus on quantitative results for 3D keypoint prediction, while rotation-level evaluation (after IK) is deferred to the supplementary material. As for 3D keypoints, we report the following metrics: MPJPE (Mean Per Joint Position Error): the mean Euclidean distance between predicted and ground-truth joint positions (lower is better). MPJVE (Mean Per Joint Velocity Error): the average velocity difference per joint, capturing temporal consistency and motion plausibility. CD-Skeleton (Chamfer Distance): computes the symmetric Chamfer distance between predicted and groundtruth 3D joint sets, taking into account their kinematic structure. See supplementary material for details. 4.3. Comparison with SOTA Approach To our best knowledge, the only existing state-of-the-art method that attempts category-agnostic animal motion capture is GenZoo [23]. However, its current pipeline mainly supports quadruped species and struggles to generalize to more diverse or non-quadruped skeletons. For comprehensive comparison, we evaluate both methods on the TrueFigure 3. Qualitative comparison with GenZoo on the Truebones Zoo dataset. Our method produces smoother trajectories and maintains stable, anatomically plausible motions across wide variety of skeleton types, including non-quadrupeds. In contrast, GenZoo is limited to quadruped structures and often fails to generalize to more diverse or complex skeletal configurations. Visualizations highlight our approachs superior accuracy, robustness, and generalization ability. bones Zoo dataset, using the CD-Skeleton metric to measure the structural accuracy of the predicted skeletal motion. CD-Skeleton Model Quad Non-Quad All genzoo ours 0.4466 0. 0.4740 0.2821 0.4580 0.2549 Table 1. Experiment Results in Truebones Zoo dataset. As shown in Table 1, our approach achieves significantly lower CD-Skeleton errors than GenZoo across all categories. On the overall test set, our method reduces the average error from 0.4580 to 0.2549, indicating substantial improvement in capturing and reconstructing diverse skeletal motions, especially for non-quadruped species where existing methods perform poorly. Figure 3 presents representative qualitative results on the Truebones Zoo dataset. Compared to GenZoo, our predictions exhibit smoother motion trajectories, higher anatomical fidelity, and robust stability across both quadruped and non-quadruped skeletonsincluding bipeds, birds, reptiles, and even non-biological assets. GenZoo, while currently the most widely applicable animal motion capture method, is fundamentally constrained by its reliance on quadruped skeleton templates and struggles to generalize to broader categories. For further qualitative comparison, we provide side-byside visualizations of our results and GenZoos on our project homepage, showcasing the advantages of our approach in both accuracy and generalization. 4.4. Ablation Study Since no prior method directly addresses prompt-based, category-agnostic 3D motion capture, we conduct ablations on our own framework. We compare the following variants: Ours w/o image: removes the reference image-set encoder and corresponding cross-attention modules. Ours w/o mesh: removes mesh features from both reference and video streams. Ours w/o GMHA: removes the graph multi-head attention over the skeleton. Ours: the full model with all modalities and modules. As shown in Table 2, removing any modality or module leads to clear performance drops, especially in the rare and unseen splits. The mesh and graph-attention branches are crucial for robust transfer to new species, highlighting the importance of explicit topology and geometry modeling. Table 3 further examines the impact of encoder and decoder layer configurations. Our chosen architecture (4 encoder layers, 12 decoder layers) achieves consistently strong performance across seen, rare, and unseen subsets, notably lowering MPJVE on unseen data to 0.38 while maintaining competitive MPJPE. These results indicate that our method not only improves reconstruction accuracy but also generalizes robustly to rare and unseen motion patterns, highlighting the effectiveness of our architectural design. 4.5. Qualitative Results on Truebones Zoo-Test Figure 4 presents representative Truebones Zoo-test results. Row 1 shows input video Jugar attack. Row 2 displays the same-species reference and predicted mocap outputs. Rows 35 show results when retargeting to skeletons of three different species. Our approach generalizes robustly across species and maintains temporally consistent, anatomically plausible 3D motion even with significant appearance and shape variation. 4.6. Qualitative Results on Objaverse In addition to animal skeletons, our framework also supports human-like rigs, enabling both human motion capture and cross-domain retargeting between humans and animals. Our model can transfer motion from humans to animals and vice versa, demonstrating strong versatility across different skeleton types. Representative qualitative resultsincluding humanoid mocap, human-to-animal, and animal-to-human retargetingare provided on our project homepage. Figure 4. Truebones Zoo-test results. Each row visualizes one evaluation sequence. Row 1: Input video frames. Row 2: Samespecies reference skeleton and predicted mocap results. Rows 3 5: Reference skeletons from three different species and retargeted motions by our method. Our method generalizes across species and produces stable, anatomically plausible 3D motion. Figure 5. Real-world (Wild) results. Similar layout as Figure 4. Row 1: Input wild video frames. Row 2: Same-species reference skeleton and predicted mocap outputs. Rows 34: Cross-species reference skeletons and our retargeted motion predictions. Despite real-world challenges, our method maintains robustness and stability. 4.7. In-the-Wild Generalization To further assess robustness, we apply our trained model to variety of in-the-wild animal videos collected from the Internet, including birds (chickens, eagles, seagulls), quadrupeds (tigers, leopards, elephants, cats, dogs), and other animals such as crabs, fish, and snakes. As shown in Figure 5, our method successfully reconstructs plausible 3D skeletal motion despite unseen topologies, demonstrating strong generalization. 4.8. Arbitrary Cross-Species Retargeting unique feature of our approach is prompt-based retargeting across arbitrary asset typeseven for reference skeletons entirely unrelated to the subject in the input video. 7 Seen Rare Unseen Method MPJPE MPJVE MPJPE MPJVE MPJPE MPJVE Ours w/o image Ours w/o mesh Ours w/o GMHA Ours 10.25 14.36 8.26 8.06 1.35 1.26 1.10 0. 14.72 21.20 14.01 12.04 0.87 0.77 0.75 0.74 41.57 46.01 26.57 25.63 0.44 0.59 0.38 0.38 Table 2. Ablation study on input modalities and modules. Lower is better (). Results are reported on the Truebones Zoo-test set across three generalization levels: seen, rare, and unseen species. Method Variant 1 Variant 2 Variant 3 Ours Architecture Seen Rare Unseen Layer (Enc.) Layer (Dec.) MPJPE MPJVE MPJPE MPJVE MPJPE MPJVE 1 2 4 4 12 12 16 12 7.37 8.46 8.00 8.06 0.75 1.02 0.83 0.88 17.86 12.92 13.72 12.04 0.78 0.76 0.78 0. 31.82 29.13 27.01 25.63 0.75 0.74 0.75 0.38 Table 3. Ablation results with encoder/decoder layer configurations. Metrics are reported on the Truebones Zoo-test set under seen, rare, and unseen generalization. Figure 6. More Truebones in-the-wild mocap results. Our method generalizes to diverse range of species and scenarios. Figure 7. Unconstrained cross-species retargeting. Examples of using our model to retarget motion from one species to another, yielding diverse, creative, and physically plausible animations. 1st row: from chicken to Raptor, 2nd row: Flamingo to Jaguar. Although not explicitly trained for cross-species transfer, our model leverages structural, visual, and geometric cues to synthesize plausible retargeted motion. 5. Conclusion We observe wide range of creative results: bird videos drive quadrupeds to perform flapping-like actions or animate pterosaurs; fish swimming is transferred to crocodiles or snakes; dog running animates bipedal birds; crocodile tail-whipping is retargeted to leopards or parrots. Such unconstrained retargeting enables new workflows for animation (see Figures 6 and 7 for more results). Given the lack of directly comparable baselines, we focus on extensive qualitative analysis and ablation, providing thorough visualization of our results and highlighting the practical versatility of our approach. In this work, we reformulate the motion capture problem as Category-Agnostic Motion Capture (CAMoCap), novel paradigm in which monocular video and an arbitrary rigged asset function as input prompts to generate rotationbased animations tailored to the target character. We further propose MoCapAnything, reference-guided factorized architecture that initially estimates 3D joint trajectories and subsequently reconstructs asset-specific rotations through constraint-aware inverse kinematics, while mitigating cross-modal discrepancies between RGB and joint representations via an intermediate coarse 4D mesh. Using our proposed reorganized the Truebones Zoo benchmark, comprising 1,038 annotated clips with 60 test sequences 8 and providing standardized skeleton-mesh-rendered video triples, MoCapAnything consistently produces temporally stable, animation-ready outputs across diverse rigging systems, demonstrating notable in-domain precision, robust generalization to in-the-wild scenarios, and semantically meaningful cross-species motion retargeting capabilities. Limitations and future work. Our performance depends on the quality of the pretrained image-to-3D reconstructor and assumes access to rig with known it also operates primarily in camera joint structure; space without explicit physics or contact reasoning. Future directions include end-to-end, contactand physicsaware IK, world-grounded trajectory recovery, reducing reliance on 4D reconstruction (e.g., video-only geometry priors), text-only or multimodal prompts beyond rendered images, and extensions to multi-character interaction."
        },
        {
            "title": "References",
            "content": "[1] Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Bregier, Philippe Weinzaepfel, Gregory Rogez, and Thomas Lucas. Multi-hmr: Multi-person whole-body human mesh recovery in single shot. In European Conference on Computer Vision, pages 202218. Springer, 2024. 3 [2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 2 [3] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cascaded pyramid netIn Proceedings of work for multi-person pose estimation. the IEEE conference on computer vision and pattern recognition, pages 71037112, 2018. 2 [4] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, and Lei Zhang. Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation. In CVPR, 2020. 2 [5] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Beyond static features for temporally consistent 3d human pose and shape from video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19641973, 2021. 3 [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. 6 [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. [8] Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit Anytop: Character Bermano, and Daniel Cohen-Or. animation diffusion with any topology. arXiv:2502.17327, 2025. 4, 5 arXiv preprint [9] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 3 [10] Or Hirschorn and Shai Avidan. graph-based approach arXiv preprint for category-agnostic pose estimation. arXiv:2311.17891, 2023. 2 [11] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 71227131, 2018. 1, 3 [12] Angjoo Kanazawa, Shubham Tulsiani, Alexei Efros, and Jitendra Malik. Learning category-specific mesh reconstrucIn Proceedings of the Eurotion from image collections. pean conference on computer vision (ECCV), pages 371 386, 2018. 3 [13] Muhammed Kocabas, Nikos Athanasiou, and Michael Black. Vibe: Video inference for human body pose and In Proceedings of the IEEE/CVF conshape estimation. ference on computer vision and pattern recognition, pages 52535263, 2020. 3 [14] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and camera motion estimation from inIn 2024 International Conference on 3D the-wild videos. Vision (3DV), pages 397408. IEEE, 2024. 3 [15] Jia Li, Wen Su, and Zengfu Wang. Simple pose: Rethinking and improving bottom-up approach for multi-person pose estimation. In Proceedings of the AAAI conference on artificial intelligence, pages 1135411361, 2020. [16] Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu. Human pose regression with In Proceedings of the residual log-likelihood estimation. IEEE/CVF international conference on computer vision, pages 1102511034, 2021. 2 [17] Yanjie Li, Sen Yang, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, and Shu-Tao Xia. Simcc: simple coordinate classification perspective for human pose estimation. In European Conference on Computer Vision, pages 89106. Springer, 2022. 2 [18] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, and Jiajun Wu. Learning the 3d fauna of the web. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97529762, 2024. 3 [19] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu Li. One-stage 3d whole-body mesh recovery with component In Proceedings of the IEEE/CVF Conaware transformer. ference on Computer Vision and Pattern Recognition, pages 2115921168, 2023. 3 [20] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. SMPL: skinned multiperson linear model. ACM transactions on graphics (TOG), 34(6):116, 2015. 1, 3 [21] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In European conference on computer vision, pages 483499. Springer, 2016. 2 [22] Tomasz Niewiadomski, Anastasios Yiannakidis, Hanz Cuevas-Velasquez, Soubhik Sanyal, Michael J. Black, SilCoRR, via Zuffi, and Peter Kulits. Generative zoo. abs/2412.08101, 2024. 3 [23] Tomasz Niewiadomski, Anastasios Yiannakidis, Hanz Cuevas-Velasquez, Soubhik Sanyal, Michael J. Black, Silvia Zuffi, and Peter Kulits. Generative zoo. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 6 [24] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. CoRR, abs/2304.07193, 2023. 2, 4 [25] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. 1, 3 [26] Matan Rusanovsky, Or Hirschorn, and Shai Avidan. Capex: Category-agnostic pose estimation from textual point explanation. arXiv preprint arXiv:2406.00384, 2024. [27] Matan Rusanovsky, Or Hirschorn, and Shai Avidan. Capex: Category-agnostic pose estimation from textual point exIn The Thirteenth International Conference on planation. Learning Representations, 2025. 1 [28] Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, and Wenming Tan. End-to-end multi-person pose estimation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1106911078, 2022. 2 [29] Min Shi, Zihao Huang, Xianzheng Ma, Xiaowei Hu, and Zhiguo Cao. Matching is not enough: two-stage framework for category-agnostic pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73087317, 2023. 2 [30] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing world-grounded humans with accurate 3d motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2070 2080, 2024. 3 [31] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose esIn Proceedings of the IEEE/CVF conference on timation. computer vision and pattern recognition, pages 56935703, 2019. 2 wei Liu, Lei Yang, et al. Aios: All-in-one-stage expresIn Proceedings of sive human pose and shape estimation. the IEEE/CVF conference on computer vision and pattern recognition, pages 18341843, 2024. [33] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. 1 [34] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16531660, 2014. 2 [35] Truebones. Truebones motion capture mocap files, n.d. Accessed: 2025-05-22. 1, 2, 6 [36] Dongkai Wang, Shiyu Xuan, and Shiliang Zhang. Locllm: Exploiting generalizable human keypoint localization via large language model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 614623, 2024. 2 [37] Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from inthe-wild videos. In European Conference on Computer Vision, pages 467487. Springer, 2024. [38] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning arIn Proceedings of the ticulated 3d animals in the wild. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87928802, 2023. 3 [39] Yuefan Wu*, Zeyuan Chen*, Shaowei Liu, Zhongzheng Ren, and Shenlong Wang. CASA: Category-agnostic skeletal animal reconstruction. In NeurIPS, 2022. 3 [40] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 466481, 2018. 2 [41] Yabo Xiao, Kai Su, Xiaojuan Wang, Dongdong Yu, Lei Jin, Mingshu He, and Zehuan Yuan. Querypose: Sparse multiperson pose regression via spatial-aware part-level query. Advances in Neural Information Processing Systems, 35: 1246412477, 2022. 2 [42] Lumin Xu, Sheng Jin, Wang Zeng, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, and Xiaogang Wang. Pose for everything: Towards category-agnostic pose estimation. In European conference on computer vision, pages 398416. Springer, 2022. 2 [43] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. [44] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William Freeman, and Ce Liu. Lasr: Learning articulated shape reIn Proceedings of construction from monocular video. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1598015989, 2021. 3 [32] Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi-Sing Leung, Zi- [45] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan. Viser: Video10 specific surface embeddings for articulated 3d shape reconstruction. Advances in Neural Information Processing Systems, 34:1932619338, 2021. 3 [46] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. [47] Gengshan Yang, Shuo Yang, John Z. Zhang, Zachary Manchester, and Deva Ramanan. Physically plausible reconstruction from monocular videos. In ICCV, 2023. 3 [48] Jie Yang, Ailing Zeng, Shilong Liu, Feng Li, Ruimao Explicit box detection unifies arXiv preprint Zhang, and Lei Zhang. end-to-end multi-person pose estimation. arXiv:2302.01593, 2023. 2 [49] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani. Lassie: Learning articulated shapes from sparse image ensemble via 3d part discovery. Advances in Neural Information Processing Systems, 35:1529615308, 2022. 3 [50] Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, et al. Whac: World-grounded huIn European Conference on Computer mans and cameras. Vision, pages 2037. Springer, 2024. 3 [51] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recovery with dynamic cameras. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1103811049, 2022. 3 [52] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael J. Black. 3D menagerie: Modeling the 3D shape and pose of animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 3 11 MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos 6. More Visualization Results 7. Implementation Details"
        },
        {
            "title": "Supplementary Material",
            "content": "In this section, we summarize additional qualitative results from our supplementary webpage. These visualizations highlight the effectiveness of our approach across controlled multi-species datasets, in-the-wild videos, and crossspecies retargeting scenarios, showing that our model produces high-fidelity and temporally smooth motion under broad range of conditions. Comparison with GenZoo. We compare our results with GenZoo, single-image animal pose and shape estimator trained on synthetic quadruped data. Without temporal modeling, GenZoo exhibits frame-wise inconsistencies and pose fluctuations when applied to video sequences, even for quadruped inputs. In contrast, our method models motion dynamics explicitly, yielding smoother and more coherent 4D reconstructions that better follow ground-truth trajectories. Mocap Results. The supplementary webpage provides additional mocap visualizations. From Truebones Zoo, we show examples spanning multiple animal species with diverse skeletal structures; from Objaverse, we include bipedal characters to demonstrate adaptability across different asset types. We also present in-the-wild cases such as flying birds and crocodiles to illustrate performance on real video inputs. Arbitrary Motion Retargeting. We further include motion retargeting examples: Zoo2Zoo transfer across different animal species, Human2Zoo transfer applying human motions to animal skeletons, and Zoo2Human transfer mapping animal motions to human skeleton. For In-theWild2Human results, motions from videos of animals such as eagles and leopards are retargeted to human skeleton. These examples show that our model handles large variations in morphology, topology, and motion dynamics. IK Visualization. We also provide IK fitting visualizations, showing recovered joint rotations and the improved temporal stability and orientation consistency achieved through geometric initialization, temporal warm-starting, and twist-regularized refinement. A. Dataset and Environment Details Dataset Processing. All meshes are first scaled by the bounding box of their rest pose, normalizing each mesh into unit-volume space. For sequence data, we remove the global translation of every frame, compute sequencelevel super bounding box, and uniformly scale the entire sequence into the range [1, 1]3. For in-the-wild video inputs, we assume fixed camera position throughout the sequence. Environment and Hardware. The network consists of 12 layers for decoder, and prompt encoder composed of 4 layers. All experiments are conducted on 8 NPUs, each equipped with 32 GB of memory. The model is trained for 60 epochs using the Adam optimizer, requiring approximately 36 hours in total. We use learning rate of 1 104 and batch size of 1 per NPU. For each iteration, we randomly sample sequence of 24 frames as the input sequence and another random frame as the reference frame. B. Inverse Kinematics Fitting Given predicted sequence of 3D joint locations {Xt,i} and kinematic tree with rest-pose offsets oi and parent indices p(i), our goal is to recover temporally stable joint rotations Rt,i SO(3) such that the forward kinematics (FK) matches the observed joints: Pt,i = (cid:40) 0, Pt,p(i) + Rt,p(i) oi, p(i) = 1, otherwise. Because FK is not injective, position-only constraints do not fully determine local orientation, especially twist around the bone axis. We therefore combine geometric initialization, temporal warm-starting, and differentiable refinement with twist suppression. Geometric Initialization. For each frame, we compute closed-form IK estimate Rgeo t,i . For single-child joints, we align rest-pose and observed bone vectors via axisangle rotation. For multi-child joints, we solve the orthogonal Procrustes problem: Rgeo t,i = arg min RSO(3) (cid:88) (cid:13) (cid:13)R vrest i,k vobs t,i,k (cid:13) 2 (cid:13) , where vrest are rest-space bone directions and vobs are normalized directions from predicted joints. This provides 1 consistent orientations at branching structures (e.g., pelvis, shoulders). Temporal Warm-Starting. To avoid frame-to-frame drift, optimization for frame is initialized using the solution from the previous frame: θ(0) = θ t1. Differentiable Refinement. Local rotations are parameterized as axisangle vectors θt,i R3 and refined via the loss: Lt = Lpos + λpriorLprior + λtwistLtwist. The FK position loss is: Lpos = 1 (cid:88) (cid:13) (cid:13)Pt,i(θt) Xt,i (cid:13) 2 (cid:13) . geometric prior encourages solutions close to the closed-form initialization: Lprior = 1 (cid:88) θt,i θgeo t,i 2. Twist Suppression. Since bone-axis twist is underconstrained, we penalize rotation components parallel to the bone direction ui = oi/oi. Let θt,i = αt,iˆat,i. The twist magnitude is: We minimize: αtwist t,i = αt,i (ˆat,i ui). Ltwist = 1 (cid:88) (cid:0)αtwist t,i (cid:1)2 . This term suppresses candy-wrapper artifacts while pre-"
        },
        {
            "title": "Notation",
            "content": "Let Skeleton and Skeleton be defined as: Joint positions: XA = {xA XB = {xB R3 = 1, . . . , }, R3 = 1, . . . , }. where is the number of joints. Kinematic hierarchy, defined by parent array: pA, pB {1, 1, . . . , }N , = 1 (or pB = 1) indicates root joint. where pA Although the parent arrays may differ, the metric assumes known correspondence of joint indices between the two skeletons."
        },
        {
            "title": "Distance From Joint to the Other Skeleton",
            "content": "For each joint of Skeleton A, we compute its distance to the closest point on the bone segments of Skeleton B. Skeleton consists of line segments defined by its kinematic tree: = {(xB , xB pB ) pB = 1}. For joint xA , its distance to Skeleton is defined as: d(xA , B) = min (b1,b2)SB (cid:13) (cid:13)xA Πb1,b2(xA )(cid:13) (cid:13) , where Πb1,b2(v) denotes the orthogonal projection of point onto the line segment connecting b1 and b2. This projection is computed as: Πb1,b2 (v) = b1+clip (cid:18) (v b1) (b2 b1) b2 b12 (cid:19) , 0, (b2b1), where clip(t, 0, 1) = max(0, min(t, 1)) ensures the projected point lies on the segment. Similarly, we can compute the distance from joints of Skeleton to Skeleton A. Skeleton-to-Skeleton Distance serving natural motion around long chains such as tails. The asymmetric distance from Skeleton to Skeleton is: Summary. The combination of geometric IK, temporal warm-starting, and twist-regularized refinement yields stable and anatomically consistent joint rotations, significantly improving reconstruction quality. Further implementation details are provided in the code release. 8. Evaluation Metrics D(A B) = 1 (cid:88) i=1 d(xA , B). The symmetric distance is defined as: Dsym(A, B) ="
        },
        {
            "title": "Interpretation",
            "content": "(D(A B) + D(B A)) . This section describes the computation of the proposed metric(CD-Skeleton) that evaluates the alignment between two articulated skeletons. Each skeleton is represented by set of 3D joint positions and kinematic hierarchy defined by parent array. This metric evaluates how closely each joint of one skeleton lies to the structure of the other skeleton, capturing differences in global pose, limb orientation, and proportions. The symmetric version provides balanced measure when neither skeleton should be considered the reference."
        }
    ],
    "affiliations": [
        "Huawei Central Media Technology Institute",
        "Huawei Technologies Co., Ltd."
    ]
}