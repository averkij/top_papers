{
    "paper_title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models",
    "authors": [
        "Zhaopan Xu",
        "Pengfei Zhou",
        "Weidong Tang",
        "Jiaxin Ai",
        "Wangbo Zhao",
        "Xiaojiang Peng",
        "Kai Wang",
        "Yang You",
        "Wenqi Shao",
        "Hongxun Yao",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 4 5 2 1 . 3 0 5 2 : r PEBench: Fictitious Dataset to Benchmark Machine Unlearning for"
        },
        {
            "title": "Multimodal Large Language Models",
            "content": "Zhaopan Xu1,2, Pengfei Zhou4, Weidong Tang6, Jiaxin Ai3, Wangbo Zhao4, Xiaojiang Peng5, Kai Wang4, Yang You4, Wenqi Shao2,3, Hongxun Yao1, Kaipeng Zhang2,3 1HIT, 2Shanghai AI Laboratory, 3Shanghai Innovation Institude, 4NUS, 5SZTU, 6XDU h.yao@hit.edu.cn, zhangkaipeng@pjlab.org.cn, https://pebench.github.io"
        },
        {
            "title": "Abstract",
            "content": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce benchmark, named PEBench, which includes dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs. 1. Introduction With the rapid development and widespread application of large language models (LLMs), ethical and safety concerns have drawn much attention, particularly due to the large volumes of data scraped from the internet during the training [2, 17]. To address these ethical and safety concerns, Machine Unlearning (MU) has been proposed as remedy. MU aims to remove the influence of unwanted data and its effects on model capabilities from pretrained models without requiring retraining from scratch. Recent advancements Figure 1. Example of an image of Joe Biden speaking at the White House. Before unlearning (a) MLLMs have the ability to generate responses related to various visual concepts (Identify and Event). The goal of Machine Unlearning (MU) for MLLMs is to selectively forget specific concepts within the model. When the unlearning target is Identity (b), the model mistakenly identifies Joe Biden as different person. When the unlearning target is Event (c), the model misinterprets the speech as concert. in MU for LLMs have demonstrated its potential as an effective approach to tackle this issue of LLMs [25, 31]. Recently, building upon LLMs, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in multimodal applications. Similar to LLMs, they face variety of privacy and security issues. In particular, beyond text, multimodal data brings new challenges for MU. However, comprehensive benchmarks and datasets for evaluating MU for MLLMs remain limited. are"
        },
        {
            "title": "There",
            "content": "for MLLMs: two MU benchmarks MMUBench [21] and CLEAR [8]. MMUBench is tailored to the unlearning of real-world entities (Fig. 2a), while CLEAR focuses on fictional personal entities (Fig. Figure 2. Comparison between previous MU benchmarks for MLLMs and our PEBench. 2b). However, these benchmarks are limited to discrete entities. In contrast, visual concepts within images are broader (as shown in Fig. 1a), encompassing both concrete entities like individuals and general contexts, such as event scenes. Current benchmarks not only overlook the diversity of unlearning targets, but more importantly, they ignore the coupling phenomenon of these commonly present concepts in images. This raises scientific question: when forgetting an entity, does it affect other concepts within the same image (such as event scenes)? This could lead to insufficient evaluation of multimodal unlearning. To develop more comprehensive benchmark, we categorize multimodal unlearning targets into two types based on the scope of visual concepts: Identity (Fig. 1b) and Event (Fig. 1c). Both of these unlearning targets have significant practical applications. The Identity category may address unlearning related to personal privacy and intellectual property, while the Event category may related to harmful or illegal content, such as fake news scenes. In light of this, we introduce new benchmark, PEBench, designed to evaluate the unlearning of Personal entities and Events. The evaluation of MLLMs unlearning requires assessing both the generality [39] and scope [7, 11] of the unlearning target visual concept. Specifically, generality in unlearning not only involves forgetting the specific visual concept in training samples but also generalizing to similar samples that share common characteristics [28]. Each unlearning target requires the collection of multiple images with consistent visual concepts. The scope of unlearning means forgetting the target while simultaneously not affecting causally unrelated visual concepts. For example, unlearning person-specific targets without affecting scene recognition (and vice versa). To perform this test, the two visual concepts should be coupled in the dataset. PEBench is synthetic to ensure both consistency and coupling requirements. As shown in Fig 2c, the dataset consists of 200 fictitious individuals, each linked to images from 40 distinct event scenes, resulting in total of 8,000 images. In addition to ensuring consistency and coupling, we chose to use synthetic data for dataset construction because, with synthetic data absent from MLLM training, PEBench enables the creation of an ideal unlearned model (upper bound) trained solely on this data. We benchmark six MU methods, providing new insights into their strengths and weaknesses in unlearning personal entities and events, as well as the underlying mechanisms. Moreover, we show that these methods struggle to unlearn both personal entities and events simultaneously. We believe our benchmark will significantly contribute to the future development of MU for MLLMs. Key contributions are summarized as follows: We propose PEBench, new benchmark framework specifically designed to assess machine unlearning (MU) performance in Multimodal Large Language Models (MLLMs). PEBench provides comprehensive dataset of fictitious personal entities and event scenes, addressing gaps in current MU evaluation and overcoming the limitations of existing benchmarks. PEBench enables the evaluation of both private information removal and the unlearning of general concepts through consistent imagery across diverse entities and scenes. By incorporating this fictitious data into the training paradigm, PEBench simulates the retraining process and the selective removal of harmful data, ensuring that the evaluation closely mirrors real-world scenarios. We benchmark six different MU methods, revealing their strengths and weaknesses and offering insights into the underlying unlearning mechanisms. This contributes valuable guidance for future improvements in MU methods and enhances the security of multimodal models. 2. Related Work 2.1. Machine Unlearning Motivated by privacy concerns [12], machine unlearning (MU) was introduced in [3] to remove specific data from machine learning models. While traditional models focused on classification, generative large language models (LLMs) like ChatGPT and GPT-4 bring new challenges Figure 3. Overview of the PEBench framework, illustrating the data curation and evaluation processes. due to their scale and complexity, raising privacy concerns from retained knowledge. Approaches for MU in these models include gradient ascent (GA)[38], which adjusts model parameters to induce mis-predictions on forget set samples[14], though it can cause catastrophic failures [27]. To mitigate this, KL-divergence constraints [37] and framing unlearning as minimization problem with negative examples [41] have been proposed. For multimodal models, [21] introduced token-level KL-divergence loss for MU in MLLMs, marking an initial attempt to apply MU in multimodal contexts. Nevertheless, systematic evaluation of these methods on multimodal models remains limited. To address this gap, we introduce PEBench, benchmark for assessing capabilities in multimodal settings. 2.2. Unlearning Benchmarks Standardized benchmarks are essential to evaluate the effectiveness of unlearning methods for large language models (LLMs). Existing datasets have been used to test unlearning in various contexts, such as harmful content removal [29], elimination of personal identification data [15, 31], and copyright protection [9]. While these datasets provide useful insights, they fall short of capturing the multimodal data interactions present in MLLMs. Benchmarking for MU in MLLMs is still in its early stages, with limited resources specifically tailored to evaluate privacy and security across both textual and visual domains. 3. PEBench 3.1. Data Curation To address the need to benchmark diverse concepts for MU in MLLMs, and avoid data leakage by MLLMs pertaining, we propose fictitious benchmark, PEBench. Our objective is to construct fully synthetic multimodal dataset that includes both specific personal entities and general event scenes, enabling comprehensive evaluation of MU in MLLMs. The dataset construction involves two main steps: generating text descriptions for person-event pairs and producing corresponding images, as illustrated in Fig 3. Text Descriptions Generation. We generate person-event descriptions using GPT-4 [1] to ensure diversity among individuals. First, we manually define attributes such as profession, age, gender, and birthplace. These attributes are then used as prompts for GPT-4 to produce detailed textual descriptions. Specifically, professions are categorized into 15 primary fields with subcategories, resulting in total of 75 specific occupations based on Wikipedia references. Birthplaces are divided into 7 continents and further into 55 regions. Each profession is paired randomly with birthplace, along with randomly assigned age and gender. These attributes serve as seeds to generate unique names Figure 4. (a) Age distribution by gender shows balanced representation across age groups and between male and female individuals. (b) Area distribution demonstrates geographic diversity, with individuals from various continents, ensuring globally representative dataset. (c) Job distribution displays wide range of professions, covering multiple fields such as healthcare, education, arts, business, and technology, highlighting the datasets comprehensive coverage of occupational diversity. Fig. 5. To overcome these limitations, we propose twostep strategy. First, we guide the generation process by using fixed seed and carefully crafted prompts to generate realistic and stylistically coherent images using Flux1. Finally, we generate virtual dataset with diverse characters and consistent styles across multiple scenes and individuals, as shown in Fig. 2 c. 3.2. Quality Control To ensure high-quality dataset, we implement series of steps to filter out unqualified data and maintain data integrity. The specific procedures are outlined below: Diversity and Balance. We use image generation models to construct the dataset, requiring diverse and comprehensive prompt suite. The distribution of our prompt suite is visualized in Fig. 4(d). Diversity. The dataset covers broad range of demographic areas and job categories. Fig.4(b) shows balanced geographic distribution, while Fig.4(c) illustrates diverse job distribution, including professions such as students, professors, healthcare workers, and those in technology and engineering. This diversity enhances the models generalizability across various demographics and professional backgrounds. Balance. The Age Pyramid by Gender chart shows balanced age and gender distribution, minimizing biases. This ensures fair representation and supports robust model training, as seen in Fig. 4(a). Through controlling for diversity and balance, we have constructed comprehensive appearance characteristics for each individual. While some attributes may overlap, such as in Fig. 6(b), individuals of the same gender, similar age, and region may have different professional attributes, resulting in distinct clothing and appearance features. Identity Consistency. We use facial recognition model (FaceNet)[35] to verify each characters appearance across 1https://github.com/black-forest-labs/flux Figure 5. Comparison of image generation results between our method and PhotoMaker. The images generated by our method demonstrate consistency in both character appearance and scene setting across different contexts. and appearance descriptions for each fictional character. For events, we prompt GPT-4 to generate descriptions for 40 detailed and thematically rich scenes, which are combined with each of the 200 individuals. To effectively evaluate event unlearning, we ensure the scenes contain specific themes and contextual details, making them more informative than simple image captions. This setup provides broad range of unique person-event combinations. For detailed generation, please refer to the Supplementary File. Images Generation. The next step is to generate corresponding images for the text descriptions from the previous stage. The challenge here is to maintain consistency in the appearance of the same person across different event scenes, while also ensuring that the style of the scenes remains consistent for different individuals. Simply using text-to-image (T2I) model like Stable Diffusion [34] often fails to achieve such consistency in both the characters and the scene styles across multiple images. To address this, techniques like IPAdapter [40] and Photomaker [23] are commonly used to maintain consistency in generated images. However, these methods often struggle with generating recognizable character appearances, and the resulting images typically lack realism, as shown in Figure 6. (a) The facial recognition model effectively checks identity consistency, even for samples that may be difficult for humans to distinguish. (b) persons professional attributes influence the appearance characteristics of the virtual character. different scenes. If discrepancies are found, images are regenerated to maintain uniformity (as shown in Fig.6(a)). Imaging Quality. Inspired by Image Quality Assessment (IQA) and Video Quality Assessment [13], we control the quality of generated images across four dimensions to filter out low-quality images. brief overview of each dimension is provided below. Subject Consistency. We evaluate the consistency of the subject described in the temporal context by assessing its appearance in the generated image, using the DINO [4] model to check subject consistency between the image and event description. Background Consistency. The consistency of background scenes is evaluated by calculating CLIP [32] feature similarity across set of images from one event. Distortion Detection. We assess image distortion (e.g., over-exposure, noise, blur) using the MUSIQ [16] image quality predictor trained on the SPAQ [10] dataset. Aesthetic Quality. We evaluate the aesthetic value of each image using the LAION aesthetic predictor [19], which reflects aspects like composition, color harmony, photo-realism, and overall artistic quality. Human Verification To ensure the generated images align with the data statistics shown in Figure 6 and maintain high quality, the dataset was manually evaluated by two authors of this paper and two volunteers with undergraduate degrees. The verification process consists of two aspects: appearance consistency and image quality. Appearance Consistency. We manually checked whether the appearance of all 200 individuals matched the descriptions in the prompts. If all four reviewers confirmed consistency, the image was considered valid; otherwise, we adjusted the random seed to better align with the appearance features. These features include region, age, profession, and other physical traits. Image Quality Check. This included checking images of event scenes. We reviewed 10% of the dataset and confirmed that 98% of the images filtered by humans were also detected by the model. 3.3. Evaluation Pipeline Retraining the model from scratch after removing the forgotten data is the gold standard for unlearning [27]. However, due to scalability challenges with MLLMs, establishing performance upper bound is difficult. To address this, we simulate retraining on virtual data by splitting the dataset into forget set Df and retain set Dr, and train two models: the goal model and the finetuned model. Goal Model is fine-tuned on the retain set Dr, representing the ideal scenario where dangerous data has been removed. Since our dataset consists of synthetic images with virtual person names and event descriptions, it can be considered as data not seen during MLLMs training. This model serves as the upper bound for evaluating unlearning effectiveness. Finetuned Model is trained on the entire dataset D, including the forget set, and then subjected to an unlearning algorithm to remove the forget sets influence. This model serves as the baseline without unlearning. The effectiveness of unlearning is assessed by comparing the performance of the finetuned model with the goal model, where the goal model provides the upper bound for unlearning performance (as shown in Fig 3). 3.3.1. Datasets Splits Forget Set. PEBench includes two unlearning targets: person and events, which we evaluate separately. For person, we select subset (default set to 20) and treat all images of these individuals as the forget set. The goal is for the model to lose the ability to recognize the names of these individuals in the images. We further split this forget set into two parts: Df test for evaluating the generalization performance of the unlearning algorithm. train for training and Df train and Df test for training and evaluation. For events, we select set of events to forget (default set to 5), and the corresponding images are included in the forget set. The objective is for the model to no longer accurately describe the specific event scenes depicted in these images. Similar to individuals, we divide the event forget set into Df Retain Set. The portion of the dataset remaining after removing the forget set is designated as the Retain Set, Dr = Df . The Retain Set is used to test the unlearning models ability to preserve performance on the data that is not part of the forget set. After unlearning the forget set, the model should still perform well on the retain set. Real Set. The Real Set consists of images collected from the real world that are semantically similar to those in the Retain and Forget Sets. Similar to the Retain Set, it is used to test the accuracy of the unlearning process. In our setup, the Real Set is collected from the MIKE dataset [20], which ensures that each concept is visually distinctive. World Set test the models performance on general world knowledge, ensuring that the unlearning process does not degrade the models ability to retain important factual knowledge. We use the POPE benchmark [22] to assess the models general utility. 3.3.2. Evaluation Metrics People Unlearning Metrics measure the effectiveness of the unlearning targeted individuals while ensuring that models performance on other individuals and tasks is not negatively impacted. The following metrics are used: Efficacy. Measures the success rate of forgetting individuals in the forget set of training. The goal is for the model to no longer recognize the individuals names. Specifically, efficacy evaluates the success rate on Df Generality. Assesses the success rate on Df train. test, which contains the same individuals but did not participate in the MU training. Retain. Evaluates the models accuracy in recognizing data from the retain set Dr, ensuring that the unlearning process does not negatively affect retained knowledge. Scope. Tests whether the process of forgetting people affects the recognition of event scenes. We calculate the ROUGE-L [5] score on Df train between the models outputs and the ground truth event scene descriptions. Real. Evaluates the models accuracy in successfully recognizing individuals, such as Harry Potter or famous singers, from the MIKE dataset [20]. World Fact. Measures the impact of unlearning on nontargeted knowledge. We employ the POPE benchmark to assess this. Event Unlearning Metrics. The goal of event unlearning is to forget specific themes from the ground truth while obtaining model that is indistinguishable from one trained solely on the retain set. The goal model, which is trained only on the retain set, can be regarded as the ideal unlearning model. We aim to evaluate the similarity between the unlearned model and the goal model, while also considering the forgetting of the ground truth. Measuring the quality of event unlearning is challenging [18, 36]. common approach is to compute the conditional probability for question and answer [6]. However, this method suffers from answer length bias and requires answers with fixed structure. Our test scenarios involve rich, detailed descriptions that are complex and may not be accurately evaluated by conditional probability. In this work, we propose the GPT-4 Evaluation (G-Eval) to assess event forgetting. Specifically, we prompt GPT-4 to evaluate the similarity between the unlearned models output, the ground truth, and the goal model. score of 0 indicates the output is identical to the ground truth, while score of 1 indicates it matches the goal model. The score ranges between 0 and 1, with higher scores reflecting better unlearning performance. Similar to People Unlearning, we calculate the metrics: Efficacy, Generality, and Retain. G-Eval evaluates the similarity between the unlearned models output, the goal model, and the ground truth on Df train, Df test, and Dr. Scope. Tests whether the forgetting of events affects the recognition of people within the same imag. We calculate the accuracy of person recognition on Df train. Real. Evaluates the models accuracy in captioning scene images from the MIKE dataset [20]. We calculate the ROUGE-L score between the goal model and the unlearned models output. World Fact. We employ the POPE benchmark to assess this in the same way as person unlearning. 4. Experiment 4.1. Experiment setup Training. We use LLAVA (7B) [26] as the unlearned model. The optimizer is Adam with learning rate of 1e-5. Lora is employed to fine-tune LLAVA with batch size of 16. The training is set for 1 epoch. We utilize two A100 40GB GPUs for model training. Baselines. In this study, we evaluate the performance of six recently proposed MU methods to provide comprehensive comparison of different approaches to unlearning. We assess the 6 recently proposed MU methods, including Preference Optimization (PO) [30], Gradient Ascent (GA) [38], Gradient Difference (GD) [24], KL [37], SIU [21] and DPO [33]. For each of these methods, we follow the original implementation details as closely as possible to ensure an accurate evaluation. 4.2. Experiment Results Tab 1 summarizes the performance of various unlearning methods on PEBench, using the metrics defined in Sec 3.3.2. Notably, separate training and testing were conducted for each unlearning target (people and events) to ensure accurate evaluation. Key findings are outlined below. The Importance of Including Both Unlearning Targets in Evaluation. Our results highlight the necessity of evaluating both people and event unlearning targets for comprehensive understanding of machine unlearning (MU). While most methods achieve nearly 100% efficacy for people unlearning, event unlearning efficacy varies significantly among methods, emphasizing the importance of including event unlearning metrics in the evaluation framework. The Importance of Scope as Key Metric. The scope of unlearning is critical factor [28]. For people as the unlearning target, ROUGE-L drops from 0.99 to an average of 0.88. For events, the accuracy of people recognition decreases from 100% to an average of 84.5%. This demonstrates that unlearning algorithms impact other visual concepts within the same image, an aspect previously overlooked in benchmarks. With the coupled visual concepts in PEBench, this metric can now be assessed effectively. Retention Challenges. While people unlearning achieves Method Efficacy Generality Precision Precision Person Unlearning Retain Precision ROUGE-L Scope Real World Fact Efficacy Generality Retain G-Eval G-Eval G-Eval POPE Precision Scope Precision ROUGE-L Real World Fact POPE Events Unlearning Finetune (Base) PO [30] GA [38] GD [24] KL [37] SIU [21] DPO [33] Goal (Upper Bound) 0.0 100.00 100.00 98.89 100.00 100.00 100.00 100.00 2.24 100.00 100.00 98.89 99.70 100.00 100.00 100.00 97.53 4.12 3.89 21.48 5.00 10.36 8.64 96.38 0.98 0.89 0.91 0.86 0.81 0.90 0.92 0.99 100.0 86.64 71.64 76.87 73.88 80.43 82.63 100. 85.88 78.52 78.01 77.08 78.73 79.02 78.38 87.52 0.18 0.21 0.51 0.58 0.55 0.48 0.43 0.97 0.20 0.22 0.49 0.56 0.51 0.46 0.41 0.98 0.99 0.98 0.62 0.88 0.84 0.74 0.80 0.99 100.00 98.86 78.50 81.50 80.75 84.50 83.10 100.00 0.56 0.44 0.24 0.30 0.25 0.48 0.35 0. 85.88 77.23 78.82 79.07 78.75 80.07 79.28 87.52 Table 1. Performance overview of different MU methods evaluated on PEBench. The performance metrics include Efficacy, Generality, Retain, Real, and World Fact. higher score represents better performance. Finetune represents the baseline performance (lower bound for unlearning), and Goal represents the ideal unlearning model (upper bound). Figure 7. Left: Heatmap visualization of the unlearning and retain accuracy of MU methods on PEBench. The x-axis shows the tested targets (People and Events) for the unlearned model, while the y-axis indicates the unlearning target. The figure is separated into different regions representing corresponding evaluation metrics and unlearning scopes (A for unlearning effectiveness, for retainability, for desirable retain cases; 1 for People unlearning, 2 for Event unlearning). Diagonal regions (A1 and A2) indicate unlearning efficacy accuracy, while off-diagonal values (B1, B2, C1, and C2) represent retain accuracy. Higher values in lighter colors denote better performance. Right: Representative cases illustrating each region with text descriptions for sample prompts. Desirable cases () show successful unlearning or retention as intended, while undesirable cases () indicate failure to unlearn or unintended retention. high efficacy and generality, it hampers the models ability to recognize individuals in the retain set. Notably, the GD method performs better in retain set recognition, highlighting that balancing loss for both forget and retain sets is effective. In contrast, GAs approach to forgetting the forget set significantly degrades retain set performance. Gradient Difference Benefits for Events Unlearning. For events unlearning, methods based on gradient difference minimization (GD and KL) outperform others across various metrics. Specifically, GD builds on GA by adding an objective to preserve performance on the retain set. GD and KL maintain excellent unlearning performance while improving retain set performance by 0.07 and 0.03, respectively, demonstrating that incorporating retain set loss helps preserve the models understanding of event scenes. 4.3. Visualization In Fig. 7 (left), we present heatmap (based on the method of GD [24]) that illustrates the person/event unlearning accuracy (diagonal) and retain accuracy (off-diagonal) for MLLMs. The sections of the heatmap are labeled to indicate the connections and distinctions between individuals and events during the unlearning process. From this visualization, several key insights can be observed: Distinct behaviors in unlearning people compared to events. The darker diagonal color for individuals (e.g. A1) Method Efficacy Generality Retain Real World Fact Efficacy Generality Retain Real World Fact Person Unlearning Events Unlearning Finetune (Base) GD [24] GD+BGD KL [37] KL+BGD Goal (Upper Bound) 0.0 55.00 63.50+8.5 36.36 48.10+11.7 100.00 2.24 55.00 62.58+7.6 36.36 48.10+11.7 100.00 97.53 39.72 28.3211.4 22.41 18.673.7 96.38 100.0 95.80 88.657.2 58.88 55.343.5 100.00 85.88 77.08 78.56+1.5 70.23 68.621.6 87.52 0.18 0.36 0.47+0.1 0.34 0.42+0.1 0. 0.20 0.34 0.50+0.2 0.32 0.41+0.1 0.98 0.99 0.88 0.730.2 0.82 0.760.1 0.99 0.56 0.37 0.45+0.1 0.40 0.42+0.02 0.55 85.88 77.08 78.36+1.3 66.54 67.04+0.5 87.52 Table 2. Performance overview of simultaneously unlearn people and events. + (or ) indicates the performance gain (or decrease) compared to the base method. compared to events (e.g., A2) indicates that unlearning individuals achieve higher efficacy. However, the noticeable gap between the off-diagonal values (B1 and B2) shows significantly worse retention performance for individuals, suggesting that while unlearning individuals effectively achieve the forgetting goal, the unlearning scope is poorly controlled, leading to substantial negative impacts on the retain set. In contrast, unlearning events demonstrate opposite characteristics: achieving efficacy is more challenging, but the impact on the retain set is minimal. These observations highlight the need to treat people and event unlearning targets separately when designing and evaluating machine unlearning strategies for MLLMs. Retaining the generation performance of unlearned targets, is evident from the ROIs (C1/C2), both regions show strong retain performance. This means that unlearning specific targets does not easily affect another domain of retain set, suggesting separation of unlearning scopes between people and events. 4.4. Simultaneously Unlearning Both Targets We conducted experiments to simultaneously unlearn both people and event concepts. During training, the model was guided to provide incorrect answers regarding individuals names and event scenes. This approach ensured the model learned to forget both concepts. During testing, we evaluated the unlearning performance for each target separately to assess the models efficacy. As shown in Tab. 2, simultaneously unlearning both targets resulted in decline in performance for both objectives, indicating that forgetting both concepts within images is challenging. For people, unlearning efficacy dropped significantly (e.g., in the KL method, from 100 to 36.36), while retention performance improved, showing trade-off between efficacy and retention. Similarly, event unlearning performance also declined, highlighting the difficulty of unlearning multiple concepts together. We found that when the distributions or information of the two unlearning targets conflict significantly, the model may experience collapse or notable performance drop. Additionally, the imbalance in data volumeone event corresponds to all 200 individualsleads to coupling effect between visual concepts, which hinders people unlearning, as discussed in Sec 4.2 with the scope metric. To address this imbalance and the scope issue, we propose Balance Gradient Difference (BGD), which includes data balancing and multi-task balancing. Specifically, data balancing adjusts the sampling ratio dynamically, reducing the event data early in training and gradually increasing it, while ensuring balance through random sampling. Multi-task balancing separate loss functions are defined for each targets: LBD = α L(Df name, w) β L(Df event, w) + γ L(Dr, w), (1) where α, β, and γ are weights, which we set by default to 0.3, 0.2, and 0.5 based on task difficulty. As shown in Tab. 2, after applying BGD, unlearning performance significantly improved, particularly for person unlearning, demonstrating the effectiveness of our method. This scenario is highly relevant in real-world applications, where images often contain multiple intertwined concepts. For example, unlearning fake news images involves jointly forgetting both people and event scenes. 5. Conclusion In this work, we introduced PEBench, comprehensive benchmark designed to evaluate machine unlearning (MU) methods in multimodal large language models (MLLMs), focusing on both personal entities and event scenes. Our experiments highlight the importance of including diverse unlearning targets, as assessing both people and event unlearning metrics provides more thorough evaluation of MU performance. While all methods exhibit strong efficacy in unlearning specific individuals, significant variability is observed in event unlearning performance, underscoring the need for targeted approaches that can handle different types of information. This study demonstrates the value of PEBench as standardized framework for assessing MU in MLLMs, setting foundation for more effective and nuanced unlearning techniques."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610623, 2021. 1 [3] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pages 463480. IEEE, 2015. 2 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 5 [5] Lin Chin-Yew. Rouge: package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out, 2004, 2004. 6 [6] Kyunghyun Cho, Bart Van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259. [7] Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 12:283298, 2024. 2 [8] Alexey Dontsov, Dmitrii Korzh, Alexey Zhavoronkin, Boris Mikheev, Denis Bobkov, Aibek Alanov, Oleg Rogov, Ivan Oseledets, and Elena Tutubalina. Clear: Character unlearning in textual and visual modalities. arXiv preprint arXiv:2410.18057. 1 [9] Ronen Eldan and Mark Russinovich. Whos harry potarXiv preprint approximate unlearning in llms. ter? arXiv:2310.02238, 2023. 3 [10] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone phoIn Proceedings of the IEEE/CVF conference on tography. computer vision and pattern recognition, pages 36773686, 2020. [11] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint arXiv:2111.13654. 2 [12] Chris Jay Hoofnagle, Bart van der Sloot, and Frederik Zuiderveen Borgesius. The european union general data protection regulation: what it is and what it means. Information & Communications Technology Law, 28(1):6598, 2019. 2 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 5 [14] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504, 2022. 3 [15] Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, and Jun Zhao. Rwku: Benchmarking real-world knowledge unlearning for large language models. arXiv preprint arXiv:2406.10890, 2024. [16] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 5 [17] Hadas Kotek, Rikker Dockum, and David Sun. Gender bias In Proceedings and stereotypes in large language models. of the ACM collective intelligence conference, pages 1224, 2023. 1 [18] Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. The brainy student: Scalable unlearning by selectively disobeying the teacher. 2023. 6 [19] LAION-AI. Aesthetic predictor. https://github. com/LAION-AI/aesthetic-predictor, 2022. Accessed: 2022-04-16. 5 [20] Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, and Bozhong Tian. Mike: new benchmark for fine-grained multimodal entity knowledge editing. arXiv preprint arXiv:2402.14835, 2024. 5, 6 [21] Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, and Sheng Bi. Single image unlearning: Efficient machine unlearning in multimodal large language models. arXiv preprint arXiv:2405.12523, 2024. 1, 3, 6, 7, [22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 6 [23] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 4 [24] Bo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In Conference on Lifelong Learning Agents, pages 243254. PMLR, 2022. 6, 7, 8, 11 [25] Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang Liu. Large language model unlearning via embeddingcorrupted prompts. arXiv preprint arXiv:2406.07933, 2024. 1 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. [27] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang [41] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868, 2024. 3 Yao, Hang Li, Kush Varshney, et al. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787, 2024. 3, 5 [28] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, et al. Rethinking machine unlearning for large language models. Nature Machine Intelligence, pages 114, 2025. 2, 6 [29] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems, 35: 2759127609, 2022. 3 [30] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary Lipton, and Zico Kolter. Tofu: task of fictitious unlearning for llms. arXiv preprint arXiv:2401.06121, 2024. 6, 7, [31] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks. arXiv preprint arXiv:2309.17410, 2023. 1, 3 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 6, 7, 11 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 [35] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. 4, 12 [36] Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the necessity of auditable algorithmic definitions for machine unlearning. In 31st USENIX security symposium (USENIX Security 22), pages 40074022, 2022. 6 [37] Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. Machine unlearning of pre-trained large language models. arXiv preprint arXiv:2402.15159, 2024. 3, 6, 7, 8, [38] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. arXiv preprint arXiv:2310.10683, 2023. 3, 6, 7, 11 [39] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. Advances in Neural Information Processing Systems, 37:105425105475, 2025. 2 [40] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is organized as follows: Sec. presents the details of unlearning approaches considered in this work. Sec. presents the details of data curation. Sec. presents the details of G-Eval. Sec. presents the case of machine unlearning. A. Unlearning Methods Gradient Ascent (GA) [38]: This method updates the model parameters by maximizing the likelihood of misprediction for the samples within the forget set Df . Given sample Df , the loss can be denoted by: L(Df , w) = 1 Df (cid:88) xDf ℓ(x, w). (2) Preference Optimization (PO) [30]: This approach guides the model to align with newly generated responses such as do not know the answer and its variants for questions related to the forget set Df . Simultaneously, it incorporates retain set term to ensure the models predictions for the retained set remain unaffected. The total objective function is defined as follows: Lidk = L(Dr, w) + L(Df idk, w). (3) Gradient Difference (GD) [24], This method extends gradient ascent by simultaneously focusing on forgetting the samples in the forget set Df and preserving performance on the retain set Dr. The objective is to balance increasing the loss for the forget set and minimizing the impact on the retained set. The resulting loss function to be minimized is expressed as: Ldiff = L(Df , w) + L(Dr, w). (4) KL [37]: This method extends Gradient Ascent by incorporating an additional objective to minimize the KullbackLeibler (KL) divergence between the predictions of the original model Mori and the newly trained model Mnew on the retain set Dr. The KL divergence loss is defined as: LKL = 1 Dr (cid:88) sDr 1 s (cid:88) i= KL (cid:0)Mori(s<i)(cid:13) (cid:13)Mnew(s<i)(cid:1) . SIU [21], This method is specifically designed for unlearning person recognition. For individuals in the forget set, it replaces the matched names in image-text pairs with randomly assigned names. The loss function combines crossentropy (CE) loss and KL divergence loss, where the KL loss applies mask to the person names to preserve the models original capabilities. The loss function for people unlearning is defined as: Lpeople = L(Df , w) + Km KL(Df , w). (7) where Df and, Km is the mask applied to the person names. represents the reconstructed mismatched text, For event unlearning, as there is no corresponding mask, the loss function is simplified to focus solely on the forget set. The loss function is defined as: Levent = L(Df , w), (8) where Df represents the reconstructed mismatched text for events in the forget set. Direct Preference Optimization (DPO) [33]: DPO directly optimizes language models to align with human preferences without the need for explicit reward modeling or reinforcement learning. For unlearning, this approach is framed as preference optimization problem, where the preference is shifted towards outputs that relabel or neutralize unwanted data. This ensures the model effectively forgets targeted information while aligning with desired outputs. The loss function is defined as: (cid:104) LDP O(πθ, πref ) = Ex,yDf yDf idk (cid:17)(cid:105) , β log πθ(yx) πref (yx) πθ(yx) πref (yx) (9) (cid:16) β log log σ where LDP is the preference optimization loss, πθ and πref represent the unlearning target model and the reference model trained on Df idk, respectively. σ is the logistic function, and β is the DPO scaling coefficient."
        },
        {
            "title": "The total objective function combines task performance",
            "content": "and unlearning effectiveness and is defined as: = λ1L(Df idk, θ) + λ2LDP O(πθ, πref ), (10) where λ1 and λ2 are weighting values that balance task performance and the unlearning process. (5) B. Data Curation The overall objective function combines the Gradient Ascent loss on the forget set and the KL divergence loss, expressed as: Ltotal = L(Df , w) + LKL. (6) B.1. Text Descriptions Generation To create diverse character descriptions, we categorized occupations into 15 primary fields based on Wikipedia references, resulting in 75 specific professions listed in Tab 4. the generated characters across different images representing the same identity. Specifically, we set cosine similarity threshold of 0.6; images falling below this threshold are regenerated with different random seed. The prompts used as input to Flux are listed below, and examples of the generated images are shown in Fig.8."
        },
        {
            "title": "Flux Prompting Strategy for Images Generation",
            "content": "Images Generation Prompt: photo of Job Age-year-old Gender from Birthplace, named Name,with characters appearance.: events Generate this photo with the Person ID: Person ID and with file name IMG 6105.CR2. Table 3. Flux hyper-parameters. Num Outputs Num Inference Steps Guidance True Gs Width Height 1 40 2. 3.5 512 512 C. GPT-4 Evaluation (G-Eval) As discussed in the Event Unlearning Metrics of Section 3.3.2, G-Eval measures the performance of event forgetting. To evaluate this capability, we designed few-shot prompt, as illustrated in Figure 9. D. Case of Machine Unlearning We visualize the results of LLava using the Gradient Difference (GD) unlearning method. Figure 10 demonstrates the outcomes of people unlearning, while Figure 11 showcases the results of event unlearning. For birthplaces, we divided them into 7 continents and identified 55 representative regions, as shown in Tab 5. These attributes were then randomly paired and supplemented with randomly assigned age and gender, forming comprehensive set of background attributes. Using this information, we designed prompts for ChatGPT-4 to generate detailed character descriptions. In total, we generated descriptions for 200 unique fictional characters, ensuring diversity across professions, geographic origins, and demographic traits. An example of the prompting strategy is illustrated below: GPT-4 Prompting Strategy for Character Descriptions People Prompt: want to create fictional character with the following attributes: Job: {} Birthplace: {} Age: {} Gender: {} generate characters appearance and name: For generating event descriptions, we prompted GPTto create 40 detailed and thematically rich scenarios: GPT-4 Prompting Strategy for Event Descriptions Events Prompt: Please generate 40 event scenarios, each with distinct theme and detailed description of its features. The description of each event should include the following: Theme: The core content or purpose of the event. Features: Specific details of the event, such as participants, environment, atmosphere, location, etc., ensuring the description is vivid, concrete, and visually rich. In Tab 6, Tab 7 and Tab 8, we present the 40 generated event descriptions along with the corresponding keywords that encapsulate the core elements of each event scenario. B.2. Images Generation The image generation process is divided into two steps. First, we construct prompts based on the text descriptions and use Flux (flux-1-dev) 2 to generate images corresponding to these prompts. Second, we use an off-the-shelf facial recognition model, FaceNet [35], to ensure consistency of 2https://github.com/black-forest-labs/flux Task Name Science & Research Healthcare & Medicine Technology & Engineering Environmental & Agriculture Arts & Creative Fields Business & Finance Public Service & Community Support Education & Culture Media & Communications Architecture & Construction Law & Policy Retail & Services Sports & Fitness Logistics & Transportation Energy & Natural Resources Unemployed Students General Prompt Format Biologist, Physicist, Archaeologist, Ecologist Doctor, Nurse, Physical Therapist, Psychologist Software Developer, Electrical Engineer, Mechanical Engineer, Cybersecurity Specialist Environmental Scientist, Agronomist, Forester, Soil Scientist Painter, Musician, Writer, Graphic Designer Accountant, Market Analyst, Financial Advisor, Project Manager Police Officer, Firefighter, Social Worker, Nonprofit Coordinator Teacher, Trainer, Librarian, Museum Curator Journalist, Broadcaster, Content Creator, Public Relations Specialist Architect, Civil Engineer, Construction Worker, Surveyor Lawyer, Judge, Policy Analyst, Legislative Assistant Retail Manager, Customer Service Representative, Hotel Concierge, Sales Associate Athlete, Fitness Coach, Physical Trainer, Yoga Instructor Logistics Manager, Truck Driver, Pilot, Shipping Coordinator Petroleum Engineer, Geologist, Renewable Energy Consultant, Miner Job Seeker, Stay-at-home Parent, Retired, Freelancer, Entrepreneur, Consultant, Artist Primary School Student, Junior High Student, High School Junior, Undergraduate Student, Community College Student, Masters Student, Doctoral Student, Research Assistant, Apprentice, Technical School Student Table 4. The categorization of jobs across various domains, including science, healthcare, technology, arts, and public services. The second column provides specific examples of jobs within each category, offering comprehensive overview of the datasets occupational diversity. Region North America South America Europe Africa Asia Australia & Oceania Middle East Cities New York City, USA; Toronto, Canada; Mexico City, Mexico; Vancouver, Canada; San Juan, Puerto Rico Sao Paulo, Brazil; Buenos Aires, Argentina; Caracas, Venezuela; Quito, Ecuador; Lima, Peru Paris, France; Berlin, Germany; Stockholm, Sweden; Helsinki, Finland; Zurich, Switzerland; Lisbon, Portugal; Dublin, Ireland; Warsaw, Poland; Vienna, Austria; Reykjavik, Iceland; Bucharest, Romania Cairo, Egypt; Cape Town, South Africa; Lagos, Nigeria; Nairobi, Kenya; Accra, Ghana; Dakar, Senegal; Addis Ababa, Ethiopia; Casablanca, Morocco; Kigali, Rwanda Tokyo, Japan; Mumbai, India; Seoul, South Korea; Bangkok, Thailand; Istanbul, Turkey; Dubai, United Arab Emirates; Jakarta, Indonesia; Hanoi, Vietnam; Amman, Jordan; Doha, Qatar; Ulaanbaatar, Mongolia; Male, Maldives; Phnom Penh, Cambodia; Beijing, China; Shanghai, China Sydney, Australia; Wellington, New Zealand; Brisbane, Australia; Suva, Fiji; Port Moresby, Papua New Guinea Riyadh, Saudi Arabia; Tehran, Iran; Baghdad, Iraq; Beirut, Lebanon; Muscat, Oman Table 5. Cities categorized by their respective regions, highlighting diverse geographical. Event media interview park jogging farm visit dinner with friends landmark visit zoo visit shopping mall public lecture gym workout dance event coffee shop reading airport waiting concert attendance beach relaxing business meeting museum tour car driving grocery shopping Description Participating in an interview with local media outlet. The setting is well-lit studio or casual setup, depending on the persons profession. The conversation is captured by small crew with minimal background distractions. Exercising or relaxing in nearby park. The park is peaceful with trees and walking paths, serene backdrop for professionals, students, or retirees enjoying nature. Visiting local farm, surrounded by green fields and farm animals. The atmosphere is peaceful and natural, perfect for relaxing or learning about agriculture. Enjoying meal with friends or family at local restaurant. The restaurant has cozy, informal setting, suitable for unwinding after busy day. Visiting notable city landmark, adding cultural aspect to their day. The clear weather and bustling tourist atmosphere offer nice break from their routine. Exploring local zoo, observing animals in their habitats. The setting is educational and family-friendly, perfect for learning about wildlife. Walking through busy shopping mall, either to relax or purchase essentials. The mall is brightly lit, with various stores and other people enjoying bustling atmosphere. Attending or presenting lecture at university or community center. The atmosphere is formal, with people attentively listening, suitable for professionals, students, or anyone interested in continuous learning. Engaging in workout at local gym. The gym has spacious areas for various exercises and equipment, creating focused and energetic environment for fitness enthusiasts of all ages. Dancing or socializing at club or festive event. The atmosphere is vibrant, with colorful lights and music setting lively mood. Enjoying coffee break in cozy cafe. The ambiance is quiet and relaxed, perfect for reading, working on laptop, or chatting with friends. Waiting at an airport terminal for flight, surrounded by other travelers. The modern, glass-walled terminal offers views of the runway, creating calm and organized atmosphere. Attending live concert in an open-air or indoor venue. The crowd is lively, cheering and enjoying the music in spirited environment. Relaxing by the seaside, with gentle waves and clear sky. This peaceful setting is ideal for break from their routine, whether alone or with family. Participating in business or professional meeting in modern conference room. The background shows large windows with city view, creating productive atmosphere. Exploring museum filled with historical or artistic exhibits. The lighting is dim with spotlights on displays, creating reflective environment for visitors. Driving through scenic area, either in the city or countryside, during sunset. The road is lined with buildings or natural landscapes, creating calm and picturesque atmosphere. Picking up essentials at well-organized grocery store. The bright lighting and neatly stocked shelves create comfortable and efficient shopping experience. Keywords interview, local media, studio, casual setup, conversation, crew, minimal distractions, well-lit, profession, professional park, jogging, exercising, relaxing, nature, trees, walking paths, serene, peaceful, outdoors farm, visit, green fields, farm animals, peaceful, natural, agriculture, learning, outdoors, relaxing dinner, friends, family, restaurant, cozy, informal, meal, unwinding, relaxed, evening landmark, city, tourist, cultural, visit, weather, atmosphere, bustling, break, routine zoo, animals, habitats, education, family-friendly, wildlife, exploring, local, learning, nature shopping, mall, bustling, stores, shopping experience, brightly lit, people, relaxing, purchasing, atmosphere lecture, public, university, community center, formal, attendees, presentation, education, learning, professional gym, workout, exercises, fitness, spacious, equipment, energetic, environment, focus, physical dance, event, club, music, socializing, vibrant, colorful, lights, festive, lively coffee shop, reading, cozy, relaxed, ambient, quiet, laptop, break, friends, work airport, waiting, travel, terminal, flight, runway, modern, organized, passengers, calm concert, live music, crowd, lively, spirited, performance, audience, indoor, outdoor, energy beach, relaxing, seaside, waves, clear sky, peaceful, break, family, serene, outdoors business, meeting, conference room, professional, city view, windows, productive, discussion, corporate, formal museum, tour, historical, artistic, exhibits, spotlights, dim lighting, reflective, atmosphere, culture car, driving, scenic, sunset, road, landscapes, city, countryside, picturesque, travel grocery, shopping, store, essentials, organized, bright lighting, efficient, comfortable, experience, shopping Table 6. Event Descriptions with Corresponding Keywords (part one). Each event description provides detailed explanation of the scenario and is associated with list of extracted keywords that capture the essence of the scene. These keywords are used for evaluation purposes in our framework. Event marathon running art gallery visit family gathering bookstore browsing mountain cabin retreat office working train commute mountain hiking school presentation restaurant dining night sky stargazing snowshoeing city bike ride fashion show fishing trip train station waiting charity event Description Running in local marathon event. The streets are lined with cheering crowds, and the weather is clear, creating an energetic and community-oriented environment. Strolling through an art gallery or exhibition. The gallery has soft lighting and showcases various artworks, allowing for calm, introspective experience. Spending time with family at comfortable home setting. The room is warmly lit with family mementos and friendly, welcoming atmosphere. Browsing through books in quaint bookstore. The small, quiet setting is filled with shelves of books, perfect for leisurely exploration. Relaxing at cabin in the mountains. The area is peaceful, surrounded by trees and distant mountain views, creating tranquil and refreshing setting. Working or studying at desk in modern office. The room has large windows with natural light, creating productive and quiet atmosphere for focused tasks. Traveling on busy train, either standing or seated, surrounded by passengers absorbed in various activities. The setting is organized, creating routine commute experience. Hiking along scenic mountain trail. The view of mountains and clear sky adds refreshing and peaceful ambiance to the experience. Delivering or observing presentation in classroom. The students are attentive, creating an academic atmosphere suited for sharing knowledge. Dining at an upscale restaurant. The lighting is dim, and the decor is elegant, creating an intimate and refined ambiance. Observing the night sky at an outdoor stargazing event. Telescopes are set up, and the setting is quiet with clear view of the stars, creating magical atmosphere. Exploring snowy forest on snowshoeing trail. The setting is quiet, with only the sound of footsteps in the snow, creating peaceful winter atmosphere. Riding bike along city streets or designated trails. The background showcases tall buildings or park areas, creating blend of urban and natural scenery. Attending fashion show. The atmosphere is glamorous, with runway spotlighting models and guests observing the latest trends in fashion. Fishing by serene lake. The landscape is surrounded by greenery, and the atmosphere is peaceful with only natures sounds in the background. Waiting at quiet train station platform, with schedules displayed on an electronic board. The atmosphere is calm, with passengers nearby preparing for their commute. Participating in community charity event in large hall. The room is decorated for the occasion, with guests mingling and the mood warm and friendly. Keywords marathon, running, event, streets, cheering, crowds, clear weather, community, energy, fitness art gallery, visit, exhibits, artwork, soft lighting, calm, introspective, atmosphere, culture, reflection family, gathering, home, warmly lit, mementos, friendly, welcoming, atmosphere, comfort, together bookstore, shelves, exploration, reading, leisure, relaxed browsing, quaint, books, quiet, mountain, cabin, retreat, peaceful, trees, views, tranquil, refreshing, nature, serene office, working, desk, modern, conference room, windows, natural light, focused, quiet, productive train, commute, busy, seated, standing, passengers, routine, travel, organized, routine mountain, hiking, trail, scenic, view, clear sky, peaceful, refreshing, nature, outdoors school, presentation, classroom, students, attentive, academic, learning, sharing knowledge, formal, education restaurant, dining, upscale, dim lighting, elegant, refined, intimate, ambiance, meal, gourmet night sky, stargazing, outdoors, telescopes, quiet, clear view, stars, magical, peaceful, event snowshoeing, forest, snow, winter, trail, quiet, footsteps, peaceful, nature, serene bike ride, city, streets, trails, urban, scenery, buildings, park, nature, dynamic fashion, show, runway, models, glamorous, spotlight, trends, observation, fashionable, elegant fishing, trip, lake, serene, greenery, nature, outdoors, peaceful, relaxing, scenic train station, waiting, platform, calm, passengers, quiet, departure, travel, routine, organized charity, event, community, hall, guests, mingling, decorated, mood, warm, friendly Table 7. Event Descriptions with Corresponding Keywords (part two). Each event description provides detailed explanation of the scenario and is associated with list of extracted keywords that capture the essence of the scene. These keywords are used for evaluation purposes in our framework. Event nature photography library studying Description Taking photographs in scenic forest or park. The atmosphere is quiet and filled with the sounds of nature, perfect for capturing the beauty of the outdoors. Studying or reading in quiet library. The tall bookshelves and soft lighting create an ideal setting for focused learning. boat trip biking trail city walk Taking relaxing boat trip along calm river or lake. The sky is clear, and the scenic landscape adds to the peacefulness of the outing. Riding bike along nature trail, with trees lining the path. The refreshing environment and dappled sunlight create peaceful atmosphere. Walking through lively city center. The street is lined with shops and bustling with people, providing vibrant and dynamic urban experience. Keywords photography, quiet, scenic, capturing, beauty, peaceful nature, forest, park, outdoors, library, studying, bookshelves, quiet, focused, reading, learning, atmosphere, soft lighting, introspective boat trip, river, lake, relaxing, scenic, peaceful, water, landscape, clear sky, nature bike, trail, nature, trees, path, outdoors, scenic, sunlight, peaceful, refreshing city walk, lively, shops, bustling, urban, dynamic, streets, people, downtown, exploring Table 8. Event Descriptions with Corresponding Keywords (part three). Each event description provides detailed explanation of the scenario and is associated with list of extracted keywords that capture the essence of the scene. These keywords are used for evaluation purposes in our framework. Figure 8. Visualization of the generated dataset. The examples demonstrate consistency in character appearance across different scenes for the same identity (rows) and consistency in scene style across different characters for the same event (columns). Figure 9. Instructions for G-Eval. Figure 10. Visualization of person unlearning results. The original model correctly identifies the individuals in the images by their names, while the unlearned model outputs different names, indicating successful unlearning of the original identity information. Figure 11. Visualization of event unlearning results. The original model provides precise descriptions of the scenes, while the unlearned model offers alternative descriptions, diverging from the original content to meet the unlearning objective. The goal model demonstrates the target descriptions for comparison."
        }
    ],
    "affiliations": [
        "HIT",
        "NUS",
        "SZTU",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "XDU"
    ]
}