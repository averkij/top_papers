{
    "paper_title": "Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections",
    "authors": [
        "Ankit Dhiman",
        "Manan Shah",
        "Rishubh Parihar",
        "Yash Bhalgat",
        "Lokesh R Boregowda",
        "R Venkatesh Babu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We tackle the problem of generating highly realistic and plausible mirror reflections using diffusion-based generative models. We formulate this problem as an image inpainting task, allowing for more user control over the placement of mirrors during the generation process. To enable this, we create SynMirror, a large-scale dataset of diverse synthetic scenes with objects placed in front of mirrors. SynMirror contains around 198K samples rendered from 66K unique 3D objects, along with their associated depth maps, normal maps and instance-wise segmentation masks, to capture relevant geometric properties of the scene. Using this dataset, we propose a novel depth-conditioned inpainting method called MirrorFusion, which generates high-quality geometrically consistent and photo-realistic mirror reflections given an input image and a mask depicting the mirror region. MirrorFusion outperforms state-of-the-art methods on SynMirror, as demonstrated by extensive quantitative and qualitative analysis. To the best of our knowledge, we are the first to successfully tackle the challenging problem of generating controlled and faithful mirror reflections of an object in a scene using diffusion based models. SynMirror and MirrorFusion open up new avenues for image editing and augmented reality applications for practitioners and researchers alike."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 7 7 6 4 1 . 9 0 4 2 : r Reflecting Reality: Enabling Diffusion Models to Produce Faithful Mirror Reflections Ankit Dhiman1,2* Manan Shah1* Rishubh Parihar1 Yash Bhalgat3 Lokesh Boregowda2 Venkatesh Babu1 1Vision and AI Lab, IISc Bangalore 2Samsung & Institute India - Bangalore 3Visual Geometry Group, University of Oxford Figure 1. We present MirrorFusion, diffusion-based inpainting model, which generates high-quality geometrically consistent and photorealistic mirror reflections given an input image and mask depicting the mirror region. Our method shows superior quality generations as compared to previous state-of-the-art diffusion-based text-to-image and inpainting methods. All the images were generated by prefixing the mirror text prompt: perfect plain mirror reflection of to the input object description."
        },
        {
            "title": "Abstract",
            "content": "We tackle the problem of generating highly realistic and plausible mirror reflections using diffusion-based generative models. We formulate this problem as an image inpainting task, allowing for more user control over the placement of mirrors during the generation process. To enable this, we create SynMirror, large-scale dataset of diverse synthetic scenes with objects placed in front of mirrors. SynMirror contains around 198K samples rendered from 66K unique 3D objects, along with their associated depth maps, normal maps and instance-wise segmentation masks, to capture relevant geometric properties of the scene. Using this dataset, we propose novel depth-conditioned inpainting *Equal Contribution. 1 method called MirrorFusion, which generates high-quality geometrically consistent and photo-realistic mirror reflections given an input image and mask depicting the mirror region. MirrorFusion outperforms state-of-the-art methods on SynMirror, as demonstrated by extensive quantitative and qualitative analysis. To the best of our knowledge, we are the first to successfully tackle the challenging problem of generating controlled and faithful mirror reflections of an object in scene using diffusion based models. SynMirror and MirrorFusion open up new avenues for image editing and augmented reality applications for practitioners and researchers alike. The project page is available at: https://val.cds.iisc.ac.in/reflecting-reality.github.io/. 1. Introduction Recent diffusion-based generative models [11, 12, 38, 42, 46] have achieved remarkable results, producing visually appealing images across various domains. These models can be conditioned using several modalities, such as text [11], depth-image [49], sketch [17], for controlled generation [56, 59, 61], enabling various interesting applications. Despite their success, these models struggle to capture subtle geometric cues such as shadows, lighting and specular reflections, as noted in previous studies [44, 53]. Specifically, the task of generating realistic and controllable mirror reflections remains an unsolved challenge. Existing methods, which tackle perspective issues [50, 61] and address specular reflections for object removal [53] do not address mirror reflections in particular. To illustrate this limitation, we prompt Stable Diffusion2.1 [41] with the instruction to generate scene with mirror reflection. Fig. 2 shows that Stable Diffusion-2.1 fails to generate plausible and consistent mirror reflections. Further, various state-of-the-art inpainting methods such as Stable Diffusion Inpainting [41] and PowerPaint [64] also fail at the task of generating plausible and controlled reflections, as shown in Fig. 1 (b) & (c). In this work, we pose the problem of generating mirror reflections as an Image Inpainting task. This formulation provides two distinctive advantages: (1) Posing it as an inpainting method aids the reflection generation process to take cues from the input image and (2) allow greater control on the placement of mirrors. Existing datasets for tasks such as mirror segmentation, detection or novel-view synthesis as shown in Tab. 1 contain mirrors reflecting generic backgrounds and also lack the scale making them unsuitable for the task of training generative models for generating photo-realistic mirror reflections. Therefore, to address this, we introduce SynMirror, training dataset and MirrorBench, benchmark dataset designed to train and evaluate the capability of generative models to produce photo-realistic mirror reflections.SynMirror contains 198, 204 samples from rendering 66, 068 unique 3D objects sourced from Objaverse [5] and Amazon Berkeley Objects (ABO) [3]. Samples from SynMirror are created by rendering synthetic scenes with objects placed in front of mirror using the cycles rendering engine from Blender. To further obtain instance segmentation, depth and normal maps, we utilize Blenderproc [6], procedural Blender pipeline for photorealistic rendering. The generated scenes have diverse mirrors, floor textures and backgrounds. When recent inpainting methods such as BrushNet [14] are fine-tuned on SynMirror, we observe that they fail in generating correct geometry and depth of an object in the mirror reflection as shown in Fig. 7. We hypothesize that providing additional cues, such as depth maps, could help Figure 2. Images generated from Stable Diffusion 2.1 [41]. Textto-image models, when prompted to generate reflections, struggle to generate consistent and controlled mirror reflections. to alleviate the issue in generating geometrically consistent reflections on the mirror. To this end, we propose MirrorFusion, depth conditioned inpainting method that generates high-quality controlled and photo-realistic mirror reflections. Our method significantly outperforms state-ofthe-art diffusion-based inpainting methods on SynMirror, as evidenced by extensive quantitative and qualitative evaluations. We summarize our contributions below: To the best of our knowledge, we are the first to address and tackle the challenging problem of generating controlled photo-realistic and geometrically consistent mirror reflections of objects using diffusion models by formulating it as an image inpainting task. For this purpose, we present SynMirror, large-scale synthetic dataset of objects with accurate mirror reflections. We also create MirrorBench, subset of SynMirror, for benchmarking the capabilities of generative models in generating photo-realistic mirror reflections of diverse objects. We further propose MirrorFusion, novel depth conditioned inpainting method, which produces photo-realistic and controlled mirror reflections in the masked region of an input image, when trained on SynMirror. 2. Related Work Diffusion based generative models. Diffusion-based models [46] have revolutionized the field of image synthesis [11, 42]. These generative models are further extended to other modalities such as video [12, 45], audio [19]and text [23]. Further, text-to-image(T2I) models [36, 40, 41, 43] have the capability to generate photo-realistic images with any arbitrary text prompt. However, these methods do not work for generating realistic reflections, as shown in Fig. 2. Image inpainting methods. Recent advancements in diffusion models [11] have led to significant progress in the inpainting task. Diffusion based inpainting methods [1, 4, 28, 33, 42, 58] have shown tremendous improvement in this 2 task compared to GAN based methods [26, 63]. common approach to inpaint with diffusion models involves modifying the standard denoising strategy: sampling masked regions from pre-trained diffusion model and unmasked areas from the given image. While this method produces satisfactory results, it does not generalize to complex scenes and shapes. Stable Diffusion Inpainting [41] fine-tunes diffusion model by taking the noisy latents, mask and masked image as inputs to the U-Net architecture. Methods like HD-Painter [30] further enhance this method. recent method, BrushNet [14], divides the masked image features and noisy latents into separate branches, which increases textual coherence and improves masked image preservation. We show in Fig. 1 that these methods do not perform well for generating reflections on the mirror. Reflection in vision tasks. Reflection has been extensively explored for image enhancement tasks such as single image reflection removal [13, 21, 52]. This task is illposed in nature and requires additional priors to be solved. Other methods [18, 20, 22] use multiple images to solve this task. Specifically, they use polarization cues to remove reflection from the input image. Further, reflection cues are used to detect the glass/reflective surfaces in the real world [25, 31]. Recently, PromptRR [51] uses diffusion models for solving the single image reflection removal task. Further, [27, 34, 57] solves the challenge of reconstructing mirror reflections in 3D scene for the novel-view synthesis task. In this work, we attempt to solve the challenging task of generating controlled photo-realistic and geometrically consistent mirror reflections for an object in an input image, which has not been addressed in previous works to the best of our knowledge. 3. SynMirror: synthetic dataset of mirror reflections We observe that state-of-the-art diffusion models struggle to generate geometrically consistent results for mirror reflections as shown in Fig. 1 and Fig. 2. We hypothesize that the cause of this inferior performance is due to the limited number of samples of images with realistic mirror reflections in various existing datasets used to train these models. Further, we find that existing mirror datasets are inadequate for training generative models as they are primarily designed for reflective mirror detection [55] and lack object diversity [48], which is required to incorporate the priors of mirror reflections in diffusion models. To address this, we propose SynMirror, first-of-its-kind large-scale synthetic dataset on mirror reflections, with diverse mirror types, objects, camera poses, HDRI backgrounds and floor textures. We provide comprehensive characteristic comparison of SynMirror with existing mirror datasets in Tab. 1. Note that SynMirror is more than six times larger than all the existing mirror datasets combined. Further, our Table 1. comparison between SynMirror and other mirror datasets. The proposed dataset has more attributes and is six times larger in size than all other existing datasets combined. Dataset Type MSD [55] Mirror-NeRF [57] DLSU-OMRS [9] TROSD [47] PMD [24] RGBD-Mirror [32] Mirror3D [48] Real Real & Synthetic Real Real Real Real Real Size 4018 9 scenes 454 11060 6461 3049 7011 Attributes RGB, Masks RGB, Masks, Multi-View RGB, Mask RGB, Mask RGB, Masks RGB, Depth RGB, Masks, Depth SynMirror (Ours) Synthetic RGB, Depth, Masks, Normals, Multi-View data generation pipeline renders color images, instance segmentation masks, depth maps and normal maps as shown in Fig. 3. Additionally, we create MirrorBench, subset of SynMirror, which serves as challenging benchmark for generative tasks on mirror reflections. SynMirror can also be leveraged to benchmark other downstream tasks such as monocular depth estimation and novel-view synthesis. 3.1. Dataset Generation and Processing Object Source. SynMirror consists of 3D assets from two widely used 3D object datasets - Objaverse [5] and Amazon Berkeley Objects (ABO) [3]. Objaverse is large scale dataset consisting of 800K 3D assets with diverse categories and ABO contains catalogued 3D models with complex geometries that correspond to real world household objects. However, some objects from Objaverse are poorly rendered or have low-quality textures. Thus, we use filtered list of 64K 3D objects as filtered by OBJECT 3DIT [35]. Despite this initial filtering, some spurious objects do not show mirror reflections pertaining to specific shader properties of objects. We elaborate on the filtering method to remove spurious objects in Appendix A.1. Post filtering, we obtain subset of 58, 115 high-quality 3D assets from Objaverse. We include all 7, 953 objects from the ABO dataset to cover wide range of object shapes and appearances, and thus resulting in 66, 068 total objects. Scene Setting. We use virtual environment in Blender to compose scenes with realistic reflections. We follow set of heuristic rules to compose scene with the 3D asset, floor, and mirror; an example is shown in Fig. 3. We model mirrors as rectangular planes of varying sizes and frame textures. The floor is modeled as plane with diverse set of textures sampled from 1182 CC-textures [6]. We compose scene, by first placing mirror vertically at fixed location. Next, we define region at fixed distance to the mirror where the 3D object will be placed, represented as unit cube. We normalize the object to fit in the unit cube. We also rotate the 3D object around its axis to increase diversity in object poses. For modeling the background, we use 359 high-quality HDRI environment maps from PolyHeaven [10]. We categorize the floor textures and 3 Figure 3. SynMirror: a) Dataset creation pipeline - We sample diverse 3D objects, mirrors as 2D planes and diverse floor textures to compose scene in blender environment. To enhance realism, we sample high-quality HDRI environment maps as backgrounds. We sample cameras from varied viewpoints, capturing the mirror and the object, and use Blender to render RGB images and dense 2D annotations. b) Samples from SynMirror - The generated scenes have complex geometry, textures, and high diversity. The renderings have accurate dense annotations for semantic, depth and normal maps at the original image resolution. HDRI maps into indoor and outdoor categories to simulate realistic indoor/outdoor scenes enhancing photorealism of the renderings. For illuminating the scene, we use an area light placed slightly above and behind the object at 45 degree angle pointing towards the object and the mirror. Rendering. We te pool of 19 camera poses by interpolating between two extreme camera poses while ensuring that the object and its reflection in the mirror are visible. For each scene iteration, we randomly sample 3 camera poses and render the scene from these virtual cameras by leveraging BlenderProc [6], to obtain RGB, depth maps, surface normal maps, and semantic labels. To obtain high-quality photo-realistic renderings, we render at 512 512 resolution using 1024 cycles from Blenders Cycles renderer. This allows us to create rich and comprehensively annotated SynMirror dataset for studying variety of mirror related tasks. More details about the dataset are provided in Appendix A. 4. Method We briefly introduce diffusion models in Sec. 4.1. Then, we present our method MirrorFusion in Sec. 4.2. Fig. 4 provides an overview of our method. 4.1. Preliminaries Diffusion models are family of generative models that generate images by iterative denoising. In the forward diffusion process, Gaussian noise ϵ (0, 1); is sequentially added for timesteps to clean sample x0 to get noisy sample xT . In the backward diffusion process, clean image x0 is generated by iterative denoising of noisy image xT . The iterative denoising process is modeled with denoising network ϵθ conditioned on the timestep {1, } and optional conditioning (e.g. text prompts, inpainting masks). The denoiser is trained with simple mean square loss LDM as follows: LDM = Ex0,ϵN (0,I),tϵ ϵθ (zt, t, c) 2 (1) Training diffusion models directly on the large resolution images x0 is computationally demanding as it needs several denoising steps to generate single image. Latent Diffusion Models [41] propose to apply diffusion process in compressed latent space of pre-trained Variational Autoencoder. This enables efficient training and fast inference for generating large-resolution images. 4.2. MirrorFusion Though trained on large-scale datasets, existing state-ofthe-art diffusion models fail to generate consistent reflections with accurate object shape and scene appearance as shown in Fig. 1. We propose MirrorFusion, novel framework for generating accurate mirror reflections by formulating it as an inpainting problem. Given an input scene image and mirror mask, MirrorFusion fills the masked region with the consistent reflection of the object and the scene. Generating accurate reflections requires precise 3D understanding of the scene to reason about the distance of objects from the mirror and the shapes of objects. Hence, modeling reflections with just 2D inpainting model is suboptimal, and we need to inject explicit 3D cues during the inpainting process. Thus, we propose to condition the inpainting approach with depth maps. The geometric signal from the depth map enables us to generate accurate and consistent reflections that adhere to the 3D structure of the scene and the object. Fig. 4 shows the overview of our method. 4 Figure 4. Overview of the architecture. We encode the input image using pre-trained image encoder from Stable Diffusion to get zm. Subsequently, we resize the mirror mask and depth map to obtain resized mask xm and depth xd. Then, we concatenate noisy latents zt, zm, xm and xd which are fed into the Conditioning U-Net ϵ θ. Each layer of the Generation U-Net ϵθ is conditioned via zero θ. Additionally, ϵθ is conditioned by text embeddings. The pre-trained decoder then decodes convolutions with corresponding layers of ϵ the denoised latent to produce an image with mirror reflections. Detailed information can be found in Sec. 4.2 4.2.1 Model architecture conditioning. We set to be 1.0 for all our experiments. MirrorFusion is diffusion-based inpainting model conditioned on the input mirror mask and depth map. We use base dual branch architecture for inpainting following BrushNet [14] as shown in Fig. 4. The core idea is to clone pretrained diffusion model ϵθ without cross-attention layers to ϵ θ. Subsequently, the features from the conditioning θ are inserted into the generation model ϵθ using model ϵ zero-convolutional layers. During training, only the conditioning model is updated, keeping the generation model frozen. This conditioning mechanism provides strong hierarchical conditioning for generation without altering the original generation model. 4.2.2 Depth conditioning Geometric information about objects and scenes is crucial for generating 3D consistent reflections. Recent works [2, 37] show that injecting depth maps enables 3D geometric control in the diffusion models. Inspired by this, we utilize depth-conditioning for our inpainting architecture. Specifically, the noisy latent zt, masked image latent zm, inpainting mask xm, and the depth map xd are all concatenated and passed as input to the conditioning U-Net ϵ . The generation U-Net ϵ is an unaltered text-to-image diffusion model, which takes noisy latent zt and predicts cleaner version zt1. Each layer of generation U-Net ϵi is conditioned with the corresponding layer of conditioning U-Net using zeroconvolutions (Z) as follows: ϵθ (zt, t, c)i = ϵθ (zt, t, c)i+wZ (cid:16) ϵ is the preservation scale to adjust (cid:17) θ ([zt, zm, xm, xd] , t)i (2) the influence of Impact of Depth Conditioning. We demonstrate the importance of depth conditioning for the reflection generation task as shown in Fig. 5. From Fig. 5 (a), it can be clearly seen that BrushNet [14] fine-tuned on SynMirror fails to generate accurate mirror reflection of the object in the input image with high fidelity. For simple object like baseball ball, the w/o depth BrushNet-FT model generates ball in which the shape is not preserved. Similarly, in Fig. 5 (b), the shape of chair is asymmetrical. These examples show that depth information provided with the proposed normalization scheme generates better reflections on the mirror. 4.2.3 Depth Normalization The range of input depth is between [0, ). The encoder of the U-Net expects the input to be in the range [1, 1]. Hence, we need to normalize the input depth. As discussed in [15], using affine-invariant depth scaling can bring the input depth into the desired range. Image reflection tasks only require the relative distance between the mirror and the scene it reflects. Depth values behind the mirror will not be critical for the reflection generation task. Hence, we use specifically tailored normalization for our task which is computed as: (cid:18) ˆd = dclipped dmax + depth (cid:19) 0.5 2, (3) where dclipped is input depth clipped between range [0, dmax +depth]. dmax represents the maximum depth on the mirror mask m. We set depth to the value of 0.5. This normalization aids us in adapting to any of the pre-trained monocular depth estimation methods [15, 54]. Figure 5. Impact of depth conditioning on the reflection generation quality. Notice the irregular shape of the baseball and chair marked in red. In comparison, our method preserves the structure of the object (marked in green). Inference. During inference, we assume predefined mask is available to indicate where the mirror reflection should be generated. The user can easily create this mask, giving better control over the generation of the reflections. We leverage Marigold [15], monocular depth estimation method to generate the scene depth map. Then, we feed these inputs to our pipeline as shown in Fig. 4. We show in Appendix C.4 that we can utilize alternative methods, such as DepthAnything [54] as preferred monocular depth estimation technique, demonstrating the robustness of our method to the choice of different monocular depth estimation techniques. 5. Experiments & Results In this section, we discuss the dataset, baseline comparisons, and extensive experiments used to evaluate our model. We provide additional training and implementation details in Appendix B. Dataset. As discussed in Sec. 3.1, SynMirror consists of 66, 068 objects and 198, 204 rendered images. We sample 1000 objects from the full dataset to create MirrorBench, benchmark to evaluate our method and various other baselines. In this benchmark, we have 1497 known class samples, i.e., these object categories were seen during training, and 1494 unknown class samples, i.e., these object categories were unseen during training. MirrorBench thus comprises of total of 2991 images. We also show the generalization capabilities of our method on few samples from the Google Scanned Objects(GSO) [7] dataset. Baselines. As discussed in Sec. 4, we formulate generating reflection of an object as an image-inpainting problem. We evaluate various state-of-the-art inpainting methods on MirrorBench. We compare our method with pretrained Stable-Diffusion-Inpainting [41], PowerPaint [64] and BrushNet [14]. We denote zero-shot methods by appending -ZS to their names. We fine-tune BrushNet on SynMirror and refer to this fine-tuned version as BrushNet-FT hereafter. Figure 6. Additional Results. Our method effectively preserves the shape of objects, as demonstrated in (a) the lawn chair and (b) the swivel chair. Check in the zoomed-in regions. Additionally, our method accurately positions the objects within the mirror (c) and (d), corroborating the effectiveness of depth-conditioning in our method. Text-prompts used are described in Appendix E.2. Metrics. We benchmark based on four aspects: masked region preservation, reflection generation quality, Reflection Geometry and text alignment. Masked Region Preservation. We report Peak-Signalto-Noise ratio (PSNR), Structural Similarity (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) [60] in the unmasked region between generated and the real image. This shows how much original image content is preserved by an inpainting method. Reflection Generation Quality. For measuring the quality of the reflection of the object and the scene, we compute PSNR, SSIM and LPIPS on the masked region containing the object and floor of the ground truth image. Reflection Geometry. We measure the geometric accuracy of the generated reflection using Intersection over Union (IoU) between the segmentation mask of the ground-truth object and the generated object in the reflection region specified by the input mirror mask. We utilize SAM [16] to get the mask of an object in the reflection region. More details are provided in Appendix E.3. Text Alignment. To evaluate the text-image consistency between the generated image and the text prompts, we use CLIP [39] Similarity. 5.1. Qualitative Results Comparison with Zero-shot Baselines. We observe that all zero-shot baselines fail in generating realistic reflections on the mirror. PowerPaint generates the lipstick at the incorrect position, whereas Stable Diffusion 1.5 Inpainting 6 Figure 7. Reflection generation comparison with general inpainting methods. We compare our results with zero-shot baselines Stable Diffusion 1.5 Inpainting-ZS, PowerPaint-ZS and BrushNet-ZS. Further, we finetune BrushNet on SynMirror and refer to it as BrushNet-FT. The top four rows compare results on unknown categories, and the bottom two rows show results on known categories from MirrorBench. Zero-shot methods either fail to generate reflection on the mirror or generate reflection at an incorrect position. In comparison, BrushNet-FT generates plausible reflections, but with geometric inaccuracies. Our method improves on shape preservation of the object, floor texture and correct placement of the object in the mirror reflection. generates two reflections when only one is present. No zeroshot method is able to provide plausible reflection for cement mixer as shown in Fig. 7 (top row). BrushNet-FT performs better than the zero-shot methods, which shows the utility of the proposed dataset. However, it has issues such as the incorrect size of the object in the generated reflections for cement mixer and lipstick as shown in the first two rows of Fig. 7. In comparison, our method is able to generate realistic reflections of the objects. 7 Table 2. Image Generation Quality. We compare the quality of the inpainted image with fine-tuned baseline method. The best results are shown in bold. Our method outperforms the baseline across all metrics, proving its effectiveness. Metrics Models Masked Image Preservation Text Alignment PSNR SSIM LPIPS CLIP Sim Brushnet-FT [14] Ours 23.06 24.22 0.84 0.84 0.058 0.051 24.90 25. Table 3. Reflection Quality. We compare the quality of the generated reflection image with the baseline method. We observe that our method has better object quality metrics. Best results are shown in bold. Metrics Models Reflection Generation Quality Reflection Geometry PSNR SSIM LPIPS Brushnet-FT [14] Ours 19.15 20.35 0.84 0.84 0.082 0.075 IoU 0.566 0.567 5.2. Ablation studies Tab. 2 & 3 quantitatively compare BrushNet-FT and MirrorFusion on the image quality and the generated reflection quality. These values are reported on MirrorBench. We generate 4 outputs for each test sample using different random seeds. We then select the image with the best mask SSIM score as the representative image out of the four images. The reported value for any metric is the average of that metric for all representative images across the dataset. MirrorFusion (Ours) with depth cues outperforms BrushNetFT, which doesnt take depth as an input. This corroborates the necessity of adding depth as an input to the model. We discuss limitations and societal impact in Appendix D. 6. Conclusion In this work, we propose SynMirror, large-scale scale challenging and diverse dataset to train generative models for the task of generating realistic mirror reflections. We identify shortcomings in current models and propose MirrorFusion, novel inpainting method conditioned on depth maps for generating geometrically accurate mirror reflections. Extensive qualitative and quantitative results on MirrorBench shows the superior performance of MirrorFusion in comparison to other methods. Our work is the first step towards generating geometrically accurate and plausible mirror reflections using diffusion based generative models. We believe that SynMirror and MirrorBench will pave way for research in several mirror-related tasks. Figure 8. Change of Viewpoints for mirror and object. Our method preserves the shape of the object from different viewpoints. This illustrates our methods ability to utilize 3D cues and generate accurate reflection of the object. Figure 9. Generalization on GSO [7]. Our method generates accurate reflections for unseen real-world scanned objects. This substantiates the generalization capabilities of our method. Additional Results. Fig. 6 shows more comparisons between BrushNet-FT and our method. First row shows that there are some structural inaccuracies in the generated reflection. Check how BrushNet-FT is not able to get the structure of the lawn chair in Fig. 6 (a) and swivel chair in Fig. 6 (b). However, our method is able to generate the reflection of the object in geometrically accurate position. Further in Fig. 6 (c) and (d), notice that the reflection is generated at wrong position in the mirror by BrushNet-FT. Qualitative Results on GSO. We further benchmark the performance of our method on completely held-out set of GSO objects. Fig. 9 compares our method with the finetuned baseline: BrushNet-FT. Notice that the bowl is floating in the air for BrushNet-FT and the size of the bag in the reflection is large and unnatural. In comparison, our method generates the reflection with better photo-realism and geometric accuracy. Change of Viewpoints. To evaluate the consistency of our method in generating reflections across varying viewpoints, we designed continuous trajectory for testing. The results inferred from our method, as depicted in Fig. 8, demonstrate that the reflection of the sofa-seat remains consistent as the viewpoint shifts. Additionally, our method preserves high-fidelity reflections for the floor. 8 Algorithm 1 Determine if 3D Object is Spurious Input: 3D model Output: True, if 3D model is spurious, else False for material do 1: for child do 2: 3: 4: 5: for node .material do if (N .name == Mix-Shader) and (N .input.name == Fac) and (N .linked.name == Light Path) then RETURN(True) end if 6: 7: 8: 9: 10: 11: end for end for end for A.2. Preparation of MirrorBench MirrorBench aims to benchmark various generative models at the task of generating perfect mirror reflections. MirrorBench is created by sampling around 1, 000 objects from SynMirror, with 3 rendered samples per object, totalling to 2, 991 samples. Fig. 12 shows samples of MirrorBench, which consist of two types: 1. Unknown class objects, referring to categories not present in the training set. We take the first 500 objects from Objaverse in Unknown category, sorted in the increasing order of category frequency and keep the remaining categories in the training set as Known categories. There are 1494 samples generated from the objects of Unknown category. 2. Known class objects, referring to categories included in the training set. There are 1497 images from this category. This includes renderings from around 250 objects from Objaverse and around 250 objects from ABO."
        },
        {
            "title": "Contents",
            "content": "A. Dataset A.1. Filtering out Spurious objects A.2. Preparation of MirrorBench . . . . . . . . . . . . . . . . . B. Implementation Details B.1. Training Details: MirrorFusion . . B.2. Training Details for Baseline Methods . . B.3. Inference Details . . . . . . . . . . . . . . . . . . . . . . . C. Additional Results . . . . . . (GSO) . . C.1. More Results on Google Scanned Objects . . . . . . . C.2. Results on real-world scenes. . . . C.3. Comparison with Commercial Products. . . . C.4. Robustness to pre-trained monocular depth . . . . . . . C.5. More Qualitative Comparisons . estimation methods . . . . . . . . . . . . . . . . . . . . . . . D. Limitations and Social Impact E. Additional Details . . . E.1. Results from recent T2I methods . . E.2. Text prompts used in the experiments . . . . E.3. Generation of Segmentation Masks for com- . . . . puting metrics . . . . . . . . . . . . . . 9 9 12 12 12 12 13 13 13 14 14 14 15 15 15 16 A. Dataset Our dataset consists of 198, 204 rendered images from 66, 068 objects: 58, 115 objects from Objaverse [5] and 7, 953 from the ABO [3] dataset. We utilize captions provided by Cap3D [29] during training. We provide more details in Sec. 3. To illustrate the diversity in 3D objects, floor textures and HDRI backgrounds, we present more samples in Fig. 10 and 11. A.1. Filtering out Spurious objects We discuss how we filter 3D objects from Objaverse [5] and Amazon Berkeley Objects (ABO) [3] in Sec. 3.1. In spite of the initial filtering, we observe some spurious objects, for which the reflection is not visible in the mirror. Algorithm 1 illustrates the pseudo-code to identify such spurious objects. Specifically, using Blenders Python API, we check the material property of each child in the input mesh of 3D object. We expect the 3D objects to be in standard 3D formats: *.glb, *.gltf, *.obj, *.fbx. If any node in the material property has the attributes: Mix-Shader, and the name of the input to this node is Fac, and the name of the linked node is Light Path, then we observe that the reflection of such 3D model does not appear in the mirror. We prune out such objects from the initial filtered list. The new filtered list will be made public along with the dataset for future research. Figure 10. Samples from SynMirror. 10 Figure 11. Samples from SynMirror. 11 experiment where we make the generation U-Net trainable. We call this model MirrorFusion*. We use the same training hyper-parameters and consider the checkpoint at 17, 000 steps. From Fig. 17 and Fig. 18, we can see improved results compared to the frozen generation U-Net. However, the VRAM requirements and training time almost double, due to the increase in the number of trainable parameters. B.2. Training Details for Baseline Methods Fine-tuning of BrushNet [14]. Keeping the generation UNet frozen, we fine-tune BrushNet using the input mask and masked image using the same hyperparameters used to train MirrorFusion. We do not randomly drop text prompts and select the checkpoint at 17, 000 steps for evaluation. We refer to this model as BrushNet-FT in Sec. 5 of the main paper and compare our results against it. We found that initializing the weights from the Stable Diffusion v1.5 [41] checkpoint was superior as compared to initializing from the pre-trained BrushNet [14] checkpoint. B.3. Inference Details During inference, we set the classifier free guidance scale (CFG) to 7.5 and use the UniPC scheduler [62] for 50 timesteps across all experiments. Figure 12. Samples from MirrorBench. The first two rows contain samples from Unknown categories and the bottom two rows contain samples from Known categories. Notice the challenging nature of MirrorBench. We provide more details in Appendix A.2 B. Implementation Details B.1. Training Details: MirrorFusion We follow the BrushNet [14] architecture for MirrorFusion and provide depth conditioning as discussed in Sec. 4.2. The Generation and Conditional U-Net weights are initialized from the Stable Diffusion v1.5 [41] checkpoint. During training, the weights of the generation U-Net are kept frozen, while the weights of the conditioning network are updated. The extra channels processing the down-sampled depth and mask images in the first convolution layer of the conditioning U-Net are initialized to zero. We train MirrorFusion on SynMirror, using the original input image resolution of 512 512. We utilize the AdamW optimizer with learning rate 1e 5. We train our model for 20, 000 steps on 8 NVIDIA A6000 GPUs with an effective batch size of 16, which takes around 12 hours. During training, we randomly drop text prompts 20% of the time to allow the model to take cues from the input depth map. We find the checkpoint at 15, 000 to produce the best qualitative results and use it for further inference. We also run an additional 12 Figure 13. Performance on Real-world scenes We show results on images from MSD [55] dataset (a) & (b) and examples from images captured using smartphone device (c) & (d). Appendix C.2 describes the experimental details and text prompts used for the inference. We observe that BrushNet-FT does not generate accurate reflections, whereas our method is able to generate plausible reflections on the mirror. Figure 14. Qualitative Comparison with Commercial Products We compare our results with Adobe Firefly. Our method is significantly better than the existing commercial product. This highlights the challenging nature of the task and the effectiveness of our proposed method in addressing it. and white mug on grey surface. (o) & (p). perfect plane mirror reflection of blue ball with an orange cover. C. Additional Results C.2. Results on real-world scenes. C.1. More Results on Google Scanned Objects (GSO) We provide additional results on 3D models from Google Scanned Objects (GSO) [7] in Fig. 15. GSO contains realworld scanned objects. We create renderings using these objects with the pipeline discussed in Sec. 3. We notice that our method MirrorFusion* consistently generates accurate reflections of objects and floors in the mirror. However, BrushNet-FT, is not able to generate the reflection of the floor correctly in image with blue ball (Fig. 15 (o), and (p)) and carton (Fig. 15 (l)) Further, it does not get the appearance of the pencil-box right, as shown in Fig. 15 (g) and (h). Additionally, it generates the reflection with the wrong structure in Fig. 15 (c) and (d). These results further substantiate the generalization capabilities of our method. Text prompts used for results in Fig. 15 are as follows: (a) & (b). perfect plane mirror reflection of sofa with purple cushioning. (c) & (d). perfect plane mirror reflection of yellow chair. (e) & (f). perfect plane mirror reflection of white stool with purple top. We present real-world examples from the MSD [55] dataset in Fig. 13 (a) and (b), utilizing the ground truth (GT) masks provided within the dataset as the corresponding mirror masks. Since our method requires depth, we infer it from Marigold and normalize it as described in Sec. 4.2.1. We observe that the baseline method fails to position the object accurately and produces incorrect color in Fig. 13 (a). In contrast, our method generates better reflections on the mirror. We also capture more examples from hand-held smartphone device in Fig. 13 (c) & (d). We manually annotate the mask corresponding to the mirror location and infer the depth from Marigold [15] as described above. Similar to the previous observation, our method preserves the shape of the object. Check the lid in Fig. 13 (c) and the roundness of the ball in Fig. 13 (d). These results show that our method generates better reflections than the baselines on real-world settings. Our method shows promising results on real-world settings, but still has scope for improvement, showing the challenging nature of this task. Text prompts used for generating the results in Fig. 13 are as follows: (a). perfect plane mirror reflection of rose gold col- (g) & (h). perfect plane mirror reflection of purple ored portable power-bank. bag with bluish circular patterns. (b). perfect plane mirror reflection of white ceramic (i) & (j). perfect plane mirror reflection of camouteapot. flaged military-style bag. (c). perfect plane mirror reflection of black round (k) & (l). perfect plane mirror reflection of cardbox with black lid on it. board box on patterned floor. (d). perfect plane mirror reflection of green color (m) & (n). perfect plane mirror reflection of yellow round ball. Figure 15. Qualitative Comparison on unseen 3D assets from GSO. We show results from (a) & (b) 3D Dollhouse Sofa, (c) & (d) 3D Dollhouse Swing, (e) & (f) 3D Dollhouse TablePurple, (g) & (h) Big Dot Aqua Pencil Case, (i) & (j) Digital Camo Double Decker Lunch Bag, (k) & (l) INTERNATIONAL PAPER Willamette 4 Brown Bag , (m) & (n) Room Essentials Mug White Yellow and (o) & (p) Toys Us Treat Dispenser Smart Puzzle Foobler. Appendix C.1 describes how images are generated and text-prompts used for the inference. We observe that BrushNet-FT does not generate accurate reflections in (c),(d),(f),(g),(h) whereas our method is able to generate correct reflections on the mirror. C.3. Comparison with Commercial Products. We compare our method with commercial products such as Adobe Firefly in Fig. 14. Our method significantly outperforms existing commercial solutions. Results from Fig. 14 highlight the challenging nature of the task of generating plausible mirror reflections and the critical gap that exists in current state-of-the-art methods. Text prompts used in Fig. 14 are as follows: 1st row. perfect plane mirror reflection of black bottle of liquor. 2nd row. perfect plane mirror reflection of red kettle-ball with handle. C.4. Robustness to pre-trained monocular depth estimation methods Our method is invariant to the choice of the pre-trained monocular depth estimation method. We present results from two state-of-the-art methods, Marigold [15] and DepthAnythingV2 [54], in Fig. 16. We observe minimal variation in the generation of reflections between both options, thereby confirming the robustness of our approach to the preference of the pre-trained monocular depth estimation method. Text prompts for Fig. 16 are as follows, each row uses the same text prompt: 1st row. perfect plane mirror reflection of rectangular cabinet with door, two drawers, truncated triangular base, and triangular top. 2nd row. perfect plane mirror reflection of swivel chair with curved backrest, slanted seat, curved armrests, and triangular top. C.5. More Qualitative Comparisons As discussed in Sec. 5, we compare our method with zeroshot baselines, denoted by -ZS and baselines trained on SynMirror, denoted by -FT. We provide additional results in Fig. 17 and 18. Consistent with the findings in the main paper, our method generates better mirror reflections while preserving the fidelity of both the objects appearance 14 Shower Gel Box. 5th row. perfect plane mirror reflection of black cowboy hat. D. Limitations and Social Impact Limitations. As our method leverages synthetic data to train model capable of producing realistic mirror reflections, the model still has scope for improvement in generating reflections for highly complex objects and scenarios. Although our model generates plausible results on realworld images, there is significant scope for improvement, which can be achieved by using more advanced photorealistic simulators or collecting large-scale real-world images. We aim to address these issues in our future work. Social Impact. Our method uses diffusion-based generative models, which, despite their potential, can be exploited for spreading misinformation. Therefore, it is crucial to use these models responsibly. E. Additional Details E.1. Results from recent T2I methods We present additional results from the recent Stable Diffusion 3 [8] model in Fig. 19. Text prompts are generated by using the prefix: perfect plane mirror reflection of and suffix: in front of the mirror positioned at an angle with respect to the mirror. to the object description of the input image. We observe that standalone text-to-image methods are inadequate in generating controlled and realistic mirror reflections. E.2. Text prompts used in the experiments This section provides the text prompts for the image generations in the main paper. Figure 1. Each row in this figure uses the same text prompt. Text prompts are as follows: First row. perfect plane mirror reflection of swivel chair with curved backrest, slanted seat, slender metal frame, and padded seat and backrest. Second row. perfect plane mirror reflection of large red, yellow, and black industrial cement mixer. Figure 2. Text prompts are already mentioned in the Figure of the main paper. Figure 5. Text prompts are as follows: (a). perfect plane mirror reflection of white golf ball Figure 16. Choice of pre-trained monocular depth estimation method during inference. We observe negligible differences in the reflection generation across both choices, Marigold [15] and DepthAnythingV2 [54], supporting the stability of our method regardless of the chosen pre-trained monocular depth estimation technique. We use Marigold in all our experiments. and the floor. Fig. 17 Each row in this figure uses the same text prompt. Text prompts are as follows: 1st row. perfect plane mirror reflection of multifunctional electronic device, including HDMI Blu-ray player, stereo receiver, amplifier, CD, and DVD player. 2nd row. perfect plane mirror reflection of red flashlight with metal pipe. 3rd row. perfect plane mirror reflection of red kettlebell with handle. 4th row. perfect plane mirror reflection of concrete block. 5th row. perfect plane mirror reflection of wooden barrel. Fig. 18 Each row in this figure uses the same text prompt. Text prompts are as follows: 1st row. perfect plane mirror reflection of large, red, rusty metal barrel. 2nd row. perfect plane mirror reflection of small with red stripe and the letter on it. stuffed animal toy. 3rd row. perfect plane mirror reflection of modern office chair with blue upholstered seat, back, and headrest. 4th row. perfect plane mirror reflection of Gaft (b). perfect plane mirror reflection of chair with curved slatted frame, tufted backrest, and curved seat. Figure 6. Text prompts are as follows: (a). perfect plane mirror reflection of modern wooden chaise lounge with white cushion. 15 (b). perfect plane mirror reflection of swivel chair with curved backrest, slender armrest, and swivel base. (c). perfect plane mirror reflection of black cylindrical with lid. (d). perfect plane mirror reflection of wooden box with intricate floral and heart-shaped carvings on each side, featuring dark brown hue with visible wood grain texture. Figure 7. Each row in this figure uses the same text prompt. Text prompts are as follows: 1st row. perfect plane mirror reflection of large red, yellow, and black industrial cement mixer. 2nd row. perfect plane mirror reflection of gold lipstick container. 3rd row. perfect plane mirror reflection of cylindrical object with cream-colored exterior and central hollow core; vertical seams divide the outer surface. 4th row. perfect plane mirror reflection of weathered wooden treasure chest with metal reinforcements, large metal ring on the side, and mossy accents. 5th row. perfect plane mirror reflection of grey cabinet with gold legs and chest of drawers. 6th row. perfect plane mirror reflection of black stone with intricate swirl designs on it. Figure 8. Each row in this figure uses the same text prompt. Text prompts are as follows: 1st row. perfect plane mirror reflection of slantedtop cuboid footstool. 2nd row. perfect plane mirror reflection of footstool with cuboid base, spherical top, seat, and backrest. Figure 9.Text prompts are as follows: (a). perfect plane mirror reflection of white ceramic bowl on textured gray surface.. (b). perfect plane mirror reflection of camouflaged military-style bag E.3. Generation of Segmentation Masks for computing metrics We compare the accuracy of the geometry of the generated reflection by comparing IoU between the segmentation mask of the reflection in the ground-truth object and the segmentation mask of the reflection in the generated object in Sec. 5. We utilize SAM to generate these segmentation masks. We provide initial seed points to SAM [16] along with rough bounding box. SAM then segments out the reflection of the object in ground truth as well as the generated image. Camera viewpoint variations within our dataset pose challenge for reliable seed point initialization. We address this by manually creating mapping to select seed points based on the camera pose. To accelerate the evaluation, we cache the segmentation masks of the ground-truth images. 16 Figure 17. Qualitative Comparison. We observe that the state-of-the-art inpainting method BrushNet-ZS is not able to generate plausible reflections (2nd column). BrushNet-FT which is fine-tuned on SynMirror is able to generate plausible reflections, 3rd column, but fails to accurately get the shape of the object. For example, the top surface of dvd-player in 1st row is completely missing. The flashlight reflections structure and appearance do not correspond with the object (2nd row). Compared to these baselines MirrorFusion generates plausible reflections. Still there is issue in the shape of the flashlight in 2nd row. These issues are mitigated by MirrorFusion*, which generates realistic, plausible and geometrically accurate reflections on the mirror. Figure 18. Qualitative Comparison. Similar to the observation in Fig. 17, we observe that the state-of-the-art inpainting method BrushNet-ZS is not able to generate plausible reflections (2nd column). BrushNet-FT which is fine-tuned on SynMirror is able to generate plausible reflections, 3rd column but fails to get shape of the object in the reflection. For example, observe the chair in 3rd row, the head of the chair is missing. The pose of the toy in 2nd row does not correspond to that of the real object. Compared to this MirrorFusion and MirrorFusion* generates plausible reflections on the mirror. 18 Figure 19. Additional results of images generated from Stable Diffusion 3 [8]. Text-to-image models struggle to produce consistent and controlled mirror reflections when prompted to generate them. We use the prefix perfect plane mirror reflection of and suffix in front of the mirror positioned at an angle with respect to the mirror. along with the object description."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 2 [2] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 5 [3] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object unIn Proceedings of the IEEE/CVF conference derstanding. on computer vision and pattern recognition, pages 21126 21136, 2022. 2, 3, 9 [4] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 43344343, 2024. 2 [5] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 9 [6] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Knauer, Klaus H. Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: procedural pipeline for photorealistic rendering. Journal of Open Source Software, 8(82):4901, 2023. 2, 3, 4 [7] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 6, 8, [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. arXiv, 2024. 15, 19 [9] Mark Edward M. Gonzales, Lorene C. Uy, and Joel P. Ilao. Designing lightweight edge-guided convolutional neural network for segmenting mirrors and reflective surfaces. Computer Science Research Notes, 3301:107116, 2023. 3 [10] Poly Haven. Poly haven : The public 3d asset library, 2024. Accessed: 2024-08-10. 3 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 fusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [13] Qiming Hu and Xiaojie Guo. Single image reflection sepIn Proceedings of the aration via component synergy. IEEE/CVF International Conference on Computer Vision, pages 1313813147, 2023. 3 [14] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. arXiv preprint arXiv:2403.06976, 2024. 2, 3, 5, 6, 8, 12 [15] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9492 9502, 2024. 5, 6, 13, 14, 15 [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 6, 16 [17] Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and YiZhe Song. Its all about your sketch: Democratising sketch control in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72047214, 2024. 2 [18] Naejin Kong, Yu-Wing Tai, and Sung Yong Shin. Highquality reflection separation using polarized images. IEEE Transactions on Image Processing, 20(12):33933405, 2011. 3 [19] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. 2 [20] Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan, Wenxiu Sun, and Qifeng Chen. Polarized reflection reIn Proceedings moval with perfect alignment in the wild. of the IEEE/CVF conference on computer vision and pattern recognition, pages 17501758, 2020. [21] Anat Levin, Assaf Zomet, and Yair Weiss. Separating reflections from single image using local features. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., pages II. IEEE, 2004. 3 [22] Rui Li, Simeng Qiu, Guangming Zang, and Wolfgang Heidrich. Reflection separation via multi-bounce polarization state tracing. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIII 16, pages 781796. Springer, 2020. 3 [23] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. 2 [24] Jiaying Lin, Guodong Wang, and Rynson W.H. Lau. Progressive mirror detection. In Proc. CVPR, 2020. 3 [12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video dif- [25] Jiaying Lin, Zebang He, and Rynson WH Lau. Rich context aggregation with reflection prior for glass surface detection. 20 In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1341513424, 2021. 3 on Computer Vision and Pattern Recognition, pages 7695 7704, 2024. 5 [26] Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, and Jing Liao. Pd-gan: Probabilistic diverse gan for In Proceedings of the IEEE/CVF conimage inpainting. ference on computer vision and pattern recognition, pages 93719381, 2021. 3 [27] Jiayue Liu, Xiao Tang, Freeman Cheng, Roy Yang, Zhihao Li, Jianzhuang Liu, Yi Huang, Jiaqi Lin, Shiyong Liu, Xiaofei Wu, et al. Mirrorgaussian: Reflecting 3d gaussians for reconstructing mirror reflections. arXiv preprint arXiv:2405.11921, 2024. 3 [28] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. [29] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. 9 [30] Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Hd-painter: high-resolution and prompt-faithful text-guided arXiv preprint image inpainting with diffusion models. arXiv:2312.14091, 2023. 3 [31] Haiyang Mei, Xin Yang, Yang Wang, Yuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng Wei, and Rynson WH Lau. Dont hit me! glass detection in realworld scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3687 3696, 2020. 3 [32] Haiyang Mei, Bo Dong, Wen Dong, Pieter Peers, Xin Yang, Qiang Zhang, and Xiaopeng Wei. Depth-aware mirror segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30443053, 2021. 3 [33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [34] Jiarui Meng, Haijie Li, Yanmin Wu, Qiankun Gao, Shuzhou Yang, Jian Zhang, and Siwei Ma. Mirror-3dgs: Incorporating mirror reflections into 3d gaussian splatting. arXiv preprint arXiv:2404.01168, 2024. [35] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36, 2024. 3 [36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [37] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In Proceedings of the IEEE/CVF Conference [38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 4, 6, 12 [42] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. Image-to-image diffusion models. [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [44] Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, David Forsyth, and Anand Bhattad. Shadows dont lie and lines cant bend! generative models dont know projective geometry... for now. arXiv preprint arXiv:2311.17138, 2023. 2 [45] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [47] Tianyu Sun, Guodong Zhang, Wenming Yang, Jing-Hao Xue, and Guijin Wang. Trosd: new rgb-d dataset for transparent and reflective object segmentation in practice. IEEE Transactions on Circuits and Systems for Video Technology, 33(10):57215733, 2023. [48] Jiaqi Tan, Weijie Lin, Angel Chang, and Manolis Savva. Mirror3D: Depth refinement for mirror surfaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3 21 [62] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 12 [63] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli Shechtman, Connelly Barnes, Jianming Zhang, Ning Xu, Sohrab Amirghodsi, and Jiebo Luo. Image inpainting with cascaded modulation gan and object-aware training. In European Conference on Computer Vision, pages 277296. Springer, 2022. 3 [64] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023. 2, 6 [49] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Gecco: Geometrically-conditioned point diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21282138, 2023. [50] Rishi Upadhyay, Howard Zhang, Yunhao Ba, Ethan Yang, Blake Gella, Sicheng Jiang, Alex Wong, and Achuta Kadambi. Enhancing diffusion models with 3d perspective geometry constraints. ACM Transactions on Graphics (TOG), 42(6):115, 2023. 2 [51] Tao Wang, Wanglong Lu, Kaihao Zhang, Wenhan Luo, TaeKyun Kim, Tong Lu, Hongdong Li, and Ming-Hsuan Yang. Promptrr: Diffusion models as prompt generators for single image reflection removal. arXiv preprint arXiv:2402.02374, 2024. 3 [52] Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, and Hua Huang. Single image reflection removal exploiting misIn Proaligned training data and network enhancements. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81788187, 2019. 3 [53] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. arXiv preprint arXiv:2403.18818, 2024. 2 [54] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 5, 6, 14, 15 [55] Xin Yang, Haiyang Mei, Ke Xu, Xiaopeng Wei, Baocai Yin, and Rynson W.H. Lau. Where is my mirror? In The IEEE International Conference on Computer Vision (ICCV), 2019. 3, [56] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arxiv:2308.06721, 2023. 2 [57] Junyi Zeng, Chong Bao, Rui Chen, Zilong Dong, Guofeng Zhang, Hujun Bao, and Zhaopeng Cui. Mirror-nerf: Learning neural radiance fields for mirrors with whitted-style ray tracing. In Proceedings of the 31st ACM International Conference on Multimedia, pages 46064615, 2023. 3 [58] Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, T. Jaakkola, and Shiyu Chang. Towards coherent image inpainting using denoising diffusion implicit models. In International Conference on Machine Learning, 2023. 2 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2 [60] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [61] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024."
        }
    ],
    "affiliations": [
        "Samsung & Institute India - Bangalore",
        "Vision and AI Lab, IISc Bangalore",
        "Visual Geometry Group, University of Oxford"
    ]
}