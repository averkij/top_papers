{
    "paper_title": "FocusedAD: Character-centric Movie Audio Description",
    "authors": [
        "Xiaojun Ye",
        "Chun Wang",
        "Yiren Song",
        "Sheng Zhou",
        "Liangcheng Li",
        "Jiajun Bu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 5 1 2 1 . 4 0 5 2 : r FocusedAD: Character-centric Movie Audio Description Xiaojun Ye Zhejiang University China yexiaojun@zju.edu.cn Sheng Zhou Zhejiang University China shengzhou_zju@zju.edu.cn Chun Wang Zhejiang University China zjuheadmaster@zju.edu.cn Liangcheng Li Zhejiang University China liangcheng_li@zju.edu.cn Yiren Song National University of Singapore Singapore yiren@nus.eud.sg Jiajun Bu Zhejiang University China bjj@zju.edu.cn Abstract Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, novel framework that delivers character-centric movie audio descriptions. It includes: (i) Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-evalNamed and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD. CCS Concepts Computing methodologies Computer vision tasks. Keywords Movie audio description, Multi-modal LLM"
        },
        {
            "title": "1 Introduction\nMovie Audio Description (AD) is a narration service that verbally\ndescribes visual content in video content that are not conveyed\nthrough the existing soundtrack [18]. AD is developed to primarily\nassist Blind and Visually Impaired (BVI) audiences in understanding\nmovie storylines [36]. Due to the prohibitive cost of employing\nskilled annotators for AD creation, the amount of AD-equipped\nvideos remains limited, highlighting the need for automatic AD\ngeneration systems. Recent advancements in multi-modal large\nlanguage models (MLLMs) for video understanding [15, 57] have\nmade automatic movie audio description generation feasible.",
            "content": "BVI audiences need to simultaneously process both AD and the original soundtrack including dialogues and ambient sounds, resulting in high cognitive load. Therefore, AD narratives must be concise Both authors contributed equally to this research. Corresponding author. Figure 1: FocusedAD: We propose an automated charactercentric AD generation model that emphasizes main character regions appearances and actions while incorporating narrative context. Characters appearing in the movie clip are annotated with colored bounding boxes. to prevent information overload. Due to the length constraints, AD needs to compress complex visual information into limited textual descriptions while preserving narrative crucial elements [39] such as characters description [23] and scene transitions [1]. Particularly in character-centric scenes, the appearance, actions, and expressions of individual characters, along with inter-character interactions and relationships, carry greater narrative significance than static scene arrangements and plot-irrelevant objects [51]. However, existing works [17, 18, 54] typically employ image encoders to extract key frame features followed by LLM for generating dense video captions, resulting in relatively smooth attention distribution across scene-level features [52]. This methodology struggles to achieve an optimal balance between descriptive detail and narrative coherence, often producing redundant object-centric (e.g., \"red chair, blue table\") rather than event-driven storytelling (e.g., \"the character anxiously surveys the room\"). Thus, it is essential for automated AD generation models to possess the ability to focus on narrative-salient regions, feature that current methods inadequately address. Furthermore, in order to facilitate the comprehension of the story for audiences, particularly individuals in the BVI, AD requires explicit character identification by name (e.g. \"Harry walks in\") [17]. This poses more stringent requirements on content description compared to conventional video captioning [42], which employs ambiguous expressions such as \"he/she\" or \"a man/woman\". Following MM 25, October 2731, 2025, Dublin, Ireland Xiaojun Ye, Chun Wang et al. character region detection, concurrent with description generation, the model ought to establish precise mappings between detected regions and character identities, thereby enabling effective character name references in the generated AD [54]. This demands enhanced capabilities in character tracking and identification across different frames, particularly challenging in multi-character scenarios. However, certain models [18, 54] rely on subtitles to extract character names, where the frequency of character name occurrences is substantially lower than AD requirements. Other models [17, 50] incorporate additional character recognition modules with inadequate performance due to feature shifts caused by variations in characters appearances and makeup. These approaches struggle to effectively identify characters associated with ongoing actions, resulting in semantic ambiguity and reduced narrative coherence. In this paper, we present FocusedAD, novel model that recognize effective main character and generate ad which focus on narratively relevant elements, as illustrated in fig. 1. To enable the generated AD to focus on narrative-salient regions, we first introduce Character Perception Module (CPM) to identify main characters in arbitrary frames of current movie clips. The module employs bi-directional propagation to obtain character-specific region lists throughout the movie clips. The detected character regions are subsequently fed into two distinct modules. In order to address the challenges of active character recognition, we introduce an automated clustering-based pipeline for optimal character bank query generation, applicable to both our training and testing sets. With the character best query bank, we effectively identify main characters by computing distances between detected faces and each characters best query, incorporating this information as visual priors into the Dynamic Prior Module (DPM), which dynamically adapts to scenes with varying numbers of characters. Finally, we introduce the Focused Caption Module (FCM), performing joint reasoning over scene visual tokens Tğ‘† , character tokens Tğ¶ , and text tokens Tğ‘Š to generate character-centric ADs, which is then evaluated against ground-truth ADs. Our approach addresses critical limitation of current MLLMs that tend to generate redundant content when processing video captioning tasks, resulting in movie ADs that more effectively facilitate story comprehension for BVI audiences. Our primary contributions can be summarized as follows: We propose FocusedAD, model that leverages both temporal context and character information to generate storycoherent and character-centric AD. Through the soft prompt mechanism, our model dynamically adapts to scenes containing varying numbers of characters. We develop an automated data construction pipeline for creating an optimal character query bank, effectively addressing the limitation of insufficient character information in both training and testing AD datasets. We show promising results on character-centric AD, as seen from both qualitative and quantitative evaluations, and also achieve impressive zero-shot state-of-the-art performance on both the MAD-eval-Named benchmark and our newly introduced Cinepile-AD test set."
        },
        {
            "title": "2 Related Work\n2.1 Video Caption\nVideo captioning is the task of automatically generating natural lan-\nguage descriptions that accurately describe the visual content and\ntemporal dynamics of video sequences [28, 33]. A closely related\ntask to AD is dense video captioning [22, 34, 49], which aims to\ngenerate multiple captions along with their corresponding tempo-\nral localization within the video. Recent methodologies [13, 16, 26]\naddress this task through the joint optimization of captioning and\nlocalization modules, facilitating enhanced inter-task interactions.\nHowever, dense video captioning typically describes discrete event\nsegments in isolation, lacking the narrative coherence across seg-\nments, which is essential for generating AD.",
            "content": "With the emergence of multi-modal Large Language Models (MLLMs) [4, 30], video caption has seen new approaches either through visual instruction tuning [24] or LLM prompting [27]. Although MLLMs demonstrate more robust adaptability across diverse scenarios and can address domain discrepancies through Supervised Fine-tuning (SFT), existing models still rely heavily on global scene level features, resulting in their inability to balance between fine-grained descriptions and semantically irrelevant redundant narratives. Different from previous work, we propose to leverage Character Perception Module and Focused Caption Module to generate character-centric caption."
        },
        {
            "title": "2.2 Audio Description\nAD offers the narration describing visual elements in movies, ex-\npected to weave a coherent narrative for storyline [54]. Early re-\nsearch in the AD field focused primarily on data collection and\npreprocessing [39, 44, 45]. However, existing AD-annotated movie\ndatasets suffer from various limitations such as restricted keyframe-\nonly content [21] or frame-level feature constraints [44]. Initial\nstudies of models concentrated on predicting AD insertion times-\ntamps based on audiovisual content [46] or scripts [12, 44]. Recent\nstudies [17, 18, 54] have explored the integration of visual fea-\ntures with LLMs for the description of visual information. However,\nthese automated AD generation pipelines still rely on global fea-\ntures, causing models to distribute attention across the entire scene,\nrather than focus on the details relevant to the story. Unlike previ-\nous works, our FocusedAD specifically focuses on narrative-salient\nregions and generates storyline-coherent descriptions.",
            "content": "Compared with general video captioning, AD is expected to incorporate effective character names into descriptions [10, 20]. Thus, character recognition serves as prerequisite for the task, with numerous works proposing automatic identification pipelines based on face recognition [11] or ReID [25, 47]. Current models are exploring methods to incorporate character names in generated text, either through Named Entity Recognition (NER) from subtitles [18] or face recognition based on actor portraits [17]. Departing from previous approaches, we propose method based on automated clustering to obtain character best query bank for more robust character identity features generation, effectively addressing the variations in character visual features caused by changes in lighting conditions and scene contexts. FocusedAD: Character-centric Movie Audio Description MM 25, October 2731, 2025, Dublin, Ireland Figure 2: Overview of FocusedAD: FocusedAD takes movie clips as input and captures the character best query bank through clustering. The Character Perception Module identifies main characters in key frames and bi-directionally propagates character regions across the entire key frame sequence. Then, through the Dynamic Prior Module, it dynamically integrates visual and text priors using soft prompts. Finally, the Focused Caption Module takes scene-level tokens, character-level tokens, and soft prompts as input to generate character-centric audio descriptions."
        },
        {
            "title": "3.1 Character Perception Module\nAs shown in fig. 3, given that narrative progression in audio de-\nscriptions derives from either scene transitions or main character\nactivities, we implement a dual-strategy approach. For scenes with-\nout characters, the model describes environmental changes, while",
            "content": "for movie clips containing characters, our goal is to identify active main characters in any frame of the input clips and output the frame indices where main characters are detected, guiding the model to focus on character actions and appearances. To this end, we (i) calculate character best queries by extracting character portraits from the movie, further calibrated by clustering algorithms, (ii) train character detection module that predicts the active characters given their best queries and the current movie clip ,and (iii) propagate the detected main character region in both directions in multiple key frames. Character Query Clustering. Actors portrait images can differ considerably in appearance from the character in the movie due to various factors, such as hairstyle, makeup, dress, ageing, or camera viewpoint [35]. To improve the accuracy of character recognition, we propose calibration mechanism to get the best character query. Initially, each movie contains ğ½ main characters, For the ğ‘—-th main character, we can build portrait collection ğ‘ƒ ğ‘— = {ğ‘ ğ‘— ğ‘› }, 1 where ğ‘› denotes the number of cropped portraits. These portraits are encoded into visual feature embedding ğ¸ ğ‘— = {ğ‘’ ğ‘— ğ‘› }. 1 Our goal is to obtain an optimal set of character representations ğ‘„ = {ğ‘1, ğ‘2, ..., ğ‘ğ½ }, where ğ‘ ğ‘— denotes the best query for the ğ‘—-th main character. , ..., ğ‘ ğ‘— , ..., ğ‘’ ğ‘— , ğ‘ ğ‘— 2 , ğ‘’ ğ‘— Our approach begins by quantifying the visual coherence within each character identity through intra-class distance measurement: ğ·int (ğ‘ ğ‘— ) = 1 ğ¸ ğ‘— ğ‘’ğ‘– ğ¸ ğ‘— ğ‘’ğ‘– ğ‘ ğ‘— (1) MM 25, October 2731, 2025, Dublin, Ireland Xiaojun Ye, Chun Wang et al. Subsequently, we evaluate character distinctiveness by calculating the inter-class distance: ğ·ext (ğ‘ ğ‘— ) = min ğ‘— ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— 2, (2) where ğ‘ ğ‘— denotes the best query for the ğ‘— -th character. Finally, We compute the centroid of each cluster as the best query list ğ‘„ by maximizing the following objective function: ğ‘“ (ğ‘„) = ğ‘š ğ‘—=1 (cid:18) ğ·ext (ğ‘ ğ‘— ) ğ·int (ğ‘ ğ‘— ) + ğœ– (cid:19) , (3) where ğœ– = 106 to prevent division by zero. predictions, as well as any newly detected character regions. The architecture consists of ğ¿ stacked transformer blocks, with the first block taking the image encoding of context frames as input. Each block sequentially performs self-attention, followed by crossattention to frame memories and character regions stored in the memory bank, and concludes with an MLP layer. Character Recognition. Given sequence of key frames = {I1, I2, ..., Iğ¾ } for the movie clip ğ‘¥ğ‘– , we perform main character recognition by iterating through each key frame. Specially, we process the video frame by frame using MTCNN [55] as our face detector to locate facial regions in each frame. For frame ğ‘˜ , assuming ğ¶ faces are detected, we obtain set of cropped face images {ğ‘¥ğ‘˜ ğ´}. Each detected face image is mapped to 1281 dimensional Euclidean space through the FaceNet [41]: , ..., ğ‘¥ğ‘˜ , ğ‘¥ğ‘˜ 2 ğ‘“ (ğ‘¥ğ‘˜ ğ‘ )2 = 1, ğ‘ ) R128, s.t. ğ‘“ (ğ‘¥ğ‘˜ (4) ğ‘ denotes the ğ‘-th face (anchor) detected in ğ‘¥ğ‘˜ -th key frame. where ğ‘¥ğ‘˜ The model is optimized using triplet loss to ensure embeddings from the same identity are close while different identities are separated in the embedding space. For each detected face embedding ğ‘“ (ğ‘¥ğ‘˜ ğ‘ ), we compute the squared ğ¿2 distance to each character best query: ğ· (ğ‘“ (ğ‘¥ğ‘˜ ğ‘ ), ğ‘ ğ‘— ) = ğ‘“ (ğ‘¥ğ‘˜ (5) We employ threshold ğ‘¢ to determine the corresponding charğ‘ ) ğ‘ ğ‘— 2 2 , ğ‘ ğ‘— ğ‘„ acter name ğ¶ğ‘˜ ğ‘ in the main character list for the current face ğ‘¦ğ‘˜ ğ‘ : ğ· (ğ‘“ (ğ‘¦ğ‘˜ ğ‘ ), ğ‘ ğ‘— ), arg min ğ‘— 0,...,ğ½ 1 Unknown, if min ğ‘— 0,...,ğ½ 1 ğ· (ğ‘“ (ğ‘¦ğ‘˜ ğ‘ ), ğ‘ ğ‘— ) < ğ‘¢ (6) otherwise ğ¶ğ‘˜ ğ‘ = Temporal Character Region Propagation. Our Temporal Character Region Propagation Module allows the character regions on any frame of the movie clip as input. After receiving initial region, the module propagates these regions to obtain the masklet of the character across the entire movie clip, localizing the segmentation mask of the target on every key frame. Specially, we firstly share the visual features extracted from an MAE [19] pre-trained Hiera [40] image encoder across all objects and global features within the movie clip. Then, we utilize the memory encoder [37] to generate memory by downsampling the output mask using convolutional module and summing it element-wise with the unconditioned frame embedding from the image encoder, followed by light-weight convolutional layers to fuse the information. Meanwhile, we maintain memory bank that keeps track of past predictions for the target character in the movie clip by maintaining FIFO queue of memories from up to ğ¾ context frames, and stores prompt information in FIFO queue of up to the frames which recognizes main characters. The memory attention operation takes the per-frame embedding from the image encoder and conditions it on the memory bank, before the mask decoder ingests it to form prediction. Finally, we employ memory attention to condition the current frame features on both historical frame features and Figure 3: Character Perception Module traverses the key frame sequence, detecting main characters in any frame and obtaining their segmented regions. Videos are processed in streaming fashion, where each frame cross-attends to the main character memories from context frames. Finally, both the region prediction and key frame embeddings are stored into memory bank."
        },
        {
            "title": "3.2 Dynamic Prior Module\nCompared to video caption where the annotation describes \"what is\nin the video\", movie AD describes the visual events in the scene that\nare relevant to the broader story, often centered around events, char-\nacters and the interactions between them. To tackle these temporal\ndependencies, we propose to include two components to incorpo-\nrate the essential contextual information from movies: (i)previous\nAD and subtitles, (ii)dynamic template prompt for both single and\nmulti main character scene.",
            "content": "Text Prior. Events occurring in the current movie clips are not temporally independent. Incorporating previous audio descriptions and subtitles as background context helps the model maintain narrative continuity. Specially, Our model takes the previous ADs ğ‘‡ğ‘¡ <ğ‘– and intervening subtitles ğ‘†ğ‘¡ ğ‘– to generate the AD ğ‘‡ğ‘– for the current clip. The past movie ADs and subtitles are few sentences, which are first concatenated into single paragraph, then tokenized and converted to sequence of word embeddings ğ‘Š . Dynamic instruction template. Given the number ğ´ of detected character regions in the ğ‘˜ key frame and the corresponding set of character names {ğ¶1, ğ¶2, ..., ğ¶ğ´ }, ğ´ ğ´, where ğ´ represents the number of main characters among the ğ´ detected faces. We define the dynamic prompt generation function ğ‘“ğ·ğ‘–ğ‘¡ as: ğ‘“ğ·ğ‘–ğ‘¡ (ğ´, {ğ¶ğ‘– }) = ğµ ğ¶1 + ğµ ğ‘€ + (cid:205)ğ´ ğ‘–=1 where ğµ denotes the base description prompt, ğ¶ğ‘– denotes the prompt template for the i-th character, and ğ‘€ denotes the prefix prompt for multi-character scenes. if ğ´ = 0 if ğ´ = 1 ğ¶ğ‘– + ğµ if ğ´ > 1 (7) , Soft prompt. While the visual encoders in MLLMs have been trained on large-scale video-text pairs, they may not be sufficient FocusedAD: Character-centric Movie Audio Description MM 25, October 2731, 2025, Dublin, Ireland to comprehensively extract all crucial visual information required for the complex task of movie audio description. It has been demonstrated that increasing the amount and quality of pretraining multimodal datasets can significantly improve the movie story understanding capability of MLLMs. An attractive alternative approach is to leverage existing AD datasets for descriptive style fine-tuning. In fig. 4, we present our Dynamic instruction template along with the soft prompt (a trainable vector) designed for scenes with varying numbers of characters. Our soft prompting method for instructions exhibits several distinctive properties. Unlike standard prompt tuning methods that serve specific tasks, it is designed to adaptively select valuable information from text prior. Our soft prompting approach guides the model toward AD-specific descriptive styles while incorporating contextual information in more flexible manner, effectively modeling the relationships between characters, scenes, and inter-character dynamics, including differential weighting of character descriptions based on their narrative significance. Figure 4: Instruction template with soft prompt. We use well-designed instruction template with trainable soft prompts to inject the text prior and visual prior into Focused Caption Module."
        },
        {
            "title": "3.3 Focused Caption Module\nFor the input movie clips, we extract a sequence of key frames ğ‘¥ğ‘– =\n{I1, I2, ..., Iğ¾ } based on a predetermined frame stride, accompanied\nby active main character regions. To generate character-level token\nrepresentations, we use the shared visual encoder to extract frame-\nlevel feature ğ¹ğ‘¥ğ‘– âˆˆ Rğ¾ Ã—ğ»ğ¼ Ã—ğ‘Šğ¼ Ã—ğ·ğ¼\n, where ğ»ğ¼ , ğ‘Šğ¼ , ğ·ğ¼ denote the\nheight, width and dimension of the image feature and ğ¾ represents\nthe number of key frames, respectively. Following [30], we employ\nan MLP as a visual connector to map global key frame visual features\nto interleaved scene visual tokens ğ‘†.",
            "content": "Each binary mask ğ‘€ of character obtained from Character Perception Module is then resized to match the shape of the image feature. We utilize the Mask Pooling operation upon image feature to extract character-level spatial feature ğ¹ğ¶ Rğ¾ ğ·ğ¼ for each mask, which pools all features within the region ğ‘€ to generate character-level representation. Finally, we employ MLP layer to adapt and produce the character-level token ğ¶ Rğ¾ ğ·ğ¶ for each character region. To aggregate distinct temporal character-level representations across multiple key frames over time duration while minimizing redundant tokens, we employ the Temporal Token Merge Module [52], which takes spatial object ğ¶ Rğ¾ ğ·ğ¶ tokens as input. We first compute the cosine similarity between each pair of adjacent tokens, formulated as: Sğ‘š,ğ‘š+1 = Cğ‘š Cğ‘š+1 Cğ‘š Cğ‘š+1 , 0 ğ‘š < ğ¾, (8) We predefine constant ğœ‡ = 4, then select the top ğ‘˜ ğœ‡ similarity scores from ğ‘†. Based on the selected similarity scores, we merge the corresponding ğ‘˜ ğœ‡ token pairs into ğœ‡ unions, where each union contains multiple highly correlated tokens. We perform average pooling on each union to generate ğœ‡ representative tokens, which are then processed through MLP layers to produce the final output ğ¶ Rğœ‡ğ·ğ¶ . Finally, the scene-level tokens Tğ‘† , character-level tokens Tğ¶ and text tokens Tğ‘Š are sent to the LLM to obtain the current AD ğ‘‡ğ‘– , formulated as ğ‘‡ğ‘– = Î¦(Tğ‘†, Tğ¶, Tğ‘Š ), where Î¦ denotes the LLM."
        },
        {
            "title": "4 Implementation Details\n4.1 Training Datasets\nStoryboard-v2. Our main objective is to generate audio descrip-\ntions for movie clips that focus on character-centric details. For this\ngoal, the model need to be trained on the AD datasets. However,\nexisting AD datasets have significant limitations. MAD [44] only\nprovides video data in the form of CLIP visual features in order to\navoid copyright restrictions. The narrations in Movie101 [53] lack\nevent-level sentence segmentation, which proves detrimental to\nthe training process. VPCD [11] annotates character body trajec-\ntories and features, lacking descriptions of character actions. The\nMCVD [14] focuses on single, explicit events but lacks contextual\ninformation.",
            "content": "To address these limitations, we design an automated pipeline based on the video generation dataset Storyboard20K [48], to construct Storyboard-v2 as our training dataset, illustrated in fig. 5. Storyboard20K comprises 150K keyframes sourced from movies in MovieNet [21] and LSMDC [39], with rich multi-modal annotations designed to model complex narrative structures in films. Our constructed Storyboard-v2 comprises 11,250 triplets, each containing no-dialogue movie clip, the corresponding best character query bank for that movie, and movie ad ground truth. Figure 5: Samples of Storyboard-v2. Our dataset involves three main part, i.e., (i)movie clips, (ii) character regions, (iii) movie audio description ground-truth Specifically, our automated AD collection pipeline consists of four stages: (i) Given that Storyboard20K only provides single keyframe per movie clip, making it challenging to directly apply to audio description model training, we initially extract complete MM 25, October 2731, 2025, Dublin, Ireland Xiaojun Ye, Chun Wang et al. movie clips from the full-length films based on the timestamps to obtain segments requiring audio description. (ii) To address the lack of character name-to-feature mapping information in existing datasets and the domain gap between IMDB actor images and character appearances that hinders audio description generation, we utilize Storyboard20Ks coarse-grained character annotations. Despite limitations (e.g., some character bounding boxes only encompass partial body regions and suffer from blurring issues), we first crop all character-annotated regions and employ FaceNet [41] to filter out non-facial regions. For the remaining facial images, we apply the clustering algorithm described in 3.1 to compute best queries that serve as character representations. These representative embeddings are then used for character recognition across all movie clips. fig. 6 demonstrates the clustering results for obtaining the best query from the movie Up in the Air. This approach effectively mitigates feature shifts caused by discrepancies between portraits and actual movie appearances (e.g., hairstyle, age, camera angles) and variations in actor appearance across different scenes. (iii) Additionally, based on the complete movie narrative arc, we collect previous audio descriptions to serve as contextual priors. (iv) We categorize AD annotations into three distinct types: Type 1: ADs with matching character names and visual regions, used to train the models focused region description capabilities; Type 2: ADs containing character names without corresponding visual features in video frames, used to train the models contextual feature utilization; Type 3: ADs without character mentions but containing main characters in keyframes, used to train the models dynamic character weighting through the soft prompt. Figure 6: The clustering results for obtaining the best query from the movie Harry Potter and the Deathly Hallows."
        },
        {
            "title": "4.2 Test Datasets\nMAD-eval-Named [18] is constructed from a quality-filtered set\nof 162 movies in the LSMDC dataset [39]. After excluding all films\npresent in both LSMDC training and test sets, MAD-eval-Named\nretained 10 completely independent movies as our evaluation subset.\nThis dataset preserves the original character name information\nprovided by LSMDC and utilizes non-anonymized AD annotations,",
            "content": "comprising video segments with an average duration of 3-5 seconds along with their corresponding audio descriptions. Cinepile-AD is derived from the visual description component of the CinePile benchmark [38] . CinePiles visual descriptions are human-generated narrations explaining critical visual elements in movie scenes, originally created for audio accessibility. These descriptions were aligned with video clips via ASR, ensuring temporal and contextual consistency. The Cinepile dataset is sourced from YouTube through systematic collection process. We first obtain source movie clips from YouTube and employ WhisperX [7] to transcribe subtitles from the complete audio track. The segments without dialogue are identified as narrative movie clips requiring audio description. We then extract the corresponding visual description annotations as ground truth based on the surrounding subtitle context. Additionally, we collect previous audio descriptions and subtitles as contextual information for each segment. Post-Processing. The character information for movies can be collected from online databases or review websites like IMDb4. In detail, for each movie in MAD-eval-Named and Cinepile-AD datasets, we download the top 5 to 8 cast information from IMDb including the actor names, their character role name, and the actor portrait image, and implement character query clustering following the approach detailed in 3.1. To ensure the model does not rely on acoustic information, we removed all audio content from the movie clips. Given the memory constraints of LLM, and as reported in [18] that the trend for the recurrent setting flattens when the context ADs are longer than 3 sentences, we collect three preceding audio descriptions along with interleaved subtitles as text prior for each segment in our dataset."
        },
        {
            "title": "4.4 Training and Inference Details\nOn the Storyboard-v2 datasets, we use a batch size of 8 sequences,\neach of which contains 16 consecutive character region-movie clips-\nAD triples from a movie. Overall that gives 8 Ã— 16 triples for every\nbatch. From each video clip, we employ a frame stride of 15 to\nextract keyframes.",
            "content": "The AdamW [31] is used as the optimizer and the cosine annealing scheduler [32] is used to adjust learning rate. The learning rate is 2105 with global batch size of 128 for one epoch. We conduct FocusedAD: Character-centric Movie Audio Description MM 25, October 2731, 2025, Dublin, Ireland training using 4 NVIDIA H20 GPUs. For text generation, greedy search and beam search are commonly used sampling methods. We stop the text generation when full stop mark is predicted, otherwise we limit the sequence length to 60 tokens. We use beam search with beam size of 5 and mainly report results by the top-1 beam-searched outputs."
        },
        {
            "title": "5.1 Evaluation Metrics\nClassic Metrics. To evaluate the quality of text compared with the\nground-truth, we use existing tool [43] to calculate classic caption-\ning metrics to compare the generated AD to the ground-truth AD,\nnamely SPICE [5] (S) ,METEOR [8] (M) and BertScore [56] (BertS).\nRedundancy-aware Metric. We propose a novel semantics-\nbased metric (R) to evaluate redundancy in generated descriptions.\nGiven a ground-truth annotation sequence A = [ğ‘¤ğ‘\n, . . . , ğ‘¤ğ‘\n, ğ‘¤ğ‘\nğ‘š]\n2\n1\nand a model-generated description B = [ğ‘¤ğ‘\n, . . . , ğ‘¤ğ‘\nğ‘›] for the\n1\nsame video clip (where ğ‘š and ğ‘› denote the respective lengths after\nencoding), our metric identifies redundant text segments in B by\ncomparing their semantic similarity to A. Specifically, if a word\nvector in B exhibits a similarity score below a predefined threshold\nğœƒ with all vectors in A, it is flagged as redundant. To ensure ro-\nbustness, our word vectors are derived from a rigorously validated\nvocabulary, excluding discourse markers and auxiliary words. This\napproach quantifies how concisely a model captures GT semantics.\nThe redundancy score is computed as follows:",
            "content": ", ğ‘¤ğ‘ 2 Exp. TP character bank SP Region BertS A1 A2 B1 B2 C1 C2 best query best query none actor best query best query 5.2 6. 2.6 4.8 6.4 7.4 7.6 7.6 6.7 7.2 7.9 8.1 54.7 55. 54.2 54.6 55.9 57.7 Table 1: Ablation studies for AD generation on the MADeval-Named. Performance is reported in terms of SPICE (S), METEOR (M) and BertScore (BertS). SP means soft prompt. TP means text prior. K-means clustering to generate the best query list for characters significantly enhances the accuracy of character recognition and consequently improves the quality of movie audio descriptions (C2 vs B2). This comparison also shows the necessity and effectiveness of our Character Perception Module. In fig. 7, we demonstrate the impact of varying thresholds in Character Recognition on character identification accuracy. We use the model C2 in table 1, and evaluated five candidate thresholds with increments of 0.1. Our experiments reveal that the model achieves optimal character recognition accuracy at threshold value of ğ‘¢ = 1.3. This optimal threshold likely reflects the common occurrence of profile faces in movies. Setting an excessively high threshold would misclassify main characters as other people, while threshold that is too low could lead to confusion between different main characters or misidentification of background characters as main characters. R(ğ‘¤ğ‘ ğ´) = 1 max ğ‘¤ğ‘ ğ´ 0, sim(ğ‘¤ğ‘, ğ‘¤ğ‘ ), if max sim(ğ‘¤ğ‘, ğ‘¤ğ‘ ) < ğœƒ otherwise R(ğµ ğ´) = 1 ğµvalid ğ‘¤ğ‘ ğµvalid R(ğ‘¤ğ‘ ğ´) (9) (10) ,where ğ‘¤ğ‘ and ğ‘¤ğ‘ denote word/token vectors from and B, respectively; ğœƒ is the similarity threshold; ğµvalid represents the number of valid (content-bearing) words in B."
        },
        {
            "title": "5.2 Audio Description on GT Movie Clips\nThis section focuses on the effectiveness of each proposed com-\nponent in the FocesedAD pipeline, based on the ground-truth AD\ntime movie clips, as shown in table 1.",
            "content": "Effect of Character Perception Module. By default, the model takes the predicted active characters in the scene from the Character Perception Module. From the comparison of rows B1, B2 and C2, we can draw the following observations: (i) injecting character names gives clear performance gain (C2 vs B1), highlighting the dependency of the AD task on character names; (ii) employing Figure 7: Ablation study on the film Les MisÃ©rables to evaluate changes in FocusedAD indicators under varying thresholds. This film is selected for its representative nature, as its metrics closely align with the average of MAD-eval-Named. Effect of Dynamic Prior Module. In the Dynamic Prior Module, we incorporate previous ADs and subtitles as context for the model (C1 vs C2), while introducing soft prompt mechanism that embeds both text and visual priors into the dynamic instruction template (A1 vs A2). The results show that AD generation can be further improved by combining these methods, demonstrating that our Dynamic Prior Module helps the model integrate previous contextual priors to flexibly focus on the content that needs to be described in the current segment when generating ADs. MM 25, October 2731, 2025, Dublin, Ireland Xiaojun Ye, Chun Wang et al. Figure 8: Qualitative results of our method. The top two movie clips demos are from MAD-eval-Named and the bottom two movie clips demos are from Cinepile-AD. The Character Perception Module can recognize active main characters and feed their names into the AD generation pipeline. For visualization purposes, we display the portrait images of characters that have the closest distance to the best query, but the model actually utilizes the best query features as input. Effect of Focused Caption Module. table 1 demonstrates the benefits of incorporating the Focused Caption Module (A2 vs C2). Our results indicate that generating rich character-level tokens through the object encoder and integrating them with scene-level visual representations and language embeddings effectively guides the model to allocate more attention to main character regions. This mechanism enables the model to focus predominantly on describing main characters appearances and actions, facilitating the generation of ADs that better focus on narrative-relevant regions. MM-Narrator [54], which relies on the closed-source GPT-4 [3] model and extensive in-context learning datasets. We next compare our model with state-of-the-art general video understanding MLLMs, including Qwen2.5-VL [6] and VideoLLaMA3 [9]. Despite their supervised fine-tuning on extensive videorelevant fine-grained datasets, these models primarily focus on scene-level features. In contrast, our FocusedAD emphasizes charactercentric details, generating predictions that better align with movie audio description preferences."
        },
        {
            "title": "5.3 Qualitative Results\nfig. 8 shows four qualitative examples. It shows that Character\nPerception Module is able to recognize active characters reason-\nably well, and Focused Caption Module can associate the active\ncharacters with the movie audio descriptions. Given that character\nnames appear in most AD ground truth, with 31% of movie clips\nshowing character faces at frame 0 and 26% showing faces in the\nmiddle frames, the ability to identify main characters at any frame\nand propagate this information throughout the movie clips is an\nessential capability for high-quality AD generation.",
            "content": "Models AutoAD-I MM-Narrator Qwen2.5-VL VideoLLaMA3 FocusedAD 4.4 5.2 3.2 4.1 7.4 MAD-eval-Named Cinepile-AD BertS BertS 7.4 6.7 4.9 7.5 8.1 24.2 - 50.5 54.6 57.7 - - 55.6 45.2 44.1 - - 3.5 2.9 15.4 - - 5.3 5.9 14.7 - - 51.2 52.9 64. - - 53.0 42.4 34.8 Table 2: Compared with other works on movie AD generation task on MAD-eval-Named and Cinepile-AD. denotes redundancy and is better when lower."
        },
        {
            "title": "6 Conclusion\nThis paper introduces FocusedAD, a novel framework for automatic\nmovie audio description generation that addresses the critical chal-\nlenges of character-centric narration and narrative-relevant detail\nselection. We propose a comprehensive system that integrates char-\nacter perception, dynamic prior information, and focused caption\ngeneration. Our approach propagates character regions across video",
            "content": "FocusedAD: Character-centric Movie Audio Description MM 25, October 2731, 2025, Dublin, Ireland sequences and generating descriptions that focus on story-critical elements. Additionally, we introduce an automated character query bank construction pipeline that resolves the character identification challenges present in existing AD datasets. The experimental results demonstrate that FocusedAD achieves state-of-the-art performance across multiple benchmarks, including impressive zero-shot results on MAD-eval-Named and our newly introduced Cinepile-AD dataset. This work represents significant step toward fully automated, high-quality movie audio description generation, bringing us closer to human-level performance in this challenging task. References [1] [n. d.]. American Council of the Blind. https://adp.acb.org/ [2] 2024. Qwen2 Technical Report. (2024). [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems 35 (2022), 2371623736. [5] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image caption evaluation. In Computer Vision ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14. Springer, 382398. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [7] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023. Whisperx: Time-accurate speech transcription of long-form audio. arXiv preprint arXiv:2303.00747 (2023). [8] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 6572. [9] Zesen Cheng Zhiqiang Hu Yuqian Yuan Guanzheng Chen Sicong Leng Yuming Jiang Hang Zhang Xin Li Peng Jin Wenqi Zhang Fan Wang Lidong Bing Deli Zhao Boqiang Zhang, Kehan Li. 2025. VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding. arXiv preprint arXiv:2501.13106 (2025). https://arxiv.org/abs/2501.13106 [10] Andrew Brown, Ernesto Coto, and Andrew Zisserman. 2021. Automated video labelling: Identifying faces by corroborative evidence. In 2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE, 7783. [11] Andrew Brown, Vicky Kalogeiton, and Andrew Zisserman. 2021. Face, body, voice: Video person-clustering with multiple modalities. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 31843194. [12] VirgÃ­nia Campos, Luiz MG GonÃ§alves, Wesnydy Ribeiro, Tiago MU AraÃºjo, ThaÃ­s Do Rego, Pedro HV Figueiredo, Suanny FS Vieira, Thiago FS Costa, Caio Moraes, Alexandre CS Cruz, et al. 2023. Machine generation of audio description for blind and visually impaired people. ACM Transactions on Accessible Computing 16, 2 (2023), 128. [13] Aman Chadha, Gurneet Arora, and Navpreet Kaloty. 2020. iPerceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering. arXiv preprint arXiv:2011.07735 (2020). [14] David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. 190200. [15] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. 2024. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems 37 (2024), 1947219495. [16] Shaoxiang Chen and Yu-Gang Jiang. 2021. Towards bridging event captioner and sentence localizer for weakly supervised dense event captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84258435. [17] Tengda Han, Max Bain, Arsha Nagrani, Gul Varol, Weidi Xie, and Andrew Zisserman. 2023. Autoad ii: The sequel-who, when, and what in movie audio description. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1364513655. [18] Tengda Han, Max Bain, Arsha Nagrani, GÃ¼l Varol, Weidi Xie, and Andrew Zisserman. 2023. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1893018940. [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1600016009. [20] Qingqiu Huang, Wentao Liu, and Dahua Lin. 2018. Person search in videos with one portrait through visual and temporal links. In Proceedings of the European conference on computer vision (ECCV). 425441. [21] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. 2020. Movienet: holistic dataset for movie understanding. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16. Springer, 709727. [22] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision. 706715. [23] Anna Kukleva, Makarand Tapaswi, and Ivan Laptev. 2020. Learning interactions and relationships between movie characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 98499858. [24] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 (2023). [25] Siyuan Li, Li Sun, and Qingli Li. 2023. Clip-reid: exploiting vision-language model for image re-identification without concrete text labels. In Proceedings of the AAAI conference on artificial intelligence, Vol. 37. 14051413. [26] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. 2018. Jointly localizing and describing events for dense video captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition. 74927500. [27] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al. 2023. Mm-vid: Advancing video understanding with gpt-4v (ision). arXiv preprint arXiv:2310.19773 (2023). [28] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2022. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1794917958. [29] Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 21172125. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. [31] Ilya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016). [32] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [33] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353 (2020). [34] Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, and Bohyung Han. 2019. Streamlined dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 65886597. [35] Arsha Nagrani and Andrew Zisserman. 2018. From benedict cumberbatch to sherlock holmes: Character identification in tv series without script. arXiv preprint arXiv:1801.10442 (2018). [36] Elisa Perego. 2016. Gains and losses of watching audio described films for sighted viewers. Target 28, 3 (2016), 424444. [37] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. 2024. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024). [38] Ruchit Rawal, Khalid Saifullah, Miquel FarrÃ©, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. 2024. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813 (2024). [39] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt Schiele. 2017. Movie description. International Journal of Computer Vision 123 (2017), 94120. [40] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. 2023. Hiera: hierarchical vision transformer without the bells-and-whistles. In International conference on machine learning. PMLR, 29441 29454. [41] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition. 815823. [42] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. 2022. End-to-end generative pretraining for multimodal video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 17959 17968. [43] Shikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. 2017. Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural MM 25, October 2731, 2025, Dublin, Ireland Xiaojun Ye, Chun Wang et al. Language Generation. CoRR abs/1706.09799 (2017). http://arxiv.org/abs/1706. 09799 [44] Mattia Soldan, Alejandro Pardo, Juan LeÃ³n AlcÃ¡zar, Fabian Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. 2022. Mad: scalable dataset for language grounding in videos from movie audio descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 50265035. [45] Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron Courville. 2015. Using descriptive video services to create large data source for video annotation research. arXiv preprint arXiv:1503.01070 (2015). [46] Yujia Wang, Wei Liang, Haikun Huang, Yongqi Zhang, Dingzeyu Li, and Lap-Fai Yu. 2021. Toward automatic audio description generation for accessible videos. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 112. [47] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang. 2017. Joint detection and identification feature learning for person search. In Proceedings of the IEEE conference on computer vision and pattern recognition. 34153424. [48] Jinheng Xie, Jiajun Feng, Zhaoxu Tian, Kevin Qinghong Lin, Yawen Huang, Xi Xia, Nanxu Gong, Xu Zuo, Jiaqi Yang, Yefeng Zheng, et al. 2024. Learning longform video prior via generative pre-training. arXiv preprint arXiv:2404.15909 (2024). [49] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi PontTuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. 2023. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10714 10726. [50] Xiaojun Ye, Junhao Chen, Xiang Li, Haidong Xin, Chao Li, Sheng Zhou, and Jiajun Bu. 2024. MMAD: Multi-modal Movie Audio Description. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 1141511428. [51] Youngjae Yu, Jiwan Chung, Heeseung Yun, Jongseok Kim, and Gunhee Kim. 2021. Transitional adaptation of pretrained models for visual storytelling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12658 12668. [52] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. 2024. Videorefer suite: Advancing spatial-temporal object understanding with video llm. arXiv preprint arXiv:2501.00599 (2024). [53] Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, and Qin Jin. 2023. Movie101: New Movie Understanding Benchmark. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 46694684. doi:10.18653/v1/2023.acl-long.257 [54] Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, ChungChing Lin, Zicheng Liu, and Lijuan Wang. 2024. Mm-narrator: Narrating longform videos with multimodal in-context learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1364713657. [55] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE signal processing letters 23, 10 (2016), 14991503. [56] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). [57] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. 2024. Streaming dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1824318252."
        }
    ],
    "affiliations": [
        "National University of Singapore Singapore",
        "Zhejiang University China"
    ]
}