{
    "paper_title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
    "authors": [
        "Yi Wan",
        "Jiuqi Wang",
        "Liam Li",
        "Jinsong Liu",
        "Ruihao Zhu",
        "Zheqing Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at https://github.com/Pokee-AI/PokeeResearchOSS."
        },
        {
            "title": "Start",
            "content": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold Yi Wan*, Jiuqi Wang*, Liam Li, Jinsong Liu, Ruihao Zhu, Zheqing Zhu"
        },
        {
            "title": "Pokee AI",
            "content": "October 22, 2025 Abstract Tool-augmented large language models (LLMs) are emerging as deep research agentssystems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, 7B-parameter deep research agent built under unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. chainof-thoughtdriven multi-call reasoning scaffold further enhances robustness through adaptive recovery from tool failures and self-verification on generated answers. At test time, multiple research threads are executed independently and synthesized to produce the best answer. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at https://github.com/Pokee-AI/PokeeResearchOSS. 5 2 0 2 1 2 ] A . [ 3 2 6 8 5 1 . 0 1 5 2 : r Figure 1: Performance on HLE, GAIA and BrowseComp among 7B-scale deep research models."
        },
        {
            "title": "1 Introduction",
            "content": "With their ability to use tools for complex, open-domain tasks, tool-augmented large language models (LLMs) are increasingly deployed as deep research agents, i.e., systems capable of gathering, reasoning over, and 1 Figure 2: Performance on 7 QA Benchmarks among 7B-scale deep research models. synthesizing evidence from diverse sources to provide well-grounded answers to sophisticated queries. typical deep research workflow requires an agent to decompose the users query, identify relevant external information, and synthesize it into coherent, verifiable response. Recent progress in deep research agents has been driven by two parallel directions: (1) the development of smaller, cost-efficient LLM backbones suitable for scalable deployment [SJM+25, JZY+25, SQG+25, GFX+25, ZFH+25, WZA+25]; and (2) the design of training recipes that interleave reasoning with external actions [TT25, Tea25, LJD+25]. Despite these advances, existing agents remain constrained by several key limitations. First, most training pipelines still depend on token-overlap metrics such as F1 or ROUGE [WZA+25, DCL+25, MHF+25, LZY+25], which correlate weakly with human judgments of usefulness, factual grounding, and instruction adherence. Moreover, current agents exhibit brittle tool-use behavior: single malformed function call, fetch error, or transient API failure can derail an entire trajectory, with little opportunity for self-correction [SWY+25, XHJ+25, SMPN25]. In addition, plausible research threads can miss critical information, producing misleading inferences and therefore incorrect answers, especially for challenging questions. To address these challenges, we introduce PokeeResearch-7B, 7-billion-parameter deep research agent developed under unified reinforcement learning framework optimized for reliability, human alignment, and practical deployment. Our contributions center on three core innovations: Human Value-Driven Training Pipeline: We enhance agent alignment with Reinforcement Learning from AI Feedback (RLAIF) instantiated with the REINFORCE Leave-One-Out (RLOO) algorithm [KvHW19, ACG+24]. RLAIF provides rich, LLM-based reward signals calibrated to human values, including factual accuracy, citation faithfulness, and instruction compliance. RLOO offers unbiased gradient estimation in an on-policy fashion for policy improvement. This algorithm distinguishes itself from algorithms that are only approximately on-policy and follow biased gradient direction, such as the PPO family of algorithms. Robust Reasoning Scaffold: To enhance the robustness of our deep research agent, we employ three complementary techniques. Self-correction: Rather than blindly executing potentially malformed tool calls, the agent proactively diagnoses errors and suggests corrections. Self-verification: The agent incorporates an answer verification step that filters out easily detectable false outputs. Research threads synthesis: To improve answer accuracy on challenging questions at test time, the agent launches multiple independent 2 research threads in parallel. These research threads are jointly analyzed and synthesized by the agent to produce the final answer. Together, these design choices yield deep research agent that (1) optimizes directly for human-salient dimensions of answer quality rather than surface overlap; and (2) maintains robustness across imperfect models through adaptive, verifiable reasoning. At 7B scale, PokeeResearch-7B demonstrates that carefully engineered reinforcement learning and reasoning scaffolds can produce research-grade agents that are both cost-efficient and resilient in open-domain settings. We show in 10 benchmarks that PokeeResearch-7B presents state-of-the-art performance among 7B-size deep research agents."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we give review of existing benchmarks for deep research and summarize recent progress in building deep research agents. Information Seeking Benchmarks. The development of sophisticated information retrieval agents has necessitated increasingly complex evaluation benchmarks. Traditional datasets such as Natural Questions [KPR+19] and TriviaQA [JCWZ17] primarily assess single-hop reasoning, which can often be addressed using parametric knowledge or simple query sequences. Their multi-step extensions, including HotpotQA [YQZ+18] and Musique [TBKS22], evaluate structured multi-hop reasoning but remain insufficiently challenging for advanced agents. Recent evaluation frameworks have evolved to present more nuanced challenges: GAIA [MFW+23] introduces real-world complexity requiring sophisticated reasoning chains, while WebWalkerQA [WYJ+25] emphasizes dynamic web navigation skills. The BrowseComp suite [WSP+25, ZLY+25] establishes standardized metrics for web browsing competency across multiple languages. At the frontier of evaluation difficulty, Humanitys Last Exam [PGH+25] presents multi-modal challenges that push the boundaries of current capabilities, with state-of-the-art agents achieving merely 25.4% success rates. Information Seeking Agents. Recent advances in large language modelsparticularly commercial agents such as DeepResearch [Ope25] and Grok-3 [x.a25]have driven significant progress in automated informationseeking agents. However, the proprietary nature of these models limits scientific understanding and hinders reproducibility. The open research community has responded with agents like WebDancer [WLF+25], WebThinker [LJD+25], and R1-Searcher [SJM+25], which leverage the ReAct reasoning framework [YZY+23] but exhibit performance limitations on demanding benchmarks. Training paradigms remain an active area of investigation: traditional supervised fine-tuning approaches often suffer from poor generalization in dynamic environments [ZZW+25, ZDZ+25], while reinforcement learning methodologies [JZY+25, SLW+25, ZFH+25, MHF+25, TWG+25] show promise for developing adaptive exploration strategies despite challenges in stability and sample efficiency. Recent innovations include StepSearchs process-supervised RL approach [WZA+25], Tool-stars hierarchical reward framework combining SFT initialization with multi-tool RL optimization [DCL+25], and WebSailors trajectory filtering methodology followed by reasoning-focused RL fine-tuning [LZY+25]."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first describe the deep research workflow of PokeeResearch-7B. Then, we describe the tools provided to the agent and the training pipeline. Finally, we provide our reward design for the training algorithm and test time design for our agent."
        },
        {
            "title": "3.1 Deep Research Workflow",
            "content": "The deep research workflow runs in research-verification cycles. Upon receiving research question from the user, the agent initiates research thread, where the agent alternates between research mode and verification mode, starting in research mode. Research Mode. The research mode is multi-turn process. In each turn, the agent produces textual response that either (1) issues tool call enclosed by the special tags <tool_call> and </tool_call> to gather necessary information to answer the question; or (2) provides an answer enclosed by <answer> and </answer>. If tool call is generated, the corresponding tool is executed and the tool call response 3 is returned to the agent. Then, the research process continues. We point out that as long as the tags <tool_call> and </tool_call> is produced, our agent does not simply terminate the research whenever tool call fails. Instead, the agent continues to experiment with new tool calls until either success or turn limit is reached. Finally, if an answer is generated, the agent transitions to verification mode. Verification Mode. In this mode, the agent examines the entire research thread to assess whether the generated answer is correct. If the verification succeeds, the thread ends. Otherwise, the agent re-enters research mode to continue refinement. This researchverification cycle repeats until the context limit is reached or the verification approves the answer. Unlike most prior works, where the research thread concludes with answer generation, our framework introduces the additional answer verification step in which the agent evaluates its own answer. This step can effectively prevent several observed common failures: (1) the agents answer does not fully address all aspects of the question; (2) the agent produces an answer despite insufficient supporting evidence; and/or (3) the agents reasoning contains simple logical errors that could be easily detected. We hypothesize that such failure modes can be identified during self-verification, owing to the generationverification gap of large language models [SZE+24]."
        },
        {
            "title": "3.2 Tools and Training Pipeline",
            "content": "Tools. In our design, we enable the agent to interact with the web to efficiently gather information by equipping it with set of specialized tools. Here, each tool targets distinct functionalities in the research process, from initial discovery to relevant information extraction. Web Searching Tool: We provide Serper [Ser25] to our agent to find information over the Internet. This tool takes list of string queries as input and searches for information related to it through Google. Afterwards, the tool returns structured list of results, including URLs and descriptive snippets, for each query. This allows the agent to survey the landscape of available information and constantly identify promising sources for further investigation; Web Reading Tool: We also provide Jina Reader [Jin25] to our agent to allow it to read through the search results. This tool takes list of URLs as input and returns concise summary of the primary content from each corresponding webpage. These summaries provide quick and high-level understanding of webpage, helping the agent determine if deeper analysis of the page is warranted. Training Data and Algorithm. In our training pipeline, we adopt the MiroRL-GenQA dataset [TT25], which features complicated question-answer pairs. These questions are non-trivial and require multi-turn research to address. For fair experiment setup, no training data from the benchmarks is used in testing. To train our agent, we use the RLOO algorithm [KvHW19, ACG+24]. This is true on-policy algorithm and the parameter update follows an unbiased policy gradient estimate. It achieves lower variance compared to the classic REINFORCE algorithm by drawing more than one samples from the same start state. Formally, in our problem setting, given the current policy π parameterized by θ, for prompt x, the algorithm samples i.i.d. completions {y(i)}k i=1 πθ( x) and obtain their rewards R(x, y). For each sample, form leave-one-out baseline [KvHW19]: bi = 1 1 R(cid:0)x, y(j)(cid:1). j=i Then compute the advantage: Ai = R(cid:0)x, y(i)(cid:1) bi. RLOO updates policy parameters θ following θ θ + α 1 i=1 Ai log πθ (cid:0)y(i) x(cid:1), Pk (cid:0)y(i) x(cid:1). where α is step size parameter. This update follows the direction of an unbiased policy gradient estimate 1 i=1 Ai log πθ In words, RLOO operates by sampling multiple completions per prompt from the current policy and using leave-one-out baseline formed from the other samples. This centering removes any effect of prompt-level constant shift in rewards, yielding an unbiased gradient estimate with lower variance[ACG+24]. To align with the special workflow of our agent outlined in Section 3.1, during training, the agent receives large positive reward (see Section 3.3 for more details) if the agent generates an answer in the research thread and the answer is correct; if the agents response follows the right format, the agent receives small positive reward; otherwise, it receives reward of zero. The agents goal is to maximize the reward across all user questions. RLOO can be related to the well-known Group Relative Policy Optimization (GRPO) algorithm [SWZ+24]. Like RLOO, GRPO also samples multiple completions per prompt and obtains advantages by subtracting rewards by the mean of rewards of completions (GRPO subtracts rewards by the mean of rewards of ALL completions, introducing small bias in the policy gradient estimate, which should not be concern as the bias is multiplicative). Unlike RLOO, GRPO 1) uses surrogate advantage by dividing the original advantages by the standard deviation of the rewards, and 2) applies PPO-style update. The standard deviation normalization rescales updates based on within-group dispersion, making the parameter update biased one; moreover, GRPO employs multiple updates per batch and applies clipping and KL regularization as in PPO, making it only approximately on-policy. In our experiments, we found that the standard deviation normalization often results in performance collapsing when training for long enough time. We also observed that RLOO demonstrates faster learning progress, possibly due to its true on-policy nature. Figure 3: Illustration of the advantages of AI feedback over traditional lexical metrics."
        },
        {
            "title": "3.3 Reward Design",
            "content": "Our reward design mainly focuses on answer correctness, which can be measured through three complementary evaluation approaches: F1 score, Exact Match (EM), and AI feedback. Each metric provides distinct signals on the quality of an answer, and we explore their relative merits in our empirical analysis. F1 Score: The F1 score measures the harmonic mean of precision and recall between the set of tokens in the generated answer and the ground truth (see Section for formal definition). This token-level metric provides nuanced assessment of content overlap, rewarding answers that are substantially correct even if they are not lexically identical to the ground truth. The F1-based reward is denoted as RF1. Exact Match (EM): The Exact Match metric is stricter binary evaluation that awards score of 1 if the normalized predicted answer is identical to any of the provided ground-truth answers, and 0 otherwise. Although less flexible than F1, it serves as clear indicator of complete accuracy. We denote this reward as REM. AI Feedback: To address the limitations of purely lexical metrics (see Remark 1 below), we introduce an AI feedback approach. Here, we leverage an external LLM as an impartial judge to assess whether the agents answer is semantically equivalent to the ground truth. The judge model is provided with both the ground truth and the agents answer and returns binary judgment indicating semantic correctness. This approach captures nuanced semantic information that token-level metrics may miss. We denote the AI reward as RAI. 5 In our experiments, we adopt AI feedback plus small format reward as our reward signal during training. The AI feedback directly aligns with our ultimate goal of producing semantically correct answers and the format reward helps adherence to the required output format. Remark 1 (On the Advantages of AI Feedback) Although lexical metrics such as F1 score and EM provide valuable signals, they exhibit inherent limitations that motivate our emphasis on AI feedback. The token level F1 score, despite its flexibility, can produce misleadingly high scores for factually incorrect answers that share substantial token overlap with the ground truth. For instance, as shown on the left-hand side of Figure 3, the two answers differ by only one word, but the agents answer would be factually incorrect if the query asks for the birthday of the object. In contrast, EM suffers from excessive strictness, penalizing semantically correct answers that are not identical to the ground truth. In the middle of Figure 3, EM would judge the agents answer as incorrect because it differs from the ground truth, despite the additional information is accurate. The AI-based approach addresses these limitations by directly optimizing for semantic correctness, which is our ultimate objective. This alignment between the reward signal and the desired outcome mitigates reward hacking behaviors and provides more reliable training signals for complex reasoning tasks."
        },
        {
            "title": "3.4 Test Time Design",
            "content": "In our preliminary experiments, we observed that running multiple independent research threads can yield divergent answers, particularly for challenging questions. In such cases, the correct answer typically appeared in only one or two threads, if at all. Interestingly, the threads leading to incorrect answers often seemed reasonable when viewed in isolation but became less convincing when compared against those producing the correct answer. This discrepancy can arise, for example, when thread misses key informationleading the agent to form plausible yet incomplete inferencewhile another thread happens to retrieve the missing information. Motivated by this observation, we hypothesize that introducing synthesis stage to evaluate and compare multiple research threads can enhance overall accuracy. Specifically, for each question, the agent launches independent research threads in parallel. Each thread is subsequently summarized by our model to preserve only the essential findings and reasoning steps. These summaries are then jointly analyzed by our model, which synthesizes the evidence and determines the final answer. We refer to this approach as Research Threads Synthesis (RTS)."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we describe our empirical analysis in detail."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Benchmarks. In our experiments, we test the performance of PokeeResearch-7B in the following popular benchmarks. Due to the limitations of the base model we choose, we only evaluate on the text-only versions of the following benchmarks. Natural Questions (NQ) [KPR+19]: large-scale question-answering dataset derived from real Google search queries, testing the agents ability to answer factoid questions using Wikipedia articles. TriviaQA [JCWZ17]: reading comprehension dataset containing trivia questions paired with evidence documents, evaluating the agents capacity to locate and extract relevant information from web sources. PopQA: dataset focused on questions about popular entities and topics, assessing the agents performance on queries requiring up-to-date knowledge from current web content. HotpotQA [YQZ+18]: multi-hop reasoning dataset requiring the agent to gather and synthesize information from multiple documents to answer complex questions. 2WikiMultiHopQA [HNSA20]: challenging benchmark designed specifically for multi-hop reasoning over Wikipedia, where answering requires connecting information across multiple articles. Musique [TBKS22]: multi-hop question-answering benchmark that tests compositional reasoning abilities, requiring the agent to perform sequential information gathering and inference steps. 6 Bamboogle (BAMB) [PZM+22]: dataset containing questions that cannot be answered using the models parametric knowledge alone, necessitating active web search and information retrieval. GAIA [MFW+23]: benchmark presenting real-world complexity with sophisticated reasoning chains, evaluating the agents ability to handle realistic, challenging research tasks. BrowseComp [WSP+25]: standardized evaluation suite for web browsing competency, testing the agents ability to navigate and extract information from dynamic web pages across multiple languages. Humans Last Exam [PGH+25] comprehensive benchmark assessing an agents general reasoning, factual recall, and multi-domain understanding, serving as holistic test of advanced language and reasoning capabilities. Baselines. We compare our approach against several recent deep research agents: R1-Searcher [SJM+25], Search-R1 [JZY+25], ZeroSearch [SQG+25], ASearcher [GFX+25], and DeepResearcher [ZFH+25]. These agents employ various combinations of web searching, reading, and browsing tools. All baselines use Qwen2.57B (Base or Instruct)1 as their backbone model, matching ours for fair comparison. We use Serper for search across all methods, and ASearcher additionally uses Jina for webpage reading, consistent with our approach. We note that other deep research works, such as [LLZ+25, LJD+25], employ larger models or longer context lengths. We exclude them from consideration to ensure fair comparison. Training Details. We use Qwen2.5-7B-Instruct [Tea24] as our backbone model, which is powerful opensource LLM that has been instruction-tuned on large corpus of human-generated data. The model is fine-tuned using the RLOO algorithm. We fine-tine the backbone model for 140 steps. For each step, the model is trained using batch of 64 questions and the number of independent research threads used by the RLOO algorithm is 8. We set the maximum number of interaction turns to 100, allowing the agent to iteratively refine its research process. The learning rate is 3e-6. The context limit is 32768. No entropy or KL regularization is applied. Evaluation Details. We construct our test dataset by randomly sampling 125 text-only questions from each benchmark, except for GAIA, which contains only 103 text-only questions. This results in total of 1,228 questions. For each question and each agent (except for our agent with RTS) under evaluation, we run four research threads independently, with the context limit being 32768. If research thread produces an answer, we submit both the generated answer and the corresponding ground-truth answer to Gemini-Flash-2.5-Lite model, which judges whether the generated answer is correct. Formally, the accuracy for given questionagent pair is defined as: accuracy . = # research threads where the answer is correct 4 We report the mean accuracy across all questions within each benchmark (mean@4). For our agent with RTS, we perform four independent runs, each consisting of four independent research threads. The final answer in each run is derived by synthesizing the outputs of the four threads. This answer is then sent to the judge, and accuracy is calculated in the same manner."
        },
        {
            "title": "4.2 Results and Analysis",
            "content": "We compare our deep research agent, with and without RTS, against the baselines. For RTS, we experimented with four research threads. Table 1 summarizes the results. Both versions of our agent outperform all baselines across all benchmarks (mean@4). The improvement from RTS is especially notable on challenging benchmarks such as HLE, GAIA, and BrowseComp, while its effect is minor on other benchmarks, which are relatively simpler. Upon inspection, we also find that our self-verification is helpful in improving the answer correctness. Below, we provide an excerpt on how it helps PokeeResearch-7B to correct an initially inaccurate response. In this example, after finding the initial answer, the agent enters the verification mode, and determines the initial answer is incorrect based on the retrieved information and its own judgment. Afterwards, it re-enters the research mode, and manages to find out the correct one with additional tool calls. The complete log of this is detailed in Section of the appendix. 1When agents report results for both versions, we use the better-performing one. 7 Method R1searcher SearchR1 ZeroSearch ASearcher DeepResearcher PokeeResearch PokeeResearch-RTS"
        },
        {
            "title": "HLE GAIA BrowseComp",
            "content": "5.4 13.0 8.6 13.8 6.0 15.2 17.6 8.3 18.7 9.9 22.1 24.03 36.9 41.3 1.0 0.4 1.4 3.2 1.8 5.4 8.4 Method R1searcher SearchR1 ZeroSearch ASearcher DeepResearcher PokeeResearch PokeeResearch-RTS BAMB 2WIKI TQ NQ POPQA MUSIQUE HOTPOTQA 63.2 67.8 51.4 68.8 71.0 74.5 75. 61.4 62.8 33.6 69.2 58.8 74.0 75.0 77.2 81.0 61.6 85.2 82.2 91.3 91.8 59.6 67.6 48.2 71.2 60.2 75.1 75.0 51.8 59.6 38.0 58.2 55.2 59.8 60.0 35.8 33.2 19.0 35.8 26.8 39.8 41.4 62.4 63.2 32.4 71.0 56.6 71.2 71. Table 1: Performance comparison of deep research agents built on open-sourced 7B models across prevalent benchmarks."
        },
        {
            "title": "Question",
            "content": "In one Walter t Waverley e what was The Heart i t n ?"
        },
        {
            "title": "Response",
            "content": "i from and i e r . \" web_search \" , [ \" The Heart i t n \" arguments \" : { \" r _ t \" : \" Walter t Waverley e The Heart need i out which e <think> The Heart i t n c a r s i n one Walter t Waverley e . n c e </think> <tool_call> { \" name \" : Walter t Waverley e \" , l i h c \" ] } } </tool_call> r <tool_response> . . . . . . <answer> In Walter t Waverley e M o a f p London e r l pardon h newborn baby . </answer> r You have v d an answer . h Old Tolbooth s n Edinburgh , and n l s , who c e murdering r e r o young woman named n Deans from Edinburgh \" The Heart i t n , \" The Heart l . . . . . . ##The i a n mode r ##. i n <verification> need e y n q t . i e l The s n asked about what \" The Heart i t n \" was one Walter t Waverley e . The answer v d o t about c i n t o t answer 8 \" The Heart i t n \" t and s i a o a have a s e a and e s e l . However , o not l t t i e l f e e . The answer \" The Heart i t n \" h t f u h v , t Old Tolbooth s n Edinburgh . </verification> <verification_result> INCORRECT </verification_result> r The answer v f t mode and ree r e i o be o c . a n p t e d k from r a mode . ##The e h mode r ##. i n <think> . . . . . . <answer> In Walter t Waverley e t Old Tolbooth s n Edinburgh , which was c i county i t n i h motion by P e R s j n f young woman named n Deans from Edinburgh London e r l pardon h o d d by some be t s work . </answer> r You have v d an answer . t , who c e murdering newborn baby . The e e t f \" The Heart i t n , \" Edinburgh and l time . The e o t e f t e i s e answer ##The i a n mode r ##. i n <verification> need e y n q t . i e l The s n asked about what \" The Heart i t n \" was one Walter t Waverley e . The answer v d t i h author r c e on work . The answer r l e f t e and v s </verification> <verification_result> \" The Heart i t n \" n c e , n l and gave t about o t about c i n t o . e t l i e t , and CORRECT </verification_result>"
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced PokeeResearch-7B, 7-billion-parameter deep research agent built under unified reinforcement learning framework to advance the reliability, alignment, and scalability of tool-augmented LLMs. By integrating Reinforcement Learning from AI Feedback (RLAIF) with RLOO, PokeeResearch-7B optimizes directly for human-salient dimensions such as factual accuracy, citation faithfulness, and instruction adherence. Complemented by chain-of-thoughtdriven multi-call reasoning scaffold, the model demonstrates robust and adaptive tool use, enabling it to diagnose, recover from, and mitigate common failure modes in dynamic research environments. Empirically, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents across ten open-domain benchmarks, validating the effectiveness of its design in both reasoning quality and operational resilience. More broadly, our results highlight that reliability and alignmentnot just scaleare central to progress in research-grade AI agents. We believe PokeeResearch-7B represents step toward new generation of autonomous, verifiable, and human-aligned research agents, and we hope its principles inspire future work in scalable, self-correcting LLM-based reasoning."
        },
        {
            "title": "References",
            "content": "[ACG+24] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [DCL+25] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410, 2025. [GFX+25] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl, 2025. [HNSA20] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. [JCWZ17] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [Jin25] Jina AI. Jina Reader. https://jina.ai/, 2025. [JZY+25] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [KPR+19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [KvHW19] Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get baseline for free! ICLR 2019 Workshop on Deep RL Meets Structured Prediction, 2019. [LJD+25] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025. [LLZ+25] Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501, 2025. [LZY+25] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. [MFW+23] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [MHF+25] Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai, Xing Gao, Yu Yang, et al. o2-searcher: searching-based agent model for open-domain open-ended question answering. arXiv preprint arXiv:2505.16582, 2025. [Ope25] OpenAI."
        },
        {
            "title": "Deep",
            "content": "research system card. https://cdn.openai.com/ deep-research-system-card.pdf, 2025. Accessed: 2025-08-03. [PGH+25] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [PZM+22] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. [Ser25] Serper.dev. Serper - the worlds fastest & cheapest google search api. https://serper.dev/, 2025. [SJM+25] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [SLW+25] Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. Search and refine during think: Autonomous retrieval-augmented reasoning of llms. arXiv preprint arXiv:2505.11277, 2025. [SMPN25] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. [SQG+25] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. [SWY+25] Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, and Yurui Qiu. Failure makes the agent stronger: Enhancing accuracy through structured reflection for reliable tool interactions. arXiv preprint arXiv:2509.18847, 2025. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [SZE+24] Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities of large language models, 12 2024. [TBKS22] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. [Tea24] Qwen Team. Qwen2.5: party of foundation models, September 2024. [Tea25] Tongyi DeepResearch Team. Tongyi deepresearch: new era of open-source ai researchers. https://github.com/Alibaba-NLP/DeepResearch, 2025. [TT25] MiroMind Foundation Model Team and MiroMind AI Infra Team. Mirorl: An mcp-first reinforcement learning framework for deep research agent. https://github.com/MiroMindAI/ MiroRL, 2025. [TWG+25] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric video reasoning. arXiv preprint arXiv:2506.13654, 2025. [WLF+25] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025. [WSP+25] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [WYJ+25] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. 11 [WZA+25] Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint arXiv:2505.15107, 2025. [x.a25] x.ai. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, 2025. Accessed: 2025-08-03. [XHJ+25] Qian Xiong, Yuekai Huang, Ziyou Jiang, Zhiyuan Chang, Yujia Zheng, Tianhao Li, and Mingyang Li. Butterfly effects in toolchains: comprehensive analysis of failed parameter filling in llm tool-agent systems. arXiv preprint arXiv:2507.15296, 2025. [YQZ+18] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [YZY+23] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [ZDZ+25] Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. [ZFH+25] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [ZLY+25] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. [ZZW+25] Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, et al. Evolvesearch: An iterative self-evolving search agent. arXiv preprint arXiv:2505.22501, 2025."
        },
        {
            "title": "A Definition of Metrics",
            "content": "For completeness, we include the definition of the F1 score and EM here. F1 Score: The F1 score measures the harmonic mean of precision and recall between the set of tokens in the generated answer and the ground truth. Before comparison, both texts are normalized by converting to lowercase and removing punctuation. Let and denote the sets of tokens of the generated answer and . the ground truth, respectively. Define = C/G, and recall is defined as . = C/T . We compute the word-level F1 score as follows, . = . Then, precision is defined as F1(G, ) . = 2P + . (1) This approach provides nuanced assessment of content overlap, rewarding answers that are substantially correct even if they are not lexically identical to the ground truth. Exact Match: The Exact Match (EM) reward is stricter evaluation metric. This binary measure awards score of 1 if the normalized predicted answer is identical to any of the ground-truth answers, and 0 otherwise. Although less flexible than the F1 score, it serves as clear indicator of complete accuracy. Complete Log for the Example of Self-Verification Below is the complete log of the example provided in Section 4."
        },
        {
            "title": "Question",
            "content": "In one Walter t Waverley e what was The Heart i t n ? s t e [ { \" \" : \" l \" : \" l \" : themes from and i e r . t l s \" web_search \" , \" c t \" : was g l e r t e \" , t , a y , and [ \" The Heart i t n \" The Heart MidL i \" \" The Heart i t n \" \" arguments \" : { \" r _ t \" : \" The Heart i t n need i out which e \" p : / /www. c . com/ e h a t c o by Walter two t , n and i Deans . \" } , \" Walter t Waverley e The Heart h v h i Walter t Waverley e . \" p : / / en . i i . / i / The_Heart_of_Midlothian \" , \" The Heart i t n Walter t Waverley e \" , \" The Heart MidL i : t , Walter , Anderson , Ta ylo \" , <think> The Heart i t n c a r s i n one Walter t Waverley e . n c e </think> <tool_call> { \" name \" : Walter t Waverley e \" , l i h c \" ] } } </tool_call> r <tool_response> [ { \" query \" : \" r _ u \" : r s / e u andw i / r d h s w e t \" , \" The Heart i t n by Walter t \" c t \" : t , l e 1 8 1 8 , i o t through s y { \" \" : \" The Heart i t n Wikipedia \" , u s i u volumes on 25 y 1 8 1 8 , under \" p : / /www. amazon . com/ HeartMid LothianS WalterS t /dp / 1 9 7 9 2 4 8 0 4 4 \" , \" l \" : \" c t \" : Waverley e . under \" p : / /www. d d . com/ book /show / 6 9 9 4 8 4 . The_Heart_of_Mid_Lothian \" , \" l \" : \" c t \" : n l m z i r k s . . . \" } , { \" \" : Edinburgh mob h www. amazon . com/ HeartM o a WS t /dp/ B00110579C \" , l i : Walter t : Amazon . com : Books \" , t v . The e r e { \" \" : \" p : / /www. l . com/ k r / h / w e o / l / theh t m o a thew r ? l =AfmBOopTUfsTEV6D3PpUO8XJpywm WfZpiZl9prOSAMhfpkTtQjzGQQD \" , by Walter t Heart i t n & Woodstock . by t , Walter . Used ; Good ; Hardcover . d o : Good ; t : i r . . . \" } , { \" \" : \" p : / /www. ebay . com/ itm / 1 2 3 0 9 6 0 4 5 7 8 9 \" , l i l t e Waverley . . . \" , l i . Author : Walter t . Bound an l 20 th t h \" p : / /www. epubbooks . com/ book/825 theh t midl i \" , Heart i t n by Walter t epubBooks \" , l J i o i d r a n a e n o both r v . . . \" } , { \" \" : r / l / r d h / h / t s t / s i n / \" , u a by MM. r : Very good . e d ! \" } , { \" \" : \" The Heart Of l i The Waverley \" The Waverley e \" SIR WALTER SCOTT Heart Of \" l : The Heart Of was g l u s i u volumes on 25 y 1 8 1 8 , \" The Heart i t n \" was u 1 8 1 8 , one n o n e l i with England , \" The Heart MidL i by Walter t e e o r Walter t \" p : / /www. abebooks . com/ book \" l \" : \" c t \" : e Waverley . . . \" The e f . At t meted out by s e a i e b i r n , e e Deans , Edinburgh \" c t \" : \" c t \" : \" c t \" : . . . \" } , { \" \" : . . . \" } , { \" \" : e Tolbooth , Goodreads \" , h 1730 ( 1 8 m ) \" , \" Walter \" p : / / s \" l \" : \" l \" : \" l \" : \" l \" : t : a r : The l \" The \" \" } , \" The Heart Of 13 f i l c , h h . . . \" } , \" l \" : \" l \" : \" l \" : \" p : / / \" } , { \" \" : \" } , { \" \" : \" n Deans \" c t \" : \" c t \" : \" c t \" : was g l i . \" } ] } , { \" query \" : \" The Heart i t n s n n Waverley Edinburgh e S Walter t v The Heart i t n s b h \" The Heart i t n by Walter \" n a a t u00b7 David , d h s u00b7 Reuben , \" The Heart i t n : \" p : / / e i s . / \" p : / / en . i i . / i / \" The Heart i t n Wikipedia \" , e i Everyman b y t \" p : / / en . i i . / i / Jeanie_Deans \" , \" r e u00b7 Captain Porteous , i o \" p : / /www. n n b . com/w/ theh t h v h i Walter t Waverley e . was renowned an example an honest , i , [ { \" \" : \" n Deans Wikipedia \" , , n b o d u00b7 r Robertson ( George Staunton ) e h r s \" , \" c t \" : , dairyman u00b7 n and i \" Heart l i by Walter t , s i n AbeBooks \" , \" s i n Thus . An undated , [ . 1 9 0 6 ] i Walter t s i n l , i . l i w e o / 1 1 2 2 9 4 7 8 7 3 \" , Complete by Walter t , Paperback \" , u s i u volumes on 25 y 1 8 1 8 , under \" Walter t Waverley e The Heart i t n r e \" , \" r _ u \" : \" l \" : r e 1 8 1 8 . She was one c most e t h c s i h 19 th t ; i u r . The_Heart_of_Midlothian \" , \" c t \" : y guard u00b7 David Deans , i f e u00b7 n Deans , l , o r g r u00b7 i Deans , { \" \" : www. c . com/ e h a r / e u andw i / r l i s w e o \" , t Deans l . . . \" } , { \" \" : 2025/05/06/ l s s w e o theh t i t n / \" , \" l s i Walter t The Heart i t n \" , young i r named Reuben l , t o { \" \" : andw i / r d h a y majorc a r \" , i t n : l s Major r e EBSCO\" , Heart i t n \" by Walter t a t n g n e i and moral l g f l 18 thc u c a . \" } , { \" \" : www. t c . . ed . ac . uk/ works / e / l i . html \" , Heart i t n ( e My Landlord ) \" , \" c t \" : o , n Deans , was Helen Walker , who had r e l \" p : / /www. t . com/ i / r d h / r e \" , Heart i t n r e eNotes . com \" , \" c t \" : c r h c , emerges moral compass . p b g c e p n and p , \" p : / /www. k s . com/ r i h t i t n / r e . html \" , \" l \" : \" As i e t o o p a , n Deans n c { \" \" : l i by Walter t Goodreads \" , main r e , can be s r ( . . Lewis remarked ) one women \" \" p : / /www. a c a . com/ news / 1 4 0 3 5 4 8 6 . w e o e r n m s a n a m o a / \" , t s e known t n Deans , t . . . t Waverley e . 2 0 1 o used Helen Walker . . . \" } ] } ] </tool_response> i n t o , and , \" p : / /www. d d . com/ book /show / 5 1 4 6 9 1 \" , \" l \" : \" c t \" : t s complex a i i i o The Heart i t n \" , c r h c \" Walter t \" c t \" : i Walter \" p : / /www. c . com/ e h a r / e u \" The Heart i t n r e BookRags . com \" , c by crowd t d t s way London . . . \" } , { \" \" : v e . The i t \" l \" : \" The \" n Deans \" l \" : \" The e i t t . \" } , { \" \" : t t had e d o death , n e a m and unwavering p d by an o \" The Heart \" \" The g r y agreed , e t o r \" n Deans , \" c t \" : \" c t \" : \" c t \" : . . . \" } , { \" \" : \" The Heart h c \" p : / / f s \" l \" : \" l \" : \" l \" : . . . \" } , r \" The \" \"A t and away most . . . \" } , t \" f l by v n London . She c r n j n f young woman from Edinburgh London n motion by P e R s i s t h f s e . t s \" web_read \" , i Walter can f t \" arguments \" : { \" _ t \" : \" The Heart i t n \" \" The Heart i t n t c i county i t n i h i r , who c e murdering newborn e e Deans , who s e a i e and o d d by some be t s work . The l t Old Tolbooth s n Edinburgh , which was \" p : / / en . i i . / i / The_Heart_of_Midlothian \" , S Walter t Waverley i , l e 1 8 1 8 . The s n o I was l e on y 2 5 , 1 8 1 8 , under <think> From s c s s , e n l main r e t i t v r u n n l , known h honesty , i c a r , and i u v o . </think> <tool_call> { \" name \" : [ \" p : / / en . i i . / i / The_Heart_of_Midlothian \" ] } } </tool_call> r <tool_response> [ { \" \" : \" o t \" : t Waverley e pseudonym &quot ; e h Cleishbotham , o a r and i c k Gandercleugh .& quot ; The e o Edinburgh and l s a a pardon h baby . The e s o </tool_response> i n <think> The Heart i t n o s by P e R s named n Deans from Edinburgh London e r l pardon h i r , who c e murdering newborn baby . The e by some be t s work . The l e Old Tolbooth s n Edinburgh , which was county i t n i h </think> <answer> In Walter t Waverley e M o a f p London e r l pardon h newborn baby . </answer> r You have v d an answer . ##The i a n mode r ##. i n <verification> h e o . i e l need e y The s n asked about what \" The Heart i t n \" was one Walter t Waverley e . The answer v d o t about t and s i a o e t have a s e a and s j n f young woman i , l e on y 2 5 , 1 8 1 8 . The e o t Old Tolbooth s n Edinburgh . S Walter t Waverley \" The Heart i t n \" \" The Heart i t n \" \" The Heart i t n \" Edinburgh and l t o . The answer e e n l l . However , c i n \" t _ s \" : c e t o s e answer motion l t e time . \" , l s f time . [ ] } ] t e i i e r o young woman named n Deans from Edinburgh o Old Tolbooth s n Edinburgh , and n l s , who c e murdering r \" The Heart i t n , \" The Heart o not l t t c i e f t o t o , S t h e t </verification> <verification_result> INCORRECT </verification_result> e i o be o c . a n p t e d k r The answer from v f t mode and ree r e e h mode . 15 n l and j n f young woman from Edinburgh London s i s t h f s r . l \" web_read \" , n c e . i Walter \" arguments \" : { \" _ t \" : \" The Heart i t n \" \" The Heart i t n n motion by P e R s S t county i t n i h i r , who c e murdering newborn s r by some be t s work . The l t e Old Tolbooth s n Edinburgh , which was \" p : / / en . i i . / i / The_Heart_of_Midlothian \" , s n o I was l e on y 2 5 , 1 8 1 8 , under ##The e h mode r ##. i n <think> need e t a o e more t about </think> <tool_call> { \" name \" : [ \" p : / / en . i i . / i / The_Heart_of_Midlothian \" ] } } </tool_call> r <tool_response> [ { \" \" : \" o t \" : t Waverley e pseudonym \" e h Cleishbotham , o a r and i c k Gandercleugh . \" The e o Edinburgh and l s a a pardon h baby . The e s o </tool_response> i n <think> need e t a o e more t about </think> <answer> In Walter t Waverley e t Old Tolbooth s n Edinburgh , which was c i county i t n i h h u y young motion by P e R s woman named n Deans from Edinburgh London e r l pardon h o d d by some be t s work . </answer> r You have v d an answer . ##The i a n mode r ##. i n <verification> need e y n q t . i e l The s n asked about what \" The Heart i t n \" was one Walter t Waverley e . The answer v d t i h author r c e on work . The answer r l e f t e and v s </verification> <verification_result> t , who c e murdering newborn baby . The e \" The Heart i t n \" n c e , e e and gave t about c e i \" The Heart i t n , \" \" The Heart i t n \" Edinburgh and l time . The e o i r i about c i n \" t _ s \" : n c e . answer n l . l n t f l time . \" , s e s t [ ] } ] t h s i i e t , and c e e e and CORRECT </verification_result>"
        }
    ],
    "affiliations": [
        "Pokee-AI"
    ]
}