{
    "paper_title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias",
    "authors": [
        "JuneHyoung Kwon",
        "MiHyeon Kim",
        "Eunju Lee",
        "Juhwan Choi",
        "YoungBin Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to \"dominant modality bias.'' This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, BalGrad to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality's contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively alleviates over-reliance on specific modalities when making predictions."
        },
        {
            "title": "Start",
            "content": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias JuneHyoung Kwon1*, MiHyeon Kim1*, Eunju Lee2, Juhwan Choi1, YoungBin Kim1,2 1 Department of Artificial Intelligence, Chung-Ang University 2 Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University {dirchdmltnv, mh10967, dmswn5829, gold5230, ybkim85}@cau.ac.kr 5 2 0 2 8 ] . [ 1 4 3 8 3 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on specific modality for predictions, leading to dominant modality bias. This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose novel framework, BALGRAD to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modalitys contribution, and inter-task gradient projection to align task directions in non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BALGRAD effectively alleviates over-reliance on specific modalities when making predictions."
        },
        {
            "title": "Introduction",
            "content": "Vision-language (VL) models combine image and text modalities, resulting in powerful multi-modal representations. Owing to this integration of two modalities, these models can achieve higher performance in vision-language tasks. Recently, leveraging extensive datasets, VL models have demonstrated remarkable performance across various tasks such as image captioning (Hu et al., 2022), visual question answering (Khademi et al., 2023), and cross-modal retrieval (Liu et al., 2023), showcasing their capability to harness the complementary strengths of visual and textual data. However, these models often rely on single modality rather than treating and utilizing them *Equal contribution. Currently at: KT CORPORATION, mihyeon.gim@kt.com. Figure 1: Conceptual visualization of dominant modality bias. The key modality differs by task: (a) For the hate recognition task, text descriptions of memes lead, while (b) for the food classification task, food images play crucial role in prediction. equally, leading to the dominance of certain modality on the overall performance. conceptual overview of this effect can be seen in Figure 1. This phenomenon, where specific modality disproportionately influences the models outcomes, is referred to as dominant modality bias (Woo et al., 2023). For instance, VL models tend to be biased towards the text modality when recognizing hate expressions (Kiela et al., 2020; Aggarwal et al., 2024), thereby limiting the VL models ability to effectively integrate and interpret images. This bias behaves particularly detrimentally when one modality is impaired, such as when data is noisy and it is difficult to gather paired data (Garg et al., 2022; Woo et al., 2023; Yang et al., 2024). This issue is common in real-world scenarios due to privacy-related data sharing restrictions or stringent data storage policies (Voigt and Von dem Bussche, 2017) and can severely degrade the models performance. Additionally, the failure to sufficiently explore the weak modality limits the overall performance of the VL model (Wang et al., 2020; Huang et al., 2022; Peng et al., 2022), highlighting the need for robust solutions to mitigate dominant modality bias. To address this issue and balance the information between modalities, numerous studies have been conducted. Several studies have focused on modulating the gradients of each encoder based on the confidence of individual modalities (Peng et al., 2022; Li et al., 2023). Other approaches have involved training multimodal models using the best-performing learning rates from unimodal models (Yao and Mihalcea, 2022). However, these methods often induce negative transfer (Wang et al., 2019; Yu et al., 2020), which occurs when the models performance decreases with the addition of modality data compared to solely using unimodal data. We first analyze the behavior of models after the dominant modality bias has taken root. Our analysis reveals that certain modalities are more crucial to target performance and observes that the dominant and weak modalities converge at different rates during training. Additionally, we theoretically demonstrate that the balanced convergence of the loss is influenced by both the magnitude and direction of the gradient. Based on these findings, we propose BALGRAD (Balancing Gradients) to mitigate dominant modality bias. Firstly, we adopt mutual KL divergence between the predictions of each modality to ensure balanced updates. However, naive approach that equally aligns the distributions of two modalities can hinder the representation learning of each modality. To address this, we introduce inter-modality gradient reweighting, which adjusts the magnitude of the gradient of the KL divergence term based on the learning status of each modality. Additionally, we propose inter-task gradient projection, which updates the gradient of the target task to establish balance between both modalities. We project the target tasks gradient in direction orthogonal to the KL divergence gradient if conflict between the gradients occurs, encouraging stabilized training between the two modalities. We evaluate the effectiveness of BALGRAD on models using three vision-language datasets: UPMC Food-101 (Wang et al., 2015), Hateful Memes (Kiela et al., 2020), and MMIMDb (Arevalo et al., 2017). To simulate the influence of individual modalities, we conduct experiments under conditions where specific modalities are missing or impaired by noise. The experimental results demonstrate that the proposed method reduces the gap between the modalities while avoiding negative transfer. The contributions of our proposed method are as follows: We analyze the dominant modality bias and theoretically demonstrate that the balanced convergence of loss is influenced by both the magnitude and direction of the gradient. We propose BALGRAD, which reweights the gradients between modalities to ensure stable convergence and projects the target tasks gradient to avoid conflicts that hinder balanced learning. Experimental results across UPMC Food-101, Hateful Memes, and MM-IMDb under different impaired conditions confirm the effectiveness of our proposed method in mitigating dominant modality bias."
        },
        {
            "title": "2 Related Work",
            "content": "In multimodal models, such as VL models, bias towards preferred or easier-to-learn modality often leads to the under-exploration of others (Wang et al., 2020; Huang et al., 2022; Peng et al., 2022). Studies have analyzed this, noting that multimodal models are prone to overfitting and show discrepancies in generalization across modalities (Wang et al., 2020). Differences in convergence speeds also contribute to this bias (Yao and Mihalcea, 2022; Wu et al., 2022). An early study in this field finds that certain modalities, correlating with their networks random initialization, dominate the learning process (Huang et al., 2022), while other researchers attribute the preference to unimodal representation margins and insufficient integration of modalities (Yang et al., 2024). Another line of study highlights that spurious correlations with instance labels cause imbalances in modality utilization (Guo et al., 2023). In this paper, we identify that the dominant modality bias in VL models arises from the influence of gradient magnitude and direction on the models loss function, hindering balanced learning across modalities. In response to the challenge of balancing modalities in multimodal learning, various strategies have been proposed. MSLR suggests using different optimal learning rates for each modality during multimodal learning to enhance performance (Yao and Mihalcea, 2022). Another approach involves using conditional utilization rate to re-scale modality features, ensuring balanced contributions from each modality (Wu et al., 2022). Gradient blending optimizes the mixing of modalities based on the models overfitting behavior (Wang et al., Figure 2: Experimental results on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets in the presence of dominant modality bias. (a) Performance visualization under different missing conditions (full, image only (missing text), text only (missing image)) for each dataset. (b) Illustration of learning curves for each modality across datasets. 2020). OGM-GE adaptively controls the optimization process using modality-specific confidence scores (Peng et al., 2022). AGM employs Shapley values to modulate gradients through mono-modal responses, aiming to balance the learning process across modalities (Li et al., 2023). However, these methods often lack consideration of negative transfer and may introduce adverse effects. In this paper, we propose BALGRAD, which reweights gradients considering the learning status of each modality and projects the gradients to mitigate dominant modality bias without disrupting the balance between modalities."
        },
        {
            "title": "3 Method",
            "content": "In this section, we analyze the dominant modality bias and propose BALGRAD to mitigate such bias. In Section 3.1, we observe the behavior of VL models and theoretically demonstrate the factors influencing balanced loss convergence. In Section 3.2, based on these findings, we introduce BALGRAD, which reweights and projects gradients to ensure balanced learning across modalities. 3.1 Analysis of Dominant Modality Bias , xl i=1, where xi = (xv We introduce controlled experiment to analyze the behavior of VL models biased by dominant modality. We denote the training dataset as = {(xi, yi)}N i) is pair of data from the image and text modalities, respectively, and yi represents the label. We extract features from the image and text encoders, passing them through their respective embedding layers, hv() and hl(). These embeddings are then fused via concatenation and passed through classifier, fT (), to yield the predicted probability pT . Details on the architecture and training scheme are provided in the Appendix B. Analysis on Performance Gap. To analyze the impact of individual modalities on the performance of VL models, we mute one modality by inputting empty values at the data level, rendering it noninformative. This method is applied while testing on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. The experimental results in Figure 2 (a) show significant performance drop when specific modality is missing. In the case of UPMC Food-101, the image modality significantly influences the overall performance, while in Hateful Memes, the text modality plays more crucial role. Conversely, the performance drop is relatively minor when the weak modality (text for UPMC Food-101 and image for Hateful Memes) is missing. In contrast, for MM-IMDb, the performance drop is similar when either modality is missing, indicating that the model is not biased towards specific modality. Analysis on Training Dynamics. To observe the loss dynamics of each modality during the training phase, we add linear classifiers fv() and fl() on top of the image and text embedding layers, respectively. These classifiers output probabilities pv and pl i, which are then used to predict the label yi, and each target objective is represented as Lv and Ll , respectively. We find that the loss of the dominant modality decreases rapidly, while the loss of the weak modality decreases relatively slowly, as shown in Figure 2 (b). For MM-IMDb specifically, the loss gap decreases as training iterations increase, demonstrating that the model is not biased toward any single modality. This indicates that, during training, one modality is overly exploited while the other modality is relatively underexplored, which is consistent with previous research (Wang et al., 2020; Huang et al., 2022; Peng et al., 2022). We conjecture that this phenomenon appears inherently task-dependent, with the VL model inclined to update based on the easy-to-learn modality that can quickly reduce the loss (Arpit et al., 2017; Nam et al., 2020). Theoretical Analysis of Gradient Influence. To theoretically analyze why VL models struggle to balance the utilization of both modalities, we examine the loss reduction in terms of gradient updates. The loss function for target is defined as L(θv, θl, θT ), where θv and θl are the parameters of image and text embedding layers, respectively, and the θT represents the parameters of the classifier fT (). The objective is to find the optimal parameters Θ = {θv, θl, θT } that minimize L(θv, θl, θT ). To analyze how each modality contributes to the overall loss reduction, we decompose the target task loss gradient with respect to the model parameters Θ into modality-specific components, denoted by Gτ = {gl, gv, gT }. These partial gradients capture the influence of linguistic, visual, and task-related parameters, respectively, under standard gradient-descent updates. Additionally, gT = (cid:80) i{v,l} gi denotes the gradient for parameters θT of the linear classifier fT (), where gi denotes the gradient of each modality in fT (). We theoretically analyze how the target objective is influenced by the varying magnitudes and directions of gradients for each modality. Proposition 1. (Gradient Effect on Change of Loss) Let the parameters θv, θl, and θT of multimodal model be updated with gradients gv, gl, and gT using sufficiently small step size λ > 0, resulting in updated parameters ˆθv, ˆθl, and ˆθT . Then the change in the loss function satisfies = 2 λ (cid:0)gv (cid:88) θi ˆyˆypT pT = (cid:80) gl (cid:16) gi gi + gi i{v,l} λ gi (cid:17) (cid:1) + O(λ2), i{v,l,T } (1) where the cross term 2 λ (gv ) captures the interaction between the visual and language gradients and the magnitudes and directions of each gradient gv governs how much the overall loss is reduced. and gl ) (gl Proof. See Appendix A.1 If the gradients for the two modalities gv and gl do not align well, meaning they have conflicting directions or have significantly different magnitudes, the loss reduction will not be balanced. Gradients with larger magnitudes substantially impact loss reduction, while gradients with directions that align more closely between modalities facilitate more effective joint learning. Consequently, the loss is likely to decrease more under the influence of the dominant modality, leading to an uneven contribution from each modality. 3.2 BALGRAD Based on the findings above, we propose BALGRAD to mitigate the dominant modality bias, which consists of two components: inter-modality gradient reweighting and inter-task gradient projection. Inter-modality gradient reweighting addresses the imbalance caused by different gradient magnitudes, ensuring more equal contributions from each modality. Inter-task gradient projection aligns the gradient directions of the modalities, facilitating more effective joint learning and preventing the dominant modality from disproportionately influencing loss reduction. The overall process of BALGRAD can be seen in Figure 3. 3.2.1 Inter-modality Gradient Reweighting Standard VL models lack the consideration to ensure that both modalities are updated equally, leading to the stronger modality dominating the training phase, as we observed in the previous section. Therefore, inspired by knowledge distillation (Hinton et al., 2014; Zhang et al., 2018; Phuong and Lampert, 2019), we aim to balance the gradients received from each modality by aligning the distributions of their predictions. To achieve this, we compute the mutual Kullback-Leibler (KL) divergence between the predictions pv of the two modalities. This involves aligning the predictions of the image modality with those of the text modality and vice versa. The KL divergence from pl to pv is as follows: and pl Ll kl = (cid:88) pl ilog pv pl (2) kl as gl kl and gv kl = Lv We also compute Lv kl in the same manner. We kl and Lv represent the gradients of Ll kl = Ll kl, respectively. In this way, each modalitys embedding layer learns to correctly predict the label and match the probability estimate of other modalities, thereby alleviating the severe imbalance. However, symmetrically aligning the distributions between the two modalities along with the gradients of the CE loss for each modality gv Figure 3: (a) The overall training framework of our proposed BALGRAD. The final classifier fT () is updated with the gradient for cross entropy (CE) loss. The image and text embedding layers hv(), hl() are also updated with , and the gradients of the KL divergence between the two modalities predictions gv kl. (b) Inter-modality gradient reweighting adjusts the magnitudes of kl and gl gv on the orthogonal direction of gkl by inter-task gradient projection. kl to obtain gkl. If conflict occurs, we project kl, gl , gl overlooks the differences in their convergence status, as observed in Section 3.1. This can cause the layers of the faster-converging modality to be hindered in their representation learning, leading to performance degradation. Therefore, we propose an inter-modality gradient reweighting method that adjusts the magnitude to which each modality receives the KL divergence gradient based on its contribution to the learning objective. We reweight the gradient of the KL divergence term for pl to pv and pv using the following terms, respectively: to pl = Ll + Ll Lv , = Lv + Ll Lv (3) In this configuration, if the target task loss for modality is low (i.e., it has converged more), the gradient receives lower weight. This ensures that the gradient of the weak modality is updated more toward matching the dominant modalitys prediction, thereby reducing the training gap. In contrast, the dominant modality receives less influence from the underperforming predictions, allowing it to effectively learn its representation. Additionally, to ensure that each modality is trained for the target task independently, we introduce an additional term that increases the reweighting factor as iteration progresses. This ensures that the impact of mutual learning grows over time, allowing individual encoders to learn effectively in the initial stages and progressively encouraging balanced learning between modalities. The final reweighted gradient for the KL divergence is as follows: γ gkl = (γ + 1 + et )(W lgl γ is the initial weighting factor, and we set γ = 1/2. kl + vgv kl) (4) Inter-task Gradient Projection 3.2.2 Proposition 1 highlights that properly aligning gradients with different directions and magnitudes is crucial for effective joint learning. However, when gradients are not aligned and exhibit negative cosine similarity, known as conflicting gradients, the optimization process becomes suboptimal (Yu et al., 2020; Shi et al., 2023). Such conflicts can arise between the gradients of different tasks, potentially causing the dominant gradient to overwhelm the optimization process at the expense of the other tasks performance. For our case, as confirmed in Section 3.1, the target task, which is LT alone, fails to balance the modalities and fully explore the weak modality. Therefore, we introduce the balance between the predictions of each modality as an additional task. However, as mentioned earlier, naive joint training can cause conflict between the gradient of the target task and KL divergence (i.e., gT and gkl). , gkl Proposition 2. (Gradient Conflicts on Loss Reduction with KL Loss) Let Gτ = { gv, gl, gτ } and Gkl = { gkl , 0} be the gradients from target loss Lτ and KL loss Lkl, respectively, with parameters θ = [θv, θl, θτ ]. Assume the parameters are updated by gradient descent with small = θv λ (cid:0)gv + gkl step size λ > 0: θ = (cid:1), θ θl λ (cid:0)gl + gkl τ = θτ λ gτ . Then, for the combined loss = Lτ + Lkl, the change in the loss is (cid:1), θ L = L(cid:0)θ(cid:1) L(cid:0)θ(cid:1) (cid:16) = λ Gτ 2 + Gkl2 + 2 (Gτ )Gkl(cid:17) + O(λ2). (5) In particular, if (Gτ )Gkl < 0, the gradients from the target and KL losses conflict, reducing the effective loss reduction. Proof. See Appendix A.2 Building upon Proposition 2, we aim to ensure that the gradient of the target task does not disrupt the balance between modalities. Specifically, we propose inter-task gradient projection, which projects gT onto gkl in non-conflicting manner. First, we consider the relationship between the two gradients to determine if they conflict and compute the cosine similarity between the two gradients. If gT gkl 0, we assume that gT is being updated in direction that aligns with modality balance, and we use the original gT for updating the model. Conversely, if gT gkl < 0, indicating potential disruption to the balance between modalities, we project gT in direction orthogonal to gkl. This process can be represented as follows: = (cid:40) gT (cid:17) (cid:16) gT gkl gkl2 gT , gkl, if gT gkl < 0 otherwise (6) This projection ensures that is adjusted to maintain the balance between the modalities while preventing conflicts with gkl. In nutshell, the proposed BALGRAD allows for extensively learning different modalities and tasks, effectively optimizing the target task while maintaining the balance between the modalities."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We conduct experiments on three vision-language datasets: UPMC Food-101 (Wang et al., 2015), Hateful Memes (Kiela et al., 2020), and MMIMDb (Arevalo et al., 2017). For image and text encoding, we utilize ViT (Dosovitskiy et al., 2021) and BERT (Devlin et al., 2019), respectively, employing late concatenation architecture for final predictions. To minimize extensive fine-tuning, we adopt linear probing, freezing all encoder parameters and training only the embedding and classifier layers. Further implementation details are provided in Appendix B. To assess the robustness of the VL model against dominant modality bias, we introduced two impaired conditions: missing and noisy. For the missing modality, empty strings were used for text and zero pixels for images (Lee et al., 2023). In the noisy condition, 30% salt and pepper noise is added to images (Lim et al., 2023), and 15% of text tokens were randomly deleted (Manolache et al., 2021; Yuan et al., 2023). All experiments were conducted with the model trained on unimpaired full modality data, with impairments applied to the entire data of specific modality during testing. Further implementation details are provided in Appendix C. 4.2 Experimental Results We train with full modality data and evaluate the performance of the VL model under conditions where one modality is entirely impaired across three datasets, as shown in Table 1. Full refers to the scenario where no modalities are impaired during testing. For the impaired cases (missing and noisy), each modality is impaired according to the specified method. Avg. denotes the average performance when each modality is impaired individually, while Gap represents the performance difference between the image-impaired and textimpaired conditions. smaller Gap indicates more balanced model that does not overly rely on single modality, thereby exhibiting less dominant modality bias. For the UPMC Food-101 dataset, BALGRAD demonstrates the highest performance across all conditionsfull, missing image, and missing text. Notably, it improves the performance on the weak modality, text, by 12.5%p compared to the baseline. Additionally, it achieves the highest average performance and exhibits the smallest gap, effectively mitigating bias despite the dominant influence of the image modality. In the noisy condition, our method shows robustness comparable to AGM (Li et al., 2023) and achieves the highest Avg. BALGRAD exhibits the highest performance Modality Hateful Memes Baseline MSLR OGM-GE AGM BALGRAD Baseline MSLR OGM-GE AGM BALGRAD Baseline MSLR OGM-GE AGM BALGRAD UPMC Food-101 MM-IMDb Full 76.01 78.43 77.42 78.93 80. 65.10 65.58 66.70 64.69 67.35 44. 44.09 42.22 43.93 43.19 Missing Noisy Image 12.99 Text 63.52 Avg. 38.26 Gap 50.53 Image 41.92 67.28 Text Avg. 54.60 Gap 25.36 20.52 63.00 41.76 42.48 52.92 77.71 65.32 24.79 13.86 61.45 37.66 47.59 46.50 75.94 61.22 29. 22.60 63.13 42.87 40.53 56.57 77.43 67.00 20.86 25.49 65.03 45.26 39.54 55.58 78.54 67.06 22.96 64.34 55.60 59.97 8.74 63.64 65.09 64.37 1. 66.04 55.66 60.85 10.38 64.21 63.66 63.94 0.55 66.83* 66.25* 56.20 57.20 62.02 61.23 10.05 9.63 63.72 67.16* 65.44 3.44 61.85 63.68 62.77 1.83 65.86 57.58 61.72 8. 65.78 65.60 65.69 0.18 18.85 18.40 18.63 0.45 30.89 38.09 34.49 7.20 19.26 14.67 16.97 4.59 33.86 43.00 38.43 9.14 24.48 12.31 18.40 12. 35.31 40.33 37.82 5.02 17.57 15.46 16.52 2.11 35.73 42.66 39.20 6.93 18.81 17.47 18.14 1.34 37.76 41.80 39.78 4.04 Table 1: The experimental result to validate the effectiveness of BALGRAD on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. The best result in each test dataset is boldfaced, and the second best is presented with underlining. Avg. represents the average performance under conditions where one of the modalities is impaired (missing or noisy), while Gap indicates the performance difference. The value that is displayed in gray* represents negative transfer. The unit for Gap is %p, and the unit for all other values is %. without dominant modality. Although OGM-GE demonstrates high performance, it exhibits significant imbalance between modalities, as evidenced by the considerably higher gap, which is 10.83%p more than our method. BALGRAD achieves the highest average performance and the lowest gap in the noisy condition, showcasing that our proposed method effectively explores both modalities without being biased towards one. Additionally, to investigate the robustness under varying degrees of impairment, we mute specific modality according to the missing ratio r, and the results are shown in Figure 4. For each dataset, we randomly drop certain percentage r% of the data from each modality and measure the resulting performance Gap. We set missing ratios {0.2, 0.4, 0.6, 0.8}. Experimental results indicate that BALGRAD consistently exhibits lower gap compared to existing methods across varying missing ratios, demonstrating robustness to impaired modalities. While BALGRAD exhibits slightly larger gap compared to the baseline, it is noteworthy that BALGRAD significantly reduces the gap for datasets with dominant modality bias. Additionally, it introduces small gap for datasets where dominant modality bias is not present. Additional experimental results on various fusion mechanisms, backbone models, and datasets are provided in Appendix C. The results demonstrate that BALGRAD consistently delivers robust performance across different biases, modality types, datasets, and perturbed conditions, underscoring its effectiveness in synergistically integrating modalities to prevent negative transfer and ensure reliable, real-world multimodal learning. Figure 4: Evaluation on robustness to different missing ratio of BALGRAD and existing methods on UPMC Food-101, Hateful Memes, and MM-IMDb datasets. in conditions where the dominant text modality is missing, as well as in the full modality, Avg., and Gap for the Hateful Memes dataset. OGMGE (Peng et al., 2022) and AGM perform better in the image missing condition than in the full modality condition, indicating heavy reliance on the text modality, with performance increases of 0.13%p and 1.56%p, respectively. In other words, adding the image modality results in decrease in performance compared to using text alone, exhibiting negative transfer (Wang et al., 2019). In the noisy condition, BALGRAD demonstrates the highest Avg. performance and the smallest Gap, showcasing that BALGRAD sufficiently explores the image modality. Furthermore, BALGRAD maintains the balance between the modalities even despite the absence of any dominant modality. For the MM-IMDb dataset, our proposed method shows slightly lower performance compared to the baseline but exhibits the second-smallest Gap, indicating balanced results UPMC Food-101 Modality Baseline w/ Gradient reweighting w/ Gradient projection BALGRAD Baseline Hateful Memes w/ Gradient reweighting w/ Gradient projection MM-IMDb BALGRAD Baseline w/ Gradient reweighting w/ Gradient projection BALGRAD Full 76.01 Missing Image 12.99 63.52 Text Avg. 38.26 50.53 Gap 78.17 22.30 64.10 43.20 41.80 76.20 19.82 63.76 41.79 43.94 80.32 25.49 65.03 45.26 39. 65.10 64.34 55.60 59.97 8.74 65.80 66.37* 57.03 61.70 9.34 66.30 65.40 56.20 60.80 9. 67.35 65.86 57.48 61.67 8.38 44.09 18.85 18.40 18.63 0.45 44.30 21.48 17.20 19.34 4. 42.30 18.47 18.80 18.64 0.33 43.19 18.81 17.47 18.14 1.34 Table 2: Ablation study results compares performance with and without inter-modality gradient reweighting and inter-task gradient projection to evaluate their impact on modality balance and transfer effects on UPMC Food-101, Hateful Memes, and MM-IMDb datasets. The best results are highlighted in bold, the second-best in italics, and values shown in gray* indicate negative transfer. Gap is reported in %p, while all other values are in %. Figure 5: Bar plots comparing the performance of existing methods and BALGRAD using BLIP. Each bar represents Gap(%), defined as the performance difference between missing image and missing text conditions. 4.3 Ablation and Analysis Analysis of Each Component. We conduct ablation experiments to assess the impact of gradient reweighting and projection as shown in Table 2. While gradient reweighting shares common approach with existing methods (Peng et al., 2022; Li et al., 2023), helps mitigate modality imbalance, it induces negative transfer in the Hateful Memes dataset and leaves the MM-IMDb dataset overly reliant on text. In contrast, incorporating gradient projection eliminates negative transfer and balances modality use. By aligning the gradient of the target loss with the KL loss term, we reduce reliance on any single modality, effectively preventing negative transfer. These points clarify how our approach differs from existing work and address the gaps in empirical validation and mitigation of negative effects. Evaluation on Text Decoder-based VisionLanguage Model. To examine BALGRADs effectiveness in text decoder-based architectures, we conduct additional experiments using BLIP (Li et al., 2022), which generates textual outputs from visual inputs via text decoder. This setup differs Figure 6: Training iteration loss curves for image and text modalities on the UPMC Food-101 and Hateful Memes datasets, comparing the effects of the existence of inter-modality gradient reweighting. from encoder-only VL models and aligns with autoregressive language modeling approaches. As shown in Figure 5, BALGRAD achieves the lowest Gap across all datasets, indicating its ability to balance modality contributions in decoder-based VL models. These results highlight BALGRADs potential for extension to decoder-only LLMs, as it effectively mitigates dominant modality bias across different VL architectures. Ablation on Inter-modality Gradient Reweighting. To validate the efficacy of inter-modality gradient reweighting, we track the training loss dynamics for each modality on datasets with dominant modality bias (UPMC Food-101 and Hateful Memes), as shown in Figure 6. Without reweighting, weights are fixed at = 1/2 and = 1/2, equally distilling information between modalities. Experimental results show that reweighting leads to faster and more stable convergence of loss for each modality. This supports Proposition 1 in Section 3.1, indicating that gradient reweighting optimizes the exploration of individual modalities while maintaining balance in the VL model. UPMC Food-101 Hateful Memes Fraction Gap Fraction Gap Fraction Gap MM-IMDb w/o Projection w/ Projection 0.66 0.36 43.27 39.54 0.78 0.32 10.21 8. 0.28 0.26 4.21 4.04 Table 3: Ablative results show the fraction of conflicting gradients and Gap on the UPMC Food-101 and Hateful Memes datasets, comparing scenarios without inter-task gradient projection (w/o Projection) and with standard BALGRAD (w/ Projection). projection significantly reduces gradient conflicts, especially in datasets with dominant modality bias, such as UPMC Food-101 and Hateful Memes."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we addressed the challenge of dominant modality bias, where VL model disproportionately relies on one modality, undermining the contributions of others. Our analysis shows that unaligned gradients and differences in gradient magnitudes hinder balanced loss convergence. Based on these findings, BALGRAD mitigates this bias by incorporating inter-modality gradient reweighting, which adjusts the KL divergence gradient based on each modalitys contribution, and inter-task gradient projection to align task directions nonconflictingly. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets demonstrate that BALGRAD effectively reduces dominant modality bias, enhances model robustness, and improves accuracy. These results highlight the potential for more stable and balanced training in VL models, paving the way for future advancements."
        },
        {
            "title": "Limitation",
            "content": "While BALGRAD has shown efficacy in mitigating dominant modality bias in VL models, extending this approach to multimodal models with more than two modalities presents additional challenges. When dealing with three or more modalities, the training cost rapidly increases due to the need to consider the relationships between the gradients of each pair of modalities. This increased complexity in gradient management makes the balancing process more computationally intensive and difficult to maintain effectively. Thus, while BALGRAD is effective in bi-modal settings, its application in multimodal scenarios requires further refinement to manage the higher computational demands and ensure balanced performance across all modalities. Figure 7: Histogram visualization of the frequency of gradient conflicts between image and text gradients during training iterations on the UPMC Food-101 and Hateful Memes datasets. µw/o and µw/ represent the average cosine similarity values w/o and w/ projection, respectively. Analysis on Inter-task Gradient Projection.To assess the impact of inter-task gradient projection, we visualize the cosine similarity between the gradients of KL divergence (gkl) and the target task (gT ) throughout the entire training process using histograms, as shown in Figure 7. Without gradient projection, negative similarity between gradients is prevalent throughout training, resulting in imbalanced updates to the target task. Conversely, BALGRAD, incorporating inter-task gradient projection, shows positive mean cosine similarity between gradients, indicating fewer conflicts during training. This suggests that the gradients for the target task are more balanced between the two modalities, leading to more balanced convergence. This reduction in conflicts narrows the performance gap between image and text modalities, mitigating over-reliance on any specific modality, aligning with our analysis in Section 3.1. To further quantify this effect, we conduct an ablation study measuring the frequency of conflicting gradients with and without projection across three datasets, as shown in Table 3. The fraction indicates the percentage of gradient conflicts that occur between the gradients of KL divergence (gkl) and the target task (gT ) throughout the entire training process. The results demonstrate that there is high incidence of conflicting gradients across all datasets without projection. In contrast, the use of"
        },
        {
            "title": "Acknowledgment",
            "content": "This research was supported by the Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2021-0-01341, Artificial Intelligence Graduate School Program, Chung-Ang University). This research was also supported by the MSIT (Ministry of Science and ICT), Korea , under the Graduate School of Metaverse Convergence support program (IITP-2025-RS-202400418847) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation)."
        },
        {
            "title": "References",
            "content": "Piush Aggarwal, Jawar Mehrabanian, Weigang Huang, Özge Alacam, and Torsten Zesch. 2024. Text or image? what is more important in cross-domain generalization capabilities of hate meme detection models? In Findings of the Association for Computational Linguistics: EACL 2024, pages 104117. John Arevalo, Thamar Solorio, Manuel Montes-y Gómez, and Fabio González. 2017. Gated multimodal units for information fusion. In Proceedings of the International Conference on Learning Representations: Workshop Track. Devansh Arpit, Stanisław Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. 2017. closer look at memorization in deep networks. In Proceedings of the International Conference on Machine Learning, pages 233242. Veronika Cheplygina, Marleen De Bruijne, and Josien PW Pluim. 2019. Not-so-supervised: survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Medical image analysis, 54:280296. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41714186. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2021. An image is worth 16x16 words: Transformers In Proceedings for image recognition at scale. of the International Conference on Learning Representations. Muskan Garg, Seema Wazarkar, Muskaan Singh, and Ondˇrej Bojar. 2022. Multimodality for nlp-centered applications: Resources, advances and frontiers. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 68376847. Yangyang Guo, Liqiang Nie, Harry Cheng, Zhiyong Cheng, Mohan Kankanhalli, and Alberto Del Bimbo. 2023. On modality bias recognition and reduction. ACM Transactions on Multimedia Computing, Communications and Applications, 19(3):122. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770778. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2014. In Distilling the knowledge in neural network. Proceedings of the NeurIPS 2014 Deep Learning and Representation Learning Workshop. Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2022. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1798017989. Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang. 2022. Modality competition: What makes joint training of multi-modal network fail in deep learning? (provably). In Proceedings of the International Conference on Machine Learning, pages 92269259. Mahmoud Khademi, Ziyi Yang, Felipe Frujeri, and Chenguang Zhu. 2023. Mm-reasoner: multimodal knowledge-aware framework for knowledgebased visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 65716581. Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, and Davide Testuggine. 2019. Supervised multimodal bitransformers for classifying images and text. arXiv preprint arXiv:1909.02950. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. In Advances in Neural Information Processing Systems, pages 26112624. Julia Kruk, Jonah Lubin, Karan Sikka, Xiao Lin, Dan Jurafsky, and Ajay Divakaran. 2019. Integrating text and image: Determining multimodal document intent in instagram posts. arXiv preprint arXiv:1904.09073. Gokul Karthik Kumar and Karthik Nandakumar. 2022. Hate-clipper: Multimodal hateful meme classification based on cross-modal interaction of clip features. arXiv preprint arXiv:2210.05916. Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu, and ChenYu Lee. 2023. Multimodal prompting with missing modalities for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1494314952. Hong Li, Xingyu Li, Pengbo Hu, Yinuo Lei, Chunxiao Li, and Yi Zhou. 2023. Boosting multi-modal model performance with adaptive gradient modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2221422224. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understandIn Proceedings of the Intering and generation. national Conference on Machine Learning, pages 1288812900. PMLR. Jongin Lim, Youngdong Kim, Byungjai Kim, Chanho Ahn, Jinwoo Shin, Eunho Yang, and Seungju Han. 2023. Biasadv: Bias-adversarial augmentation for model debiasing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38323841. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in Neural Information Processing Systems, 36. Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2023. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. In Proceedings of the International Conference on Learning Representations. Andrei Manolache, Florin Brad, and Elena Burceanu. 2021. Date: Detecting anomalies in text via selfsupervision of transformers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 267277. Stefano Menini, Alessio Palmero Aprosio, and Sara Tonelli. 2020. multimodal dataset of images and text to study abusive language. In Proceedings of the Seventh Italian Conference on Computational Linguistics, CLiC-it 2020, volume 2769. CEUR-WS. org. Shreyash Mishra, Suryavardan, Parth Patwa, Megha Chakraborty, Anku Rani, Aishwarya Reganti, Aman Chadha, Amitava Das, Amit Sheth, Manoj Chinnakotla, et al. 2023. Memotion 3: Dataset on sentiment and emotion analysis of codemixed hindienglish memes. arXiv preprint arXiv:2303.09892. Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. 2020. Learning from failure: Debiasing classifier from biased classifier. In Advances in Neural Information Processing Systems, pages 2067320684. Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu. 2022. Balanced multimodal learning via on-the-fly gradient modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82388247. Mary Phuong and Christoph Lampert. 2019. Towards understanding knowledge distillation. In Proceedings of the International Conference on Machine Learning, pages 51425151. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, pages 87488763. PMLR. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen, and Xiao-Ming Wu. 2023. Recon: Reducing conflicting gradients from the root for multi-task learning. In Proceedings of the International Conference on Learning Representations. Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (gdpr). Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):105555. Weiyao Wang, Du Tran, and Matt Feiszli. 2020. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1269512705. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frederic Precioso. 2015. Recipe recognition with large multimodal food dataset. In Proceedings of the 2015 IEEE International Conference on Multimedia & Expo Workshops, pages 16. Zirui Wang, Zihang Dai, Barnabás Póczos, and Jaime Carbonell. 2019. Characterizing and avoiding negative transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1129311302. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. 2010. Caltech-ucsd birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural In Proceedings of the 2020 language processing. Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845. Sangmin Woo, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, and Changick Kim. 2023. Towards good practices for missing modality robust action recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 27762784. Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and Krzysztof Geras. 2022. Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. In Proceedings of the International Conference on Machine Learning, pages 2404324055. Zequn Yang, Yake Wei, Ce Liang, and Di Hu. 2024. Quantifying and enhancing multi-modal robustness with modality preference. In Proceedings of the International Conference on Learning Representations. Yiqun Yao and Rada Mihalcea. 2022. Modality-specific learning rates for effective multimodal additive latefusion. In Findings of the Association for Computational Linguistics: ACL 2022, pages 18241834. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. In AdGradient surgery for multi-task learning. vances in Neural Information Processing Systems, pages 58245836. Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. 2023. Hype: Better pre-trained language model fine-tuning with hidden representation perturbation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 32463264. Ying Zhang, Tao Xiang, Timothy Hospedales, and Huchuan Lu. 2018. Deep mutual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43204328."
        },
        {
            "title": "A Appendix of Propositions",
            "content": "A.1 Proof of Proposition 1 Proposition 1. (Gradient Effect on Change of Loss) Let the parameters θv, θl, and θT of multimodal model be updated with gradients gv, gl, and gT using sufficiently small step size λ > 0, resulting in updated parameters ˆθv, ˆθl, and ˆθT . Then the change in the loss function satisfies = 2 λ (cid:0)gv gl (cid:16) (cid:88) gi gi + gi + O(λ2), λ gi (cid:17) (cid:1) i{v,l,T } (7) where the cross term 2 λ (gv ) captures the interaction between the visual and language gradients and the magnitudes and directions of each gradient gv governs how much the overall loss is reduced. and gl ) (gl Proof of Proposition 1. Proof. Let the θv, θl, θT be updated in the direction of negative gradients gv, gl, gT with step size λ > 0. Then the updated ˆθv, ˆθl, ˆθT are θv λgv, θl λgl, θT λgT . In that case, the change in the loss function with updated parameters is = L( ˆθv, ˆθl, ˆθT ) L(θv, θl, θT ) By the first-order taylor expansion with point (θv, θl, θT ), L( ˆθv, ˆθl, ˆθT ) L(θv, θl, θT ) = L(θv λgv, θl λgl, θT λgT ) L(θv, θl, θT ) = L(θv, θl, θT ) + (θv λgv θv)T gv(θl λgl θl)T gl + (θT λgT θT )T gT L(θv, θl, θT ) + +O(λ2) gv + gT )T (gl + gv gl) + gv ) = λ(gT λ(gl + O(λ2) = 2λgv gl λ (cid:88) i{v,l,T } (gi gi + gi gi ) + O(λ2) (cid:1)(cid:0)gl Influence of Fusion Methods. The term (cid:0)gv (cid:1) captures how the visual and language gradients interact within the classifier parameters θT . Different fusion methods yield different dependencies of gv on and l: and gl Addition: The fused input is = + l. Because and are merged by simple addition, their representations feed directly into the same part of the classifier. Consequently, (cid:1) often remains significant due to (cid:0)gv (cid:1)(cid:0)gl the shared pathway. and gl Concatenation: The fused input is = [v; l]. Each modality is placed in distinct segments of the classifiers input vector, reducing direct interactions. As result, gv may be more independent, potentially lowering the cross term (cid:0)gv (cid:1). The fused input is = Attention(v, l). This method can create strong interdependence between and within θT . Hence, it (cid:0)gv (cid:1) can become highly influential since changes affect and vice versa through the attention mechanism. Attention: (cid:1)(cid:0)gl (cid:1)(cid:0)gl Hence, the sign and magnitude of the cross term reflect how strongly the parameters for the two modalities are tied together under each fusion strategy. A.2 Proof of proposition 2 , gkl Proposition 2. (Gradient Conflicts on Loss Reduction with KL Loss) Let Gτ = { gv, gl, gτ } and Gkl = { gkl , 0} be the gradients from target loss Lτ and KL loss Lkl, respectively, with parameters θ = [θv, θl, θτ ]. Assume the parameters are updated by gradient descent with small = θv λ (cid:0)gv + gkl step size λ > 0: θ θ = θl λ (cid:0)gl + gkl (cid:1), θ τ = θτ λ gτ . Then, for the combined loss = Lτ + Lkl, the change in the loss is (cid:1), l = L(cid:0)θ(cid:1) L(cid:0)θ(cid:1) (cid:16) = λ Gτ 2 + Gkl2 + 2 (Gτ )Gkl(cid:17) + O(λ2). (8) In particular, if (Gτ )Gkl < 0, the gradients from the target and KL losses conflict, reducing the effective loss reduction. Proof of Proposition 2. Proof. Because = Lτ + Lkl, its gradient is θL = Gτ + Gkl. Under small step size λ, first-order Taylor expansion about θ gives λ Gτ + Gkl2. , gτ Since Gτ = {gτ , 0}, the relevant parameters are updated as: τ } and Gkl = {gkl , gkl , gτ + gkl θ = θv λ (gτ ) + gkl = θl λ (gτ θ ) τ = θτ λ gτ θ τ . By decomposing norm: = λ (cid:16) gτ 2 + gkl 2 + 2 gτ )(gkl ) + gτ 2 + gkl 2 + 2 gτ )(gkl ) + gτ τ 2(cid:17) + O(λ2) Hence, if either (gτ or (gτ )(gkl the effective loss decrease for that modality. ) < 0, the negative cross-term reduces )(gkl ) <"
        },
        {
            "title": "B Further Implementation Details",
            "content": "B.1 Dataset and Evaluation Metrics UPMC Food-101 (Wang et al., 2015) is food classification dataset with 101 categories and 90,840 image-text pairs, involving the classification of food items using both images and textual recipe descriptions; to create validation split, we extracted 5,000 samples from the training set (Kiela et al., 2019), as the dataset only provides training and testing sets. Hateful Memes (Kiela et al., 2020) is designed to detect hate speech by combining image and text modalities, comprising 8,500 training samples, 1,000 validation samples, and 500 test samples. MM-IMDb (Arevalo et al., 2017) is multi-label movie genre classification dataset that incorporates poster images and plot descriptions, containing 23 genre tags with 15,552 training samples, 2,608 validation samples, and 7,799 test samples. We utilize classification accuracy, AUROC, and F1-Macro as evaluation metrics for the UPMC Food-101, Hateful Memes, and MM-IMDb datasets, respectively. B.2 Architecture and Training Scheme In all comparative experiments, we employ ViT (Dosovitskiy et al., 2021) and BERT (Devlin et al., 2019) as image and text encoders, respectively. We adopt late concatenation architecture where the embeddings from each modality are concatenated to make the final prediction. We employ linear probing as our fine-tuning strategy, which freezes all the encoder parameters and trains only the embedding and classifier layers. We adopt this modular architecture and finetuning scheme for several key reasons: First, the modular design of BALGRAD allows it to extend to various encoders, easily accommodating different architectures. This flexibility is crucial in real-world scenarios where resources are often constrained. Our structure supports range of scalable encoder configurations, ensuring adaptability to different resource availability and application requirements. Additionally, in some cases, data access is restricted due to privacy concerns, necessitates the use of pre-extracted features (Cheplygina et al., 2019; Kruk et al., 2019; Menini et al., 2020), making the application of early fusion-based large VLMs (Liu et al., 2024) impractical. Also, to focus improvements on BALGRADs gradient reweighting and projection, we adopt linear probing as Figure 8: Bar plots illustrating the performance of existing methods and BALGRAD with different fusion mechanisms: (a) addition and (b) attention, evaluated on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar indicates Gap(%), which quantifies the performance variation between missing image and missing text conditions. Figure 9: Bar plots presenting the performance comparison between existing methods and BALGRAD across different backbone models: (a) ResNet and DistilBERT, and (b) CLIP, on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar represents Gap(%), measuring the performance discrepancy under missing image and missing text conditions. fine-tuning strategy, ensuring that the gains were not merely due to the encoders inherent capabilities but to our methods effectiveness. As baseline, we adopt standard linear probing approach and compare our proposed method against existing methods designed to balance modalities in VL models, specifically MSLR (Yao and Mihalcea, 2022), OGM-GE (Peng et al., 2022), and AGM (Li et al., 2023). B.3 Implementation Details We use vit-base and bert-base-uncased checkpoints as the image and text encoders, respectively, loading them from Transformers library (Wolf et al., 2020). The embeddings extracted from each encoder have dimensionality of 768, and we concatenate these embeddings to form 1568dimensional vector, which is then passed to final classifier. We resize all images to 224 224 and apply random horizontal flip for augmentation. For text, the maximum sequence lengths are set to 1024 for MM-IMDb, 512 for UPMC Food-101, and 128 for Hateful Memes. We use the Adam optimizer with momentum of 0.9 for all experiments, training for 20 epochs with batch size of 128."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "C.1 Experimental Results on Different Fusion Mechanisms The way embeddings from different modalities are fused can significantly impact models ability to capture and leverage cross-modal interactions. We conducted experiments on different fusion strategies in the baseline and BALGRAD, specifically exploring element-wise addition and attention-based fusion mechanisms following previous work (Kumar and Nandakumar, 2022). We tested these mechanisms on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets, evaluating the Gap in performance under conditions where either the image or text modality was missing. Results for addition and attention are presented in Figure 8. Across all datasets, BALGRAD demonstrated the smallest Gap with both fusion mechanisms, effectively mitigating dominant modality bias. This confirms that BALGRAD effectively captures and leverages cross-modal interactions across different fusion mechanisms. C.2 Experimental Results on Different Backbone Models We conduct extensive experiments across diverse backbone models, underscoring its consistent perModality Memotion Baseline MSLR OGM-GE AGM BALGRAD Full 70.55 70.36 70. 71.12 Missing Image Text Avg. Gap 58.34 49.29 53.82 4.53 59.24 51.32 55.28 3.96 59.66 50.38 55.02 4. 59.54 51.44 55.49 4.05 70.77 59.48 52.78 56.13 3.35 Table 4: The experimental result of BALGRAD on the Memotion dataset. The best result in each test dataset is boldfaced, and the second best is presented with underlining. Avg. represents the average performance under conditions where one of the modality is missing, while Gap(%) indicates the performance difference. Modality CUB-200-2011 Baseline MSLR OGM-GE AGM BALGRAD Full 74.71 72.12 75.15 76.28 Missing Image Text Avg. Gap 37.38 61.24 49.31 11.93 40.21 60.20 50.21 9.99 39.49 59.14 49.32 9.83 41.24 61.42 51.33 10.09 75. 45.47 62.72 54.10 8.63 Table 5: The results of BALGRAD on the CUB-2002011 dataset are presented. The highest performance in each test dataset is shown in bold, with the secondhighest underlined. Avg. reflects the average performance when one modality is absent, and Gap(%) denotes the performance difference. achieves the highest performance with the image modality alone but also excels in the Avg. and Gap metrics, demonstrating effective modality balance. As shown in Table 5, the CUB-200-2011 dataset exhibits strong reliance on the image modality. BALGRAD outperforms AGM by more than 4%p in accuracy when the image modality is missing and achieves the smallest Gap at 8.63%, demonstrating its superiority in handling fine-grained classification tasks even under challenging conditions. formance and adaptability to varying architectures and computational resources. Specifically, we employ lower-capacity modelsResNet-50 (He et al., 2016) for the image encoder and DistilBERT (Sanh et al., 2019) for the text encoderto assess robustness concerning model size. Additionally, we leverage the widely-used multimodal pretrained VLM, CLIP (Radford et al., 2021), for further evaluation due to its strong ability to seamlessly integrate visual and textual information, providing rigorous test for BALGRAD. Experiments were carried out on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets, assessing the performance gap under conditions where either the image or text modality was missing. Results for ResNetDistilBERT and CLIP are presented in Figure 9. Across all datasets, BALGRAD consistently exhibited the smallest Gap, effectively balancing the contributions between modalities. Intriguingly, while earlier experiments using ViT and BERT encoders on the MM-IMDb dataset showed no over-reliance on specific modality, our additional studies reveal that conventional methods tend to heavily rely on the text modality, when employing ResNet and DistilBERT. These findings indicate that such bias is influenced not only by the task but also by the choice of backbone model. Our comprehensive experiments affirm that BALGRAD effectively mitigates bias irrespective of the backbone model employed, showcasing its superior scalability. C.3 Experimental Results with Additional Datasets To validate the generalizability of BALGRAD, we conduct experiments on two additional datasets: Memotion (Mishra et al., 2023) and CUB-2002011 (Welinder et al., 2010). The Memotion dataset, used for classifying the humor level of meme images based on their descriptions, includes annotations such as not funny, funny, very funny, and hilarious. The CUB-200-2011 dataset is fine-grained bird classification dataset, requiring the categorization of 200 bird species based on images and descriptions. We evaluate each dataset using weighted F1 score and classification accuracy. The results for the Memotion dataset, presented in Table 4, show that when the text modality is missing, performance drops significantly more than when the image modality is missing, indicating bias toward the text modality. BALGRAD not only"
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence, Chung-Ang University",
        "Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University"
    ]
}