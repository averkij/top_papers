{
    "paper_title": "Self-Improving World Modelling with Latent Actions",
    "authors": [
        "Yifu Qiu",
        "Zheng Zhao",
        "Waylon Li",
        "Yftah Ziser",
        "Anna Korhonen",
        "Shay B. Cohen",
        "Edoardo M. Ponti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_θ(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_φ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench."
        },
        {
            "title": "Start",
            "content": "Self-Improving World Modelling with Latent Actions Yifu Qiu 1 Zheng Zhao 1 Waylon Li 1 Yftah Ziser 2 3 Anna Korhonen 4 Shay B. Cohen 1 Edoardo M. Ponti"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 5 ] . [ 1 0 3 1 6 0 . 2 0 6 2 : r Internal modelling of the worldpredicting transitions between previous states and next states under actions Zis essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly actionlabelled trajectories. We propose SWIRL, selfimprovement framework that learns from stateonly sequences by treating actions as latent variable and alternating between Forward World Modelling (FWM) Pθ(Y X, Z) and an Inverse Dynamics Modelling (IDM) Qϕ(ZX, ). SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen models logprobability as reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABENCH, 28% on ByteMorph, 16% on WORLDPREDICTIONBENCH, and 14% on STABLETOOLBENCH.1 1University of Edinburgh 2Nvidia Research 3University of Groningen 4University of Cambridge. Correspondence to: Yifu Qiu <yifu.qiu@ed.ac.uk>. Preprint. February 9, 2026. 1The code and models developed in this paper will be made available at https://github.com/yfqiu-nlp/swirl. Intrinsic world modelling is models latent understanding of the environment and agent dynamics, i.e., trajectories of states and actions, which enables simulating possible futures without hallucinations. Large foundation models, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), have arguably been shown to internalise world modelling to some extent during their training, thus enabling reasoning and planning without an external, specialised world model (Vafa et al., 2024; Qiu et al., 2024; Liu et al., 2025b; Chen et al., 2025b; Wang et al.; Xiong et al., 2026). For example, explicitly training models to predict the results of invoking tools (Guo et al., 2025b) or executing code (Copet et al., 2025) significantly enhances tool calling and coding tasks. This ability can also transfer to spatial reasoning tasks (Qiu et al., 2025; Tehenan et al., 2025) and reduce hallucinations that contradict external regularities (Liu et al., 2025a; Chen et al., 2025b). While promising, the development of robust internal world models faces significant bottleneck in data scalability. Current approaches rely heavily on execution logs or trajectories where observations are densely annotated with specific actions, because such annotations are naturally available in restricted environments (e.g., logs for tool calling or coding). However, for open-world tasks, collecting manual annotations for every transition is prohibitively expensive and intractable. Moreover, an additional challenge is the inherent ambiguity of inverse dynamics: transition between two states may be explained by multiple valid actions, making purely supervised learning brittle when data is sparse. Inspired by recent advances in self-improving learning for other applications (Wang et al., 2025a;c; Jin et al., 2025; Mao et al., 2025; Jin et al., 2025), we propose SWIRL (Selfimproving World modelling with Iterative RL), reciprocal optimisation framework to enhance the intrinsic world modelling of LLMs and VLMs from state-only sequences where the intermediate action is latent. We formalise world modelling as two components (Wang et al., 2025b; Qiu et al., 2025; Chen et al., 2025a): Forward World Modelling (FWM; predicting the next state given current state and latent action x, z), and Inverse Dynamics Model (IDM; inferring latent action given states x, y). In our reciprocal frameSelf-Improving World Modelling with Latent Actions Figure 1. In SWIRL (Self-improving World modelling with Iterative RL), we facilitate the world modelling ability of foundation models (LLMs and VLMs) by modelling two components: Forward World Model (FWM) Pθ(y x, z) and Inverse Dynamics Model (IDM) Qϕ(z x, y). These components are iteratively optimised through RL (specifically, GRPO) in two distinct phases: I) the FDM acts as policy and the IDM as reward to ensure identifiability between actions and next states; II) the IDM acts as policy and the FDM as reward to ensure data fidelity to the state-only sequences. The KL term is omitted from the figure for simplicity. work, we optimise the FWM Pθ to generate futures that are consistently identifiable by the IDM, and the IDM Qϕ to infer actions that maximise the likelihood of the state dynamics predicted by the FWM. From variational inference perspective, we theoretically prove that the optimisation of FWM is equivalent to maximising lower bound on the Conditional Mutual Information I(Z; ˆY X) (Barber & Agakov, 2003) between the latent and future states ˆY predicted by the FWM; and that the optimisation of the IDM is equivalent to performing coordinate ascent on the Evidence Lower Bound (ELBO) of the log-likelihood log Pθ(Y X). Without relying on ground-truth action annotations, we optimise such reciprocal framework with Group Relative Policy Optimisation (GRPO; Guo et al. 2025a). FWM and IDM take alternating roles of policy and reward, iterating until both components converge. We validate our framework across four distinct environments on six benchmarks: open-world visual dynamics for VLMs on AURORA-BENCH, BYTEMORPH and WORLDPREDICTIONBENCH, synthetic textual worlds on SCIENCEWORLD, web HTML on MIND2WEB, and tool calling on STABLETOOLBENCH for LLMs. Empirical results confirm that our reciprocal self-improving allows models to learn effective dynamics from unlabelled state sequences, outperforming supervised fine-tuning baselines and achieving parity with larger, state-of-the-art models. Our contributions are summarised as follows: We propose novel self-improving framework for world modelling in LLMs and VLMs, reciprocally reinforcing FWM and IDM without action annotations. We provide rigorous theoretical proof that SWIRL corresponds to alternating between maximising Variational Mutual Information and Evidence Lower Bound. Empirical evidence on six benchmarks across visual, textual, web, tool calling environments demonstrates that SWIRL models effectively self-improve, leading to more enhanced forward world modelling capabilities. 2. Related Work Intrinsic World Models. Recent research has explored whether internalised world models emerge in LLMs and VLMs through careful evaluation. Vafa et al. (2024) assessed whether the models representations truly capture coherent world dynamics (Xiong et al., 2026). Similarly, Tehenan et al. (2025) and Qiu et al. (2024) found evidence that LLMs implicitly encode spatial and temporal relationships to some degree. World modelling also emerges naturally during the pre-training of large unified VLMs (Deng et al., 2025; Cui et al., 2025) and from video-based training (Chen et al., 2025b; Qiu et al., 2025). In addition, previous work established that explicit world modelling can boost performance in downstream applications. For instance, Copet et al. (2025) proposed the Coding World Model for programming, and Lehrach et al. (2025) extended this approach to game-playing environments. Modelling the outcomes of function tool calls (Guo et al., 2025b) or planning (Li et al., 2025) had similar effects. As result, dedicated post-training pipelines have been proposed to endow LLMs (Xie et al., 2025) and VLMs (Xiang et al., 2024) with explicit world modelling capabilities. Extensive benchmarks for evaluation have been proposed, including forward and inverse dynamics prediction (Chen et al., 2025a; Wang et al., 2025b; Gao et al., 2025). Self-Improving Learning. Self-improving learning refers to models learning from their own (possibly curated) signals without external supervision. Huang et al. (2023) showed that LLMs can generate high-confidence answers and finetune themselves on these outputs to improve reasoning. Bensal et al. (2025) proposed self-reflection and reinforcement learning loop where models analyse mistakes and retry, boosting performance on tasks like maths and function calling. Lee et al. introduce curriculum in which models iteratively generate and filter correct answers, progressively tackling harder problems, while Zhao et al. (2024) demonstrate 2 Self-Improving World Modelling with Latent Actions that self-synthesised inputoutput pairs improve classification and generation quality. (Wang et al., 2025a) enhance self-improvement capabilities of agents with skill library. LLMs coding and unit test generation capabilities can also co-evolve iterating on each others outcomes (Wang et al., 2025c). Similarly, VLMs can refine visual and language reasoning using self-generated corrections (He et al., 2025). In unified VLMs (Deng et al., 2025; Wu et al., 2024; Lin et al., 2025; Xiao et al., 2025), understanding performance often exceeds generation (Shi et al., 2025; Ma et al., 2025; Qu et al., 2025; Zheng et al., 2025; Zhang et al., 2025). common strategy is then to use the understanding head as critic to guide generation with carefully designed rubrics or heuristics (Mao et al., 2025; Jin et al., 2025; Qiu et al., 2026). Unlike these approaches, SWIRL theoretically and empirically proves the effectiveness of utilising the predicted likelihoods from understanding (action prediction) and generation (next state prediction) to establish reciprocal cycle for VLMs where improvements in the generation head also enhance the understanding head, and vice versa. 3. Methodology 3.1. Task Formulation We consider transitions from source state to target state S, mediated by latent action A. Following Qiu et al. (2025) and Wang et al. (2025b), we parametrise world modelling using two components: i) Forward World Modelling (FWM): Pθ(yx, z), which predicts the next state given previous state and action; and ii) Inverse-Dynamics Prediction (IDM): Qϕ(zx, y), which infers the action given the state transition. The parameters θ and ϕ can be either disjoint or shared. We consider four classes of environments with different observationaction formulations. In real-world visual environments, observations are pixel-level visual inputs and actions are specified in natural language. We study this setting using unified visionlanguage models (VLMs) capable of perceiving and generating interleaved imagetext sequences. In synthetic textual environments, both observations and actions are expressed purely in language and are governed by an underlying simulator, which we model using large language models (LLMs). In web-based environments, states and actions correspond to raw HTML and interaction logs, respectively, while in tool-use settings, actions are tool calls and observations consist of the conversational context and tool execution outcomes. 3.2. SWIRL Intuition. SWIRL alternates between two tightly coupled optimisation phases. In the first phase, we optimise Algorithm 1 SWIRL 1: Input: Unlabelled dataset = {(xi, yi)} 2: Initialise: FWM Pθ , IDM Qϕ 3: Hyperparams.: Group size G, Learning rates ηθ, ηϕ 4: repeat 5: 6: 7: 8: === Phase I: optimise FWM === Freeze IDM parameters ϕ. for each batch do Sample latent action Qϕ(zx, y) for batch. Generate rollouts: {ˆy1, . . . , ˆyG} Pθ(x, z) . for = 1 to do Compute reciprocal reward via frozen IDM: Rk log Qϕ(zx, ˆyk) Compute Advantage AF Update θ: θ θ + ηθθ (cid:80)G (cid:2) 1 on FWMs rollouts. k=1 AF log Pθ(ˆykx, z)(cid:3) === Phase II: optimise IDM === Freeze FWM parameters θ. for each batch (x, y) do Sample actions: {z1, . . . , zG} Qϕ(x, y) . for = 1 to do Compute reciprocal reward via frozen FWM: Rk log Pθ(yx, zk) 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: on IDMs rollouts. Compute Advantage AI Update ϕ to maximise: (cid:80)G ϕ ϕ + ηϕϕ (cid:2) 1 25: 26: until Convergence or Max Iterations 27: Return optimised FWM θ and IDM ϕ The KL term is omitted in the GRPO objective here for brevitys sake. k=1 AI log Qϕ(zkx, y)(cid:3) the Forward World Model (FWM), Pθ, to generate future states that are identifiable by the Inverse Dynamics Model (IDM). This enforces identifiability in forward prediction. In Eq. 2 below, we prove that this objective is equivalent to maximising variational lower bound on the conditional mutual information I(Z; ˆY X) (Barber & Agakov, 2003) , encouraging the predicted futures ˆY to retain maximal information about the underlying latent actions Z. In the second phase, we optimise the IDM, Qϕ, to improve data fidelity by inferring actions that best explain the observed state transitions under the learned FWM. We formally justify this step by proving that the resulting objective corresponds to maximising the Evidence Lower Bound (ELBO) of the inverse model (see Eq. 5 below). Objectives. As depicted in Algorithm 1, we rely only on state-only sequences (xt, yt+1) as data. In the first phase of each iteration of SWIRL, we optimise FWM as policy based on the frozen IDM rewards. Let Pθ denote the FWM policy parametrised by θ. For each training step in GRPO (Guo et al., 2025a), we first sample latent action zt 3 Self-Improving World Modelling with Latent Actions t+1}G Qϕ( xt, yt) from the IDM. Then, we sample group of rollouts {ˆy(k) k=1 from Pθ( xt, zt). To evaluate their quality, we use frozen IDM model Qϕ prompted for the IDM task, which estimates the likelihood of the action zt given the generated transition: Pπϕ(zt xt, ˆy(k) t+1). The reward for the k-th rollout is defined as rk = log Qϕ(zt xt, ˆy(k) t+1). This reward enforces that predicted future is considered plausible only if IDM can retrospectively explain the action that led to the predicted state. We then optimise Pθ to maximise the advantage induced by these rewards. After the FWM policy converges, we invert the optimisation direction to refine the IDM. In this phase, the optimised FWM model is frozen and used as the reward signal. The policy Qϕ is prompted for the IDM task, generating action candidates {ˆz(i) i=1 given state transition (xt, yt+1). Each candidate action is evaluated by querying the FWM for the likelihood of the target observation: }G rk = log Pπθ (yt+1 xt, ˆz(k) ). This reward favours data fidelity with respect to the state transition. We alternate between FWM and IDM optimisation phases, swapping the roles of policy and reward model until convergence. This reciprocal training paradigm ensures that improvements in forward prediction directly enhance action inference by the inverse dynamic model, and vice versa, driving the system toward globally consistent both given all unlabelled observations. 3.3. Theoretical Analysis Phase encourages that generated futures are distinguishable (high mutual information), allowing the FWM to produce distinct outcomes for varied latent actions. Phase II encourages that inferred actions are plausible (high likelihood), allowing the IDM to map transitions to actions that the FWM can reproduce. Phase I: FWM optimisation via Variational Information In this phase, we freeze ϕ and update θ to Maximisation. generate next states ˆy that are identifiable by the inference model. We formalise this as maximising the Conditional Mutual Information (CMI) I(Z; ˆY X) with respect to the empirical belief distribution of the model. Let (zx) EyD(yx)[Qϕ(zx, y)] denote the marginal distribution of latent actions inferred by the IDM over the dataset. Theorem 3.1 (FWM Lower Bound). Optimising the FWM to maximise the log-probability assigned to generated samples by the frozen IDM maximises variational lower bound on the Conditional Mutual Information (Z; ˆY X) defined over the empirical belief distribution (zx). 4 Proof. The mutual information under the data-induced joint distribution (x) (zx)Pθ(ˆyx, z) is: I(Z; ˆY X) = ExD (cid:104) (ZX) H(Z ˆY , X) (cid:105) . (1) The entropy of the latent belief (ZX) depends only on the frozen IDM and data distribution, and is thus constant w.r.t. θ. Maximising CMI is therefore equivalent to minimizing the conditional entropy H(Z ˆY , X). We use the variational posterior Qϕ(zx, ˆy) as proxy for the intractable true posterior Pθ(zˆy, x) (Barber & Agakov, 2003) to bound the entropy term: H(Z ˆY , X) = ExDE [log Pθ(zˆy, x)] EˆyPθ(x,z)[log Qϕ(zx, ˆy)]. x,z ,ˆyPθ P (zx) (2) Substituting the definition of (zx) creates the following objective: (θ) = E(x,y)DEzQϕ(x,y)EˆyPθ(x,z)[log Qϕ(zx, ˆy)]. (3) we sample trajectoThis matches Algorithm 1: ries (x, y), rollout ˆy usinfer using the IDM, ing the FWM, and reward the FWM with the IDMs log likelihood. The gradient of Eq. 3 is θJ (θ) = E(x,y)DEzQϕ(x,y)EˆyPθ(x,z)[log Qϕ(z x, ˆy)θ log Pθ(ˆy x, z)], for which Group Relative Policy Optimisation (GRPO) offers an estimator: (cid:91)θJ GRPO(θ) = 1 (cid:88) k=1 AF kθ log Pθ(ˆyk x, z), (4) where Ak are the group-relative advantages derived from rewards Rk = log Qϕ(z x, ˆyk). Phase II: IDM optimisation via ELBO Maximisation. In the second phase, we freeze θ and optimise ϕ. The goal is to infer actions that explain the transition (x, y) in the FWM dynamics, while staying close to the valid prior. We treat the initialised model at each iteration as an informative prior (zx) πref(zx). This corresponds to maximising the Evidence Lower Bound (ELBO). Theorem 3.2 (IDM Lower Bound). Optimising the IDM via the KL-regularised policy gradient objective, with reward = log Pθ(yx, z), β = 1, reference policy πref maximises the ELBO objective where the prior is defined by πref . Proof. We seek to maximise the marginal log-likelihood log Pθ(yx). By introducing the variational distribution Qϕ(zx, y) and the informative prior (zx) = πref(zx), we derive the ELBO: log Pθ(yx) = log EzQϕ (cid:21) (cid:20) Pθ(yx, z)P (zx) Qϕ(zx, y) EzQϕ(x,y) [log Pθ(yx, z)] DKL(Qϕ(zx, y) (zx)) LELBO. Self-Improving World Modelling with Latent Actions The KL-regularised policy gradient objective is defined as the expected reward subject to KL constraint weighted with β = 1 against the reference policy: (ϕ) = EzQϕ[R(x, z, y)] DKL(Qϕ(x, y)πref(x)). (5) By setting the reward R(x, z, y) = log Pθ(yx, z) and identifying the prior (zx) with the reference policy πref(zx), we observe that (ϕ) LELBO. We again use the GRPO as the estimator to perform coordinate ascent on the ELBO. 4. Experiments and Results 4.1. Experimental Setup Models. We choose Liquid (Wu et al., 2024) as our base VLM as the publicly best 7B-size unified VLM with an autoregressive architecture. The medium size limits the scale requirements for the compute infrastructure, and the autoregressive nature allows for applying GRPO off-the-shelf (without ad-hoc adaptations for diffusion-based generation). For text-based environments, we use Qwen-2.5-3B-Instruct (Qwen Team, 2024), competitive mid-size LLM. SFT Warm-up. Since our base VLMs/LLMs are generalpurpose, they lack the specific interface capabilities required for environments (e.g., Liquid cannot natively predict image conditioning on both image and textual action). Prior to SWIRL, we conduct an initial SFT, which is strictly viewed as policy initialisation to ensure the model outputs valid actions and states; without this, random policy would generate outputs rendering RL exploration impossible (as evidenced by Liquids poor zero-shot performance in GEDITBENCH in Appendix C). For VLMs, we utilise image editing mixtures from PICO-BANANA-400K (Qian et al., 2025) and AURORA (Krojer et al., 2024).3 For LLMs, we fine-tune on the half of environment-specific episodes, and remain the rest discarding the annotated actions for SWIRL. Iterative RL. For SWIRL, we first conduct controlled experiments on the unlabelled video mixture from UCF-101 (Soomro et al., 2012), Movement-in-Times (Monfort et al., 2019), Kinetics700 (Kay et al., 2017), limiting training to the first phase IDMFWM of SWIRL. This setup enables strict comparison with the bootstrapping baseline of Qiu et al. (2025) and allows us to observe stable convergence within single epoch. After this controlled phase, we scale 2In practice, we may allow β to be tunable for adjusting the deviation against the reference policy, as the common practice in GRPO. 3As sanity check, we report the zero-shot and SFT performance of Liquid on GEDIT-BENCH compared to other VLMs in Appendix C. training by uniformly sampling 30K videos per iteration from large-scale unlabelled video corpus, VIDGEN-1M (Tan et al., 2024). We extract the frame pairs from videos as in (Chen et al., 2025d). Each iteration is trained for one epoch, and the model alternates between FWM and IDM optimisation phases as described in Section 3.2. This protocol ensures clean comparison to baselines in (Qiu et al., 2025) while measuring the improvement across iterations. Benchmarks. We evaluate visual world modelling under two settings. Firstly, for single-turn next-observation prediction, we use AURORA-BENCH and BYTEMORPH, both of which formulate dynamics prediction as action-conditioned image editing tasks with strong emphasis on correctness in action-centric dynamics. Secondly, to evaluate long-horizon world modelling, we adopt WORLDPREDICTIONBENCH, which supports multi-step rollouts of up to four future observations across five subtasks, enabling comprehensive assessment of FWM consistency. To validate the generalisation of SWIRL, we conduct experiments across three grounded environments on LLM, each is unique scheme of state transition. We utilise SCIENCEWORLD (Wang et al., 2022) to evaluate physical dynamics, where LLM must predict the textual consequences of scientific actions within simulated world. We also employ MIND2WEB (Deng et al., 2023), which challenges the model to forecast the updated HTML DOM tree following user interactions (e.g., clicks) on web elements. Finally, we assess functional tool dynamics via STABLETOOLBENCH (Guo et al., 2024), where the objective is to simulate the execution output of API calls conditioned on the current conversational state and the invoked tool. Baselines. For visual world modelling, we compare SWIRL against directly fine-tuned Liquid model (SFT). We reproduce two strong baselines from Qiu et al. (2025): TEST-TIME VERIFICATION, which selects the best sample from the fine-tuned Liquid based on IDM scores, and BOOTSTRAP, which fine-tunes Liquid on silver data annotated by the IDM. We also include specialised diffusion-based image editing models, including GoT (Fang et al., 2025), SmartEdit (Huang et al., 2024), InstructPix2Pix (Brooks et al., 2023), and the Chameleon model family, following the protocol of Qiu et al. (2025). To position our approach against recent (larger or diffusion-based) unified VLMs, we also evaluate BAGEL (Deng et al., 2025), OmniGen (Xiao et al., 2025), OmniGen2 (Wu et al., 2025), BLIP3o-NEXT (Chen et al., 2025c), and UniWorld-V1 (Lin et al., 2025). For text-based environments, prior work under this formulation is limited. We therefore primarily compare against SFT baselines and larger LLMs to ensure fair comparison. 5 Self-Improving World Modelling with Latent Actions Table 1. Quantitative comparison on AURORA-BENCH. We report GPT-4o-as-a-judge scores, DiscEdit (DE), and CLIP scores across five datasets (and their Average). Blue cells indicate where SWIRL outperforms Liquid-SFT , its direct baseline. METHOD UNIFIED VLMS BAGEL-14B OMNIGEN2 BLIP3O-NEXT OMNIGEN UNIWORLD-V1 EXISTING BASELINES INSTRUCTPIX2PIX GOT SMARTEDIT CHAMELEON-SFT CHAMELEON-BOOTSTRAP MAGICBRUSH ACTION GENOME SOMETHING WHATSUP KUBRIC AVERAGE GPT-4O DE CLIP GPT-4O DE CLIP GPT-4O DE CLIP GPT-4O DE CLIP GPT-4O DE CLIP GPT-4O DE CLIP 8.14 7.04 3.42 6.86 7. 0.44 0.95 0.48 0.92 0.70 0.74 0.48 0.94 0.40 0.91 6.72 4.96 3.04 5.70 7.06 0.08 0.90 0.12 0.90 0.44 0.67 0.18 0.85 0.10 0.88 6.90 6.00 2.86 7.11 7.37 0.16 0.82 0.21 0.82 0.35 0.66 0.46 0.80 0.25 0.77 5.40 6.60 3.00 7.52 8. 0.22 0.95 0.28 0.92 0.40 0.78 0.24 0.93 0.46 0.91 5.82 5.54 3.40 5.69 6.76 0.18 0.94 0.14 0.89 0.54 0.74 0.16 0.91 0.42 0.84 6.44 6.05 3.14 6.59 7.36 0.22 0.91 0.25 0.89 0.49 0.72 0.30 0.89 0.33 0.86 3.12 1.20 0.96 0.00 1.88 5.96 1.61 2.62 1.58 3.92 6.71 3.08 2.81 0.76 3.70 2.52 2.48 3.11 0.88 7.30 3.27 2.74 3.11 0.98 7.30 1.43 3.14 3.41 3.26 3.48 OUR BASELINES LIQUID-SFT LIQUID-BOOTSTRAP LIQUID-SFT W/ TEST-TIME VERIFICATION 5.46 5.27 0.28 0.92 0.30 0.89 2.76 3.02 0.34 0.79 0.42 0. 3.00 2.86 0.27 0.74 0.29 0.75 3.60 3.88 0.32 0.85 0.28 0.87 7.00 5.57 0.60 0.90 0.30 0. 4.36 4.11 0.36 0.84 0.32 0.84 = 2 = 4 = 8 SWIRL (IDM FWM) SWIRL (ITERATIVE) SWIRL (ITER. + SHARE) 5.71 6.10 6.18 6.48 6.62 6. 0.14 0.93 0.14 0.93 0.16 0.92 0.26 0.92 0.30 0.92 0.28 0.92 3.00 3.38 3.28 3.58 3.52 3.40 0.38 0.80 0.36 0.81 0.36 0.81 0.36 0.80 0.48 0.80 0.40 0. 3.40 3.44 3.74 3.44 3.96 3.73 0.21 0.76 0.25 0.76 0.21 0.74 0.33 0.74 0.21 0.74 0.31 0.74 4.38 4.38 4.48 4.08 4.32 4. 0.36 0.86 0.40 0.87 0.28 0.86 0.32 0.86 0.36 0.86 0.30 0.85 6.18 6.04 6.28 6.59 6.96 7.45 0.44 0.90 0.38 0.90 0.46 0.90 0.50 0.90 0.54 0.90 0.62 0. 4.53 4.66 4.77 4.83 5.06 5.00 0.31 0.85 0.31 0.85 0.29 0.85 0.36 0.84 0.38 0.84 0.39 0.84 Evaluation Metrics. For visual forward world modelling, we adopt GPT-4o-as-a-judge for holistic evaluation, following (Qiu et al., 2025; Fang et al., 2025) with 10-point scheme. We also report DiscEdit (DE) and CLIP scores for AURORA-BENCH as in (Krojer et al., 2024). For text environments, we use BLEU, BERTScore, and ROUGE-L. 4.2. Single-turn Visual Dynamics Prediction AURORA-BENCH. We present the quantitative evaluation of visual world modelling on AURORA-BENCH in Table 1. We compare SWIRL against state-of-the-art unified VLMs, specialised diffusion-based editing models, and ablations of our method. The primary comparison of interest is against Liquid-SFT, our direct supervised fine-tuning baseline. As highlighted in blue, SWIRL delivers consistent and significant improvements across all five benchmarks. Compared to the baselines from (Qiu et al., 2025), SWIRL surpasses the bootstrapping strategy using IDM to synthesise trajectories from unlabelled videos, and computation-heavy inference techniques like Test-time Verification (with up to = 8 samples). Our best iterative strategy raises the average GPT4o evaluation score from 4.36 (SFT) to 5.06, which also outperforms the non-iterative supervision from IDM only (SWIRL (IDM FWM)) at 4.83, indicating the effectiveness of SWIRL by alternating policy and reward roles between FWM and IDM. Furthermore, despite starting from weaker base model such as Liquid, and relying on more Table 2. GPT4o-as-a-judge on the ByteMorph Benchmark. In addition to reporting unified VLMs performance, blue cells indicate where SWIRL outperforms Liquid-SFT , its direct baseline. METHOD BAGEL-14B OMNIGEN OMNIGEN2 BLIP3O-NEXT UNIWORLD-V1 LIQUID-SFT SWIRL (IDMFWM) (ITERATIVE) (ITER. + SHARE) Camera Zoom Camera Motion Object Motion Human Motion Interaction Average 32.89 47.76 49.08 24.21 55. 49.34 52.17 55.39 13.16 68.03 49.33 44.88 61.67 24.33 55.91 45.03 43.59 65.52 34.22 48.10 40.89 40.92 61.63 28.53 58.03 44.48 44.40 60.14 27.28 55.56 57. 56.51 43.13 38.07 40.70 43.38 57.37 54.08 53. 50.13 58.22 55.86 62.50 58.69 62.10 48.69 48.08 53.78 56.53 54.50 53.81 54.57 53.77 55.72 lightweight post-training,4 SWIRL remains highly competitive with other state-of-the-art unified VLMs such as OmniGen2 and BAGEL, while substantially outperforming diffusion-based editors like InstructPix2Pix. Qualitative examples for SWIRL are presented in Appendix J. BYTEMORPH. As the BYTEMORPH results in Table 2 show, all variants of SWIRL substantially raise the Average score over the Liquid-SFT baseline, e.g. SWIRL ITERATIVE goes from 43.38 to 53.77 (+26.4%). Performance 4We use only around 400K samples from (Qian et al., 2025; Krojer et al., 2024) to initialise the editing ability of Liquid. 6 Self-Improving World Modelling with Latent Actions Table 3. Long-Horizon World Modelling on WORLDPREDICTIONBENCH. We report GPT-4o-as-a-judge scores. The left section averages performance across the available horizon for each subtask (Turns 16 for EgoExo, EPIC, IKEA; Turns 14 for COIN, CrossTask). The right section details the decay of the Overall score at each prediction turn. Blue cells indicate where SWIRL surpasses Liquid-SFT . MODEL BAGEL OMNIGEN2 BLIP3O-NEXT OMNIGEN UNIWORLD-V1 LIQUID-SFT SWIRL (IDM FWM) SWIRL (ITERATIVE) SWIRL (ITER.+SHARE) AVERAGE TASK SCORE (HORIZON 16) OVERALL SCORE BY TURN COIN CROSS EGOEXO EPIC IKEA AVG. 4.31 3.78 1.74 3.61 3.27 2.11 2.61 2.22 2.06 3.94 3.53 1.58 3.48 3.42 2.06 2.45 2.01 2. 2.88 2.73 1.24 3.20 3.20 1.59 1.84 1.76 1.82 3.37 3.01 1.37 3.14 2.57 1.80 1.95 1.86 1.66 3.01 3.24 1.70 3.56 3.39 1.35 1.56 1.49 1. 3.30 3.28 1.31 3.45 3.35 1.63 1.89 1.74 1.68 T=1 4.29 3.25 2.46 3.25 3.40 3.09 3.24 3.23 2.95 T= 4.10 2.64 1.82 2.83 3.41 2.09 2.42 2.08 2.04 T=3 3.47 4.05 1.20 4.00 3.68 1.40 1.85 1.53 1.53 T= 3.22 3.75 0.95 3.92 3.59 1.17 1.59 1.32 1.24 T=5 2.47 3.06 0.83 3.28 3.09 1.07 1.25 1.15 1.23 T= 2.23 3.11 0.63 3.45 2.97 0.97 1.08 1.11 1.11 Table 4. Main Results on Textual Environments. We evaluate performance on SCIENCEWORLD, MIND2WEB, and STABLETOOLBENCH. We report BERTScore (BS) and ROUGE-L (R-L) for the first two tasks, and BLEU across all tasks for STABLETOOLBENCH as in (Guo et al., 2024). Bold indicates the best performance in each column, blue cells indicate where SWIRL outperforms Liquid-SFT . SCIENCEWORLD MIND2WEB STABLETOOLBENCH MODEL BS R-L BS R-L ID HIGH ID LOW ID MED OOD OOD FAIL AVERAGE QWEN-2.5-3B-INSTRUCT QWEN-2.5-7B-INSTRUCT QWEN-2.5-14B-INSTRUCT QWEN-2.5-32B-INSTRUCT OLMO-3-7B-INSTRUCT DEEPSEEK-7B-CHAT QWEN-2.5-3B-SFT QWEN-2.5-3B-SWIRL 82.87 80.48 81.46 81.01 79.27 81.90 96.06 96.06 18.68 12.50 15.52 14.53 11.28 16.35 89.73 89.92 82.11 77.03 75.43 79.01 76.05 80. 18.01 7.35 3.02 11.86 6.00 14.32 7.74 5.93 6.23 7.13 8.28 5.38 92.37 92.44 48.97 49.10 16.03 16.47 7.74 9.89 8.68 7.13 13.33 15. 12.87 16.90 9.48 8.30 8.58 10.22 7.62 4.44 17.51 21.20 6.59 9.17 6.48 7.75 9.15 8.52 14.86 15.57 3.18 3.20 3.27 4.34 7.54 4. 2.99 2.92 6.95 7.30 6.65 7.31 9.18 7.54 12.85 14.61 on global camera control (Camera Zoom/Motion) remains comparable. This is expected since the unlabelled in-thewild videos used during the RL phase (e.g., VIDGEN-1M) are predominantly static and provide limited supervision for camera control. In contrast, SWIRL yields pronounced improvements on Object/Human Motion and Interaction, indicating effective learning of fine-grained dynamics. Notably, SWIRL matches the performance of larger or more heavily supervised VLMs (e.g., BAGEL and UniWorld-v1). 4.3. Multi-turn Visual Dynamics Prediction WORLDPREDICTIONBENCH. To assess long-horizon world modelling in addition to single-step prediction, we evaluate performance on WORLDPREDICTIONBENCH. The model must predict future observations autoregressively up to = 6 time steps, using its own previous predictions as context. This setup tests the models ability to maintain physical consistency and resist the compounding covariate shift inherent in sequential generation. Table 3 summarises the GPT-4o-as-a-judge scores across all subsets in WORLDPREDICTIONBENCH. The best variant of our models demonstrates significant improvement in temporal consistency compared to Liquid-SFT. While SFT yields competitive performance at the immediate next step (T = 1), it suffers from rapid degradation as the horizon increases, dropping from score of 3.09 to 0.97 by = 6. In contrast, SWIRL (ITERATIVE) maintains significantly higher fidelity throughout the rollout trajectory, achieving +14.4% relative improvement over the baseline at = 6 (1.11). We also remark that repeated selfimprovement is not beneficial in this dataset, as performance peaks at the first iteration (IDM FWM). 4.4. Textual Environments Table 4 shows the quantitative evaluation on textual environments. When applied to Qwen-2.5-3B-Instruct in physical and digital simulation environments (SCIENCEWORLD and MIND2WEB), both SWIRL and SFT are near-saturation in semantic accuracy (>92 BERTScore) and comparable in exact lexical matching (ROUGE-L). The advantage of our approach becomes most pronounced in STABLETOOLBENCH, which requires the simulation of API execution outcomes. Here, we observe substantial improvements in generalisation capabilities, with our method surpassing SFT by +4.03 7 Self-Improving World Modelling with Latent Actions Figure 3. We compare our proposed SWIRL against SFT baselines (continual training and merging) across five benchmarks in AURORA-BENCH. The x-axis represents the number of training samples. Our method (solid blue line) demonstrates superior data efficiency, achieving higher GPT-4o evaluation scores. the inherent ambiguity of visual dynamics and action verbalisation. SFT enforces strict token-level imitation of the IDMs specific pseudo-labels; however, single visual transition often corresponds to multiple valid descriptions. Forcing the model to mimic one specific verbalisation can lead to overfitting noise and suppressing valid alternative predictions. In contrast, our GRPO framework effectively relaxes this constraint by encouraging consistency assessed by IDM, rather than exact replication. By exploring the FWMs rollout space and action trajectories that the IDM model recognises as physically plausible, SWIRL learns more robust and generalisable FWM that is not limited by the specific phrasing of the teacher annotations. Sharing θ and ϕ. We study the trade-off between parameter efficiency and training stability by comparing sharedand separate-weight designs (Figure 2 bottom and Table 1). Using separate weights yields stable optimisation, with the IDM score improving monotonically from 6.37 to 6.57 over three iterations (Figure 2, Left). In contrast, shared weights (θ = ϕ) reduce memory footprint but introduce instability, as reflected by performance drop at Iteration 3 (to 6.52). This instability is also reflected in downstream performance: on AURORA-BENCH, shared weights slightly underperform the separate variant (5.06 vs. 5; Table 1), and on longhorizon evaluation  (Table 3)  , the shared model consistently lags behind (e.g., 1.74 vs. 1.68 overall). We attribute this gap to gradient interference between the heterogeneous objectives of FWM (visual generation) and IDM (inferring the linguistic action), suggesting that more robust unification mechanisms are needed to effectively share representations across modalities (Shi et al., 2025; Ma et al., 2025; Qu et al., 2025; Zheng et al., 2025). 5. Conclusion We introduce SWIRL, unified framework for enabling VLMs and LLMs to intrinsically model future states conditioned on the current state and latent actions, without relying Figure 2. Performance of FWM and IDM in each iteration of SWIRL. We visualise the training dynamics across iterations for two settings: maintaining separate weights (Left) versus sharing parameters (Right). We present the evaluation performance on top, and the training rewards in the bottom. We present only the first iterations reward curves for brevity. BLEU on ID-Low and +3.69 BLEU on ID-Medium splits. This establishes new state-of-the-art for this model scale, outperforming also significantly larger open-weight instruction models (e.g., Qwen-2.5-32B, DeepSeek-7B). Overall, this suggests that the SWIRL can generalise to internalise complex API dynamics effectively for LLMs. 4.5. Analysis Iterative RL. We run an analysis to determine the potential for cumulative gain through iterative self-improvement beyond the first round. Figure 2 tracks the training dynamics across three iterations, where each iteration consists of updating the FWM using the current IDM as reward (Phase I), followed by updating the IDM using the improved FWM as reward (Phase II). We observe clear virtuous cycle: enhancing the FWMs forecasting capability enables it to provide more robust verification for action prediction, which in turn yields more precise reward signal for the subsequent FWM updates. The reward curves demonstrate that FWM and IDM in separate-weight setup improve the training rewards effectively, and their optimisation converges with GRPO. RL vs SFT. To isolate the contribution of the optimisation objective, we compare SWIRL against direct SFT using the same set of unlabelled videos initially annotated by our IDM model. As shown in Figure 3 with details in F, SWIRL significantly outperforms the SFT baselines (both continued training and data merging), which stagnate or degrade as data scales. We attribute this disparity to Self-Improving World Modelling with Latent Actions on human-annotated trajectories. By interpreting actions as latent variables and alternately optimising forward world modelling and inverse dynamics modelling objectives with GRPO, our method induces self-improving world modelling purely from unlabelled data. We provide theoretical guarantees establishing the learnability of each optimisation phase, formally linking our objectives to variational mutual information bounds and evidence lower bounds. Empirically, we demonstrate the effectiveness of SWIRL across diverse settings, including real-world visual dynamics with VLMs and textual environments (physical and digital simulations and tool calling) with LLMs, validating its generality."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents SWIRL, framework that enables LLMs and VLMs to self-improve their internal world models using unlabelled data. By reducing reliance on expensive human-annotated trajectories, our work advances the efficiency and accessibility of training capable reasoning agents. This has positive implications for the development of general-purpose assistants that can better understand physical dynamics and digital environments. However, we acknowledge potential societal consequences associated with these capabilities. First, our method utilises self-improving loops on unlabelled in-the-wild data. Without human curation, there is risk that the model may internalise, reinforce, or amplify biases and harmful patterns present in the raw distribution of web and video data. Future work employing this framework should incorporate safety filters or constitutional AI principles within the reward mechanism to mitigate this risk. Second, the improvements in Visual Dynamics Prediction and Web HTML interaction imply step forward in generative video capabilities and autonomous web agents. While these advancements aid in creative tools and automation, they also lower the barrier for generating deepfakes or creating agents capable of bypassing web-based security measures (e.g., CAPTCHAs) or conducting automated interactions at scale. We emphasise the necessity of developing robust detection tools for synthetic media and implementing strict access controls and guardrails for autonomous agents operating in real-world web environments."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is supported by the ERC Starting Grant AToMFM (101222956) awarded to Edoardo M. Ponti. The authors acknowledge the use of resources provided by the IsambardAI National AI Research Resource (AIRR). Isambard-AI is operated by the University of Bristol and is funded by the UK Governments Department for Science, Innovation and Technology (DSIT) via UK Research and Innovation; and the Science and Technology Facilities Council [ST/AIRR/IA-I/1023] (McIntosh-Smith et al., 2024)."
        },
        {
            "title": "References",
            "content": "Barber, D. and Agakov, F. The im algorithm: variational approach to information maximization. In Proceedings of the 17th International Conference on Neural Information Processing Systems, NIPS03, pp. 201208, Cambridge, MA, USA, 2003. MIT Press. Bensal, S., Jamil, U., Bryant, C., Russak, M., Kamble, K., Mozolevskyi, D., Ali, M., and AlShikh, W. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726, 2025. Brooks, T., Holynski, A., and Efros, A. A. InstructPix2Pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Chang, D., Cao, M., Shi, Y., Liu, B., Cai, S., Zhou, S., Huang, W., Wetzstein, G., Soleymani, M., and Wang, P. Bytemorph: Benchmarking instruction-guided imarXiv preprint age editing with non-rigid motions. arXiv:2506.03107, 2025. Chen, D., Chung, W., Bang, Y., Ji, Z., and Fung, P. Worldprediction: benchmark for high-level world modeling and long-horizon procedural planning. arXiv preprint arXiv:2506.04363, 2025a. Chen, D., Moutakanni, T., Chung, W., Bang, Y., Ji, Z., Bolourchi, A., and Fung, P. Planning with reasoning arXiv preprint using vision language world model. arXiv:2509.02722, 2025b. Chen, J., Xue, L., Xu, Z., Pan, X., Yang, S., Qin, C., Yan, A., Zhou, H., Chen, Z., Huang, L., et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025c. Chen, X., Zhang, Z., Zhang, H., Zhou, Y., Kim, S. Y., Liu, Q., Li, Y., Zhang, J., Zhao, N., Wang, Y., et al. Unireal: Universal image generation and editing via learning realworld dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025d. Copet, J., Carbonneaux, Q., Cohen, G., Gehring, J., Kahn, J., Kossen, J., Kreuk, F., McMilin, E., Meyer, M., Wei, Y., et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., et al. Emu3. 5: Native 9 Self-Improving World Modelling with Latent Actions multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Fang, R., Duan, C., Wang, K., Huang, L., Li, H., Yan, S., Tian, H., Zeng, X., Zhao, R., Dai, J., et al. GoT: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Gao, Q., Pi, X., Liu, K., Chen, J., Yang, R., Huang, X., Fang, X., Sun, L., Kishore, G., Ai, B., et al. Do vision-language models have internal world models? towards an atomic evaluation. arXiv preprint arXiv:2506.21876, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Guo, S., Domingues, O. D., Avalos, R., Courville, A., and Strub, F. Sample, predict, then proceed: Selfverification sampling for tool use of llms. arXiv preprint arXiv:2506.02918, 2025b. Guo, Z., Cheng, S., Wang, H., Liang, S., Qin, Y., Li, P., Liu, Z., Sun, M., and Liu, Y. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714, 2024. He, J., Lin, H., Wang, Q., Fung, Y. R., and Ji, H. Selfcorrection is more than refinement: learning framework for visual and language reasoning tasks. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 64056421, 2025. Huang, J., Gu, S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. In Proceedings of the 2023 conference on empirical methods in natural language processing, pp. 10511068, 2023. Huang, Y., Xie, L., Wang, X., Yuan, Z., Cun, X., Ge, Y., Zhou, J., Dong, C., Huang, R., Zhang, R., et al. SmartEdit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 83628371, 2024. Jin, W., Niu, Y., Liao, J., Duan, C., Li, A., Gao, S., and Liu, X. Srum: Fine-grained self-rewarding for unified multimodal models. arXiv preprint arXiv:2510.12784, 2025. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Krojer, B., Vattikonda, D., Lara, L., Jampani, V., Portelance, E., Pal, C., and Reddy, S. Learning Action and ReasoningCentric Image Editing from Videos and Simulations. In NeurIPS, 2024. URL https://arxiv.org/abs/ 2407.03471. Spotlight Paper. Lee, N., Cai, Z., Schwarzschild, A., Lee, K., and Papailiopoulos, D. Self-improving transformers overcome easyto-hard and length generalization challenges. In Fortysecond International Conference on Machine Learning. Lehrach, W., Hennes, D., Lazaro-Gredilla, M., Lou, X., Wendelken, C., Li, Z., Dedieu, A., Grau-Moya, J., Lanctot, M., Iscen, A., et al. Code world models for general game playing. arXiv preprint arXiv:2510.04542, 2025. Li, Y., Wang, H., Qiu, J., Yin, Z., Zhang, D., Qian, C., Li, Z., Ma, P., Chen, G., Ji, H., et al. From word to world: Can large language models be implicit text-based world models? arXiv preprint arXiv:2512.18832, 2025. Lin, B., Li, Z., Cheng, X., Niu, Y., Ye, Y., He, X., Yuan, S., Yu, W., Wang, S., Ge, Y., et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Liu, E., Gangal, V., Zou, C., Huang, X., Yu, M., Chang, A., Tao, Z., Kumar, S., and Feng, S. Y. unified definition of hallucination, or: Its the world model, stupid. arXiv preprint arXiv:2512.21577, 2025a. Liu, Z., Huan, Z., Wang, X., Lyu, J., Tao, J., Li, X., Huang, F., and Xu, H. World models with hints of large language models for goal achieving. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 5072, 2025b. Ma, C., Jiang, Y., Wu, J., Yang, J., Yu, X., Yuan, Z., Peng, B., and Qi, X. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Mao, W., Yang, Z., and Shou, M. Z. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380, 2025. 10 Self-Improving World Modelling with Latent Actions McIntosh-Smith, S., Alam, S. R., and Woods, C. Isambardai: leadership class supercomputer optimised specifically for artificial intelligence, 2024. URL https: //arxiv.org/abs/2410.11199. Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S. A., Yan, T., Brown, L., Fan, Q., Gutfruend, D., Vondrick, C., et al. Moments in time dataset: one million videos for event understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 18, 2019. ISSN 0162-8828. doi: 10.1109/TPAMI.2019.2901464. Qian, Y., Bocek-Rivele, E., Song, L., Tong, J., Yang, Y., Lu, J., Hu, W., and Gan, Z. Pico-banana-400k: largescale dataset for text-guided image editing. arXiv preprint arXiv:2510.19808, 2025. Qiu, X., Jia, H., Zeng, Z., Shen, S., Meng, C., Yang, Y., and Zhu, L. Unified generation and self-verification for vision-language models via advantage decoupled preference optimization. arXiv preprint arXiv:2601.01483, 2026. Qiu, Y., Zhao, Z., Ziser, Y., Korhonen, A., Ponti, E., and Cohen, S. B. Are large language model temporally grounded? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 70577076, 2024. Tehenan, M., Moya, C. B., Long, T., and Lin, G. Linear spatial world models emerge in large language models. arXiv preprint arXiv:2506.02996, 2025. Vafa, K., Chen, J. Y., Rambachan, A., Kleinberg, J., and Mullainathan, S. Evaluating the world model implicit in generative model. Advances in Neural Information Processing Systems, 37:2694126975, 2024. Wang, J., Yan, Q., Wang, Y., Tian, Y., Mishra, S. S., Xu, Z., Gandhi, M., Xu, P., and Cheong, L. L. Reinforcement learning for self-improving agent with skill library. arXiv preprint arXiv:2512.17102, 2025a. Wang, K., Zhang, P., Wang, Z., Gao, Y., Li, L., Wang, Q., Chen, H., Lu, Y., Yang, Z., Wang, L., et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Wang, Q., Huang, W., Zhou, Y., Yin, H., Bao, T., Lyu, J., Liu, W., Zhang, R., Wu, J., Fei-Fei, L., et al. Enact: Evaluating embodied cognition with world modeling of egocentric interaction. arXiv preprint arXiv:2511.20937, 2025b. Wang, R., Jansen, P., Côté, M.-A., and Ammanabrolu, P. Scienceworld: Is your agent smarter than 5th grader? arXiv preprint arXiv:2203.07540, 2022. Qiu, Y., Ziser, Y., Korhonen, A., Cohen, S. B., and Ponti, E. M. Bootstrapping world models from dynamics models in multimodal foundation models. arXiv preprint arXiv:2506.06006, 2025. Wang, Y., Yang, L., Tian, Y., Shen, K., and Wang, M. Cure: Co-evolving coders and unit testers via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025c. Qu, L., Zhang, H., Liu, Y., Wang, X., Jiang, Y., Gao, Y., Ye, H., Du, D. K., Yuan, Z., and Wu, X. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25452555, 2025. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Shi, Y., Dong, Y., Ding, Y., Wang, Y., Zhu, X., Zhou, S., Liu, W., Tian, H., Wang, R., Wang, H., et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025. Soomro, K., Zamir, A. R., and Shah, M. UCF101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Tan, Z., Yang, X., Qin, L., and Li, H. Vidgen-1m: largescale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., Liu, Z., Xia, Z., Li, C., Deng, H., Wang, J., Luo, K., Zhang, B., Lian, D., Wang, X., Wang, Z., Huang, T., and Liu, Z. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. Wu, J., Jiang, Y., Ma, C., Liu, Y., Zhao, H., Yuan, Z., Bai, S., and Bai, X. Liquid: Language models are scalable and unified multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. Xiang, J., Liu, G., Gu, Y., Gao, Q., Ning, Y., Zha, Y., Feng, Z., Tao, T., Hao, S., Shi, Y., et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Li, C., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Self-Improving World Modelling with Latent Actions Xie, K., Yang, I., Gunerli, J., and Riedl, M. Making large language models into world models with precondition and effect knowledge. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 75327545, 2025. Xiong, Z., Ye, X., Yaman, B., Cheng, S., Lu, Y., Luo, J., Jacobs, N., and Ren, L. Unidrive-wm: Unified understanding, planning and generation world model for autonomous driving. arXiv preprint arXiv:2601.04453, 2026. Zhang, J., Li, T., Li, L., Yang, Z., and Cheng, Y. Are unified vision-language models necessary: Generalization across understanding and generation. arXiv preprint arXiv:2505.23043, 2025. Zhao, C., Jia, X., Viswanathan, V., Wu, T., and Neubig, G. Self-guide: Better task-specific instruction folarXiv preprint lowing via self-synthetic finetuning. arXiv:2407.12874, 2024. Zheng, D., Zhang, M., Li, H., Zou, K., Liu, H., Guo, Z., Feng, K., Liu, Y., Luo, Y., Feng, Y., et al. Architecture decoupling is not all you need for unified multimodal model. arXiv preprint arXiv:2511.22663, 2025. 12 A. Theoretical Derivation Self-Improving World Modelling with Latent Actions In this section, we provide the detailed theoretical justification for SWIRL. We formalise the interaction between the Forward World Modelling (FWM) and Inverse Dynamics Model (IDM) as an alternating maximisation of Identifiability (via Variational Mutual Information) and Data Fidelity (via the Evidence Lower Bound). We define the following notation: S: The source observation (current state st). S: The ground-truth target observation (next state st+1), distributed according to the data distribution D(yx). ˆy S: generated target observation sampled from the forward model. A: The latent action driving the transition. We employ two parametrised models: 1. Forward World Modelling (FWM): Pθ(ˆyx, z), parametrised by θ. FWM parametrised the environments transition distribution. 2. Inverse Dynamics Model (IDM): Qϕ(zx, y), parametrised by ϕ. IDM parametrised an approximate posterior over latent actions given state transition. A.1. Phase I: Learnability of the Forward Model (FWM) In the first phase, we freeze the IDM parameters ϕ and optimise the FWM parameters θ. Our objective is to generate trajectories ˆy that are identifiable by the inference model. We formalise this as maximising the Conditional Mutual Information (CMI) between the latent action and the generated observation ˆY , conditioned on the source state X. Crucially, in our algorithm, the latent actions used for training are not sampled from fixed uninformative prior, but are inferred from the ground-truth data using the current IDM. We denote this Empirical Belief Distribution as (zx): (zx) EyD(yx) [Qϕ(zx, y)] Theorem A.1. Optimising the FWM to maximise the likelihood assigned by the frozen IDM to generated samples maximises Variational Lower Bound of the Conditional Mutual Information (Z; ˆY X) defined over the empirical belief distribution. (6) Proof. The Conditional Mutual Information under the joint distribution induced by the data and the frozen IDM is: (Z; ˆY X) = (ZX) H(Z ˆY , X) (7) The term (ZX) is the entropy of the marginal distribution of actions inferred from the dataset. Since ϕ is frozen in this phase and the dataset is fixed, (zx) is constant with respect to θ. Therefore, maximising the mutual information is equivalent to minimizing the conditional entropy H(Z ˆY , X). The conditional entropy is defined as: H(Z ˆY , X) = ExDE P (zx) EˆyPθ(x,z) [log (zˆy, x)] (8) The true posterior (zˆy, x) is intractable as it requires marginalising over the action space. We utilise the frozen IDM Qϕ(zx, ˆy) as variational approximation. By the non-negativity of the KL divergence DKL(P (zˆy, x)Qϕ(zx, ˆy)) 0, we have the lower bound: Eˆy[log (zˆy, x)] Eˆy[log Qϕ(zx, ˆy)] Substituting this into the entropy term yields the variational lower bound for the CMI: (Z; ˆY X) (ZX) + ExDE P (zx) EˆyPθ(x,z) [log Qϕ(zx, ˆy)] Expanding the definition of (zx) from Eq. (1), the optimization objective becomes: JFWM(θ) = ExDEyD(yx) (cid:2)EzQϕ(x,y) (cid:2)EˆyPθ(x,z) [log Qϕ(zx, ˆy)](cid:3)(cid:3) 13 (9) (10) (11) Self-Improving World Modelling with Latent Actions A.2. Phase II: Learnability of the Inverse Model (IDM) In the second phase, we freeze θ and optimise the IDM parameters ϕ. We seek to maximise the log-likelihood of the observed ground-truth dynamics log Pθ(yx) (Data Fidelity). We model this via the Evidence Lower Bound (ELBO), treating the initialised policy at the start of the iteration as the reference prior, denoted πref (zx). Theorem A.2. Optimising the IDM via the KL-regularised policy gradient objective with reward R(x, z, y) = log Pθ(yx, z), β = 1, and reference policy πref maximises the Evidence Lower Bound (ELBO). Proof. We express the marginal log-likelihood of the data by introducing the variational distribution Qϕ(zx, y): log Pθ(yx) = log (cid:88) zA Pθ(yx, z)πref (zx) = log EzQϕ(x,y) (cid:20) Pθ(yx, z)πref (zx) Qϕ(zx, y) (cid:21) Applying Jensens inequality (concavity of log): log Pθ(yx) EzQϕ [log Pθ(yx, z) + log πref (zx) log Qϕ(zx, y)] = EzQϕ [log Pθ(yx, z)] DKL(Qϕ(zx, y) πref (zx)) This is the standard Evidence Lower Bound. The KL-regularised policy gradient objective (Phase II) is defined as: (ϕ) = EzQϕ(x,y) [R(x, z, y)] β DKL(Qϕ(x, y) πref (x)) By setting the reward to the FWM log-likelihood, = log Pθ(yx, z), we observe that: (ϕ) ELBO(ϕ) (12) (13) (14) (15) Thus, the IDM update step performs coordinate ascent on the evidence lower bound of the data likelihood, ensuring the inferred actions explain the ground-truth transitions under the current forward dynamics. B. Implementation Details B.1. Hyperparameters For the LIQUID-SFT model, we fine-tune Liquid-7B (Wu et al., 2024) on PICO-BANANA-400K (Qian et al., 2025) and AURORAs training set. Training is performed for 5 epochs with batch size of 128. We use learning rate of 2 105 with cosine learning rate schedule, allocating 5% of the total training steps for warm-up. For the non-iterative setup (SWIRL (IDM FWM)), the model is trained with batch size of 64 and learning rate of 1 106. cosine learning rate schedule with 100 warm-up steps is applied. GRPO is used with rollout size of 8 and KL regularization coefficient β = 0.1. For the iterative setup (SWIRL (ITERATIVE)), we use batch size of 128 and learning rate of 2 107, together with cosine schedule and 50 warm-up steps. In the SWIRL (ITERATIVE + SHARE) variant, the learning rate is increased to 5 107 while all other settings remain unchanged. In both iterative GRPO setups, we set the decoding temperature to 0.75 and Top-P to 0.96 to control the quality of predicted futures. Additionally, we apply logit processor to constrain Liquids generation to image tokens for FWM and textual tokens for IDM. For LLM experiments on STABLETOOLBENCH, we fine-tune models using GRPO with DeepSpeed for memory-efficient distributed training. Optimization is performed with learning rate of 5 105 under cosine learning rate schedule with 25 warm-up steps, using an effective batch size of 128. For each prompt, we sample 64 rollouts. The maximum prompt and completion lengths are set to 8,126 and 4,096 tokens, respectively. Unless otherwise specified, we use KL regularization coefficient of β = 0.1. During GRPO rollout, we set the decoding temperature to 0.7 and Top-P to 0.96. For MIND2WEB and SCIENCEWORLD, we adopt the same configuration, except that the maximum prompt and completion lengths are both set to 4,096 tokens, and the number of GRPO rollouts is reduced to 16. 14 Self-Improving World Modelling with Latent Actions For VLM experiments, training is conducted using DeepSpeed on 32 NVIDIA H200 140GB GPUs for FWM and 8 NVIDIA H200 140GB GPUs for IDM. For LLM experiments, both FWM and IDM are trained on 8 NVIDIA Grace-Hopper (GH100) GPUs. B.2. Evaluation Prompt for GPT-4o as Judge For AURORA-BENCH, we use the same prompt as in Deng et al. (2025). For BYTEMORPH, we employ the official GPT-4o judge template provided by Chang et al. (2025). Since WORLDPREDICTIONBENCH (Chen et al., 2025a) does not natively support multi-turn image editing evaluation, we adopt the prompt template shown in Figure 4. This template is designed to balance the penalties for directly copying the source image and for excessive editing. WorldPrediction GPT-4o-as-a-Judge Prompt You are evaluating predicted observations for procedural plan. The plan belongs to dataset {dataset}, sample {sample_uid}. Overall plan: {plan_description}. Focus on step {step_index} of {total_steps}, where the action is {action_label}. You are given the source observation (input to the model) and the candidate observation (model output). There is NO ground-truth reference image. Your task is to judge whether the candidate correctly applies the stated action to the source. Do NOT score based on overall visual quality or realism. Evaluation rules: 1. Infer what *must change* and what *must remain unchanged* given the action. 2. Compare candidate against source to check whether the required change is present. 3. If the candidate is identical or near-identical to the source when the action implies visible change, assign low score. 4. Penalise unintended changes outside the scope of the action. Return JSON object like {\"score\": <0-10>, \"explanation\": \"...\"} summarising correctness. Figure 4. Prompt template used to instruct GPT-4o to evaluate multi-turn visual dynamics prediction in the WorldPrediction benchmark. C. Sanity Check Results for General Image Editing. Table 5 reports sanity-check evaluation of general image editing performance on GEDIT-BENCH. Although Liquid does not have native image editing support by design, we find that after our supervised fine-tuning (SFT) pipeline, using mixture of AURORA (Krojer et al., 2024) and PICO-BANANA-400K (Qian et al., 2025), the resulting model (Liquid-SFT) acquires functional image editing behaviour. We illustrate the qualitative examples for general image editing in Figure 5. Concretely, Liquid without SFT indicates totally failure in GEDIT-BENCH, indicating the necessarily in the warm-up stage before SWIRL. Liquid-SFT achieves non-trivial scores in both semantic alignment and perceptual quality, confirming that the model can follow image editing instructions and produce coherent visual outputs. While Liquid-SFT remains significantly behind state-of-the-art proprietary and public image editing models, this result is not intended to be competitive; rather, it serves as capability verification that our SFT pipeline successfully activates basic visual editing skills in base model not originally designed for this task. We emphasise that this capability plays an important role as warm-up stage for subsequent world modelling objectives. In later stages, the model is trained to (i) predict the next visual observation given the current state (forward world modelling), and (ii) perform inverse dynamics modelling (IDM), where the model receives pair of observations and predicts the intervening action in language. The ability to perform instruction-followed image transformations provides minimal but necessary foundation for these more structured visualtemporal reasoning tasks. Finally, the ablation study highlights the importance of PICO-BANANA-400K in our data mixture. Removing this component leads to consistent degradation across all metrics (average score dropping from 3.06 to 2.43), indicating that Pico-banana provides critical signal for stabilising and enriching visual instruction-following behaviour. This result suggests that even for non-native capabilities, carefully curated visual editing data is essential for preserving functional performance after SFT. 15 Self-Improving World Modelling with Latent Actions Table 5. General image editing performance on GEDIT-BENCH. As sanity check, we evaluate the image editing performance of our SFT baseline (Liquid-SFT) against state-of-the-art image editing models. We report Semantics, Quality, and the Average score. Results indicate that our model maintains functional editing capabilities after our SFT pipeline. Models Proprietary GPT-4o Gemini 2.0 Public VLMs Emu3.5-33B BAGEL-14B OmniGen2 OmniGen-v1-diffusers UniWorld-V1 BLIP3o-NEXT-edit-VAE Liquid Zero-shot Liquid-SFT Liquid-SFT w/o PICO-BANANA-400K Semantics Quality Average 7.85 6.73 8.27 7.36 6.35 5.51 5.05 3. 0.04 4.04 2.92 7.62 6.61 7.28 6.83 6.03 5.37 6.85 5.56 1.43 3.04 2.87 7.53 6.32 7.53 6.52 5.57 4.62 4.80 3. 0.03 3.06 2.43 Table 6. AURORA-BENCH Evaluation of IDM. We compare our proposed method against baselines on captioning/description quality. We report GPT-4 evaluation scores and reference-based metrics (BERTScore, ROUGE-L, BLEU). Blue cells indicate where ours positively improve Liquid-SFT. Method GPT4o BERT OmniGen2 Chameleon Liquid-AURORA Liquid-SFT SWIRL (FWM IDM) SWIRL (Iterative) SWIRL (Iter. + Share) 6.15 4.52 6.38 6.38 6.58 6.57 14.5 45.0 31.0 41.0 40.1 40.3 40.3 R-L 15.3 44.0 31.0 36.0 39.2 40.3 39.1 BLEU 7.5 20.0 28.0 21.0 20.3 21.1 21.1 We include the qualitative examples produced by our approach for general image editing in Figure 5. D. IDM Analysis AURORA-BENCH Evaluation. To validate the reliability of our reward mechanism, we evaluate the Inverse Dynamics Model (IDM) performance directly on AURORA-BENCH  (Table 6)  . fundamental premise of our framework is that inferring the action responsible for state transition (IDM) is more tractable task than generating high-dimensional future states (FWM). The results support this hypothesis: the IDM model achieves high action prediction accuracy (6.38 out of 10 as GPT score), serving as strong discriminator. Additionally, we observe that the iterative paradigm can also leverage the reward signal from FWM, effectively improving GPT4o scores from 6.38 to 6.58 as our best approach. This stability is vital for our self-improving, ensuring that the reward signal remains informative and accurately penalises physical inconsistencies. Interpretability of Latent Actions and Reward Hacking. potential concern in learning with the latent actions is reward hacking, where actions might degenerate into short, distinctive ciphers (e.g., single keywords or artifacts) to be hacked for an easier optimisable FWM. We analyse the evolution of complexities of the generated actions across three iterations for SWIRL in Figure 6. Contrary to the hypothesis of reward collapse, we observe that the uniqueness of generated actions remains consistently 16 Self-Improving World Modelling with Latent Actions Figure 5. Qualitative of SWIRL for general image editing including Subjects add/replace/remove, style transfer, altering the colour or background. Figure 6. Evolution of Latent Action Space throughout SWIRL. We analyse the semantic properties of generated actions across iterations. Uniqueness indicates the percentage of distinct actions in the generated set. Length denotes average token count. PPL Ratio denotes the ratio of predicted perplexity from an LLM (specifically, GPT-2) on the predicted action against the ground-truth action, which measures the naturalness of the actions in language form. During SWIRL, the model maintains high diversity and naturalness without collapsing into short ciphers. 17 Self-Improving World Modelling with Latent Actions high (> 94%) across all iterations for both shared and separate weight configurations. The similar observation happens for naturalness that the predicted actions are quite stably natural (as evidenced by the predicted perplexity of an LLM, GPT-2) throughout SWIRLs training iterations, without collapsing into any unnatural artifacts that could be easily hacked by FWM or IDM. Looking into the lengths of predicted actions, the model does not \"shortcut\" by generating brief tokens as well; instead, the average action length (around 16.8 tokens) consistently exceeds the ground truth average (around 7.2 tokens). This indicates that the IDM learns to provide more descriptive and detailed instructions to ensure the transition is consistently identifiable, rather than drifting into simplified cipher. Qualitative inspection confirms that predictions remain physically meaningful and there is no model collapse happening on SWIRL (e.g., \"tearing paper into two pieces\", \"Swap the positions of the two objects.\", \"turning bottle upside down\"), preserving natural language interpretability. E. Inference-time Verification As sanity check for SWIRL, we investigate the validity of the coverage hypothesis intrinsic to our GRPO formulation. For the IDM reward to effectively guide learning, the set of sampled rollouts must contain at least one candidate that sufficiently approximates the ground-truth future state. We evaluate the \"Best-of-N \" performance of Liquid-SFT, where the candidate with the highest GPT4o score is selected, by scaling the number of rollouts . In Table 1, we observe monotonic improvement for INFERENCE-TIME VERIFICATION as increases, confirming that the base policy possesses the latent capability to generate high-fidelity transitions in multiple rollouts. This finding empirically validates our training premise: expanding the rollout space should increases the likelihood of capturing valid transitions, thereby providing the IDM discriminator with the necessary high-quality positive examples to reinforce. F. Detailed Results for Comparing SFT and SWIRL. Table 7. Detailed Ablation: Data Efficiency (RL vs. SFT). We report GPT-4o evaluation scores across five benchmarks and the aggregated average. We compare our proposed Reinforcement Learning (RL) fine-tuning against two Supervised Fine-Tuning (SFT) baselines: SFT-Continue (continual training with the additional samples) and SFT-Merge (we concatenate all samples and fine-tune model). Columns represent the number of training samples seen. Bold indicates the best performance for that specific sample count. Rows highlighted in blue denote our method. Benchmark Method 3.2k 6.4k 9.6k 12.8k Number of Training Samples MagicBrush Action Genome Something Whatsup Kubric Average (All) SFT-Continue SFT-Merge SWIRL (RL) SFT-Continue SFT-Merge SWIRL (RL) SFT-Continue SFT-Merge SWIRL (RL) SFT-Continue SFT-Merge SWIRL (RL) SFT-Continue SFT-Merge SWIRL (RL) SFT-Continue SFT-Merge SWIRL (RL) 5.50 5.62 4.71 2.62 2.76 2. 2.90 2.92 3.42 3.18 2.69 3.80 5.81 6.68 6.90 3.98 4.15 4.27 4.94 5.56 5.56 2.72 2.98 3. 2.86 2.54 3.31 3.69 2.62 4.26 5.51 6.66 6.70 3.94 4.07 4.63 4.70 4.80 6.61 2.82 2.94 3. 2.70 2.66 3.10 3.16 2.74 4.04 5.90 6.69 6.51 3.84 3.95 4.67 4.56 5.62 6.19 2.88 2.58 3. 2.74 2.56 3.38 3.51 2.86 4.12 5.79 6.49 6.65 3.88 4.01 4.73 Table 7 presents detailed comparison between SWIRL and two supervised fine-tuning baselines under controlled data Self-Improving World Modelling with Latent Actions budgets. Across all five benchmarks and all training sample counts, RL consistently exhibits superior data efficiency compared to both SFT-Continue and SFT-Merge. While SFT baselines often plateau or even degrade as more samples are introduced, RL shows clear trend of performance improvement as additional data becomes available. Notably, RL begins to outperform SFT at relatively small data scales (6.4K samples) and the performance gap widens as training proceeds. On Action Genome, Something, and Whatsup, Kubric, which emphasises structured physical dynamics, RL achieves the strongest or near-strongest performance at every scale, indicating that reward-guided optimisation better captures latent world structure than direct imitation. Aggregated across all benchmarks, RL achieves the highest average score at every data scale, with gains becoming increasingly pronounced at larger budgets (4.27 4.73). These results suggest that RL not only improves final performance but also utilises limited data more effectively, aligning with our hypothesis that IDMs rewarding mechanism provides stronger inductive bias for intrinsic world modeling than purely supervised objectives. G. Detailed Results for WORLDPREDICTIONBENCH Table 8 presents granular analysis of predictive performance across six consecutive autoregressive turns on WorldPredictionBench. This evaluation creates challenging stress test for temporal consistency, as errors generated in early turns compound over the predictive horizon. We observe two primary trends. First, as expected, all models exhibit performance decay as the horizon increases (N decreases as tasks are completed or fail). However, clear divergence emerges between training paradigms. The direct supervised baseline (Liquid-SFT) suffers from rapid degradation, dropping from an overall score of 3.09 in Turn 1 to 1.17 by Turn 4. In contrast, our proposed SWIRL demonstrate significantly improved robustness against this compounding error. The Ours (Best) configuration, maintains score of 1.59 at Turn 4. This indicates that the reciprocal cycle does not merely memorise single-step transitions but internalises more robust physical dynamics that persist over long-horizon rollouts. While large-scale unified models like Bagel provide high upper bound, our method close the gap compared to SFT baseline. Table 8. Long-Horizon Evaluation on WorldPredictionBench. We report GPT-4o evaluation scores across 6 consecutive prediction turns. We compare our proposed method variants against the direct SFT baseline (Liquid-SFT), the strong unified baseline (Bagel), and other state-of-the-art VLMs. Rows highlighted in blue denote our methods. The sample count (N ) decreases in later turns as completed tasks are filtered out. Bold indicates the best performance among the Liquid-based family (SFT vs. Ours). Method COIN CrossTask EgoExo EPIC IKEA Overall Turn 1 (N = 400) Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) Ours (Best) Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 Turn 2 (N = 400) Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) 4.71 3.37 3.60 3.11 3.54 3.44 2.65 3.41 3.26 4.37 2.42 2.25 2. 3.55 2.75 3.11 2.70 2.75 2.26 2.15 2.64 3.23 3.75 1.83 2.06 2.28 3.42 3.20 3.30 2.88 3.20 2.73 2.33 4.35 2.83 3.38 2.08 2.25 2. 4.27 2.57 2.48 2.41 2.61 2.67 2.20 2.73 3.89 4.04 1.52 1.78 1.65 4.29 3.09 3.23 2.95 3.16 3.05 2.46 3.25 3.40 4.10 2.09 2.08 2. Continued on next page 4.46 3.41 3.44 3.93 3.27 3.59 2.76 3.46 3.59 4.34 2.37 1.90 2.07 19 Self-Improving World Modelling with Latent Actions Table 8 continued from previous page Method COIN CrossTask EgoExo EPIC IKEA Overall Ours (Best) 2.66 2. 2.11 2.78 1.79 2.42 Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 Turn 3 (N = 400) Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) Ours (Best) Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 Turn 4 (N = 210) Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) Ours (Best) Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 3.13 2.05 2.99 3. 3.96 1.46 1.61 1.65 2.02 4.16 1.25 4.05 3.57 4.21 1.20 1.40 1.38 2.20 4.40 1.00 4.00 3.00 Turn 5 (N = 140) Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) Ours (Best) Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 Turn 6 (N = 115) 2.80 1.80 3.22 3.49 3.27 1.27 1.39 1.51 2.02 4.34 1.05 3.95 3. 3.70 1.20 1.30 1.60 1.60 3.40 0.70 3.30 2.70 2.09 1.58 2.34 3.06 2.85 1.42 1.62 1.75 1. 3.30 1.06 3.81 3.25 2.53 1.29 1.38 1.40 1.43 3.29 1.00 3.79 3.83 2.47 1.23 1.26 1.57 1.54 2.91 0.91 3.57 3.29 2.38 1.65 3.55 3. 3.30 1.75 1.65 1.25 1.73 4.00 1.25 2.93 2.93 3.88 1.32 1.41 1.15 1.71 3.18 1.06 3.21 2.44 3.26 1.30 1.17 1.27 1.07 2.83 1.03 2.57 2. 2.08 1.60 2.37 3.86 3.04 1.17 1.33 1.31 1.46 4.14 1.24 4.48 4.35 2.64 1.04 1.23 1.10 1.32 3.95 0.89 4.31 4.32 2.11 0.91 1.09 1.05 1. 3.21 0.71 3.43 3.40 2.64 1.82 2.83 3.41 3.47 1.40 1.53 1.53 1.85 4.05 1.20 4.00 3.68 3.22 1.17 1.32 1.24 1.59 3.75 0.95 3.92 3. 2.47 1.07 1.15 1.23 1.25 3.06 0.83 3.28 3.09 Continued on next page 20 Self-Improving World Modelling with Latent Actions Table 8 continued from previous page Method COIN CrossTask EgoExo EPIC IKEA Overall Bagel Liquid-SFT Ours (Iterative) Ours (Iter.+Share) Ours (Best) Other Baselines OmniGen2 BLIP3o-NEXT OmniGen-v1 UniWorld-V1 2.13 1.04 1.12 1.23 1.23 2.54 0.73 3.04 2.58 3.00 1.17 1.39 1.17 1.22 2.96 0.91 2.22 1.74 1.95 0.88 1.02 1.05 0. 3.39 3.55 4.05 0.50 2.23 0.97 1.11 1.11 1.08 3.11 0.63 3.45 2.97 H. Detailed Results for Iterative Results Table 9. Detailed Iterative Dynamics (FWM & IDM). We report the detailed breakdown of performance across three iterations for our two ablation settings: (1) Separate weights for Generator and Critic, and (2) Shared weights. Columns under FWM Evaluation represent the generation quality across five benchmarks. The IDM Score column represents the Critics alignment accuracy. Bold numbers indicate the peak performance within each experimental setting. Setting Iteration MagicBrush AG Something Whatsup Kubric Avg Avg Score FWM IDM Option 1: Separate Weights (No Sharing) Iter 0 Iter 1 Iter 2 6.24 6.62 6.42 Option 2: Shared Weights (ϕ = θ) Iter 0 Iter 1 Iter 2 6.24 6.00 6.38 3.36 3.52 3.44 3.36 3.38 3.18 3.72 3.96 3.82 3.72 3.73 3. 4.32 4.32 4.20 4.32 4.46 4.32 7.14 6.96 6.96 7.14 7.45 7.27 4.96 5.06 4.98 4.96 5.00 4. 6.37 6.52 6.56 6.37 6.57 6.52 Table 9 examines the iterative training dynamics of Forward World Modeling (FWM) and Inverse Dynamics Modeling (IDM) under two architectural choices: using separate weights for the FWM and IDM versus shared weights. We report results across three training iterations to highlight peak performance and learning progression. The separate-weight configuration achieves the highest peak FWM performance, with Iteration 1 attaining the best average FWM score (5.06) and consistently strong results across all benchmarks. In particular, gains on MagicBrush, Action Genome, and Something indicate that decoupling the FWM and IDM allows each component to specialise more effectively, leading to higher-quality forward predictions when the system is optimally aligned. While the shared-weight setup exhibits competitive and stable behaviour, it does not surpass the peak generative performance achieved by the separate-weight model. Notably, although IDM accuracy continues to improve slightly in later iterations for both settings, the separate-weight design reaches its optimal balance between FWM quality and IDM alignment earlier in training. This suggests that architectural decoupling enables more expressive forward modeling, even if later iterations yield diminishing returns. Overall, these results indicate that separating FWM and IDM parameters is advantageous for maximising peak forward world modelling performance, motivating our choice of the separate-weight configuration in the main experiments where peak capability is the primary objective. 21 Self-Improving World Modelling with Latent Actions Table 10. Ablation on GRPO Rollout Size (G). We report the peak Assemble score achieved across all checkpoints for each rollout configuration. While larger group sizes (G = 64) achieve the best performance, moderate group sizes (G = 16) achieve comparable performance with much less compute. #Rollout MB AG Something Whatsup Kubric Avg. = 8 = 16 = 32 = 64 6.04 5.78 5.72 5. 3.38 3.24 3.30 3.50 3.45 3.60 3.37 3.80 4.26 4.50 4.18 4.16 6.98 6.98 6.80 7.06 4.80 4.80 4.68 4.90 I. Ablation on GRPO Rollout Size. We analyse the effect of rollout size. The group size is critical hyperparameter in GRPO, as it governs the accuracy of the baseline estimation and the diversity of the exploration within each optimisation step. We empirically evaluate {8, 16, 32, 64} across AURORA-BENCH, with results summarised in Table 10. We observe that scaling the rollout size generally improves model performance, achieving peak average score of 4.90 at = 64. We attribute this to the reduced variance in advantage estimation: larger pool of generations provides more robust approximation of the expected return, enabling the policy to distinguish high-quality trajectories more effectively. Notably, while = 64 yields the best absolute performanceparticularly in complex environments like Kubric, the smaller configuration of = 16 remains highly competitive (4.80 avg.) while requiring much less memory and compute during the generation phase. J. Qualitative Examples We provide some qualitative examples in Figure 7 to illustrate the visual predictions of SWIRL across three distinct benchmarks: AURORA-BENCH, BYTEMORPH, and WORLDPREDICTIONBENCH. AURORA-BENCH. The top panel demonstrates results on the AURORA benchmark, covering various subsets such as MagicBrush, Something, Emu, Kubric, and Whatsup. These examples highlight the models capability to perform diverse action-centric image editing tasks, ranging from the general editing (e.g., Add supernova explosion in the sky) and background replacement (e.g., changing to Yellowstone National Park) to precise geometric transformations and spatial reasoning. For instance, in the Something and Kubric subsets, the model successfully executes actions concerning physical regularities like flip the bottle upside down and distinct spatial rearrangements like moving specific plate to the left. BYTEMORPH. The middle panel displays qualitative results for BYTEMORPH, focusing on fine-grained control over camera and object dynamics. We visualise three distinct categories: Camera Zoom, Camera Motion, and Object Motion. The results show the models ability to synthesise coherent view changes, such as zooming out to reveal context (e.g., the coastline and second boat) or shifting the camera angle upward. Additionally, the Object Motion example demonstrates the generation of localised movement, specifically raising an animals head while maintaining scene consistency. WORLDPREDICTIONBENCH. The bottom panel illustrates long-horizon predictions on the WORLDPREDICTIONBENCH. Here, we show multi-step predictions where the model generates subsequent states based on the previous predicted image as the context and textual action descriptions. The examples depict long-horizon consistency in procedural tasks, such as replacing car key battery (Start Put in battery Close cover) and arranging bedding (Start Take out cover Arrange nicely). 22 Self-Improving World Modelling with Latent Actions Figure 7. Qualitative examples produced by SWIRL for AURORA-BENCH, BYTEMORPH and WORLDPREDICTIONBENCH. We present the sample for each subset in these benchmarks."
        }
    ],
    "affiliations": [
        "Nvidia Research",
        "University of Cambridge",
        "University of Edinburgh",
        "University of Groningen"
    ]
}