{
    "paper_title": "DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding",
    "authors": [
        "Jiaming Zhou",
        "Xuxin Cheng",
        "Shiwan Zhao",
        "Yuhang Jia",
        "Cao Liu",
        "Ke Zeng",
        "Xunliang Cai",
        "Yong Qin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 1 1 6 1 3 2 . 1 0 6 2 : r DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Jiaming Zhou1,2 Xuxin Cheng2 Shiwan Zhao1 Yuhang Jia1,2 Cao Liu2 Ke Zeng2 Xunliang Cai2 Yong Qin1 1College of Computer Science, Nankai University 2Meituan LongCat Interaction Team https://github.com/NKU-HLT/DIFFA"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with diffusion counterpart can substantially improve audio understanding under matched settings, albeit at proof-of-concept scale without largescale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is viable backbone for large-scale audio understanding."
        },
        {
            "title": "Introduction",
            "content": "Diffusion large language models (dLLMs) (Nie et al., 2025; Zhu et al., 2025) have recently emerged as promising alternative to conventional autoregressive (AR) decoders. Instead of generating tokens strictly left-to-right, dLLMs perform iterative denoising over partially masked sequences, enabling any-order token modeling and naturally supporting parallel token updates during decoding. Recent study (Ni et al., 2025) in the text domain further shows that dLLMs can act as strong data learners: when the amount of unique training data is constrained, they continue to improve and can even surpass AR models by leveraging super-dense compute and implicit Monte Carlo-style data augmentation. These properties are particularly appealing for audio understanding, where high-quality audiotext supervision across speech, sound, and music is far more expensive to collect than text-only data, and where the latency of strictly sequential AR decoding becomes bottleneck for long-form and interactive applications. Despite these advantages, state-of-the-art large audio language models (LALMs) are still predominantly AR-based: systems such as Qwen-3-Omni (Xu et al., 2025b), Qwen-2.5-Omni (Xu et al., 2025a), and Kimi-Audio (Ding et al., 2025) couple powerful speech encoders with AR LLMs and achieve strong results on wide range of audio understanding and dialogue benchmarks. This raises natural question: can dLLMs be turned into competitive and practical audio backbones that match these AR LALMs under realistic data and latency budgets? Initial evidence is encouraging. recent work on diffusion-based large audio-language model (DIFFA) (Zhou et al., 2025) compares an 8B AR backbone with its diffusion counterpart under matched data, adapter design, and training recipe, and finds substantial gains on audio understanding benchmarks such as MMAU (Sakshi et al., 2025) and MMSU (Wang et al., 2025b) after simply replacing the AR backbone. This suggests that the generative paradigm itself can strongly influence audio performance and that dLLMs have significant potential as audio backbones. However, that study remains largely proof of concept: the model is trained mainly on speech-centric supervision with relatively small Whisper encoder (Radford et al., 2023), keeps the diffusion backbone frozen, and does not exploit large-scale instruction data, preference-based objectives, or practical inference acceleration. It therefore does not answer whether dLLMs can be scaled into strong and practical LALMs that reliably compete with state-of-the-art AR models. This work was done during an internship at Meituan. Xunliang Cai and Yong Qin are corresponding authors. Correspondence to: qinyong@nankai.edu.cn DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding In this paper, we present DIFFA-2, substantially strengthened diffusion-based large audio language framework that addresses this question. DIFFA-2 aims to turn dLLMs from proof-of-concept backbone into competitive and practical audio model through comprehensive semanticacoustic alignment and scalable training. Concretely, it adopts four-stage training strategy that progressively aligns audio representations, incorporates large-scale supervised fine-tuning, and applies variance-reduced preference optimization (VRPO) (Zhu et al., 2025), and combines this with factor-based parallel decoding (Wu et al., 2025) at inference time to enable practical audio-based interaction without relying on AR decoding. Our contributions are as follows: We present DIFFA-2, strengthened diffusion-based large audio language model with improved acoustic modeling and semanticacoustic alignment for unified understanding of speech, sound, and music. We introduce progressive four-stage training curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and preference-based reinforcement learning using fully open-source data, together with factor-based parallel decoding for practical diffusion inference. With only 11,000 hours of automatic speech recognition (ASR) data and 3,767 hours of supervised fine-tuning data, DIFFA-2 updates only about 1.1% of parameters and achieves strong performance on audio understanding benchmarks including MMSU, MMAU, and MMAR (Ma et al., 2025), remaining highly competitive with strong AR-based LALMs on these benchmarks. We open-source both the training and inference pipeline to facilitate future research on dLLM-based audio models."
        },
        {
            "title": "2 Preliminaries",
            "content": "LLaDA (Nie et al., 2025) is non-autoregressive language modeling framework based on discrete random masking process. Instead of factorizing the sequence likelihood in left-to-right manner, LLaDA introduces stochastic corruption mechanism and trains mask predictor to approximate the reverse denoising process. This design allows the model to leverage bidirectional context and enables parallel token prediction during inference. Formally, given clean target sequence x0 = (x1 0 ), LLaDA defines forward masking process that independently replaces each token with special mask symbol with probability (0, 1], resulting in corrupted sequence xt. The mask predictor pθ(x0 xt), parameterized by standard Transformer decoder, is trained to reconstruct the original tokens at masked positions. The pre-training objective is given by 0, . . . , xL L(θ) Et,x0,xt (cid:34) 1 (cid:88) i=1 I[xi = M] log pθ(xi 0 xt) (cid:35) , (1) where denotes the sequence length. This objective yields tractable upper bound on the negative log-likelihood (Shi et al., 2024; Ou et al., 2025), while avoiding autoregressive factorization. Supervised fine-tuning (SFT) under the LLaDA framework follows the same corruptionreconstruction principle. Given promptresponse pair (p0, r0), only the response sequence is subject to random masking, producing corrupted response rt, while the prompt remains fully observed. The SFT objective is defined as Et,p0,r0,rt 1 (cid:88) i=1 I[ri = M] log pθ(ri 0 p0, rt) , (2) where denotes the response length. At inference time, LLaDA performs generation through an iterative decoding procedure. Starting from fully masked response sequence, the model predicts token values at masked positions and selectively re-applies masks to lowconfidence tokens. By repeating this denoising process for fixed number of steps, the model gradually refines its predictions and produces the final output sequence."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Model Architecture DIFFA-2 follows the overall framework of DIFFA, but substantially strengthens acoustic representation and cross-modal alignment. The model consists of three main components: frozen Whisper-Large-V3 encoder, dual-adapter audio interface, and dLLM backbone. The semantic adapter consists of two-layer convolution subsampling module 2 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Figure 1: Overview of DIFFA-2, including the dual-adapter architecture, multi-stage training pipeline (Stages 14), and iterative diffusion-based inference for general audio understanding. followed by two-layer linear projection, reducing the temporal resolution from 50 Hz to 12.5 Hz and aligning temporally aggregated audio features with textual semantics. The acoustic adapter is implemented as two-layer Q-former (Li et al., 2023) with 64 trainable query vectors attending to intermediate encoder states, enabling effective capture of paralinguistic and non-linguistic acoustic cues, such as prosody, emotion-related patterns, environmental sounds, and music. The key design idea is to expose the backbone to two complementary views of the audio signal for speech, sound, and music: content-oriented stream that is temporally aligned with textual semantics, and compact acoustic summary stream that highlights prosodic, stylistic, and non-linguistic cues, while keeping the diffusion backbone lightweight to train. 3.2 Training Data Overview Our training pipeline follows progressive curriculum corresponding to the four stages detailed in Sec. 3.3. Broadly, we utilize large-scale transcription data for semantic alignment, diverse audio-centric instruction data for acoustic enrichment, and curated preference pairs for reinforcement-style alignment. Detailed data construction and statistics are provided in Appendix A. Transcription Data for Semantic Alignment. For Stage 1, we leverage major ASR corpora, including LibriSpeech (Panayotov et al., 2015) and GigaSpeech (Chen et al., 2021), by framing speech recognition as an instructionfollowing task. We utilize Qwen3-32B (Yang et al., 2025) to generate 25 distinct instruction templates, which are applied to original transcriptions. SFT Data. To move beyond pure speech recognition and enrich the models acoustic and paralinguistic understanding (Stages 2 and 3), we construct four complementary types of audio question answering (AQA) data: (i) Caption-grounded AQA: Using multi-domain audio and speech corpora with existing captions or paralinguistic annotations, we prompt strong LLM to synthesize diverse and high-quality grounded answers. This ensures broad yet controlled coverage of speech, environmental sounds, and music. (ii) Direct audio QA via text-to-speech (TTS), where text QA pairs from general text datasets are converted into speech via TTS, covering simple and complex queries. (iii) Multiple-choice AQA: This subset, derived from existing benchmarks, emphasizes fine-grained discrimination and objective evaluation of specific audio attributes. (iv) ASR Subset: subset from ASR data. 3 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Preference Alignment Data. In Stage 4 (preference optimization), we construct high-quality preference triplets consisting of an audio input, question, and reference answer. We prompt an LLM to generate \"rejected\" responses that are fluent and superficially plausible but contain subtle audio-related factual errors (e.g., incorrect gender, rhythm, or sound events). Only pairs where the reference is unambiguously superior are retained. These chosenrejected pairs are utilized for VRPO to sharpen the models sensitivity to nuanced paralinguistic cues and complex audio reasoning. 3.3 Progressive Four-Stage Training To fully exploit the dual-adapter architecture and the heterogeneous training data, we adopt progressive four-stage training curriculum. Throughout all stages, the Whisper-large-v3 encoder is kept frozen, and supervised objectives operate only on the adapters and the diffusion backbone. Stage 1: Semantic alignment on ASR. In the first stage, we freeze the dLLM backbone and train only the semantic adapter under an ASR-style objective. The goal is to align it with the semantic space, so that its outputs can be seamlessly consumed by the diffusion backbone. Stage 2: Joint semanticacoustic alignment. In stage 2, we still keep the diffusion backbone frozen, but jointly align both semantic and acoustic adapters with synthesized SFT data that explicitly incorporates paralinguistic and acoustic cues, thereby enhancing both semantic and acoustic understanding in speech, audio, and music. Stage 3: Unfreezing the diffusion backbone with LoRA. In this stage, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2022) to fine-tune the diffusion backbone, thereby strengthening the models audio understanding capabilities. LoRA strikes balance between adaptation capacity and training efficiency while effectively mitigating catastrophic forgetting. The formulation of the SFT loss applied across the first three stages is detailed in Section 3.4. Stage 4: Preference optimization with VRPO. Finally, to refine instruction following and acoustic sensitivity, we apply VRPO (Zhu et al., 2025) in Stage 4. Unlike standard direct preference optimization (DPO) (Rafailov et al., 2023), which can suffer from high variance in the evidence lower bound (ELBO) estimates for dLLMs, VRPO employs variance reduction to stabilize training (details in Sec. 3.5). Using chosenrejected AQA pairs, DIFFA-2 learns to prefer responses that are more faithful to subtle audio cues (e.g., prosody, emotion, background sound) from curated preference data, further enhancing its audio understanding capabilities. 3.4 Supervised Fine-Tuning of DIFFA-2 Given textual prompt x, an audio input a, and target response y, DIFFA-2 models pθ(y x, a) through diffusion denoising process over text tokens, with audio embeddings inserted into the prompt. Audio embeddings (from the adapters) and text prompt tokens remain fully visible and are never masked; only the response tokens are corrupted and denoised. Let r0 denote the full response sequence, with length L. During training, the special token <endoftext> is used both as padding and as the end-of-sequence marker, and the model is required to predict it. At each training iteration, tokens in r0 are independently masked by special mask token with probability (0, 1], yielding noised response rt. The model is then optimized using diffusion-style masked prediction objective: Lsf ta = Et,a,p,r0,rt 1 (cid:88) i=1 1[ri = M] log pθ(ri 0 a, p, rt) , where rt represents the masked response corresponding to masking ratio of t. This loss trains DIFFA-2 to reconstruct masked tokens in rt given both audio and textual context, enabling the backbone to exploit bidirectional context and multimodal cues. 3.5 Variance-Reduced Preference Optimization For preference alignment, we adopt VRPO proposed in (Zhu et al., 2025). This method leverages DPO-style objective function with Monte Carlo estimates of ELBO, and incorporates optimal budget allocation as well as antithetic sampling strategies to explicitly reduce the variance of these estimates. As illustrated in Figure 1, we perform (=4) independent sampling steps, and crucially, share identical masking patterns between the policy model and the reference model when calculating (cid:92)log pθ and (cid:92)log pref . This design effectively implements antithetic sampling over diffusion trajectories, thereby stabilizing the preference learning process even when handling long, acoustically rich input sequences. DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table 1: Performance breakdown on the MMSU benchmark across perception and reasoning dimensions in Semantics (Seman.), Phonology (Phono.), and Paralinguistics (Para.) domains. \"w/ FPD\" denotes factor-based parallel decoding. Models Size Qwen3-Omni GPT-4o-Audio Gemini 2.0 Flash 30B-A3B - - DIFFA-2 DIFFA-2 (w/ FPD) Kimi-Audio Qwen2.5-Omni MiniCPM-O DIFFA Qwen2-Audio Qwen-Audio-Chat Phi-4-multimodal Baichuan-Audio GLM-4-Voice Salmonn LTU 8B 8B 7B 7B 8B 8B 8B 8B 8B 11B 9B 7B 7B Perception Reasoning Seman. Phono. Para. Avg Seman. Phono. Para. Avg 72.13 59.70 47.17 60.63 59.69 57.64 61.11 56.56 52.67 52.14 57.21 38.72 39.63 27.80 31.55 21.34 55.83 41.56 41.30 39.04 39.57 42.30 43.96 34.05 36.65 32.87 38.52 34.86 31.26 24.52 29.08 22.46 38.85 53.20 21.44 39.67 30.62 40.83 41.92 45.58 40.93 45.06 35.74 43.52 33.20 43.97 36.48 40.54 35.12 40.28 35.56 39.02 24.70 35.69 29.56 33.41 27.09 31.48 27.34 26.18 28.71 29.83 18.73 20. 86.64 80.83 70.69 85.29 85.02 81.77 82.40 80.71 81.53 77.62 58.61 57.81 57.96 46.10 36.43 22.65 82.19 78.74 70.69 77.58 77.28 76.65 76.77 74.72 72.68 64.81 59.78 65.94 63.92 48.16 26.22 25.53 43.58 78.88 26.25 71.96 36.16 47.83 43.58 76.40 42.09 76.00 55.22 76.03 46.87 75.21 46.71 73.57 45.67 72.92 46.67 68.90 25.60 55.93 42.09 57.59 34.35 55.70 44.35 46.76 25.26 30.04 24.74 24. Overall 65.63 56.38 51.03 60.45 60.10 59.28 59.09 56.53 56.04 53.27 46.92 44.96 43.09 35.51 30.01 22.61 We first estimate log-likelihoods (cid:92)log pθ(y x, a) using Monte Carlo ELBO estimation: (cid:92)log pθ(y x, a) = 1 K (cid:88) k=1 ELBO(k) θ (y x, a), (3) where is the sample budget. We analogously obtain (cid:92)log pref (y x, a) using frozen reference model. VRPO then applies DPO-style objective to the estimated log-ratios: sθ(y) = (cid:92)log pθ(y x, a) (cid:92)log pref (y x, a), (cid:16) LVRPO = log σ β(cid:2)sθ(y+) sθ(y)(cid:3)(cid:17) , (4) (5) where y+ and denote the preferred and rejected responses, respectively, and β controls the strength of preference enforcement. 3.6 Inference Procedure At inference time, we pad the prompt and audio input to the target length and initialize the response sequence rT as fully masked. DIFFA-2 then performs iterative denoising over steps, gradually refining rt from coarse to fine. At each transition s, the model predicts the masked tokens conditioned on the audio input a, prompt p, and current corrupted sequence rt: ˆrt = arg max pθ(r0 a, p, rt), (6) and re-masks the lowest-confidence fraction of tokens to form rs, enabling iterative refinement with bidirectional context. Following LLaDA (Nie et al., 2025), we adopt semi-autoregressive strategy that decodes the response in left-to-right blocks, while predicting tokens within each block in parallel and selectively re-masking them across diffusion steps. To further accelerate decoding, we use the factor-based parallel decoding strategy from fast-dLLMs (Wu et al., 2025), which adaptively chooses how many tokens to update in parallel based on model confidence rather than fixed threshold. Intuitively, the algorithm allows more aggressive parallel decoding when the model is confident and reduces parallelism in uncertain regions. The formal decision rule and implementation details are provided in Appendix B.4. 5 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table 2: Performance breakdown on the MMAU benchmark. \"w/ FPD\" denotes factor-based parallel decoding. Model Size Qwen3-Omni Gemini 2.0 Flash GPT-4o-Audio 30B-A3B - - DIFFA-2 Qwen2.5-Omni DIFFA-2 (w/ FPD) Kimi-Audio MiniCPM-O Phi-4-multimodal Qwen2-Audio DIFFA Baichuan-Audio Qwen-Audio-Chat Salmonn GLM-4-Voice LTU 8B 7B 8B 7B 8B 8B 8B 8B 11B 8B 7B 9B 7B Sound Music Speech Avg Test-mini Test Test-mini Test Test-mini Test Test-mini Test 78.68 71.17 64. 76.28 72.97 75.38 75.68 71.47 65.47 67.27 46.25 - 55.25 41.14 - 20.42 73.70 68.93 63.20 70.83 69.53 69.43 70.70 - 62.67 61.17 - 59.46 56.73 42.10 27.63 20.67 69.46 65.27 56.29 63.47 61.68 61.08 66.77 65.57 64.37 56.29 43.41 - 44.00 37.13 - 15.97 72.22 59.30 49. 60.10 62.50 59.57 65.93 - 61.97 55.67 - 49.10 40.90 37.83 27.84 15.68 69.37 75.08 66.67 69.06 60.96 68.47 62.16 63.06 67.27 55.26 59.46 - 30.03 26.43 - 15.92 66.46 72.87 69.33 70.18 67.93 70.15 56.57 - 63.80 55.37 - 42.47 27.95 28.77 35.44 15.33 72.50 70.50 62. 69.60 65.20 68.30 68.20 66.70 65.70 59.60 49.71 - 43.10 34.90 - 17.44 70.77 67.03 60.82 67.00 66.64 66.34 64.40 - 62.81 57.40 - 50.34 41.86 36.23 30.30 17."
        },
        {
            "title": "4 Experimental Setup",
            "content": "4.1 Training and Inference Setup DIFFA-2 is trained with the four-stage curriculum in Section 3 using only fully open-source corpora. In total, we use about 11,000 hours of ASR data in Stage 1 and 3,767 hours of curated supervised fine-tuning data in Stages 23, together with roughly 3,000 preference pairs for Stage 4; detailed breakdown of datasets, sample counts, and prompts is given in Appendix A. We adopt LLaDA-8B-Instruct as the dLLM backbone and update only lightweight components (semantic adapter, acoustic adapter, and LoRA parameters), resulting in roughly 1.1% trainable parameters overall (see Table B.3). Inference configurations for each benchmark are summarized in Appendix B.4. 4.2 Baselines We compare DIFFA-2 with both proprietary and open-source audio LLMs. As proprietary reference points, we include GPT-4o-Audio (OpenAI et al., 2024) and Gemini 2.0 Flash (Team et al., 2025). Among open-source models, we focus on strong omni/audio baselines such as Qwen3-Omni (Xu et al., 2025b), Qwen2.5-Omni (Xu et al., 2025a), Kimi-Audio (Ding et al., 2025), and the first-generation DIFFA (Zhou et al., 2025). The full list of baselines is given in Appendix B.1. 4.3 Benchmarks We evaluate DIFFA-2 on four representative benchmarks. Among them, MMSU (Wang et al., 2025b), MMAU (Sakshi et al., 2025), and MMAR (Ma et al., 2025) are audio understanding benchmarks and constitute the primary focus of our study, while VoiceBench (Chen et al., 2024b) is included only as an auxiliary evaluation of semantic dialogue ability. Further details on benchmarks are provided in Appendix B.2."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Results on Benchmarks MMSU. Table 1 reports fine-grained breakdown on MMSU. Among open LALMs of comparable size, DIFFA-2 attains the best overall accuracy (60.45), slightly outperforming Kimi-Audio (59.28), Qwen2.5-Omni (59.09), and other 711B baselines, while remaining within roughly 5 points of the larger proprietary Qwen3-Omni. DIFFA-2 also achieves the highest perception average (45.58), with clear gains on paralinguistic perception (41.92) and solid performance on semantic and phonological perception. On reasoning, DIFFA-2 reaches the best average (76.40) among open models, with noticeable improvements in semantic and phonological reasoning compared with Kimi-Audio and Qwen2.5-Omni. Relative to the first-generation DIFFA, DIFFA-2 improves both perception and reasoning (overall 6 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table 3: Performance breakdown on the MMAR benchmark. The All category denotes the comprehensive evaluation across mixed sound, music, and speech modalities. \"w/ FPD\" denotes factor-based parallel decoding. Models Size Qwen3-Omni Gemini 2.0 Flash GPT-4o-Audio 30B-A3B - - Qwen2.5-Omni DIFFA-2 DIFFA-2 (w/ FPD) MiniCPM-O Baichuan-Omni-1.5 DIFFA Salmonn Salmonn Qwen2-Audio OpenOmni Qwen-Audio-Chat LTU 7B 8B 8B 8B 7B 8B 13B 7B 8B 8B 8B 7B Single Modality (%) Mixed Modalities (%) Sound Music Speech Sound-Music Sound-Speech Music-Speech All 59.39 61.21 53.94 55.76 54.55 53.94 49.70 41.21 37.58 30.30 30.91 33.33 20.61 27.88 19.39 54.37 50.97 50.97 41.75 41.75 36.41 36.41 33.01 31.07 29.61 25.73 24.27 22.33 20.39 19. 70.41 72.11 70.41 54.42 53.40 52.72 50.00 40.48 39.46 34.69 34.35 32.31 35.37 22.11 13.95 90.91 81.82 63.64 45.45 45.45 36.36 36.36 36.36 36.36 9.09 9.09 9.09 18.18 9.09 18.18 74.77 72.48 72.48 55.96 58.26 56.88 53.67 48.62 43.12 34.86 37.61 31.19 27.06 25.23 24. 63.41 65.85 62.20 57.32 54.88 53.66 45.12 39.02 45.12 35.37 28.05 30.49 23.17 25.61 21.95 70.83 70.83 75.00 54.17 37.50 45.83 54.17 41.67 25.00 41.67 37.50 25.00 25.00 20.83 16.67 Avg (%) 65.90 65.60 63. 51.40 50.80 50.20 48.60 40.70 37.20 33.20 32.80 30.00 27.00 23.50 19.20 Table 4: Word error rate (WER%) and real-time factor (RTF) of Stage-1 with dLLMs and AR backbone on Librispeechclean and Librispeech-other testing set. \"w/ FPD\" denotes factor-based parallel decoding. Model WER RTF clean other clean other LLaMA-Audio (S1) DIFFA-2 (S1) DIFFA-2 (S1 w/ FPD) 2.43 2.72 3.05 5.09 5.34 5. 0.1402 0.6792 0.0820 0.1418 0.7489 0.0867 +4.41 points), indicating that the upgraded acoustic front-end and four-stage training pipeline translate into stronger speech understanding on MMSU. MMAU. On MMAU  (Table 2)  , DIFFA-2 again performs competitively with strong AR LALMs. It achieves the best average accuracy among open models on both Test-mini and Test splits (69.60 and 67.00) surpassing Qwen2.5-Omni and Kimi-Audio, and approaching larger or proprietary systems such as Qwen3-Omni and Gemini. DIFFA-2 is particularly strong on sound and speech, where it attains the highest scores among open models, while its music performance is slightly behind Kimi-Audio and MiniCPM-O but remains competitive without any music-specialized design. Compared with DIFFA, DIFFA-2 gains nearly 20 points in average accuracy on Test-mini (49.71 69.60), suggesting that the enhanced acoustic modeling and multi-stage training generalize well to high-level audio understanding across sound, music, and speech. MMAR. The MMAR benchmark  (Table 3)  evaluates single and mixed audio modalities with more compositional queries. DIFFA-2 achieves an average accuracy of 50.80%, substantially improving over DIFFA (37.20%, +13.6 points) and outperforming other 8B open baselines such as MiniCPM-O (48.60%) and Qwen2-Audio (30.00%). On single-modality tasks (sound, music, speech), DIFFA-2 consistently improves over DIFFA and narrows the gap to Qwen2.5-Omni. For the most challenging SoundMusicSpeech mixtures, DIFFA-2 underperforms relative to Qwen2.5Omni, likely reflecting the lack of mixed-modality supervision in the training data. Overall, MMAR indicates that DIFFA-2 effectively extends diffusion-based modeling to multi-source audio composition, while complex three-way mixtures remain challenging regime compared with the strongest AR-based LALMs. VoiceBench. We additionally evaluate on VoiceBench to assess dialogue-style spoken interaction. As shown in Appendix C, DIFFA-2 lags behind heavily instruction-tuned omni models such as GPT-4o-Audio and Qwen3-Omni, but still improves over DIFFA and several open-source baselines, which is consistent with our design focus on audio understanding rather than extensive conversational tuning. Summary. Across MMSU, MMAU, and MMAR, DIFFA-2 consistently outperforms the first-generation DIFFA and often matches or surpasses strong open AR LALMs of similar size, while approaching larger proprietary systems on several metrics. Gains are particularly clear in semantic and phonological reasoning and in sound and speech 7 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table 5: Ablation of DIFFA-2 and LLaMA-Audios multi-stage training on MMAU and MMSU. LLaMA-Audio is based on LLaMA 3.1 backbone and then trained with the same data and settings. Model LLaMA-Audio (S2) LLaMA-Audio (S3) DIFFA-2 (S2) DIFFA-2 (S3) DIFFA-2 (S4) MMAU Sound Music Speech 65.47 73.87 72.07 74.77 76. 54.79 65.87 52.69 62.57 63.47 62.16 62.46 66.97 67.27 69.07 Overall 60.80 67. 63.90 68.20 69.60 MMSU (Perception) MMSU (Reasoning) Sem. Phon. Para. Avg Sem. Phon. Para. Avg 39.21 50.39 52.91 59.53 60.63 30.91 39. 36.58 37.54 39.04 30.23 36.67 31.32 40.63 41.92 32.69 41.22 38.54 44.16 45.58 51.35 75. 84.03 84.66 85.29 63.97 73.18 75.74 76.15 77.58 44.18 45.07 46.57 44.48 43.58 55.45 70. 75.50 75.70 76.40 Overall 43.71 55.31 56.43 59.41 60.45 understanding, whereas performance on VoiceBench remains behind omni models that are heavily tuned for dialogue and alignment. These results indicate that, under realistic data constraints, diffusion-based backbones can serve as competitive audio understanding models, and that additional dialogue-centric supervision could further close the gap on interactive voice assistant benchmarks. 5.2 Ablation Study We first compare Stage-1 ASR performance between diffusion and autoregressive backbones  (Table 4)  . Under the same ASR-style training on LibriSpeech, the AR baseline LLaMA-Audio (S1) attains slightly lower word error rate (WER) than DIFFA-2 (S1), which is consistent with the advantage of strictly left-to-right decoding for monotonic transcription. When we enable factor-based parallel decoding for DIFFA-2 (S1, w/ FPD), WER increases moderately, but the real-time factor (RTF) drops substantially and becomes lower than that of the AR baseline. This indicates that diffusion backbone does not inherently improve low-level recognition accuracy, but its inference latency can be made competitive with, or better than, an AR backbone by using an appropriate parallel decoding scheme in ASR task. On audio understanding benchmarks, DIFFA-2 with factor-based parallel decoding attains accuracy that is close to the standard setting, suggesting that it provides practical knob to trade off accuracy and latency for diffusion-based audio models. Table 5 reports the multi-stage training ablation on MMAU and MMSU, comparing DIFFA-2 with LLaMA-Audio under matched data and training curriculum. With only Stage 2 (adapter-only alignment), DIFFA-2 already achieves higher overall scores than LLaMA-Audio (S2) on both benchmarks, suggesting that the diffusion backbone benefits more from the same semanticacoustic alignment. Advancing from Stage 2 to Stage 3 improves both models, and DIFFA-2 gains more on reasoning-oriented metrics. Adding Stage 4 (VRPO) further improves DIFFA-2, yielding the best overall performance and more balanced gains across perception and reasoning. Overall, the ablation highlights difference between transcription and audio understanding. For token-level ASR, the AR backbone retains small advantage in WER, in line with its sequential decoding nature. For holistic audio QA, DIFFA-2 with diffusion backbone achieves stronger performance under the same data and multi-stage training, which may be attributed to the corruptionreconstruction training objective of dLLMs, which has been shown in text domains to make more effective use of limited data, making it better aligned with audio understanding benchmarks, although we do not completely disentangle backbone pre-training effects."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Large Audio Language Models Recent LALMs primarily adopt autoregressive backbones. common design couples speech encoder to an LLM via lightweight bridging modules (e.g., Qwen2-Audio (Chu et al., 2024), SALMONN (Tang et al., 2024b), AudioFlamingo2 (Ghosh et al., 2025), with omni models further supporting streaming and multi-modality (Xu et al., 2025a,b). Another line tokenizes audio into discrete sequences (Zhang et al., 2023; Défossez et al., 2024), while Kimi-Audio (Ding et al., 2025) fuses discrete and continuous representations. These models largely rely on AR decoding. 6.2 Diffusion Large Language Models Diffusion large language models generate sequences by iteratively denoising corrupted tokens, offering bidirectional context modeling and parallel token updates. Early work on diffusion for discrete text (Austin et al., 2021; Shi et al., 2024; Sahoo et al., 2024) established the feasibility of this paradigm, and LLaDA (Nie et al., 2025; Zhu et al., 2025) scaled it to large language models with strong performance on understanding and reasoning tasks. Recent efforts 8 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding improve inference efficiency via training-free acceleration, including KV-cache-like reuse with confidence-aware parallel decoding and adaptive length prediction (Li et al., 2025a; Wei et al., 2025). Our work is closely related to diffusion LMs such as LLaDA and fast-dLLMs (Wu et al., 2025). LLaDA and its variants focus on text generation and do not consider audio encoders or audio-specific training curricula."
        },
        {
            "title": "7 Conclusions",
            "content": "This paper presents DIFFA-2, an enhanced dLLMs-based LALM for audio understanding. Despite having only 1.1% (99M) trainable parameters and utilizing modest 14.8k hours of open-source data, DIFFA-2 achieves substantial performance gains over its predecessor. Evaluations on MMSU, MMAU, and MMAR benchmarks demonstrate that DIFFA-2 is competitive with leading autoregressive models. These results establish dLLMs-based modeling as highly competitive alternative for universal audio understanding tasks."
        },
        {
            "title": "Limitations",
            "content": "Although DIFFA-2 achieves strong results on audio understanding benchmarks, several limitations remain. First, our training objectives and data curation are geared toward fine-grained audio understanding rather than open-domain spoken dialogue. Consequently, DIFFA-2 is exposed to only limited conversational and alignment-style supervision, which is reflected in its mid-range performance on VoiceBench compared with heavily instruction-tuned AR Omnimodels. Designing more balanced training recipe that jointly targets audio understanding and spoken dialogue is an important direction for future work. Second, we focus exclusively on text-based audio understanding and do not consider speech generation or streaming, full-duplex interaction. DIFFA-2 is evaluated in an offline speech-in/text-out setting; integrating it into end-to-end speech-in/speech-out systems and assessing user-centric metrics such as latency and interaction quality are important next steps. Third, we apply simple training-free factor-based parallel decoding scheme to reduce diffusion steps and observe clear latency gains with negligible accuracy loss, but DIFFA-2 is not yet uniformly faster than strong AR audio LLMs under all settings. We view this as systems-level design choice rather than fundamental limitation of dLLMs backbones; adapting more advanced training-free acceleration methods from text dLLMs to audio is promising but orthogonal direction for future work."
        },
        {
            "title": "References",
            "content": "Adaeze Adigwe, Noé Tits, Kevin El Haddad, Sarah Ostadabbas, and Thierry Dutoit. The emotional voices database: Towards controlling the emotion dimension in voice generation systems. arXiv preprint arXiv:1806.09514, 2018. Andrea Agostinelli, Timo Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325, 2023. Afroz Ahamad, Ankit Anand, and Pranesh Bhargava. Accentdb: database of non-native english accents to assist neural speech recognition. In Proceedings of The 12th Language Resources and Evaluation Conference, pp. 53535360. European Language Resources Association, 2020. Jacob Austin, DanielD. Johnson, Jonathan Ho, Daniel Tarlow, and Riannevanden Berg. Structured denoising diffusion models in discrete state-spaces. arXiv: Learning,arXiv: Learning, Jul 2021. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 15331544, 2013. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42(4):335359, 2008. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, and et al. Slam-omni: Timbre-controllable voice interaction system with single-stage training. arXiv preprint arXiv:2412.15649, 2024a. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024b. 9 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwenaudio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. Fma: dataset for music analysis, 2016. URL https://arxiv.org/abs/1612.01840. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Anuj Diwan, Zhisheng Zheng, David Harwath, and Eunsol Choi. Scaling rich style-prompted text-to-speech datasets, 2025. URL https://arxiv.org/abs/2503.04713. Seungheon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. Lp-musiccaps: Llm-based pseudo music captioning. In Ismir 2023 Hybrid Conference, 2023. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset, 2019. URL https://arxiv.org/abs/1910.09387. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with wavenet autoencoders, 2017. URL https://arxiv.org/ab s/1704.01279. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMA-omni: Seamless speech interaction with large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=PYmrUQmMEw. Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, Meng Zhao, Yi-Fan Zhang, Shaoqi Dong, Yangze Li, Xiong Wang, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. Sreyan Ghosh, Sonal Kumar, Ashish Seth, Chandra Kiran Reddy Evuru, Utkarsh Tyagi, Sakshi, Oriol Nieto, Ramani Duraiswami, and Dinesh Manocha. Gama: large audio-language model with advanced audio understanding and complex reasoning abilities. arXiv preprint arXiv:2406.11768, 2024. Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/f orum?id=xWu5qpDK6U. Yuan Gong, Jin Yu, and James Glass. Vocalsound: dataset for improving human vocal sounds recognition, 2022. URL https://arxiv.org/abs/2205.03433. Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and understand. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum ?id=nBZBPXdJlC. Haolin He, Xingjian Du, Renhe Sun, Zheqi Dai, Yujia Xiao, Mingru Yang, Jiayi Zhou, Xiquan Li, Zhengxi Liu, Zining Liang, et al. Measuring audios impact on correctness: Audio-contribution-aware post-training of large audio language models. arXiv preprint arXiv:2509.21060, 2025. William Held, Ella Li, Michael Ryan, Weiyan Shi, Yanzhe Zhang, and Diyi Yang. Distilling an end-to-end voice assistant without instruction training data. arXiv preprint arXiv:2410.02678, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al. Step-audio: Unified understanding and generation in intelligent speech interaction. arXiv preprint arXiv:2502.11946, 2025. 10 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Zeyu Jin, Jia Jia, Qixin Wang, Kehan Li, Shuoyi Zhou, Songtao Zhou, Xiaoyu Qin, and Zhiyong Wu. Speechcraft: In Proceedings of the 32nd ACM fine-grained expressive speech dataset with natural language description. International Conference on Multimedia, pp. 12551264, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pp. 119132. Association for Computational Linguistics, 2019. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin. Beyond fixed: Training-free variable-length denoising for diffusion large language models. arXiv preprint arXiv:2508.00819, 2025a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al. Baichuan-audio: unified framework for end-to-end speech interaction. arXiv preprint arXiv:2502.17239, 2025b. Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, et al. Baichuan-omni technical report. arXiv preprint arXiv:2410.08565, 2024. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, and Chao-Han Huck Yang et al. Desta2.5-audio: Toward general-purpose large audio language model with self-generated cross-modal alignment, 2025. URL https://arxiv.org/abs/25 07.02768. Ziyang Ma, Yinghao Ma, Yanqiao Zhu, Chen Yang, Yi-Wen Chao, Ruiyang Xu, Wenxi Chen, Yuanzhe Chen, Zhuo Chen, Jian Cong, Kai Li, Keliang Li, Siyou Li, Xinfeng Li, Xiquan Li, Zheng Lian, Yuzhe Liang, Minghao Liu, Zhikang Niu, tianrui wang, Yuping Wang, Yuxuan Wang, Yihao Wu, Guanrou Yang, Jianwei Yu, Ruibin Yuan, Zhisheng Zheng, Ziya Zhou, Haina Zhu, Wei Xue, Emmanouil Benetos, Kai Yu, EngSiong Chng, and Xie Chen. In The ThirtyMMAR: challenging benchmark for deep reasoning in speech, audio, music, and their mix. ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview.net/forum?id=fgmrBJemlQ. Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research, 2023. URL https://arxiv.org/abs/2303.17395. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: large-scale speaker identification dataset. In Interspeech 2017, Aug 2017. doi: 10.21437/interspeech.2017-950. URL http://dx.doi.org/10.21437/inters peech.2017-950. Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, and Michael Qizhe Shieh. Diffusion language models are super data learners. arXiv preprint arXiv:2511.03276, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, JUN ZHOU, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=KnqiC0znVF. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, and Adam Perelman et al. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing In The Thirteenth International discrete diffusion secretly models the conditional distributions of clean data. Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=sMyXP8Tanm. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Karol J. Piczak. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pp. 10151018. ACM Press, 2015. doi: 10.1145/2733373.2806390. 11 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 527536. Association for Computational Linguistics, 2019. Paul Primus, Florian Schmid, and Gerhard Widmer. Tacos: Temporally-aligned audio captions for language-audio pretraining. arXiv preprint arXiv:2505.07609, 2025. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. ArXiv, abs/2406.07524, 2024. URL https://api.semanticscholar.org/CorpusID:270380319. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. MMAU: massive multi-task audio understanding and reasoning benchmark. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/f orum?id=TeVAZXr3yv. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. In ICLR, 2024a. URL https://openreview .net/forum?id=14rn7HpKVk. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=14rn7HpKVk. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanfor d_alpaca, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, and et al. Gemini: family of highly capable multimodal models, 2025. URL https://arxiv.org/abs/2312.11805. OpenBMB MiniCPM-o Team. Minicpm-o 2.6: gpt-4o level mllm for vision, speech, and multimodal live streaming on your phone, 2025. Irina-Elena Veliche, Zhuangqun Huang, Vineeth Ayyat Kochaniyan, Fuchun Peng, Ozlem Kalinli, and Michael L. Seltzer. Towards measuring fairness in speech recognition: Fair-Speech dataset. In Interspeech 2024, pp. 13851389, 2024. doi: 10.21437/Interspeech.2024-2273. Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, et al. Opens2s: Advancing fully open-source end-to-end empathetic large speech language model. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 906917, 2025a. Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, and Helen Meng. Mmsu: massive multi-task spoken language understanding and reasoning benchmark. arXiv preprint arXiv:2506.04779, 2025b. Xiong Wang, Yangze Li, Chaoyou Fu, Yike Zhang, Yunhang Shen, Lei Xie, Ke Li, Xing Sun, and Long MA. Freezeomni: smart and low latency speech-to-speech dialogue model with frozen LLM. In Forty-second International Conference on Machine Learning, 2025c. URL https://openreview.net/forum?id=s1EImzs5Id. Linye Wei, Wenjue Chen, Pingzhi Tang, Xiaotian Guo, Le Ye, Runsheng Wang, and Meng Li. Orchestrating dualboundaries: An arithmetic intensity inspired acceleration framework for diffusion language models, 2025. URL https://arxiv.org/abs/2511.21759. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025. URL https://arxiv.org/abs/2505.22618. 12 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024a. Zhifei Xie and Changqiao Wu. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024b. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit. Dataset from University of Edinburgh, 2019. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, et al. Diffa: Large language diffusion models can listen and understand. arXiv preprint arXiv:2507.18452, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models, 2025. URL https://arxiv.org/abs/2505.19223. 13 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding"
        },
        {
            "title": "A Data Details and Prompt Templates",
            "content": "A.1 ASR Data We use LibriSpeech (Panayotov et al., 2015) and GigaSpeech (Chen et al., 2021) as ASR corpora. For each transcript, we construct instruction-style ASR samples by applying 25 instruction templates generated by Qwen-32B (e.g., Please transcribe the following audio... or What is the exact content of this recording?). The full list of instruction templates is shown in Table A.1. Examples of ASR Prompts in Stage 1 1. Please transcribe the audio to text. 2. Convert this speech to text. 3. What is being said in this audio? 4. Transcribe the following audio clip. 5. Please write down what you hear in the audio. 6. Convert the spoken words to written text. 7. What words are spoken in this recording? 8. Please provide transcription of this audio. 9. Turn this speech into text format. 10. Write out what is said in the audio file. Figure A.1: Examples of ASR Prompts in Stage 1 A.2 SFT Data Part 1: Audio-caption-based AQA. We collect diverse set of audio and speech datasets covering speech, environmental sounds, and music, including ParaSpeechCaps (Diwan et al., 2025), AudioCaps (Kim et al., 2019), WavCaps (Mei et al., 2023), VocalSound (Gong et al., 2022), NSynth (Engel et al., 2017), FMA-medium (Defferrard et al., 2016), ESC-50 (Piczak, 2015), Clotho (Drossos et al., 2019), AccentDB (Ahamad et al., 2020), EmoV-DB (Adigwe et al., 2018), IEMOCAP (Busso et al., 2008), MELD (Poria et al., 2019), VCTK (Yamagishi et al., 2019), Meta FAIR ASR (Veliche et al., 2024), and VoxCeleb1 (Nagrani et al., 2017). From the Desta 2.5 (Lu et al., 2025) dataset, we select subset of question types and paralinguistic annotations, and use Qwen3-32B to generate an answer for each audioquestion pair. The exact prompts are listed in Figure A.2. Part 2: Direct Audio QA. We construct three categories of direct audio QA data: simple QA, complex QA, and empathetic QA. We first collect text-only QA pairs from Alpaca (Taori et al., 2023), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and WebQuestions (Berant et al., 2013), and then synthesize speech with CosyVoice2 (Du et al., 2024), using speaker prompts randomly sampled from the LibriSpeech training set. We categorize samples into simple or complex based on answer length and apply different instruction templates for each type. We further augment this with the English subset of OpenS2S (Wang et al., 2025a). Detailed statistics and prompt templates are shown in Tables A.1 and Figure A.3. Part 3: ASR Data. We randomly sample 5% of the ASR data used in Stage 1 and add it to the Stage 2 training set to preserve ASR-related acoustic grounding. Part 4: Multi-choice AQA Data We employ the AudioMCQ (He et al., 2025) corpus, which integrates multi-choice AQA instances derived from AudioCaps, Clotho, CompA-R (Ghosh et al., 2024), MusicCaps (Agostinelli et al., 2023), LP-MusicCaps (Doh et al., 2023), SpeechCraft (Jin et al., 2024), and TACOS (Primus et al., 2025). We follow the official data construction recipe of AudioMCQ without chain-of-thought, and then combine it with the Stage 2 data used for adapter training. Detailed dataset statistics are provided in Table A.1. A.3 Preference Data and Prompts To construct preference data for VRPO, we start from high-quality audio QA instances sampled from SFT data. Given an audio input, question, and reference answer, we prompt language model to generate fluent but partially 14 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table A.1: Dataset Statistics categorized by domain (Sound, Music, and Speech). Duration represents unique audio hours."
        },
        {
            "title": "Dataset",
            "content": "Samples Duration (h) Sound & General Audio AudioCaps Clotho ESC50 TACOS VocalSound WavCaps_AudioSetSL WavCaps_Freesound30s Subtotal Music FMA_medium LP-MusicCaps-MTT MusicCaps Nsynth Subtotal Speech AccentDB_extended CompA-R EmoV_DB IEMOCAP MELD SpeechCraft VCTK-Corpus AlpacaTrain MetaFairASR NaturalQuestions Opens2s Paraspeechcaps TrivalQA VoxCeleb1 WebQuestions Subtotal"
        },
        {
            "title": "Total",
            "content": "181,453 75,090 9,000 33,320 20,208 108,308 155,287 582,666 16,896 15,560 2,568 296,382 331,406 50,622 197,218 68,930 50,304 6,450 228,008 176,968 19,307 46,342 9,022 49,368 184,552 79,565 297,284 2,576 1,466,516 2,380,588 383.47 18.04 1.39 57.78 23.51 296.95 432.80 1,213.94 140.74 126.54 7.14 329.31 603. 19.28 170.60 9.49 5.23 0.93 483.82 44.04 25.17 48.66 10.25 67.47 497.07 104.29 340.39 2.49 1,829.18 3,646.85 incorrect answer that introduces subtle audio-related errors. The full prompt templates and examples for preference data construction are listed in Figure A.4."
        },
        {
            "title": "B Additional Experimental Details",
            "content": "All models and datasets utilized in this study are released under open-source licenses, in compliance with the terms of their respective original distributions. 15 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Prompts of Audio QA Data Creation in Stage 2 [System] You are helpful voice assistant. Imagine you can hear the audio clips. Focus on the audios and respond directly to the prompts. [User] This is the audio: {Audio Description}. {Text Prompt} [Assistant] - ... Figure A.2: Prompts of Audio QA Data Creation in Stage 2 Examples of Prompts in Stage 2 [Simple QA Prompts] 1. Listen to the audio question and reply with concise and factual answer. 2. Respond to the spoken query with brief, accurate answer based on the audio. 3. Give straightforward response to the question in the audio. 4. Respond directly to the audio question, stating only what is necessary. [Complex QA Prompts] 1. Listen to the audio question and provide comprehensive, detailed answer covering all aspects. 2. Respond to the spoken message with an in-depth explanation addressing every part of the query. 3. Offer full, structured answer to the voice question, including background and supporting details. [Sympathetic QA Prompts] 1. Listen to the voice message and respond in natural, conversational manner, showing empathy and genuine understanding. 2. Reply to the spoken message as if engaged in friendly, real-life conversation, maintaining warmth and authenticity. 3. Respond in smooth, natural tone that conveys human touch and kindness. Figure A.3: Examples of prompts in Stage 2 B.1 Baseline Models For completeness, we list here all baseline models used in our experiments. In the main text, we highlight subset of representative systems (e.g., Qwen2.5-Omni, Kimi-Audio, Qwen2-Audio, MiniCPM-O, and DIFFA); the full set of models covered in our evaluation is summarized in Table B.1. B.2 Benchmark Details MMSU (Wang et al., 2025b) is large-scale benchmark for assessing perception and reasoning in realistic spokenlanguage scenarios. It contains 5,000 audioquestionanswer triplets across 47 tasks, covering both linguistic and paralinguistic phenomena such as phonetics, prosody, semantics, emotion, and speaker traits. Tasks are divided into perceptionand reasoning-oriented categories, enabling comprehensive evaluation of fine-grained audio understanding. MMAU (Sakshi et al., 2025) evaluates advanced audio understanding via human-annotated multiple-choice questions over speech, music, and environmental sounds. It emphasizes high-level reasoning and expert knowledge rather than low-level perception. We report results on the Test-mini split. 16 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Preference Data Generation Prompt [System] You are creating training data for an audio question answering model (Audio-QA). You will NOT see the raw audio, only textual description of it. You are given QUESTION and REFERENCE_ANSWER which should be treated as the GOOD answer. Your task is to generate ONE BAD_ANSWER to the same question. Requirements for BAD_ANSWER: - It must be fluent and look superficially reasonable. - BUT it must be partially incorrect with respect to the audio description (e.g., wrong attribute, wrong event, missing key detail). - The error should be about the AUDIO-related content (e.g., rhythm, sound type, emotion, gender), rather than just style or verbosity. - DO NOT make the bad answer obviously nonsensical or completely unrelated. - It must still directly answer the question (not \"I don't know\"). At the end, you must also CHECK whether the REFERENCE_ANSWER is clearly better than the BAD_ANSWER. If they are too similar or you are not confident, mark the pair as unusable. Output your result in STRICT JSON with the following fields only: { \"bad_answer\": \"...\", \"explanation\": \"...\", \"usable\": true or false } Do NOT add any extra text outside the JSON object. [User] Audio Description: {audio_desc} Question: {question} Reference Answer (GOOD): {gold_answer} [Assistant]: {bad_answer} Figure A.4: Preference Data Generation Prompt MMAR (Ma et al., 2025) is multi-task audio reasoning benchmark designed to assess reasoning consistency and generalization across heterogeneous audio modalities. It focuses on joint perceptual grounding and reasoning over diverse audio inputs, providing complementary evaluation of audio reasoning robustness. VoiceBench (Chen et al., 2024b) targets semantic dialogue ability in spoken interaction. It is constructed by converting text-based benchmarks into audio queries using TTS and evaluates general knowledge, instruction following, and safety. Since VoiceBench primarily measures semantic dialogue performance rather than audio understanding, we include it only as supplementary evaluation. B.3 Training Details DIFFA-2 is trained using four-stage schedule with progressively reduced learning rates. LoRA is applied to the dLLMs backbone in Stage 3 with rank of 8 and scaling factor α of 16, while preference optimization is performed in Stage 4. We employ 64 NVIDIA A100 GPUs for the first three stages and 4 A100 GPUs for the final stage. The entire training pipeline takes approximately 5 days to complete. 17 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table B.1: Baseline models used in our experiments."
        },
        {
            "title": "Reference",
            "content": "GPT-4o-Audio Gemini 2.0 Flash Qwen3-Omni Qwen2.5-Omni Kimi-Audio MiniCPM-O Qwen2-Audio Baichuan-Omni-1.5 Baichuan-Audio GLM-4-Voice Step-Audio LLaMA-Omni Slam-Omni Freeze-Omni Mini-Omni / Mini-Omni2 Moshi DiVA VITA LTU Salmonn Qwen-Audio-Chat DIFFA (OpenAI et al., 2024) (Team et al., 2025) (Xu et al., 2025b) (Xu et al., 2025a) (Ding et al., 2025) (Team, 2025) (Chu et al., 2024) (Li et al., 2024) (Li et al., 2025b) (Zeng et al., 2024) (Huang et al., 2025) (Fang et al., 2025) (Chen et al., 2024a) (Wang et al., 2025c) (Xie & Wu, 2024a,b) (Défossez et al., 2024) (Held et al., 2024) (Fu et al., 2024) (Gong et al., 2024) (Tang et al., 2024a) (Chu et al., 2023) (Zhou et al., 2025) Table B.2: Stage-wise training configuration of DIFFA-2."
        },
        {
            "title": "Stage",
            "content": "Stage 1 Stage 2 Stage 3 Stage 4 LR"
        },
        {
            "title": "Batch Warmup Epochs",
            "content": "1104 5105 5105 5106 1280 196 196 4 1000 1000 1000 200 12 10 10 1 B.4 Inference Details At inference time, we pad the prompt and audio input to the target length and initialize the response sequence rT as fully masked. DIFFA-2 then performs iterative denoising over steps, progressively refining the response from coarse to fine (Figure 1). At each denoising transition s1 s2, the model predicts masked tokens conditioned on the audio input a, prompt p, and the current corrupted sequence rt: ˆrs1 = arg max pθ(r0 a, p, rs1). (7) Token-level confidence scores are used to re-mask fraction of low-confidence tokensproportional to s2/s1to form the next intermediate sequence rs2, enabling iterative refinement with full bidirectional context. Following LLaDA (Nie et al., 2025), we also adopt semi-autoregressive decoding strategy that generates the response in block-wise left-to-right manner. Within each block, tokens are decoded in parallel and selectively re-masked across diffusion steps, balancing generation quality and efficiency. 18 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding"
        },
        {
            "title": "Module",
            "content": "Table B.3: Statistics of parameters #Params"
        },
        {
            "title": "Trainable",
            "content": "Whisper-Large-V3 Encoder Semantic Adapter Acoustic Adapter dLLMs Backbone LoRA Modules"
        },
        {
            "title": "Total",
            "content": "637M 36.4M 47.9M 8.03B 14.7M 8.77B 0 36.4M 47.9M 0 14.7M 99.0M Prompt for Audio Understanding Tasks [System] You are helpful voice assistant. [User] This is the audio: {Audio Infomation}. Choose the most suitable answer from options A, B, C, and to respond the question in next line. Do not provide any additional explanations or content. Question:{Text Question} Options:{Text Options} [Assistant] - ... Figure B.1: Prompt for audio understanding tasks Factor-based parallel decoding. To further accelerate decoding, we adopt the factor-based parallel decoding strategy proposed in fast-dLLMs (Wu et al., 2025). This strategy extends threshold-based decoding by adaptively determining how many tokens to decode in parallel based on model confidence, rather than relying on fixed confidence threshold. Given the marginal confidence estimates of candidate tokens within decoding block, we first sort confidences in descending order and select the largest number of tokens such that (n + 1)(cid:0)1 c(n)(cid:1) < f, (8) where c(n) denotes the n-th highest confidence and is decoding factor hyperparameter. This criterion allows more aggressive parallel decoding when the model is confident, while conservatively reducing parallelism in uncertain regions. By leveraging this factor-based strategy on top of the diffusion denoising process, DIFFA-2 achieves improved inference efficiency while preserving generation quality. Inference hyperparameters used for each benchmark is presented in Table B.4. Key parameters include the maximum answer length, the block length for incremental decoding, and the total denoising steps. We synchronize the denoising budget with the sequence length to achieve peak generation quality. For factor-based parallel decoding, we set to 1.0. Note that the steps are invalid when the factor parallel decoding is applied. All inference experiments are conducted on single NVIDIA A100 GPU. The prompt for audio understanding tasks is provided in Figure B.1. 19 DIFFA-2: Practical Diffusion Large Language Model for General Audio Understanding Table B.4: Inference hyperparameters used for each benchmark"
        },
        {
            "title": "MMSU\nMMAU\nMMAR",
            "content": "AlpacaEval CommonEval SD-QA MMSU* OBQA IFEval AdvBench 128 16 16 16 128 128 128 16 16 256 128 128 16 16 32 32 32 16 16 32 32 128 16 16 16 128 128 128 16 16 256 128 Table C.1: Performance breakdown on VoiceBench. Metrics cover diverse direct QA and alignment tasks. Note that MMSU in VoiceBench is derived from MMLU-Pro, which differs from the MMSU benchmark."
        },
        {
            "title": "Model",
            "content": "AlpacaEval CommonEval SD-QA MMSU OBQA IFEval AdvBench Overall GPT-4o-Audio Kimi-Audio Qwen-2.5-Omni Phi-4-multimodal DIFFA-2 GLM-4-Voice DiVA Qwen2-Audio Freeze-Omni Step-Audio DIFFA LLaMA-Omni VITA Slam-Omni Mini-Omni2 Moshi 4.78 4.46 4.50 3.81 3.86 3.97 3.67 3.74 4.03 4.13 3.78 3.70 3.38 1.90 2.32 2."
        },
        {
            "title": "C Additional Experiments",
            "content": "4.49 3.97 3.84 3.82 3.67 3.42 3.54 3.43 3.46 3.09 2.96 3.46 2.15 1.79 2.18 1.60 75.50 63.12 56.40 39.78 40.78 36.98 57.06 35.72 53.45 44.21 34.45 39.69 27.94 4.16 9.31 15.64 80.25 62.17 61.70 42.19 38.13 39.75 25.76 35.72 28.14 28.33 29.57 25.93 25.70 26.06 24.27 24.04 89.23 83.52 80.90 65.93 61.53 53.41 25.49 49.45 30.98 33.85 35.60 27.47 29.01 25.27 26.59 25.93 76.02 61.10 53.50 45.35 40.76 25.92 39.16 26.33 23.40 27.96 26.56 14.87 22.82 13.38 11.56 10.12 98.65 100.00 99.20 100.00 85.58 88.08 98.27 96.73 97.30 69.62 76.54 11.35 26.73 94.23 57.50 44. 86.43 76.93 74.04 63.69 59.63 55.99 55.70 55.34 54.72 49.77 48.22 37.50 34.68 33.84 31.32 27.45 VoiceBench (Table C.1) focuses on spoken dialogue, instruction following, and safety rather than pure audio understanding. DIFFA-2 is trained with very limited dialogue-style audio data, and its overall score (59.63) is therefore clearly lower than heavily instruction-tuned AR omnimodels such as GPT-4o-Audio, Kimi-Audio, and Qwen2.5-Omni. Nevertheless, DIFFA-2 remains competitive with or better than several open-source baselines (e.g., GLM-4-Voice, DiVA, Qwen2-Audio, Freeze-Omni), and improves markedly over DIFFA (48.22) across most VoiceBench metrics. This gap between strong performance on MMSU/MMAU/MMAR and mid-range performance on VoiceBench highlights the design focus of DIFFA-2: it is optimized primarily for fine-grained audio understanding rather than large-scale conversational tuning."
        }
    ],
    "affiliations": [
        "College of Computer Science, Nankai University",
        "Meituan LongCat Interaction Team"
    ]
}