{
    "paper_title": "A Technical Study into Small Reasoning Language Models",
    "authors": [
        "Xialie Zhuang",
        "Peixian Ma",
        "Zhikai Jia",
        "Zheng Cao",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across a wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present a compelling alternative due to their remarkable computational efficiency and cost effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategies, including supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMs. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 4 0 4 3 1 . 6 0 5 2 : r Xialie Zhuang1,3, Peixian Ma2, Zhikai Jia3, Zheng Cao3,*, Shiwei Liu4 1University of Chinese Academy of Sciences, China 2The Hong Kong University of Science and Technology (Guangzhou), China 3SCITIX (SGP) TECH PTE. LTD., Singapore 4University of Oxford, UK *Corresponding authors: zcao@scitix.ai June 17,"
        },
        {
            "title": "ABSTRACT",
            "content": "The ongoing evolution of language models has led to the development of large-scale architectures that demonstrate exceptional performance across wide range of tasks. However, these models come with significant computational and energy demands, as well as potential privacy implications. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters present compelling alternative due to their remarkable computational efficiency and cost-effectiveness, particularly in resource-constrained environments. Despite these advantages, the limited capacity of 0.5 billion parameter models poses challenges in handling complex tasks such as mathematical reasoning and code generation. This research investigates various training strategiesincluding supervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as their hybrid implementations, to enhance the performance of 0.5B SRLMS. We analyze effective methodologies to bridge the performance gap between SRLMS and larger models and present insights into optimal training pipelines tailored for these smaller architectures. Through extensive experimental validation and analysis, our work aims to provide actionable recommendations for maximizing the reasoning capabilities of 0.5B models."
        },
        {
            "title": "Introduction",
            "content": "The field of Language Models (LMs) has experienced significant transformation with the advent of increasingly large models, which have demonstrated outstanding performance across range of tasks [1, 2, 3]. However, the pursuit of such massive models has introduced considerable challenges, including substantial computational requirements, increased energy consumption, and growing privacy concerns. In this context, Small Reasoning Language Models (SRLMs) with approximately 0.5 billion parameters have emerged as compelling alternative [4, 5]. These models not only provide enhanced computational efficiency but also offer cost-effectiveness, making them especially suitable for deployment in resource-constrained environments. Despite their efficiency, SRLMs face inherent limitations due to their reduced size. The constrained capacity of 0.5 billion parameters presents significant challenge when addressing complex tasks such as mathematical reasoning and code generation. These tasks require not only an understanding of intricate concepts but also the ability to produce precise and logical outputs. Early empirical evidence suggests that while SRLMs show potential in these areas, they exhibit noticeable performance gap compared to their larger counterparts. This gap underscores the need for effective enhancement techniques to improve the capabilities of 0.5 billion parameter models. In recent studies, researchers have employed variety of training strategies and pipelines to align the output of LMs with target preferences, such as upervised fine-tuning (SFT), knowledge distillation (KD), and reinforcement learning (RL) [6, 7]. However, in specific tasks, the training strategies of SRLMs have not been clearly explored, which makes it quite challenging to explore their capability boundaries and application fields. In addressing this gap, our research investigates range of training strategies, including SFT, KD, RL, as well as their hybrid configurations. Each of these methodologies presents distinct advantages and challenges, particularly regarding the enhancement of 0.5B SRLMs for mathematical reasoning. Our objective is to offer comprehensive analysis of these approaches and their applicability to them. Our research is driven by several key research questions: RQ1: What are the true capability boundaries of 0.5B models in reasoning task? To address this inquiry, we will undertake comprehensive comparison of 0.5B models with both closed-source and large-scale models. Through an analysis of baseline performance and the optimal outcomes achieved via various training methodologies, we aim to explore the gap between SRLMs and larger LMs. This comparative study will yield insights into the potential of 0.5B models and underscore the significance of training approaches in unlocking their full capabilities. RQ2: How do different training strategies, including SFT, RL, and KD, specifically enhance the performance of 0.5B models in these tasks? In this exploration, we will meticulously examine each training methods nuances and assess their individual contributions to model performance. By isolating the effects of SFT, RL, and KD, our objective is to deliver comprehensive understanding of how these strategies can be effectively utilized to optimize the capabilities of 0.5B models in mathematical reasoning. RQ3: What is the optimal combination of SFT, RL, and KD methods for enhancing the performance of 0.5B models in these domains? Here, we will investigate the synergistic effects of integrating various training techniques. By experimenting with diverse hybrid approaches, we aim to identify the most effective pipeline that harnesses the strengths of SFT, RL, and KD. This question seeks to reveal how the combination of these methodologies can lead to significant enhancements in the performance of 0.5B models, thereby offering practical framework for optimizing their capabilities in mathematical reasoning and code generation tasks. In summary, this research makes the following key contributions: We provide thorough analysis of the capabilities of 0.5B language models in mathematical reasoning and code generation, highlighting their potential and limitations. We systematically evaluate the effectiveness of various enhancement pipelines, including SFT, RL, and KD, and their hybrid combinations, offering valuable insights into the most effective methods for improving the performance of 0.5B models. We establish practical guidance for applying these enhancement techniques to 0.5B models, helping to maximize their potential in specialized tasks. We propose future research directions aimed at further enhancing the capabilities of SRLMs, with the goal of making advanced AI more accessible and efficient for wider range of applications."
        },
        {
            "title": "2 Background",
            "content": "2.1 Mathematical Reasoning Mathematical reasoning is complex cognitive task that requires understanding, manipulating, and solving mathematical problems. For language models, tasks involving mathematical reasoning necessitate comprehending mathematical concepts, applying logical reasoning, and performing calculations to reach correct solutions. This includes solving word problems, proving theorems, and carrying out algebraic manipulations. The model must show an understanding of mathematical principles, follow multi-step reasoning processes, and generate accurate, well-structured answers. 2.2 Small Reasoning Language Models The field of language modeling has experienced remarkable evolution over the past decade, marked by significant advancements in both methodology and application. Early language models, characterized by their relatively small size and limited capacity, struggled to capture the complexities inherent in human language. These initial models were often constrained by limited datasets, resulting in suboptimal performance across diverse range of linguistic tasks. The landscape began to transform with the introduction of pre-training techniques, where researchers found that training on extensive corpora of text led to considerable enhancements in models abilities to comprehend and generate natural language [1, 2, 3]. This pivotal shift heralded the era of large language models (LLMs), which rapidly became the predominant paradigm in natural language processing. The emergence of LLMs represented significant milestone in AI research, epitomized by models such as GPT-3 [8], which boasted billions of parameters and demonstrated unprecedented proficiencies in language understanding and generation. These models exhibited remarkable capabilities across wide array of tasks, including text completion, translation, question answering, and summarization. Their success was largely attributed to their immense size, enabling them to discern intricate patterns and relationships within vast datasets. However, the substantial computational and resource demands associated with LLMs posed significant challenges [9]; training and deploying these models required extensive computational power, considerable energy consumption, and substantial financial investment, thereby limiting accessibility for smaller research teams and organizations with constrained resources. In light of the limitations presented by LLMs, researchers began to explore the potential of developing smaller models that could deliver comparable performance while mitigating resource requirements. This exploration led to the advent of Small Reasoning Language Models (SRLMs), typically defined as models possessing approximately 0.5-1.0 billion parameters, such as TinyLlama [10], Qwen2.5 series [11] and Qwen3 series [12]. SRLMs aim to strike balance between performance and efficiency, rendering them suitable for deployment in resource-constrained environments. The impetus for SRLMs development arose from several key factors. Firstly, there was an escalating demand for models capable of operating on edge devices and within environments characterized by limited computational resources [5]. Secondly, privacy concerns associated with cloud-based LLMs prompted search for models that could be executed locally without jeopardizing sensitive data. Finally, the environmental impact of training and deploying large models became pressing concern, making smaller models an attractive and sustainable alternative. SRLMs have found diverse applications across numerous domains where resource efficiency and deployment flexibility are vital. Their compact size renders them ideal for utilization in mobile applications, Internet of Things devices, and other contexts with limited computational capacity [13]. Furthermore, their reduced computational requirements facilitate faster response times and lower latency, which are critical for real-time applications. In the realm of mathematical reasoning, SRLMs have been successfully employed to develop systems capable of solving mathematical problems, assisting in educational contexts, and supporting research endeavors in mathematics. In the area of code generation, they have been instrumental in creating tools that enhance programmers productivity, automate routine programming tasks, and aid in the acquisition of new programming languages. Despite their smaller size, SRLMs have demonstrated commendable performance across various tasks. Through rigorous optimization and the adoption of advanced training techniques, researchers have been able to enhance SRLMs capabilities, allowing them to approach the performance levels of their larger counterparts. This progress has unveiled new opportunities for applying AI in scenarios where resource constraints previously posed significant barriers. However, the smaller size of SRLMs also imposes inherent limitations on their ability to learn complex patterns and relationships within data. Such constraints may hinder their performance on tasks requiring deep understanding and sophisticated reasoning, particularly in advanced mathematical problem-solving [14] and intricate code generation [15, 16]. To address these challenges, researchers have developed range of enhancement techniques, including supervised finetuning, reinforcement learning [7], and knowledge distillation [17]. These methodologies aim to maximize the potential of SRLMs while expanding their capabilities. The objective of this research is to investigate these enhancement techniques and their applications to 0.5 billion parameter models within the specific domains of mathematical reasoning and code generation. By tackling the challenges inherent in SRLMs and capitalizing on the opportunities they present, we aspire to contribute to the advancement of efficient, specialized AI models that deliver high performance while minimizing resource requirements."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Our experimental methodology focuses on rigorously evaluating the discussed enhancement pipelines on selected 0.5B models. 3.1 Data Preparation In this study, we utilize the GSM8K training set as primary resource for both SFT and RL training intended to enhance the mathematical reasoning capabilities of 0.5B models. It comprises 7K multi-step arithmetic problems and undergoes rigorous preprocessing regimen that includes deduplication, tokenization, and Chain of Thought (CoT) augmentation. This process is designed to create well-structured examples that facilitate reasoning. In addition to advancing mathematical reasoning, this training initiative aims to explore the potential for knowledge transfer. Specifically, we evaluate whether the logical and structured reasoning skills acquired through GSM8K can enhance the models performance in other math reasoning. In this study, we utilize the GSM8K training set [18] as primary resource for both SFT and RL training intended to enhance the mathematical reasoning capabilities of 0.5B models. It comprises 7K multi-step arithmetic problems and undergoes rigorous preprocessing regimen that includes deduplication, tokenization, and Chain of Thought (CoT) 3 augmentation [19]. This process is designed to create well-structured examples that facilitate reasoning. In addition to advancing mathematical reasoning, this training initiative aims to explore the potential for knowledge transfer. Specifically, we evaluate whether the logical and structured reasoning skills acquired through GSM8K can enhance the models performance in other math reasoning. 3.2 Foundation Models We investigate the performance of three widely utilized SRLMs, including the Qwen2.5-0.5B-Instruct [11], Qwen2.50.5B [11], to assess their efficacy across various natural language processing tasks, particularly in mathematical reasoning and code generation. The Qwen2.5-0.5B-Instruct is specifically engineered for instruction-following tasks, demonstrating significant proficiency in generating responses that closely align with user prompts. In contrast, the Qwen2.5-0.5B serves as versatile, general-purpose model, providing robust foundation for multitude of applications. 3.3 Training Strategies for Exploration In this section, we introduce methodologies that applied in the training process of SRLMs. Supervised Fine-Tuning. Supervised Fine-Tuning (SFT) involves the adaptation of pre-trained models for specific tasks, particularly models with 0.5 billion parameters, with the primary objective of optimizing learning from limited resources while mitigating the risks of catastrophic forgetting. Among various methodologies, Low-Rank Adaptation (LoRA) is notable for its efficiency, as it integrates trainable low-rank matrices into Transformer layers, significantly reducing both the number of trainable parameters and memory requirements, thereby alleviating computational burdens and addressing issues of catastrophic forgetting. However, the effectiveness of SFT is inherently dependent on the scale and quality of labeled data, which can impose generalization ceiling and result in surface-level mimicry. Additionally, challenges such as distribution distortion and catastrophic forgetting remain pertinent, particularly for smaller model architectures. This study seeks to conduct comprehensive evaluation of both full-parameter fine-tuning and LoRA fine-tuning strategies to identify the optimal SFT approach. Reinforcement Learning. In this study, we employ the Group Relative Policy Optimization (GRPO) algorithm in reinforcement learning (RL), as it mitigates the necessity for value model, reduces memory requirements, and allows for clear articulation of reward targets, making it an optimal choice for the effective optimization of the policy model [20]. pivotal aspect of GRPO is the design of an effective reward function, which must provide clear and informative signals to guide the models learning process. Consequently, we have developed specific reward function tailored to enhance the models performance in mathematical reasoning and code generation tasks. The formulation of the reward function is delineated as follows: Format Reward. We push the model to enclose the reasoning process within <think>...</think> tags and to present the final answer enclosed within <answer>...</answer> tags. The structure of the format reward function is delineated as follows: Sf ormat = (cid:26)1, 1, if format is correct if format is incorrect (1) Accuracy Reward. The accuracy of reasoning results is key criterion in the evaluation. We prioritize the Accuracy Reward as second component of the reward functions: Saccuracy = (cid:26)1, 1, if answer is correct if answer is incorrect (2) In section A, we give detailed description of RL algorithm. Knowledge Distillation. Knowledge Distillation (KD) transfers capabilities from large teacher models to smaller student models. In our pursuit of effective knowledge distillation for SRLMs, we employ the GSM8K distillation dataset, which is generated using the CAMEL framework [21]. This dataset comprises collection of mathematical problem-solving traces, where each entry includes problem statement accompanied by comprehensive step-by-step solution. Our adherence to the methodology outlined by the CAMEL framework ensures that the distillation data is of high quality and maintains structured format, which is essential for the efficient transfer of knowledge from the teacher 4 model to our smaller models. By leveraging this specialized dataset, we aim to significantly enhance the mathematical reasoning capabilities of our models through the process of knowledge distillation. Hybrid Approaches. Hybrid approaches that integrate SFT, RL, and KD have emerged as an effective strategy to leverage the strengths of these methodologies. Typically, SFT is implemented first to establish robust baseline, adapting the pre-trained model to the specific task using labeled data. This initial phase sets the groundwork for further refinement through RL, which encourages exploration and utilizes feedback mechanisms to address the generalization limitations inherent in SFT. Additionally, KD is employed to transfer knowledge from larger teacher model to smaller 0.5B student model, thereby imparting general reasoning capabilities. This foundational knowledge can subsequently be specialized through either further SFT or RL. We hope to explore hybrid training pipeline paradigm suitable for SRLMs in our experiments, which outperform individual approaches by strategically balancing their strengths and weaknesses. While SFT offers strong foundation, RL enhances the models adaptability and generalization. However, the effectiveness of these combinations can be task-dependent. For instance, in certain scenarios, such as tool-calling tasks, pure RL methods, like Generalized Policy Optimization (GRPO), have shown superior performance compared to the SFT-then-RL sequence, indicating that SFT might introduce suboptimal biases in specific contexts. Consequently, the decision to adopt hybrid approach should be grounded in empirical validation, ensuring that the added complexity yields significant enhancements in the models capabilities across varied tasks. 3.4 Evaluation Benchmark & Metric The evaluation of SRLMs training is conducted across several benchmarks, including OlympiadBench [22], MATH500 [23], MINERVA [24], AMC23 [25], and GSM8K test set [18]. OlympiadBench consists of advanced mathematical problems similar to those found in mathematical olympiads. MATH500 challenges models with complex mathematical concepts and problem-solving techniques. MINERVA focuses on scientific reasoning, requiring the integration of scientific knowledge with mathematical skills. AMC23 offers unique set of mathematical challenges, while GSM8K specifically assesses the ability to handle grade school-level arithmetic problems that involve multi-step reasoning. Each benchmark is selected for its unique contribution to evaluating different aspects of mathematical reasoning. Performance of each LMs will be measured using accuracy as the primary metric across all mathematical reasoning benchmarks . All evaluations are conducted using the LightEval framework [26] with the following configuration: models are evaluated with maximum model length of 32768 tokens and 95% GPU memory utilization, using bfloat16 data type and data parallel processing across multiple GPUs. Generation parameters include maximum new tokens of 32768, temperature of 0.6, and top-p sampling of 0.95 to ensure robust and consistent evaluation results. We will also report computational costs, including training time and GPU resources utilized for each enhancement pipeline. 3.5 Implementation Settings For SFT training, we employ different configurations for full fine-tuning and LoRA-based training. For full fine-tuning, we set the learning rate to 4.0e-05 and train for 4 epochs with maximum sequence length of 8192. For LoRA-based training, we increase the learning rate to 1.0e-04. For GRPO-based RL training, we set the learning rate to 1.0e-06, with 16 generations per training step and maximum completion time of 2048. 3.6 Environment All experiments conducted in this study are performed on server operating under the Ubuntu 20.04 Linux distribution. This server is equipped with Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60 GHz CPU. The environment for training open-source LLMs comprises configuration of 8 H100 GPUs, each with 80 GB memory and delivering 312 TFLOPS performance capacity when utilizing BF16 precision."
        },
        {
            "title": "4 Experimental Results",
            "content": "In this section, we investigate the capability boundaries of SRLMs through series of experiments. Experiment 1 conducts comparative analysis of existing SRLMs against various parameter-scale LMs to evaluate performance differences. Experiment 2 explores the influence of distinct single training strategies on the improvement of SRLMs Experiment 3 examines the effects of hybrid training strategy, providing insights into the advantages of integrating multiple training approaches. 5 Table 1: Benchmarking of different scale LMs. We follow the results of 7B and 1.5B models from [27, 28]. represents that we can not extract answer from models response. Dash (-) represents unavailable official results. All results are evaluated by Lighteval framework [26]. Foundation Model OlympiadBench MATH500 MINERVA AMC23 GSM8K Qwen2.5-7B-Instruct Qwen2.5-Math-7B-Instruct Deepseek-R1-Distilled-7B Openthinker-7B Qwen2.5-1.5B-Instruct Qwen2.5-Math-1.5B-Instruct Still-3-1.5B-Preview Deepseek-R1-Distilled-1.5B Qwen2.5-0.5B-Instruct Qwen2.5-0.5B Qwen3-0.6B (No Thinking) Qwen3-0.6B (Thinking) LMs (7B) 75.2 79.8 92.8 83.2 LMs (1.5B) 9.0 51.2 84.4 83. SRLMs (0.5B) 31.4 - 55.6 56.4 38.2 40.7 49.8 - - 16.7 45.4 31.4 6.2 - 19.4 20.2 35.2 34. - - - 29.0 26.5 6.3 - 16.5 12.8 After Efficienct Training - SRLMs (0.5B) Qwen2.5-0.5B-Instruct Qwen2.5-0.5B 7.6 5. 32.4 23.6 8.5 4.4 55.0 45.0 77.5 74.5 - 22.5 66.7 55.0 5.0 - 25.0 27.5 10.0 5. 90.6 - - 0.2 - - - 45.5 - 73.7 74.5 54.0 34.2 4.1 Experiment 1: Comparison and Analysis of Different LMs Answer of RQ1: The findings indicate that while SRLMs exhibit distinct limitations in mathematical reasoning tasks, employing effective training strategies can markedly enhance their performance. Although these models do not yet fully match the capabilities of their larger counterparts, the improvements gained through targeted training efforts suggest that SRLMs have the potential to approach the performance levels of larger models. As demonstrated in Table 1, there is distinct performance disparity between SRLMs and the larger scale LMs across various mathematical reasoning benchmarks. Specifically, the Qwen2.5-0.5B-Instruct model demonstrates baseline performance of merely 6.2% on OlympiadBench and 31.4% on MATH500, significantly lagging behind the 7 billion and 1.5 billion parameter models, while the Qwen2.5-7B-Instruct model achieves an impressive 38.2% on OlympiadBench and 75.2% on MATH500, Qwen2.5-1.5B-Instruct model attains 16.7% on OlympiadBench and 51.2% on MATH500. These results underscore the inherent limitations associated with 0.5B SRLMs, attributing their subpar performance to their Relatively small parameter scale and week reasoning ability. The implementation of effective training strategies has led to significant enhancements in the performance of 0.5B models. For instance, the Qwen2.5-0.5B-Instruct model, following optimization, demonstrates accuracy of 7.6% on OlympiadBench and 32.4% on MATH500. Despite significant enhancements, the performance of 0.5B SRLMs continues to fall short compared to their larger LMs. However, some optimized SRLMs have demonstrated notable reduction in this performance gap. For example, Qwen3-0.6B [12] achieve performance metrics of 20.2% on OlympiadBench and 56.4% on MATH500 after undergoing similar training optimizations. These results underscore that appropriate training methodologies can effectively extend the capability boundaries of SRLMs. With further refinements in training strategies and improvements in model architecture, 0.5B models have the potential to attain performance levels that are increasingly comparable to those of larger models. This outcome demonstrates that the true capability boundaries of SRLMs are not static; rather, they can be expanded through optimized training methodologies. 6 Table 2: Comparison of different training strategies. represents that we can not extract answer from models response. Dash (-) represents unavailable official results. All results are evaluated by Lighteval framework [26]. Method OlympiadBench MATH500 MINERVA AMC23 GSM8K Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-0.5B+KD Qwen2.5-0.5B+KD (Lora) Qwen2.5-0.5B+SFT Qwen2.5-0.5B+SFT (Lora) Qwen2.5-0.5B+RL Baselines - 6.2 - 31. Training on Base Model 3.3 2.2 3.7 0.7 5.2 10.0 7.6 9.2 1.2 23.6 Training on Instruct Model Qwen2.5-0.5B-Instruct+KD Qwen2.5-0.5B-Instruct+KD (Lora) Qwen2.5-0.5B-Instruct+SFT Qwen2.5-0.5B-Instruct+SFT (Lora) Qwen2.5-0.5B-Instruct+RL 3.7 3.0 2.4 3.7 7. 15.2 16.6 12.2 14.8 32.4 - 6.3 1.8 2.5 1.4 0.3 4.4 3.3 1.8 2.9 2.9 8.5 - 5.0 7.5 0.0 5.0 2.5 5. 2.5 10.0 5.0 7.5 7.5 - 45.5 18.7 9.7 21.6 2.1 34.2 42.3 40.0 30.9 31.4 54.0 4.2 Experiment 2: Comparison and Analysis of Different Training Strategies Answer of RQ2: The impact of different training strategies on SRLMs is complex. SFT and KD do not always result in performance improvements and can even lead to declines in certain cases. In contrast, RL proves to be more effective and dependable method for enhancing model capabilities. This suggests that RL is better suited for unlocking the full potential of Small Reasoning Language Models in mathematical reasoning tasks. Although SFT and KD have their advantages, RL appears to provide more stable and efficient pathway for enhancing capabilities in this context. The experimental results in Table 2 provide detailed analysis of various training strategies applied to the Qwen2.5-0.5B series SRLMs across multiple mathematical benchmarks. The experiment on the impact of KD shows that this strategy does not consistently lead to performance improvements. The application of Knowledge Distillation (KD) to the Qwen2.5-0.5B-Instruct model leads to performance declines across most benchmarks. Notably, there is reduction from 6.2% to 3.3% on the OlympiadBench and significant drop from 45.5% to 18.7% on GSM8K. These findings indicate that KD may not effectively enhance performance in all mathematical reasoning tasks and can sometimes compromise the models specialized abilities. The empirical findings reveal that applying SFT to the Qwen2.5-0.5B-Instruct model results in noticeable decline in performance across various benchmarks. For instance, performance on OlympiadBench diminishes significantly from 6.2% to 3.7%, and on MINERVA, it plummets from 6.3% to 1.4%. These results indicate that, despite the widespread use of SFT, it does not consistently lead to performance improvements and may, in certain cases, result in suboptimal behaviors depending on the specific models and tasks involved. The integration of KD, SFT and LoRA may lead to decline in performance on certain evaluations. For example, the Qwen2.5-0.5B-Instruct model fine-tuned with both techniques achieved only 0.7% on the OlympiadBench and 1.2% on another MATH500 benchmark. These disappointing results suggest that the combination of SFT and LoRA may not be suitable for enhancing this models effectiveness on these particular tasks, indicating that careful consideration is needed when choosing fine-tuning strategies to optimize performance. RL has proven to be highly effective approach for improving model performance, particularly in tasks that require mathematical reasoning. When enhanced with RL, the Qwen2.5-0.5B model demonstrates significant gains, achieving performance metrics of 5.2% on OlympiadBench and 23.6% on MATH500. Furthermore, applying RL to the baseline Qwen2.5-0.5B-Instruct model leads to even more impressive results, with scores increasing to 7.6% on OlympiadBench and 32.4% on the same benchmark. These findings highlight the potential for RL to considerably elevate mathematical reasoning capabilities, especially when it is built upon strong foundational model. This suggests that integrating 7 Table 3: Comparison of different hybrid training strategies. represents that we can not extract answer from models response. represents that the model fails to converge the reward curve during RL training, resulting in training collapse. Dash (-) represents unavailable official results. All results are evaluated by Lighteval framework [26]. Method OlympiadBench MATH500 MINERVA AMC23 GSM8K Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-0.5B+KD+RL Qwen2.5-0.5B+KD (Lora)+RL Qwen2.5-0.5B+SFT+RL Qwen2.5-0.5B+SFT (Lora)+RL Baselines - 6. - 31.4 Hybrid Training on Base Model 3.3 1.8 - 0.5 10.6 4.4 - 1.0 Hybrid Training on Instruct Model Qwen2.5-0.5B-Instruct+KD+RL Qwen2.5-0.5B-Instruct+KD (Lora)+RL Qwen2.5-0.5B-Instruct+SFT+RL Qwen2.5-0.5B-Instruct+SFT (Lora)+RL - 4.6 3.1 2.2 - 19.4 13.8 16.0 Qwen2.5-0.5B+RL Qwen2.5-0.5B-Instruct+RL 5.2 7.6 23.6 32.4 Direct RL Training on Instruct Model - 6.3 2.2 0.7 - 1.1 - 3.7 4.8 3.7 4.4 8.5 - 5.0 0.0 5.0 - 0. - 5.0 10.0 5.0 5.0 7.5 - 45.5 29.6 2.8 - 1.2 - 47.9 45.5 44.3 34.2 54. direct RL into the training process not only enhances specific benchmarks but also signifies broader implication for developing models that excel in complex reasoning tasks. 4.3 Experiment 3: Comparison and Analysis of Hybrid Training Strategies Answer of RQ3: While hybrid strategies can significantly enhance model performance, their effectiveness is contingent upon both the models pre-training and the specific requirements of the task at hand. The optimal combination of SFT, KD, RL methods varies according to these factors, necessitating meticulous tuning to mitigate potential training instability. Notably, for achieving substantial and stable performance improvements, RL emerges as the superior approach, particularly when applied to models such as Qwen2.5-0.5B-Instruct, which are pre-trained for dialog interactions. This highlights the critical need to consider both the models architectural attributes and the specific dynamics of the training process when selecting the most suitable enhancement strategy. The experimental results presented in Table 3 provide valuable insights into the efficacy of hybrid training strategies for 0.5B models. Generally, hybrid approaches demonstrate the potential to enhance model performance relative to single training strategies; however, they also introduce some potential risks for SRLMs. For instance, the Qwen2.50.5B+KD+RL configuration achieves notable performance improvements on certain benchmarks, such as MATH500, where scores rise from 31.4% to 45.5%. Nevertheless, this gain is not universal, as some hybrid strategies, including Qwen2.5-0.5B+SFT+RL and Qwen2.5-0.5B-Instruct+SFT+Lora+RL, have been observed to cause training collapse, highlighting the potential instability inherent in the combination of multiple training methodologies. Hybrid training strategies demonstrate increased effectiveness when applied to models possessing intrinsic dialog capabilities. Notably, the Qwen2.5-0.5B-Instruct+KD+Lora+RL configuration exhibits substantial performance enhancements across various benchmarks, achieving 47.9% on GSM8K. This finding indicates that models specifically pre-trained for instruction following are better positioned to capitalize on the synergistic benefits of KD and RL. Additionally, the Qwen2.5-0.5B-Instruct+SFT+RL setup shows marked improvements on MATH500 and MINERVA, further reinforcing the notion that hybrid strategies are optimally effective when they build upon solid foundation of instruction-following capabilities. Despite the advancements in hybrid approaches, direct RL continues to demonstrate superior performance. Notably, the Qwen2.5-0.5B-Instruct+RL configuration outperforms most hybrid training strategies, which underscore that RL is particularly beneficial for SRLMs."
        },
        {
            "title": "5 Future Direction",
            "content": "Based on our comprehensive investigation of enhancement pipelines for 0.5B parameter language models, several promising directions emerge for future research and development. Model Release Plan. Building upon the insights gained from this study, we plan to release series of incrementally trained SRLMs that demonstrate the effectiveness of our proposed enhancement pipelines. These models will be made publicly available to facilitate reproducible research and enable the broader community to benefit from our findings. The release will include: Models enhanced through our best-performing RL strategies (particularly GRPO-based approaches) Hybrid models that combine multiple enhancement techniques Comprehensive documentation of training procedures, hyperparameters, and evaluation protocols Advanced Training Methodologies. Future work should explore more sophisticated training paradigms specifically designed for SRLMs. This includes developing SRLMs-aware RL algorithms that account for the unique characteristics and limitations of smaller models, as well as investigating novel reward shaping techniques that can guide these models more effectively toward desired behaviors without causing training instabilities. Enhanced Knowledge Distillation. Our results indicate that current KD approaches may not fully unlock the potential of 0.5B models. Future research should focus on developing more effective distillation techniques, such as multi-teacher distillation, progressive distillation, and attention-guided knowledge transfer methods that can better bridge the capacity gap between large teacher models and small student models. Efficiency and Sustainability. As the field moves toward more sustainable AI development, future research should emphasize developing training techniques that maximize performance gains while minimizing computational costs and environmental impact. This includes investigating more efficient PEFT methods, developing better pre-training strategies for SRLMs, and exploring federated learning approaches for distributed model enhancement."
        },
        {
            "title": "6 Conclusion",
            "content": "This research investigates the capabilities of 0.5B SRLMs in the domains of mathematical reasoning and code generation, specifically addressing the inherent challenges associated with their limited capacity. We examine various training strategies, including SFT, KD, and RL, providing comprehensive analysis of both their potential and inherent limitations. Our empirical findings indicate that while SRLMs can be significantly enhanced through these diverse training methodologies, the effectiveness of each approach varies based on the specific task and model requirements. For instance, employing SFT integrated with Chain-of-Thought (CoT) data from KD has been demonstrated to substantially improve mathematical reasoning capabilities. In contrast, RL facilitates the transfer of valuable knowledge from larger, more complex models, enabling SRLMs to leverage existing advancements. Importantly, our analysis reveals that hybrid strategies, which combine elements of these methodologies, often yield superior outcomes compared to singular approaches. This insight underscores the necessity of tailoring training practices to the specific challenges posed by the task at hand. Overall, this research contributes to deeper understanding of the effective utilization and enhancement of 0.5B models for specialized applications. It offers practical guidance for leveraging SRLMs in resource-constrained environments, thus promoting their applicability in real-world scenarios where computational resources may be limited."
        },
        {
            "title": "References",
            "content": "[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. [2] Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang, and Yuyu Luo. survey of nl2sql with large language models: Where are we, and where are we going? arXiv preprint arXiv:2408.05109, 2024. [3] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. survey on large language models: Applications, challenges, limitations, and practical usage. Authorea Preprints, 2023. [4] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, et al. survey of small language models. arXiv preprint arXiv:2410.20011, 2024. [5] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas Lane, and Mengwei Xu. Small language models: Survey, measurements, and insights. arXiv preprint arXiv:2409.15790, 2024. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219, 2025. [8] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30:681694, 2020. [9] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. [10] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. [11] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [12] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [13] Yinghong Li, Yudong Yan, Zhuohao Tong, Yu Wang, Yinqi Yang, Mingze Bai, Dan Pu, Jiazheng Xie, Chuan Liu, Bo Li, et al. Efficient fine-tuning of small-parameter large language models for biomedical bilingual multi-task applications. Applied Soft Computing, 175:113084, 2025. [14] Leonardo Ranaldi and Andre Freitas. Aligning large and small language models via chain-of-thought reasoning. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18121827, 2024. [15] Débora Souza, Rohit Gheyi, Lucas Albuquerque, Gustavo Soares, and Márcio Ribeiro. Code generation with small language models: deep evaluation on codeforces. arXiv preprint arXiv:2504.07343, 2025. [16] Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. arXiv preprint arXiv:2504.08600, 2025. [17] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. [18] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 10 [19] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [21] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [22] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [23] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [24] math ai. Minervamath, 2025. https://huggingface.co/datasets/math-ai/minervamath. [25] AI-MO. Amc23, 2025. https://huggingface.co/datasets/AI-MO/aimo-validation-amc. [26] Nathan Habib, Clémentine Fourrier, Hynek Kydlíˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. [27] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Surpassing o1https://pretty-radio-b75.notion.site/ Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. preview with 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Deepscaler: [28] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient."
        },
        {
            "title": "A Detailed Training Strategies",
            "content": "For reinforcement learning phase, we use the Group Relative Policy Optimization (GRPO) algorithm to enhance our training protocol. This method not only eliminates the need for value model but also reduces memory requirements. Additionally, GRPO allows us to clearly define our reward targets. These advantages make it an excellent choice for effectively optimizing the policy model. For each natural language question, the policy model generates set of answer candidates {o1, o2, . . . , oG} along with their corresponding reasoning processes derived from the previous policy πold. These candidates are rigorously assessed using composite reward function that assigns specific reward scores. By focusing on the relative performance of the answer candidates within the group, GRPO efficiently computes the rewards for each output, thus steering the policy update in alignment with our defined objectives. JGRPO(θ) =E vP (V),{oi}G (cid:34) i=1πθold (Ov) 1 (cid:88) i=1 (cid:0)min (cid:0)rratio Ai, clip (cid:0)rratio , 1 ϵ, 1 + ϵ(cid:1) Ai (cid:1) βDKL(πθπref)(cid:1) (cid:35) , (3) = πθ(oiV ) where rratio πold(oiV ) represents the importance sampling ratio, which quantifies the relative likelihood of generating output oi under the new policy πθ compared to πold; Ai represents the group-relative advantage for each output; the hyperparameter ϵ and β control the update step and divergence regularization; πref represents the reference policy."
        }
    ],
    "affiliations": [
        "SCITIX (SGP) TECH PTE. LTD., Singapore",
        "The Hong Kong University of Science and Technology (Guangzhou), China",
        "University of Chinese Academy of Sciences, China",
        "University of Oxford, UK"
    ]
}