{
    "paper_title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics",
    "authors": [
        "Maojia Song",
        "Renhang Liu",
        "Xinyu Wang",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Soujanya Poria",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 3 1 5 0 . 0 1 5 2 : r DEMYSTIFYING DEEP SEARCH: HOLISTIC EVALUATION WITH HINT-FREE MULTI-HOP QUESTIONS AND FACTORISED METRICS Maojia Song, Renhang Liu Singapore University of Technology and Design (SUTD) maojia song@mymail.sutd.edu.sg, renhang liu@sutd.edu.sg Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang Tongyi Lab, Alibaba Group {xinyu.wxy, jiangyong.jy, xiepengjun.xpj, f.huang}@alibaba-inc.com Soujanya Poria Nanyang Technological University (NTU) soujanya.poria@ntu.edu.sg Jingren Zhou Tongyi Lab, Alibaba Group jingren.zhou@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, benchmark of hint-free multi-hop questions paired with controlled Wikipedia sandbox that ensures full traceability of model actions, and holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 stateof-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose fundamental gaptodays systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow EvidenceLoop that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetectives diagnostic framework can guide concrete architectural improvements, establishing our benchmark as critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents."
        },
        {
            "title": "INTRODUCTION",
            "content": "Web agentsautonomous systems that navigate and extract information from the internetare becoming essential for extending language models beyond their parametric knowledge, as they tackle complex information-seeking tasks by integrating external search with internal knowledge, aggregating evidence from multiple sources, and synthesising it into coherent answers Nakano et al. (2021); Ferrag et al. (2025). Among the evaluation scenarios for these systems Deep search is central and foundational task: it requires uncovering hidden facts or entities through multi-step reasoning, noise filtering, and tackling the persistent cant find it challenge that even expert human Equal Contributions. 1 Figure 1: Comparison of different question formulations in multi-hop deep search. Left: PathHinting (PH) benchmarks such as HotpotQA embed the reasoning path directly in the question text, effectively reducing reasoning to execution. Middle: Specification-Hinting (SH) benchmarks such as BrowseComp obscure the target entity behind multiple attributes, testing filtering rather than autonomous exploration. Right: Our Hint-Free (HF) formulation in WebDetective removes both path and specification hints, requiring agents to autonomously discover reasoning chains within controlled Wikipedia sandbox. searchers face. Recent efforts have sought to advance performance by designing stronger agent systems Li et al. (2025b;c); Sun et al. (2025); Jiang et al. (2025); Li et al. (2025a), while others have introduced new benchmarks to better characterise its nature Du et al. (2025); Wong et al. (2025). However, we identify critical yet overlooked dimension in current deep search evaluation: the presence of various forms of hinting in question formulations, which fundamentally alters the nature of the task. As shown in Figure 1, classical multi-hop QA datasets such as HotpotQA Yang et al. (2018a) exhibit Path-Hinting (PH), where questions explicitly narrate the reasoning chainfor example, Who is the husband of the stepmother of the brother of Kane Cornes? directly instructs the agent to sequentially find the brother, then the stepmother, then her husband, reducing reasoning to execution. More recent benchmarks, including BrowseComp Wei et al. (2025) and WebShaper Tao et al. (2025), attempt to mitigate this issue through Specification-Hinting (SH): rather than naming the target entity, questions describe it indirectly through multiple attributes. For instance, instead of asking directly about Graham Cornes, the query specifies Which radio presenter at 5AA, former footballer, at least 20 years senior to his wife who was 2007 Labor candidate?requiring constraint satisfaction through filtering rather than genuine exploratory reasoning. Such guided reasoning bypasses models need to autonomously search for information and construct reasoning paths, failing to test critical capability of web-agents: the capacity to autonomously discover which connections matter, generate hypotheses about possible reasoning trajectories, and adaptively explore the information space without guidance. When agents are provided either an explicit path (PH) or unique signature (SH), they operate with strong scaffolding that is rarely available in real-world scenarios. Furthermore, existing evaluations suffer from critical limitation: they typically report only aggregate pass rates, collapsing diverse failure modes into single metric. This obscures crucial distinctionsan agent that searches exhaustively but fails to connect evidence exhibits fundamentally different limitations than one that gives up prematurely or misuses its parametric knowledge. Without understanding these failure modes, it becomes hard to diagnose system weaknesses or guide improvements. In this work, we introduce WebDetective, benchmark that rethinks hint-free deep search evaluation through the co-design of questions and their environment. We construct Hint-Free (HF) Multi-Hop questions that avoid both path narration and attribute fingerprintsfor example, Who is the father of Kane Cornes?requiring agents to autonomously discover relevant contexts and reasoning chains. controlled Wikipedia sandbox prevents shortcuts by revealing information only when the correct reasoning path is followed, ensuring that intermediate links must be discovered 2 sequentially. This co-design guarantees that successful answers reflect genuine multi-hop reasoning and enables two-level evaluation framework with precise attribution of failure modes: we can separate knowledge sufficiency (from search or parametric sources) from generation quality, and determine whether an agent failed by halting too early, missing connections, or refusing appropriately when evidence was lacking. Such fine-grained diagnostics are possible only because the benchmark enforces unique path to the correct answer and records exactly how far along that path each agent progresses. Additionally, we further design an agentic workflow baseline EvidenceLoop that explicitly incorporates context retention, memory management, and verification steps, offering first attempt at addressing the unique challenges posed by hint-free deep search. Through our diagnostic evaluation framework, we uncover fundamental brittleness in current systems when reasoning paths must be discovered rather than given, exposing critical gaps between existing capabilities and the requirements of genuine autonomous deep search."
        },
        {
            "title": "2 THE WEBDETECTIVE BENCHMARK",
            "content": "2.1 HINT-FREE MULTI-HOP QUESTION ANSWERING Given question and knowledge corpus C, an agent must find the answer by discovering and composing sequence of evidence pieces = {e1, . . . , en} from C. Each ei is an atomic fact on the page of entity vi that links (semantically or relationally) to vi+1, forming reasoning chain v0 v1 vn, where v0 is the starting entity (typically mentioned in q) and vn yields a. The reasoning function Rfunc : specifies how these facts are composedvia logical inference, relation transitivity, or domain rules. We call any information embedded in that reveals Rfunc or fingerprints vn hint h. We define two prevalent hint types in existing multi-hop QA benchmarks Yang et al. (2018b); Chen et al. (2019); Wei et al. (2025): Path-Hinting (PH): The question directly encodes the reasoning chain, with hPH = Encode(Rfunc) revealing its structure. For example, Who is the husband of the stepmother of the brother of Kane Cornes? decomposes into: find brother stepmother husband. The agent no longer discovers the path, but executes the specified hPH. Specification-Hinting (SH): The question hides the target behind constraints, with hSH = {s1, s2, . . . , sk} narrowing the search space to unique entity. For instance, Which radio presenter at 5AA, former footballer, at least 20 years senior to his wife who was 2007 Labor candidate? yields hSH = {radio presenter at 5AA, former footballer, 20+ years senior to wife, wife was 2007 Labor candidate}, collectively fingerprinting Graham Cornes; the task reduces to constraint satisfaction (match all si) rather than discovering which connections matter for reasoning. In contrast, we propose Hint-Free (HF) Multi-Hop QA, where = . Questions contain only the essential query, without path narration or attribute fingerprintsfor example, Who is the father of Kane Cornes? Answering requires discovering evidence pieces e1 (Kane Cornes has brother Chad Cornes), e2 (Chads stepmother is Nicole Cornes), e3 (Nicoles husband is Graham Cornes), and composing them via familial reasoning to derive = Graham Cornes. Crucially, the agent must independently uncover both the evidence chain and the reasoning function Rfunc, capturing the fundamental ability to transform simple information need into an autonomously constructed reasoning structure. 2.2 THE CO-DESIGN PRINCIPLE While hint-free questions eliminate linguistic scaffolding, we observe that question design alone is insufficient to ensure genuine multi-hop reasoning. In open corpora or live web environments, even well-designed hint-free questions permit shortcuts that bypass the intended reasoning chain. Consider our example Who is the father of Kane?in Wikipedia or web search, direct co-occurrences of Kane and Graham Cornes may exist in unrelated contexts, or intermediate entities like Chad Cornes could be found through direct search, allowing agents to bypass the intended reasoning path. Moreover, because both answers and intermediates are usually accessible through multiple 3 paths, it becomes impossible to tell whether an agent truly discovered the reasoning chain or simply relied on shortcuts or prior knowledge. To address this, we introduce co-designed evaluation system that jointly constructs questions and environment to enforce reasoning-path discovery. Our controlled sandbox applies selective entity masking: for chain v0 v1 vn (where v0 appears in the question and vn yields the answer), each intermediate vi is masked everywhere except on vi1s page: vi is discoverable agent visits page(vi1) This enforces strict sequential access, eliminating shortcuts. For example, in Who is the father of Kane Cornes?, Chad Cornes appears only on Kanes page, Nicole Cornes only on Chads, and Graham Cornes only on Nicoles, requiring the chain Kane Chad Nicole Graham. Success thus guarantees full path discovery, while failures can be precisely diagnosedwhether the agent failed to explore, missed relevant step, or struggled with synthesis. This design turns multi-hop QA evaluation from probabilistic judgment into deterministic verification."
        },
        {
            "title": "2.3 BEYOND PASS RATE: A DIAGNOSTIC EVALUATION FRAMEWORK",
            "content": "Traditional multi-hop QA evaluation reduces performance to single pass rate, obscuring distinct failure modes: an agent that searches exhaustively but fails to synthesise evidence is fundamentally different from one that refuses prematurely or hallucinates from parametric knowledge. Our sandbox, with guaranteed unique reasoning paths, enables precise diagnosis by separating knowledge sufficiency (whether the agent has obtained the necessary evidence through search or memory) from generation quality (whether it can synthesise correct answer or appropriately refuse). This decomposition reveals that similar pass rates can mask very different underlying capabilities. See Appendix for details. Knowledge Discovery Metrics. We assess whether agents acquire necessary information through two complementary metrics. Knowledge Sufficiency determines if an agent possesses all required evidence = {e1, ..., en} for answering either from search or parametric knowledge. We track which evidence the agent discovered through search by monitoring visited pages in our sandbox. For any missing evidence ei / Efound, we probe the models parametric knowledge with targeted ?). The Search Score extends this by crediting models queries (e.g., Kane Cornes has brother that efficiently combine partial search with parametric knowledgerecognizing that if an entity discovered through search has meaningful relationship to the answer stored in parametric memory, this represents legitimate reasoning that demonstrates efficient knowledge utilization. Generation Quality Metrics. Conditioned on knowledge sufficiency, we classify cases as knowledge-sufficient (S) or insufficient (I), and attempted (A) or refused (N ). This yields two key metrics. Good Refusal (GR) evaluates abstention when evidence is lacking, with F1GR balancing recall (avoiding hallucination) and precision (refusing only when justified). Knowledge Utilisation (KU) measures synthesis when evidence is present, with F1KU balancing recall (using available evidence) and precision (answers grounded rather than speculative). These complementary F1 scores capture refusal discipline and evidence integration. We define unified Generation Score as GenScore = F1GR + F1KU KnowledgeScore, where Knowledge Score is the fraction of instances with knowledge sufficiency (all evidence gathered). The sufficiency weighting prevents gaming by agents that simply refuse all questions, ensuring models are rewarded only when they both acquire the necessary evidence and handle it appropriatelythrough correct synthesis or justified refusal. Knowledge Degradation Analysis. Even when models achieve knowledge sufficiency, they often fail to generate correct answers, revealing gap between evidence possession and synthesis. To diagnose this gap, we design two tests. The Knowledge Forget test captures cases where models fail to apply parametric knowledge in full-context reasoning, despite succeeding on isolated probes. The Lead-astray test captures failures caused by noisy search contextsirrelevant pages, failed attempts, or exploration clutterthat prevent models from synthesising answers they could otherwise 4 produce from clean evidence. Together, these metrics go beyond simple pass rates by pinpointing whether errors stem from inadequate search, over-confident hallucination, weak synthesis, or degraded reasoning under noise, thereby clarifying the capabilities required for robust multi-hop reasoning."
        },
        {
            "title": "2.4 DATASET CONSTRUCTION",
            "content": "We build WebDetective through pipeline that transforms single-hop Wikipedia QA pairs into verified multi-hop reasoning chains, ensuring every hop is necessary. See section and section for details. Source Data and Chain Discovery. Starting from Wikipedia QA pairs where each question references starting entity v0 and has an answer vn, we remove the direct link v0 vn and use BFS on the hyperlink graph to find the shortest alternative path v0 v1 vn. For each edge (vi, vi+1), we extract the sentence ei in the page of vi that links vi and vi+1, forming the evidence chain = {e1, . . . , en}. Verification of Necessity. Since many graph paths are irrelevant, we apply three automated checks with strong LM (Qwen-3-235B-A22B): (1) Parametric Inaccessibility: LM(q) = vn, ensuring the answer is not retrievable from parametric memory alone. (2) Evidence Sufficiency: LM(q, E) = vn, confirming the full chain supports the answer. (3) Evidence Necessity: LM(q, {ei}) = vn for all ei, verifying no hop is redundant. Human Validation. Surviving questions are manually checked to ensure all evidence is required, reasoning is logically sound, and no hints are embedded in the wording. The final benchmark contains 200 validated questions with varied hop counts and types."
        },
        {
            "title": "3 A BASELINE ATTEMPT: THE EVIDENCELOOP FRAMEWORK",
            "content": "To address the unique challenges posed by hint-free multi-hop reasoning, we develop EvidenceLoop, an agentic workflow baseline that explicitly incorporates context retention, memory management, and verification steps to maintain reasoning coherence across extended search trajectories. Unlike standard ReAct implementations that can lose track of evidence across many search iterations, our workflow introduces structured mechanisms for tracking discovered entities, maintaining evidence chains, and verifying reasoning paths before answer generation. We provide detailed description of the EvidenceLoop architecture, including its controller configuration, memory modules, and verification procedures in Appendix D. Iterative Refinement with Fallback. Our framework balances exploration breadth with computational efficiency through Rmax iterations. In each round r, solver agents explore different reasoning paths in parallel, each operating with up to actions and guided by an aggregated context (C 0 = ). Their outputs are refined through two stages: (1) an extraction agent distills key findings, entity references, and promising paths; and (2) an aggregation agent synthesises them into r+1, retaining valuable evidence while discarding noise. This process allows early iterations to explore broadlysports connections, geographic links, family tieswhile progressively focusing on the most promising directions. If no conclusive answer is found after Rmax rounds, final fallback stage consolidates all evidence into final and invokes synthesis-only solver, distinguishing failures due to insufficient exploration from those due to poor evidence composition. Iterative refinement is supported by persistent memory that Evidence Memory System. records all evidence retrieved during search. Each piece of evidence is assigned unique Evidence ID (EID) and stored with its full content. Agents receive both summaries and EID references (e.g., Kane has brother Chad [EID-042]), enabling them to reason over concise contexts while retaining the ability to fetch full documents on demand through retrieve action. This dual representation prevents agents from being overwhelmed by long documents or lossy compressions: they work with focused summaries but never lose access to the complete evidence trail. EIDs also enable systematic traceability, as later modules can verify claims against their original sources. Verification. To ensure evidence-grounded reasoning, any proposed answer must be decomposed into atomic claims {c1, . . . , cm}, each linked to an EID. verification agent checks that (1) each claim is entailed by the source content, (2) the claims collectively support the proposed answer, and 5 Table 1: Comparison of 25 state-of-the-art models with ReAct-style tool use capabilities. Metrics cover Knowledge Discovery (Knowledge Sufficiency, Search Score), Generation Quality (Generation Score, Good Refusal F1, Knowledge Utilisation F1), Knowledge Degradation (Forget, Leadastray), and Pass@1. Bold values denote best results: higher is better for Knowledge Discovery, Generation Quality, and Pass@1, while lower indicates greater robustness for Knowledge Degradation. Provider Model Knowledge Discovery Knowledge Suff. (%) Search Score (%) GPT-5 OpenAI (2025a) GPT-5-Chat OpenAI (2025a) o3-Pro OpenAI (2025c) o3 OpenAI (2025c) o3-Mini OpenAI (2025c) o4-Mini OpenAI (2025d) GPT-OSS-120B OpenAI (2025b) Claude-Opus-4.1 Anthropic (2025) Claude-Opus-4-Think Anthropic (2025) Claude-Sonnet-4-Think Anthropic (2025) Gemini-2.5-Pro Google DeepMind (2025) Gemini-2.5-Flash-Think Google DeepMind (2025) Grok-4 xAI (2025) Qwen3-235B-Think Yang et al. (2025) Qwen3-30B-Think Yang et al. (2025) Tongyi-DeepResearch Tongyi DeepResearch Team (2025) Doubao-1.6-Think ByteDance Seed Team (2025) Doubao-1.6-Flash ByteDance Seed Team (2025) GLM-4.5-Inner Zhipu AI Team (2025) GLM-4.5-Air-Inner Zhipu AI Team (2025) OpenAI Anthropic Google xAI Alibaba ByteDance Zhipu AI Moonshot AI Kimi-K2-0711 Moonshot AI (2025) Kimi-K2-0905 Moonshot AI (2025) DeepSeek DeepSeek-R1 DeepSeek-AI et al. (2025) DeepSeek-V3.1-Think DeepSeek-AI et al. (2024) DeepSeek-V3.1 DeepSeek-AI et al. (2024) Our Team EvidenceLoop 79.00 58.00 71.00 70.00 48.50 68.00 16.00 74.00 68.00 66.50 65.50 59.00 74.00 72.50 56.50 53. 64.00 54.50 63.50 55.50 54.50 53.00 61.50 61.50 55.50 61.50 80.00 59.50 78.00 76.00 57.00 72.00 23. 76.50 73.50 73.50 73.00 64.50 77.50 72.00 59.00 57.50 68.50 57.50 67.50 60. 59.00 55.00 65.50 56.50 58.50 62.50 Generation Quality Knowledge Degradation Generation Good Refusal Knowledge Util. Forget Score (%) F1 (%) F1 (%) (%) 23.21 15.74 20.86 18.29 9.10 12.69 2.75 28.53 21.00 26.19 11.64 16. 34.71 11.15 7.25 4.20 19.24 20.00 22.19 12.31 9.72 13.17 10.57 13.62 16. 12.61 8.89 26.23 9.37 3.29 21.05 19.75 23.59 28.57 30.53 34.59 10.87 40.56 37.63 6.56 12.51 0. 42.03 53.95 34.79 26.39 16.36 28.79 18.81 27.97 36.49 17.98 49.58 28.05 49.40 48.97 16.48 17.56 10. 48.54 31.23 44.19 24.68 16.35 56.19 24.19 13.16 15.69 18.11 19.46 35.09 17. 19.31 20.89 15.55 16.34 22.23 23.79 17.72 47.41 21.83 24.29 46.39 27.94 100.00 27.03 43.38 45.11 44.27 57. 23.65 63.45 79.65 43.93 49.22 68.81 25.98 44.14 43.12 49.06 37.40 44.72 28. 41.46 Lead-astray (%) 32.91 31.90 25.35 24.29 42.27 59.56 0.00 31.08 32.35 21.80 35.11 35.59 27. 19.31 16.81 41.12 39.84 21.10 40.16 40.54 36.70 33.96 51.22 44.72 50.45 41. Pass@1 (%) 50.50 29.50 56.00 53.50 21.50 21.00 24.00 44.50 29.00 38.50 28.50 17.50 50. 21.50 12.50 18.50 16.00 13.50 33.50 19.00 23.50 24.00 20.00 17.00 24.50 25. (3) the answer directly addresses the question. Verification occurs during execution: rejected proposals return specific feedback, allowing solvers to repair reasoning within the remaining budget B, while accepted answers immediately terminate exploration. This mechanism prevents hallucinations from propagating, enforces tight evidence grounding, and improves efficiency by halting search as soon as verification succeeds."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate 25 state-of-the-art models with ReAct-style tool use, including those from OpenAI, Anthropic, Google, xAI, Alibaba, ByteDance, Zhipu AI, Moonshot AI, and High-Flyer. All models interleave reasoning, search, and observations in our controlled Wikipedia sandbox on WebDetective with 200 hint-free multi-hop questions (24 hops), under limits of 40 tool calls and 32K-token context. Unless noted, decoding uses temperature=0.6, top p=0.95. We report six metrics: (1) Knowledge Score, knowledge sufficiency; (2) Search Score, retrieval effectiveness; (3) Generation Score, weighted F1 of Good Refusal and Knowledge Utilization; (4) Good Refusal F1; (5) Knowledge Utilization F1; and (6) Pass@1. We additionally analyse Forget and Lead-astray to probe knowledge degradation (section 4.2.2). For our EvidenceLoop, we set breadth=3, iteration=3. Comprehensive results appear in table 1. 4.1 MAIN RESULTS Frontier models are far from saturating the task. Even the strongest systems reach only 50% Pass@1 on our benchmark: o3-Pro tops out at 56.0%, while GPT-5 and Grok-4 both achieve 50.5%; Claude-Opus-4.1 is at 44.5%, and many others fall well below 40%. This illustrates the challenging nature of our benchmark, WebDetective. Search, generation, and final accuracy are decoupled. High retrieval does not translate proportionally into better synthesis or Pass@1. For example, GPT-5 attains an 80.0% Search Score but only 23.21% Generation Score and 50.5% Pass@1; o3-Pro similarly has 78.0 Search but 20.86 Generation (56.0% Pass@1). Conversely, Grok-4 achieves the highest Generation Score (34.71) with 77.5 Search and 50.5% Pass@1, while Qwen3-235B-Thinking posts 72.0% Search yet just 11.15% 6 Table 2: Emergent model profiles from metric interplay analysis. Profile Metric Pattern Knowledge Refusal Utilization Powerful but Overconfident Well-Calibrated Elite Synthesis Bottleneck Conservative Middle Weak and Confused Self-Aware of Weakness Ideal (Unachieved) High High High Med Med Low High Low Med Low Med Low High High High High Low Med Low Low High Pass@ Example Models Failure Mode 50-56% 44-51% 18-22% 29-39% 20-22% 13-18% Doubao variants, Gemini-Flash Comprehensive inability (appropriate) Hallucination from overconfidence Minor: unnecessary caution Cannot compose multi-hop reasoning Under-utilizes capabilities Poor synthesis + poor calibration GPT-5, o3-Pro, o3 Grok-4, Claude-Opus-4.1 Qwen3-235B, Tongyi-DR Claude-Sonnet-4, GLM-4.5 o4-Mini, DeepSeek-R1 None None - optimal behavior Generation and 21.5% Pass@1. These gaps indicate that information synthesis, not just retrieval, is key bottleneck. Refusal ability is underdeveloped. Good-refusal performance is generally low: the best we observe is 53.95% F1 (Doubao-1.6-Flash). Many frontier models underperform markedlye.g., GPT5 (8.89%), o3-Pro (9.37%), and o4-Mini (19.75%)and even strong generalists like Claude-Opus4.1 remain modest (28.57%). This highlights weak calibrated abstention when evidence is insufficient. 4.2 ANALYSIS 4.2.1 UNDERSTANDING MODEL FAILURE MODES THROUGH METRIC PATTERNS To better understand the diverse failure modes in multi-hop reasoning, we analyze the interplay between our three core metrics: Knowledge Sufficiency (ability to gather evidence), Good Refusal F1 (calibration of uncertainty), and Knowledge Utilization F1 (synthesis capability). Rather than examining metrics in isolation, we investigate how their combinations reveal distinct behavioral profiles. We categorize performance using empirically-derived thresholds: Knowledge Sufficiency (High: > 70%, Medium: 60-70%, Low: < 60%), Good Refusal F1 (High: > 40%, Medium: 25-40%, Low: < 25%), and Knowledge Utilization F1 (High: > 45%, Medium: 25-45%, Low: < 25%). Analyzing all 23 models, we observe that they cluster into six distinct profiles based on these metric combinations, with certain theoretically plausible patterns notably absent from the empirical data. We present the taxonomy in table 2. The Powerful but Overconfident profile (GPT-5, o3-Pro, o3) achieves the highest pass rates (50-56%) through strong evidence gathering and synthesis, but exhibits dangerous overconfidence with refusal rates below 10% despite 21-30% knowledge insufficiency. These models prefer hallucination over admission of uncertainty. In contrast, the wellcalibrated Elite (Grok-4, Claude-Opus-4.1) achieves similar knowledge sufficiency and utilisation but maintains moderate refusal rates (29-38%), demonstrating that strong capabilities do not need to preclude epistemic awareness, although this calibration costs approximately 5-6% in pass rate. The Synthesis Bottleneck profile reveals critical failure mode: models like Qwen3-235BThinking achieve high knowledge sufficiency (72.5%) but catastrophically fail at synthesis (< 25% utilisation). Despite possessing evidence, they cannot compose multi-hop reasoning chains, yet their low refusal rates indicate unawareness of this limitation. The Conservative Middle models (Claude-Sonnet-4-Think, GLM-4.5-Inner) exhibit consistent mediocrity across all metrics, suggesting excessive cautiontheir moderate utilisation (31-44%) despite reasonable knowledge gathering (63-68%) indicates they refuse even when capable of answering. At the lower performance tiers, we observe striking divergence in self-awareness. Self-Aware of Weakness models (Doubao variants, Gemini-2.5-Flash-Think) appropriately refuse in 40-54% of cases, correctly recognising their limitations in both search and synthesis. Conversely, Weak and Confused models (o4-Mini, DeepSeek-R1) exhibit similar capability limitations but fail to recognise them, attempting answers despite 16-18% utilisation rates. Our analysis reveals three distinct failure modes in the multi-hop reasoning pipeline. Search failure affects 21-46% of attempts even in top models, indicating that evidence discovery remains chal7 lenging. Synthesis failure is more severeeven with sufficient knowledge, utilisation rates peak at 56%, suggesting that composing multi-hop reasoning chains remains fundamental bottleneck. Calibration failure manifests bidirectionally: top-performing models are systematically overconfident (refusing < 10% despite significant insufficiency), while weaker models may over-refuse or, worse, lack any calibration signal. Notably, no model in our evaluation achieves both high utilisation and high refusala perfectly calibrated model would excel at synthesis while maintaining appropriate uncertainty, but current architectures appear to force tradeoff where strong synthesis capability invariably leads to overconfidence. This suggests fundamental tension between competence and epistemic humility in existing architectures. The emergence of these distinct profiles suggests that improving multi-hop reasoning requires targeted interventions. Models in the Synthesis Bottleneck category need architectural improvements to reasoning composition, not better search. Overconfident models need calibration mechanisms that dont sacrifice performance. Most importantly, the absence of any model achieving high performance across all three metricseven Grok-4 and Claude-Opus-4.1, the best-balanced models, only reaches 50.5% and 44.5% pass ratedemonstrates that robust multi-hop reasoning remains an open challenge, with synthesis capability being the universal limiting factor. 4.2.2 KNOWLEDGE DEGRADATION IN SYNTHESIS Even when models achieve knowledge sufficiency (KS(d) = 1), they often fail to generate the correct answer. We call this knowledge degradation: evidence is present in context, yet models forget, ignore, or misuse it during synthesis. To analyse this effect, we focus on two diagnostics, Forget and Lead-astray, which reveal two distinct synthesis failures: models either fail to recall known knowledge (Forget) or become disrupted by noisy search contexts (Lead-astray). Knowledge degradation patterns. From table 1, models with lower Forget and Lead-astray generally exhibit higher Knowledge Utilization, which in turn coincides with higher Generation Score and Pass@1. For instance, Grok-4 (Forget 23.65%, Lead-astray 27.70%) attains the highest Knowledge Utilization F1 (56.19%), the highest Generation Score (34.71%), and 50.5% Pass@1. Similarly, o3-Pro (Forget 21.83%, Lead-astray 25.35%) reaches 49.40% Knowledge Utilization, 20.86% Generation Score, and the best Pass@1 (56.0%). GPT-5 shows comparable pattern with very low Forget (17.72%) and strong Knowledge Utilization (49.58%), alongside 23.21% Generation Score and 50.5% Pass@1. In contrast, when Forget is high, Knowledge Utilization collapses and downstream metrics follow suit: GPT-OSS-120B records the lowest Knowledge Utilization (10.73%) with Forget at 100.00% (Lead-astray 0.00%), yielding only 2.75% Generation Score and 24.0% Pass@1; Qwen3-30B-Thinking has Forget 79.65% (Lead-astray 16.81%), with 13.16% Knowledge Utilization, 7.25% Generation Score, and 12.5% Pass@1. Similar degradations appear for Gemini2.5-Flash-Think (Forget 57.63%, Knowledge Utilization 16.35%) and Tongyi-DeepResearch-30B (Forget 43.93%, Knowledge Utilization 15.69%). Forgetting dominates misdirection. Averaging across all models, the mean difference Forget Lead-astray is +10.35% points. This gap indicates that, on WebDetective, failures after achieving knowledge sufficiency are more often due to not using already-available evidence (forgetting during synthesis) than to being led astray by spurious context. In other words, the principal bottleneck lies in evidence integration at answer time rather than in resisting distractors. 4.2.3 ROBUSTNESS TO TEST-TIME SCALING To assess the robustness of our benchmark, we examine test-time scaling (TTS) along two axes. First, we scale context length for strong ReAct model (Claude-Opus-4.1) to test whether larger budgets improve performance. Second, we vary breadth and iteration counts in EvidenceLoop to probe whether extensive exploration can exploit WebDetective. These analyses test whether WebDetective can be artificially boosted by TTS or instead faithfully reflect underlying system capabilities. In fig. 2, we observe two main trends. For Claude-Opus-4.1, enlarging the context window from 8K to 32K tokens brings negligible gains: Generation Score plateaus at about 34%, Pass@1 at about 50%, and Search Score increases by less than 1%. For EvidenceLoop, expanding the controller from breadth=1, iteration=2 to breadth=3, iteration=2 raises Search Score slightly (45% 46%, 8 Figure 2: Scaling under test-time scaling (TTS). +1%), leaves Generation Score unchanged at 21%, and improves Pass@1 from 49% to 56% (+7%). These results indicate that our benchmark is robust to naıve test-time scaling. Neither larger context budgets nor shallow exploration suffice to hack WebDetective; achieving further gains requires genuine advances in model reasoning and knowledge utilisation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced WEBDETECTIVE, benchmark for evaluating web agents on hint-free multi-hop deep search within controlled Wikipedia sandbox. Unlike prior datasets that embed reasoning paths (PH) or entity fingerprints (SH), our design enforces autonomous discovery of reasoning chains while enabling fine-grained attribution of failure modes. Evaluation of 25 state-of-the-art models reveals consistent weaknesses: systems often retrieve sufficient evidence but fail to utilise it effectively, and appropriate refusals remain nearly absent. Our proposed agentic workflow EvidenceLoop demonstrates that explicit verification and systematic evidence tracking can partially close this gap, underscoring that performance cannot be trivially improved by test-time scaling alone."
        },
        {
            "title": "REFERENCES",
            "content": "Sierra AI. τ -bench: Benchmarking ai agents for the real-world, 2024. Anonymous. Meqa: benchmark for multi-hop event-centric question answering with explanations. In NeurIPS, 2024a. Anonymous. Theagentcompany: Benchmarking llm agents on consequential real world tasks. arXiv:2412.14161, 2024b. Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025. Accessed: 2025. ByteDance Seed Team. Doubao 1.6 thinking and flash models. https://seed.bytedance. com/, 2025. Part of the Doubao/Seed model family. Accessed: 2025. Jifan Chen, Shih-ting Lin, and Greg Durrett. Multi-hop question answering via reasoning chains. arXiv:1910.02610, 2019. DeepSeek-AI, Wenfeng Liang, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. DeepSeek-AI, Wenfeng Liang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents, 2025. URL https://arxiv.org/ abs/2506.11763. Abul Ehtesham et al. systematic review of key retrieval-augmented generation (rag) systems. arXiv:2507.18910, 2025. Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald. Trace the evidence: Constructing knowledgegrounded reasoning chains for retrieval-augmented generation. In EMNLP Findings, 2024. Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. From llm reasoning to autonomous ai agents: comprehensive review, 2025. URL https://arxiv.org/abs/ 2504.19678. Google DeepMind. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. https://storage.googleapis. com/deepmind-media/gemini/gemini_v2_5_report.pdf, March 2025. Accessed: 2025. Jie He et al. Mintqa: multi-hop question answering benchmark for evaluating llms on new and tail knowledge. arXiv:2412.17032, 2024. Xanh Ho et al. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. In COLING, 2020. Soyeong Jeong et al. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In NAACL, 2024. Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. s3: You dont need that much data to train search agent via rl, 2025. URL https: //arxiv.org/abs/2505.14146. Carlos Jimenez et al. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. Tom Kwiatkowski et al. Natural questions: benchmark for question answering research. TACL, 2019. 10 Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating superhuman reasoning for web agent, 2025a. URL https://arxiv.org/abs/2507.02592. Ronghan Li et al. Different paths to the same destination: Diversifying llms generation for multi-hop open-domain question answering. Knowledge-Based Systems, 309, 2024a. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025b. URL https://arxiv.org/abs/2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability, 2025c. URL https://arxiv.org/abs/2504.21776. Zijian Li et al. Graph neural network enhanced retrieval for question answering of llms. arXiv:2406.06572, 2024b. Hao Liu et al. Hoprag: Multi-hop reasoning for logic-aware retrieval-augmented generation. arXiv:2502.12442, 2025. Xiao Liu et al. Agentbench: Evaluating llms as agents. arXiv:2308.03688, 2023. Moonshot AI. Kimi k2: Open agentic intelligence technical report. https://github.com/ MoonshotAI/Kimi-K2/blob/main/tech_report.pdf, July 2025. Accessed: 2025. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Cheng Niu et al. Ragtruth: hallucination corpus for developing guardrails in rag systems. arXiv preprint, 2024. OpenAI. Introducing swe-bench verified, 2024. OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, August 2025a. Accessed: 2025. OpenAI. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025b. OpenAI. Openai o3 and o4-mini system card. https://cdn.openai.com/ o3-and-o4-mini-system-card.pdf, April 2025c. Accessed: 2025. OpenAI. Introducing openai o4-mini. https://openai.com/index/ introducing-o3-and-o4-mini/, April 2025d. Accessed: 2025. Fabio Petroni et al. Kilt: benchmark for knowledge intensive language tasks. In NAACL, 2021. Es Shahul et al. Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint, 2024. Aditi Singh et al. Agentic retrieval-augmented generation: survey on agentic rag. arXiv:2501.09136, 2025. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching, 2025. URL https://arxiv.org/abs/2505.04588. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025. URL https://arxiv.org/abs/2507.15061. 11 Tongyi DeepResearch Team. Tongyi deepresearch: The leading open-source deep research agent. https://github.com/Alibaba-NLP/DeepResearch, 2025. 30B-parameter agentic model for long-horizon research tasks. Harsh Trivedi et al. Musique: Multihop questions via single-hop question composition. TACL, 2022. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading comprehension across documents. In TACL, 2018. Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, and Ke Wang. Widesearch: Benchmarking agentic broad info-seeking, 2025. URL https://arxiv.org/abs/2508.07999. xAI. Grok 4: The most intelligent model in the world. https://x.ai/news/grok-4, July 2025. Accessed: 2025. An Yang, Binyuan Hui, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018a. URL https://arxiv.org/abs/1809.09600. Zhilin Yang et al. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018b. Zhipu AI Team. Glm-4.5: Reasoning, coding, and agentic abilities. August 2025. Technical report available at https://z.ai/blog/glm-4.5. Shuyan Zhou et al. Webarena: realistic web environment for building autonomous agents. arXiv:2307.13854, 2023. Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. Fanoutqa: multi-hop, multi-document question answering benchmark for large language models. In ACL, 2024. 12 17 18 18 19 20 20 21 21 21 22 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "Table of Contents",
            "content": "A Formal Metrics Definition Dataset Construction Dataset Statistics The EvidenceLoop framework D.1 Core Architecture: Iterative Refinement with Fallback . D.2 Evidence Memory System . . D.3 Verification: Ensuring Evidence-Grounded Reasoning . . . . . . . . . . . . . . . Failure Case Studies Related Work F.1 Multi-Hop Question Answering Benchmarks . F.2 Retrieval-Augmented Generation and Agents . . F.3 Evaluation Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A FORMAL METRICS DEFINITION",
            "content": "Traditional multi-hop QA evaluation reduces agent performance to single pass rate, obscuring the diverse failure modes that occur in complex reasoning tasks. An agent that searches exhaustively but fails to synthesise evidence exhibits fundamentally different limitations than one that refuses prematurely or hallucinates from parametric knowledge. Our co-designed sandbox, with its guaranteed unique reasoning paths, enables unprecedented diagnostic precision in distinguishing these failure modes. We introduce two-level evaluation framework that separates knowledge sufficiency from generation quality. First, we assess whether an agent possesses the requisite knowledgeeither through successful search or parametric memoryto answer the question. Second, conditioned on knowledge sufficiency, we evaluate the agents ability to either correctly synthesise an answer or appropriately refuse when evidence is insufficient. Knowledge Sufficiency Assessment: We assess whether an agent possesseseither through search or parametric knowledgeall information necessary to answer the question. Given the required evidence chain = {e1, ..., en}, we first identify which evidence the agent discovered through search by tracking visited pages in our sandbox. For any missing evidence ei / Ef ound, we then test whether the agent can access this information parametrically. Specifically, for each missing piece of evidence ei, we construct focused probe query pi that tests for that specific knowledge. For instance, if the agent never visited Kanes page and thus missed discovering that Kane Cornes has brother Chad Cornes, we probe with: Kane Cornes has brother . We define Probe(pi) as function that submits probe pi to the base model and returns whether the models response matches the expected answer for evidence ei. For instance with evidence chain of length nd, we define: kd = if ei Ef ound (found via search) 1 1 if ei / Ef ound Probe(pi) = correct 0 otherwise The instance is knowledge sufficient if: KS(d) = (cid:81)nd i=1 kd = 1 We define the overall Knowledge Score as the fraction of instances where the agent achieves knowledge sufficiency: KnowledgeScore = (1) This metric directly measures search effectivenessa low KnowledgeScore indicates the agent fails to discover necessary evidence through exploration, regardless of its ability to synthesize answers. Search Score: While our masking mechanism enforces the canonical reasoning path v0 v1 ... vn, we observe that models may leverage alternative valid reasoning strategies. Specifically, if an entity vx (reachable through search from v0) has meaningful relationship to the answer vn stored in the models parametric knowledge, the model can combine partial search with memory to reach the correct answer. This represents legitimate form of reasoning that demonstrates efficient use of both search and parametric knowledge. To capture this capability, we define SearchScore that credits models for finding correct answers through any valid combination of search and parametric knowledge, provided their search efficiency meets or exceeds the ground truth path: SearchScore = KnowledgeScore + (2) where = {d : correct(d) hops(d) hopsGT(d) KS(d) = 0} represents instances where the model: Produces the correct answer despite not having complete knowledge sufficiency through the canonical path 14 Uses no more search hops than the ground truth path length Effectively combines discovered entities with parametric knowledge This metric recognizes that effective multi-hop reasoning isnt solely about following predetermined paths, but about efficiently discovering and leveraging available informationwhether through complete evidence chains or intelligent combination of partial search with existing knowledge. The hop constraint ensures models arent simply performing exhaustive search, but are discovering meaningful connections that enable efficient reasoning. Search Score: While our masking mechanism enforces the canonical reasoning path v0 v1 ... vn, we observe that models may leverage alternative valid reasoning strategies. Specifically, if an entity vx (reachable through search from v0) has meaningful relationship to the answer vn stored in the models parametric knowledge, the model can combine partial search with memory to reach the correct answer. This represents legitimate form of reasoning that demonstrates efficient use of both search and parametric knowledge. To capture this capability, we define SearchScore that credits models for finding correct answers through any valid combination of search and parametric knowledge, provided their search efficiency meets or exceeds the ground truth path: SearchScore = KnowledgeScore + (3) where = {d : correct(d) searched(d) hops(d) hopsGT(d) KS(d) = 0} represents instances where the model: Produces the correct answer despite not having complete knowledge sufficiency through the canonical path Actually performs web search (not relying solely on parametric knowledge) Uses no more search hops than the ground truth path length Effectively combines discovered entities with parametric knowledge The requirement that searched(d) = true ensures we only reward genuine search-memory combination strategies, not pure parametric recall. This metric recognizes that effective multi-hop reasoning isnt solely about following predetermined paths, but about efficiently discovering and leveraging available information through intelligent combination of partial search with existing knowledge. The hop constraint ensures models arent simply performing exhaustive search, but are discovering meaningful connections that enable efficient reasoning. Generation Quality Assesement: Given the knowledge sufficiency assessment, we evaluate generation quality through conditional framework that captures the fundamental tension in multi-hop QA: agents must synthesize answers when they have sufficient evidence while appropriately refusing when they dont. Let = {d1, ..., dN } denote the evaluation dataset with instances. We partition along two dimensions: Knowledge dimension: = {d : KS(d) = 1} = (knowledge insufficient instances) (knowledge sufficient instances) Response dimension: = {d : agent attempts answer} = {d : agent refuses} (4) (5) (6) (7) where attempts are further partitioned into = Ac Aw, with Ac denoting correct answers (matching ground truth) and Aw denoting wrong answers. Note that = D. The intersection of these dimensions creates critical regions that reveal different agent capabilities and failure modes: 15 Knowledge sufficient, answers correctly (S Ac): The ideal scenariothe agent possesses all evidence and successfully synthesizes the correct answer. This demonstrates knowledge utilization, the ability to compose multi-hop reasoning without forgetting intermediate steps or being disrupted by irrelevant information. Knowledge sufficient, answers wrongly (S Aw): synthesis failuredespite having all necessary evidence, the agent produces an incorrect answer. This reveals breakdowns in reasoning composition, where evidence possession doesnt translate to correct synthesis. Knowledge sufficient, refuses (S ): Over-cautionthe agent has sufficient evidence but refuses to answer. This represents failure to recognize that the evidence chain is complete, missing opportunities to provide helpful answers. Knowledge insufficient, refuses (I ): The second ideal scenariogood refusal. The agent lacks critical evidence and appropriately declines to answer, demonstrating epistemic awareness and avoiding hallucination. Knowledge insufficient, attempts answer (I A): The most problematic behaviorthe agent lacks evidence but attempts an answer anyway (whether correct by luck or wrong), typically through hallucination, guessing, or over-reliance on partial information. This visualization reveals that generation quality isnt monolithican agent might excel at refusing when uncertain but fail to synthesize known information, or vice versa. For instance, an overly conservative agent might achieve perfect good refusal by refusing everything (large region), while an overly confident agent might attempt every question (large region) leading to frequent hallucinations in the zone. To capture these complementary capabilities, we define two core metrics: Good Refusal (GR) measures the agents ability to appropriately abstain when lacking evidence. It evaluates overlap with Ihigh recall indicates the agent successfully avoids hallucination by refusing most knowledge-insufficient cases, while high precision ensures refusals are justified (not bleeding unnecessarily into S). RecallGR = I , PrecisionGR = I , F1GR = 2 RecallGR PrecisionGR RecallGR + PrecisionGR (8) Knowledge Utilization (KU) assesses the agents ability to synthesize correct answers when evidence is available. It examines Ac within Shigh recall means the agent leverages available evidence effectively, while high precision indicates that attempts are typically grounded in sufficient knowledge. RecallKU = Ac S , PrecisionKU = Ac A , F1KU = 2 RecallKU PrecisionKU RecallKU + PrecisionKU (9) Importantly, these metrics are non-competingimproving one shouldnt decrease the other in well-designed system. An ideal agent achieves high F1GR (refusing when and only when knowledge is insufficient) while maintaining high F1KU (correctly answering when evidence is available). To capture both capabilities while preventing gaming, we define unified Generation Score: GenScore = F1GR + F1KU 2 (10) The S/N weighting (KnowledgeScore) is crucial for preventing metric exploitation: without it, models could game the evaluation by adopting degenerate strategyperforming minimal search and refusing nearly all questions. Such model would achieve high F1GR (correctly refusing the many knowledge-insufficient cases) while contributing nothing useful, yet still obtain substantial GenScore. This creates perverse incentive where models might optimize for conservative refusal rather than improving search capabilities. The weighting ensures that models cannot exploit the evaluation structurethey must demonstrate effective evidence discovery to achieve competitive scores, aligning the metric incentives with the actual goal of multi-hop reasoning systems. Unlike simple pass rates, our metrics provide actionable insights: low KnowledgeScore indicates inadequate search strategies, low GR scores reveal over-confident hallucination, and low KU scores 16 expose synthesis failures despite having evidence. This diagnostic precision, enabled by our codesigned evaluation environment, illuminates the specific capabilities required for robust multi-hop reasoning. Knowledge Forget Test. We test LM(q, Efound) where Efound = Evisited EGT represents evidence from ground-truth URLs that the model actually visited. When this fails despite KS(d) = 1, it reveals knowledge forget: the model cannot leverage its parametric knowledge to fill missing pieces when answering the full question, even though it correctly answers individual probes Probe(pi) for each missing evidence ei EGT Efound. Lead-astray Test. When LM(q, Efound) succeeds but the model fails in its actual search trajectory, we identify lead-astray: the model can synthesize the answer from clean evidence but is disrupted by the accumulated search context (failed attempts, irrelevant pages, exploration noise). Formally, for the set of knowledge-sufficient instances = {d : KS(d) = 1 incorrect(d)} where the model fails despite having all necessary knowledge: ForgetRate = {d : LM(qd, found) = d} LeadAstrayRate = {d : LM(qd, d actual output(d) = d} found) = S These metrics decompose knowledge-sufficient failures: ForgetRate identifies when models cannot integrate parametric knowledge with partial search results, while LeadAstrayRate reveals when noisy search trajectories corrupt otherwise successful reasoning."
        },
        {
            "title": "B DATASET CONSTRUCTION",
            "content": "To instantiate our hint-free multi-hop QA benchmark, we develop systematic pipeline that transforms single-hop Wikipedia QA pairs into verified multi-hop reasoning chains while ensuring each hop is necessary for answering. Source Data and Chain Discovery. We begin with corpus of Wikipedia-based QA pairs where each question targets specific paragraph on Wikipedia page (the starting entity v0) and has an answer that is another Wikipedia entity (vn). These seed questions are designed to be unambiguous and simple, avoiding any linguistic hints about reasoning paths. To construct multi-hop chains, we first block the direct connection between v0 and vn, then perform breadth-first search (BFS) to find the shortest alternative path v0 v1 ... vn through Wikipedias hyperlink graph. For each edge (vi, vi+1) in the discovered path, we extract the sentence ei from vis Wikipedia page that contains the hyperlink to vi+1, forming the evidence chain = {e1, e2, ..., en}. Verification of Reasoning Necessity. Not all discovered paths constitute valid answers to the questionmost arbitrary paths from v0 to vn through Wikipedias link graph are completely unrelated to what the question asks. For instance, path connecting two people through their universities and shared colleagues is irrelevant for question asking about family relationships. We implement three-stage verification process using strong language model (Qwen-3-235B in our implementation), denoted as LM() which takes text input and generates an answer: 1. Parametric Inaccessibility: We verify that LM(q) = vn, ensuring the answer cannot be directly retrieved from the models parametric memory without evidence. 2. Evidence Sufficiency: We confirm that LM(q, E) = vn, validating that the complete evidence chain enables correct answer generation. 3. Evidence Necessity: For each evidence piece ei, we verify that LM(q, {ei}) = vn, ensuring every hop in the chain is required for reasoning. This ablation test eliminates questions where evidence pieces are redundant or where shortcuts exist. Human Validation and Dataset Statistics. Questions passing automated verification undergo human annotation by 2 researchers with NLP expertise. Each question is independently reviewed following structured protocol: 17 1. Annotation Protocol: For each question, annotators receive the question q, evidence chain = {e1, ..., en}, and answer vn. They verify three criteria: Necessity: Whether the question can be answered without the evidence chain using only general knowledge Sufficiency: Whether the evidence chain logically derives the answer without requiring external information No hints: Whether the question avoids linguistic cues that reveal intermediate reasoning steps 2. Validation Process: Each question requires 2-3 minutes of review. Annotators trace through the reasoning chain step-by-step, attempting to answer the question both with and without the evidence to ensure all pieces are necessary. Questions where intermediate entities could be guessed from the question phrasing or where the evidence chain has logical gaps are rejected. 3. Dataset Filtering: Of approximately 450 machine-verified questions reviewed, 200 questions (44%) pass human validation. Common rejection reasons include: evidence chains not targeting the questions, evidence chains with missing logical connections, and questions containing subtle hints about the reasoning path (e.g., mentioning attributes that implicitly identify intermediate entities). This manual verification process, totaling approximately 20 hours of annotation effort, ensures our final dataset contains only questions that genuinely require discovering and composing the complete multi-hop reasoning chain."
        },
        {
            "title": "C DATASET STATISTICS",
            "content": "Our final WebDetective benchmark comprises 200 hint-free multi-hop questions, carefully curated through our verification pipeline. Figure 3 presents comprehensive analysis of the dataset characteristics. Question Complexity. The dataset exhibits controlled complexity suitable for diagnostic evaluation. Questions require 2 to 6 hops of reasoning (mean: 2.85 hops), with the distribution heavily weighted toward 2-hop (55%) and 3-hop (31%) questions, while maintaining challenging subset of 4+ hop questions (14%). This distribution balances tractability with sufficient complexity to stresstest multi-hop reasoning capabilities. Each question involves 3 to 8 Wikipedia entities (mean: 3.73), with the modal question requiring exactly 3 entities to form the complete reasoning chain. Question Types and Domains. The dataset spans diverse question types, with What questions comprising 34% of the dataset, Who questions 27%, and Which questions 13.5%, ensuring broad coverage of information-seeking patterns. Questions are concise (mean: 71.4 characters) with typically short answers (mean: 28.6 characters), reflecting natural information needs without verbose specifications that might hint at reasoning paths. Evidence Requirements. The evidence distribution aligns with hop counts, with most questions requiring 2-3 pieces of evidence (52.5% and 31% respectively). This controlled evidence requirement enables precise diagnosis of where reasoning failswhether at initial discovery, intermediate steps, or final synthesis. The datasets careful balance of complexity, diversity, and diagnostic precision makes it suitable for evaluating the full spectrum of multi-hop reasoning capabilities, from basic 2-hop familial relationships to complex 5-hop chains requiring sustained context retention across multiple search iterations."
        },
        {
            "title": "D THE EVIDENCELOOP FRAMEWORK",
            "content": "The hint-free nature of our benchmark exposes fundamental limitations in current multi-hop reasoning approaches. Without linguistic scaffolding, agents must autonomously discover which connections matter among thousands of factsa challenge that, as our results show, causes even state-ofthe-art models to achieve only 50% accuracy. To better understand these challenges and establish 18 Figure 3: Dataset statistics for WebDetective benchmark. The figure shows: (a) Distribution of question types, (b) Number of entities per question, (c) Evidence count distribution, (d) Question and answer length in characters, (e) Hop length distribution by analysis setting, and (f) Search query usage patterns. The dataset exhibits controlled complexity with predominantly 2-3 hop questions while maintaining challenging longer chains. baseline for future work, we design an agentic workflow that explicitly targets the unique difficulties our benchmark reveals: the need for broad exploration without context explosion, evidence retention across long trajectories, and synthesis from accumulated but noisy search contexts. D.1 CORE ARCHITECTURE: ITERATIVE REFINEMENT WITH FALLBACK Our framework attempts to balance exploration breadth with computational feasibility through Rmax iterations. Illustrated in Figure 4, each iteration launches parallel solver agents {Ar } that explore different reasoning paths simultaneously. Each agent Ar receives the question q, an aggregated context from previous iterations (with 0 = initially), and executes up to actions. 1, ..., Ar After each iteration, we employ two-stage refinement process: 1. An extraction agent processes the reasoning contexts from all parallel agents to identify key findings, evidence references, and promising paths 2. An aggregation agent synthesizes these extracted insights into refined context r+1 for the next round, preserving valuable discoveries while discarding exploration noise This iterative refinement addresses core challenge our benchmark exposes: early rounds might explore many directionssports connections, geographic locations, family relationsbut the extraction-aggregation pipeline identifies which paths warrant deeper exploration, preventing the context explosion that causes single-pass approaches to fail while avoiding premature path com19 mitment. If no conclusive answer emerges after Rmax iterations, final aggregation agent consolidates all discovered evidence into comprehensive context final. This context is then passed to synthesis-only solver that attempts to derive the answer purely from the accumulated evidence without additional search actionseffectively testing whether the failure stems from insufficient exploration or poor evidence composition. Figure 4: Overview of the EvidenceLoop framework. The system employs parallel solver and extractor agents that perform search and reasoning to generate proposals with supporting claims. These proposals are then verified through memory retrieval, with the aggregated context fed into subsequent iterations until verification succeeds or termination condition is met. D.2 EVIDENCE MEMORY SYSTEM Enabling this iterative refinement is our Evidence Memory System M. When any agent performs search or visits page, the system: 1) Stores complete results in persistent memory; 2) Assigns unique Evidence IDs (EIDs) for reference; and 3) Returns both full content and EID to the agent. The EID system serves multiple critical functions in our framework. First, during extraction and aggregation between iterations, the extraction and aggregation agent produces summaries that preserve EID references alongside extracted factsfor example, Kane has brother Chad [EID-042], Chads stepmother is Nicole [EID-089]. This allows subsequent solver agents to receive concise, actionable summaries while retaining the ability to retrieve full evidence on demand through the retrieve action as an external tool, which takes an EID and returns the complete original content from memory. Second, these EIDs enable systematic verification (detailed in Section D.3), where verification agents can trace claims back to their original sources and validate reasoning chains against the actual evidence. The memory system transforms how evidence flows through iterations. Rather than forcing agents to work with either overwhelming full documents or lossy compressions, agents can work with focused summaries while maintaining access to complete evidence through EID-based retrieval. This design ensures that even as contexts become more refined across iterations, agents never lose access to the complete evidence trail that supports their reasoning, allowing them to dive deep into specific evidence when needed for detailed analysis or verification. D.3 VERIFICATION: ENSURING EVIDENCE-GROUNDED REASONING The verification mechanism prevents premature or hallucinated answers from propagating through our system. When any solver agent Ar proposes an answer, it must decompose the answer into atomic claims {c1, c2, ..., cm}, where each claim cj is explicitly linked to an EID from the memory systeme.g., Kane has brother Chad [EID-042]. No unsupported claims are permitted. 20 The verification agent evaluates each proposal: (q, answer, {cj, EIDj}m j=1) {YES, NO(feedback)} For each claim-evidence pair, the verifier retrieves the full content from via the EID and validates: (1) whether the source genuinely entails the claimed fact, (2) whether the claims collectively derive the answer, and (3) whether the answer correctly addresses the original question. Verification occurs during solver execution. Rejections provide specific feedback back to the solver, allowing immediate gap-filling within the remaining action budget B, while acceptance immediately terminates all iterations. This ensures both evidence grounding and computational efficiencysolvers can correct incomplete reasoning in real-time while avoiding unnecessary exploration once the answer is verified."
        },
        {
            "title": "E FAILURE CASE STUDIES",
            "content": "We identify four recurring failure patterns through qualitative analysis: Premature Search Termination: After small amount of failed searches, models enter learned helplessness state where they immediately conclude the answer is unavailable and refuse further exploration. Even explicit prompting like please continue searching fails to restart the search process, with models insisting no more information exists in the sources. This occurs despite obvious next steps being availablefor instance, finding Chad as Kanes brother but refusing to visit Chads page directly, instead declaring the search futile. Context-Induced Instruction Degradation: As search context accumulates, models progressively lose their ability to follow basic instructions. In shorter context, they proper use proper <answer> tags and structured reasoning, but gradually degrade to dropping tags intermittently, then completely abandoning format requirements, producing streamof-consciousness text without capitalization or proper syntax. Evidence Tracking Failure: Models lose track of discoveries across search iterations, repeatedly searching for entities theyve already found or failing to maintain entity relationships discovered earlier. They cannot distinguish between not found due to masking versus havent searched yet, leading to redundant searches or premature abandonment of viable paths. The accumulated context becomes noise rather than useful evidence. Redundant Search Loops: Models frequently re-visit or re-search pages theyve already explored, especially after intermediate reasoning steps. For example, after visiting Kanes page, finding Chad, visiting Chads page, then spending time reasoning about the relationships, the model might search for Kane Cornes again or revisit Kanes Wikipedia page, essentially restarting from scratch. While not technically incorrect, this pattern wastes action budget and rapidly fills context with duplicate information, accelerating context degradation and reducing the effective search depth the model can achieve before hitting token limits or action budgets."
        },
        {
            "title": "F RELATED WORK",
            "content": "F.1 MULTI-HOP QUESTION ANSWERING BENCHMARKS Multi-hop QA benchmarks evaluate models ability to compose information across multiple reasoning steps. Early datasets like HotpotQA Yang et al. (2018b) and WikiHop Welbl et al. (2018) established foundational evaluation frameworks but suffer from systematic biases. Recent benchmarks have expanded coverage: FanOutQA Zhu et al. (2024) addresses multi-document reasoning, MINTQA He et al. (2024) targets long-tail knowledge with 28K+ questions, and MEQA Anonymous (2024a) focuses on event-centric reasoning chains. However, these benchmarks embed hints that fundamentally alter the reasoning task. We identify two categories of hints prevalent in existing benchmarks. Path-hinting occurs when questions linguistically encode reasoning chains (e.g., What dance academy did the starring actress 21 from The Glory of Tang Dynasty graduate from?), reducing the task to executing pre-specified steps. Specification-hinting provides excessive constraints that make answers discoverable through constraint satisfaction rather than reasoning (e.g., combining East German team, founded 1966, player born in 90s). Unlike MuSiQue Trivedi et al. (2022) or 2WikiMultiHopQA Ho et al. (2020), which contain implicit structural hints, WebDetective introduces genuinely hint-free questions requiring autonomous reasoning path discovery. F.2 RETRIEVAL-AUGMENTED GENERATION AND AGENTS The evolution from static RAG pipelines to agentic architectures represents fundamental shift in how LLMs interact with external knowledge Singh et al. (2025); Ehtesham et al. (2025). While traditional RAG systems like TRACE Fang et al. (2024) achieve improvements through knowledgegrounded reasoning chains, they operate within predetermined patterns. Agentic RAG systems employ adaptive strategies: Adaptive-RAG Jeong et al. (2024) adjusts retrieval depth based on question complexity, while graph-based approaches like GNN-Ret Li et al. (2024b) and HopRAG Liu et al. (2025) leverage graph neural networks for multi-hop reasoning, achieving 10% accuracy improvements on benchmarks like 2WikiMQA. Recent advances in 2025 emphasize diverse reasoning paths. DP-CoT Li et al. (2024a) addresses single-path limitations through passage-level and sentence-level evidence generation. However, our evaluation reveals these advances fail to overcome hint-free challenges: median Generation Scores of 20% across tested models indicate current architectures cannot effectively discover reasoning chains without linguistic scaffolding. F.3 EVALUATION FRAMEWORKS Traditional metrics like exact match and F1 scores collapse diverse failure modes into single values, obscuring why models fail Kwiatkowski et al. (2019); Petroni et al. (2021). Recent frameworks attempt more nuanced evaluation: RAGAS Shahul et al. (2024) provides reference-free RAG metrics, while RAGTruth Niu et al. (2024) enables hallucination analysis. For agents, AgentBench Liu et al. (2023) evaluates across eight environments, tau-bench AI (2024) addresses multi-turn interactions, and TheAgentCompany Anonymous (2024b) introduces workplace tasks with simulated colleagues. Web-based benchmarks have evolved significantly. WebArena Zhou et al. (2023) provides realistic web environments requiring long-horizon planning but lacks controlled evaluation for precise failure attribution. SWE-bench Jimenez et al. (2024) evaluates code generation on GitHub issues, with SWE-bench Verified OpenAI (2024) addressing underspecified problems. While these benchmarks test complex capabilities, they dont address the specific challenge of verifying multi-hop reasoning paths. Our diagnostic framework decomposes evaluation into knowledge sufficiency (whether agents possess required evidence) and conditional generation quality (synthesis given sufficient knowledge). This separation reveals that models achieve 79% knowledge sufficiency but only 23% generation scores, indicating synthesis and relevance determinationnot searchas primary bottlenecks."
        }
    ],
    "affiliations": [
        "Nanyang Technological University (NTU)",
        "Singapore University of Technology and Design (SUTD)",
        "Tongyi Lab, Alibaba Group"
    ]
}