{
    "paper_title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
    "authors": [
        "Viacheslav Surkov",
        "Chris Wendler",
        "Mikhail Terekhov",
        "Justin Deschenaux",
        "Robert West",
        "Caglar Gulcehre"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain. Code is available at https://github.com/surkovv/sdxl-unbox"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 6 6 3 2 2 . 0 1 4 2 : r Preprint UNPACKING SDXL TURBO: INTERPRETING TEXT-TOIMAGE MODELS WITH SPARSE AUTOENCODERS Viacheslav Surkov Chris Wendler Mikhail Terekhov Justin Deschenaux Robert West Caglar Gulcehre School of Computer and Communication Sciences EPFL Lausanne, Switzerland"
        },
        {
            "title": "ABSTRACT",
            "content": "Sparse autoencoders (SAEs) have become core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbos denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain. Code is available at https://github.com/surkovv/sdxl-unbox."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-image generation is rapidly evolving field. The DALL-E model first captured public interest (Ramesh et al., 2021), combining learned visual vocabularies with sequence modeling to produce high-quality images based on user input prompts. Todays best text-to-image models are largely based on text-conditioned diffusion models (Rombach et al., 2022; Saharia et al., 2022b; Podell et al., 2023; Sauer et al., 2023b; Betker et al., 2023; Pernias et al., 2023). This can be partially attributed to the stable training dynamics of diffusion models, which makes them easier to scale than previous approaches such as generative adversarial neural networks (Dhariwal & Nichol, 2021). As result, they can be trained on internet scale image-text datasets like LAION-5B (Schuhmann et al., 2022a) and learn to generate photorealistic images from text. However, the underlying logic of the neural networks that enable the text-to-image pipelines we have today, due to their black-box nature, is not well understood. Unfortunately, this lack of interpretability is typical in the deep learning field. For example, advances in image recognition (Krizhevsky et al., 2012) and language modeling (Devlin, 2018; Brown, 2020) come mainly from scaling models (Hoffmann et al., 2022), rather than from an improved understanding of their internals. Recently, the emerging field of mechanistic interpretability has sought to alleviate this limitation by reverse engineering visual models (Olah et al., 2020) and transformer-based LLMs (Rai et al., 2024). At the same time, diffusion models have remained under-explored. Correspondence to viacheslav.surkov@epfl.ch. 1The code base contains scripts to collect training data and train SAEs. Additionally, it allows to browse pre-trained SAE features with demo application. 1 Preprint This work focuses on SDXL Turbo, recent open-source few-step text-to-image diffusion model. We import from toolbox originally developed for language models, which allows inspection of the intermediate results of the forward pass (Chen et al., 2024; Ghandeharioun et al., 2024; Cunningham et al., 2023; Bricken et al., 2023). Moreover, some even enable reverse engineering of the entire taskspecific subnets (Marks et al., 2024). In particular, sparse autoencoders (SAEs) (Yun et al., 2021; Cunningham et al., 2023; Bricken et al., 2023) are considered breakthrough in interpretability for LLMs. They have been shown to decompose intermediate representations of the LLM forward pass often difficult to interpret due to polysemanticity2 into sparse sums of interpretable and monosemantic features. These features are learned in an unsupervised way, can be automatically annotated using LLMs (Caden et al., 2024), and facilitate subsequent analysis, for example, circuit extraction (Marks et al., 2024). Contributions. In this work, we investigate whether we can use SAEs to draw insights about the computation performed by the one-step generation process of SDXL Turbo, which is recent opensource few-step text-to-image diffusion model. To facilitate our analysis, we developed library called SDLens that allows us to cache and manipulate intermediate results of SDXL Turbos forward pass. We use our library to create dataset of SDXL Turbos intermediate feature maps of several transformer blocks inside SDXL Turbos U-net on 1.5M LAION-COCO prompts (Schuhmann et al., 2022a;b). We then use these feature maps to train multiple SAEs for each transformer block. Finally, we perform quantitative and qualitative analysis of the SAEs learned features: 1. We empirically show the potential of SAEs to learn highly interpretable features in diffusion-based text-to-image models. 2. We develop visualization techniques to analyze the interpretability and causal effects of the learned features. 3. We perform two case studies in which we visualize and interpret the active features in different transformer blocks, finding evidence that certain transformer blocks of SDXL Turbos pipeline specialize in image composition, adding details, and style.3 4. We follow up our qualitative case studies by designing multiple quantitative experiments showing that our hypotheses also hold up on larger sample sizes. 5. As part of our quantitative analysis, we create an automatic feature annotation pipeline for the transformer block, which appears responsible for image compositions. Thus, we show that SAEs learn interpretable features that causally affect SDXL Turbos image generation process. Importantly, the learned features provide insight into the computational details of SDXL Turbos forward pass, such as the different roles of the investigated transformer blocks. By open-sourcing our library and SAEs, we lay the foundation for further research in this area. Note on visualizations. Qualitative analysis by inspection of generated images is crucial for this type of research. Many essential visualizations are included in App. and the supplementary material4 to improve the readability of the main text."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 SPARSE AUTOENCODERS Let h(x) Rd be an intermediate result during forward pass of neural network on input x. In fully connected neural network, h(x) could correspond to vector of neuron activations. In transformers, which are neural network architectures that combine attentions with fully connected layers and residual connections, h(x) could either refer to the content of the residual stream after layer, an update to the residual stream by layer, or vector of neuron activations within fully connected block. 2A phenomenon where single neuron or feature encodes multiple, unrelated concepts (Elhage et al., 2022) 3The blocks down.2.1 and up.0.1 have been already known in the community as composition and style blocks (Spinelli, 2024). However, in this paper we provide the first thorough and fine-grained investigation of these blocks. 4The supplementary material is hosted here https://drive.google.com/file/d/ 1MMvQyj5AcQOBhbW0P3zLFldYSDnMZ71d/view?usp=sharing. 2 Preprint It has been shown (Yun et al., 2021; Cunningham et al., 2023; Bricken et al., 2023) that in many neural networks, especially LLMs, intermediate representations can be well approximated by sparse sums of nf learned feature vectors, i.e., h(x) nf (cid:88) ρ=1 sρ(x)fρ, (1) where sρ(x) are the input-dependent coefficients, most of which are equal to zero and f1, . . . , fnf Rd is learned dictionary of feature vectors. Importantly, the features are usually interpretable. Sparse autoencoders. To implement the sparse decomposition from equation 1, the vector containing the nf coefficients of the sparse sum, is parameterized by single linear layer followed by ReLU activations, called the encoder, = ENC(h) = σ(W ENC(h bpre) + bact), (2) in which Rd is the latent that we aim to decompose, σ() = max(0, ), ENC Rnf is learnable weight matrix and bpre and bact are learnable bias terms. We omitted the dependencies = h(x) and = s(h), which are clear from the context. Similarly, the learnable features are parametrized by single linear layer called decoder, = DEC(s) = DECs + bpre, (3) in which DEC = (f1 fnf ) Rdnf is learnable matrix. Its columns take the role of learnable features and bpre is learnable bias term. 5 2.2 FEW STEP DIFFUSION MODELS: SDXL TURBO Diffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022a) sample from an unknown distribution by learning to iteratively denoise corrupted samples, starting from pure noise. The corruption process is defined on training samples from p. Mathematically, the images are corrupted with Gaussian noise and are distributed according to qt(xtx0) := (αtx0, σ I), (4) where x0 corresponds to real image from p, 0 , αt, σ2 such that the signal-to-noise ratio SN := αt σ2 matrix. Additionally, the coefficients αT 1, σ2 1 are typically chosen such that xT (0, I). are positive real-valued scalars is monotonically decreasing, and, is the identity The denoising process is implemented through learned distribution pθ(xt1xt). The simplest way to generate samples using pθ(xt1xt) is to first generate sample of pure noise xT (0, I), followed by iterative applications of pθ, which yields sequence xT , xT 1, ..., x1, x0, where x0 approximates sample from p. The vector θ represents the parameters of the neural network that defines pθ(xt1xt). The denoising distribution pθ(xt1xt) is parameterized to be Gaussian and neural network is trained to optimize the parameters of this distribution. Latent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song & Ermon, 2020). However, training denoising network in pixel space is computationally expensive (Hoogeboom et al., 2023). Thus, Rombach et al. (2022) use pre-trained variational autoencoder to first compress images into latent representations and define diffusion process in the latent space of the variational autoencoder instead. To make this difference clear, they write pθ(zt1zt), in which now zt refers to noisy latent instead of noisy image. SDXL Turbo. To speed-up inference of latent diffusion models, Sauer et al. (2023b) distill pretrained model called Stable Diffusion XL (SDXL) (Podell et al., 2023). The distilled model is referred to as SDXL Turbo because it allows high-quality sampling in as little as 1-4 steps. In comparison, the original SDXL model is trained with noise schedule of 1000 steps, but in practice, sampling with 20 to 50 steps still generates high-quality images. 5An extended version of this section, including training details, is in App. F. 3 Preprint Neural network architecture. The denoising network of SDXL Turbo estimating pθ(zt1zt) is implemented using U-net similar to Rombach et al. (2022). The U-net is composed of downsampling path, bottleneck, and an up-sampling path. Both the down-sampling and up-sampling paths are composed of 3 individual blocks. The individual block structure differs slightly, but both downand up-sampling blocks consist of residual layers, with some blocks including cross-attention transformer layers while others do not. Finally, the bottleneck layer is also composed of attention and residual layers. Importantly, the text conditioning is achieved via cross-attention to text embeddings performed by 11 transformer blocks embedded in the down-, up-sampling path, and bottleneck. An architecture diagram displaying the relevant blocks can be found in App. Fig. 4."
        },
        {
            "title": "3 SPARSE AUTOENCODERS FOR SDXL TURBO",
            "content": "With the necessary definitions at hand, in this section we show way to apply SAEs to SDXL Turbo. In the following, we assume that all SDXL Turbo generations are done using 1-step process. Where to apply the SAEs. We apply SAEs to updates performed within the cross-attention transformer blocks responsible for incorporating the text prompt (depicted in App. Fig. 4). Each of these blocks consists of multiple transformer layers, which attend to all spatial locations (selfattention) and to the text prompt embeddings (cross-attention). Formally, the ℓth cross-attention transformer block updates its inputs in the following way D[ℓ]out ij = D[ℓ]in ij + TRANSFORMER[ℓ](D[ℓ]in, c)ij, (5) in which D[ℓ]in, D[ℓ]out Rhwd denote the residual stream before and after application of the ℓth cross-attention transformer block respectively. The transformer block itself calculates the function TRANSFORMER[ℓ] : Rhwd Rhwd. Note that we omitted the dependence on input noise zt and text embedding for both D[ℓ]in(zt, c) and D[ℓ]out(zt, c). We train SAEs on the residual updates TRANSFORMER[ℓ](D[ℓ]in, c)ij Rd denoted by D[ℓ]ij := TRANSFORMER[ℓ](D[ℓ]in, c)ij = D[ℓ]out ij D[ℓ]in ij . (6) That is, we jointly train one encoder ENC[ℓ] and decoder DEC[ℓ] pair per transformer block ℓ and share it over all spatial locations i, j. For notational convenience we omit block indices from now. We do this for the 4 (out of 11) transformer blocks (App. Fig. 4) that we found have the highest impact on the generation (see App. A), namely, down.2.1, mid.0, up.0.0 and up.0.1. Feature maps. We refer to Rhwd as dense feature map and applying ENC to all image locations results in the sparse feature map Rhwnf with entries Sij = ENC(Dij). (7) We refer to the feature map of the ρth learned feature using Sρ Rhw. This feature map Sρ contains the spatial activations of the ρth learned feature. Its associated feature vector fρ Rd is column in the decoder matrix DEC = (f1 fnf ) Rdnf . Using this notation, we can represent each element of the dense feature map as sparse sum Dij nf (cid:88) ρ= ijfρ, with Sρ Sρ ij = 0 for most ρ {1, . . . , nf }. (8) Training. In order to train an SAE for transformer block, we collected dense feature maps Dij from SDXL Turbo one-step generations on 1.5M prompts from the LAION-COCO (Schuhmann et al., 2022b). Each feature map has dimensions of 16 16, resulting in training dataset of 384M dense feature vectors per transformer block. For the SAE training process, we followed the methodology described in (Gao et al., 2024), using the TopK activation function and an auxiliary loss to handle dead features. For more details on the SAE training, see App. and for training metrics see App. B. 4 Preprint"
        },
        {
            "title": "4 QUALITATIVE ANALYSIS OF THE TRANSFORMER BLOCKS",
            "content": "We perform visual qualitative analysis to gain deeper insight into the behavior and characteristics of the learned features across transformer blocks. First, we introduce feature visualization techniques and then use them to conduct two case studies. For the sake of simplicity in the notation, we omit the transformer block index ℓ. 4.1 FEATURE VISUALIZATION TECHNIQUES We start by introducing necessary notation and formally describing the methods used for feature visualization. Spatial activations. We visualize sparse feature map Sρ Rhw containing activations of feature ρ across the spatial locations by up-scaling it to the size of the generated images and overlaying it as heatmap over the generated images. In the heatmap, red indicates the highest activation of features, and blue represents the lowest non-zero activation. Top dataset examples. For given feature ρ, we sort dataset examples according to their average spatial activation aρ = 1 wh (cid:88) (cid:88) i=1 j=1 Sρ ij R. (9) We use equation 9 to define the top dataset examples and to sample from the top 5% quantile of the activating examples (aρ > 0). We will refer to them as top 5% images for feature ρ. Note that Sρ ij always depends on an embedding of the input prompt and input noise z1, via Sij(c, z1) = ENC(Dij(c, z1)), which we usually omit for ease of notation. As result, aρ also depends on and z1. When we refer to the top dataset examples, we mean our (c, z1) pairs with the largest values for aρ(c, z1). Activation modulation. We design interventions that allow us to modulate the strength of the ρth feature. Specifically, we achieve this by adding or subtracting multiple of the feature ρ on all of the spatial locations i, proportional to its original activation Sρ ij ij = Dij + βSρ ijfρ, (10) in which Dij is the update performed by the transformer block before and ij after the intervention, β is modulation factor, and fρ is the ρth learned feature vector. In the following, we will refer to this intervention as activation modulation intervention. Activation on empty context. Another way of visualizing the causal effect of features is to activate them while doing forward pass on the empty prompt c(). To do so, we turn off all other features at the transformer block ℓ of intervention and turn on the target feature ρ. Formally, we modify the forward pass by setting Dout ij = Din ij + γkµρfρ, (11) ij replaces residual stream plus transformer block update, Din in which Dout ij is the input to the block, fρ is the ρth learned feature vector, γ is hyperparameter to adjust the intervention strength, and µρ is feature-dependent multiplier obtained by taking the average activation across positive activations of ρ (collected over subset of 50.000 dataset examples). Multiplying it by aims to recover the coefficients lost by setting the other features to zero. Further in the text, we will refer to this intervention as empty-prompt intervention, and the images generated using this method with γ set to 1, as empty-prompt intervention images. Note that we directly added/subtracted feature vectors to the dense vectors for both intervention types instead of encoding, manipulating sparse features, and decoding. This approach helps mitigate side effects caused due to reconstruction loss (see App. B). 5 Preprint (a) Top 5 features of down.2.1 (b) Top 5 features up.0.1 (c) Top 5 features of up.0.0 (d) Top 5 features of mid.0 Figure 1: The top 5 features of down.2.1 (a), up.0.1 (b), up.0.0 (c) and mid.0 (d) for the prompt: cinematic shot of professor sloth wearing tuxedo at BBQ party. Each row represents feature. The first column depicts feature heatmap (highest activation red and lowest nonzero one blue). The columns with titles containing show feature modulation interventions, those containing the intervention of turning on the feature on the empty prompt, and the ones containing depict top dataset examples. Floating point values in the title denote β and γ values. 4.2 CASE STUDY I: MOST ACTIVE FEATURES ON PROMPT Combining the feature visualization techniques in Fig. 1, we depict the features with the highest average activation when processing the prompt: cinematic shot of professor sloth wearing tuxedo at BBQ party. We present analysis of the transformer blocks in order of decreasing interpretability. An extended version of this case study demonstrating top 9 features per transfromer block instead of 5, is available in App. Fig. 5. Down.2.1. The down.2.1 transformer block appears to contribute to the image composition. Several features relate to the prompt: 4539 sloth, 4751 tuxedo, 2881 party. Activation modulation interventions with negative β (A. -6.0 columns) result in removing or changing scene objects in ways that align with the heatmap (hmap column) and the top examples (C columns): 1674 removes the light chains in the back, 4608 the umbrellas/tents, 4539 the 3D animation-like sloth face, and, 4751 changes the type of suit. Similarly, enhancing the same features (A. 6.0 column) makes the corresponding elements more visible and distinct. Notably, activating the features on the empty prompt often creates meaningful images with related elements (B. column). For reference, with the fixed random seed we use, the empty prompt generation without interventions resembles painting of piece of nature with lot of green and brown tones. 6 Preprint While top dataset examples (C.0, C.1) and empty prompt intervention (B.) mostly agree with the feature activation heatmaps (hmap column), some of them provide additional insights, e.g., 2881, which activates on the suit, seems to correspond to (masqueraded) characters in (festive) scene. Up.0.1. Based, on our observations, the features of up.0.1 appear to contribute to the style. Interestingly, turning on the up.0.1 features on the empty prompt (B. column) results in texturelike images. Furthermore, when activating them locally (A. columns), their contribution to the output is highly localized, and most of inactive image area remains unchanged. For the up.0.1 we find it remarkable that often the features ablations and amplifications are counterparts: 500 (light, shadow), 2727 (shadow, light), 3936 (blue, orange). Up.0.0. For the third transformer block, up.0.0, we observe that most top dataset examples and their activations (C columns) are quite interpretable: 3603 corresponds to party decorations, 5005 to the upper part of tent, 775 to buttons on suit, 153 to the lower animal jaw, 1550 to collars. All the features exhibit an expected causal effect on the generation when ablated or enhanced (A. columns). The activation regions of the features often are very concentrated. Similarly to up.0.1, activation modulation interventions leave inactive image regions mostly unaffected. For the empty prompt, activating these features produces abstract-looking images that are hard to relate to the other columns. Thus, we excluded this visualization technique and instead added one more dataset example. In summary, the learned features of this transformer block primarily add local details to the generation and, importantly, they are effective only within suitable context. Mid.0. The specific role of the fourth block (mid.0) is not well understood. We find it more difficult to interpret because most interventions in the mid.0 block have subtle effects. We did not include empty-prompt intervention results because they barely affect the generation. Despite these subtle effects, dataset examples (C. columns) and heatmaps (hmap column) mostly agree with each other and are specific enough to be interpretable: 4755 activates on bottom right part of faces, 4235 on left part of (animal) faces, 1388 on people in the background, and, 5102 on outlines the left border of the main object in the scene. We hypothesize that mid.0s features are more abstract, potentially encoding spatial location6 and relations between objects. 4.3 CASE STUDY II: RANDOM FEATURES In this case study, we explore the learned features independently of any specific prompt. In App. Fig. 6 and Fig. 7, we demonstrate the first 5 and last 5 learned features for each transformer block. In addition, we provide similar visualisations for the first 100 features of each layer in the supplementary material. As SAEs are randomly initialized before the training process, these sets can be considered as random samples of features. Each feature visualization consists of 3 images of top 5% images for this feature, and their perturbations with activation modulation interventions. For down.2.1 and up.0.1, we also include the empty-prompt intervention images. Additionally, we provide visualizations of several selected features in App. Fig. 8 and demonstrate the effects of their forced activation on unrelated prompts in App. Fig. 9. Overall, our insights gained from Case Study (Sec. 4.2) appear to generalize on random feature samples. In particular, this suggests that significant portion of the learned features are interpretable. Additionally, when studying features in isolation, it becomes apparent that distinctions between the blocks are blurred. For example, some down.2.1 features correspond to elements of style, e.g., anime style and cartoon style feature in App. Fig. 8. Likewise, some up.0.1 features dont only change style but also add and remove elements of the scene, e.g., dog eyes (App. Fig. 6 up.0.1 feature 3)."
        },
        {
            "title": "5 QUANTITATIVE EVALUATION OF THE LEARNED FEATURES",
            "content": "In this section, we follow up on qualitative insights by collecting quantitative evidence. 6SDXL Turbo does not utilize positional encodings for the spatial locations in the feature maps. Therefore, ij . These probes achieved high we did brief sanity check and trained linear probes to detect i, given Din accuracy on holdout set: 97.9%, 98.48%, 99.44%, 95.57% for down.2.1, mid.0, up.0.0, up.0.1. 7 Preprint 5.1 ANNOTATION PIPELINE Feature annotation with an LLM followed by further evaluation is common way to assess feature properties such as specificity, sensitivity, and causality (Caden et al., 2024). We found it applicable to the features learned by the down.2.1 transformer block, which have strong effect on the generation. Thus, they are amendable to automatic annotation using visual language models (VLMs) such as GPT-4o (OpenAI, 2024). In contrast, for the features of other blocks with more subtle effects, we found VLM-generated captions to be unsatisfactory. In order to caption the features of down.2.1, we prompt GPT-4o with sequence of 14 images. The first five images are irrelevant to the feature (i.e., the feature was inactive during the generation of the images), followed by progression of 4 images with increasing average activation values, and finished by five images with the highest average activation values. The last nine images are provided alongside their so-called coldmaps: version of an image with weakly active and inactive regions being faded and concealed. The prompt template and examples of the captions can be found in App. E. 5.2 EXPERIMENTAL DETAILS We perform series of experiments in order to get statistical insights into the features learned. We will report the majority of the experimental scores in the format (S). When the score is reported in the context of SDXL Turbo transformer block, it means that we computed the score for each feature of the block and set and to mean and standard deviation across the feature scores. For the baselines, we calculate the mean and standard deviation across the scores of 100-element sample. Table 1: Metrics for SDXL Turbo blocks and baselines. (a) Specificity, texture score, and color activation for different blocks and baselines. (b) Manhattan distances between original and intervened images at varying intervention strengths. Block Specificity Texture Color Block - -5 5 10 Down.2.1 Mid Up.0.0 Up.0.1 Random Same Prompt 0.89 (0.06) Textures 0.71 (0.11) 0.16 (0.02) 86.2 (14.9) 0.62 (0.11) 0.14 (0.01) 84.7 (16.3) 0.66 (0.12) 0.18 (0.03) 86.3 (16.5) 0.65 (0.11) 0.20 (0.02) 73.8 (20.6) 0.50 (0.10) 0.13 (0.02) 90.7 (54.9) 0.18 (0.02) Down.2.1 148.2 / 116.0 124.2 / 94.4 101.4 / 78.7 128.9 / 105.60 Mid 59.9 / 29.82 39.4 / 18.5 Up.0.0 88.6 / 37.08 77.7 / 23.7 Up.0.1 98.9 / 34.74 73.1 / 16.4 69.2 / 32.2 105.3 / 38.4 125.0 / 26.8 33.2 / 15.2 63.6 / 23.3 68.6 / 21.9 Interpretability. Features are usually considered interpretable if they are sufficiently specific, i.e., images exhibiting the feature share some commonality. In order to measure this property, we compute the similarity between images on which the feature is active. High similarity between these images is proxy for high specificity. For each feature, we collect 10 random images among top 5% images for this feature and calculate their average pairwise CLIP similarity (Radford et al., 2021; Cherti et al., 2023). This value reflects how semantically similar the contexts are in which the feature is most active. We display the results in the first column of Table 1 (a), which shows that the CLIP similarity between images with the feature active is significantly higher then the random baseline (CLIP similarity between random images) for all transformer blocks. This suggests that the generated images share similarities when feature is active. For down.2.1 we compute an additional interpretability score by comparing how well the generated annotations align with the top 5% images. The resulting CLIP similarity score is 0.21 (0.03) and significantly higher then the random baseline (average CLIP similarity with random images) 0.12 (0.02). To obtain an upper bound on this score we also compute the CLIP similarity to an image generated from the feature annotation, which is 0.25 (0.03). Causality. We can use the feature annotations to measure features causal strength by comparing the empty prompt intervention images with the caption.7 The CLIP similarity between intervention images and feature caption is 0.19 (0.04) and almost matches the annotation-based interpretability score of 0.21 (0.03). This suggests that feature annotations effectively describe to the corresponding 7We require feature captions for the causality and sensitivity analyses, we only have them for down.2.1. 8 Preprint empty-prompt intervention images. Notably, the annotation pipeline did not use empty-prompt intervention images to generate captions. This fact speaks for the high causal strength of the features learned on down.2.1. Sensitivity. feature is considered sensitive when activated in its relevant context. As proxy for the context, we have chosen the feature annotations obtained with the auto-annotation pipeline. For each learned feature, we collected the 100 prompts from 1.5M sample of LAION-COCO with the highest sentence similarity based on sentence transformer embeddings of all-MiniLM-L6-v2 (Reimers & Gurevych, 2019). Next, we run SDXL Turbo on these prompts and count the proportion of generated images in which the feature is active on more than 0%, 10%, 30% of the image area, resulting in 0.60 (0.32), 0.40 (0.34), 0.27 (0.30) respectively, which is much higher than the random baseline, which is at 0.06 (0.09), 0.003 (0.006), 0.001 (0.003). However, the average scores are < 1 and thus not perfect. This may be caused by incorrect or imprecise annotations for subtle features and, therefore, hard to annotate with VLM and SDXL Turbo failing to comply with some prompts. Relatedness to texture. In Fig. 1 and App. Fig. 6 the empty prompt interventions of the up.0.1 features resulted in texture-like pictures. To quantify whether this consistently happens, we design simple texture score by computing CLIP similarity between an image and the word texture. Using this score, we compare empty-prompt interventions of the different transformer blocks with each other and real-world texture images. The results are in the second column of Table 1 (a) and suggest that empty-prompt intervention images of up.0.1 and up.0.0 resemble textures and some of the down.2.1 images look like textures as well. For up.0.0, we did not observe any connection of these images to the top activating images. Interestingly, the score of up.0.1 is higher than the one of the real-world textures dataset (Cimpoi et al. (2014)). Color sensitivity. In our qualitative analysis, we suggested that the features learned on up.0.1 relate to texture and color. If this holds, the image regions that activate feature should not differ significantly in color on average. To test that, we calculate the average color for each feature: this is weighted average of pixel colors with the feature activation values as weights. To determine the average color of each feature we compute it over sample of 10 images of the features top 5% images. Then, we calculate Manhattan distances between the colors of the pixels and the average color on the same images (the highest possible distance is 3 255 = 765). Finally, we take weighted average of the Manhattan distances using the same weights. We report these distances for different transformer blocks and for the images generated on random prompts from LAION-COCO. We present the results in the third column of Table 1 (a). The average distance for the up.0.1 transformer block is, in fact, the lowest. Intervention locality. We suggested that the features learned on up.0.0 and up.0.1 influence intervened generations locally. We estimate how the top 5% images change inside and outside the active regions to quantitatively assess this claim. To exclude weak activation regions from consideration, we say that pixel is inside the active area if the corresponding 32x32 patch has an activation value larger than 50% of the image patches, and it is outside the active area if the corresponding 32x32 patch has activation value of zero. In Table 1 (b), we report Manhattan distances between the original images and the intervened images outside and inside the active areas for activation modulation intervention strengths -10, -5, 5, 10. The features for up.0.0 and up.0.1 have higher effect inside the active area than outside, in contrast to down.2.1 for which this difference is smaller."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Image editing with diffusion models. Numerous studies have analyzed diffusion models attribute editing capabilities. Yue et al. (2024) learn disentangled representation on human faces (Liu et al., 2015; Karras et al., 2019) and bedrooms (Yu et al., 2015). The models of Yue et al. (2024) exhibit interpolation abilities between the attributes of two reference images. Similarly, Wang & Golland (2023) interpolate between pairs of images by fine-tuning latent diffusion model. Through experiments on synthetic and real datasets, Deschenaux et al. (2024) demonstrate that even if diffusion model is trained on an incomplete subset of the data distribution, it can still generate samples from the full distribution. Kim et al. (2022) show that one can guide pre-trained diffusion model using text instructions leveraging CLIP (Radford et al., 2021). Meng et al. (2022) edit images using an off-the-shelf diffusion model that was not explicitly trained for image editing using the stochastic differential equation formalism of diffusion models. For example, the method of Meng et al. (2022) 9 Preprint can transform color strokes into photo-realistic images, similar to Park et al. (2019). Importantly, unlike Park et al. (2019), Meng et al. (2022) does not fine-tune the diffusion model for this task. (Zhang et al., 2023; Kawar et al., 2023; Baumann et al., 2024) demonstrate fine-grained attribute editing using single reference image. Analyzing the latent space of diffusion models. Kwon et al. (2023) show that diffusion models naturally have semantically meaningful latent space. Park et al. (2023) analyze the latent space of diffusion models using Riemannian geometry. Li et al. (2024) and Dalva & Yanardag (2024) present self-supervised methods for finding semantic directions in the latent space. Similarly, Gandikota et al. (2023) show that the attribute variations lie in low-rank space by learning LoRA adapters (Hu et al., 2021) on top of pre-trained diffusion models. Brack et al. (2023) and Wang et al. (2023) demonstrate effective semantic vector algebraic operations in the latent space of DMs, as observed by Mikolov et al. (2013). However, none of those works explicitly train SAEs to interpret and control the latent space. Mechanistic interpretability using SAEs. Sparse autoencoders have recently been popularized by Bricken et al. (2023), in which they show that it is possible to learn interpretable features by decomposing neuron activations in MLPs in 2-layer transformer language models. At the same time, parallel work decomposed the elements of the residual stream (Cunningham et al., 2023), which followed up on (Sharkey et al., 2022). To our knowledge, the first work that applied sparse autoencoders to transformer-based LLM was (Yun et al., 2021), which learned joint dictionary for features of all layers. Recently, sparse autoencoders have gained much traction, and many have been trained even on state-of-the-art LLMs (Gao et al., 2024; Templeton & et al., 2024; Lieberum et al., 2024). In addition, great tools are available for inspection (Lin & Bloom, 2023) and automatic interpretation (Caden et al., 2024) of learned features. Marks et al. (2024) have shown how to use SAE features to facilitate automatic circuit discovery. The studies most closely related to our work are (Bau et al., 2019), (Ismail et al., 2023) and (Daujotas, 2024). Ismail et al. (2023) apply concept bottleneck methods (Koh et al., 2020) that decompose latent concepts into vectors of interpretable concepts to generative image models, including diffusion models. Unlike the SAEs that we train, this method requires labeled concept data. Daujotas (2024) decomposes CLIP (Radford et al., 2021; Cherti et al., 2023) vision embeddings using SAEs and use them for conditional image generation with diffusion model called Kandinsky (Razzhigaev et al., 2023). Importantly, using SAE features, they are able to manipulate the image generation process in interpretable ways. In contrast, in our work, we train SAEs on intermediate representations of the forward pass of SDXL Turbo. Consequently, we can interpret and manipulate SDXL Turbos forward pass on finer granularity, e.g., by intervening on specific transformer blocks and spatial positions. Another closely related work to ours is (Bau et al., 2019), in which neurons in generative adversarial neural networks are interpreted and manipulated. The interventions in (Bau et al., 2019) are similar to ours, but on neurons instead of sparse features. In order to identify neurons corresponding to semantic concept, Bau et al. (2019) require semantic image segmentation maps."
        },
        {
            "title": "7 CONCLUSION AND DISCUSSION",
            "content": "We trained SAEs on SDXL Turbos opaque intermediate representations. This study is the first in the academic literature to mechanistically interpret the intermediate representations of modern textto-image model. Our findings demonstrate that SAEs can extract interpretable features and have significant causal effect on the generated images. Importantly, the learned features provide insights into SDXL Turbos forward pass, revealing that transformer blocks fulfill specific and varying roles in the generation process. In particular, our results clarify the functions of down.2.1, up.0.0, and up.0.1. However, the role of mid.0 remains less defined; it seems to encode more abstract information and interventions are less effective. We follow up with discussion of the results and their implications for future research. Based on our observations, we suggest preliminary hypothesis about SDXL Turbos generation process: down.2.1 decides on top-level composition, mid.0 encodes low-level semantics, up.0.0 adds details based on the two above, and up.0.1 fills in color, texture, and style. While our work provides important insights into the mechanisms of SDXL Turbo, we studied its transformer blocks in isolation. Further research is required to understand how the features of 10 Preprint SDXL Turbo interact between blocks and how this affects the overall functionality of the model. promising direction would be the application of advanced interpretability techniques such as those explored by Marks et al. (2024), which compute circuits showing how different layers and attention heads wire together. Therefore, these techniques would provide insight into our hypothesis stated above. In addition, the complex nature of some of the learned visual features deserves special attention. Although some features (for example, learning on down.2.1 and up.0.1) exhibit their effect even when turning them on during empty-prompt generations, other features (typically, the ones learned on up.0.0) require an appropriate context to show their effect. This complexity poses additional challenges for the automatic annotation of features. Our preliminary observations in this direction suggest that current visual language models struggle to detect such behaviors. Furthermore, we question whether captioning the visual features with few short sentences adequately captures most features roles. Our work highlights the potential of SAEs in revealing the internal structure of diffusion models like SDXL Turbo, and it could help future researchers answer more sophisticated questions about image generation. For example, how does SDXL Turbo add illumination effects, render wool, hair, or reflections of objects in the water?"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Danila Zubko for his early contributions to the projects initial discussions and development. Robert Wests lab is partly supported by grants from the Swiss National Science Foundation (200021 185043, TMSGI2 211379), Swiss Data Science Center (P22 08), H2020 (952215), Microsoft, and Google."
        },
        {
            "title": "REFERENCES",
            "content": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua Tenenbaum, William Freeman, and Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. In International Conference on Learning Representations, 2019. Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, and Bjorn Ommer. Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions, 2024. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 2536525389. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/4ff83037e8d97b2171b2d3e96cb8e677-Paper-Conference.pdf. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, and Adam Jermyn et al. TransTowards monosemanticity: Decomposing language models dictionary learning. former Circuits, October 2023. URL https://transformer-circuits.pub/2023/ monosemantic-features. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Juang Caden, Paulo Goncalo, Drori Jacob, and Belrose Nora. Open source automated interpretability for sparse autoencoder features, 2024. URL https://blog.eleuther.ai/ autointerp/. Accessed: 2024-09-27. Haozhe Chen, Carl Vondrick, and Chengzhi Mao. Selfie: Self-interpretation of large language model embeddings. arXiv preprint arXiv:2403.10949, 2024. 11 Preprint Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Yusuf Dalva and Pinar Yanardag. Noiseclr: contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2420924218, June 2024. Gytis Daujotas. URL interpreting-and-steering-features-in-images. Accessed: 2024-09-27. 2024. https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/ Interpreting steering features images, and in Justin Deschenaux, Igor Krawczuk, Grigorios Chrysos, and Volkan Cevher. Going beyond compositions, DDPMs can produce zero-shot interpolations. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=1pj0Sk8GfP. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL https://arxiv.org/abs/2105.05233. Soch Joram Duchi John. Multivariate normal distribution: Kullback-leibler divergence, 05 2020. URL https://statproofbook.github.io/P/mvn-kl.html. Accessed on October 31, 2024. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. Transformer Circuits, September 2022. URL https://transformer-circuits. pub/2022/toy_model/index.html. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis, 2021. URL https://arxiv.org/abs/2012.09841. Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models, 2023. URL https://arxiv. org/abs/2311.12092. Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope: unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102, 2024. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https: //arxiv.org/abs/1406.2661. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. Preprint Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images, 2023. URL https://arxiv.org/abs/2301.11093. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Aya Abdelsalam Ismail, Julius Adebayo, Hector Corrada Bravo, Stephen Ra, and Kyunghyun Cho. In The Twelfth International Conference on Learning Concept bottleneck generative models. Representations, 2023. William Johnson, Joram Lindenstrauss, and Gideon Schechtman. Extensions of lipschitz maps into banach spaces. Israel Journal of Mathematics, 54(2):129138, 1986. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019. URL https://arxiv.org/abs/1812.04948. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models, 2022. URL https://arxiv.org/abs/2206.00364. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Imagic: Text-based real image editing with diffusion models, 2023. URL and Michal Irani. https://arxiv.org/abs/2210.09276. Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation, 2022. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models, 2023. URL https://arxiv.org/abs/2107.00630. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International conference on machine learning, pp. 53385348. PMLR, 2020. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space, 2023. URL https://arxiv.org/abs/2210.10960. Hang Li, Chengzhi Shen, Philip Torr, Volker Tresp, and Jindong Gu. Self-discovering interpretable diffusion latent directions for responsible text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1200612016, June 2024. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147, 2024. Jae Hyun Lim and Jong Chul Ye. Geometric gan, 2017. URL https://arxiv.org/abs/ 1705.02894. Johnny Lin and Joseph Bloom. Neuronpedia: Interactive reference and tooling for analyzing neural networks, 2023. URL https://www.neuronpedia.org. Software available from neuronpedia.org. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015. Calvin Luo. Understanding diffusion models: unified perspective, 2022. URL https: //arxiv.org/abs/2208.11970. 13 Preprint Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. URL https://arxiv.org/abs/2108.01073. Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge?, 2018. URL https://arxiv.org/abs/1801.04406. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013. URL https://arxiv.org/abs/1301.3781. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in inceptionv1. Distill, 5(4):e00024002, 2020. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o. Accessed: 2024-09-28. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization, 2019. URL https://arxiv.org/abs/1903.07291. Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 2412924142. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/4bfcebedf7a2967c410b64670f27f904-Paper-Conference.pdf. Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. arXiv Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. preprint arXiv:2306.00637, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. URL https://arxiv.org/abs/2307.01952. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. practical review of mechanistic interpretability for transformer-based language models. arXiv preprint arXiv:2407.02646, 2024. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents, 2022. URL https://arxiv.org/abs/ 2204.06125. 14 Preprint Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: An improved text-to-image synthesis with image prior and latent diffusion. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 286295, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv. org/abs/1908.10084. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. URL https://arxiv.org/abs/1505.04597. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022a. URL https://arxiv.org/abs/ 2205.11487. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022b. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. URL https://arxiv.org/abs/2202.00512. Axel Sauer, Kashyap Chitta, Jens Muller, and Andreas Geiger. Projected gans converge faster, 2021. URL https://arxiv.org/abs/2111.01007. Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis, 2023a. URL https://arxiv. org/abs/2301.09515. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023b. URL https://arxiv.org/abs/2311.17042. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022a. Christoph Schuhmann, Andreas Kopf, Richard Vencu, Theo Coombes, Romain Beaumont, and Benjamin Trom. Laion coco: 600m synthetic captions from laion2b-en, 2022b. URL https: //laion.ai/blog/laion-coco/. Accessed: 2024-10-01. Lee Sharkey, features Dan Braun, of out Interim research beren. ing autoencoders, superposition with https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/ interim-research-report-taking-features-out-of-superposition. Accessed: 2024-09-27. report: sparse 2022. and TakURL Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/ abs/1503.03585. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution, 2020. URL https://arxiv.org/abs/1907.05600. 15 Preprint Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. Matteo Spinelli. Advanced style transfer with the mad scientist node. YouTube video, 2024. URL https://www.youtube.com/watch?v=ewKM7uCRPUg. Accessed: 2024-09-17. Adly Templeton and Tom Conerly et al. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet, 2024. URL https://transformer-circuits.pub/2024/ scaling-monosemanticity/. Accessed: 2024-09-27. Clinton Wang and Polina Golland. Interpolating between images with diffusion models, 2023. text-controlled generative models. Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. based) K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural cessing Systems, volume 36, pp. 3533135349. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 6f125214c86439d107ccb58e549e828f-Paper-Conference.pdf. Concept algebra for (scoreIn A. Oh, T. Naumann, A. Globerson, Information ProURL Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. URL http://dblp.uni-trier.de/db/journals/corr/corr1506.html# YuZSSX15. Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, and Hanwang Zhang. Exploring diffusion time-steps for unsupervised representation learning, 2024. Zeyu Yun, Yubei Chen, Bruno Olshausen, and Yann LeCun. Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors. arXiv preprint arXiv:2103.15949, 2021. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N. Metaxas, and Jian Ren. Sine: Single image In Proceedings of the IEEE/CVF Conference on editing with text-to-image diffusion models. Computer Vision and Pattern Recognition (CVPR), pp. 60276037, June 2023."
        },
        {
            "title": "A FINDING CAUSALLY INFLUENTIAL TRANSFORMER BLOCKS",
            "content": "As first step, we narrow down design space of the 11 cross-attention transformer blocks (see Fig. 4) to those with the highest causal impact on the output. In order to assess their causal impact on the output we qualitatively study the effect of individually ablating each of them (see Fig. 2). As can be seen in Fig. 2 each of the middle blocks down.2.1, mid.0, up.0.0, up.0.1 have relatively high impact on the output respectively. In particular, the blocks down.2.1 and up.0.1 stand out. It seems like most colors and textures are added in up.0.1, which in the community is already known as style block Spinelli (2024). Ablating down.2.1, which is also already known in the community as composition block, impacts the entire image composition, including object sizes, orientations and framing. The effects of ablating other blocks such as mid.0 and up.0.0 are more subtle. For mid.0 it is difficult to describe in words and up.0.0 seems to add local details to the image while leaving the overall composition mostly intact."
        },
        {
            "title": "B SAE TRAINING RESULTS",
            "content": "We trained several SAEs with different sparsity levels and sparse layer sizes and observed no dead features. To assess reconstruction quality, we processed 100 random LAION-COCO prompts through one-step SDXL Turbo process, replacing the additive component of the corresponding transformer block with its SAE reconstruction. 16 Preprint Figure 2: We generate images for the prompts dog playing with ball cartoon., photo of colorful model., An astronaut riding on pig on the moon., photograph of the inside of subway train. There are frogs sitting on the seats. One of them is reading newspaper. The window shows the river in the background. and cinematic shot of professor sloth wearing tuxedo at BBQ party. while ablating the updates performed by different cross-attention layers (indicated by the titles). The title baseline corresponds to the generation without interventions. The explained variance ratio and the output effects caused by reconstruction are shown in Table 2. Fig. 3 presents random examples of reconstructions from an SAE with the following hyperparameters: = 10, nf = 5120, trained on down.2.1. The reconstruction causes minor deviations in the images, and the fairly low LPIPS (Zhang et al., 2018) and pixel distance scores also support these findings. However, to prevent these minor reconstruction errors from affecting our analysis of interventions, we decided to directly add or subtract learned directions from dense feature maps. Table 2: Distances and explained variance ratio in generated images. Mean represents the average pixel Manhattan distance between original and reconstruction-intervened images, with maximum possible value of 765. Median represents the median Manhattan distance per pixel, averaged over all images. LPIPS refers to the average LPIPS score, measuring perceptual similarity. Explained variance ratio denotes the ratio of variance explained by the trained SAEs to the total variance. nf 5 10 20 5120 640 5120 640 5120 Configuration Mean Median 83.29 50.04 52.64 26.82 55.89 30.69 52.67 34.53 74.68 41.49 48.82 24.60 49.19 25.86 47.50 31.11 73.65 41.79 46.80 23.10 48.43 25.80 43.06 26.85 64.97 34.77 44.02 21.72 42.08 21.54 39.77 24.84 59.29 31.47 39.95 19.44 40.15 21.06 31.97 18.15 56.37 29.04 37.28 17.82 35.73 18.03 30.31 17. down mid up0 up down mid up0 up down mid up0 up down mid up0 up down mid up0 up down mid up0 up LPIPS 0.3383 0.2032 0.2276 0.2073 0.3036 0.1845 0.1969 0.1775 0.2893 0.1772 0.1908 0.1638 0.2582 0.1627 0.1624 0.1453 0.2291 0.1459 0.1499 0.1196 0.2190 0.1328 0.1302 0.1104 Explained Variance Ratio (%) 56.0 43.4 44.8 50.3 67.8 50.8 57.2 59.5 62.8 51.5 52.5 58.7 73.7 58.8 64.2 67.1 69.9 60.0 60.9 66.7 78.8 66.5 70.6 74.2 17 Preprint Figure 3: Images generated from 10 random prompts taken from the LAION-COCO dataset are shown in the first row. In the second row, down.2.1 updates are replaced by their SAE reconstructions (k = 10, nf = 5120). The third row visualizes the differences between the original and reconstructed images. Figure 4: Cross-attention transformer blocks in SDLXs U-net. 18 Preprint CASE STUDY I: MOST ACTIVE FEATURES ON PROMPT Fig. 5 demonstrates an extended version of Case Study I, showcasing 9 top features instead of 5."
        },
        {
            "title": "D FEATURES INCLUDING PROMPTS",
            "content": "Feature plots. We provide the same plots as in Fig. 6 but for the last six feature indices of each transformer block in Fig. 7 and the corresponding prompts in Table 4. Additionally, provide some selected features for down.2.1 and up.0.1 in Fig. 8 and the corresponding prompts in Table 5. Intervention plots. Additionally, we provide plots in which we turn on features from Fig. 8 but in unrelated prompts (as opposed to top dataset example prompts that already activate the features by themselves). For simplicity here we simply turn on the features across all spatial locations, which does not seem to be well suitable strategy for up.0.1, which usually acts locally. To showcase, the difference we created one example image in Fig. 10, in which we manually draw localized masks to turn on the corresponding features."
        },
        {
            "title": "E ANNOTATION PIPELINE DETAILS",
            "content": "We used GPT-4o to caption learned features on down.2.1. For each feature, the model was shown series of 5 unrelated images, progression of 9 images, the i-th of those corresponds to 10% average activation value of the maximum. Finally, we show 5 images corresponding to the highest average activations. Since some features are active on particular parts of images, the last 9 images are provided alongside their so-called coldmaps: version of an image with weakly active and inactive regions being faded and concealed. The images were generated by 1-step SDXL Turbo diffusion process on 50000 random prompts of LAION-COCO dataset. E.1 TEXTUAL PROMPT TEMPLATE Here is the prompt template for the VLM. System. You are an experienced mechanistic interpretability researcher that is labeling features from the hidden representations of an image generation model. User. You will be shown series of images generated by machine learning model. These images were selected because they trigger specific feature of sparse auto-encoder, trained to detect hidden activations within the model. This feature can be associated with particular object, pattern, concept, or place on an image. The process will unfold in three stages: 1. **Reference Images:** First, youll see several images *unrelated* to the feature. These will serve as reference for comparison. 2. **Feature-Activating Images:** Next, youll view images that activate the feature with varying strengths. Each of these images will be shown alongside version where non-activated regions are masked out, highlighting the areas linked to the feature. 3. **Strongest Activators:** Finally, youll be presented with the images that most strongly activate this feature, again with corresponding masked versions to emphasize the activated regions. Your task is to carefully examine all the images and identify the thing or concept represented by the feature. Heres how to provide your response: - **Reasoning:** Between <thinking> and </thinking> tags, write up to 400 words explaining your reasoning. Describe the visual patterns, objects, or concepts that seem to be consistently present in the feature-activating images but not in the reference images. 19 Preprint (a) Top 9 features of down.2.1 (b) Top 9 features up.0.1 (c) Top 9 features of up.0.0 (d) Top 9 features of mid.0 Figure 5: The top 9 features of down.2.1 (a), up.0.1 (b), up.0.0 (c) and mid.0 (d) for the prompt: cinematic shot of professor sloth wearing tuxedo at BBQ party. Each row represents feature. The first column depicts feature heatmap (highest activation red and lowest nonzero one blue). The column titles containing show feature modulation interventions, the ones containing the intervention of turning on the feature on the empty prompt, and the ones containing depict top dataset examples. Floating point values in the title denote β and γ values. Preprint - **Expression:** Afterward, between <answer> and </answer> tags, write concise phrase (no more than 15 words) that best captures the common thing or concept across the majority of feature-activating images. Note that not all feature-activating images may perfectly align with the concept youre describing, but the images with stronger activations should give you the clearest clues. Also pay attention to the masked versions, as they highlight the regions most relevant to the feature. User. These images are not related to the feature: {Reference Images} User. This is row of 9 images, each illustrating increasing levels of feature activation. From left to right, each image shows progressively higher activation, starting with the image on the far left where the feature is activated at 10% relative to the image that activates it the most, all the way to the far right, where the feature activates at 90% relative to the image that activates it the most. This gradual transition highlights the features growing importance across the series. {FeatureActivating Images} User. This row consists of 9 masked versions of the original images. Each masked image corresponds to the respective image in the activation row. Areas where the feature is not activated are completely concealed by white mask, while regions with activation remain visible.) {Feature-Activating Images Coldmaps} User. These images activate the feature most strongly. {Strongest Activators} User. These masked images highlight the activated regions of the images that activate the feature most strongly. The masked images correspond to the images above. The unmasked regions are the ones that activate the feature. {Strongest Activators Coldmaps} E.2 EXAMPLE OF PROMPT IMAGES The images used to annotate feature 0 are shown in Fig. 11. E.3 EXAMPLES OF GENERATED CAPTIONS We present the captions generated by GPT-4o for the first and last 10 features in Table 6."
        },
        {
            "title": "F SPARSE AUTOENCODERS AND SUPERPOSITION",
            "content": "Let h(x) Rd be some intermediate result of forward pass of neural network on the input x. In fully connected neural network, the components h(x) could correspond to neurons. In transformers, which are residual neural networks with attention and fully connected layers, h(x) usually either refers to the content of the residual stream after some layer, an update to the residual stream by some layer, or the neurons within fully connected block. In general, h(x) could refer to anything, e.g., also keys, queries, and values. It has been shown (Yun et al., 2021; Cunningham et al., 2023; Bricken et al., 2023) that in many neural networks, especially LLMs, intermediate representations can be well approximated by sparse sums of nf learned feature vectors, i.e., h(x) nf (cid:88) ρ=1 sρ(x)fρ, (12) where sρ(x) are the input-dependent8 coefficients most of which are equal to zero and f1, . . . , fnf Rd is learned dictionary of feature vectors. Importantly, these learned features are usually highly interpretable (specific), sensitive (fire on the relevant contexts), causal (change the output in expected ways in intervention) and usually do not correspond directly to individual neurons. There are also some preliminary results on the universality of these learned features, i.e., that different training runs on similar data result in the corresponding models picking up largely the same features (Bricken et al., 2023). 8In the literature this input dependence is usually omitted. 21 Preprint (a) down.2. (b) mid.0 (c) up.0.0 (d) up.0.1 Figure 6: We visualize 6 features for down.2.1 (a), mid.0 (b), up.0.0, and up.0.1. We use three columns for each transformer block and three rows for each feature. For down.2.1 and up.0.1 we visualize the two samples from the top 5% quantile of activating dataset examples (middle) together feature ablation (left) and feature enhancement (right), and, activate the feature on the empty prompt with γ = 0.5, 1, 2 from left to right. For mid.0 and up.0.0 we display three samples with ablation and enhancement. Captions are in Table 3. 22 Preprint Table 3: Prompts for the top 5% quantile examples in Fig. 6 Block Feature Prompt down.2.1 mid. up.0.0 up.0.1 0 0 1 1 2 2 3 3 4 4 5 5 0 0 0 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 0 0 0 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 0 0 1 1 2 2 3 3 4 4 5 file folder with the word document management on it. Two blue folders filled with dividers. kitchen with an island and bar stools. An unfinished bar with stools and wood counter. The Taj Mahal, or white marble building in India. The Taj Mahal, or white marble building in India. man and woman standing next to each other. Two men in suits hugging each other outside. An old Forester whiskey bottle sitting on top of wooden table. Red roses and hearts on wooden table. beaded brooch with pearls and copper. An image of brooch with diamonds. The Boss TS-3W pedal has an electronic tuner. An engagement ring with blue sapphire and diamonds. The womens pink sneaker is shown. white ceiling fan with three blades. ceiling fan with three blades and light. The ceiling fan is dark brown and has two wooden blades. The black dress is made from knit and has metallic sleeves. The back view of woman wearing black and white sports bra. The woman is wearing striped swimsuit. An old-fashioned photo frame with little girl on it. The woman is sitting in her car with her head down. The contents of an empty bottle in box. An old painting of man in uniform. The model wears an off-white sweatshirt with green panel. The Statue of Liberty stands tall in front of blue sky. Cheese and crackers on cutting board. Two cufflinks with coins on them. Three pieces of luggage are shown in blue. Three wine glasses with gold and silver designs. Three green wine glasses sitting next to each other. New Years Eve with champagne, gold, and silver. The birdhouse is made from wood and has brown roof. The garage is white with red shutters. Two garages with one attached porch and the other on either side. An elegant white lace purse with gold clasp. The red handbag has gold and silver designs. pink and green floral-colored purse. magazine rack with magazines on it. The year-in-review page for this digital scrap. The planner sticker kit is shown with gold and black accessories. clock with numbers on the face. silver watch with roman numerals on the face. An automatic watch with silver dial. Four pieces of wooden furniture with blue and white designs. The green chair is in front of white rug. The wish chair with black seat. The wooden toy kitchen set includes bread, eggs, and flour. The office chair is brown and black. An aerial view of the white sand and turquoise water. An aerial view of the beach and ocean. The patriarch of Ukraine is shown speaking to reporters. German Chancellor Merkel gestures as she speaks to the media. Four pictures showing dogs wearing orange vests. Two dogs are standing on the ground next to flowers. man standing in front of wooden wall. blue mailbox sitting on top of wooden floor. The baseball players are posing for team photo. The baseball players are holding up their trophies. 23 Preprint (a) down.2.1 (b) mid.0 (c) up.0.0 (d) up.0.1 Figure 7: We visualize last 6 features for down.2.1 (a), mid.0 (b), up.0.0, and up.0.1. We use three columns for each transformer block and three rows for each feature. For down.2.1 and up.0.1 we visualize two samples from the top 5% quantile of activating dataset examples (middle) together feature ablation (left) and feature enhancement (right), and, activate the feature on the empty prompt with γ = 0.5, 1, 2 from left to right. For mid.0 and up.0.0 we display three samples with ablation and enhancement. Captions are in Table 4. Preprint Table 4: Prompts for the top 5 % quantile examples in Fig. 7 Block Feature Prompt down.2. mid.0 up.0.0 up.0.1 5114 5114 5115 5115 5116 5116 5117 5117 5118 5118 5119 5119 5114 5114 5114 5115 5115 5115 5116 5116 5116 5117 5117 5117 5118 5118 5118 5119 5119 5119 5114 5114 5114 5115 5115 5115 5116 5116 5116 5117 5117 5117 5118 5118 5118 5119 5119 5114 5114 5115 5115 5116 5116 5117 5117 5118 5118 5119 5119 Black and white Converse sneakers with the word black star. Black and white Converse sneakers with the word Chuck. woman holding up photo of herself. man holding up tennis ball in the air. The Nike Womens U.S. Soccer Team DRI-Fit 1/4 Zip Top. The womens gray and orange half-zip sweatshirt. large group of people sitting in front of basketball court. Hockey players are playing in an arena with spectators. The black and white plaid shirt is shown. The different colors and sizes of t-shirts. ball of yarn on white background. Two balls of colored wool are on the white surface. People holding signs in front of building. Two men dressed in suits and ties are holding up signs. large group of people holding flags and signs. kitchen with white cabinets and blue stove. The kitchen is clean and ready for us to use. kitchen with white cabinets and stainless steel appliances. The steering wheel and dashboard in car. The interior of car with dashboard controls. The dashboard and steering wheel in car. Three men are celebrating goal on the field. Two men in Red Bull racing gear standing next to each other. Two men are posing for the camera at an event. Someone is holding up their nail polish with pink and black designs. The nail is very cute and looks great with marble. White stily nails with gold and diamonds. The Mighty Thor comic book. The camera is showing its flash drive. truck with bikes on the back parked next to camper. The Acer laptop is open and ready to use. The Lenovo S13 laptop is open and has an image of person jumping off the keyboard. laptop with the words Hosting Event on it. horse with black nose and brown mane. The horse leather oil is being used to protect horses. An oil painting on canvas of horse. The sun is shining brightly over Saturn. football player throws the ball to another team. Car door light logo sticker for Hyundai. An artistic black and silver sculpture with speakers. The pink brushes are sitting on top of each other. Four kings playing cards in the hand. man is fixing an air conditioner. The black Land Rover is parked in front of large window. flat screen TV mounted on the wall above fireplace. table with many different tools on it. camera with many different items including flash cards, lenses, and other accessories. The contents of an open suitcase and some clothes. An old Navajo rug with multicolored designs. The pillow is made from an old kilim. An image of noni juice with some fruits. bottle and glass on the counter with green juice. Someone cleaning the shower with sponge. man on skateboard climbing wall with ropes. man taking selfie in front of some camera equipment. person holding up business card with the words cycle transportation. Two photos are placed on top of an open book. An open book with pictures of children and their parents. An engagement ring with diamonds on top. An oval ruby and diamond ring. Preprint (a) down.2.1 (b) up.0.1 Figure 8: We visualize 6 features for down.2.1 (a) and up.0.1 (b). We use 5 columns for each transformer block and three rows for each feature. We visualize three samples from the top 5% quantile of activating dataset examples (middle) together feature ablation (left) and feature enhancement (right). Captions are in Table 5. 26 Preprint Table 5: Prompts for the top 5% quantile examples in Fig. 8 Block Feature Prompt down.2.1 up.0. 4998 4998 4998 4074 4074 4074 2301 2301 2301 56 56 56 59 59 59 89 89 89 4955 4955 4955 4977 4977 4977 3718 3718 3718 90 90 90 1093 1093 1093 2165 2165 2165 cartoon bee wearing hat and holding something. Two cartoon pictures of the same man with his hands in his pockets. cartoon bear with purple shirt and yellow shorts. An anime character with cat ears and dress. Two anime characters, one with white hair and the other with red eyes. An anime book with two women in blue dresses. man with white hair and red eyes holding chain. An animated man with white hair and beard. The character is standing with horns on his head. Two men in uniforms riding horses with swords. woman riding on the back of brown horse. Two jockeys on horses racing down the track. red jar with floral designs on it. An old black vase with some design on it. vase with birds and flowers on it. StarCraft 2 is coming to the Nintendo Wii. Overwatch is coming to Xbox and PS3. The hero in Overwatch is holding his weapon. An African wild dog laying in the grass. The woman is posing for photo in her leopard print top. An animal print cube ottoman with brown and white fur. white tiger with blue eyes standing in the snow. bottle and tiger are shown next to each other. mural on the side of building with tiger. Giraffes are standing in the grass near vehicle. Two giraffes standing next to each other in the grass. giraffe standing next to an ironing board. lion is roaring its teeth in the snow. lion sitting in the grass looking off into the distance. Two lions with flowers on their backs. The sun is shining over mountains and trees. Bride and groom in front of lake with sun flare. The milky sun is shining brightly over the trees. The silhouette of person riding bike at sunset. The Dark Knight rises from his cave in Batmans poster. yellow sign with black design depicting tractor. Table 6: down.2.1 first 10 and last 10 feature captions. Block Feature Caption down.2.1 0 1 2 3 4 5 6 7 8 9 5110 5111 5112 5113 5114 5115 5116 5117 5118 5119 Organizational/storage items for documents and office supplies Luxury kitchen interiors and designs Architectural Landmarks and Monumental Buildings Upper body clothing and attire Rustic or Natural Wooden Textures or Surfaces Intricately designed and ornamental brooches Technical diagrams and instructional content Feature predominantly activated by visual representations of dresses Home decor textiles focusing on cushions and pillows Eyewear: glasses and sunglasses Concept of containment or organized enclosure Groups of people in collective settings Modern minimalist interior design Indoor plants and greenery Feature sensitivity focused on sneakers Handling or manipulating various objects Athletic outerwear, particularly zippered sporty jackets Spectator Seating in Sporting Venues Textiles and clothing materials, focus on textures and folds Yarn and Knitting Textiles Preprint Superposition. By associating task-relevant features with directions in Rd instead of individual components of h(x) Rd, it is possible to represent many more features than there are components, i.e., nf >> d. As result, in this case, the learned dictionary vectors f1, . . . , fnf cannot be orthogonal to each other, which can lead to interference when too many features are on (thus the sparsity requirement). However, it would be theoretically possible to have exponentially (in d) many almost orthogonal directions embedded in Rd.9 Using representations like this, the optimization process during training can trade off the benefits of being able to represent more features than there are components in with the costs of features interfering with each other. Such representations are especially effective if the real features underlying the data do not co-occur with each other too much, that is, they are sparse. In other words, in order to represent single input (Michael Jordan) only small subset of the features (person, ..., played basketball) is required (Elhage et al., 2022; Bricken et al., 2023). The phenomenon of neural networks that exploit representations with more features than there are components (or neurons) is called superposition (Elhage et al., 2022). Superposition can explain the presence of polysemantic neurons. The neurons, in this case, are simply at the wrong level of abstraction. The closest feature vector can change when varying neuron, resulting in the neuron seemingly reacting to or steering semantically unrelated things. Sparse autoencoders. In order to implement the sparse decomposition from equation 12, the vector containing the nf coefficients of the sparse sum is parameterized by single linear layer followed by ReLU activations, called the encoder, = ENC(h) = σ(W ENC(h bpre) + bact), (13) in which Rd is the latent that we aim to decompose, σ() = max(0, ), ENC Rnf is learnable weight matrix and bpre and bact are learnable bias terms. We omitted the dependencies = h(x) and = s(h) that are clear from context. Similarly, the learnable features are parametrized by single linear layer, called decoder, = DEC(s) = DECs + bpre, (14) in which DEC = (f1 fnf ) Rdnf is learnable matrix of whose columns take the role of learnable features and bpre is learnable bias term. Training. The pair ENC and DEC are trained in way that ensures that is sparse sum of feature vectors (as in equation 1). Given dataset of latents h1, . . . , hn, both encoder and decoder are trained jointly to minimize proxy to the loss (cid:88) (cid:88) hi2 2 + λsi0 = DEC(ENC(hi)) hi2 2 + λENC(hi)0, (15) min ENC,W DEC bpre,bact i=1 i= where hi = h(xi), si = ENC(h(xi)) (when we refer to components of we use sρ instead), the 2 is reconstruction loss, si0 regularization term ensuring the sparsity of the activations and λ the corresponding trade-off term. ihi2 In practice, si0 cannot be efficiently optimized directly, which is why it is usually replaced with si1 or other proxy objectives. Technical details. In our work, we make use of the top-k formulation from Gao et al. (2024), in which si0 is ensured by introducing the top-k function TopK into the encoder: = ENC(h) = σ(TopK(W ENC(h bpre) + bact)). (16) As the name suggests, TopK returns vector that sets all components except the top ones to zero. In addition (Gao et al., 2024) use an auxiliary loss to handle dead features. During training, sparse feature ρ is considered dead if sρ remains zero over the last 10M training examples. the L2-reconstruction loss and the topThe resulting training loss is composed of two terms: auxiliary L2-reconstruction loss for dead feature reconstruction. For single latent h, the loss is defined L(h, h) = h2 2 + αh aux2 2 (17) 9It follows from the Johnson-Lindenstrauss Lemma (Johnson et al., 1986) that one can find at least exp(dϵ2/8) unit vectors in Rd with the dot product between any two not larger than ϵ. 28 Preprint In this equation, the aux is the reconstruction based on the top kaux dead features. This auxiliary loss is introduced to mitigate the issue of dead features. After the end of the training process, we observed none of them. Following (Gao et al., 2024), we set α = 1 32 and kaux = 256, performed tied initialization of encoder and decoder, normalized decoder rows after each training step. The number of learned features nf is set to 5120, which is four times the length of the input vector. The value of is set to 10 as good trade-off between sparsity and reconstruction quality. Other training hyperparameters are batch size: 4096, optimizer: Adam with learning rate: 104 and betas: (0.9, 0.999). FEW STEP DIFFUSION MODELS: SDXL TURBO Diffusion models. Diffusion models are class of generative models that were introduced by SohlDickstein et al. (2015) and are core component of many of the recent large-scale text-to-image generative models (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022a). Notably, Ho et al. (2020); Song & Ermon (2020) demonstrate that diffusion model are viable alternative to GANs (Goodfellow et al., 2014) for image generation. Additionally, diffusion models enjoy stable training dynamics, are easier to scale than GANs (Dhariwal & Nichol, 2021), and offer likelihood estimates of samples (Song et al., 2021). Diffusion models sample from an unknown distribution by learning to iteratively denoise corrupted samples, starting from pure noise. The corruption process is defined on training samples from p. Mathematically, the images are corrupted with Gaussian noise and are distributed according to qt(xtx0) := (αtx0, σ2 I), (18) where x0 corresponds to real image from p, 0 , αt, σ2 are positive real-valued scalars such that the signal-to-noise ratio SN := αt is monotonically decreasing. Additionally, the coefficients σ2 αT 1, σ2 1 are typically chosen such that xT (0, I). In this work, the number of corruption steps is fixed to 1000, as we study the pre-trained models from (Sauer et al., 2023b). Given this predetermined corruption process, the diffusion model learns to reverse it to recover clean data. The denoising process is implemented via distribution pθ(xt1xt). The simplest way to generate samples using pθ(xt1xt) is to first generate sample of pure noise xT (0, I), followed by iterative applications of pθ, yielding sequence xT , xT 1, ..., x1, x0, where x0 approximates samples from p. The vector θ represents the parameters of neural network that defines pθ(xt1xt). There exist many objectives to learn to reverse the corruption process (Ho et al., 2020; Kingma et al., 2023; Song & Ermon, 2020), but pθ is generally trained to minimize the Kullback-Leibler divergence between adjacent steps of the corruption process: DKL [qt(xt1xt, x0)pθ(xt1xt)] for {1, ..., }, where qt(xt1xt, x0) is Gaussian distribution whose mean and variance can be computed in closed-form using Bayes rule and the definition in eq. (18) (Ho et al., 2020). The denoising distribution pθ(xt1xt) is parameterized to be Gaussian. The Kullback-Leibler divergence between two Gaussians admits simple closed-form solution (Duchi John, 2020), hence, the objective can be efficiently implemented. The neural network used to parameterize pθ(xt1xt) can be trained to learn different quantities (Luo, 2022; Salimans & Ho, 2022; Karras et al., 2022). possible approach is to directly output the mean µt of pθ(xt1xt), while the variance is either fixed or learned as well. In this work, the neural network is parameterized to predict the noise added to the original sample during the forward process (eq. (18)). This is achieved by minimizing the objective w(t)ϵϵθ(αtx0 +σtϵ, t)2, where is weighting function (Ho et al., 2020). Once ϵθ is trained, the mean of pθ(xt1xt) is computed as 1 (xt σtϵθ) (Rombach et al., 2022). Since our primary goal is to analyze pre-trained diffusion αt model, we refer the interested reader to Rombach et al. (2022); Luo (2022); Salimans & Ho (2022) for more details. Latent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song & Ermon, 2020). However, training denoising network in pixel space is difficult and expensive (Hoogeboom et al., 2023). As such, Rombach et al. (2022) use pre-trained auto-encoder to first compress images, similar to VQGAN (Esser et al., 2021), and define diffusion process in the latent space instead of the pixel space. To make this difference clear they write pθ(zt1zt), in which now zt refers to noisy latent instead of noisy image. 29 Preprint Distilled diffusion for fast inference. To speed-up inference of latent diffusion models, Sauer et al. (2023b) distill pre-trained Stable Diffusion XL (SDXL) model (Podell et al., 2023). The distilled model is referred to as SDXL Turbo as it allows high-quality sampling in as little as 1-4 steps. The original SDXL model is trained with noise schedule of 1000 steps, but in practice, sampling with 20 to 50 steps still generates high-quality images. The speed-up in SDXL Turbo is achieved through combination of two objectives. First, Sauer et al. (2023b) define an adversarial game, similar to GANs (Goodfellow et al., 2014). The discriminator is implemented using lightweight classification heads on top of frozen features extracted at different layers of DINOv2 backbone (Oquab et al., 2024). Concretely, the objective of the discriminator is given by adv = Ex0 LD (cid:34) (cid:88) (cid:35) (1 Dk(Fk(x0)))+ + γR1(ϕ) + Eˆxθ (cid:34) (cid:88) (1 + Dk(Fk(ˆxθ)))+ , (19) (cid:35) k=1 k=1 where (x)+ = max(0, x) is the positive part, Fk denotes the k-th features tensor from the DINOv2 backbone, Dk the k-th classification head, ϕ is the discriminator parameters, R1 is an L2 penalty term on the norm of the gradients, introduced by Mescheder et al. (2018) and γ is scalar hyperparameter. Instead of traditional classification loss, Sauer et al. (2023b) use the hinge loss (Lim & Ye, 2017), following Sauer et al. (2021; 2023a). Finally, ˆxθ represents the prediction of the diffusion model being distilled for the ground-truth image x0 given noisy sample xt obtained through the forward diffusion process defined in eq. (18). Sauer et al. (2023b) found that distilling diffusion model using the adversarial objective only resulted in model with FID of 20.8. To further improve performance, they also distilled the noise predictions of the teacher model. Importantly, both the teacher and student models were initialized with the same pre-trained weights. After adversarial distillation (Sauer et al., 2023b), the model learns to map noise to samples in one step Neural network architecture. The denoising network of SDXL Turbo estimating pθ(zt1zt) is implemented using U-net similar to Rombach et al. (2022). The U-net is composed of downsampling path, bottleneck, and an up-sampling path. Both the down-sampling and up-sampling paths are composed of 3 individual blocks. The individual block structure differs slightly but both downand up-sampling blocks consist of residual layers as well as cross-attention transformer blocks. Finally, the bottleneck layer is also composed of attention and residual layers. As for the original U-net architecture (Ronneberger et al., 2015), the corresponding blocks in the up-sampling and and down-sampling path are connected via skip connection. Importantly, the text conditioning is achieved via cross-attention to text embeddings performed by in total 11 transformer blocks embedded in the down-, up-sampling paths and bottleneck. An architecture diagram displaying the relevant blocks can be found in App. Fig. 4. 30 Preprint (a) down.2.1 (b) up.0.1 Figure 9: We turn on the features from Fig. 8 on three unrelated prompts photo of colorful model, cinematic shot of dog playing with ball, and cinematic shot of classroom with excited students. 31 Preprint (a) Intervention history (b) Result Figure 10: Local edits showcase up.0.1s ability to locally change textures in the image without affecting the remaining image. Multiple consecutive interventions are possible (a). The first in (a) row depicts the original image and each subsequent row we add an intervention by drawing heatmap with brush tool and then turning on the feature labelling the row only on that area. The other number (240) is the absolute feature strength of the edit. Figure (b) shows the final result in full resolution (512x512). Preprint Figure 11: The images used by GPT-4o to generate captions for feature 0. From top to bottom: irrelevant images to feature 0; image progression from left to right, showing increasing activation of SAE feature 0, with low activation on the left and high activation on the right; Coldmaps representing the image progression; images corresponding to the highest activation of feature 0; Coldmaps corresponding to these highest activation images."
        }
    ],
    "affiliations": [
        "School of Computer and Communication Sciences EPFL Lausanne, Switzerland"
    ]
}