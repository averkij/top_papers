{
    "paper_title": "BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling",
    "authors": [
        "Li weile",
        "Liu Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer."
        },
        {
            "title": "Start",
            "content": "BlackGoose Rimer: Harnessing RWKV-7 as Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling Li weile, Liu Xiao alic2591709191@gmail.com liu.xiao.in@gmail.com 5 2 0 2 8 ] . [ 1 1 2 1 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7s time mix and channel mix components into the transformerbased time series model Timer [Liu et al., 2024], we achieve substantial performance improvement of approximately 1.13x to 43.3x and 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and //github.com/Alic development. Li/BlackGoose Rimer https :"
        },
        {
            "title": "1 Introduction\nTime series modeling stands as a fundamental pil-\nlar of machine learning [Wen et al., 2022], driving a\nwide array of applications such as forecasting, anomaly\ndetection, and decision-making across fields like fi-\nnance [Dong et al., 2024], healthcare, and environ-\nmental science1. With the rapid expansion of data\nin these domains, scaling time series models to effi-\nciently process large datasets has emerged as a criti-\ncal challenge. This issue parallels the scalability hur-\ndles encountered in large language models (LLMs),\nwhere computational efficiency and performance at\nscale are paramount. However, the distinct properties\nof time series data—namely, temporal dependencies,\nhigh dimensionality, and often the demand for real-\ntime analysis—pose unique difficulties that traditional",
            "content": "1This is an ongoing project Figure 1: The benchmarks reveal that Rimer, with significantly reduced parameter count of just 1.6 million, consistently outperforms or matches the performance of Timer, which relies on much larger 37.8 million parameters, across multiple metrics. approaches struggle to address effectively. Over the years, researchers have investigated numerous architectures to tackle these challenges [Zhang et al., 2025; Lang et al., ; Wang et al., ; Hou and Yu, 2024; ?], including transformers, Long Short-Term Memory networks (LSTMs), and Gated Recurrent Units (GRUs). While these models excel at capturing temporal patterns in smaller-scale settings, their efficiency and performance often degrade when applied to large datasets, highlighting the need for innovative solutions that can scale without compromising accuracy. This gap in capability has motivated the development of new methodologies tailored to the demands of large-scale time series modeling. In this paper, we introduce novel approach to meet these needs through RWKV-7, an architecture that integrates meta-learning into its state update mechanism [Grazzi et al., 2024]. RWKV-7 features two core innovationstime mix and channel mix [Peng et al., 2023] which enhance its ability to model complex temporal dynamics while maintaining computational efficiency. By embedding these components into the transformer-based time series model Timer, we achieve remarkable 1.13x to 43.3x improvement in performance and 4.5x reduction in training time compared to existing methods with 1/23 parameters. Furthermore, our approach leverages fewer parameters, offering lightweight yet powerful solution for scaling time series models. Our contributions in this work are twofold: Revisiting RNNs for Time Series with RWKV-7: We reintroduce the recurrent neural network (RNN) paradigm to time series modeling by adopting RWKV-7, novel architecture tailored for largescale time series data. This approach harnesses the sequential processing strengths of RNNs while overcoming their traditional challenges in handling extensive datasets, resulting in improved scalability and performance. Benchmarking Against Transformer-Based Models: We perform comprehensive benchmarks to evaluate our RWKV-7-based model against previous transformer-based time series models. Our experiments highlight significant improvements, showcasing the effectiveness of our approach in terms of predictive accuracy and computational efficiency."
        },
        {
            "title": "2.2 Test time scaling\nIn the era of big data, time series models have become\nessential tools across industries like finance and health-\ncare for tasks such as forecasting and anomaly detec-\ntion. However, as the volume and velocity of time se-\nries data increase, efficiently scaling these models dur-\ning the test phase—when predictions are made on new,\nunseen data—poses a significant challenge. Traditional\nmethods often struggle with heightened computational\ndemands and potential accuracy losses when process-\ning longer sequences, higher-frequency data, or addi-\ntional features. This paper presents a novel framework",
            "content": "Figure 2: The RWKV-7 architecture is RNN model that processes sequences using repeated RWKV blocks, each containing:1.A time mix block to blend current and past information.2.WKV heads for attention-like processing with an internal state to maintain memory.A channel mix block to transform the data further. to address test time scaling [Sun et al., 2024] in time series models, utilizing adaptive computational techniques to maintain robust performance while preserving efficiency. Through extensive testing on diverse datasets, we showcase substantial gains in both speed and predictive accuracy, delivering scalable solution for real-world applications. This work pushes forward the field of time series modeling and offers actionable insights for deploying these models in large-scale, realtime settings."
        },
        {
            "title": "3 Methodology\n3.1 PRELIMINARIES\nThe core structure of RWKV-7 (figure 2) is dynamic evo-\nlution WKV state, have time mix and channel mix two\nmain components.",
            "content": "the core state update rule: Statet = Statet1 (cid:16) diag(wt) ˆκT (at (cid:17) ˆκt) + vT kt at (1)"
        },
        {
            "title": "3.2 Implicit RWKV-7 layers\nDEQ (Deep equilibrium models) [Bai et al., 2019] mod-\nels aim to define layers as fixed-point solutions, where\nthe output (or state) of a layer is the equilibrium point\nof a function, rather than being computed through a\ntraditional sequential update. This is particularly use-\nful for recurrent or transformer-like architectures, as it",
            "content": "allows for infinite-depth behavior without explicitly unrolling the recurrence. In DEQ framework, our objective is to define Statet (or related hidden state ht) as the solution to fixedpoint equation of the form: RMSE MAE Timer-37.8M 0.6488 Rimer-1.6M 0.2409 0.2127 0.0814 MAPE R2 0.61% 0.9755 0.81% 0. Table 1: ECL = (z, x) where (2) is the equilibrium state, represents the input (including previous states and external inputs), and is function (often neural network layer) parameterized by learnable weights. For recurrent setting, this can be adapted to incorporate temporal dynamics, where the fixed-point equation depends on both the current input and the previous state. Instead of explicitly computing Statet from Statet1, we redefine the update implicitly. Lets denote ht = Statet, and consider the original equation as part of transformation that we want to express as fixed-point problem. The original update can be written as: ht = (ht, Statet1, xt) by introducing self-recurrent term, (cid:16) Statet1 at κt ht + (cid:16) vT ht = φ + (cid:17) (diag(wt) κT (at (3) (4) ˆκt)) (cid:17) where , , are Learnable weight matrices,Activation φ is relu to make non-linear.This DEQ form preserves the core dynamics state while reformulating it as an implicit layer,using latent-space iterations which can greatly improve expressive and efficiency."
        },
        {
            "title": "4 Evaluation\nTo evaluate the performance of our proposed Rimer\nmodel against the baseline Timer, we conducted exper-\niments across four diverse datasets,ELC, ETTH, Traf-\nfic, and Weather using metrics including RMSE, MAE,\nMAPE, and R², with datasets that are publicly avail-\nable in our Rimer repository. Our Rimer model, fea-\nturing meta-learning in its state update rule and im-\nplemented with Triton operators, was trained on a\nLinux ROCm platform with AMD GPUs (Radeon Pro\nW7900 for training and Radeon RX6750XT for infer-\nence), achieving a 4.5x training time speedup com-\npared to Timer due to its 1.6 million parameters ver-\nsus the latter’s 37.8 million; this setup highlights our\nmodel’s unique strength in broad hardware compati-\nbility, supporting training and inference across AMD\nGPUs, NVIDIA GPUs, and CPUs, a versatility enabled\nby Triton operators and developed on the ROCm plat-\nform.",
            "content": "The evaluation demonstrates that Rimer-1.6M , despite its significantly smaller size, delivers competitive or superior performance compared to the much larger Timer-37.8M model across multiple time series modeling tasks. This highlights its remarkable efficiency and effectiveness. RMSE MAE Timer-37.8M 0.5770 Rimer-1.6M 0.0133 0.4050 0.0112 MAPE R2 6.5% 0.9968 0.16% 0.9998 Table 2: ETTH RMSE MAE Timer-37.8M 0.0055 Rimer-1.6M 0.0025 0.0015 0.0006 R2 MAPE 19.94% 0.8955 0.9838 4.01% Table 3: Traffic RMSE MAE Timer-37.8M 6.1765 Rimer-1.6M 5.4311 3.6839 1.3621 MAPE R2 0.88% 0.8411 0.34% 0.8794 Table 4: Weather These results reveal that Rimer-1.6M, with just 1.6 million parameters, not only competes with but often outperforms Timer-37.8M, which relies on 37.8 million parameters. Its consistently higher values across all datasets demonstrate superior ability to capture temporal patterns, while its lower MAPE in ETTH and Traffic datasets suggests greater robustness to relative errors, potentially due to better handling of outliers or smaller values. The lightweight architecture of Rimer1.6M over 23 times smaller than Timer-37.8M, also implies significant computational advantages, such as faster training and inference times, making it highly practical for large-scale applications. In conclusion, the benchmark underscores Rimer1.6M as compelling alternative to traditional transformer-based models like Timer-37.8M. Its ability to achieve strong performance with fraction of the parameters highlights the potential of RWKV-based architectures for efficient, effective, and scalable time series modeling, addressing both performance demands and resource constraints in real-world forecasting tasks."
        },
        {
            "title": "5 Conclusion\nThe integration of RWKV-7 core components into the\nTimer architecture by replacing its transformer back-\nbone has yielded a significant performance boost, as\ndemonstrated through our experimental evaluations.\nThis enhancement is attributed to the expressive and\nefficient nature of the RWKV-7 architecture, which,",
            "content": "despite being fundamentally type of recurrent neural network (RNN), effectively combines the strengths of traditional RNNs with the scalability and parallelization benefits typically associated with transformers. This hybrid design enables Rimer to outperform the original Timer across diverse datasets, leveraging its lightweight structure (1.6 million parameters versus 37.8 million) and broad hardware compatibility to achieve 4.5x training time speedup. These results underscore the potential of RWKV-7 as promising alternative for large-scale time series modeling, offering compelling balance of performance, efficiency, and adaptability for future research and real-world applications."
        },
        {
            "title": "Acknowledgments",
            "content": "We extend our gratitude to Cao Qihuan for his generous guidance, as well as to Li Zhiyuan and the RWKV community for their contributions to RWKV-FLA."
        },
        {
            "title": "References",
            "content": "[Bai et al., 2019] Shaojie Bai, Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in neural information processing systems, 32, 2019. [Dong et al., 2024] Chengqi Dong, Zhiyuan Cao, Kevin Zhou, and Jia Liu. Dft: dual-branch framework of fluctuation and trend for stock price prediction. arXiv preprint arXiv:2411.06065, 2024. [Grazzi et al., 2024] Riccardo Grazzi, Julien Siems, Jorg KH Franke, Arber Zela, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues. arXiv preprint arXiv:2411.12537, 2024. [Hou and Yu, 2024] Haowen Hou and Richard Yu. Rwkv-ts: Beyond traditional recurrent neural nettime series work for arXiv preprint arXiv:2401.09093, 2024. tasks. [Khosravi et al., 2023] Maryam Khosravi, Ali Reza Khanteimouri, Seyed Mohammad Reza Azghani, Seyed Mohammad Mehdi Hosseini, and Seyed Mohammad Mehdi Hosseini. Using meta-learning to recommend an appropriate time-series forecasting model. BMC Public Health, 23(1):2432, 2023. [Lang et al., ] Jinwei Lang, Li-Zhuang Yang, and Hai Li. Multi-modal dynamic brain graph representation learning for brain disorder diagnosis via temporal sequence model. Available at SSRN 5114041. [Liu et al., 2024] Yong Liu, Haoran Zhang, Chenyu Li, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Timer: Generative pre-trained transformers are large time series models, 2024. [Peng et al., 2023] Bo Peng, Bo Li, Wenhan Dai, Shujian Zhang, Jianzhong Qi, Wenjun Zeng, and Xuewei Li. Rwkv: Reinventing rnns for the transformer era, 2023. [Prudencio and Ludermir, 2004] Ricardo B. C. Prudencio and Theresa Ludermir. Meta-learning approaches to selecting time series models. Neurocomputing, 59:145170, 2004. [Sun et al., 2024] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. [Wang et al., ] Qingyong Wang, Guohao Lv, Lichuan Gu, et al. Chembr: novel generative model based on bidirectional molecular ring constraints. [Wen et al., 2022] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: survey, 2022. [Yueyu et al., 2025] Lin Yueyu, Li Zhiyuan, Peter Yue, and Liu Xiao. Arwkv: Pretrain is not what we need, an rnn-attention-based language model born from transformer. arXiv preprint arXiv:2501.15570, 2025. [Zhang et al., 2025] Xin Zhang, Liangxiu Han, Stephen White, Saad Hassan, Philip Kalra, James Ritchie, Carl Diver, and Jennie Shorley. Tabulatime: novel multimodal deep learning framework for advancing acute coronary syndrome prediction through environmental and clinical data integration. arXiv preprint arXiv:2502.17049, 2025."
        }
    ],
    "affiliations": []
}