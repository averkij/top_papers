{
    "paper_title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
    "authors": [
        "Yang Liu",
        "Chuanchen Luo",
        "Zimo Tang",
        "Yingyan Li",
        "Yuran Yang",
        "Yuanyong Ning",
        "Lue Fan",
        "Junran Peng",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, a novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish a long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 4 0 9 8 1 . 6 0 5 2 : r TC-Light: Temporally Consistent Relighting for Dynamic Long Videos Yang Liu1,2 Chuanchen Luo3 Zimo Tang6 Yingyan Li1,2 Yuran Yang5 Yuanyong Ning5 Lue Fan1,2 Junran Peng4 Zhaoxiang Zhang1,2 1NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences, 3Shandong University 2University of Chinese Academy of Sciences 5Tencent 4University of Science and Technology Beijing 6Huazhong University of Science and Technology {liuyang2022, liyingyan2021, lue.fan, zhaoxiang.zhang}@ia.ac.cn u202315173@hust.edu.cn yangyuran@bupt.edu.cn yyning@tencent.com chuanchen.luo@sdu.edu.cn jrpeng4ever@126.com"
        },
        {
            "title": "Abstract",
            "content": "Editing illumination in long videos with complex dynamics has significant value in various downstream tasks, including visual content creation and manipulation, as well as data scaling up for embodied AI through sim2real and real2real transfer. Nevertheless, existing video relighting techniques are predominantly limited to portrait videos or fall into the bottleneck of temporal consistency and computation efficiency. In this paper, we propose TC-Light, novel paradigm characterized by the proposed two-stage post optimization mechanism. Starting from the video preliminarily relighted by an inflated video relighting model, it optimizes appearance embedding in the first stage to align global illumination. Then it optimizes the proposed canonical video representation, i.e., Unique Video Tensor (UVT), to align fine-grained texture and lighting in the second stage. To comprehensively evaluate performance, we also establish long and highly dynamic video benchmark. Extensive experiments show that our method enables physically plausible relighting results with superior temporal coherence and low computation cost. The code and video demos are available at https://dekuliutesla.github.io/tclight/."
        },
        {
            "title": "Introduction",
            "content": "Lighting and its interaction with both real and synthetic environments fundamentally shapes how humansand embodied agentsperceive the world. The ability to relight camera-captured image sequences, especially in complex, highly dynamic scenes, is invaluable for range of downstream applications, spanning from filmmaking [47] to augmented reality [34]. Crucially, this capability also benefits embodied AI: by adapting the illumination distribution in video while preserving intrinsic image details, it helps bridge the sim-to-real gap and enable real-to-real transfer, thus unlocking access to massive high-quality data that is essential for scaling up embodied intelligence. Despite its importance, video relighting remains highly challenging problem, particularly when camera motion is highly dynamic and foreground objects frequently enter and exit scenes, as shown in Fig. 1. Most existing relighting techniques [58, 34, 22, 46, 28, 55] are tailored for static images. As shown in Sec. 4.2, naively inflating them to video model with existing zero-shot strategies struggles to balance the consistency and quality. Moreover, the considerable training cost and scarcity of video lighting datasets hinder fine-tuning pretrained model for this task. Besides, Correponding author. Preprint. Under review. Figure 1: Relighting results on long videos under various dynamic scenes, averaging 256 frames per clip. Though the video involves frequent changes of foreground objects (row (a)), highly dynamic camera motions (row (b)), the TC-Light realizes consistent and physically plausible relighting results. Row (c) also shows its potential to mitigate the sim2real gap for synthetic renderings. though video relighting models are emerging, they are either restricted on portrait video [57, 9, 6], or burdened by considerable computation overhead [60, 16] on long video, as validated in Sec. 4.2. To address the limitations outlined above, we propose TC-Light. We utilize the SOTA image relighting model IC-Light [58] as the baseline, and inflate it to video model in zero-shot manner with incorporation of our proposed decayed multi-axis denoising. It provides preliminary video relighting result. The core innovation of TC-Light lies in two-stage post-optimization framework that substantially improves temporal consistency. The first stage introduces per-frame appearance embedding to compensate for exposure discrepancy. It is optimized with photometric loss against the preliminarily relighted video and flow-based loss between adjacent frames. This enforces global illumination consistency and facilitates consequent optimization. The second stage compresses the output to canonical representation, i.e., Unique Video Tensor (UVT), according to priors including optical flow and depth of the source video. UVT is then optimized by minimizing the warping error across decompressed frames while aligning the content with the first stage result. As shown in Tab. 2, our optimization procedure is extremely efficient and introduces minimal VRAM overhead. To comprehensively assess the effectiveness of our model, we introduce challenging benchmark tailored for complex and highly dynamic scenes. It comprises 58 videos of averagely 256 frames per clip, spanning both indoor and outdoor environments, realistic and synthetic settings, and wide range of lighting and weather conditions. Extensive experiments demonstrate that our method achieves high-quality, temporally consistent video relighting while maintaining low computational overhead, highlighting its strong potential for downstream applications such as embodied AI. Our main contributions are as follows: 2 novel optimization-based video relighting paradigm for long videos with high and complicated dynamics, significantly improving the temporal consistency of the relighting result. We establish new long-video relighting benchmark characterized by high motion dynamics and broad scene diversity, covering various environments and data domains. Extensive experiments validate that our method achieves SOTA performance in producing temporally consistent, naturally relighted videos with minimal computational cost."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Learning-based Illumination Editing Over the past few years, deep neural networks have become one of the main forces behind research in the field of illumination control. Pioneering works [53, 40, 42, 12] train convolutional encoderdecoder networks on light-stage data. The learned prior knowledge enables models to relight portrait according to the specified light conditions. More recently, large diffusion-based generators have gained popularity for illumination editing. LightIt [31] explicitly conditions the diffusion process on estimated shading and normal maps, giving fine-grained lighting control ability, while SwitchLight [28] incorporates physics-guided architecture to simulate light-surface interactions better. [59, 4] leverage video foundation models to generate realistic lighting variations over static image. IC-Light [58], the current state of the art, learns illumination mixture and decomposition from large quantity of data. Building on these advances in image relighting, video relighting has started to gain traction. [57, 9] learns to disentangle light and intrinsic appearance on portrait videos. [6] represents talking faces as relightable NeRFs guided by predicted albedo and shading features. Extending IC-Light, Light-A-Video [60] introduces zero-shot cross-frame attention modification, while RelightVid [16] trains temporally inflated IC-Light with carefully designed video relighting dataset. However, these methods are either restricted to portrait scenarios or struggle with computational efficiency on long videos. In contrast, our model delivers high-quality relighting with strong temporal consistency and low computation cost, even in complex and highly dynamic scenes. 2.2 Diffusion-based Video Editing The diffusion model [19] has become the go-to model for visual domain transfer and content editing. Based on training paradigms, recent advancements can be grouped into three categories: (i) trainingbased models extend pretrained image diffusion models with temporal layers and are trained on large-scale video datasets, such as [36, 8, 39, 37, 44, 56]. CCEdit [17] and FlowVid [35] further integrate depth and flow cues for improved consistency and control. (ii) training-free models mainly rely on cross-frame attention to enforce temporal coherence. TokenFlow [18] and FLATTEN [11] guide attention using estimated optical flow. RAVE [23] enhances latent interactions by denoising over reorganized latent grid, while Slicedit [10] uses spatiotemporal slices to inject motion priors. VidToMe [33], on the other hand, exploits temporal redundancy through token merging and unmerging. (iii) one-shot-tuned models typically learn canonical video representation in few iterations and propagate its edits across frames. StableVideo [7] learns to represent video as foreground and background atlas. CoDeF [41] learns hash table and decoding MLP to map frames to single canonical image. Video-3DGS [51] adapts deformable 3DGS [25] to model input video. Our method combines (ii) and (iii) and proposes an explicit, compact, and efficient canonical representation, i.e., Unique Video Tensor. It enables optimization to be finished within several minutes, which is much faster than 10-30 minutes cost [51] of CoDF and Video-3DGS. Our method also inherits the diffusion model design from training-free algorithms to reduce overall memory and time cost, enabling the processing of long videos."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Task Setting. As shown in Fig. 2, we take RGB video as input. The axes of the video space-time volume are denoted by (x, y, t), where xy planes correspond to video frames and yt planes are defined as spatiotemporal slices [10]. Since the camera motion is highly dynamic, the target illumination 3 Figure 2: TC-Light overview. Given the source video and text prompt p, the model tokenizes input latents in xy plane and yt plane seperately. The predicted noises are combined together for denoising (cf. Sec. 3.2). Its output then undergoes two-stage optimization to enhance temporal consistency of illumination and texture, which are respectively detailed in Sec. 3.3.1 and Sec. 3.3.2. can no longer be simply appointed by static image or an HDR environment map. Due to superior flexibility and operability, we use textual prompts as the control signal and relight the entire frame. Latent Diffusion Models (LDMs). Denoising Diffusion Probabilistic Models (DDPMs) [19] are class of generative models that aim to recover target data distribution through an iterative denoising process. Due to the high computational cost of operating directly in pixel space, LDMs [45, 48, 50] perform diffusion in lower-dimensional latent space. Given clean image x0, and pretrained autoencoder {E(), D()}, LDMs first encode the image into latent space z0 = E(x0). The forward diffusion process then gradually corrupts z0 with Gaussian noise ϵ over time steps τ = 1, ..., zτ = ατ z0 + 1 ατ ϵ, (1) where {ατ } is monotonically decreasing noise schedule. The reverse process begins from pure noise zT (0, I). With guidance from control signal (image, text, depth, etc) c, the trained UNet [49] ϵθ estimates the noise direction and progressively removes the noise from zT . After the final denoising step, the estimated clean latent ˆz0 is decoded by D() to obtain the generated image ˆx0 = D( ˆz0), which approximates the training distribution. 3.2 Lifting Image Diffusion Model to Video Space Considering outstand ability in physical plausibility and intrinsic property preservation, we adapt IC-Light [58] into zero-shot video diffusion model. Concretely, we (i) enhance its diffusion blocks to capture spatiotemporal dependencies and (ii) introduce consistency prior from original frames. For (i), we apply the token merging and unmerging technique of VidToMe [33] to self-attention blocks. It divides the video frames into chunks and applies intra-chunk local token merging and inter-chunk global token merging, enabling shortand long-term consistency. The derived model ϵθ serves as the basis of (ii). Since it reduces the token count fed to the self-attention module, the computation cost is significantly decreased. For full details, please refer to the original VidToMe paper [33]. For (ii), we propose decayed multi-axis denoising. Similar to Slicedit [10], the denoiser has two components with shared weights ϵxy θ (, p) that tokenizes each frame and merges tokens from local temporal slots, while ϵyt θ (, ) tokenizes the yt planes (cf. Sec. 3.1) and merges tokens from local image width slot. Note that ϵxy θ conditions on target prompt p, while ϵyt θ takes empty prompt as input (making the denoiser unconditional). The noises separately predicted by two parts according to the same input latents are combined together [10] 4 ϵV θ (, p) = γϵxy θ (, p) + (cid:112)1 γϵyt θ (, ), (2) where hyperparameter γ [0, 1] balances effect from ϵyt θ would overly biases texture and lighting toward the source video, and therefore lead to unnatural relighting results, as validated in Fig. 3 and Fig. 4. To alleviate this problem, we replace γ with timestepdependent γτ that exponentially decays during denoising. To further align predicted noise from ϵyt θ to that of ϵxy θ , we use Adaptive Instance Normalization (AIN) [20] to adjust feature statistics θ . However, the unconditional ϵyt ϵV θ (, p) = γτ ϵxy (cid:99)ϵyt θ (, ) = σϵxy θ θ (, p) + (cid:112)1 γτ (cid:99)ϵyt θ (, ), (cid:32) ϵyt (cid:33) θ (, ) µϵyt σϵyt + µϵxy θ θ , θ (3) (4) where µ and σ are the channel-wise mean and standard deviation of each frame. This design preserves motion guidance from the source video while reducing unwanted texture and lighting bias, as validated by ablation studies in Sec. 4.3. The output denoised video is denoted as {It}. 3.3 Post Optimization for Temporal Consistency Although the video diffusion extension in Sec. 3.2 has introduced spatial-temporal awareness and motion prior from source video, noticeable illumination and texture flicker persist. To efficiently remove these artifacts, we introduce two-stage post-optimization framework, as illustrated in parts (b) and (c) of Fig. 2. 3.3.1 Stage I: Exposure Alignment As shown in part (b) of Fig. 2, the first stage introduces per-frame appearance embedding Et to compensate for exposure misalignment between adjacent frames. Inspired by [26], we model Et as 3 4 affine transformation matrix, initialized to the identity and optimized via Adam [29]. Its supervision combines photometric term with flow-warp alignment term using hyperparameter λe Lexposure = (1 λe) Lphoto (cid:16) It, It (cid:17) + λeL1 (cid:16) It Mt, Warpt+1t (cid:16) It+1 (cid:17) (cid:17) , Mt (5) where the homogeneously transformed pixel color It (x, y) = Et [It (x, y) 1]T . The photometric loss Lphoto is the weighted sum of L1 loss and D-SSIM loss [26], ensuring the transformed frame retains its original content and structure. The second term warps the next frame back to the current timestamp t, according to forward and backward flows Ff wd,t and Fbwd,t estimated through MemFlow [14] or provided by the dataset. Then it applies an L1 penalty L1 to align their exposures. To mask out regions with unreliable flow or occlusion, we apply soft mask Mt Mt = sigmoid (β (ξf low Ef low)) sigmoid (β (ξrgb Ergb)) , Ef low = Norm (cid:0)Fbwd,t + Warpt1t (Ff wd,t1)(cid:1) , Ergb = It Warpt+1t (It+1) . (6) (7) Here, β is constant scaling factor, ξf low and ξrgb are thresholds set from the statistics of error map Ef low and Ergb. This soft mask is also applied in the second stage of optimization. As shown in Tab. 3, soft masking outperforms the hard one in both temporal consistency and prompt alignment. 3.3.2 Stage II: Optimization over Unique Video Tensor In the second stage, we refine illumination and texture details. Compared with vanilla video, its canonical representation can incorporate spatial-temporal priors and facilitate consistency [41, 51]. But popular NeRF or 3DGS are too complex and costly for learning (cf. Sec. 2.2). Instead, we compress the video to one-dimensional RGB vector of shape (N, 3), as shown in part (c) of Fig. 2. Specifically, we define d-dimensional index κ(x, y, t) for each pixel based on priors extracted from the source video. An example index could be [22, 127, 0, 255], where the first element is flow 5 Table 1: Datasets [38, 15, 52, 13, 2, 27, 32, 24] contained in established benchmark. Nseq. and Nf rames denote number of sequence and average frames. C, F, D, respectively denote RGB image, Optical Flow, Depth, Instance Segmentation. Notably, AgiBot here denotes AgiBot Digital World. Due to lacking of extrinsics, its depth is indeed not applicable. Only DRONE is self-collected data. Datasets SceneFlow CARLA Waymo NavSim AgiBot DROID InteriorNet SCAND DRONE Vehicle Vehicle Vehicle Agent Synthetic Modality Nseq. Nf rames Width Height Vehicle C,F,D,S 4 300 960 512 C,D,S 8 208 960 536 5 198 960 640 5 250 960 Robot 8 305 640 480 Robot 12 243 960 536 Robot C,D,S 5 300 640 480 Robot Drone 6 289 960 536 5 213 1280 720 ID (grouping pixels with shared optical flow), and the rest are 8-bit quantized RGB values. It is also allowed to extend this 4-element index to more elements with voxel coordinate (from depth projection) or any other cues that indicate spatialtemporal similarity and locality. All pixels with identical κ are gathered via averaging to form one element of the one-dimensional vector, where is the number of unique κ. Take the source video {I in } as an example, the gathering and scattering operations are formulated as (κn) = Avg (cid:0)(cid:8)I in (x, y) κ(x, y, t) = κn (cid:9)(cid:1) , Iin (x, y) = (κ(x, y, t)) , (8) (x, y) reconstructs the original in where is referred to as the Unique Video Tensor (UVT). With an appropriate definition of κ, the scattered Iin (x, y) with minimal information loss, as validated in Tab. 4. For relighting, the ideal edited video frames must preserve consistent motion and intrinsic image details with the source; thus, they share the same index tensor κ for UVT representation. Accordingly, we compress the first-stage output It (x, y) into via Eq. (8), which then serves as the primary optimization target. This formulation not only facilitates optimization but also naturally embeds spatial-temporal similarity priors (cf. Sec. 4.3). With CUDA parallelism, the gathering and scattering process can be performed instantly. The optimization of is supervised by Lunique = λtvLtv (cid:17) (cid:16)It + (1 λu) LSSIM (cid:16)It+1 (cid:16)It Mt, Warpt+1t +λuL1 (cid:17) (cid:16)It, It (cid:17) Mt + (cid:17) , (9) where It (x, y) = (κ(x, y, t)), and λtv and λu [0, 1] balance the loss terms. The total variation loss Ltv suppresses noise. Notably, Eq. (9) applies SSIM loss instead of photometric loss. This leaves space to fine-grained appearance and illumination adjustment without altering image structure. Finally, the optimized (cid:98)U is used to reconstruct (cid:98)It(x, y) according to Eq. (8) as the final output."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setting Implementation Details. Following IC-Light [58], we apply = 25 sampling steps and classifierfree guidance scale of 2.0. When inflated to video model with VidToMe [33], the local and global token merging ratios are 0.6 and 0.5, respectively, to accommodate high video dynamics. In our decayed multiaxis denoising strategy, the initial γτ is set to 0.2 and decays exponentially to 0.002 until the final sampling step. For the post-optimization stages, we use Adam [30] as optimizer and run 35 epochs in the first stage and 70 in the second with batch size of 16, ensuring fast yet sufficient convergence. κ(x, y, t) mainly contains quantized RGB and estimated masked flow, and optionally depth if provided. Emperically, the weighting coefficients λtv is set to 0.01, λe and λu are set to 0.8. Following [25], the learning rate in the first stage decays from 0.01 to 0.001, while the second stage uses fixed learning rate of 0.05. Additional details are included in the Appendix. Dataset. To comprehensively evaluate the generation capability, we collect video clips with high motion dynamic and broad scene diversity. This benchmark, as detailed in Tab. 1, covers scenarios 6 Figure 3: Qualitative comparison of results. The proposed TC-Light avoids unnatural relighting like Slicedit [10] and COSMOS-Transfer1 [3] in (a) and blurring like [3] in (b), or inconsistent illumination like per-frame IC-Light [58] and VidToMe [33] as highlighted by the red squares. like autonomous driving, robot manipulation, and navigation, as well as drone flight. It includes data collected from synthetic and realistic environments under various weather conditions. Each clip is long video with high dynamics and on average 256 frames, making it extremely challenging. To obtain edit prompts, we use some prompts from [58] and generate others using COSMOS [1]. Metrics. Following prior works [43, 33, 23, 56], we assess the relighting performance along following four dimensions: (i) Temporal consistency is quantified via motion smoothness (Motion-S) [21] and structural warping error (Warp-SSIM). Motion-S evaluates the continuity and physical plausibility of motion in the edited sequence, whereas Warp-SSIM computes the SSIM between frame and its warped neighbors using flow from RAFT [54]. (ii) Textual alignment is measured by average CLIP embedding similarity between the text prompt and all edited frames (CLIP-T). (iii) User preference is evaluated by study on 19 randomly selected videos and 65 valid submissions collected. Participants choose their preferred relighting results among our method and established baselines, from which we derive the BradleyTerry preference rate (User-PF) [5]. Additional details are included in the Appendix. (iv) Computation efficiency is reported in terms of runtime speed (FPS) and peak GPU memory consumption (VRAM) during editing. All experiments are conducted on 40GB A100 GPU. Additionally, to appraise the reconstruction quality of UVT, we report average PSNR, SSIM, and LPIPS between the original and reconstructed frames. Baselines. We benchmark our approach against several recent state-of-the-art techniques, whose code is publicly available at the time of writing. These include per-frame IC-Light (denoted as IC-Light*) and its video extensions, Light-A-Video [60] and RelightVid [16]. We also implement two IC-Light variants by incorporating leading zero-shot video editing methods: VidToMe [33] and Slicedit [10]. For fairness, we disable the image downsampling to 512 512 resolution before the diffusion step in Slicedit. In addition, we compare two advanced training-based methodsVideoDirector [56] and COSMOS-Transfer1 [3]. For the latter, due to out-of-memory (OOM) issues when applying full multimodal control on long videos, we employ only its edge branch, which offers favorable balance between preserving image details and adhering to relighting prompts. 4.2 Comparison with SOTA Quantitative and qualitative comparisons with state-of-the-art methods are reported in Tab. 2 and Fig. 3, respectively. The result indicates that per-frame relighting (IC-Light*) follows prompts well and produces physically plausible illumination, but the adapted illumination suffers from severe flicker, as shown in columns (a) and (b) of Fig. 3. IC-Light would even randomly hallucinate 7 Table 2: Comparison with existing methods. \"OOM\" here means the method is unable to finish the task due to an out-of-memory error. For fair comparison, the base models of VidToMe and Slicedit are replaced with IC-Light here. Ours-light applies post-optimization to VidToMe, while Ours-full further introduces decayed multi-axis denoising. Experiments are conducted on 40G A100. The best and the second best of each metric are separately highlighted in red and blue. Method Motion-S WarpSSIM CLIP-T User-PF FPS Time(s) VRAM(G) IC-Light* [58] VidToMe [33] Slicedit [10] VideoDirector [56] Light-A-Video [60] RelightVid [16] Cosmos-T1 [3] Ours-light Ours-full 94.52% 95.38% 96.48% OOM OOM OOM 96.83% 97.39% 97.80% 71.22 73.69 85.37 OOM OOM OOM 83. 88.53 91.75 0.2743 0.2731 0.2653 OOM OOM OOM 0.2529 0.2700 0.2679 10.97% 0.123 6.97% 0.409 18.39% 0.122 OOM OOM OOM OOM OOM OOM 16.06% 0. 23.66% 0.359 23.96% 0.204 2075 626 2101 OOM OOM OOM 2543 771 1255 16.49 11.65 17.87 OOM OOM OOM 34. 14.36 14.37 non-existent objects in textureless regions (cf. column (c) of Fig. 3), further degrading consistency. Extending IC-Light* with VidToMe [33] yields modest gains in temporal coherence but dramatically lowers computation cost for long videos, so we adopt it as our primary baseline. Slicedit [10] significantly suppresses flicker and hallucinations, yet its computation overhead exceeds that of IC-Light*. Besides, its output remains overly biased by the original appearance of the source video. As result, it produces unnatural relighting in many cases, as shown in column (a) of Fig. 3. We also evaluated the T2V-model-based video editing approach [56] and concurrent video relighting techniques [60, 16]. Unfortunately, they all failed on long clips due to OOM errors caused by high computation resource demands. For the same reason, Cosmos-Transfer1 [3] can only operate in single-modality mode under GPU constraints, yet still requires over 30 GB GPU memory and more than 30 minutes per clip. Moreover, on video with high dynamics, it suffers from more severe blur and loss of details, as shown in columns (a) and (b) of Fig. 3. These failures are likely because Cosmos-Transfer1 is limited to the data domain of its training data, which contains less varied, moderately dynamic videos. In contrast, our TC-Light first enables physically plausible relighting on long videos with high dynamics, while outperforming all baselines in temporal consistency and preference rate by large margin, as shown in Tab. 2. The light version adds only 2.4 minutes and 2.7 GB of VRAM overhead compared to the VidToMe baseline, while faithfully preserving object identity, albedo, and adherence to text prompts, as shown in Fig. 3. Incorporating our decayed multi-axis denoising further enhances temporal coherence, with modest trade-off in efficiency and quality. Limited by page, we provide additional visualization and performance of different scenarios types in the Appendix. 4.3 Ablation This section analyzes the contribution of each component in our model. The first stage optimization, as shown in Tab. 3 and Fig. 4, markedly boosts consistency by aligning cross-frame exposure. The 6-7th rows of Tab. 3 also illustrate that, initializing UVT optimization from the first-stage results converges more efficiently than directly optimizing UVT for the same overall epochs. The second stage optimization, as shown in Tab. 3, further reinforces temporal coherence. Tab. 4 confirms that UVT can compress the source video with near-zero loss, which underpins our design in Sec. 3.3.2. Using UVT as the second-stage target not only boosts consistency but also cuts computational overhead. Additionally, replacing hard mask with soft mask consistently improves both WarpSSIM and CLIP-T metrics, demonstrating its importance. Incorporating the depth cues alongside UVT yields more compact representation (also in Tab. 4), which aids illumination alignment and release computation burden. In contrast, instance segmentation masks provide no clear benefit and are thus omitted from the final implementation. For the diffusion module, multi-axis denoising notably enhances temporal consistency. However, it tends to inherit appearance distribution from the source video, causing drift from the target prompt and sometimes unnatural lighting, as shown in Fig. 4. The introduced AIN and weight decay mitigate these issues, achieving promising balance between consistency and faithful prompt alignment. 8 Table 3: Ablation over module component. The experiments here are conducted on CARLA [15] and the Interiornet [32] subset, which both provide depth and instance mask as priors. There are 13 sequences in total and 254 frames on average, covering scenes of indoor and outdoor scenarios. The gray row denotes modification that is aborted and not included in the following experiments. Method Motion-S WarpSSIM CLIP-T Baseline +1st Stage +2nd Stage(video) +2nd Stage(UVT) +soft mask from scratch(UVT) +depth +instance +multi-axis +AIN +weight decay 94.51% 95.71% 96.40% 96.44% 96.44% 96.30% 96.56% 96.50% 98.41% 98.38% 97.75% 77.60 81.29 90.58 91.04 91.05 90.65 91.12 91.01 95.52 95.44 93. 0.2871 0.2868 0.2876 0.2866 0.2868 0.2866 0.2863 0.2851 0.2813 0.2832 0.2865 FPS 0.693 0.651 0.552 0.563 0.559 0.552 0.569 0.545 0.310 0.310 0.310 Time(s) VRAM(G) 364 388 460 449 452 458 444 462 805 805 805 10.63 11.33 13.53 11.81 11.81 12.40 11.57 11.67 11.57 11.57 11.57 Table 4: Ablation over Unique Video Tensor (UVT). Here, %Cmpr is the compression rate after applying UVT on the source video. The subscripts and f+d indicate that, besides color cues, the UVT representation incorporates optical flow cues and both flow and depth cues, respectively. Scene CARLA InteriorNet %Cmprf %39.2 %49.0 SSIMf PSNRf LPIPSf 50.71 0.9940 46.17 0.9908 0.025 0. #Cmprf +d %29.2 %12.8 SSIMf +d PSNRf +d LPIPSf +d 48.98 40.86 0.9925 0.9755 0.028 0.047 4.4 Limitation and Discussion Despite achieving impressive results, our method is still limited by its base models. For instance, the current version of IC-Light [58] still struggles to relight hard shadows or make large modifications to low-light images. Similarly, since IC-Light is pretrained on 512 resolution and fine-tuned on 1024 resolution, our model struggles to preserve image details if the resolution is lower than 512. Besides, since the optimization process relies on the optical flow estimation model, artifacts sometimes occurs in textureless areas where flow becomes unreliable. Furthermore, the temporal consistency loss has the tendency to smooth the texture of flickering areas, and therefore might sacrifice some details. Though the proposed decayed multiaxis denoising alleviates the problem, developing temporally more consistent and computationally more efficient denoising strategy is desired in future work. Figure 4: Ablation on main module components. The experiment is conducted on one sequence of the InteriorNet [32] subset, where the text prompt is \"This video showcases modern interior space, which is dimly lit\". The baseline here denotes VidToMe [33] in Tab. 2."
        },
        {
            "title": "5 Conclusion",
            "content": "In summary, we present TC-Light, one-shot-tuned framework that delivers temporally consistent and physically plausible relighting on long, highly dynamic videos. The optimization-based illumination alignment provides new paradigm for video relighting. Central to our approach is the Unique Video Tensoran explicit, canonical, and differentiable video representation that enables highly efficient optimization. Over the established long video relighting benchmark, TC-Light achieves state-of-the-art performance in both consistency and efficiency, making it particularly well suited for sim2real and real2real data scaling in embodied AI."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [3] Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. [4] Shrisha Bharadwaj, Haiwen Feng, Victoria Abrevaya, and Michael J. Black. Genlit: Reformulating single-image relighting as video generation, 2024. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [6] Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. Real-time 3d-aware portrait video relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62216231, 2024. [7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2304023050, 2023. [8] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-avideo: Controllable text-to-video generation with diffusion models, 2023. [9] Jun Myeong Choi, Max Christman, and Roni Sengupta. Personalized video relighting with an at-home light stage. In European Conference on Computer Vision, pages 394410. Springer, 2024. [10] Nathaniel Cohen, Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. Slicedit: Zero-shot video editing with text-to-image diffusion models using spatio-temporal slices. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 91099137. PMLR, 2127 Jul 2024. [11] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. [12] Sourya Dipta Das, Nisarg Shah, Saikat Dutta, and Himanshu Kumar. Dsrn: an efficient deep network for image relighting. In 2021 IEEE International Conference on Image Processing (ICIP), pages 27882792. IEEE, 2021. [13] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Navsim: In Advances in Neural Data-driven non-reactive autonomous vehicle simulation and benchmarking. Information Processing Systems (NeurIPS), 2024. [14] Qiaole Dong and Yanwei Fu. Memflow: Optical flow estimation and prediction with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19068 19078, 2024. [15] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 116. PMLR, 2017. [16] Ye Fang, Zeyi Sun, Shangzhan Zhang, Tong Wu, Yinghao Xu, Pan Zhang, Jiaqi Wang, Gordon Wetzstein, and Dahua Lin. Relightvid: Temporal-consistent diffusion model for video relighting. arXiv preprint arXiv:2501.16330, 2025. [17] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67126722, 2024. [18] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [20] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [22] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing Systems, 2024. [23] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65076516, 2024. [24] Haresh Karnan, Anirudh Nair, Xuesu Xiao, Garrett Warnell, Sören Pirk, Alexander Toshev, Justin Hart, Joydeep Biswas, and Peter Stone. Socially compliant navigation dataset (scand): large-scale dataset of demonstrations for social navigation. IEEE Robotics and Automation Letters, 2022. [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. [26] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics (TOG), 43(4):115, 2024. [27] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. [28] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physics-driven architecture and pre-training framework for human portrait relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25096 25106, 2024. 11 [29] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR (Poster), 2015. [30] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. [31] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. Lightit: Illumination modeling and control for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93599369, 2024. [32] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark, Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang, and Stefan Leutenegger. Interiornet: Mega-scale multi-sensor photo-realistic indoor scenes dataset. In British Machine Vision Conference (BMVC), 2018. [33] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74867495, 2024. [34] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli, Miloš Hašan, Zexiang Xu, Ravi Ramamoorthi, and Manmohan Chandraker. Physically-based editing of indoor scene lighting from single image. In European Conference on Computer Vision, pages 555572. Springer, 2022. [35] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82078216, 2024. [36] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing, 2023. [37] Haoyu Ma, Shahin Mahdizadehaghdam, Bichen Wu, Zhipeng Fan, Yuchao Gu, Wenliang Zhao, Lior Shapira, and Xiaohui Xie. Maskint: Video editing via interpolative non-autoregressive masked transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7403 7412, 2024. [38] N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2016. arXiv:1512.02134. [39] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors, 2023. [40] Thomas Nestmeyer, Jean-François Lalonde, Iain Matthews, and Andreas Lehrmann. Learning physicsguided face relighting under directional light. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51245133, 2020. [41] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80898099, 2024. [42] Rohit Pandey, Sergio Orts-Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Ryan Fanello. Total relighting: learning to relight portraits for background replacement. ACM Trans. Graph., 40(4):431, 2021. [43] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. [44] Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, and Yueting Zhuang. Instructvid2vid: Controllable video editing with natural language instructions. In 2024 IEEE International Conference on Multimedia and Expo (ICME), pages 16. IEEE, 2024. [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [46] Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, and He Zhang. Relightful harmonization: Lighting-aware portrait background replacement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64526462, 2024. 12 [47] Christian Richardt, Carsten Stoll, Neil A. Dodgson, Hans-Peter Seidel, and Christian Theobalt. Coherent spatiotemporal filtering, upsampling and rendering of rgbz videos. Computer Graphics Forum, 31(2pt1):247256, May 2012. [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [51] Inkyu Shin, Qihang Yu, Xiaohui Shen, In So Kweon, Kuk-Jin Yoon, and Liang-Chieh Chen. Enhancing temporal consistency in video editing by reconstructing videos with 3d gaussian splatting, 2024. [52] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [53] Tiancheng Sun, Jonathan Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Trans. Graph., 38(4):791, 2019. [54] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [55] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and Xuaner Zhang. Sunstage: Portrait reconstruction and relighting using the sun as light stage. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2079220802, 2023. [56] Yukun Wang, Longguang Wang, Zhiyuan Ma, Qibin Hu, Kai Xu, and Yulan Guo. Videodirector: Precise video editing via text-to-video models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [57] Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF international conference on computer vision, pages 802812, 2021. [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. [59] Yuxin Zhang, Dandan Zheng, Biao Gong, Jingdong Chen, Ming Yang, Weiming Dong, and Changsheng Xu. Lumisculpt: consistency lighting control network for video generation, 2024. [60] Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025. 13 Figure 5: Qualitative results on additional long highly dynamic videos."
        },
        {
            "title": "A Additional Experimental Results",
            "content": "Fig. 5 presents additional visualizations of our relighting results across diverse range of scenarios. Whether under nighttime or daytime conditions, in outdoor or indoor environments, or from aerial or ground-level viewpoints, the proposed TC-Light method consistently produces temporally coherent and physically plausible illumination edits, demonstrating strong generalization capabilities. Fig. 6 provides qualitative comparisons against state-of-the-art methods across additional scenarios. As shown, our model effectively adheres to textual instructions while generating relighting results that are both natural and temporally consistent. We also provide corresponding quantitative evaluations on synthetic and real-world scenarios. As reported in Tab. 5, performance on real-world scenes consistently exceeds that on synthetic ones. 14 Figure 6: Additional qualitative comparison of results. The proposed TC-Light avoids unnatural relighting like Slicedit [10] and COSMOS-Transfer1 [3] in (a) and (b), or temporal inconsistency like per-frame IC-Light [58] and VidToMe [33] as highlighted by the red squares. Table 5: Comparison on synthetic [38, 15, 2, 32] and realistic scenarios [52, 13, 27, 24]. The average resolutions are respectively 794 503 and 960 555, while the frame numbers are 272 and 246. \"OOM\" here means the method is unable to finish the task due to an out-of-memory error. Ourslight applies post-optimization to VidToMe, while Ours-full further introduces decayed multi-axis denoising. The best and the second best of each metric are separately highlighted in red and blue. Method IC-Light* [58] VidToMe [33] Slicedit [10] VideoDirector [56] Light-A-Video [60] RelightVid [16] Cosmos-T1 [3] Ours-light Ours-full Synthetic Motion-S WarpSSIM CLIP-T Motion-S WarpSSIM Realistic CLIP-T 93.43% 94.61% 96.28% OOM OOM OOM 96.31% 97.02% 97.36% 66.02 69.45 84.90 OOM OOM OOM 80. 88.63 91.07 0.2779 0.2776 0.2717 OOM OOM OOM 0.2537 0.2707 0.2695 95.14% 95.82% 96.38% OOM OOM OOM 96.78% 97.46% 97.90% 77.13 79.33 88.89 OOM OOM OOM 83.57 89.42 92.67 0.2837 0.2815 0.2715 OOM OOM OOM 0. 0.2816 0.2792 This discrepancy likely arises from the training data of the video model Cosmos-Transfer1 [3] and the foundational image model IC-Light [58], which are biased towards realistic scenes. Furthermore, the higher resolution and richer textures of real-world data mitigate hallucinations in textureless regions and help better preserve the intrinsic details of source frames for IC-Light. Such attributes are particularly critical for the consistency of methods with comparatively limited temporal modeling, namely, IC-Light* and VidToMe, which exhibit substantially higher Motion-S and WarpSSIM metrics on real-world videos than on synthetic ones. In contrast, our approach attains state-of-the-art temporal consistency across both scenario types while maintaining favorable balance with prompt adherence."
        },
        {
            "title": "B Details of Assets",
            "content": "In Tab. 6, we summarize the license and resolution for each subset. All source videos are resized and center-cropped to their designated resolutions. Considering the computation source limitation, we 2https://waymo.com/open/terms/ 3https://interiornet.org/ 15 Table 6: Licenses and video resolution of datasets [38, 15, 52, 13, 2, 27, 32, 24] contained in established benchmark. Notably, AgiBot here denotes AgiBot Digital World. DRONE is our self-collected subset. Sceneflow has no license, but is only allowed for research purposes. Datasets SceneFlow CARLA Waymo NavSim AgiBot DROID InteriorNet SCAND DRONE Width Height License 960 512 N/A 960 536 960 640 CC-BY Custom2 960 536 CC BYNC-SA 4. 640 480 CC BYNC-SA 4.0 960 536 Apache -2.0 640 480 Custom3 960 536 CC0 1.0 1280 N/A sample videos that contain over 600 frames to around 300 frames. The DRONE subset includes three clips captured using our DJI Mini4 Pro and two additional clips obtained from DroneStock4, which are released under the CC0 1.0 License. For AgiBot Digital World [2], where the robots head moves in coordination with its body while performing tasks, relighting is performed from the head-mounted camera view. For each scene of DROID [27], we apply relighting to both the static side camera and the dynamic left wrist camera views. For Waymo [52] and NavSim [13], relighting is conducted using the front-facing camera view. This paper also benefits from the code of IC-Light [58] (Apache-2.0 License), VidToMe [33] (MIT License), Slicedit [10] (MIT License), VideoDirector [56] (MIT License), Light-A-Video [60] (Apache-2.0 License), RelightVid [16] (CC BY-NC-SA 4.0 License), and Cosmos-Transfer1 [3] (Apache-2.0 License)."
        },
        {
            "title": "C Additional Implementation Details",
            "content": "For competing methods, we adopt the hyperparameters from their official implementations for VideoDirector [56], Light-A-Video [60], RelightVid [16], and Cosmos-T1 [3]. We replace base models of VidToMe [33] and Slicedit [10] with IC-Light [58], and therefore we align their classifierfree guidance scale and diffusion sampling steps with those in [58]. Additionally, we set VidToMes local and global token-merging ratios to 0.6 and 0.5, respectively, mirroring the setting of our approach. For Slicedit, we adjust the weighting factor γ in Eq. (2) from the default 0.2 to 0.05 to better balance temporal coherence and instruction adherence. All other hyperparameters remain at their default values. The modified VidToMe serves as the baseline of our model design. During implementation, each κ(x, y, t) comprises three components: (1) per-pixel flow ID, (2) quantized RGB color, and, optionally, (3) world-frame voxel coordinate. For (1), flow IDs are derived from the optical flow estimated by the state-of-the-art MemFlow method [14] and the binary mask obtained by thresholding the soft mask in Eq. (6) of the main paper (values > 0.5 are set to 1; otherwise 0). In the initial frame, pixels receive unique flow IDs from 0 to HW 1, where and denote image height and width. In subsequent frames, pixel inherits the flow ID of its predecessor if connected by an unmasked flow; otherwise, it is assigned new ID. This injects motion priors into the UVT representation. For (2), we quantize RGB values to 7 bits, ensuring that all pixels sharing the same UVT element differ by less than 2/255 in any channel. This constraint mitigates erroneous flows that escape the mask and reinforces representation in regions exhibiting view-dependent effects. For (3), when per-frame depth maps are available, they are reprojected into point cloud using the camera intrinsics and extrinsics to determine world-frame coordinates. This point cloud is then voxelized at specified voxel size, and each pixels voxel coordinate is appended to κ(x, y, t), yielding more compact representation of static regions. For the CARLA [15] and InteriorNet [32] datasets, voxel sizes are set to 0.05 and 0.02 m, respectively. Notably, dynamic objects at different timestamps may spatially overlap in 3D, but they remain distinguishable by their flow IDs and quantized RGB colors. Consequently, each object at each timestep is represented by distinct set of UVT elements, while the L1 temporal consistency loss preserves object identity across frames. 16 Figure 7: screenshot of the user study. 17 Figure 8: Results from user study with 65 valid submissions. The methods are arranged in alphabetical order. This figure reports the frequency that each method is chosen as the firstand second-most preferred video."
        },
        {
            "title": "D User Study",
            "content": "We conducted an online user study with 78 anonymous participants, evaluating 19 randomly selected videotext pairs from our datasets. The compared methods were IC-Light* [58], VidToMe [33], Slicedit [10], Cosmos-Transfer1 [3], Ours-light, and Ours-full. screenshot of the questionnaire interface is shown in Fig. 7. For each question, methods were anonymized and relighted videos were presented in random order; participants selected the two most preferred results. In compliance with the NeurIPS Code of Ethics, each participant received compensation of $0.70. Besides, we ensured that all collected data remained confidential and was not disclosed to any institutions or individuals. Since each video spanned 1020 seconds, completing the questionnaire took on average 13.5 minutes. Submissions requiring less than four minutes were deemed unreliable and excluded, yielding 65 valid responses. Fig. 8 reports the frequency with which each method was chosen among the top two. Our full model achieved the highest preference rate, while the light variant ranked second. Although IC-Light* and VidToMe follow instructions well (cf. Tab. 2 of the main paper), their inferior temporal consistency make them much less preferred by users. Finally, we computed BradleyTerry preference scores [5] as comprehensive metric of user preference, as presented in the Tab. 2 of the main paper."
        },
        {
            "title": "E Social Impact",
            "content": "Positive Impacts. The proposed TC-Light framework for long video relighting stands to benefit wide range of applications in both industry and research. First, by enabling consistent and physically plausible illumination editing at low computational cost, it can substantially lower the barrier to high-quality visual content creation, empowering independent filmmakers, educators, and artists to produce compelling video narratives without access to specialized hardware. Second, the capability to scale illumination-diverse training data through sim2real and real2real transfer can accelerate progress in embodied AIrobots and autonomous agents exposed to rich, temporally coherent visual environments may learn more robust perception and planning behaviors, thereby advancing safety and reliability in humanrobot interaction. Finally, by fostering more efficient video synthesis pipelines, TC-Light may encourage energy-aware design practices in large-scale media processing systems, contributing to reduced resource consumption and attendant carbon emissions. Negative Impacts. Despite these benefits, improved video relighting carries potential risks if misused. Enhanced realism in dynamic relighting could facilitate the creation of deceptive multimedia, includ4https://dronestock.com/ 18 ing deepfake videos that manipulate shadows and highlights to conceal tampering or impersonate individuals, thereby eroding trust in digital media. Moreover, large-scale deployment of relighting tools raises privacy concerns: adversarial actors might relight surveillance footage to obscure identities or fabricate altered event sequences. To mitigate these harms, we advocate for gated access to pretrained models, integration of provenance metadata to flag relit content, and collaboration with platform providers to monitor and throttle suspicious bulk relighting requests."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
        "Shandong University",
        "Tencent",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology Beijing"
    ]
}