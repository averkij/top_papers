{
    "paper_title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild",
    "authors": [
        "Henghui Ding",
        "Chang Liu",
        "Nikhila Ravi",
        "Shuting He",
        "Yunchao Wei",
        "Song Bai",
        "Philip Torr",
        "Kehuan Song",
        "Xinglin Xie",
        "Kexin Zhang",
        "Licheng Jiao",
        "Lingling Li",
        "Shuyuan Yang",
        "Xuqiang Cao",
        "Linnan Zhao",
        "Jiaxuan Zhao",
        "Fang Liu",
        "Mengjiao Wang",
        "Junpei Zhang",
        "Xu Liu",
        "Yuting Yang",
        "Mengru Ma",
        "Hao Fang",
        "Runmin Cong",
        "Xiankai Lu",
        "Zhiyang Che",
        "Wei Zhan",
        "Tianming Liang",
        "Haichao Jiang",
        "Wei-Shi Zheng",
        "Jian-Fang Hu",
        "Haobo Yuan",
        "Xiangtai Li",
        "Tao Zhang",
        "Lu Qi",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 2 3 1 1 . 4 0 5 2 : r PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild Henghui Ding*, Chang Liu*, Nikhila Ravi*, Shuting He*, Yunchao Wei*, Song Bai*, Philip Torr* Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang Xuqiang Cao, Linnan Zhao, Jiaxuan Zhao,"
        },
        {
            "title": "Fang Liu",
            "content": "Mengjiao Wang, Junpei Zhang, Hao Fang, Runmin Cong, Xu Liu, Xiankai Lu, Yuting Yang, Zhiyang Che,"
        },
        {
            "title": "Mengru Ma\nWei Zhan",
            "content": "Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Haobo Yuan, Xiangtai Li, Tao Zhang, https://pvuw.github.io/ Lu Qi, Jian-Fang Hu Ming-Hsuan Yang"
        },
        {
            "title": "Abstract",
            "content": "This report provides comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and The challenge features two future research directions. tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motionguided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/. 1. Introduction Pixel-level understanding of dynamic and complex visual scenes remains core yet unresolved problem in computer vision [9, 10, 19, 23, 34, 42]. While traditional research has predominantly focused on semantic segmentation within static images [57], such approaches fall short in capturing the temporal continuity of the real world. In contrast, video segmentation [9, 10, 20, 21, 36, 37] offers more realistic framework, aligning better with applications that demand spatiotemporal reasoningsuch as autonomous driving, aerial navigation, and mobile video editing. These use cases underscore growing shift toward scene under- *Authors are CVPR 2025 PVUW Challenge organizers. All others are challenge participants from the top-3 teams of MOSE and MeViS tracks. (cid:0) Corresponding to Henghui Ding (henghui.ding@gmail.com), the Institute of Big Data, Fudan University, Shanghai, China. standing methods that are not only spatially precise but also temporally coherent. To advance research in this direction, we introduce the Pixel-level Video Understanding in the Wild (PVUW) workshop, which emphasizes the challenges posed by unconstrained, real-world environments [13]. PVUW seeks to narrow the gap between static and dynamic scene understanding, encouraging the development of robust algorithms that can generalize across diverse, time-varying visual conditions. Through this initiative, we aim to catalyze innovation toward deploying perception systems capable of reliable operation in the wild. Recent advances in Large Language Models and multimodal LLMs have significantly reshaped computer vision [35]. Alongside, foundational models like SAM2 [34] have leveraged large-scale data to achieve strong generalization. Notably, progress in tasks such as Video Object Segmentation (VOS) [10] and Referring Video Object Segmentation (RVOS) [9] highlights the fields continued momentum toward more robust and unified vision systems. Building on these developments, the goal of our workshop and challenge is to keep pace with cutting-edge research, offer challenging, yet realistic benchmark to evaluate state-of-the-art models, and provide valuable insights into both the current trends and future directions of video understanding. Following past challenges, we aim to continuously provide challenging and diverse benchmarking data that are taken in real world, and in this year, we have added more latest data that are first time released. 2. The PVUW 2025 Challenge This year, we center our challenge around two focused tracks: the MOSE Track, which benchmarks advanced VOS methods in complex and densely populated scenes; and the MeViS Track, which evaluates models on language-guided 1 Table 1. MOSE Track results and top 20 of the final rankings. Table 2. MeViS Track results and top 20 of the final rankings. Rank Team (cid:140) 1 5 2 5 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 BrainyBots DeepSegMa JIO SCU Leung wulutuluman mima LK186******96 STELATOS9 MaxBitter XiaomiYU7 menghaoran zjy05140514 keeper zhaojinhui LuxeedR7 HuaweiAITOM9 YuLinLin ccHub ZhiMu ppbb 83.59 82.50 80.28 79.93 79.89 79.80 79.80 79.65 79.64 79.47 79.59 79.46 79.48 79.44 79.40 79.23 79.15 78.93 78.79 78. &F Rank Team 90.92 90.07 87.57 87.33 87.21 87.21 87.10 87.16 87.10 86.92 86.79 86.85 86.83 86.83 86.85 86.58 86.55 86.68 86.59 86.11 87.26 86.28 83.92 83.63 83.55 83.51 83.45 83.41 83.37 83.20 83.19 83.15 83.15 83.14 83.12 82.91 82.85 82.80 82.69 82. (cid:140) 1 5 2 5 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 MVP-Lab ReferDINO-iSEE Sa2VA Pengsong ssam2s strong kimchi seilvik90 yiweima xmu maclab xinming zhangtao-whu yiweima TransVG321 xmu-xiaoma666 MYOLO kker101 X-CLIP tbao LuQiLXX mengyuan 58.83 56.79 52.68 53.06 52.00 51.78 50.61 50.93 50.63 51.24 51.22 50.49 50.10 49.86 49.80 50.02 49.64 49.05 48.48 48.63 &F 65.14 64.07 59.84 58.76 58.33 58.27 59.22 58.65 58.32 57.33 57.19 57.30 57.30 56.92 56.97 56.55 56.84 56.59 54.69 54.42 61.98 60.43 56.26 55.91 55.16 55.02 54.91 54.79 54.48 54.28 54.21 53.90 53.70 53.39 53.38 53.29 53.24 52.82 51.59 51.53 video segmentation, with particular emphasis on motionguided language expressions. 2.1. Two Video Segmentation Tracks Track 1: MOSE Track Complex Video Object Segmentation (MOSE) [10] aims to track and segment objects in videos of complex environments. This track is based on the MOSE [10] dataset, which is new video object segmentation benchmark designed to study object tracking and segmentation in complex, real-world scenes. Unlike previous video segmentation datasets [32, 43] that focus on salient and isolated objects, MOSE features crowded environments, frequent occlusions, and object disappearances. It consists of 2,149 video clips and 5,200 objects across 36 categories, with over 430,000 high-quality segmentation masks. MOSE challenges existing VOS models and highlights the performance gap in complex scenarios, encouraging further research into robust segmentation techniques. This years testing set is part of MOSE testing set, but with more challenging newly taken data added. The ground truths of all videos in the testing sets are confidential and has never been released before. This year, we have 81 teams registered to the MOSE track on the platform, and 43 teams of them submitted their results on the testing phase. Top results are shown in Table 1. The top three teams are imaplus, KirinCZW, and dumplings. The first place team achieved &F score of 87.26% on the testing set. Track 2: MeViS Track Motion Expression guided Video Segmentation (MeViS) [9] focuses on segmenting objects in video based on sentence describing the motion of the objects, which is based on the MeViS dataset. The MeViS dataset [9] is large-scale benchmark designed for motion-guided language-based video object segmentation. Unlike previous referring image segmentation or referring video segmentation works [8, 11, 1618, 2529, 3941, 47] that focus on static object attributes, MeViS emphasizes motion-centric language expressions to identify and segment target objects in complex video scenes. It features wide range of motion expressions paired with videos containing crowded and dynamic environments. Benchmarking results show that existing referring video object segmentation methods struggle with this task, highlighting the need for new methods that can better leverage motion as primary cue in language-guided video segmentation. Similarly, the testing set of this track comes from MeViS testing set, with newly added videos and confidential ground-truths. For MeViS Track, this year we have attracted 77 teams to registered, from which 31 teams participated in the testing phase. The top three teams are MVP-Lab, ReferDINO-Plus, and HarborY, as shown in Table 2. 2.2. Evaluation Both tracks are evaluated using standard metrics consistent with prior PVUW challenges [12, 13] and benchmarks such as DAVIS [32] and YouTube-VOS [43]. Specifically, we adopt region similarity (J ), contour accuracy (F), and their average (J &F), with &F serving as the primary ranking metric. All evaluations are conducted on the publicly accessible CodaLab platform. Sec. 3 and Sec. 4 presents the solutions from the top-3 2 teams of MOSE track and MeViS track, respectively. 3. MOSE Track Top Solution 3.1. 1st Team in MOSE Track: BrainyBots Title: Members: STSeg Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang Affiliation: Xidian University, China We optimize our solution across both training and inference stages. During training, we fine-tune SAM2 and TMO on the MOSE dataset to better adapt them to the challenges of video object segmentation in complex environments. For inference, we leverage an ensemble of five modelsSAM2, TMO, Cutie, XMem, and LiVOSon the MOSE test set. The predicted masks from these models are aggregated to construct rich pseudo-labels. Based on these, we dynamically select the most suitable model per video instance to ensure optimal segmentation quality. Detailed fine-tuning strategies are provided in our full report. Adaptive Pseudo-labels Guided Model Refinement Pipeline After analyzing the dataset, we found it challenging to achieve good results in all scenarios using single model. Therefore, we propose an Adaptive Pseudo-labels Guided Model Refinement Pipeline (PGMR), as shown in Fig 1 with specific implementation steps as follows: Multi-Model Inference: Independent Processing and Result Collection. In video frame segmentation and tracking tasks, we first employ multi-model independent inference to process the same set of video frames. Each model demonstrates unique performance advantages in different scenarios based on its design features and training data. To fully leverage the strengths of each model, we have designed parallel inference framework that ensures each model can operate independently and produce results without interference from other models. This framework allows multiple models to perform inferences on the same set of video frames simultaneously, enabling each model to perform at its best without being influenced by others. The output results of each model are collected separately and include segmentation masks, tracking IDs, and confidence scores. Segmentation masks are used to accurately delineate the boundaries of target objects within video frames while tracking IDs are employed to continuously track the positional changes of target objects throughout the video sequence and confidence scores reflect the models assessment of each prediction. Pseudo-Label Fusion: Generating Baseline Result. To optimize the performance of video frame segmentation and tracking tasks, it is crucial to integrate the inference results of multiple models into comprehensive pseudolabel. This pseudo-label serves as key baseline for Figure 1. Overview of the PGMR Framework. Inference and Pseudo-Label-Based Model Selection: Employing five models to conduct inference operations, and the model with optimal performance for different video contents is intelligently selected. the subsequent optimization process and helps identify the model that performs optimally for different video contents. The generation of the pseudo-label involves several steps: Firstly, consistency check is carried out by comparing the segmentation masks and tracking IDs of different models to identify the regions where the model results are consistent and those where they are inconsistent. Then, confidence weighting is performed. Weights are assigned to each model based on its historical performance and the confidence scores associated with its predictions. Finally, voting mechanism is employed for the regions where the models produce conflicting results, and conflict resolution strategy is adopted. The fused pseudo-label, as key intermediate link, bridges the gap between the outputs of individual models and the performance of the unified optimization system. It enables the intelligent selection of the model that demonstrates the best performance for different video contents. Model Recommendation Mechanism: Intelligent Task Allocation. Based on the generated pseudo-label, we have developed dynamic model recommendation mechanism to ensure that each video frame is processed by the most suitable model. First, feature extraction is conducted to analyze video frames and extract key information of scene complexity, the number of objects, and the distribution of object sizes. Subsequently, we have established compact model performance database to record the historical performance of each model across various feature scenarios. Finally, recommendation algorithm is employed to recommend the optimal model for each video frame based on the extracted frame features and the information stored in the model performance database. By implementing this model recommendation mechanism, the system is able to dynamically allocate tasks to the most suitable model for each video frame. 3.2. 2nd Team in MOSE Track: DeepSegMa Title: Members: DeepSegMa Xuqiang Cao, Linnan Zhao, Jiaxuan Zhao, Fang Liu Affiliation: Key Laboratory of Intelligent Perception and Image Understanding, China Method. An overview of our framework is presented in Figure 2. To better align with the characteristics of the MOSE dataset, we construct tailored dataset, MOSE+, and introduce set of targeted data augmentation strategies to mimic real-world variations in appearance, pose, illumination, and structural consistency. During inference, we employ mask confidence control mechanism, followed by temporal fusion across frames to generate the final segmentation outputs. Each component is detailed below. Baseline Model. We use transformer-based segmentation framework with object-guided attention, mask-aware memory, and spatiotemporal reasoning. The model effectively captures temporal cues and spatial details through dual memory modules and multi-scale decoding, enabling robust performance under challenging scenarios like occlusion, motion blur, and small-object clutter. This strong baseline lays solid foundation for our enhancement strategies. Loss Function. To achieve high-precision segmentation and temporal consistency, we design multi-task loss that combines pixel-wise accuracy, region-level overlap, classification discriminability, and robustness to occlusion. The total loss is defined as: Ltotal = λ1LCE +λ2LDice+λ3LSim+λ4LM askIoU , (1) where LCE denotes cross-entropy loss for foregroundbackground classification, LDice enhances region consistency, LSim enforces similarity between memory and query features, and LM askIoU constrains predicted mask quality. These losses are computed across multiple frames and candidate masks to jointly supervise spatiotemporal modeling. Data Augmentation. To improve generalization and robustness, we introduce set of targeted augmentation strategies during training. Unlike static image tasks, video segmentation demands consistency across frames while simulating realistic variations. Our approach integrates both frame-consistent and frame-inconsistent perturbations: Consistent geometric transformations: Random horizontal flipping, affine transformations (rotation, shear), and multi-scale resizing are applied across all frames in clip to simulate viewpoint and object deformation. Mixed color perturbations: Brightness, contrast, and saturation changes are applied globally, while grayscale conversion and inconsistent color jittering are selectively applied to individual frames, enhancing robustness to lighting changes and visual ambiguity. Figure 2. Overview of Team DeepSegMas method. Normalization: Images are transformed into tensors and normalized using ImageNet mean and standard deviation for stable convergence and pretrained compatibility. These augmentations significantly improve the models ability to handle structure variation, appearance change, and dynamic scenes in MOSE-like scenarios. Inference Strategy. To improve model robustness and adaptability in complex video scenarios, we introduce set of tailored strategies during inference. Mask Confidence Control Strategy. We observe that the quality of predicted masks can be significantly affected by post-processing in different scenarios, such as small objects, heavy occlusions, and target overlaps. To address this, we adopt control strategy based on dynamic adjustment of the mask output distribution, using two key parameters: sigmoid scale and sigmoid bias. The sigmoid scale controls the sharpness of the output boundaries, while the sigmoid bias adjusts the overall activation level, thereby influencing the target coverage and boundary quality. Experiments on the validation set show that setting the sigmoid scale to 7.5 and the sigmoid bias to -4.0 yields the best performance. Data. To improve generalization and target modeling in complex scenarios, we construct an enhanced training set named MOSE+, based on the original MOSE dataset. This augmented set is composed of video segments from multiple public VOS datasets, selected to match the characteristics of MOSE, including frequent occlusions, dense small objects, object reappearance, and high similarity among targets. Specifically, we integrate carefully chosen sequences from datasets such as BURST [1], DAVIS [32], OVIS [33], and YouTubeVIS [45], unify their annotations and resolution formats, and seamlessly merge them with MOSE to form consistent training set, enhancing semantic understanding and robustness. Please refer to the main technical report of DeepSegMa for model training and experiment details. 3.3. 3rd Team in MOSE Track: JIO Title: Members: Mengjiao Wang, Junpei Zhang, Xu Liu,"
        },
        {
            "title": "FVOS",
            "content": "Affiliation: Yuting Yang, Mengru Ma International for Intelligent Perception and Computation, China"
        },
        {
            "title": "Joint Research Center",
            "content": "Figure 3. Network Architecture of FVOS. Method. Our approach primarily consists of three components: model fine-tuning training, morphological post-processing, and multi-scale segmentation result fusion. Figure 3 illustrates the network architecture adopted in our framework, which primarily relies on Transformers for feature extraction and attention computation. MOSE Fine-tuning. Our training process is as follows: First, we fine-tune the pre-trained model on the MOSE dataset for total of 10 epochs, submitting results from the validation set of each epoch. The best-performing model from this stage is selected as the pre-trained model to begin new round of training. In this second stage, we conduct training for total of 40 epochs, selecting the best-performing model for testing with optimal parameters. Finally, the single best-performing model is selected to generate the initial single-model segmentation results. Morphological Post-Processing. After training, we noticed that there exists distinct gap between adjacent objects. This is because the model predicts separate objects individually before merging them during inference, thus the edge regions are not well aligned. To address this problem, we propose using morphological operations, especially dilation, for post-processing [4]. During the inference of the network, the binary segmentation masks for each object are first obtained and collected. For the current object, dilation operations are performed on both the object itself and all other objects. The adjacency between other objects and the current object is determined by checking whether the dilated masks overlap. If objects are deemed adjacent, the overlapping regions are filled and applied to the current object. Finally, object mask merging is performed following the rule of prioritizing higherindexed objects, yielding the final segmentation results. Test time data augmentation and multi-scale Figure 4. magnification operations. (b) clockwise by 90. (c) clockwise by 180. (d) clockwise by 270. (e) horizontal flipping. (f) multi-scale magnification. (a) original image. Based on our experiments, using kernel size of 2 yields better improvements in the segmentation results. Multi-Scale Results Fusion. We also adopted common test-time data augmentation methods, including rotating the original image clockwise by 90, 180, and 270, horizontal flipping, as well as multi-scale processing by resizing the image to several scales, as shown in Figure 4. Specifically, starting from the original size, we resized the dataset with increments of 0.125 to reconstruct it at multiple scales. After experimenting with several scales, we finally selected 7 different scales ranging from 1 to 1.75 for fusion. 4. MeViS Track Top Solution 4.1. 1st Team in MOSE Track: MVP-Lab Title: Members: Affiliation: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation Hao Fang, Runmin Cong, Xiankai Lu, Zhiyang Che, Wei Zhan Shandong University The input of RVOS contains video sequence = (cid:8)Xt R3HW (cid:9)N t=1 with frames and corresponding referring expression = {Tl}L l=1 with words. Baseline. We adopt Sa2VA [46] as our baseline to obtain t=1 that are correlated with mask sequences = {Mt}N language descriptions: = rvos (S, ) , (2) where rvos denotes the Sa2VA model. The overall architecture of Sa2VA is shown in Fig. 5. It contains two parts: the LLaVA-like model and SAM 2. Pre-trained LMMs. Sa2VA adopts pre-trained LLaVAlike models as the LMMs. It contains one visual encoder, one visual projection layer, and one LLM. The visual encoder takes input images, video, and sub-images as inputs. The visual projection layer maps inputs into visual tokens. These tokens, combined with the input text tokens, are the input of LLMs and the LLMs generate the text token Figure 5. The architecture of Sa2VA [46]. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM 2 decoder receives the image and video features from the SAM 2 encoder, along with the [SEG] token, to generate corresponding image and video masks. prediction based on them. Note that Sa2VA adopts pretrained LMMs following previous works [22, 44] to leverage their strong capability. It applies the same pipeline [38] to both image and video chat datasets without modification. Decoupled Design. Sa2VA append SAM 2 alongside the pre-trained LLaVA model. It does not take the SAM 2s output tokens (visual features or decoder outputs) into LLM. There are three reasons. 1) Sa2VA makes the combination as simple as possible without increasing extra computation costs. 2) Adding extra tokens needs an extra alignment process. 3) Via this design, it can fully make our work as plug-in-play framework to utilize pre-trained LMMs since the LMM community goes fast. Thus, Sa2VA adopts decoupled design without introducing further communication between LLaVA and SAM 2. Tuning SAM 2 Decoder via SEG Tokens. Sa2VA connects SAM 2 and LMM via the special token [SEG]. The hidden states of the [SEG] token are used as new type of prompt and fed into SAM 2s Decoder to generate segmentation masks. The hidden states of [SEG] can be seen as novel spatial-temporal prompt for SAM 2. SAM 2 segments the corresponding object mask in image and video based on the spatial-temporal prompt. During training, the SAM 2 decoder can be tuned to understand the spatialtemporal prompt, and gradients can be backpropagated through the [SEG] token to the LMM, allowing it to output the spatial-temporal prompt better. Inference. For RVOS tasks, Sa2VA designs simple framework to achieve strong results on public benchmarks. In particular, for giving input video, it adopts [SEG] token to generate the masks of the key frames. Then, it uses the memory encoded by the key frame features to generate the mask for the remaining frames. Sa2VA defaults to extracting the first five frames of the input video as key frames into LLM, but MeViS is long video dataset, which results in significant loss of video information. To address Algorithm 1: RVOS Inference Pipeline 1 Input: Video length ; Number of key frames ; Video frames SN (X1, X2, X3,. . ., XN ); Language description ; 2 Output: Sequence of masks M1, M2, M3,. . ., MN ; 3 Run: Sa2VA Model for RVOS; 4 Uniform sampling to extract key frames: SM SN ; 5 Visual embeddings: Ev Encoder(SM ); 6 Language embeddings: El Encoder(T ); 7 Answers: LLM({Ev, El}); 8 Prompt embedding: Pl Linear(Find(A, [SEG])); 9 for = 1, 2, . . . , do 10 SAM 2 feature: Fi Encoder(X0); Mask: Mi Decoder({Pl, Fi}); Update Memory: em Cross-Attention({M em, Mi}); 13 for = + 1, + 2, . . . , do 14 SAM 2 feature: Fi Encoder(X0); Mask: Mi Decoder({M em, Fi}); Update Memory: em Cross-Attention({M em, Mi}); 12 15 16 17 emit M1, M2, M3,. . ., MN ; this, as shown in Algorithm 1, we uniformly sample key frames across the entire video to provide the LLM with more comprehensive temporal context. In SAM 2, These key frames are fed into CLIP and flattened to visual sequential tokens for LLM processing. The LLM takes the visual and language tokens as input and uses these tokens to extract information about the video to the prompt generate the [SEG] token. encoder encodes boxes or clicks to prompt embeddings for object referring. Different from SAM 2, Sa2VA use two linear layers to project the [SEG] token into the language prompt embedding, which serves as an extension of the SAM 2 prompt encoders. With the language prompt embedding, it uses the SAM 2 decoder to generate the masks of the key frames. Then, Sa2VA use the memory encoder of SAM 2 to generate memory based on the output key-frame masks. Finally, memory attention in SAM-2 uses this memory, along with prior non-key-frame masks, to generate the remaining frame masks. Aggregation. We find that Sa2VA does not necessarily perform better with larger number of parameters and more sampling frames, as each configuration has its own strengths in different videos. And for some videos that cannot be accurately segmented by LMMs, the classic RVOS model may handle them very well. So we integrate the results of multiple expert models to mitigate the erroneous predictions of single model: = use (cid:0)MK(cid:1) , (3) where MK is the sets of mask sequences output by Sa2VA models with different configurations and other RVOS models [14], use denotes pixel-level binary mask If there are more than (N + 1)/2 pixels with voting. value equal to 1, we divide the pixel into the foreground, otherwise, it is divided into the background. 4.2. 2nd Team in MOSE Track: ReferDINO-iSEE Title: Members: Affiliation: ReferDINO-Plus: ReferDINO with SAM2 Tianming Liang, Haichao Jiang, Wei-Shi Zheng, Jian-Fang Hu Sun Yat-sen University Figure 6. Overview of ReferDINO-Plus. For each videodescription pair, we input it into ReferDINO to derive the object masks Mr and the corresponding scores Sr across the frames. Then, we select the mask with the highest score as the prompt for SAM2, producing refined masks Ms. Finally, we fuse the two series of masks through the conditional mask fusion strategy. The overall framework of our solution ReferDINO-Plus is presented in Figure 6. For each video-description pair, we input it into ReferDINO to derive the object masks and the corresponding scores across the frames. Then, we select the mask with the highest score as the prompt for SAM2, producing refined masks. Finally, we fuse the two series of masks through the conditional mask fusion strategy, to generate the final masks for each frame. Cross-modal Dense Reasoning via ReferDINO. ReferDINO [24] is strong RVOS model inheriting object-level vision-language knowledge from GroundingDINO [31], and is further endowed with pixel-level dense prediction and cross-modal spatiotemporal reasoning. Given video clip of frames and text description, ReferDINO performs cross-modal reasoning and segmentation, deriving mask sequence {M t=1 throughout the video. Following previous works [9, 15, 24], we combine the multiple object masks with scores higher than preset threshold σ to handle multi-object cases. t=1 and the corresponding scores {St r}T r}T Post Enhancement with SAM2. SAM2 [34] is powerful prompt-based segmentation model capable of efficiently generating high-quality object masks across video frames given cues such as clicks, bounding boxes, or masks. We integrate SAM2 to enhance the mask precision and temporal consistency of ReferDINO predictions. After obtaining frame-wise masks and their associated confidence scores, we select the highest-scoring mask as reference prompt. Using this reference frame and mask, SAM2 then propagates and refines the segmentation across the entire video, yielding sequence of masks T t=1. Conditional Mask Fusion. Although the masks from SAM2 are more reliable and stable, we observe that SAM2s overall performance on MeViS is significantly weaker than that of ReferDINO. In our experiments, we identify the main reason as that, for multi-object mask prompts, SAM2 tends to degenerate them into single-object masks, leading to substantial target loss in subsequent frames. To address this issue, we design Conditional Mask Fusion (CMF) principle: for single-object cases, we output only the masks from SAM2; for multi-object cases, we combine both the masks from ReferDINO and SAM2. However, it remains challenging to determine whether an expression involves multiple objects. In our solution, we define it as multi-object case if the mask area of SAM2 is less than 2/3 of ReferDINOs. Formally, this process can be described as follows: = (cid:40) Ms Ms + Mr if A(Ms) < 2 otherwise, 3 A(Mr), (4) where A() indicates the mask area. Note that our CMF is applied individually to each frame, which empirically achieves better performance. 4.3. 3rd Team in MOSE Track: Sa2VA Title: Members: Affiliation: Sa2VA Haobo Yuan1, Xiangtai Li2, Tao Zhang3, Lu Qi2, Ming-Hsuan Yang1 1UC Merced 2Bytedance 3Wuhan University Meta Architecture. As shown in Fig. 5, Sa2VA consists of an MLLM and SAM2. The MLLM accepts inputs of images, videos, and text instructions, and outputs text responses based on the text instructions. When the user instruction requires the model to output segmentation results, the text response will include the segmentation token [SEG]. The segmentation tokens hidden states serve as implicit prompts and are converted through SAM2 into image and video-level object segmentation masks. MLLM. The SOTA MLLM InternVL 2.5 [2] is adopted as the MLLM, demonstrating powerful capabilities in single-image, multi-image, and video understanding and InternVL 2.5 adopts LLaVA-like [30] conversation. architecture, consisting of an InternVIT [3], an MLP projector, and Large Language Model. High-resolution images are first divided into several sub-images and thumbnail, then encoded by InternVIT into vision tokens, which are mapped through one MLP and combined with text tokens as input to the LLM. The LLM will autoregressively output responses, which may text include segmentation tokens. The segmentation tokens hidden states from the last LLM transformer layer are processed through an MLP to serve as the prompt input for SAM2 [34]. SAM2. SAM2 generates object segmentation 7 results for some high-resolution video frames based on the segmentation prompts output by the MLLM. Subsequently, SAM2 propagates these frame segmentation results to obtain object segmentation results for the entire video. Sa2VA Model Training. The original Sa2VA is cotrained on multiple datasets, including image/video VQA datasets, caption datasets, and image/video referring segmentation datasets, including MeViS. For this challenge, we do not fine-tune the model for MeViS, where we only focus on test time modifications on Sa2VA. Naive Ref-VOS Inference Pipeline. The origin pipeline of Sa2VA begins by extracting the first five frames (k1, k2, . . ., kK are set to 1, 2, 3, 4, and 5 respectively) of the input video as keyframes, ensuring that they capture the critical context for the following processing. These key frames are fed into CLIP and flattened to visual sequential tokens for LLM processing. The LLM takes the visual and language tokens as input and uses these tokens to extract information about the video to generate the [SEG] token. In SAM2, the prompt encoder encodes boxes or clicks to prompt embeddings for object referring. Different from SAM-2, we use two linear layers to project the [SEG] token into the language prompt embedding, which serves as an extension of the SAM-2 prompt encoders. Using the language prompt embedding, we employ the SAM-2 decoder to generate keyframe masks. We then encode these masks into memory via SAM-2s memory encoder. Finally, the memory attention module produces the remaining masks based on the keyframe and prior non-key-frame masks. Test Long-Interleaved Inference. time augmentation for Sa2VA on MeVIS The Naive Ref-VOS inference pipeline directly uses the first several frames as the keyframes. However, this may lead to suboptimal performance when the initial frames lack sufficient context This is especially for accurate reference embedding. evident when the language prompt requires longer temporal reasoning. To address this issue, we propose an inference strategy named Long-Interleaved Inference (LII). We intentionally lengthen the time duration of the key frames to capture more context in the video. Specifically, we interleave keyframes across longer temporal window rather than selecting them consecutively from the beginning. We sample keyframes at fixed intervals throughout the video, ensuring both early and late contextual signals are incorporated into the reference embedding. To keep the whole method simple and not overly dependent on hyperparameters, we use the same interval in all videos. The whole algorithm is similar to the naive Ref-VOS inference pipeline, and the main difference is the key frame selection strategy. k1, k2, . . ., kK can be set to fixed set of values before the execution of the entire pipeline. With the Long-Interleaved Inference strategy, the keyframes are no longer clustered at the beginning but are spread across longer video clip. This design encourages the model to capture long-term dependencies, which is particularly beneficial in scenarios where the object appearance or scene context changes over time. Other Attempts. We also try model ensembling strategy during the competition, which shows performance degradation and is not adopted in the final result. For the model ensembling strategy, we use two separate SAM-2 decoders during inference. The first one is from the Sa2VA, which is trained with the one-shot instruction tuning process and different from the original SAM-2 decoder as shown in Figure 5. The other one is from the original SAM-2. In the process of predicting the key frame masks, we have to use the SAM-2 decoder of Sa2VA because we need to use [SEG] token as prompt. We input the key frame masks into the second SAM-2 decoder to infer the rest of the masks. We hope to use this approach to separate reasoning and tracking. However, we observe performance drop and do not apply this strategy. 5. Conclusion and Discussion This years PVUW challenge has attracted record number of participants. This high level of engagement highlights the growing interest and relevance of pixel-level video understanding within the research community. From the top-performing methods, several key insights emerge. First, we observe the critical importance of high-quality data. Datasets such as MOSE and MeViS, which offer finegrained annotations, enable methods powered by largescale pre-trained models like SAM 2 to achieve strong performance. Second, multi-modal large language models (LLMs) are beginning to demonstrate significant potential in video understanding, particularly in language-guided video tasks. With the continued evolution of LLMs, we expect them to play an increasingly vital role in this field. These findings offer clear directions for future research. The importance of scalingboth in model capacity and the quality of training datahas been reinforced across many submissions. As LLMs continue to improve in multimodal capabilities, we believe they will further advance the state of video understanding. Looking ahead, we will continue updating both the training and testing sets of the MOSE and MeViS datasets, and we remain committed to pushing the boundaries of pixel-level video understanding."
        },
        {
            "title": "References",
            "content": "[1] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: benchmark for unifying object recognition, segmentation and tracking in video. In WACV, 2023. 4 [2] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of 8 open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 7 [3] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Internvl: Scaling up vision foundation Lewei Lu, et al. In models and aligning for generic visual-linguistic tasks. CVPR, 2024. 7 [4] Mary Comer and Edward Delp III. Morphological operations for color image processing. Journal of electronic imaging, 8(3):279289, 1999. 5 [5] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature and gated multiIn Proceedings scale aggregation for scene segmentation. of the IEEE conference on computer vision and pattern recognition, pages 23932402, 2018. 1 [6] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Boundary-aware feature Thalmann, and Gang Wang. In Proceedings of propagation for scene segmentation. the IEEE/CVF international conference on computer vision, pages 68196829, 2019. [7] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Semantic correlation promoted shape-variant context for segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88858894, 2019. 1 [8] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16321 16330, 2021. 2 [9] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26942703, 2023. 1, 2, 7 [10] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2022420234, 2023. 1, 2 [11] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. VLT: Vision-language transformer and query generation IEEE Transactions on Pattern for referring segmentation. Analysis and Machine Intelligence, 45(6):79007916, 2023. 2 [12] Henghui Ding, Lingyi Hong, Chang Liu, Ning Xu, Linjie Yang, Yuchen Fan, Deshui Miao, Yameng Gu, Xin Li, Zhenyu He, et al. LSVOS challenge report: Large-scale In ECCV complex and long video object segmentation. Workshop, 2024. [13] Henghui Ding, Chang Liu, Yunchao Wei, Nikhila Ravi, Shuting He, Song Bai, Philip Torr, Deshui Miao, Xin Li, Zhenyu He, et al. PVUW 2024 challenge on complex video In ECCV Workshop, understanding: Methods and results. 2024. 1, 2 [14] Hao Fang, Feiyu Pan, Xiankai Lu, Wei Zhang, and Runmin Cong. Uninext-cutie: The 1st solution for lsvos challenge rvos track. arXiv preprint arXiv:2408.10129, 2024. 6 [15] Shuting He and Henghui Ding. Decoupling static and hierarchical motion perception for referring video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13332 13341, 2024. 7 [16] Shuting He and Henghui Ding. RefMask3D: Languageguided transformer for 3d referring segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 83168325, 2024. 2 [17] Shuting He, Henghui Ding, Chang Liu, and Xudong Jiang. GREC: Generalized referring expression comprehension. arXiv preprint arXiv:2308.16182, 2023. [18] Shuting He, Henghui Ding, Xudong Jiang, and Bihan Wen. SegPoint: Segment any point cloud via large language model. In European Conference on Computer Vision, pages 349367. Springer, 2024. 2 [19] Syed Ariff Syed Hesham, Yun Liu, Guolei Sun, Henghui Ding, Jing Yang, Ender Konukoglu, Xue Geng, and Xudong Jiang. Exploiting temporal state space sharing for video In Proceedings of the IEEE/CVF semantic segmentation. Conference on Computer Vision and Pattern Recognition, 2025. 1 [20] Lei Ke, Henghui Ding, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Video mask transfiner for high-quality video instance segmentation. In European Conference on Computer Vision, pages 731747. Springer, 2022. 1 [21] Lei Ke, Martin Danelljan, Henghui Ding, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Mask-free video instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22857 22866, 2023. 1 [22] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 6 [23] Xiangtai Li, Henghui Ding, Haobo Yuan, Wenwei Zhang, Jiangmiao Pang, Guangliang Cheng, Kai Chen, Ziwei Liu, and Chen Change Loy. Transformer-based visual IEEE transactions on pattern segmentation: survey. analysis and machine intelligence, 2024. 1 [24] Tianming Liang, Kun-Yu Lin, Chaolei Tan, Jianguo Zhang, Wei-Shi Zheng, and Jian-Fang Hu. Referdino: Referring video object segmentation with visual grounding foundations. arXiv preprint arXiv:2501.14607, 2025. 7 [25] Chang Liu, Xudong Jiang, and Henghui Ding. Instancespecific feature propagation for referring segmentation. IEEE TMM, 2022. 2 [26] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2359223601, 2023. [27] Chang Liu, Henghui Ding, Yulun Zhang, and Xudong Jiang. Multi-modal mutual attention and iterative interaction for referring image segmentation. IEEE TIP, 2023. 9 segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 78527861, 2024. [41] Jianzong Wu, Xiangtai Li, Xia Li, Henghui Ding, Yunhai Tong, and Dacheng Tao. Towards robust referring image IEEE Transactions on Image Processing, segmentation. 2024. [42] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, et al. Towards open vocabulary learning: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(7):50925113, 2024. 1 [43] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, Dingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and Thomas Huang. Youtube-vos: Sequence-to-sequence video object segmentation. In ECCV, 2018. 2 [44] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. 6 [45] Linjie Yang, Yuchen Fan, and Ning Xu. The 2nd largescale video object segmentation challenge - video object segmentation track, 2019. 4 [46] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. 5, [47] Hui Zhang and Henghui Ding. Prototypical matching and open set rejection for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 69746983, 2021. 2 [28] Chang Liu, Xudong Jiang, and Henghui Ding. Primitivenet: decomposing the global constraints for referring segmentation. Visual Intelligence, 2(1):16, 2024. [29] Chang Liu, Xiangtai Li, and Henghui Ding. Referring image editing: Object-level image editing via referring expressions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1312813138, 2024. 2 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 7 [31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding DINO: Marrying dino with In grounded pre-training for open-set object detection. European Conference on Computer Vision, pages 3855. Springer, 2024. 7 [32] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 2, [33] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: benchmark. International Journal of Computer Vision, 130 (8):20222039, 2022. 4 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. SAM 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 7 [35] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng Tu, Yu-Gang Jiang, and Dacheng Tao. survey of multimodal-guided image editing with text-to-image diffusion models. arXiv:2406.14555, 2024. 1 [36] Guolei Sun, Yun Liu, Henghui Ding, Thomas Probst, and Luc Van Gool. Coarse-to-fine feature mining for video In proceedings of the IEEE/CVF semantic segmentation. conference on computer vision and pattern recognition, pages 31263137, 2022. 1 [37] Guolei Sun, Yun Liu, Henghui Ding, Min Wu, and Luc Van Gool. Learning local and global temporal contexts IEEE Transactions on for video semantic segmentation. Pattern Analysis and Machine Intelligence, 46(10):6919 6934, 2024. 1 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [39] Yaxian Wang, Henghui Ding, Shuting He, Xudong Jiang, Bifan Wei, and Jun Liu. Hierarchical alignment-enhanced adaptive grounding network for generalized referring expression comprehension. In AAAI, 2025. 2 [40] Changli Wu, Yihang Liu, Jiayi Ji, Yiwei Ma, Haowei Wang, Gen Luo, Henghui Ding, Xiaoshuai Sun, and Rongrong Ji. 3D-GRES: Generalized 3d referring expression"
        }
    ],
    "affiliations": [
        "the Institute of Big Data, Fudan University, Shanghai, China"
    ]
}