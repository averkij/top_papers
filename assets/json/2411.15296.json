{
    "paper_title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
    "authors": [
        "Chaoyou Fu",
        "Yi-Fan Zhang",
        "Shukang Yin",
        "Bo Li",
        "Xinyu Fang",
        "Sirui Zhao",
        "Haodong Duan",
        "Xing Sun",
        "Ziwei Liu",
        "Liang Wang",
        "Caifeng Shan",
        "Ran He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 6 9 2 5 1 . 1 1 4 2 : r JOURNAL OF LATEX CLASS FILES, NOVEMBER 1 MME-Survey: Comprehensive Survey on Evaluation of Multimodal LLMs Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, Fellow, IEEE, Caifeng Shan(cid:0), Senior Member, IEEE, and Ran He, Senior Member, IEEE AbstractAs prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks. Index TermsMultimodal Large Language Model, Vision-Language Model, Model Evaluation, Benchmark."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "L ARGE Language Models (LLMs) [1] are sweeping across the whole artificial intelligence community. Through scaling up the size of model parameters and training corpus, LLMs exhibit emergent capabilities, such as following instructions [2] and learning from context [3]. Distinct from previous paradigms that train specific model for specific task, LLMs are competent in solving wide array of general tasks through prompting. Furthermore, LLMs can only support language while our world is naturally multimodal, encompassing information of various forms, e.g. vision and audio [4]. This limitation spurs the rise of newer family of models, i.e. MLLMs [5], [6]. Building upon LLMs, MLLMs are further equipped with capabilities of processing multimodal information, which considerably expands the task coverage of models. In the process of MLLM development, model evaluation has played crucial role since it quantitatively reflects Chaoyou Fu is the project leader. The authors are from the MME team (MME, Video-MME, MME-RealWorld), the MMBench team (MMBench, MMBench-Video, OpenCompass, VLMEvalKit), and the LLaVA team (LLaVA-NeXT, LLaVA-OneVision, LMMs-Eval). Chaoyou Fu and Caifeng Shan are with Nanjing University. E-mail: bradyfu24@gmail.com, cfshan@nju.edu.cn. Yi-Fan Zhang, Liang Wang, and Ran He are with Institute of Automation, Chinese Academy of Sciences. E-mail: yifanzhang.cs@gmail.com, {wangliang, rhe}@nlpr.ia.ac.cn. Shukang Yin and Sirui Zhao are with University of Science and Technology of China. E-mail: {xjtupanda, sirui}@mail.ustc.edu.cn. Bo Li and Ziwei Liu are with Nanyang Technological University. E-mail: drluodian@gmail.com, ziwei.liu@ntu.edu.sg. Xinyu Fang and Haodong Duan are with Shanghai AI Laboratory. E-mail: {fangxinyu, duanhaodong}@pjlab.org.cn. Corresponding author: Caifeng Shan (cfshan@nju.edu.cn). model strengths and drawbacks. This feedback efficiently facilitates the iteration of models and pushes for the advancement of the field. The upgraded models, in turn, stimulate the rise of new benchmarks that entail more advanced capabilities. As MLLMs have evolved at an amazing speed in recent years, dazzling new specifically designed evaluation benchmarks have emerged, as shown in Fig. 1. This brings inconvenience to researchers searching for apt benchmarks and those who aim to optimize current evaluation methods or introduce new benchmarks. To this end, this work presents comprehensive and systematic survey for MLLM evaluation, aiming to cover four key issues: 1) What capabilities are assessed? We organize hierarchical taxonomy of existing benchmarks. On the top level, these benchmarks can be categorized as evaluations of foundational capabilities, model behavior, and extended applications. 2) How to build benchmark? To be specific, we collate typical approaches in the pipeline of benchmark construction, including the gathering of samples and the annotation of Question-Answer (QA) pairs. We also discuss what may require special care during the assessment of models, e.g. data contamination, benchmark diversity, and sample size. 3) How to measure the performance? In terms of evaluation methods, we illustrate three representative ways to gauge the performance of MLLMs: human-based, LLM/MLLM-based, and script-based evaluation. In addition, we also introduce two major types of evaluation metrics as well as four evaluation toolkits. 4) Where is the direction of the next benchmark? We JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 2 Fig. 1: Time line of existing MLLM benchmarks. The center shows the number of benchmarks born at each time. discuss from the perspective of well-defined capability taxonomy, capability-oriented evaluation, task-oriented evaluation, and incorporating more modalities. We hope this survey can help researchers find the appropriate benchmarks more easily and spark explorations of benchmarks that better reflect model strengths and weaknesses, as well as more efficient and reasonable evaluation methods. We will regularly update new evaluation papers on our project page, organizing the community to work together to promote progress in this area."
        },
        {
            "title": "2 BACKGROUND",
            "content": "In this section, we briefly introduce the essentials of MLLMs, including the architecture and training. For more comprehensive illustration, we recommend the relevant work [5] that discuss MLLMs in detail."
        },
        {
            "title": "2.1 Architecture of MLLM",
            "content": "A typical MLLM comprises three modules: modality encoder [7], LLM, and connector between them, as presented in Fig. 2. Take the vision-language model as an example, given text query and vision sample, the vision encoder extracts features from the vision sample, while the connector aligns the vision features with the text embedding space. Subsequently, the aligned vision features are concatenated with the text embeddings of user queries as input. The LLM takes this multimodal input and generates natural language response. Similar to how LLMs process information, the core of MLLM is unified autoregressive modeling: p(wowV , wT ) (cid:89) t= (wtw<t, wV , wT ) (1) where wo = {wo,t}L t=1 is the output word token sequence of length L, wV represents the processed vision tokens, and wT corresponds to text embeddings of the user query. Fig. 2: Typical MLLM architecture. Tokenizer and DeTokenizer are used for the processing of text, as the standard flow of LLM. With respect to other modalities, specialized encoders and connectors are often required to convert them into tokens, as well pre-trained generators [10], [11] to enable multimodal generation capabilities. There are also methods that employ purely discrete modeling to achieve both understanding and generation [12]."
        },
        {
            "title": "2.2 Training of MLLM",
            "content": "We can see from Fig. 3, comprehensive training process of MLLMs consists of three stages, i.e. pre-training, instruction tuning, and alignment tuning. Pre-training. The main objective of the pre-training stage is to align different modalities [8] and inject multimodal world knowledge into models. The pre-training stage typically involves large-scale text based paired data, such as image caption data [9]. Generally speaking, the captions are translations of images, describing the content in natural language. To align vision with text, MLLMs learn to predict the ground-truth captions of the corresponding images in an autoregressive way. Instruction Tuning. Its purpose is to teach MLLMs to follow instructions from users and complete the required tasks. Tuning in this way, MLLMs can generalize to new tasks defined by new instructions, thereby boosting zeroshot performance. Instruction data can be derived from the adaptation of existing multi-task datasets, e.g. VQA, or JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 3 extensive public datasets, including 47 standard text-related visual benchmarks. The evaluation finds that, while MLLM surpasses the SOTA in commonsense tasks, it significantly lags behind leading supervised models in tasks such as image classification, OCR, and VQA. Similarly, LAMM [21] uses public datasets for evaluation, expanding beyond 9 common image tasks. The research indicates that MLLMs perform poorly in large-scale counting problems, only capable of rough estimations, and also struggle with fine-grained attribute differentiation. Although MLLMs possess object localization abilities, accurately predicting bounding boxes remains challenging, which can be effectively mitigated with further fine-tuning. Considering the limitations of existing traditional benchmarks, researchers have begun to design new evaluation datasets specifically for the characteristics of MLLMs. For example, MME [24] establishes comprehensive benchmark encompassing 14 perception and cognition tasks, where the latter consists of commonsense reasoning, numerical calculation, text translation, and code reasoning. Similarly, MMBench [22] features 20 distinct ability dimensions, including object localization and social reasoning. Seed-Bench [23] shares similarities with MME and MMBench but consists of lager number of multiple-choice question. SEED-Bench2 [25] further expands the QA pairs from 19K to 24K, covering 27 evaluation dimensions. MMT-Bench [26] scales up the dataset even further, incorporating 31K QA pairs from diverse scenarios. These benchmarks highlight some common traits. For instance, the performance of models improves significantly with increasing LLM scale [22], [26]. Fine-grained perception tasks, such as spatial localization and pixel-level perception, generally pose significant challenges to MLLMs [22], [24], [26], [35]. Besides, MLLMs often struggle with understanding charts and visual mathematics, with this limitations becoming more pronounced as dataset size increases [25], [26]. The interleave-image-text problem remains difficult to resolve, with related strategies during the training phase only partially alleviating the issue [22], [26]. Lastly, with recent advancements in MLLMs, the performance of open-source models has increasingly matched or even surpassed that of closed-source counterparts [22], [24], [35], demonstrating the rapid progress of the opensource community. Real-world usage scenarios have become focal point for researchers seeking to understand how models perform in practical applications. For instance, RealWorldQA1 evaluates fundamental spatial understanding capabilities sourced from real-life scenarios. These scenarios, though relatively straightforward for humans, often challenge stateof-the-art models. Similarly, BLINK [27] identifies tasks such as relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning that humans can solve within blink but present significant challenges for current MLLMs. WV-Bench [29] and VisIT-Bench [30] underscores the importance of evaluating human preferences and instruction-following capabilities in real-world applications. MME-RealWorld [35] places greater emphasis on quality and difficulty compared to its predecessor, containing the largest manually annotated QA pairs and 1. https://huggingface.co/datasets/visheratin/realworldqa Fig. 3: Illustration of three training stages of MLLMs. In the first stage, image-caption pairs are usually used for the modality alignment. In the second stage, the model is tuned on various QA pairs to make it capable of following instrctions. The third stage is responsible for making the model conform to human preferences. from self-instruction [13], [14], where data are synthesized by advanced MLLMs like GPT-4o. Given an image and an instruction, the model is trained to predict the response to the instruction, usually in conversational format. Alignment Tuning. It helps MLLMs align with specific human preferences, e.g. generate responses with fewer hallucinations [15], [16], [17]. The data used for this phase involves annotations of which response is better. This preference for response can either come from humans or from AI. The learning objective encourages response similar to the favored one while penalizing the unfavorable response."
        },
        {
            "title": "3.1.1 Comprehensive Evaluation",
            "content": "A primary objective behind designing MLLM is to develop intelligent chatbots capable of comprehensively answering human queries related to perception and reasoning. large number of evaluation benchmarks have emerged to assess the comprehensive capabilities of MLLMs. VQA v2 [18] is an early benchmark that includes 453K manually annotated QA pairs for model evaluation. It includes open-ended questions such as counting objects and distinguishing colors, but the answers are usually concise, such as one word. VizWiz [19] appears at approximately the time of VQA v2. It contains 8K QA pairs derived from the daily life scenarios of visually impaired individuals, effectively capturing the real-world needs of disabled users. However, these traditional benchmarks often fail to measure the emergent capabilities of todays MLLMs, such as powerful reasoning. There has been some works to bring together existing traditional benchmarks for comprehensive evaluation. For example, LVLM-eHub [20] compiles JOURNAL OF LATEX CLASS FILES, NOVEMBER"
        },
        {
            "title": "Comprehensive\nEvaluation",
            "content": "VQA v2 [18], VizWiz [19], LVLM-eHub [20], LAMM [21], MMBench [22], Seed-Bench [23], MME [24], SEED-Bench-2 [25], MMT-Bench [26], RealWorldQA, BLINK [27], MMStar [28], WV-Bench [29], VisIT-Bench [30] , MM-Vet [31], TouchStone [32], InfiMM-Eval [33], CV-Bench [34], MME-RealWorld [35]"
        },
        {
            "title": "OCR",
            "content": "TextVQA [36], OCR-VQA [37], WebSRC [38], OCRBench [39], SEED-Bench-2-Plus [40], VCR [41]"
        },
        {
            "title": "Chart and\nDocumentation",
            "content": "ChartQA [42], DocVQA [43], InfoVQA [44], DocGenome [45], MMLongBench-Doc [46], CharXiv [47], AI2D [48] , ComTQA [49], VisualMRC [50], LEAF-QA [51], FigureQA [52]"
        },
        {
            "title": "Mathematical",
            "content": "MathVista [53], MATH-Vision [54], OlympiadBench [55], MathVerse [56], We-Math [57]"
        },
        {
            "title": "Multidisciplinary",
            "content": "ScienceQA [58], MMMU [59], CMMU [60], CMMMU [61], MMMU-Pro [62]"
        },
        {
            "title": "Multilingual",
            "content": "CMMMU [61], CMMU [60], AlignMMBench [63], MTVQA [64], M3Exam [65], Urdu-VQA [66], Swahili-STR [67], ViOCRVQA [68], CVLUE [69] Foundational Capability (3.1)"
        },
        {
            "title": "Instruction Following",
            "content": "MIA-Bench [70] Multi-Round QA ConvBench [71], MMDU [72] Multi-Image NLVR2 [73], SparklesEval [74], MMDU [72], Mementos [75], MIRB [76], ReMI [77], MuirBench [78]"
        },
        {
            "title": "Interleaved Data",
            "content": "VEGA [79], SparklesEval [74], MMMU [59]"
        },
        {
            "title": "High Resolution",
            "content": "V*Bench [80], MME-RealWorld [35]"
        },
        {
            "title": "Visual Grounding",
            "content": "RefCOCO [81], RefCOCO+ [82], RefCOCOg [82], Ref-L4 [83] Fine-Grained Perception"
        },
        {
            "title": "Hallucination",
            "content": "FOCI [84], MMVP [85], LLVisionQA [86] Video-MME [87], MVBench [88], MLVU [89], LVBench [90], MMBench-Video [91], Event-Bench [92], VN-Bench [93], EgoSchema [94], TempCompass [95], MSVD-QA [96], TGIF-QA [97], ActivityNet-QA [98], MSRVTT-QA [96] POPE [99], GAVIE [100], M-HalDetect [101], HaELM [102], MMHal-Bench [103], Bingo [104], PhD [105], HallusionBench [106], AMBER [107] OpenChair [108], MHaluBench [109], VHTest [110], VALOR-Eval [111], VideoHallucer [112], HQH [113], R-Bench [114], VLind-Bench [115] Model Self-Analysis (3.2)"
        },
        {
            "title": "Safety",
            "content": "VLBiasBench [116], Bingo [104], MM-SpuBench [117] VLLM-safety-bench [118], MultiTrust [119], AttackVLM [120], AdvDiffVLM [121], MOSSBench [122] r t r c Extended Applications (3.3)"
        },
        {
            "title": "Causation",
            "content": "CELLO [123]"
        },
        {
            "title": "Medical Image",
            "content": "VQA-RAD [124], PathVQA [125], SLAKE [126], PMC-VQA [127], OmniMedVQA [128], RadBench [129], GMAI-MMBench [130]"
        },
        {
            "title": "Sentiment Analysis",
            "content": "EmoBench [131], FABA-Bench [132]"
        },
        {
            "title": "Remote Sensing",
            "content": "RSVQA [133], RSIVQA [134], VQA-TextRS [135], RSVG [136], RSVGD [137], RSIEval [138], RRSIS-D [139], VRSBench [140]"
        },
        {
            "title": "Agent",
            "content": "AppAgent [141], Mobile-Eval [142], GPT4Tools [143]"
        },
        {
            "title": "Code Generation",
            "content": "CharMimic [144], Web2Code [145]"
        },
        {
            "title": "GUI",
            "content": "RefExp [146], Screen2Words [147], ScreenQA [148] , Rico-semantics [149], ScreenAI [150], Widget [151]"
        },
        {
            "title": "Transfer Capability",
            "content": "VLAA [118], BenchLMM [152], MMCBench [153],"
        },
        {
            "title": "Knowledge Editing",
            "content": "MMEdit [154], VLKEB [155],"
        },
        {
            "title": "Embodied AI",
            "content": "EQA [156], EPIC-KITCHENS [157], Ego4D [158], EMQA [159], SQA3D [160], MoTIF [161], RH20T-P [162] EmbodiedScan [163]"
        },
        {
            "title": "Autonomous Driving",
            "content": "BDD-X [164], HAD [165], Talk2Car [166], Rank2Tell [167], DRAMA [168], NuScenes-QA [169], DriveLM [170], LingoQA [171], NuPrompt [172], Reason2Drive [173], MME-RealWorld [35], IDKB Dataset [174] Fig. 4: Categories of MLLM benchmarks. the largest image resolution. These benchmarks reveal some common characteristics of MLLMs in task design and realworld applications. Fine-grained perception tasks continue to challenge existing models [27], [29]. In contrast, models perform relatively well in artistic style recognition and relative depth perception tasks [27]. Additionally, while closedsource models such as GPT-4o generally outperform other models [27], [29], human performance in these tasks still significantly exceeds that of these general models. To facilitate the quantification of results, many studies simplify evaluation into binary or multi-choice problems [22], [24], [35]. However, relying solely on the corJOURNAL OF LATEX CLASS FILES, NOVEMBER 5 Fig. 5: Examples of different MLLM evaluation tasks. The answer can be Open-Ended, Yes-or-No, or Multi-Choice. rectness of final answers overlooks the crucial reasoning process, which is essential for understanding models capabilities. Hence, some works directly use the openended generation results and employ LLM-based evaluators to assess the performance, although this also faces the problem of inaccurate LLM scoring. For example, MMVet [31] introduces diverse question formats, requiring models to integrate various core vision-language capabilities to provide solutions. Similarly, TouchStone [32] emphasizes real-world dialogue capability and argues that evaluating only multiple-choice questions inadequately reflects multimodal dialogue capabilities. InfiMM-Eval [33] takes comprehensive approach by evaluating models on deductive, abductive, and analogical reasoning across various tasks. Notably, it assesses intermediate reasoning steps, aligning evaluations with practical scenarios like problem-solving in mathematics. These benchmarks reveal the capacities and challenges faced by MLLMs in handling complex tasks. Closed-source models excel in these areas [31], [33], but often struggle with understanding complex localization, structural relationships, charts, and visual mathematics [32]. High-resolution data particularly helps models in recognizing small objects, dense text, and fine-grained details [32]. Additionally, while CoT strategies significantly enhance reasoning abilities in closed-source models, their impact on open-source models remains limited. In the process of development, benmarks are constantly revising and improving according to past experience. For example, MMStar [28] identifies that many existing benchmarks allow models to solve problems using only textual inputs, potentially misleading assessments of true multimodal performance. To address this, it manually collects 1.5K QA pairs strongly correlated with visual information and introduces metrics to evaluate data leakage and genuine multimodal power. CV-Bench [34] recognizes the scarcity of vision-centric benchmarks and collects 2.6K samples to assess 2D and 3D visual understanding."
        },
        {
            "title": "3.1.2 Optical Character Recognition (OCR)",
            "content": "Current multimodal benchmarks increasingly focus on evaluating model performance in Optical Character Recognition (OCR) tasks, driving technological advancements in areas such as document understanding and transportation. Benchmarks have evolved from single scenarios to complex multiple scenarios. For instance, TextVQA [36] and OCR-VQA [37] focus on standard text recognition tasks, while InfoVQA [44] and WebSRC [38] introduce more intricate structural reasoning tasks, such as understanding web page structures and inferring information from infographics. SEED-Bench-2-Plus [40] and OCRBench [39] further broaden the scope of tasks by including diverse data types like charts, maps, and web pages, demonstrating that models can perform comparably to state-of-the-art supervised models in recognizing regular text, irregular text, occluded text, and artistic text. Additionally, VCR [41] addresses variants of OCR where text is embedded in images and partially occluded, requiring models to restore the specific content of the text from the images. However, many MLLMs still face challenges in fine-grained OCR capabilities, handwriting, non-semantic text, and multilingual text recognition [39], [40], [41], [175]. MLLMs like GPT-4V have shown exceptional performance in several evaluations [39], [40], [176], but still lag behind models trained specially on OCR tasks [175]. Besides, the impact of different data types on JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 model performance varies significantly. For instance, knowledge graphs and maps pose more challenges than simple charts [40]. This suggests that optimizing models for specific data types or introducing professional OCR component could lead to substantial performance improvements [177]."
        },
        {
            "title": "3.1.3 Chart and Documentation",
            "content": "Charts and documents are important data types in practical applications, designed to convey information in an efficient way. Unlike natural images, these data are highly structured and dense in information, requiring models to understand the layouts and the relationships between the embedded elements. With the purpose of developing models that can understand and reason with such data, benchmarks for different types of charts [42], [44], [47], [48], [49], [50], [51], [52] and documents [43], [45], [46] have been proposed. ChartQA [42] focuses on VQA with charts, such as bar, line, and pie plots. The questions range from ones that demand simple data retrieval to more complex compositional ones that require both data extraction and math reasoning. DocVQA [43] is built for VQA on document images scraped from industry documents. The questions generally focus on simpler information extraction tasks. InfoVQA [44] centers on understanding infographic images, type of data designed to convey information compactly. Due to this nature, the layouts and structures of infographics are more diverse than conventional charts. Questions in this benchmark generally require basic reasoning and arithmetic skills. As MLLMs evolve, recent benchmarks shift to the understanding of more complex charts and documents. For example, DocGenome [45] focuses on the analysis of scientific papers, with tasks ranging from information extraction and layout detection to VQA and code generation. CharXiv [47] centers on challenging charts from scientific papers. MMLongBench-Doc [46] focuses on general long document understanding, where documents span an average of 47.5 pages. Although the performance gap between proprietary models and open-source models is closing on more traditional benchmarks like ChartQA, DocVQA, and InfoVQA, it is still wide on more challenging benchmarks like CharXiv and MMLongBench-Doc. Moreover, current MLLMs still struggle with 1) reasoning questions that require more than simple information extraction [47] and 2) long-context document understanding [46], where understanding long multimodal context is critical."
        },
        {
            "title": "3.1.4 Mathematical Reasoning",
            "content": "Visual math problem-solving capability is critical aspect of MLLM evaluation, giving rise to many specifically designed benchmarks. MathVista [53] is an early attempt in this direction, which collates samples from existing datasets as well as newly created ones. The images vary from mathematical illustrations, such as geometry diagrams and bar charts, to different scenes and domains, such as abstract scenes and medical images. Subsequent works develop more challenging benchmarks [54], [55], and design more fine-grained settings for evaluation [56], [57]. For instance, We-Math [57] decomposes problem into sub-problems based on knowledge concepts and evaluates MLLMs at the level of basic knowledge concepts. To evaluate to what extent MLLMs 6 understand math diagrams, MathVerse [56] transforms each problem into 6 different versions, each of which contains different proportions of vision and text content. Overall, though some promising results are achieved by GPT-4V [53], some critical issues remain unresolved. First, most current MLLMs struggle to understand complex visual diagrams [53] and rely heavily on textual questions [56]. Second, most MLLMs tend to solve composite problems through rote memorization without the capabilities to correctly answer the sub-problems [57]."
        },
        {
            "title": "3.1.5 Multidisciplinary",
            "content": "The mastery of multidisciplinary knowledge is an important indicator of the models expertise. Multiple benchmarks for this sort of evaluation have been developed. ScienceQA [58] is benchmark of scientific questions with annotations of lectures and explanations for ease of chain-of-thought evaluation. The benchmark covers grade-level (112) knowledge across various domains. MMMU [59] is more challenging benchmark that covers broad subjects and collegelevel questions, including engineering, art and design, business, science, humanities and social science, and medicine. The format of questions further develops from single image-text pair into interleaved text and images. Similarly, CMMU [60] (grade-level knowledge) and CMMMU [61] (college-level knowledge) are domain-specific benchmarks in Chinese contexts. The comprehensive evaluation of these works reveals that even the advanced models (such as GPT4V and Gemini Ultra) can only achieve accuracies lower than 60%, which suggests huge space for improvement towards AGI."
        },
        {
            "title": "3.1.6 Multilingual",
            "content": "MLLMs are progressively developed towards multilingualism to benefit larger community. Apart from the predominant English, researchers have collected benchmarks in other languages to accommodate evaluation under other cultural contexts and customs, including Chinese [60], [61], [63], [69], Urdu [66], Swahili [67], Vietnamese [68], and multi-languages [64], [65]. For example, CMMMU [61] follows MMMU [59] and collects multidisciplinary benchmark in Chinese. Works like ViOCRVQA [68], Urdu-VQA [66], and Swahili-STR [67] evaluate OCR and VQA capabilities in other languages. Video-MME [178] is dedicated to multilingual evaluation category that includes the worlds dominant languages. MTVQA [64] and M3Exam [65] develop multilingual benchmarks across 9 different languages. The evaluation reveals that the performance varies greatly when evaluated in different languages. Notably, both proprietary models and open-source models perform better in Indo-European languages that use the Latin alphabet, such as German, French, and Italian, which might be attributed to their visual and linguistic similarities with English [64]."
        },
        {
            "title": "3.1.7 Instruction Following",
            "content": "Instruction following refers to the ability to comply with user instructions and perform specified tasks. As foundational capability, instruction following directly influences response quality and user experience. MIA-Bench [70] is designed to evaluate how well MLLMs can adhere to complex JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 instructions. It comprises set of 400 image-prompt pairs with layered instructions, each of which focuses on specific point, e.g. length limit, genre, and grammar. Evaluation results reveal that the proprietary GPT-4o achieves the best performance (score 88.58), while the best-forming opensource model, LLaVA-NeXT-110b [179] only achieves score of 79.84, suggesting gap in following complex instructions. Moreover, strong correlation between LLM size and MIABench performance is observed, validating the scaling law in instruction following capability."
        },
        {
            "title": "3.1.8 Multi-Round QA",
            "content": "Current MLLMs are generally developed as multi-round chatbots, while most benchmarks remain at the single-round QA stage. Multi-round QA benchmarks are developed to align with real-world conversation scenarios, simulating the human-AI interaction setting with long-context history. ConvBench [71] develops progressive evaluation scheme, with each round focusing on specific capability, i.e. perception, reasoning, and creation. The evaluation is performed at both the single-round level and the overall conversation level. Evaluation results reveal that insufficient fine-grained perception in MLLMs leads to reasoning and creation failures. MMDU [72] engages in multi-turn and multi-image conversations, where conversation sample can include up to 20 images and 27 turns. The analysis points out that the gap between open-source models and closed-source ones can be attributed to limited conversational instruction tuning data."
        },
        {
            "title": "3.1.9 Multi-Image Understanding",
            "content": "With the evolution of MLLMs, researchers have explored upgrading vision capabilities from single image to multiple image. In line with this tendency, some benchmarks for multiple images are compiled. NLVR2 [73] is early benchmark, where each sample contains pair of similar images and natural language caption. The task is to decide whether the caption is true with respect to the pair of images. Recently proposed benchmarks are more specifically designed for the evaluation of MLLMs. For example, SparklesEval [74] challenges models conversational proficiency across multiple images and multiple turns. The user prompt is presented in the flexible form of interleaved text and images. Each instance contains two rounds of dialogue with four images in total. Similarly, MMDU [72] is multi-image and multiround benchmark with maximum of 20 images and 27 turns inside single sample. There are some other benchmarks pay more attention to reasoning with multiple highly correlated images. Mementos [75] is designed to evaluate MLLMs capabilities of understanding sequential images, covering daily life, robotics, and comics domains. MIRB [76] aims to assess the ability to answer by aggregating and reasoning with information from multiple images. It encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. ReMI [77] designs 13 tasks with various input formats and relationships between images, e.g. from same or different concepts. MuirBench [78] devises 12 multi-image understanding tasks, e.g. scene understanding and visual retrieval, with diverse multi-image relations like 7 multiview and temporal relations. To ensure robust assessment, each instance is paired with an unanswerable variant with minimal semantic differences. Evaluations suggest that though open-source models are approaching the performance of advanced closed-source models like GPT-4V on single-image benchmarks, large gap remains in multi-image reasoning ones [76]. Moreover, current MLLMs generally find it challenging to solve multi-image problems: even best-forming proprietary models GPT-4o/Gemini Pro only achieve 68.0%/49.3% in accuracy, while open-source models trained on single images can barely generalize to multi-image questions, reaching an accuracy lower than 33.3% [78]."
        },
        {
            "title": "3.1.10 Interleaved Images and Text",
            "content": "Interleaved images and text are natural forms of information delivery, and prevalent on the Internet in media like blogs and news. While most benchmarks adopt the image-text non-interleaved format, there are multiple benchmarks have been developed to evaluate models ability to understand interleaved content. In MMMU [59], the format of questions is interleaved text and images. SparklesEval [74] adopts similar format and two-round prompt fashion. VEGA [79] is specifically designed for the evaluation of interleaved image-text comprehension. The proposed task requires models to discern useful images and text from superfluous ones and derive the correct answer. Evaluation results show that advanced proprietary MLLMs such as GPT-4V and Gemini 1.5 pro only achieve modest performance, suggesting large room for improving interleaved information processing."
        },
        {
            "title": "3.1.11 High Resolution",
            "content": "Processing images of high resolution is an important capability of MLLMs, especially in practical applications like autonomous driving. V*Bench [80] is designed to assess performance in processing high-resolution images and focusing on correct visual details. This benchmark contains 191 high-resolution images with resolution of 2,246x1,582 on average. Two sub-tasks are designed: the attribute recognition task aims to recognize the attribute such as color or material of an object; the spatial relationship reasoning task requires the model to determine the spatial relationships between two objects. MME-RealWorld [35] has 13,366 images averaging 2,0001,500 resolution, including the real-world tasks of video monitoring, autonomous driving, remote sensing, diagram table, and OCR in the wild. The evaluation results show that even the most advanced MLLMs have not achieved more than 60% accuracy, suggesting the difficulty of these scenarios."
        },
        {
            "title": "3.1.12 Visual Grounding",
            "content": "Visual grounding is classical computer vision task that aims to locate the most relevant object/region specified by natural language query [181], [182]. The query is usually short expression, such as woman in red. On the traditional benchmarks like RefCOCO [81], RefCOCO+ [82], and RefCOCOg [82], MLLMs have already achieved performance comparable to SOTA specialist models [183], [184]. In view of the relatively high labeling error rates in RefCOCO series, JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 Benchmarks QA Pairs Answer Type Metrics Categories Data Collection TABLE 1: Statistics of representative MLLM benchmarks. 8 Annotation Manually Manually Manually Manually EmbodiedAI Incorporating Samples From Existing Datasets Automatically Constructing Deterministic Deterministic Deterministic Deterministic Deterministic Deterministic Deterministic Comprehensive Evaluation Incorporating Samples From Existing Datasets Visual Grounding Incorporating Samples From Existing Datasets Visual Grounding Incorporating Samples From Existing Datasets Visual Grounding Incorporating Samples From Existing Datasets OCR Incorporating Samples From Existing Datasets EmbodiedAI(Video) Gathering Data From the Internet Deterministic Comprehensive Evaluation Incorporating Samples From Existing Datasets Deterministic Multi-Image Gathering Data From the Internet Deterministic Comprehensive Evaluation Incorporating Samples From Existing Datasets Deterministic Deterministic Deterministic Video Understanding Gathering Data From the Internet High Resolution Incorporating Samples From Existing Datasets Multidisciplinary Gathering Data From the Internet Deterministic Chart and Documentation Gathering Data From the Internet Deterministic Autonomous Driving(Video) Incorporating Samples From Existing Datasets Scoring Video Understanding Gathering Data From the Internet Deterministic Chart and Documentation Gathering Data From the Internet Deterministic Hallucination Incorporating Samples From Existing Datasets Deterministic Chart and Documentation Gathering Data From the Internet Deterministic Chart and Documentation Gathering Data From the Internet Automatically Constructing Deterministic OCR Incorporating Samples From Existing Datasets Automatically Constructing Open-Ended/Multi-Choice Deterministic Mathematical Gathering Data From the Internet Deterministic Comprehensive Evaluation Gathering Data From the Internet Deterministic Embodied AI(Video) Gathering Data From the Internet Scoring Comprehensive Evaluation Gathering Data From the Internet Deterministic Comprehensive Evaluation Incorporating Samples From Existing Datasets Deterministic Remote Sensing Gathering Data From the Internet Automatically Constructing Deterministic Chart and Documentation Gathering Data From the Internet Open-Ended/Multi-Choice Deterministic Deterministic Medical Image Remote Sensing Incorporating Samples From Existing Datasets Incorporating Samples From Existing Datasets Open-Ended/Multi-Choice Deterministic Multidisciplinary Gathering Data From the Internet Deterministic Chart and Documentation Gathering Data From the Internet Open-Ended/Yes-or-No Deterministic Medical Image Gathering Data From the Internet Automatically Constructing Open-Ended Open-Ended Deterministic Autonomous Driving(Video) Gathering Data From the Internet Deterministic Autonomous Driving(Video) Gathering Data From the Internet Manually Manually 288.8k Open-Ended/Multi-Choice Deterministic Comprehensive Evaluation Incorporating Samples From Existing Datasets Automatically Constructing Open-Ended/Yes-or-No Deterministic Medical Image Gathering Data From the Internet Automatically Constructing Deterministic Video Understanding Gathering Data From the Internet Automatically Constructing Deterministic Chart and Documentation Gathering Data From the Internet Automatically Constructing Open-Ended/Multi-Choice Deterministic Medical Image Incorporating Samples From Existing Datasets LLMs/MLLMs Deterministic Video Understanding Gathering Data From the Internet Deterministic Deterministic Deterministic Scoring Deterministic Scoring Deterministic Scoring Scoring Agent GUI GUI Gathering Data From the Internet LLMs/MLLMs Gathering Data From the Internet Gathering Data From the Internet Hallucination Incorporating Samples From Existing Datasets Safety Incorporating Samples From Existing Datasets Hallucination Incorporating Samples From Existing Datasets OCR Incorporating Samples From Existing Datasets Hallucination Hallucination Incorporating Samples From Existing Datasets LLMs/MLLMs Modifying Existing Data Manually VQA v2 [18] RefCOCO [81] RefCOCO+ [81] RefCOCOg [81] EmbodiedQA [156] TextVQA [36] Ego4D [158] VizWiz [19] NLVR2 [73] MME [24] Video-MME [87] MME-RealWorld [35] ScienceQA [58] DocVQA [43] BDD-X [164] ActivityNet-QA [98] AI2D [180] POPE [99] ChartQA [42] FigureQA [52] OCR-VQA [37] MMBench [22] EPIC-KITCHENS [157] MathVista [53] MM-Vet [31] SEED-Bench [23] RSVQA [133] InfoVQA [44] SLAKE [126] RSIVQA [134] VisualMRC [50] MMMU [59] PATHVQA [125] HAD [165] Talk2Car [166] LVLM-eHub [20] VQA-RAD [124] MVBench [88] LEAF-QA [51] GPT4tools [143] Screen2Words [147] Widget Captioning [151] MMHAL-BENCH [103] AttackVLM [120] PMC-VQA [127] EgoSchema [94] M-HalDetect [101] OCRBench [39] GAVIE [100] HallusionBench [106] 453.0k 10.0k 10.0k 14.0k 5.0k 5.7k 12.7k 8.0k 2.0k 2.3k 2.7k 29.4k 21.0k 50.0k 6.9k 58.0k 4.5k 3.0k 32.7k 1.0M 100.0k 3.2k 10.9k 6.1k 218 19.2k 299.6k 3.2k 2.1k 111.0k 30.6k 11.5k 6.0k 5.7k 451 4.0k 1.6M 652 21.5k 12.3k 96 50.0k 2.0k 5.0k 3.2k 1.0k 1.0k 1.1k Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Yes-or-No Yes-or-No Multi-Choice Multi-Choice Multi-Choice Open-Ended Open-Ended Open-Ended Multi-Choice Yes-or-No Open-Ended Yes-or-No Open-Ended Multi-Choice Open-Ended Open-Ended Multi-Choice Open-Ended Open-Ended Open-Ended Open-Ended Multi-Choice Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Open-Ended Multi-Choice Open-Ended Open-Ended Open-Ended Yes-or-No Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually Manually new Ref-L4 [83] benchmark is proposed. Compared with predecessors, it features broader category coverage, more annotations, and longer referring expressions composed of an extensive vocabulary. The evaluation results reveal that SOTA open-source models can achieve an average accuracy of about 66%, leaving much room for improvement. Moreover, current MLLMs are sensitive to the scale of instances, generally performing worse on targets of small sizes."
        },
        {
            "title": "3.1.13 Fine-Grained Perception",
            "content": "Different from general coarse-grained classification tasks, fine-grained perception focuses on more fine-grained recognition of objects, e.g. answering the specific dog breed rather than single dog, which can be more important for downstream applications. FOCI [84] is new benchmark designed for the evaluation of MLLMs in this task. It uses 4 domain subsets from ImageNet-21k as the base, and then collects 5 additional popular classification datasets as supplement. MMVP [85] identifies 9 distinct patterns that CLIP-based models generally underperform and design corresponding questions, such as orientation and direction, color, and appearance. Evaluations of SOTA MLLMs suggest that both open-source and closed-source models struggle with visual details, where only Gemini and GPT-4V achieve performance higher than random guessing. LLVisionQA [86] evaluates the ability to perceive and discern low-level attributes, such as blur and brightness. The results suggest that most open-source MLLMs can achieve an accuracy above 50%, significantly outperforming random guessing (37.94% in accuracy) without explicit training on low-level visual attributes. However, open-source models still lag behind closed-source GPT-4V or humans. Notably, GPT4V achieves performance on par with junior-level humans (73.36% vs. 74.31%)."
        },
        {
            "title": "3.1.14 Video Understanding",
            "content": "Traditional video-QA benchmarks, such as MSVD-QA [96], TGIF-QA [97], and ActivityNet-QA [98], are generally domain and task specific. For example, MSVD-QA [97] mainly covers action and object recognition, and the answer is very JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 brief. ActivityNet-QA [98] mainly encompasses videos of general human activities. With the success of MLLMs in the image realm, more and more works are devoted to leveraging MLLMs for video understanding. Along with the progress of MLLMs, more challenging and comprehensive video understanding benchmarks have emerged. Video-MME [87] is an early exploration in this regard, featuring various video domains (6 domains, 30 subfields) and lengths (11 seconds to 1 hour). The used modalities include video frames, subtitles, and audio. The videos are manually collected, and all QA pairs are manually annotated to ensure quality. MVBench [88] defines pool of temporal tasks and leverages ChatGPT to automatically reannotate existing video datasets with their original annotations. MMBench-Video [91] is characteristic of free-form questions and detailed answers for videos spanning from 30 seconds to 6 minutes. MLVU [89], LVBench [90], Event-Bench [92], VNBench [93], and the long video track of Video-MME mainly focus on long-video understanding, which challenges models capabilities in comprehension of long multimodal context. Specifically, MLVU [89] features diversified video content, video durations, and evaluation tasks. LVBench [90] selects videos longer than 30 minutes and defines 6 core capabilities for long video understanding. Event-Bench [92] pays attention to the event understanding ability with three-level hierarchy, including atomic event, composite event, and overall understanding. VN-Bench [93] designs video-needle-in-a-haystack framework, belonging to synthetic method for benchmark generation. By inserting irrelevant images or text into videos, it can evaluate tasks such as retrieval, ordering, and counting. There are some benchmarks concern more on specific scenarios and nuanced capabilities. EgoSchema [94] covers QA samples of egocentric videos. TempCompass [95] includes the evaluation of fine-grained temporal perception capabilities, such as the play speed of videos, directions of the camera or object, and changes in object attributes. Overall, current MLLMs, both proprietary and opensource, suffer from performance degradation when processing longer videos [87], [89], [90], [93], which suggests that context length may be critical factor to be considered. Moreover, open-source MLLMs perform poorly on temporal perception tasks and tend to rely on static visual cues [95]. Therefore, enhancements in temporal perception abilities are in urgent need for future works. 9 Benchmarks in this category seek to identify hallucinations more comprehensively. POPE [99] designs simple discriminative task: the benchmark gauges the degree of object hallucination by simply prompting whether specific object exists in an image. M-HalDetect [101] instead evaluates generative performance, specifically modeling descriptions on the sub-sentence level. AMBER [107] includes both discriminative and generative tasks, covering the evaluation of existence, attribute, and relation hallucinations. In line with the progress of MLLMs for video understanding, VideoHallucer [112] is proposed to comprehensively evaluate hallucinations in video understanding, covering subcategories such as object relation, temporal, and semantic detail hallucinations. Meanwhile, some works explore automatic and efficient constructions of evaluation samples, where the image is synthetic instead of natural. For instance, PhD [105], MHaluBench [186], VHTest [110], and OpenCHAIR [108] adopt text-to-image generative models, e.g. Dall-E 3, to synthesize desired images. Researchers have also developed more targeted benchmarks to probe model tendencies and categorize the causes of hallucination. GAVIE [100] observes the bias towards positive instances and introduces both positive and negative instructions for various tasks, such as attribute detection, OCR, and VQA. HallusionBench [106] incorporates control groups of visual questions for ease of analyzing models response tendencies and failure modes. Bingo [104] identifies two categories of hallucination causes, i.e. bias and interference, and designs corresponding visual questions for investigation. Similarly, VLind-Bench [115] is aimed to assess the extent to which MLLMs lean toward language priors and lead to hallucinations. These more in-depth studies have brought deeper understanding of the formation mechanism of hallucinations. According to the evaluation results, there are mainly two factors that result in hallucinations: 1) Current MLLMs suffer from insufficient visual capabilities [104], [106]. For example, MLLMs are prone to be misled by simple image manipulations [106] or leading questions [104]. Moreover, when faced with multiple images, even the advanced GPT4V struggles with discerning nuanced differences [104] or reasoning temporal relations [106], which indicates insufficient capabilities of handling image sequence. 2) Bias in models. MLLMs can exhibit varied performance for different types of visual questions, often correlated with regions, cultures, and languages [104]. This may be due to the imbalance of training data that is remembered in the model."
        },
        {
            "title": "3.2.2 Bias",
            "content": "In order to better understand MLLM itself, researchers have developed various benchmarks to study the behavior or characteristics of models, including hallucination, model bias, safety, and causal analysis. In this section, we introduce typical aspects of model analysis."
        },
        {
            "title": "3.2.1 Hallucination",
            "content": "The term multimodal hallucination is used to describe the phenomenon, where the response content generated by MLLMs is inconsistent with the visual content [185]. Hallucination is critical issue damaging model reliability and hindering its practical application. Model bias is critical issue that hinders the usability of MLLMs. Current benchmarks have explored different aspects of model bias and shed light on possible reasons. VLBiasBench [116] identifies response bias unaligned with human values. Specifically, the benchmark covers 9 categories of social biases, such as age, gender, and physical appearance. The evaluations on open-source and closed-source models reveal that the open-source models like LLaVA [187] and Shikra [188] generally show different degrees of bias, while the advanced closed-source model, Gemini [189], consistently exhibits weak bias. This suggests huge gap between open-source and closed-source models in terms JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 of social bias control. Bingo [104] recognizes regional bias in the model performance of MLLMs, i.e. the models show highly varied performance when prompted with visual questions of different regional/cultural contexts. Three categories of bias are considered, including region, OCR, and factual bias. MM-SpuBench [117] probes spurious bias, tendency to utilize spurious correlations for predictions. The authors ascribe this to the learning process of models, where coarse-grained alignment between visual tokens and textual descriptions can lead to false correlations. These false priors embedded in parametric memory can interfere with predictions in counterintuitive scenarios. For example, high co-occurrences of two objects/attributes may lead to false predictions, such as recognizing scene with microwave as kitchen. The evaluation results indicate that closedsource models generally outperform open-source models. Moreover, modality alignment plays critical role in suppressing spurious biases, where better alignment techniques can improve robustness to spurious bias."
        },
        {
            "title": "3.2.3 Safety",
            "content": "Model safety is of central concern for model deployment in practice. Benchmarks of this type mainly consider robustness, including Out-Of-Distribution (OOD) robustness and adversarial robustness, as well as jailbreaks. OOD Robustness. It mainly considers MLLMs ability to generalize to unseen domains, such as different styles of images not encountered in the training corpus. For example, OODCV-VQA and Sketchy-VQA [118] incorporate rarely seen images in real-life scenarios and simple sketchy images, respectively. Moreover, OOD text instructions adapted from the original questions are also included. MultiTrust [119] further considers images from other domains, e.g. MRI and infrared images. The evaluation results show that MLLMs are better at understanding OOD visual content rather than following OOD text instructions [118]. This might indicate insufficient capabilities to generalize to new instructions. Adversarial Robustness. Adversarial attacks on MLLMs aim to trick models into making wrong responses. Correspondingly, adversarial robustness is critical aspect to evaluate, which measures how robust models are to malicious attacks. AttackVLM [120] develops framework to synthesize adversarial samples and evaluate the adversarial robustness of open-source MLLMs. The evaluation results reveal the adversarial vulnerability of open-source models like LLaVA [14] and MiniGPT-4 [190]. AdvDiffVLM [121] aims to improve the efficiency and transferability for the generation of adversarial samples. Experimental results show that, compared with open-source models, closedsource models exhibit better adversarial robustness, suggesting large room for improvement. Jailbreaks. It centers on models capabilities to reject attempts to elicit illegal responses [119], [191]. VLLM-safetybenchmark [118] designs two jailbreak strategies targeting LLM and ViT respectively, to assess model resilience. MultiTrust [119] incorporates three tasks to test models robustness against jailbreaking, including 1) inserting detailed jailbreaking prompts into images, 2) combining normal textual prompts with jailbreaking prompts inserted into images, and 3) jailbreaking prompts paired with positively or negatively correlated images. These studies reveal that 10 1) compared with modern LLMs that need to be jailbroken with well-designed prompts, MLLMs are more vulnerable when simple yet harmful instructions are embedded in images [119] and 2) current tuning of MLLMs impairs safety protocols embedded in LLMs [118], [119]. Moreover, MOSSBench [122] evaluates MLLMs oversensitivity to certain visual stimuli, rejecting harmless queries regardless of the benign contexts. Three types of stimuli are included in the benchmark samples, including exaggerated risk, negated harm, and counterintuitive interpretation. The evaluation of 20 MLLMs highlights that oversensitivity is prevalent across current MLLMs, especially for those safer models, which potentially suggests trade-off between the safety and conservatism of model responses."
        },
        {
            "title": "3.2.4 Causation",
            "content": "It refers to the cause-and-effect relationship where change in one variable results in change in another variable [123]. The ability to understand this relationship, i.e. causal reasoning, is an important ability to understand and analyze our world. Recently, some works have explored the evaluation of MLLMs in terms of causal reasoning capabilities. CELLO [123] introduces unified definition of causality involving humans and/or objects, and constructs benchmark of 12 causal tasks. Evaluations show that current MLLMs, such as BLIP-2 [192] and Claude3 Sonnet [193], exhibit weak causal reasoning abilities with some underperforming random guessing."
        },
        {
            "title": "3.3 Extended Applications",
            "content": "With the rapid development of MLLMs, researchers have actively explored the application in downstream tasks and developed corresponding benchmarks in the fields such as medicine and emotion. Compared with general evaluation, these benchmarks focus more on the mastery of domain knowledge and skills."
        },
        {
            "title": "3.3.1 Medical Image",
            "content": "Medical images directly reflect the state of the human body and are critical parts of clinical decision-making. number of benchmarks have been developed to evaluate the performance of MLLMs in analyzing this type of image. VQA-RAD [124] is an early benchmark designed for the VQA task on radiology images, encompassing 11 question types, including plane, modality, organ system, etc. The question and answer are generally simple and concise, with the answer spanning only one or few words. PathVQA [125] is similar benchmark focusing on pathology images. SLAKE [126] is bilingual (Chinese and English) benchmark with more annotations in terms of modalities, including segmentation masks and bounding boxes. Recent benchmarks are heading towards more comprehensiveness. PMC-VQA [127] encompasses more image domains, including radiology, pathology, microscopy, signals, etc. RadBench [129] covers both 2D and 3D scan images and 5 distinct tasks, including modality recognition, disease diagnosis, VQA, report generation, and rationale diagnosis. GMAI-MMBench [130] incorporates 39 medical image modalities, 18 clinical-related tasks, 18 departments, and JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 4 perceptual granularities in the VQA format. OmniMedVQA [128] covers more than 20 anatomical regions and 12 different modalities, such as MRI, CT, and X-ray, with images sourced from authentic medical scenarios. Evaluation results with 12 open-source MLLMs show that current MLLMs perform poorly on OmniMedVQA, where most MLLMs slightly outperforming random guessing. Moreover, even the best-forming medical-domain MLLM, MedVInT [127], does not perform so well as general-purpose models like BLIP-2 [192] (41.50% vs. 50.69% in accuracy), which might be attributed to lack of large-scale training with high-quality image-text pairs from medical domains. These results suggest that there is long way off in developing medical-purpose MLLMs."
        },
        {
            "title": "3.3.2 Emotion Analysis",
            "content": "Emotion analysis aims to extract human emotions from data of various modalities, such as vision, text, and audio. Different from common tasks that are largely objective, emotion analysis entails interpreting highly subjective and emotional multimodal content, thus posing new challenges. With its powerful generalization and reasoning capabilities, MLLMs are expected to make breakthrough in this task. EmoBench [131] contains tasks ranging from general emotion and intention understanding (multiclass classification from pre-defined sets) to emotion detection in social media (binary classification, Yes/No), with data sourced from existing datasets. FABA-Bench [132] focuses i.e. on facial emotion analysis, emotion recognition and action unit recognition. The evaluation results on these benchmarks reveal that MLLMs fine-tuned with emotion-related data can achieve superior performance compared with zero-shot MLLMs, including advanced closed-source models like GPT-4V. This suggests that it is essential to inject emotion-domain knowledge for downstream tasks of emotion analysis. incorporating two tasks,"
        },
        {
            "title": "3.3.3 Remote Sensing",
            "content": "Remote sensing is multidisciplinary field that involves the acquisition and analysis of information about the Earths surface and atmosphere from distance, typically using satellites or aerial sensors. Remote sensing plays crucial role in numerous applications, such as environmental monitoring, urban planning, agriculture, and disaster management. Multiple benchmarks have been developed to advance the understanding of remote sensing images. Early works such as RSVQA [133] builds evaluation sets in the form of traditional VQA, covering tasks like classification, object counting, and detection. The questions and answers in the RSVQA benchmark are concise and built from pre-defined pipelines based on elements (e.g. road and water area) and associated attributes (e.g. shape and size) or positional relations. The two subsets of the benchmark contain images of low resolution (256px) and high resolution (512px), respectively. More recent benchmarks enjoy wider coverage of tasks and QA pairs. For instance, RSIEval [138] manually annotates captions and visual questions. Apart from common object-related questions involving existence, quantity, or color, the benchmark also includes questions that require reasoning/external knowledge, such as What season was this image taken in?. Similarly, VRSBench [140] 11 is comprehensive benchmark that includes image captioning, visual grounding, and VQA tasks. Notably, the bounding box annotations are oriented to facilitate the evaluation of more advanced grounding capabilities. There are also some benchmarks like RSVG [136], RSVGD [137], and RRSIS-D [139] focus on visual grounding in remote sensing images, trying to locate objects using bounding boxes or segmentation masks, given natural language query. The evaluation results show that even GPT-4V struggles to handle VQA and grounding tasks [140], which suggests the necessity to inject domain knowledge into MLLMs. Moreover, specifically fine-tuned MLLMs can achieve comparable or superior performance to specialist models [138], indicating the potential of using MLLMs to solve remote sensing tasks."
        },
        {
            "title": "3.3.4 Agent",
            "content": "An intelligent agent can perceive the environment and take action to fulfill target tasks. Recently, developing multimodal agents that can process and reason with multimodal information, e.g. vision, audio, and text, has aroused wide attention [194], where MLLMs are playing pivotal role. In line with this progress, multiple benchmarks have been built to gauge the performance of MLLMs in acting as agents. AppAgent [141] mainly assesses agents abilities to perform 50 tasks on 10 smartphone applications like Google Maps, as the instruction like change my profile name to AppAgent. The used metrics include successful rate, reward, and average steps. Mobile-Eval [142] is similar benchmark designed to evaluate mobile agents. This benchmark contains 3 instructions for each of the 10 mainstream Apps. GPT4Tools [143] centers on the capability of tool usage, with metrics designed for different aspects, including overall successful rates and successful rates in terms of applying specific tools, such as thought, tool name, and tool arguments. Evaluation results show that even advanced GPT-4 struggles to plan and execute smartphone application queries in zero-shot way, partially due to the challenges of accurately predicting coordinates [141], [142] or insufficient knowledge of the specific applications, which entails more explorations to solve."
        },
        {
            "title": "3.3.5 Code Generation",
            "content": "Code generation is an essential capability of MLLMs, which has wide range of applications in real life, such as assisting in writing code or providing automatic solutions for complicated problem. ChartMimic [144] concerns two chart-to-code generi.e. direct mimic and customized mimic. ation tasks, The latter refers to generating new charts with similar styles/aesthetics and customized data. The benchmark covers various types of figures with 1000 humancurated triplets, i.e. figure, Python code, and instruction. WCGB [145] revolves around webpage-to-code generation, aiming to assess the ability to translate webpage screenshots into HTML code. According to the evaluation results, the code generation ability of the LLM backbone plays an important role [145] in multimodal code generation. Opensource MLLMs still largely lag behind closed-source models, with the best-performing Phi-3-Vision [195] only achieving half the performance of GPT-4V. Besides, open-source JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 models exhibit notable deficiencies in generating executable code, most of which achieve rate of below 60% [144]."
        },
        {
            "title": "3.3.6 Graphical User Interface (GUI)",
            "content": "Current multimodal benchmarks are extending into the domain of GUI to evaluate the performance of MLLMs in percepting and reasoning GUI elements. Starting with the early RefExp [146] benchmark, which focuses on object localization within UI screens, the research has evolved to more complex tasks. Widget Captioning [151] increases the challenge by requiring models to generate descriptive language for UI elements, testing their perception capability. Screen2Words [147] further pushes the boundaries by demanding models to generate content and functionality descriptions for UI nodes, thereby testing their understanding of page layout and functionality. As research progresses, ScreenQA [148] simplifies the evaluation process by using only image and text inputs to focus on basic QA tasks for locating and identifying UI elements based on textual prompts. Rico-semantics [149] annotates 500K UI element attributes and relationships, enhancing the evaluation dimensions to assess models understanding of UI element shapes and semantic associations. In these benchmarks, MLLMs exhibit several notable limitations. First, at the task level, current models struggle with understanding the design of small icons and UI components specific to certain domains, and exhibit deficiencies in fine-grained spatial comprehension [146]. Particularly in the perception and localization of UI elements, existing MLLMs face significant challenges. Second, at the model level, open-source models generally perform poorly in these tasks, while proprietary models like GPT-4V show relatively superior performance. With further supervised fine-tuning on GUI data, the performance of these models approaches that of GPT-4V [196]. Lastly, at the performance level, the effectiveness of current models is highly correlated with the training data. For most open-source MLLMs, GUI data falls into out-of-distribution territory, limiting the models performance on such tasks [196]."
        },
        {
            "title": "3.3.7 Transfer Capability",
            "content": "MLLMs have demonstrated strong generalization ability, but there are still challenges when there are significant differences in image style between the testing and the training data. Recent research has begun to focus on this issue. For example, VLAA [118] introduces two benchmarks to evaluate MLLMs performance in out-of-distribution generalization and adversarial robustness, revealing that even GPT-4V struggles with understanding sketch images. In contrast, BenchLMM [152] delves deeper into the impact of image style on model performance, assessing MLLMs robustness across three different styles, including artistic image styles, imaging sensor styles, and application styles, with each encompassing five sub-styles. MMCBench [153] concentrates on examining the consistency of model outputs under common perturbations, reflecting the robustness against various types of noise. At the task level, MLLMs perform well with simple object appearance queries, particularly excelling in yes/no questions. However, their performance deteriorates 12 in recognizing object quantities in out-of-distribution visual scenes [118]. At the model level, proprietary models demonstrate greater transferability across different artistic styles but still face performance degradation [118], [152]. Additionally, while large models excel in handling noisy inputs, model size does not directly correlate with robustness, where some smaller models even perform better in certain scenarios [153]."
        },
        {
            "title": "3.3.8 Knowledge Editing",
            "content": "With the widespread deployment of MLLMs, it is becoming increasingly important to maintain the accuracy and timeliness of MLLM knowledge while avoiding high retraining costs. In this context, MMEdit [154] pioneers research into the evaluation of MLLM editing by introducing two subtasks: Editing Visual Question Answering (E-VQA) and Editing Image Captioning (E-IC). MMEdit expands traditional editing evaluation principles i.e. reliability, locality, and universality, into multimodal settings. However, the use of synthetic images for evaluating universality may not fully reflect model performance in real-world scenarios and does not adequately address portability issues. To this end, VLKEB [155] introduces more comprehensive evaluation approach and further extends the benchmark, focusing on the challenges of locality and portability in the knowledge editing process. Overall, multimodal editing needs to address not only the differences between vision and language, but also enhance the universality and portability across various scenarios while maintaining editing results."
        },
        {
            "title": "3.3.9 Embodied AI",
            "content": "Despite decades of exploration, achieving human-level intelligence in embodied AI remains significant challenge. This entails equipping agents with capabilities, such as learning, perception, reasoning, decision-making, and control, to perform general-purpose tasks in open, unstructured, and dynamic environments. The advent of MLLMs offers promising avenue, leveraging their advanced understanding and reasoning abilities to address the challenges in embodied AI. Consequently, numerous Embodied AI benchmarks have been developed to evaluate the performance in areas related to embodied intelligence. The earliest Embodied Question Answering (EQA) [156] focuses on navigation and information gathering from first-person perspective within 3D environments. This approach evolves with datasets like EPIC-KITCHENS [157] and Ego4D [158], which expand the task scope to include behavior understanding, hand-object interactions, social interactions, and provide comprehensive capabilities ranging from retrospective memory to future behavior prediction. With the emergence of EMQA [159] and SQA3D [160], the evaluation involves more complex spatial, temporal, and reasoning understanding. MoTIF [161] and EgoTaskQA [197] further introduce task execution and causal analysis within GUI environments, offering diagnostic insights into scenes, time, space, and causal relationships. Recent datasets such as EmbodiedScan [163] and RH20T-P [162] showcase significant increase in data scale and task complexity, focusing on 3D detection, grounding, and primitive tasks in robotics. In terms of model performance, MLLMs exhibit significant potential when handling these complex tasks [198], JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 yet also reveal limitations in precise localization, spatial perception, and the integration of external knowledge [162]. Proprietary models demonstrate strong capabilities in certain visual perception tasks [198], [199], but still rely on additional modules or post-processing steps to address deficiencies in complex spatial perception [162]. For example, when using GPT-4V as planner, it encounters trouble due to hallucination issues, often requiring supplementary modules like symbolic planners [198]. Additionally, prompt design plays crucial role when employing MLLMs as planners [198], where well-structured CoT can effectively reduce perception errors [162]. In comparison, fine-tuned models perform better in perception tasks than untuned proprietary models [200], while open-source models like LLaVA show relatively weaker results."
        },
        {
            "title": "3.3.10 Autonomous Driving",
            "content": "The characteristics of MLLM make it natural fit for autonomous driving scenarios. There are several benchmarks developed to assess specific levels of competence. These benchmarks have evolved from early simple tasks to comprehensive tasks that cover complex scene understanding and multi-step reasoning. Initially, BDDX [164] and HAD [165] focus on predicting vehicle behavior through textual descriptions and human suggestions, while understanding the underlying reasons. Subsequently, Talk2Car [166] and Rank2Tell [167] shift the focus to object recognition in driving scenes, requiring models to identify the most relevant objects based on text descriptions and rank their importance. With the introduction of DRAMA [168], research begins to address safety issues in driving scenarios by providing benchmark for risk localization and interpretation. NuScenes-QA [169] emphasizes the importance of 3D point cloud data, presenting more challenging tasks such as object counting, attribute recognition, and comparison tasks. NuPrompt [172] and LingoQA [171] further extend task complexity, particularly in 3D tracking and driving behavior inference. SUP-AD [201] introduces the need for scene-level understanding, offering not only object-level tasks but also detailed scene-level annotations to assess model capabilities. DriveLM [170] and Reason2Drive [173] enhance the evaluation of models in multi-step reasoning and interpretation, especially by simulating human decision-making processes in autonomous driving through graph-structured inference chains. MLLMs show good generalization and interpretative abilities when handling simple perception tasks such as image and point cloud data recognition [202], [203], especially in conventional driving scenarios. However, current models still fall short in complex tasks such as direction recognition, robustness to special lighting/weather conditions, visual localization, and spatial reasoning [203]. These tasks often require augmentation with traditional models [201]. Proprietary models in such tasks tend to outperform opensource models in certain aspects [171], but fine-tuned opensource models can surpass traditional autonomous driving pipelines and even exceed the performance of directly used proprietary models in some tasks [174], [201]. Nonetheless, achieving high-level autonomous driving capabilities requires extensive training data covering wide range of traffic and driving scenarios, and MLLMs need further improvement in understanding inputs from specialized sensors like radar [202]."
        },
        {
            "title": "4.1 Data Collection",
            "content": "Incorporating samples from existing datasets is common and perhaps the most popular method of data collection. Compared to manually gathering data, directly using samples from public datasets is more cost-effective, but can also increase the risk of data leakage. For instance, MMTBench [26] and SEED-Bench-2 [25] utilize data from public datasets, yet take annotation methods to reconstruct QA pairs to reduce the negative impact of data leakage. Modifying existing data is also an optional data collection method, particularly suitable for benchmarks that require specific data for evaluation. However, if manual modification is required, it can be labor-intensive process. For example, VCR [41] uses machine learning algorithms to occlude image captions to study the ability of restoring occluded text. MMCBench [153] adds noise to data to study model robustness, while HallusionBench [106] manually edites images to study model hallucination performance. Gathering data from the Internet is another commonly used data collection method. This approach effectively avoids overlapping with existing training data or benchmarks, but incurs higher labor costs and may lead to copyright issues, requiring careful filtering during the collection process. For example, ScienceQA [58], MMBench [22], and MMERealWorld [35] gather massive Internet data for annotation."
        },
        {
            "title": "4.2 Annotation",
            "content": "The annotation process can also be divided into three categories. The most cost-effective method is to automatically construct QA pairs by extracting relevant information from existing datasets using templates. This category primarily includes two approaches: the simplest one directly constructs from existing datasets using some selection criteria, and the other one rewrites existing annotations based on certain rules. For the former, MM-Vet [31] includes some questions derived from its original annotations, and LVLMeHub [20] uses entirely annotations from existing datasets. MathVista [53] selects samples that meet specific requirements from various benchmarks to create new benchmark. The annotation quality in these cases largely depends on the quality of the existing benchmarks and may have overlap with other benchmarks. OCR-VQA [37] designs fixed-genre questions to generate corresponding questions and answers for each image, while EQA [156] uses programs for automatic annotation. This annotation method is feasible when the data format is relatively uniform. Prompting LLMs or MLLMs to generate QA pairs is currently popular annotation method. As the performance of LLMs/MLLMs improves, using them for annotation JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 14 Incorporated samples from existing datasets: VQA v2 [18], VizWiz [19], LVLM-eHub [20], LAMM [21], Seed-Bench [23], MME [24], MMT-Bench [26], SEED-Bench-2 [25], OCRBench [39], TextVQA [36], Screen2Words [147], MMEdit [154], SQA3D [160]"
        },
        {
            "title": "Data Collection",
            "content": "Modifying existing data: HallusionBench [106], Bingo [204], MMCBench [153], Ferret-UI [205], VCR [41]"
        },
        {
            "title": "Construcion",
            "content": "(4) Gathering data from the Internet: ScienceQA [58], EPIC-KITCHENS [157], MMBench [22], InfiMM-Eval [33], MMEvalPro [206], WebSRC [38], MMMU [59], SEED-Bench-2-Plus [40], InfoVQA [44], Widget Captioning [151], MME-RealWorld [35] Automatically constructing QA pairs from existing datasets: SEED-Bench-2 [25], LVLM-eHub [20], LAMM [21], CV-Bench [34], OCR-VQA [37], DriveLM [170], EQA [156], NuScenes-QA [169] i P t a E"
        },
        {
            "title": "Annotation",
            "content": "Prompting in LLMs or MLLMs to generate QA pairs: MMStar [28], Seed-Bench [23], MMT-Bench [26], BLINK [27], SEED-Bench-2-Plus [40], VCR [41], ChartQA [42], EmbodiedScan [163], MMEdit [154], VLKEB [155] Manually annotation: TextVQA [36], VizWiz [19], MMBench [22], MME [24], Video-MME [178], MME-RealWorld [35], InfiMM-Eval [33], VisIT-Bench [30] , MoTIF [161], Widget Captioning [151]"
        },
        {
            "title": "Human",
            "content": "Screen2Words [147], Bingo [204] , M-HalDetect [101], WV-Arena [29], LVLM-Arena [20] Judge (5) LLM/MLLM BenchLMM [152],VisIT-Bench [30] , TouchStone [32], WV-Bench [29], MM-Vet [31], MMT-Bench [26], InfiMM-Eval [33], MMBench [22], BLINK [27],"
        },
        {
            "title": "Script",
            "content": "LAMM [21], VizWiz [19], MMStar [28], Seed-Bench [23], MME [24], MME-RealWorld [35], Video-MME [178]"
        },
        {
            "title": "Methods",
            "content": "Traditional evaluation metrics: Accuracy [24], [31], [35], [39], F1 [38], [149], [149], [161], mAP [21], CIDEr and BLUE4 [19], [21], [33], [151], Average Normalized Levenshtein Similarity (ANLS) [44], Exact Match (EM) [38], [41], [160], [207]"
        },
        {
            "title": "Deterministic",
            "content": "Metric (6) Non-Deterministic Novel metrics addressing traditional limitations: CircularEval [22], Log Probability [40], ADRScore [33], Lingo-Judge [171] Scoring: WV-Bench [29], VisIT-Bench [30] , LVLM-eHub [20], TouchStone [32], MMDU [72], GAVIE [100], MIA-Bench [70] Comparison: VisIT-Bench [30], WV-Bench [29], OpenCompass MultiModal Arena [208] Fig. 6: Evaluation pipeline of MLLMs. combined with subsequent human review can yield reasonably high-quality benchmark. Benchmarks like MMStar [28], Seed-Bench [23], MMT-Bench [26], and SEEDBench [23] have adopted this approach. This annotation process is inherently limited by the performance of the LLMs/MLLMs used. For instance, in MME-RealWorld [35], the best-performing model, Qwen2-VL, achieved only 40% accuracy in some tasks, indicating that relying on models inevitably introduces significant noise, which compromises the quality of the annotations. Manual annotation generally incurs the highest cost but ensures better quality. Benchmarks like VQA v2 [18], VizWiz [19], and TextVQA [36] are annotated by Amazon Mechanical Turk workers. MMBench [22], MME [24], VideoMME [178], and MME-RealWorld [35] are also benchmarks that are purely manually annotated. However, due to the high labor costs, the data scale of these benchmarks is usually limited. To date, the largest purely manually annotated dataset, MME-RealWorld [35], employed 32 annotators and contains 29K QA pairs."
        },
        {
            "title": "4.3 Common Challenges and Future Trends in Bench-\nmark Construction",
            "content": "In constructing benchmarks for MLLMs, several important considerations must be taken into account to ensure that the evaluation truly reflects the models capabilities. These issues, if overlooked, can lead to misleading conclusions about models performance. Multiple-Choice Question Leakage. Many benchmarks evaluate models using multiple-choice questions (MCQs) [22], [24], [35], which are easier to automate in terms of evaluation and statistical analysis. However, this format introduces the possibility of models guessing the correct answer without truly understanding the question or image. The structure of multiple-choice questions, by nature, may inadvertently provide clues within the answer options, allowing models to game the system rather than demonstrating genuine reasoning capabilities. To address this, some benchmarks require models to provide reasoning steps along with their answers. This ensures that the model demonstrates true understanding of the question and its components, as seen in benchmarks like ScienceQA [58] and MMEvalPro [206]. These benchmarks force the model to show how it arrived at the answer, rather than merely selecting an option. In addition, some benchmarks have designed more challenging or deceptive options to avoid trivial answers, making it harder for models to succeed without robust reasoning. Thus, the format of the questions should be carefully selected based on the evaluation scenario, ensuring that the models ability to JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 reason, not just guess, is properly evaluated. Data Leakage. Data leakage happens when models are evaluated using data they have already encountered during training. The issue becomes worse when benchmarks are built from existing academic datasets, where training and evaluation sets can overlap. To this end, it is vital to minimize overlap with public datasets during the benchmark construction, especially those used for training [24]. Using new or carefully selected data can lower the risk of leakage. Besides, techniques like deduplication and thorough data checks can help spot and remove cases where the same or very similar examples are used in both training and testing. Vision-Centric Evaluation. One significant issue with many existing benchmarks is that models can answer questions correctly without even processing the visual input, relying solely on the accompanying text. This undermines the purpose of evaluating MLLMs in multimodal tasks. In benchmarks like MMMU [59], it is found that many of the questions could be answered without understanding or even viewing the image, reducing the reliance on the vision modality. To counter this, new benchmarks such as VideoMME [178], MMEvalPro [206], and CV-Bench [34] have been designed with vision-centric tasks, where the visual content is essential for answering the questions. These benchmarks focus on ensuring that the model cannot answer the question solely based on text and must integrate visual information to provide accurate responses. For example, MMEvalPro [206] uses triplet designs (a combination of an image, question, and deliberately confusing options) to ensure that the MLLM has thoroughly processed the visual input. When constructing new benchmarks, it is essential to carefully curate questions where the image significantly impacts the answer, ensuring that the model genuinely understands and utilizes visual content. Benchmark Diversity and Sample Size. As MLLMs grow more capable, the complexity and variety of tasks they are evaluated on need evolve accordingly. Simple tasks are no longer sufficient to expose the limitations of these models. Additionally, benchmarks with small number run the risk of producing unrobust evaluation results. The high variance can lead to unreliable conclusions about model performance. As such, the representative MME-RealWorld [35] presents challenging real-world scenario oriented tasks, and increases the size of QA pairs to 29K, making it the largest manually annotated benchmarks to date."
        },
        {
            "title": "5 EVALUATION JUDGE",
            "content": "In this section, we introduce three evaluation strategies: human evaluation, LLM/MLLM-based evaluation, and scriptbased evaluation. The cost of these strategies decrease in the aforementioned order, though each comes with its own set of advantages and disadvantages."
        },
        {
            "title": "5.1 Human Evaluation",
            "content": "Human evaluation of model response is considered the most effective method, as the ultimate goal of MLLMs is for human use. For instance, Screen2Words [147] conducts Mechanical Turk study to ask humans to assess the quality of generated screen summaries and validate 15 how automatic metrics correlate with human judgment. Bingo [104] employs human annotators to evaluate the accuracy of GPT-4Vs responses to analyze the model biases. M-HalDetect [101] uses human evaluation to assess hallucination rates, demonstrating that human evaluation is more accurate than model-based assessments. WV-Arena [29] adopts human voting method to score models and uses Elo rating to compare multiple models. However, incorporating human evaluation undoubtedly increases time expenditure and labor costs. Besides, if the number of evaluators is small, there is concern that individual preferences might influence the scores."
        },
        {
            "title": "5.2 LLM/MLLM-based Evaluation",
            "content": "LLM/MLLM-based evaluation methods can be categorized involveinto two types based on the degree of model ment: 1) Shallow Model Involvement. In these scenarios, LLMs/MLLMs are only partially involved in the evaluation process, such as finding the most matching answer for responsed string. For instance, MMbench [22] and BLINK [27] use GPT-4 and GPT-3.5-turbo respectively as choice extractors. This method is useful to MLLMs that are not strong in instruction following, providing flexible alternative to match predictions and correct answers. 2) Full Model Responsibility. This approach is primarily used for open-ended tasks where the format and content of correct answers are not fixed. In such cases, LLMs/MLLMs can compare reference answers with generated answers or score directly. For example, MM-Vet [31] leverages GPT-4 to assist in evaluation, with GPT-4 automatically generating scores for each sample based on the input question, ground truth, and model output. Similarly, TouchStone [32] and LLaVAbench [14] uses GPT-4 to directly compare generated answers with reference answers. Incorporating other models in the evaluation process reduces human labor. However, this method is plagued with systematic biases [209], such as sensitivity to the ordering of responses. Moreover, the evaluation results are significantly constrained by the capabilities of the LLMs/MLLMs themselves [35]. There are times when different LLMs result in completely different evaluation results."
        },
        {
            "title": "5.3 Script-based Evaluation",
            "content": "Script-based evaluation methods are simpler and are commonly used in benchmarks based on multiple-choice or Yes-or-No questions. These evaluations compare results according to predefined rules. The MME series [24], [35], [87] adopt this approach by first performing regular expression matching on the output results to find the generated options, and then directly comparing the matched results with the ground truth. For example, for multiple-choice question, the output of MLLM is The answer is A, and the ground true answer is B. The role of the script is to extract and compare it with to determine whether it is right or wrong. This method evaluates quickly but also has certain drawbacks. The final accuracy is heavily dependent on the effectiveness of the regular expression matching. If model has poor instruction-following capabilities and the outputs are messy, this evaluation method may fail. Therefore, when JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 using this evaluation method, it is crucial to construct appropriate prompts that make MLLMs output regular."
        },
        {
            "title": "6.1 Deterministic Metrics",
            "content": "These metrics typically rely on standardized evaluation tools, enabling objective assessment with minimal human intervention. Compared to subjective human evaluations, deterministic metrics offer the benefits of saving time, reducing bias, and ensuring consistency across different assessments. We have categorized the key deterministic metrics based on their prevalence in existing literature as follows:"
        },
        {
            "title": "6.1.1 Traditional Evaluation Metrics",
            "content": "Exact Match (EM) [38], [41], [160], [207]: This metric determines whether the models output is an exact replica of the ground truth, making it straightforward measure in many tasks. Accuracy [24], [34], [35], [176], [178]: It is widely used in tasks like multiple-choice QA, assessing how often the models output option matches the correct answer. F1 Score [148], [150], [161], [167]: It is particularly useful in binary classification tasks, which balances precision and recall to provide comprehensive measure of the performance. mean Average Precision (mAP) [21], [146], [149], [196]: This metric is crucial for evaluating the grounding abilities, such as accurately identifying and localizing elements within an image. CIDEr, BLEU4 [19], [147], [151], [173], and ANLS [150]: These metrics evaluate the similarity between generated text and reference text, with CIDEr and BLEU focusing on n-gram overlap, and ANLS (Average Normalized Levenshtein Similarity) assessing the minimum number of edits needed to convert one sequence into another, particularly for open-ended answers. Task-Specific Metrics: Tailored metrics are employed in specialized fields. For instance, in autonomous driving, metrics like absolute error and correlation distance [164], [165] are used to predict factors like acceleration and steering angles. For object trajectory prediction, Average Multiple Object Tracking Precision (AMOTA) [172] is commonly used, while in redteaming scenarios, metrics such as Attack Success Rate (ASR) [118] are prevalent."
        },
        {
            "title": "6.1.2 Novel Metrics Addressing Traditional Limitations",
            "content": "While traditional metrics are valued for their simplicity and clarity, they can fall short in capturing the nuances of complex tasks. To address these limitations, researchers have developed more sophisticated metrics, particularly for evaluating tasks that involve multi-step reasoning. Some of these innovative metrics include: 16 CircularEval [22]: This metric is designed to counteract the bias that some models exhibit towards specific options in multiple-choice questions. CircularEval requires the model to correctly answer question after multiple random shufflings of the choices, ensuring more reliable evaluation of its understanding. Log Probability [25]: In situations where models ability to follow instructions is lacking, it may generate irrelevant content rather than directly answering the question. This metric assesses the log probability of the first generated token, and the option that produces the highest probability is considered the models choice. ADRScore [33]: Traditional text generation metrics like BLEU and CIDEr often evaluate outputs holistically, potentially overlooking the logical progression of reasoning steps. ADRScore is an aggregated metric specifically designed to assess the performance of models in tasks requiring chain-of-thought reasoning. Lingo-Judge [171]: This metric involves training classifier that evaluates the question, human response, and model response to determine the correctness of the models output. Studies have shown that Lingo-Judge correlates strongly with human ratings, making it robust and reliable evaluation tool."
        },
        {
            "title": "6.2 Non-Deterministic Metrics",
            "content": "Non-deterministic metrics encompass the evaluation methods that the model outputs are assessed either by other models or by human experts. There are two primary evaluation approaches within this category, including scoring and direct comparison."
        },
        {
            "title": "6.2.1 Scoring",
            "content": "Scoring is commonly employed strategy for open-ended tasks. In this approach, the model or human judges assign scores based on provided requirements. For instance, in LLaVA-Bench [14], the authors utilize detailed prompts, instructing GPT-4 to Please rate the helpfulness, relevance, accuracy, and level of detail of the responses. Each assistant then receives an overall score on scale from 1 to 10. Similarly, MIA-Bench [210] adopts GPT-4o to score the responses of MLLMs for each instruction, returning total score between 0 and 10. Additionally, Bingo [204] uses human annotators to evaluate the accuracy of GPT-4Vs responses, assigning score of 1 for correct answers and 0 for incorrect ones. The reliability of this scoring strategy depends on the model or expert and whether the researchers provided detailed prompts or user requirements, and the evaluation results of different models and experts may vary greatly. Moreover, some methods also rely on reference text, such as in TouchStone [32], where the model compares human-written caption with the one generated by the model and predicts the degree of hallucination."
        },
        {
            "title": "6.2.2 Comparison",
            "content": "As opposed to scoring, direct comparison involves comparing the evaluated models outputs with optimal outputs using advanced models or experts. This approach is often considered more straightforward and stable than scoring. Common metrics include Win-rate, which represents the JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 proportion of victories in comparison tests with other models, and Elo ranking, rating method originally developed for chess that not only considers the win rate but also the strength of the opponents in each victory or defeat. Elo rankings are dynamically updated, adjusting scores based on the relative strength of opponents, thereby creating more balanced rating system. For example, in VisIT-Bench [30], the authors collect human preference-based comparison results to compile human-guided Elo rankings and win rates for the evaluated models."
        },
        {
            "title": "7 EVALUATION TOOLKIT",
            "content": "With the rapid iteration of MLLMs, it is essential for researchers to assess their capabilities across various aspects both during iterations and after new releases. However, the fragmented distribution of evaluation datasets, the tedious preparation work, and the potential for conflicts and mismatched results due to varying environmental requirements pose challenges to the rigorous evaluation of multimodal models on many benchmarks and the integration of results. The existence of toolkits facilitate the efficient integration of multiple datasets and models, enabling simultaneous evaluation across multiple datasets while simplifying environment configuration. For the evaluation of LLMs, OpenAI introduces the Evals framework, which can test different dimensions of GPT models and allows for the creation of custom scripts for more comprehensive evaluations. OpenCompass [211], on the other hand, supports multiple evaluation datasets and models through one-stop evaluation approach, offering versatility and high scalability. In the context of MLLMs, representative toolkits include VLMEvalKit [212] the evaluation of [213]. Additionally, and LMMs-Eval agent capabilities and performance on medical tasks for MLLMs should not be overlooked. Toolkits such as agentstudio [214] and MultiMedEval [215] have made it easier to assess these capabilities."
        },
        {
            "title": "7.1 VLMEvalKit",
            "content": "VLMEvalKit is comprehensive, user-friendly, and easily extensible MLLM evaluation toolkit, which is designed to facilitate researchers to quickly evaluate the performance of existing MLLMs on multiple benchmarks. Currently, the codebase supports more than 70 different MLLMs, including proprietary APIs and open-source models, and more than 20 multimodal benchmarks covering wide range of tasks and scenarios. For the vast array of datasets from different sources and formats, VLMEvalKit standardizes the preprocessing of data and stores multimodal content using paths or base64 encoding. Each evaluation benchmark is associated with corresponding TSV file, which can be downloaded with single click and verified for integrity using MD5 checksums. Each row in the TSV file represents single evaluation sample, including index, question, answer, image or image_path and choices for multi-choice questions. Each dataset class inherits from corresponding base class, such as VQA, MCQ, Y/N, based on its question type and supports the .build_prompt() interface. The evaluation 17 scripts use this function to construct multimodal messages from evaluation samples, creating interleaved sequences of various modal contents. VLMEvalKit implements unified .generate() interface for different MLLMs. This interface accepts multimodal information, including multi-turn conversations with images, as input and returns response string. The custom processing of multimodal information by the model is determined and constructed using the additional parameter dataset_name of the .generate() interface and the models own .build_prompt() interface. It also supports differentiated adjustments to inference hyperparameters according to dataset_name. To accelerate multimodal inference, VLMEvalKit leverages Pythons multiprocessing capabilities to support parallel inference with commercial APIs. It also fully utilizes computational resources to achieve multi-GPU distributed inference for open-source models. By temporarily storing results in .pkl files, the final inference results are complete and can be restored with minimal cost. For the final evaluation, VLMEvalKit additionally designs an LLM-based judge extractor to assist in answer matching for MCQ and Y/N question types. This extractor can be invoked through commercial APIs or deployed using LMDeploy. For ongoing support of evaluation results, the VLMEvalKit team has established public leaderboard2. This ensures that all evaluation results are openly accessible and reproducible, providing the community with an efficient means to assess and compare the performance of different models."
        },
        {
            "title": "7.2 LMMs-Eval",
            "content": "LMMs-Eval is unified and standardized benchmark for evaluating MLLMs, supporting over 50 tasks and more than dozen models. By preprocessing datasets and recording model outputs, LMMs-Eval enables one-click evaluation across multiple tasks, thereby reducing the overhead associated with data collection and fragmented evaluation results. Additionally, LMMs-Eval establishes unified framework encompassing various evaluation settings to achieve standardized and fair assessments. However, challenges remain, such as the high cost of comprehensive task evaluation and potential issues with evaluation dataset contamination. To address this issue, the pruned evaluation toolkit, LMMs-Eval lite, has been proposed. By utilizing greedy algorithm to solve the k-center problem, LMMs-Eval lite identifies subset of benchmarks where the absolute scores and relative rankings of models are similar to those of the full set. Correlation calculations ensure that the selected subset maintains adequate testing capability. LMMs-Eval lite encompasses 6 task categories, including 15 datasets, and employs specific strategies to aggregate the scores of each subset. This allows researchers to efficiently evaluate model performance during the training phase. Traditional evaluation benchmarks use fixed questions and answers for static assessment, which do not align with real-world usage scenarios. While benchmarks like VibeEval [216] and LLaVA-Bench (Wilder) [179] utilize realworld data to test model capabilities, the continuous updating of training data makes data contamination inevitable. 2. https://rank.opencompass.org.cn/leaderboard-multimodal JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 Fig. 7: Major components and evaluation pipeline of the toolkit. By integrating various types of datasets and models, the evaluation toolkit facilitates the efficient acquisition and timely updating of assessment results, enabling comprehensive performance comparisons across models. LiveBench addresses this by periodically collecting real news from the internet and using powerful commercial multimodal models to construct QA pairs, forming monthly evaluation dataset. This approach ensures the data remains authentic and minimizes contamination, while controlling the number of questions helps manage the cost of dataset construction and updates."
        },
        {
            "title": "7.3 MultiMedEval",
            "content": "In the medical domain, LLaVA-Med [217] has been finetuned using data from PMC, effectively creating an assistant capable of multimodal medical question answering. Similarly, BioMedGPT [218] integrates multi-scale cross-modal biomedical data to establish general-purpose MLLM capable of handling various tasks. However, existing medical evaluation benchmarks span multiple modalities, including X-ray, CT, general medical knowledge, and radiology, and encompass various tasks such as QA and summarization. This necessitates toolkit to unify and simplify the evaluation methods across these benchmarks. Consequently, MultiMedEval [215] is developed to meet this need. MultiMedEval encompasses 6 medical tasks, including natural language inference, report summarization, visual QA, and medical image classification, and covers 23 datasets from 11 different medical modalities. Researchers can easily install and use MultiMedEval via PyPi. After setting up the datasets to be evaluated, they only need to implement callable batcher function for their models to initiate the evaluation and obtain the results in JSON format. Each call to this function will pass list of prompts and return corresponding list of decoded model responses for subsequent evaluation. Custom parameters such as data storage directories and batch size can be configured using the SetupParams and EvalParams classes."
        },
        {
            "title": "7.4 AgentStudio",
            "content": "Enabling MLLMs to interact with the environment through external tools offers practical and realistic assessment of their performance. Notably, virtual agents, which receive computer states and respond to instructions by invoking functions or manipulating software, have shown significant progress. However, these agents are developed for various domains, including gaming, online shopping, and web navigation, and there is lack of unified real-world setting and accompanying infrastructure to comprehensively evaluate their fundamental capabilities. AgentStudio [214] is toolkit designed based on realworld environments, compatible with multiple operating systems and devices, encompassing the entire lifecycle of testing virtual agents. To address the previous inconsistencies in action spaces and observation spaces, AgentStudio provides unified framework. It supports function calls and mouse/keyboard controls to manipulate any software and allows agents to use tools to access the internal structured state of programs like HTML. This avoids the limitations of single observation methods and promotes research on multimodal agent observation. AgentStudio also features visual interface to monitor agent behavior and supports fully online, interactive environments, reflecting the complexity of real-world scenarios more accurately. Additionally, AgentStudio offers an interactive annotation pipeline for labeling GUI-based data to evaluate the fundamental capabilities of models. It has been found that current advanced proprietary models, such as GPT-4V, still fall short in achieving robust and precise mouse action localization when acting as virtual agents and exhibit varied performance across different operating systems. AgentStudio also introduces benchmark suite consisting of 77 realworld tasks, providing comprehensive evaluation of existing agents tool usage, compositional generalization, and other capabilities with three levels of increasing difficulty."
        },
        {
            "title": "7.5 Further Development",
            "content": "Beyond the image modality, the development of MLLMs in video, audio, and other modalities has also heightened the need for comprehensive model evaluation. VLMEvalKit has taken the lead by incorporating several mainstream video understanding benchmarks and video MLLMs. However, JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 issues remain, such as the lack of standardized video formats, differing frame selection methods across models, and the inability to perform integrated evaluations of video and audio. These aspects require further standardization and unification. Besides, there is ongoing demand within the community for evaluations of models in speech and 3D understanding, which could be future direction for the development of comprehensive MLLM evaluation toolkits. In more specific scenarios, MLLMs can currently interact as virtual agents. However, existing virtual agent evaluation toolkits still heavily rely on manual assessment and lack sufficient scenario diversity, necessitating further development. Furthermore, in fields such as medicine and embodied intelligence, there are issues with insufficient integration or lack of evaluation toolkits, preventing unified performance assessments of various models. Therefore, it is essential to develop specialized evaluations alongside general ones to bring models closer to practical applications."
        },
        {
            "title": "8 CHALLENGES AND FUTURE DIRECTIONS",
            "content": "With the development of MLLMs, the need for comprehensive evaluation has gradually received increasing attention. While the academic and industrial communities have introduced over hundred benchmarks for assessing these models, several challenges persist in the current evaluation landscape. Firstly, there is notable lack of universally accepted, standardized capability taxonomy, with existing evaluation sets often defining their own disparate ability dimensions. Secondly, current evaluation benchmarks exhibit gaps in their coverage of critical capabilities, particularly in areas such as instruction following, complex multi-modal reasoning, multi-turn dialogue experience, and creativity assessment. Thirdly, there is dearth of task-specific evaluations for MLLMs, especially in commercially relevant domains such as invoice recognition, multi-modal knowledge base comprehension, and UI understanding and manipulation. Lastly, while existing multi-modal evaluation sets predominantly focus on image and video modalities, there remains significant deficit in the assessment of capabilities related to audio and 3D representations. Addressing these challenges will be crucial for developing more robust and comprehensive evaluation methodologies for large multimodal models in the future. of MLLMs, the taxonomy should be grounded in strong theoretical foundations, aligning with established research in psychology and cognitive science."
        },
        {
            "title": "8.2 Capability-Oriented Evaluation",
            "content": "Despite rapid development, current MLLM evaluations remain not comprehensive enough, focusing primarily on assessing perception and reasoning abilities through objective questions [22], [24], [26], [59]. This creates discernible gap between evaluation methodologies and real-world application scenarios. Moreover, optimizing models based on objective assessment results often leads developers to incorporate an abundance of objective question corpus during instruction tuning, potentially compromising the quality of actual dialogue experiences. Although subjective multimodal evaluation platforms such as WildVision [29] and OpenCompass MultiModal Arena3 have emerged, more research is needed to develop assessment methods that align more closely with practical usage scenarios. Current evaluation strategies largely rely on collecting or crafting specific questions to assess particular capabilities. However, complex multi-modal problems typically require the integration of multiple skills. For example, chart-related question may involve OCR, spatial relationship recognition, reasoning, and calculations. The lack of decoupled assessment [219] for these distinct capabilities represents significant limitation in current evaluation frameworks. Furthermore, existing multimodal evaluations do not adequately cover crucial abilities, such as instruction following, with only few benchmarks such as VisIT-Bench [30] and MIA-Bench [70] addressing this aspect. Multiturn dialogue, the primary mode of human interaction with multimodal models, remains weakness for most current models, and the corresponding evaluations are still in their infancy, such as ConvBench [71] and MMDU [72]. In the realm of complex multimodal reasoning, current evaluations predominantly focus on solving mathematical [53], [54], [56] and examination problems [59], [61], necessitating improvements in both difficulty and alignment with everyday use cases. Notably, the evaluation of multimodal creative tasks, key application area for these models, such as text generation based on image and textual prompts, remains largely unexplored, highlighting critical gap in the current evaluation landscape."
        },
        {
            "title": "8.3 Task-Oriented Evaluation",
            "content": "The rapid proliferation of multimodal benchmarks has rapidly expanded the breadth and depth of multimodal evaluation. These benchmarks typically define anywhere from few to dozens of capability dimensions. However, there is significant degree of overlap among these dimensions between different benchmarks. For example, capabilities such as OCR, celebrity recognition, and scene understanding are commonly found in benchmarks like MME [24] and MMBench [22]. The redundancy underscores the urgent need for an intricately developed, widely accepted capability taxonmomy within the multimodal domain to facilitate coherent progress. The design of such taxonomy presents multiple avenues of exploration. Given that alignment with human cognition is crucial feature As MLLM are still in the early stages of development, their business applications remain limited. Consequently, current evaluations predominantly focus on assessing fundamental capabilities rather than performance in real-world applications. Moving forward, it is crucial to develop evaluation frameworks that assess MLLM performance on specific tasks, particularly those with commercial value. Such tasks may include large-scale document processing, multimodal knowledge base comprehension, anomaly detection, and industrial visual inspection. When constructing task-specific evaluations for MLLMs, it is essential to consider not only the performance metrics, but also computational costs and 3. https://opencompass.org.cn/arena?type=multimodal JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 inference speeds, comparing them against traditional computer vision-based methods such as OCR [220], [221], object detection [222], [223], [224], [225], action recognizers [226], [227], [228], to assess their practical applicability. Furthermore, significant function of MLLMs lies in their potential in planning, interacting with environments as agents to solve complex problems [229], [230]. Developing diverse virtual environments that allow MLLMs to interact and demonstrate their problem-solving capabilities as agents will likely become critical component of future evaluations. Currently, evaluation efforts in this domain are in their early stages [231], [232], [233], indicating promising area for future research and development in the field of multimodal AI assessment."
        },
        {
            "title": "8.4 Incorporating More Modalities",
            "content": "Current evaluations of MLLMs primarily focus on image and video modalities, with limited attention given to other modalities. In the audio domain, models like QwenAudio [234] evaluate the audio-related capability on traditional audio tasks such as Automatic Speech Recongition [235], Speech-to-Text Translation [236], Acoustic Scene Classification [237], and Vocal Sound Classification [238]. However, there remains notable gap in assessing capabilities related to speech meta-information recognition, such as accent recognition, emotion detection, and beyond. The field of 3D modality evaluation is also in its infancy, with works like ScanRefer [239] and ScanReason [240] representing early efforts. Furthermore, with the emergence of omni MLLMs like GPT-4o, Gemini, and VITA [6], there is an urgent need to develop evaluation frameworks that can assess models simultaneous perception and crossmodal reasoning capabilities across multiple modalities. This evolving landscape underscores the importance of expanding and diversifying evaluation methodologies to keep pace with the rapidly advancing capabilities of MLLMs, ensuring comprehensive assessment across wider range of modalities and their interactions."
        },
        {
            "title": "9 CONCLUSION\nMLLMs are developing rapidly, and the evaluation\nbenchmarks are escorting them. This paper has presented\na thorough survey of MLLM evaluation, focusing on four\nfoundational dimensions: what capabilities are evaluated,\nhow to construct benchmarks, how to assess performance,\nand where is the direction of the next benchmark. We hope\nthat this survey will answer questions from researchers\nabout MLLM evaluation, and help with the development\nitself.\nof",
            "content": "the evaluation benchmarks and the model"
        },
        {
            "title": "REFERENCES",
            "content": "[2] [1] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., survey of large language models, arXiv:2303.18223, 2023. 1 B. Peng, C. Li, P. He, M. Galley, and J. Gao, Instruction tuning with gpt-4, arXiv:2304.03277, 2023. 1 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language models are few-shot learners, in NeurIPS, 2020. 1 T. Baltruaitis, C. Ahuja, and L.-P. Morency, Multimodal machine learning: survey and taxonomy, IEEE Trans. Pattern Anal. Mach. Intell., 2018. 1 [3] [4] 20 [5] [6] [7] [8] [9] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, survey on multimodal large language models, arXiv:2306.13549, 2023. 1, 2 C. Fu, H. Lin, Z. Long, Y. Shen, M. Zhao, Y. Zhang, X. Wang, D. Yin, L. Ma, X. Zheng et al., Vita: Towards open-source interactive omni multimodal llm, arXiv:2408.05211, 2024. 1, 20 P. Xu, X. Zhu, and D. A. Clifton, Multimodal learning with transformers: survey, IEEE Trans. Pattern Anal. Mach. Intell., 2023. 2 J. Zhang, J. Huang, S. Jin, and S. Lu, Vision-language models for vision tasks: survey, IEEE Trans. Pattern Anal. Mach. Intell., 2024. 2 Y. Zhu, Y. Wu, N. Sebe, and Y. Yan, Vision+ x: survey on multimodal learning in the light of data, IEEE Trans. Pattern Anal. Mach. Intell., 2024. 2 [10] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, Diffusion models in vision: survey, IEEE Trans. Pattern Anal. Mach. Intell., 2023. [11] F. Zhan, Y. Yu, R. Wu, J. Zhang, S. Lu, L. Liu, A. Kortylewski, C. Theobalt, and E. Xing, Multimodal image synthesis and editing: The generative ai era, IEEE Trans. Pattern Anal. Mach. Intell., 2023. 2 [12] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.- C. Zhu, and J. Gao, Chameleon: Plug-and-play compositional reasoning with large language models, in NeurIPS, 2024. 2 [13] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, Self-instruct: Aligning language model with self generated instructions, arXiv:2212.10560, 2022. 3 [14] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. 3, 10, 15, 16 [15] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang et al., Aligning large multimodal models with factually augmented rlhf, arXiv:2309.14525, 2023. 3 [16] T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun et al., Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback, arXiv:2312.00849, 2023. 3 [17] L. Li, Z. Xie, M. Li, S. Chen, P. Wang, L. Chen, Y. Yang, B. Wang, and L. Kong, Silkie: Preference distillation for large visual language models, arXiv:2312.10665, 2023. [18] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, Making the in vqa matter: Elevating the role of image understanding in visual question answering, in CVPR, 2017. 3, 4, 8, 14 [19] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, Vizwiz grand challenge: Answering visual questions from blind people, in CVPR, 2018. 3, 4, 8, 14, 16 [20] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models, arXiv:2306.09265, 2023. 3, 4, 8, 13, 14 [21] Z. Yin, J. Wang, J. Cao, Z. Shi, D. Liu, M. Li, X. Huang, Z. Wang, L. Sheng, L. Bai et al., Lamm: Language-assisted multimodal instruction-tuning dataset, framework, and benchmark, in NeurIPS, 2024. 3, 4, 14, 16 [22] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu et al., Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. 3, 4, 8, 13, 14, 15, 16, 19 [23] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, Seedbench: Benchmarking multimodal llms with generative comprehension, arXiv:2307.16125, 2023. 3, 4, 8, 14 [24] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun et al., Mme: comprehensive evaluation benchmark for multimodal large language models, arXiv:2306.13394, 2023. 3, 4, 8, 14, 15, 16, [25] B. Li, Y. Ge, Y. Ge, G. Wang, R. Wang, R. Zhang, and Y. Shan, Seed-bench-2: Benchmarking multimodal large language models, arXiv:2311.17092, 2023. 3, 4, 13, 14, 16 [26] K. Ying, F. Meng, J. Wang, Z. Li, H. Lin, Y. Yang, H. Zhang, W. Zhang, Y. Lin, S. Liu et al., Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi, arXiv:2404.16006, 2024. 3, 4, 13, 14, 19 [27] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna, Blink: Multimodal large language JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 models can see but not perceive, arXiv:2404.12390, 2024. 3, 4, 14, 15 [28] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin et al., Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. 4, 5, [29] Y. Lu, D. Jiang, W. Chen, W. Y. Wang, Y. Choi, and B. Y. Lin, Wildvision: Evaluating vision-language models in the wild with human preferences, arXiv:2406.11069, 2024. 3, 4, 14, 15, 19 [30] Y. Bitton, H. Bansal, J. Hessel, R. Shao, W. Zhu, A. Awadalla, J. Gardner, R. Taori, and L. Schimdt, Visit-bench: benchmark for vision-language instruction following inspired by real-world use, arXiv:2308.06595, 2023. 3, 4, 14, 17, 19 [32] [31] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, Mm-vet: Evaluating large multimodal models for integrated capabilities, in ICML, 2024. 4, 5, 8, 13, 14, 15 S. Bai, S. Yang, J. Bai, P. Wang, X. Zhang, J. Lin, X. Wang, C. Zhou, and J. Zhou, Touchstone: Evaluating vision-language models by language models, arXiv:2308.16890, 2023. 4, 5, 14, 15, 16 [33] X. Han, Q. You, Y. Liu, W. Chen, H. Zheng, K. Mrini, X. Lin, Y. Wang, B. Zhai, J. Yuan et al., Infimm-eval: Complex openended reasoning evaluation for multi-modal large language models, arXiv:2311.11567, 2023. 4, 5, 14, 16 S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan et al., Cambrian-1: fully open, vision-centric exploration of multimodal llms, arXiv:2406.16860, 2024. 4, 5, 14, 15, 16 [34] [35] Y.-F. Zhang, H. Zhang, H. Tian, C. Fu, S. Zhang, J. Wu, F. Li, K. Wang, Q. Wen, Z. Zhang et al., Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv:2408.13257, 2024. 3, 4, 7, 8, 13, 14, 15, 16 [36] A. Singh, V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in CVPR, 2019. 4, 5, 8, [37] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty, Ocr-vqa: Visual question answering by reading text in images, in ICDAR, 2019. 4, 5, 8, 13, 14 [38] X. Chen, Z. Zhao, L. Chen, D. Zhang, J. Ji, A. Luo, Y. Xiong, and K. Yu, Websrc: dataset for web-based structural reading comprehension, arXiv:2101.09465, 2021. 4, 5, 14, 16 [39] Y. Liu, Z. Li, B. Yang, C. Li, X. Yin, C.-l. Liu, L. Jin, and X. Bai, On the hidden mystery of ocr in large multimodal models, arXiv:2305.07895, 2023. 4, 5, 8, 14 [40] B. Li, Y. Ge, Y. Chen, Y. Ge, R. Zhang, and Y. Shan, Seed-bench2-plus: Benchmarking multimodal large language models with text-rich visual comprehension, arXiv:2404.16790, 2024. 4, 5, 6, 14 [41] T. Zhang, S. Wang, L. Li, G. Zhang, P. Taslakian, S. Rajeswar, J. Fu, B. Liu, and Y. Bengio, Vcr: Visual caption restoration, arXiv:2406.06462, 2024. 4, 5, 13, 14, 16 [42] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, Chartqa: benchmark for question answering about charts with visual and logical reasoning, arXiv:2203.10244, 2022. 4, 6, 8, 14 [43] M. Mathew, D. Karatzas, and C. Jawahar, Docvqa: dataset for vqa on document images, in WACV, 2021. 4, 6, 8 [44] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar, Infographicvqa, in WACV, 2022. 4, 5, 6, 8, 14 [45] R. Xia, S. Mao, X. Yan, H. Zhou, B. Zhang, H. Peng, J. Pi, D. Fu, W. Wu, H. Ye et al., Docgenome: An open large-scale scientific document benchmark for training and testing multi-modal large language models, arXiv:2406.11633, 2024. 4, 6 [46] Y. Ma, Y. Zang, L. Chen, M. Chen, Y. Jiao, X. Li, X. Lu, Z. Liu, Y. Ma, X. Dong et al., Mmlongbench-doc: Benchmarking long-context document understanding with visualizations, arXiv:2407.01523, 2024. 4, 6 [47] Z. Wang, M. Xia, L. He, H. Chen, Y. Liu, R. Zhu, K. Liang, X. Wu, H. Liu, S. Malladi et al., Charxiv: Charting gaps in realistic chart understanding in multimodal llms, arXiv:2406.18521, 2024. 4, 6 [48] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in ECCV, 2016. 4, 6 [49] W. Zhao, H. Feng, Q. Liu, J. Tang, S. Wei, B. Wu, L. Liao, Y. Ye, H. Liu, H. Li et al., Tabpedia: Towards comprehensive visual table understanding with concept synergy, arXiv:2406.01326, 2024. 4, 6 [50] R. Tanaka, K. Nishida, and S. Yoshida, Visualmrc: Machine reading comprehension on document images, in AAAI, 2021. 4, 6, 8 [51] R. Chaudhry, S. Shekhar, U. Gupta, P. Maneriker, P. Bansal, and A. Joshi, Leaf-qa: Locate, encode attend for figure question answering, in WACV, 2020. 4, 6, 8 S. E. Kahou, V. Michalski, A. Atkinson, . Kdr, A. Trischler, and Y. Bengio, Figureqa: An annotated figure dataset for visual reasoning, arXiv:1710.07300, 2017. 4, 6, 8 [52] [53] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.- W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv:2310.02255, 2023. 4, 6, 8, 13, 19 [54] K. Wang, J. Pan, W. Shi, Z. Lu, M. Zhan, and H. Li, Measuring multimodal mathematical reasoning with math-vision dataset, arXiv:2402.14804, 2024. 4, 6, 19 [55] C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang et al., Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, arXiv:2402.14008, 2024. 4, [56] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, P. Gao et al., Mathverse: Does your multimodal llm truly see the diagrams in visual math problems? arXiv:2403.14624, 2024. 4, 6, 19 [57] R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song, Z. GongQue, S. Lei, Z. Wei, M. Zhang et al., We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv:2407.01284, 2024. 4, 6 [58] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, in NeurIPS, 2022. 4, 6, 8, 13, 14 [59] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi, in CVPR, 2024. 4, 6, 7, 8, 14, 15, 19 [60] Z. He, X. Wu, P. Zhou, R. Xuan, G. Liu, X. Yang, Q. Zhu, and H. Huang, Cmmu: benchmark for chinese multi-modal multitype question understanding and reasoning, arXiv:2401.14011, 2024. 4, 6 [61] G. Zhang, X. Du, B. Chen, Y. Liang, T. Luo, T. Zheng, K. Zhu, Y. Cheng, C. Xu, S. Guo et al., Cmmmu: chinese massive multi-discipline multimodal understanding benchmark, arXiv:2401.11944, 2024. 4, 6, [62] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, M. Yin, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig, Mmmupro: more robust multi-discipline multimodal understanding benchmark, arXiv:2409.02813, 2024. 4 [63] Y. Wu, W. Yu, Y. Cheng, Y. Wang, X. Zhang, J. Xu, M. Ding, and Y. Dong, Alignmmbench: Evaluating chinese multimodal alignment in large vision-language models, arXiv:2406.09295, 2024. 4, 6 J. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao et al., Mtvqa: Benchmarking multilingual text-centric visual question answering, arXiv:2405.11985, 2024. 4, 6 [64] [65] W. Zhang, M. Aljunied, C. Gao, Y. K. Chia, and L. Bing, M3exam: multilingual, multimodal, multilevel benchmark for examining large language models, in NeurIPS, 2023. 4, 6 [66] H. Maryam, L. Fu, J. Song, T. A. Shafayet, Q. Luo, X. Bai, and Y. Liu, Dataset and benchmark for urdu natural scenes text detection, recognition and visual question answering, arXiv:2405.12533, 2024. 4, 6 [67] F. W. Douamba, J. Song, L. Fu, Y. Liu, and X. Bai, The first swahili language scene text detection and recognition dataset, arXiv:2405.11437, 2024. 4, [68] H. Q. Pham, T. K.-B. Nguyen, Q. Van Nguyen, D. Q. Tran, N. H. Nguyen, K. Van Nguyen, and N. L.-T. Nguyen, Viocrvqa: Novel benchmark dataset and vision reader for visual question answering by understanding vietnamese text in images, arXiv:2404.18397, 2024. 4, 6 [69] Y. Wang, Y. Liu, F. Yu, C. Huang, K. Li, Z. Wan, and W. Che, Cvlue: new benchmark dataset for chinese vision-language understanding evaluation, arXiv:2407.01081, 2024. 4, 6 [70] Y. Qian, H. Ye, J.-P. Fauconnier, P. Grasch, Y. Yang, and Z. Gan, Mia-bench: Towards better instruction following evaluation of multimodal llms, arXiv:2407.01509, 2024. 4, 6, 14, 19 JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 22 [71] S. Liu, K. Ying, H. Zhang, Y. Yang, Y. Lin, T. Zhang, C. Li, Y. Qiao, P. Luo, W. Shao et al., Convbench: multi-turn conversation evaluation benchmark with hierarchical capability for large vision-language models, arXiv:2403.20194, 2024. 4, 7, 19 [72] Z. Liu, T. Chu, Y. Zang, X. Wei, X. Dong, P. Zhang, Z. Liang, Y. Xiong, Y. Qiao, D. Lin et al., Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms, arXiv:2406.11833, 2024. 4, 7, 14, 19 [73] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi, corpus for reasoning about natural language grounded in photographs, arXiv:1811.00491, 2018. 4, 7, 8 [74] Y. Huang, Z. Meng, F. Liu, Y. Su, N. Collier, and Y. Lu, Sparkles: Unlocking chats across multiple images for multimodal instruction-following models, arXiv:2308.16463, 2023. 4, 7 [75] X. Wang, Y. Zhou, X. Liu, H. Lu, Y. Xu, F. He, J. Yoon, T. Lu, G. Bertasius, M. Bansal et al., Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences, arXiv:2401.10529, 2024. 4, 7 [76] B. Zhao, Y. Zong, L. Zhang, and T. Hospedales, Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning, arXiv:2406.12742, 2024. 4, [77] M. Kazemi, N. Dikkala, A. Anand, P. Devic, I. Dasgupta, F. Liu, B. Fatemi, P. Awasthi, D. Guo, S. Gollapudi et al., Remi: dataset for reasoning with multiple images, arXiv:2406.09175, 2024. 4, 7 [78] F. Wang, X. Fu, J. Y. Huang, Z. Li, Q. Liu, X. Liu, M. D. Ma, N. Xu, W. Zhou, K. Zhang et al., Muirbench: comprehensive benchmark for robust multi-image understanding, arXiv:2406.09411, 2024. 4, 7 [79] C. Zhou, M. Zhang, P. Chen, C. Fu, Y. Shen, X. Zheng, X. Sun, and R. Ji, Vega: Learning interleaved image-text comprehension in vision-language large models, arXiv:2406.10228, 2024. 4, 7 [80] P. Wu and S. Xie, V?: Guided visual search as core mechanism [81] [82] [83] in multimodal llms, in CVPR, 2024. 4, 7 S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, Referitgame: Referring to objects in photographs of natural scenes, in EMNLP, 2014. 4, 7, 8 J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, Generation and comprehension of unambiguous object descriptions, in CVPR, 2016. 4, 7 J. Chen, F. Wei, J. Zhao, S. Song, B. Wu, Z. Peng, S.-H. G. Chan, and H. Zhang, Revisiting referring expression comprehension evaluation in the era of large multimodal models, arXiv:2406.16866, 2024. 4, 8 [84] G. Geigle, R. Timofte, and G. Glava, African or european swallow? benchmarking large vision-language models for finegrained object classification, arXiv:2406.14496, 2024. 4, 8 S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie, Eyes wide shut? exploring the visual shortcomings of multimodal llms, in CVPR, 2024. 4, 8 [85] [86] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai et al., Q-bench: benchmark for general-purpose foundation models on low-level vision, arXiv:2309.14181, 2023. 4, 8 [87] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang et al., Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, arXiv:2405.21075, 2024. 4, 8, 9, 15 [88] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo et al., Mvbench: comprehensive multi-modal video understanding benchmark, in CVPR, 2024. 4, 8, 9 J. Zhou, Y. Shu, B. Zhao, B. Wu, S. Xiao, X. Yang, Y. Xiong, B. Zhang, T. Huang, and Z. Liu, Mlvu: comprehensive benchmark for multi-task long video understanding, arXiv:2406.04264, 2024. 4, [89] [90] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, S. Huang, B. Xu, Y. Dong, M. Ding et al., Lvbench: An extreme long video understanding benchmark, arXiv:2406.08035, 2024. 4, 9 [91] X. Fang, K. Mao, H. Duan, X. Zhao, Y. Li, D. Lin, and K. Chen, Mmbench-video: long-form multi-shot benchmark for holistic video understanding, arXiv:2406.14515, 2024. 4, 9 [92] Y. Du, K. Zhou, Y. Huo, Y. Li, W. X. Zhao, H. Lu, Z. Zhao, B. Wang, W. Chen, and J.-R. Wen, Towards event-oriented long video understanding, arXiv:2406.14129, 2024. 4, 9 [93] Z. Zhao, H. Lu, Y. Huo, Y. Du, T. Yue, L. Guo, B. Wang, W. Chen, and J. Liu, Needle in video haystack: scalable synthetic framework for benchmarking video mllms, arXiv:2406.09367, 2024. 4, [94] K. Mangalam, R. Akshulakov, and J. Malik, Egoschema: diagnostic benchmark for very long-form video language understanding, in NeurIPS, 2024. 4, 8, 9 [95] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou, Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. 4, 9 [96] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang, Video question answering via gradually refined attention over appearance and motion, in ACM MM, 2017. 4, 8 [97] Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim, Tgif-qa: Toward spatio-temporal reasoning in visual question answering, in CVPR, 2017. 4, 8 [98] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao, for understanding complex web Activitynet-qa: dataset videos via question answering, in AAAI, 2019. 4, 8, [99] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, Evaluating object hallucination in large vision-language models, in EMNLP, 2023. 4, 8, 9 [100] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, Mitigating hallucination in large multi-modal models via robust instruction tuning, in ICLR, 2023. 4, 8, 9, 14 [101] A. Gunjal, J. Yin, and E. Bas, Detecting and preventing hallucinations in large vision language models, in AAAI, 2024. 4, 8, 9, 14, 15 [102] J. Wang, Y. Zhou, G. Xu, P. Shi, C. Zhao, H. Xu, Q. Ye, M. Yan, J. Zhang, J. Zhu et al., Evaluation and analysis of hallucination in large vision-language models, arXiv:2308.15126, 2023. 4 [103] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang et al., Aligning large multimodal models with factually augmented rlhf, arXiv:2309.14525, 2023. 4, 8 [104] C. Cui, Y. Zhou, X. Yang, S. Wu, L. Zhang, J. Zou, and H. Yao, Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges, arXiv:2311.03287, 2023. 4, 9, 10, 15 [105] J. Liu, Y. Fu, R. Xie, R. Xie, X. Sun, F. Lian, Z. Kang, and X. Li, Phd: prompted visual hallucination evaluation dataset, arXiv:2403.11116, 2024. 4, 9 [106] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob et al., Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, in CVPR, 2024. 4, 8, 9, 13, 14 [107] J. Wang, Y. Wang, G. Xu, J. Zhang, Y. Gu, H. Jia, M. Yan, J. Zhang, and J. Sang, An llm-free multi-dimensional benchmark for mllms hallucination evaluation, arXiv:2311.07397, 2023. 4, 9 [108] A. Ben-Kish, M. Yanuka, M. Alper, R. Giryes, and H. AverbuchElor, Mocha: Multi-objective reinforcement mitigating caption hallucinations, arXiv:2312.03631, 2023. 4, [109] X. Chen, C. Wang, Y. Xue, N. Zhang, X. Yang, Q. Li, Y. Shen, J. Gu, and H. Chen, Unified hallucination detection for multimodal large language models, arXiv:2402.03190, 2024. 4 [110] W. Huang, H. Liu, M. Guo, and N. Z. Gong, Visual hallucinations of multi-modal large language models, arXiv:2402.14683, 2024. 4, 9 [111] H. Qiu, W. Hu, Z.-Y. Dou, and N. Peng, Valor-eval: Holistic coverage and faithfulness evaluation of large vision-language models, arXiv:2404.13874, 2024. 4 [112] Y. Wang, Y. Wang, D. Zhao, C. Xie, and Z. Zheng, Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large videolanguage models, arXiv:2406.16338, 2024. 4, 9 [113] B. Yan, J. Zhang, Z. Yuan, S. Shan, and X. Chen, Evaluating the quality of hallucination benchmarks for large vision-language models, arXiv:2406.17115, 2024. 4 [114] M. Wu, J. Ji, O. Huang, J. Li, Y. Wu, X. Sun, and R. Ji, Evaluating and analyzing relationship hallucinations in lvlms, arXiv:2406.16449, 2024. [115] K.-i. Lee, M. Kim, S. Yoon, M. Kim, D. Lee, H. Koh, and K. Jung, Vlind-bench: Measuring language priors in large vision-language models, arXiv:2406.08702, 2024. 4, 9 [116] J. Zhang, S. Wang, X. Cao, Z. Yuan, S. Shan, X. Chen, and W. Gao, Vlbiasbench: comprehensive benchmark for evaluating bias in large vision-language model, arXiv:2406.14194, 2024. 4, 9 [117] W. Ye, G. Zheng, Y. Ma, X. Cao, B. Lai, J. M. Rehg, and A. Zhang, Mm-spubench: Towards better understanding of spurious biases in multimodal llms, arXiv:2406.17126, 2024. 4, 10 JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 23 [118] H. Tu, C. Cui, Z. Wang, Y. Zhou, B. Zhao, J. Han, W. Zhou, H. Yao, and C. Xie, How many unicorns are in this image? safety evaluation benchmark for vision llms, arXiv:2311.16101, 2023. 4, 10, 12, 16 [119] Y. Zhang, Y. Huang, Y. Sun, C. Liu, Z. Zhao, Z. Fang, Y. Wang, H. Chen, X. Yang, X. Wei et al., Benchmarking trustworthiness of multimodal large language models: comprehensive study, arXiv:2406.07057, 2024. 4, [120] Y. Zhao, T. Pang, C. Du, X. Yang, C. Li, N.-M. M. Cheung, and M. Lin, On evaluating adversarial robustness of large visionlanguage models, in NeurIPS, 2024. 4, 8, 10 [121] Q. Guo, S. Pang, X. Jia, and Q. Guo, Efficiently adversarial examples generation for visual-language models under targeted transfer scenarios using diffusion models, arXiv:2404.10335, 2024. 4, 10 [122] X. Li, H. Zhou, R. Wang, T. Zhou, M. Cheng, and C.-J. Hsieh, Mossbench: Is your multimodal language model oversensitive to safe queries? arXiv:2406.17806, 2024. 4, 10 [123] M. Chen, B. Peng, Y. Zhang, and C. Lu, Cello: Causal evaluation of large vision-language models, arXiv:2406.19131, 2024. 4, 10 [124] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman, dataset of clinically generated visual questions and answers about radiology images, Sci. Data, 2018. 4, 8, [125] X. He, Y. Zhang, L. Mou, E. Xing, and P. Xie, Pathvqa: 30000+ questions for medical visual question answering, arXiv:2003.10286, 2020. 4, 8, 10 [126] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu, Slake: semantically-labeled knowledge-enhanced dataset for medical visual question answering, in ISBI, 2021. 4, 8, 10 [127] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie, Pmc-vqa: Visual instruction tuning for medical visual question answering, arXiv:2305.10415, 2023. 4, 8, 10, 11 [128] Y. Hu, T. Li, Q. Lu, W. Shao, J. He, Y. Qiao, and P. Luo, Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm, in CVPR, 2024. 4, 11 [129] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, Towards generalist foundation model for radiology, arXiv:2308.02463, 2023. 4, 10 [130] P. Chen, J. Ye, G. Wang, Y. Li, Z. Deng, W. Li, T. Li, H. Duan, Z. Huang, Y. Su et al., Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai, arXiv:2408.03361, 2024. 4, [131] Q. Yang, M. Ye, and B. Du, Emollm: Multimodal emotional understanding meets large language models, arXiv:2406.16442, 2024. 4, 11 [132] Y. Li, A. Dao, W. Bao, Z. Tan, T. Chen, H. Liu, and Y. Kong, Facial affective behavior analysis with instruction tuning, arXiv:2404.05052, 2024. 4, 11 [133] S. Lobry, D. Marcos, J. Murray, and D. Tuia, Rsvqa: Visual question answering for remote sensing data, IEEE Trans. Geosci. Remote Sens., 2020. 4, 8, 11 [134] X. Zheng, B. Wang, X. Du, and X. Lu, Mutual attention inception network for remote sensing visual question answering, IEEE Trans. Geosci. Remote Sens., 2021. 4, 8 [135] M. M. Al Rahhal, Y. Bazi, S. O. Alsaleh, M. Al-Razgan, M. L. Mekhalfi, M. Al Zuair, and N. Alajlan, Open-ended remote sensing visual question answering with transformers, Int. J. Remote Sens., 2022. 4 [136] Y. Sun, S. Feng, X. Li, Y. Ye, J. Kang, and X. Huang, Visual grounding in remote sensing images, in ACM MM, 2022. 4, [137] Y. Zhan, Z. Xiong, and Y. Yuan, Rsvg: Exploring data and models for visual grounding on remote sensing data, IEEE Trans. Geosci. Remote Sens., 2023. 4, 11 [138] Y. Hu, J. Yuan, C. Wen, X. Lu, and X. Li, Rsgpt: remote sensing vision language model and benchmark, arXiv:2307.15266, 2023. 4, 11 [139] S. Liu, Y. Ma, X. Zhang, H. Wang, J. Ji, X. Sun, and R. Ji, Rotated multi-scale interaction network for referring remote sensing image segmentation, in CVPR, 2024. 4, 11 [140] X. Li, J. Ding, and M. Elhoseiny, Vrsbench: versatile visionlanguage benchmark dataset for remote sensing image understanding, arXiv:2406.12384, 2024. 4, 11 [141] Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, and G. Yu, Appagent: Multimodal agents as smartphone users, arXiv:2312.13771, 2023. 4, [142] J. Wang, H. Xu, J. Ye, M. Yan, W. Shen, J. Zhang, F. Huang, and J. Sang, Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, arXiv:2401.16158, 2024. 4, 11 [143] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan, Gpt4tools: Teaching large language model to use tools via selfinstruction, in NeurIPS, 2024. 4, 8, 11 [144] C. Shi, C. Yang, Y. Liu, B. Shui, J. Wang, M. Jing, L. Xu, X. Zhu, S. Li, Y. Zhang et al., Chartmimic: Evaluating lmms cross-modal reasoning capability via chart-to-code generation, arXiv:2406.09961, 2024. 4, 11, 12 [145] S. Yun, H. Lin, R. Thushara, M. Q. Bhat, Y. Wang, Z. Jiang, M. Deng, J. Wang, T. Tao, J. Li et al., Web2code: largescale webpage-to-code dataset and evaluation framework for multimodal llms, arXiv:2406.20098, 2024. 4, 11 [146] N. Wichers, D. Hakkani-Tr, and J. Chen, Resolving referring expressions in images with labeled elements, in SLT, 2018. 4, 12, 16 [147] B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li, Screen2words: Automatic mobile ui summarization with multimodal learning, in UIST, 2021. 4, 8, 12, 14, 15, 16 [148] Y.-C. Hsiao, F. Zubach, G. Baechler, V. Carbune, J. Lin, M. Wang, S. Sunkara, Y. Zhu, and J. Chen, Screenqa: Large-scale questionanswer pairs over mobile app screenshots, arXiv:2209.08199, 2022. 4, 12, [149] S. Sunkara, M. Wang, L. Liu, G. Baechler, Y.-C. Hsiao, A. Sharma, J. Stout et al., Towards better semantic understanding of mobile interfaces, arXiv:2210.02663, 2022. 4, 12, 14, 16 [150] G. Baechler, S. Sunkara, M. Wang, F. Zubach, H. Mansoor, V. Etter, V. Carbune, J. Lin, J. Chen, and A. Sharma, Screenai: vision-language model for ui and infographics understanding, arXiv:2402.04615, 2024. 4, 16 [151] Y. Li, G. Li, L. He, J. Zheng, H. Li, and Z. Guan, Widget captioning: Generating natural language description for mobile user interface elements, arXiv:2010.04295, 2020. 4, 8, 12, 14, 16 [152] R. Cai, Z. Song, D. Guan, Z. Chen, X. Luo, C. Yi, and A. Kot, Benchlmm: Benchmarking cross-style visual capability of large multimodal models, arXiv:2312.02896, 2023. 4, 12, 14 [153] J. Zhang, T. Pang, C. Du, Y. Ren, B. Li, and M. Lin, Benchmarking large multimodal models against common corruptions, arXiv:2401.11943, 2024. 4, 12, 13, 14 [154] S. Cheng, B. Tian, Q. Liu, X. Chen, Y. Wang, H. Chen, and N. Zhang, Can we edit multimodal large language models? in EMNLP, 2023. 4, 12, [155] H. Huang, H. Zhong, T. Yu, Q. Liu, S. Wu, L. Wang, and T. Tan, Vlkeb: large vision-language model knowledge editing benchmark, arXiv:2403.07350, 2024. 4, 12, 14 [156] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, Embodied question answering, in CVPR, 2018. 4, 8, 12, 13, 14 [157] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, The epic-kitchens dataset: Collection, challenges and baselines, IEEE Trans. Pattern Anal. Mach. Intell., 2021. 4, 8, 12, 14 [158] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al., Ego4d: Around the world in 3,000 hours of egocentric video, in CVPR, 2022. 4, 8, 12 [159] S. Datta, S. Dharur, V. Cartillier, R. Desai, M. Khanna, D. Batra, and D. Parikh, Episodic memory question answering, in CVPR, 2022. 4, 12 [160] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang, Sqa3d: Situated question answering in 3d scenes, in ICLR, 2023. 4, 12, 14, 16 [161] A. Burns, D. Arsan, S. Agrawal, R. Kumar, K. Saenko, and B. A. Plummer, dataset for interactive vision-language navigation with unknown command feasibility, in ECCV, 2022. 4, 12, 14, 16 [162] Z. Chen, Z. Shi, X. Lu, L. He, S. Qian, H. S. Fang, Z. Yin, W. Ouyang, J. Shao, Y. Qiao et al., Rh20t-p: primitivelevel robotic dataset towards composable generalization agents, arXiv:2403.19622, 2024. 4, 12, [163] T. Wang, X. Mao, C. Zhu, R. Xu, R. Lyu, P. Li, X. Chen, W. Zhang, K. Chen, T. Xue et al., Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai, in CVPR, 2024. 4, 12, 14 [164] J. Kim, A. Rohrbach, T. Darrell, J. Canny, and Z. Akata, Textual explanations for self-driving vehicles, ECCV, 2018. 4, 8, 13, 16 JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 24 [165] J. Kim, T. Misu, Y.-T. Chen, A. Tawari, and J. Canny, Grounding human-to-vehicle advice for self-driving vehicles, in CVPR, 2019. 4, 8, 13, 16 [166] T. Deruyttere, S. Vandenhende, D. Grujicic, L. Van Gool, and M.-F. Moens, Talk2car: Taking control of your self-driving car, arXiv:1909.10838, 2019. 4, 8, [167] E. Sachdeva, N. Agarwal, S. Chundi, S. Roelofs, J. Li, M. Kochenderfer, C. Choi, and B. Dariush, Rank2tell: multimodal driving dataset for joint importance ranking and reasoning, in WACV, 2024. 4, 13, 16 [168] S. Malla, C. Choi, I. Dwivedi, J. H. Choi, and J. Li, Drama: Joint risk localization and captioning in driving, in WACV, 2023. 4, 13 [169] T. Qian, J. Chen, L. Zhuo, Y. Jiao, and Y.-G. Jiang, Nuscenesqa: multi-modal visual question answering benchmark for autonomous driving scenario, in AAAI, 2024. 4, 13, 14 [170] C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, P. Luo, A. Geiger, and H. Li, Drivelm: Driving with graph visual question answering, arXiv:2312.14150, 2023. 4, 13, 14 [171] A.-M. Marcu, L. Chen, J. Hnermann, A. Karnsund, B. Hanotte, P. Chidananda, S. Nair, V. Badrinarayanan, A. Kendall, J. Shotton, and O. Sinavski, Lingoqa: Video question answering for autonomous driving, arXiv:2312.14115, 2023. 4, 13, 14, 16 [172] D. Wu, W. Han, T. Wang, Y. Liu, X. Zhang, and J. Shen, Language prompt for autonomous driving, arXiv:2309.04379, 2023. 4, 13, 16 [173] M. Nie, R. Peng, C. Wang, X. Cai, J. Han, H. Xu, and L. Zhang, Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving, arXiv:2312.03661, 2023. 4, 13, 16 [174] Y. Lu, Y. Yao, J. Tu, J. Shao, Y. Ma, and X. Zhu, Can lvlms obtain drivers license? benchmark towards reliable agi for autonomous driving, arXiv:2409.02914, 2024. 4, 13 [175] Y. Shi, D. Peng, W. Liao, Z. Lin, X. Chen, C. Liu, Y. Zhang, and L. Jin, Exploring ocr capabilities of gpt-4v (ision): quantitative and in-depth evaluation, arXiv:2310.16809, 2023. [176] J. Tang, Q. Liu, Y. Ye, J. Lu, S. Wei, C. Lin, W. Li, M. F. F. B. Mahmood, H. Feng, Z. Zhao et al., Mtvqa: Benchmarking multilingual text-centric visual question answering, arXiv:2405.11985, 2024. 5, 16 [177] R. Zhang, Y. Zhang, J. Chen, Y. Zhou, J. Gu, C. Chen, and T. Sun, Trins: Towards multimodal language models that can read, in CVPR, 2024. 6 [178] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang et al., Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, arXiv:2405.21075, 2024. 6, 14, 15, 16 [179] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, Llavanext: Improved reasoning, ocr, and world knowledge, https:// llava-vl.github.io/blog/2024-01-30-llava-next/, 2024. 7, 17 [180] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in ECCV, 2016. 8 [181] R. Hong, D. Liu, X. Mo, X. He, and H. Zhang, Learning to compose and reason with language tree structures for visual grounding, IEEE Trans. Pattern Anal. Mach. Intell., 2019. 7 [182] W. Tang, L. Li, X. Liu, L. Jin, J. Tang, and Z. Li, Context disentangling and prototype inheriting for robust visual grounding, IEEE Trans. Pattern Anal. Mach. Intell., 2023. 7 [183] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: frontier large vision-language model with versatile abilities, arXiv:2308.12966, 2023. [184] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song et al., Cogvlm: Visual expert for pretrained language models, arXiv:2311.03079, 2023. 7 [185] S. Yin, C. Fu, S. Zhao, T. Xu, H. Wang, D. Sui, Y. Shen, K. Li, X. Sun, and E. Chen, Woodpecker: Hallucination correction for multimodal large language models, arXiv:2310.16045, 2023. 9 [186] X. Chen, C. Wang, Y. Xue, N. Zhang, X. Yang, Q. Li, Y. Shen, J. Gu, and H. Chen, Unified hallucination detection for multimodal large language models, arXiv:2402.03190, 2024. 9 [187] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in CVPR, 2024. 9 [188] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, referential dialogue llms Shikra: Unleashing multimodal magic, arXiv:2306.15195, 2023. 9 [189] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., Gemini: family of highly capable multimodal models, arXiv:2312.11805, 2023. 9 [190] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, arXiv:2304.10592, 2023. 10 [191] A. Wei, N. Haghtalab, and J. Steinhardt, Jailbroken: How does llm safety training fail? in NeurIPS, 2024. 10 [192] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, 2023. 10, 11 [193] Anthropic, Introducing the next generation of claude, https: //www.anthropic.com/news/claude-3-family, 2024. 10 [194] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou, J. Yang, H. Su, J. Zhu et al., Llava-plus: Learning to use tools for creating multimodal agents, arXiv:2311.05437, 2023. 11 [195] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl et al., Phi-3 technical report: highly capable language model locally on your phone, arXiv:2404.14219, 2024. 11 [196] K. You, H. Zhang, E. Schoop, F. Weers, A. Swearngin, J. Nichols, Y. Yang, and Z. Gan, Ferret-ui: Grounded mobile ui understanding with multimodal llms, arXiv:2404.05719, 2024. 12, 16 [197] B. Jia, T. Lei, S.-C. Zhu, and S. Huang, Egotaskqa: Understanding human tasks in egocentric videos, in NeurIPS, 2022. 12 [198] N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, and K. Ikeuchi, Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration, arXiv:2311.12015, 2023. 12, 13 [199] Y. Hu, F. Lin, T. Zhang, L. Yi, and Y. Gao, Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning, arXiv:2311.17842, 2023. [200] Y. Qin, E. Zhou, Q. Liu, Z. Yin, L. Sheng, R. Zhang, Y. Qiao, and J. Shao, Mp5: multi-modal open-ended embodied system in minecraft via active perception, arXiv:2312.07472, 2023. 13 [201] X. Tian, J. Gu, B. Li, Y. Liu, C. Hu, Y. Wang, K. Zhan, P. Jia, X. Lang, and H. Zhao, Drivevlm: The convergence of autonomous driving and large vision-language models, arXiv:2402.12289, 2024. 13 [202] C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu, Z. Yang, K.-D. Liao et al., survey on multimodal large language models for autonomous driving, in WACV, 2024. 13 [203] L. Wen, X. Yang, D. Fu, X. Wang, P. Cai, X. Li, T. Ma, Y. Li, L. Xu, D. Shang et al., On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving, arXiv:2311.05332, 2023. 13 [204] C. Cui, Y. Zhou, X. Yang, S. Wu, L. Zhang, J. Zou, and H. Yao, Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges, arXiv:2311.03287, 2023. 14, [205] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang, and Y. Yang, Ferret: Refer and ground anything anywhere at any granularity, arXiv:2310.07704, 2023. 14 [206] J. Huang, L. Chen, T. Guo, F. Zeng, Y. Zhao, B. Wu, Y. Yuan, H. Zhao, Z. Guo, Y. Zhang et al., Mmevalpro: Calibrating multimodal benchmarks towards trustworthy and efficient evaluation, arXiv:2407.00468, 2024. 14, 15 [207] Y. Li, X. Liu, X. Wang, S. Wang, and W. Lin, Fakebench: Uncover the achilles heels of fake images with large multimodal models, arXiv:2404.13306, 2024. 14, 16 [208] S. A. Lab, Multi-modal arena, https://opencompass.org.cn/ arena?type=multimodal, 2023. 14 [209] P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, Large language models are not fair evaluators, arXiv:2305.17926, 2023. 15 [210] Y. Qian, H. Ye, J.-P. Fauconnier, P. Grasch, Y. Yang, and Z. Gan, Mia-bench: Towards better instruction following evaluation of multimodal llms, arXiv:2407.01509, 2024. [211] O. Contributors, Opencompass: universal evaluation platform for foundation models, https://github.com/open-compass/ opencompass, 2023. 17 [212] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, J. Wang et al., Vlmevalkit: An openfor evaluating large multi-modality models, Y. Zang, P. Zhang, source toolkit arXiv:2407.11691, 2024. 17 [213] K. Zhang, B. Li, P. Zhang, F. Pu, J. A. Cahyono, K. Hu, S. Liu, Y. Zhang, J. Yang, C. Li et al., Lmms-eval: Reality check on the evaluation of large multimodal models, arXiv:2407.12772, 2024. 17 JOURNAL OF LATEX CLASS FILES, NOVEMBER 2024 [239] D. Z. Chen, A. X. Chang, and M. Niener, Scanrefer: 3d object localization in rgb-d scans using natural language, in ECCV, 2020. 20 [240] C. Zhu, T. Wang, W. Zhang, K. Chen, and X. Liu, Empowering 3d visual grounding with reasoning capabilities, arXiv:2407.01525, 2024. 20 [214] L. Zheng, Z. Huang, Z. Xue, X. Wang, B. An, and S. Yan, Agentstudio: toolkit for building general virtual agents, arXiv:2403.17918, 2024. 17, 18 [215] C. Royer, B. Menze, and A. Sekuboyina, Multimedeval: benchmark and toolkit for evaluating medical vision-language models, arXiv:2402.09262, 2024. 17, 18 [216] P. Padlewski, M. Bain, M. Henderson, Z. Zhu, N. Relan, H. Pham, D. Ong, K. Aleksiev, A. Ormazabal, S. Phua et al., Vibe-eval: hard evaluation suite for measuring progress of multimodal language models, arXiv:2405.02287, 2024. 17 [217] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao, Llava-med: Training large language-and-vision assistant for biomedicine in one day, in NeurIPS, 2024. 18 [218] K. Zhang, J. Yu, Z. Yan, Y. Liu, E. Adhikarla, S. Fu, X. Chen, C. Chen, Y. Zhou, X. Li et al., Biomedgpt: unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks, arXiv:2305.17100, 2023. 18 [219] Y. Qiao, H. Duan, X. Fang, J. Yang, L. Chen, S. Zhang, J. Wang, D. Lin, and K. Chen, Prism: framework for decoupling and assessing the capabilities of vlms, arXiv:2406.14544, 2024. 19 [220] Y. Du, C. Li, R. Guo, X. Yin, W. Liu, J. Zhou, Y. Bai, Z. Yu, Y. Yang, Q. Dang et al., Pp-ocr: practical ultra lightweight ocr system, arXiv:2009.09941, 2020. 20 [221] Z. Kuang, H. Sun, Z. Li, X. Yue, T. H. Lin, J. Chen, H. Wei, Y. Zhu, T. Gao, W. Zhang et al., Mmocr: comprehensive toolbox for text detection, recognition and understanding, in ACM MM, 2021. 20 [222] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in CVPR, 2016. 20 [223] K. He, G. Gkioxari, P. Dollr, and R. Girshick, Mask r-cnn, in ICCV, 2017. [224] S. Ren, K. He, R. Girshick, and J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 2016. 20 [225] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in ECCV, 2020. 20 [226] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, Temporal segment networks: Towards good practices for deep action recognition, in ECCV, 2016. 20 [227] C. Feichtenhofer, H. Fan, J. Malik, and K. He, Slowfast networks for video recognition, in ICCV, 2019. 20 [228] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, Revisiting skeleton-based action recognition, in CVPR, 2022. 20 [229] Z. Yang, J. Liu, Y. Han, X. Chen, Z. Huang, B. Fu, and G. Yu, Appagent: Multimodal agents as smartphone users, arXiv:2312.13771, 2023. 20 [230] J. Wang, H. Xu, J. Ye, M. Yan, W. Shen, J. Zhang, F. Huang, and J. Sang, Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, arXiv:2401.16158, 2024. 20 [231] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei et al., Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, arXiv:2404.07972, 2024. 20 [232] C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. Bishop, W. Li, F. Campbell-Ajala et al., Androidworld: dynamic benchmarking environment for autonomous agents, arXiv:2405.14573, 2024. 20 [233] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried, Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, arXiv:2401.13649, 2024. [234] Y. Chu, J. Xu, X. Zhou, Q. Yang, S. Zhang, Z. Yan, C. Zhou, and J. Zhou, Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models, arXiv:2311.07919, 2023. 20 [235] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: an asr corpus based on public domain audio books, in ICASSP, 2015. 20 [236] C. Wang, A. Wu, J. Gu, and J. Pino, Covost 2 and massively multilingual speech translation. in Interspeech, 2021. 20 [237] K. Drossos, S. Lipping, and T. Virtanen, Clotho: An audio captioning dataset, in ICASSP, 2020. [238] Y. Gong, J. Yu, and J. Glass, Vocalsound: dataset for improving human vocal sounds recognition, in ICASSP, 2022."
        }
    ],
    "affiliations": ["Nanjing University", "Institute of Automation, Chinese Academy of Science", "University of Science and Technology of China", "Nanyang Technological University", "Shanghai AI Laboratory"]
}