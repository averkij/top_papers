{
    "paper_title": "MemMamba: Rethinking Memory Patterns in State Space Model",
    "authors": [
        "Youjin Wang",
        "Yangjingyi Chen",
        "Jiahao Yan",
        "Jiaxuan Lu",
        "Xiao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 7 2 3 0 . 0 1 5 2 : r Under review as conference paper at ICLR MEMMAMBA: RETHINKING MEMORY PATTERNS IN STATE SPACE MODEL Youjin Wang School of Statistics Renmin University of China Beijing, China Jiahao Yan Gao Ling Institute of Artificial Intelligence Renmin University of China Beijing, China Xiao Sun Shanghai Artificial Intelligence Laboratory Shanghai, China Yangjingyi Chen Shanghai University of Finance and Economics Shanghai, China Jiaxuan Lu Shanghai Artificial Intelligence Laboratory Shanghai, China"
        },
        {
            "title": "ABSTRACT",
            "content": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering fundamental question: what is the nature of Mambas long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontalvertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, novel architectural framework that integrates state summarization mechanism together with crosslayer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19-PPL and Passkey Retrieval, while delivering 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves breakthrough in the complexitymemory trade-off, offering new paradigm for ultra-long sequence modeling."
        },
        {
            "title": "INTRODUCTION",
            "content": "Long-sequence data typically refers to continuous sequences spanning thousands to millions of time steps or tokens, which pervade modern machine learning applications, from modeling book-length documents in NLP, to analyzing DNA sequences in bioinformatics, to processing complex multimodal medical records. central challenge in sequence modeling is how to capture ultra-long-range Independent first author: Youjin Wang (wangyoujin@ruc.edu.cn) These authors contributed equally as the second authors. Corresponding authors: Jiaxuan Lu (lujiaxuan@pjlab.org.cn); Xiao Sun. 1 Under review as conference paper at ICLR 2026 dependencies while maintaining efficiency. Traditional architectures exhibit significant limitations when dealing with such data. Recurrent neural networks (RNNs) and their variants (LSTM, GRU) are inherently sequential and suffer from vanishing or exploding gradients, making them unstable for long dependencies (Pascanu et al., 2013) (Hochreiter & Schmidhuber, 1997). Transformers introduced paradigm shift with self-attention and global context modeling (Vaswani et al., 2017), but their quadratic complexity in sequence length renders them inefficient for truly long contexts (Brown et al., 2020). The trade-off between expressiveness and scalability has created an architectural impasse. Recent advances in selective state-space models (SSMs), notably the Mamba architecture (Gu & Dao, 2023), offer compelling alternative. By decoupling sequence length from computation, Mamba achieves linear-time complexity O(n) and constant-time recurrent inference O(1), positioning itself as promising foundation for long-sequence modeling. However, despite this computational leap, its memory fidelity degrades rapidly at scale. As sequence length grows, Mamba and its successors (e.g., Mamba-2) exhibit sharp declines in tasks demanding strong memory retention, such as 5-shot MMLU or long-range key-value retrieval (Waleffe et al., 2024). This leads to fundamental question: How does Mambas memory pattern evolve with distance and depth, and what underlies its degradation? This paper introduces new lens for understanding and advancing long-sequence models. We present the first systematic analysis of Mambas memory mechanism. Through mathematical derivation and information-theoretic analysis, we characterize its memory decay behavior and introduce the horizontalvertical memory fidelity framework, which quantifies critical information loss from two perspectives: token-level semantic transmission and cross-layer information coupling. Our analysis reveals that, although Mambas state update ensures computational stability, the contribution of early information decays exponentially during both intra-layer recursion and inter-layer propagation, fundamentally constraining its long-range memory capacity. Building on these insights, we propose MemMamba, novel architecture that reimagines statespace modeling as structured memory system. Inspired by how humans take notes while reading long texts, MemMamba integrates lightweight state summarization with cross-layer and cross-token attention to dynamically preserve and reuse salient information, all while maintaining linear computational complexity. This note-taking mechanism alleviates long-range forgetting, breaking the classical trade-off in SSMs. Empirically, MemMamba achieves breakthrough improvements across multiple long-sequence benchmarks. On the PG19 language modeling task, it maintains stable perplexity (17.35) even at 60k tokens, where Mamba and DeciMamba (Ben-Kish et al., 2025) of similar parameter scale collapse completely. On the Passkey Retrieval task, MemMamba preserves 90% retrieval accuracy at 400k tokens. On the cross-document retrieval task under noisy conditions, it significantly outperforms both Transformers and existing state-space variants. The main contributions of this work are summarized as follows: Memory-theoretic insight. We formalize Mambas information bottlenecks through the horizontalvertical memory fidelity framework, offering new perspective on longsequence degradation. Architectural innovation. MemMamba introduces state summarization and cross-layer / cross-token attention to simulate note-taking and bridge across-time and across-layer memory decay, without compromising efficiency. Empirical breakthroughs. On language modeling, sparse retrieval, and cross-document reasoning tasks, MemMamba consistently outperforms Mamba variants and Transformer baselines, achieving 48% inference speedup, setting new bar for memory retention in efficient sequence models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 STATE SPACE MODELS State space models (SSMs) have become strong candidates for long-sequence modeling due to their linear-time complexity and recursive inference. Since S4 (Gu et al., 2021), SSMs have made con2 Under review as conference paper at ICLR 2026 tinuous progress in language, speech, and time-series modeling. Mamba (Gu & Dao, 2023) stands out by leveraging selective SSM mechanism that enhances expressiveness, achieving performance comparable to or surpassing Transformers in language modeling, genomics, and reasoning tasks. Building on Mambas success, follow-up works focus on three main directions: 1) Architectural optimization: BiMamba (Liang et al., 2024) improves long-range dependency modeling with bidirectional state updates, and Vision Mamba (Zhu et al., 2024) adapts Mamba for vision tasks. 2) Computational efficiency: FastMamba (Wang et al., 2025) improves training and inference speed via parallelization and caching, enabling scalability to longer sequences. 3) Application extension: Mamba has been applied to molecular dynamics (Hu et al., 2025), speech recognition (Zhang et al., 2025), EEG signal understanding (Liu et al., 2025), image recognition (Lin et al., 2025; Lu et al., 2025), event analysis (Lin et al., 2024), and long-text understanding, showcasing its cross-modal generalization capabilities. Nevertheless, most of these efforts primarily target architectural design or efficiency improvements, which further highlights the central challenge of long-sequence modeling: how to continuously enhance models ability to capture long dependencies while preserving computational efficiency?"
        },
        {
            "title": "2.2 LONG-SEQUENCE MODELING",
            "content": "Long-sequence modeling is critical issue in AI and cognitive science. Early models like LSTMs and GRUs introduced gating for long-term dependencies, while NTMs and DNCs added external memory. Memory Networks proposed slot-based storage, and Hopfield networks improved associative memory. Neuroscience-inspired models, such as spiking neural networks and HTM, have also emerged. The Transformer has become the standard for modeling long-range dependencies. The Compressive Transformer improves efficiency with compressed memory, though at the cost of information loss (Rae et al., 2019). Megalodon supports million-token contexts but excels in extreme-length tasks (Ma et al., 2024). Sparse-attention models like Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) reduce complexity, but struggle with ultra-long sequences. These limitations have led to SSM-based approaches like Mamba. DeciMamba extends context length 25 through dynamic pooling, boosting performance by 220% on TriviaQA (Ben-Kish et al., 2025), but risks losing fine-grained information. mmMamba integrates multimodal distillation for 20.6 speedup (Li et al., 2024), but requires costly distillation data. DocMamba reduces memory overhead by 88.3% for document processing (Hu et al., 2024), though gains are task-specific. LongMamba improves long-context extrapolation, but faces stability issues (Ye et al., 2025). Mamba-2 refines architecture and stability, outperforming the original Mamba, but still lags behind Transformers in tasks requiring strong copying or in-context learning (Gu & Dao, 2024). These advances highlight Mambas potential in long-sequence tasks. However, most prior work focuses on structural or context extension. The question of how models remember and forget critical information remains largely unexplored. Our work focuses on memory patterns in Mamba to address long-range forgetting and expand the design space for memory-augmented SSMs."
        },
        {
            "title": "INVESTIGATION OF MEMORY PATTERNS",
            "content": "3.1 MEMORY DECAY IN MAMBA The Mamba model builds on the mechanism of selective state space models (SSMs), achieving efficient sequence modeling through dynamic state compression, with explicit state update equations forming its computational core. While Mamba has significant advantages in computational efficiency, it tends to suffer from memory decay when modeling long-range dependencies, and it is also limited in capturing fine-grained local information. The state update of Mamba is defined as: ht = ht1 + xt, yt = ht, (1) Under review as conference paper at ICLR 2026 where Rdsds is the state transition matrix satisfying < 1 to guarantee BIBO stability, ht denotes the hidden state at step t, and xt is the input at step t. To measure how the contribution of important information decays over long distances, we define the notion of information contribution, i.e., the degree to which an input affects subsequent states of the model. For an input xtk occurring steps earlier, its contribution to the current state ht can be expressed as: Contribution(xtk ht) = Ak xtk Ak xtk. As increases (i.e., as the input becomes further in the past), Ak decays exponentially (e.g., Ak eαk with α > 0), causing early inputs to be almost completely forgotten. (2)"
        },
        {
            "title": "3.3 MEMORY DECAY IN TRANSFORMER",
            "content": "The Transformer model relies on the self-attention mechanism, where queries (Q), keys (K), and values (V) interact through Softmax normalization to perform weighted aggregation, thereby enabling global dependency modeling. However, this mechanism incurs quadratic complexity with respect to sequence length, which constitutes major bottleneck for long-sequence processing. While Transformers retain long-range dependencies more effectively than Mamba, their high computational cost also induces memory truncation effects in practice. The time complexity of Transformer self-attention (TC) is: TC = O(L n2 d), (3) where is the number of layers, is the sequence length, and is the feature dimension. For ultra-long sequences (e.g., = 105), the quadratic n2 term results in 1010 operations, which far exceeds the capacity of current hardware. In practice, approximations such as sliding-window attention (with window size = 512) or sparse attention are employed, but these truncations inevitably discard information outside the window. We therefore define the notion of effective modeling length (EML), the maximum sequence length within which dependencies can be effectively captured: EML = inability to capture long-range dependencies. (4) detailed mathematical derivation of this truncation-induced information loss is provided in Appendix A.1. where we further elaborate the intermediate steps and theoretical implications. 3.4 HORIZONTAL AND VERTICAL MEMORY FIDELITY Our theoretical derivations and preliminary experiments reveal that key information loss can be decomposed into two complementary aspects: horizontal information loss among tokens within layer, and vertical information loss across layers (see Appendix A.1 for details). To capture both dimensions of degradation, we propose the HorizontalVertical Memory Fidelity Framework, which provides principled lens for analyzing memory retention in long-sequence models. Definitions. We define the Expected Token Memory Fidelity (ETMF) as the degree to which semantic information of tokens is preserved during horizontal propagation across time steps, and the Expected Cross-Layer Memory Fidelity (ECLMF) as the degree to which information is preserved during vertical transmission across layers. ETMF focuses on token-level semantic fidelity, while ECLMF focuses on layer-wise propagation fidelity. Significance. These two metrics provide complementary perspectives: ETMF reflects whether longrange token semantics remain faithful after recursive propagation, while ECLMF quantifies the degradation of information across layers. Together, they highlight the dual challenges of memory decay and extrapolation limits in Mamba, offering principled tools for evaluating and interpreting memory behavior. Moreover, ETMF and ECLMF can guide architectural enhancements such as cross-layer attention or redundant encoding strategies. 4 Under review as conference paper at ICLR Figure 1: Overall workflow of MemMamba. The framework is composed of stacked MemMamba Block Layers, where each layer preserves critical context via the Note Block and enables long-range interaction through sparse cross-layer attention. Figure 2: Workflow of MemMamba Block Layer. Each block integrates three components: state space model (SSM) updates, cross-token attention, and periodically triggered cross-layer attention. The full mathematical definitions, derivations, and implementation details of ETMF and ECLMF are provided in the Appendix. where we also include clarifying remarks and algorithmic procedures to facilitate understanding and practical application."
        },
        {
            "title": "4 METHOD",
            "content": "Existing state space models demonstrate superior linear complexity in long-sequence modeling, yet their recursive update mechanisms lead to gradual decay of distant dependencies. This phenomenon resembles the forgetting curve observed in cognitive science: when humans read long documents without taking notes, early key information is often overwritten or lost. Inspired by this analogy, we propose the MemMamba Network, which preserves critical context within limited representation space and provides indexing for long-range interactions across layers and tokens, ensuring that essential signals are not diluted as sequences grow longer (Baevski et al., 2020). 5 Under review as conference paper at ICLR 2026 MemMamba is composed of stacked MemMamba Block Layers. Each layer integrates three components: state space model (SSM) updates, cross-token attention, and periodically triggered cross-layer attention. To avoid redundant computation, the cross-token mechanism is executed at every layer, whereas cross-layer attention is only activated every layers. At layer and time step t, the input xl first undergoes threshold-based evaluation: if the token is likely to be forgotten, it is compressed and stored in the Note Block, which updates the state pool Sl t. The state pool is then compared against the current SSM state. If forgetting is detected, cross-token attention is performed between the state pool and the current input to restore forgotten information. Specifically, summary st 1l is retrieved from St 1l and fused with xl through cross-token attention: if Itoken(xl if Istate(zt 1l) > τ2 ctoken,l t) > τ1 sl t), Sl = l(xl = Attention(Q = xl = Insert(St 1l, sl t), t, = st 1l, = st 1l). (5) (6) Cross-layer attention is triggered every layers. When mod = 0, state pools from previous layers are aggregated at corresponding token positions, and cross-layer attention is applied. For each token in the current layer, summaries from the last layers are collected and aggregated into cross-layer context sR(l): clayer,l = Attention(Q = xl t, = sR(l), = sR(l)). (7) This dual-threshold and sparse cross-layer mechanism ensures that cross-token supplementation occurs at every layer, while cross-layer memory interaction is sparsely activated, striking balance between memory retention and computational efficiency. 4.1 NOTE BLOCK The Note Block dynamically identifies and extracts key information during sequence processing, mimicking the human note-taking process while reading. It compresses and stores important tokens into state pool. For an input xl t, its importance is measured by the scoring function Itoken. If the score exceeds threshold, the Take note operation is executed and the compressed summary is inserted into the state pool; otherwise, the token is skipped: t) > τ1; ; ; ; sl (8) where l() denotes dimensionality reduction operator (e.g., linear projection or pooling). The summary sl is then inserted into the state pool: = l(xl Itoken(xl t), = Insert(Sl Sl t1, sl t). (9) The state pool has limited capacity and adopts FIFO or priority-based replacement strategies, ensuring that only high-information summaries are retained. 4.2 MEMMAMBA BLOCK The MemMamba Block is the core intra-layer computation unit, combining SSM updates, thresholdtriggered cross-token attention, and periodically triggered cross-layer attention. The update rules are: (cid:40)Attention(cid:0)Q = , = l t1, = t1 (cid:1), if Istate (cid:0)z (cid:1) > τ2, ctoken,l = clayer,l = 0, otherwise. (cid:40)Attention(cid:0)Q = , = sR(l), = sR(l)(cid:1), if mod = 0, 0, = l+1 l+ + tok (cid:0)ctoken,l (cid:1) + (cid:0)clayer,l lay otherwise. (cid:1). (10) (11) (12) where Ftokl and then passed into the SSM update as input. lay are fusion functions (e.g., gating or residual mapping). The fused result is 6 Under review as conference paper at ICLR 2026 Table 1: Perplexity (PPL) comparison across models under different context lengths. Best results are highlighted in red, second-best in blue. Lower PPL is better. Results >100 are denoted as INF. All experiments are conducted under the same hardware and software environment. PPL variance across multiple runs is within 0.7. Model Parm 1K 2K 4K 10K 20K 30K 40K 50K 60K Mamba DeciMamba (Ben-Kish et al., 2025) Com-Transformer (Rae et al., 2019) Megalodon (Ma et al., 2024) MemMamba INF 130M 21.00 19.60 18.77 19.29 31.63 INF INF 150M 21.90 20.06 18.55 21.98 23.15 27.05 40.48 INF INF 400M 33.09 NA NA NA NA NA NA NA NA 200M 66.14 66.55 66.43 65.02 64.81 64.3 64.21 64.02 63.92 200M 19.35 18.23 17.52 17.71 18.25 17.33 17.54 17.97 17. INF Table 2: Passkey retrieval accuracy across different context lengths.At 400k tokens, due to GPU memory limits, only subset of samples are evaluated. Longer sequences require larger memory (evaluated on 20-core 80GB H800). Model 1K 2K 4K 8K 16K 32K 64K 128K 256K 400K Pythia-160M (Ben-Kish et al., 2025) Mamba-130M (Ben-Kish et al., 2025) DeciMamba-130M (Ben-Kish et al., 2025) MemMamba 1. 1.0 0.0 0.0 1.0 1.0 1. 1.0 1.0 1.0 1.0 1.0 1. 1.0 1.0 1.0 0.0 0.8 1. 1.0 0.0 0.0 1.0 1.0 0. 0.0 1.0 1.0 0.0 0.0 1. 1.0 0.0 0.0 1.0 1.0 0. 0.0 0.6 0."
        },
        {
            "title": "5 EXPERIMENTAL ANALYSIS",
            "content": "Datasets. We evaluate the proposed method on three long-sequence benchmarks. The main experiment is conducted on the PG19-PPL dataset (around 100M tokens), which consists of English novels from Project Gutenberg published around 1919. The average length is 69k tokens, and the task is language modeling, evaluated by perplexity (PPL) to measure semantic consistency and narrative coherence. Beyond language modeling, we further evaluate on two synthetic retrieval benchmarks to characterize long-range memory and retrieval ability at finer granularity. In the Passkey Retrieval task, target token is randomly inserted into an extremely long input sequence, and the model is required to precisely retrieve this information at prediction time. Since the location of key information is uncertain, this task particularly tests whether the model can maintain long-term memory under sparse cues. The Document Retrieval task covers multi-domain documents and supports both simple and detailed retrieval modes, providing comprehensive evaluation of memory and reasoning across documents and domains. Settings. All experiments are implemented in PyTorch 2.1.2 and Python 3.10 on Ubuntu 22.04 with CUDA 11.8. Training is performed on single NVIDIA RTX 4090 GPU (24GB) and 25 vCPUs (Intel Xeon Platinum 8481C). Our MemMamba is 24-layer SSM-based model. Each state summary vector is compressed to 64 dimensions, and the state pool size is fixed at 50. The training sequence length is 8,000 tokens, and the model is trained for 100k steps using the AdamW optimizer (learning rate = 1e-4, weight decay = 0.1). We adopt constant learning rate scheduling, gradient accumulation (4 steps), and gradient clipping (max norm = 1). The random seed is fixed to 123 to ensure reproducibility. (Detailed model and hardware configurations are provided in Appendix A.6.) 7 Under review as conference paper at ICLR 2026 Table 3: Performance of different models under varying numbers of noisy documents.Higher scores indicate better performance. Best results at each noise level are highlighted in red. Model Mamba (Ben-Kish et al., 2025) DeciMamba (Ben-Kish et al., 2025) MemMamba 10 0.68 0.72 0.8 20 0.71 0.74 0. 120 0.01 0.48 0.52 160 0 0.19 0.44 200 0 0.12 0."
        },
        {
            "title": "5.1 COMPARISON WITH BASELINES",
            "content": "Language Modeling. We compare MemMamba and its mechanisms against several state-of-theart long-sequence models on PG19: DeciMamba (an efficient Mamba variant optimized for reduced computation), Megalodon (enhanced sequence representations tailored for extremely long contexts), Compressive Transformer (memory compression for efficiency), and Pythia (a modular LLM with high flexibility). Results are reported in Table 1. MemMamba outperforms all baselines at most context lengths. Notably, in ultra-long sequences of 30k60k tokens, although performance degrades for all models, MemMamba shows much stronger robustness and stability. This indicates that state summarization and cross-layer attention effectively mitigate Mambas memory decay in long-range dependencies. We further analyze model performance under varying parameter scales (see Figure 5). We find that MemMamba achieves performance comparable to 12B parameter models even at very small parameter scales. Passkey Retrieval. MemMamba maintains high retrieval accuracy even with input lengths of several hundred thousand tokens. When the target token is placed more than 200k tokens away from the prediction point, MemMamba still retrieves the key information accurately, whereas Mamba and Pythia completely fail at such lengths. Compared to DeciMamba, MemMamba achieves higher accuracy on extremely long sequences (400k tokens), demonstrating more robust long-range memory retention. Document Retrieval. On the document retrieval benchmark, MemMamba achieves leading performance under both simple and detailed retrieval settings. As the number of noisy documents increases, Mambas performance drops sharply, while DeciMamba shows partial improvement but remains unstable. In contrast, MemMamba consistently maintains higher scores under high-noise conditions, highlighting its advantage in cross-document and cross-domain reasoning tasks. Overall, MemMamba achieves consistent improvements over state-of-the-art baselines across language modeling, sparse retrieval, and cross-document reasoning. Compared to Transformer-based models, it shows stronger scalability on ultra-long sequences; compared to Mamba variants, it significantly enhances long-term memory retention while preserving linear complexity. We attribute its advantages to effective compression of key information via state summarization, and alleviation of deep-layer memory decay through cross-layer attention. 5.2 ABLATION STUDIES Core mechanisms. Under identical parameter budgets and training settings, MemMamba maintains low and stable PPL across contexts: at 1.5k tokens its PPL is 19.35, and as the context grows to 60k tokens the PPL fluctuates only within 17.3318.25. Removing both the state summarization and the cross-layer / cross-token attention causes stark contrast. Efficiency. On the same hardware in single-process setting, we benchmark the inference speed for MemMamba, DeciMamba, and Transformer across sequence lengths [1000, 2000, 4000, 10000, 20000, 30000, 40000, 50000, 60000], using 100 samples per length. Despite the extra computations introduced to enhance modeling capacity, MemMambas end-to-end latency is only 0.52 that of the Transformer (i.e., 48% speedup). Unlike conventional models whose recursive updates can degrade efficiency, MemMamba leverages compact representations and cross-layer / cross-token at8 Under review as conference paper at ICLR 2026 Figure 3: Ablation results of the core mechanisms. The same hardware conditions and training configurations are used. Figure 4: Comparison of ETMF and ECLMF across different Mamba variants tention to optimize information flow, thereby sustaining high computational efficiency on ultra-long sequences. In addition, the sparse skip mechanism further reduces redundant computation, ensuring stable linear complexity O(n+m) (with the sequence length and the number of retrieved summaries/attention interactions). Together, these design choices underpin MemMambas efficiency advantage on long-sequence tasks, delivering significantly faster inference while preserving modeling strength relative to baseline models. In addition, we evaluate the efficiency of our model. Under the same hardware conditions, MemMamba achieves approximately 50% improvement in efficiency compared with the Transformer. We further test the ETMF and ECLMF scores of the original Mamba, DeciMamba, and MemMamba. For both metrics, higher scores indicate better retention of early token information. The results show that MemMamba significantly outperforms both the original Mamba and DeciMamba.Although DeciMamba shows slight advantage in extremely long-range cross-layer transmission, its instability poses substantial drawback. 5.3 PROOF OF LINEAR COMPLEXITY Despite the introduction of state summarization and cross-layer attention, MemMamba still preserves linear complexity in both time and space. Specifically, its computational cost scales with sequence length and hidden dimension as O(n d), in contrast to O(n2d) for Transformers. This is achieved by constraining the state dimension ds and the attention pool size to be constants. Detailed derivations and proofs are provided in Appendix A.4."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce MemMamba, memory-centric extension to state space models that bridges the long-standing gap between scalability and long-range dependency modeling. By augmenting the Mamba architecture with dynamic state summarization and lightweight cross-layer and cross-token attention, MemMamba offers principled solution to the memory decay problem that limits ex9 Under review as conference paper at ICLR 2026 isting SSMs. Our information-theoretic framework formalizes this degradation and motivates new architectural direction: integrating structured memory without sacrificing efficiency. Empirically, MemMamba achieves state-of-the-art results on wide range of long-sequence benchmarks including PG19, Passkey Retrieval, and multi-document reasoning. Meanwhile, complexity analysis confirms that it retains linear time and space scaling, delivering 48% speedup over baseline architectures. More broadly, MemMamba represents step toward new generation of memory-centric neural architectures that treat retention and reasoning as first-class citizens. Future work will explore extensions to multimodal settings, integration with retrieval-augmented systems, and scaling MemMamba as foundation for efficient, high-fidelity memory across complex real-world tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. In Proceedings of Advances in Neural Information Processing Systems 33 (NeurIPS 2020), volume 33, pp. 1244912460, 2020. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 70597071, 2020. Amnon Ben-Kish, Itay Zimerman, Salman Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In Proceedings of the 1st Conference on Language Modeling (COLM), 2023. Albert Gu and Tri Dao. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the International Conference on Machine Learning, 2024. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 17351780, 1997. doi: 10.1162/neco.1997.9.8.1735. Jingjing Hu, Dan Guo, Zhan Si, Deguang Liu, Yunfeng Diao, Jing Zhang, Jinxing Zhou, and Meng Wang. Mol-mamba: Enhancing molecular representation with structural & electronic insights. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 317325, 2025. Pengfei Hu, Zhenrong Zhang, Jiefeng Ma, Shuhang Liu, Jun Du, and Jian Shu Zhang. Docmamba: state-space model for document processing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2409524103, 2024. doi: 10.1609/AAAI.v39i22.34584. Submitted to ACL/NeurIPS 2024. Zongshu Li, Guibo Zhu, Dongyi Yi, and Jinqiao Wang. Multimodal mamba: versatile multimodal model for seamless integration into diverse downstream tasks. In Proceedings of the 13th International Conference on Computing and Pattern Recognition (ICCPR 24), pp. 303313, 2024. doi: 10.1145/3704323.3704364. Aobo Liang, Xingguo Jiang, Yan Sun, Xiaohou Shi, and Ke Li. Bi-mamba+: Bidirectional mamba for time series forecasting. arXiv preprint arXiv:2404.15772, 2024. 10 Under review as conference paper at ICLR Yuhui Lin, Jiahao Zhang, Siyuan Li, Jimin Xiao, Ding Xu, Wenjun Wu, and Jiaxuan Lu. Event uskt: U-state space model in knowledge transfer for event cameras. arXiv preprint arXiv:2411.15276, 2024. Yuhui Lin, Jiaxuan Lu, Yue Yong, and Jiahao Zhang. Mv-gmn: State space model for multi-view action recognition. arXiv preprint arXiv:2501.13829, 2025. Hanwen Liu, Yifeng Gong, Zuwei Yan, Zeheng Zhuang, and Jiaxuan Lu. Msgm: multi-scale spatiotemporal graph mamba for eeg emotion recognition. arXiv preprint arXiv:2507.15914, 2025. Jiaxuan Lu, Junyan Shi, Yuhui Lin, Fang Yan, Yue Gao, Shaoting Zhang, and Xiaosong Wang. Hypergraph mamba for efficient whole slide image understanding. arXiv preprint arXiv:2505.17457, 2025. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. In Proceedings of the Neural Information Processing Systems (NeurIPS), 2024. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, pp. 2347 2355, 2013. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Claude E. Shannon. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423, 1948. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural InforŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. mation Processing Systems, 2017. Raphael Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. Aotao Wang, Haikuo Shao, Shaobo Ma, and Zhongfeng Wang. Fastmamba: high-speed and efficient mamba accelerator on fpga with accurate quantization. arXiv preprint arXiv:2505.18975, 2025. Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Pavlo Molchanov, Jan Kautz, and Yingyan Celine Lin. Longmamba: Enhancing mambas long context In Proceedings of the International capabilities via training-free receptive field enlargement. Conference on Learning Representations (ICLR 2025), 2025. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems, volume 33, pp. 1728317297, 2020. Xiangyu Zhang, Qiquan Zhang, Hexin Liu, Tianyi Xiao, Xinyuan Qian, Beena Ahmed, Eliathamby Ambikairajah, Haizhou Li, and Julien Epps. Mamba in speech: Towards an alternative to selfattention. IEEE Transactions on Audio, Speech and Language Processing, 2025. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 11 Under review as conference paper at ICLR"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ON THE FORGETTING OF CRITICAL INFORMATION The core of critical-information loss lies in the irreversible loss introduced by compression/approximation (Shannon, 1948). For generic model, by the information-theoretic compression limit (Shannons theorem), the per-layer entropy loss without cross-layer sharing satisfies Accumulated loss leads to the disappearance of key information. Importantly, Transformers and Mamba exhibit fundamentally different loss mechanisms: H(sl) H(ht). (13) A.1.1 CRITICAL-INFORMATION LOSS IN TRANSFORMERS Transformers often rely on low-rank approximations (e.g., Nystrom) or sparse attention to compress information. If critical features do not lie within the projection subspace, the loss is irreversible, which manifests as an unbounded reconstruction error: ˆx2 if projection subspace, (14) where denotes the original critical signal and ˆx its reconstruction; the projection subspace is the low-rank subspace used by the approximation. A.1.2 CRITICAL-INFORMATION LOSS IN MAMBA In Mamba, state compression makes it easy to forget long-range critical information both within and across layers: 1. Intra-layer dependence. Each layers state h(l) depends only on the previous layers h(l1) . After rounds of state decay, early critical information essentially vanishes: h(L) (cid:89) l=1 A(l),τ h(1) t0 (L 10), (15) is the hidden state of layer at step t, h(1) where h(L) t0 denotes the early signal at layer 1 and time t0, τ = t0 is the temporal gap, and A(l),τ is the τ -step transition operator at layer l. 1. Inter-layer dependence. Without effective cross-layer coupling or cross-layer attention, early-layer critical information is almost impossible to retain in deep layers. A.1.3 THEORY OF CROSS-LAYER TRANSMISSION OF CRITICAL INFORMATION Rd denote the hidden state of layer at time t. linearized approximation of the intraLet h(l) layer recursion (keeping the dominant linear operators and treating nonlinearities as residual ε) is = A(l)h(l) h(l) t1 + (l)z(l1) + ε(l) , where A(l) is the temporal recursion operator (state update/compression matrix), z(l1) from the previous layer, and ε(l) is residual term. (16) is the input In the no cross-layer interaction configuration of vanilla Mamba, the coupling term is negligible (U (l) 0). Fixing the time span τ = t0 and tracing the influence of an early input xt0 on later state h(L) , we expand over time and depth (ignoring intermediate-input contributions): t (A(l))τ h(l) h(l) t0 + τ (cid:88) (A(l))τ s(cid:16) s=1 (l)z(l1) t0+s + ε(l) t0+s (cid:17) . (17) Under review as conference paper at ICLR 2026 When (l) 0, the contribution from early layers is only preserved by (A(l))τ h(l) depth L, the contribution of xt0 is approximated as t0 . Stacking to (A(L))τ (cid:16) h(L) (A(L1))τ (A(1))τ h(1) t0 (cid:17) + (other inputs/residuals). (18) By submultiplicativity of matrix norms (let := max1lL A(l)), the early-information contribution to h(L) is upper bounded by (cid:12) (cid:12)contrib(xt0! !h(L) (cid:12) (cid:12) (cid:12) ALτ h(1) (cid:12) t0 . ) (19) Hence, if < 1, information decays exponentially in both τ and L; even if A! !1 but each A(l) is low-rank/projective, components orthogonal to the projection subspace are irretrievably discarded. A.2 DETAILED DERIVATIONS FOR HORIZONTAL/VERTICAL MEMORY FIDELITY A.2.1 EXPECTED TOKEN MEMORY FIDELITY (ETMF) Focusing on information transmission across tokens within layer, starting from the (single-layer) Mamba update = A(l)h(l) h(l) t1 + (l)x(l) + ε(l) , we quantify token-to-token transmission via the expected cosine similarity: ETMF := Ei,j (cid:2) cos(ti, ˆtj)(cid:3), (20) (21) where ti is the original token representation at time (e.g., embedding E[xi] or the first-layer state h(1) ), and ˆtj is the reconstructed/predicted representation at time j. Low ETMF indicates severe sei mantic distortion of long-range tokens due to exponential decay in the absence of explicit attention. To reduce cost in practice, we adopt self-reconstruction approximation that computes cosine take tj = E[xj] (with possibly tied similarity at the same position (i = j). Concretely: to the output head). From the last-layer state h(L) out, pj = softmax(logitsj/τ ) (temperature τ = 1), and reconstruct ˆtj = (cid:80) pj(v)E[v]. The cosine cos(tj, ˆtj) averaged over sequence/batch yields ETMF. To capture distance-sensitive effects, one may compute ETMF = Ei[cos(ti, ˆti + )] for 8, 16, 32; due to higher cost, we include this as supplementary analysis (see Sec. 5). While self-reconstruction is effective for single-point fidelity, it can underestimate cross-distance loss; future work can tune τ or sample multiple to improve fidelity estimation. , compute logits logitsj = h(L) A.2.2 EXPECTED CROSS-LAYER MEMORY FIDELITY (ECLMF) For cross-layer transmission, using the multi-layer recursion t1 + (l)z(l1) = A(l)h(l) h(l) + ε(l) , (22) with z(l1) the dependence on early layers decays exponentially with depth: the previous-layer output and (l) the inter-layer projection, if (l)! !0 and A(l)! <!1, We therefore define h(L) (cid:16) (cid:89) (A(i))τ (cid:17) h(1) t0 , i=1 h(L) ALτ , h(1) t0 . ECLMFl + := I!(cid:0)h(l); h(l+G)(cid:1) H!(cid:0)h(l)(cid:1) , (23) (24) where I(; ) is mutual information and H() is entropy. Because estimating high-dimensional MI is expensive, we use reconstruction surrogate: (cid:92)ECLMFl + = 1 h(l+G) D(h(l))F h(l)F + ϵ , (25) 13 Under review as conference paper at ICLR 2026 with lightweight decoder (ridge regression by default) and ϵ = 106. In practice: choose gap G! !2, 5, 10; collect Hl, Hl + G! !RBT D; mask paddings and flatten to X, ! !RN D; fit = (X + λI)1X with λ = 104; compute ˆY = XW and = ˆY /(XF + ϵ); the score is 1 r, averaged over and samples to yield ECLMFG. Linear decoding assumes primarily linear cross-layer mappings with noise and correlates empirically with information preservation (e.g., canonical correlations). One can replace it with small MLP decoders or Gaussian MI estimators (see Sec. A) to validate the surrogate; we adopt the linear version for efficiency. ETMF and ECLMF are complementary: ETMF gauges token-level (horizontal) semantic fidelity, whereas ECLMF measures layer-wise (vertical) memory integrity. Together they constitute our horizontalvertical memory framework, explaining Mambas memory decay and extrapolation limits and offering actionable metrics for introducing cross-layer attention and redundancy. Empirical results  (Fig. 5)  confirm that state summarization and cross-layer attention in MemMamba markedly improve both ETMF and ECLMF, mitigating long-range forgetting. A.3 THEORETICAL DERIVATIONS FOR THE MODEL DESIGN MemMamba breaks the complexitymemory trade-off by combining bounded-error state summarization with linear-complexity low-rank cross-layer attention. Theoretically, we show linear time/space complexity O(n), long-range critical-information recall 90%, BIBO stability, and non-vanishing gradients; under equal budgets it outperforms Transformers and SOTA Mamba variants (DeciMamba/LongMamba). Empirical results match these derivations, supporting ultra-long sequence modeling both theoretically and practically. A.3.1 ERROR BOUND OF POOLING APPROXIMATION MemMambas state summarization and cross-layer attention entail Frobenius-normoptimal estimations. For sequence length and window size (m = n/w), define state matrix H! !Rnd, partitioned into blocks Hi, and compute summaries by max pooling: Reconstruction H! !Rnd is obtained by broadcasting within each window: s[i, :] = max 1 wHi[j, :]. with 1w the all-ones vector. The squared Frobenius error is H[(i 1)w + 1 : iw, :] = s[i, :] 1 w, HF 2 = (cid:88) = 1m(cid:12) (cid:12)Hi s[i, :] 1 (cid:12) (cid:12) 2 . (26) (27) (28) Since s[i, :] is the columnwise maximum of Hi, let = max i, j, k(s[i, k] Hi[j, k]) (local fluctuation upper bound, often small for text); then (cid:12) (cid:12)Hi s[i, :] 1 (cid:12) (cid:12) 2 2, (29) and hence Although this scales with controllable, while maxima (key signals) are preserved by design. HF nd, . (30) n, is typically very small; the reconstruction error is bounded and mwd, = A.3.2 EQUAL-BUDGET COMPARISON: MEMMAMBA VS. TRANSFORMER/MAMBA We compare under compute budget (total FLOPs) and memory budget . (1) Equal compute (CMemMamba = CTransformer = C). Transformer complexity: MemMamba complexity: = O(LT n2 dT ); ; nT (cid:114) LT dT . = O(Lonodo); ; no Lodo . 14 (31) (32) Under review as conference paper at ICLR 2026 Figure 5: Comparison of perplexity (PPL) across models at different context lengths. Thus, for the same C, no can be orders of magnitude larger than nT (e.g., C=1012 yields nT 104 vs. no106, i.e., 100 longer contexts for MemMamba). (2) Equal memory (MMemMamba = MTransformer = ). Mamba / MemMamba memory scales linearly: MMamba = O(Lmnmdm), (33) is much higher (e.g., hence no! !nm at fixed , but MemMambas long-range recall RecallMemMamba! 1 δ, δ<0.05) than Mambas (which decays with A). Therefore, under equal memory, MemMamba achieves higher accuracy. MMemMamba = O(Lonodo), A.3.3 BIBO STABILITY OF MEMMAMBA Consider the update h(t+1) = Ah(t) + B(cid:0)x(t+1) + αc(t+1) (34) where ct(t+1) = (cid:80) = 1lαiWvsi with bounded si (si S), and inputs x(t) are bounded. Then h(t+1) A, h(t) + B, + αkS. (35) (cid:1), If < 1, as we get h(t+1) B, + αcs 1 < , (36) establishing BIBO stability (no divergence or pathological decay). A.3.4 CONVERGENCE OF GRADIENT PROPAGATION For the fusion = + αc, the gradients are (37) Since sc is bounded (Softmax derivative 1), we have xL! !αsL! >!0, avoiding the AL vanishing-gradient issue in vanilla Mamba and ensuring optimization convergence. xL = xL, (I + αxc), sL = xL, α, sc. A.3.5 LONG-SEQUENCE RECALL: VANILLA MAMBA VS. MEMMAMBA Let the key feature from steps in the past have strength = γ. Vanilla Mamba: the contribution to the current state is AkBγ, yielding recall RecallMamba AkBγ θ , 15 (38) Under review as conference paper at ICLR 2026 with detection threshold θ. For k=100, RecallMamba < 0.01, indicating severe memory decay. MemMamba: the summarized state si retains si, γ , cross-layer weights αi ρ (correlation ρ! !0.5), and the fused signal satisfies x!!f α(γ ). Thus RecallCSA α(γ ) θ 0.9, (39) e.g., with α=0.8, =0.1γ, θ=0.7γ. Hence RecallMemMamba! !0.9, substantially exceeding Mamba and Transformer. A.4 THE COMPLEXITYMEMORY TRADE-OFF IN SEQUENCE MODELING A.4.1 LINEAR TIME COMPLEXITY MemMamba combines state summarization with attention for long-sequence modeling. For length and dimension d, let = [h1, . . . , hn] and define the summary by max pooling with cost O(nd). The Mamba block updates s[j] = max(H), ht = Aht 1 + Bxt, (40) (41) with cost O(nd) assuming constant state dimension ds (e.g., 32). Cross-layer attention interacts with the state pool via score = softmax(QK/ d), (42) where = Wqx, = Wss, = Wvx, costing O(nd) with constant pool size (e.g., 50). Fusion then gives = xt + α score V, (43) still O(nd). Summing across layers yields O(Lnd); treating L, as constants, the complexity is linear in n. Further, since QK has rank n, it is low-rank matrix. By Nystrom theory, the approximation error obeys (cid:12) (cid:12)QK (cid:92) QK(cid:12) (cid:12)2 σk + 1. (44) Because rank(QK)! !k, we have σk+1 = 0, i.e., zero approximation error in the idealized setting, while the computation O(nkd) is far smaller than Transformers O(n2d). A.4.2 LINEAR SPACE COMPLEXITY We analyze memory usage asymptotically. Let the sequence length be and the model width be C. Transformer: memory grows as O(n2) with sequence length (e.g., attention maps), creating severe bottleneck. Mamba: memory is linear, O(nC). MemMamba: thanks to state summarization, active memory scales effectively as O(nC) with small constants; with constant-size pool, the dominant terms remain linear. Overall space is dominated by inputs O(nd), Mamba states O(nds), and the state pool O(knds) (constant k), yielding total O(nd). The attentions extra memory is O(nd), preserving linearity. In addition, we compare MemMamba with other SOTA Mamba variants and Transformers under different parameter scales. The results show that MemMamba consistently outperforms baselines at the same or even smaller scales, demonstrating superior parameter efficiency. 16 Under review as conference paper at ICLR 2026 Figure 6: Effect of different pooling functions on modeling quality. Figure 7: Impact of state-pool size and window size on PPL. A.5 SENSITIVITY ANALYSIS We assess robustness with respect to key hyperparameters, including window size w, choice/size of the state pool (k), and fusion methods, which directly affect memory fidelity, efficiency, and training stability (cf. bounds in Sec. 3). We report results over broad ranges to identify optimal trade-offs. The results indicate: (i) within wide ranges, window size and state-pool size have negligible effect on performance, implying strong robustness; (ii) among pooling choices, the simple max variant consistently performs best, outperforming mean, T-Max-Avg, and S3Pool. Thus, MemMamba does not require fine-tuning to remain stable, while stronger pooling can further improve local-fidelity if needed. We also compare five fusion methodsgated, residual, elementwise pro-duct, 1D convolution, and weightedacross sequence lengths from 1k to 60k tokens.Residual and weighted fusion show lower PPL at most lengths, indicating better long-range modeling, whereas 1D convolution degrades on very long sequences, likely due to rising computational costs. Detailed results are in Table 4. In summary, MemMamba is robust to most configuration choices. Window size and pool size have little impact across broad ranges; among pooling functions, the simple max choice offers the best fidelityefficiency balance. For fusion, differences are small on short sequences, but residual and weighted fusion dominate on long contexts, while 1D convolution degrades due to complexity. Overall, MemMamba maintains stable performance without heavy tuning; using max pooling and weighted fusion provides the most reliable accuracyefficiencystability trade-off. A.6 IMPLEMENTATION DETAILS All experiments are implemented in PyTorch 2.1.2 and Python 3.10 on Ubuntu 22.04 with CUDA 11.8. Training is conducted on single NVIDIA RTX 4090 (24GB) and 25 vCPUs (Intel Xeon Platinum 8481C). 17 Under review as conference paper at ICLR 2026 Table 4: PPL comparison of fusion methods across context lengths (values adjusted to match means). Fusion Method 1K 2K 4K 10K 20K 30K 40K 50K 60K Gated Residual Elementwise Prod. 1D Convolution Weighted 20.00 19.97 19.89 19.86 19.35 18.88 18.75 18.74 18.72 18.23 18.18 18.15 18.02 18.04 17. 18.36 18.62 18.20 18.29 17.71 18.91 19.17 18.83 18.80 18.26 17.99 18.64 17.56 17.45 17.33 18.19 18.95 18.10 18.01 17.54 18.63 19.31 18.69 18.94 17.98 18.01 19.11 17.49 17.41 17. Table 5: Data split statistics. Split Train Valid Test Books Tokens 28,602 1,973,136,207 50 3,007,061 100 6,966,499 Our MemMamba is 24-layer SSM-based model with cross-layer and cross-token attention modules added to each layer, following prior observations that vanilla Mamba forgets rapidly outside this range. Each state summary vector is compressed to 64 dimensions, and the state-pool size is fixed at 50. The training sequence length is 8k tokens. We train for 100k steps using AdamW (learning rate = 1e4, weight decay = 0.1), with constant LR schedule, gradient accumulation (4 steps), and gradient clipping (max norm = 1). The random seed is set to 123 for reproducibility. For the language modeling experiments (PPL), we use the pg19 corpus with the data split in Table 5. During evaluation, we benchmark nine context lengths (1k, 2k, 4k, 10k, 20k, 30k, 40k, 50k, 60k). Each sample is divided into 10 windows with 50 labels per window. The train and validation set sizes are 500 and 50, respectively, and the maximum training input length is 2k tokens. For documents longer than the training length, we apply random truncation to maintain input compatibility."
        }
    ],
    "affiliations": [
        "Gao Ling Institute of Artificial Intelligence Renmin University of China Beijing, China",
        "School of Statistics Renmin University of China Beijing, China",
        "Shanghai Artificial Intelligence Laboratory Shanghai, China",
        "Shanghai University of Finance and Economics Shanghai, China"
    ]
}