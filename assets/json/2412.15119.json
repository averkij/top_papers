{
    "paper_title": "Parallelized Autoregressive Visual Generation",
    "authors": [
        "Yuqing Wang",
        "Shuhuai Ren",
        "Zhijie Lin",
        "Yujin Han",
        "Haoyuan Guo",
        "Zhenheng Yang",
        "Difan Zou",
        "Jiashi Feng",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive models have emerged as a powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token prediction process. In this paper, we propose a simple yet effective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependencies-tokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop a parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves a 3.6x speedup with comparable quality and up to 9.5x speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https://epiphqny.github.io/PAR-project."
        },
        {
            "title": "Start",
            "content": "Yuqing Wang1 Shuhuai Ren3 Zhijie Lin2 Yujin Han1 Haoyuan Guo2 Zhenheng Yang2 Difan Zou1 Jiashi Feng2 Xihui Liu1* 1University of Hong Kong 2ByteDance Seed 3Peking University 4 2 0 2 9 1 ] . [ 1 9 1 1 5 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Autoregressive models have emerged as powerful approach for visual generation but suffer from slow inference speed due to their sequential token-by-token predicIn this paper, we propose simple yet eftion process. fective approach for parallelized autoregressive visual generation that improves generation efficiency while preserving the advantages of autoregressive modeling. Our key insight is that parallel generation depends on visual token dependenciestokens with weak dependencies can be generated in parallel, while strongly dependent adjacent tokens are difficult to generate together, as their independent sampling may lead to inconsistencies. Based on this observation, we develop parallel generation strategy that generates distant tokens with weak dependencies in parallel while maintaining sequential generation for strongly dependent local tokens. Our approach can be seamlessly integrated into standard autoregressive models without modifying the architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that our method achieves 3.6 speedup with comparable quality and up to 9.5 speedup with minimal quality degradation across both image and video generation tasks. We hope this work will inspire future research in efficient visual generation and unified autoregressive modeling. Project page: https: //epiphqny.github.io/PAR-project. 1. Introduction Autoregressive modeling has achieved remarkable success in language modeling [4, 35, 36, 51, 52], inspiring its application to visual generation [7, 11, 23, 33, 37, 39, 47, 49, 54, 55, 65]. These models show great potential for visual tasks due to their strong scalability and unified modeling capabilities [13, 48, 61]. Current autoregressive visual generation approaches typically rely on sequential token-by-token generation paradigm: visual data is first encoded into token sequences using an autoencoder [11, 56, 67], then an auProject lead. Corresponding author. Figure 1. Comparison of different parallel generation strategies. Both strategies generate initial tokens [1,2,3,4] sequentially then generate multiple tokens in parallel per step, following the order [5a-5d] to [6a-6d] to [7a-7d], etc. (a) Our approach generates weakly dependent tokens across non-local regions in parallel, preserving coherent patterns and local details. (b) The naive method generates strongly dependent tokens within local regions simultaneously, while independent sampling for strongly correlated tokens can cause inconsistent generation and disrupted patterns, such as distorted tiger faces and fragmented zebra stripes. toregressive transformer [57] is trained to predict these tokens following raster scan order [7]. However, this strictly sequential generation process leads to slow generation speed, severely limiting its practical applications [27, 61]. In this work, we aim to develop an efficient autoregressive visual generation approach that improves generation speed while maintaining the generation quality. An intuitive way to improve generation efficiency is to predict multiple tokens in parallel at each step. In language modeling, methods like speculative decoding [6, 24, 26] and Jacobi decoding [21, 43] achieve parallel generation through auxiliary draft models or iterative refinement. In the visual domain, approaches like MaskGIT [5] employ non-autoregressive paradigms with masked modeling strategies, while VAR [49] achieves faster speed through next-scale prediction that requires specially designed multiscale tokenizers and longer token sequences. However, the 1 Figure 2. Visualization comparison of our parallel generation and traditional autoregressive generation (LlamaGen [47]). Our approach (PAR) achieves 3.6-9.5 speedup over LlamaGen with comparable quality, reducing the generation time from 12.41s to 3.46s (PAR-4) and 1.31s (PAR-16) per image. Time measurements are conducted with batch size of 1 on single A100 GPU. introduction of additional models and specialized architectures increases model complexity, and may limit the flexibility of autoregressive models as unified solution across different modalities. In this work, we ask: can we achieve parallel visual generation while maintaining the simplicity and flexibility of standard autoregressive models? We find that parallel generation is closely tied to token dependenciestokens with strong dependencies need sequential generation, while weakly dependent tokens can be generated in parallel. In autoregressive models, each token is generated through sampling (e.g., top-k) to maintain diversity. Parallel generation requires independent sampling of multiple tokens simultaneously, but the joint distribution of highly dependent tokens cannot be factorized for independent sampling, leading to inconsistent predictions, as demonstrated by the distorted local patterns in Fig. 1 (b). For visual data, such dependencies are naturally correlated with spatial distanceswhile locally adjacent tokens exhibit strong dependencies, spatially distant tokens often have weak correlations. This motivates us to reconsider how to organize tokens for generation: by identifying spatially distant tokens with weak correlations, we can group them for simultaneous prediction. Such non-local grouping allows us to maintain sequential generation for strongly dependent local tokens while enabling parallel generation across different spatial regions. Moreover, we observe that initial tokens in each local region play crucial role in establishing the global structure - generating them in parallel could lead to conflicting structures across regions, such as repeated parts in different regions without global coordination(see middle row of Fig. 5). Therefore, the initial tokens in each local region should be generated sequentially to establish the global visual structure. Based on these insights, we propose simple yet effective approach for parallel generation in autoregressive visual models. Our key idea is to identify and group weakly dependent visual tokens for simultaneous prediction while maintaining sequential generation for strongly dependent ones. To achieve this, we first divide the image into local regions and generate their initial tokens sequentially to establish global context, then perform parallel generation by identifying and grouping tokens at corresponding positions across spatially distant regions. The process is illustrated in Fig. 1 (a). Our approach can be seamlessly implemented within standard autoregressive transformers through reordering mechanism, with few learnable token embeddings to facilitate the transition between sequential and parallel generation modes. By ensuring each prediction step has access to all previously generated tokens across regions, we maintain the autoregressive property and preserve global context modeling capabilities. With non-local parallel generation, our approach significantly reduces the number of inference steps and thereby accelerates generation, while 2 maintaining comparable visual quality through careful token dependency handling. We verify the effectiveness of our approach on both image and video generation tasks using ImageNet [9] and UCF-101 [44] datasets. For image generation, our method achieves around 3.9 fewer generation steps and 3.6 actual inference-time speedup with comparable generation quality. With more aggressive parallelization, we achieve around 11.3 reduction in steps and 9.5 actual speedup with minimal quality drop (within 0.7 FID for image and 10 FVD for video). The qualitative comparison of generation results between our method and the baseline is shown in Fig. 2. The experiments demonstrate the effectiveness of our approach across different visual domains and its compatibility with various tokenizers like VQGAN [11] and MAGVIT-v2 [67]. In summary, we propose simple yet effective parallelized autoregressive visual generation approach that carefully handles token dependencies. Our key idea is to identify and group weakly dependent tokens for simultaneous prediction while maintaining sequential generation for strongly dependent ones. Our approach can be seamlessly integrated into standard autoregressive models without architectural modifications. Through extensive experiments with different visual domains and tokenization methods, we demonstrate considerable speedup while preserving generation quality, making autoregressive visual generation more practically usable for real-world applications. 2. Related Work Autoregressive Visual Generation. Autoregressive modeling has been explored in visual generation for years, from early pixel-based approaches [39, 54, 55] to current tokenbased methods. Modern approaches typically follow twostage paradigm: first compressing visual data into compact token sequences through discrete tokenizers [11, 56, 67], then training transformer [57] to predict these tokens autoregressively in raster scan order [23, 37, 65]. This paradigm has been successfully extended to video generation [20, 62, 63], where tokens from different frames are predicted sequentially. However, the strictly sequential generation process leads to slow inference speed that scales with sequence length. Parallel Prediction in Sequential Generation. Various approaches have been proposed to accelerate sequential generation. In language modeling, speculative decoding [6, 24, 26] employs draft model to generate candidate tokens for main model verification, while Jacobi decoding [21, 43] enables parallel generation through iterative refinement. In visual generation, MaskGIT [5] adopts non-autoregressive approach with BERT-like masked modeling strategies, taking different modeling paradigm from traditional autoregressive generation. VAR [49] proposes next-scale prediction that progressively generates tokens at increasing resolutions, though requiring specialized multilevel tokenizers and longer token sequences. In contrast, our approach enables efficient parallel generation while preserving the autoregressive property and model simplicity, readily applicable to various visual tasks without specialized architectures or additional models. 3. Method In this section, we present our approach for parallelized visual autoregressive generation. We first discuss the relationship between token dependencies and parallel generation in Sec. 3.1. Based on these insights, we propose our parallel generation approach in Sec. 3.2. Finally, we present the model architecture and implementation details that realize this process within autoregressive transformers in Sec. 3.3. 3.1. Token Dependencies and Parallel Generation Standard autoregressive models adopt token-by-token sequential generation, which significantly limits generation efficiency. To improve efficiency, we explore the possibility of generating multiple tokens in parallel. However, critical question arises: which tokens can be generated in parallel without compromising generation quality? In this section, we analyze the relationship between token dependencies and parallel generation through pilot studies, providing guidance for designing parallelized autoregressive visual generation models. Pilot Study. In language modeling, researchers have attempted to group adjacent tokens for multi-token prediction [6, 21, 24, 26, 43, 45, 59]. However, our pilot study reveals that directly predicting adjacent tokens leads to significant quality degradation in visual generation (see Fig. 1(b) and Tab. 4 (d)). In autoregressive generation, each token is generated through sampling strategies (e.g., top-k) to maintain diversity. When generating multiple tokens in parallel, these tokens need to be sampled independently. However, for adjacent visual tokens with strong dependencies, their joint distribution cannot be factorized into independent distributions, as each token is heavily influenced by its neighbors. The impact of such independent sampling is clearly demonstrated in the figure, where generating adjacent tokens in parallel leads to inconsistent local structures like distorted tiger faces and fragmented zebra stripes, as tokens are sampled without considering their neighbors decisions. Design Principles. These observations suggest that parallel generation should focus on weakly correlated tokens to minimize the impact of independent sampling. For visual tokens, dependencies naturally decrease with spatial distance - tokens from distant regions typically have weaker correlations than adjacent ones. This motivates us to perform parallel generation across distant regions rather than 3 Figure 3. Illustration of our non-local parallel generation process. Stage 1: sequential generation of initial tokens (1-4) for each region (separated by dotted lines) to establish global structure. Stage 2: parallel generation at aligned positions across different regions (e.g., 5a-5d), then moving to next aligned positions (6a-6d, 7a-7d, etc.) for parallel generation. Same numbers indicate tokens generated in the same step, and letter suffix (a,b,c,d) denotes different regions . within local neighborhoods. However, we find that not all distant tokens can be generated in parallel. The initial tokens of each regions are particularly crucial as they jointly determine the global image structure. Parallel generation of these initial tokens, despite their spatial distances, could lead to conflicting global decisions, resulting in issues like repeated patterns or incoherent patches across regions (see the middle row in Fig. 5). Based on these insights, we propose three key design principles for parallelized autoregressive generation: 1) generate initial tokens for each region sequentially to establish proper global structure; 2) maintain sequential generation within local regions where dependencies are strong; and 3) enable parallel generation across regions where dependencies are weak through proper token organization. 3.2. Non-Local Parallel Generation by sampling from the conditional probability distribution: v(i) 1 P(v(i) 1 v(<i) 1 ), {1, ..., 2}, (2) where v(i) 1 denotes the initial token of the i-th region. Since the number of regions (M 2) is small and fixed, this sequential generation introduces minimal overhead while providing crucial global context for subsequent parallel generation. Stage 2: Parallel Generation of Cross-region Tokens. After initializing tokens for all regions, we proceed with parallel generation of the remaining tokens. As illustrated in Fig.3 (2), at each step, we identify the next position within each region following raster scan order and simultaneously predict tokens at this position across all regions (e.g., tokens 5a-5d are generated in parallel). The parallel generation at each step can be formulated as: Based on the above principles, we propose our approach that enables parallel token prediction while maintaining autoregressive properties. The process is illustrated in Fig. 3. Cross-region Token Grouping. Let {vi}HW denote sequence of visual tokens arranged in grid. We first partition the token grid into regions. Each (cid:16) region contains := tokens. We then group tokens at corresponding positions across different regions. Let v(r) denote the token at position in region r, where {1, ..., 2} and {1, ..., k}. We then organize these tokens into groups based on their corresponding positions across regions: i=1 (cid:17) j 1 (cid:110) ], [v(1) 2 , , v(M 2) 1 , , v(M 2) [v(1) (cid:111) ] . (1) This organization groups together tokens at the same relative position across different regions, facilitating our parallel generation process. , ., v(M 2) ], , [v(1) 2 Stage 1: Sequential Generation of Initial Tokens of Each Region. We first generate one initial token for each region sequentially (marked as 1-4 in Fig. 3) to establish the global context. As shown in Fig. 3 (1), we start with the top-left region and generate the initial token for each region 4 {v(r) }M 2 r=1 P({v(r) }M 2 r=1v<j), (3) }M 2 where {v(r) r=1 represents the set of tokens at position across all regions to be generated in parallel, and v<j includes both initial tokens and tokens from previous parallel steps. For example, with = 2 on 24 24 token grid, after generating 4 initial tokens sequentially, we predict 2 = 4 tokens in parallel at each subsequent step, reducing the total number of generation steps from 576 to 147 (i.e., 4+ 5764 ). While enabling parallel prediction, our approach maintains the autoregressive property as each prediction is still conditioned on all previous tokens. The key difference is that tokens at corresponding positions across regions, which exhibit weak dependencies, are now generated simultaneously instead of sequentially. 4 3.3. Model Architecture Details We illustrate our parallel generation framework using standard autoregressive transformer for class-conditioned image generation. Framework Implementation. As shown in Fig. 4 (a), our model architecture consists of an autoregressive transformer that processes the input sequence and generates visual tokens. The input sequence begins with class token (C) followed by visual tokens to be generated. To achieve Figure 4. Overview of our parallel autoregressive generation framework. (a) Model implementation. The model first generates initial tokens sequentially [1,2,3,4], then uses learnable tokens [M1,M2,M3] to help transition into parallel prediction mode. (b) Comparison of visible context between our parallel prediction approach (left) and traditional single-token prediction (right). The colored cells In traditional AR, when predicting token 6d, the model can access all previous tokens indicate available context during generation. including 6a 6c. Without full attention, our parallel approach would limit each token (e.g., 6b) to only see tokens up to the same position in the previous group (e.g., up to 5b). We enable group-wise full attention to allow access to the entire previous group. n-token parallel prediction, we design special sequence structure with three distinct parts: 1) initial sequential tokens [1,2,...,n] that are generated one at time, 2) transition part with 1 learnable tokens [M1,M2,M3] that helps the model enter parallel prediction mode, and 3) subsequent token groups that are predicted tokens at time (e.g., [5a, 5b, 5c, 5d], [6a, 6b, 6c, 6d]). For predicting each group, the model takes all previous tokens as input while maintaining fixed offset of tokens between input and target sequences. The learnable tokens share the same dimension as regular tokens for seamless integration. To maintain spatial relationships under our reordered sequence, we employ 2D Rotary Position Embedding (RoPE) [46], which preserves each tokens original spatial position information regardless of its sequence position. The above designs enable parallel prediction while preserving the standard autoregressive transformer architecture. Group-wise Bi-directional Attention with Global Autoregression. Our framework combines sequential generation of initial tokens with parallel generation of subsequent token groups. As illustrated in Fig.4 (b), in traditional autoregressive models, when predicting token 6d, the model can access all previous tokens including 6a 6c. However, naive parallel generation with causal masking would restrict each token (e.g., 6b) to only see tokens up to the same position in the previous group (e.g., up to 5b), limiting the available context. To address this limitation while maintaining parallelism, we enable bi-directional attention within each prediction group while preserving causal attention between groups. This allows each token in the current group to access the entire previous group as context (e.g., all tokens [5a 5d] are visible when predicting any token in [6a 6d]). This design enriches the local context for parallel prediction while maintaining the global autoregressive property, ensuring compatibility with standard optimizations like KV-cache. Extension to Video Generation. Our parallel generation"
        },
        {
            "title": "Heads",
            "content": "PAR-L PAR-XL PAR-XXL PAR-3B 343M 775M 1.4B 3.1B 24 36 48 24 1024 1280 1536 3200 16 20 24 32 Table 1. Model sizes and architecture configurations of PAR. The configurations are following previous works [32, 36, 47, 51]. framework can be naturally extended to video generation. The tokenization process reduces both spatial and temporal dimensions, resulting in tokens arranged in grid, where each latent frame aggregates information from multiple input frames. We treat these temporally compressed tokens similarly to image tokens and apply our parallel generation strategy along the spatial dimensions, with the only modification being the use of 3D position embeddings. While we also explored parallel generation along the temporal dimension, we found it less effective than spatial parallelization. This is because temporal dependencies exhibit stronger sequential characteristics that are fundamental to video coherence, making them less suitable for parallel prediction compared to spatial relationships. The exploration of effective temporal parallel strategies remains as future work. 4. Experiments 4.1. Experimental Setup Image Generation. For fair comparison with existing token-by-token autoregressive visual generation methods, we adopt similar settings as [47], using VQGAN tokenizer [11] with 16,384 codebook size and 16 downsampling ratio. Models are trained on ImageNet-1K [9] for 300 epochs, with 384384 images tokenized into 2424 sequences. We evaluate on the ImageNet validation set at 256256 resolution using FID [14] as the primary metric, complemented by IS and Precision/Recall [22]. We ex5 Figure 5. Qualitative comparison of parallel generation strategies. Top: Our method with sequential initial tokens followed by parallel distant token prediction produces high-quality and coherent images. Middle: Direct parallel prediction without sequential initial tokens leads to inconsistent global structures. Bottom: Parallel prediction of adjacent tokens results in distorted local patterns and broken details. Type Model #Para. FID IS Precision Recall Steps Time(s) GAN BigGAN [3] GigaGAN [19] StyleGan-XL [40] ADM [10] CDM [16] LDM-4 [38] DiT-XL/2 [34] MaskGIT [5] VAR-d30 [49] 112M 6.95 569M 3.45 166M 2.30 554M 10.94 4.88 400M 3.60 675M 2.27 224.5 225.5 265.1 101.0 158.7 247.7 278.2 227M 6. 182.1 2B 1.97 334.7 MAR [25] 943M 1. 303.7 VQGAN [11] VQGAN [11] VQGAN-re [11] ViT-VQGAN [64] ViT-VQGAN-re [64] RQTran. [23] RQTran.-re [23] LlamaGen-L [47] LlamaGen-XL [47] LlamaGen-XXL [47] LlamaGen-3B [47] PAR-L-4 PAR-XL-4 PAR-XXL-4 PAR-3B-4 PAR-XXL-16 PAR-3B-16 227M 18.65 15.78 1.4B 5.20 1.4B 4.17 1.7B 3.04 1.7B 7.55 3.8B 3.80 3.8B 343M 3.07 775M 2.62 2.34 1.4B 2.18 3.1B 343M 3.76 775M 2.61 2.35 1.4B 2.29 3.1B 1.4B 3.1B 3.02 2.88 80.4 74.3 280.3 175.1 227.4 134.0 323.7 256.1 244.1 253.9 263. 218.9 259.2 263.2 255.5 270.6 262.5 Diffusion Mask VAR MAR AR AR AR 0.89 0.84 0.78 0.69 0.83 0. 0.81 0.81 0.78 0.83 0.80 0.80 0.81 0.84 0.82 0.82 0.82 0.81 0. 0.38 0.61 0.53 0.63 0.57 0.51 0.61 0.62 0.26 0.52 0.57 0.59 0.58 0.50 0.56 0.57 0.58 0.56 0.56 1 1 1 250 8100 250 250 10 64 256 256 256 1024 1024 256 256 576 576 576 576 147 147 147 147 51 0.08 44.68 11.97 0.13 0.27 28.24 5.05 5.05 6.38 >6.38 >6.38 5.58 5. 12.58 18.66 24.91 12.41 3.38 4.94 6.84 3.46 2.28 1.31 Table 2. Class-conditional image generation on ImageNet 256256 benchmark. or indicate lower or higher values are better. -re means using rejection sampling. PAR-4 and PAR-16 means generating 4 and 16 tokens per step in parallel, respectively. 6 periment with model sizes from 343M to 3.1B parameters (Tab.1), reporting both generation steps and latency time. Video Generation. We evaluate on the UCF-101 [44] dataset using our reproduced MAGVIT-v2 tokenizer [67]. Each 17-frame video (128128 resolution) is compressed by 8 spatially and 4 temporally into 5 16 16 token sequence (1280 tokens per video). For fair comparison, we implement both next-token prediction and our parallel generation approach using the same architecture. The position of video codes is encoded via 3D positional embeddings. Our reproduced MAGVIT-v2 tokenizer uses 64K visual vocabulary instead of the original 262K to facilitate model training. We use Frechet Video Distance (FVD) [53] to evaluate generation quality. Detailed training configurations for both video and image generation are provided in the supplementary material. 4.2. Main Results 4.2.1. Image Generation Tab. 2 presents comprehensive comparisons of classconditional image generation with various state-of-the-art methods, including GAN [3, 19, 40](one-shot generation), Diffusion [10, 16, 34, 38] (iterative denoising), Mask [5] (mask token prediction), VAR [49] (next-scale prediction), MAR [25] (continuous mask token prediction), and AR [11, 47, 64] (autoregressive generation). Our PAR achieves competitive performance while maintaining faster inference speed than most state-of-the-art models. Specifically, when comparing with representative models from different categories, our method shows advantages. Compared with the mask-based method MaskGIT [5], our method achieves substantially better generation quality (FID 2.29 vs. 6.18) despite requiring more steps. For VAR [49], while it achieves slightly better FID (1.97 vs. 2.29), our method maintains simpler framework with fewer tokens per image and preserves the pure autoregressive nature, making it more flexible for multi-modal integration. Compared to our baseline model LlamaGen [47], PAR achieves 3.9 reduction in generation steps (147 vs. 576) and 3.58 speedup in wall-clock time (3.46s vs. 12.41s) while maintaining comparable quality (FID 2.29 vs. 2.18). With more aggressive parallelization, PAR-3B-16 further accelerates generation to 1.31s (9.5 speedup) with only 0.7 FID degradation compared to the baseline, demonstrating the effectiveness of our parallel generation strategy in balancing efficiency and quality. 4.2.2. Video Generation We evaluate our approach on the UCF-101 [44] dataset for class-conditional video generation. Table 3 shows comparisons with various state-of-the-art methods across different categories. Among recent works, MAGVIT-v2 [67] achieves strong performance with an FVD of 58 using Type Method #Param FVD Steps Time(s) Diffusion Mask. AR AR VideoFusion [29] Make-A-Video [41] HPDM-L [42] MAGVIT [66] MAGVIT-v2 [67] CogVideo [17] TATS [12] OmniTokenizer [60] MAGVIT-v2-AR [67] N/A N/A 725M 306M 840M 9.4B 321M 650M 840M 173 81.3 66.3 76 58 626 332 191 PAR-1 PAR-4 PAR-16 94.1 792M 792M 99.5 792M 103.4 - - - - - - - 5120 1280 1280 323 - - - - - - - 336.70 - 43.30 11.27 3.44 Table 3. Comparison of class-conditional video generation methods on UCF-101 benchmark. FVD measures generation quality, where lower values () indicate better performance. PAR1 represents our token-by-token baseline, while PAR-4 and PAR-16 indicate our parallel generation variants with different speedup ratios, achieving competitive FVD scores with significantly reduced generation steps and wall-clock time. masked token prediction, while its autoregressive variant MAGVIT-v2-AR obtains an FVD of 109 with 1280 generation steps. Our next-token-prediction baseline (PAR-1) achieves competitive FVD of 94.1, demonstrating the effectiveness of our implementation. More importantly, our parallel generation variants significantly reduce both generation steps and wall-clock time while maintaining comparable quality. Specifically, PAR-4 reduces the generation steps from 1280 to 323 with minimal FVD increase (99.5 vs. 94.1), achieving 3.8 speedup (11.27s vs. 43.30s). Further parallelization with PAR-16 achieves 12.6 speedup (3.44s vs. 43.30s) with 103.4 FVD, while reducing generation steps to 95. Due to space limit, we provide visualization results of video generation in supplementary materials. 4.3. Ablation Study In this section, we conduct comprehensive ablation studies to investigate the effectiveness of our key design choices on the ImageNet 256256 validation set (Tab. 4). Unless specified, we use the PAR-XL model with parallel group size n=4 as default setting. Initial sequential token generation. We first evaluate the importance of initial sequential token generation by comparing models with and without this phase in Tab. 4 (a). Results show that initial sequential generation reduces FID from 3.67 to 2.61, with only 3 additional steps (147 vs. 144). We also visualize the comparison in Fig. 5. Without initial sequential generation (middle row), the generated images exhibit inconsistent global structures, such as misaligned dogs with duplicated body parts, as initial tokens are generated without awareness of each other. In contrast, our approach with initial sequential generation (top row) produces more coherent and natural-looking images. The results illustrate the importance of initial sequential token 7 FID IS steps w/o 3.67 2.61 221.36 259.17 144 147 (a) Importance of initial sequential token generation. Sequential generation of initial tokens improves FID by 1.06 with negligible step increase. 1 4 16 FID IS steps 2.34 2.35 3. 253.90 263.24 270.57 576 147 51 (b) Number of parallel predicted tokens (PAR-XXL). n=1 is the tokenby-token baseline. n=4 reduces steps by 4 with similar FID (2.35 vs. 2.34), while n=16 reduces steps by 11.3 at the cost of 0.67 FID. attn FID IS steps causal full 3.64 2.61 228.08 259.17 147 147 (c) Attention pattern between parallel tokens. Full attention allows complete context access from previous parallel groups (vs. causal attentions limited access), bringing 1.03 FID improvement. order pattern FID IS steps one raster one distant raster multi distant multi 2.62 2.64 5.64 2.61 244.08 262.72 265.46 259.17 576 576 147 147 (d) Comparison of different scan orders under single-token and multitoken prediction. Our region-based distant ordering shows similar performance with raster scan in single-token setting, but significantly outperforms in multi-token prediction (2.61 vs. 5.64 FID)."
        },
        {
            "title": "Params",
            "content": "FID IS steps 343M 3.76 775M 2.61 2.35 1.4B 2.29 3.1B 218.92 259.17 263.24 255.46 147 147 147 (e) Scaling of model size (4 parallel). Generation quality steadily improves with more parameters, from 343M (FID 3.76) to 3.1B (FID 2.29). Table 4. Ablation studies on image generation model designs. generation for establishing proper global structure. Number of parallel predicted tokens. The number of tokens predicted in parallel (n) controls the trade-off between efficiency and quality. As shown in Tab. 4(b), with = 4 (M = 2), our approach reduces generation steps from 576 to 147 while maintaining comparable quality (FID 2.35 vs. 2.34). Further increasing to = 16 (M = 4) achieves more aggressive parallelization with only 51 steps, at the cost of slight quality degradation (FID increase of 0.67). This is consistent with our analysis that tokens from distant regions have weaker dependencies and can be generated in parallel. As shown in Fig. 2, both PAR-4 and PAR-16 preserve visual fidelity while achieving significant speedup (3.46s and 1.31s vs. 12.41s). Impact of attention pattern. To enable effective parallel prediction while preserving rich context modeling, we study different attention patterns between parallel predicted tokens. With = 4 parallel tokens, enabling full attention within groups reduces FID from 3.64 to 2.61 compared to causal attention, as it allows each token to access complete context from previous groups. This supports our design of combining bi-directional attention within groups with autoregressive attention between groups for effective parallel generation. Impact of token ordering and prediction pattern. We compare raster scan and our distant ordering under different prediction settings. As shown in Tab. 4(d), while both achieve comparable quality in single-token prediction (FID 2.62 vs. 2.64), their performance differs significantly with multi-token prediction - raster scan degrades severely (FID 5.64) while our distant ordering maintains quality (FID 2.61). This indicates that the choice of parallel predicted tokens is critical. When using raster scan, adjacent tokens with strong dependencies are forced to generate simultaneously, leading to distorted local patterns as shown in Fig. 5 (bottom row). In contrast, our region-based distant ordering groups weakly correlated tokens for parallel prediction, preserving both local details and global coherence (top row). Model scaling analysis. We study how our parallel prediction approach scales with model size. As shown in Tab. 4 (e), increasing model size from 343M to 3.1B parameters steadily improves generation quality (FID decreases from 3.76 to 2.29). Comparing with sequential generation baseline (LlamaGen) in Tab. 2, while smaller models show noticeable quality gap (343M: FID 3.76 vs. 3.07), larger models achieve comparable performance (775M: 2.61 vs. 2.62; 1.4B: 2.35 vs. 2.34) while reducing generation steps from 576 to 147. This demonstrates that increased model capacity helps effectively mitigate the quality trade-off from parallel prediction, suggesting stronger capability in modeling joint distribution of parallel tokens. 5. Conclusion We propose Parallelized Autoregressive Visual Generation (PAR), simple yet effective approach that enables efficient parallel generation while preserving the advantages of autoregressive modeling. Our key finding is that the feasibility of parallel generation depends on token dependencies - tokens with weak dependencies can be generated in parallel while strongly dependent tokens lead to inconsistent results. Based on this insight, our PAR organizes tokens based on their dependency strengths rather than spatial proximity. The effectiveness of our approach across different visual domains validates this token dependency-based strategy for efficient autoregressive visual generation. We hope our work can inspire future research on visual generation and other sequence prediction tasks."
        },
        {
            "title": "References",
            "content": "[1] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. 4 [2] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018. 4 [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 6, 7 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Sastry, et al. Language models are fewshot learners. In NeurIPS, 2020. 1 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 1, 3, 6, 7 [6] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, JeanBaptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. 1, [7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 16911703, 2020. 1 [8] Thomas Cover. Elements of information theory."
        },
        {
            "title": "John",
            "content": "Wiley & Sons, 1999. 4 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 3, 5, 1, 2, 4 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, pages 87808794, 2021. 6, 7 [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. 1, 3, 5, 6, 7, 4 [12] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In ECCV, pages 102118, 2022. 7 [13] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws arXiv preprint for autoregressive generative modeling. arXiv:2010.14701, 2020. 1 [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. [15] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 1 [16] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):22492281, 2022. 6, 7 [17] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In ICLR, 2022. 7 [18] Edwin Jaynes. Probability theory: The logic of science. Cambridge university press, 2003. 4 [19] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up In CVPR, pages 10124 gans for text-to-image synthesis. 10134, 2023. 6, [20] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 3 [21] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. 1, 3 [22] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. NeurIPS, 32, 2019. 5 [23] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. 1, 3, 6 [24] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In ICML, 2023. 1, 3 [25] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 6, [26] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 1, 3 [27] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 1 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. 1 [29] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modarXiv preprint els for high-quality video generation. arXiv:2303.08320, 2023. 7 [30] XuanLong Nguyen, Martin Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood IEEE Transactions on ratio by convex risk minimization. Information Theory, 56(11):58475861, 2010. 4 [31] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [32] OpenLM-Research. https : / / huggingface . co / openlm - research / open _ llama_3b, 2023."
        },
        {
            "title": "Openllama",
            "content": "3b. [33] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, pages 40554064, 2018. 1 9 [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 6, 7 [35] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 1, 5 [37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821 8831, 2021. 1, 3 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 6, 7 [39] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik Improving the pixelcnn with disKingma. Pixelcnn++: cretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. 1, 3 [40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 6, 7 [41] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2022. [42] Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, and Sergey Tulyakov. Hierarchical patch diffusion models for high-resolution video generation. In CVPR, 2024. 7 [43] Yang Song, Chenlin Meng, Renjie Liao, and Stefano Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In ICML, 2021. 1, 3 [44] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 3, 7, 1 [45] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. 3 [46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [47] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 5, 6, 7 [48] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [49] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 1, 3, 6, 7 [50] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pages 15. IEEE, 2015. 4 [51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 5 [52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [53] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [54] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In NeurIPS, 2016. 1, 3 [55] Aaron Van Den Oord, Nal Kalchbrenner, and Koray In ICML, Kavukcuoglu. Pixel recurrent neural networks. 2016. 1, [56] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. 1, 3 [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 3 [58] Sergio Verdu. α-mutual information. In 2015 Information Theory and Applications Workshop (ITA), pages 16. IEEE, 2015. 4 [59] Chunqi Wang, Ji Zhang, and Haiqing Chen. Semiautoregressive neural machine translation. arXiv preprint arXiv:1808.08583, 2018. 3 [60] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. [61] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [62] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. 3 [63] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 3 [64] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 6, 7 10 [65] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 1, 3 [66] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023. [67] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 1, 3,"
        },
        {
            "title": "Appendix",
            "content": "The supplementary material includes the following additional information: Sec. provides more implementation details for PAR. Sec. provides more visualization results. Sec. provides the analysis of visual token dependencies. A. Implementation details for PAR Image Generation. For image generation, we train our models on the ImageNet-1K [9] training set, consisting of 1,281,167 images across 1,000 object classes. Following the setting in LlamaGen [47], we pre-tokenize the entire training set using their VQGAN [11] tokenizer and enhance data diversity through ten-crop transformation. For inference, we adopt classifier-free guidance [15] to improve generation quality. The detailed training and sampling hyperparameters are listed in Tab. 5. config value training hyper-params optimizer learning rate weight decay optimizer momentum batch size learning rate schedule ending learning rate total epochs warmup epochs precision max grad norm dropout rate attn dropout rate class label dropout rate AdamW [28] 1e-4(L,XL)/2e4(XXL,3B) 5e-2 (0.9, 0.95) 256(L,XL)/ 512(XXL,3B) cosine decay 0 300 15 bfloat16 1.0 0.1 0.1 0.1 sampling hyper-params temperature guidance scale 1.0 1.60 (L) 1.435 (XXL) / 1.345 (3B) / 1.50 (XL) / Table 5. Detailed Hyper-parameters for Image Generation. Video Generation. For video generation, we train our models on the UCF-101 [44] training set, which contains 9.5K training videos spanning 101 action categories. Videos are processed as 8fps random clips and tokenized by our reimplementation of MAGVIT-v2 [67] (as their code is not publicly available), achieving reconstruction FVD score of 32 on UCF-101. For inference, we use classifier-free guidance [15] with top-k sampling to improve generation quality. The detailed training and sampling hyper-parameters are listed in Tab. 6. config value training hyper-params optimizer learning rate weight decay optimizer momentum batch size learning rate schedule ending learning rate total epochs warmup epochs precision max grad norm dropout rate attn dropout rate class label dropout rate AdamW [28] 1e-4 5e-2 (0.9, 0.95) 256 cosine decay 0 3000 150 bfloat16 1.0 0.1 0.1 0.1 sampling hyper-params temperature guidance scale top-k 1.0 1.15 8000 Table 6. Detailed Hyper-parameters for Video Generation. B. More Visualization Results In Fig.6 and Fig.7, we provide additional visualization results of PAR-4 and PAR-16 image generation on ImageNet [9] dataset, respectively. In Fig.8, we provide the visualization results of video generation using our model on the UCF-101[44] dataset. The results are sampled from 128128 resolution videos with 17 frames. As shown in the figure, even with 16 parallelization (PAR-16), our method shows no obvious quality degradation compared to single-token prediction (PAR1), producing smooth motion and stable backgrounds across frames. C. Analysis of Visual Token Dependencies In Sec.3.1, we demonstrated through pilot studies that parallel generation of adjacent tokens leads to quality degradation due to strong dependencies, while tokens from distant regions can be generated simultaneously. In this section, we provide theoretical perspective of conditional entropy 1 Figure 6. Additional image generation results of PAR-4 across different ImageNet [9] categories. Figure 7. Additional image generation results of PAR-16 across different ImageNet [9] categories. 2 Figure 8. Video generation results on UCF-101 [44]. Each row shows sampled frames from 17-frame sequence at 128128 resolution, generated by PAR-1, PAR-4, and PAR-16 respectively across different action categories. 3 to explain this observation and our design. We use conditional entropy to measure the token dependencies quantitatively - lower conditional entropy between tokens indicates stronger dependency, while higher conditional entropy suggests weaker dependency and thus potential for parallel generation. We further validate our PAR design from the perspective of conditional entropy - In AR-based generation, each step predicts conditional distribution of the next tokens given all previous tokens. Higher conditional entropy indicates higher difficulty for the model to predict the next tokens. In this section, we first introduce the estimation of conditional entropy in Sec.C.1, and then validate our proposed approach by analyzing the relationship between token dependencies and spatial distances in Sec. C.2. C.1. Conditional Entropy Estimation Given visual token sequence {v1, v2, ..., vn}, our goal is to estimate the conditional entropy H(vk{vj}j<k) where the token feature vi Rd and {vj}j<k is the set of (all) visual tokens that precede vk in the sequence. This conditional entropy measures the uncertainty of the current token vk given the previously occurring visual tokens, thereby characterizing the dependency between vk and the set {vj}j<k. It is important to emphasize that we do not require the exact value of H(vk{vj}j<k). Instead, we aim to reflect the trends in H(vk{vj}j<k) under different scenarios, such as given different sets of {vj}j<k and considering different positions of vk given the same set of {vj}j<k. In particular, we characterize the relationship between the token vk and the previous ones as the following model vk = ({vj}j<k) + ϵk (4) where vk is the next token we focus on and {vj}j<k is the conditioning token(s), () is deterministic function, and ϵk is the random additive error term. Then the conditional entropy H(vk{vj}j<k) satisfies H(vk{vj}j<k) = H(f ({vj}j<k) + ϵk{vj}j<k) = H(ϵk{vj}j<k), (5) where the second equation holds since () is deterministic function. However, exactly calculating H(ϵk{vj}j<k) is intractable as we cannot access the entire data distribution. To this end, inspired by prior research on bounding techniques for entropy and mutual information estimation [1, 2, 30, 31, 50, 58], we seek their upper bound as proxy for showing the trends of the conditional entropy for different tokens. In particular, we have H(ϵk{vj}j<k) H(ϵk) 1 2 log((2πe)dΣ), (6) where Σ denotes the covariance matrix of ϵk. Notably, the first inequality naturally holds and the second inequality follows from the maximum entropy theory [8, 18], which is achievable when ϵk follows Gaussian distribution. Based on Eq. 6, we can estimate the trend of conditional entropy changes by calculating the determinant of the residual covariance matrix, i.e., Σ. In order to obtain the additive errors ϵ, we consider training parameterized model fθ() to get the function and characterize ϵ as the residual errors. The detailed algorithm is provided in Algorithm 1. Figure 9. Visualization of token conditional entropy maps. Each map shows the conditional entropy of all tokens when conditioned on reference token (blue square). Darker red indicates lower conditional entropy and thus stronger dependency with the reference token. The visualization shows that tokens exhibit strong dependencies with their spatial neighbors and weak dependencies with distant regions. Algorithm 1 Conditional Entropy Estimation Input: 1: m: number of data points 2: {vi,1, vi,2, ..., vi,n}m each vi,j Rd i=1: visual token sequences, where Xi {vi,j}j<k Yi vi,k Append (Xi, Yi) to (X , Y) 3: k: index of the target token 4: fθ: parameterized model Output: Estimated conditional entropy ˆH(vk{vj}j<k) 5: Initialize empty lists and 6: for = 1 to do 7: 8: 9: 10: end for 11: Train model fθ to estimate given using (X , Y) 12: Initialize empty list for residuals 13: for (X, ) in (X , Y) do 14: 15: 16: 17: end for 18: Compute residual covariance matrix ˆΣ of Ek 19: ˆH(vk{vj}j<k) 1 20: return ˆH(vk{vj}j<k) Ypred fθ(X) ϵk Ypred Append ϵk to Ek 2 log((2πe)d ˆΣ) C.2. Entropy Analysis on ImageNet Data and PAR Based on the conditional entropy estimation method introduced above, we conduct experiments on ImageNet to analyze token dependencies and validate our parallel generation strategy. We randomly sample 10,000 images from ImageNet [9] and extract their features using VQGAN [11] By comparing the conditional entropy difference between sequential (one token at time) and parallel generation (predicting multiple tokens simultaneously), we can quantify the increased difficulty introduced by parallel generation at each position. We conduct experiments with 4-token parallel prediction under two ordering strategies: our proposed generation order that first generates the initial four tokens sequentially to establish global structure, then generates tokens from different spatial blocks in parallel, and the raster scan ordering that directly predicts consecutive tokens simultaneously after the initial four tokens. k,r and par For our proposed order, we aim to characterize the entropy increase caused by the parallel generation, when compared to the entirely sequential generation methods. In particular, let v(r) be the token at position in region r, we dek,r by the sets of the previous tokens of v(r) fine seq for sequential and parallel generations (see Fig. 10(a)(b)). Then the conditional entropy of the sequential generation (single-token) and parallel generation (multi-token) are defined as H(v(r) k,r ). We characterize the entropy increase caused by the parallel generation, i.e., k,r ) and H(v(r) par seq H(v(r) par k,r ) H(v(r) seq k,r ). (7) As comparison, we also consider the raster scan order, where the tokens are exactly arranged based on their positions, denoted as v1, v2, . . .. In this setting, given the current token vk, we define seq and par by the sets of the previous tokens of vk for sequential and parallel generations (see Fig. 10(d)(e)). Then, we will also characterize the entropy increase caused by the parallel generation in the raster scan order, i.e., H(vkV par ) H(vkV seq ). (8) The numerical results of (7) and (8) are presented in Fig. 10(c) and (f). It can be seen that both orderings maintain identical conditional entropy for the first four tokens due to the sequential generation. For subsequent tokens, our proposed order leads to significantly smaller conditional entropy increases compared to the raster scan order. This indicates that when switching from sequential to parallel generation, generating tokens from different spatial blocks introduces less prediction difficulty than generating consecutive tokens in raster scan order. The result quantitatively validates our design. Figure 10. Conditional entropy differences between parallel and sequential generation in different orders. (a)(d) show parallel (4 tokens) generation strategies and (b)(e) show sequential generation strategies for our proposed order and raster scan order respectively. Numbers indicate generation step in each order. (c)(f) visualize the conditional entropy increase when switching from sequential to parallel generation for each order, where darker red indicates larger entropy increase and thus higher prediction difficulty. Both orders generate the first four tokens sequentially (shown as white regions in entropy maps). Our proposed order that generates tokens from different spatial blocks in parallel shows smaller entropy increases compared to raster scan order that generates consecutive tokens simultaneously, indicating parallel generation across spatial blocks introduces less prediction difficulty than generating adjacent tokens simultaneously. encoder, followed by vector quantization to obtain continuous features from the codebook. We first analyze the dependencies between tokens at different positions. For each position in the feature map, we calculate the conditional entropy H(vivj) where = j, given the token vj at the j-th position and considering all tokens vi at other positions. It should be noted that Algorithm 1 is not limited to H(vk{vj}j<k) where the given visual tokens {vj} must satisfy < k. This is because any given tokens vj and vi can be considered to satisfy Eq. 4, making the proposed method applicable in calculating H(vivj). Fig. 9 presents the experimental results. We observe that given different token positions vi, the adjacent tokens typically exhibit lower conditional entropy (shown in redder colors). This indicates that the dependencies between adjacent tokens are stronger compared to the dependencies between tokens that are farther apart in position. This observation aligns with the spatial locality in visual data, where nearby regions have stronger correlations than distant ones. Next, we analyze how different token ordering strategies affect the difficulty of parallel generation in Fig. 10. To simulate the prediction difficulty during generation, we compute each token's conditional entropy given all its previous tokens - higher conditional entropy indicates more uncertainty and thus higher prediction difficulty at that position."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Peking University",
        "University of Hong Kong"
    ]
}