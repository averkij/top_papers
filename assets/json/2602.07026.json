{
    "paper_title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
    "authors": [
        "Xiaomin Yu",
        "Yi Xin",
        "Wenjie Zhang",
        "Chonghan Liu",
        "Hanzhen Zhao",
        "Xiaoxing Hu",
        "Xinlei Yu",
        "Ziyue Qiao",
        "Hao Tang",
        "Xue Yang",
        "Xiaobin Hu",
        "Chengwei Qin",
        "Hui Xiong",
        "Yu Qiao",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs."
        },
        {
            "title": "Start",
            "content": "Modality GapDriven Subspace Alignment Training Paradigm For Multimodal Large Language Models Xiaomin Yu1,2, Yi Xin3,4, Wenjie Zhang1, Chonghan Liu5, Hanzhen Zhao2 Xiaoxing Hu6, Xinlei Yu2, Ziyue Qiao7, Hao Tang8, Xue Yang6 Xiaobin Hu2, Chengwei Qin1, Hui Xiong1, Yu Qiao4, Shuicheng YAN2 1HKUST(GZ), 2NUS, 3sh AILab, 4SII, 5UCLA, 6SJTU, 7GBU, 8PKU Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering robust path for the efficient scaling of MLLMs. Date: February 10, 2026 Leader: Xiaomin Yu (yuxm02@gmail.com) Correspondence: Chengwei Qin, Xiaobin Hu Github: https://github.com/Yu-xm/ReVision.git 6 2 0 2 2 ] . [ 1 6 2 0 7 0 . 2 0 6 2 : r"
        },
        {
            "title": "Anisotropic",
            "content": "V"
        },
        {
            "title": "Stable Bias",
            "content": "Subspace U"
        },
        {
            "title": "Visual Representation",
            "content": "Figure 1 Geometric decomposition of the modality gap. We characterize the intrinsic shape of the modality gap within frozen reference frame. Unlike prior isotropic assumptions, we reveal that the gap consists of systematic Stable Bias and Anisotropic Residuals. This precise modeling serves as the theoretical foundation for our statistical alignment strategy."
        },
        {
            "title": "Introduction",
            "content": "Hidden Harmony Is Better Than the Obvious. Heraclitus Multimodal contrastive learning [11, 21, 23, 33, 39] has established itself as the standard paradigm for aligning visual and text representations. However, as shown in Fig.1, despite extensive training on massive image-text pairs, persistent empirical observation remains: For two modalities that express the same underlying semantics, their representations typically do not coincide. Instead, the two modalities inherently occupy distinct, systematically offset regions of the joint representation space. This phenomenon is known as the Modality Gap. [15, 40] While this gap hinders direct cross-modal interchangeability, it implies that if we can precisely bridge this geometric misalignment, we can effectively leverage abundant text data as substitute [36] for expensive image-text data in Multimodal Large Language Models (MLLMs) training. Prior research has largely advanced along two directions, yet both face limitations: ❶ Geometric Correction: These approaches attempt to post-hoc correct this gap via explicit geometric projections [34, 40]. However, most of these works are limited to single, small-scale tasks [12, 17, 19, 22, 24] such as Image Captioning, failing to unlock the true potential of modality gap theory for model scaling. Furthermore, they typically rely on isotropic noise assumptions, failing to account for the complex anisotropic structures in high-dimensional spaces. ❷ Text-only Large-scale Training: These methods [36] leverage modality gap to synthesize pseudo-visual supervision signals from pure text. While promising, existing methods suffer from substantial performance degradation on fine-grained visual tasks, exposing pronounced distributional gap between synthetic text representations and real-world image data. These limitations point to fundamental mismatch: existing assumptions are overly simplified, resulting in lack of precise modeling of the modality gaps geometric shape, which in turn hinders its application in large-scale training. This mismatch naturally motivates two core research questions: ❶ On Shape: Can we move beyond simple mean assumptions to precisely characterize the intrinsic geometric shape of the modality gap within stable reference frame? ❷ On Scale: Can we leverage this precise shape modeling to design scalable training paradigm that substitutes expensive paired data with massive, easily accessible unpaired data, thereby achieving efficient MLLM scaling? To address the first question regarding shape, we conduct the first empirical study that trains contrastive 2 dual-encoder [28] from scratch to precisely track and model the evolution of the modality gap. Based on this microscopic analysis, we propose unified theoretical framework: we no longer treat the modality gap as random fluctuations but mathematically decompose it within frozen reference frame (Rd = ). By explicitly separating the effective task subspace (U ), where gradients concentrate and semantic information resides, from its orthogonal complement (V ), we reveal the dual geometric structure of the modality gap: it comprises not only stable bias component but also residual component characterized by specific second-moment properties. This discovery allows us to transcend simple mean-based descriptions and fully capture the anisotropic distribution of the modality gap across different subspaces. Building on this precise geometric modeling, we further address the second question regarding scale. Our approach highlights perspective different from prior work: while acquiring high-quality paired image-text data [4, 10, 13] is expensive, obtaining massive amounts of unpaired single-modality data is extremely easy. We believe that leveraging such large-scale unpaired data is sufficient to precisely reconstruct the shape of the modality gap via statistical laws, without relying on expensive paired samples. Guided by this insight, we introduce two core contributions: ReAlign. Building on the geometric analysis in Sec. 3, we introduce ReAlign, training-free pre-alignment strategy that maps text representations into the image representation distribution using statistics derived from large-scale unpaired data. ReAlign operates through three-stage procedure. First, Anchor Alignment matches first-order statistics (means). Second, Trace Alignment matches the global variance scale. Finally, Centroid Alignment explicitly rectifies the geometric drift induced by spherical projection. Together, these stages achieve precise cross-modal alignment at the statistical level using only linear transformations and normalization, without requiring any additional training. ReVision. We incorporate ReAlign into two-stage MLLM training pipeline termed ReVision. In the first stage, Modality Substitution Pretraining, the ReAlign operator is used to convert large-scale long-form text into pseudo-visual representations. An adapter is trained on these representations while keeping the LLM backbone frozen, enabling the model to absorb rich world knowledge and visual semantics purely from text data, without relying on costly imagetext pairs. In the second stage, Visual Instruction Tuning, real images are introduced for standard supervised learning to supplement fine-grained visual details that may be lost under purely statistical alignment, thereby refining the models ability to follow complex instructions. Our main contributions are summarized as follows: ❶ Fixed-frame Modality-gap Framework: We establish theoretical framework that decomposes the modality gap into stable biases and anisotropic residuals within fixed reference frame, moving beyond oversimplified isotropic assumptions. ❷ ReAlign Strategy: We introduce training-free statistical alignment strategy that precisely maps text representations into the visual distribution through three-step procedure: Anchor, Trace, and Centroid alignment. ❸ ReVision Paradigm: We propose scalable MLLM training paradigm that leverages ReAlign to effectively substitute expensive paired data with abundant unpaired text for efficient model scaling. ❹ Scaling Validation: Through comprehensive benchmarking, we demonstrate that ReVisions text-only pretraining surpasses traditional baselines trained on large-scale paired image-text data, validating the superior efficiency and scalability of our paradigm."
        },
        {
            "title": "2 The Isotropic Fallacy",
            "content": "The C3 [40] framework established the dominant paradigm for addressing the modality gap, and subsequent state-of-the-art strategies have inherited its assumption, characterizing the gap simply as superposition of centroid shift and random alignment noise. While their centroid correction effectively rectifies the first-order bias, their treatment of the residual gap relies on critical simplification: that the residual fluctuations are isotropic. We argue that this assumption is geometrically flawed. Multimodal contrastive representation distributions are inherently anisotropic, where information is encoded in hierarchical spectral structure rather than uniform sphere. As we demonstrate in Appendix D, imposing an isotropic prior onto this structured manifold induces spectral whitening effect, which dilutes the fine-grained semantic hierarchy and distorts the angular topology. This mismatch suggests that merely adjusting the mean is insufficient; the geometric shape of the noise matters. To address this, we must first rigorously characterize the true geometry of these fluctuations. In the following section, we introduce formal decomposition framework to reveal that the modality gap is driven not by isotropic noise, but by highly structured, direction-dependent biases and residuals."
        },
        {
            "title": "3.1 Modality Gap Decomposition Framework",
            "content": "We analyze the modality gap within fixedreference framework. Using datadriven subspace construction, we explicitly separate bias and residual components. Our objective is to characterize, around fixed reference time, the slow drift that persists late in training together with its secondmoment structure. This perspective has two advantages: ❶ bias and residual terms can be estimated separately; ❷ all theoretical claims reduce to second-moment conditions estimable from finite-sample statistics. To this end, we train dual-encoder model using the InfoNCE loss [20] on large-scale image-text dataset and define fixed reference frame decomposed into principal subspace and its orthogonal complement . Fixed Reference Frame Construction. Let ex(t), ey(t) Rd be unitnormalized embeddings of paired modalities and for the same sample at training step t. For any random vector Z, write µZ := E[Z] and ΣZ := Cov(Z). We fix reference time t0 and construct the effective task subspace from large held-out probe set by computing the empirical covariance: ˆΣ(t0) := Covp (cid:0)ex(t0)(cid:1) + Covp (cid:0)ey(t0)(cid:1), (1) := span{q1, . . . , qr}, (2) where ˆΣ(t0) = QΛQ, qk is the k-th column of Q, and the dimension is determined by an energy threshold. Let: := Rd, (3) be the orthogonal complement. We denote PU and PV as the orthogonal projectors onto and , and keep them fixed for all times t0 during our empirical analysis. BiasResidual Decomposition. For paired example at time t, define the paired difference (modality gap) as (t) := ex(t) ey(t). Under the fixed frame (U, ), we define the projected mean components: and the zeromean residuals: γ(t) := PV E[(t)] V, β(t) := PU E[(t)] U, δ(t) := PU ζ(t) := PV (cid:0)(t) E[(t)](cid:1) U, (cid:0)(t) E[(t)](cid:1) V. 4 (4) (5) (6) (7) Figure 2 Geometric Statistics of the Modality Gap. (a) Geometric Gradient Constraint. The reference leakage ratio (blue) closely tracks the geometric baseline sin θ(Ut, ) (red), confirming that gradients are confined within the evolving task subspace Ut. (b) Passive Bias Evolution. The orthogonal bias component γ(t) exhibits high cosine stability (blue) with only slow cumulative drift (red), indicating passive evolution driven by subspace rotation rather than direct optimization. (c) Semantic Signal Locking (U-side). In the semantic subspace , the condition number κ(ΣU ) (blue) remains extremely high (> 103), showing strong anisotropy. The correlation ρalign (red) rapidly converges to 1, confirming that residual variance is locked to the gradient covariance structure. (d) Orthogonal Noise Decoupling (V-side). In the orthogonal subspace , the residual noise maintains stretched shape (κ > 101, blue). Crucially, the bias vector γ maintains an angle of 90 (red) relative to the principal noise direction, proving that the static bias and dynamic noise are geometrically decoupled and orthogonal. We refer to γ(t) as the Constant Orthogonal Bias (COB) and β(t) as the Principal Modality Bias (PMB). This yields the exact orthogonal decomposition: (t) = PU (t) + PV (t) = β(t) + δ(t) + γ(t) + ζ(t), (8) Temporally, we allow slow variation within short window of τ discrete steps {t0, . . . , t0 + τ 1}. Additionally, we track the instantaneous samemodality span Ut by measuring the largest principal angle θ(Ut, ) and the leakage ratio PV gt/gt of instantaneous gradients gt to test the theoretical claims visualized in subsection 3.23.3."
        },
        {
            "title": "3.2 Bias Terms",
            "content": "We first focus on the distribution characteristics of gradients within the subspace, which directly govern the evolution behavior of bias terms. Subspace Concentration of Gradients. Empirically, we observe key phenomenon: near convergence, the reference leakage ratio PV L(t)/L(t) is extremely small. This indicates that gradients are not uniformly distributed but are highly concentrated near the probe-estimated top-r principal subspace Ut. Contrastive-set Span. This observation aligns with the structural properties of the InfoNCE loss. Consider dot-product similarities acting on unit-normalized embeddings, and let (B) denote the span of embeddings used as positives/negatives at step t. Theoretically, embedding-level gradients are linear combinations of these contrastive-set embeddings and must lie within this span: L(t) (B) . (9) This property explains why gradients constitute task-relevant space, causing the reference leakage ratio to remain low near convergence. Geometric Baseline. To quantify this effect, at each logging step we re-estimate Ut and measure the reference gradient leakage ratio: leakref(t) := PV gt gt , (10) where gt denotes the probe gradient. Fig. 2(a) compares this metric with the geometric baseline sin θ(Ut, ). According to the Geometric Leakage principle, fixing reference projectors PU , PV , for any nonzero vector Ut, its projection onto is strictly constrained by the largest principal angle θ(Ut, ): PV g PV PUt = sin θ(Ut, ), (11) 5 meaning the maximal fraction of vector in Ut that can lie in is sin θ(Ut, ). Passive Drift Mechanism. Fig. 2(a) shows that leakref(t) consistently tracks the scale of sin θ(Ut, ) (note: the systematic excess of leakref(t) over the baseline will be discussed in subsequent sections as evidence for weak extra-geometric coupling term). Since gradients concentrate within Ut, as training approaches convergence, the angle θ(Ut, ) becomes minimal, rendering any direct gradient correction along the direction negligible. Consequently, as shown in Fig. 2(b), the dataset-level mean component γ(t) = PV E[(t)] typically exhibits only slow drift. This evolution is essentially passive byproduct of subspace rotation and normalization effects, rather than being driven by strong, directed optimization updates within the space. PMB. Within the in-subspace component , the PMB β(t) = PU E[(t)] plays distinct role. Subtracting β(t) centers the -component of paired differences, such that: δ(t) := PU (cid:0)(t) E[(t)](cid:1). (12) If β(t) is not removed, mean effects and second moments mix, causing the empirical covariance to conflate the modality-gap mean with residual geometry. The anisotropy of δ(t) is discussed in subsection 3.3. COB Drift. As shown in Fig. 2(b). For the orthogonal component γ(t) (the orthogonal mean in ), we track two metrics: ❶ The relative drift magnitude: drift(t) := γ(t) γ(t0) max{γ(t0), ε} . (13) ❷ The cosine stability cos(cid:0)γ(t), γ(t 1)) computed on the same probe embeddings. Empirical results show that γ(t) exhibits small but non-negligible cumulative drift while maintaining high step-to-step cosine stability. Based on this, we theoretically model γ(t) as slowly varying process where increments are typically small when θ(Ut, ) is small: γ(t + 1) = γ(t) + η(t), (14) where η(t) is empirically small near convergence."
        },
        {
            "title": "3.3 Residual Terms",
            "content": "Having decomposed the modality gap into mean biases, we now examine the second-moment geometric properties of the zero-mean residuals δ(t) and ζ(t) . While standard analyses often simplify residuals as isotropic white noise, finite-sample statistics reveal that these fluctuations exhibit extreme anisotropy and stable geometric structure throughout the training process. The noise shape is critical: when vectors are normalized and measured by cosine similarity, the angular distribution of an ellipsoidal distribution on the unit hypersphere undergoes systematic deformation. This creates dominant directions on the spherical manifold, rendering mean shift alone insufficient to restore comparable geometric scales. We characterize this geometry via the window-averaged covariance matrices: We track their condition number κ(Σ) and trace tr(Σ) as fundamental statistics representing shape and scale. ΣU := Cov(δ(t)), ΣV := Cov(ζ(t)). (15) 3.3.1 U-SIDE: SEMANTICALLY-COUPLED Anisotropy and Signal Locking. In the principal subspace , the evolution of the residual structure presents two key phenomena, as illustrated in Fig. 2(c): ❶ Extreme Anisotropy: κ(ΣU ) remains at uniformly high level (> 103) throughout training, indicating that the variance does not diffuse spherically but concentrates 6 on low-dimensional subspace spanned by few principal directions. ❷ Signal Locking: To quantify the alignment with the task feature structure, we define: ρalign(t) := corr[λ(ΣU (t)), λ(GU (t))], (16) where GU (t) denotes the gradient covariance restricted to , and λ() denotes the vector of eigenvalues arranged in descending order. We observe that ρalign(t) rapidly converges to near-unity (> 0.98) early in training and remains stable. Signal-Dependent Variance. These results support mechanism where residual fluctuations are coupled with the semantic signal: embedding vectors fluctuate most intensely along the semantic axes that the model is primarily optimizing. Consequently, ΣU encodes the geometric weight of semantic importance. This implies that any cross-modal alignment strategy must account for the scale differences in across modalities, rather than treating them as identically distributed perturbations on uniform hypersphere."
        },
        {
            "title": "3.3.2 V-SIDE: ORTHOGONAL ANISOTROPY",
            "content": "In the orthogonal complement , the residual ζ(t) represents fluctuations outside the task-spanned space. Although its total variance tr(ΣV ) is significantly lower than that of the U-side, its shape subtly but decisively affects the normalized angular distribution. Non-Isotropic Background. We model ζ(t) as generalized elliptical distribution: ζ(t) EV (0, ΣV , g), (17) utilizing robust shape estimation to track the anisotropy intensity κ(ΣV ). As shown in Fig. 2(d), contrary to the assumption of isotropic background noise (κ 1), ΣV maintains highly stretched configuration (κ > 101). Orthogonality and Decoupling. Crucially, Fig. 2(d) reveals striking geometric property: the angle between the constant bias γ(t) and the principal direction of the residual noise ΣV consistently hovers around 90. This indicates that the static drift (bias) and the dynamic fluctuation are geometrically decoupled. Phantom Drift. This orthogonality invalidates simple alignment strategies. If the noise were aligned with the bias, correcting the bias might partially mitigate the noise effect. However, since they are orthogonal, subtracting the centroid γ(t) leaves the highly anisotropic noise structure ΣV untouched in the orthogonal direction. Upon spherical projection, this uncorrected anisotropic noise creates spurious angular concentration, distorting the angular distribution of the hypersphere and producing false directionality. We term this spurious, shape-induced directionality Phantom Drift. This finding theoretically justifies our Trace Alignment in subsection 4.2, which is designed specifically to reshape this anisotropic variance, independent of the mean correction."
        },
        {
            "title": "4 ReAlign:Training-Free Modality Alignment",
            "content": "To address the isotropic fallacy identified in Sec. 2 and building upon the modality gap decomposition in Subsec. 3.23.3, we propose ReAlign, training-free, geometry-preserving three-stage alignment strategy. Unlike isotropic perturbations that corrupt intrinsic data structures, ReAlign aims to affinely map the representations of the source modality to the statistical distribution of the target modality within the shared embedding space. The method comprises three closed-form steps: ❶ anchor alignment to eliminate first-order bias; ❷ scale alignment to match global energy while preserving spectral structure; and ❸ centroid alignment to rectify the non-linear drift induced by spherical projection."
        },
        {
            "title": "4.1 Step 1: Anchor Alignment",
            "content": "We first address the first-order distributional shift caused by the mean difference between ey and ex. Let ey, ex Rd be unit-normalized embeddings for two modalities and x, with population means: µy := E[ey], µx := E[ex]. (18) 7 Source (µy) V Target (µx) Step 1 Anchor Aligned Step 2 Centroid µ Drift Scale U Step 3 Phantom Drift Correction Unit Sphere Aligned (ˆey ) Gap 0 Final Norm Match Center Match ey = (ey µy) + µx ey = µx + s(ey µy) = e µ + µx ˆey = /e (a) Original State (b) Step1: Anchor Alignment (c) Step2: Trace Alignment (d) Step3: Centroid Align (e) Final State Figure 3 The ReAlign Pipeline. (a) Original State. The Source modality (y) and Target modality (x) exhibit discrepancies in both mean centroids and global trace. (b) Step 1: Anchor Alignment. The source is centered and shifted to the target anchor µx to eliminate first-order bias. (c) Step 2: Trace Alignment. Embeddings are scaled to match the target trace Tx via linear affine transformation. Note that the subsequent spherical projection induces µ +µx), non-linear centroid drift µ. (d) Step 3: Centroid Alignment. An explicit correction rectifies this drift (e realigning the mass center to the stable reference. (e) Final Output. Final re-normalization yields ˆey, ensuring precise distribution alignment on the unit manifold. = To map modality into the reference frame of modality x, we first center the source embeddings and then shift them to the target anchor: ey = (ey µy) + µx. (19)"
        },
        {
            "title": "4.2 Step 2: Trace Alignment",
            "content": "Next, we adjust the scale of the residuals, corresponding to the residual energy analyzed in subsection 3.3. We define the global trace of the centered embeddings as: We compute global scaling factor that ratios the target trace to the source trace: Ty := E[ey µy2], Tx := E[ex µx2]. (cid:115) = Tx Ty + ϵ . Combining Step 1 and Step 2, we obtain the affine-transformed embeddings ey: ey = µx + s(ey µy). (20) (21) (22) Geometry-Preserving Property. Importantly, operation (22) is linear affine transformation. Unlike noise injection, scalar multiplication by strictly preserves the covariance structure and anisotropy of the source modality. This implies that the semantic hierarchy of the data is preserved, thereby avoiding the geometric distortion described in section 2. At this stage, we apply the first spherical projection: = ey/ey."
        },
        {
            "title": "4.3 Step 3: Centroid Alignment\nWhile Steps 1 and 2 perfectly align statistics in Euclidean space Rd, the non-linear projection onto the unit\nhypersphere inevitably distorts the distribution, causing the actual mass center to Phantom Drift.",
            "content": "To correct this secondary deviation on the manifold, we perform final centroid alignment. Let µ := E[e y] be the drifted centroid after the first normalization. We recalculate the residuals and anchor them back to the target µx: (23) This step explicitly compensates for the spherical drift = µ µx. The final substituted embedding is obtained by re-normalizing: = e µ + µx. ˆey = e , ˆex = ex. 8 (24)"
        },
        {
            "title": "5 ReVision: A Scalable Training Paradigm",
            "content": "We introduce ReVision, two-stage training paradigm for MLLMs. Building on the geometry-preserving ReAlign strategy, ReVision synthesizes pseudo-modality features from large-scale unpaired long-form text corpora. This design enables low-cost semantic injection, allowing the model to absorb extensive world knowledge during pretraining without relying on expensive, high-quality paired data. The overall process consists of two stages: Modality Substitution Pretraining and Vision Instruction Tuning."
        },
        {
            "title": "5.1 Stage 1: Modality Substitution Pretraining",
            "content": "Let Eimg and Etext denote the frozen image and text encoders. Freed from the reliance on paired visual-text data, we leverage Etext to encode large-scale unpaired text corpora. Unlike traditional multimodal datasets limited by data scarcity, this strategy allows training to scale to vast raw text resources. As result, the model receives dense semantic supervision and can absorb substantially broader world knowledge during pretraining than standard approaches. We define training-free modality substitution operator Syx based on Section 4. Given text sample y, this operator maps its embedding into the image space distribution: Here, ex serves as pseudo-visual embedding. Owing to the geometry-preserving nature of ReAlign, it preserves the rich semantics of while strictly adhering to the anisotropic geometric statistics of real images. ex = Syx(Etext(y)). (25) We train the adapter ϕ (with LLM θ frozen) to reconstruct the text conditioned on the pseudo-visual embedding ex: Lpre(ϕ) = log pθ(yt y<t, Tϕ(ex)). (26) (cid:88)"
        },
        {
            "title": "5.2 Stage 2: Visual Instruction Tuning",
            "content": "t=1 While Stage 1 establishes geometric compatibility, Stage 2 focuses on enhancing capabilities in more challenging scenarios. In this stage, real image embeddings ex = Eimg(x) are introduced. Real images provide fine-grained visual details that may be abstracted away under purely statistical alignment, and are essential for handling complex instructions and intricate reasoning tasks. We fine-tune the model using standard supervised instruction tuning: Lsft(θ, ϕ) = (cid:88) t=1 log pθ(rt r<t, I, Tϕ(ex)), (27) where (I, r) denotes an imageinstruction pair."
        },
        {
            "title": "5.3 Inference",
            "content": "During inference, the model directly takes real images as input. This inherent compatibility stems from the asymmetric alignment strategy. By aligning the text representation distribution to the image representation distribution during pretraining, the model supports single-image inference without relying on statistics from multiple images for calibration, incurring no additional computational overhead."
        },
        {
            "title": "6 Experiments",
            "content": "In this section, we systematically investigate the effectiveness of ReVision under the proposed geometrypreserving framework. Our experiments are designed to address three core research questions (RQs). To ensure fair comparison, all methods employ the same model architecture. We use LLM2CLIP-Openai-L-14336 [11] as the encoder and Llama-3-8B-Instruct [7] as the LLM backbone. For ReVision training, we use bunny-pretrain [10] during the Modality Substitution Pretraining stage and InternVL-Chat-V1-2-SFT [6] for the Visual Instruction Tuning stage. 9 Figure 4 We measure the modality gap between aligned centroids on Bunny and DenseFusion. While the baseline C3 stagnates at geometric bottleneck ( 0.0023) due to isotropic assumptions, ReAlign reduces the gap to the 104 scale by effectively modeling anisotropic covariance. Table 1 Performance comparison of different geometric alignment strategies. Blind represents text-only question inputs without an image. Method General Reasoning Hallucination Avg. MME MMStar SQA RealWorldQA MMMU MMMU-P VisuLogic LogicVista CRPE POPE HallBench Blind 3.37 W/o. Align 73.63 C3 Align 76.16 ReVision 79.65 8.80 35.73 34.60 36. 6.17 75.23 75.52 76.71 5.36 43.53 43.14 47.97 19.60 28.82 30.69 31.51 12.44 25.38 27.20 28.39 0.30 24.40 25.50 27.70 1.56 21.03 19.91 22. 0.60 12.90 71.59 80.82 79.99 72.43 81.78 72.53 15.25 42.38 43.53 46.58 7.85 47.50 48.06 51.16 RQ1: Does preserving anisotropy reduce the modality gap more effectively than isotropic corruption? We first verify the geometric alignment quality by quantifying the Euclidean distance between the centroids of the aligned modalities. We performed alignment on 100k samples from the Bunny-pretrain [10] and DenseFusion [13] datasets. As illustrated in Fig. 4, the original representations exhibit substantial gap ( 0.4). While C3 [40] reduces the gap significantly via centroid subtraction, its reliance on isotropic noise limits the precision of the fit. The residual misalignment suggests that spherical noise cannot perfectly cover the anisotropic visual manifold. By explicitly modeling the covariance structure and correcting for manifold drift, ReAlign reduces the gap by orders of magnitude. On Bunny, the gap drops to 2.64 104, and on DenseFusion to 1.39 104. Notably, we observe that while the initial gaps vary across datasets (0.3918 for Bunny, 0.4276 for DenseFusion), C3 stagnates at similar level ( 0.0023) for both. This performance plateau indicates geometric bottleneck: isotropic noise lacks the flexibility to adapt to the representation variations of different distributions. In contrast, ReAlign breaks this bottleneck by adaptively matching the covariance trace of each dataset, demonstrating superior adaptability to the intrinsic geometric structure of diverse data sources. RQ2. How effective is ReAlign in large-scale MLLM training scenarios? To ensure fair comparison, all baselines adopt the same two-stage paradigm as ReVision: Stage 1 utilizes 1M text samples from Bunnypretrain, followed by Stage 2 using the InternVL-Chat-V1-2-SFT. We compare ReVision against: ❶ Blind: We directly evaluate Qwen3-235B-A22B-Instruct [32] on text-only questions without providing image inputs. This demonstrates that without visual perception, even the most powerful LLMs falter on multimodal tasks. ❷ W/o. Align: Utilizing raw text representations. ❸ C3 Align: Adopting the C3 strategy, which performs centroid alignment and injects isotropic Gaussian noise into text representations. Results are shown in the Table. 1. ReVision achieves the highest average score of 51.16, significantly outperforming C3 (48.06). In reasoningintensive benchmarks, ReVision surpasses C3. This supports our point that C3s isotropic noise induces whitening effect that erodes the fine-grained semantic hierarchy essential for complex reasoning. ReAlign preserves the spectral decay, maintaining the structural richness of the features. ReVision demonstrates clear advantage in hallucination metrics (CRPE:81.78, HallBench:46.58). We attribute this to the correction of Phantom Drift. By ensuring the centroid is aligned on the hypersphere, ReVision prevents the projection layer from overfitting to spurious directional biases, enabling more faithful visual grounding. RQ3: Can scaling up low-cost text-only pretraining surpass the performance of expensive paired image-text training? Finally, we investigate whether ReVision can challenge the traditional paradigm that relies on 10 Table 2 Comparison with SOTA methods and paired image-text baselines. The cost indicates the relative expense of data acquisition. Method General Reasoning Hallucination Avg. Cost MME MMStar SQA RealWorldQA MMMU MMMU-P VisuLogic LogicVista CRPE POPE HallBench 68.81 60.24 Unicorn 75.84 72.20 ReVision* w/. Image 76.01 73.59 ReVision-2M 74.94 36.40 76.35 35.13 34.33 35.40 42.35 43.72 44.18 45.23 36.87 30.22 34.80 33. 34.05 27.64 27.70 29.59 26.80 25.70 25.90 26.80 29.53 21.03 23.27 24.38 64.21 42.32 79.59 71.93 80.87 69.13 80.14 72.18 43.01 46.37 47.11 48.26 43.94 48.05 48.91 49. 3.98 0.37 1.00 0.74 expensive paired image-text data. To ensure fairness, we sampled subset of 417k examples from the SFT dataset for this comparison, aligning exactly with the SFT data scale used in Unicorn [36]. We define the Cost metric based on the expense of data synthesis using GPT5 APIs, normalizing the cost of 1M image-text pairs to 1.0. We compare four pre-training settings in the Table. 2: ❶ Unicorn: 1M unpaired text samples, text-only method using mean shift. ❷ ReVision*: 1M unpaired text samples. ❸ w/. Image: 1M ground-truth image-text pairs (upper bound). ❹ ReVision-2M: 2M unpaired text samples. Comparing the text-only methods, ReVision-2M (49.75) dramatically outperforms Unicorn (43.94). Since Unicorn relies on simple mean-shifting, its synthesized features lack the correct geometric shape, leading to poor manifold penetration. ReVision generates pseudo-features that statistically mimic real visual distributions, proving that how you align matters as much as what you align. Additionally, we observe that Unicorn achieves notably high scores on Reasoning benchmarks. This discrepancy stems from the SFT data distribution: the InternVL-Chat-V1-2-SFT used in our controlled experiments focuses primarily on general visual conversation and basic perception, whereas Unicorns original SFT dataset incorporates massive volume of complex reasoning tasks. This aligns with our perspective that unlocking deep reasoning potential relies heavily on complex SFT tasks, suggesting that ReVisions performance could be further elevated with more challenging instruction tuning data. Importantly, ReVision-2M (49.75) surpasses the w/. Image baseline (48.91) trained on 1M real paired samples. Comparing with paired data, ReVision-2M (49.75) outperforms the 1M paired baseline (48.91) at only 74% of the cost (0.74 vs. 1.00). This highlights promising scaling trajectory: continuously scaling up low-cost text data can surpass the performance of expensive paired data with significantly lower overhead. This is non-trivial finding. It suggests that: ❶ Quality via Quantity: The dense world knowledge contained in large-scale text (2M) can compensate for the lack of explicit visual signals, provided the geometric alignment is precise. ❷ Cost-Efficiency: By trading expensive multimodal data for inexpensive text data, ReVision offers scalable path. We achieve superior performance while reducing the data cost by 26% compared to the paired data, achieving superior performance at low data cost."
        },
        {
            "title": "7 Appendix Organization.",
            "content": "Appendix elucidates the structural causes of the modality gap; Appendices and provide theoretical mathematical proofs; Appendix analyzes the limitations of the isotropic assumption; Appendix evaluates the robustness of ReAlign; Appendix analyzes the failure strategy of Blockwise Covariance Alignment; Appendix explores the Long-Caption Paradox; Appendix details experiment settings; Appendix presents qualitative examples; Appendices and review related work and discuss broader impacts, respectively."
        },
        {
            "title": "8 Conclusion",
            "content": "We address the Modality Gap by establishing the Fixed-frame Theory. Moving beyond isotropic assumptions, we decompose the gap into stable biases and anisotropic residuals, revealing it as structured geometric phenomenon rather than random noise. Guided by these insights, we introduce ReAlign, training-free statistical alignment strategy, and ReVision, scalable paradigm leveraging unpaired text as substitute for expensive image-text pairs. Experiments show ReVision significantly mitigates the gap and outperforms baselines, proving that high-quality visual structures can be learned from pure text. This work offers robust, cost-effective pathway for scaling MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Zhenyang Cai, Ke Ji, Xiang Wan, et al. Towards injecting medical visual knowledge into multimodal llms at scale. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 73467370, 2024. [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [7] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [8] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. [9] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [10] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. [11] Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, et al. Llm2clip: Powerful language model unlocks richer visual representation. arXiv preprint arXiv:2411.04997, 2024. [12] Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. Decap: Decoding clip latents for zero-shot captioning via text-only training. arXiv preprint arXiv:2303.03032, 2023. [13] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Lingyu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. Advances in Neural Information Processing Systems, 37:1853518556, 2024. [14] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [15] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:1761217625, 2022. [16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 12 [17] Yang Liu, Xiaomin Yu, Gongyu Zhang, Zhen Zhu, Christos Bergeles, Prokar Dasgupta, Alejandro Granados, and Sebastien Ourselin. Arcsin: Adaptive ranged cosine similarity injected noise for language-driven visual tasks. arXiv preprint arXiv:2402.17298, 2024. [18] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [19] David Nukrai, Ron Mokady, and Amir Globerson. Text-only training for image captioning using noise-injected clip. arXiv preprint arXiv:2211.00575, 2022. [20] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [22] Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier. Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655, 2022. [23] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. [24] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text generation for visualsemantic arithmetic. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1791817928, 2022. [25] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pages 99299939. PMLR, 2020. [26] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In European Conference on Computer Vision, pages 471490. Springer, 2024. [27] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [28] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi Stephen Chen, Xinggang Wang, et al. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2197021980, 2023. [29] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [30] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [31] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [33] Can Yaras, Siyi Chen, Peng Wang, and Qing Qu. Explaining and mitigating the modality gap in contrastive multimodal learning. arXiv preprint arXiv:2412.07909, 2024. [34] Lingjie Yi, Raphael Douady, and Chao Chen. Decipher the modality gap in multimodal contrastive learning: From convergent representations to pairwise alignment. arXiv preprint arXiv:2510.03268, 2025. [35] Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025. 13 [36] Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, Kejian Wu, Zhaoxin Fan, Ziyue Qiao, and Donglin Wang. Unicorn: Text-only data synthesis for vision language model training. arXiv preprint arXiv:2503.22655, 2025. [37] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [38] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513415186, 2025. [39] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [40] Yuhui Zhang, Elaine Sui, and Serena Yeung-Levy. Connect, collapse, corrupt: Learning cross-modal tasks with uni-modal data. arXiv preprint arXiv:2401.08567, 2024."
        },
        {
            "title": "A Modality Gap Phenomenon Essential Causes",
            "content": "This appendix section explains why modality gap exists at all in the setting studied in Section 3. We emphasize structural necessity: the modality gap is not merely consequence of dataset imperfections, but consequence of three architectural constraints that jointly create persistent null channel where the modality gap can survive. Recall. At each training step t, we define the modality gap as the population mean of the paired difference (t) := ex(t) ey(t). Fixing reference frame Rd = , we decompose the gap as originally defined in Eq. (8), where β(t) = PU E[(t)] is the in-subspace mean (PMB), γ(t) = PV E[(t)] is the orthogonal mean (COB), and δ, ζ are zero-mean residuals. Empirically, γ(t) exhibits slow drift late in training (Eq. 14), and the reference gradient leakage into , as defined in Eq. (10), is small and tracks the geometric baseline sin θ(Ut, ) (Eq. 11). Thesis. persistent modality gap requires the following three hard structural conditions. A.1 Necessary condition #1: Dual-Encoder Isolation Structural requirement. The model must consist of two independent encoders for modalities and y, with no interaction before the final embedding space. Concretely, each modality is mapped by its own network producing unit-normalized embeddings ex(t), ey(t) Rd. Necessity. This dual-tower isolation is the structural origin of the modality gap. Because the two encoders have different architectures and inductive biases, their initial embeddings generally occupy different regions. Consequently, the population mean difference E[(t)] is generically nonzero at initialization, yielding nontrivial bias components β(t), γ(t) as defined in Eq. (4) and (5). If early fusion were present, this mismatch would be suppressed in intermediate layers, preventing coherent mean gap from persisting to the final embedding layer. A.2 Necessary condition #2: Enforced spherical topology Structural requirement. There must be an explicit L2-normalization layer at the encoder outputs, e(t) = z(t)/z(t), so embeddings live on unit hypersphere. Necessity. This non-linear projection is the geometric source of Phantom Drift. As analyzed in Sec. 3.2, the orthogonal residuals ζ are highly anisotropic (κ 1). Even if the noise is zero-mean in Euclidean space (E[ζ] = 0), the non-linear normalization induces spurious centroid shift on the manifold. Mathematically, for latent mean µ: (cid:21) (cid:20) µ + ζ µ + ζ = µ µ . (28) This mechanism turns anisotropic second moments (ΣV ) into angular distortions. Without normalization, mean correction would be linear and direct, leaving no purely geometric centroid drift. A.3 Necessary condition #3: Dot-Product Similarity Head Structural requirement. The interaction head must be based on dot-product similarity between unit embeddings, as in the InfoNCE loss, rather than metric that penalizes mismatch in all dimensions. Necessity. Dot-product interaction creates an unconstrained structural subspace . ❶ Span-restricted gradients: As proven in Eq. (9), gradients are linear combinations of contrastive-set embeddings, meaning L(t) (B) . ❷ Geometric suppression into : As training approaches convergence, gradients concentrate within Ut. Any corrective signal along is strictly bounded by the subspace drift (Eq. 11). Because the dot-product loss only backpropagates gradients along the directions spanned by the contrastive batch, the 15 orthogonal component forms null space for direct optimization. Consequently, the orthogonal bias γ(t) is not aggressively optimized away and exhibits the passive, slow drift modeled in Subsec.3.2. A.4 Summary Synthesizing these conditions, we establish that the Modality Gap is an architectural inevitability: ❶ Dualtower encoding initializes the misalignment (β, γ); ❷ L2-normalization distorts anisotropic noise into Phantom Drift; ❸ Dot-product heads, by restricting gradients to the task subspace (Eq. 9), throttle the models ability to correct the orthogonal bias γ. Thus, under standard multimodal contrastive learning settings, the modality gap is not failure of training but structural byproduct that requires explicit geometric rectification."
        },
        {
            "title": "B Proof of Theoretical Claims",
            "content": "B.1 Proof: Contrastive Gradients Lie in the Contrastive-Set Span We consider the InfoNCE objective utilized in Subsec. 3.1 with dot-product similarities acting on unitnormalized embeddings. Setup. At training step t, let minibatch Bt = {(xi, yi)}B ex,i(t) Rd, i=1. Denote the embeddings as: ey,j(t) Rd, (29) subject to ex,i(t) = ey,j(t) = 1. For specific anchor ex,i(t), the one-way InfoNCE loss is defined as: Li(t) := log exp(ex,i(t), ey,i(t)/τ ) j=1 exp(ex,i(t), ey,j(t)/τ ) (cid:80)B . (30) Let the softmax probability weights be: pij(t) := exp(ex,i(t), ey,j(t)/τ ) m=1 exp(ex,i(t), ey,m(t)/τ ) (cid:80)B , where (cid:88) j=1 pij(t) = 1. (31) Lemma B.1 (InfoNCE embedding-gradient is linear combination of the contrastive set). For each i, the gradient with respect to the anchor embedding satisfies: ex,i Li(t) = 1 τ (cid:16) (cid:88) j=1 pij(t) ey,j(t) ey,i(t) (cid:17) span{ey,1(t), . . . , ey,B(t)}. (32) Symmetrically, for each j: ey,j Li(t) = 1 τ (cid:0)pij(t) 1{j = i}(cid:1) ex,i(t) span{ex,i(t)}. Proof. Expanding Eq. (30): Li(t) = 1 τ ex,i(t), ey,i(t) + log (cid:16) (cid:88) j=1 exp(ex,i(t), ey,j(t)/τ ) (cid:17) . (33) (34) Differentiating w.r.t. ex,i(t) in Eq. (34) directly yields Eq. (32). Eq. (33) follows similarly via the chain rule. Corollary B.2 (Consistency with Main Text). Let the contrastive span be (B) (cid:16) := span {ex,i(t)}B i=1 (cid:17) . The total batch loss L(t) = 1 (cid:80)B i=1 Li(t) implies that all embedding-level gradients lie within {ey,j(t)}B (B) j=1 , confirming the claim in Eq. (9) of the main text. 16 B.2 Proof: Geometric Leakage Bound Recall from Subsec. 3.1 that is the fixed reference subspace, := , and Ut is the instantaneous principal subspace. Lemma B.3 (Leakage equals sin θ(Ut, )). Let θ(Ut, ) be the largest principal angle between subspaces Ut and . Then for any vector Ut: sup gUt, g=0 PV g = PV PUt 2 = sin θ(Ut, ). (35) This confirms the geometric baseline presented in Eq. (11). Proof. Since Ut, we have PV = PV PUtg. The operator norm (spectral norm) PV PUt2 is determined by the singular values of the product of projectors. Let and be orthonormal bases for and Ut. By the CS decomposition definition, the singular values of PV PUt correspond to the sines of the principal angles between and Ut. The supremum is given by the maximum singular value, which is sin θ(Ut, ). B.3 Derivation of COB Drift Dynamics We justify the slow drift model for γ(t) defined in Eq. (14). Recall γ(t) := PV E[ex(t) ey(t)]. We model the optimization update as gradient step followed by spherical normalization π(z) = z/z: e(t + 1) = π(e(t) ηg(t)). (36) Proposition B.4 (Projected Mean Increment). Using first-order Taylor expansion for the spherical projection π(), the increment in the orthogonal bias is bounded by: γ(t + 1) γ(t) η E(cid:2)PV gx(t) + PV gy(t)(cid:3) + O(η2). (37) Proof. The Jacobian of π(z) evaluated at unit norm vector e(t) is = e(t)e(t). Thus, the update is approximately: e(t + 1) e(t) η(I e(t)e(t))g(t). (38) Applying the orthogonal projector PV to the difference e(t + 1) e(t) implies that the change is proportional to PV g(t) (since PV e(t) is negligible for task-aligned embeddings). As shown in Appendix B.2, PV g(t) is strictly constrained by sin θ(Ut, ). Near convergence, when θ(Ut, ) 0, the update step magnitude becomes negligible. B.4 Theoretical Basis of Phantom Drift In Subsec. 3.3, we identified that orthogonal residuals ζ(t) are anisotropic. Here, we prove that simply subtracting the mean is insufficient due to the non-linear Phantom Drift effect upon spherical projection. Lemma B.5 (Central Symmetry Implies Zero Mean). If random vector ζ is centrally symmetric with respect to the origin (i.e., ζ d= ζ), then E[ζ/ζ] = 0. Proposition B.6 (Phantom Drift Mechanism). Consider variable = + ζ, where is small bias vector and ζ is zero-mean, anisotropic noise vector. Assuming the regime ζ, the expectation of the projected variable is: (cid:21) (cid:20) + ζ + ζ = (cid:124) (cid:20) ζ ζ (cid:123)(cid:122) = (cid:21) (cid:125) + (cid:124) (cid:20) 1 ζ (cid:18) (cid:123)(cid:122) (cid:19)(cid:21) ζζ ζ2 (cid:125) + O(m2). (39) Remark B.1. The matrix depends entirely on the covariance structure (shape) of ζ. If ζ is isotropic, is scaled identity matrix, preserving the direction of m. However, if ζ is highly anisotropic, as shown in Fig. 2(d), acts as mixing matrix that rotates the effective mean direction. This rotation is the Phantom Drift mentioned in Subsec. 3.3, justifying the centroid correction in Eq. (23) beyond the initial anchor alignment. 17 B.5 Proof of ReAlign Correctness We verify the properties of the ReAlign framework proposed in Sec. 4. Let denote the source modality (text) and denote the target modality (image). B.5.1 Step 1 & 2: Anchor and Trace Alignment Recall the definitions from Subsec. 4.1 and Subsec. 4.2: µy = E[ey], µx = E[ex], Ty = Eey µy2, Tx = Eex µx2. The affine transformation (Eq. 22) is: ey = µx + s(ey µy), where = (cid:115) Tx Ty + ϵ . Lemma B.7 (First and Second Moment Matching in Rd). The transformed embedding ey satisfies: E[ey] = µx, Eey µx2 Tx. (40) (41) (42) Proof. Linearity of expectation gives E[ey] = µx + s(E[ey] µy) = µx. For the second moment: ey µx2 = s2ey µy2. Taking expectations, Eey µx2 = s2Ty Tx. B.5.2 Step 3: Centroid Alignment After the first spherical projection typically µ = µx due to Phantom Drift. ReAlign Step 3 applies Eq. (23): = ey/ey, the mean shifts to µ = E[e y]. As discussed in Appendix B.4, ey = µ + µx. (43) Proposition B.8 (Restoration of Target Centroid). The corrected pre-normalized variable ey satisfies E[ey] = µx. Proof. Direct calculation shows: E[ey] = E[e y] µ + µx = µ µ + µx = µx. B.6 Robust Shape Estimation Details In Subsec. 3.3, we model the residuals using Generalized Elliptical Distribution (Eq. 17). To robustly estimate the shape ΣV ignoring the heavy-tailed radial component, we utilize Tylers M-estimator, which corresponds to the Maximum Likelihood Estimator (MLE) for the Angular Central Gaussian (ACG) distribution. Definition B.1 (ACG Distribution). For d1, the ACG density with shape parameter Σ is: p(v Σ) Σ1/2(vΣ1v)d/2. The fixed-point iteration used to compute the condition number κ(ΣV ) in Fig. 2(d) is: ˆΣk+1 = n (cid:88) i=1 viv ˆΣ1 vi , normalized s.t. tr( ˆΣ) = d. (44) (45) The convergence of this estimator guarantees robust characterization of the anisotropy discussed in the paper. 18 U--V Weak Coupling Analysis In Subsec. 3.2, the purely geometric leakage baseline predicts that for vectors constrained to the instantaneous span Ut, the fraction that can fall into the fixed orthogonal complement is controlled by the principal-angle quantity sin θ(Ut, ) (cf. Eq. 11). Empirically, however, the measured reference leakage of embedding gradients, PV gt/gt, can persistently exceed this geometric baseline near convergence. This suggests the existence of weak but systematic extra-geometric mechanism that transfers in-subspace fluctuations into . C.1 Residual-Level Coupling Model To capture the deviation from the ideal geometric picture, we introduce linear coupling between the -residual δ(t) and the -residual ζ(t) in the fixed reference frame. Specifically, we posit: ζ(t) = ζ0(t) + δ(t), εU , (46) where : summarizes extra-geometric effects, and ζ0(t) EC(0, ΣV ) is an uncorrelated background component. We extend to Rd by defining Lv := LPU for any Rd. Lemma C.1 (Second-moment identities under the coupling model). Under Eq. (46), assuming E[δ(t)] = E[ζ0(t)] = 0 and E[δ(t)ζ0(t)] = 0, the cross-covariance and -side covariance satisfy: E(cid:2)δ(t)ζ(t)(cid:3) = ΣU L, Cov(ζ(t)) = ΣV + LΣU L. Proof. Using ζ(t) = ζ0(t) + Lδ(t) and E[δ] = 0: E[δζ ] = E(cid:2)δ(ζ0 + Lδ)(cid:3) = E[δζ 0 ] (cid:124) (cid:123)(cid:122) (cid:125) =0 +E[δδ]L = ΣU L. Similarly, for the covariance: Cov(ζ) = Cov(ζ0 + Lδ) = Cov(ζ0) + Cov(Lδ) = ΣV + LΣU L, where the cross-terms vanish due to the uncorrelated assumption E[δζ 0 ] = 0. C.2 Estimating by Regression (47) (48) (49) Eq. (47) shows that is identifiable from second moments. The population least-squares map satisfies = E[ζδ]Σ . In practice, we estimate in the reference bases of and using ridge regression for stability. i=1 at step t, let = [δ1, . . . , δn] Rrn and = [ζ1, . . . , ζn] R(dr)n. The Given probe samples {(δi, ζi)}n estimator is: (cid:98)Lλ = ZD(DD + λIr)1. (50) We report the spectral norm (cid:98)Lλ and the explained variance ζδ = 1 (cid:98)LλD2 /Z2 . C.3 From Residual Coupling to Gradient Leakage We interpret as residual-level coupling map. To connect it to the gradient leakage observed in Fig. 2(a), we introduce an explicit assumption. Assumption (Gradient Coupling). For embedding-level gradients gt, we assume the fixed-frame -component admits an approximate linear explanation by the same L: PV gt geom gt + LPU gt, (51) where geom represents the purely geometric projection due to subspace rotation. 19 Theorem C.2 (Geometric versus Extra-Geometric Leakage). Let Ut be the instantaneous span at time and PV the fixed projector onto . Under the Gradient Coupling assumption, the leakage ratio is bounded by: PV g sin θ(Ut, ) (cid:125) (cid:123)(cid:122) geometric baseline (cid:124) + (cid:124)(cid:123)(cid:122)(cid:125) coupling strength . Proof. By the triangle inequality applied to Eq. (51): PV P geom g + LPU g. (52) (53) From Lemma B.2 (Appendix B), the geometric component is strictly bounded by the principal angle: geom sin θ(Ut, )g. For the coupling term, using the definition of the operator norm: LPU LPU Lg (since PU g). Substituting these bounds back and dividing by yields: PV g sin θ(Ut, ) + L. (54) This confirms that the total leakage is the sum of the subspace drift and the residual coupling intensity. Beyond the Isotropic Assumption: Geometric Analysis In this section, we provide comprehensive empirical verification of the alignment quality. While the Modality Gap metric measures the Euclidean distance between centroids, it fails to capture the structural fidelity of the aligned distribution. To address this, we examine the geometric properties from three perspectives. D.1 Spectral Analysis: Preserving Semantic Hierarchy The eigenspectrum of the feature covariance matrix characterizes the intrinsic geometry of the data manifold. Deep representations typically exhibit power-law decay (λk kα), indicating rich semantic hierarchy where variance concentrates in principal directions. To ensure fair comparison, we normalize the global trace of all baselines to match the target modality before computing the spectrum. In Fig. 5(a), we plot the log-scaled singular values and quantify the geometric structure using the Power-Law Exponent (α). Observation. ❶ C3 exhibits significantly elevated tail and flatter slope, with an α value of approximately 1.06, compared to α 1.33 for the original text. This confirms that injecting unstructured Gaussian noise reduces spectral anisotropy, producing whitening effect that effectively dilutes the fine-grained semantic structure. ❷ ReAlign. In contrast, ReAlign maintains decay rate (α 1.33) that perfectly matches the intrinsic geometry of the source text. While the visual modality naturally possesses lower rank (α 1.83), ReAlign achieves alignment without artificially distorting the texts inherent semantic hierarchy. D.2 Angular Distribution: Matching Cosine Topology We verify the angular alignment by computing the Kernel Density Estimate (KDE) of pairwise cosine similarities. We further quantify the distributional discrepancy using the Jensen-Shannon (JS) Divergence. Observation: As shown in Fig. 5(b), the angular topology provides rigorous test of structural preservation. ❶ C3. The noise injection results in severe distributional shift, destroying the semantic relations on the hypersphere. Quantitative results show high JS Divergence of 0.1904. ❷ ReAlign. Our method achieves JS Divergence as low as 0.0067, effectively overlapping with the target prior. This demonstrates that ReAlign restores centroid alignment while preserving the correct angular relationships between samples. Figure 5 Geometric Fidelity Analysis via Spectral and Angular Properties. (a) Semantic Hierarchy: The eigenspectrum analysis reveals that C3 (red line) exhibits flattened slope with an elevated tail (α 1.06), indicating that unstructured noise injection dilutes fine-grained semantic structure. In contrast, ReAlign (blue line) maintains power-law decay (α 1.33) that matches the intrinsic geometry of the source text. (b) Angular Topology Matching: KDE plots of cosine similarities demonstrate that C3 causes severe distributional shift (JS Divergence = 0.1924), destroying angular relationships. ReAlign achieves near-perfect overlap with the target prior (JS Divergence = 0.0066), validating its ability to restore centroid alignment while preserving the topological structure. Figure 6 Global Alignment Visualization via PCA. We visualize the manifold alignment across three settings: (a) Original Text forms distinct cluster separated from the image modality with negligible mixing (0.32%). (b) C3 expands the text distribution via noise but fails to effectively penetrate the visual manifold (1.31%). (c) ReAlign successfully shifts the text distribution into the visual support region, achieving mixing rate of 4.35%. This represents relative improvement of over 3 compared to the C3 baseline, confirming significant manifold penetration. D.3 Representation Visualization We use Principal Component Analysis (PCA) for qualitative visualization of the global alignment. As shown in Fig. 6. ❶ Qualitative Analysis: The Original Text (Left) forms cluster that is distinct and separated from the image modality. The C3 baseline (Middle) expands the text distribution via noise but fails to penetrate the visual manifold. ReAlign (Right) effectively shifts the text distribution into the visual support region. ❷ Quantitative Analysis: We validate this mixing in the high-dimensional space (D = 1280) using the k-NN Mixing Rate with = 20. The unaligned text shows negligible mixing, at only 0.32%. While C3 slightly improves this to 1.31%, ReAlign achieves mixing rate of 4.35%. This represents relative improvement of over 3 compared to the strong C3 baseline, and an improvement of over 10 compared to the unaligned state, confirming significant manifold penetration."
        },
        {
            "title": "E Robustness Analysis",
            "content": "In this section, we systematically evaluate the stability and scalability of ReAlign along three critical dimensions: ❶ sample complexity for estimating alignment statistics, ❷ numerical stability and computational efficiency, 21 Figure 8 (a) Comparison of alignment residuals using Float32 vs. Float64 accumulators. (b) Trends of processing time (O(N )) and peak memory usage (O(1)) across dataset sizes. and ❸ sensitivity to domain shifts. E.1 Data Efficiency & Stability distinct advantage of ReAlign is its computational efficiency: it relies solely on low-order moments derived from unpaired data, avoiding the computational burden of iterative optimization. Here, we quantify the minimum sample size required for stable alignment. We randomly sample unpaired images and texts from the pretraining corpus, with {102, 5 102, 103, 5103, 104, 5104, 105, 5105}. For each , we estimate the ReAlign population means µ and second-order covariance shapes on these subsets. To ensure numerical stability in high dimensions, we apply standard shrinkage regularization to the covariance estimates. We align the embeddings using these estimates and measure the Modality Gap on held-out test set. We report the mean standard deviation over = 5 independent trials. Figure 7 Impact of unpaired sample size on the modality gap for statistical estimation. As illustrated in Fig. 7, the modality gap decreases rapidly with and stabilizes once exceeds approximately 10,000 samples. ❶ Rapid Convergence: Unlike neural network training, which requires millions of gradient steps, moment-based estimation converges quickly via the Law of Large Numbers. Empirically, the performance plateaus after moderate sample size (N 104). ❷ Implication: This confirms that ReAlign can be calibrated to new distributions on the fly with negligible computational cost, eliminating the need for massive paired datasets. E.2 Numerical Stability & Complexity We conduct empirical experiments to validate the robustness and efficiency of our implementation. Accumulating millions of high-dimensional vectors is prone to significant round-off errors. To quantify this, we compared the alignment residual µerr2 (the distance between the computed mean and the true ground truth) using single-precision (Float32) versus double-precision (Float64) accumulators. As shown in Fig. 8(a), utilizing Float32 introduces an irreducible alignment error floor. Specifically, at = 500, 000 samples, the error reaches 9.56 108, which is non-negligible for precise manifold alignment. In contrast, our Float64 implementation maintains precision near machine epsilon ( 1015), ensuring that the geometric alignment is limited only by statistical properties rather than numerical instability. 22 We analyze the time and space complexity of the ReAlign algorithm by varying the dataset size from 100k to 1M samples. ❶ Time Complexity: As illustrated in Fig. 8(b), the processing time scales strictly linearly with data size (O(N )), increasing from 0.05s for 100k samples to 0.57s for 1M samples. ❷ Space Complexity: Crucially, the peak memory usage remains constant at approximately 48.95 MB, regardless of the dataset size (O(1)). This confirms that ReAlign is highly scalable and capable of processing billions of tokens on standard hardware without memory bottlenecks. E.3 Domain Mismatch Sensitivity We believe that applying general-domain statistics to specialized domains violates the specific distributional reality, leading to geometric misalignment. We quantify this effect using cross-domain statistics transfer protocol. We utilize two distinct distributions: ❶ General: BunnyPretrain. ❷ Medical: PubMedVision-Pretrain [3], representing specialized scientific domain. Protocol. Let Dstat denote the source domain for statistical estimation, and Deval denote the target domain for evaluation. We measure the modality gap on held-out data from Deval under four settings: ❶ General General: In-domain baseline. ❷ General Medical: Cross-domain transfer. ❸ Medical Medical: In-domain alignment. ❹ Medical General: Reverse transfer. Fig. 9 shows the results. ❶ Transfer Degradation: Using General statistics to align Medical data results in substantially larger modality gap compared to the in-domain baseline. ❷ Specificity Requirement: Conversely, applying Medical statistics to General data also degrades performance. These results indicate that the covariance structure of the modality gap is domain-dependent. Therefore, domain-specific calibration is strictly necessary for precise geometric alignment. Figure 9 Comparison of modality gap performance under in-domain and cross-domain statistical alignment for General and Medical domains. Failure Strategy Analysis: Blockwise Covariance Alignment We present training-free procedure that maps embeddings from one modality into the low-order statistical structure of another, using only firstand second-order moments estimated from large unpaired samples. All operations are performed in the fixed reference frame of Sec. 3.1, where the embedding space decomposes as Rd = with fixed projectors PU , PV . The procedure consists of three closed-form steps: ❶ Anchor align to remove first-order bias effects; ❷ Geometry align on and via whiteningcoloring; and ❸ Centroid align to rectify centroid shifts induced by the final spherical projection. Throughout, we use: normalize(z) = z2 , (55) and treat expectations/covariances as population quantities approximated by empirical statistics from large unpaired datasets. F.1 Step 1: Anchor Align We first align first-order statistics by translating each modality representation toward shared anchor µ in the shared representation space and then projecting back to the unit sphere. Let ea, eb Rd be unit-normalized embeddings for two modalities and b, with population means: µa := E[ea], µb := E[eb]. (56) 23 In the fixed frame (U, ), these means decompose into in-subspace (PMB) and orthogonal (COB) components. To map modality into the embedding distribution of modality b, we choose the anchor µ = µb and define ea = normalize(ea µa + µb), eb = eb. (57) In this step, we translate ea so that its mean matches µb, then re-project onto the unit sphere. The subsequent Step 2 then matches both modalities to shared target geometry (Σ , Σ ). F.2 Step 2: Geometry Align Next, we match second-order statistics on and separately. All linear transforms are applied in Euclidean space, and the output is then projected back to the unit sphere. After Step 1, we decompose anchored embeddings using the fixed projectors PU , PV and estimate blockwise covariances: ΣU,a = Cov(PU ea), ΣV,a = Cov(PV ea). Analogously, define ΣU,b and ΣV,b for modality b. We set: Σ = ΣU,b, Σ = ΣV,b, (58) (59) so that modality is mapped to match the covariance structure of modality on both and . For symmetric substitution, (Σ ) can be chosen as any shared target geometry. Define the blockwise whiteningcoloring transforms: , Σ TU,a = (Σ )1/2 Σ1/2 U,a , TV,a = (Σ )1/2 Σ1/2 V,a . (60) Using these, construct the substituted embedding by applying the two transforms on their respective blocks: ˆea = normalize (cid:16) TU,aPU ea + TV,aPV ea (cid:17) . (61) F.3 Step 3: Centroid Align Although Step 2 aligns the covariance structure, the final projection onto the unit sphere creates non-linear distortion that inevitably shifts the population centroid away from the target anchor µ. We term this phenomenon phantom drift. To strictly maintain first-order alignment, we explicitly estimate this drift and perform final re-centering step. Let ˆµa = E[ˆea] be the drifted mean of the geometrically aligned embeddings from Step 2. We apply final correction to pull the distribution back to the target anchor: ef inal = normalize(ˆea ˆµa + µ). (62) This ensures that the final substituted embeddings are both geometrically aligned in terms of covariance and accurately centered around the target semantic anchor. F.4 Failure Analysis: Why Fine-Grained Alignment Fails? Although the Blockwise Covariance Alignment strategy theoretically offers stricter geometric match by aligning full second-order moments within subspaces, empirical results indicate that it consistently underperforms the proposed ReAlign method across all benchmarks. We attribute this failure to two primary factors, supported by the quantitative analysis in Fig. 10. Estimation Variance and Numerical Instability. The ReAlign method relies on matching the global trace = tr(Σ). Scalars are statistically robust and converge rapidly. In contrast, Blockwise Alignment requires estimating and inverting full covariance matrices. As illustrated in Fig. 10(a), the empirical covariance spectrum of the text embeddings exhibits rapid decay with high condition number (κ 1.10 103). Computing the whitening transform Σ1/2 on such an ill-conditioned matrix inadvertently amplifies noise in the tail dimensions (where eigenvalues λ 0) by orders of magnitude. This leads to numerical instability and 24 Figure 10 Failure mechanism analysis of Blockwise Covariance Alignment. (a) Spectrum Decay: The high condition number ( 1.1 103) of text embeddings induces numerical instability during covariance inversion, amplifying tail noise. In contrast, ReAlign maintains stability via isotropic scaling. (b) Semantic Preservation: KNN neighborhood overlap rates reveal that Blockwises aggressive whitening causes collapse of the local semantic topology (retaining only 10% overlap), whereas ReAlign effectively preserves 87% of the semantic structure. Table 3 Performance comparison between BC Align and ReAlign. The results demonstrate that ReVision significantly outperforms BC Align across all benchmarks. This confirms that the numerical instability and excessive distortion of the semantic manifold in BC Align lead to degraded model performance. Method General Reasoning Hallucination Avg. MME MMStar SQA RealWorldQA MMMU MMMU-P VisuLogic LogicVista CRPE POPE HallBench BC Align 76.12 ReVision 79. 34.33 36.13 76.42 76.71 45.36 47.97 30.69 31.51 27.95 28.39 24.40 27. 22.60 22.82 80.93 71.08 81.78 72.53 46.27 46.58 48.74 50.16 exploding updates in minor feature directions, whereas ReAligns isotropic scaling remains stable regardless of the spectral shape. Semantic Distortion via Aggressive Whitening. Geometric alignment operates on the assumption that the source and target manifolds share topologically isomorphic structure. ReAlign adopts conservative approach, isotropic scaling via Eq. (24), which preserves the relative angular distances between source embeddings. Blockwise whitening, however, applies an anisotropic rotation and scaling to force the covariance shapes to match exactly. We quantified the impact of this transformation on semantic structure using k-Nearest Neighbor (KNN, = 10) overlap rates. As shown in Fig. 10(b), this over-alignment causes catastrophic collapse of the local semantic topology: Blockwise alignment retains only 10.1% of the original semantic neighborhoods. In sharp contrast, ReAlign preserves 87.3% of the local structure, ensuring that fine-grained semantic relationships remain intact for the LLM. While Blockwise Covariance Alignment is geometrically more rigorous, ReAlign strikes superior trade-off between geometric compatibility and semantic preservation. Its reliance on robust first-order and scalar second-order statistics provides stable initialization that is crucial for scalable MLLM pretraining. The Long-Caption Paradox Intuitively, utilizing longer, denser captions should theoretically provide richer semantic supervision, thereby enhancing cross-modal alignment. This expectation is particularly strong when employing advanced text encoders like LLM2CLIP, which are explicitly engineered to handle long contexts. However, our empirical investigation reveals counter-intuitive phenomenon which we term the Long-Caption Paradox. As demonstrated in Table. 4, the model pretrained on the concise Bunny dataset consistently outperforms the model trained on the linguistically rich DenseFusion dataset. This performance gap persists Table 4 Performance Comparison validating the Long-Caption Paradox. Quantitative results demonstrate that ReVision consistently outperforms ReVision-Long across General, Reasoning, and Hallucination benchmarks. This performance gap confirms that for statistical modality alignment, the geometric compactness and high signal-to-noise ratio of short captions are more critical than the raw length of the textual description. Method General Reasoning Hallucination Avg. MME MMStar SQA RealWorldQA MMMU MMMU-P VisuLogic LogicVista CRPE POPE HallBench ReVision-Long 75.74 79.65 ReVision 33.40 36.13 74.84 76.71 46.41 47.97 30.57 31. 27.26 28.39 24.70 27.70 25.28 22.82 80.42 71.47 81.78 72.53 45.95 46.58 48.73 50. Figure 11 Geometric Analysis of the Long-Caption Paradox. (a) Effective Rank: Measurements reveal that Long captions exhibit high effective rank ( 52.9) similar to the visual modality ( 57.5). This indicates diffuse, high-entropy covariance structure that is difficult to align. In contrast, Short captions ( 41.0) function as compact, low-rank approximation of the visual content, offering greater statistical stability. (c) Initial Modality Gap: The inclusion of non-visual linguistic noise in long captions acts as disturbing force, significantly widening the initial modality gap (µ 0.51) by approximately 30% compared to concise captions (µ 0.39). despite both models utilizing the exact same ReAlign strategy and encoder. We attribute this paradox not to single factor, but to structural constraints verified by our geometric analysis:❶ Representation Capacity, ❷ Diffuse Covariance, and ❸ Signal-to-Noise Ratio. G.1 Truncation-Induced Supervision Mismatch Even with advanced encoders like LLM2CLIP that extend the context window, there remains hard physical limit on the input sequence length. Unlike the visual encoder, which processes the image holistically, the text encoder operates within fixed context window. When dense caption exceeds this limit, the trailing semantic content is inevitably truncated and discarded before encoding. This truncation creates fundamental discrepancy between the modalities. The visual embedding encodes the global visual information, whereas the textual embedding only captures the truncated prefix of the caption. Consequently, the model attempts to align the full image to an incomplete textual description, as the details described in the truncated tail are absent from the feature space. G.2 Diffuse Covariance Structure From the perspective of ReAlign, long captions introduce specific geometric challenges. We quantify this using the effective rank to measure the complexity and compactness of the embedding manifolds. As shown in Fig. 11(a), our measurements reveal that visual embeddings possess high Effective Rank ( 57.5), reflecting their rich detail. Long captions attempt to mimic this complexity, exhibiting similarly high rank ( 52.9). However, this high rank implies diffuse covariance structure, high-dimensional cloud with high entropy. In contrast, Short captions exhibit significantly lower Effective Rank ( 41.0). This suggests that short captions act as low-rank approximation of the visual content, filtering out background noise and retaining only the most structurally salient principal components. Aligning compact manifold to the visual manifold is statistically more stable than aligning diffuse, high-entropy cloud. 26 Figure 12 Visualization of Linguistic Noise in Dense Captions. We highlight non-visual segments (marked in red) within long captions, such as abstract inferences, contextual associations, and subjective interpretations. These tokens lack direct visual grounding and geometrically act as noise vectors, pulling the semantic centroid away from the true visual anchor. G.3 Linguistic Noise & Modality Gap Finally, we distinguish between Linguistic information and Visual Information. larger token count does not equate to higher visual utility. Dense captions often contain non-visual context (The atmosphere was tense) or abstract inferences, as visualized in Fig. 11(c). Geometrically, these tokens act as noise vectors that pull the semantic centroid away from the visual anchor. This is empirically verified by the initial modality gap. As shown in our experiments, the long-caption dataset exhibits significantly larger gap (µ 0.51) compared to the Short-Caption dataset (µ 0.39). Conclusion: The extra linguistic information in long captions effectively acts as disturbing force, increasing the modality gap by 30%. For statistical modality alignment, the visual grounding and geometric compactness of the text distribution are more critical than raw length."
        },
        {
            "title": "H Experiments Setting",
            "content": "H.1 CLIP Pretraining Setting To empirically validate theoretical hypotheses, we established controlled pretraining environment utilizing TinyCLIP architecture comprising 40 million parameter ViT B/32 vision encoder and 19 million parameter Transformer text encoder. The model was trained on 2 million randomly sampled image-text pairs from LAION 400M with 224 by 224 resolution inputs using the AdamW optimizer with weight decay of 0.0001 and cosine learning rate schedule peaking at 0.0001 with warmup ratio of 0.1. The training process was 27 distributed across 8 NVIDIA A6000 GPUs with global batch size of 4096. Distinct from standard protocols, we integrated custom online geometric monitoring system that utilizes dedicated held-out probe set of 200,000 samples to perform high-frequency spectral analysis every 20 steps. This protocol dynamically updates the reference subspace basis until step 800 before freezing it for stable drift measurement and employs 16 accumulated batches to approximate the Fisher Information Matrix for curvature analysis while tracking the top 128 eigen components to verify spectral decay. All linear algebraic computations are executed in Float32 precision to ensure numerical stability against the BF16 training backdrop. H.2 MLLM Training Setting We employ Llama-3-8B-Instruction as the LLM backbone, connected to the input modalities via two-layer MLP projector with GELU activation. Our core design utilizes aligned text representations as pseudo-cisual tokens. These text-derived embeddings are mapped directly into the LLMs feature space through the MLP. Training Pipeline. The training process consists of two stages: ❶ Stage 1: Modality Substitution Pretraining. We train the projector for 1 epoch on the filtered Bunny-1M dataset with learning rate of, while keeping the LLM parameters frozen. ❷ Stage 2: Visual Instruction Tuning. We perform full-parameter fine-tuning on the InternVL-Chat-V1.2 dataset for 1 epoch. The projector is initialized with weights from Stage 1, and the learning rate is reduced to 1e-5. The experiments were conducted on 8 NVIDIA H200 GPUs. With total data scale of approximately 2.2M samples, the complete training pipeline was finished in 12 hours. H.3 Eval Setting We evaluate ReVision across three primary dimensions using comprehensive suite of benchmarks: General Perception (MME test [8], MMStar [5], ScienceQA-image dev&test [18], and RealWorldQA), Complex Reasoning (MMMU validation single-image [37], MMMU-Pro single-image [38], VisuLogic train [31], and LogicVista [30]), and Hallucination Assessment (CRPE [26], POPE [14], and HallusionBench [9]). For all benchmarks, we report accuracy (acc) as the unified metric to ensure consistent and rigorous comparison. H.4 Cost Analysis We quantify the data acquisition cost using the formula Ctotal = Pin Tin + Pout Tout, where Ctotal denotes the total cost, and and represent the token count and price per million tokens, respectively. To ensure standardized comparison, we assume unified pricing based on GPT-5 APIs for all methods, with rates set at Pin = $1.25 and Pout = $10.00 per million tokens. Under this setting, the methods exhibit distinct cost profiles: Unicorn incurs the highest expense of $1893.27 (17.50M input, 187.14M output), whereas the standard ReVision (1M) is the most economical at $176.10 (11.64M input, 16.15M output). Scaling to ReVision-2M brings the cost to $352.20 (23.28M input, 32.30M output), which remains notably lower than the paired w/. Image baseline cost of $476.05 (221.64M input, 16.15M output). For clarity, all costs are normalized relative to the w/. Image baseline (1.0)."
        },
        {
            "title": "I Qualitative Analysis",
            "content": "To intuitively understand the improvements brought by ReVision, we conduct comprehensive evaluation across three distinct cognitive levels: General Visual Perception, Domain-Specific Knowledge, and Logical Reasoning. As visualized in Fig. 13, the model demonstrates exceptional capability in handling complex Abstract and Spatial Reasoning tasks. Specifically, in the matrix pattern completion case, the model moves beyond simple texture matching to identify the underlying geometric progression rules. This capacity for mental simulation is further evidenced in the geometry folding problem, where the model successfully performs mental rotation to reconstruct 3D cube from 2D net, accurately predicting the spatial adjacency of symbols. Beyond abstract logic, the model exhibits robust Fine-Grained Perception and Knowledge Integration. In the domain of data analysis, it accurately interprets the intersection points of two trend lines in statistical chart, extracting precise numerical values rather than merely describing general trends. Furthermore, the model effectively grounds visual signals into specific world knowledge. This is demonstrated in the geospatial recognition scenario, where it combines visual map boundaries with geographical facts to identify the easternmost state, 28 Figure 13 Qualitative analysis examples of ReVision. This figure shows the models visual perception ability across various complex tasks. and in the physical common sense task, where it correctly deduces magnetic attraction based on the orientation of poles."
        },
        {
            "title": "J Related Work",
            "content": "The Modality Gap refers to the systematic distributional distance between representations of paired data from distinct modalities within shared embedding space, despite the theoretical expectation that semantically identical pairs should align. Early empirical research first identified this geometric anomaly as cone effect [15], observing that embedding vectors tend to occupy narrow cone rather than spanning the full hypersphere [25]. The C3 framework [40] provided the first formal description of this structure, characterizing the gap as superposition of constant orthogonal displacement and random alignment noise. While these frameworks offer foundational understanding, they predominantly rely on the assumption that the noise term is isotropic. Our work challenges this simplification, demonstrating that the residual noise exhibits high anisotropy, thereby necessitating more precise second-moment modeling. The absence of this theoretical perspective has directly limited recent explorations into training MLLMs [1, 2, 16, 27, 29, 35] using pure text. Unicorn [36] pioneered 29 the use of the modality gap to convert text representation into the pseudo-visual representation. However, due to their reliance on simple mean shifting, which implies an isotropic assumption, the synthesized representation fails to match the complex geometric shape of the real visual representation. This further highlights the necessity of constructing novel training paradigm based on precise covariance alignment."
        },
        {
            "title": "K Broader Impact",
            "content": "Our work advances the paradigm of Data-Efficient AI, specifically by decoupling the acquisition of visual semantics from the reliance on massive paired datasets. By demonstrating that the heavy lifting of knowledge injection can be offloaded to massive unpaired text during the Pretraining stage, while reserving limited real images solely for the SFT stage, we fundamentally alter the resource landscape for training MLLMs. This paradigm shift has profound implications in several key areas: Democratization of Multimodal Research: The prohibitive cost of collecting and cleaning billion-scale imagetext pairs has historically concentrated MLLM development within few well-resourced institutions. By shifting the data requirement to abundant unpaired text for the bulk of training, our approach significantly lowers the barrier to entry, enabling academic labs and smaller research groups to train high-performance models from scratch. Expansion to Low-Resource Domains: In specialized fields (e.g., medical imaging, minority languages, or technical diagrams), paired data is scarce, yet textual knowledge is often available. Our ReVision paradigm allows models to learn visual concepts primarily through domain-specific text corpora during pretraining, requiring only handful of examples for SFT. This opens new avenues for deploying MLLMs in domains where data scarcity previously made it impossible. Mitigation of Copyright and Privacy Risks: Large-scale web-scraped image datasets often contain copyrighted artwork and sensitive personal identifiable information (PII). By minimizing the reliance on raw images during the data-hungry pretraining phase and relying instead on text, our method offers potential pathway to reduce legal and ethical risks associated with dataset curation."
        }
    ],
    "affiliations": [
        "GBU",
        "HKUST(GZ)",
        "NUS",
        "PKU",
        "SII",
        "SJTU",
        "UCLA",
        "sh AILab"
    ]
}