{
    "paper_title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "authors": [
        "Yufan Deng",
        "Xun Guo",
        "Yuanyang Yin",
        "Jacob Zhiyuan Fang",
        "Yiding Yang",
        "Yizhi Wang",
        "Shenghai Yuan",
        "Angtian Wang",
        "Bo Liu",
        "Haibin Huang",
        "Chongyang Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF"
        },
        {
            "title": "Start",
            "content": "MAGREF: Masked Guidance for Any-Reference Video Generation"
        },
        {
            "title": "ByteDance Intelligent Creation",
            "content": "Project Page: https://magref-video.github.io/magref.github.io/ 5 2 0 2 9 2 ] . [ 1 2 4 7 3 2 . 5 0 5 2 : r Figure 1: We present MAGREF, flexible video generation framework that synthesizes novel videos conditioned on set of reference images and text prompt. MAGREF maintains visual consistency across multiple subjects and diverse scenes while adhering precisely to the given textual instructions. ABSTRACT Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, unified framework for any-reference video generation that introduces masked guidance to enable coherent multisubject video synthesis conditioned on diverse reference images and textual prompt. Specifically, we propose (1) region-aware dynamic masking mechanism that enables single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from singlesubject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and highfidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF."
        },
        {
            "title": "1 INTRODUCTION\nRecent advances in diffusion models [Ho et al. 2020a; Peebles and\nXie 2023; Song et al. 2020] have markedly enhanced the ability to\ngenerate realistic and coherent videos from text prompts or images,\nattracting considerable attention from both academia and indus-\ntry [Blattmann et al. 2023a; OpenAI 2024; Pika 2025; Runway 2025].\nThese models are typically trained on large scale video–text or im-\nage–text datasets, underpin two dominant paradigms in video gen-\neration: text-to-video (T2V) and image-to-video (I2V). T2V models\nleverage language models to encode textual prompts as conditional\ninputs for video synthesis. In contrast, I2V models generate video\nsequences based on a given static image, usually the first frame,\ncombined with corresponding textual descriptions. Although both\nparadigms have demonstrated impressive capabilities in produc-\ning high-fidelity and temporally coherent videos, user demands\nhave steadily evolved beyond simple text-based or single image\nconditioned generation. Increasingly, creators seek to guide video\ngeneration using multiple reference images, enabling fine-grained\ncontrol over visual appearance and identity. This shift has led to",
            "content": "growing interest in multi-subject driven video generation, which aims to integrate diverse visual cues into coherent and personalized video sequence. However, the use of multimodal conditions, combining both images and text, along with an increasing number of reference images, significantly expands the conditioning space in multi-subject driven video generation. This often results in complex interactions between multiple identities, objects, and backgrounds which significantly complicates the model training. Consequently, it becomes extremely challenging to consistently preserve each subjects identity across varying poses and frames, to accurately capture inter subject and subjectscene relationships, and to faithfully adhere to user specified scene layout, semantics, and motion dynamics. This gives rise to two key challenges: (1) Generation stability under any reference composition. The model must remain stable when conditioned on various combinations of reference images, avoiding common artifacts such as semantic mismatches between subjects and backgrounds, and visual inconsistencies caused by conflicting reference inputs. (2) Fine-grained identity consistency. The model must preserve each subjects coherent and detailed appearance throughout the video, including subtle features such as facial structure and accessories. Recent works have explored various strategies to address these challenges. [Wei et al. 2025; Yuan et al. 2024; Zhang et al. 2025] have achieved promising results in single identity preserving video synthesis. However, these models depend on external identity modules and single image reference, leading to limited scalability and reduced flexibility in real world applications. Moreover, methods such as [Jiang et al. 2025; Liu et al. 2025; Zhong et al. 2025] simplify the conditioning process by directly concatenating visual tokens along the token dimension. These approaches are typically built upon T2V generation frameworks and require large scale datasets for effective training. As result, they often struggle with identity preservation, particularly for fine-grained facial features, and exhibit limited generalization to out of domain inputs. SkyReels-A2 [Fei et al. 2025] offers an alternative I2V based design, processing reference images along the channel dimension and applying temporal mask. However, its vanilla masking strategy directly copies mask frames across time, which compromises the modeling capacity of the I2V backbone. This results in instability during video generation, with inconsistent frame quality and identity drift persisting. In this paper, we aim to address two fundamental challenges in multi-subject driven video generation: reference-image consistency and visual stability. We observe that the consistency between the generated video and the input reference images largely depends on how the reference conditions are injected into the model. Accurate and fine-grained integration of multi-subject visual information is crucial for maintaining visual alignment with the references. Additionally, the stability of the generated frames is highly sensitive to the training paradigm. We suppose that carefully designed training scheme is essential to adapt multi-subject driven generation tasks to pre-trained video models while preserving temporal coherence. Based on these observations, we propose MAGREF (Masked Guidance for Any-Reference Video Generation), unified and flexible framework designed to handle varied combinations of reference Deng et al. images (e.g., human identities, objects, backgrounds) when used in conjunction with textual prompts. Specifically, we begin by constructing blank canvas and randomly positioning each provided subject reference image onto it. To guide the generation process, we introduce region-aware dynamic masking strategy that indicates the spatial locations of each subject on the given reference canvas. This strategy enables precise and consistent conditioning, effectively bridging reference-image information with temporal dynamics during video synthesis. To alleviate fine-grained information loss commonly encountered in token-wise concatenation mechanism, MAGREF leverages pixelwise reference encoding on the channel dimension, significantly enhancing appearance fidelity. This mechanism introduces minimal architectural modifications, ensuring maximal preservation of pre-trained model capabilities and offering robust solution for multisubject video synthesis. Overall, the key contributions of MAGREF are as follows: We introduce region-aware dynamic masking mechanism that naturally aligns with the native I2V paradigm, requiring minimal architectural changes. It supports various numbers and flexible configurations of subject references, enabling the model to adapt seamlessly to complex compositional scenarios involving humans, objects, and backgrounds. We introduce pixel-wise channel concatenation mechanism that better preserves detailed appearance features, facilitating domain adaptation to specific tasks and accelerating training convergence. Extensive experiments demonstrate that MAGREF generates high-quality, identity consistent any reference videos and achieves highly competitive performance, outperforming most existing open-source methods and rivaling proprietary state-of-the-art systems."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Video generation models. Recent advancements in video generation often rely on Variational Autoencoders (VAEs) [Kingma 2013; Li et al. 2024; Van Den Oord et al. 2017] to compress raw video data into low-dimensional latent space. Within this compressed latent space, large-scale generative pre-training is conducted using either diffusion-based methods [Ho et al. 2020b; Song et al. 2021] or auto-regressive approaches [Chen et al. 2020; Ren et al. 2025; Yu et al. 2023a]. Leveraging the scalability of Transformer models [Peebles and Xie 2023; Vaswani 2017], these methods have demonstrated steady performance improvements [Blattmann et al. 2023b; Brooks et al. 2024; Yang et al. 2024]. This advancement significantly expands the possibilities for content generation and inspires follow-up research on text-to-video [Guo et al. 2024a; Kong et al. 2024; Ronneberger et al. 2015; Wan et al. 2025; Yang et al. 2024; Yin et al. 2024; Yu et al. 2023b] and image-to-video [Blattmann et al. 2023b; Chen et al. 2023b,a; Guo et al. 2024b; Xing et al. 2024; Ye et al. 2023; Zeng et al. 2024; Zhang et al. 2023] generation models. Subject-driven image and video generation. Generating identityconsistent images and videos based on given reference images requires the model to accurately capture the ID features. Previous methods can be broadly divided into two categories: tuning-based and training-free solutions. Tuning-based methods [Chen et al. 2025; MAGREF: Masked Guidance for Any-Reference Video Generation Wei et al. 2024; Wu et al. 2024; Zhou et al. 2024a] typically utilize efficient fine-tuning techniques, such as LoRA [Hu et al. 2022] or DreamBooth [Ruiz et al. 2023], to adapt pre-trained models. These approaches embed identity-specific information while retaining the original capabilities of the foundation model. However, these methods require fine-tuning for each new identity, which limits their practicality. In contrast, training-free methods adopt feedforward inference approach, eliminating the need for fine-tuning with new identity input. StoryDiffusion [Zhou et al. 2024b] employs consistent self-attention mechanism and semantic motion predictor to ensure smooth transitions and consistent identity. MSDiffusion [Wang et al. 2024b] introduces grounded resampler and multi-subject cross-attention mechanism to extract detailed features of the reference subjects. Recent works have explored various strategies for personalized video generation. For example, ConsisID[Yuan et al. 2024] maintains facial identity consistency through frequency decomposition. ConceptMaster[Huang et al. 2025] and VideoAlchemy[Chen et al. 2025] leverage CLIP[Radford et al. 2021] image encoder together with Q-Former [Li et al. 2023] to extract and fuse visual-text embeddings, which are then incorporated into the diffusion model to enable multi-concept video customization. Another line of work [Hu et al. 2025] introduces Multimodal Large Language Models (MLLMs), such as Qwen2-VL [Wang et al. 2024a] and LLaVA [Liu et al. 2023], to enhance the interaction between textual prompts and visual references. Building on Wan2.1[Wan et al. 2025], methods like ConcatID[Zhong et al. 2025], VACE[Jiang et al. 2025], Phantom[Liu et al. 2025], and SkyReels-A2 [Fei et al. 2025] further explore reference conditioning mechanisms. These models either concatenate the image latents with noisy latents in the denoising sequence, or inject reference features as conditional inputs to guide the diffusion process."
        },
        {
            "title": "3.1 Data Curation\nWe build a data processing pipeline that efficiently ingests training\nvideos, generates text labels, and extracts references—such as faces,\nand objects, dedicating for training of multi-subject video genera-\ntion task. The entire data processing pipeline includes three stages\nas shown in Figure 2, i.e., (1) general filtering and caption, (2) object\nprocessing and filtering, as well as (3) face processing and filtering.\nIn the first stage, we aim to ensure a clean and high quality\ntraining datasets. Thus, we perform rigorous data pre-processing\nand filtering. Specifically, given a set of training videos, we begin\nby applying scene change detection to segment each video into",
            "content": "Figure 2: Illustration of our data curation process. We present three-stage pipeline to collect high-quality video data for network training. multiple clips, denoted as 𝑉1, 𝑉2, . . . , 𝑉𝑛. Following the practices in previous video generation approaches [Yang et al. 2024; Zheng et al. 2024], we assess the aesthetic quality and motion magnitude of each clip 𝑉𝑖 , discarding clips with low aesthetic appeal or minimal motion to ensure high-quality training data. For the filtered video clips, we exploit Qwen2.5-VL [Bai et al. 2025] to generate caption, 𝐶𝑖 , that depicts its overall content focusing on motion aspect. The next two stages target multi-subject video generation, extracting objects and human references. For each filtered clip 𝑉𝑖 , Qwen2.5-VL identifies object labels from captions (e.g., cat, hat). GroundingDINO [Liu et al. 2024] detects each objects bounding box, and SAM2 segments it into reference images 𝐼 Obj 𝑖,𝑘 . To reduce noise, we refine masks using erosion/dilation, remove human related or small/abnormally shaped objects, and apply NMS to eliminate overlaps with human faces above 25%. The final stage focuses on human subjects. We detect faces across all frames of the clip and adjacent clips using InsightFace1. Face embeddings are used for identity (ID) assignment, while face orientation attributesyaw, pitch, and rollhelp filter out tilted or low quality detections. For each ID, faces are ranked by detection confidence and pose quality, and 10 are uniformly sampled to form the candidate set to sample 𝐼 Face . 𝑖 Each training sample consists of set of object segmentation masks, human segmentation masks, cropped faces, and their corresponding text labels. Formally, we define the training data as: R𝑖 = { 𝐶𝑖, 𝐼 Face 𝑖 , 𝐼 Obj 𝑖,1 , 𝐼 Obj 𝑖,2 , . . . , 𝐼 Obj 𝑖,𝑘 }, (1) where 𝐶𝑖 represents the text labels, 𝐼 Face reference, and 𝐼 Obj 𝑖 𝑖,𝑗 corresponds to the object references. For each training example, this set is paired with its corresponding video clip 𝑉𝑖 , ensuring alignment between the reference data and the video content. denotes the cropped face 1https://github.com/deepinsight/insightface Deng et al. Figure 3: Overview of our framework. Our method is built upon the pre-trained Wan2.1 [Wan et al. 2025] video generation base model. We introduce region-aware dynamic masking strategy to encode multi-reference images and concatenate them with noise latents along the channel dimension."
        },
        {
            "title": "3.2 Video Generation via Masked Guidance\nWe propose a novel framework, MAGREF, for coherent multi-subject\nvideo generation. Our approach introduces a simple but effective\nmethod for encoding multiple reference images through channel-\nwise concatenation. To handle various number of reference images,\nwe further design a region-aware dynamic masking mechanism\nthat allows a unified model to flexibly adapt to a wide range of\nmulti-subject generation tasks. An overview of the framework is\npresented in Figure 3, the architecture and functionality of each\nmodule are detailed in the following sections.",
            "content": "Region-aware dynamic masking mechanism. To accurately inject multi-subject information while aligning with the I2V modeling paradigm, we introduce region-aware dynamic masking mechanism that combines channel-wise composition with spatial masking. Specifically, we first place all reference images onto blank canvas at distinct spatial locations and then encode the composite using the VAE to obtain its latent representation. corresponding binary mask is generated to indicate the spatial location of each subjects features within the layout (as shown in Figure 4). This mask explicitly informs the model of the presence and spatial position of each subject, while preserving compatibility with the native I2V modeling paradigm. To avoid overfitting to fixed spatial arrangements and to improve generalization to varying numbers and orders of subjects, we apply random shuffling strategy during training. As shown in Figure 4, subject regions are randomly permuted across spatial locations within the masked map. This encourages the model to rely on mask-guided features rather than spatial positions alone, making it agnostic to the combination and order of subjects on the canvas. Unlike prior approaches that inject reference images into the temporal sequence or append visual tokens after patchification always disrupting positional encoding and struggling with variable length inputs. Our region-aware dynamic masking mechanism enables flexible and identity consistent conditioning by preserving pixel-aligned spatial structure and supporting variable number of subjects. By explicitly retaining the spatial layout and visual cues of each subject during encoding, this design not only facilitates fine-grained appearance preservation and more effective domain adaptation but also maintains compatibility with the native I2V modeling paradigm. Furthermore, by operating at the channel level rather than the token level, our method significantly accelerates training convergence while avoiding the overhead associated with tokenbased concatenation. Pixel-wise channel concatenation. Achieving coherent and identityconsistent multi-subject video generation requires precise identityaware information of each subjects. Existing methods either inject VAE representations of reference images along the temporal dimension [Jiang et al. 2025] or concatenate visual tokens from the reference images after patchification [Zhong et al. 2025]. However, these strategies introduce critical challenge: incorporating reference images as additional frames or concatenation tokens disrupts the original positional embedding, particularly when handling various number of reference images. As result, the model struggles to properly interpret multi-subject conditions, leading to inconsistencies between the generated video and the provided reference images. In MAGREF, we introduce lightweight yet effective strategy that applies pixel-wise masks and combines them via channel concatenation. Unlike previous methods [Hu et al. 2025; Liu et al. 2025], which concatenate reference images along the token dimension and rely on self-attention mechanisms, our approach establishes pixellevel alignment between the reference images and the generated videos, ensuring better fidelity in preserving the unique identity of each subject. Formally, let 𝑁 reference images {𝐼1, 𝐼2, . . . , 𝐼𝑁 } correspond to 𝑁 distinct subjects. These images are randomly combined into single composite image 𝐼ref. To match the temporal dimension of the video frames, zero-padding is applied to the composite image along the temporal axis. The padded composite image is then processed by the VAE encoder 𝐸 () to produce unified latent feature map: 𝐹ref = 𝐸 (𝐼ref) R𝑇 𝐶 𝐻 𝑊 , (2) MAGREF: Masked Guidance for Any-Reference Video Generation Figure 4: Illustration of our region-aware dynamic masking strategy. We concatenate the reference images into single composite image, which is then encoded by VAE (𝐸) to obtain the reference features. pixel-wise binary mask is downsampled to spatially indicate the presence of each reference region. We introduce random shuffling strategy to increase the robustness dealing various number of subjects. where 𝑇 , 𝐶, 𝐻 , and 𝑊 represent the number of frames, channels, height, and width of the feature map, respectively. This process ensures that the reference image representation is temporally aligned with the video frames, facilitating seamless integration of reference features across the entire video sequence. Next, the raw video frames are processed through the same VAE encoder 𝐸 (), producing latent feature maps. Gaussian noise is then added to these latents, yielding 𝑍 R𝑇 𝐶 𝐻 𝑊 , where 𝑇 represents the number of frames. Additionally, let 𝑀 R𝑇 4𝐻 𝑊 be the sequence of masks. Finally, we concatenate the noised video latents 𝑍 , the reference image representation 𝐹ref, and the feature masks 𝑀 along the channel dimension to construct the final input 𝐹input: 𝐹input = (𝑍 𝐹ref 𝑀) R𝑇 (2𝐶+4) 𝐻 𝑊 , (3) where denotes channel-wise concatenation. This composite input 𝐹input is then fed into the subsequent modules of the framework to facilitate coherent and identity preserving multi-subject video generation."
        },
        {
            "title": "4 EXPERIMENTS\n4.1 Experimental Setup",
            "content": "Dataset. We train our model on self-curated, high-quality video dataset (see Section 3.1 for details on our data curation pipeline). Specifically, the original training set comprises approximately 22 million video clips. After rigorous filtering process, we obtain around 1.3 million high-quality clips, each paired with one human and several object references, stored as RGBA images. Each video clip contains 81 frames. Finally, we deduplicate and select subset of 0.5 million unique video clips to serve as the final training set. Benchmark. Due to the limitations of existing benchmarks for multi-subject video generation, we construct systematic and task-specific benchmark to comprehensively evaluate our video generation framework under both single-ID and multi-subject settings. The benchmark consists of 120 subject-text pairs, categorized into two major groups: single-ID and multi-subject. The single-ID group comprises 60 test cases with single ID reference image, while the multi-subject group includes 60 diverse and challenging cases featuring two-person, three-person, and mixed scenarios such as human-object-background compositions. subset of cases is adapted from ConsisID [Yuan et al. 2024] and A2-Bench [Fei et al. 2025], while the rest are carefully curated to ensure coverage across wide range of subject types, background contexts, and interaction patterns. Each case contains no more than three reference images, along with natural language prompt that maintains high aesthetic quality and semantic alignment. This controlled setting ensures consistent difficulty levels while enabling fine-grained and robust evaluation of generative performance across varying degrees of visual and semantic complexity. By incorporating diverse subject appearances, prompt lengths, and compositional arrangements, the benchmark provides rigorous testbed for assessing the models ability to synthesize coherent and diverse videos under real-world conditions. Evaluation Metrics. We evaluate the quality of generated videos across four key dimensions: Identity Preservation: We employ FaceSim [Deng et al. 2019], which calculates the cosine similarity between face embeddings extracted from pretrained face recognition model (e.g., ArcFace). This metric evaluates how well the generated subject identity is preserved across frames. Visual Quality: We adopt the Aesthetic Score [christophschuhmann 2024], which reflects human perceptual preferences by leveraging learned aesthetic prediction model trained on highquality image datasets. This metric provides proxy for overall visual appeal and composition. Text Relevance: We use the GmeScore [Zhang et al. 2024], retrieval-based metric built on Qwen2-VL model fine-tuned for vision-language alignment. It measures the semantic consistency between the generated video content and its corresponding longform textual prompt. Motion Strength: We compute the Motion Score [Bradski et al. 2000] by measuring the average magnitude of inter-frame optical flow. This metric captures the overall strength and intensity of motion, reflecting how dynamic the generated video is. Together, these metrics provide comprehensive evaluation framework, jointly assessing identity consistency, perceptual quality, semantic alignment, and temporal dynamic. Training strategies. We train our model using the FusedAdam optimizer, configured with 𝛽1 = 0.9, 𝛽2 = 0.999, and weight decay of 0.01. The learning rate is initialized at 1 105 and follows cosine annealing schedule with periodic restarts. To stabilize training and prevent exploding gradients, we apply gradient clipping with maximum norm of 1.0, which benefits the optimization process. Model training is conducted on 32 NVIDIA H100 GPUs over span of 4 days. Input videos are processed at resolution of 480P, with sequence length of 81 frames and temporal stride of one. Deng et al. Figure 5: Qualitative evaluation results of our method on test cases involving single ID. Our model consistently generates videos that maintain the subjects identity while accurately following the input text prompt. Figure 6: Qualitative evaluation results of our method on test cases involving multiple concepts, such as persons, animations, and scenes. Our model is capable of understanding and encoding multiple subjects based on the reference images. MAGREF: Masked Guidance for Any-Reference Video Generation The training loss follows the standard diffusion loss formulation, as outlined in [Wan et al. 2025]."
        },
        {
            "title": "4.2 Qualitative Results\nFigure 5 and Figure 6 demonstrate qualitative examples of our\nmethod, which synthesizes coherent videos from reference images\npaired with textual descriptions. The results clearly indicate that\nour model successfully preserves the distinct visual attributes of\nthe provided subjects, such as facial features, hairstyles, accessories,\nand environmental contexts. For instance, in Figure 5, the generated\nvideo accurately retains the blonde hair and gentle interaction with\nblossoms, precisely following the prompt. Similarly, the identity of\nLeonardo DiCaprio is well preserved, while the generated hat and\nclothing closely follow the text prompt and blend seamlessly with\nhis facial features. Furthermore, Figure 6 demonstrates our model’s\nability to handle complex scenarios involving multiple visual con-\ncepts, including multiple subjects, diverse objects, and background.\nAn illustrative example features a man outdoors wearing a blue and\nblack jacket, interacting with a dog, with a specific building serving\nas the background. Additionally, the examples involving two or\neven three individuals interacting with each other demonstrate\nthe model’s ability to understand complex scenarios. We conduct\nqualitative comparisons against both open-source and proprietary\nstate-of-the-art models on single-ID and multi-subject video genera-\ntion tasks. As shown in Figure 7, our method demonstrates superior\nperformance in out-of-domain scenarios with single reference im-\nages, achieving the best results in terms of text-action alignment,\nvisual quality, and identity consistency. Furthermore, for multi-\nsubject video generation, our approach continues to outperform\nexisting methods, as illustrated in Figure 8.",
            "content": "These results confirm that our method can accurately capture interactions among people, objects, and environments, generating contextually appropriate and visually compelling motions. It is worth noting that our model was trained solely on single-ID data with objects, without any background conditioning."
        },
        {
            "title": "4.3 Quantitative Results\nIn addition to the qualitative evaluation, we conduct a thorough\nquantitative assessment of both open-source and closed-source\nmethods on our newly constructed benchmark dataset using four\nkey metrics: Aesthetics, Motion, FaceSim, and GmeScore. Table 1\nsummarizes our model’s performance compared to the SOTA base-\nlines. For each method, we report single-ID scores, and multi-subject\nscores when available (separated by “/”).",
            "content": "For Face Similarity (FaceSim), arguably the most critical metric in subject-driven video generation, our method establishes new state-of-the-art when considering both single-ID and multi-subject generation settings. Notably, the performance improves significantly when multiple reference images are provided, highlighting the models strong capability to capture and maintain entangled identity features in zero-shot manner. For aesthetics scores, our method achieves the best or at least comparable performance to all existing approaches, especially under the multi-subject setting. The motion scores captures the dynamics of the generated video and is often entangled with the aesthetics score. In other words, high motion score may come at the expense of visual quality, potentially corresponding to videos with unrealistic or collapsed motion patterns. For example, Skyreels-A2 achieves relatively higher motion score but suffers from lower aesthetics score, suggesting trade-off between motion dynamics and visual quality. In contrast, our method strikes better balance between these two aspects, maintaining both coherent motion and high visual fidelity. The text-video alignment, measured by GmeScore, shows that our method achieves performance on par with top-tier methods."
        },
        {
            "title": "4.4 Ablation Studies\nIn the ablation studies, we demonstrate the effectiveness of the two\nkey contributions of our method: region-aware dynamic masking\nmechanism and pixel-wise channel concatenation mechanism.",
            "content": "Region-aware dynamic masking mechanism. Region-aware dynamic masking mechanism is introduced to accommodate variable number of reference images in spatially flexible and content-aware manner. Specifically, we train the model using two masking mechanisms: fine-grained Region-aware Dynamic Masking Mechanism (top of Figure 4) and coarse-grained Vanilla Masking Mechanism adopted by SkyReels-A2 [Fei et al. 2025] (bottom of Figure 4). As shown in Figure 9 (b) and (c), the vanilla approach, which concatenates reference images along the channel dimension leads to frame-level inconsistencies and identity drift, especially during long video generation. Even discarding initial warm-up frames, subsequent outputs frequently suffer from quality degradation, failing to preserve identity fidelity and motion coherence. This suggests that the channel-wise image concatenation and coarse vanilla masking lead to strong interference, undermining the temporal consistency and visual stability of the generated results. In contrast, the regionaware dynamic masking mechanism precisely modulates the spatial and temporal influence of reference images. This allows the model to better exploit fine-grained visual cues and seamlessly inherit the I2V training paradigm, ultimately enabling the generation of high-fidelity videos that exhibit coherent motion. Pixel-wise channel concatenation mechanism. We conducted ablation experiments to compare two concatenation mechanisms: the proposed pixel-wise channel concatenation mechanism and the token-wise concatenation mechanism adopted in prior work [Hu et al. 2025; Liu et al. 2025]. As illustrated in Figure 9(d), the proposed pixel-wise channel concatenation demonstrates superior identity preservation, particularly in restoring fine-grained facial structures. In contrast, when reference images are incorporated as additional tokens, the model must rely entirely on self-attention mechanisms to extract and propagate identity-related features. This indirect encoding provides weaker supervision for identity signals during training, as the model relies solely on self-attention, which tends to diffuse identity cues across tokens and leads to inconsistencies in the generated output (see Figures 9(e) and (f)). These observations underscore the advantages of our approach: By directly embedding identity information in spatially aligned and semantically rich feature representations, our method enhances the models ability to preserve identity in generative tasks. Deng et al. Figure 7: Comparison of our model versus existing approaches on single ID generation. We compare MAGREF with existing subject-consistent video generation model. MAGREF demonstrates superior performance compared to open-source models and achieves competitive results relative to commercial models such as [Kling 2025] and [Hailuo 2025]. Our method balances between the input concept and prompt, MAGREF follows the instruction wearing glasses while maintaining the input identity. Figure 8: Comparison of our model versus existing works on multi-subject generation. We compare MAGREF with existing subject-consistent video generation model. Our MAGREF is able to generate videos conditioned on flexible references images, maintaining high-quality ID even conditioned on multiple persons. MAGREF: Masked Guidance for Any-Reference Video Generation Table 1: Quantitative evaluation results on single-ID and multi-subject test cases. For each method, we present both the single-ID and multi-subject scores, if available, separated by /. In each column, the best results are highlighted in bold and the second best ones are underlined. Type Method FaceSim Aesthetics GmeScore Motion EchoVideo [Wei et al. 2025] FantasyID [Zhang et al. 2025] ConsisID [Yuan et al. 2024] Concat-ID [Zhong et al. 2025] HunyuanCustom [Hu et al. 2025] SkyReels-A2 [Fei et al. 2025] Phantom [Liu et al. 2025] VACE [Jiang et al. 2025] Hailuo [Hailuo 2025] Pika 2.1 [Pika 2025] Vidu 2.0 [Vidu 2025] Kling 1.6 [Kling 2025] 0.455 / 0.304 / 0.406 / 0.417 / 0.534 / 0.514 / 0.275 0.434 / 0.554 0.621 / 0.346 0.537 / 0.265 / 0.242 0.360 / 0.312 0.373 / 0.389 0.399 / 0.456 / 0.418 / 0.441 / 0.502 / 0.399 / 0.371 0.508 / 0.439 0.528 / 0. 0.528 / 0.524 / 0.477 0.473 / 0.425 0.509 / 0.458 0.684 / 0.727 / 0.720 / 0.737 / 0.728 / 0.636 / 0.668 0.722 / 0.693 0.695 / 0.690 0.714 / 0.750 / 0.684 0.704 / 0.688 0.678 / 0.664 0.356 / 0.234 / 0.380 / 0.311 / 0.135 / 0.315 / 0.300 0.141 / 0.129 0.118 / 0.106 0.318 / 0.288 / 0.286 0.145 / 0.091 0.500 / 0.311 Open-source Closed-source Ours MAGREF 0.567 / 0.581 0.514 / 0.487 0.716 / 0. 0.270 / 0.237 Figure 9: Ablation study on masking and concatenation schemes. Left: Comparison of different masking mechanisms. Our proposed dynamic masking mechanism maintains identity consistency and visual coherence under varying reference conditions (Top row). In contrast, the vanilla masking mechanism, which concatenates reference images along the channel dimension, results in temporal inconsistency and identity drift (The second row: our re-implementation; Bottom row: SkyReels-A2 [Fei et al. 2025]). Right: Comparison of different concatenation mechanisms. Pixel-wise channel concatenation preserves fine-grained reference features, improving consistency with reference images. In contrast, token-wise concatenation dilutes identity-specific cues and weakens identity preservation (The second row: our re-implementation; Bottom row: Phantom [Liu et al. 2025])."
        },
        {
            "title": "5 CONCLUSION\nIn this work, we present MAGREF, a unified framework for any-\nreference video generation that combines pixel-wise channel con-\ncatenation with a region-aware dynamic masking mechanism to\nsynthesize coherent videos featuring multiple distinct subjects. Ex-\ntensive experiments demonstrate that MAGREF achieves state-of-\nthe-art performance, effectively generalizing from single-subject\nscenarios to complex multi-subject compositions with strong tem-\nporal consistency and fine-grained identity preservation. Notably,",
            "content": "our approach remains model-agnostic and can be integrated into variety of video generation backbones. However, the overall generation quality is inherently constrained by the capabilities of the underlying video foundation model, which affects both visual fidelity and temporal stability. In future work, we plan to incorporate more advanced foundation models to enhance resolution, motion coherence, and longterm consistency. Moreover, we aim to extend MAGREF to support unified multi-modal generation by leveraging the reasoning and grounding capabilities of multi-modal large language models (MLLMs), enabling synchronized generation across video, audio, and text. REFERENCES Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923 (2025). Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023a. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023b. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023). Gary Bradski, Adrian Kaehler, et al. 2000. OpenCV. Dr. Dobbs journal of software tools 3, 2 (2000). Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. 2024. Video Generation Models as World Simulators. https: //openai.com/research/video-generation-models-as-world-simulators. Accessed: 2023-10-20. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. 2023b. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023). Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020. Generative Pretraining From Pixels. In Proceedings of the 37th International Conference on Machine Learning. PMLR, 16911703. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. 2025. Multi-subject Open-set Personalization in Video Generation. arXiv preprint arXiv:2501.06187 (2025). Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. 2023a. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations. christophschuhmann. 2024. improved-aestheticpredictor Lab (2024). https://github.com/christophschuhmann/improved-aestheticpredictor/tree/main improved-aesthetic-predictor. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 46904699. Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. 2025. SkyReels-A2: Compose Anything in Video Diffusion Transformers. arXiv preprint arXiv:2504.02436 (2025). Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. 2024b. I2V-adapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers. 112. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024a. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In The Twelfth International Conference on Learning Representations. Hailuo. 2025. Hailuo. Hailuo Lab (2025). https://hailuoai.video/ Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020a. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020b. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https://openreview. net/forum?id=nZeVKeeFYf9 Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. 2025. HunyuanCustom: Multimodal-Driven Architecture for Customized Video Generation. arXiv preprint arXiv:2505.04512 (2025). Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. 2025. ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning. arXiv preprint arXiv:2501.04698 (2025). Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. 2025. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598 (2025). arXiv preprint Diederik Kingma. 2013. Auto-encoding variational bayes. arXiv:1312.6114 (2013). Deng et al. Kling. 2025. Image to video elements feature. https://klingai.com/image-to-video/ multi-id/new/ Accessed: 2025-02-26. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024). Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning. PMLR, 1973019742. Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. 2024. WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model. arXiv preprint arXiv:2411.17459 (2024). Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems 36 (2023), 3489234916. Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, and Xinglong Wu. 2025. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079 (2025). Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2024. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision. Springer, 3855. OpenAI. 2024. Sora. https://openai.com/sora/ Accessed: 2025-02-26. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 41954205. Pika. 2025. Pika Art 2.0s Scene Ingredients: Redefining Personalized Video Creation. https://pikartai.com/scene-ingredients/ Accessed: 2025-02-26. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. 2025. VideoWorld: Exploring Knowledge Learning from Unlabeled Videos. arXiv preprint arXiv:2501.09781 (2025). Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer, 234241. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2250022510. Runway. 2025. Runway. https://runwayml.com/ Accessed: 2025-02-26. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 (2020). Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations. Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). Vidu. 2025. Reference to video. https://www.vidu.com/ Accessed: 2025-02-26. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. 2025. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025). Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). Xiaowei Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. 2024b. Msdiffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209 (2024). Jiangchuan Wei, Shiyue Yan, Wenfeng Lin, Boyuan Liu, Renjie Chen, and Mingyu Guo. 2025. EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion. arXiv preprint arXiv:2501.13452 (2025). Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. 2024. DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control. arXiv preprint arXiv:2410.13830 (2024). Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. 2024. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239 (2024). Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. 2024. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision. Springer, 399417. MAGREF: Masked Guidance for Any-Reference Video Generation Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024). Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023). Yuanyang Yin, Yaqi Zhao, Mingwu Zheng, Ke Lin, Jiarong Ou, Rui Chen, Victor SheaJay Huang, Jiahao Wang, Xin Tao, Pengfei Wan, et al. 2024. Towards Precise Scaling Laws for Video Diffusion Transformers. arXiv preprint arXiv:2411.17470 (2024). Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2023a. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1045910469. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. 2023b. Language Model Beats DiffusionTokenizer is Key to Visual Generation. arXiv preprint arXiv:2310.05737 (2023). Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. 2024. Identity-Preserving Text-to-Video Generation by Frequency Decomposition. arXiv preprint arXiv:2411.17440 (2024). Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. 2024. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 88508860. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. 2023. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145 (2023). Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint arXiv:2412.16855 (2024). Yunpeng Zhang, Qiang Wang, Fan Jiang, Yaqi Fan, Mu Xu, and Yonggang Qi. 2025. Fantasyid: Face knowledge enhanced id-preserving video generation. arXiv preprint arXiv:2502.13995 (2025). Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. 2024. Open-Sora: Democratizing Efficient Video Production for All. https://github.com/hpcaitech/Open-Sora Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. 2025. Concat-ID: Towards Universal Identity-Preserving Video Synthesis. arXiv preprint arXiv:2503.14151 (2025). Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Nanxuan Zhao, Jing Shi, and Tong Sun. 2024a. SUGAR: Subject-Driven Video Customization in Zero-Shot Manner. arXiv preprint arXiv:2412.10533 (2024). Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. 2024b. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems 37 (2024), 110315110340."
        }
    ],
    "affiliations": []
}