{
    "paper_title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
    "authors": [
        "Zhenyu Pan",
        "Yiting Zhang",
        "Zhuo Liu",
        "Yolo Yunlong Tang",
        "Zeliang Zhang",
        "Haozheng Luo",
        "Yuwei Han",
        "Jianshu Zhang",
        "Dennis Wu",
        "Hong-Yu Chen",
        "Haoran Lu",
        "Haoyang Fang",
        "Manling Li",
        "Chenliang Xu",
        "Philip S. Yu",
        "Han Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 6 8 5 1 0 . 0 1 5 2 : r ADVEVO-MARL: SHAPING INTERNALIZED SAFETY THROUGH ADVERSARIAL CO-EVOLUTION IN MULTIAGENT REINFORCEMENT LEARNING Zhenyu Pan1, Yiting Zhang2, Zhuo Liu3, Yolo Yunlong Tang3, Zeliang Zhang3, Haozheng Luo1, Yuwei Han2, Jianshu Zhang1, Dennis Wu1, Hong-Yu Chen1, Haoran Lu1, Haoyang Fang4, Manling Li1, Chenliang Xu3, Philip S. Yu2, Han Liu1 1Northwestern University 3University of Rochester 2University of Illinois at Chicago 4Carnegie Mellon University"
        },
        {
            "title": "ABSTRACT",
            "content": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates single-point-of-failureonce compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce public baseline for advantage estimation: agents within the same functional group share group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preservingand sometimes improvingtask accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead."
        },
        {
            "title": "INTRODUCTION",
            "content": "LLM-based agents exhibit advanced capabilities in software engineering (Pan et al., 2025a; 2024a), question answering system (Pan et al., 2024b;c), and scientific discovery (Shao et al., 2025). Building on this progress, multi-agent systems (MAS) coordinate specialized agents with diverse expertise to harness collective intelligence for solving increasingly complex real-world problems. However, as MAS become more capable, they also face growing safety challenges (Raza et al., 2025). On one hand, MAS inherit vulnerabilities from single agents, particularly their susceptibility to jailbreak attacks, where malicious actors attempt to bypass safety guardrails. On the other hand, the complex interaction dynamics among agents, along with the presence of potentially unauthorized or adversarial agents, significantly expand the attack surface beyond that of isolated systems (He et al., 2025). To mitigate these risks, researchers mainly explore two broad categories of defense: (i) empowering each agent to locally verify the benignness of its inputs before generating responses (tse Huang et al., 2025), and (ii) deploying external inspector agents to monitor and regulate information flow throughout interactions (Xiang et al., 2025). While these approaches are effective to some extent, they suffer from notable limitations. External guard agents introduce single point of failureonce compromised, the system is left defenselessand scaling up the number of guards quickly incurs 1 prohibitive computational costs, rendering them impractical for large-scale deployments (Chennabasappa et al., 2025). Meanwhile, individual agents have limited capacity to detect or resist sophisticated, cross-agent attacks, making self-verification in isolation insufficient (Zhu et al., 2025). natural intuition is to embed safety awareness within task agents through targeted safety training. Yet training agents individually overlooks the collaborative dynamics required for effective multi-agent defense, and conventional safety training based on static datasets often leads to overfitting and poor generalization against adaptive adversaries (Geissler et al., 2024). To address these challenges, we introduce AdvEvo-MARL, co-evolutionary multi-agent reinforcement learning (MARL) framework that embeds safety awareness directly within task agents. The core idea is to jointly evolve attackers, which generate increasingly sophisticated jailbreak prompts, and defenders, which must both resist these attacks and fulfill their assigned tasks. AdvEvo-MARL initializes training with curated pool of adversarial prompts derived from representative attack strategies. Since attackers lack prior knowledge of effective jailbreak tactics, we first warm them up using carefully designed seed prompts from this pool by supervised fine-tuning (SFT). During following MARL, attackers rewrite and refine these prompts to create more potent adversarial inputs, while defenders are simultaneously optimized to withstand these evolving threats and maintain task performance. To further stabilize training and foster coordination, we introduce public baseline for advantage estimation: agents within the same functional group (e.g., attackers or defenders) share the groups mean return as their baseline. This mechanism enables agents to learn from peer behaviors, reduces variance in policy updates, and strengthens intra-group cooperation. With training, attackers evolve beyond static attack templates, while defenders acquire more robust and generalizable safety behaviors. This co-evolutionary process drives continuous safety enhancement, mitigating the risk of overfitting to fixed attack distributions and enabling resilience against adaptive adversaries. Experiments on three representative MAS attack scenariosagent manipulation, message corruption, and user instruction hijackingdemonstrate the effectiveness of AdvEvo-MARL in enhancing system robustness. Further task benchmarks show minimal performance degradation, and in some cases even improved task capabilities, underscoring the potential of AdvEvo-MARL as standardized framework for building MAS that are both safe and capable. In summary, our main contributions are three-folds: We propose AdvEvo-MARL, novel multi-agent reinforcement learning framework that internalizes safety awareness within each agent through adversarial co-evolution. In this evolving paradigm, attackers and defenders iteratively compete and improve, leading to increasingly robust strategies on both sides. We introduce public baseline mechanism for advantage estimation, where agents within the same functional group (e.g., attackers or defenders) use the groups mean return as baseline. This design promotes collaborative learning among agents and enables more stable policy updates during training. Experiments across multiple representative MAS attack settings demonstrate consistent safety gainsachieving up to maximum of 18.33% improvement. Further evaluations on standard task benchmarks reveal minimal degradation and, in several cases even enhanced task performance, underscoring AdvEvo-MARLs effectiveness in simultaneously promoting multi-agent system safety and task utility."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work builds on two main research lines. The first examines safety in MAS, where adversarial threats such as agent manipulation and message corruption motivates defenses like self-verification, guard agents, and peer inspection. The second explores multi-agent reinforcement learning (MARL), which has enabled coordinated training and has recently been applied to LLM-based systems. These perspectives motivate our proposed AdvEvo-MARL, which unifies safety and MARL by co-evolving attackers and defenders to embed intrinsic safety awareness into agents."
        },
        {
            "title": "2.1 SAFETY IN MULTI-AGENT SYSTEMS.",
            "content": "LLMs are known to exhibit safety vulnerabilities, especially when exposed to adversarial attacks. Equipping agents with external tools or memory systems further expands the attack surface (Raza et al., 2025; Chen et al., 2024). While multi-agent systems (MAS) built upon such agents demonstrate strong task-solving capabilities, they are also vulnerable to wide range of threats, most commonly: (1) manipulating agents to induce malicious behaviors (Yu et al., 2024), and (2) corrupting communication messages or workflow execution (He et al., 2025; Zhang et al., 2024). To mitigate these risks, several defense strategies have been proposed. Some works leverage self-verification, encouraging each agent to assess the benignness of its inputs before responding (Fan & Li, 2025; tse Huang et al., 2025), while others employ dedicated guard agent to monitor and rectify message flows (tse Huang et al., 2025). Another line of research collects safety-oriented interaction trajectories and trains graph neural networks to detect and correct unsafe responses (Wang et al., 2025). Furthermore, decentralized defenses have also been explored, where agents inspect one another to form peer-based protection (Fan & Li, 2025). Although these approaches provide partial safeguards, they face key limitations. Individual agents often lack the capacity to detect sophisticated attacks, while centralized guard agents introduce single point of failure and impose computational overhead in complex systems. In contrast, we advocate embedding safety awareness directly into each agent through reinforcement learning, enabling intrinsic defense capabilities and fundamentally improving the robustness of MAS."
        },
        {
            "title": "2.2 MULTI-AGENT REINFORCEMENT LEARNING.",
            "content": "Reinforcement learning (RL) has proven effective in post-training LLMs (Shao et al., 2024; Team et al., 2025; Pan & Liu, 2025), with methods such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) yielding substantial performance gains (Shao et al., 2024). More recently, RL has also been applied to enhance agentic behaviors in language-based systems (Jin et al., 2025). Multi-agent reinforcement learning (MARL), exemplified by algorithms like MAPPO and QMIX (Kang et al., 2023; Rashid et al., 2020), extends RL to coordinated multi-agent settings (Liu et al., 2025). Several recent studies adapt MARL to LLM-based systems: one line of work applies MARL to improve collaborative agent behaviors in structured game environments (Park et al., 2025); another develops hierarchical MAS with high-level planners and low-level executors using parameter sharing to enhance meta-reasoning (Wan et al., 2025); yet another treats each Retrieval-Augmented Generation (RAG) module as an agent, applying MARL to jointly optimize task performance (Chen et al., 2025). However, most methods train single backbone model with shared parameters across agents (Pan et al., 2025b), limiting true agent-level diversity. In contrast, our framework trains multiple distinct backbone models collaboratively under RL, enabling genuine co-evolution. Building on these advances, our work explores MARL as vehicle to improve MAS safety. By co-evolving attackers and defenders in an adversarial learning environment, we embed safety awareness directly into task agents through continuous interaction and adaptation, fostering robust and generalizable defense capabilities."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "We formulate the interaction among learning agents as partially observable Markov game: = (S, {Ai}N i=1, P, {Oi}N i=1, γ, ), (1) where denotes the state space, Ai represents the action space of agent i, is the state transition function, Oi is the observation function for agent i, γ is the discount factor, and is the finite time horizon. Each agent {1, . . . , } follows stochastic policy πi(ai oi), conditioned on local observation oi Oi(s), and jointly contributes to the environment evolution via the composite action = (a1, . . . , aN ). In the context of LLMs-based agents, instead of treating each token as an action, we define the action of an agent as generating complete response that consists of token sequence. The agents are partitioned into two disjoint sets: attackers and defenders D, = and = {1, . . . , }. The attackers attempt to compromise systems safety guardrail, while the defenders must resist adversarial attacks and preserve task performance. All agents interact over the course of steps. At the end of each episode, the system produces final output = Φ(τ ), where τ = (s0, a0, s1, . . . , sT ) denotes the complete trajectory induced by the multi-agent interaction. This 3 Figure 1: Framework. AdvEvo-MARL begins by warming up attacker agents through supervised fine-tuning to embed prior knowledge of jailbreak behaviors. Then, attackers and defenders learn to co-evolve via adversarial multi-agent reinforcement learning. During policy updates, agents within the same functional group (i.e., attackers or defenders) leverage public baseline which is computed as the mean return of their respective group to estimate their individual advantages for optimization. output is then evaluated by the environment or trusted judge to form global reward G(τ ), upon which each agent receives its own local reward ri. The learning goal is to co-evolve attackers and defenders under shared dynamics and finally induce stable and robust equilibrium between attacker and defender populations. This is captured by the following game-theoretic objective, where {πk}N denotes the joint policy of all agents: k=1 max {πj }jD min {πi}iA"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "E τ {πk}N k=1 (cid:88) jD rj(τ ) (cid:88) iA ri(τ ) . (2) In this section, we introduce AdvEvo-MARL, multi-agent reinforcement learning framework designed to improve the safety of multi-agent systems. We first provide an overview of AdvEvoMARL, then detail the attacker warm-up procedure, and finally present the adversarial RL pipeline with public-baseline-based advantage estimation."
        },
        {
            "title": "4.1 OVERVIEW",
            "content": "As shown in fig. 1, AdvEvo-MARL unfolds in two stages. First, an attacker warm-up phase uses supervised fine-tuning to inject prior knowledge of jailbreak strategies, preventing trivial or ineffective attacks at the start of training. Upon this initialization, we introduce an adversarial co-evolutionary RL stage where attackers and defenders are jointly optimized through repeated interactions, enabling defenders to acquire robust and adaptive safety behaviors against evolving threats. To stabilize learning and encourage group-consistent updates, agents within the same role leverage public baseline for advantage estimation, reducing variance and promoting effective collaboration."
        },
        {
            "title": "4.2 BOOTSTRAPPING ADVERSARIAL GENERATION VIA ATTACKER WARM-UP",
            "content": "As attackers lack prior understanding of jailbreak behaviors and adversarial prompting techniques, we first conduct warm-up training before MARL. We construct dataset Dadv consisting of paired samples of the form (xbehavior, xattack), where xbehavior is the trivial harmful questions, and xattack is the re-written attack prompts using certain jailbreak techniques. Specifically, we begin by sampling 1,000 harmful behaviors from existing public datasets, ensuring broad coverage across diverse categories of harmful content. We then apply representative jailbreak strategies to generate corresponding adversarial attack prompts, obtaining an initial jailbreak prompt dataset Dinit. Given the original questions and their associated attack variants, we employ an advanced reasoning model to synthesize multi-step reasoning traces that illustrate how to construct effective adversarial prompts. To ensure quality, we filter out invalid reasoning trajectories that are contradictory, off-topic, or vague using LLM-as judge method. The resulting dataset Dadv contains approximately 4,000 high-quality training samples. AdvEvo-MARL leverages imitation learning to equip attackers with jailbreak knowledge from the curated Dadv, thereby accelerating exploration in the early stages of training."
        },
        {
            "title": "4.3 ADVEVO-MARL: SAFE AND CAPABLE MULTI-AGENT SYSTEMS VIA CO-EVO RL",
            "content": "To build safe and capable MAS, we embeds safety awareness directly into agents through adversarial co-evolution, enabling them to withstand evolving attacks while maintaining strong task performance. Importantly, we trains multiple backbone models collaboratively under RL, rather than relying on single shared-parameter model, ensuring genuine co-evolution across diverse agents. Training Algorithm Following the attacker warm-up stage, both attackers and defenders are jointly optimized within co-evolutionary multi-agent reinforcement learning process. All agents are trained using REINFORCE++ to improve both system safety and task performance (Hu, 2025). To facilitate collaborative learning and stabilize policy updates, we introduce public baseline for advantage estimation. Specifically, during each rollout episode, the advantage for each agent is computed relative to the mean return of all agents within the same role group (i.e., attackers or defenders), rather than being estimated solely from its own return trajectory. Formally, for episode τ we define: bA(τ ) = 1 (cid:88) iA rA (τ ), bD(τ ) = 1 (cid:88) jD rD (τ ), (3) where bA and bD denote the mean return value of attackers and defenders respectively. The resulting advantage estimate for any agent {1, . . . , } is then given by ˆAk(τ ) = rk(τ ) (cid:26)bA(τ ), bD(τ ), if A, if D. Finally, the training loss for agent is defined as: LREINFORCE++ (θk) = Et (cid:104) min (cid:16) rt,k (θk) ˆAt,k, clip (rt,k (θk) , 1 ε, 1 + ε) ˆAt,k (cid:17)(cid:105) + βKLEt [KL (πθk ( xt,k) πref,k ( xt,k))] , (4) (5) where rt,k(θk) = πθk (at,kxt,k) (at,kxt,k) denotes the importance sampling ratio, clipping clip restricts updates magnitude, and the KL term measures divergency between learned policy πθk and reference policy πref,k to regulate training. πθold Reward Modeling To care distinct objectives of attackers and defenders, we design separate reward mechanisms for each agent type. Attackers receive rewards based on whether the final system output achieves the intended malicious goal, as evaluated by global reward signal. In contrast, defenders are responsible for both resisting jailbreak attempts and fulfilling their assigned tasks. Relying solely on the global reward, however, can introduce misaligned incentives for defenders: an individual agent may receive misleading feedback due to the behavior of others. For instance, when some agents generate unsafe responses but the aggregated system output remains benign. 5 To address this issue, we assign rewards based on both individual agents response and the final system output. Therefore, the rewards of defenders are evaluated at both the local response level and the global system level, as combination of task performance and safety compliance. All agents also receive formatting reward that enforces their outputs to put reasoning process between <think> and </think> and enclose final response with <response> and </response> tags. The overall reward is formulated as: γf αs s, if A, Rk = αs + βt + γf f, if D, (6) where s, t, and represent the rewards for safety, task utility, and format compliance respectively. For both safety and task performance, reward of 1 is assigned if the output is safe or correct, and 1 otherwise. For formatting, reward of 0.5 is given if the response satisfies the pre-defined structure, and 0.1 otherwise. In practice, we prioritize safety in the first half of training (αs = 1, βt = 0.5), and reverse the weights afterward to emphasize task performance."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Experiments cross 3 representative multi-agent attack scenarios and 3 task-specific benchmarks to assess its effectiveness in enhancing both safety and task utility. We first describe the experimental setup. Then we report results on red team attacks to demonstrate the robustness of our approach against adversarial threats. Next, we present task evaluations to assess the models general task performance. Finally, we conduct ablation studies to validate the design choices of AdvEvo-MARL."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Multi-agent systems. To ensure comprehensive evaluation under varying communication structures, we consider three representative system topologies in our experiments. (1) Chain mode: agents interact sequentially. Each agent can only observe the message from its immediate predecessor. (2) Tree mode: hierarchical structure where two child agents exchange messages and parent agent summarizes the communication history to produce final output. (3) Complete mode: fully connected topology where each agent can send and receive messages to and from all other agents. All experiments are conducted with three agents. Unless otherwise specified, we use QWen2.5 instruction-tuned models (3B and 7B) as the backbone foundation models. Attack methods. We choose three widely adopted attack strategies mainly focusing on jailbreak attacks and harmful information propagation within MAS. (1) NetSafe (Yu et al., 2024), alters agent behavior by injecting dark traits into profile configurations. One agent is randomly selected as malicious attacker in each episode. (2) AutoInject (tse Huang et al., 2025), randomly injects adversarial prompts into communication messages between agents. (3) UserHijack, manipulate user instructions to insert targeted adversarial content, simulating compromised user input. Baselines. We compare AdvEvo-MARL against several baseline methods. (1) Vanilla instructiontuned QWen2.5 series 3B and 7B models without safety training as backbone models. (2) Challenger, self-verification strategy where individual agent verify the benignness of its input before generating response. All agents are equipped with this self-checking mechanism in our setting. (3) Inspector, introduces external guard agents to detect and correct malicious messages. We only deploy one inspector agent to monitor all message flows during interaction. Datasets. In training, we sample 4,000 problems from levels 35 of MATH-500 dataset (Lightman et al., 2024) to serve as regular task prompts for defenders. In adversarial training, we use the described seed pool for attack rewriting. For system safety evaluation, we follow the original NetSafe protocol and adopt its official evaluation dataset. Meanwhile, we construct 300 adversarial prompts pool by sampling JailbreakBench, Wild Jailbreak, and Strong Reject. These prompts are used in both AutoInject and UserHijack settings. For general task evaluation, we select 3 prevailing benchmarks: (1) mathematical reasoning: AIME24 & AIME25 (AIME, 2025), challenging highschool mathematics requiring deep thinking and creative problem-solving, each containing 30 questions in total; (2) coding: LiveCodeBench (v6, 2025.01 - 2025.05) (Jain et al., 2024), collecting coding problems from live online platforms, providing realistic, dynamic, challenging environment 6 Table 1: Attack success rate (ASR) and contagion rate (CR) on NetSafe, AutoInject, and UserHijack attack scenarios across chain, tree, and complete graph topology systems. Lower ASR and CR indicate stronger robustness. Best-performing result is highlighted in bold and second-best is underlined. NetSafe AutoInject UserHijack AIME GPQA LiveCodeBench AIME GPQA LiveCodeBench ASR CR ASR CR 10.89% 11.88% 3.33% 0% 3.33% 0% 3.89% 3.33% ASR 3.03% 5.05% CR 3.7% 5.39% ASR 5.14% 1.14% CR 5.62% 1.14% ASR 15% 3.33% CR ASR CR ASR CR 15.56% 10.61% 6.79% 19.24% 17.76% 8.67% 2.29% 5.64% 7.78% 4.55% 15% 19.44% 19.7% 22.05% 11.88% 36.14% 21.78% 40.35% 13.33% 8.91% 17.57% 13.33% 16.39% 20.2% 20.54% 12.57% 15.81% 16.67% 19.43% 25.25% 28.74% 1.98% 3.96% 1.98% 16.57% 33.33% 37.78% 24.24% 32.59% 26.29% 35.27% 8.76% 25.58% 25.58% 17.68% 21.64% 22.29% 28.53% 21.65% 1.67% 9.44% 11.11% 18.25% 14.29% 16.44% 15% 8.33% 13.64% 14.93% 19.43% 21.14% 16.67% 17.69% 16.16% 12.35% 14.29% 18.24% 7.59% 16% 21.21% 21.63% 7.43% 2.57% 13.33% 10.66% 4.55% 2.23% 9.24% 2.72% 1.67% 8.33% 0% 3.28% 4.57% 6.93% 4.38% 3.03% 4.57% 3.54% 2.29% 24% 15% 0% 8% GPT-3.5 GPT-4o-mini Vanilla-3B Vanilla-7BChallenger-3b Inspector-3b Challenger-7b Inspector-7b AdvEvo-MARL-3B (ours) AdvEvo-MARL-7B (ours) 6.93% 35.64% 0.99% 19.14% 8.91% 0% 9.9% 0% 0% 0% 0% 10% 1.11% 1.11% 0.51% 1.85% 0.57% 0.19% 1.67% 0.56% 4.04% 3.03% 6.86% 8.29% 15.05% 6.29% 1.52% 2.19% 2.29% 8.33% 9.44% 7.07% 8.25% 2.29% 0% 2.5% 0% 0% 0.25% 1.71% 0.38% 3.43% 1.71% 1.57% 10% 16.67% 15.43% 24.57% 6.67% 2.92% 4.55% 2.15% 6.29% 3.71% 17.5% 9.6% 27.72% 41.34% 18.33% 13.75% 13.64% 9.34% 11.43% 10.29% 16.83% 26.98% 31.67% 22.5% 37.37% 22.6% 22.86% 18.29% 3% 22.77% 38.61% 3.33% 8.91% 13.86% 5.43% 10% 38.61% 51.49% 13.33% 7.92% 13.64% 9.09% 17.14% 10.29% 3.86% 8.91% 12.87% 3.33% 1.67% 9.58% 4.04% 3.54% 4.57% 4.57% 1.77% 4.04% 1.71% 3.79% 4.58% 4.04% 35% 35% 30% 10% 30% 5% 41.25% 33.84% 42.68% 29.71% 37.43% 33.33% 26.26% 28.03% 29.14% 36.67% 25.76% 40.4% 26.29% 37.86% 30.83% 30.3% 10.29% 28.86% 34.58% 24.75% 26.89% 21.71% 21.25% 10.61% 20.45% 10.29% 27% 22% 9.6% 29% Chain Tree GPT-3.5 GPT-4o-mini Vanilla-3B Vanilla-7B Challenger-3b Inspector-3b Challenger-7b Inspector-7b AdvEvo-MARL-3B (ours) AdvEvo-MARL-7B (ours) 1.98% 34.98% 6.89% 24.42% 1.67% 0% 0.56% 3.89% 1.01% 1.01% 2.86% 2.02% 1.71% 0% 2.29% 0.19% 8.33% 16.11% 4.04% 11.45% 9.25% 23.24% 7.24% 5.14% 9.9% 4.44% 5.05% 0% GPT-3.5 GPT-4o-mini Vanilla-3B Vanilla-7B Challenger-3B Inspector-3B Challenger-7B Inspector-7B"
        },
        {
            "title": "Complete",
            "content": "0% 21.53% 1.24% 0% 0% 0% 22.5% 0.42% 0.51% 0% 0.38% 1.14% 0.51% 0% 2.57% 26.67% 42.92% 18.18% 37.5% 34.86% 51.29% 0.83% 0.51% 1.89% 1.14% 2.43% 0.29% 0% 42.57% 54.7% 36.67% 71.11% 16.06% 37.27% 26.86% 42.57% 26.67% 53.33% 30.81% 56.82% 65.43% 33.66% 43.07% 33.33% 67.78% 31.31% 43.68% 25.14% 39.29% 28.33% 48.75% 29.29% 48.99% 34.86% 60.86% 58.33% 34.85% 65.53% 33.14% 57.29% 3.96% 27.23% 3.33% 30.83% 6.03% 26.89% 4.57% 18.14% 8.33% 48.75% 10.61% 50.25% 19.14% 53.14% 24.5% 30.69% 61.67% 7.58% 29.55% 1.14% 14.43% 36% 40% 0% 0% 22.28% 2.97% 17.82% 15% 5% 58.89% 22.73% 37.5% 15.43% 18.86% 38.33% 53.33% 33.84% 50.38% 29.71% 47.78% 5.57% 29.47% 17.29% 8.33% 41.25% 16.57% 37.63% 17.86% 39.57% 46% 4% AdvEvo-MARL-3B (ours) AdvEvo-MARL-7B (ours) 0.27% 0% 3.65% 2.58% 6.73% 50.42% 5.05% 26.26% 3.43% 13.14% 6.57% 3.33% 45.22% 7.07% 29.46% 1.14% 4% 47.22% 17.68% 33.7% 16.29% 34.86% 6.67% 36.11% 11.11% 28.48% 14.86% 40.48% for coding capability evaluation; (3) general reasoning: GPQA-diamond (Rein et al., 2024), 100 graduate-level Q&A problems encompassing physics, chemistry, biology and other scientific domains. Metrics. We employ three metrics to comprehensively evaluate both the robustness and utility of multi-agent systems. (1) Attack success rate (ASR): the proportion of evaluation samples where the system ultimately produces harmful response. (2) Contagion rate (PR): the ratio of agents that exhibit unsafe behaviors at any point during the interaction episode, reflecting the systems process-level safety. (3) Task performance: we adopt accuracy (Acc) for mathematical and general reasoning tasks, and Pass@1 for coding tasks."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Table 1 presents comprehensive comparison of ASR and CR across range of models, system topologies, and adversarial settings. Among all open-source baselines, AdvEvo-MARL consistently achieve the lowest ASR and CR across nearly all configurations, demonstrating superior robustness against adversarial compromise in multi-agent systems. Specifically, our models maintain ASR consistently below 10% in simpler topology systems such as chain and tree, and remain competitive even in the more challenging complete graph topology, with maximum ASR of 17.68%, where high interconnectivity greatly facilitates adversarial propagation. In contrast, other open-source baselines frequently fail to maintain low ASR across all topologies. For example, in the tree setting, some models experience up to 38.61% system-level compromise, and in the complete graph setup, ASR can rise as high as 65.53%. In certain cases, these models even underperform relative to their vanilla counterparts: under the UserHijack setting, Challenger7B reaches 38.33% ASR, 10% increase over its non-defended variant. Notably, in chain and tree topologies, our models achieve low or even near-zero ASR, often matching or outperforming proprietary models (e.g., GPT-4o-mini). Another key observation is that AdvEvo-MARL maintains low ASR while significantly suppressing CR even as the adversarial setting becomes more aggressive and the communication topology more interconnected. In contrast, many open-source defended baselines exhibit moderate ASR but much higher CR often ranging from 30% to even 60% in densely connected environments, suggesting insufficient coordination or internal consistency when faced with adversarial attack contagion. Yet our models strive to retain CR below 35% across all evaluation settings. This highlights AdvEvo-MARLs 7 Figure 3: Performance variations under different training configurations. Left: robustness performance, AdvEvo-MARL consistently maintains the lowest ASR, Right: task performance, AdvEvoMARL improves task utility across all settings, reaching maximum 4% gain on LiveCodeBench. superiority not only to improve individual agents safety awareness, but also to facilitate collaboration among agents to disrupt adversarial attack spread across the system. We further evaluate the impact of AdvEvo-MARL on the systems task capabilities across three representative benchmarks. Experimental results in fig. 2 show that our models retain strong task performance, with only maximum 3% accuracy drop observed among the 3B variants. Notably, the AdvEvo-MARL-7B model being trained exclusively on mathematical tasks, not only preserves its original task competence but even outperforms its vanilla counterpart across all datasets, especially those deemed as out-of-distribution. These findings provide clear evidence that safety-oriented training can be achieved without definitely sacrificing task ability. AdvEvo-MARL enables the development of agents that are both robust and performant, underscoring its potential as principled framework for building safe Figure 2: Task benchmark performance. AdvEvo-MARL exhibits minimal degradation and even improved results. yet capable multi-agent systems."
        },
        {
            "title": "5.3 DYNAMIC ATTACKS AND COLLABORATIVE DEFENSE",
            "content": "To evaluate the effectiveness of dynamic attacker modeling, we compare our MARL-based attacker framework with static attacker baseline. In the static setting, adversarial prompts are drawn from fixed pool without adaptation. In contrast, our method enables attackers to continuously generate and refine attack prompts through co-evolution with defenders. As shown in fig. 3, our MARL-based attacker achieves significantly lower ASR under NetSafe threat, revealing 12% reduction, indicating that defenders trained with evolving attackers exhibit superior robustness. In the AutoInject and UserHijack settings, AdvEvo-MARL also yields marginally lower ASR, suggesting consistent safety improvements across threat models. Evaluations on task datasets also reveal that AdvEvo-MARL outperforms the static attacker baseline across all settings, achieving maximum 4% performance gain. These results suggest that the presence of dynamic attackers can encourage defenders to develop generalizable task-solving capabilities, highlighting the dual benefits of AdvEvo-MARL for enhancing both safety and utility. We further investigate how our dynamic attacker evolves throughout the MARL training process. To quantify this progression, we measure the semantic similarity between generated attack prompts and all seed attacks to obtain diversity scores. Notably, as shown in Figure 4, the diversity of adversarial prompts generated by the attacker, reveals non-monotonic but ultimately increasing trend over the course of training. Despite fluctuations in early stages, the diversity steadily increases in the later phase, indicating that the attackers learns to produce increasingly varied and novel jailbreak attacks. This increased diversity coincides with enhanced robustness in the trained defenders which 8 suggests causal link. As attackers evolve and diversify, defenders are less likely to overfit and more capable of generalizing to previously unseen threats. These results underscore that training with dynamic attacker not only produces stronger adversarial prompts but also drives the emergence of more resilient and generalizable defense behaviors. Another interesting question is whether training defenders in MAS setting yields benefits over training them individually. Following the setup above, the empirical results in fig. 3 demonstrate that AdvEvo-MARL exhibits the highest system safety and task utility across all evaluated settings. Notably under the NetSafe scenario, our models achieve 12% gain in robustness and prominent 4% enhancement in task utility comparatively. These improvements can be attributed to the emergence of collaborative defense behaviors that arise only through joint training. Such coordination and mutual adaptation among agents are difficult to achieve when agents are trained in isolation. Figure 4: Attacker-generated prompts Diversity."
        },
        {
            "title": "5.4 PUBLIC BASELINE BASED MULTI-AGENT REINFORCEMENT LEARNING",
            "content": "(a) Evaluation accuracy. (b) Attack success rate. (c) Attacker Reward. (d) Defender Reward. (e) Response Length. Figure 5: Performance comparison of AdvEvo-MARL training with public baseline for advantage estimation (Baseline) and without using public baseline variant (No Baseline). In this section, we evaluate the effectiveness of our public baseline for advantage estimation in training. As shown in fig. 5, the included public baseline leads to more stable and efficient learning dynamics. Specifically, the public-baseline configuration consistently improves both task utility and system safety, as reflected by steadily improved accuracy and controlled ASR during training and evaluation  (fig. 3)  . In contrast, the standard training setup exhibits non-stationary behavior and even degraded performance in later stages. Moreover, we observe notable reduction in defender response length under the standard setup, approximately 13.3% drop during last 50 training steps, indicating tendency to produce shorter, less informative outputs. This behavior reflects defensive overcompensation aimed at minimizing risk, but at the cost of task completeness. Finally, as shown in the attackers reward trajectory, defenders trained with the public baseline are more effective in suppressing the attacker, leading to lower attacker rewards over time, and the defenders also achieve higher rewards via the entire training course. These results provide additional evidence that 9 public-baseline training fosters more robust and generalizable defense policies under adversarial pressure."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We propose AdvEvo-MARL, multi-agent safety training framework that enhances the robustness and utility of multi-agent systems through co-evolutionary reinforcement learning. By co-training attackers and defenders in dynamic adversarial environment, AdvEvo-MARL enables agents to continuously adapt to evolving threats, developing stronger and more generalizable defense capabilities. To facilitate collaborative learning and stabilize training, we introduce public baseline mechanism for advantage estimation, where agents within the same role group (e.g., attackers or defenders) share common baseline calculated from group-level returns. Extensive experiments across representative attack strategies and task benchmarks demonstrate that AdvEvo-MARL substantially improves system safety while preserving and even enhancing task performance. These results highlight AdvEvo-MARL as promising and unified framework for building safe and capable multi-agent systems."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "Our work aims to enhance safety and task utility of multi-agent systems by explicitly addressing their vulnerabilities through co-evolutionary training of attackers and defenders. We adopt proactive approach that surfaces how current MAS can be compromised and how defense capabilities can be internalized via reinforcement learning. While our methodology involves generating adversarial attacks, these are used solely for the purpose of strengthening defense mechanisms within multiagent systems. We believe that open, systematic study of such adversarial attacks is critical for the development of safe and resilient AI systems, and ensuring that MAS have broader positive social impacts."
        },
        {
            "title": "REFERENCES",
            "content": "AIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. Improving retrieval-augmented generation through multi-agent reinforcement learning. arXiv preprint arXiv:2501.15228, 2025. Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. Agentpoison: Red-teaming llm agents via poisoning memory or knowledge bases. Advances in Neural Information Processing Systems, 37:130185130213, 2024. Sahana Chennabasappa, Cyrus Nikolaidis, Daniel Song, David Molnar, Stephanie Ding, Shengye Wan, Spencer Whitman, Lauren Deason, Nicholas Doucette, Abraham Montilla, et al. Llamafirewall: An open source guardrail system for building secure ai agents. arXiv preprint arXiv:2505.03574, 2025. Falong Fan and Xi Li. Peerguard: Defending multi-agent systems against backdoor attacks through mutual reasoning. arXiv preprint arXiv:2505.11642, 2025. Florian Geissler, Karsten Roscher, and Mario Trapp. Concept-guided llm agents for human-ai safety codesign. In Proceedings of the AAAI Symposium Series, volume 3, pp. 100104, 2024. Pengfei He, Yupin Lin, Shen Dong, Han Xu, Yue Xing, and Hui Liu. Red-teaming llm multi-agent systems via communication attacks. arXiv preprint arXiv:2502.14847, 2025. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403. 07974. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Hongyue Kang, Xiaolin Chang, Jelena Miˇsic, Vojislav Miˇsic, Junchao Fan, and Yating Liu. Cooperative uav resource allocation and task offloading in hierarchical aerial computing systems: mappo-based approach. IEEE Internet of Things Journal, 10(12):1049710509, 2023. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=v8L0pN6EOi. Shuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652, 2025. 11 Zhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse, 2025. URL https://arxiv.org/abs/2503.18470. Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang, Han Liu, and Yongbin Li. Codev-bench: How do llms understand developer-centric code completion?, 2024a. URL https://arxiv.org/abs/2410.01353. Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Chain-of-action: Faithful and multimodal question answering through large language models. arXiv preprint arXiv:2403.17359, 2024b. Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. Conv-coa: Improving open-domain question answering in large language models via conversational chain-of-action, 2024c. URL https: //arxiv.org/abs/2405.17822. Zhenyu Pan, Xuefeng Song, Yunkun Wang, Rongyu Cao, Binhua Li, Yongbin Li, and Han Liu. Do code llms understand design patterns? In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. 209212. IEEE, 2025a. Zhenyu Pan, Yiting Zhang, Yutong Zhang, Jianshu Zhang, Haozheng Luo, Yuwei Han, Dennis Wu, Hong-Yu Chen, Philip S. Yu, Manling Li, and Han Liu. Evo-marl: Co-evolutionary multiagent reinforcement learning for internalized safety, 2025b. URL https://arxiv.org/abs/ 2508.03864. Chanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim. Maporl: Multi-agent post-co-training for collaborative large language models with reinforcement learning. arXiv preprint arXiv:2502.18439, 2025. Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in neural information processing systems, 33:1019910210, 2020. Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis. Trism for agentic ai: review of trust, risk, and security management in llm-based agentic multi-agent systems. arXiv preprint arXiv:2506.04133, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Erzhuo Shao, Yifang Wang, Yifan Qian, Zhenyu Pan, Han Liu, and Dashun Wang. Sciscigpt: Advancing human-ai collaboration in the science of science. arXiv preprint arXiv:2504.05559, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Jen tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Michael R. Lyu, and Maarten Sap. On the resilience of llm-based multi-agent collaboration with faulty agents, 2025. URL https://arxiv.org/abs/2408.00989. Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, et al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv preprint arXiv:2503.09501, 2025. Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, and Yang Wang. G-safeguard: topology-guided security lens and treatment on llm-based multi-agent systems. arXiv preprint arXiv:2502.11127, 2025. 12 Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Nathaniel Bastian, et al. Guardagent: Safeguard llm agents via knowledge-enabled reasoning. In ICML 2025 Workshop on Computer Use Agents, 2025. Miao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao, Chenlong Yin, Qijiong Liu, Qingsong Wen, Kun Wang, and Yang Wang. Netsafe: Exploring the topological safety of multi-agent networks, 2024. URL https://arxiv.org/abs/2410.15686. Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, and Yang Zhang. Breaking agents: Compromising autonomous llm agents through malfunction amplification. arXiv preprint arXiv:2407.20859, 2024. Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, and Jiaxuan You. Safescientist: Toward risk-aware scientific discoveries by llm agents. arXiv preprint arXiv:2505.23559, 2025."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Northwestern University",
        "University of Illinois at Chicago",
        "University of Rochester"
    ]
}