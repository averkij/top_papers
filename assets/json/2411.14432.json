{
    "paper_title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
    "authors": [
        "Yuhao Dong",
        "Zuyan Liu",
        "Hai-Long Sun",
        "Jingkang Yang",
        "Winston Hu",
        "Yongming Rao",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 2 3 4 4 1 . 1 1 4 2 : r Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models Yuhao Dong1*, Zuyan Liu2,3*, Hai-Long Sun2,4, Jingkang Yang1, Winston Hu2, Yongming Rao2,3, Ziwei Liu1 1 S-Lab, NTU 2 Tencent 3 Tsinghua University 4 Nanjing University https://github.com/dongyh20/Insight-V"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present InsightV, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design two-step pipeline with progressive strategy to generate sufficiently long and diverse reasoning paths and multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design multi-agent system consisting of reasoning agent dedicated to performing long-chain reasoning and summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agents generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks. 1. Introduction The development of artificial general intelligence requires models that can seamlessly understand and respond to multi- *Authors contributed equally to this research. Corresponding authors. Figure 1. Illustration and Performance of Insight-V. Insight-V consists of two agents, one dedicated to reasoning and the other to summarization, driving significant improvements in performance across various visual reasoning benchmarks. modal data. Recent advancements in Large Language Models (LLMs) [9, 36, 37, 41] and Multi-modal LLMs (MLLMs) [6, 2123, 29, 38, 49] have significantly facilitated this progress across various fields, ranging from common question-answering [6, 17, 22, 27, 38] to autonomous driving [32, 42] and robotics [8, 47]. Despite the substantial progress made in enhancing the performance of MLLMs on wide range of tasks, enabling MLLMs to perform humanlevel reasoning remains key challenge. This area remains underexplored and has yet to fully realize its potential. Existing efforts [44, 48] to enhance the reasoning capabilities of LLMs through long-chain reasoning have 1 demonstrated considerable progress, largely benefiting from the availability of structured, high-quality data and wellestablished training pipelines. In contrast, teaching MLLMs to perform long-chain visual reasoning remains significant challenge, primarily due to the lack of large-scale, highquality datasets and efficient and effective training strategies. Compared to text-only data, visual reasoning data is not only more expensive to collect but also requires significant human labor for detailed annotation and validation, due to the absence of an effective data generation pipeline. Moreover, while previous work [58] has demonstrated that directly applying chain-of-thought [44] reasoning can improve the capabilities of MLLMs, other research [56, 57] suggests that current training approaches have limited effectiveness in enhancing CoT reasoning. This highlights the inability of current MLLMs to leverage visual cues for precise step-bystep problem-solving, emphasizing the need for an effective training procedure that enables MLLMs to reason in detail while maintaining clear visual perception. To address these challenges, we propose Insight-V, which incorporates two innovative designs to enhance reasoning capabilities. First, we introduce data generation pipeline consisting of two key steps: progressive strategy to generate structured, long-chain reasoning data with diverse reasoning paths, and multi-granularity assessment system to evaluate and score these paths at different levels. Through automatic generation, assessment, and ranking strategies, the pipeline effectively operates without the need for human labor and makes the reasoning dataset more scalable for enhancing reasoning capabilities. To further improve MLLM reasoning beyond data scaling, we design multi-agent system, as illustrated in Figure 1, that decomposes the problem-solving process into two distinct steps: reasoning and summarization. The reasoning agent generates detailed reasoning process for the input query, while the summarization agent identifies key information within the reasoning process and selectively answers the question. To refine the quality of the reasoning, we employ an iterative DPO approach to enhance reasoning capabilities. The two agents collaborate to further improve the reasoning quality. Our findings demonstrate that this system significantly enhances the performance of various MLLMs across broad range of visual reasoning benchmarks. We evaluate Insight-V by integrating our system into the widely used LLaVA-NeXT [22] model. To further highlight the potential of our design in advancing state-of-the-art models, we construct robust base MLLM and apply our system to demonstrate its ability to enhance SOTA models across various visual reasoning benchmarks. Notably, InsightV improves LLaVA-NeXT in an average performance of 7.0% across seven challenging visual reasoning benchmarks, while 2.9% improvement is observed with the strong base MLLM, underscoring the effectiveness and generalizability of Insight-V. In summary, Insight-V offers 1) scalable data generation pipeline for long-chain, high-quality reasoning data, 2) multi-agent system that decomposes visual reasoning tasks into reasoning and summarization, and 3) two-stage training pipeline to enhance visual reasoning capabilities. Together, these contributions address key challenges in visual reasoning, providing solid foundation for future research in MLLM reasoning. 2. Related Work 2.1. Vision-Language Reasoning Recent advancements in MLLMs [1, 17, 2023, 27, 29, 38] have equipped these models with robust capabilities across diverse domains, including visual understanding [20, 38], mathematics [19], college-level questions [6], and scientific inquiries. In visual understanding, most research [18, 22, 28, 43, 46] emphasizes fine-grained detail analysis and localization, training models to perform visual reasoning with tailored datasets to enhance interpretive capabilities. For mathematics and expert-level reasoning, existing methods [11, 56, 57] predominantly derive from Chain-ofThought [44] approaches, training MLLMs to generate stepby-step reasoning across various subjects. However, these approaches often focus primarily on improving dataset quality through Chain-of-Thought, overlooking the importance of structured reasoning paths and extended reasoning chains in advancing model reasoning capabilities. Additionally, significant challenges arise when relying on single model to manage the entire reasoning process for complex tasks, underscoring the need for multi-agent system to decompose and enhance this process. In this work, we tackle these challenges by introducing scalable reasoning data generation pipeline and implementing multi-agent system for reasoning and summarization decomposition, enhancing the overall reasoning capabilities of existing MLLMs. 2.2. Vision-Language Alignment To align the model more closely with human preferences, several alignment techniques are employed for MLLMs. widely used approach is Reinforcement Learning from Human Feedback [2] (RLHF), which iteratively refines the models responses based on human feedback, enhancing both response quality and interpretability. To further improve MLLM capabilities, Direct Preference Optimization [39] (DPO) is introduced to simplify the alignment process. By directly training on human preference data, DPO optimizes the models outputs to better match human-selected responses. However, traditional DPO is primarily focused on offline scenarios, and as the model evolves, the effectiveness of this approach may significantly diminish. To address this, Iterative DPO [5] has been proposed, which optimizes pref2 erence pairs through DPO at each iteration. It then generates new preference pairs for the next iteration using the updated model and evaluates them with reward model. In this paper, we use iterative DPO to achieve stronger alignment and enhance the models reasoning capabilities. 3. Methodology In this section, we provide comprehensive description of the proposed Insight-V system, detailing its architecture and key contributions. Section 3.1 presents an overview of Insight-V, highlighting the core concepts of our approach. The design of Insight-V is structured around three primary components: 1) carefully constructed pipeline for structured, scalable reasoning data generation, as described in Section 3.2; 2) multi-agent MLLM system that supports complex visual reasoning, as detailed in Sections 3.3; and 3) streamlined yet effective training pipeline to enhance overall performance, as outlined in Section 3.4. Together, these components form cohesive system that effectively tackles the challenges of performing detailed, long-chain reasoning while preserving visual perception capabilities. 3.1. Overview Advancing reasoning capabilities of LLMs has been focal point of extensive research. Despite these efforts, the reasoning potential within multi-modal LLMs has barely been explored. Most approaches aim to strengthen reasoning at the inference stage, assuming the model has already acquired robust reasoning skills. Other approaches optimize model parameters using chain-of-thought data, enabling models to mimic human reasoning processes. However, these methods present significant challenges for current general-purpose MLLMs, as they require the model to develop reasoning skills while retaining prior capabilities, which often results in only modest performance gains. Additionally, the lack of structured, high-quality training data impedes training models with advanced reasoning capabilities. To fully leverage the reasoning capabilities of MLLMs, we propose Insight-V, novel system comprising two MLLMs dedicated to reasoning and summarization, respectively. The reasoning model is tasked with generating detailed reasoning process to assist in problem-solving, while the summary model evaluates this reasoning as supplementary information to assess its relevance and utility for answering the question. We also construct structured, high-quality dataset to train both agents. We posit that this multi-agent system can enhance the reasoning strengths of MLLMs by decomposing the problem-solving process into distinct reasoning and summarization phases, thereby driving substantial performance improvements. Figure 2. Data Generation Pipeline of Insight-V. The reasoning processes are generated progressively through reasoning generator, and then fed into multi-granularity assessment system to ensure high-quality reasoning. 3.2. Construction of Structured Reasoning Data Previous studies [56, 58] have explored the integration of reasoning capabilities into MLLMs. However, training MLLMs to develop robust reasoning skills remains considerable challenge, particularly due to data limitations. To address this, we introduce our data generation pipeline in this section, designed to produce high-quality, long-chain reasoning data using progressive generation process and multi-granularity assessment. As shown in Figure 2, this scalable approach enables us to generate high-quality data to enhance the models reasoning capabilities effectively. Progressive Long-Chain Reasoning Data Generation. For each input query, we first employ reasoning generator to produce structured reasoning process in JSON format to address the problem. At each step, the reasoning generator provides brief summary of the current step, detailed reasoning response, and an action for the following step. If the action is continue, the model proceeds with an additional reasoning step in the next iteration; if the action is summary, the model generates final summary and answer based on the complete reasoning process in the subsequent iteration. Specifically, For multi-modal model , input image I, and question Q, the data generation process for each step is represented as follows: Rt = (I, Q, [R1 Rt1], A), Rans = (I, Q, [R1 Rn]), where Rt and Rans denote the response at the t-th step and the final answer, respectively, Ri represents the reasoning generated by the model at the i-th step, represents the total reasoning steps, and is the action determined in the previous step. By repeating this process times, we can iteratively sample structured responses for each query. The generation parameters are adjusted to encourage the 3 model to produce outputs with various information and steps, allowing us to identify the most effective reasoning chain for each question. Multi-Granularity Assessment. After obtaining the structured responses, we utilize an assessment pipeline to ensure data quality. Specifically, we first apply strong LLM, such as Qwen2 [37], for direct answer filtering. As shown in Figure 2, the model is provided with the generated final answer and the ground truth answer and is tasked with determining whether the generated answer is correct, serving as an approximate indicator of the validity of the associated reasoning chain. Once responses with incorrect answers are filtered out, the remaining reasoning processes are passed to reasoning path scoring agent. Here, an advanced multimodal model, such as Qwen2-VL [38], is supplied with the image, question, reasoning path, and ground truth answer, and is prompted to evaluate the reasoning path. The scoring agent assesses each response based on the step-by-step accuracy of the reasoning path and the level of detail in the reasoning. To ensure consistency in scoring across different data samples, we aggregate all responses for each question and process them in single pass. The model then generates scores for each response, ranging from 1 to 100. Through the above two steps, we construct structured, high-quality dataset that provides detailed reasoning for each question, effectively supporting the training of our models. 3.3. Model Design After constructing the dataset, we develop multi-agent framework to enhance overall reasoning capabilities through collaborative agent interaction. Specifically, we first train reasoning agent to generate detailed reasoning process for each problem. Then, summary agent is employed to answer the question, selectively utilizing the reasoning process based on its assessment. Together, these two agents collaborate to improve reasoning performance effectively. Reasoning Agent. Previous approaches typically combine reasoning and question-answering within single process, which poses challenges for MLLMs. Generating long-chain reasoning process can introduce errors, and directly answering questions based on flawed reasoning often leads to poorer results. To address this, we propose specialized reasoning agent designed to generate detailed, step-by-step reasoning process in response to an input query. We construct the reasoning dataset by selecting the highestscoring reasoning path for each question. After training on this dataset, the model transforms into reasoning agent with enhanced reasoning capabilities, enabling it to generate more detailed, structured reasoning processes. Summary Agent. Summarization plays critical role in enabling models to accurately answer questions. After Figure 3. Overview of Insight-V Model Design. We derive multi-agent system from single model. By decomposing the task into reasoning and summarization, the two agents collaborate to enhance the overall reasoning capability. generating multi-step reasoning, summarization provides cohesive understanding of the reasoning process, ultimately guiding the model to the final answer. However, since the response generated by the reasoning agent may contain errors, we develop summarization model robust to inaccuracies in the reasoning path, selectively incorporating or disregarding elements as needed. This approach maximizes the reasoning models effectiveness while minimizing the risk of introducing misleading information. To enhance the robustness of the summary agent, we carefully curate its training dataset. We utilize the collected dataset, which is comprised of two types: data with optimal reasoning processes and data with flawed reasoning processes for the summarization task. This method prevents the model from simply copying reasoning outcomes and encourages critical evaluation of reasoning quality. To further promote critical analysis by the summary agent, we select flawed reasoning samples based on their performance scores. Specifically, we draw flawed responses from varying score ranges to create dataset with different levels of error, prompting the model to assess reasoning processes at various granularities. To better align the summary model with the reasoning agent, we also incorporate question-reasoning pairs generated by the reasoning agent to enhance collaboration between the two agents. Additionally, to preserve the original multi-modal capabilities, we supplement the dataset with standard question-answering data to sustain the summary agents performance in direct question-answering. 3.4. Training Pipeline The training pipeline for Insight-V is designed to be straightforward and efficient, utilizing two-stage strategy. For both the reasoning agent and the summary agent, we begin with well-trained MLLM. In the first stage, we apply supervised fine-tuning to train the agents to fulfill their designated roles using corresponding datasets. In the second stage, we 4 implement direct preference optimization, following prior research [45, 57]. This optimization is applied to the reasoning model, aligning it with human reasoning processes in simple yet effective manner. These two stages enable the development of robust system with enhanced visual reasoning capabilities. 3.4.1. Supervised Fine-tuning for Multi-agent System To perform supervised fine-tuning and obtain the two agents, we first train base multi-modal model, following established methodologies. This model can address general visual question-answering tasks and gain foundational visionlanguage skills. We compile high-quality image-text dataset focused on knowledge learning to train the base model. This data is sourced from various open-source academic datasets, including LLaVA-NeXT [22], Cauldron [15], and Cambrian-1 [43]. Once the base model is trained, we further fine-tune the two agents, initializing them from the base model. For the reasoning agent, we utilize the curated reasoning dataset to develop step-by-step reasoning capabilities. For the summary agent, we formulate dataset as outlined in Section 3.3 and sample about one million general image-text pairs from the dataset used for the base model, preserving its original visual perception abilities. 3.4.2. Enhanced Reasoning with RL Preference learning has gained increasing focus in the field of large language models. The primary aim is to finetune model outputs to align better with human (or expert) preferences, creating outputs more suited to real-world applications. Lets assume preference dataset defined as = {(x(i), y(i) )}i=1,...,D, where each x(i) is prompt, and y(i) and y(i) represent the preferred and less preferred responses, respectively. We denote yw yl to signify that yw is preferred over yl for prompt x. , y(i) Since the true distribution of human preferences cannot be directly observed, we approximate it with latent reward model r(x, y), assuming that higher rewards correspond to stronger preferences. Following the approach by Rafailov et al. [39], we can model the human preference distribution using the Bradley-Terry (BT) model [3]: p(y1 y2 x) = exp(r(x, y1)) exp(r(x, y1)) + exp(r(x, y2)) = σ(r(x, y1) r(x, y2)), where σ denotes the logistic function. To estimate the parameters of the reward model, we can apply maximum likelihood estimation by minimizing the negative log-likelihood: LR(rϕ, D) = E(x,yw,yl)D[log σ(rϕ(x, yw)rϕ(x, yl))], where rϕ is parameterized reward model. This approach allows us to approximate the preference distribution and finetune the model to capture human-like preferences effectively. 5 The traditional DPO algorithm operates in an offline setting. During DPO training, as model parameters continuously change, the preference dataset generated offline can gradually diverge from the models current distribution, which weakens the effectiveness of the DPO algorithm. To address this issue, we employ an iterative DPO algorithm. By conducting multiple rounds of DPO training and sampling, this approach enables the model to better approximate an online setting during training, thus further enhancing its performance. Specifically, our approach involves training sequence of models M1, . . . , MT , where each subsequent model Mt+1 utilizes preference data Dt generated by the t-th model. We apply this complete training process to the fine-tuned reasoning agent, enabling the model to better align with human preferences and produce structured, detailed reasoning steps for complex questions, which supports the summary agent more effectively. 4. Experiments We conduct extensive experiments across multiple visionlanguage benchmarks to validate the effectiveness of our method. In this section, we first introduce the implementation details of Insight-V in Section 4.1. Then we present comparison with state-of-the-art MLLMs, outlining the primary results of our method on visual reasoning tasks as well as additional results on general image understanding in Section 4.2. Moreover, we offer further analytical experiments and essential ablation studies on design choices in Section 4.3, along with qualitative results shown in Section 4.4 for more insights. 4.1. Implementation Details We integrate the Insight-V system with various MLLMs to demonstrate the broad applicability of our approach. Our initial implementation with Insight-V on LLaVA-NeXTLLaMA3 [22] illustrates the methods effectiveness. To further validate its generalizability and establish solid baseline against state-of-the-art MLLMs, we additionally train base multi-modal model using the Qwen-2.5-7B [41] LLM. During pretraining, we utilize the 558K captioning dataset from LLaVA-1.5 [21], unfreezing the connector parameters. This is followed by supervised fine-tuning with curated dataset of approximately 4 million images, using learning rate of 2e-5 as guided by prior researches [21, 22]. This two-stage training process equips the baseline model with essential visual perception abilities, achieving competitive results on vision-language benchmarks. We then initialize two agents from the baseline model, performing targeted fine-tuning to obtain the final agents. For the reasoning agent, we compile dataset of 200K images and train the model over 2 epochs with learning rate of 5e-6. For the summary agent, we use dataset of 1.2 million images, applying learning rate of 1e-5 and trainTable 1. Results on Visual Reasoning Tasks. We conduct evaluation experiments across 7 benchmarks, covering both general reasoning and task-specific reasoning assessments. Insight-V exhibits notable effectiveness and generalizability when applied to LLaVA-NeXT and our baseline model, surpassing other state-of-the-art MLLMs by large margin. Model Size MMMU MMMU-Pro MMBench MME ChartQA MMStar MathVista Average DeepSeek-VL [29] VILA-1.5 [20] Cambrian-1 [43] InternLM-XComposer2 [7] POINTS [26] IXC-2.5 [55] Bunny-LLaMA3 [12] MM-1.5 [54] MiniCPM-LLaMA3-V 2.5 [49] MiniCPM-V-2.6 [50] Qwen2-VL [38] Idefics3-LLaMA3 [14] Ovis1.5-LLaMA3 [31] LLaVA-NeXT-LLaMA3 [22] + Multi-Agent + Iterative DPO (Insight-V-LLaVA) Our Base Model + Multi-Agent + Iterative DPO (Insight-V) 7B 8B 8B 7B 7B 7B 8B 7B 8B 7B 7B 8B 8B 8B 8B 8B 7B 7B 7B 35.4 38.6 42.7 41.1 51.4 42.9 43.4 41.8 45.8 49.8 53.7 46.6 48.3 36.9 40.8 42.0 47.1 49.7 50. - - - - - - - - 19.6 27.2 - 22.9 23.6 13.2 17.8 21.0 22.6 23.8 24.9 73.5 75.3 75.9 77.6 78.0 79.4 77.2 - 77.2 78.0 81.0 77.5 76.6 72.3 77.6 81.7 81.3 82.2 82. -/- 1634.9/- 1547.1/- 2220.4 2184.1 2233.1 1588.9/321.1 1514.9/346.4 2024.6 2268.7 - 1937.4 1948.5 1611.1/346.0 1603.7/469.3 1583.9/485.4 1573.7/482.5 1662.2/629.3 1685.1/627.0 59.1 - 73.3 71.8 - 82.2 - 78.6 - - 83.0 74.8 76.4 69.4 74.6 77.4 75.7 81.2 81. 37.1 39.7 - 56.2 60.9 59.9 - - 51.8 57.5 60.7 55.9 57.3 43.1 52.6 57.4 57.0 58.6 61.5 36.1 - 49.0 59.5 63.0 63.7 34.4 47.6 54.3 60.6 61.4 58.4 63.0 45.9 47.4 49.8 56.9 58.7 59. - - - - - - - - - - - 48.1 49.4 40.2 44.5 47.2 (+7.0) 48.7 50.7 51.6 (+2.9) Table 2. Results on other multimodal benchmarks. InsightV enhances reasoning capabilities without compromising general visual perception and even achieves improvements on benchmarks requiring perception ability more. Table 3. Ablations on the Insight-V Design Choice. The multiagent design outperforms other configurations, highlighting the critical role of reasoning and summarization decomposition. Model TextVQA DocVQA OCRBench AI2D LLaVA-NeXT-LLaMA3 + Multi-Agent + Iterative DPO (Insight-V-LLaVA) Our Base Model + Multi-Agent + Iterative DPO (Insight-V) 65.2 68.9 70.5 75.4 77.0 76.8 78.2 81.8 82. 90.2 91.4 91.5 553 631 663 713 738 735 71.5 75.7 77.3 79.7 80.1 79.8 ing for 1 epoch. Additionally, we apply Direct Preference Optimization (DPO) to the reasoning agent, using approximately 15K preference data and training for 1 epoch at learning rate of 5e-7. This DPO process is iteratively conducted across 3 rounds by using the model from the previous stage to generate preference data, thus enhancing the agents reasoning capabilities. The lmms-eval [16] is utilized for fast evaluation. Further training details are provided in the appendix. 4.2. Main Results on Visual Reasoning Setup. We present comprehensive experimental results on various visual reasoning benchmarks, which require the model to handle complex and challenging questions, demonstrating strong reasoning capabilities. We select several wellknown and representative benchmarks, covering comprehensive evaluation, chart understanding, and mathematical problem-solving. MMMU [52] and MMMU-Pro [53] evaluate the models expert-level perception and reasoning abilities across various topics. MMBench [24] is widely used benchmark for comprehensive MLLM evaluation on images. MME [10] includes 14 challenging subtasks for assessing Model MMMU ChartQA MathVista MMStar Avg Baseline Vanilla - Direct SFT Multi-Turn Supervised Summary Agent Only Multi-Agent 47.1 47.0 48.1 47.5 49. 75.7 79.2 79.6 76.3 81.2 56.9 57.6 57.9 57.3 58.7 57.0 58.4 58.2 57.9 58.6 59.2 60.6 61.0 59.8 62.1 visual perception and cognition. ChartQA [33] focuses on logical reasoning with charts, while MathVista [30] assesses problem-solving skills in math, using GPT-4-0613 [35] as the evaluator, following standard practice. MMStar [4] tests range of tasks with varying difficulty levels. Main Results. The experimental results in Table 1 confirm the effectiveness and generalizability of Insight-V on visual reasoning tasks. Applied to LLaVA-NeXT and our baseline model, the Insight-V system substantially boosts performance across challenging visual reasoning benchmarks, achieving competitive outcomes against state-of-theart methods. We report an average improvement across all benchmarks: Insight-V yields an average increase of 4.9% and 3.2% when applied to LLaVA-NeXT and the base model, underscoring the systems effectiveness. Following Direct Preference Optimization, an additional improvement of 2.6% and 1.0% was observed, illustrating the value of reinforcement learning algorithms in enhancing reasoning capabilities. Specifically, on general reasoning benchmarks, MMMU, MMMU-Pro, and MMBench, we noted gains of 3.1%, 2.3%, and 1.0%, respectively. The highest improvement was on MME, with 9.1% increase, highlighting 6 Table 4. Ablations on the DPO training strategy. Iterative DPO progressively enhances the models reasoning capabilities, leading to improved performance. Model MMMU ChartQA MathVista MMStar Avg Insight-V (Multi-Agent) + RLAIF + DPO + Iterative DPO 49.7 49.5 50.8 50. 81.2 81.4 80.8 81.5 58.7 59.1 59.3 59.9 58.6 59.2 59.9 61.5 62.1 62.3 62.7 63.3 Effectiveness of Multi-agent System. To evaluate the effectiveness of the multi-agent system, we comprehensively compare Insight-V with alternative design choices. We begin by reporting the performance of the summary agent without reasoning process to underscore the importance of the reasoning agent. For comprehensive comparison, we also restructure the collected data using Chain-of-Thought (CoT) template and train model to perform sequential reasoning and subsequently answer questions. Additionally, we enable model that can handle multi-turn conversations, facilitating reasoning and summarization in multi-turn format. We use MMMU, ChartQA, MathVista, and MMStar as representative benchmarks to assess the performance across different methods. As shown in Table 3, the results indicate that the multiagent system plays critical role in enhancing the systems visual understanding capabilities. Using only the summary agent without reasoning process results in limited improvements in reasoning tasks, as the model lacks the necessary reasoning framework for optimal performance. Training the model to perform chain-of-thoughts reasoning, which is denoted as Insight-V (Vanilla-Direct SFT), yields modest gains, as this approach does not sufficiently emphasize critical judgments within the reasoning process, merging reasoning and summarization into single task. Furthermore, models trained with multi-turn conversations still produce sub-optimal results, underscoring the importance of multiagent system that separates reasoning and summarization. Data Scaling Law of Reasoning Agent. To assess the effectiveness of the reasoning agent, we perform ablation experiments on the data volume used for training. As shown in Figure 4, we compare reasoning agents trained on varying data sizes: 50K, 100K, 150K, and 200K samples. For fair and comprehensive comparison, we employ the same summary agent across evaluations and report results on six benchmarks. The findings clearly indicate that the reasoning agent benefits from increased data. With limited data, the reasoning agent struggles to generalize and fails to provide useful input to the summary model, resulting in performance even worse than baseline models. Conversely, training on larger datasets enhances the reasoning agents capabilities, enabling it to perform step-by-step reasoning and provide valuable insights that support the summary agent in solving Figure 4. Ablations on the amount of training data. The reasoning agent benefits from data scaling, providing more valuable insights for the summary agent. Insight-Vs strength in tasks requiring both perception and reasoning. On ChartQA, which entails complex chart-based reasoning, we achieved 5.8% increase. Performance gains on MMStar and MathVista, with improvements of 4.5% and 3.0%, further validate our approachs versatility in tackling math problems that demand sophisticated reasoning and numerical calculation skills. This strong performance across all benchmarks underscores the efficacy of Insight-V and its potential to advance visual reasoning capabilities. Results on Other Multi-modal Benchmarks. To verify that Insight-V enhances reasoning capabilities without compromising basic visual perception skills, we conduct experiments on benchmarks requiring only fundamental image understanding. We selected TextVQ [40], DocVQA [34], OCRBench [25], and AI2D [13] for this purpose, as these benchmarks focus on basic visual interpretation rather than complex reasoning. As shown in Table 2, integrating Insight-V enables models to maintain strong performance across general visual perception benchmarks. Additionally, applying Insight-V yields performance improvements on both LLaVA-NeXT and our base model, as it enhances general visual understanding by guiding the models attention to relevant regions, thereby improving visual perception accuracy. Following DPO, the model demonstrates advanced reasoning capabilities while achieving competitive results on standard image understanding benchmarks compared to baseline models, underscoring Insight-Vs robustness. Consistent results of both models further confirm Insight-Vs generalizability. 4.3. Further Analysis In this section, we present comprehensive experiments to validate the design choices of Insight-V, emphasizing our approachs key contributions. Additionally, we include case study to further demonstrate the qualitative effectiveness of Insight-V. 7 Figure 5. Qualitative Results of Insight-V. We present qualitative comparisons of Insight-V with Chain-of-Thought and learning Insight-V with direct SFT (Vanilla). For the Insight-V system, the reasoning agent delivers more coherent and structured reasoning process, guiding the summary agent toward the correct answer, whereas other methods struggle with complex reasoning tasks and fail to solve such challenging problems. tasks effectively. 4.4. Qualitative Results Effects of RL Algorithms. We assess DPO algorithms to identify optimal alignment strategies for enhancing the reasoning process. To examine the effects of dataset composition on DPO training, we compare our curated dataset against the widely-used RLAIF-V [51] dataset, which contains 80K DPO data pairs for alignment. We also investigate the potential benefits of iterative DPO in advancing the models reasoning capabilities. For an unbiased comparison, we subsample the RLAIF-V dataset to approximately 15K preference data points, aligning it with the size of our dataset. The results, summarized in Table 4, show an average improvement of approximately 0.2% on the evaluated benchmarks when trained with the RLAIF-V dataset, whereas our curated dataset achieves greater gains of 0.6%. This suggests that DPO dataset based on model-generated rationales, rather than externally sourced data, more effectively boosts reasoning accuracy. Moreover, conducting two additional rounds of DPO training using the same methodology yields further performance gains of 0.6%, underscoring that iterative DPO training progressively refines the models ability to generate precise and high-quality reasoning processes compared to single training pass. 8 We present qualitative comparisons in Figure 5 to illustrate the improvements introduced by integrating the Insight-V system. Specifically, we provide an example that highlights advancements in the reasoning process. In this example, the model is challenged with multiple-choice question based on table. We compare our approach with direct Chain-of-Thought application, and the model undergoes supervised fine-tuning without incorporating multi-agent system, which is denoted as Insight-V (Vanilla). It is evident that applying Chain-of-Thought directly results in suboptimal reasoning and leads to incorrect answers. Although fine-tuning single model yields somewhat better reasoning, the model begins to falter as the reasoning chain lengthens, ultimately arriving at incorrect answers. This outcome arises because the model is required to generate both reasoning steps and the final answer simultaneously, limiting its judgment capabilities and weakening its robustness in handling flawed reasoning chains. In contrast, employing Insight-V enables more logical step decomposition and structured reasoning chain. The model can analyze each option step-bystep and perform detailed calculations, which the other two methods cannot achieve. The fine-tuned summary model is able to evaluate the reasoning process and determine whether to derive the final answer based on it, significantly enhancing system robustness and ensuring correct answers. 5. Conclusion In this paper, we introduce Insight-V, novel system that combines scalable data generation pipeline for long-chain, high-quality reasoning data and an effective multi-agent training pipeline to enhance the reasoning capabilities of MLLMs. By developing this system, we provide scalable approach to model training aimed at improving reasoning performance. Our extensive evaluation across various benchmarks demonstrates the effectiveness of our approach, paving the way for equipping MLLMs with enhanced reasoning capabilities."
        },
        {
            "title": "Appendix",
            "content": "A. More Implementation Details In this section, we provide detailed explanation of the implementation of the DPO strategy. To collect preference data, we sample 16 outputs for each image-text pair to ensure diversity and maintain data quality. Each question, along with its ground truth answer and corresponding reasoning processes, is then presented to advanced LLMs such as Qwen2.5-72B. The model evaluates all reasoning paths in single forward pass, assigning scores to each. Reasoning paths with scores above 85 are selected as positive examples. To increase the tasks complexity, we do not use the lowestscoring reasoning path as the rejected example. Instead, we choose reasoning path with score around 25, ensuring that the DPO-trained model does not overfit specific data patterns. During DPO training, the parameter β is set to 0.1, and standard supervised fine-tuning loss is incorporated to stabilize the training process. B. Analysis Experiments of Multi-agent System To further validate the effectiveness of the proposed multiagent system, we conduct additional analysis experiments highlighting the superior performance of Insight-V. Insight-V Generates More Accurate Reasoning Paths and Demonstrates Robustness to Flawed Reasoning. To demonstrate that the summary model of Insight-V effectively evaluates the quality of reasoning paths and selectively answers questions based on these paths, we conduct analysis experiments on MMStar. These experiments highlight why Insight-V benefits from the integration of the summary agent, leading to improvements in performance. As illustrated in Figure 6, we compute the confusion matrix for the reasoning path and the final answer. reasoning path is classified as True Positive if both the reasoning path and the final answer are correct, represented in the bottom-right corner of the matrix. Conversely, if the reasoning path is incorrect but the final answer is correct, it is categorized as False Negative, shown in the upperleft corner of the matrix. The results clearly demonstrate that Insight-V generates more accurate reasoning paths, as depicted in the figure. Moreover, even when the reasoning path is incorrect, Insight-V is still capable of producing the correct final answer, showcasing its superior ability to selectively utilize reasoning paths compared to direct fine-tuning with Chain-of-Thought data. Figure 6. Analysis of Multi-agent System. Insight-V enhances reasoning capabilities while enabling the ability to selectively answer questions based on the provided reasoning process. C. Discussion and Limitations Insight-V represents an initial exploration into building models capable of o1-like reasoning. Our findings indicate that leveraging MLLMs to perform single-step reasoning and organizing these steps into structured, long-chain reasoning paths, is promising approach. After fine-tuning on this dataset, the model demonstrates the ability to perform longchain reasoning. Additionally, we implement multi-agent system to decompose the question-answering process into distinct reasoning and summarization stages, enabling the system to focus on reasoning while selectively incorporating its results into the summarization process. As an early attempt to develop robust reasoning models, we acknowledge several limitations that require future improvements to create systems capable of matching the performance of GPT-o1. First, enhancing sampling efficiency is critical. Currently, the process depends on other models for multi-granularity assessment, which could be made more efficient by evaluating reasoning results at each step and pruning redundant samples. This would streamline the system and improve its overall efficiency. Furthermore, training two models of the same size may not be scalable. Improving the reasoning agent could allow for training smaller, cost-effective summarization agent, as summarization is inherently less complex task than reasoning. This adjustment would not only reduce resource requirements but also improve the systems scalability. Improving overall reasoning quality is equally critical. This includes enabling the model to reflect on previous reasoning steps or implementing inference scaling strategies, 9 which can provide stronger foundation for the summary model to effectively answer questions. In conclusion, we hope our method serves as foundational attempt to inspire and guide future research in this emerging and exciting field."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. 2023. 2 [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 2 [3] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 5 [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. 6 [5] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. 2 [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. 1, [7] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 6 [8] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An arXiv preprint embodied multimodal language model. arXiv:2303.03378, 2023. 1 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6 [11] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [12] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal arXiv preprint learning from data-centric perspective. arXiv:2402.11530, 2024. 6 [13] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, pages 235251. Springer, 2016. 7 [14] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding vision-language arXiv preprint models: arXiv:2408.12637, 2024. 6 insights and future directions. [15] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 5 [16] Bo Li. Xinrun du, yuhao dong, haotian liu, yuanhan zhang, ge zhang, chunyuan li, and ziwei liu. Lmms-eval: Accelerating the development of large multimoal models, 2024. [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2 [18] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. 2 [19] Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang Zhang. Unimath: foundational and multimodal mathematical reasoner. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 71267133, 2023. 2 [20] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 2, 6 [21] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. 1, 5 [22] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 2, 5, [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. 1, 2 [24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 6 [25] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 7 10 [26] Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou. Improving your visionPoints: language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024. 6 [27] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialtemporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, [28] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 2 [29] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 1, 2, 6 [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6 [31] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. 6 [32] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. arXiv preprint arXiv:2312.00438, 2023. 1 [33] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [34] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 7 [35] OpenAI. Gpt-4 technical report. ArXiv:abs/2303.08774, 2023. 6 [36] OpenAI. Hello gpt-4o openai. OpenAI Blog, 2024. 1 [37] QwenTeam. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1, [38] QwenTeam. Qwen2-vl: To see the world more clearly. Wwen Blog, 2024. 1, 2, 4, 6 [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 2, 5 [40] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 7 [41] Qwen Team. Qwen2.5: party of foundation models, 2024. 1, [42] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 1 [43] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, 5, 6 [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 1, 2 [45] Tianyi Xiong, Bo Li, Dong Guo, Huizhuo Yuan, Quanquan Gu, and Chunyuan Li. Llava-onevision-chat: Improving chat with preference learning, 2024. 5 [46] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. arXiv preprint arXiv:2403.11703, 2024. 2 [47] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. arXiv preprint arXiv:2310.08588, 2023. [48] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. 1 [49] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint 2408.01800, 2024. 1, 6 [50] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6 [51] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 8 [52] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 6 [53] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao 11 Yu, Ge Zhang, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 6 [54] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 6 [55] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 6 [56] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 2, 3 [57] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. 2, 5 [58] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. 2,"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "S-Lab, NTU",
        "Tencent",
        "Tsinghua University"
    ]
}