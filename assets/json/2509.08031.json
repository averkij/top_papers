{
    "paper_title": "AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs",
    "authors": [
        "Sidharth Surapaneni",
        "Hoang Nguyen",
        "Jash Mehta",
        "Aman Tiwari",
        "Oluwanifemi Bamgbose",
        "Akshay Kalkunte",
        "Sai Rajeswar",
        "Sathwik Tejaswi Madhusudhan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce AU-Harness, an efficient and comprehensive evaluation framework for LALMs. Our system achieves a speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight a lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. AU-Harness provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 1 3 0 8 0 . 9 0 5 2 : r AU-HARNESS: AN OPEN-SOURCE TOOLKIT FOR HOLISTIC EVALUATION OF AUDIO-LLMS Sidharth Surapaneni, Hoang Nguyen, Jash Mehta, Aman Tiwari, Oluwanifemi Bamgbose, Akshay Kalkunte, Sai Rajeswar, Sathwik Tejaswi Madhusudhan ServiceNow University of Texas at Austin https://au-harness.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Audio Language Models (LALMs) are rapidly advancing, but evaluating them remains challenging due to inefficient toolkits that limit fair comparison and systematic assessment. Current frameworks suffer from three critical issues: slow processing that bottlenecks large-scale studies, inconsistent prompting that hurts reproducibility, and narrow task coverage that misses important audio reasoning capabilities. We introduce AU-Harness, an efficient and comprehensive evaluation framework for LALMs. Our system achieves speedup of up to 127% over existing toolkits through optimized batch processing and parallel execution, enabling large-scale evaluations previously impractical. We provide standardized prompting protocols and flexible configurations for fair model comparison across diverse scenarios. Additionally, we introduce two new evaluation categories: LLM-Adaptive Diarization for temporal audio understanding and Spoken Language Reasoning for complex audio-based cognitive tasks. Through evaluation across 380+ tasks, we reveal significant gaps in current LALMs, particularly in temporal understanding and complex spoken language reasoning tasks. Our findings also highlight lack of standardization in instruction modality existent across audio benchmarks, which can lead up performance differences up to 9.5 absolute points on the challenging complex instruction following downstream tasks. AU-Harness provides both practical evaluation tools and insights into model limitations, advancing systematic LALM development."
        },
        {
            "title": "INTRODUCTION",
            "content": "The emergence of Large Audio Language Models (LALMs) has opened new frontiers, extending capabilities beyond textual inputs to speech, sounds, and multimodal inputs Tang et al.; Cui et al. (2024). This progress has accelerated the development of frontier LALMs and audio-focused benchmarks. Recent multimodal LALMs like Gemini 2.5 Comanici et al. (2025), Qwen2.5-Omni Xu et al. (2025) have demonstrated substantial audio understanding capabilities well beyond the traditional Automatic Speech Recognition (ASR) tasks. However, despite these advances, evaluation toolkits have received comparatively little attention. The lack of efficient, customizable, and consistent evaluation frameworks for fair model comparison becomes increasingly problematic as audio tasks and model complexity continue to grow. Existing efforts including AIR-Bench Yang et al. (2024), AudioBench Wang et al. (2025a), KimiEval Ding et al. (2025), and DynamicSUPERB-2.0 Huang et al. (2024) have broadened task coverage from ASR to spoken question answering and scene understanding. However, prevailing toolkits still face three persistent limitations. First, throughput: many pipelines under-utilize batching and parallelism, creating bottlenecks that preclude large-scale, systematic comparisons. Second, reproducibility: ad-hoc prompting and non-standardized input formatting lead to evaluation variance across setups. Third, task scope: evaluations rarely probe prompted temporal understanding (e.g., diarization) or spoken reasoning with unified, reproducible protocols. Work done during internship at ServiceNow GitHub Repository: https://github.com/ServiceNow/AU-Harness 1 Most current evaluation frameworks depend on simplistic yet inefficient input processing pipelines that struggle to scale with the increasing volume and complexity of audio benchmarks and LALMs. These limitations not only constrain the throughput of large-scale evaluations but also hinder fair and reproducible comparisons across models of different sizes and architectures. As the field progresses toward more diverse and challenging audio tasks, the shortcomings of current evaluation infrastructure may pose critical bottleneck, ultimately hampering the potential progress of LALMs. Unlike previous evaluation frameworks, we introduce an efficient vLLM batching orchestration together with effective data sharding to scale the evaluations across multiple nodes and hardware architectures, leading to improved efficiency for audio benchmark evaluations. Beyond computational efficiency, evaluation toolkits often suffer from notable lack of customizable configurations for different audio task configurations, severely limiting their utility for diverse research and application needs. Insufficient attention to task-specific prompting remains significant challenge for LALM evaluation and comparison across different benchmarks. Prompt sensitivity, where LALMs outcomes might significantly change when slight variations in prompt phrasing, has been widely acknowledged across audio understanding tasks Cui et al. (2024). In addition, audio benchmarks and evaluation kits remain restricted to spoken language understanding. To be on par with text-based language modeling evaluation, we construct and integrate the spoken language reasoning tasks with our evaluation kit for comprehensive audio-to-text generation support. In addition, we also provide the support for LLM-adaptive diarization evaluation where LLM prompting results in different I/O. To the best of our understanding, our proposed evaluation kit is among the first to introduce the complex audio reasoning tasks and support LLM-Adaptive Diarization evaluations. Our contributions are as follows: We propose an efficient evaluation engine that leverages vLLM batching and dataset sharding to scale evaluations to multi-node infrastructures without sacrificing fidelity. unified, configurable framework that standardizes prompting and metrics across benchmarks, enabling fair, reproducible comparisons and easy task integration. Expanded evaluation coverage with LLM-Adaptive Diarization and Spoken Language Reasoning to assess temporal grounding and audio-conditioned reasoning in LALMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Audio Benchmarks Benchmarks play critical role in the development of audio language models. SUPERB Yang et al. (2021) established core task axes (Content, Speaker, Semantics, Paralinguistics) for audio model evaluation. DynamicSUPERB Huang et al. (2024) and DynamicSUPERB2.0 Huang et al. expanded coverage to instruction-tuned and sequence generation tasks across speech, music, and environmental audio. Instruction-following and agentic behaviors have been probed by AIR-Bench Yang et al. (2024) and VoiceBench Chen et al. (2024). More recently, AudioBench Wang et al. (2025a) unified eight task families over 26 datasets for AudioLLMs. Complementary 2025 efforts broaden breadth and depth: X-ARES Zhang et al. (2025) systematically assesses general audio encoders across domains, AHELM ahe (2025) aggregates multi-aspect evaluation for audio-language models (reasoning, robustness, safety, multilinguality), MECAT Niu et al. (2025) targets fine-grained, multi-perspective audio understanding with expert-guided captions and QA. CodecBench Wang et al. (2025b) benchmarks codecs from acoustic and semantic perspectives and few evaluate prompted diarization or spoken reasoning with reproducible protocols. While comprehensive, these benchmarks provide limited standardization around prompting and reasoning diagnostics central to LALMs. Many of these still fall short in evaluating the audio reasoning capability and adapting evaluations to LLM prompting approaches that are comparable with text-based LMs, motivating the development of our proposed Spoken Language Reasoning and LLM-Adaptive Diarization evaluation tasks. Audio Evaluation Kits In parallel with Audio Benchmark development, Audio Evaluation Kits have received less attention. This can be primarily attributed to the straightforward nature and minimal setup requirements of the early audio tasks, as presented in Huang et al. (2024) and Yang et al. (2024). However, the rapid growth of LALMs and the increasing complexity of newly curated audio 2 benchmarks have underscored the critical need for comprehensive evaluation kits, as exemplified through the development of extensive evaluation kits Ding et al. (2025); Wang et al. (2025a); Chen et al. (2024). For instance, AudioBench Wang et al. (2025a) offers versatile evaluation support for up to 8 tasks across 26 datasets. VERSA Shi et al. (2025) introduces comprehensive framework to evaluate the quality of various speech, audio and music signals, with the focus on text-to-audio applications. Despite these advancements, most current evaluation kits operate on the simplified assumption that single model is evaluated against single benchmark per run. Addressing this limitation, we introduce an efficient, customizable evaluation kit to support the massive growth of the current LALMs and audio benchmarks."
        },
        {
            "title": "3 LALM EVALUATION CHALLENGES",
            "content": "EvalKit vLLM support Multi-turn LLM-Adaptive Diarization AudioBench Kimi-Eval VoiceBench DynamicSUPERB-2.0 AU-Harness Spoken Language Reasoning Table 1: Feature comparison of contemporary LALM evaluation toolkits. We evaluate key technical capabilities across existing frameworks: vLLM integration for efficient batching, multi-turn dialogue support for conversational scenarios, LLM-Adaptive Diarization for temporal understanding through prompting, and Spoken Language Reasoning for complex audio-conditioned cognitive tasks. Our framework is the first to provide comprehensive support across all dimensions. EvalKit RTF () Processed Samples per Second () AudioBench Kimi-Eval VoiceBench AU-Harness 19.9 7.1 87.9 3.6 (48.75%) 0.66 1.87 0.15 3.65 ( 95.19%) Table 2: Throughput efficiency comparison across LALM evaluation frameworks. Results averaged over 500 samples from LibriSpeech-test-clean (1.05 hours total audio). Real-time Factor (RTF, better) measures processing time relative to audio duration. Processed Samples per Second ( better) quantifies raw throughput. Our framework achieves 48.75% RTF reduction and 95.19% throughput increase over the best competing baseline, demonstrating substantial efficiency gains through vLLM integration and request orchestration. 3.1 INFERENCE EFFICIENCY Existing LALM evaluation kits have been designed based on the oversimplistic assumption that single model should be evaluated against single benchmark per run. This constraint has prevented researchers from conducting systematic, large-scale comparisons across LALMs and audio benchmarks efficiently, slowing the iterative process of model development and refinement. The current evaluation kits also under-utilize parallel processing capabilities available in the high-performance computing clusters, resulting in failures in incorporating benefits of available hardware infrastructures. Two essential task-agnostic metrics for evaluating the efficiency of LALM evaluation frameworks are Real-time Factor (RTF) and Processed Samples per Second. RTF measures the processing time of an evaluation framework relative to the duration of the processed audio Arriaga et al. (2024). The lower RTF is more desirable, indicating more efficient audio evaluation framework. On the other hand, Processed Samples per Second directly quantifies the models processing speed by measuring the average number of audio samples processed per second. It serves as complementary measure to RTF, providing more granular view of the models throughput and computational efficiency. To quantify the efficiency of existing evaluation frameworks, we conduct study on 500 audio samples (approximately 1.05 hours) of Librispeech-test-clean. As observed in Table 2, existing 3 audio evaluation kits exhibit high RTF and slow sample processing speed. As the number of samples continues to increase with more diverse datasets, this challenge can significantly slow down the inference progress."
        },
        {
            "title": "3.2 CUSTOMIZABLE EVALUATION CONFIGURATIONS",
            "content": "Despite the strong support for various tasks and LALMs, current LALM evaluation kits provide insufficient customizations for evaluation configurations. Multi-turn Dialogue Support Previous audio evaluation toolkits have largely been constrained to tasks centered on single-turn user interactions. However, as the field moves toward building interactive and context-aware voice assistants, the ability to evaluate multi-turn tasks becomes increasingly critical. Multi-turn evaluation enables more realistic assessment of dialogue continuity, contextual reasoning, and the models capacity to adapt dynamically across extended conversations. Without such support, current evaluation approaches risk overlooking key aspects of usability and robustness that are essential for next-generation LALMs in the realistic agentic voice systems. Customizable Filtering The lack of customizable filtering poses significant barrier for researchers aiming to conduct in-depth analyses of current LALM limitations. Without the ability to refine evaluation datasets based on specific criteria, it is challenging to gain granular understanding of model performance across diverse audio conditions. For instance, while certain LALMs might perform reliably on 10-second audio chunks, they might be unable to handle short-form audios typically encountered in dialogue-state tracking systems. Task Hierarchical Structure & Task-Metric Aggregation While DynamicSUPERB-2.0 Huang et al. (2024) provides comprehensive set of tasks (up to 180 tasks), it lacks mechanisms for categorizing and conducting targeted evaluation runs on specific task categories. This limitation reduces its practical value for researchers and practitioners aiming to benchmark or improve LALMs capabilities on targeted task categories. Table 3: Comprehensive task coverage analysis across audio evaluation benchmarks. We systematically compare task support across major frameworks spanning 20212025, organized by six core categories: Speech Recognition, Paralinguistics, Audio Understanding, Spoken Language Understanding, Spoken Language Reasoning, and Safety & Security. Our framework provides the most comprehensive coverage, uniquely supporting LLM-Adaptive Diarization and novel Spoken Language Reasoning tasks (Speech Function Calling, Speech-to-Coding) absent from prior work. Task Category Task Name Speech Recognition Paralinguistics Audio Understanding Spoken Language Understanding Spoken Language Reasoning Safety & Security ASR Code-switching ASR Long-form ASR Emotion Recognition Gender Recognition Accent Recognition Speaker Recognition Speaker Diarization Music Understanding Scene Understanding Speech QA Spoken Query QA Speech Translation Dialogue Summarization Intent Classification Speech Function Calling Speech-to-Coding Speech Instruction Following Safety Spoofing SUPERB (2021) DynamicSUPERB (2024) VoiceBench (2024) AIR-Bench (2024) AudioBench (2025) DynamicSUPERB-2.0 (2025) Ours 3.3 COMPREHENSIVE TASK CATEGORY COVERAGE As demonstrated in Table 3, despite the wide coverage of tasks, previous benchmarks fail to support more complex audio reasoning and fine-grained diarization tasks. LLM-Adaptive Diarization key limitation of prior evaluation kits is the lack of support for diarization tasks adapted to the prompting-focused capabilities and requirements of contemporary 4 Figure 1: Architecture overview of AU-Harness evaluation framework. Our system comprises three core components: (1) Config module for hierarchical task configuration and standardized prompting, (2) Request Controller managing token-based concurrency limits across all engines with adaptive retry mechanisms, and (3) Concurrent Engines executing parallel model evaluation with dataset sharding. The Request Controller maintains global token pool accessible to all engines, enabling efficient resource utilization and scalable throughput. Multiple concurrent connections between the controller and inference models illustrate parallel request dispatch, with each engine supporting multi-model evaluation on targeted datasets. . LALMs. To address this gap, we define LLM-adaptive Diarization as class of tasks aimed at identifying what is spoken and when given continuous audio inputs purely through prompting rather than neural modeling. Unlike conventional audio understanding tasks, these tasks require models to segment the audio streams and localize the timing of specific information with them. Exemplars include speaker diarization Anguera et al. (2012) and emotion diarization Wang et al. (2023), both of which demand precise timestamp predictions for accurate evaluation. In the context of LALMs, this poses additional challenges, particularly regarding the precision of temporal predictions an issue frequently observed in text-based LLMs Feng et al. (2025). As result, LLM-Adaptive Diarization calls for the development of specialized prompting strategies and adaptive evaluation metrics tailored to the unique characteristics of LALMs beyond the traditional widely adopted Diarization Error Rate metric Galibert (2013). Spoken Language Reasoning Existing benchmarks remain largely centered on the audio understanding tasks, with limited emphasis on tasks requiring deeper cognitive and reasoning abilities Peng et al. (2024). Following Natural Language Processing (NLP) community, we define Spoken Language Reasoning tasks as those that involve integrating information from multiple sources to derive new conclusions without relying solely on models memorization, knowledge-based storage and provided context Yu et al. (2024). Unlike traditional reasoning tasks in NLP, the key distinction here is that both the knowledge and instructions may be conveyed through audio, not just text."
        },
        {
            "title": "4 AU-HARNESS",
            "content": "In response to the challenges existing in current audio understanding evaluation toolkits, we propose our standardized, efficient, highly customizable evaluation framework, AU-Harness, as detailed in Figure 1. Our AU-Harness is composed of 3 primary components, including: Config, Request Controller and Concurrent Engines. The Config module defines structured and hierarchical representation of customizable configurations, enabling flexible and transparent evaluation settings. The Request 5 Figure 2: Task distribution and coverage in AU-Harness. Our framework encompasses six major task categories with balanced representation: Speech Recognition (ASR variants), Paralinguistics (emotion, speaker, accent recognition), Spoken Language Understanding (QA, translation, summarization), Audio Understanding (scene, music), Spoken Language Reasoning (function calling, coding, instruction following), and Safety & Security (robustness, spoofing detection). The distribution reflects comprehensive coverage from basic perception to complex reasoning, with novel emphasis on prompted temporal understanding and audio-conditioned cognitive tasks. Controller is responsible for managing token requests and coordinating execution across the framework. Finally, the Concurrent Engines module carries out task-specific evaluations in parallel, where each engine can support multi-model evaluation tailored to particular tasks. In the following sections, we introduce our architecture design in more detail to address the aforementioned challenges in Section 3. 4. INFERENCE EFFICIENCY As illustrated in Figure 1, our proposed AU-Harness maximizes inference efficiency through tokenbased request scheduling architecture. More specifically, we introduce Central Request Controller that maintains and regulates pool of available tokens accessible to all models across all evaluation engines. Here, token refers to concurrency slot representing permission to issue one inference request (not model input token), which is acquired before dispatch and released upon completion. Each concurrent engine-specific requester periodically draws from the global pool. Within each of these engines, multiple models are executed concurrently on targeted dataset, with inference calls dispatched in parallel to fully exploit available computational resources. This architecture ensures that evaluation throughput is not bottlenecked by model or engine-specific constraints, but rather governed solely by user-defined request limits set globally, providing both scalability and predictable performance guarantees. Furthermore, we allow user-specified retry counts on request errors, enabling users to set higher request limits with the assurance that occasional failures will be re-tried and successfully completed, thereby offering tunable balance between throughput and reliability. 6 Furthermore, our AU-Harness implements layered request synchronization strategy that adaptively staggers request wait times across concurrent models. This design increases the probability that all models processing given dataset segment complete their inference in temporally aligned manner. By reducing discrepancies in model response times, the strategy minimizes idle periods within each engine, thereby mitigating intra-engine waiting time and improving overall throughput efficiency. We further implement dataset sharding, which partitions the evaluation dataset into disjoint subsets that can be processed in parallel across multiple model endpoints. To maximize efficiency, sharding is performed proportionally to each endpoints capacity for concurrent requests, ensuring balanced utilization of heterogeneous resources. This enables near-linear scaling of inference throughput, effectively distributing the computational workload and minimizing bottlenecks. Finally, our native integration with vllm leverages range of inference-level optimizations, further accelerating model execution and overall evaluation system."
        },
        {
            "title": "4.2 CUSTOMIZABLE EVALUATION CONFIGURATIONS",
            "content": "AU-Harness is highly customizable with structured task coverage as presented in Figure 2, from dataset usage to inference to evaluations. Multi-turn Dialogue support Through synchronous, turn-based evaluation chains, with outputs added to context, we are able to evaluate multi-turn audio and text datasets across LALMs. The simplicity and conciseness of code allows for future contributions for more complex and custom multi-turn tasks as well. Customizable Filtering Great effort has been made into making AU-Harness as customizable as possible, while still being intuitive to use. First, any number of open and closed source models, across any number of datasets, can be run. Each model can have its own specific temperature and max-token settings, which override (also customizable) task-specific temperature settings. Each model endpoint also has specific run specifications that can be changed, such as concurrent allowed requests, error retry limit, timeout before retry, and audio chunk size. This is all in order to maxmize resource utilization and minimize evaluation time. Evaluation customization AU-Harness framework is also designed for granular control over evaluation steps by allowing for customizable metric assignment on per-dataset and per-task basis. For instance, LLM-as-judge supports configurable concurrency to maximize the throughput for evaluation stage. For more comprehensive understanding of model performance, the framework offers configurable aggregation metrics. This capability allows for the multi-dimensional analysis of task and metric results, providing holistic view that extends beyond simple, individual scores or subtasks."
        },
        {
            "title": "4.3 COMPREHENSIVE TASK CATEGORY COVERAGE",
            "content": "LLM-Adaptive Diarization Following Wang et al. (2024), we adapt diarization tasks as special category of ASR. More specifically, to alleviate the potential issues of (1) precise temporal predictions and (2) timing mismatch of ASR and Diarization systems, we incorporate the speaker information into the transcripts and prompt LLMs to generate the ASR hypotheses. The generated hypotheses are then post-processed and evaluated on the word-level via Word-diarization Error Rate (WDER) Shafey et al. (2019) and concatenated minimum-permutation word error rate (cpWER) Watanabe et al. (2020). Further details of the difference are provided in Figure 3. Spoken Language Reasoning Derived from text-based reasoning tasks, we introduce three novel audio-based reasoning tasks by converting the audio instructions into audio context via Text-toSpeech (TTS) system. Our work centers on 3 major reasoning tasks, including: Speech Function Calling: Speech Function Calling aims to assess the LALMs comprehension of spoken instructions and their ability to map spoken natural language queries into structured, executable function calls with appropriate arguments. We achieve the goal 1Figure is adapted from NeMo documentation 7 Figure 3: LLM-Adaptive Diarization methodology comparison. 1 Traditional diarization (top, bottom-right) outputs time-stamped audio segments with speaker annotations, ideal for specialized neural architectures. LLM-Adaptive approach (bottom-left) integrates speaker information directly into transcripts, enabling evaluation through prompting-based generation evaluated via word-level metrics (WDER, cpWER). This approach leverages LALMs inherent language modeling capabilities while addressing temporal precision challenges through specialized evaluation protocols. by expanding BFCL-v3 Patil et al. (2024) by systematically converting textual instructions into spoken counterparts. Speech-to-Coding: Speech-to-Coding evaluates LALMs capability to translate spoken instructions into formal programming language. Adapted from the renowned Spider textto-SQL benchmark Yu et al. (2018), we construct the Speech-Spider benchmark where LALMs are expected to convert spoken instructions into valid SQL commands. Speech Instruction Following: Proficiency in interpreting and executing intricate, potentially multi-step audio instructions is critical skill for LALMs. To evaluate this capability, we develop Speech-IFEval and Speech-MTBench benchmarks, deriving from the well-know text-based IFEval Zhou et al. (2023) and MTBench Zheng et al. (2023). Table 4: Comprehensive LALM performance across diverse audio understanding tasks. We evaluate three representative models: Voxtral-Mini-3B, Qwen2.5-Omni-7B, and GPT-4oacross 19 tasks spanning Speech Recognition, Paralinguistics, Spoken Language Understanding, Audio Understanding, Spoken Language Reasoning, and Safety & Security. Metrics include standard benchmarks (WER, BLEU) and LLM-as-judge evaluations using GPT-4o-mini. Results reveal significant capability gaps, particularly in temporal reasoning (diarization) and complex instruction-following scenarios. *Performance affected by Azure OpenAI content filtering. Task Category Speech Recognition Paralinguistics Spoken Language Understanding Audio Understanding Spoken Language Reasoning Safety and Security Task Name ASR Emotion Gender Accent Speaker Recognition Speaker Diarization Spoken QA Spoken Query QA Speech Translation Spoken Dialogue Summarization Intent Classification Scene Understanding Music Understanding Speech Function Calling Speech-to-Coding Speech Instruction Following Speech Instruction Following Safety Spoofing Dataset Librispeech MELD IEMOCAP VoxCeleb mmau mini CallHome public sg BigBench Audio Covost2(zh-CN EN) mnsc sds (P3) SLURP audiocaps qa mu chomusic test Speech-BFCL Speech-Spider Speech-MTBench IFEval advbench avspoof Metric WER llm judge binary llm judge binary llm judge binary llm judge binary WDER llm judge detailed llm judge big bench audio BLEU llm judge detailed llm judge binary llm judge detailed llm judge binary bfcl match score sql score (EM) llm judge mt bench instruction following score redteaming judge llm judge binary Num samples Voxtral-Mini-3B Qwen2.5-Omni-7B GPT-4o 2617 2610 1003 4818 1000 112 688 1000 4898 100 200 313 1187 1240 1001 80 345 520 200 2.1 28.4 54.9 13 45.8 35.38 62.12 43.5 15.27 52.2 42.5 14.96 45.4 78.5 30.17 64.12 38.06 78.5 91.5 1.74 49.8 85.8 28.7 62.3 35.40 69.4 53.8 28.41 52 57 38.4 59.3 68 38.46 62.88 50.83 98.3 30 6.25 20.2 0* 0* 42 37.12 70.2 65 21.68 61.2 48 15.08 50.2 86.65 45.15 62.44 72.15 88.1 0*"
        },
        {
            "title": "5 RESULTS & DISCUSSION",
            "content": "Empirical evaluations across all task categories using our proposed AU-Harness are provided in Table 4. We use GPT-4o-mini as judge for LLM-judge metrics due to its advanced capability. 5."
        },
        {
            "title": "INFERENCE EFFICIENCY",
            "content": "Table 5: Experimental setup for efficiency comparison across evaluation frameworks. We conduct controlled experiments using 500 samples from three diverse datasets: MELD-Emotion (short emotional speech), LibriSpeech-clean (medium-length read speech), and ClothoAQA (long-form descriptive audio). Total audio duration varies from 1,476 to 11,376 seconds, enabling assessment across different audio characteristics and evaluation modalities (LLM-judge vs. traditional metrics). MELD-Emotion Librispeech-clean ClothoAQA # Samples Audio Duration (seconds) Evaluation Metric 500 1,476 LLM-Judge 500 3,780 WER 500 11,376 LLM-Judge Evaluation Settings We perform empirical evaluation to compare AU-Harness against existing evaluation kits: AudioBench Wang et al. (2025a), VoiceBench Chen et al. (2024), and KimiEval Ding et al. (2025). Our analysis focuses on the two key metrics RTF and Processed Samples per Second detailed in Section 3.1. We leverage 500 audio samples from 3 diverse datasets: librispeech-clean-test, ClothoAQA, and MELD-Emotion. The evaluation is conducted on three different LALMs, including: Qwen2.5-Omni-7B Xu et al. (2025), Phi-4-Multimodal Abouelenin et al. (2025) and Voxtral-Mini-3B Liu et al. (2025). For conciseness, we report the averaged metric across all 3 LALMs. More specific details are further provided in Table 5. To provide comprehensive and fair comparison with other evaluation kits, regardless of their underlying implementation, we introduce two additional runtime scenarios beyond individual dataset runtimes, namely Sequential and Parallel. First, Sequential runtime represents the most inefficient runtime by assuming each benchmark is executed in sequential manner, where no data or model parallelization algorithms are introduced. On the other hand, Parallel presents the theoretical upperbound for optimal runtime. The final runtime is calculated by taking the longest runtime among all evaluated datasets. This scenario presumes an ideal, zero-overhead parallelization environment where communication protocols among parallel processes and other overheads do not impact the runtime. This is considered best-case runtime for our framework and existing evaluation kits across all presented datasets and models. Evaluation Comparison As shown in Figure 4, AU-Harness consistently outperforms existing evaluation kits across all runtime scenarios in two key efficiency metrics. More specifically, our AUHarness achieves up to 127% improvement in Processed Samples per Second and 59% reduction in RTFs compared to the next most competitive evaluation frameworks. More importantly, our Parallel runtime, illustrated in Figure 5, is significantly more efficient than competing frameworks. These empirical results validate our framework as highly efficient tool for LALM evaluation. 5.2 INSTRUCTION MODALITY GAP Table 6: Empirical evaluations to assess the impact of different instruction modalities (Audio and Text) on Spoken Language Reasoning tasks. We conducted investigations on the modality instruction differences across 4 reasoning benchmarks including Speech-IFEval, Speech-BFCL, Speech-Spider and Specch-MTBench. Our empirical study reveals the significant performance gap between audio and text instructions, highlighting the need for more thorough investigation when instruction-following benchmarks are converted from text to audio. Instruction Modality Text Audio Speech-IFEval 45.20 38.06 Speech-BFCL Speech-Spider 26.67 24.88 88.00 78.50 Speech-MTBench 69.81 63.19 (a) Processed Samples per Second () (b) Real-time Factor () Figure 4: Efficiency comparison across evaluation frameworks and runtime scenarios. (a) Processed Samples per Second ( better) and (b) Real-time Factor ( better) measured across three datasets (MELD-Emotion, LibriSpeech-test-clean, ClothoAQA) and three runtime conditions: Individual (dataset-specific), Sequential (worst-case serialized execution), and Parallel (optimal concurrent execution). Our framework consistently outperforms existing toolkits across all scenarios, with most significant gains in parallel execution, demonstrating effective utilization of concurrent processing capabilities. Figure 5: Parallel runtime efficiency analysis across evaluation frameworks. Scatter plot comparing frameworks under optimal parallel execution conditions, plotting Real-time Factor (x-axis, better) against Processed Samples per Second (y-axis, better). Our framework (rightmost cluster) achieves superior performance in both dimensions, demonstrating the effectiveness of token-based request scheduling, dataset sharding, and vLLM integration for large-scale LALM evaluation. When text-based benchmarks are converted to the audio-based counterparts, the impact of instruction modality is often overlooked. However, this distinction can have significant impact on the downstream task evaluation performance, especially for more complex instruction-following tasks. As observed in Table 6, leveraging audio instruction modality instead text can have major impact on the performance evaluation. For instance, on challenging task of Audio Function Calling (i.e. Speech-BFCL), we observe performance degradation of up to 9.5 points. This observation highlights potential core limitation of the contemporary LALMs in following audio instructions. Therefore, careful and thorough reassessment of different instruction modality is needed to accurately measure models true reasoning capabilities in multimodal context."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced modular and extensible evaluation framework for large audio-language models that emphasizes broad task coverage, ease of use, and adaptability. Its modular design enables researchers and practitioners to extend the codebase, customize benchmarks, and integrate new models or tasks without major restructuring. While efficiency gains are realized through dataset sharding proportional to endpoint capacity and seamless vLLM integration, the broader value of our framework lies in enabling flexible, large-scale evaluations that were previously difficult to conduct in reproducible and accessible manner. By lowering the barrier to benchmarking and fostering customization, we aim to support both systematic research and practical deployment, contributing toward more standardized and transparent evaluation ecosystem for LALMs."
        },
        {
            "title": "LIMITATIONS",
            "content": "Backend dependency and reproducibility. Our efficiency gains rely on vLLM integration, models without mature backends revert to conventional execution with reduced throughput. Support for closed-source endpoints depends on chat-completions APIs, limiting batching control and introducing provider rate limits. Even with deterministic configs, runs may vary due to endpoint queueing and transient failures, requiring documentation of capacity and request budgets for crossinstitutional comparability. Standardization vs. task fidelity. Standardized prompting improves reproducibility but cannot eliminate prompt sensitivity. For open-ended tasks, canonical prompts may bias results toward specific behaviors. Our LLM-Adaptive Diarization uses word-level metrics (WDER, cpWER) as proxies for temporal precision, which remains imperfect under speech overlap or rapid transitions. The community needs multiple documented prompt families and complementary temporal measures to triangulate performance fairly. Coverage and generalization gaps. While we extend beyond ASR to diarization and spoken reasoning, coverage remains skewed toward English and common domains. Environmental audio, music understanding, and low-resource languages are underrepresented. Moreover, the relationship between standardized benchmark performance and real-world audio-language capabilities where contexts are noisier, more diverse, and less structured requires further empirical validation. These limitations highlight challenges in audio-language evaluation. Achieving reproducible, comprehensive, and valid assessment requires community coordination around prompting standards, temporal diagnostics, and multilingual breadth. Our framework aims to make systematic progress on these fronts practical for the broader ecosystem. ACKNOWLEDGMENTS We extend our gratitude to the CLAE team at ServiceNow for their invaluable feedback on the architecture design of our evaluation framework."
        },
        {
            "title": "REFERENCES",
            "content": "Ahelm: holistic evaluation of audio-language models, 2025. URL https://arxiv.org/ abs/2508.21376. Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Xavier Anguera, Simon Bozonnet, Nicholas Evans, Corinne Fredouille, Gerald Friedland, and Oriol Vinyals. Speaker diarization: review of recent research. IEEE Transactions on audio, speech, and language processing, 20(2):356370, 2012. Carlos Arriaga, Alejandro Pozo, Javier Conde, and Alvaro Alonso. Evaluation of real-time transcriptions using end-to-end asr models. arXiv preprint arXiv:2409.05674, 2024. 11 Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King. Recent advances in speech language models: survey. arXiv preprint arXiv:2410.03751, 2024. Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al. Kimi-audio technical report. arXiv preprint arXiv:2504.18425, 2025. Guhao Feng, Kai Yang, Yuntian Gu, Xinyue Ai, Shengjie Luo, Jiacheng Sun, Di He, Zhenguo Li, and Liwei Wang. How numerical precision affects arithmetical reasoning capabilities of llms. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 4685, 2025. Olivier Galibert. Methodologies for the evaluation of speaker diarization and automatic speech recognition in the presence of overlapping speech. 2013. Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy Liu, Chen-An Li, Yu-Xiang Lin, WeiCheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, et al. Dynamic-superb phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks. In The Thirteenth International Conference on Learning Representations. Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, et al. Dynamic-superb: Towards dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1213612140. IEEE, 2024. Alexander Liu, Andy Ehrenberg, Andy Lo, Clement Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, et al. Voxtral. arXiv preprint arXiv:2507.13264, 2025. Yadong Niu, Tianzi Wang, Heinrich Dinkel, Xingwei Sun, Jiahao Zhou, Gang Li, Jizhong Liu, Xunying Liu, Junbo Zhang, and Jian Luan. Mecat: multi-experts constructed benchmark for finegrained audio understanding tasks, 2025. URL https://arxiv.org/abs/2507.23511. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic In Advances in Neural Information Processing Systems, evaluation of large language models. 2024. Jing Peng, Yucheng Wang, Yangui Fang, Yu Xi, Xu Li, Xizhuo Zhang, and Kai Yu. survey on speech large language models. arXiv preprint arXiv:2410.18908, 2024. Laurent El Shafey, Hagen Soltau, and Izhak Shafran. Joint speech recognition and speaker diarization via sequence transduction. In Proc. Interspeech 2019, pp. 396400, 2019. Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, et al. Versa: versatile evaluation toolkit for speech, audio, and music. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations), pp. 191209, 2025. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations. 12 Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F. Chen. AudioBench: universal benchmark for audio large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 42974316, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/ 2025.naacl-long.218. URL https://aclanthology.org/2025.naacl-long.218/. Lu Wang, Hao Chen, Siyu Wu, Zhiyue Wu, Hao Zhou, Chengfeng Zhang, Ting Wang, and Haodi Zhang. Codecbench: comprehensive benchmark for acoustic and semantic evaluation, 2025b. URL https://arxiv.org/abs/2508.20660. Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, and Hank Liao. Diarizationlm: Speaker diarization post-processing with large language models. In Proc. Interspeech 2024, pp. 37543758, 2024. Yingzhi Wang, Mirco Ravanelli, and Alya Yacoubi. Speech emotion diarization: Which emotion appears when? In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 17. IEEE, 2023. Shinji Watanabe, Michael Mandel, Jon Barker, Emmanuel Vincent, Ashish Arora, Xuankai Chang, Sanjeev Khudanpur, Vimal Manohar, Daniel Povey, Desh Raj, et al. Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings. In Proc. CHiME 2020, pp. 17, 2020. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, et al. Air-bench: Benchmarking large audio-language models via In Proceedings of the 62nd Annual Meeting of the Association for generative comprehension. Computational Linguistics (Volume 1: Long Papers), pp. 19791998, 2024. Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Lin, Andy Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al. Superb: Speech processing universal performance benchmark. 2021. Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural language reasoning, survey. ACM Computing Surveys, 56(12):139, 2024. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 39113921, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425. Junbo Zhang, Heinrich Dinkel, Yadong Niu, Chenyu Liu, Si Cheng, Anbei Zhao, and Jian Luan. Xares: comprehensive framework for assessing audio encoder performance, 2025. URL https: //arxiv.org/abs/2505.16369. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        }
    ],
    "affiliations": [
        "ServiceNow",
        "University of Texas at Austin"
    ]
}