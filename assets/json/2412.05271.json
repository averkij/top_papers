{
    "paper_title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
    "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Hao Tian",
        "Zhaoyang Liu",
        "Lixin Gu",
        "Xuehui Wang",
        "Qingyun Li",
        "Yimin Ren",
        "Zixuan Chen",
        "Jiapeng Luo",
        "Jiahao Wang",
        "Tan Jiang",
        "Bo Wang",
        "Conghui He",
        "Botian Shi",
        "Xingcheng Zhang",
        "Han Lv",
        "Yi Wang",
        "Wenqi Shao",
        "Pei Chu",
        "Zhongying Tu",
        "Tong He",
        "Zhiyong Wu",
        "Huipeng Deng",
        "Jiaye Ge",
        "Kai Chen",
        "Min Dou",
        "Lewei Lu",
        "Xizhou Zhu",
        "Tong Lu",
        "Dahua Lin",
        "Yu Qiao",
        "Jifeng Dai",
        "Wenhai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 1 7 2 5 0 . 2 1 4 2 : r Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling Zhe Chen4,1, Weiyun Wang5,1, Yue Cao4,1, Yangzhou Liu4,1, Zhangwei Gao7,1, Erfei Cui7,1, Jinguo Zhu1, Shenglong Ye1, Hao Tian2, Zhaoyang Liu1, Lixin Gu1, Xuehui Wang1, Qingyun Li1, Yimin Ren1, Zixuan Chen2, Jiapeng Luo2, Jiahao Wang2, Tan Jiang2, Bo Wang2, Conghui He1, Botian Shi1, Xingcheng Zhang1, Han Lv1, Yi Wang1, Wenqi Shao1, Pei Chu1, Zhongying Tu1, Tong He1, Zhiyong Wu1, Huipeng Deng1, Jiaye Ge1, Kai Chen1, Min Dou1, Lewei Lu2, Xizhou Zhu3,1, Tong Lu4, Dahua Lin6,1, Yu Qiao1, Jifeng Dai3,1(cid:66), Wenhai Wang6,1(cid:66) 1Shanghai AI Laboratory 2SenseTime Research 3Tsinghua University 4Nanjing University 5Fudan University 6The Chinese University of Hong Kong 7Shanghai Jiao Tong University Code: https://github.com/OpenGVLab/InternVL Model: https://huggingface.co/OpenGVLab/InternVL2_5-78B HF Demo: https://huggingface.co/spaces/OpenGVLab/InternVL"
        },
        {
            "title": "Abstract",
            "content": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and testtime configurations. Through extensive evaluations on wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, realworld comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL"
        },
        {
            "title": "Introduction",
            "content": "In recent years, multimodal large language models (MLLMs) [60, 137, 246, 36, 35, 248, 140, 228, 192, 275, 143, 54, 170] have emerged as pivotal technology in artificial intelligence, capable of processing and understanding information from multiple modalities such as text, images, and videos. These models promise breakthroughs across fields like natural language processing, computer vision, and human-computer interaction. However, developing large-scale MLLMs remains challenging task, requiring significant computational resources, sophisticated architectures, and the ability to effectively integrate diverse data types in scalable manner. Various attempts have been made to address these challenges, including enhancing model architectures [220, 232, 5, 172, 157, 210], scaling vision encoders [252, 66, 36, 293, 185] and language models [231, 235, 64, 19, 229, 221, 62], incorporating more diverse and high-quality datasets [124, 234, 25, 155], and refining the test-time scaling process [215, 249, 230] to boost performance. Notable commercial models, like GPT-4o [192] * equal contribution; interns at OpenGVLab, Shanghai AI Laboratory; (cid:66) corresponding authors (daijifeng@tsinghua.edu.cn, wangwenhai@pjlab.org.cn). Figure 1: Performance of various MLLMs on the OpenCompass leaderboard. InternVL 2.5 showcases strong multimodal capabilities, rivaling closed-source models like GPT-4o [192] and Claude-3.5-Sonnet [8]. However, since the OpenCompass score is derived from 8 academic VQA benchmarks and covers only subset of overall capabilities, we still need further effort to match the performance with closed-source models. and Claude-3.5-Sonnet [8], have demonstrated exceptional performance, their closed nature limits transparency and accessibility, leaving gaps in the open-source community. While open-source multimodal models such as the InternVL series [36, 35, 71] and Qwen-VL series [13, 246] have provided high-performance, transparent alternatives, they still fall short in terms of achieving the desired levels of performance and efficiency. In this work, we introduce InternVL 2.5, an advanced large-scale MLLM series that builds upon the foundational architecture of InternVL 2.0. Continuing the objectives of the entire InternVL series, we aim to bridge the performance gap between commercial closed-source models and open-source multimodal models. In InternVL 2.5, we systematically explore various factors in MLLM, including how changes in vision encoders, language models, dataset sizes, and inference times affect the overall performance of the model, demonstrating the relationship between scaling and performance in multimodal models. Specifically, we have some interesting findings: (1) Large vision encoders significantly reduce the dependency on training data when scaling up MLLMs. As shown in Table 3, compared to Qwen2-VL-72B [246] equipped with 600M vision encoder, our InternVL2.578B with 6B vision encoder can achieve better performance using only 1/10 of the training tokens. This greatly reduces the exploration cost when scaling up MLLMs; (2) Data quality matters. Upgrading InternVL from 2.0 to 2.5 doubled the dataset size, but strict filtering greatly improved quality. For example, we carefully excluded the anomalous samples (e.g., repetitive patterns), achieving substantial improvements in Chain-of-Thought (CoT) reasoning tasks such as MMMU [289] and complex challenges like the OlympiadBench [80]. Note that, most existing open-source MLLMs tend to underperform when using CoT [249]. (3) Test-time scaling is beneficial for difficult multimodal QA. For challenging tasks such as MMMU, the InternVL2.5-78B with CoT reaches 70.1%, which is 3.7 points higher than the direct response. Subsequently, we have successfully verified that CoT can be further combined with majority voting and bring additional improvements. Our contributions can be summarized as follows: (1) We release InternVL 2.5 to the open-source community, providing powerful tool for the development and application of multimodal AI systems and encouraging further research in this domain. (2) We investigate how scaling different components of the MLLMs such as vision encoders, language models, dataset sizes, and inference time affect performance. (3) Through extensive evaluations on diverse benchmarksincluding multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processingInternVL 2.5 exhibits competitive 2 Figure 2: Overall architecture. InternVL 2.5 retains the same model architecture as InternVL 1.5 [35] and InternVL 2.0, i.e. the widely-used ViT-MLP-LLM paradigm, which combines pre-trained InternViT-300M or InternViT-6B with LLMs [19, 229] of various sizes via an MLP projector. Consistent with previous versions, we apply pixel unshuffle operation to reduce the 1024 visual tokens produced by each 448448 image tile to 256 tokens. Moreover, compared to InternVL 1.5, InternVL 2.0 and 2.5 introduced additional data types, incorporating multi-image and video data alongside the existing single-image and text-only data. performance, rivaling leading commercial models like GPT-4o [192] and Claude-3.5-Sonnet [8]. It is the first open-source MLLM to surpass 70% on the MMMU validation set [289], setting new benchmark and highlighting the potential of open-source solutions in advancing multimodal AI."
        },
        {
            "title": "2.1 Overall Architecture",
            "content": "As shown in Figure 2 and Table 2, InternVL 2.5 retains the same model architecture as its predecessors, InternVL 1.5 [35] and InternVL 2.0, following the ViT-MLP-LLM paradigm widely adopted in various MLLM studies [150, 151, 36, 316, 162, 246, 124, 256]. In this new version, our implementation of this architecture integrates newly incrementally pre-trained InternViT-6B or InternViT-300M with various pre-trained LLMs of different sizes and types, including InternLM 2.5 [19] and Qwen 2.5 [229], using randomly initialized 2-layer MLP projector. As in the previous version, to enhance scalability for high-resolution processing, we simply applied pixel unshuffle operation, reducing the number of visual tokens to one-quarter of the original. Consequently, in our model, 448448 image tile is represented by 256 visual tokens. In terms of input data preprocessing, we adopted similar dynamic resolution strategy as InternVL 1.5, dividing images into tiles of 448448 pixels based on the aspect ratio and resolution of the input images. The key difference, starting from InternVL 2.0, is that we additionally introduced support for multi-image and video data, as shown in Figure 2(b). Different data types correspond to different preprocessing configurations, which we will detail in Section 3.1."
        },
        {
            "title": "2.2 Vision Encoder",
            "content": "InternVL employs InternViT [36] as the vision encoder. To better document the training progression of InternViT, we have provided detailed information in Table 1. InternViT currently has two different model sizes, including InternViT-6B and InternViT-300M. InternViT-6B. InternViT-6B-224px was first introduced in our CVPR paper [36], and its structure follows the vanilla ViT [61], with minor adjustments incorporating QK-Norm [53] and RMSNorm [294]. It had 5.9B parameters, 48 layers, hidden size of 3200, and 25 heads, and it was trained using contrastive loss [195]. Due to the limited gains at that time, we adopted an incremental pre-training strategy to continuously refine its weights. Specifically, we connected InternViT-6B to an LLM via an MLP projector and, following brief MLP warmup, jointly trained the InternViT-6B using next token prediction loss (as shown in Figure 4(a)) to enhance its visual feature extraction capabilities. In the V1.0 and V1.2 versions, we used fixed resolution of 448448 3 Model Name InternViT-6B-224px InternViT-6B-448px-V1.0 InternViT-6B-448px-V1.2 InternViT-6B-448px-V1.5 InternViT-6B-448px-V2.5 InternViT-300M-448px-Distill InternViT-300M-448px InternViT-300M-448px-V2.5 Train Res. Width Depth MLP 12800 fixed 224 12800 fixed 448 12800 fixed 448 12800 dynamic 448 12800 dynamic 448 4096 fixed 448 4096 dynamic 448 4096 dynamic 448 3200 3200 3200 3200 3200 1024 1024 48 48 45 45 45 24 24 24 #Heads QK-Norm Norm Type Loss Type 25 25 25 25 25 16 16 16 RMS RMS RMS RMS RMS LN LN LN CLIP NTP NTP NTP NTP Cosine NTP NTP #Param 5.9B 5.9B 5.5B 5.5B 5.5B 0.3B 0.3B 0.3B Table 1: Details of InternViT-6B and InternViT-300M models. fixed 224 refers to training images resized to 224224, while dynamic 448 means the model is trained with dynamic high resolution, with each image tile being 448448. CLIP refers to the contrastive loss, Cosine represents the cosine distillation loss, while NTP indicates the next token prediction loss. Model Name InternVL-Chat-V1.5 InternVL2-1B InternVL2-2B InternVL2-4B InternVL2-8B InternVL2-26B InternVL2-40B InternVL2-Llama3-76B InternVL2.5-1B InternVL2.5-2B InternVL2.5-4B InternVL2.5-8B InternVL2.5-26B InternVL2.5-38B InternVL2.5-78B InternVL2.5-Pro #Param 25.5B 0.9B 2.2B 4.2B 8.1B 25.5B 40.1B 76.3B 0.9B 2.2B 3.7B 8.1B 25.5B 38.4B 78.4B Vision Encoder InternViT-6B-448px-V1.5 InternViT-300M-448px InternViT-300M-448px InternViT-300M-448px InternViT-300M-448px InternViT-6B-448px-V1.5 InternViT-6B-448px-V1.5 InternViT-6B-448px-V1.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-300M-448px-V2.5 InternViT-6B-448px-V2.5 InternViT-6B-448px-V2.5 InternViT-6B-448px-V2.5 InternViT-6B-448px-V2.5 Language Model internlm2-chat-20b Qwen2-0.5B-Instruct internlm2-chat-1.8b Phi-3-mini-128k-instruct internlm2_5-7b-chat internlm2-chat-20b Nous-Hermes-2-Yi-34B Hermes-2-Theta-Llama-3-70B Qwen2.5-0.5B-Instruct internlm2_5-1_8b-chat Qwen2.5-3B-Instruct internlm2_5-7b-chat internlm2_5-20b-chat Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct OpenCompass 61.7 48.3 54.0 60.6 64.1 66.4 69.7 71.0 54.5 59.8 65.1 68.1 71.3 73.3 75.5 Table 2: Pre-trained models used in the InternVL series. In the InternVL 2.5 series, we upgraded both the vision encoder and the language model, resulting in improved performance. The OpenCompass scores for InternVL 1.5 and InternVL 2.0 were collected from the OpenCompass leaderboard, while the scores for InternVL 2.5 series were obtained through our local testing. for training, but in later versions, we switched to dynamic resolution training to improve high-resolution processing. As detailed in the InternVL 1.5 report [35], we removed the last three layers of InternViT-6B448px-V1.2, reducing its depth from 48 to 45 layers, as these layers were more tuned to the CLIP loss objective, prioritizing global alignment over local information. As result, all subsequent versions, including the latest InternViT-6B-448px-V2.5, have 45 layers and 5.5B parameters. InternViT-300M. InternViT-300M-448px-Distill is distilled variant of the teacher model, InternViT-6B-448pxV1.5, utilizing cosine distillation loss. This model comprises 0.3B parameters, 24 layers, hidden size of 1024, and 16 attention heads. Unlike the 6B version, the 0.3B variant employs standard LayerNorm [11] without QK-Norm [53]. To reduce distillation costs, we initialized this model using CLIP-ViT-Large-336px [195] where applicable, despite some architectural differences. After distillation, we integrated this model with an LLM and, following similar procedure as described above, trained the vision encoder with dynamic high-resolution and the NTP loss. Then, we extracted the vision encoder and released it as InternViT-300M-448px. In this report, we further refined the InternViT-300M by incrementally pre-training the previous weights on more diverse data mixture using the NTP loss, leading to the enhanced InternViT-300M-448px-V2.5."
        },
        {
            "title": "2.3 Large Language Model",
            "content": "In Table 2, we provide an overview of the language models used across different versions of InternVL, including InternVL 1.5, InternVL 2.0, and the latest InternVL 2.5. As shown, earlier versions primarily built on language models such as InternLM 2 [19], Qwen 2 [268], Phi 3 [1], Yi [279], and Llama 3 [64]. To achieve better 4 Figure 3: Illustration of the data formats for various data types. (a) For single-image datasets, the maximum number of tiles nmax is allocated to single image, ensuring maximum resolution for the input. (b) For multiimage datasets, the total number of tiles nmax is distributed proportionally across all images within the sample. (c) For video datasets, the method simplifies the approach by setting nmax = 1, resizing individual frames to fixed resolution of 448448. performance, in the InternVL 2.5 series, we have comprehensively upgraded the language backbones to the latest state-of-the-art models, including InternLM 2.5 [19] and Qwen 2.5 [229]."
        },
        {
            "title": "3.1 Dynamic High-Resolution for Multimodal Data",
            "content": "In InternVL 2.0 and 2.5, we extend the dynamic high-resolution training approach introduced in InternVL 1.5 [35], enhancing its capabilities to handle multi-image and video datasets. The process mainly consists of the following steps: Closest Aspect Ratio Matching. Given an input image with dimensions H, the aspect ratio is computed as = . The objective is to resize the image into tiles of size (where = 448) while selecting the closest aspect ratio that minimizes distortion. The number of tiles, ntiles, is constrained within predefined range [nmin, nmax]. To find the optimal aspect ratio for resizing, we define the set of target aspect ratios as: = {i/j 1 i, n, [nmin, nmax]} . (1) The closest aspect ratio rbest is selected by minimizing the difference between the original aspect ratio and each target aspect ratio rtarget: rbest = arg min rtargetR rtarget . (2) In cases where multiple aspect ratios produce the same difference (e.g., 1:2 and 2:4), we prioritize the aspect ratio that results in an area less than or equal to twice the original image size. This helps to some extent in preventing the excessive enlargement of low-resolution images. Image Resizing and Splitting. Once the best aspect ratio is determined, the image is resized to new dimensions Wnew Hnew, where ibest and jbest are the factors corresponding to rbest: Wnew = ibest, Hnew = jbest. (3) The image is then split into tiles of size S, with the number of tiles calculated as ntiles = ibest jbest. Each tile is cropped from the resized image to ensure consistent size. Thumbnail Generation. Optionally, if the number of tiles ntiles > 1, the original image is resized to square of dimensions to generate an additional thumbnail Ithumb. This thumbnail is appended to the list of tiles, providing global view alongside the localized tiles. In cases where ntiles = 1, there is no thumbnail to append, and the mechanism naturally skips this step. Data Formats for Different Data Types. As shown in Figure 3, the dynamic high-resolution method in InternVL 2.0 and 2.5 extends beyond single-image datasets to also support multi-image and video datasets. Figure 4: Illustration of the training pipeline and progressive scaling strategy. (a) Single model training pipeline: The training process is divided into three stagesStage 1 (MLP warmup), optional Stage 1.5 (ViT incremental learning), and Stage 2 (full model instruction tuning). The multi-stage design progressively enhances vision-language alignment, stabilizes training, and prepares modules for integration with larger LLMs. (b) Progressive scaling strategy: The ViT module trained with smaller LLM in earlier stages can be easily integrated with larger LLMs, enabling scalable model alignment with affordable resource overhead. For single-image datasets, the maximum number of tiles nmax is allocated to single image, ensuring that it is processed at the highest possible resolution. In this scenario, visual tokens are enclosed within <img> and </img> tags, with no additional auxiliary tags used. In the case of multi-image datasets, the total number of tiles nmax is distributed across all images within one sample. Each image is identified by an auxiliary tag like Image-1 to clearly label individual images. The images themselves are enclosed within <img> and </img> tags, denoting the start and end of the image data. The number of tiles assigned to each image Ii is proportional to the total number of images Nimage, following the equation: (cid:18) nmax, = max 1, (cid:22) nmax Nimage (cid:23)(cid:19) . (4) For video data, this approach is simplified by setting nmax = 1. Each video frame is resized to fixed resolution of 448448, eliminating the need for tiling. This is because, during training, large number of frames (e.g., 32 or 64) are typically extracted from single video. For our model, even without high-resolution input, this still results in 8,192 or 16,384 visual tokens. Each video frame, labeled with tags like Frame-1, is enclosed within the <img> and </img> tags, similar to image data."
        },
        {
            "title": "3.2 Single Model Training Pipeline",
            "content": "The training pipeline for single model in InternVL 2.5 is structured across three stages, designed to enhance the models visual perception and multimodal capabilities. Each stage progressively integrates vision and language modalities, balancing performance optimization with training efficiency. 6 Settings Dataset Trainable Packed Batch Size Learning Rate Context Length Image Tile Threshold ViT Drop Path Weight Decay Training Epochs Training Tokens Settings Dataset Trainable Packed Batch Size Learning Rate Context Length Image Tile Threshold ViT Drop Path Weight Decay Training Epochs Training Tokens InternVL2.5-1B Stage 2 Fine-tune Mixture InternVL2.5-2B Stage 2 Fine-tune Mixture Stage 1 Pre-train Mixture InternVL2.5-4B Stage 2 Fine-tune Mixture Stage 1 Pre-train Mixture InternVL2.5-8B Stage 1.5 Pre-train Mixture Stage 2 Fine-tune Mixture Stage 1 Pre-train Mixture Full Model MLP 512 2e-5 16384 48 0.0 0.01 277B 512 4e-5 16384 48 0.1 0.01 4 176B Full Model MLP 512 2e-5 16384 48 0.0 0.01 164B 512 4e-5 16384 48 0.1 0.01 4 176B Full Model MLP ViT+MLP Full Model 512 4e-5 16384 48 0.1 0.01 2 88B 512 2e-4 16384 48 0.0 0.05 22B 1024 1e-5 16384 48 0.1 0.05 76B 512 4e-5 16384 48 0.1 0.05 1 44B Stage 1 Pre-train Mixture MLP 512 2e-4 16384 48 0.0 0.01 191B Stage 1 Pre-train Mixture MLP 512 2e-4 16384 48 0.0 0.05 31B InternVL2.5-26B Stage 1.5 Pre-train Mixture ViT+MLP 1024 1e-5 16384 48 0.4 0.05 146B Stage 2 Fine-tune Mixture Full Model 512 2e-5 16384 48 0.4 0.05 1 44B InternVL2.5-38B InternVL2.5-78B Stage 1 Pre-train Mixture MLP 512 2e-4 16384 48 0.0 0.05 107B Stage 2 Fine-tune Mixture Full Model 512 2e-5 16384 48 0.4 0.05 1 44B Stage 1 Pre-train Mixture MLP 512 2e-4 16384 48 0.0 0.05 76B Stage 2 Fine-tune Mixture Full Model 512 2e-5 16384 48 0.4 0.05 1 44B Table 3: Training configurations and hyperparameters for InternVL 2.5. This table presents the training setups for various scales of InternVL 2.5 models. The configurations are carefully optimized to ensure efficient scaling and performance across different parameter sizes and training stages. Notably, Qwen2-VL [246] processed cumulative total of 1.4T tokens, while our InternVL2.5-78B is trained on just 120B tokens. Stage 1: MLP Warmup. As shown in Figure 4(a), the training begins with warming up the MLP projector, which is the initial bridge between visual and language representations. In this stage, only the MLP projector is trained while both the vision encoder (i.e., InternViT [36]) and language model are frozen. To achieve optimal performance, we begin using the dynamic high-resolution training strategy from this stage, even though it increases the training cost. In this phase, we utilize the pre-training data mixture as outlined in Table 4. The data is formatted in structured ChatML style and optimized with the NTP loss. Additionally, higher learning rate is applied to accelerate convergence, allowing the MLP to quickly adapt to the LLMs input space and establish robust cross-modal alignment. The MLP warmup phase ensures the model is well-prepared to handle multimodal tasks before unlocking additional trainable components in later stages, thereby improving training stability. Stage 1.5: ViT Incremental Learning (Optional). As shown in Figure 4(a), Stage 1.5 introduces incremental learning for the vision encoder. During this stage, both the vision encoder and MLP projector are trainable, and training is conducted using the same pre-training data mixture and NTP loss as in Stage 1. The aim of this stage is to enhance the vision encoders ability to extract visual features, allowing it to capture more comprehensive information, especially for domains that are relatively rare in web-scale datasets (e.g., LAION-5B [203]), such as multilingual OCR data and mathematical charts, among others. As shown in Table 3, lower learning rate is used in this stage to prevent catastrophic forgetting, ensuring the encoder doesnt lose previously learned capabilities. Additionally, the vision encoder only needs to be trained once unless new domain requirements or data are introduced. Once trained, it can be reused with different LLMs without retraining (see Figure 4(b) and Section 3.3), making Stage 1.5 optional. This is particularly beneficial when the encoder has already been optimized for some specific tasks, allowing it to integrate with LLMs of various sizes without significant additional costs. Stage 2: Full Model Instruction Tuning. In the final stage, as illustrated in Figure 4(a), the entire modelcomprising the ViT, MLP, and LLMis trained on high-quality multimodal instruction datasets. Data quality is especially important here, as the LLM, responsible for generating the final user-facing output, is now trainable. Even small amount of noisy data (e.g., few thousand samples) can lead to abnormal model behavior, like repetitive output or specific erroneous results. To mitigate the degradation of the LLM, we implement strict data quality controls during this stage. Additionally, the training hyperparameters in this stage are kept simple, with unified learning rate applied to the entire model rather than different learning rates for various components. After completing this stage, InternVL 2.5s full training process is finished. Although further improvements could be made through Stage 3post-training with higher-quality data or other training methods (e.g., preference optimization)we plan to leave this for the future."
        },
        {
            "title": "3.3 Progressive Scaling Strategy",
            "content": "As shown in Figure 4, we propose progressive scaling strategy to efficiently align the vision encoder (e.g., InternViT) with LLMs. While we previously adopted similar strategy in the training of InternVL 1.5 and 2.0, this is the first time the approach has been formalized into clear methodology. This strategy adopts staged training approach, starting with smaller, resource-efficient LLMs and progressively scaling up to larger LLMs. This approach stems from our observation that even when the ViT and LLM are jointly trained using NTP loss, the resulting visual features are generalizable representations that can be easily understood by other LLMs. Specifically, in Stage 1.5, the InternViT is trained alongside smaller LLM (e.g., 20B), focusing on optimizing fundamental visual capabilities and cross-modal alignment. This phase avoids the high computational costs associated with training directly with large LLM. Using shared-weight mechanism, the trained InternViT can be easily transferred to larger LLM (e.g., 72B) without requiring retraining. Consequently, when training larger model, Stage 1.5 can be skipped, as the optimized InternViT module from earlier stages is reused. This not only accelerates training but also ensures that the vision encoders learned representations are preserved and effectively integrated into the larger model. By employing this progressive scaling strategy, we achieve scalable model updates at fraction of the cost typically associated with large-scale MLLM training. For example, Qwen2-VL [246] processes cumulative total of 1.4 trillion tokens, whereas our InternVL2.5-78B is trained on only about 120 billion tokensless than one-tenth of Qwen2-VL. This approach proves particularly advantageous in resource-constrained settings by maximizing the reuse of pre-trained components, minimizing redundant computations, and enabling the efficient training of models capable of addressing complex vision-language tasks."
        },
        {
            "title": "3.4 Training Enhancements",
            "content": "To enhance the models adaptability to real-world scenarios and overall performance, two key techniques are introduced. These optimizations are essential in improving the user experience and the models benchmark performance. Random JPEG Compression. To avoid overfitting during training and enhance the models real-world performance, we apply data augmentation technique that preserves spatial information: JPEG compression. Specifically, random JPEG compression with quality levels between 75 and 100 is applied to simulate the degradation commonly found in internet-sourced images. This augmentation improves the models robustness to noisy, compressed images and enhances the user experience by ensuring more consistent performance across varied image qualities. Loss Reweighting. Token averaging and sample averaging are two widely applied strategies for weighting the NTP loss. Token averaging computes the average NTP loss across all tokens, whereas sample averaging first averages the NTP loss within each sample (across tokens) and then averages across the number of samples. These strategies can be expressed in unified format: = wi wj (cid:80) Li, wi = (cid:26) 1 x0 , 1 x1 , for token averaging for sample averaging, (5) where Li and wi denote the loss and weight of token i, respectively, and denotes the number of tokens in the response to which token belongs. When using token averaging, each token contributes equally to the final loss, which can result in gradients biased toward responses with more tokens, leading to drop in benchmark performance. In contrast, sample averaging ensures that each sample contributes equally, but it can cause the model to favor shorter responses, negatively impacting the user experience. To mitigate bias toward either longer or shorter responses during 8 Figure 5: Dataset configuration. In InternVL 2.0 and 2.5, data augmentation is applied selectively, enabled for image datasets and disabled for videos and text. The maximum tile number (nmax) controls the resolution of inputs, with higher values for multi-image datasets and lower values for videos. The repeat factor (r) balances dataset sampling by adjusting the frequency of each dataset, ensuring robust and balanced training. training, we apply reweighting strategy where wi = 1 the contribution of responses with different lengths. x0.5 . This approach, named square averaging, balances"
        },
        {
            "title": "4.1 Dataset Configuration",
            "content": "In InternVL 2.0 and 2.5, the organization of the training data is controlled by several key parameters to optimize the balance and distribution of datasets during training, as shown in Figure 5. Data Augmentation. Firstly, data augmentation (i.e., JPEG compression introduced in Section 3.4) is applied conditionally, allowing for enhanced robustness by enabling or disabling augmentation techniques based on dataset characteristics. Specifically, we enable this augmentation for all image datasets, while disabling it for all video datasets, to ensure that different video frames have the same image quality. Maximum Tile Number. The parameter nmax defines the maximum number of tiles allowed per dataset, effectively controlling the resolution of the image or video frame fed into the model. This ensures flexibility in handling datasets of varying complexity and type. For example, we can set nmax = 24 or 36 for multi-image datasets, high-resolution documents, or infographics, use nmax = 6 or 12 for most other low-resolution image datasets, and set nmax = 1 for video datasets. This adjustment was first introduced in InternVL 2.0, whereas in InternVL 1.5, uniform value of nmax = 12 was applied across all datasets. Repeat Factor. Finally, the repeat factor determines the sampling frequency of each dataset. With (0, 4], this parameter enables down-sampling when < 1, reducing the datasets weight during training, or up-sampling when > 1, effectively increasing the number of epochs for that dataset. This mechanism finely adjusts the relative proportions of datasets, ensuring balanced distribution across training data. By adjusting r, especially in multi-task learning, the data from each domain or task receives appropriate training, preventing overfitting or underfitting of any single dataset, leading to more balanced model performance."
        },
        {
            "title": "4.2 Multimodal Data Packing",
            "content": "In InternVL 2.0 and 2.5, we implement data-packing strategy to enhance GPU utilization and improve training efficiency. This approach reduces padding by concatenating multiple samples into longer sequences, thereby maximizing the utilization of the models input sequence capacity. Specifically, for multimodal models like InternVL, data packing should account for two dimensions: (a) Sequence length for the LLM, which corresponds to the standard input sequence length used in language models. This remains essential in multimodal tasks; (b) Image tile number for the ViT, which denotes the number of image tiles processed by the vision encoder. Efficient management of this dimension is crucial for optimizing training efficiency. To handle these dimensions efficiently, our data-packing strategy comprises the following steps: (1) Select: During the selection phase, the algorithm operates similarly to standard dataset without datapacking, directly sampling independent data. Each sampled item is truncated into multiple smaller items and treated as separate samples. This ensures that the sequence length and image tile count of each sample are within the predefined thresholds lmax (context length) and tmax (image tile limit), respectively. (2) Search: For given independent sample, the algorithm searches for another sample from the buffer list to pack them together. The resulting packed sample must have sequence length shorter than lmax and include fewer than tmax image tiles. If multiple buffers satisfy these requirements, the one with the longest sequence 9 Figure 6: Visualization of abnormal samples in open-source datasets. Abnormal samples are prevalent across various data types, including single-image, multi-image, video, and pure text datasets, with repetitive outputs being prominent issue. We identify this as one of the most detrimental problems for test-time scaling, often leading models into loops in long-form outputs and CoT reasoning tasks. length and the maximum number of image tiles is selected. In practice, the buffer list is maintained in descending order and binary search is performed to accelerate the search process. (3) Pack: The sampled data and the selected buffer are packed into single sequence. If no buffer is selected in the previous step, the sample remains unchanged and proceeds directly to the next phase. Notably, tokens in the packed data can only attend to the context within their respective samples and cannot attend to tokens from other packed samples. Furthermore, the positional index of each sample is maintained independently. (4) Maintain: In the maintenance phase, if packed sample exceeds lmax or contains more than tmax image tiles, it is immediately yielded for training. Otherwise, the packed sample is inserted into the buffer list. If the buffer list exceeds its capacity, the sample with the longest sequence length and the highest number of image tiles is yielded to maintain buffer efficiency."
        },
        {
            "title": "4.3 Data Filtering Pipeline",
            "content": "During model development, we observed that LLMs are significantly more sensitive to data noise than vision encoders. As shown in Figure 4, during Stage 2, when all model weights are fully trainable, even small fraction of anomalous samplessuch as outliers or repetitive data, numbering only few thousandcan lead to aberrant model behavior during inference. While conventional wisdom assumes that minor noise in large-scale datasets can be ignored, our findings indicate otherwise: even tiny fraction of noisy samples can degrade MLLM performance and user experience. Among these anomalies, we identify repetitive generation as one of the most detrimental issues. In many open-source or synthetic datasets, small number of repetitive samplescomprising merely thousands of examples in our Stage 2 data mixturecan cause the model to spiral into repetitive loops, particularly in long-form outputs or CoT reasoning tasks. This phenomenon undermines the effectiveness of test-time scaling strategies. To address this challenge and support future research, we designed an efficient data filtering pipeline to remove low-quality samples, thereby minimizing the risk of repetitive generation. As shown in Figure 8, our data filtering pipeline consists of two modules. For pure-text data, we implemented three key strategies: (1) LLM-Based Quality Scoring: We begin by categorizing datasets into distinct domains 10 Figure 7: Statistics of the fine-tuning data mixture. The dataset shows consistent growth from InternVL 1.5 to 2.5 in terms of (a) the number of samples and (b) the number of tokens across multiple dataset types, including single-image, multi-image, video, and text. These statistics reflect iterative improvements in data scale and diversity, which enhance the models multimodal understanding capabilities. Figure 8: Dataset filtering pipeline. For text data, we use three methods: (a) LLM-based quality scoring to assign domain-specific quality scores and filter low-quality samples; (b) Repetition detection to identify and remove data with repetitive patterns; and (c) heuristic rule-based filtering to detect anomalies using predefined rules. For multimodal data, only (b) repetition detection and (c) heuristic rule-based filtering are applied to mitigate repetitive patterns and ensure dataset integrity. (e.g., disciplinary, programming, mathematics, general). Next, we assign quality score, ranging from 0 to 10, to each sample using pre-trained LLM [229] with domain-specific prompt. Samples with scores below specified threshold (e.g., 7) are then removed to ensure data quality. (2) Repetition Detection: We use an LLM combined with specialized prompt to identify repetitive samples. These samples are then subjected to manual review, and those scoring below threshold (e.g., 3) are removed to maintain data quality. (3) Heuristic Rule-Based Filtering: We apply specific rules, such as filtering out sentences with abnormal lengths, excessively long sequences of zeros, text with an excessive number of duplicate lines, etc, to identify anomalies in the data. Although this approach may occasionally produce false positives, it improves the detection of anomalous samples. All flagged samples are manually reviewed before final removal. For multimodal data, given the limitations of open-source MLLMs in scoring such data, we focused on mitigating repetitive patterns through two strategies: (1) Repetition Detection: We exempted high-quality academic datasets and used specific prompt to identify repetitive samples in the remaining data. These samples were removed following the same manual review process we applied to textual data. (2) Heuristic Rule-Based Filtering: Similar heuristic rules are applied, followed by manual verification to ensure dataset integrity. This rigorous data-filtering pipeline significantly reduced the occurrence of anomalous behaviors, particularly repetitive generation, with notable improvements in CoT reasoning tasks. However, we recognize that data filtering alone cannot completely eliminate such issues. This may be due to the inherent noise introduced during the LLMs pre-training process, which our multimodal post-training efforts can only partially mitigate without fundamentally resolving the issue of repetitive outputs. Future work will explore preference optimization and other strategies to further suppress anomalies and enhance both model performance and user experience. 11 Task Type: Single/Multi-Image Datasets Dataset Captioning General QA Mathematics Chart OCR Knowledge Grounding Document Conversation Medical FaceCaption [49], COCO-Caption [214], OpenImages-Caption [116], Objects365-Caption [208], TextCap [211], Laion-ZH [203], Laion-EN [203], Laion-COCO [204], LLaVAR [305], InternVL-SA-1B-Caption [113], MMInstruct [155], GRIT-Caption [194], ShareGPT4V [29], LVIS-Instruct-4V [244], ShareCaptioner [29], OmniCorpus [133], ShareGPT4o [35] GQA [98], OKVQA [178], A-OKVQA [205], Visual7W [317], VisText [226], VSR [147], TallyQA [2], Objects365-YorN [208], IconQA [167], Stanford40 [273], VisDial [51], VQAv2 [74], Hateful-Memes [111] MAVIS [300], GeomVerse [107], MetaMath-Rendered [281], MapQA [23], GeoQA+ [20], Geometry3K [164], UniGeo [26], GEOS [206], CLEVR-Math [144] ChartQA [181], PlotQA [187], FigureQA [105], LRV-Instruction [148], ArxivQA [132], MMC-Inst [149], TabMWP [166], DVQA [104], UniChart [182], SimChart9K [263], Chart2Text [191], FinTabNet [312], SciTSR [39], Synthetic Chart2Markdown LaionCOCO-OCR [204], Wukong-OCR [75], ParsynthOCR [89], SynthDoG-EN [112], SynthDoG-ZH [112], SynthDoG-RU [112], SynthDoG-JP [112], SynthDoG-KO [112], IAM [180], EST-VQA [253], ST-VQA [17], NAF [52], InfoVQA [183], HME100K [288], OCRVQA [188], SROIE [97], POIE [115], CTW [287], SynthText [79], ArT [40], LSVT [222], RCTW-17 [209], ReCTs [301], MTWI [82], TextVQA [212], CASIA [146], TextOCR [213], Chinese-OCR [14], EATEN [78], COCO-Text [238], Synthetic Arxiv OCR, Synthetic Image2Latex, Synthetic Handwritten OCR, Synthetic Infographic2Markdown KVQA [207], A-OKVQA [205], ViQuAE [123], iNaturalist2018 [237], MovieNet [95], ART500K [176], KonIQ-10K [91], IconQA [167], VisualMRC [225], ChemVLM Data [129], ScienceQA [165], AI2D [109], TQA [110], Wikipedia-QA [81], Synthetic Multidisciplinary Knowledge / QA Objects365 [208], GRIT [278], RefCOCO [280], GPT4Gen-RD-BoxCoT [27], All-Seeing-V1 [251], All-Seeing-V2 [250], V3Det [243], TolokaVQA [236] DocReason25K [93], DocVQA [184], Docmatix [121], Synthetic Arxiv QA ALLaVA [25], SVIT [309], Cambrain-GPT4o [234], TextOCR-GPT4V [102], MMDU [159], Synthetic Real-World Conversations PMC-VQA [303], VQA-RAD [120], ImageCLEF [72], SLAKE [145], Medical-Diff-VQA [94], PMC-CaseReport [260], GMAI-VL [134] Screen2Words [240], WebSight [122] Mementos [254], ShareGPT4Video [30], VideoGPT+ [174], ShareGPT4o-Video [35] VideoChat2-IT [131], EgoTaskQA [99], NTU RGB+D [152], CLEVRER [276], STAR [259], LSMDC [201] GUI Type: Video Datasets Captioning General QA Table 4: Summary of the pre-training data mixture of InternVL 2.5. Notably, we exclusively use conversaitonformat instruction data, and at this stage, only the MLP or both MLP and ViT parameters are trainable, allowing the incorporation of both lowand high-quality data."
        },
        {
            "title": "4.4 Pre-training Data Mixture",
            "content": "To comprehensively enhance the models performance and strengthen its ability to handle complex tasks in real-world scenarios, we collect broader and more diverse range of domain-specific data compared to the training corpus of InternVL 1.5 and 2.0. Notably, during the development of our models, we exclusively utilized conversation-format instruction data. At this stage, since only the parameters of MLP, or MLP and ViT are trainable, both low-quality and high-quality data will be incorporated. The goal is to enrich the models world knowledge as much as possible by exposing it to diverse domain data, thereby improving its generalization capabilities. As shown in Table 4, our training corpus is sourced from captioning, general QA, mathematics, charts, OCR, knowledge, grounding, documents, conversation, medical, and GUI tasks."
        },
        {
            "title": "4.5 Fine-tuning Data Mixture",
            "content": "As shown in Figure 7, from InternVL 1.5 to 2.0 and then to 2.5, the dataset has undergone iterative improvements in scale, quality, and diversity. In terms of data scale, the number of samples grows from 5.1M in InternVL 1.5 to 7.3M in InternVL 2.0, and further doubles to 16.3M in InternVL 2.5. For diversity, our training data spans multiple domains, including general QA, charts, documents, OCR, science, medical, GUI, code, mathematics, et al., while covering multiple modalities such as single-image, multi-image, video, and text. In InternVL 2.5, single-image data constituted the majority with 45.92% of tokens, while multi-image data accounted for 9.37%, video data contributed 39.79%, and pure-text data made up 4.92%. Compared to earlier versions, multi-image and video data achieved the most notable increases, leading to the enhanced multiimage and long video comprehension abilities of InternVL 2.5. Quality improvements were achieved through unifying conversation templates, using language models to score and refine data, removing repetitive patterns, 12 Task Type: Single-Image Datasets Dataset Captioning General QA Mathematics Chart OCR Knowledge Document Grounding Science Conversation Medical GUI TextCaps (en) [211], ShareGPT4o (en & zh) [35], InternVL-SA-1B-Caption (en & zh) [36], NewYorkerCaptionContest (en) [88], MMInstruct (en & zh) [155] VQAv2 (en) [74], GQA (en) [98], OKVQA (en) [178], Visual7W (en) [317], MMInstruct (en & zh) [155], VSR (en) [147], FSC147 (en) [197], Objects365-YorN (en) [208], Hateful-Memes (en) [111] GeoQA+ (en) [20], CLEVR-Math (en) [144], Super-CLEVR (en) [141], MapQA (en) [23], MAVIS (en) [300], Geometry3K (en) [164], TallyQA (en) [2], MetaMath (en) [281], GEOS (en) [206], UniGeo (en) [26], GeomVerse (en) [107], CMM-Math (zh) [154] ChartQA (en) [181], MMTab (en) [310], PlotQA (en) [187], FigureQA (en) [105], VisText (en) [226], LRV-Instruction (en) [148], ArxivQA (en) [132], TabMWP (en) [166], MMC-Inst (en) [149], DVQA (en) [104], UniChart (en) [182], SimChart9K (en) [263], Chart2Text (en) [191], FinTabNet (zh) [312], SciTSR (zh) [39], Synthetic Chart2Markdown (en) OCRVQA (en) [188], InfoVQA (en) [183], TextVQA (en) [212], ArT (en & zh) [40], HME100K (en) [288], COCO-Text (en) [238], CTW (zh) [287], LSVT (zh) [222], RCTW-17 (zh) [209], VCR (en & zh) [302], EST-VQA (en & zh) [253], ST-VQA (en) [17], EATEN (zh) [78], LLaVAR (en) [305], CASIA (zh) [146], Chinese-OCR (zh) [14], CyrillicHandwriting (ru) [239], IAM (en) [180], NAF (en) [52], POIE (en) [115], ReCTs (zh) [301], MTWI (zh) [82], TextOCR (en) [213], SROIE (en) [97], Synthetic Arxiv OCR (en), MTVQA (ko & ja & it & ru & de & fr & th & ar & vi) [227], Synthetic Image2Latex (en), Synthetic Handwritten OCR (zh), Synthetic Infographic2Markdown (en & zh) KVQA (en) [207], A-OKVQA (en) [205], ViQuAE (en) [123], iNaturalist2018 (en) [237], MovieNet (en) [95], ART500K (en) [176], KonIQ-10K (en) [91], Synthetic Multidisciplinary Knowledge / QA (en & zh) DocVQA (en) [42], Docmatix (en) [121], DocReason25K (en) [93], Sujet-Finance-QA-Vision (en) [217] RefCOCO/+/g (en) [280, 177], GPT4Gen-RD-BoxCoT (en) [27], All-Seeing-V2 (en) [250], V3Det (en & zh) [243], DsLMF (en) [272], COCO-ReM (en & zh) [214], TolokaVQA (en) [236] AI2D (en) [109], ScienceQA (en) [165], TQA (en) [110], ChemVLM Data (en & zh) [129] ALLaVA (en & zh) [25], Viet-ShareGPT4o (vi) [59], Cambrain-GPT4o (en) [234] , RLAIF-V (en) [282], Laion-GPT4V (en) [119], TextOCR-GPT4V (en) [102], WildVision-GPT4o (en) [171], Synthetic Real-World Conversations (en & zh) PMC-VQA (en) [303], VQA-RAD (en) [120], ImageCLEF (en) [72], PMC (en) [261], SLAKE (en & zh) [145], GMAI-VL (en & zh) [134], VQA-Med (en) [15], Medical-Diff-VQA (en) [94], PathVQA (en) [83], PMC-CaseReport (en) [260] Screen2Words (en) [240], WebSight (en) [122], Widget-Caption (en) [136], RICOSCA (en) [55], Seeclick (en) [37], ScreenQA (en) [92], AMEX (en) [22], AITW (en) [198], Odyssey (en) [168], UIBert (en) [12], AndroidControl (en) [135], Mind2Web (en) [57], OmniACT (en) [106], WaveUI (en) [4] Type: Multi-Image Datasets General QA Document Type: Video Datasets Img-Diff (en) [101], Birds-to-Words (en) [100], Spot-the-Diff (en) [100], MultiVQA (en) [100], NLVR2 (en) [216], ContrastiveCaption (en) [100], DreamSim (en) [100], InternVL-SA-1B-Caption (en & zh) [36] MP-DocVQA (en) [233], MP-Docmatix (en) [121] Captioning General QA GUI Type: Text Datasets Vript (en & zh) [269], OpenVid (en) [190], Mementos (en) [254], ShareGPT4o-Video (en & zh) [35], ShareGPT4Video (en & zh) [30], VideoGPT+ (en) [174] VideoChat2-IT (en & zh) [130, 131], EgoTaskQA (en) [99], NTU RGB+D (en) [152], CLEVRER (en) [276], LLaVA-Video (en) [307], FineVideo (en) [67], PerceptionTest (en) [193], HiREST (en) [291], STAR (en) [259], EgoSchema (en) [175], ScanQA (en) [10], LSMDC (en) [201] GUI-World (en) [24] UltraFeedback (en) [48], UltraChat (en) [58], Unnatural-Instructions (en) [90], NoRobots (en) [196], MOSS (en) [221], LIMA (en) [314], SlimOrca (en) [142], WizardLM-Evol-Instruct-70K (en) [265], Llama-3-Magpie-Pro (en) [266], Magpie-Qwen2-Pro (en & zh) [266], KOpen-HQ-Hermes-2.5-60K (ko) [179], Firefly (zh) [270], Dolly (en) [44], OpenAI-Summarize-TLDR (en) [21], Know-Saraswati-CoT (en) [114], FLAN (en) [258], FLANv2 (en & zh) [41] Code-Feedback (en) [311], Glaive-Code-Assistant (en) [73], XCoder-80K (en) [255], LeetCode (en & zh), Evol-Instruct-Code (en) [173], InternLM2-Code (en & zh) [19] Long-Instruction-with-Paraphrasing (en & zh) [286], LongCite (en & zh) [298], LongQLoRA (en) [271], LongAlpaca (en) [34] GSM8K-Socratic (en) [43], NuminaMath-CoT/TIR (en) [128], Orca-Math (en) [189], MathQA (en) [6], InfinityMATH (en) [295], InternLM2-Math (en & zh) [19] Synthetic Multidisciplinary Knowledge / QA (en) General QA Code Long Context Mathematics Knowledge Table 5: Summary of the fine-tuning data mixture of InternVL 2.5. We expanded our fine-tuning data mixture through extensive collection of open-source datasets and self-synthesized data. This mixture is predominantly in English (en) and Chinese (zh), with smaller portions in other languages, including Korean (ko), Japanese (ja), Italian (it), Russian (ru), German (de), French (fr), Thai (th), Arabic (ar), and Vietnamese (vi). 13 Model Name LLaVA-OneVision-0.5B [124] InternVL2-1B [35] InternVL2.5-1B Qwen2-VL-2B [246] Aquila-VL-2B [76] InternVL2-2B [35] InternVL2.5-2B Phi-3.5-Vision-4B [1] InternVL2-4B [35] InternVL2.5-4B Ovis1.6-Gemma2-9B [169] MiniCPM-V2.6 [274] Qwen2-VL-7B [246] InternVL2-8B [35] InternVL2.5-8B InternVL-Chat-V1.5 [35] InternVL2-26B [35] InternVL2.5-26B Cambrian-34B [234] VILA-1.5-40B [143] InternVL2-40B [35] InternVL2.5-38B GPT-4V [192] GPT-4o-20240513 [192] Claude-3.5-Sonnet [8] Gemini-1.5-Pro [200] LLaVA-OneVision-72B [124] NVLM-D-72B [50] Molmo-72B [54] Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B MMMU (val) 31.4 36.7 40.9 41.1 47.4 36.3 43.6 43.0 47.9 52.3 55.0 49.8 54.1 52.6 56.0 46.8 51.2 60.0 49.7 55.1 55.2 63.9 63.1 69.1 68.3 62.2 56.8 59.7 54.1 64.5 62.7 70.1 MMMU (test) 32.8 35.8 34.7 38.2 41.4 46.3 44.3 48.9 41.0 43.8 51.8 46.9 49.3 57.6 54.6 55.1 61.8 MMMU-Pro (std10 / vision / overall) 16.0 / 13.6 / 14.8 23.3 / 15.5 / 19.4 25.3 / 17.2 / 21.2 21.6 / 14.9 / 18.2 27.3 / 20.1 / 23.7 26.3 / 13.1 / 19.7 28.2 / 21.3 / 24.7 36.4 / 29.0 / 32.7 30.2 / 24.2 / 27.2 34.1 / 27.0 / 30.5 32.5 / 25.4 / 29.0 38.2 / 30.4 / 34.3 29.5 / 19.9 / 24.7 32.8 / 27.1 / 30.0 41.6 / 32.6 / 37.1 35.9 / 14.1 / 25.0 36.3 / 32.1 / 34.2 48.0 / 44.0 / 46.0 54.0 / 49.7 / 51.9 55.0 / 48.0 / 51.5 49.4 / 44.4 / 46.9 38.0 / 24.0 / 31.0 49.2 / 43.3 / 46.2 41.9 / 38.0 / 40.0 51.4 / 45.9 / 48.6 MathVista (mini) 34.8 37.7 43.2 43.0 59.0 46.3 51.3 43.9 58.6 60.5 67.2 60.6 58.2 58.3 64.4 53.5 59.4 67.7 53.2 49.5 63.7 71.9 58.1 63.8 67.7 63.9 67.5 66.6 58.6 70.5 65.5 72.3 MATH-Vision (mini / full) 12.2 / 11.1 16.8 / 14.4 19.7 / 12.4 21.1 / 18.4 15.8 / 12.1 13.5 / 14.7 17.4 / 15.5 17.8 / 16.5 21.7 / 20.9 / 18.8 16.1 / 17.5 22.0 / 16.3 20.4 / 18.4 22.0 / 19.7 15.8 / 15.0 23.4 / 17.0 28.0 / 23.1 21.4 / 16.9 32.2 / 31.8 / 24.0 / 30.4 / 19.2 / 25.9 23.7 / 23.6 34.9 / 32.2 MathVerse (mini) 17.9 18.4 28.0 21.0 26.2 25.3 30.6 24.1 32.0 37.1 25.7 31.9 37.0 39.5 28.4 31.1 40.1 36.3 49.4 32.8 50.2 39.1 42.8 51.7 Olympiad Bench 0.3 1.7 0.4 2.0 1.1 3.0 1.9 4.9 0.6 3.5 8.8 3.9 12.1 18.0 25.9 5.5 11.6 Table 6: Comparison of multimodal reasoning and mathematical performance. MMMU [289] and MMMU-Pro [290] are multidisciplinary reasoning benchmarks, while MathVista [163], MATH-Vision [245], MathVerse [299], and OlympiadBench [80] are mathematics benchmarks. Part of results are collected from [54, 8, 290, 245, 299, 80] and the OpenCompass leaderboard [46]. applying heuristic rules to filter low-quality samples, and rewriting short responses into high-quality and longer interactions. This ensured robust dataset for model training."
        },
        {
            "title": "5 Evaluation on Multimodal Capability",
            "content": "To comprehensively evaluate InternVLs performance on multimodal tasks, we employ diverse set of benchmarks, including both well-established classic datasets and newly introduced ones provided by VLMEvalKit [63]. These benchmarks span wide range of categories, aiming to provide thorough and balanced assessment of InternVLs capabilities across various multimodal tasks."
        },
        {
            "title": "5.1.1 Benchmarks",
            "content": "We evaluate InternVLs multimodal mathematical and reasoning capabilities through comprehensive assessment across various discipline-related benchmarks. MMMU [289]: MMMU is benchmark evaluating MLLMs on college-level tasks across six disciplines, testing expert-level reasoning and advanced perception in specific fields. We report the maximum accuracy achieved across both direct-answer and CoT reasoning approaches on the MMMU validation and test sets. MMMU-Pro [290]: MMMU-Pro is an upgraded version of the MMMU benchmark, designed to more accurately and rigorously evaluate the multimodal understanding and reasoning capabilities of models in wide range of 14 Figure 9: CoT prompts used in our model testing. By leveraging these prompts for CoT reasoning, we can scale up testing time, significantly enhancing the performance of InternVL 2.5 models on MMMU [289]. academic disciplines. We report three metrics: standard (10 options), vision, and overall (the average of standard and vision). Here, standard and vision are the maximum scores from the CoT and direct-answer settings, consistent with the original paper. MathVista [163]: MathVista is benchmark for evaluating MLLMs mathematical reasoning in visual contexts, encompassing reasoning types such as algebra, geometry, and statistics. We report the scores on the testmini set. MATH-Vision [245]: MATH-Vision is high-quality dataset of 3,040 visually contextualized math problems sourced from real competitions. We report performance on both the testmini and full sets. MathVerse [299]: MathVerse is visual math benchmark for evaluating MLLMs in solving diagram-based math problems. It comprises 2,612 high-quality, multi-subject math problems, each transformed into six distinct versions with varying degrees of visual and textual information. We report performance on the testmini set. OlympiadBench [80]: OlympiadBench is bilingual, multimodal benchmark with high-difficulty math and physics problems from Olympiad competitions and Gaokao. Each problem is annotated with expert-level step-by-step reasoning, enabling detailed assessment of logical deduction and problem-solving abilities. This benchmark is challenging, and well-defined CoT prompt can significantly improve performance."
        },
        {
            "title": "5.1.2 Evaluation Results",
            "content": "Multidisciplinary reasoning ability reflects models capacity to comprehend, process, and manipulate abstract concepts, which is crucial for complex problem-solving and decision-making tasks. In the left section of Table 6, we provide comparison of InternVL 2.5s performance on multidisciplinary reasoning-related benchmarks, including MMMU [289] and MMMU-Pro [290]. Here, we test both direct-answer and CoT reasoning performance, reporting the higher score. The results suggest that our model achieves encouraging improvements over existing open-source models, such as LLaVAOneVision [124], NVLM [50], VILA 1.5 [143], and Qwen2-VL [246], as well as notable progress compared to earlier versions of the InternVL2 series. Specifically, InternVL2.5-78B achieves score exceeding 70 on the MMMU validation set, representing 7.4-point improvement over InternVL2-Llama3-76B. These results indicate that our models performance is moving closer to that of some advanced closed-source models, such as GPT-4o [192], Claude-3.5-Sonnet [8], and Gemini-1.5-Pro [200]. Additionally, through majority voting, the score of InternVL2-Llama3-76B on the MMMU benchmark is improved from 62.7 to 65.3 when using CoT. We observe similar phenomenon in InternVL 2.5 as well, which demonstrates that test-time scaling can improve the CoT reasoning of MLLMs. Mathematical reasoning reflects higher-level reasoning capability and enhances the potential of MLLMs in scientific and engineering applications. In the right-hand section of Table 6, we present InternVL 2.5s performance across four multimodal mathematical benchmarks. These results demonstrate significant progress over InternVL 2.0. Notably, InternVL2.5-78B achieved an accuracy of 72.3% on the MathVista test-mini set [163]. Additionally, on the challenging OlympiadBench [80], the InternVL 2.5 series showed an overall improvement compared to the 2.0 series. We attribute part of this advancement to our data filtering pipeline. 15 Model Name AI2D (w / wo M) 64.1 / 70.5 69.3 / 77.8 74.7 / 84.6 75.0 / 74.1 / 82.3 74.9 / 83.5 77.8 / 87.6 78.9 / 87.8 81.4 / 90.5 84.4 / 82.1 / LLaVA-OneVision-0.5B [124] 57.1 / InternVL2-1B [35] InternVL2.5-1B Qwen2-VL-2B [246] Aquila-VL-2B [76] InternVL2-2B [35] InternVL2.5-2B Phi-3.5-Vision-4B [1] InternVL2-4B [35] InternVL2.5-4B Ovis1.6-Gemma2-9B [169] MiniCPM-V2.6 [274] Molmo-7B-D [54] Qwen2-VL-7B [246] InternVL2-8B [35] InternVL2.5-8B InternVL-Chat-V1.5 [35] InternVL2-26B [35] InternVL2.5-26B Cambrian-34B [234] VILA-1.5-40B [143] InternVL2-40B [35] InternVL2.5-38B GPT-4V [192] GPT-4o-20240513 [192] Claude-3-Opus [8] Claude-3.5-Sonnet [8] Gemini-1.5-Pro [200] LLaVA-OneVision-72B [124] NVLM-D-72B [50] Molmo-72B [54] Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B / 93.2 83.0 / 92.1 83.8 / 91.7 84.5 / 92.8 80.7 / 89.8 84.5 / 92.5 86.4 / 94.4 79.5 / 69.9 / 86.6 / 94.5 87.6 / 95.1 78.2 / 89.4 84.6 / 94.2 70.6 / 88.1 81.2 / 94.7 79.1 / 94.4 85.6 / 85.2 / 94.2 / 96.3 88.1 / 87.6 / 94.8 89.1 / 95.7 ChartQA (test avg) 61.4 72.9 75.9 73.5 76.5 76.2 79.2 81.8 81.5 84.0 82.4 84.1 83.0 83.3 84.8 83.8 84.9 87.2 75.6 67.2 86.2 88.2 78.5 85.7 80.8 90.8 87.2 83.7 86.0 87.3 88.3 88.4 88.3 TextVQA (val) 70.5 72.0 79.7 76.4 73.4 74.3 72.0 74.4 76.8 80.1 81.7 84.3 77.4 79.1 80.6 82.3 82.4 76.7 73.6 83.0 82.7 78.0 77.4 67.5 74.1 78.8 80.5 82.1 83.1 85.5 84.4 83. DocVQA (test) 70.0 81.7 84.8 90.1 85.0 86.9 88.7 69.3 89.2 91.6 90.8 92.2 94.5 91.6 93.0 90.9 92.9 94.0 75.5 93.9 95.3 88.4 92.8 89.3 95.2 93.1 91.3 92.6 93.5 96.5 94.1 95.1 InfoVQA (test) 41.8 50.9 56.0 65.5 58.3 58.9 60.9 36.6 67.0 72.1 72.6 76.5 74.8 77.6 72.5 75.9 79.8 46.0 78.7 83.6 75.1 79.2 55.6 74.3 81.0 74.9 81.9 84.5 82.0 84.1 OCR Bench 565 754 785 809 772 784 804 599 788 828 830 852 694 866 794 822 724 825 852 600 460 837 842 645 736 694 788 754 741 853 877 839 854 SEED-2 Plus 54.3 59.0 62.4 63.0 60.0 60.9 62.2 63.9 66.9 65.7 69.0 67.5 69.7 66.3 67.6 70.8 69.2 71.2 53.8 72.0 44.2 71.7 69.7 71.3 CharXiv (RQ / DQ) 18.1 / 30.7 19.0 / 38.4 21.0 / 40.6 21.3 / 49.7 24.5 / 48.0 24.9 / 61.7 31.0 / 57.1 31.2 / 56.1 32.9 / 68.6 29.2 / 58.5 33.4 / 62.4 35.9 / 73.5 27.3 / 59.7 24.0 / 38.7 32.3 / 66.0 42.4 / 79.6 37.1 / 79.9 47.1 / 84.5 30.2 / 71.6 60.2 / 84.3 43.3 / 72.0 38.9 / 75.2 42.4 / 82.3 VCR-EN-Easy (EM / Jaccard) 21.5 / 48.4 91.5 / 97.0 81.5 / 70.0 / 32.9 / 59.2 93.2 / 97.6 39.3 / 60.4 33.7 / 61.1 93.7 / 97.8 73.9 / 85.7 89.7 / 93.8 37.9 / 61.5 92.6 / 97.4 14.7 / 51.4 74.5 / 86.7 94.4 / 98.0 79.7 / 89.3 84.7 / 92.6 94.7 / 98.2 52.0 / 65.4 91.6 / 96.4 62.0 / 77.7 63.9 / 74.7 62.7 / 77.7 91.3 / 94.6 83.2 / 91.3 95.7 / 94. Table 7: Comparison of OCR, chart, and document understanding performance. We evaluate OCR-related capabilities across 9 benchmarks, including AI2D [109], ChartQA [181], TextVQA [212], DocVQA [184], InfoVQA [183], OCRBench [158], SEED-2-Plus [125], CharXiv [257], and VCR [302]. Part of results are collected from [64, 54, 8, 257, 302] and the OpenCompass leaderboard [46]. Specifically, we observed that the 2.0 models frequently encountered deadlocks during CoT reasoning, failing to reach correct final answers, while this issue has been mitigated in the 2.5 series."
        },
        {
            "title": "5.2.1 Benchmarks",
            "content": "We assess InternVLs OCR, chart, and document understanding capabilities through comprehensive evaluation on variety of OCR-related datasets. AI2D [109]: AI2D is dataset of over 5,000 elementary school science diagrams, each with detailed annotations and corresponding multiple-choice questions. For fair comparison, we report results for both mask and no mask settings on the test set. ChartQA [181]: ChartQA is dataset focused on assessing models abilities to interpret and reason with data visualizations such as charts and graphs. Our evaluation metric is the average relaxed accuracy across both human and augmented test sets in ChartQA. TextVQA [212]: TextVQA is dataset designed to benchmark visual reasoning based on text within images. It requires models to read and interpret text in images to accurately answer related questions. We report the VQA accuracy on the TextVQA validation set. 16 DocVQA [42]: DocVQA is dataset aimed at evaluating models ability to comprehend and retrieve information from text within document images. Performance is reported on the test set using the ANLS metric, which captures answer accuracy by measuring text similarity. InfoVQA [183]: InfographicVQA is dataset aimed at evaluating models ability to interpret and reason with complex infographics that combine text, graphics, and visual elements. Performance is measured using the ANLS metric on the test set. OCRBench [158]: OCRBench evaluates the OCR capabilities of MLLMs across five tasks: text recognition, scene text VQA, document VQA, key information extraction, and handwritten math expression recognition, with maximum score of 1000. SEEDBench-2-Plus [125]: SEED-Bench-2-Plus evaluates MLLMs on text-rich visual tasks, with 2,300 humanannotated questions across charts, maps, and webs. We report the average accuracy on this dataset. CharXiv [257]: CharXiv is comprehensive evaluation suite featuring 2,323 charts from scientific papers. It includes two types of questions: reasoning questions (RQ) requiring synthesis of complex visual information, and descriptive questions (DQ) assessing basic chart element understanding. VCR [302]: Visual Caption Restoration (VCR) is task that involves restoring partially hidden text within images by understanding both the visual content and the text. We report the Exact Match (EM) score and Jaccard similarity on the VCR-EN-Easy subset."
        },
        {
            "title": "5.2.2 Evaluation Results",
            "content": "Table 7 provides detailed comparison of InternVL 2.5 with its predecessor InternVL 2.0, other representative open-source models (e.g., Qwen2-VL [246], LLaVA-OneVision [124]), and closed-source models (e.g., GPT4o [192], Claude-3.5-Sonnet [8]) on OCR-related tasks. Across most benchmarks, InternVL 2.5 achieves significant improvements over InternVL 2.0 at all model scales and demonstrates performance comparable to the current state-of-the-art model, Qwen2-VL-72B [246], reflecting the effectiveness of the improvements in training strategies and data quality. However, at the 2B scale, InternVL2.5-2B underperforms compared to Qwen2-VL-2B on benchmarks such as TextVQA [212], DocVQA [184], and InfoVQA [183]. We suspect that, in addition to differences in data and training strategies, model architecture may also play significant role. Specifically, Qwen2-VL-2B features 600M vision encoder and 1.5B language model, whereas InternVL2.5-2B employs smaller 300M vision encoder paired with 1.8B language model. It appears that, for smaller-scale MLLM (e.g., 2B), the size of the vision encoder plays relatively important role in OCR performance, given the same total parameter budget. Additionally, InternVL 2.5 demonstrates exceptional performance on the visual caption restoration (VCR) task [302]. The 2.5 series achieves significant improvement over InternVL 2.0 on this task, with the 2B model reaching EM/Jaccard scores of 93.2/97.6, far surpassing the previous generations 32.9/59.2. This improvement can be attributed to the introduction of small portion of the VCR training set (approximately 22K samples). We find that the models poor performance on VCR tasks was not due to inadequate OCR capabilities but rather to its insufficient instruction-following ability for task-specific directives. By leveraging these few but focused samples, InternVL 2.5 exhibits remarkable enhancement in its instruction-following ability for the VCR task, resulting in substantial performance boost."
        },
        {
            "title": "5.3.1 Benchmarks",
            "content": "We assess InternVLs capabilities in multi-image relation perception and understanding across various multiimage benchmarks. BLINK [70]: The BLINK benchmark evaluates the core visual perception capabilities of MLLMs through 14 tasks inspired by classic computer vision challenges. Over half of the questions involve multiple images. Our results are reported on the validation set. Mantis-Eval [100]: Mantis-Eval is meticulously curated small-scale benchmark for evaluating MLLMs reasoning capabilities across multiple images. It comprises 217 challenging, human-annotated problems covering topics such as size perception and weight comparison. 17 Model Name LLaVA-OneVision-0.5B [124] InternVL2-1B [35] InternVL2.5-1B Qwen2-VL-2B [246] InternVL2-2B [35] InternVL2.5-2B Phi-3.5-Vision-4B [1] InternVL2-4B [35] InternVL2.5-4B Qwen2-VL-7B [246] MiniCPM-V2.6 [274] InternVL2-8B [35] InternVL2.5-8B InternVL-Chat-V1.5 [35] InternVL2-26B [35] InternVL2.5-26B Cambrian-34B [234] InternVL2-40B [35] InternVL2.5-38B GPT-4V [192] GPT-4o-20240513 [192] Claude-3.5-Sonnet [8] Gemini-1.5-Pro [200] LLaVA-OneVision-72B [124] Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B BLINK (val) 52.1 38.6 42.0 44.4 43.8 44.0 58.3 46.1 50.8 53.2 53.0 50.9 54.8 46.6 56.2 61.8 57.2 63.2 54.6 68.0 55.4 56.8 63.8 Mantis Eval 39.6 46.1 51.2 48.4 54.8 61.3 62.7 69.0 65.4 67.7 66.8 69.6 75.6 71.4 78.3 62.7 77.6 73.7 77.0 MMIU 37.3 38.5 39.8 43.5 43.3 43.8 42.0 46.7 37.4 42.6 49.4 47.9 55.3 55.7 53.4 53.4 44.2 55.8 Muir Bench 25.5 29.3 29.9 32.5 40.6 40.5 45.2 48.7 51.1 38.5 50.6 61.1 54.4 62.7 62.3 68.0 54.8 51.2 63.5 MMT (val) 49.5 50.3 55.1 50.4 54.5 53.6 55.7 62.4 64.0 60.8 60.0 62.3 58.0 60.6 66.9 66.2 70.0 64.3 65.4 64.5 71.8 67.4 70. MIRB (avg) 31.5 35.6 32.1 36.4 39.9 51.7 50.0 52.5 50.3 53.7 55.7 55.2 61.2 53.1 58.2 61.1 RealWorld QA 55.6 50.3 57.5 62.6 57.3 60.1 53.6 60.7 64.3 70.1 65.0 64.4 70.1 66.0 68.3 74.5 67.8 71.8 73.5 61.4 75.4 60.1 67.5 71.9 77.8 72.2 78.7 MME-RW (EN) 40.2 44.2 47.3 48.8 52.1 55.3 56.5 53.5 59.1 49.4 58.7 61.8 44.1 61.8 64.0 45.2 51.6 38.2 63.0 62.9 WildVision (win rate) 17.8 43.4 31.8 44.2 44.2 49.4 54.4 62.0 56.6 62.2 65.2 63.2 66.4 71.8 80.6 65.8 71.4 R-Bench (dis) 55.6 59.0 56.8 62.2 55.5 64.5 66.1 64.0 67.9 70.1 67.9 70.1 72.9 73.3 72.1 65.6 77.7 74.1 77.2 Table 8: Comparison of multi-image and real-world understanding performance. Multi-image benchmarks include BLINK [70], Mantis-Eval [100], MMIU [186], MuirBench [241], MMT-Bench [277], and MIRB [308]. Real-world benchmarks encompass RealWorldQA [47], MME-RealWorld [306], WildVision [171], and RBench [126]. Part of the results are sourced from the benchmark papers and the OpenCompass leaderboard [46]. MMIU [186]: MMIU is an extensive benchmark suite developed to rigorously assess the performance of MLLMs in multi-image tasks. It encompasses 7 distinct types of multi-image relationships and spans 52 diverse tasks, providing comprehensive framework for evaluation. MuirBench [241]: MuirBench is comprehensive benchmark for evaluating MLLMs capabilities in multi-image understanding. It spans 12 tasks and 10 types of multi-image relations and enhances model assessment with unanswerable instance variants. MMT-Bench [277]: MMT-Bench evaluates MLLMs on multimodal tasks like driving and navigation, focusing on recognition, reasoning, and planning, with many sub-tasks requiring multi-image understanding. To speed up testing, results are reported on the validation set. MIRB [308]: MIRB is benchmark designed to evaluate the ability of MLLMs to understand and reason across multiple images. It contains four task categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. The reported performance is the average score across these four categories."
        },
        {
            "title": "5.3.2 Evaluation Results",
            "content": "As multi-image content becomes an increasingly common form of information exchange on the internet, it is essential for models to possess the ability to simultaneously understand and analyze relationships between multiple images. In the left part of Table 8, we evaluate the multi-image understanding capabilities of InternVL 2.5 across six diverse benchmarks: BLINK [70], Mantis-Eval [100], MMIU [186], MuirBench [241], MMTBench [277], and MIRB [308]. These benchmarks test range of skills, including reasoning across images, integrating information, and addressing task-specific requirements. InternVL 2.5 achieves consistent improvements over InternVL 2.0 across all model scales, reflecting enhanced reasoning ability and better integration of multi-image information. For instance, at the 2B scale, InternVL2.5-2B delivers significant gains on Mantis-Eval (54.8 vs. 48.4) and MuirBench (40.6 vs. 32.5). These advancements can be largely attributed to the inclusion of additional multi-image datasets, as detailed in Section 4.5. These 18 datasets, which were carefully curated and of high quality, played critical role in improving the models ability to understand and reason across multiple visual inputs. At larger scales, InternVL 2.5 demonstrates substantial progress and achieves competitive performance with advanced closed-source models. For example, InternVL2.5-78B scores 55.8 on MMIU, closely matching GPT-4os 55.7, and achieves score of 70.8 on MMT-Bench, surpassing GPT-4os 65.4. These results highlight the importance of scaling model size and incorporating high-quality training data specifically tailored for multi-image tasks. However, on BLINK and MuirBench, our model still exhibits performance gap of around 5 points compared to GPT-4o [192], suggesting that further improvements are needed, potentially through the inclusion of additional high-quality multi-image training data."
        },
        {
            "title": "5.4.1 Benchmarks",
            "content": "We assess InternVLs performance on suite of real-world benchmarks designed to evaluate its capabilities on realistic and complex tasks. RealWorldQA [47]: RealWorldQA is benchmark designed to evaluate the real-world spatial understanding capabilities of MLLMs. It contains more than 700 images, each accompanied by question and verifiable answer, from various real-world scenarios. MME-RealWorld [306]: MME-RealWorld is benchmark for evaluating MLLMs on complex, high-resolution image tasks across 43 real-world scenarios in 5 domains. Here, we test the English full set of the dataset. WildVision [171]: WildVision-Bench is benchmark designed to evaluate MLLMs in the wild with human preferences. It comprises 500 high-quality samples meticulously curated from real-world user QA interactions. The benchmark uses win rate metric to quantify the performance of models, providing insights into their ability to meet human expectations in practical applications. R-Bench [126]: R-Bench is benchmark designed to evaluate the robustness of MLLMs against real-world image distortions, measuring their resilience in handling corrupted images in practical scenarios. We report the absolute robustness overall score for the MCQ task, which is the average score across low, mid, and high difficulty levels, corresponding to R-Bench-Dis in VLMEvalKit."
        },
        {
            "title": "5.4.2 Evaluation Results",
            "content": "Given the complexity and dynamic nature of real-world environments, models must be robust enough to handle wide range of challenging conditions. As shown in the right part of Table 8, InternVL 2.5 achieves leading performance across four real-world understanding benchmarks, including RealWorldQA [47], MMERealWorld [306], WildVision [171], and R-Bench [126], and significantly outperforms the previous version, InternVL 2.0. This indicates that InternVL 2.5 has stronger potential for practical application in complex and ever-changing real-world scenarios. In benchmarks like RealWorldQA, MME-RealWorld, and R-Bench, which involve multiple-choice questions, InternVL 2.5 demonstrates strong real-world perceptual and understanding abilities. Differently, the WildVision benchmark uses GPT-4o [192] as the judge model to evaluate the performance of various MLLM against the reference model, Claude-3-Sonnet [8]. In this benchmark, the models output quality and user experience are key metrics. Although InternVL2.5-78B performs well in providing concise answers, it still shows gap when generating longer responses to match human preferences. Specifically, InternVL2.5-78B scores 71.4, while GPT-4o scores 80.6, indicating notable difference in user experience. These results indicate that, while InternVL 2.5 delivers accurate and concise responses across most tasks, there is potential for improvement in generating more personalized and detailed answers. Future work will focus on enhancing the models performance in open-ended tasks and complex interactions, aiming to better align with human preferences, bridge the gap in user experience with GPT-4o."
        },
        {
            "title": "5.5.1 Benchmarks",
            "content": "We evaluate InternVLs comprehensive multimodal capabilities through range of benchmarks, including: 19 MME [68]: MME is the first comprehensive evaluation benchmark designed for MLLMs. It assesses models perception and cognitive abilities across 14 subtasks, including object presence, counting, position, color recognition, as well as commonsense reasoning, numerical computation, text translation, and code reasoning. We report the overall score across all tasks. MMBench [156]: MMBench evaluates the multimodal understanding of MLLMs through nearly 3,000 multiplechoice questions spanning 20 dimensions. It supports both English and Chinese versions, and we present the models performance scores on the test set. MMBench v1.1 [156]: Compared to MMBench, MMBench v1.1 features refined dataset with small number of noisy or low-quality questions removed, resulting in subtle improvement in overall data quality. We report the models performance on the English version of the test set. MMVet [283]: MMVet is benchmark designed to assess the integrated capabilities of MLLMs on complex tasks. It evaluates six core competencies: recognition, knowledge, spatial awareness, language generation, OCR, and mathematics, across 16 integrated tasks. Note that VLMEvalKit uses GPT-4-Turbo as the scoring model for this benchmark, which yields slightly lower scores compared to the official evaluation server. MMVet v2 [284]: Expanding on MMVet, MMVet v2 introduces an enhanced benchmark with new capability: image-text sequence understanding, allowing for the assessment of models ability to process interleaved content. Here, we utilize the official evaluation server for scoring, which employs GPT-4-0613 as the scoring model. MMStar [28]: MMStar is benchmark for evaluating the multimodal capabilities of MLLMs. It includes 1,500 carefully curated samples focusing on advanced visual and language understanding, minimizing data leakage, and emphasizing visual dependency."
        },
        {
            "title": "5.5.2 Evaluation Results",
            "content": "Comprehensive multimodal evaluation benchmarks, such as MME [68], the MMBench series [156], the MMVet series [283, 284], and MMStar [28], provide valuable and widely adopted frameworks for assessing model performance across diverse set of multimodal tasks. As shown in the left section of Table 9, the InternVL 2.5 models consistently outperform the previous InternVL 2.0 series across various model sizes, especially for smaller models with 1B-8B parameters. For example, in the MMBench v1.0 benchmark, which evaluates tasks in both English and Chinese, the InternVL 2.5 models show significant improvements. The InternVL2.5-4B achieves score of 81.1/79.3, surpassing the InternVL2-4Bs 78.6/73.9, while the InternVL2.5-8B reaches 84.6/82.6, outperforming the InternVL2-8Bs 81.7/81.2. It is also noteworthy that, while we have significantly improved the performance of smaller models on the MMVet series benchmarks, our largest model, InternVL2.5-78B, still does not surpass the Qwen2-VL-72B [246]. Currently, the state-of-the-art models on MMVet v2 remain closed-source models like GPT-4o [192] and Claude3.5-Sonnet [8]. This highlights the gap between open-source models and closed-source ones in multimodal integrated capability. We recognize this as an important direction for future development."
        },
        {
            "title": "5.6.1 Benchmarks",
            "content": "We evaluate InternVLs tendency toward hallucinations across four different benchmarks, including: HallusionBench [77]: HallusionBench is benchmark for evaluating image-context reasoning in MLLMs through Yes/No judgment question format, focusing on challenges such as language hallucination and visual illusion. We report performance using the average scores of its three metrics: aAcc, fAcc, and qAcc. MMHal-Bench [223]: MMHal-Bench is benchmark designed to evaluate hallucinations in MLLMs. It includes 96 challenging questions derived from images in the OpenImages dataset, along with their corresponding groundtruth answers and image content. Scoring is conducted using GPT-4o, with scores ranging from 0 to 6. CRPE [250]: CRPE is benchmark that measures the hallucination level of the relation between objects using multiple-choice questions. We report accuracy on the relation subset for this benchmark. POPE [139]: POPE is benchmark for evaluating object hallucination in MLLMs, utilizing binary questions to quantify and analyze hallucination tendencies. We report the average F1 score across three categories: random, popular, and adversarial. Model Name MME (sum) MMB (EN / CN) LLaVA-OneVision-0.5B [124] 1438.0 61.6 / 55.5 1794.4 65.4 / 60.7 InternVL2-1B [35] 1950.5 70.7 / 66.3 InternVL2.5-1B 1872.0 74.9 / 73.5 Qwen2-VL-2B [246] 1876.8 73.2 / 70.9 InternVL2-2B [35] 2138.2 74.7 / 71.9 InternVL2.5-2B Phi-3.5-Vision-4B [1] 76.0 / 66.1 2059.8 78.6 / 73.9 InternVL2-4B [35] 2337.5 81.1 / 79.3 InternVL2.5-4B 2326.8 83.0 / 80.5 Qwen2-VL-7B [246] 2348.4 81.5 / 79.3 MiniCPM-V2.6 [274] 2210.3 81.7 / 81.2 InternVL2-8B [35] 2344.1 84.6 / 82.6 InternVL2.5-8B 2194.2 82.2 / 82.0 InternVL-Chat-V1.5 [35] 2260.7 83.4 / 82.0 InternVL2-26B [35] 2373.3 85.4 / 85.5 InternVL2.5-26B Cambrian-34B [234] 80.4 / 79.2 2307.5 86.8 / 86.5 InternVL2-40B [35] 2455.8 86.5 / 86.3 InternVL2.5-38B 1926.6 81.0 / 80.2 GPT-4V [192] 83.4 / 82.1 GPT-4o-20240513 [192] Claude-3-Opus [8] 1586.8 63.3 / 59.2 82.6 / 83.5 Claude-3.5-Sonnet [8] Gemini-1.5-Pro [200] 73.9 / 73.8 LLaVA-OneVision-72B [124] 2261.0 85.8 / 85.3 2482.7 86.5 / 86.6 Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] 2414.7 86.5 / 86.3 2494.5 88.3 / 88.5 InternVL2.5-78B MMBv1.1 (EN) 59.6 61.6 68.4 72.2 70.2 72.2 72.1 75.8 79.3 80.7 78.0 79.5 83.2 80.3 81.5 84.2 78.3 85.1 85.5 80.0 83.1 60.1 80.9 74.6 85.0 85.9 85.5 87.4 MMVet (turbo) 32.2 32.7 48.8 49.5 39.5 60.8 43.2 51.0 60.6 62.0 60.0 54.2 62.8 61.5 62.1 65.0 53.2 65.5 68.8 67.5 69.1 51.7 70.1 64.0 60.6 74.0 65.7 72.3 MMVetv2 (0613) 36.1 43.2 39.6 52.3 46.6 55.4 52.3 58.1 51.5 57.2 60.8 63.8 62.1 66.3 71.0 55.8 71.8 66.9 66.9 68.4 65.5 MMStar 37.7 45.7 50.1 48.0 50.1 53.7 47.5 54.3 58.3 60.7 57.5 62.0 62.8 57.3 61.2 66.5 54.2 65.4 67.9 56.0 64.7 45.7 65.1 59.1 65.8 68.3 67.4 69. HallBench (avg) 27.9 34.0 39.0 41.7 37.9 42.6 40.5 41.9 46.3 50.6 48.1 45.2 50.1 50.3 50.7 55.0 41.6 56.9 56.8 46.5 55.0 37.8 55.5 45.6 49.0 58.1 55.2 57.4 MMHal (score) 2.25 2.49 2.52 2.94 2.75 3.31 3.40 3.60 3.33 3.65 3.11 3.55 3.70 3.75 3.71 4.00 3.83 3.89 CRPE (relation) 57.5 60.9 66.3 70.2 71.1 75.5 74.4 75.2 75.8 78.4 75.4 75.6 79.1 77.6 78.3 76.6 77.6 78.8 POPE (avg) 87.3 89.9 88.3 90.6 87.2 90.9 88.1 87.3 86.9 90.6 88.4 88.0 90.6 88.4 90.7 86.9 89.0 90.8 Table 9: Comparison of comprehensive multimodal understanding and hallucination performance. Comprehensive multimodal benchmarks include MME [68], MMBench series [156], MMVet series [283, 284], and MMStar [28]. Hallucination benchmarks encompass HallusionBench [77], MMHal [223], CRPE [250], and POPE [139]. Part of the results are sourced from the benchmark papers and the OpenCompass leaderboard [46]."
        },
        {
            "title": "5.6.2 Evaluation Results",
            "content": "We evaluate the performance of InternVL on four key hallucination evaluation benchmarks: HallusionBench [77], MMHal [223], CRPE [250], and POPE [139]. These benchmarks assess the frequency of hallucinations, or factual inaccuracies, across multimodal tasks, providing measure of model reliability in handling complex inputs like text and images. The InternVL 2.5 models show significant progress over the InternVL 2.0 series, particularly in smaller models (e.g., 1B-8B parameters). For instance, InternVL2.5-1B and InternVL2.5-2B demonstrate improved scores on all hallucination benchmarks, with the 1B model achieving 39.0 score on HallusionBench, up from 34.0 in the earlier version. Similarly, the 2B model improved to 42.6, outperforming the previous 2B model by nearly 5 points. These results indicate substantial gains in reducing hallucinations while handling multimodal data. The largest model, InternVL2.5-78B, also shows improvements, reducing hallucinations compared to both prior versions and other leading models. It scores 57.4 on HallusionBench, competing with top models like Qwen2VL-72B (58.1) and GPT-4o (55.0). Although InternVL2.5-78B demonstrates relatively low hallucination rates on these hallucination evaluation benchmarks, some hallucinations are still inevitably present when generating long responses in practical use. This is challenge we plan to tackle in future work."
        },
        {
            "title": "5.7.1 Benchmarks",
            "content": "We evaluate InternVLs visual grounding capability via referring expression comprehension (REC) on the RefCOCO, RefCOCO+, and RefCOCOg datasets, where the model identifies target objects in images from given descriptions. 21 Model Name Grounding-DINO-L [153] UNINEXT-H [267] ONE-PEACE [247] Shikra-7B [27] Ferret-v2-13B [297] CogVLM-Grounding-17B [248] MM1.5 [296] Qwen2-VL-7B [246] TextHawk2 [285] InternVL2-8B [35] InternVL2.5-8B Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B RefCOCO test-A 93.2 94.3 94.2 90.6 95.0 94.8 92.5 93.6 93.0 91.1 94.5 95.3 94.8 95.6 test-B 88.2 91.5 89.3 80.2 88.9 89.0 86.7 87.3 87.6 80.7 85.9 90.7 88.4 92. val 90.6 92.6 92.6 87.0 92.6 92.8 91.7 91.9 87.1 90.3 93.2 92.2 93.7 RefCOCO+ test-A 89.0 89.6 92.2 87.4 92.1 92.9 88.7 90.5 90.0 87.9 91.5 93.8 93.1 94.7 test-B 75.9 79.8 83.2 72.1 81.4 83.4 77.8 79.5 80.4 71.4 78.8 85.6 82.8 86.9 val 82.8 85.2 88.8 81.6 87.4 88.7 85.8 86.2 79.8 85.2 90.1 88.8 90.4 RefCOCOg test val 87.0 86.1 89.4 88.7 89.3 89.2 82.2 82.3 90.0 89.4 90.8 89.8 87.1 87.8 87.3 88.1 88.2 82.7 82.7 87.6 86.7 90.4 89.9 90.3 89.5 92.2 92.7 avg. 86.6 88.9 89.8 82.9 89.6 90.3 87.9 88.2 82.9 87.6 91.1 90.0 92.3 Table 10: Comparison of visual grounding performance. We evaluate InternVLs visual grounding capability on RefCOCO, RefCOCO+, and RefCOCOg datasets [108, 177]. Parts of the results are collected from [246]. RefCOCO [108]: Built on COCO, this dataset contains 19,994 images with 142,210 referring expressions for 50,000 objects, split into subsets like test (people-focused) and test (other objects) for REC tasks. RefCOCO+ [108]: Similar to RefCOCO but emphasizing attribute-based descriptions without absolute location cues. It includes 19,992 images and 141,564 expressions, requiring models to focus on descriptive attributes. RefCOCOg [177]: With 25,799 images and 95,010 expressions, this dataset features longer, more complex expressions, and challenging models to manage intricate language in REC tasks."
        },
        {
            "title": "5.7.2 Evaluation Results",
            "content": "Visual grounding is critical for connecting textual descriptions with visual content, enabling accurate multimodal interaction. Table 10 compares InternVL 2.5 with its predecessor, InternVL 2.0, at the 8B and 78B scales, alongside other leading MLLMs (e.g., CogVLM-Grounding-17B [248], Qwen2-VL [246]) and specialized grounding models (e.g., Grounding-DINO-L [153], UNINEXT-H [267], ONE-PEACE [247]), evaluated on the RefCOCO [108], RefCOCO+ [108], and RefCOCOg [177] datasets. InternVL2.5-8B improves its predecessors performance, with the average score rising from 82.9 to 87.6, achieving comparable results to Qwen2-VL-7B (87.6 vs. 87.9), though slightly behind Ferret-v2-13B [297] and CogVLM-Grounding-17B [248], which benefit from fine-tuning for grounding and larger model sizes. At the larger scale, InternVL2.5-78B achieves state-of-the-art performance with an average score of 92.3, 2.3-point improvement over InternVL2-Llama3-76B, surpassing Qwen2-VL-72B [246]. These gains highlight the effectiveness of our data and training optimizations, significantly enhancing localization capabilities."
        },
        {
            "title": "5.8.1 Benchmarks",
            "content": "We assess InternVLs multimodal multilingual understanding capabilities using three representative benchmarks: MMMB and Multilingual MMBench [218]: MMMB is large-scale multilingual multimodal benchmark with 6 languages, 15 categories, and 12,000 questions. The languages evaluated are English (en), Chinese (zh), Portuguese (pt), Arabic (ar), Turkish (tr), and Russian (ru). Multilingual MMBench extends MMBench [156] to these 6 languages using GPT-4 translation for multilingual understanding evaluation. MTVQA [227]: MTVQA is multilingual benchmark tailored for text-centric visual question answering. It includes high-quality, expert human annotations across nine languages, specifically addressing the visual-text misalignment challenge in multilingual contexts. We report the average score of MTVQA."
        },
        {
            "title": "5.8.2 Evaluation Results",
            "content": "Multilingual ability is critical for MLLMs as it expands their application and improves cross-language communication. For global deployment, MLLMs must effectively handle both high-resource and low-resource languages. 22 Model Name InternVL2-1B [35] InternVL2.5-1B Qwen2-VL-2B [246] InternVL2-2B [35] InternVL2.5-2B InternVL2-4B [35] InternVL2.5-4B mPLUG-Owl2 [275] Qwen2-VL-7B [246] InternVL2-8B [35] InternVL2.5-8B InternVL-Chat-V1.5 [35] InternVL2-26B [35] InternVL2.5-26B InternVL2-40B [35] InternVL2.5-38B GPT-4V [192] GPT-4o [192] Gemini-1.0-Pro [228] Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B en 73.2 78.8 78.3 79.4 81.4 82.0 83.7 67.3 83.9 83.4 84.3 82.6 83.8 86.2 85.3 86.4 75.0 75.0 86.8 85.3 86.3 zh 67.4 70.2 74.2 71.6 74.4 76.1 81.0 61.0 82.4 81.5 83.1 80.8 81.7 83.8 84.1 85.1 74.2 71.9 85.3 85.1 85. MMMB ar pt 53.5 55.5 55.0 61.5 68.3 72.6 43.5 54.0 48.3 58.2 54.3 75.6 76.0 79.7 45.8 59.7 79.0 81.2 66.3 76.1 69.3 78.6 65.2 76.3 68.8 78.0 73.3 81.6 70.3 81.1 84.3 84.1 73.5 71.5 69.9 70.6 84.8 85.2 82.8 82.8 84.8 85.1 tr 43.8 45.3 61.8 46.4 46.4 51.2 70.5 45.4 74.7 69.2 71.5 68.6 69.3 73.7 74.2 82.8 69.0 69.6 84.2 83.0 83.1 ru 55.2 61.1 72.8 48.1 53.2 67.4 79.9 62.6 82.4 75.7 79.5 74.0 76.3 82.8 81.4 84.9 73.1 72.7 85.3 83.7 85.4 en 67.9 72.5 72.1 73.8 76.5 77.3 82.3 66.2 81.8 82.9 83.8 81.1 82.7 86.1 86.2 87.5 77.6 73.6 86.9 87.8 90.0 Multilingual MMBench tr zh 31.8 61.2 37.8 64.7 54.4 71.1 31.3 69.6 33.9 71.6 46.5 72.4 68.1 81.1 47.7 59.4 74.5 81.6 66.0 81.8 67.8 83.2 66.7 80.2 69.6 81.8 75.0 85.5 74.2 85.8 84.0 88.6 70.5 74.4 69.8 72.1 84.4 87.2 85.0 87.3 84.9 89.7 ar 43.3 43.0 61.1 29.8 37.3 43.6 73.4 37.9 75.6 60.5 64.3 56.2 61.9 67.5 64.0 84.5 72.3 61.1 83.5 83.1 83. pt 50.8 57.0 69.9 51.4 55.9 72.6 78.9 58.2 79.1 76.0 79.4 76.9 77.8 80.7 82.8 85.3 72.5 70.3 85.8 85.9 87.4 MTVQA (avg) 12.6 21.4 20.0 10.9 21.8 15.3 28.4 25.6 20.9 27.6 20.5 17.7 28.5 20.6 31.7 22.0 27.8 30.9 22.0 31.9 ru 52.7 53.2 69.3 42.3 44.8 61.2 76.2 60.4 79.3 74.4 77.3 71.0 74.4 79.6 81.8 85.9 74.8 70.5 85.3 85.7 86.3 Table 11: Comparison of multimodal multilingual performance. We evaluate multilingual capabilities across 3 benchmarks, including MMMB [218], Multilingual MMBench [218] and MTVQA [227]. The languages evaluated are English (en), Chinese (zh), Portuguese (pt), Arabic (ar), Turkish (tr), and Russian (ru). As shown in Table 11, we evaluated our models performance on three multilingual benchmarks: MMMB [218], Multilingual MMBench [218], and MTVQA [227]. comparison between InternVL2.5-78B and Qwen2-VL-72B [246] reveals that, despite differences in training data, model architecture, and training strategies, their multilingual performance is quite similar. This suggests that the multilingual capabilities of MLLMs are largely inherited from the underlying language model. Both models share the same LLM, indicating that strong multilingual LLM forms the foundation for effective multilingual performance in MLLMs."
        },
        {
            "title": "5.9.1 Benchmarks",
            "content": "Video-MME [69]: Video-MME is benchmark for evaluating MLLMs in full-spectrum video analysis. It features wide variety of video types across multiple domains and durations, with multimodal inputs including video, subtitles, and audio. For this benchmark, we test with four settings: 16, 32, 48, and 64 frames, and report the maximum results. We report results for both with subtitle and without subtitle settings. MVBench [131]: MVBench is video understanding benchmark designed to comprehensively evaluate the temporal awareness of MLLMs in the open world. It covers 20 challenging video tasks, ranging from perception to cognition, which cannot be effectively solved using single frame. We test this benchmark using 16 frames. MMBench-Video [65]: MMBench-Video is quantitative benchmark for evaluating MLLMs video understanding and temporal reasoning skills, covering diverse domains, multi-shot long videos, and features like hallucination, commonsense reasoning, and temporal reasoning. For this benchmark, we test with four different settings: 16, 32, 48, and 64 frames, and report the maximum scores. MLVU [315]: MLVU is comprehensive benchmark designed to evaluate MLLMs in long video understanding tasks, featuring videos ranging from 3 minutes to 2 hours. It includes nine different evaluation tasks divided into three categories: holistic understanding, single-detail understanding, and multi-detail understanding. We evaluate four settings: 16, 32, 48, and 64 frames, and report the highest M-Avg results. LongVideoBench [262]: LongVideoBench focuses on referring reasoning tasks that involve long-frame inputs, requiring the model to accurately retrieve and reason about detailed multimodal information based on referring queries. We test four settings16, 32, 48, and 64 framesand report the best results on the validation set. Model Name InternVL2-1B [35] InternVL2.5-1B Qwen2-VL-2B [246] InternVL2-2B [35] InternVL2.5-2B InternVL2-4B [35] InternVL2.5-4B VideoChat2-HD [130] MiniCPM-V-2.6 [274] LLaVA-OneVision-7B [124] Qwen2-VL-7B [246] InternVL2-8B [35] InternVL2.5-8B InternVL2-26B [35] InternVL2.5-26B Oryx-1.5-32B [160] VILA-1.5-40B [143] InternVL2-40B [35] InternVL2.5-38B GPT-4V/4T [3] GPT-4o-20240513 [192] GPT-4o-20240806 [192] Gemini-1.5-Pro [200] VideoLLaMA2-72B [38] LLaVA-OneVision-72B [124] Qwen2-VL-72B [246] InternVL2-Llama3-76B [35] InternVL2.5-78B Video-MME (wo / sub) 42.9 / 45.4 50.3 / 52.3 55.6 / 60.4 46.2 / 49.1 51.9 / 54.1 53.9 / 57.0 62.3 / 63.6 45.3 / 55.7 60.9 / 63.6 58.2 / 63.3 / 69.0 56.3 / 59.3 64.2 / 66.9 57.0 / 60.2 66.9 / 69.2 67.3 / 74.9 60.1 / 61.1 66.1 / 68.6 70.7 / 73.1 59.9 / 63.3 71.9 / 77.2 75.0 / 81.3 61.4 / 63.1 66.2 / 69.5 71.2 / 77.8 64.7 / 67.8 72.1 / 74.0 MVBench 57.5 64.3 63.2 60.2 68.8 64.0 71.6 62.3 56.7 67.0 65.8 72.0 67.5 75.2 70.1 72.0 74.4 43.7 62.0 59.4 73.6 69.6 76.4 MMBench-Video (val) 1.14 1.36 1.30 1.44 1.45 1.73 1.22 1.70 1.44 1.57 1.68 1.67 1.86 1.52 1.61 1.78 1.82 1.53 1.63 1.87 1.30 1.70 1.71 1. MLVU (M-Avg) 51.6 57.3 54.3 61.4 59.9 68.3 47.9 64.0 68.9 64.2 72.3 72.3 56.7 71.0 75.3 49.2 64.6 66.4 69.9 75.7 LongVideoBench (val total) 43.3 47.9 46.0 52.0 53.0 55.2 54.9 55.6 54.6 60.0 56.1 59.9 60.6 63.3 59.1 66.7 64.0 61.3 61.1 63.6 CG-Bench v1.1 (long / clue acc.) 41.8 / 56.4 40.1 / 56.4 41.3 / 56.2 42.2 / 58.5 Table 12: Comparison of video understanding performance. We evaluate InternVLs video understanding capabilities across 6 benchmarks. For Video-MME [69], MMBench-Video [65], MLVU [315], and LongVideoBench [262], we test with four different settings: 16, 32, 48, and 64 frames, and report the maximum results. For MVBench [131], we conduct testing using 16 frames. For CG-Bench [7], we use 32 frames. CG-Bench [7]: CG-Bench is benchmark for evaluating long video understanding in MLLMs. Unlike existing benchmarks, it focuses on models ability to retrieve relevant clues for answering questions. It includes 1,219 curated videos and over 12,000 question-answer pairs. Two novel clue-based evaluation methods are introduced to assess genuine video understanding. We test this benchmark using 32 frames."
        },
        {
            "title": "5.9.2 Evaluation Results",
            "content": "Video understanding is vital for assessing MLLMs ability to process temporal and multimodal information. To evaluate this comprehensively, we tested six benchmarks: Video-MME [69], MVBench [131], MMBench-Video [65], MLVU [315], LongVideoBench [262], and CG-Bench [7], covering diverse tasks from short video comprehension to long video reasoning. As shown in Table 12, InternVL 2.5 achieves consistent improvements over InternVL 2.0 across all benchmarks. For example, our smallest model, InternVL2.5-1B improves Video-MME scores from 42.9/45.4 to 50.3/52.3 and MVBench from 57.5 to 64.3. Moreover, we find that InternVL 2.5 demonstrates better scalability when handling increasing input frames compared to its predecessor, as shown in Figure 10. We attribute these improvements to two key enhancements: (1) The inclusion of more highquality video data, which has significantly enhanced the models video understanding capabilities. (2) Adjusting the training frame sampling strategy from 424 to 832 frames (as shown in Figure 5(c)) enhanced the models ability to process richer video information. Consequently, while InternVL 2.0 models typically perform best at Figure 10: Performance on LongVideoBench with varying input video frames. 24 - C 8 . 1 - 2 r Settings 47.3 5-shot 46.1 5-shot 48.6 5-shot 33.1 0-shot 37.3 0-shot 15.3 0-shot 75.8 0-shot 74.0 0-shot 56.5 0-shot 57.9 0-shot 37.9 0-shot 42.7 4-shot 11.0 4-shot 13.9 0-shot 34.8 4-shot 40.9 3-shot 28.2 0-shot 41.3 2 - 2 r I 46.4 47.1 48.6 32.3 31.5 13.2 76.9 72.6 58.7 53.7 36.3 40.7 7.0 12.3 32.3 33.1 23.4 39.2 (-2.1) - C 8 . 1 - 5 . 2 r I 50.5 62.7 60.4 54.7 32.3 10.1 61.4 78.5 56.9 76.2 43.4 53.3 39.5 11.4 41.5 42.8 33.8 47.6 - h 7 - 5 . 2 r I 72.8 78.2 77.9 78.7 64.0 21.1 88.1 90.5 84.9 94.8 73.1 85.1 60.6 23.4 74.4 63.0 51.6 69.5 2 - 5 . 2 r I 52.6 57.0 56.2 52.6 31.2 11.8 78.0 77.4 59.1 68.2 40.9 55.1 33.5 12.0 52.4 50.6 34.2 48.4 (+0.8) - C 0 2 - 2 r I 66.5 64.7 61.8 63.5 61.8 23.6 92.2 86.2 76.4 85.3 70.1 80.7 34.9 22.1 71.3 70.8 55.8 64.0 6 2 - 2 r I 68.2 68.1 67.7 62.3 61.8 28.8 93.2 86.5 79.9 87.5 69.8 80.0 35.5 15.3 67.1 66.2 54.2 64.2 (+0.2) - C 0 2 - 5 . 2 r I 73.3 79.4 80.2 81.0 67.3 21.3 94.0 91.3 86.4 95.9 78.4 88.5 54.7 23.9 69.5 70.0 61.0 71.5 6 2 - 5 . 2 r I 76.6 81.9 83.8 86.9 69.0 36.1 95.8 92.2 87.9 95.8 78.9 82.9 53.7 15.4 68.9 72.0 61.6 72.9 (+1.4) 8 - 2 r I 73.2 79.2 80.1 75.0 62.0 28.1 94.2 90.8 85.9 94.9 72.7 75.6 39.5 15.6 69.5 58.8 48.2 67.2 (-2.3) 8 - 5 . 2 r I 74.6 78.7 79.7 77.3 63.4 29.4 94.7 90.8 83.5 94.1 73.4 77.8 49.9 23.8 75.0 68.5 55.2 70.0 (+0.5) Dataset MMLU CMMLU C-Eval GAOKAO TriviaQA NaturalQuestions C3 RACE-High WinoGrande HellaSwag BBH GSM8K MATH TheoremQA HumanEval MBPP MBPP-CN Average Gain Table 13: Comparison of language capabilities across multiple benchmarks. These results were obtained using the OpenCompass toolkit for testing. Training InternVL 2.0 models led to decline in pure language capabilities. InternVL 2.5 addresses this by collecting more high-quality open-source data and filtering out low-quality data, achieving better preservation of pure language performance. 16 or 32 frames but degrade with more input frames, InternVL 2.5 could benefit from increasing input frames, showing better scalability for long video understanding. Our largest model, InternVL2.5-78B, achieves leading performance among open-source models and approaches the performance of closed-source systems. Compared to open-source models, InternVL2.5-78B surpasses Qwen2-VL-72B on MVBench (76.4 vs. 73.6) and MMBench-Video (1.97 vs. 1.70), though its Video-MME score with subtitles is slightly lower (74.0 vs. 77.8). Against closed-source models like GPT-4o [192] and Gemini-1.5-Pro [138], InternVL2.5-78B demonstrates competitive performance. On Video-MME, it scores 72.1/74.0, closely matching GPT-4o (71.9/77.2) and Gemini-1.5-Pro (75.0/81.3). However, on LongVideoBench, it achieves 63.6, slightly trailing Gemini-1.5-Pro (64.0) and GPT-4o (66.7). This highlights the remaining challenges in long video understanding for open-source models, indicating room for further improvement."
        },
        {
            "title": "6 Evaluation on Language Capability",
            "content": "To thoroughly assess the language capabilities of LLMs and MLLMs, we evaluate their performance across five core dimensions using diverse set of datasets. These benchmarks encompass tasks like comprehensive examination, language and knowledge, reasoning, mathematics, and coding."
        },
        {
            "title": "6.1 Benchmarks",
            "content": "Comprehensive Examination. We conduct thorough evaluation of LLMs and MLLMs using various examrelated datasets: (1) MMLU [85] includes 57 subtasks covering diverse topics such as humanities, social sciences, and STEM, evaluated with 5-shot approach. (2) CMMLU [127], focused on Chinese context, features 67 subtasks spanning general and Chinese-specific domains, also tested in 5-shot setting. (3) C-Eval [96] contains 52 subtasks across four difficulty levels, evaluated in 5-shot setting. (4) GAOKAO-Bench [304], derived from Chinese college entrance exams, offers comprehensive coverage of both subjective and objective question types, with objective questions evaluated in 0-shot setting. 25 Language and Knowledge. For language and knowledge-based assessments, we use range of datasets designed to test the capabilities: (1) TriviaQA [103], which includes both reading comprehension and open-domain QA tasks with multiple answers per question, evaluated in 0-shot setting. (2) NaturalQuestions [117], featuring user-generated questions validated by experts, also evaluated in 0-shot manner. (3) C3 [219], free-form multiple-choice Chinese machine reading comprehension dataset, with 0-shot results reported. (4) RACE [118], reading comprehension dataset containing English exam questions for Chinese middle and high school students aged 12 to 18, with results reported for the high school subset in 0-shot setting. Reasoning. To measure reasoning capabilities, we use datasets like (1) WinoGrande [202], which tests commonsense reasoning through 44,000 multiple-choice questions requiring pronoun disambiguation, evaluated in 0-shot setting. (2) HellaSwag [292] challenges models with natural language inference scenarios and four outcome options, demanding selection of the most logical conclusion, also evaluated in 0-shot manner. (3) BigBench Hard (BBH) [224] comprises 23 tasks specifically chosen for their difficulty in surpassing human performance, further evaluating reasoning depth, with 0-shot results reported. Mathematics. In the domain of mathematics, (1) GSM8K-Test [43] offers approximately 1,300 elementary-level situational problems, evaluated in 4-shot setting. (2) MATH [86] presents 12,500 high school competition-level problems across subjects like algebra and calculus, each with detailed solutions, also evaluated in 4-shot manner. (3) TheoremQA [33] introduces 800 STEM-focused problems requiring theorem application in fields like mathematics, physics, and finance, with 0-shot results reported. Coding. To evaluate coding capabilities, we employ the following benchmarks: (1) HumanEval [31]: This benchmark includes 164 Python programming tasks, each paired with detailed specifications, serving as standard for assessing coding performance. It is evaluated in 4-shot setting. (2) MBPP [9]: Comprising 974 entry-level programming tasks, MBPP covers wide range of challenges, from simple arithmetic problems to more complex sequence definitions, evaluated in 3-shot setting. (3) MBPP-CN [9]: Chinese adaptation of MBPP designed to assess multilingual programming capabilities. This extension broadens the evaluation scope to include linguistic and contextual diversity, with 0-shot results reported."
        },
        {
            "title": "6.2 Evaluation Results",
            "content": "In the development of MLLMs, maintaining strong pure language capabilities remains critically important. Following the approach of InternLM2 [19], we conducted comprehensive evaluation of our models performance across 17 pure language benchmarks using the OpenCompass toolkit [46]. These benchmarks are categorized into five major groups, providing thorough assessment of the models pure language abilities. The results show that InternVL 2.0 demonstrates slight decline in pure language performance compared to its foundational LLM counterparts. For example, InternVL2-2B achieved an average score of 39.2, decrease of 2.1 points compared to InternLM2-1.8B-Chat. Similarly, InternVL2-8B scored an average of 67.2, 2.3 points lower than InternLM2.5-7B-Chat. To address this issue, we curated large collection of high-quality open-source pure language instruction data and applied rigorous filtering pipelines to eliminate low-quality samples, thereby enhancing the overall data quality. These improvements in InternVL 2.5 have effectively mitigated the decline in language performance, enabling the model to match or even surpass the original LLM in several tasks. This demonstrates that supplementing and optimizing with high-quality language data can not only preserve MLLMs pure language capabilities but also establish stronger foundation for multimodal tasks."
        },
        {
            "title": "7 Evaluation on Vision Capability",
            "content": "In this section, we present comprehensive evaluation of the vision encoders performance across various domains and tasks. The evaluation is divided into two key categories: (1) image classification, representing global-view semantic quality, and (2) semantic segmentation, capturing local-view semantic quality. This approach allows us to assess the representation quality of InternViT across its successive version updates. 7."
        },
        {
            "title": "7.1.1 Benchmarks",
            "content": "We assess the global-view semantic quality of InternViT through comprehensive evaluation on diverse image classification datasets. 26 ImageNet-1K [56]: widely-used large-scale dataset containing over 1 million images across 1,000 classes, commonly used for benchmarking image classification models. ImageNet-ReaL [16]: re-labeled version of ImageNets validation set, providing multi-label annotations that are more accurate and robust, following an enhanced labeling protocol. ImageNet-V2 [199]: dataset designed to evaluate the robustness of models trained on ImageNet-1K, featuring new test images collected using the original ImageNet methodology. ImageNet-A [87]: challenging dataset of naturally occurring, unmodified images that are often misclassified by ResNet models. It highlights the limitations of models when exposed to adversarially difficult examples in real-world settings. ImageNet-R [84]: rendition dataset with 30K images across 200 ImageNet classes, composed of art, sketches, toys, sculptures, and other creative representations. It assesses the robustness of models in recognizing abstract renditions of common objects. ImageNet-Sketch [242]: This dataset contains 51K sketch images, with approximately 50 sketches per ImageNet class. It is constructed via Google Image queries using the class name followed by sketch of, testing models ability to generalize to abstract, hand-drawn representations."
        },
        {
            "title": "7.1.2 Settings",
            "content": "In this study, two evaluation methods, linear probing [32] and attention pooling probing, are employed to assess the performance of the InternViT models: Linear Probing [32]: This method involves freezing the pre-trained model and training only linear classifier on top. It evaluates the quality of the learned features without updating the backbone, providing insights into how effectively the pre-trained model captures semantic information usable by simple linear classifier in downstream tasks like image classification. Attention Pooling Probing: In contrast, attention pooling probing evaluates the model by adding an attention pooling layer on top of the frozen features. This approach allows the vision encoder to retain richer information in the final layer, as attention pooling can dynamically select task-relevant features for classification without interference from unrelated information. For both experiments, we use ImageNet-1K [56] as the training set and evaluate the models on the ImageNet-1K validation set along with several ImageNet variants (i.e., ImageNet-ReaL [16], ImageNet-V2 [199], ImageNetA [87], ImageNet-R [84], and ImageNet-Sketch [242]) to benchmark their domain generalization capabilities. The models are trained using SGD as the optimizer, with peak learning rate of 0.2, momentum of 0.9, and no weight decay. cosine learning rate decay schedule is applied over 10 training epochs, with 1 warmup epoch. We use input resolutions of 448448, with patch size of 14 and total batch size of 1024. Data augmentation techniques, such as random resized cropping and horizontal flipping, are employed during training. The code and logs of these classification experiments will be released on our GitHub repository1."
        },
        {
            "title": "7.1.3 Evaluation Results",
            "content": "As shown in Table 14, the results reveal an interesting trend across the version updates of InternViT: as the model progresses, the performance of linear probing declines substantially, with all versions showing an average below the gray baseline. In contrast, attention pooling probing consistently outperforms the gray baseline despite some fluctuations. This results in growing trend in the average score difference (from 3.5 to 6.7), denoted as , across successive InternViT versions. This suggests that features in the models final layer become less linearly separable, likely as representations evolve to capture more complex, open-ended semantic information. The attention pooling mechanism effectively selects relevant features from this enriched representation space, offsetting challenges from reduced linear separability. Additionally, these findings imply that InternViT maintains key pre-training attributes through iterative updates without catastrophic forgetting. With each version, its representations grow more diverse, capturing open-set semantics and enhancing generalizationan advantage particularly valuable for MLLMs requiring high abstraction for real-world tasks. 1https://github.com/OpenGVLab/InternVL/tree/main/classification 27 Linear Probing Attention Pooling Probing Model Name res. 224 88.2 InternViT-6B-224px InternViT-6B-224px 448 87.8 InternViT-6B-448px-V1.0 448 87.0 InternViT-6B-448px-V1.2 448 87.0 InternViT-6B-448px-V1.5 448 86.5 InternViT-6B-448px-V2.5 448 86.6 IN-1K IN-ReaL IN-V2 IN-A IN-R IN-Ske avg. IN-1K IN-ReaL IN-V2 IN-A IN-R IN-Ske avg. 82.5 89.2 81.3 88.8 80.6 88.7 79.4 88.6 77.9 88.4 78.5 88.3 79.9 77.5 89.8 79.8 77.2 87.1 78.8 77.2 85.5 78.5 77.1 83.9 78.1 69.8 82.9 77.8 73.7 82.7 82.3 84.7 93.1 82.0 85.4 91.3 82.0 88.7 92.8 82.0 88.7 92.7 81.6 86.0 92.2 81.3 86.9 92. 72.7 70.5 72.0 71.6 70.9 70.8 69.1 65.8 65.1 59.7 60.1 60.0 91.1 91.0 91.0 91.1 91.2 91.2 90.4 90.2 90.0 89.9 89.9 90.1 85.5 3.0 84.8 3.5 85.9 5.3 85.8 6.4 85.1 7.2 85.2 6.7 Table 14: Image classification performance across different versions of InternViT. We use IN-1K [56] for training and evaluate on the IN-1K validation set as well as multiple ImageNet variants, including IN-ReaL [16], IN-V2 [199], IN-A [87], IN-R [84], and IN-Sketch [242]. Results are reported for both linear probing and attention pooling probing methods, with average accuracy for each method. represents the performance gap between attention pooling probing and linear probing, where larger suggests shift from learning simple linear features to capturing more complex, nonlinear semantic representations."
        },
        {
            "title": "7.2.1 Benchmarks",
            "content": "We evaluate the local-view semantic quality of InternViT using two representative semantic segmentation datasets, ADE20K and COCO-Stuff-164K. ADE20K [313]: comprehensive dataset containing over 20,000 images with annotations across 150 object and background categories, widely used for scene parsing. It provides detailed pixel-level labels for both objects and parts, facilitating range of fine-grained segmentation tasks. COCO-Stuff-164K [18]: An extension of the original COCO images with pixel-level annotations, adding 91 stuff classes (like grass and sky) to 80 thing categories (like people and cars), covering total of 172 classes. With these comprehensive labels, the dataset supports tasks in scene parsing and semantic segmentation, enabling richer context understanding in image analysis."
        },
        {
            "title": "7.2.2 Settings",
            "content": "In this study, three evaluation methodslinear probing, head tuning, and full tuningare employed to assess the performance of the InternViT models on semantic segmentation tasks: Linear Probing: Linear probing applies frozen backbone with linear segmentation head, offering insight into the linear separability of learned features. This method provides baseline for evaluating pixel-level semantic information with minimal adaptation, though it may not fully capture the encoders capacity for complex features. Head Tuning: In head tuning, the InternViT is frozen while the UperNet [264] head remains trainable, allowing the model to utilize stronger head to reduce its dependence on linear separability. This setup mitigates the decline in linear separability caused by the complex, open-ended features, enabling more precise evaluation of the vision encoders capabilities. Full Tuning: Full tuning involves making both the InternViT backbone and the UperNet [264] segmentation head trainable, allowing the model to adapt all layers for the target task and minimizing reliance on pre-existing linear separability. This setup provides an alternative perspective for evaluating the vision encoders capacity to extract visual features. We use AdamW [161] with peak learning rate of 4e-5 and polynomial decay schedule. Layer-wise learning rate decay (0.95) is applied in full tuning. Weight decay is set to 0.05 for both head and full tuning, and none for linear probing. The input resolution is 504504, with patch size of 14 and batch size of 16. Training consists of 1.5K warmup iterations and 80K total iterations. drop path rate of 0.4 is applied in full tuning. We utilize default data augmentation from MMSegmentation [45]. All the code and logs related to these experiments will be released on GitHub2. 2https://github.com/OpenGVLab/InternVL-MMDetSeg 28 Model Name InternViT-6B-224px InternViT-6B-448px-V1.0 InternViT-6B-448px-V1.2 InternViT-6B-448px-V1.5 InternViT-6B-448px-V2.5 Linear Probing Head Tuning (UperNet) ADE20K COCO avg. ADE20K COCO avg. ADE20K COCO avg. 55.3 54.9 55.2 55.2 55.2 42.8 38.5 36.1 36.3 35. 47.2 43.6 40.7 40.9 39.4 54.9 55.4 55.2 55.0 55.4 58.9 58.1 58.8 58.8 58.6 45.0 41.0 38.4 38.6 37.5 51.6 51.7 51.7 51.5 51.8 51.9 52.4 52.0 52.0 52. 48.9 49.4 48.8 49.1 49.7 Full Tuning (UperNet) 1 2 6.9 11.3 13.6 13.4 15.1 10.2 13.9 16.8 16.6 17.7 Table 15: Semantic segmentation performance across different versions of InternViT. The models are evaluated on ADE20K [313] and COCO-Stuff-164K [18] using three configurations: linear probing, head tuning, and full tuning. The table shows the mIoU scores for each configuration and their averages. 1 represents the gap between head tuning and linear probing, while 2 shows the gap between full tuning and linear probing. larger value indicates shift from simple linear features to more complex, nonlinear representations."
        },
        {
            "title": "7.2.3 Evaluation Results",
            "content": "As shown in Table 15, the semantic segmentation performance of InternViT models is evaluated across three configurationslinear probing, head tuning, and full tuningon ADE20K [313] and COCO-Stuff-164K [18]. The results reveal distinct trends in how the models feature representations evolve across version updates. Linear probing results show decline in mIoU scores as the model versions progress, with average scores dropping from 45.0 in InternViT-6B-224px to 37.5 in InternViT-6B-448px-V2.5. This indicates that as InternViT updates, the features become less linearly separable, reflecting shift toward capturing more complex and open-ended information. In head tuning, the models display different trend compared to linear probing. All other versions of InternViT surpass the baseline InternViT-6B-224pxs mIoU score of 51.9, showing no performance decline. This leads to increasing 1 values, growing from 6.9 in InternViT-6B-224px to 15.1 in InternViT-6B-448px-V2.5. The rise in 1 suggests that while the features become less linearly separable, their quality remains intact, effectively capturing complex information. Similarly, full tuning yields consistent results, as seen in the 2 values. The increase in 2 from 10.2 in InternViT-6B-224px to 17.7 in InternViT-6B-448px-V2.5 further supports this trend. Overall, the increasing values of 1 and 2 across model versions highlight the shift from simple, linearly separable features to more complex, nonlinear representations. This evolution aligns with InternViTs growing capability to extract visual information as its versions progress within the development of InternVL. It demonstrates the effectiveness of our ViT incremental learning strategy in enhancing the vision encoders ability to extract open-ended features."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we introduce InternVL 2.5, an advanced open-source multimodal large language model (MLLM) series that builds upon the architecture of InternVL 2.0 with significant improvements in training, testing strategies, and data quality. We systematically explore the relationship between model scaling and performance, analyzing vision encoders, language models, dataset sizes, and test-time configurations. Extensive evaluations on diverse benchmarks demonstrate that InternVL 2.5 achieves competitive performance across tasks such as multi-discipline reasoning, document understanding, video understanding, multilingual processing, etc. Notably, it is the first open-source MLLM to surpass 70% on the MMMU benchmark, narrowing the gap between open-source and commercial models like OpenAI o1. By sharing InternVL 2.5 with the community, we hope to contribute powerful tool for advancing multimodal AI research and applications, and we look forward to seeing future developments building upon this work."
        },
        {
            "title": "Acknowledgement",
            "content": "This work is supported by the National Key R&D Program of China (No. 2022ZD0160102, 2022ZD0161300), the National Natural Science Foundation of China (No. 62372223, 62376134), the China Mobile Zijin Innovation Institute (No. NR2310J7M), and the Youth PhD Student Research Project under the National Natural Science Foundation (No. 623B2050)."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 4, 14, 16, 18, 21 [2] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 80768084, 2019. 12, 13 In [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 24 [4] AgentSea. Wave-ui. https://huggingface.co/datasets/agentsea/wave-ui, 2024. 13 [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [6] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. 13 [7] Anonymous. CG-bench: Clue-grounded question answering benchmark for long video understanding. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review. 24 [8] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024. 2, 3, 14, 15, 16, 17, 18, 19, 20, 21 [9] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [10] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1912919139, 2022. 13 [11] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 4 [12] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. 13 [13] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [14] Ltd. Beijing Anjie Zhihe Technology Co. Chinese-ocr. https://huggingface.co/datasets/ longmaodata/Chinese-OCR, 2024. 12, 13 [15] Asma Ben Abacha, Sadid Hasan, Vivek Datla, Dina Demner-Fushman, and Henning Mller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes, 2019. 13 [16] Lucas Beyer, Olivier Hnaff, Alexander Kolesnikov, Xiaohua Zhai, and Aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020. 27, 28 [17] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Maral Rusinol, Ernest Valveny, CV Jawahar, and In Proceedings of the IEEE/CVF International Dimosthenis Karatzas. Scene text visual question answering. Conference on Computer Vision, pages 42914301, 2019. 12, 13 [18] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12091218, 2018. 28, 29 [19] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. 1, 3, 4, 5, 13, 26 [20] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th International Conference on Computational Linguistics, pages 15111520, 2022. 12, 13 30 [21] CarperAI. openai summarize tldr dataset. https://huggingface.co/datasets/CarperAI/openai_ summarize_tldr, 2023. 13 [22] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. [23] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. 12, 13 [24] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et al. Gui-world: dataset for gui-oriented multimodal llm-based agents. arXiv preprint arXiv:2406.10819, 2024. 13 [25] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. 1, 12, 13 [26] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. 12, 13 [27] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 12, 13, 22 [28] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, arXiv preprint Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. 20, 21 [29] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 12 [30] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 12, [31] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 26 [32] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In The International Conference on Learning Representations, pages 15971607. PMLR, 2020. 27 [33] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. Theoremqa: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 78897901. Association for Computational Linguistics, 2023. 26 [34] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. 13 [35] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 2, 3, 4, 5, 12, 13, 14, 16, 18, 21, 22, 23, [36] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 2, 3, 7, 13 [37] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. 13 [38] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 24 [39] Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, and Xian-Ling Mao. Complicated table structure recognition. arXiv preprint arXiv:1908.04729, 2019. 12, 31 [40] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In International Conference on Document Analysis and Recognition, pages 15711576, 2019. 12, 13 [41] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 13 [42] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 845855, 2018. 13, 17 [43] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 13, [44] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm. Company Blog of Databricks, 2023. 13 [45] MMSegmentation Contributors. Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020. 28 [46] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https: //github.com/open-compass/opencompass, 2023. 14, 16, 18, 21, 26 [47] X.AI Corp. Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model. https://x.ai/blog/grok-1.5v, 2024. 18, 19 [48] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. 13 [49] Dawei Dai, YuTang Li, YingGe Liu, Mingming Jia, Zhang YuanHui, and Guoyin Wang. 15m multimodal facial image-text dataset. arXiv preprint arXiv:2407.08515, 2024. 12 [50] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. 14, 15, [51] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 326335, 2017. 12 [52] Brian Davis, Bryan Morse, Scott Cohen, Brian Price, and Chris Tensmeyer. Deep visual template-free form parsing. In International Conference on Document Analysis and Recognition, pages 134141, 2019. 12, 13 [53] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512, 2023. 3, 4 [54] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 1, 14, 16 [55] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology, pages 845854, 2017. [56] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. 27, 28 [57] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. 13 [58] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023. 13 32 [59] Khang Doan, Bao Huynh, Dung Hoang, Thuc Pham, Nhat Pham, Quan Nguyen, Bang Vo, and Suong Hoang. Vintern-1b: An efficient multimodal large language model for vietnamese. arXiv preprint arXiv:2408.12480, 2024. [60] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024. 1 [61] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In The International Conference on Learning Representations, 2020. 3 [62] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 320335, 2022. 1 [63] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 14 [64] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1, 4, 16 [65] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 23, 24 [66] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. 1 [67] Miquel Farr, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https:// huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. 13 [68] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 20, 21 [69] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 23, [70] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. 17, 18 [71] Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, et al. Mini-internvl: flexible-transfer pocket multimodal model with 5% parameters and 90% performance. arXiv preprint arXiv:2410.16261, 2024. 2 [72] Alba Garcia Seco De Herrera, Henning Mller, and Stefano Bromuri. Overview of the imageclef 2015 medical classification task. In Working Notes of CLEF 2015Cross Language Evaluation Forum, CEUR, volume 1391. CEUR Workshop Proceedings, 2015. 12, 13 [73] GlaiveAI. Glaive code assistant v3 dataset. https://huggingface.co/datasets/glaiveai/ glaive-code-assistant-v3, 2024. 13 [74] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69046913, 2017. 12, 13 [75] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. 12 [76] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 14, 16 33 [77] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566, 2023. 20, [78] He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding. Eaten: Entity-aware attention for single shot visual text extraction. In International Conference on Document Analysis and Recognition, pages 254259, 2019. 12, 13 [79] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23152324, 2016. 12 [80] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. 2, 14, 15 [81] Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin. Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models. arXiv preprint arXiv:2308.10755, 2023. 12 [82] Mengchao He, Yuliang Liu, Zhibo Yang, Sheng Zhang, Canjie Luo, Feiyu Gao, Qi Zheng, Yongpan Wang, Xin Zhang, and Lianwen Jin. Icpr2018 contest on robust reading for multi-type web images. In International Conference on Pattern Recognition, pages 712, 2018. 12, 13 [83] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 13 [84] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 83408349, 2021. 27, 28 [85] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In The International Conference on Learning Representations, 2020. 25 [86] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 26 [87] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1526215271, 2021. 27, 28 [88] Jack Hessel, Ana Marasovic, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? Humor understanding benchmarks from The New Yorker Caption Contest. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2023. [89] Hezarai. Parsynth-ocr-200k. https://huggingface.co/datasets/hezarai/parsynth-ocr-200k, 2024. 12 [90] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022. 13 [91] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:40414056, 2020. 12, 13 [92] Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Victor Carbune, Jason Lin, Maria Wang, Srinivas Sunkara, Yun Zhu, and Jindong Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots. arXiv preprint arXiv:2209.08199, 2022. [93] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 12, 13 [94] Xinyue Hu, Gu, An, Zhang, Liu, Kobayashi, Harada, Summers, and Zhu. Medical-diff-vqa: large-scale medical dataset for difference visual question answering on chest x-ray images. PhysioNet, 2023. 12, 13 34 [95] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In European Conference on Computer Vision, 2020. 12, 13 [96] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024. [97] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In International Conference on Document Analysis and Recognition, pages 15161520, 2019. 12, 13 [98] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. 12, 13 [99] Baoxiong Jia, Ting Lei, Song-Chun Zhu, and Siyuan Huang. Egotaskqa: Understanding human tasks in egocentric videos. Advances in Neural Information Processing Systems, 35:33433360, 2022. 12, 13 [100] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 13, 17, [101] Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. Img-diff: Contrastive data synthesis for multimodal large language models. arXiv preprint arXiv:2408.04594, 2024. 13 [102] Jimmycarter. Textocr gpt-4v dataset. https://huggingface.co/datasets/jimmycarter/ textocr-gpt4v, 2023. 12, 13 [103] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. 26 [104] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56485656, 2018. 12, 13 [105] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, kos Kdr, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. 12, 13 [106] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161178. Springer, 2025. [107] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. 12, 13 [108] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 787798, 2014. 22 [109] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision, pages 235251, 2016. 12, 13, 16 [110] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49995007, 2017. 12, [111] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in Neural Information Processing Systems, 33:26112624, 2020. 12, 13 [112] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. 12 [113] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 12 [114] knowrohit07. know saraswati cot dataset. https://huggingface.co/datasets/knowrohit07/ know-saraswati-cot, 2023. 13 35 [115] Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang Bai. Visual information extraction in the wild: practical dataset and end-to-end solution. In International Conference on Document Analysis and Recognition, pages 3653. Springer, 2023. 12, 13 [116] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 128(7):19561981, 2020. 12 [117] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. 26 [118] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. 26 [119] LAION. Gpt-4v dataset. https://huggingface.co/datasets/laion/gpt4v-dataset, 2023. 13 [120] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5:110, 2018. 12, 13 [121] Hugo Laurenon, Andrs Marafioti, Victor Sanh, and Lo Tronchon. Building and better understanding visionlanguage models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024. 12, [122] Hugo Laurenon, Lo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024. 12, 13 [123] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv Le Borgne, Romaric Besanon, Jos Moreno, and Jess In Lovn Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 31083120, 2022. 12, 13 [124] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 3, 14, 15, 16, 17, 18, 21, 24 [125] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 16, 17 [126] Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, et al. R-bench: Are your large multimodal model robust to real-world corruptions? arXiv preprint arXiv:2410.05474, 2024. 18, [127] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. 25 [128] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 2024. 13 [129] Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu, Yaotian Yang, Xinrui Xiong, et al. Chemvlm: Exploring the power of multimodal large language models in chemistry area. arXiv preprint arXiv:2408.07246, 2024. 12, 13 [130] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 13, 24 [131] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 12, 13, 23, [132] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. 12, 13 [133] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024. 12 36 [134] Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, Guoan Wang, Chenglong Ma, Ying Chen, Ming Hu, et al. Gmai-vl & gmai-vl-5.5 m: large vision-language model and comprehensive multimodal dataset towards general medical ai. arXiv preprint arXiv:2411.14522, 2024. 12, 13 [135] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 13 [136] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020. 13 [137] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48044814, 2022. 1 [138] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 25 [139] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. 20, 21 [140] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607, 2023. 1 [141] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1496314973, 2023. [142] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification. https://https://huggingface. co/Open-Orca/SlimOrca, 2023. 13 [143] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 1, 14, 15, 16, 24 [144] Adam Dahlgren Lindstrm and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. 12, 13 [145] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging, pages 16501654. IEEE, 2021. 12, 13 [146] Brian Liu, Xianchao Xu, and Yu Zhang. Offline handwritten chinese text recognition with convolutional neural networks. arXiv preprint arXiv:2006.15619, 2020. 12, 13 [147] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. 12, 13 [148] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 12, 13 [149] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774, 2023. 12, [150] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 3 [151] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavahttps://llava-vl.github.io/blog/ Improved reasoning, ocr, and world knowledge. next: 2024-01-30-llava-next/, January 2024. 3 [152] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex Kot. Ntu rgb+d 120: large-scale benchmark for 3d human activity understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(10):26842701, 2020. 12, 37 [153] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2025. 22 [154] Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, and Liang He. Cmm-math: chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models. arXiv preprint arXiv:2409.02834, 2024. 13 [155] Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, et al. Mminstruct: high-quality multi-modal instruction tuning dataset with extensive diversity. arXiv preprint arXiv:2407.15838, 2024. 1, 12, 13 [156] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 20, 21, 22 [157] Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, and Jie Zhou. Points: Improving your vision-language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024. 1 [158] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 16, 17 [159] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833, 2024. 12 [160] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialtemporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 24 [161] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 28 [162] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 3 [163] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 14, 15 [164] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 12, 13 [165] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 12, 13 [166] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022. 12, [167] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. 12 [168] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. 13 [169] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 14, 16 [170] Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, et al. Bluelm-v-3b: Algorithm and system co-design for multimodal large language models on mobile devices. arXiv preprint arXiv:2411.10640, 2024. 1 38 [171] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. 13, 18, 19 [172] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 1 [173] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. 13 [174] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 12, 13 [175] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [176] Hui Mao, Ming Cheung, and James She. Deepart: Learning joint representations of visual arts. In Proceedings of the ACM International Conference on Multimedia, pages 11831191, 2017. 12, 13 [177] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1120, 2016. 13, 22 [178] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31953204, 2019. 12, 13 [179] MarkrAI. Kopen-hq-hermes-2.5-60k dataset. https://huggingface.co/datasets/MarkrAI/ KOpen-HQ-Hermes-2.5-60K, 2023. 13 [180] U-V Marti and Horst Bunke. The iam-database: an english sentence database for offline handwriting recognition. International Journal on Document Analysis and Recognition, 5:3946, 2002. 12, 13 [181] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 22632279, 2022. 12, 13, 16 [182] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal visionlanguage pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. 12, [183] Minesh Mathew, Viraj Bagal, Rubn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 12, 13, 16, 17 [184] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 22002209, 2021. 12, 16, 17 [185] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 1 [186] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024. 18 [187] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. 12, 13 [188] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947952, 2019. 12, [189] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. 13 39 [190] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. 13 [191] Jason Obeid and Enamul Hoque. Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. arXiv preprint arXiv:2010.09142, 2020. 12, [192] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023. 1, 2, 3, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25 [193] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 13 [194] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 12 [195] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763, 2021. 3, [196] Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. No robots. Hugging Face repository, https://huggingface.co/datasets/HuggingFaceH4/no_robots, 2023. 13 [197] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33943403, 2021. 13 [198] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36, 2024. 13 [199] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 53895400, 2019. 27, 28 [200] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 14, 15, 16, 18, 21, 24 [201] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. dataset for movie description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32023212, 2015. 12, 13 [202] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 87328740, 2020. 26 [203] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 7, 12 [204] Christoph Schuhmann, Andreas Kpf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/laion-coco/, 2022. 12 [205] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162, 2022. 12, 13 [206] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 14661476, 2015. 12, 13 [207] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. Kvqa: Knowledge-aware visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 88768884, 2019. 12, 13 [208] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 84308439, 2019. 12, 13 [209] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17). In International Conference on Document Analysis and Recognition, volume 1, pages 14291434, 2017. 12, 13 [210] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [211] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In European Conference on Computer Vision, pages 742758, 2020. 12, 13 [212] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 12, 13, 16, 17 [213] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88028812, 2021. 12, [214] Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, and Karan Desai. Benchmarking object detectors with coco: new path forward. In European Conference on Computer Vision, pages 279295. Springer, 2025. 12, 13 [215] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 1 [216] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018. 13 [217] Hamed Rahimi Sujet AI, Allaa Boutaleb. Sujet-finance-qa-vision-100k: large-scale dataset for financial document vqa. https://huggingface.co/datasets/sujet-ai/Sujet-Finance-QA-Vision-100k, 2024. [218] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. 22, 23 [219] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. 26 [220] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In The International Conference on Learning Representations, 2024. 1 [221] Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, et al. Moss: Training conversational language models from synthetic data. arXiv preprint arXiv:2307.15020, 7, 2023. 1, 13 [222] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In International Conference on Document Analysis and Recognition, pages 15571562, 2019. 12, 13 [223] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 20, [224] Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 26 [225] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1387813888, 2021. 12 [226] Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356, 2023. 12, 13 [227] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual text-centric visual question answering. arXiv preprint arXiv:2405.11985, 2024. 13, 22, 23 [228] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 23 [229] Qwen Team. Qwen2.5: party of foundation models. https://qwenlm.github.io/blog/qwen2.5/, September 2024. 1, 3, 5, 11 [230] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/, November 2024. 1 [231] Ryan Teknium, Jeffrey Quesnelle, and Chen Guang. Hermes 3 technical report. arXiv preprint arXiv:2408.11857, 2024. 1 [232] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et al. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208, 2024. 1 [233] Rubn Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. 13 [234] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 1, 12, 13, 14, 16, 18, 21 [235] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [236] Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, and Alisa Smirnova. Toloka visual question answering benchmark. arXiv preprint arXiv:2309.16511, 2023. 12, 13 [237] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87698778, 2018. 12, 13 [238] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. 12, 13 [239] Konstantin Verner. Cyrillic handwriting dataset. https://www.kaggle.com/datasets/ constantinwerner/cyrillic-handwriting-dataset, 2020. 13 [240] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498510, 2021. 12, 13 [241] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 18 [242] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. 27, 28 [243] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1984419854, 2023. 12, 13 [244] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 12 [245] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. 14, [246] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 3, 7, 8, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24 [247] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities. arXiv:2305.11172, 2023. 22 42 [248] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 1, 22 [249] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 1, [250] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024. 12, 13, 20, 21 [251] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In The International Conference on Learning Representations, 2024. 12 [252] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1440814419, 2023. 1 [253] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and bilingual scene-text visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012610135, 2020. 12, 13 [254] Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024. 12, 13 [255] Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, et al. How do your code llms perform? empowering code instruction tuning with high-quality data. arXiv preprint arXiv:2409.03810, 2024. [256] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. 3 [257] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024. 16, 17 [258] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 13 [259] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 12, 13 [260] Chaoyi Wu. Pmc-casereport. https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport, 2023. 12, 13 [261] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. arXiv preprint arXiv:2308.02463, 2023. 13 [262] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. 23, 24 [263] Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Junchi Yan, and Yu Qiao. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023. 12, [264] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In European Conference on Computer Vision, pages 418434, 2018. 28 [265] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The International Conference on Learning Representations, 2024. 13 [266] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. 13 43 [267] B. Yan, Yi Jiang, Jiannan Wu, D. Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [268] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 4 [269] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. arXiv preprint arXiv:2406.06040, 2024. 13 [270] Jianxin Yang. Firefly: chinese conversational large language model. https://github.com/ yangjianxin1/Firefly, 2023. 13 [271] Jianxin Yang. Longqlora: Efficient and effective method to extend context length of large language models. arXiv preprint arXiv:2311.04879, 2023. 13 [272] Wenjuan Yang, Xuhui Zhang, Bing Ma, Yanqun Wang, Yujia Wu, Jianxing Yan, Yongwei Liu, Chao Zhang, Jicheng Wan, Yue Wang, et al. An open dataset for intelligent recognition and classification of abnormal condition in longwall mining. Scientific Data, 10(1):416, 2023. [273] Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas Guibas, and Li Fei-Fei. Human action recognition by learning bases of action attributes and parts. In 2011 International Conference on Computer Vision, pages 13311338, 2011. 12 [274] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 14, 16, 18, 21, 24 [275] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023. 1, 23 [276] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 12, 13 [277] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. [278] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 12 [279] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 4 [280] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European Conference on Computer Vision, pages 6985, 2016. 12, 13 [281] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. 12, 13 [282] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [283] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 20, 21 [284] Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765, 2024. 20, 21 [285] Ya-Qi Yu, Minghui Liao, Jiwen Zhang, and Jihao Wu. Texthawk2: large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens. arXiv preprint arXiv:2410.05261, 2024. 22 44 [286] Yijiong Yu. \"paraphrasing the original text\" makes high accuracy long-context qa. arXiv preprint arXiv:2312.11193, 2023. 13 [287] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, and Shi-Min Hu. large chinese text dataset in the wild. Journal of Computer Science and Technology, 34:509521, 2019. 12, 13 [288] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai. Syntax-aware network for handwritten mathematical expression recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45534562, 2022. 12, 13 [289] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 2, 3, 14, 15 [290] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 14, [291] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. 13 [292] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 47914800, 2019. 26 [293] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1 [294] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 3 [295] Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu. Infinitymath: scalable instruction tuning dataset in programmatic mathematical reasoning. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 54055409, 2024. [296] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 22 [297] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 22 [298] Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. Longcite: Enabling llms to generate fine-grained citations in long-context qa. arXiv preprint arXiv:2409.02897, 2024. 13 [299] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2025. 14, 15 [300] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. 12, 13 [301] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In International Conference on Document Analysis and Recognition, pages 15771581, 2019. 12, [302] Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, and Yoshua Bengio. Vcr: Visual caption restoration. arXiv preprint arXiv:2406.06462, 2024. 13, 16, 17 [303] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 12, 13 [304] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. 45 [305] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. 12, 13 [306] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. 18, 19 [307] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 13 [308] Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742, 2024. [309] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. 12 [310] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. arXiv preprint arXiv:2406.08100, 2024. 13 [311] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658, 2024. 13 [312] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (gte): framework for joint table identification and cell structure recognition using visual context. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 697706, 2021. 12, [313] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 633641, 2017. 28, 29 [314] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. 13 [315] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 23, 24 [316] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The International Conference on Learning Representations, 2024. 3 [317] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49955004, 2016. 12,"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "SenseTime Research",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}