{
    "paper_title": "UniVid: The Open-Source Unified Video Model",
    "authors": [
        "Jiabin Luo",
        "Junhui Lin",
        "Zeyu Zhang",
        "Biao Wu",
        "Meng Fang",
        "Ling Chen",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified video modeling that combines generation and understanding capabilities is increasingly important but faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the limitations of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, a unified architecture that couples an MLLM with a diffusion decoder through a lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 0 2 4 2 . 9 0 5 2 : r UniVid: The Open-Source Unified Video Model UNIVID: THE OPEN-SOURCE UNIFIED VIDEO MODEL Jiabin Luo1 Junhui Lin2 Zeyu Zhang1 Biao Wu3 Meng Fang3 Ling Chen3 Hao Tang1 1Peking University 2AI Geeks 3Australian Artificial Intelligence Institute Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. Figure 1: We present UniVid, an open-source unified video model for both understanding and generation tasks. Our model requires only small amount of high-quality data for fine-tuning, achieveing competitive results across various tasks."
        },
        {
            "title": "ABSTRACT",
            "content": "Unified video modeling combining generation and understanding capabilities is increasingly important, yet faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the suboptimality of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, unified architecture that couples an MLLM with diffusion decoder through lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of our unified video model, achieving 2.2% improvement on VBench-Long total score compared to the previous SOTA method EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines. Code: https://github.com/AIGeeksGroup/UniVid. Website: https://aigeeksgroup.github.io/UniVid."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video intelligence encompasses two core capabilities: generation and understanding. Generation enables content creation, simulation, and data augmentation through diffusion and flow models (Shi et al., 2020; Podell et al., 2024; Wang et al., 2025; Blattmann et al., 2023a). Understanding powers perception, retrieval, analytics, and human-computer interaction via multimodal LLMs (Wang et al., 2024a; Chen et al., 2024b; Lin et al., 2024; Bai et al., 2025). Real-world applications increasingly demand unified systems that combine both capabilities within single framework. Recent efforts toward unified video modeling have converged on two paradigms. The first is an autoregressive 1 UniVid: The Open-Source Unified Video Model (AR)centric route: all modalities (text, images, video) are projected into shared discrete token space and single Transformer is trained with next-token prediction over multimodal sequences; representative examples include Emu3 (Wang et al., 2024b) and Chameleon (Lu et al., 2023). The second is hybrid diffusionAR route: multimodal AR backbone governs understanding and control signals, while diffusion video decoder renders high-fidelity frames from high-level visual tokens; recent works such as Transfusion (Zhou et al., 2024) and Show-O (Xie et al., 2025) follow this pattern. In this work, we adopt the hybrid route to retain high-quality rendering while leveraging an MLLM for semantic control and interpretability. However, even within this hybrid setting, unified video modeling faces two key challenges. First, maintaining semantically faithful conditioning in video diffusion across the flow trajectory is difficult. Text prompts convey high-level intent but under-specify pixel-aligned details; in MM-DiT-style Esser et al. (2024) models, the cross-modal signal can be diluted by the numerical imbalance between few text tokens and many visual tokens, and the role of guidance is inherently timestepdependentearly steps benefit more from strong semantic constraints, whereas later steps benefit from visual detail refinement, yielding promptvideo drift that worsens with longer, higherresolution clips. Second, extending image-centric MLLMs to video faces two key challenges: the computational cost of temporal modeling (dedicated encoders, long-context handling, large-scale training) that risks destabilizing existing capabilities, and the mismatch between videos vast temporal information and the typically small subset relevant to any question. Traditional approaches either process all frames uniformly, causing inefficiency and noise, or use fixed sampling that may miss critical evidence. Furthermore, different question types demand different strategiesstatic questions need distinctive keyframes while dynamic questions require understanding temporal transitions. To address these challenges, our motivation is twofold. First, on the generation side, we leverage multimodal understanding to construct structure-aware tokens in the language space that encode both global semantics and localized cues; these tokens are used as faithful semantic conditioning for diffusion video decoder, and we schedule cross-modal attention over flow steps so that early integration emphasizes textual intent while later steps emphasize visual refinement. Second, on the understanding side, we develop an adaptive evidence selection approach that extends imagecentric MLLMs to video without substantial architectural changes. This requires mechanism that can iteratively explore and refine the evidence set based on feedback, balance exploration of new frames with exploitation of current evidence, and learn from failure signals to improve future selections. This suggests sequential decision-making framework, but rather than traditional parameter updates, we implement form of verbal test-time reinforcement learning. We develop Pyramid Reflection, where policy improvement occurs through natural language refinementthe Reflector verbally adjusts search queries based on feedback, while SigLIP2 (Tschannen et al., 2025) enables query-driven keyframe selection that iteratively expands or prunes the evidence set. Hence, we propose UniVid, unified architecture that couples multimodal LLM with diffusion video decoder via lightweight conditioning adapter: the LLM ingests text and salient visual evidence and outputs rich semantic understandable tokens that both support reasoning and condition the decoder for text/image-to-video generation. To stabilize guidance in MM-DiT (Esser et al., 2024), we introduce Temperature Modality Alignment, timestep-aware, temperature-adjusted cross-modal attention schedule that emphasizes semantic intent early and visual refinement late, mitigating text suppression and improving prompt faithfulness. To enable efficient understanding with minimal change, we introduce Pyramid Reflection, which implements sequential decision-making through SigLIP2-based keyframe selection and an ActorEvaluatorReflector loop that verbally adjusts search strategies while progressively expanding or pruning context. Through extensive evaluation on standard benchmarks, we validate the superior capability of our unified approach, which consistently outperforms existing methods across multiple video-centric tasks, demonstrating the potential of unified modeling for comprehensive video intelligence. Our contribution can be summarized below: We introduce UniVid, unified paradigm that couples an MLLM with diffusion video decoder via lightweight conditioning adapter; the MLLM produces rich, understandable semantic tokens that both support reasoning and condition text/image-to-video generation. We propose Temperature Modality Alignment, timestep-aware, temperature-adjusted crossmodal attention schedule in MM-DiT that strengthens early semantic guidance and later shifts em2 UniVid: The Open-Source Unified Video Model Figure 2: Overall architecture of our proposed UniVid for unified video understanding and generation. UniVid couples an autoregressive-based MLLM with DiT-based diffusion decoder. The MLLMs outputs are linked through lightweight adapter to interface with the Wan (Wang et al., 2025) backbone, forming the generation branch, while simultaneously passing through the Pyramid Reflection module to connect with the LLM, thereby establishing the understanding branch. phasis to visual refinement; we further develop Pyramid Reflection with SigLIP2-based keyframe selection to enable efficient temporal reasoning with minimal architectural change and training. We conduct comprehensive experiments on MSVD-QA (Piergiovanni et al., 2022), MSRVTTQA (Piergiovanni et al., 2022), TGIF-QA (Jang et al., 2017), and ActivityNet-QA (Yu et al., 2018) for understanding, and on VBench for generation, demonstrating competitive performance and efficiency. Ablations verify the contribution of each component."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Video generation. Video generation has seen remarkable advancements with the rise of diffusion models and generative adversarial networks tailored for temporal data. Recent diffusion or flow based frameworks, such as Video Diffusion Models (Ho et al., 2022b), Imagen Video (Ho et al., 2022a), VideoCrafter2 (Chen et al., 2024a) and Stable Video Diffusion (Blattmann et al., 2023b), have produced high-fidelity clips with improved temporal consistency, enabling applications in creative generation and simulation (Liu et al., 2025; Shi et al., 2025). Latent diffusion techniques (Blattmann et al., 2023c) further improve efficiency by operating in compressed latent spaces, enabling scalable video generation. In parallel, GAN methods like MoCoGAN (Tulyakov et al., 2018) and StyleGAN-V (Skorokhodov et al., 2022) explore alternative formulations. Despite these advances, maintaining long-term temporal consistency in extended sequences remains challenging, as summarized by recent surveys and analyses (Melnik et al., 2024; Yin et al., 2025). Video understanding. Recent progress in video understanding has been driven by transformerbased architectures and self-supervised learning paradigms that effectively model spatio-temporal relationships. Methods like MViT (Fan et al., 2021), Video Swin Transformer (Liu et al., 2022), TimeSformer (Bertasius et al., 2021) and ViViT (Arnab et al., 2021) have advanced the field by capturing long-range dependencies across video frames, achieving strong performance on datasets such as Kinetics-700 (Carreira et al., 2019). Beyond supervised training, self-supervised ap- (Wei et al., proachesincluding masked modeling (VideoMAE (Tong et al., 2022), MaskFeat 3 UniVid: The Open-Source Unified Video Model 2022), OmniMAE (Girdhar et al., 2023)) and early contrastive methods (VideoMoCo (Pan et al., 2021))leverage unlabeled videos to learn robust, transferable representations, reducing dependence on costly annotations and benefiting action recognition and video segmentation. Unified multimodal models. Unified multimodal modeling has progressed from joint visionlanguage pretraining to architectures that support both understanding and generation across modalities. Foundational systems like CLIP (Radford et al., 2021) establish large-scale alignment, while BEiT-3 (Wang et al., 2023) and UnifiedMLLM (Li et al., 2024) broaden task coverage. Pushing toward unified generation, Show-o (Xie et al., 2025) integrates autoregression with discrete diffusion within single Transformer to support VQA, text-to-image, and various editing tasks. In complementary direction focused on robustness rather than general any-to-any generation, FLUID (Cuong et al., 2025) uses token-level distillation for cross-modal fusion. Open generalist systems then aim to unify understanding and generation end-to-end: BAGEL (Deng et al., 2025) offers an open, decoder-only framework with parallel language and diffusion branches trained jointly, achieving competitive results across image-centric tasks, and BLIP3-o (Chen et al., 2025) releases fully open family where diffusion transformer is coupled to strong multimodal understanding, yielding unified image understanding and generation. Extending unification from images to video, OmniVideo (Tan et al., 2025) teaches an MLLM to emit continuous visual tokens that are adapted and consumed by diffusion video decoder, enabling generation, editing, and understanding in one pipeline."
        },
        {
            "title": "3 THE PROPOSED METHOD",
            "content": "3.1 OVERVIEW Our goal is unified multimodal video model that supports both generation and understanding within single framework. To this end, we adopt three-stage hierarchical training recipe that first aligns the conditioning between the MLLM and the generator, then finetunes the MLLM and introduces Pyramid Reflection, which augments the understanding branch with temporal cues, and finally coadapts both branches end-to-end. Fig. 2 presents the overall UniVid architecture. 3.2 ARCHITECTURE Multimodal archiecture. The multimodal large language model serves as the core reasoning engine. Text inputs are processed through standard tokenizer, while visual inputs follow different encoding paths depending on the target branch. For the generation branch, images are encoded using both ViT (Dosovitskiy et al., 2021) for semantic features and VAE (Kingma & Welling, 2019) for pixel-level details. For the understanding branch, only ViT encoding is employed, as video understanding tasks primarily rely on high-level semantic understanding rather than fine-grained pixel details. The encoded visual features are then projected into the textual token space and concatenated with text tokens, allowing the LLM to output unified multimodal representations. Generation branch. The generation pathway employs DiT-based model Wan 2.2 (Wang et al., 2025) conditioned on rich semantic representations extracted from MLLM outputs through lightweight adapter. The system processes video generation in latent space using 3D VAE (Zhao et al., 2024), with conditioning signals integrated via cross-attention mechanisms. Understanding branch. For video understanding, multi-frame evidence is encoded by the ViT (Dosovitskiy et al., 2021) and fused with text; the LLM produces an initial textual answer. We then apply Pyramid Reflection, query-driven, hierarchical loop that iteratively expands or prunes keyframe context via SigLIP2 (Tschannen et al., 2025) selection and refines the frame space via an ActorEvaluatorReflector process, yielding the final answer without modifying the backbone. Conclusively, our generation builds on the MLLMs strong comprehension, while video understanding uses Pyramid Reflection to leverage the MLLM and collaborate with an LLM for efficient and accurate answers. 4 UniVid: The Open-Source Unified Video Model Algorithm 1 Pyramid Reflection as Test-time RL Require: video , question 1: Uniformly sample =64 frames; encode once and cache visual embeddings 2: From 16 frames, summarize into global caption Cg 3: Initialize state s1 (q, Cg, =), policy π with mode router expand/shrink 4: for = 1 to 3 do Action: ar π(sr) 5: expand: add frames most relevant to current search text shrink: prune to diverse key frames using cached similarities Update working set accordingly using cached embeddings Actor: answer using ordered conditioned on Cg Evaluator: score ˆrr [0, 1] as confidence signal if ˆrr τ then return answer elseReflector: refine the search text short declarative cue 6: 7: 8: 9: 10: 11: 12: 13: end for 14: return fallback answer from Cg end if Update state sr+1 (q, Cg, ) (index-only change) (verbal policy improvement) 3.3 CONDITIONAL GENERATION WITH TEMPERATURE MODALITY ALIGNMENT Given fused tokens from the understanding path, the MLLM output Zu is mapped to time-indexed conditions by lightweight adapter gϕ: Ct = gϕ(Zu, t) RMtdc, (1) where Mt is the number of conditioning tokens at timestep and dc is the conditioning dimension. Let the 3D VAE define the latent trajectory {zt} along the flow, where zt RHW C represents the latent representation with spatial dimensions , temporal frames , and channels C. The Wan 2.2 DiT predicts the velocity field under cross-attention to Ct, then we integrate the probabilityflow ODE to obtain ˆz0, which the VAE decoder converts to video frames. Inspired by TACA (Lv et al., 2025), we adapt its finding that text is suppressed in MM-DiT (Esser et al., 2024) because (i) the softmax over much larger pool of visual tokens (Nvis Ntxt) dilutes attention mass on text keys, and (ii) conditioning plays different roles across timesteps (early semantics, late detail). We therefore strengthen the visual-to-text path in Wan 2.2 (Wang et al., 2025) with simple schedule: Svt(u) = αtxt(u) Svt, [0, 1], (2) where is the normalized flow matching progress (0 early, 1 late), Svt denotes the visual-to-text attention scores, and Svt(u) represents the modulated attention scores. The modulation factor is defined as: αtxt(u) = 1 + 1, (cid:16) λtxt 2 1 + cos (cid:17)(cid:17) (cid:16) πu 0. , [0, 0.4], (0.4, 1], λtxt = 0.3. (3) Thus, text guidance is strongest early and decays to neutral (αtxt 1) late, improving prompt faithfulness without over-constraining details. For reference-image that requires identity stability, we apply small late-stage boost to visual crossattention: Svv(u) = αimg(u) Svv, (4) where Svv represents visual cross-attention scores and αimg(u) = 1, 1 + (cid:18) λimg 2 1 cos (cid:18) π(u 0.6) 0. (cid:19)(cid:19) [0, 0.6], , (0.6, 1], λimg = 0.3. (5) UniVid: The Open-Source Unified Video Model Table 1: T2V performance on VBench-Long (Huang et al., 2024). Method EasyAnimateV5.1 (Fu et al., 2024) MiniMax-Video-01 (MiniMax, 2024) Kling 1.6 (Technology, 2025) Wan2.1-T2V-1.3B (Wang et al., 2025) HunyuanVideo (Kong et al., 2024) Gen-3 (Runway, 2024) Vchitect-2.0 (VEnhancer) (Fan et al., 2025) CogVideoX1.5-5B (Yuan et al., 2024) UniVid (Ours) Method Overall Scores Technical Quality Aesthetic Quality Total Score Quality Semantic Subject Background Temporal Motion Dynamic Aesthetic Imaging 83.42 83.41 83.40 83.31 83.24 82.32 82.24 82.17 85.27 85.03 84.85 85.20 85.23 85.86 84.11 83.54 82.78 86.44 77.01 77.65 76.99 76.95 75.82 75.17 77.06 79. 80.58 98.00 97.51 97.40 97.56 97.32 97.01 96.83 96.87 98.96 97.41 97.05 96.84 97.93 97.93 96.62 96.66 97.35 97.76 99.19 99.10 99.64 99.55 99.49 99.61 98.97 98. 99.88 98.02 99.22 99.13 98.52 98.99 99.23 98.98 98.31 99.25 57.15 64.91 62.22 65.19 70.83 60.14 63.89 50.93 61.83 69.48 63.03 64.81 65.46 60.36 63.34 60.41 62. 64.21 68.61 67.17 69.70 67.01 67.56 66.82 65.35 65.02 73.03 Object Multi-Obj Action Color Spatial Scene Appearance Temporal Overall Semantic Fidelity EasyAnimateV5.1 (Fu et al., 2024) MiniMax-Video-01 (MiniMax, 2024) Kling 1.6 (Technology, 2025) Wan2.1-T2V-1.3B (Wang et al., 2025) HunyuanVideo (Kong et al., 2024) Gen-3 (Runway, 2024) Vchitect-2.0 (VEnhancer) (Fan et al., 2025) CogVideoX1.5-5B (Yuan et al., 2024) UniVid (Ours) 89.57 97.83 93.34 88.81 86.10 87.81 86.61 87.47 94.52 66.85 76.04 73.99 74.83 71.66 53.64 68.84 69. 77.45 95.60 92.40 96.20 94.00 93.42 96.40 97.20 97.20 94.20 77.86 90.36 81.26 82.00 91.60 80.90 87.04 87.55 92.10 76.11 75.50 79.08 73.04 68.09 65.03 57.55 80. 80.70 54.31 50.68 55.57 41.96 53.69 54.57 56.57 52.91 46.66 23.06 20.06 20.75 21.81 19.80 24.31 23.73 24.89 23.57 24.61 25.63 24.51 23.13 23.89 24.71 25.01 25. 25.91 26.47 27.10 26.04 25.50 26.44 26.69 27.57 27.30 27.60 3.4 PYRAMID REFLECTION FOR UNDERSTANDING Formulation. We cast video question answering as test-time reinforcement learning over small, ordered evidence set. The state at round is (sr, Wr, Cg), where sr is short search text, Wr is an ordered subset of frames, and Cg is global caption distilled once from uniformly sampled seeds. The action is to reconfigure Wr given sr, either by adding frames (expand) or by pruning to diverse core (shrink). The policy πs is retrieval rule driven by textimage similarity and diversity term; it maps to distribution over frame indices. The environment returns an answer produced by the Actor and scalar reward [0, 1] from the Evaluator. Policy improvement is carried out verbally: the Reflector emits refined sr+1 that concentrates on disambiguating cues such as before/after, first/last, motion phase, color, or role. The loop stops early when exceeds confidence threshold. Policy class. We instantiate πs with cached-embedding retriever. All candidate frames are embedded once by vision encoder; the text side uses ϕ(s). For expand we add the highest-scoring unseen frames by cosine similarity vi, ϕ(s), which suits static questions whose evidence is sparse but distinctive. For shrink we start broad to preserve chronology, then apply Maximal Marginal Relevance objective that balances relevance to ϕ(s) and pairwise dissimilarity within , which suits dynamic questions where ordering, repetition, or transitions matter. In both regimes is kept in temporal order so the Actor can compare events across [t1 tk] rather than hallucinate transitions. Value and critic signals. The Evaluator provides calibrated confidence that serves as value proxy. Its scalar reward both triggers early stopping and conditions the Reflector. When is low, the Reflector returns short declarative refinement of that encodes the suspected failure mode: missing entity, wrong time span, ambiguous referent, or occluded phase. This verbal update reshapes the retrieval distribution without touching model weights, yielding form of policy gradient in the space of prompts. Our Pyramid Reflection procedure is summarized in Algorithm 1, and the highlevel understanding pipeline is shown in Fig. 7. The theoretical details of Pyramid Reflection as test-time RL are provided in Appendix A.5. The design achieves efficiency by caching frame embeddings once and reducing exploration to lightweight index updates, while the Actor reasons over compact, temporally ordered evidence with fixed global context to maintain scene priors under tight token budgets. The adaptive routing between expansion and MMR-based shrinking aligns retrieval strategies with question structure, enabling effective temporal reasoning at low computational cost."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 DATASET AND METRICS Datasets. We evaluate UniVid on established benchmarks for both video generation and understanding. For generation, we train on curated samples from OpenVid-1M, large-scale text-to-video dataset, and evaluate on VBench, comprehensive benchmark suite for video generative models that 6 UniVid: The Open-Source Unified Video Model Table 2: Comparison on four video QA benchmarks (Piergiovanni et al., 2022; Jang et al., 2017; Yu et al., 2018). Video QA Performance Method LLM size FrozenBiLM (Yang et al., 2022) VideoChat (Li et al., 2023) LLaMA-Adapter (Zhang et al., 2023b) Video-LLAMA (Zhang et al., 2023a) Video-ChatGPT (Maaz et al., 2024) Chat-UniVi (Jin et al., 2024) Video-LLaVA (Lin et al., 2024) BT-Adapter (Liu et al., 2024) Valley-v3 (Luo et al., 2023) FreeVA (Wu, 2024) DeepStack-L (Meng et al., 2024) IG-VLM (LLaVA-v1.6) (Kim et al., 2024) SF-LLaVA-7B (Xu et al., 2024) UniVid (Ours) 1B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA Acc Score Acc Score Acc Score Acc Score 32.2 56.3 54.9 51.6 64.9 65.0 70.7 67.5 60.5 73.8 76.0 78.8 79.1 80.1 2.8 3.1 2.5 3.3 3.6 3.9 3.7 3.3 4.1 4.0 4.1 4.1 4.2 16.8 45.0 43.8 29.6 49.3 54.6 59.2 57.0 51.1 60.0 63.7 65.8 61.4 2.5 2.7 1.8 2.8 3.1 3.5 3.2 2.9 3.5 3.5 3.6 3.4 41.0 34.4 51.4 60.3 70.0 78.7 75. 2.3 3.0 3.4 4.0 4.0 4.2 4.1 24.7 34.2 12.4 35.2 45.8 45.3 45.7 45.1 51.2 49.3 54.3 55.5 58.8 2.2 2.7 1.1 2.7 3.2 3.3 3.2 3.2 3.5 3.1 3.4 3.4 3.6 provides fine-grained evaluation metrics across multiple dimensions. For understanding, we train on 20k samples from the ActivityNet-QA train dataset (Yu et al., 2018) and evaluate on four comprehensive video QA benchmarks: MSVD-QA (Piergiovanni et al., 2022) with 1,970 video clips and 50.5K QA pairs, MSRVTT-QA (Piergiovanni et al., 2022) with 10K videos, 243K QA pairs, TGIFQA (Jang et al., 2017) containing 165K QA pairs for animated GIFs, and the ActivityNet-QA test dataset (Yu et al., 2018) with 58,000 QA pairs on 5,800 complex web videos. These datasets cover diverse temporal reasoning scenarios across short to medium-length video clips, ranging from brief animated sequences to multi-minute activity videos. Evaluation metrics. For video generation, we evaluate on VBench across multiple fine-grained dimensions: Technical Quality metrics including Subject consistency, Background preservation, Temporal flickering, Motion smoothness, and Dynamic degree; Aesthetic Quality measures covering overall visual appeal and imaging quality; and Semantic Fidelity metrics assessing Object accuracy, Multi-object handling, Action fidelity, Color accuracy, Spatial relationships, Scene consistency, Appearance preservation, and Temporal coherence. For video understanding, we report average accuracy and scores on each benchmark dataset. 4. IMPLEMENTATION DETAILS We adopt three-stage hierarchical training recipe. It initializes UniVid from strong public checkpoints to reduce compute. For generation, we couple the BAGEL-7B (Deng et al., 2025) with Wan 2.2 5B TI2V model (Wang et al., 2025) via textual adapter and LoRA on DiT (Peebles & Xie, 2023), keeping other weights frozen. For understanding, we tune only the connector and the last two ViT blocks on ActivityNet QA (Yu et al., 2018) with dialog style supervision while the LLM remains frozen. Finally, we co-train both tasks to refine the connector and obtain additive gains. Sequence parallelism enables long high-resolution clips. For details, see Appendix A.2. For generation, we use flow-matching ODE sampler with classifier-free guidance and universal negative prompt. Unless noted, videos are sampled at 1280 704 resolution, 121 frames at 24 fps; the guidance scale is set to 5.0 for both T2V and I2V with 50 inference steps. At input time, the LLM receives the text prompt together with image ViT embeddings and VAE latents; it outputs conditional textual tokens. During generation, Wan 2.2 consumes these conditional textual tokens and image via cross-attention. Our Temperature Modality Alignment schedule applies cosinescheduled text gain that transitions from αtxt = 1.3 to 1.0 over the first 40% of denoising steps (u [0, 0.4]), then maintains αtxt = 1.0 for the remaining steps. This enhances text guidance during early denoising when structural decisions are made, while allowing finer details to emerge in later stages. For understanding, we uniformly sample pool of = 64 frames per video and cache their SigLIP2 image embeddings; subsequent selection reuses cached features. Global context is caption summarized from 16 uniformly spaced seed frames. Queryimage ranking uses SigLIP2 cosine similarity with L2-normalized features and batch size 64. Static questions follow 4 8 16 keyframe schedule. Dynamic questions follow 64 32 16 with MMR down-selection, λ = 0.5. Con7 UniVid: The Open-Source Unified Video Model Figure 3: Comparisons with State-of-the-Art Video Generation Models (Wang et al., 2025; MiniMax, 2024; Kong et al., 2024; Fu et al., 2024). fidence is accepted when the Evaluators score is at least 0.7 or the verdict is accept, with at most 3 rounds. The LLM determines routing between static and dynamic modes. For implementation, we use DeepSeek v3.1 to serve as the Evaluator and determine the type of questions and Qwen-plus to serve as the Reflector. Full prompt texts are listed in the Appendix A.4. 4.3 MAIN RESULTS Generation quantitative results. We evaluate UniVid on the challenging VBench-Long benchmark (Huang et al., 2024). As shown in Tab. 1, UniVid establishes new state of the art with an overall score of 85.27, outperforming prior leading systems such as EasyAnimateV5.1 (Fu et al., 2024), MiniMax-Video-01 (MiniMax, 2024), and Kling 1.6 (Technology, 2025). In particular, UniVid exhibits clear advantages in semantic alignment (80.58), highlighting its superior capability in faithfully rendering objects, actions, and multi-object interactions. On the technical side, it attains near-perfect temporal (99.88) and motion (99.25) consistency, validating the effectiveness of our long-context dynamics module. Moreover, UniVid delivers the best imaging score (73.03), reflecting sharper details and more stable visual quality compared with prior systems, as shown in Fig. 1, which demonstrates high-quality visual generation. Beyond overall scores, UniVid demonstrates consistent gains in semantic fidelity. As summarized in the Semantic Fidelity block of Tab. 1, it achieves leading results on multi-object reasoning (77.45), color faithfulness (92.10), and spatial grounding (80.70), while remaining competitive in action depiction and appearance consistency. These improvements suggest that our design choicesparticularly the integration of hierarchical scene representation with dynamic frame alignmentsubstantially enhance both controllability and alignment with textual prompts. Taken together, the results indicate that UniVid pushes forward the frontier of long-horizon text-to-video generation by simultaneously ensuring high-fidelity semantics and strong technical as well as aesthetic quality. More examples of video generation can be seen in Appendix A.3. Generation qualitative results. Fig. 3 compares UniVid with Wan2.1-T2V-1.3B (Wang et al., 2025), MiniMax-Video-01 (MiniMax, 2024), HunyuanVideo (Kong et al., 2024), and EasyAnimateV5.1 (Fu et al., 2024). Competing models often show missing basketballs or distorted cars, while UniVid generates coherent jump shots and realistic racing scenes with stable dynamics and faithful semantics. Understanding quantitative evaluation. Across MSVD-QA (Piergiovanni et al., 2022), MSRVTT-QA (Piergiovanni et al., 2022), TGIF-QA (Jang et al., 2017), and ActivityNet-QA (Yu 8 UniVid: The Open-Source Unified Video Model Figure 4: Comparisons of State-of-the-Art Video Understanding Models (Lin et al., 2024; Xu et al., 2024). et al., 2018), UniVid sets the 7B-scale state of the art on MSVD-QA and ActivityNet-QA and remains competitive on the other two (Tab. 2), despite smaller post-training set and no test-time ensembling. Joint finetuning of generation and understanding with Pyramid Reflection strengthens the abilities these datasets emphasize: better actionentity binding and object or attribute grounding in short open-domain clips, stronger temporal reasoning over frame sequences, and more reliable long-range evidence retrieval in untrimmed videos. As illustrated before, UniVid performs robust multi-frame reasoning with our Pyramid Reflection loop. Starting from global caption and automatic type detection, the system first produces an initial answer, which is then scored by the evaluator; when evidence is insufficient, the reflector issues refined, declarative query that re-ranks keyframes toward the true scene. This Pyramid Reflection steers attention from opening credits to the lane shots, yielding consistent interpretation of roles (in the example of Fig. 7: bowler and nearby teammate/coach) grounded in the visual context rather than spurious cues. The dynamic keyframe schedule reduces the number of inspected frames while maintaining accuracy, demonstrating both evidence tracing and efficiency gains in short-clip understanding. More examples of video understanding can be seen in Appendix A.3. Understanding qualitative results. We compare UniVid with Video-LLaVA (Lin et al., 2024) and SF-LLaVA (Xu et al., 2024) on video QA; as shown in Fig. 4, baselines often give plausible but incomplete statements. These examples highlight UniVids stronger actionentity binding, temporal reasoning, and resistance to distractor frames, yielding precise and concise answers. Additionally, we conduct systematic ablation experiments to validate the contributions of UniVid. The results and analyses are provided in the Appendix A.6."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced UniVid, unified video model that couples an MLLM with diffusion decoder via lightweight conditioning adapter to both understand and generate videos. Two key mechanisms enable this: Temperature Modality Alignment schedules cross-modal attention across flow steps to preserve prompt faithfulness while refining details, and Pyramid Reflection performs querydriven keyframe selection for efficient temporal reasoning. With these components, UniVid achieves state-of-the-art or competitive results on VBench-Long and multiple video-QA benchmarks while avoiding costly retraining of image-centric backbones. We release UniVid to support research on practical, controllable, and truly unified video intelligence. 9 UniVid: The Open-Source Unified Video Model"
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: training generalized multi-query transformer models from multi-head In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 checkpoints. Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 48954901. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.298. URL https://doi.org/10.18653/v1/2023. emnlp-main.298. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid. Vivit: video vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pp. 68166826, 2021. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, pp. 813824, 2021. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. CoRR, abs/2311.15127, 2023a. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023b. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2256322575, 2023c. Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 73107320, 2024a. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. CoRR, abs/2505.09568, 2025. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Intern VL: scaling up vision foundation models and aligning for generic visual-linguistic tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 2418524198. IEEE, 2024b. Van Duc Cuong, Ta Dinh Tam, Tran Duc Chinh, and Nguyen Thi Hanh. Fluid: Flow-latent unified integration via token distillation for expert specialization in multimodal learning, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Shi Guang, and Haoqi Fan. Emerging properties in unified multimodal pretraining. CoRR, 2025. 10 UniVid: The Open-Source Unified Video Model Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=YicbFdNTTy. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= FPnUhsQJ5B. Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68246835, 2021. Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. Jaskie Fu, Kun-Hao Yeh, Zhaofan Zha, Xinyu Wang, Chenghao Li, Han-Yi Shaw, Chao-Yi Li, and Pin-Yu Chen. Easyanimate: An easy-to-use framework for creating high-quality and controllable videos from single image. arXiv preprint arXiv:2403.04416, 2024. Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Omnimae: Single model masked pretraining on images and videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1040610417, 2023. Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-key In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of normalization for transformers. the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 42464253. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.FINDINGS-EMNLP.379. URL https://doi. org/10.18653/v1/2020.findings-emnlp.379. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. CoRR, 2022a. Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022b. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: toward spatiotemporal reasoning in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 13591367. IEEE Computer Society, 2017. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1370013710. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01300. UniVid: The Open-Source Unified Video Model Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zero-shot video question answering using vlm, 2024. URL https://arxiv.org/ abs/2403.18406. Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307392, 2019. doi: 10.1561/2200000056. URL https://doi.org/ 10.1561/2200000056. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. CoRR, abs/2305.06355, 2023. Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, and Tao Wang. Unifiedmllm: Enabling unified representation for multi-modal multi-tasks with large language model. arXiv preprint arXiv:2408.02503, 2024. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 59715984. Association for Computational Linguistics, 2024. Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, et al. Fpsattention: Training-aware fp8 and sparsity co-design for fast video diffusion. arXiv preprint arXiv:2506.04648, 2025. Ruyang Liu, Chen Li, Yixiao Ge, Thomas H. Li, Ying Shan, and Ge Li. Bt-adapter: Video conversation is feasible without video instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 1365813667. IEEE, 2024. doi: 10.1109/CVPR52733.2024.01296. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 32023211, 2022. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. CoRR, abs/2306.07207, 2023. Zhengyao Lv, Tianlin Pan, Chenyang Si, Zhaoxi Chen, Wangmeng Zuo, Ziwei Liu, and KwanYee K. Wong. Rethinking cross-modal interaction in multimodal diffusion transformers, 2025. URL https://arxiv.org/abs/2506.07986. Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1258512602. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.679. Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge J. Ritter. Video diffusion models: survey. Trans. Mach. Learn. Res., 2024. UniVid: The Open-Source Unified Video Model Deepstack: Deeply stacking visual Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and tokens is surprisingly simple and Yu-Gang Jiang. effective for lmms. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 29cd7f8331d13ede6dc6d6ef3dfacb70-Abstract-Conference.html. MiniMax. Minimax video generation api is now available. https://www.minimaxi.com/ en/news/video-generation-api, October 2024. Accessed: 2025-07-24. Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video representation learning with temporally adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1120511214, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 41724182. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00387. URL https://doi.org/ 10.1109/ICCV51070.2023.00387. A. J. Piergiovanni, Kairo Morton, Weicheng Kuo, Michael S. Ryoo, and Anelia Angelova. Video question answering with iterative video-text co-tokenization. In Shai Avidan, Gabriel J. Brostow, Moustapha Cisse, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, volume 13696 of Lecture Notes in Computer Science, pp. 7694. Springer, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, ICLR 2024, synthesis. Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Runway. Gen-3 alpha: new frontier for video generation. Technical report, Runway, July 2024. Accessed: 2025-07-24. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https: //arxiv.org/abs/2002.05202. Jingwei Shi, Zeyu Zhang, Biao Wu, Yanjie Liang, Meng Fang, Ling Chen, and Yang Zhao. Presentagent: Multimodal agent for presentation video generation. arXiv preprint arXiv:2507.04036, 2025. Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improving image captioning with better use of captions, 2020. URL https://arxiv.org/abs/2006.11807. language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 1b44b878bb782e6954cd888628510e90-Abstract-Conference.html. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video In IEEE/CVF Conference on generator with the price, image quality and perks of stylegan2. Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 36163626, 2022. 13 UniVid: The Open-Source Unified Video Model Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. doi: 10.1016/J.NEUCOM.2023.127063. URL https://doi.org/10.1016/j.neucom. 2023.127063. Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, and Hao Li. Omni-video: Democratizing unified video understanding and generation. CoRR, abs/2507.06119, 2025. Kuaishou Technology. Kling. https://klingai.kuaishou.com/, 2025. Accessed: 202507-24. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Michael Tschannen, Alexey A. Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier J. Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. CoRR, abs/2502.14786, 2025. Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion In 2018 IEEE Conference on Computer Vision and Pattern and content for video generation. Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 15261535, 2018. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, ZhiFan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. CoRR, abs/2503.20314, 2025. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024a. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as foreign language: Beit pretraining for vision and vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1917519186, 2023. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. CoRR, abs/2409.18869, 2024b. Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan L. Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1464814658, 2022. Wenhao Wu. Freeva: Offline MLLM as training-free video assistant. CoRR, abs/2405.07798, 2024. doi: 10.48550/ARXIV.2405.07798. URL https://doi.org/10.48550/arXiv.2405. 07798. 14 UniVid: The Open-Source Unified Video Model Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025, 2025. Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models, 2024. URL https://arxiv.org/abs/2407.15841. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. doi: 10.48550/ARXIV.2407.10671. URL https://doi.org/10.48550/arXiv.2407. 10671. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Zhiyu Yin, Kehai Chen, Xuefeng Bai, Ruili Jiang, Juntao Li, Hongdong Li, Jin Liu, Yang Xiang, Jun Yu, and Min Zhang. Asurvey: Spatiotemporal consistency in video generation, 2025. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 91279134. AAAI Press, 2018. Zhen Yuan, Yifei Chen, Shuo Zhao, Wen yi Wang, Ming-Hao Zhang, Zhiping Wang, Le Zhang, Boxi Zhao, Jian Li, Zhi-Yuan Wu, Ming Ding, and Jie Tang. Cogvideox: general-purpose video generation model. arXiv preprint arXiv:2406.06511, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1236012371, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language In Yansong Feng and Els Lefever (eds.), Proceedings of the model for video understanding. 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023, pp. 543553. Association for Computational Linguistics, 2023a. Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. Llama-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2023b. Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo CV-VAE: compatible video VAE for latent generative video Hu, and Ying Shan. 15 UniVid: The Open-Source Unified Video Model In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulmodels. InJakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural rich Paquet, formation Processing Systems 38: Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 1787533e171dcc8549cc2eb5a4840eec-Abstract-Conference.html."
        },
        {
            "title": "Annual Conference on Neural",
            "content": "Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. URL https://arxiv.org/abs/2408. 11039. 16 UniVid: The Open-Source Unified Video Model"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USE DECLARATION Large Language Models (ChatGPT) were used exclusively to improve the clarity and fluency of English writing. They were not involved in research ideation, experimental design, data analysis, or interpretation. The authors take full responsibility for all content. A.2 HIERARCHICAL POST TRAINING Initialization. To avoid the prohibitive cost of training unified video model from scratch, we bootstrap UniVid from strong, publicly available checkpoints and finetune only small subsets of parameters. Our architecture follows the BAGEL (Deng et al., 2025) design framework, adopting its multimodal integration approach with three key components: Qwen2 (Yang et al., 2024) as the LLM backbone with standard architectural choices such as RMSNorm (Zhang & Sennrich, 2019), SwiGLU (Shazeer, 2020), RoPE (Su et al., 2024), GQA (Ainslie et al., 2023), and QK-Norm (Henry et al., 2020) for training stability, SigLIP2-so400m/14 (Tschannen et al., 2025) as the ViT (Dosovitskiy et al., 2021) encoder for visual understanding with NaViT support for native aspect ratios, and pre-trained FLUX VAE with 8 downsampling and frozen weights. The framework interleaves text, ViT, and VAE tokens within the LLM using generalized causal attention, where tokens attend to all preceding modality splits while maintaining appropriate attention patterns within each modality. Data curation and formatting. For understanding, we align our data format with the dialog style used by Video-ChatGPT (Maaz et al., 2024). ActivityNet-QA annotations (video id, q, a) are converted into structured conversations. Specifically, each sample is represented as JSON object containing three fields: (1) an identifier, (2) video reference, and (3) conversations array consisting of two turns, user query and the corresponding model response. For generation, we curate subset of OpenVid-1M to form text/image to video pairs. Videos are uniformly sub-sampled and preprocessed identically to inference. Stage generation branch alignment. We couple the MLLM with Wan 2.2 and adapt the conditioning path so that MLLM-produced tokens can reliably steer synthesis. Concretely, we (i) insert textual adapter between the LLM tokens, with dynamic sequence length adaptation, and (ii) apply LoRA to the DiT cross-attention layers; all other DiT/MLLM weights remain frozen. Training uses standard flow-matching objective with classifier-free guidance dropout on text, optimizing only the context projector and LoRA parameters. This stage preserves MLLMs native understanding while aligning Wans generation to the rich semantics emitted by MLLM. Stage II understanding adaptation. We finetune for video QA on ActivityNet-QA using 20k samples from the dataset. Each sample concatenates the question with <video> placeholder, and we feed multi-frame clip obtained by uniform sampling. Frames are encoded by the ViT into visual tokens and projected to the LLM space via the connector. We adopt instruction SFT for video: compute autoregressive cross-entropy only on the assistant turns; user tokens are fully masked to prevent label leakage. To keep compute moderate while injecting temporal cues, we finetune only the last two ViT blocks and the connector for 4 epochs, keeping the LLM frozen. Stage III joint training. Finally we co-train generation and understanding to let the two branches benefit from each other. We alternate mini-batches from the two tasks and update primarily the connector within the MLLM and the textual adapter which bridges MLLM outputs to Wan 2.2, using the sum of the velocity loss for generation and the masked autoregressive loss for understanding. This joint stage optimizes the MLLM to improve both understanding and generation performance. A.3 MORE EXAMPLES OF VIDEO GENERATION AND UNDERSTANDING. We provide more examples of video understanding and generation in Fig. 5 and Fig. 6 17 UniVid: The Open-Source Unified Video Model Figure 5: The qualitative results of the video understanding. Blue for static questions, green for dynamic questions. 18 UniVid: The Open-Source Unified Video Model Figure 6: The qualitative results of T2V and TI2V generation. 19 UniVid: The Open-Source Unified Video Model Figure 7: The pipeline of the video understanding. A.4 TEXT PROMPTS USED IN THE UNDERSTANDING Role. Classify video question as static or dynamic. Output JSON only. Definitions. dynamic: requires temporal reasoning such as counting, repetition, order, or changes over time (e.g., how many times, before/after, first/last). static: can be answered from small set of unordered frames (identity, attribute, location, scene, one-shot action). Question. {question} Return. Single-line JSON with fields: qtype (\"static\" or \"dynamic\"), rationale (12 short phrases; no extra text). 1: Question Type Classification Prompt Role. Summarize chronologically ordered frame notes into compact global caption. Do not invent facts. Input. Frame-wise notes (earlier later): - {note 1} - {note 2} . . . Write. One global caption (24 sentences) that connects multiple frames, focusing on: (1) moving entities with consistent appearance and actions across time; (2) static scene objects and their states; (3) temporal hints only if explicitly evidenced (e.g., then, later, repeatedly). Style: terse and factual; no bullet lists, storytelling, or frame-by-frame recitation. 2: Frame Summarization Prompt 20 UniVid: The Open-Source Unified Video Model score (float 0..1), verdict (\"accept\" if score 0.7 else \"reject\"), Role. Precise evaluator for video-QA. Return single-line JSON only (no Markdown/code). Keys. brief reason (12 short bullets). Example user. {one shot user} Example assistant. {one shot assistant} Your task. Given the current case, output the JSON only. 3: Answer Evaluation Prompt Role. Reflector in video-understanding pipeline. You receive the question, global caption (from 16 uniformly sampled frames), the last answer (low confidence/rejected), and its evaluation JSON. Objective. Analyze why the answer likely fails (missing object, wrong span, ambiguity, etc.) and produce single short declarative retrieval text for the next round of keyframe selection. Strict rules. (1) Output JSON only with key refined query. (2) refined query 25 tokens, declarative statement (not question), capturing disambiguating cues (entities, attributes, actions, temporal hints, viewpoint). (3) If confidence is already good (score 0.7 or verdict=\"accept\"), return an empty string. (4) Prefer concrete visual cues (colors, clothing, object names, motion phase, timestamps, left/right, first/last). (5) No speculation or unseen entities. Inputs. {last answer} Return. {\"refined query\": \"...\"} Evaluation JSON: {eval json} {global caption} Global caption: Last answer: {question} Question: 4: Reflection Prompt Role. Assist video understanding via per-frame analysis. Describe the main objects and actions in this single frame concisely. Focus. (1) Living entities: distinct entities (appearance, clothing, color, species), likely roles, and what each is doing (verb phrases). (2) Static objects & scene: salient items and states (color, shape, on/off, open/closed, broken/intact), plus scene context (indoor/outdoor, location hints). Style. Specific but brief; no speculation; 24 short sentences. 5: Single-Frame Analysis Prompt Role. Answer concisely using only the question and the global video caption. Inputs. Question: {question} Global caption (may miss fine details): {global caption} Instruction. Produce one short answer (12 sentences). If information is insufficient, reply: Not enough evidence from global caption. 6: Global Answer Prompt A.5 PYRAMID REFLECTION AS TEST-TIME RL We cast Pyramid Reflection as test-time reinforcement learning procedure operating on an ordered evidence set. At round r, the state is xr = (sr, Wr, Cg), where sr is short search text, Wr is the ordered working set of frames, and Cg is global caption distilled once from uniformly sampled seeds. The action reconfigures Wr given sr via an expand or shrink policy. The Actor answers from (Wr, Cg), and the Evaluator returns score Rr [0, 1] and verdict that controls early stopping. All frame embeddings are computed once and cached; later rounds update indices and similarity or diversity scores only. Frame selection uses visionlanguage retriever with cosine similarity. Let ϕ(s) be the text embedding and {vi}N i=1 the cached frame embeddings: sim(i, s) = (cid:10) (cid:98)vi, (cid:100)ϕ(s)(cid:11). 21 (6) UniVid: The Open-Source Unified Video Model Table 3: Ablation study of UniVid on VBench-Long. w/o means without. Best results are bold. Model Overall Scores Technical Quality Aesthetic Quality Total Score Quality Semantic Subject Background Temporal Motion Dynamic Aesthetic Imaging UniVid (base) UniVid (w/o MLLM) UniVid (w/o TMA) UniVid (Full) 76.25 77.82 80. 85.27 77.11 78.69 81.51 86.44 72.82 74.32 76.04 80.58 93.82 94.55 96. 98.96 93.43 94.78 95.91 97.76 94.15 95.19 97.12 99.88 94.04 94.79 96. 99.25 57.16 58.08 59.98 61.83 58.47 59.88 62.08 64.21 65.65 66.01 67. 73.03 Model Semantic Fidelity Object Multi-Obj Action Color Spatial Scene Appearance Temporal Overall UniVid (base) UniVid (w/o MLLM) UniVid (w/o TMA) UniVid (Full) 89.53 90.80 91.51 94.52 73.32 74.37 75.42 77. 89.41 90.12 91.53 94.20 87.86 87.99 89.33 92.10 76.13 76.63 77.58 80. 42.32 43.32 44.61 46.66 19.03 20.57 21.03 23.57 21.60 22.26 23.62 25. 22.48 22.98 24.13 27.60 We define soft retrieval policy over the pool : exp (cid:0)sim(i, s)/τ (cid:1) jP exp (cid:0)sim(j, s)/τ (cid:1) . Sampling sequentially without replacement with joint probability (cid:81)K chronology yields Ws. π(i s) = (cid:80) (7) ℓ=1 π(iℓ s, i<ℓ) and respecting In the expand mode, at target size Kt we add the top unseen frames by similarity (no duplicates): = arg max iP Ssel sim(i, st1), Ssel Ssel t, = Kt Ssel. (8) In the shrink mode, with current Ssel and target Kt {32, 16}, we apply Maximal Marginal Relevance: (cid:88) (cid:104) λ sim(i, st1) (1 λ) max jS{i} sim(i, j) (cid:105) . (9) Ssel = arg max SSsel, S=Kt iS We adopt verbal policyimprovement view (Shinn et al., 2023). Let the objective be the expected Evaluator value under the retrieval policy: with J(s) = Ei1:K π(s)[V (Ws)] , (Ws) = E(cid:2)R (cid:12) (cid:12) Ws, Cg (cid:3) . Using the likelihoodratio identity with baseline yields (cid:34) sJ(s) = (cid:16) (cid:88) t=1 log π(it s, i<t) (cid:17) (cid:35) (R b) . single ascent step motivates verbal update to the search text: sr+1 = sr + η (cid:16) (cid:88) t=1 log π(it sr, i<t) (cid:17) (Rr b), (10) (11) (12) (13) where we use the softmax score function with gi(s) := ssim(i, s) and g(s) := Ejπ(s)gj(s): log π(i s) = τ 1(cid:0)gi(s) g(s)(cid:1), so the edit in aligns with frames that explain higher return through the text encoder ϕ(). Practically, the reflector inserts temporally and semantically discriminative cues (entities, colors, viewpoints, before/after, first/last, motion phase), which increases sim(i, s) for diagnostic frames and decreases it for distractors, implementing Eq. 13 in language space without parameter updates. To connect the update with both expand and shrink, we use piecewise-smooth set surrogate that trades relevance against redundancy (subgradients at ties): (Ws) = 1 (cid:88) iWs sim(i, s) γ max i=jWs sim(i, j). (14) 22 UniVid: The Open-Source Unified Video Model Figure 8: Ablation Study on Temperature Modality Alignment. Table 4: Ablation study of UniVid on four video QA benchmarks. Acc. denotes accuracy (%), Score denotes average rating (05). w/o means without. Best results are bold. Methods MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA Acc Score Acc Score Acc Score Acc Score UniVid (Base) UniVid (w/o finetune) UniVid (w/o Reflection) UniVid (Full) 64.1 71.1 73.1 80.1 3.3 3.9 4.0 4.2 48.9 52.2 55.0 61.4 2.8 3.0 3.1 3.4 54.2 63.5 64.6 75.0 3.0 3.6 3.6 4. 39.8 46.5 52.0 58.8 3.0 3.2 3.4 3.6 Since sim(i, s)/s points toward vi via ϕ(s), the gradient V (Ws) is aligned with the direction in Eq. 12. If the reflectors edit correlates with the advantage Ar = Rr b, then for sufficiently small step size η the expected first-order improvement satisfies E[J(sr+1) J(sr)] η (cid:34)(cid:42) (cid:88) s log π(it sr, i<t), sr+1 sr Ar 0. (15) (cid:43) (cid:35) Early stopping is triggered when the Evaluator score exceeds fixed threshold: stop at round if Rr τ, τ = 0.7. (16) With cached features, each round requires only similarity and diversity scoring together with reasoning over compact, temporally ordered Wr, which concentrates the Actor on temporal relations under tight token budget and improves video understanding with low computational cost. A.6 ABLATION STUDY Ablation on video generation. Tab. 3 quantifies the effect of removing key components of UniVid. Excluding multi-level language modeling (w/o MLLM) reduces semantic fidelity, especially in spatial and appearance consistency, highlighting the importance of multi-scale text grounding. Disabling Temperature Modality Alignment (w/o TMA) further degrades motion smoothness and temporal coherence. Fig. 8 visualizes these issues: without TMA, generated players exhibit unnatural fingers, distorted poses, and implausible ball trajectories, whereas the full UniVid produces coherent jump shots with realistic ball arcs. Qualitative comparisons in Fig. 3 confirm that UniVid consistently avoids missing objects and deformations that plague prior models, achieving both semantic plausibility and temporal stability. Ablation on video understanding. Tab. 4 compares four variants: lightweight base model without our training or reasoning additions, version w/o finetune that removes Stage-II video-QA finetuning, version w/o Reflection that keeps finetuning but disables the Pyramid Reflection loop, and the Full UniVid. Finetuning the understanding branch on ActivityNet-QA style instruction data already yields clear gains over the base, indicating that modest, task-aligned supervision substantially improves cross-modal grounding. Adding Pyramid Reflection further boosts accuracy, with similar trends in the QA scores, confirming that query-driven keyframe selection plus the ActorEvaluatorReflector loop improves temporal coherence and evidence retrieval. Overall, the full 23 UniVid: The Open-Source Unified Video Model system combines data-efficient tuning with iterative reasoning to deliver competitive results across all four benchmarks. A.7 LIMITATION AND FUTURE WORK Our current design couples an AR MLLM with DiT-based video decoder, which leverages both strengths but increases compute and memory, leading to slower generation. The understanding pipeline is keyframe-driven rather than natively video-aware, so it works well on short clips but struggles with long videos and fine motion. In future work, we will reduce coupling cost and integrate native video encoders such as Video ViT and 3D VAE to handle much longer videos, albeit at the expense of substantial training."
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Australian Artificial Intelligence Institute",
        "Peking University"
    ]
}