{
    "paper_title": "Forecasting Open-Weight AI Model Growth on Hugging Face",
    "authors": [
        "Kushal Raj Bhandari",
        "Pin-Yu Chen",
        "Jianxi Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the open-weight AI landscape continues to proliferate-with model development, significant investment, and user interest-it becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics in scientific literature, we propose a framework to quantify how an open-weight model's influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parameters-immediacy, longevity, and relative fitness-to track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 8 9 5 1 . 2 0 5 2 : r FORECASTING OPEN-WEIGHT AI MODEL GROWTH ON HUGGING FACE Kushal Raj Bhandari Department of Computer Science Network Science and Technology Center Rensselaer Polytechnic Institute Troy, NY, USA bhandk@rpi.edu Pin-Yu Chen IBM Research Yorktown Heights, NY, USA pin-yu.chen@ibm.com Jianxi Gao Department of Computer Science Network Science and Technology Center Rensselaer Polytechnic Institute Troy, NY, USA gaoj8@rpi.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "As the open-weight AI landscape continues to proliferatewith model development, significant investment, and user interestit becomes increasingly important to predict which models will ultimately drive innovation and shape AI ecosystems. Building on parallels with citation dynamics[1] in scientific literature, we propose framework to quantify how an open-weight models influence evolves. Specifically, we adapt the model introduced by Wang et al. for scientific citations, using three key parametersimmediacy, longevity, and relative fitnessto track the cumulative number of fine-tuned models of an open-weight model. Our findings reveal that this citation-style approach can effectively capture the diverse trajectories of open-weight model adoption, with most models fitting well and outliers indicating unique patterns or abrupt jumps in usage. Link to the website for trajectory visualization:"
        },
        {
            "title": "Introduction",
            "content": "The rapid expansion of the open-weight AI ecosystem has led to diverse landscape of models, each varying in size, company affiliation, and adoption patterns, raising critical questions about their long-term influence and impact [2]. Understanding how influential model will become is crucial for AI governance, business strategy, and scientific progress [3, 4, 5, 6]. Recent work underscores the growing impact of open-weight foundation models, highlighting both their benefits and challenges. Henderson et al. [7] examine fair use uncertainties when training on copyrighted material, while Chan et al. [8] address both the perils of fine-tuning accessible models and the importance of governance mechanisms, [9] in their discussion of agent identifiers and real-time monitoring. Eiras as further emphasized by Chan et al. [10] evaluates risks and opportunities across various development stages, advocating open-sharing practices alongside mitigation strategies. Collectively, these works illustrate the need for thoughtful consideration of open-source AIs evolving role in research, industry, security, and governance. closer look at fine-tuning patterns (figure 1) highlights the diversity of open-weight model adoption. Some base models experience rapid adoption, while others grow steadily. Understanding these dynamics is key to anticipating model prominence. Motivated by these insights, can we predict the trajectory of influence an open-weight model 1https://forecasthuggingfacemodels.onrender.com/ Forecasting Open-Weight AI Model Growth on Hugging Face Figure 1: Monthly number of fine-tuned models after base models release, with colors denoting the time when it was created. will have on the AI community? This inquiry drives our exploration into utilizing early adoption trendsparticularly the observed growth rateswhich can reliably forecast long-term impact, ultimately informing both strategic decisions and governance in the AI domain."
        },
        {
            "title": "2 Framework for Analysis",
            "content": "Building on this quantitative perspective, parallels emerge with the citation dynamics observed in scientific research[1]. We utilize the dynamics of the citation model proposed by Wang et al., hypothesizing that similar approach can effectively capture open-weight model adoption due to analogous patterns observed in usage. Specifically, we propose framework defined by three key parametersimmediacy (µi), longevity (σi), and relative fitness (λi)along with t, the time duration after model is released, m, the average number of fine-tuned models, and Φ, the cumulative normal distribution function. The adoption dynamics of an open-weight model are governed by and, ct = (cid:18) eλiΦ(cid:0) ln tµi σi (cid:1) (cid:19) 1 , Φ(x) = 1 2π (cid:90) (cid:18) exp (cid:19) t2 2 dt, (1) (2) where immediacy (µi) governs the time required for an open-weight model to reach its peak adoption, longevity (σi) captures the decay rate of the models influence, and relative fitness (λi) represents the models inherent relative influence compared to other models in the ecosystem. This dynamics effectively captures how attention and usage evolve in scenarios where an innovationa scientific publication or an open-weight modelis introduced and disseminated across community. Similar to how citation grows rapidly once paper is published and then gradually decays over time, open-weight models also tend to experience an initial burst of interest followed by sustained but diminishing engagement. By incorporating parameters like immediacy, longevity, and relative fitness, the model reflects both the short-term surge in popularity and the long-term retention trajectory. Consequently, this aligns well with observed patterns in open-source development communities, making it suitable framework for understanding and predicting model usage. Figure 4, similar to figure 1, illustrates the actual cumulative number of fine-tuned models after the base model is open source through HuggingFace. Following similar approach to modeling the cumulative number of fine-tuned models, we also examine the cumulative download trajectories for each model. Using the framework, in Appendix Section E, we empirically predict the cumulative number of downloads for the widely popular DeepSeek models. Given 20 days of downloads for deepseek-ai/DeepSeek-R1, we expect it to reach 1.3 billion cumulative downloads within 75 days. 2 Forecasting Open-Weight AI Model Growth on Hugging Face 2.1 Fitting the model to Empirical Data Using the HuggingFace ecosystem2, we fit equation 1 to track the cumulative number of fine-tuned models built from base model i, measured months after its release as an open-weight model on HuggingFace (Detailed in Appendix A). Figure 2: (a) Distribution of values for λ, µ, and σ. (b) Pairwise relationships among immediacy (µi), longevity (σi), and relative fitness (λi) on log-scale axes. From figure 2a, we can see that most models cluster around relatively narrow bands of λ, µ, and σ, yet there are notable outliers where the parameter estimates are extremely large or small. In the λi histogram, many models have values concentrated near the lower end (close to 1 or below), but several parameter fits extend out to 105 or 107. Similarly, µi is heavily concentrated in narrow region around 104, with few models at exceedingly small values (e.g., 1014 or 1044), while σi exhibits main peak around 0.1 and occasional jumps to values near 102 or 108. This pattern indicates that the citation-style adoption curve fits reasonably well for most models, yielding parameter values in plausible ranges that reflect gradual growth, moderate time shifts, and smooth transition to saturation. However, for minority of models, the curve either overcompensates or cannot accurately capture the sudden jumps or unusual trajectories, forcing the optimizer to return extreme parameter estimates. These outliers underscore that while the model works broadly well, certain edge cases or atypical usage patterns may require additional constraints or alternative modeling assumptions to avoid unrealistic parameter values. 2.2 Dependency of parameters While each parameter provides distinct lens into model adoptionµi dictating time to peak, σi controlling longevity, and λi determining overall attractivenesstheir relationships reveal more nuanced dynamics. For instance, models with high λi but low σi experience rapid adoption but short-lived influence, suggesting strong initial appeal but limited long-term utility. Conversely, models with moderate λi and high σi exhibit sustained adoption, indicating persistent engagement over time despite lacking an immediate surge in popularity. These dependencies illustrate how different models follow varied life cycles, from fleeting trends to enduring contributions, emphasizing the importance of modeling adoption beyond single-parameter distributions. Figure 2b shows the pairwise relationships among the three parametersimmediacy µi, longevity σi, and relative fitness λiplotted on log-scale axes. In the left panel (λi vs. µi), we see that some models combine very high relative fitness with smaller values of µi, indicating both rapid rise to peak adoption and strong overall influence. Others stretch to larger µi, implying they take longer to reach peak usage even if they remain moderately or highly appealing. In the middle panel (λi vs. σi), points with high λi but small σi suggest that although the model attracts many users, its influence weakens more quickly, whereas higher σi corresponds to more prolonged tail of adoption. The right 2https://huggingface.co/ 3 Forecasting Open-Weight AI Model Growth on Hugging Face panel (σi vs. µi) highlights that even models with similar times to peak can exhibit widely different decay ratessome fade rapidly while others remain in use for much longer. The broad range of parameter values across multiple orders of magnitude demonstrates the diversity of open-source adoption trajectories and highlights the frameworks flexibility in capturing such heterogeneous dynamics."
        },
        {
            "title": "3 Organization-Specific analysis on model’s importance relative to other models",
            "content": "The diversity of parameter distributions in figure 2(a) underscores the varied pathways through which open-weight models gain and sustain adoption. While some models achieve rapid prominence with enduring influence, others experience slower ascent or more transient impact. These heterogeneous adoption dynamics suggest that intrinsic qualities do not solely dictate model success but are also shaped by external factors, including company strategies and ecosystem positioning. Figure 3: Density plots illustrating the cumulative number of fine-tuned models for relative fitness of (1 λi 10) at the 2-month, 6-month, and 12-month marks, segmented by companies. By examining models with moderate relative fitness (1 λi 10), figure 3 provides insights into the temporal shifts in the frequency of fine-tuned models, revealing how different organizations models evolve in their attractiveness for general adaptation over time. At the early stage (2 months), distinct peaks in ct emerge, suggesting that models in the fitness range receive some degree of initial interest for some organizations and some have some more outstanding interests. Over time, the KDE curves for most companies begin to align in broader but partially overlapping bands, indicating that short-term disparities in how quickly each base model is fine-tuned start to level out. Meta, BAAI, and Google exhibit the highest peaks at low ct, indicating that most base models are fine-tuned quickly. At the same time, companies like StabilityAI and Qwen display broader distributions, suggesting variability in adoption, and Microsoft, Amazon, and Apple have minimal early fine-tuning activity. The gradual consolidation, where peaks shift toward higher ct values for some models and compress for others, reflects both sustained adoption and potential latecomer effects. This behavior is consistent with the idea that models with λi in the relatively higher range dominate immediately; rather, they accumulate steady stream of finetuned variants as the model gets widely adopted. At 6 months, Meta, BAAI, and StabilityAI still show strong peaks at lower ct, but Allen AI and Qwen exhibit sharper peaks at higher ct, suggesting that some of their base models gain significant traction. By 12 months, the distributions flatten, indicating more widespread fine-tuning across different base models, with companies like Meta, StabilityAI, and Qwen showing extended tails, signifying that some models continue accumulating fine-tuned variants over time. These density curves reinforce the notion that models with high relative fitness levels follow the number of finetuning trajectories that differ initially but increasingly resemble one another over longer horizons. This aligns with expectations from the citation model [1], where cumulative citations of papers with early head starts or slower uptake subside, comparable fitness values dominate the eventual adoption outcome, yielding nearly the same ultimate impact ct. Individual trajectories of models based on these companies are highlighted within Appendix D. 4 Forecasting Open-Weight AI Model Growth on Hugging Face"
        },
        {
            "title": "4 Conclusion",
            "content": "In this study, we adapted well-known citation model to examine the adoption dynamics for open-source models. Through this analogy, we introduced three parametersimmediacy, longevity, and relative fitnessto characterize early and sustained usage patterns. Our empirical analysis of monthly fine-tuning counts reveals that most models exhibit predictable trajectories, whereas minority of AI models show sudden, significant surges in popularity, challenging the assumption of simple exponential decay. We also found that organization-level factors significantly shape these usage patterns: models released by Meta, Google, BAAI, and StabilityAI display distinct adoption curves reflective of each companys open-source strategies and ecosystem support. Our citation-inspired framework helps stakeholdersfrom industry leaders optimizing release strategies to policymakers overseeing AI governancebetter understand how open-weight models evolve and gain traction. Future research can extend this approach by integrating additional data to refine long-term adoption forecasts and further clarify the influences driving model success."
        },
        {
            "title": "References",
            "content": "[1] Dashun Wang, Chaoming Song, and Albert-Laszlo Barabasi. Quantifying Long-Term Scientific Impact. Science, 342(6154):127132, October 2013. [2] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram`er, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the Opportunities and Risks of Foundation Models, July 2022. [3] Carlos J. Costa, Manuela Aparicio, Sofia Aparicio, and Joao Tiago Aparicio. The Democratization of Artificial Intelligence: Theoretical Framework. Applied Sciences, 14(18):8236, September 2024. [4] Dieuwertje Luitse and Wiebke Denkena. The great Transformer: Examining the role of large language models in the political economy of AI. Big Data & Society, September 2021. [5] EU. Artificial Intelligence Act, 2024. [6] Nestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, and Jack Clark. Artificial Intelligence Index Report 2024, 2024. [7] Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang. Foundation Models and Fair Use, March 2023. [8] Alan Chan, Ben Bucknall, Herbie Bradley, and David Krueger. Hazards from Increasingly Accessible FineTuning of Downloadable Foundation Models, December 2023. [9] Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, Lennart Heim, and Markus Anderljung. Visibility into AI Agents. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, pages 958973, New York, NY, USA, June 2024. Association for Computing Machinery. [10] Francisco Eiras, Aleksandar Petrov, Bertie Vidgen, Christian Schroeder, Fabio Pizzati, Katherine Elkins, Supratik Mukhopadhyay, Adel Bibi, Aaron Purewal, Csaba Botos, Fabro Steibel, Fazel Keshtkar, Fazl Barez, Genevieve Smith, Gianluca Guadagni, Jon Chun, Jordi Cabot, Joseph Imperial, Juan Arturo Nolazco, Lori Landay, Matthew Jackson, Phillip H. S. Torr, Trevor Darrell, Yong Lee, and Jakob Foerster. Risks and Opportunities of OpenSource Generative AI, 2024. 5 Forecasting Open-Weight AI Model Growth on Hugging Face"
        },
        {
            "title": "A Data Collection",
            "content": "We collect data on open-weight model adoption using the Hugging Face API, which provides comprehensive metadata on models uploaded to the platform. Given that HuggingFace3 serves as the primary repository for open-source AI models, we assume that the vast majority of publicly available models are hosted there . To quantify fine-tuning activity, we track the number of fine-tuned models uploaded to Hugging Face after the release of given base model, aggregating counts by month. For example, since LLaMA 2 was released on July 18, 2023, we begin counting fine-tuned models from that point onward. However, we exclude the earliest models such as GPT-2 and BERT variants, as these were only uploaded to Hugging Face on March 2, 2022, despite being released much earlier, which could distort the adoption timeline. Fine-tuned models are identified based on their tags and model names, but due to inconsistencies in labeling, it is possible that some fine-tuned models are not captured in our dataset. Additionally, Hugging Face provides only the total number of downloads for model without historical breakdowns. To address this, we began recording daily total downloads for each model starting in September 2024, allowing us to approximate temporal trends in adoption. Figure 4: Monthly cumulative number of fine-tuned models following the release of the base model, with colors indicating the base models creation years, illustrating trends in fine-tuning patterns over time."
        },
        {
            "title": "B Parameter m and t",
            "content": "In our adaptation of the citation model, we set = 1, meaning that the model predicts the cumulative number of finetuned models directly without scaling by an arbitrary reference count. This differs from the original citation model, where Wang et al. [1] set = 30 to account for the typical number of references in new paper. They noted that fixing for all papers allows for easy parameter comparison and that increasing results in smaller λ but does not affect the overall fitting or prediction when is comparable to the average number of citations per paper. In our case, because fine-tuning does not have well-defined equivalent to the number of references in paper, we normalize to 1, making the adoption curve directly interpretable in terms of the absolute count of fine-tuned models. This ensures that λi captures the relative fitness of each base model without needing an external scaling factor. Additionally, we count in months rather than the year used in the original citation model. This adjustment reflects the timescale of model adoption, where fine-tuned models typically emerge over weeks and months rather than daily fluctuations. By aggregating over months, we reduce noise and better capture long-term adoption trends, aligning with the natural timescale at which open-weight models gain traction within the community. 3https://huggingface.co/ 6 Forecasting Open-Weight AI Model Growth on Hugging Face Fitting Equation 1 on empirical data Figure 5: Each subplot represents models, where the x-axis denotes the time, t(month), after release, and the y-axis represents the cumulative count (ct i) on logarithmic scale. Red dots indicate empirical data points, while blue curves correspond to the fitted function using the extracted parameters (λi, µi, σi). The figure illustrates the cumulative adoption trends of various AI models over time, with each subplot representing specific model. The x-axis denotes time, while the y-axis shows the cumulative count on logarithmic scale. Red dots indicate empirical data points, and the blue curves correspond to the fitted functions using the extracted parameters: λi (growth rate), µi (shift parameter), and σi (scaling factor). The overall fit demonstrates that the selected functional form effectively captures cumulative growth patterns for most models. The table 1 includes the extracted parameters for the models listed in figure 5. The parameters reveal considerable variation in the cumulative number of fine-tuned model trajectories. Models such as openai/whisper-large-v3, BAAI/EVA, and microsoft/Phi-3-mini-4k-instruct exhibit high λi values, signifying rapid cumulative adoption. These models also have relatively high µi values, indicating an early adoption surge, likely due to strong initial interest and high demand. Notably, for some models like meta-llama/Llama-3.1-8B and microsoft/Phi-3.5-mini-instruct, highlighted by * in table 1, display lower λi and σi values, reflecting more gradual accumulation of usage over time. The parameter estimation resulted in λi = 0.5, µi = 2.0, and σi = 0.5. These values suggest that the equation used for fitting failed to accurately capture the underlying growth dynamics for those models, leading to an ineffective approximation of their cumulative adoption curves. This discrepancy is due to irregular growth trends, as seen in the figure 5. This suggests that the framework 1 still requires some more comprehensive adaptation since it fails to capture such growth trend. The visualization highlights diverse adoption patterns among AI models, with some experiencing rapid early adoption and subsequent saturation, while others follow steady and prolonged growth trajectory. Deviations from the fitted curves in certain cases suggest additional influencing factors, such as accessibility, licensing constraints, or specific application domains. These findings offer valuable insights into AI model adoption, providing quantitative framework for assessing their long-term impact and diffusion."
        },
        {
            "title": "D Cumulative Trajectory of Finetuned Models By Organization",
            "content": "Understanding the cumulative fine-tuning trajectory of models across different companies provides valuable insights into the adoption dynamics and impact of open-source AI models. Fine-tuning is critical mechanism through which base models are adapted to diverse applications, reflecting their versatility and the strategic priorities of the organizations that develop them. By examining the temporal evolution of fine-tuned models (ct) for each company, we can 7 Forecasting Open-Weight AI Model Growth on Hugging Face Model Name λi µi σi Qwen/Qwen1.5-0.5B Qwen/Qwen1.5-1.8B google/gemma-2b google/gemma-7b Qwen/Qwen1.5-7B openai/whisper-small meta-llama/Llama-2-7b stabilityai/stable-diffusion-xl-base-1.0 BAAI/EVA mistralai/Mistral-7B-Instruct-v0.2 meta-llama/Llama-2-7b-hf mistralai/Mistral-7B-v0.1 meta-llama/Llama-2-7b-chat-hf meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.1-8B allenai/DREAM meta-llama/Meta-Llama-3-8B-Instruct openai/whisper-tiny microsoft/phi-2 openai/whisper-large-v3 openai/whisper-medium Qwen/Qwen2-1.5B meta-llama/Meta-Llama-3-8B meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.2-1B-Instruct microsoft/Phi-3-mini-4k-instruct microsoft/speecht5 tts openai/whisper-large-v2 meta-llama/Llama-3.2-1B Qwen/Qwen2-1.5B-Instruct apple/AIM Qwen/Qwen2-0.5B Qwen/Qwen2-7B-Instruct openai/whisper-base google/gemma-2-2b meta-llama/Llama-3.2-3B mistralai/Mistral-7B-Instruct-v0.1 google/gemma-2-2b-it facebook/opt-125m Salesforce/BLIP mistralai/Mistral-7B-Instruct-v0.3 microsoft/resnet-50 facebook/esm2 t12 35M UR50D google/flan-t5-base google/flan-t5-large openai/whisper-large microsoft/Phi-3.5-mini-instruct microsoft/phi-1.5 google/gemma-2-9b-it Qwen/Qwen2.5-7B-Instruct 21.2340 21.1198 20.7799 18.9374 18.0948 294604.7393 17.2144 16.9046 454253.6120 16.1882 15.3191 15.9177 15.2853 0.5* 0.5* 24.2332 15.9664 13.4653 15.2437 528070.6635 460695.9213 16.0543 15.2420 0.5* 0.5* 114364.7070 12.3327 68.7205 0.5* 15.1078 120131.6996 32058.6364 415361.3050 11.2185 0.5* 0.5* 13.4460 0.5* 9.2155 11.6421 14.0439 9.0884 11.4140 10.3708 11.8440 364711.2741 0.5* 12.9090 280939.5667 0.5* 1.18e-15 1.00e-15 2.56e-14 9.78e-15 1.41e-19 90.9031 1.04e-17 5.80e-11 95.8721 7.18e-15 1.76e-14 1.03e-15 9.88e-12 2.0* 2.0* 4.9102 1.47e-10 2.04e-15 8.83e-18 66.4680 88.9759 4.44e-12 1.06e-10 2.0* 2.0* 142.1125 6.40e-10 13.4940 2.0* 1.70e-17 66.9603 76.6518 78.3713 6.13e-20 2.0* 2.0* 7.33e-15 2.0* 1.68e-14 0.2335 3.31e-09 4.48e-21 6.74e-19 1.28e-19 8.27e-14 64.2591 2.0* 6.94e-10 102.4015 2.0* 3.9044 3.8795 4.8182 4.5854 4.6136 22.4477 8.8424 7.8304 23.0329 7.7386 4.9636 8.2057 5.5452 0.5* 0.5* 9.2243 10.6965 4.1449 9.5035 15.8209 21.2067 6.1988 11.5625 0.5* 0.5* 37.0978 3.5563 10.0765 0.5* 4.9109 17.3784 21.8903 18.9740 2.7420 0.5* 0.5* 8.2182 0.5* 1.4702 2.7321 7.2751 1.6266 3.5063 1.9899 4.6042 15.3622 0.5* 9.6670 25.2924 0.5* Table 1: Summary of model parameters (λi, µi, σi) for different top 50 models with the largest number of fine-tuned models. Here, * indicates the framework equation 1 failed to fit the empirical data. identify influence patterns, competitive positioning, and the extent to which specific base models drive downstream innovations. The following analysis presents comparative view of fine-tuning trends across companies, highlighting variations in growth rates, early adoption, and long-term sustainability in the AI ecosystem."
        },
        {
            "title": "E Analyzing the Cumulative Number of Downloads",
            "content": "In addition to looking at number of fine-tuned models, we also briefly study the trajectory of the number of downloads for different models. As mentioned in Section A, we only have download data of models after September 2024. Figure 9 shows that the framework utilized for downloads does not quite fit the framework well since the trajectories differ 8 Forecasting Open-Weight AI Model Growth on Hugging Face Figure 6: The cumulative number of fine-tuned models (ct) over time (months) for Allen AI, Amazon, Apple, Beijing Academy of Artificial Intelligence(BAAI), CohereAI and DeepSeek. Figure 7: The cumulative number of fine-tuned models (ct) over time (months) for Meta, Google, HuggingFace, IBM, Microsoft, and MistralAI. for different open-weight models. However, this is due to the framework being fitted on empirical data with an interval of data after the models release; hence, it is fitted with incomplete data since the initial release day. Figure 10 illustrates the prediction made for the cumulative downloads for the recently popular DeepSeek models. Colored markers in each subplot represent the observed download counts at different points in time, and the black curve superimposed on each set of points illustrates the models predicted growth trajectory. The vertical scale (cumulative downloads) spans several orders of magnitude, reflecting substantial variation in popularity among the model variants. Overall, the close alignment between the observed data and the predicted curves suggests that the modeling approach described in Appendix Section2 provides reasonable fit for each DeepSeek variants download pattern. 9 Forecasting Open-Weight AI Model Growth on Hugging Face Figure 8: The cumulative number of fine-tuned models (ct) over time (months) for Nvidia, OpenAI, Qwen, Salesforce, and StabilityAI. Figure 9: The line plot of the cumulative number of downloads over time (day) for individual models ordered based on the most cumulative downloads. The blue plot is the predictive trajectory using the citation model. From the figure, each models cumulative downloads follow the hypothesized saturating pattern over time, as depicted by the black curves. Early in the launch phase, downloads increase sharply, indicating rapid adoption. After several weeks, the models trajectory starts to flatten, showing decrease in the download growth rate. We posit that as more advanced models become available, each DeepSeek variants download growth naturally slows off. Early adopters generate an initial spike, but user interest in earlier releases diminishes once new competitors emergepotentially offering higher accuracy, additional features, or improved efficiency. This leads to the observed plateau in download rates, as existing users have already adopted the model, and new users may opt for more recent, better-performing alternatives. 10 Forecasting Open-Weight AI Model Growth on Hugging Face Figure 10: Predicting number of downloads of recently popular DeepSeek models. The black line plot predicts the cumulative number of downloads of DeepSeek models up to 75 days after its release."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Network Science and Technology Center, Rensselaer Polytechnic Institute, Troy, NY, USA",
        "IBM Research, Yorktown Heights, NY, USA"
    ]
}