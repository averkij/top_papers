{
    "paper_title": "End-to-End Video Character Replacement without Structural Guidance",
    "authors": [
        "Zhengbo Xu",
        "Jie Ma",
        "Ziheng Wang",
        "Zhan Peng",
        "Jun Liang",
        "Jing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 7 8 5 8 0 . 1 0 6 2 : r MoCha: End-to-End Video Character Replacement without Structural Guidance Zhengbo Xu, Jie Ma, Ziheng Wang, Zhan Peng, Jun Liang, Jing Li"
        },
        {
            "title": "AliBaBa Group",
            "content": "Project Page: orange-3dv-team.github.io/MoCha Figure 1. Examples generated by MoCha. MoCha enables high-fidelity character replacement in source videos based on provided reference images for diverse subjects, including virtual (first row) and real-human (second row) characters. Furthermore, our approach robustly preserves original lighting conditions (third row) and effectively handles multi-character occlusion and interaction (fourth row)."
        },
        {
            "title": "Abstract",
            "content": "Controllable video character replacement with userprovided identity remains challenging problem due to the lack of paired video data. Prior works have predominantly relied on reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, pioneering framework that bypasses these limitations by requiring only single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce conditionaware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified pairedtraining data, we propose comprehensive data construcSpecifically, we design three specialized tion pipeline. 1 datasets: high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dvteam.github.io/MoCha 1. Introduction Recent breakthroughs in generative technologies, particularly diffusion models [2, 17, 18, 28], have propelled content creation into new era. Consequently, user demand for fine-grained and highly personalized editing of images and videos surges rapidly. Video character replacement [4], defined as the task of seamlessly substituting the character in video while precisely preserving the original background, scene dynamics, and character motion, demonstrates substantial practical value and growing commercial significance. Its applicability spans diverse domains, including costly post-production in film and television, personalized advertising, virtual try-on, and the creation of dynamic digital avatars. Despite its significance, video character replacement still faces significant challenges. Current research [4, 11, 13] has predominantly adopted reconstruction-based paradigm, as shown in Fig. 2. These methods first require dense, perframe segmentation mask to annotate the character spatial location and ruin its ID information. Then they utilize the structural guidance, such as pose skeletons [31] or depth maps [32] extracted from the source video, along with reference image of the new character, to re-render the video. While this paradigm produce impressive results in simple scenarios, it struggles in complex onessuch as videos with occlusion, unusual poses (e.g., acrobatics), and multicharacter interactions involving physical contact. In these cases, the multi-frame masks and structural information are prone to inaccuracies. Consequently, inaccuracies in these explicit guidance signals are further propagated and amplified during the generation process, resulting in severe visual artifacts, motion discontinuities, and temporal inconsistency in the rendered videos. Furthermore, the reliance on dense explicit guidance not only limits the flexibility and robustness of video character replacement algorithms but also incurs substantial computational overhead. Recent advances have shown that video diffusion models inherently perform temporal perception and implicit reasoning abilities [30]. Harnessing these capabilities, specifically video tracking, we challenge the reliance on perframe mask for this task. In this paper, we propose MoCha, the first end-to-end framework for video character replaceFigure 2. (a) Reconstruction-based paradigm used in baseline methods. (b) Non-reconstruction-based paradigm used in MoCha. ment that requires only single frame mask from an arbitrary video frame without any structural guidance. MoCha operates by decoupling the source characters motion and facial expressions from the background scene, and subsequently transferring these dynamics onto new reference identity through efficient in-context learning by integrating the video content, frame mask, and reference character identity. To coherently fuse these multi-modal inputs, we propose condition-aware RoPE, an extension of the original 3D RoPE [26, 28] mechanism. Furthermore, to better preserve the characters identity, we introduce posttraining stage guided by differentiable facial reward function [5]. Benefiting from these designs, MoCha demonstrates strong potential in achieving accurate and temporally consistent video character replacement. Training MoCha requires high-quality paired dataset consisting of source videos and corresponding target videos, in which the character is replaced while preserving the character motion and scene dynamics. To this end, we introduce comprehensive data construction pipeline that aggregates data from three distinct sources, as shown in Fig. 4. First, we use Unreal Engine 5 (UE5) to render paired character video, ensuring that different characters perform identical actions within the same scene. Second, we generate animated portrait videos by first replacing the person in source image using the Flux inpainting model [18], and then animating both the original and inpainted images using current portrait animation methods [7] with shared driving facial expression sequence. Third, to further enrich data diversity, we incorporate publicly available videomask datasets [10]. By training on this composite dataset, MoCha can perform video character replacement in an endto-end manner, without requiring auxiliary inputs like perframe masks or structural guidance. Our contributions can be summarized as follows: Figure 3. Overview of MoCha. Training MoCha consists of two stages: (a) In-Context Learning. We apply the condition-aware RoPE to the concatenated tokens and train the DiT backbone. (b) Post-Training. We employ an RL-based strategy to further enhance the facial consistency. We introduce high-quality and large-scale paired dataset for video character replacement. We propose MoCha, the first end-to-end framework for video character replacement that requires only single frame mask from an arbitrary source video frame without any structural guidance. MoCha also demonstrates the tracking ability of the video diffusion model. Extensive experiments demonstrate that MoCha significantly outperforms current state-of-the-art methods in terms of identity preservation, temporal consistency, and facial expression fidelity. 2. Related Works In-Context Learning in Video Generation. Recent breakthroughs in video generation have ushered in new era for content creation. State-of-the-art video diffusion models [17, 28] are now capable of producing high-fidelity, long-duration videos. Building on these foundation models, various works explore in-context learning for different downstream tasks. This approach typically involves guiding the generation process by concatenating conditioning inputs along the temporal dimension. For instance, Recammaster [1] frames camera-controlled video re-rendering as an in-context learning task, FullDiT [14] integrates multiple conditions for versatile video generation, and VFXMaster [19] applies this method to transfer visual effects from video to an image. Motivated by the demonstrated effectiveness of this paradigm, our work also adopts an in-context learning framework. ods showing significant progress. dominant paradigm among current methods is reconstruction-based synthesis, where specified character is generated within masked region of target video. HunyuanCustom [11] simply masks the target area and concatenates the reference image. VACE [13] incorporates structural guidance, such as depth or skeleton maps, into the DiT diffusion model to maintain character movement. Building upon this, Wan-Animate [4] introduces face encoder to extract fine-grained detail to maintain facial expressions. However, key limitation of these approaches is their reliance on dense, per-frame segmentation masks and structural priors. In contrast, our method, MoCha, achieves end-to-end character replacement via an implicit learning mechanism. It requires only single frame mask of the video, thus obviating the need for such explicit guidance and streamlining the synthesis process. 3. Method MoCha is designed for the video character replacement task, which requires source video Vs Rf chw, designation frame mask Ms R11hw to annotate the character and set of refernece character image {Ii R11hw}, as shown in Fig. 3. In this section, we first review the Flow matching framework that is commonly used in current video diffusion models in Sec.3.1. Then we introduce our in-context learning framework in Sec.3.2 and reward post-training in Sec.3.3. 3.1. Preliminary Video Character Replacement. Character replacement in video is key challenge in video editing, with recent methMoCha builds on the pretrained text-to-video latent diffusion model using the Rectified Flow framework [6]. The in3 put video V0 will first be compressed to latent z0 through video variational encoder [15] (VAE). Based on the clean latent z0, noisy latent zt is constructed by randomly sampled timestep and noise ϵ sampled from standard Gaussian Distribution (0, 1): zt = (1 t)z0 + tϵ (1) During training, zt is fed to Transformer-based diffusion model [22] (DiT). The model predicts the velocity vΘ(zt, t) at point zt, the gradient value satisfying dzt = vΘ(zt, t)dt. We use the Conditional Flow Matching [21] loss to optimize the model: LF = Ez0,t,ϵvΘ(zt, t) ut(z0ϵ) (2) where ut(z0ϵ) is the actual slope of the secant line through z0 and ϵ. For inference, we start from pure noise z1 and iteratively generate the output latent z0 through timestep list {ti}: zti+1 = zti + vΘ(zti, ti) (ti+1 ti) (3) where {ti} is decreasing list with t0 = 1 and tn = 0. z0 is finally decoded to the output video. 3.2. In-Context Learning To guide the generation of target video Vt based on source video Vs, designation mask for single frame, and set of reference images {Ii}, we employ an in-context learning manner due to its effectiveness demonstrated in various tasks [1, 8, 14, 19]. Concretely, we first encode these conditions with 3D VAE to achieve spatiotemporal compression. This yields the latents zt = E(Vt), zs = E(S), zm = E(M ), zIi = E(Ii). These latents are then patchified into visual tokens, namely xt Rbf chw, xs Rbf chw, xm Rb1chw, and xIi Rb1chw. Here b, , c, h, represent the batch size, frame count, channel number, height, and width, respectively. Next, we concatenate all conditional tokens with the target tokens along the frame dimension to form unified latent sequence: = [xt, xs, xm, xI1, xI2, ...] (4) where Rb(2f +1+j)chw and is the number of reference images. The latent sequence is subsequently fed into the DiT backbone and processed using full selfattention. However, naive application of 3D Rotary Positional Embeddings (RoPE) by assigning different temporal index to each condition would result in inflexible generation, such as being constrained to fixed output length. To address this, we propose condition-aware RoPE mechanism to coherently fuse these diverse inputs, as shown in Fig. 3. Concretely, we assign the same frame index, ranging from 0 to 1, to both the source video tokens xs and target video tokens xt, as they share frame-to-frame correspondence. For the reference image tokens Ii, fixed frame index of 1 is assigned, and different reference image tokens are distinguished using an offset in the height and width dimensions. key innovation of MoCha is that it supports flexible arbitrary frame mask selection, which is implemented by assigning variable frame index fM for the mask token xm based on the designation frame number , i.e. fM = (F 1)//4 + 1 (5) This in-context learning manner, equipped with our designed condition-aware RoPE, enables MoCha to effectively handle variable generation length, flexible multireference images, and arbitrary frame mask selection. 3.3. Identity-Enhancing Post-Training Recent works [20, 25] have explored diffusion post-training with reinforcement learning to align human preferences. Drawing inspiration from this, we employ similar RLbased post-training strategy to further enhance the facial consistency between the generated video and the reference images, as shown in Fig. 3 (b). The core idea is to guide the generation process through an explicit optimization objective that maximizes facial consistency between the generated video frames and ground-truth video frames. Specifically, we compute facial reward score Rf ace based on the cosine similarity between the Arcface [5] embeddings of the generated video and the reference images. To avoid reward hacking that the model simply pastes the reference image into the generated video, we also use pixel-wise Mean Squared Error (MSE) loss between the generated video and the GT video to provide dense supervision. The overall loss function is: LRL = (1 Rf ace) + Vt ˆVt (6) where ˆVt is the video generated by the model. Since finegrained details are primarily synthesized in the later timestamps of the sampling process [23], we only backpropagate the loss through the final sampling steps to reduce memory and accelerate optimization. 4. Dataset Training MoCha requires strictly aligned paired video, where different characters share the same motion, facial expressions and background dynamics. However, it is challenging to obtain such pairs in real-world settings. To address this critical limitation, we introduce comprehensive data construction pipeline that aggregates data from three distinct sources. This process is illustrated in Fig. 4. 4 Figure 4. Overview of the data construction pipeline. We propose three methods to construct the training dataset: (I) Rendered data built with Unreal Engine 5. (II) Expression-driven face animation data generated through portrait animation methods. (III) Augmented data synthesized from traditional video-mask pairs. 4.1. Rendered Data We develop scalable data generation pipeline built upon Unreal Engine 5 (UE5) to synthesize large-scale dataset of paired videos. The pipeline leverages an extensive library of virtual assets from the UE ecosystem, including 3D scenes, characters, motions, and facial expressions. Each video is rendered through the random composition of these assets. To enrich the dataset with diverse perspectives, we also implement procedural system to automatically generate natural camera trajectories. The core of our dataset lies in its paired structure. For each video, corresponding paired video is rendered by substituting the character while meticulously preserving all other parameters. Additionally, video character mask is also rendered to precisely annotate the replaced character location. Finally, for each character, set of reference images is rendered, capturing them in various poses and under different lighting conditions. 4.2. Expression-Driven Face Animation Data To enhance facial expression fidelity, we further construct an expression-driven paired dataset by the current portrait animation methods. We first collect large set of film images and use the Flux inpainting model [18] to substitute the foreground character. Then we use LivePortrait [7] to animate both images with the same facial-driven video. Simply using an image from the video as reference image may cause copy-paste problem, i.e., the model tends to directly paste the reference image to the generated video. Therefore, instead of using raw frame from the driving video as the reference, we augment its pose with Flux Kontext [2]. This creates novel reference that forces the model to decouple identity from facial animation. 4.3. Augmented Video-Mask Data Training MoCha solely on the two aforementioned synthetic datasets may lead to noticeable synthetic artifacts and lack of realism in the generated characters. To address this, we enrich our training set with real-world videos from two public datasets that provide video-mask pairs: VIVID10M [10] and VPData [3]. We apply YOLOv12 detector [27] to filter non-human videos. The references are augmented using the similar method mentioned in Sec 4.2. 5. Experiments 5.1. Experiment Settings Implementation Details. We use Wan-2.1-T2V-14B [28] as the base model, due to its excellent performance. The training process consists of two stages. In the in-context learning stage, we fine-tune all self-attention layers to handle multi-model input conditions. Training is conducted on 8 NVIDIA H20 GPUs for 30K steps. We use learning rate of 2 105, batch size of 8. In the post-training stage, to avoid rewards affecting the generation ability of the base model, we employ parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA) [9] instead of full model tuning. We apply LoRA with rank of 64 to all linear layers within the DiT architecture. The LoRA modules are then trained for 500 steps, using batch size of 8 and learning rate of 2 105. Our dataset consists of three distinct sources. We ran5 Figure 5. Comparison with state-of-the-art methods. The results show that MoCha can replace the character with more consistent animation, higher facial expressiveness, and more natural lighting effects. domly select 60K, 20K, 20K videos from rendered data, expression-driven animation data, and augmented videomask data, respectively, in total of 100K samples. All videos, masks, and reference images are resized to 480 832 resolution for training. Training Strategy. MoCha supports multiple reference images as input. During training, we always provide one base reference and, with 50% probability, include an additional face-centric reference. This stochastic referencedropout scheme improves robustness when reference cues are limited and enhances the models ability to exploit extra references to learn fine-grained facial details. In addition, we initially train the model using short video clips (e.g., 21 frames). Once the generation results stabilize, we increase the length of the input video sequences (e.g., 81 frames) in subsequent training stage. Benchmark. To facilitate comprehensive quantitative evaluation, we introduce new benchmark composed of two distinct subsets: synthetic benchmark and realworld benchmark. The synthetic benchmark is constructed by render engine, providing perfectly paired data. To ensure fair comparison, none of the scenes, characters, motions, or facial expressions in the benchmark appear in our training data. For evaluation on this benchmark, we employ widely-used quantitative metrics, including SSIM [29], LPIPS [33], and PSNR. The real-world benchmark consists of diverse set of videos collected to encompass challenging scenarios, such as multi-person interactions, rapid movements, and complex lighting conditions. For each video, we use SAM2 [24] to generate randomly selected frame mask. We also curate diverse collection of reference images, spanning various artistic styles (e.g., cartoon, photorealistic) and subject attributes (e.g., gender, body shape). To assess performance on this benchmark, we adopt the comprehensive evaluation suite from VBench [12] for thorough comparison. 5.2. Comparison on Character Replacement Baselines. We compare MoCha with the current stateof-the-art multimodal video editing methods including VACE [13], Kling [16], Wan-Animate [4] and HunyuanCustom [11]. Among them, Kling [16] is commercial product supporting online character replacement. Other methods are open-sourced and we use their official implementations for evaluation. Qualitative Results. We present several character replacement results generated by MoCha in Fig. 1. Our model demonstrates robust performance across diverse sub6 ject styles, handling both cartoon and real-human characters. Specifically, MoCha is capable of reproducing vivid facial expressions and transferring complex animation onto the reference characters. We further compare MoCha with state-of-the-art methods in Fig. 5. While all these methods can generate relatively high-quality video based on reference character, they exhibit distinct limitations. Specifically, Kling [16] and HunyuanCustom [11] successfully preserve the characteristics of the reference character, but fail to maintain the characters original motion and cannot integrate the character naturally into the source video. Wan-Animate [4] and VACE [13] preserve the clothing of target character well. However, they struggle to learn precise lighting and shading information because their reconstruction-based paradigm loses significant amount of original video information. In comparison, MoCha can replace the specified character with high identity and animation consistency, all while preserving the original environment details and lighting effects. Quantitative Results. We compare MoCha with other methods on the aforementioned quantitative metrics. It is important to note that we omit Kling from the comparison because it lacks the capability for large-scale, automated batch testing. As shown in Tab. 1, MoCha consistently achieves state-of-the-art performance across all referencebased metrics, demonstrating superior temporal coherence and structural fidelity. For VBench [12] metrics, we select six different evaluation dimensions most closely related to our character replacement task. Tab. 2 illustrates that videos generated by MoCha exhibit superior quality and consistency compared to existing methods. Table 1. Quantitative comparison with state-of-the-art methods on synthesized benchmark on structural similarity (SSIM), perceptual similarity (LPIPS) and reconstruction quality (PSNR). Method SSIM LPIPS PSNR 0.572 VACE HunyuanCustom 0.644 0.692 Wan-Animate MoCha 0.746 0.253 0.257 0.213 0.152 17.10 17.70 19.20 23. 5.3. Ablation Study Ablation on Real-Human Data. In this paper, we propose several methods to construct paired training data for the character replacement task. Among these, the rendered dataset forms the foundation of our training data since its paired video is strictly aligned. To validate the contribution of the remaining real-human datasets (i.e., the expressiondriven data and the augmented video-mask data), we compare the models performance when trained with and without their inclusion. Figure 6. Ablation on Real-Human Data. The result shows that real-human data improves the overall realism and facial fidelity of the output. Figure 7. Ablation on Identity-Enhancing Post-Training. The result shows that our post-training strategy is crucial to enhance the facial consistency. As illustrated in Fig. 6, the real-human datasets significantly enhance the generated characters facial appearance, yielding expressions that more accurately align with the source video character. Furthermore, the result indicates they effecthat these data are crucial for overall quality: tively mitigate the synthetic artifacts (e.g., an overly smooth or glossy texture) brought by the rendered data, and substantially generate more realistic and higher-fidelity characters. Ablation on Identity-Enhancing Post-Training. In Section 3.3, we introduce an RL-based identity-enhancing posttraining strategy to overcome the challenge of imperfect identity consistency in the face of the generated video relative to the reference image. As shown in Fig. 7, equipped with reward LoRA, our model significantly improves the characters facial identity preservation while simultaneously maintaining the original generation quality. 5.4. More Analysis leverages the inherent Tracking ability of video diffusion model. Our method, tracking capabilities of MoCha, video diffusion models, requiring only single-frame mask to identify the target character, rather than mask sequence for the entire video. To validate this tracking ability, we visualize the attention score map between the mask latent and the source video latent. As shown in Fig. 8, the regions corresponding to the selected character consistently main7 Table 2. Quantitative comparison with state-of-the-art methods on real-world benchmark on VBench [12] metrics. Method VACE HunyuanCustom Wan-Animate MoCha Subject Consistency Background Consistency Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness 71.19 90.03 91.25 92.25 77.89 93.68 93.42 94. 56.76 56.77 54.60 60.24 60.88 58.92 58.48 59.58 97.04 97.98 97.27 97. 97.87 98.62 98.25 98.79 Performance on Complex Cases. To demonstrate the wide applicability and practical utility of MoCha, we evaluate its performance using collection of more complex test cases. We focus on challenging examples by seeking difficult scenarios across three distinct dimensions: 1) Complex lighting effect. 2) Object interaction. 3) Multiple-character interaction. Fig. 9 illustrates MoChas performance in these scenarios. The results confirm that our model consistently generates highly realistic and high-quality video even under these conditions. Application beyond Character Replacement. Although our method is primarily developed for the character replacement task, we observe that MoCha demonstrates the capability to handle other types of subject replacement and video editing. Specifically, our model can seamlessly replace non-human subjects from the source video. Furthermore, by editing the reference characters face and clothing using image-editing models, MoCha can be readily applied to tasks such as face swapping and virtual try-on. Some novel and illustrative results showing this expanded utility are presented in the supplementary material. In summary, these findings highlight MoChas strong generalization ability and broad practical utility. 6. Conclusion In this paper, we introduce MoCha, the first end-to-end video character replacement framework. Distinct from current reconstruction-based methods that rely on dense guidance, MoCha efficiently fulfills the replacement task, requiring only single arbitrary frame mask by harnessing the tracking ability of the video diffusion model. MoCha faithfully transfers original character motion and facial expression through efficient in-context learning. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce condition-aware RoPE and an RL-based post-training stage guided by differentiable facial function. Extensive experimental results demonstrate that MoCha significantly outperforms existing methods. Finally, we show that MoCha exhibits wide applicability beyond video character replacement, extending to applications such as face swapping and virtual tryon. Figure 8. Visualization of the attention score. It shows that MoCha can automatically trace the character with the mask of only one frame. Figure 9. MoChas performance on complex scenarios. MoCha demonstrates its robustness and practical utility on complex cases that are commonly encountered in real-world film and video production. tain high attention scores across different frames. This observation confirms that the model can effectively track the specified subject throughout the video sequence, obviating the need for sequential masks."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 3, 4 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 2, 5 [3] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 5 [4] Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, et al. Wan-animate: Unified character animation and replacement with holistic replication. arXiv preprint arXiv:2509.14055, 2025. 2, 3, 6, 7 [5] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 2, 4 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [7] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 2, 5 [8] Xuanhua He, Quande Liu, Zixuan Ye, Weicai Ye, Qiulin Wang, Xintao Wang, Qifeng Chen, Pengfei Wan, Di Zhang, and Kun Gai. Fulldit2: Efficient in-context conditioning for video diffusion transformers. arXiv preprint arXiv:2506.04213, 2025. 4 [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [10] Jiahao Hu, Tianxiong Zhong, Xuebo Wang, Boyuan Jiang, Xingye Tian, Fei Yang, Pengfei Wan, and Di Zhang. Vivid10m: dataset and baseline for versatile and interactive video local editing. arXiv preprint arXiv:2411.15260, 2024. 2, 5 [11] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 2, 3, 6, 7 [12] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 6, 7, 8 [13] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 6, [14] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. 3, 4 [15] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [16] Kling. Kling. https://klingai.com, 2025. 6, 7 [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [18] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 5 [19] Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, et al. Vfxmaster: Unlocking dynamic visual effect generation via in-context learning. arXiv preprint arXiv:2510.25772, 2025. 3, 4 [20] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1319913208, 2025. 4 [21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [22] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 4 [23] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. 2023. 4 [24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 6 [25] Xiangwei Shen, Zhimin Li, Zhantao Yang, Shiyi Zhang, Yingfang Zhang, Donghao Li, Chunyu Wang, Qinglin Lu, and Yansong Tang. Directly aligning the full diffusion trajectory with fine-grained human preference. arXiv preprint arXiv:2509.06942, 2025. 4 [26] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 2 [27] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: Attention-centric real-time object detectors. arXiv preprint arXiv:2502.12524, 2025. 5 [28] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 5 [29] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [30] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2 [31] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. 2 [32] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. [33] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018."
        }
    ],
    "affiliations": []
}