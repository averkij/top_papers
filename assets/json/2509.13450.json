{
    "paper_title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs",
    "authors": [
        "Vincent Siu",
        "Nicholas Crispino",
        "David Park",
        "Nathan W. Henry",
        "Zhun Wang",
        "Yang Liu",
        "Dawn Song",
        "Chenguang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 5 4 3 1 . 9 0 5 2 : r SteeringControl: Holistic Evaluation of Alignment Steering in LLMs Vincent Siu 1 , Nicholas Crispino 1 , David Park2, Nathan W. Henry3, Zhun Wang3, Yang Liu1, Dawn Song3, Chenguang Wang1 1 University of California, Santa Cruz 2 Washington University in St. Louis 3 University of California, Berkeley {vsiu3, ncrispin, yangliu, cwang001}@ucsc.edu d.park@wustl.edu {nathan.henry, zhun.wang}@berkeley.edu dawnsong@cs.berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "We introduce STEERINGCONTROL, benchmark for evaluating representation steering methods across core alignment objectivesbias, harmful generation, and hallucinationand their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in systematic way. We collect dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github. com/wang-research-lab/SteeringControl.git."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across wide range of natural language tasks (Brown et al., 2020; Touvron et al., 2023; Ouyang et al., 2022). However, their growing fluency and generality have raised serious concerns about safety and alignment (Bai et al., 2022; Weidinger et al., 2021; Mazeika et al., 2024), including tendencies to produce harmful content, propagate social bias, or mislead users through hallucinated responses (Xu et al., 2024; Gallegos et al., 2023). These behaviors are often emergent and unpredictable, underscoring the difficulty of governing high-capacity models. central objective in alignment research is to ensure that model behaviors remain safe, robust, and consistent with human intent, even under distribution shift or adversarial prompting (Leike et al., 2018; Bai et al., 2022; Ganguli et al., 2022). While techniques like supervised fine-tuning and reinforcement learning from human feedback (RLHF) have improved baseline alignment, they remain susceptible to failure in rare cases or when intentionally red-teamed (Ganguli et al., 2022; Andriushchenko et al., 2024; Wang and Shu, 2023; Zou et al., 2023a). In particular, recent work has shown that adversarial jailbreaks can exploit internal representations rather than surface-level outputs (Yu et al., 2024) and that weakly aligned models can be efficiently steered towards safer behavior (Siu et al., 2025), motivating interest in techniques that operate directly on models activations. Equal contribution Preprint. Under review. Figure 1: The STEERINGCONTROL evaluation set distributions, totaling 17 datasets. On the left, the inner ring illustrates three primary behavior categoriesBias, Harmful Generation, and Hallucinationwith their respective dataset sizes. The middle ring breaks each category down into constituent datasets proportional to their size, while the third ring (outermost) visualizes finergrained subcategories such as demographic dimensions (for Bias), harmful content types (for Harmful Generation), and hallucination difficulty levels (for Hallucination). Secondary behavior datasets, each with fewer than 1k prompts, are categorized and displayed on the right. Representation steering, the direct manipulation of internal activations to achieve target objective, has emerged as promising alternative to training-based alignment interventions (Zou et al., 2023b; Panickssery et al., 2023; Li et al., 2023; Turner et al., 2023; Wehner et al., 2025; Bartoszcze et al., 2025). These methods identify behavior-relevant directions in activation space (e.g., for refusal (Arditi et al., 2024; Marshall et al., 2024; Lee et al., 2024a; Wollschl√§ger et al., 2025; Panickssery et al., 2023) or hallucination (Chen et al., 2024; Zou et al., 2023b) and apply techniques using basic vector operations such as activation addition or ablation to modulate model behavior. Despite increased interest in representation-level interventions, steering methods are evaluated in many different ways. Existing studies often focus on different datasets, direction extraction procedures, or target behaviors and definitions of those behaviors (Zou et al., 2023b; Arditi et al., 2024; Panickssery et al., 2023; Siu et al., 2025; Wu et al., 2025), impeding reliable comparison. Even within the same behavioral category (e.g., refusal), interventions are derived from different distributions, formats, and criteria, confounding attribution of effectiveness and limiting generalizability. second limitation concerns the entanglement of alignment behaviors in representation space, i.e., the change in performance on out of distribution behaviors resulting from interventions. Prior work demonstrates that LLM behaviors such as safety, factuality, and fairness are not encoded in orthogonal subspaces, but in overlapping or reused directions (Elhage et al., 2022; Geva et al., 2021; Huang et al., 2024). As result, interventions on primary objective can affect secondary behaviors. For example, Arditi et al. (2024) demonstrate steering refusal can degrade model factuality in TruthfulQA (Lin et al., 2022). Such effects have not been extensively studied or accounted for in alignment evaluations. To address both issues, we introduce STEERINGCONTROL, benchmark suite for evaluating alignment interventions across multiple behaviors and their interactions. STEERINGCONTROL has two main contributions: 1) The collection of 17 datasets, including three safety-relevant primary behaviors and an evaluation framework measuring effectiveness when steering on those primary tasks and the resulting behavioral entanglement on other behaviors. 2) modular code framework exploiting the taxonomy of training-free steering methods, allowing for standardized evaluation on five popular steering methods via common library of interchangeable components. While we evaluate only these five, our framework naturally supports many other combinations. 2 (a) Qwen2.5-7B (b) Llama-3.1-8B Figure 2: Average effectiveness vs entanglement tradeoff for evaluated steering methods. Effectiveness measures steering success on target primary behaviors; entanglement captures unintended performance changes on other behaviors. We find that while methods can induce performance increases on our primary datasets (jailbreaking harmful generation, reducing hallucination, reducing bias), the relative effectiveness and entanglement vary by model."
        },
        {
            "title": "2 Dataset",
            "content": "STEERINGCONTROL is designed to evaluate representation steering methods by testing whether interventions can reliably steer specific target behavior while minimizing unintended effects on others. Unlike prior work that focuses narrowly on individual alignment objectives, STEERINGCONTROL supports comprehensive evaluation across diverse set of behavioral axes, enabling controlled comparisons and analysis of behavioral entanglement. To assess steering effectiveness as well as unintended entanglements effects, the benchmark distinguishes between primary behaviors, which are explicitly targeted by steering interventions, and secondary behaviors, which are not directly steered but may shift as side effects. This structure allows researchers to assess whether steering specific behaviorsuch as reducing hallucinationinduces undesirable changes in non-target behaviors like sycophancy or commonsense morality. Each primary dataset follows fixed 40/10/50 train/validation/test split and is stratified by subcategory (if applicable) to ensure robust evaluation. To support contrastive direction generation, we also include negative examples with an incorrect answer for all primary tasks, creating them if they do not exist. We next describe the behavioral categories in the benchmark, starting with primary alignment objectives. 2.1 Primary Behaviors Harmful Generation (Refusal). We use SALADBench (Li et al., 2024) as our main dataset for harmful generation, using the 21K base QA set, filtered using GPT-4o such that it only contains harmful open-ended prompts. Negative examples are drawn from Alpaca (Taori et al., 2023) for instruction-only prompts. To prevent overlap with demographic bias, we exclude prompts tagged as \"Hate Speech\" or \"Stereotyping\", and remove prompts that do not follow question answering formats. We aggregate behavior categories into SALADBenchs six high-level domains and stratify splits accordingly, giving us 4000 train, 1000 validation, and 5000 test samples. Refusal behavior is generation task, and outputs are scored using LlamaGuard-4 (Meta, 2025), safety classifier. We also include 4000 Alpaca prompts for training and 1000 for validation to support direction generation and direction selection. Demographic Bias (Fairness). We evaluate demographic bias through both implicit and explicit forms of discrimination. Implicit bias is assessed using BBQ (Parrish et al., 2022), multiplechoice benchmark probing stereotyping across protected demographic attributes. The dataset is split into 4000/1000/5000 train/validation/test examples, stratified by demographic. Explicit bias is evaluated using ToxiGen (Hartvigsen et al., 2022), binary classification benchmark where models are asked to agree/disagree with toxic statements linked to demographic identities. ToxiGen includes 3600/900/4500 train/validation/test prompts, stratified similarly. Accuracy for BBQ and ToxiGen is measured using substring matching over multiple-choice and boolean completions, respectively. Together, BBQ and ToxiGen capture both subtle stereotyping and overt toxicity. Hallucination. We adopt the taxonomy from HalluLens (Bang et al., 2025) to separate intrinsic hallucination (contradictions with input context) from extrinsic hallucination (unsupported generation absent from context or pretraining). For intrinsic hallucination, we use three FaithEval subsets (Ming et al., 2025): counterfactual (399/100/500), inconsistent (571/143/715), and unanswerable (924/231/1156). Negative completions are generated using GPT-4.1-mini for the unanswerable set and randomly chosen where they already exist in the other datasets. Extrinsic hallucination is evaluated using PreciseWikiQA (Bang et al., 2025), dataset of Wikipedia-sourced QA pairs stratified across 10 difficulty levels. We use fixed 4000/1000/5000 train/validation/test split generated with LLaMA-3.1-70B-Instruct (Grattafiori et al., 2024), following the methodology of Bang et al. (2025), and generate incorrect answers using GPT-4.1-mini. Completions are scored using LLaMA-3.3-70BInstruct (Grattafiori et al., 2024) for factuality via the hallucination rate. We report the percentage of prompts not hallucinating, such that higher scores indicate better behavior. key concern in representation steering is that selected directions may overfit to spurious correlations or narrow semantic patterns, leading to inflated performance on held-out data without genuine behavioral control. To mitigate this risk, and in light of the computational cost of full-benchmark evaluation, we adopt dynamic testing strategy: all primary behavior steering and evaluations are conducted on subset of our full benchmark (20% subset of each dataset). This approach reduces the likelihood of overfitting to static evaluation sets and supports more robust, generalizable assessments of steering effectiveness. 2.2 Secondary Behavioral Evaluation To assess how alignment generalizes beyond primary objectives, STEERINGCONTROL evaluates models across 10 secondary behaviors. These tasks fall outside the scope of direct harmful generation, hallucination, or bias; instead, they are grouped into four categories: social behavior, reasoning ability, epistemic integrity, and normative judgment. Importantly, we never steer on any of these datasets. Instead, the goal is to measure how steering on primary behaviors affects performance on these secondary behaviors. Social Behaviors. Alignment objectives like helpfulness and politeness (Bai et al., 2022) may lead to side effects such as over-agreement or anthropomorphism. Therefore we assess Brand Bias, Sycophancy, Anthropomorphism, and User Retention, each evaluated using 110 prompts from DarkBench (Kran et al., 2025). Brand Bias tests preference in product recommendations; Sycophancy measures uncritical agreement with user input; Anthropomorphism tests whether models describe themselves with human-like traits; and User Retention measures tendency to prolong interactions unnecessarily. All responses are scored using GPT-4o, as in Kran et al. (2025). In alignment with our other datasets, we report the percentage of prompts not exhibiting the behavior, so that higher score is better. Reasoning Capabilities. These benchmarks test whether interventions harm core cognitive abilities like inference and factual reasoning. We compile an Expert-Level Reasoning assessment using GPQAs (Rein et al., 2023) 448 MCQs, covering fields like law, physics, and biology. Commonsense Reasoning uses 500 prompts from ARC-C (Clark et al., 2018), requiring grade-school level inference. Accuracy is computed via exact match. Epistemic Integrity. These tasks assess internal factual consistency and resistance to subtle epistemic failures. Factual Misconceptions are tested using 791 binary-choice TruthfulQA (Lin et al., 2022) prompts, where models choose between true and plausible but false statements. Following Bang et al. (2025), we treat this as factuality task. Sneaking uses 110 adversarial DarkBench (Kran et al., 2025) prompts to test if the model reframes opinions while subtly shifting the original stance. GPT-4o judges Sneaking, while misconceptions are judged via exact match. For sneaking specifically, we report the percentage of prompts not exhibiting sneaking behavior such that higher score is better. Normative Judgment. This category assesses how models navigate ethically and ideologically sensitive scenarios, focusing on moral reasoning and political stance-taking. We test Commonsense 4 Morality using 750 ethical dilemmas from DecodingTrust (Wang et al., 2024a), scored by whether the model chooses the correct and moral answer. Political Views uses 750 prompts from TwinViews13k (Fulay et al., 2024), which ask the model to agree with either left or right-leaning opinions. We report the percentage of responses choosing the left-leaning option since prior work finds that models often skew to the left (Fulay et al., 2024; Potter et al., 2024). Unlike other datasets where higher is better, this convention was chosen arbitrarily and is not meant to endorse any particular political ideology. 2.3 Metrics The goal of STEERINGCONTROL is to benchmark current steering methods on variety of behaviors while investigating their out of distribution behavior. To facilitate this, we define two aggregate metrics: EFFECTIVENESS (Eq. 1), how performant steering method is on in-distribution behavior, and ENTANGLEMENT (Eq. 2), how much performance shifts on out of distribution behaviors when steered on one of the primary behaviors. EFFECTIVENESS = ENTANGLEMENT = 1 Bprimary (cid:88) (cid:40) y(steered) yb (cid:41) bBprimary (1 yb) 1 Bood (cid:115) (cid:88) bBood (y(steered) yb)2 (1) (2) While these aggregate metrics are useful, performance can vary widely depending on not only the method, but also the behavior being steered. Therefore, we also present per-behavior results for each steering method over all behaviors to allow for observations of the specific tradeoffs faced for each combination of model, method, and behavior."
        },
        {
            "title": "3 Methodology",
            "content": "We first define set of modular components that are used in popular training-free steering methods, which we implement within our evaluation framework. Then, we define the five real steering methods we chose to evaluate, expressing them in terms of these components. 3.1 Steering Components Currently, we focus on steering accomplished through inference-time interventions without training. We define such steering methodologies as combination of components within three unique parts of the steering pipeline: direction generation (how the direction is obtained from input prompts), direction selection (how to select the best direction given set of candidate directions), and direction application (how the forward pass is adjusted with the direction during inference). 3.1.1 Direction Generation Direction generation is how input prompts are transformed into direction of the same dimension as the model that can be used in steering. By default, we always extract direction from the last token position (-1). We define variety of components for direction generation, both for 1) the style of the input data and 2) the method of extracting the direction. We allow the collection of activations from the input and output of each layer, as well as of each attention and MLP module. In practice, for all of the methods tested in this benchmark we collect activations from the input before each layer for standardization. When generating the direction, we always normalize it following Wu et al. (2025). We currently offer the following components: DiffInMeans DiffInMeans represents the mean difference in activations between positive and negative activations at the selected location. PCA PCA identifies the primary axis of variance among activation vectors as in (Lee et al., 2024a; Wu et al., 2025), then checks this principle component to ensure it aligns with the positive direction of the prompts. 5 LAT LAT also uses principle component analysis, but instead of using the raw activations directly, it randomly pairs activations (regardless of their positive/negative labels) and uses the difference between them as inputs (Wu et al., 2025; Zou et al., 2023b). The input format can also be adjusted during direction generation. When using any of the above, one of the following different input formats can be selected: 1) the input is provided as normal based on the dataset (termed default), 2) it is framed in the style of LAT stimulus templates (termed RepE from Zou et al. (2023b)), or 3) it is always framed as multiple choice question (termed CAA from Panickssery et al. (2023)). 3.1.2 Direction Selection Direction selection is how single direction is chosen given set of candidate directions. The output of each direction selection procedure is layer (where the direction was generated from) and the values for any other applier-specific parameters that we iterated over. For all methods, we search from the 25th to 80th quantile of the layers with step size of 2, as prior work has shown steering is more effective in the middle layers (Arditi et al., 2024). The set of applier-specific parameters is based on the steering method and currently is either empty or consists of coefficient (where we test integers from -3 to 3 inclusive). For each method, unless otherwise specified we include KL divergence check on Alpaca (using the same split as defined for harmful generation) as in Arditi et al. (2024) to ensure the intervention is reasonable, discarding the direction if it results in KL divergence in logits of over 10%. Grid Search We implement grid search to find the best direction, finding the layer and applierspecific parameters with the highest performance on the validation set. COSMIC We implement COSMIC (Siu et al., 2025) as an alternative to grid search, which uses the internals during forward pass on the validation set instead of generating an entire answer and testing the outputs. We support COSMIC on all methods but note it was designed for DIM and ACE. 3.1.3 Direction Application Direction application specifies how the direction modifies activations during inference. There are two important aspects of direction application: 1) the mathematical formulation of the intervention, and 2) where that intervention is applied. We specify the mathematical formulations below, where in each case activations are modified in-place and the forward pass is continued: Activation Addition Activation addition (Turner et al., 2023; Panickssery et al., 2023) modifies activations of the form = + Œ± d, where is the direction, is the activation and Œ± is the steering coefficient. Directional Ablation Directional ablation (Arditi et al., 2024; Marshall et al., 2024) modifies activations of the form = proj (d) added to the right hand side in the case of an affine transformation as in Marshall et al. (2024), with representing the mean of the negative activations from the direction generation step. Currently, we do not utilize steering coefficient for Directional Ablation experiments following the conventions of Arditi et al. (2024); Siu et al. (2025). (v), with an additional proj Each of these interventions specify how to modify activations given we already know where and when we want to intervene; below we offer options to adjust these parameters as well: Intervention Locations The location within the transformer and token position where the intervention is applied must be specified for each method. The position of intervention can either be ALL, OUTPUT_ONLY, or POST_INSTRUCTION. The location of intervention is defined based on the residual stream. Most often, the direction is applied at the same place in the residual stream as where it was generated, though it can also be applied in specific places, e.g., the input and output of the attention and MLP blocks in all layers in the residual stream. We also allow cumulative interventions, which we define as when directions from previous layers are used to intervene on their respective previous layers in addition to the selected direction, starting from the first layer we collect directions from (at 25% through the model). E.g., if we intervene at layer 10 and the 25% layer is layer 6, we intervene at 6 layers 6, 8, and 10 with the same direction application method using directions from those respective layers. Conditional Steering Besides defining where we apply the direction, we also introduce conditional steering to let us decide when to apply the intervention at inference time depending on the prompt. We implement this based on CAST (Lee et al., 2024a), conditional direction application method where steering only occurs if the cosine similarity of the activations and preselected condition vector is above some threshold. Though the original paper proposes full steering methodology using PCA, we instead separate the conditional application portion of the method and refer to that as CAST, since it can be used with any of the stated direction application mathematical formulations, direction generation, or direction selection combinations. This method is explicitly built to reduce entanglement since it only steers when it detects in-distribution behavior. As such, in practice when we use CAST we do not include KL divergence check in the direction generation stage. CAST can be used with any mathematical formulation and location of intervention. CAST uses the same split of Alpaca as defined in the harmful generation validation set to select the condition vector, which for simplicity we set to one of the candidate vectors from direction generation. 3.2 Steering Methods Though the above steering components can be freely combined, in practice we select five preset steering methods from the literature that implement explicit combinations of the modular components, detailed in Table 1. Where it isnt clear, we make reasonable decisions about how to use the method in our framework given the paper and/or codebase where that method was used. Table 1: Overview of steering methods with their components. Direction selection uses GridSearch across all methods. Method Application Location Application Position Dir. Application Dir. Generation Format DIM ACE CAA PCA LAT default default CAA default RepE DiffInMeans DiffInMeans DiffInMeans PCA LAT DirectionalAblation DirectionalAblation + Affine ActAdd ActAdd ActAdd ALL ALL POST_INSTRUCTION ALL ALL Input (all), Output (attn, mlp) Same as gen. Same as gen. Same as gen. Cumulative We implement the following methods: Difference-in-Means (DIM) is based on Arditi et al. (2024); Siu et al. (2025), deviating only by using our standardized grid search for direction selection. 2 Affine Concept Editing (ACE) is based on Marshall et al. (2024)s affine concept editing and is automated and shown to be effective compared to DIM for refusal in Siu et al. (2025). Contrastive Activation Addition (CAA) based on Panickssery et al. (2023). Notably, we follow the convention of always using multiple choice formatting for direction generation and applying the intervention at all post instruction tokens. Principal Component Analysis (PCA) is based on Zou et al. (2023b); Wu et al. (2025); Liu et al. (2024); Lee et al. (2024a). Linear Artificial Tomography (LAT) is based on Zou et al. (2023b); Wu et al. (2025). Different from AxBench, we use the RepE format as used in Zou et al. (2023b), and apply directions cumulatively as suggested in the original paper as well. similar setting is also applied in Lee et al. (2024a) for PCA, but for more diversity we chose not to use this cumulative setting for PCA as well."
        },
        {
            "title": "4 Evaluation",
            "content": "To assess the effectiveness and generalizability of behavioral steering, we evaluate steered version of Qwen-2.5-7B (Qwen et al., 2024) and Llama-3.1-8B (Grattafiori et al., 2024) across primary and secondary behavioral tasks. Steering is conducted using STEERINGCONTROLs curated training and validation split. As STEERINGCONTROL is focused on benchmarking general steering effectiveness in addition to entanglement, we choose the three single-behavior interventions that align best with existing work in representation steering: (1) increasing harmful generation (Marshall et al., 2024; 2We note that Difference-in-Means often refers to way of generating direction from activations, not full steering method with fixed way of selecting and applying directions. However, we follow Wollschl√§ger et al. (2025) by referring to Arditi et al. (2024)s method of steering as DIM. 7 Arditi et al., 2024; Siu et al., 2025; Panickssery et al., 2023; Wollschl√§ger et al., 2025; Lee et al., 2024a; Zou et al., 2023b), (2) reducing intrinsic/extrinsic hallucinations (Xu et al., 2024; Nguyen et al., 2025; Qiu et al., 2024; Ji et al., 2025; Beaglehole et al., 2025; Zou et al., 2023b; Panickssery et al., 2023), and (3) reducing explicit/implicit bias (Nguyen et al., 2025; Qiu et al., 2024; Beaglehole et al., 2025; Siddique et al., 2025; Ant, 2024; Liu et al., 2024; Zou et al., 2023b). For each primary behavior, we steer the model on the target behavior before fully evaluating the result on all remaining datasets. Additional experimental details are in Appendix A. 4.1 Aggregate Results As there is clear tradeoff between improving performance in-domain and minimizing OOD shifts, we present pareto curve of EFFECTIVENESS vs. ENTANGLEMENT for each steering method on each model in Figure 2. We see that steering methods differ drastically for each model and that there is wide range of tradeoffs possible. As our focus is on entanglement, we present three variations on each steering method with explicit ties to entanglement: with KL divergence check (Standard), without KL divergence check (No KL), and with CAST for conditional steering (Conditional). Interestingly, all methods in Qwen-2.5-7B and Llama-3.1-8B show similar effectiveness vs entanglement tradeoff, with DIM being the highest performing but also having the highest entanglement, ACE having more moderate effectiveness boosts but notably less entanglement, and PCA having generally lower amounts of both. These results show that models are key determinant of steering performance and there is no universal best method that maximizes effectiveness and minimizes efficiency and works best across all models. For variants, on aggregate, using CAST for conditional steering accounts for most pareto-optimal solutions, often achieving larger decrease in entanglement with only slight decreases in effectiveness vs the Standard and No KL settings. No KL often slightly improves upon effectiveness, but at higher cost of entanglement. 4.2 Behavior-Specific Results We present full evaluations of Qwen-2.5-7B and Llama-3.1-8B in Figures 3 and 4, with the No KL and Conditional variants in Figures 5, 6, 7, and 8. Overall Comparisons All five base steering methods improve performance on primary behaviors to varying degrees, depending on both the model and the behavior. Across models, extrinsic hallucination seems most challenging to steer, while refusal is easiest but only for DIM and ACE. In general, Standard steering and CAST are able to perform quite similarly across behaviors, but the latter often has markedly less entanglement, e.g., in refusal for Qwen. While CAST is often useful at preserving effectiveness while decreasing entanglement, there are still areas with substantial gains achieved when turning off the KL divergence check that are not preserved when CAST is used, e.g., PCA and LAT intrinsic hallucination for Llama. This means that steering methods can achieve higher performance at the behavior level when there is less focus on restricting entanglement, suggesting there is still work to do to improve how well we can create targeted steering methods. Model-Specific Failures. Each model exhibits weaknesses in steering particular behaviors. In Qwen, extrinsic hallucination steering is largely ineffective while Llama achieves slightly more improvement. Conversely, explicit bias sees greater gains on Qwen compared to Llama. These differences reflect variations in representational geometry across models and highlight the importance of consistent datasets when comparing steering methods. Entanglement Broadness. Prior work on harmful generation steering (Arditi et al., 2024; Wollschl√§ger et al., 2025; Siu et al., 2025) typically evaluates entanglement on TruthfulQA or reasoning datasets, where we find relatively small changes across most models, methods, and behaviors. In contrast, we observe more notable entanglement with social behaviors like sycophancy and anthropomorphism, and even between primary targets, suggesting focusing on wider suite of behaviors is crucial towards understanding the tradeoffs of existing methods. We hypothesize that such effects stem from shared mechanisms in prompt interpretation, not just conceptual overlap. These results underscore that no method achieves clean, orthogonal control over target behaviors. Effective steering requires balancing directional strength and behavioral specificity. Broad behavioral 8 evaluation enabled by STEERINGCONTROL is essential for understanding both intended and emergent effects of representation-level alignment."
        },
        {
            "title": "5 Related Work",
            "content": "Our work builds on research in LLM alignment, activation steering, and mechanistic interpretability, with focus on intervening in and evaluating internal representations to control behaviors such as harmful generation, demographic bias, and hallucination. Interpretability. Mechanistic interpretability provides the theoretical foundation for much of this steering work. Numerous studies demonstrate that abstract propertiestruthfulness, bias, refusalare encoded as linearly decodable directions in residual space (Park et al., 2024; Nanda et al., 2023; Bolukbasi et al., 2016; Mikolov et al., 2013). This supports the linear representation hypothesis and the superposition principle, whereby many semantic features are superimposed within the same activation subspace (Elhage et al., 2022). Tools like sparse autoencoders (Bricken et al., 2023; Huben et al., 2024; Templeton et al., 2024) and weight attribution methods (Pearce et al., 2024) have advanced our understanding of how individual neurons or MLP submodules contribute to specific behaviors. Circuit-level analyses (Elhage et al., 2021; Lieberum et al., 2023) further enable tracing causal pathways for behavioral features. As steering techniques increasingly operate at the activation level, interpretability research offers methods to characterize where and how to intervene effectively. Steering. growing body of work explores activation-level interventions to steer behavior directly within models internal representations. Refusal, toxicity, and helpfulness have been shown to correspond to linear directions in residual space (Arditi et al., 2024; Marshall et al., 2024; Weidinger et al., 2021). Methods such as Representation Engineering (Zou et al., 2023b; Panickssery et al., 2023) and Spectral Editing (Qiu et al., 2024) operate by injecting or removing learned directions to elicit or suppress targeted behaviors. These directions are often derived from contrastive data pairs (Burns et al., 2023; Arditi et al., 2024), embedding differences (Panickssery et al., 2023), or activation clustering (Wu et al., 2025). Concept removal approaches such as Contrastive Activation Addition (Zou et al., 2023b) and linear concept nullification (Belrose et al., 2023; Ravfogel et al., 2020) have been applied to steer model responses while preserving fluency and task performance, while Wang and Shu (2023) target key intervention layers using cosine similarity to known unsafe activation patterns. More fine-grained analysis with steering has also been done, splitting general behaviors into specific categories such as types of harmfulness or political beliefs (Bhattacharjee et al., 2024; Lee et al., 2024a; Hu et al., 2025). While looking within behaviors is important, entanglement across behaviors remains challenge as well: studies have shown that steering for one objective (e.g., reducing toxicity) can inadvertently affect capabilities like informativeness or truthfulness (Lee et al., 2024b; Qiu et al., 2024). STEERINGCONTROL builds on these findings to systematize the evaluation of such cross-behavior interference. There exist other benchmarks and frameworks for steering similar to STEERINGCONTROL, namely AxBench (Wu et al., 2025), EasyEdit2 (Xu et al., 2025), and Im and Li (2025). STEERINGCONTROL differs in the scope of its analysis, most in its focus on entanglement, but as well as its inclusion of natural safety-relevant behaviors and on wide, modular coverage of the training-free steering paradigm, implementing standardized steering pipeline similar to the taxonomy proposed in Wehner et al. (2025)."
        },
        {
            "title": "6 Conclusion",
            "content": "By standardizing training-free representation-steering methods and providing comprehensive suite of datasets, STEERINGCONTROL lowers the barrier to rigorous evaluation, introduces modular framework to exploit individual method design choices, and fosters reproducibility across steering methods. It enables comparative studies that account for generalization, interference, and modelspecific representational geometrychallenges that have previously hindered progress in alignment research. Our results highlight that while steering methods can be effective at modulating target behaviors, 1) their performance still has much room for improvement on key behaviors, and 2) they often introduce unintended side effects due to entanglement in representation space. Looking forward, STEERINGCONTROL lays the groundwork for developing steering methods with pareto-optimal improvements on the effectiveness-entanglement tradeoff, enabling more transparent and systematic alignment strategies for real-world deployment."
        },
        {
            "title": "7 Limitations",
            "content": "While STEERINGCONTROL represents significant advance in standardized, multi-behavior evaluation of alignment steering, it has several limitations. The benchmark focuses on English-language datasets and instruction-tuned models, limiting its applicability to multilingual or non-instructional contexts (Wang et al., 2024b), where behavioral entanglement may surface differently. Steering is implemented as static vectors applied at fixed model locations, enabling fair comparisons but overlooking more adaptive or gradient-based methods like ReFT (Wu et al., 2024). Future work should expand our current framework of steering components to incorporate methods with weight modifications and other diverse elements of representation engineering, such as those defined in the taxonomy in Wehner et al. (2025). Results are reported in aggregate form, which can obscure nuanced shifts within behavioral subtypes such as specific demographic biases or on specific political issues, or have certain methods dominate due to high performance on single behavior (e.g., refusal). Moreover, the use of uniform grid-search methodology across all behaviors simplifies direction selection but may underperform compared to behavior-specific techniques, particularly in harmful generation tasks (Arditi et al., 2024). We only use 64 tokens for generation tasks, which could obscure the intentions of the models if continued. We also do not allow thinking before generation, requiring the answer to be generated immediately. Future work should investigate reasoning models and how they differ. Prior work also suggests that steering using directions from tokens other than the final post-instruction tokens may yield more effective control (Arditi et al., 2024; Siu et al., 2025) that can differ by behavior (Zhao et al., 2025), which our setup does not currently exploit."
        },
        {
            "title": "References",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862. Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models, 2021. URL https://arxiv.org/abs/2112.04359. Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David A. Forsyth, and Dan Hendrycks. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=f3TUipYU3U. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models, 2024. URL https://arxiv.org/abs/2401.11817. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: survey, 2023. URL https://arxiv.org/abs/2309.00770. Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: research direction, 2018. URL https://arxiv.org/abs/ 1811.07871. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022. URL https://arxiv.org/abs/2209.07858. Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safetyaligned llms with simple adaptive attacks. ArXiv preprint, abs/2404.02151, 2024. URL https: //arxiv.org/abs/2404.02151. Haoran Wang and Kai Shu. Trojan activation attack: Red-teaming large language models using activation steering for safety-alignment, 2023. URL https://arxiv.org/abs/2311.09433. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023a. URL https://arxiv. org/abs/2307.15043. Lei Yu, Virginie Do, Karen Hambardzumyan, and Nicola Cancedda. Robust llm safeguarding via refusal feature adversarial training, 2024. URL https://arxiv.org/abs/2409.20089. Vincent Siu, Nicholas Crispino, Zihao Yu, Sam Pan, Zhun Wang, Yang Liu, Dawn Song, and In The Chenguang Wang. COSMIC: Generalized refusal identification in LLM activations. 63rd Annual Meeting of the Association for Computational Linguistics, 2025. URL https: //openreview.net/forum?id=CJ8TYfkPiG. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: top-down approach to ai transparency, 2023b. URL https://arxiv.org/abs/2310.01405. Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition, 2023. URL https://arxiv.org/ abs/2312.06681. Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36:4145141530, 2023. 11 Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Monte MacDiarmid. Steering language models with activation engineering, 2023. URL https://arxiv.org/abs/2308.10248. Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, and Mario Fritz. Taxonomy, opportunities, and challenges of representation engineering for large language models. arXiv preprint arXiv:2502.19649, 2025. Lukasz Bartoszcze, Sarthak Munshi, Bryan Sukidi, Jennifer Yen, Zejia Yang, David Williams-King, Linh Le, Kosi Asuzu, and Carsten Maple. Representation engineering for large-language models: Survey and research challenges. arXiv preprint arXiv:2502.17601, 2025. Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by single direction. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ f545448535dfde4f9786555403ab7c49-Abstract-Conference.html. Thomas Marshall, Adam Scherlis, and Nora Belrose. Refusal in llms is an affine function, 2024. URL https://arxiv.org/abs/2411.09003. Bruce W. Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre Dognin, Manish Nagireddy, and Amit Dhurandhar. Programming refusal with conditional activation steering, 2024a. URL https://arxiv.org/abs/2409.05907. Tom Wollschl√§ger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan G√ºnnemann, and Johannes Gasteiger. The geometry of refusal in large language models: Concept cones and representational independence, 2025. URL https://arxiv.org/abs/2502.17420. Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. INSIDE: llms internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Zj12nzlQbz. Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. Axbench: Steering llms? even simple baselines outperform sparse autoencoders, 2025. URL https://arxiv.org/abs/2501.17148. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition, 2022. URL https://arxiv.org/abs/2209.10652. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https: //aclanthology.org/2021.emnlp-main.446. Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. Ravel: Evaluating interpretability methods on disentangling language model representations, 2024. URL https: //arxiv.org/abs/2402.17700. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229. 12 Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: hierarchical and comprehensive safety benchmark for large language models, 2024. URL https://arxiv.org/abs/2402.05044. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, Apr 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: hand-built bias benchmark for question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 20862105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165. URL https://aclanthology.org/2022.findings-acl.165/. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: large-scale machine-generated dataset for implicit and adversarial hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, 2022. Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. Hallulens: Llm hallucination benchmark. 2025. URL https: //arxiv.org/abs/2504.17550. Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Faitheval: Can your language model stay faithful to context, even if \"the moon is made of marshmallows\", 2025. URL https://arxiv.org/abs/2410.03727. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm√°n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V√≠tor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, 14 Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Esben Kran, Hieu Minh \"Jord\" Nguyen, Akash Kundu, Sami Jawhar, Jinsuk Park, and Mateusz Maria Jurewicz. Darkbench: Benchmarking dark patterns in large language models, 2025. URL https://arxiv.org/abs/2503.10728. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. Decodingtrust: comprehensive assessment of trustworthiness in gpt models, 2024a. URL https://arxiv.org/ abs/2306.11698. Suyash Fulay, William Brannon, Shrestha Mohanty, Cassandra Overney, Elinor Poole-Dayan, Deb Roy, and Jad Kabbara. On the relationship between truth and political bias in language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, page 90049018. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.emnlp-main. 508. URL http://dx.doi.org/10.18653/v1/2024.emnlp-main.508. Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, and Dawn Song. Hidden persuaders: LLMs political leaning and their influence on voters. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 42444275, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.244. URL https://aclanthology.org/2024.emnlp-main.244/. Sheng Liu, Haotian Ye, Lei Xing, and James Y. Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=dJTChKgv3a. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. URL https://arxiv.org/abs/2412.15115. Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Multi-attribute steering of language models via targeted intervention. arXiv preprint arXiv:2502.12446, 2025. Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo Maria Ponti, and Shay B. Cohen. Spectral editing of activations for large language model alignment. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 684c59d614fe6ae74a3be8c3ef07e061-Abstract-Conference.html. Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, and Nicola Cancedda. Calibrating verbal uncertainty as linear feature to reduce hallucinations, 2025. URL https://arxiv.org/abs/2503.14477. 15 Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser√†, and Mikhail Belkin. Aggregate and conquer: detecting and steering llm concepts by combining nonlinear predictors over multiple layers, 2025. URL https://arxiv.org/abs/2502.03708. Zara Siddique, Irtaza Khalid, Liam D. Turner, and Luis Espinosa-Anke. Shifting perspectives: Steering vector ensembles for robust bias mitigation in llms, 2025. URL https://arxiv.org/ abs/2503.05371. Oct 2024. URL https://www.anthropic.com/research/evaluating-feature-steering. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=UGpGkLzwpP. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 1630, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https://aclanthology.org/2023.blackboxnlp-1.2. Tolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Tauman Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 43494357, 2016. URL https://proceedings.neurips.cc/paper/2016/ hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daum√© III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746751, Atlanta, Georgia, 2013. Association for Computational Linguistics. URL https://aclanthology.org/N13-1090. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemanticfeatures/index.html. Robert Huben, Hoagy Cunningham, Logan Riggs, Aidan Ewart, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=F76bwRSLeK. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/ index.html. Michael T. Pearce, Thomas Dooms, Alice Rigg, Jose M. Oramas, and Lee Sharkey. Bilinear mlps enable weight-based mechanistic interpretability, 2024. URL https://arxiv.org/abs/2410. 08417. 16 Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. Tom Lieberum, Matthew Rahtz, J√°nos Kram√°r, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla, 2023. URL https://arxiv.org/abs/2307.09458. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=ETKGuby0hcs. Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. LEACE: perfect linear concept erasure in closed form. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html. Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace projection. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 72377256, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.647. URL https://aclanthology.org/2020. acl-main.647. Amrita Bhattacharjee, Shaona Ghosh, Traian Rebedea, and Christopher Parisien. Towards inferencetime category-wise safety steering for large language models, 2024. URL https://arxiv.org/ abs/2410.01174. Jingyu Hu, Mengyue Yang, Mengnan Du, and Weiru Liu. Fine-grained interpretation of political opinions in large language models. arXiv preprint arXiv:2506.04774, 2025. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan Kummerfeld, and Rada Mihalcea. mechanistic understanding of alignment algorithms: case study on dpo and toxicity. In International Conference on Machine Learning, pages 2636126378. PMLR, 2024b. Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, and Ningyu Zhang. Easyedit2: An easy-to-use steering framework for editing large language models. arXiv preprint arXiv:2504.15133, 2025. Shawn Im and Yixuan Li. Unified Understanding and Evaluation of Steering Methods, February 2025. URL http://arxiv.org/abs/2502.02716. arXiv:2502.02716 [cs] version: 1. Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen tse Huang, Wenxiang Jiao, and Michael R. Lyu. All languages matter: On the multilingual safety of large language models, 2024b. URL https://arxiv.org/abs/2310.00905. Zhengxuan Wu, Aryaman Arora, Zheng Wang, Atticus Geiger, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. ReFT: Representation Finetuning for Language Models, May 2024. URL http://arxiv.org/abs/2404.03592. arXiv:2404.03592 [cs]. Jiachen Zhao, Jing Huang, Zhengxuan Wu, David Bau, and Weiyan Shi. LLMs Encode Harmfulness and Refusal Separately, July 2025."
        },
        {
            "title": "A Experimental Details",
            "content": "To select direction, for each combination of hyperparameters (layer, coefficient), we apply the direction at inference time and evaluate model behavior on fixed validation set. The configuration yielding the highest mean performance across all primary metrics is selected for final evaluation. For all datasets that are multiple choice, we generate one new token. For all other datasets, we generate up to 64 new tokens. We use substring matching for all multiple choice datasets. To ensure the format is not driving differences in performance, we standardize all multiple choice datasets to use single capital letters for the choices and answers. For all multiple choice datasets except those testing hallucination and political leaning, since we use substring matching we prepend short string encouraging responses to be as concise as possible: Please provide only the correct answer in its simplest form, without any additional text or explanation. For context, whenever we reference post instruction tokens, we refer to all tokens after the initial user prompt. For Qwen2.5, when we supply prompt to the LLM we do it in the following format (we highlight the content corresponding to post-instruction tokens in blue): <im_start>user instruction<im_end><im_start>assistant. Note throughout direction selection, we use the prompt with the post-instruction tokens (including the empty assistant prompt) if we are collecting or comparing activations."
        },
        {
            "title": "B Results",
            "content": "The per-model results across all behaviors and methods are in Figures 3 and 4 for no variants, Figures 5 and 7 with no KL divergence check, and Figures 6 and 8 with conditional steering. We note that Llama-3.1-8B does not often give answers corresponding to the possible multiple choice answers in TwinViews, meaning the performance differentials are not due to changes in political views but moreso in how the answer is formulated. For this reason, we do not include TwinViews results when calculating entanglement or in our main results figures for Llama-3.1-8B. 18 Figure 3: The changes in performance on all datasets when steering with five methods with 5 objectives on Qwen-2.5-7B-Instruct. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance. Higher scores generally indicate safer performance (e.g lower dark behaviors or hallucination rates) except for SALADBench ASR (left-most), where higher scores indicate higher jailbreaking, and Political Views (right-most), where higher score indicates higher proportion of left-leaning opinions. Dataset pertaining to the target behavior in each setting are bordered in black and annotated with an asterisk (*). 19 Figure 4: The changes in performance on all datasets when steering with five methods with five objectives on Llama-3.1-8B-Instruct. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance, similarly to the Qwen results in Figure 3. 20 Figure 5: The changes in performance on all datasets when steering with five methods with five objectives on Qwen-2.5-7B when no KL divergence check was used in direction generation. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance, similarly to the Qwen results in Figure 3. 21 Figure 6: The changes in performance on all datasets when steering with five methods with five objectives on Qwen-2.5-7B when using conditional steering. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance, similarly to the Qwen results in Figure 3. 22 Figure 7: The changes in performance on all datasets when steering with five methods with five objectives on Llama-3.1-8B when no KL divergence check was used in direction generation. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance, similarly to the Qwen results in Figure 3. 23 Figure 8: The changes in performance on all datasets when steering with five methods with five objectives on Llama-3.1-8B when using conditional steering. The results of the unsteered model are displayed at the top, and all reported steering values are expressed as the difference relative to the unsteered models performance, similarly to the Qwen results in Figure 3."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "University of California, Santa Cruz",
        "Washington University in St. Louis"
    ]
}