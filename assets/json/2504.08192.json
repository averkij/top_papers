{
    "paper_title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "authors": [
        "Aashiq Muhamed",
        "Jacopo Bonato",
        "Mona Diab",
        "Virginia Smith"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 9 1 8 0 . 4 0 5 2 : r Preprint. Under review. SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith {amuhamed,mdiab,smithv}@andrew.cmu.edu, jacopo.bonato.ext@leonardo.com Carnegie Mellon University, Leonardo Labs"
        },
        {
            "title": "Abstract",
            "content": "Machine unlearning is promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic SAE Guardrails (DSG), novel method for precision unlearning that leverages principled feature selection and dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearningoffering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning."
        },
        {
            "title": "Introduction",
            "content": "Machine unlearning, the process of removing specific information from trained LLMs, is promising tool for applications in safety, privacy, and model maintenance (Liu et al., 2025). However, the predominant gradient-based unlearning methods suffer from significant limitations (Barez et al., 2025). Existing methods struggle to precisely balance forgetting target data with preserving general utility (Thaker et al., 2024), incur high computational costs (Cha et al., 2024) , exhibit hyperparameter instability (Bu et al., 2024), degrade quickly under sequential unlearning requests (Shi et al., 2024; Gao et al., 2025), are vulnerable to relearning attacks (Hu et al., 2024; Deeb & Roger, 2024), lack data efficiency (Gao et al., 2024), and offer little interpretability (Xu et al., 2024). While interventions using Sparse Autoencoders (SAEs) (Bricken et al., 2023) offer potential path towards more targeted, activation-based unlearning, existing SAE approaches have typically underperformed gradient-based approaches due to imprecise interventions that cause unintended side effects (Farrell et al., 2024). This paper demonstrates that, contrary to previous findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic SAE Guardrails (DSG), novel method that leverages SAEs for precise, efficient, and interpretable unlearning in LLMs. DSG integrates Fisher Information-based feature selection to identify features causally linked to the forget data, with dynamic, input-dependent classifier that triggers targeted feature clamping only when necessary. This conditional intervention acts as guardrail, preventing the model from accessing specific knowledge pathways for relevant inputs while leaving general capabilities intact. Through extensive experiments on standard benchmarks, we show that DSG not only achieves superior balance between forgetting 1Code is available at https://github.com/aashiqmuhamed/DynamicSAEGuardrails 1 Preprint. Under review. and utility preservation compared to leading gradient-based and static SAE methods, but also directly addresses their core limitations. Our main contributions are: 1. We introduce DSG, new activation-based unlearning method featuring principled SAE feature selection and dynamic classifier for precise, conditional intervention. 2. We demonstrate empirically that DSG achieves superior balance between forgetting and utility preservation compared to leading gradient-based and SAE-based unlearning approaches on multiple benchmarks. 3. We show that DSG provides substantial benefits over gradient-based unlearning such as greater hyperparameter stability, improved computational efficiency, and sequential unlearning capability, enhanced resistance against relearning attacks, enhanced data efficiency even in the zero-shot setting and interpretable unlearning."
        },
        {
            "title": "2 Background and Related Work",
            "content": "2.1 Unlearning in Large Language Models Machine unlearning aims to modify trained target model M(D) to behave as if specific data, the forget set orget, had never been part of its training data (Bourtoule et al., 2021; Cao & Yang, 2015). The resulting model, Munlearn, should ideally be indistinguishable from model trained only on the retain set Dretain = orget. As retraining LLMs from scratch is computationally prohibitive, research focuses on approximate unlearning (Liu et al., 2025). These methods face the core challenge of balancing knowledge removal (forget quality) and maintaining general capabilities (utility preservation) (Shi et al., 2024; Maini et al., 2024). The dominant approach for approximate unlearning involves gradient-based optimization (Liu et al., 2024). Methods like Gradient Ascent (GA) (Jang et al., 2023), Gradient Difference (GradDiff) (Liu et al., 2022), Negative Preference Optimization (NPO) (Zhang et al., 2024), and RMU (Li et al., 2024) finetune model weights to reduce the influence of orget, often using regularization (e.g., KL divergence) to protect utility (Maini et al., 2024; Yao et al., 2024). However, these gradient-based techniques frequently suffer from significant limitations: high computational cost (requiring backward passes) (Cha et al., 2024), instability under hyperparameter tuning (Thaker et al., 2024; Bu et al., 2024), degraded performance under sequential unlearning requests (Gao et al., 2025; Shi et al., 2024), vulnerability to relearning attacks (Hu et al., 2024; Deeb & Roger, 2024; Łucki et al., 2024), poor data efficiency (Gao et al., 2024), and lack of interpretability (Barez et al., 2025; Xu et al., 2024). These widespread challenges motivate the exploration of alternative unlearning paradigms, such as the activation-based interventions explored in this work. 2.2 Sparse Autoencoders (SAEs) Modern DNNs operate in regime of superposition, where multiple features or capabilities are encoded along the same dimensions of hidden activations (Elhage et al., 2022). SAEs provide an unsupervised method for disentangling these superposed representations into interpretable features (Bricken et al., 2023; Cunningham et al., 2023). Given activations Rdmodel from specific layer or component of an LLM, an SAE decomposes and reconf (h) := σ(Wench + benc) structs these activations using encoder and decoder functions: and ˆh( ) := Wdec + bdec. In other words, SAEs express model activations as sparse linear combination of interpretable feature vectors: = ˆh + ε(h) = dSAE i=1 fi(h)vi + + ε(h) where fi(h) are scalar feature activations, vi Rdmodel are unit vector feature directions, Rdmodel is bias term, and ε(h) Rdmodel is the SAE error term. Wider SAEs continue to improve feature granularity and reduce the error term. SAEs are trained on activations collected from the model processing pretraining data where training is conducted separately for each layer or component of interest (e.g., residual stream, attention outputs) using an objective that minimizes reconstruction loss while en2 Preprint. Under review. forcing sparsity: = ˆh( (h))2 2 + λ (h)0. Here λ is sparsity penalty coefficient encouraging most feature activations to be zero for any given input. In this work, we use JumpReLU SAEs (Rajamanoharan et al., 2024), which enforce sparsity using shifted Heaviside step function. The interpretability of SAE features stems from their sparse activation patternbecause features are only active for small fraction of inputs, they must capture meaningful patterns to be useful for reconstruction. The cost of training SAEs is amortized across multiple downstream applications such as identifying and removing spurious correlations in models (Marks et al., 2024) and steering behavior (OBrien et al., 2024). 2.3 SAEs for Unlearning The interpretable nature of SAE features makes them promising candidate for targeted unlearning interventions. By identifying features that encode or mediate the knowledge intended for removal, one could potentially intervene directly on these features activations rather than resorting to less precise, global weight modifications. Farrell et al. (2024) explored this direction by developing an early SAE-based unlearning method. Their approach involved: (1) Identifying features based on their activation frequency (sparsity) on the forget set orget, while filtering out features that were also frequently active on retain set Dretain. (2) Implementing static intervention strategy during inference: whenever any of the identified forget-related features produced positive activation at given token, its value was clamped to fixed negative constant. However, because this static, token-level clamping was applied whenever target feature was activated, regardless of the overall context of the input sequence, the method suffered from degradation of the models general utility on tasks unrelated to the forget set. Consequently, this initial approach underperformed established gradient-based methods like RMU in achieving favorable balance between forget quality and utility preservation. The shortcomings of static clamping motivated our development of DSG, which introduces two key innovations: (1) more principled feature selection method grounded in Fisher Information and causal influence, aiming for better discrimination between forget-relevant and retain-relevant features, and (2) dynamic, sequence-level classification mechanism that applies the intervention conditionally, only when the input sequence as whole is deemed relevant to the forget knowledge. As we demonstrate in this work, this dynamic approach allows SAEs to achieve more effective unlearning. 2.4 Relearning Attacks significant challenge for approximate unlearning methods, particularly those modifying model weights, is their vulnerability to relearning attacks (Deeb & Roger, 2024; Łucki et al., 2024). Adversaries may recover forgotten information by further finetuning the unlearned model, sometimes even using tangentially related data (Hu et al., 2024). This suggests gradient-based weight modifications might primarily suppress, rather than erase, knowledge, leaving models susceptible if finetuning is allowed. Activation-based interventions like DSG represent fundamentally different approach, operating on internal representations during inference rather than permanently modifying weights. This distinction offers potential security advantages in the common API-based (black-box) threat model, where users can query the model but cannot access its internal parameters. First, effective circumvention techniques like obfuscation attacks (Bailey et al., 2024) typically require white-box access to model gradients and internal activations. Without this visibility, attackers in API-based settings find it difficult to craft inputs that bypass activation monitoring while still eliciting harmful outputs. Second, even when considering conventional relearning attacks through API-permitted finetuning, DSG shows improved resilience. This resilience stems from the Superficial Alignment Hypothesis (Zhou et al., 2023), which posits that activation patterns remain relatively stable during finetuning while weights change more significantly. By targeting these stable activation structures rather than the more malleable weights, DSG is more resistant to standard finetuning-based relearning attacks. Preprint. Under review."
        },
        {
            "title": "3 Dynamic SAE Guardrails (DSG)",
            "content": "DSG (Figure 1, Algorithm 1) is targeted unlearning method for LLMs that leverages the interpretability of SAEs and combines: (1) causal framing that motivates feature selection, (2) theoretically justified feature importance scoring based on Fisher Information (FI), (3) dynamic, input-dependent classification rule based on statistically optimal threshold, and (4) targeted clamping intervention to remove the influence of selected features. 3.1 Causal Framework and Problem Setup Figure 1: An illustration of DSG We frame unlearning through causal perspective where Dforget and Dretain influence model representations and outputs through multiple pathways (Shen et al., 2024). These include representation-mediated pathways (D Y), potential direct influence (D Y), and intertwined knowledge (Dforget Dretain) where conceptual overlap exists between the datasets. SAE features fj derived from serve as interpretable mediators (Pearl, 2009) of information flow through these causal pathways. From this perspective, unlearning involves blocking pathways from Dforget to while preserving the pathways from Dretain to Y. DSG implements this causal intervention do( fj = c) on forget features identified by analyzing SAE activation patterns across both datasets. 3.2 Feature Selection: Identifying Causal Mediators via Fisher Information 2] ϵ2E To identify which SAE features, Fforget mediate the causal influence of Dforget, we establish two key theoretical connections: first between FI and feature activation, and second between FI and causal influence. We then describe our percentile based feature selection. Theorem 1 (Fisher Information Approximation). For an SAE with small reconstruction error and input h, the expected squared gradient of reconstruction loss with respect to feature js decoder weights θj, is proportional to the second moment of that features activation: h[θi, ℓ(h)(cid:13) (cid:13) The proof of Theorem 1 is provided in Appendix B. This establishes that squared feature activations are proportional to the Fisher Information of the corresponding decoder weights. Theorem 2 (Fisher Information as Proxy for Causal Feature Importance). Under standard assumptions, Fisher Information associated with SAE features provides an approximation of their causal influence as mediators between specific training data and model outputs. For any SAE feature fj, the expected squared activation ED[ fj(h)2] on dataset is proportional to the causal influence of that feature as mediator of information from to model outputs. h[ fj(h)2] where w.h.p, reconstruction errors are bounded by ϵ2. Proof of Theorem 2 is in Appendix C. Under these assumptions, feature with large expected squared activation on Dforget contributes significantly to the models FI with respect to that data. This suggests that intervening on that feature (e.g., clamping its activation) would substantially affect the models output distribution when processing inputs similar to those in Dforget. Squared activation serves as computationally tractable proxy for causal influence. Importance Scores. DSG obtains token-level SAE activations from each sequence in both Dforget and Dretain, squares them, and aggregates the results into matrices Aforget RnFdSAE and Aretain RnRdSAE (nF and nR are the total numbers of tokens in the respective datasets). For each token in sequence x, we have the activation fj(hx,t) of feature on the hidden state hx,t. Each entry of the activation matrices is thus Aforget[i, j] (cid:2) fj(hx,t)(cid:3)2 for token in sequence Dforget (and similarly for Aretain). From these, we compute the average squared activation per feature as forget score(j) = 1/nF xDforget and (cid:2) fj(hx,t)(cid:3)2 t= 4 Preprint. Under review. retain score(j) = 1/nR xDretain imp ratio(j) = ratio represents the relative causal influence of feature j. , and define the relative importance by max{retain score(j),ε} , with ε > 0 to avoid division by zero. By Theorem 2 this forget score(j) t= (cid:2) fj(hx,t)(cid:3)2 Algorithm 1 Dynamic SAE Guardrails (DSG) Require: LLM with SAE features { fj}; datasets Dforget, Dretain; clamp strength c; percentiles (pratio, pdyn); feature count nfeats Feature Selection: Compute feature importance scores and threshold τratio from percentiles Identify Fforget = {j : imp ratio(j) τratio} Sort Fforget by descending forget score(j) and select top nfeats features to form Snfeats Dynamic Threshold Calibration: Compute ρ(x) = 1 Set threshold τ = Percentile({ρ(x)}xDretain , pdyn) 1[j Snfeats : fj(ht) > 0] for each Dretain Inference-Time Intervention: For input sequence x, compute ρ(x) and classify as forget-relevant if ρ(x) > τ If forget-relevant: For each token and feature Snfeats , set Otherwise: Preserve all feature activations (ht) = Percentile-Based Feature Selection. To select features most causally relevant to Dforget, we employ percentile-based approach using pratio to compute τratio as Percentile({imp ratio(j)}dSAE j=1 , pratio). Percentile(S, p) returns the value such that p% of elements in set are less than or equal to v. For example, with pratio = 95, τratio is set so 95% of features have imp ratio(j) τratio. We define the set of forget-mediating features as Fforget = {j : imp ratio(j) τratio}. To filter out noisy features, we sort features in Fforget by descending forget score(j) and select the top nfeats to form the final intervention set Snfeats. 3.3 Dynamic Sequence-Level Classification and Intervention DSG employs dynamic input-dependent classification mechanism to minimize unintended side-effects on content unrelated to the forget knowledge. Definition 1 (Forget-Set Activated Token). token xt is considered forget-set activated if at least one feature Snfeats has positive activation: fj(ht) > 0. sequence = (x1, . . . , xT) of length T, we define the statistic For an input ρ(x) = 1 fj(ht) > 0], representing the percentage of forget-set actiT vated tokens. high ρ(x) indicates that query strongly relies on features weve identified as causally linked to the forget knowledge. t=1 1[ Snfeats : Threshold Selection and Classification. We select threshold τ [0, 1] based on the distribution of ρ(x) on Dretain using τ = Percentile({ρ(x)}xDretain, pdyn) which controls the trade-off between unlearning effectiveness and performance preservation. Empirically, we find ρ(x) is stochastically larger on Dforget than on Dretain, as seen in Figure 2, which shows the distribution for both forget-domain queries (WMDP-Bio) and general knowledge queries (MMLU). τ is chosen to control the retain sets false-positive rate and separates forget-set queries effectively, achieving high recall on Dforget. On this example, DSG successfully Figure 2: Distribution of ρ(x) for unlearning on WMDP-Bio. Threshold at 95th percentile (dashed red line) separates MMLU from WMDP. 5 Preprint. Under review. transfers from retain set (WikiText) and forget set to the test query set. We define classifier C(x) = 1[ρ(x) > τ], labeling inputs as either forget-relevant or retain-relevant. Conditional Clamping. Our intervention is conditional on the classifier C(x). When C(x) = 1 (forget-relevant), for each token xt and feature Snfeats, we set (ht) = c, where is large negative constant we call clamp strength. This implements targeted do( fj(ht) = c) operation, selectively severing the causal pathway only when the input query is deemed forget-relevant. When C(x) = 0 (retain-relevant), we leave all features unchanged: (ht) = fj(ht). This preserves the models original behavior for queries unrelated to the targeted knowledge, minimizing side-effects and maintaining performance on Dretain. Theorem 3 (Neyman-Pearson Optimality). If ρ(X) is stochastically larger under Dforget than under Dretain, then among all classifiers with false-positive rate at most α, the threshold test C(x) = 1[ρ(x) > τ], where PrXDretain [ρ(X) > τ] = α, maximizes the true-positive rate. The proof appears in Appendix and states that under the stochastic dominance assumption, thresholding ρ(x) is the optimal classification approach for given false-positive rate. The dynamic clamping in DSG contrasts with static clamping methods (Farrell et al., 2024), which intervene based only on feature activation without sequence-level classification, and risk inadequate coverage on Dforget or excessive side-effects on Dretain. DSG avoids this suboptimal trade-offwe formally prove (Theorem 4, Appendix E) that for any static approach, DSG achieves equal or greater coverage on Dforget with equivalent side-effects on Dretain, providing superior unlearning-utility trade-off."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Unlearning on WMDP We evaluate DSG on the WMDP dataset (Li et al., 2024), which benchmarks hazardous knowledge unlearning across multiple domains. We focus on WMDP-Bio (biosecurity) and WMDP-Cyber (cybersecurity). For each domain, our unlearning setup uses domain-specific DforgetPubMed papers containing bio-weapon related content for WMDP-Bio and GitHub repositories for WMDP-Cyberand WikiText (Merity et al., 2016) as Dretain. We evaluate unlearning effectiveness using the WMDP multiple-choice question test sets, which were not exposed to models during the unlearning process. Following SAEBench (Karvonen et al., 2025), we evaluate unlearning only on questions the target model correctly answers across all 24 permutations of the 4 multiple-choice options. This yields 522/1273 questions for WMDP-Bio and 275/1987 questions for WMDP-Cyber. For evaluating model utility, we similarly filter MMLU questions that the model answers correctly across all permutations. This yields 305 questions from history, computer science, geography, and human aging for WMDP-Bio. For WMDP-Cyber, we use 371 MMLU questions, replacing computer science with biology. Table 1 reports the configuration that minimizes WMDP accuracy while maintaining at least 99% of the target model MMLU accuracy along with with MT-Bench scores that measure general fluency. implement DSG using gemma-2-2b-it model with Experimental Setup. We gemma-scope-2b-pt-res SAE (width 16k) (Lieberum et al., 2024) applied to layer 3 at ℓ0 142. We use Pdyn = 95 for both domains, and Pratio = 95 for WMDP-Bio and Pratio = 90 for WMDP-Cyber. We compare DSG against several baselines across broad hyperparameter sweep: GA (Jang et al., 2023), NPO (Zhang et al., 2024), SSD (Foster et al., 2024), SCRUB (Kurmanji et al., 2023),Farrell et al. (2024) and RMU (Li et al., 2024). Complete hyperparameter details are provided in Appendix G. Results. As shown in Table 1, DSG significantly outperforms all baselines on the WMDPBio unlearning task, reducing accuracy to 29.64% compared to the next best method RMU at 50.00%. It maintains high MMLU performance (99.34% average) and achieves the highest MT-Bench score (7.78), showing superior preservation of general model capabilities. The Preprint. Under review. Figure 3: Unlearning performance on WMDP-Bio (left) and WMDP-Cyber (right). Higher MMLU accuracy and lower WMDP accuracy is better. Clamp strengths (c) used for DSG points are shown as annotations. DSG Pareto-dominates the top four baseline methods (RMU, SCRUB, Farrell et al., SSD). Method WMDP Bio () MMLU () Target GA NPO SSD SCRUB Farrell et al. RMU DSG (Ours) HS Hist C. CS HS Geo H. Aging All 100.00 100.00 100.00 100.00 99.44 97.95 99.44 94.97 59.22 50. 29.64 98.18 99.99 100.00 99.09 100.00 99.08 100.00 88.88 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100. 100.00 100.00 100.00 98.82 98.82 98.82 96.47 98.81 97.62 100.00 99.35 99.35 99.68 99.35 99.03 99. 99.34 MT () 7.36 7.44 7.29 7.24 6.09 7.33 7.21 7.78 Table 1: Unlearning performance on WMDP-Bio. All represents the average MMLU score. MT-Bench scores show 0.16 variance across 5 runs. DSG shows superior unlearning effectiveness compared to baselines while maintaining high MMLU performance. results on WMDP-Cyber (Appendix G.2) reinforce these findings. Figure 3 provides more comprehensive view of the unlearning-utility trade-off landscape, plotting all configurations with MMLU accuracy above 95%. DSG Pareto-dominates all baseline methods: for any level of utility preservation (MMLU accuracy), DSG achieves more effective unlearning. This superior performance is coupled with significant practical advantages over gradientbased methods in terms of computational efficiency and hyperparameter stability. Gradient-based approaches often exhibit hyperparameter instability, where slight tuning changes can drastically alter outcomes, risking poor unlearning or utility collapse. Furthermore, they require computationally costly backward passes through the LLM for optimization. In contrast, DSG shows greater hyperparameter stability (Figure 3) and efficiency. It requires only forward passes: one to gather feature statistics initially, and then lightweight intervention during inference, completely avoiding expensive gradient calculations. This combination of efficiency and stability makes DSG particularly advantageous for large models and frequent unlearning where gradient computations impose substantial overhead. 4.2 Unlearning on Muse We evaluate DSG on MUSE (Shi et al., 2024) comprising two corpora: NEWS and BOOKS, and focusing on six dimensions: verbatim memorization, knowledge memorization, privacy leakage, utility preservation, forget set scalability, and sequential unlearning. Experimental Setup. We create separate target models for NEWS and BOOKS by finetuning gemma-2-2b-it on each corpus for 5 epochs using learning rate 105 and batch size 32. For each target model, we implement DSG using gemma-scope-2b-pt-res SAE (width 16k) applied to layer 3. For both domains, we use clamp strength 500, pratio = 95 and nfeats = 20. We use pdyn = 90 for NEWS and pdyn = 95 for BOOKS, with the lower threshold for NEWS Preprint. Under review. C1. No Verbatim Mem. VerbMem on Dforget () C2. No Knowledge Mem. KnowMem on Dforget () C3. No Privacy Leak. PrivLeak ( [5%, 5%]) C4. Utiltiy Preserv. KnowMem on Dretain () Target GA GradDiff NPO SimNPO RMU DSG (Ours) Target GA GradDiff NPO SimNPO RMU DSG (Ours) 21.15 0.62 2.81 20.98 21.14 9.60 11.80 15. 2.61 9.49 14.41 14.55 14.89 8.73 97.1% 86.7% 0.8% 0.0% 54.6% 44.2% 83.5% 39.9% 8.8% 7.9% 5.8% 44.7% 29.51 0.00 0.71 25.14 27.70 26.63 0.44 33. 0.17 21.57 28.21 34.36 32.59 1.79 NEWS 100.0% 97.6% 14.8% 6.1% 9.8% 98.5% BOOKS 99.5% 36.4% 16.8% 1.4% 3.9% 94.7% 88. -8.16 93.10 -53.42 -89.84 75.02 12.08 98.80 -1.58 -10.30 -97.24 -96.40 -97.58 -23.18 under-unlearn over-unlearn under-unlearn under-unlearn over-unlearn over-unlearn acceptable under-unlearn under-unlearn under-unlearn under-unlearn under-unlearn 26. 0.09 7.76 29.02 30.59 26.41 25.65 35.28 0.57 23.66 37.19 36.62 37.13 37.10 99.7% 71.0% 8.4% 14.2% 1.4% 4.2% 98.4% 32.9% 5.4% 3.8% 5.2% 5.2% Table 2: Unlearning performance on MUSE. We highlight in green if the method satisfies the criterion and red otherwise. For privacy leakage, large positive values suggest over-unlearning, while large negative values suggest under-unlearning. DSG shows strong performance across all metrics, achieving substantial reductions in verbatim and knowledge memorization while maintaining high utility. enabling more effective verbatim memorization removal. For both scalability and sequential unlearning, we use the best NEWS hyperparameters. We compare DSG against: GA, GradDiff (Liu et al., 2022), NPO, SimNPO (Fan et al., 2024), and RMU. Following MUSE, we train for 10 epochs using AdamW with learning rate 105 and batch size 32, selecting the last epoch checkpoint before utility falls below 90% of the target model accuracy. Complete hyperparameters can be found in Appendix H. Unlearning. Table 2 shows that DSG outperforms existing baselnes. It is effective at verbatim memorization removal (C1) with 44.2% reduction on NEWS and 44.7% on BOOKS. On knowledge memorization (C2), DSG achieves near-complete removal with 98.5% reduction on NEWS and 94.7% reduction on BOOKS, outperforming most baselines. On privacy leakage (C3), while not within the ideal range, DSG performs better than the majority of baselines. For utility preservation (C4), DSG maintains 95.8% of target model performance on NEWS and achieves 5.2% improvement on BOOKS compared to the target model. Figure 4: (a) Scalability: Performance across increasing forget set sizes. (b) Sequential Unlearning: Performance across sequential unlearning requests Scalability. Figure 4(a) shows DSG is stable and robust when scaling to larger forget sets. We evaluate performance across forget sets ranging from 0.8M to 3.3M tokens, and DSG maintains its position in the ideal region (high retain set knowledge, low forget set knowledge) even as the forget set size increases. In contrast, gradient-based methods exhibit substantial degradation, with increasingly poor tradeoffs between retaining general knowledge and forgetting targeted information. Sequential Unlearning. Figure 4(b) illustrates DSGs effectiveness across sequential unlearning requests on four disjoint NEWS folds. We implement two approaches: DSGall, 8 Preprint. Under review. which cumulatively updates feature importance scores based on each new forget data request; and DSGunion, which takes the union of features selected independently at each step and uses this combined set to calculate ρ(x) and threshold τ on DR. Both approaches perform similarly well, consistently maintaining DSG in the ideal region where other methods rapidly degrade with each additional unlearning operation. Gradient-based methods suffer from catastrophic forgetting during sequential unlearning, where each update pushes the model further from its original performance distribution. (Details in subsection H.2.) 4.3 Resistance to Relearning Attacks Figure 5: Relearning attack resistance across finetuning epochs. (a) DSG demonstrates superior resistance to relearning compared to RMU. (b) Test-time DSG preserves MMLU utility better than Train-time DSG while still providing significant protection. We evaluate DSGs resistance to relearning attacks in API-based threat models where adversaries have query access but cannot directly manipulate model weights. This resistance derives from the Superficial Alignment Hypothesis (Zhou et al., 2023), which posits that models activation geometry stabilizes during pretraining and changes minimally during finetuning. Figure 9a confirms this empirically, showing high cosine similarity between pre-finetuning and post-finetuning activation vectors, and activation magnitudes clustered around 1.0. By operating on these stable activation patterns rather than weights, DSG creates more persistent defense. While obfuscation-based attacks have been proposed against activation-based interventions (Bailey et al., 2024), they are less effective in API-based black-box settings where attackers lack direct access to gradients and model representations. Methodology. We evaluate two DSG defenses against relearning: (1) Test-time DSG, which applies intervention only at inference time after model finetuning, and (2) Train-time DSG, which integrates DSG during finetuning with frozen SAE parameters to filter gradients. We test six configurations with google/gemma-2-2b-it as base model: Base, Base+Test-time DSG, Base+Train-time DSG, Base+Train-time DSG+Test-time DSG, RMU (base model with RMU unlearning applied), and RMU+Test-time DSG. The relearning attack consists of finetuning each configuration on the WMDP-Bio test set for 10 epochs at learning rate 1e-5. Results and Analysis. Figure 5(a) demonstrates clear differences in vulnerability to relearning attacks. Weight-based methods show high susceptibility, with RMU rapidly increasing in WMDP-Bio accuracy when finetuned, eventually exceeding the base models finetuned performance. The base model itself shows an initial performance decrease before increasing, as the high learning rate temporarily undoes instruction tuning before relearning occurs. Test-time DSG provides substantial protection, with RMU+Test-time DSG maintaining near-random performance (25%) throughout training. However, Base+Test-time DSG shows gradual vulnerability to relearning, with performance slowly increasing over finetuning epochs. This gradual protection erosion reveals limitation of test-time intervention alone. Train-time DSG offers distinct protective mechanism. Models finetuned with DSG active show immediate reduction to random-level performance that persists through approximately six epochs before gradually recovering. This delayed recovery pattern suggests DSG forces the model to develop entirely new processing circuits rather than simply reactivating 9 Preprint. Under review. suppressed knowledge. Figure 9b supports this interpretation, showing significantly higher training loss on WMDP-Bio compared to MMLU when finetuning with DSG active. Combining both approaches (Train-time DSG+Test-time DSG) extends resistance through epoch 7, demonstrating how these complementary mechanisms can be layered for enhanced protection. However, these approaches involve utility trade-offs. Figure 5(b) shows that while Base Finetuned and Base Finetuned+Test-time DSG maintain comparable MMLU performance, Train-time DSG exhibits moderate utility decline at higher epoch counts. DSGs superior resistance to relearning attacks stems from its activation-based intervention that leverages the stability of activation geometry during finetuning. 4.4 Data Efficiency and Zero-shot Interpretable Unlearning Figure 6: Data efficiency analysis of DSG. (A) Performance across varying training data sizes compared to RMU. (B) Zero-shot performance on WMDP-Bio (left) and WMDP-Cyber (right) using 20 features selected via Neuropedia API with different τ thresholds (shown next to each data point). We evaluate how DSG performs with limited forget and retain data on WMDP-Bio. Figure 6A shows DSG maintaining consistent performance when trained on 20-80% of the original retain and forget datasets, preserving MMLU accuracy while keeping WMDP accuracy below 40%. Only when dataset size falls below 20% does effectiveness noticeably decline, with WMDP accuracy rising above 40%. In contrast, RMU shows inconsistent results across different dataset sizes, indicating that gradient-based methods may be more unstable to hyperparameter changes when data is limited. For zero-shot evaluation, we implement DSG without any domain-specific forget or retain data (Figure 6B), instead leveraging the interpretability of SAEs. We use Neuropedia (Lin, 2023) feature explanations to identify the forget set features by querying for concepts Biology and Cybersecurity, selecting the top 20 relevant features (details in Appendix M). Both tasks use the gemma-scope-2b-pt-res SAE (width 16k) at layer 3 (ℓ0 59). Since retain data is unavailable for dynamic threshold calibration, we sweep over static τ values, finding optimal settings (τ = 60% for WMDP-Bio, τ = 20% for WMDP-Cyber). Even with features selected purely based on their semantic descriptions and without dataset-specific tuning beyond τ, these zero-shot DSG configurations outperform RMU and Farrell et al. (2024), demonstrating the potential for effective unlearning guided directly by feature interpretability. 4.5 Ablations We evaluate each component of DSG by conducting ablation experiments on WMDP-Bio. Dynamic Classification: Figure 7A compares DSG with dynamic classification against DSG with static clamping from Farrell et al. (2024) While static clamping effectively removes forget-set information at large clamp values (c > 100), it simultaneously reduces MMLU accuracy because it treats all inputs identically regardless of their forget-relevance. In contrast, our dynamic classifier only applies interventions when necessary based the statistical distribution of forget-feature activations. This conditional approach maintains higher MMLU accuracy (> 99%) while achieving comparable or better WMDP-Bio reduction. 10 Preprint. Under review. Figure 7: DSG Ablation studies (A) Static vs. dynamic clamping comparison with varying clamp strengths [10-500] for 20 and 30 features. (B) Effect of dynamic threshold percentile (pdyn) on performance (C) Impact of importance ratio threshold (pratio, range 75-95) for 20 and 30 features. Percentile-Based Feature Selection: DSG with static clamping leverages Fischer Information for feature selection instead of feature sparsity in Farrell et al. (2024). As shown in Figure 7A, across equivalent clamping strengths, this selection approach achieves 8% lower WMDP-Bio accuracy on average while maintaining comparable MMLU performance, indicating more precise identification of forget-relevant features. Dynamic Threshold pdyn: Figure 7B shows the effect of pdyn on the forget-retain trade-off. Higher percentiles (> 95) preserve more MMLU accuracy but allow more WMDP content to pass through undetected, while lower percentiles (< 90) apply intervention more aggressively but with increased side effects on general knowledge. The optimal range 90-95 balances these considerations, removing targeted knowledge while minimizing side effects. Importance Ratio Threshold pratio: As shown in Figure 7C, varying pratio from 75-95 provides fine-grained control over feature selection. Higher values (95) select features with stronger forget-retain differentiation, yielding more targeted intervention, while lower values expand the feature set but may increase overlap with general knowledge features. Additionally we observed that the dynamic classifier can compensate for lower pratio maintaining effective forget-set filtering even when feature selection is less discriminative. Additional ablations in Appendix show that DSG is remarkably robust to clamp strength variations, and performs optimally with moderate feature counts. These findings highlight DSGs practical hyperparameter stability. Effective performance is maintained within reliable ranges for thresholds pdyn/pratio (90-95), feature counts (10-20), alongside notable robustness to clamp strength (100-500). Additionally, these hyperparameters transfer across datasets, simplifying deployment compared to gradient-based methods."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work we introduced DSG, demonstrating that SAEs with dynamic classification enable precise, activation-based unlearning that outperform gradient-based methods across multiple benchmarks. Future directions include studying how DSG generalizes across different SAE widths, base model sizes, and configurations."
        },
        {
            "title": "6 Acknowledgements",
            "content": "This work was supported in part by the National Science Foundation grants IIS2145670 and CCF2107024, and funding from Amazon, Apple, Google, Intel, Meta, the CyLab Security and Privacy Institute, and Leonardo Labs. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies."
        },
        {
            "title": "Ethics Statement",
            "content": "This work introduces Dynamic SAE Guardrails (DSG), method for targeted unlearning in large language models (LLMs). While designed to promote responsible AI by enabling the removal of unwanted knowledge, several ethical considerations arise: 11 Preprint. Under review. Potential for misuse: While our focus is on removing hazardous or unwanted knowledge, the same technology could potentially be used to censor information or suppress viewpoints, leading to undesirable social consequences if deployed without careful oversight. The zero-shot capabilities, while advantageous for data-scarce scenarios, could be misused if the user-provided keywords are biased or used to target specific groups/content unfairly. Over-reliance on interpretability: Although SAEs offer improved interpretability compared to black-box models, feature interpretations are not always definitive or fully reliable. Misinterpreting feature roles or over-relying on imperfect interpretations could lead to unintended consequences, including the removal of valuable knowledge or the failure to remove harmful content. The quality of feature interpretation depends on the quality and representativeness of the data used to train and interpret the SAE. Limitations of unlearning: As with all approximate unlearning methods, DSG does not guarantee complete removal of targeted knowledge. As we show, it reduces the likelihood of the model generating outputs related to the forget set, but subtle traces or indirect influences might persist. It is essential to acknowledge these limitations and avoid presenting DSG as perfect solution for knowledge removal. Dual-use concerns: The techniques developed in this work for improving model control and safety could also be adapted by malicious actors to develop more sophisticated attacks or to create models that resist safety interventions. We recognize this inherent dual-use nature and emphasize the need for responsible development and sharing of research findings. Computational Cost of SAE Training: The training of SAEs can be computationally demanding, raising environmental concerns. However there are several open-source SAEs, amortizing their cost, and the the inference-time efficiency of DSG offers some mitigation compared to gradient-based unlearning approaches. We believe the benefits of precise, controllable unlearning for enhancing AI safety outweigh these risks, provided the technology is developed and deployed responsibly. We encourage future work to address these limitations and explore more robust evaluation methods for unlearning."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To facilitate reproducibility, we have provided detailed descriptions of our experimental setups, including all relevant datasets, models, and hyperparameters. Specifics for each set of experiments can be found as follows: WMDP Unlearning: Complete hyperparameter settings for DSG and all baseline methods (GA, NPO, SSD, SCRUB, Farrell et al., RMU) are detailed in Appendix G.1. Model and SAE details are provided in Section 4.1. MUSE Unlearning: Hyperparameters for DSG and baseline methods (GA, GradDiff, NPO, SimNPO, RMU) are in Appendix H.1, with model details in Section 3. Relearning Attacks (Section 4.3): Model and hyperparameter configurations for the relearning experiments, including both train-time and test-time DSG interventions, are given in Appendix I.5, along with the details in the main text. Data Efficiency and Zero-shot Experiments: Model, SAE, and hyperparameter details for the data efficiency analysis and zero-shot evaluations are given in Appendix J. Ablations: All details related to the ablation studies, and chosen hyperparameters are in Appendix K. We have described the feature selection process, dynamic classification rule, and intervention mechanism in sufficient detail to allow for reimplementation (Algorithm 1 and Section 3). We intend to release the code and relevant scripts necessary to reproduce our results upon acceptance of the work. This will include implementations of DSG, the baseline methods, and the evaluation pipelines. 12 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Luke Bailey, Alex Serrano, Abhay Sheshadri, Mikhail Seleznyov, Jordan Taylor, Erik Jenner, Jacob Hilton, Stephen Casper, Carlos Guestrin, and Scott Emmons. Obfuscated activations bypass llm latent-space defenses. arXiv preprint arXiv:2412.09565, 2024. Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan OGara, Robert Kirk, Ben Bucknall, Tim Fist, et al. Open problems in machine unlearning for ai safety, 2025. Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 141159. IEEE, 2021. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac HatfieldDodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html. Zhiqi Bu, Xiaomeng Jin, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, and Mingyi Hong. Unlearning as multi-task optimization: normalized gradient difference approach with an adaptive learning rate. arXiv preprint arXiv:2410.22086, 2024. Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pp. 463480. IEEE, 2015. Sungmin Cha, Sungjun Cho, Dasol Hwang, and Moontae Lee. Towards robust and costefficient knowledge unlearning for large language models. arXiv preprint arXiv:2408.06621, 2024. Pavel Chvykov and Erik Hoel. Causal geometry. Entropy, 23(1):24, 2020. Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Aghyad Deeb and Fabien Roger. Do unlearning methods remove information from language model weights? arXiv preprint arXiv:2410.08827, 2024. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. Chongyu Fan, Jiancheng Liu, Licong Lin, Jinghan Jia, Ruiqi Zhang, Song Mei, and Sijia Liu. Simplicity prevails: Rethinking negative preference optimization for llm unlearning. arXiv preprint arXiv:2410.07163, 2024. Eoin Farrell, Yeu-Tong Lau, and Arthur Conmy. Applying sparse autoencoders to unlearn knowledge in language models. In NeurIPS 2024 Safe Generative AI Workshop, 2024. URL https://arxiv.org/abs/2410.19278. Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 1204312051, 2024. Chongyang Gao, Lixu Wang, Chenkai Weng, Xiao Wang, and Qi Zhu. Practical unlearning for large language models. arXiv preprint arXiv:2407.10223, 2024. 13 Preprint. Under review. Chongyang Gao, Lixu Wang, Kaize Ding, Chenkai Weng, Xiao Wang, and Qi Zhu. On large language model continual unlearning, 2025. URL https://arxiv.org/abs/2407.10223. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, and Virginia Smith. Jogging the memory of unlearned llms through targeted relearning attacks. arXiv preprint arXiv:2406.13356, 2024. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1438914408, 2023. Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, YeuTong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, et al. Saebench: comprehensive benchmark for sparse autoencoders in language model interpretability. arXiv preprint arXiv:2503.09532, 2025. Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. Advances in neural information processing systems, 36: 19571987, 2023. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning. In International Conference on Machine Learning, 2024. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147, 2024. Johnny Lin. Neuronpedia: Interactive reference and tooling for analyzing neural networks, 2023. URL https://www.neuronpedia.org. Software available from neuronpedia.org. Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, et al. Rethinking machine unlearning for large language models. In International Conference on Learning Representations, February 2024. URL https://openreview.net/forum?id=PLz7eW1K82. Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, et al. Rethinking machine unlearning for large language models. Nature Machine Intelligence, pp. 114, 2025. Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. Backdoor defense with machine unlearning. In IEEE INFOCOM 2022-IEEE conference on computer communications, pp. 280289. IEEE, 2022. Jakub Łucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tram`er, and Javier Rando. An adversarial perspective on machine unlearning for ai safety. arXiv preprint arXiv:2409.18025, 2024. Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary Lipton, and Zico Kolter. Tofu: task of fictitious unlearning for llms, 2024. Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. 14 Preprint. Under review. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Kyle OBrien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough Poursabzi-Sangde. Steering language model refusal with sparse autoencoders. arXiv preprint arXiv:2411.11296, 2024. Judea Pearl. Causality. Cambridge university press, 2009. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, Janos Kramar, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024. Shaofei Shen, Chenhao Zhang, Alina Bialkowski, Weitong Chen, and Miao Xu. Camu: disentangling causal effects in deep model unlearning. In Proceedings of the 2024 SIAM International Conference on Data Mining (SDM), pp. 779787. SIAM, 2024. Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A. Smith, and Chiyuan Zhang. MUSE: Machine unlearning six-way evaluation for language models. In Conference on Language Modeling Research, July 2024. URL https://openreview.net/forum?id=gAfnQ8o9Cq. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pp. 318. IEEE, 2017. Pratiksha Thaker, Shengyuan Hu, Neil Kale, Yash Maurya, Zhiwei Steven Wu, and Virginia Smith. Position: Llm unlearning benchmarks are weak measures of progress. arXiv preprint arXiv:2410.02879, 2024. Jie Xu, Zihan Wu, Cong Wang, and Xiaohua Jia. Machine unlearning: Solutions and challenges. IEEE Transactions on Emerging Topics in Computational Intelligence, 2024. Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. Machine unlearning of pre-trained large language models, 2024. Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023."
        },
        {
            "title": "A Additional Background and Related Work Details",
            "content": "This appendix provides further details on concepts mentioned in the Background and Related Work (Section 2). 15 Preprint. Under review. A.1 Formal Goal of Unlearning As introduced in the main text, machine unlearning aims to transform model M(D), initially trained on dataset = Dretain orget, into an unlearned model Munlearn. The theoretical ideal of exact unlearning requires that Munlearn be computationally indistinguishable from model M(Dretain) that was trained exclusively on the retain set Dretain from the beginning (Bourtoule et al., 2021; Cao & Yang, 2015). Due to the computational cost of retraining large language models from scratch, achieving exact unlearning is generally impractical. Therefore, the field primarily focuses on developing approximate unlearning methods. These methods aim to satisfy specific criteria related to effectively removing the influence of orget while preserving the models performance on Dretain, without incurring the cost of full retraining (Liu et al., 2025). A.2 Unlearning Evaluation Metrics and Benchmarks The evaluation of approximate unlearning methods typically involves measuring two primary aspects: Forget Quality and Utility Preservation. Forget Quality quantifies the successful removal of information pertaining to the forget set orget. Common metrics include measuring the Forget Set Performance Degradation, which involves observing reduced accuracy or increased loss on tasks specifically related to the content of orget (Shi et al., 2024; Maini et al., 2024). Memorization Metrics gauge the models reduced ability to recall specific sequences or knowledge points verbatim from orget (Shi et al., 2024). Privacy Leakage Metrics evaluate the decreased success rate of Membership Inference Attacks (MIAs) that try to infer whether given data point was part of the original orget, often quantified using the Area Under the Curve (AUC) of the MIA classifier (Shokri et al., 2017; Shi et al., 2024). Conversely, Utility Preservation assesses how well the unlearned model retains its general knowledge and capabilities on tasks unrelated to orget. This is commonly measured by evaluating Retain Set Performance Preservation, which checks for maintained accuracy on standard academic or commonsense reasoning benchmarks such as MMLU (Hendrycks et al., 2020). Additionally, General Language Modeling Performance is often assessed by ensuring minimal increase in perplexity or loss when the model processes large, generalpurpose text corpora like OpenWebText (Gokaslan et al., 2019) or WikiText (Merity et al., 2016). Finally, Fluency and Coherence of the models generated text are important, often evaluated through automated metrics, human judgment, or interaction with benchmark chatbots like MT-Bench (Zheng et al., 2023). Standardized benchmarks like MUSE (Shi et al., 2024), TOFU (Maini et al., 2024), WMDP (Li et al., 2024), and SAEBench (Karvonen et al., 2025) provide datasets, tasks, and evaluation protocols designed to measure performance across these diverse criteria. A.3 Gradient-Based Unlearning Methods Gradient-based unlearning techniques directly modify the weights θ of the original model M(D) using optimization algorithms, typically variants of gradient descent or ascent. Gradient Ascent (GA) represents basic approach where the optimization objective is to maximize the loss function (e.g., negative log-likelihood) on the forget set orget, thereby pushing the model parameters away from configurations that accurately represent this data (Jang et al., 2023). This method, however, often suffers from catastrophic forgetting of useful knowledge if not carefully regularized. Gradient Difference (GradDiff or NegGrad) attempts to balance forgetting and retention by computing gradients for both minimizing loss on Dretain and maximizing loss on orget, then applying an update based on combination (often subtraction) of these gradients (Liu et al., 2022). Negative Preference Optimization (NPO) leverages insights from preference-based finetuning methods like DPO (Rafailov et al., 2023), reformulating unlearning as learning to 16 Preprint. Under review. disprefer outputs related to orget relative to some reference, which could be outputs from the original model or data from Dretain (Zhang et al., 2024). Simplified variants like SimNPO aim to reduce the computational overhead (Fan et al., 2024). Representation Misdirection Unlearning (RMU) operates by injecting noise or applying targeted shifts to the internal activations of the model at specific layers, but only when processing inputs related to orget, while simultaneously using regularization term to keep activations on Dretain close to those of the original model (Li et al., 2024). Selective Synaptic Dampening (SSD) aims for more targeted weight modification by estimating the importance of individual parameters for both orget and Dretain (using approximations based on Fisher information) and then selectively reducing the magnitude of parameters found to be more critical for orget than for Dretain (Foster et al., 2024). SCRUB employs student-teacher knowledge distillation framework; it trains copy of the original model (the student) to diverge from the original frozen model (the teacher) on orget inputs (typically by maximizing KL divergence) while simultaneously encouraging the student to mimic the teacher on Dretain inputs (by minimizing KL divergence) (Kurmanji et al., 2023). Finally, many gradient-based methods incorporate explicit Regularization Techniques to counteract the tendency towards catastrophic forgetting. Common regularizers include minimizing the KL divergence between the probability distributions of the unlearned and original models when evaluated on Dretain (Maini et al., 2024), or directly including term in the loss function that minimizes the models prediction error on Dretain (Yao et al., 2024). A.4 Prior SAE Unlearning Work The work by Farrell et al. (2024) is an early exploration into using Sparse Autoencoders (SAEs) for machine unlearning. We describe their methodology here. First, they computed the activation sparsity for each feature in the SAE dictionary, calculated separately over the forget dataset (D orget) and the retain dataset (Dretain). Sparsity here refers to the fraction of input tokens for which given feature has non-zero activation. Second, to mitigate potential damage to the models general capabilities, they filtered out any features whose activation sparsity on the retain set Dretain exceeded predetermined threshold (e.g., feature active on more than 1% of retain tokens might be excluded). Third, from the pool of features that passed the retain-sparsity filter, they selected the top-N features that exhibited the highest activation sparsity when measured on the forget set orget. The assumption was that features frequently active on forget data are likely responsible for encoding the knowledge to be removed. Fourth, they implemented static intervention mechanism during inference: whenever any of the top-N selected features fj produced positive activation ( fj(ht) > 0) for any token t, its activation was clamped to fixed negative value (e.g., -c). This clamping was applied universally, regardless of the overall context of the input sequence. This combination of sparsity-based feature selection and static clamping ultimately proved limiting, leading to significant side effects on utility and performance inferior to contemporary gradient-based methods like RMU on benchmarks such as WMDP-Bio. Recognizing these limitations directly motivated our work (DSG), where we instead develop and apply principled feature selection and dynamic, context-aware interventions. A.5 Relearning Attacks Approximate unlearning methods, especially those modifying model weights, face another significant challenge: relearning attacks (Deeb & Roger, 2024). In these attacks, an adversary finetunes the unlearned model Munlearn to recover the supposedly forgotten information. Such recovery is sometimes possible even using only data tangentially related to the original forget set orget (Hu et al., 2024). The success of relearning attacks suggests that gradient17 Preprint. Under review. based weight modifications may primarily suppress access to knowledge rather than truly erasing it from the parameter space; subsequent finetuning can often reverse these weight adjustments, particularly if it reinforces the target concepts. The feasibility of relearning attacks strongly depends on the threat model. In an API-based (black-box) setting, where adversaries only have query access, mounting effective relearning attacks is more difficult, particularly if the provider restricts extensive finetuning or monitors queries. Activation-based intervention methods like DSG, which modify internal states rather than weights to control outputs for relevant inputs, may offer greater robustness in this black-box scenario compared to weight modification techniques. Although sophisticated obfuscation attacks targeting activation-based defenses exist (Bailey et al., 2024), they typically require white-box access (e.g., gradients, internal states). Such access is unavailable in pure API setting, limiting their threat against deployed systems focused on output safety via activation manipulation. DSGs potential resilience against relearning could stem from the relative stability of activation geometry during standard finetuning, phenomenon related to the Superficial Alignment Hypothesis (Zhou et al., 2023). If DSG reliably identifies features encoding the target knowledge based on these stable patterns and consistently applies interventions, the unlearning effect may prove more durable against finetuning-based relearning attacks compared to methods reliant on weight configurations."
        },
        {
            "title": "B Fisher Information Approximation Proof",
            "content": "Theorem 1 (Approximate Fisher Information from SAE Features). Let sparse autoencoder (SAE) with reconstruction ˆr(x) = z(x)Wdec be applied to data D, where z(x) RF represents latent activations and Wdec RFD the decoder weights. Define the reconstruction loss as: 1 2 ˆr(x) r(x)2 ℓ(x) = If the SAE is well-trained such that reconstruction error is small with high probability, then for each row θi, RD of Wdec (representing feature i), the expected squared gradient is approximately proportional to the second moment of the feature activation. Proof. We establish this result through careful analysis of the gradient structure in sparse autoencoders. Computing the Gradient of Decoder Weights. By definition of the reconstruction loss: ℓ(x) = = 1 2 1 2 ˆr(x) r(x)2 z(x)Wdec r(x)2 For row of Wdec, denoted θi, RD, we compute the gradient: θi, ℓ(x) = θi, z(x)Wdec r(x)2 (cid:21) (cid:20) 1 2 By the chain rule: θi, ℓ(x) = (z(x)Wdec r(x)) θi, (z(x)Wdec) Since z(x)Wdec is linear in θi, with coefficient zi(x), we have: θi, (z(x)Wdec) = zi(x) ID where ID is the D-dimensional identity matrix. Therefore: θi, ℓ(x) = zi(x)(ˆr(x) r(x)) 18 Preprint. Under review. Computing the Squared Gradient Norm. Taking the squared norm of this gradient: θi, ℓ(x)2 = zi(x)(ˆr(x) r(x))2 = zi(x)2ˆr(x) r(x)2 Taking the expectation over the data distribution: xD[θi, ℓ(x)2] = xD[zi(x)2ˆr(x) r(x)2] Analyzing the Small Error Regime. When the SAE is well-trained, we can characterize its performance with high-probability bound on reconstruction error. Specifically, assume there exist constants ϵ > 0 and δ > 0 such that: (cid:16) ˆr(x) r(x)2 < ϵ2(cid:17) > 1 δ where ϵ ˆr(x) and δ is small. In other words, the squared reconstruction error is bounded by ϵ2 with probability at least 1 δ. Under this high-probability bound, we can decompose the expectation: E[zi(x)2ˆr(x) r(x)2] E[zi(x)2 ϵ2 ˆr(x) r(x)2 < ϵ2] (1 δ) + Cδ ϵ2E[zi(x)2] + Cδ where is bound on the expectation in the low-probability case. For small δ and finite C, the second term becomes negligible, leaving: E[zi(x)2ˆr(x) r(x)2] ϵ2E[zi(x)2] Connection to Fisher Information. The Fisher Information Matrix for parameter θi, is defined as: I(θi,) = xD[θi, ℓ(x)θi, ℓ(x)] The trace of this matrix, which measures the overall sensitivity of the loss to changes in θi,, is precisely: Tr(I(θi,)) = xD[θi, ℓ(x)2] ϵ2E[zi(x)2] Interpretation. The above analysis shows that ( fj(x))2 = zj(x)2 serves as natural importance measure for feature j. Features with larger average squared activations contribute more significantly to reconstruction gradients and thus have higher Fisher Information content. This justifies our approach of using squared activations to identify features most strongly associated with specific knowledge domains."
        },
        {
            "title": "C Connecting Fisher Information to Causal Influence",
            "content": "In this section, we establish how the Fisher Information associated with Sparse Autoencoder (SAE) features connects to their causal influence as mediators of information flow in language models. Drawing inspiration from causal geometry (Chvykov & Hoel, 2020), we provide proof for why expected squared activation serves as measure of feature importance. Theorem 2 (Fisher Information as Proxy for Causal Feature Importance). Under assumptions of (i) near-deterministic mappings in the language model, (ii) well-defined causal effects under feature interventions, (iii) small SAE reconstruction error, and (iv) approximate feature independence, the Fisher Information associated with SAE features provides principled approximation of their causal influence. Specifically, for any feature fj, the expected squared feature activation ED[ fj(h)2] for hidden state on dataset is proportional to the causal influence of that feature as mediator of information from to model outputs. 19 Preprint. Under review. Proof. We build upon the result in Appendix B, which showed that the expected squared activation E[ fj(h)2] is proportional to the trace of the Fisher Information Matrix for the corresponding decoder weights. Causal Model Setup. Consider language model (LM) that produces hidden states h(x) Rd. Sparse Autoencoder (SAE) encodes h(x) into feature activations = (h) RdSAE, (cid:0)h(x)(cid:1). Let Dforget and Dretain be two subsets of the training data. We i.e. each feature is fj model the causal structure as: Data h(x) = (h) (model outputs) Here, RdY represents the models output vector (e.g., logits or embeddings). Assumptions. 1. Near-deterministic mapping. Conditioned on h, the model output is almost deterministic (small Gaussian noise). Formally, p(cid:0)Y h(cid:1) (cid:0)µ(h), σ2I(cid:1) with small σ2. 2. Well-defined feature interventions. We can perform do(cid:0) fj = α(cid:1), meaning forcibly setting feature to α and thus severing its normal dependence on h. 3. Small SAE reconstruction error. Writing ˆh(z) z, we assume ˆh(z) is small with high probability. 4. Approximate feature independence. Features fj(h) are sufficiently sparse or decorrelated that cross-terms can be neglected. Defining Causal Influence. We quantify the causal influence of feature fj by how much the models output distribution p(Y) changes when we intervene to set fj to its normal value fj(h) vs. forcing it to zero: (cid:104) p(cid:0)Y do( fj = fj(h))(cid:1) (cid:13) (cid:13) p(cid:0)Y do( fj = 0)(cid:1)(cid:17)(cid:105) Influence( fj) = hD (cid:16) DKL large KL means toggling fj from 0 to its actual value drastically shifts p(Y), so fj is strong mediator for D. Expanding KL Divergence. Let gj : RdY describe how feature fj shifts the models outputs. Since we forcibly set fj (an intervention), we ignore any prior correlations with h, and under near-determinism the output distribution is approximated by: p(cid:0)Y do( fj = α)(cid:1) = (cid:0)gj(α), σ2I(cid:1) For two different interventions do( fj = α) and do( fj = β), we can now derive the KL divergence between the resulting output distributions. Using the standard formula for KL divergence between multivariate Gaussians with the same covariance matrix: DKL(N (µ1, Σ)N (µ2, Σ)) = 1 2 (µ1 µ2)TΣ1(µ1 µ2) Therefore: (cid:16) DKL p(cid:0)Y do( fj = α)(cid:1) (cid:13) (cid:13) p(cid:0)Y do( fj = β)(cid:1)(cid:17) = DKL(N (gj(α), σ2I)N (gj(β), σ2I)) 1 2 (gj(α) gj(β))T(σ2I)1(gj(α) gj(β)) 1 2σ2 1 2σ2 (gj(α) gj(β))T(gj(α) gj(β)) (cid:13) gj(α) gj(β)(cid:13) (cid:13) (cid:13) 2 = = = Preprint. Under review. First-Order Taylor Expansion. To make this expression more tractable, we use first-order Taylor expansion of gj(α) around β = 0: gj(α) = gj(0) + dgj dα gj(0) + (cid:0)gj(0)(cid:1) α (cid:12) (cid:12) (cid:12)α=0 α + o(α) When α is sufficiently small, the higher-order terms o(α) become negligible. Substituting this back into our KL divergence expression for the special case where β = 0:"
        },
        {
            "title": "DKL",
            "content": "(cid:0)p(Ydo( fj = α))p(Ydo( fj = 0))(cid:1) = = = gj(0) + gj(0) α gj(0)2 gj(α) gj(0) 1 2σ2 1 2σ2 1 2σ2 1 2σ2 α2 (cid:13) gj(0) α2 (cid:13)gj(0)(cid:13) (cid:13) 2 This shows that the KL divergence (our measure of distribution change) grows quadratically with the intervention magnitude α, with proportionality constant determined by the gradient norm gj(0)2. Expected Causal Influence. Now we can compute the expected causal influence by substituting α = fj(h) and taking the expectation over D: (cid:0)p(Ydo( fj = fj(h)))p(Ydo( fj = 0))(cid:1)(cid:105) Influence( fj) = hD hD (cid:104) DKL (cid:104) 1 2σ2 fj(h)2 (cid:13) = gj(0)2 2σ2 (cid:104) hD 2(cid:105) (cid:13)gj(0)(cid:13) (cid:13) fj(h)2(cid:105) Thus, the expected causal influence of feature as mediator of information from dataset is (cid:2) fj(h)2(cid:3), with proportionality directly proportional to the expected squared activation ED that depends on the sensitivity of the models outputs to changes in gj(0)2 2σ2 constant feature j. (cid:1) Connection to Fisher Information. The Fisher Information for the SAEs decoder weights θj, satisfies I(cid:0)θj, (cid:2) fj(h)2(cid:3) since the gradient w.r.t. θj, includes fj(h) as leading factor. Therefore, E[ fj(h)2] tracks both the Fisher Information and the intervention-based notion of causal influence we derived above, establishing direct link: Causal Influence( fj) Fisher Information(θj,) E[ fj(h)2]. In other words, features most important in Fisher Information sense are precisely those with greatest causal influence on model outputs. Implications for Feature Selection. By identifying features with high squared activations on Dforget but low activations on Dretain, we can target mediators that specifically carry forget set knowledge. Clamping these features to zero during inference selectively reduces the models capacity to propagate information from Dforget while preserving performance on Dretain. Comparisons Across Datasets. For Dforget vs. Dretain, we earlier defined: forget score(j) = EDforget retain score(j) = EDretain (cid:2) fj(h)2(cid:3) (cid:2) fj(h)2(cid:3) Preprint. Under review. The ratio of causal influence of feature for Dforget versus Dretain is: EDforget EDretain [Influence( fj)] [Influence( fj)] = = = [ fj(h)2] [ fj(h)2] EDforget EDretain gj(0)2 2σ2 gj(0)2 2σ2 EDforget EDretain forget score(j) retain score(j) [ fj(h)2] [ fj(h)2] Thus, forget score(j)/retain score(j) precisely captures how much more fj mediates the forget dataset relative to the retain dataset. This is the importance ratio we defined in Section 3, which directly quantifies the relative causal influence of feature across datasets. Proof of Neyman-Pearson Optimality Theorem 3 (Neyman-Pearson Optimality). Let Dforget and Dretain be the distributions of sequences from the forget and retain sets, respectively. If ρ(X) is stochastically larger under Dforget [ρ(X) t] for all t), then among all [ρ(X) t] PrXDretain than under Dretain (i.e., PrXDforget classifiers with false-positive rate at most α, the threshold test C(x) = 1[ρ(x) > τ], where PrXDretain [ρ(X) > τ] = α, maximizes the true-positive rate. Proof. We adapt the classical Neyman-Pearson Lemma to our unlearning context. Our goal is to find the optimal decision rule for classifying inputs as either forget-relevant or retain-relevant. Consider the class of all decision rules : {clamp, no-clamp} with false-positive rate at most α. That is, all rules such that: Pr XDretain [a(X) = clamp] α For each decision rule a, define its acceptance region = {x : a(x) = clamp}. The constraint on false-positive rate translates to PrXDretain Now, define the threshold-based decision rule as: [A] α. a(x) = 1[ρ(x) > τ] where τ is chosen such that PrXDretain is = {x : ρ(x) > τ}. We need to prove that maximizes the true-positive rate among all rules with false-positive [a(X) = clamp] α, we must rate at most α. In other words, for any rule with PrXDretain show: [ρ(X) > τ] = α. The acceptance region for this rule Pr XDforget [a(X) = clamp] Pr XDforget [a(X) = clamp] We use the stochastic dominance property: for any threshold t, PrXDforget PrXDretain under Dforget than under Dretain. [ρ(X) t] [ρ(X) t]. This means that regions of higher ρ values are relatively more likely [A] α. Due Consider any decision rule with acceptance region where PrXDretain to the stochastic dominance property, we can always construct threshold-based region = {x : ρ(x) > τ} such that: 1. PrXDretain [A] (same false-positive rate) 2. PrXDforget [A] (equal or higher true-positive rate) [ A] PrXDforget [ A] = PrXDretain Preprint. Under review. This is because exchanging points from low-ρ regions in with points from high-ρ regions outside (while maintaining the same false-positive rate) will always increase the truepositive rate due to stochastic dominance. If PrXDretain which only increases the true-positive rate further. [A] < α, we can further expand to by lowering the threshold from τ to τ, Therefore, for any decision rule with false-positive rate at most α: Pr XDforget [a(X) = clamp] = Pr XDforget [A] Pr XDforget [A] = Pr XDforget [a(X) = clamp] This establishes that the threshold test a(x) = 1[ρ(x) > τ] maximizes the true-positive rate among all tests with false-positive rate at most α. Practical Implications: This theorem establishes the statistical optimality of our thresholding approach for making the binary decision of whether to apply an intervention. In particular, it shows that our dynamic classification rule maximizes coverage on forget-set queries while maintaining controlled false-positive rate on retain-set queries."
        },
        {
            "title": "E Proof of Dynamic Clamping Dominance",
            "content": "Theorem 4 (Dominance of Dynamic Clamping). Let Snfeats be fixed subset of features identified as forget-relevant. Consider the static approach astatic(x) that clamps features in Snfeats whenever they activate, and the dynamic approach adynamic(x) that first classifies input using C(x) = 1[ρ(x) > τ] and only then applies clamping. Under the stochastic dominance assumption from Theorem 3, there exists threshold τ such that adynamic achieves equal or greater coverage on Dforget than astatic while maintaining equal side-effects on Dretain, making dynamic clamping strictly dominant in the coverage-side effect trade-off. Proof. We begin by formalizing the metrics used to evaluate both approaches and precisely defining their operation. Preliminaries and Definitions. Let be the space of possible input sequences. For sequence = (x1, . . . , xT) and its corresponding hidden states ht, we define: token is triggered by Snfeats if Snfeats such that fj(ht) > 0 The fraction of triggered tokens in sequence: ρ(x) = 1 fj(ht) > 0] t=1 1[j Snfeats : We consider two distributions: Dforget: The distribution of forget-relevant queries, and Dretain: The distribution of retain-relevant queries. The Two Approaches. For both approaches, we define clamp set Bmethod as the set of inputs where the method applies some clamping. 1. Static Approach (astatic): Clamps features in Snfeats whenever they activate on any token. here the Clamp set is Bstat = {x : t, Snfeats such that fj(ht) > 0}. 2. Dynamic Approach (adynamic): Computes ρ(x) and applies threshold test ρ(x) > τ. Only clamps if the sequence passes this test. The Clamp set for threshold τ: Bdyn(τ) = {x : ρ(x) > τ}. 23 Preprint. Under review. Performance Metrics. We define: Coverage: The probability that clamping occurs on forget-set queries Coverage(method) = Pr xDforget [x Bmethod] Side-effect: The probability that clamping occurs on retain-set queries SideEffect(method) = Pr xDretain [x Bmethod] Step 1: Find the side-effect of the static approach. The static approach clamps whenever any token has an activating feature in Snfeats. Therefore: SideEffect(astatic) = Pr xDretain [x Bstat] = α Step 2: Find threshold τ that yields the same side-effect for the dynamic approach. By our assumption that ρ(x) is stochastically larger on Dforget than on Dretain, we know that PrxDretain Therefore, there exists threshold τ [0, 1] such that: [ρ(x) > τ] is strictly decreasing function of τ. This means: Pr xDretain [ρ(x) > τ] = α = Pr xDretain [x Bstat] SideEffect(adynamic(τ)) = SideEffect(astatic) Step 3: Show that coverage is greater for the dynamic approach. From Theorem 3, we know that thresholding ρ(x) at τ gives the optimal classifier for distinguishing between Dforget and Dretain at false positive rate α. More formally, among all sets with PrxDretain ρ(x) > τ} maximizes PrxDforget Since Bstat is one such set with PrxDretain [x A] = α, the set Bdyn(τ) = {x : [x Bstat] = α, we must have: [x A]. Pr xDforget [x Bdyn(τ)] Pr xDforget [x Bstat] Therefore: Coverage(adynamic(τ)) Coverage(astatic) If ρ(x) is strictly stochastically larger on Dforget than on Dretain (which holds in practice as forget-relevant features activate more frequently on forget-set queries), then this inequality is strict. We have established that for any static clamping approach, there exists threshold τ such that the dynamic approach with this threshold achieves the same side-effect on the retain set; and achieves equal or greater coverage on the forget set. This proves that dynamic clamping dominates static clamping in the coverage-side effect trade-off. 24 Preprint. Under review. Distribution of token activations on WMDP-Cyber Figure 8 plots the distribution of forget-set activated tokens on WMDP-Cyber. The threshold is chosen to control the retain sets false positive rate and we find that pdyn = 95 typically separates forget-set queries effectively achieving high recall on orget. On WMDP-Cyber, DSG successfully transfers from the retain set (WikiText) and forget set to the test query set. Figure 8: Distribution of forget-set activated tokens for WMDP-Cyber. Threshold at the 95th percentile (dashed red line) effectively separates MMLU from WMDP."
        },
        {
            "title": "G Unlearning on WMDP",
            "content": "G.1 Hyperparameter Details and Model Descriptions for Baselines To ensure comprehensive and fair comparison of unlearning methods, we conducted extensive hyperparameter sweeps for each baseline, optimizing for both the effectiveness of knowledge removal and the preservation of model utility. For all gradient-based methods, we experimented with updating parameters in layers 3, 7, and 11 (as recommended in (Li et al., 2024)), as well as all layers. Unless otherwise specified, all experiments used the google/gemma-2-2b-it (Lieberum et al., 2024) model. Dynamic SAE Guardrails (DSG). Our proposed method, DSG, is non-gradient-based intervention method that selectively removes hazardous knowledge by manipulating SAE feature activations. DSG first identifies subset of SAE features strongly indicative of the knowledge to be forgotten, based on their differential activation patterns on forget and retain datasets. During inference, DSG employs dynamic classifier to assess the relevance of input sequences. If sequence is classified as forget-relevant based on the aggregate activation of selected features, DSG dynamically clamps these features to negative value. This conditional, sequence-level clamping ensures that intervention is applied only when necessary, minimizing side effects on benign inputs and preserving model utility. We employed the gemma-scope-2b-pt-res SAE (width 16k) applied to layer 3 (ℓ0 142) (Lieberum et al., 2024). The dynamic threshold percentile (pdyn) was fixed at 95. We swept the importance ratio percentile (pratio), number of selected features, and clamp strength (c): Hyperparameter Values Tested Importance Ratio Percentile (pratio) Number of Features Clamp Strength (c) 90, 95 10, 20, 30 10, 25, 50, 100, 200, 300, 400, 500 Table 3: Hyperparameter sweep for Dynamic SAE Guardrails (DSG). Fixed values: pdyn = 95. The best configurations were: WMDP-Bio (pratio = 95, features=20, = 500) and WMDPCyber (pratio = 90, features=30, = 500). 25 Preprint. Under review. Representation Misdirection for Unlearning (RMU). RMU (Li et al., 2024) is gradientbased finetuning method that minimizes composite loss function to achieve targeted forgetting while preserving model utility. This loss combines forget loss and retain loss. The forget loss acts on the models activations on the forget dataset, increasing their norm in specific directions and making it difficult for later layers to process this information effectively. Simultaneously, the retain loss regularizes the updated models activations on the retain dataset, encouraging activations to stay close to the original models activations on benign data. Key hyperparameters include the steering coefficient, which controls how much the activations are amplified on hazardous data, and the alpha parameter (α), which balances utility preservation against knowledge removal. We focus unlearning only on the MLPs, as recommended in Li et al. (2024). Hyperparameter Values Tested Steering Coefficient Alpha (α) Batch Size Steps 1, 5, 10, 20, 100, 200, 400, 500, 800, 1000 0.01, 0.1, 1, 10, 100, 300, 500 4, 8 400, 800 Table 4: Hyperparameter sweep for RMU. Fixed values: Monitoring Layer ID=3, Learning Rate=5e-6. The best configuration for WMDP-Bio used steering coefficient 400, alpha 100, monitoring layer 3, learning rate 5e-6, batch size 8, and 400 steps. For WMDP-Cyber, we used steering coefficient 500, alpha 10, monitoring layer 3, and batch size 8 with 400 steps. Scalable Remembering and Unlearning unBound (SCRUB). SCRUB (Kurmanji et al., 2023) employs student-teacher framework for knowledge distillation-based unlearning. It trains student model, clone of the original model, to forget hazardous knowledge under the guidance of the original, frozen teacher model. During forget epochs, SCRUB maximizes the KL divergence between student and teacher logits on the forget dataset. In retain epochs, it minimizes this divergence on the retain dataset, guiding the student to mimic the teacher on benign data. We swept across values of beta (β), weighting factor balancing knowledge distillation and task-specific loss, while fixing alpha (α) and gamma (γ) at 1.0: Hyperparameter Values Tested Beta (β) Learning Rate (lr) Batch Size Steps 0.0001, 0.001, 0.01, 0.1, 1, 10 1e-4, 1e-5, 5e-6 4, 8 400, 800 Table 5: Hyperparameter sweep for SCRUB. Fixed values: α = 1.0, γ = 1.0, KL Temperature=2.0. The best configuration for WMDP-Bio used beta 0.01, learning rate 5e-6, batch size 8, and 400 maximum batches. For WMDP-Cyber, we used beta 0.1, learning rate 1e-5, batch size 8, and 400 maximum batches. Selective Synaptic Dampening (SSD). SSD (Foster et al., 2024) identifies and dampens parameters more important for the forget set than the retain set. It adapts method originally developed for image classification to language modeling by modifying the loss function to use log-perplexity. SSD calculates parameter importance scores based on gradients observed for both forget and retain datasets, then applies dampening factor to parameters with higher importance for the forget dataset. We performed grid search spanning dampening thresholds and constants: The optimal configuration for WMDP-Bio used threshold 0.5 and dampening constant 1e-3. For WMDP-Cyber, we used threshold 1.0 and dampening constant 1e-2. 26 Preprint. Under review. Hyperparameter Values Tested Threshold Dampening Constant 0.1, 0.25, 0.5, 1, 2.5, 5 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, Table 6: Hyperparameter sweep for Selective Synaptic Dampening (SSD). Static SAE Clamping (Farrell et al.). This non-gradient-based approach (Farrell et al., 2024) identifies salient SAE features and statically clamps their activations during inference to remove unwanted knowledge. Unlike our dynamic approach, this method applies feature clamping universally to all inputs whenever selected feature activates, rather than conditionally based on sequence-level classification. We varied the retain threshold, multiplier (clamp value), and number of features: Hyperparameter Values Tested Retain Threshold Multiplier (Clamp Value) Number of Features 0.01, 0.001, 0.005, 0.1, 1 10, 25, 50, 100, 200, 500 5, 10, 20, 30, 50 Table 7: Hyperparameter sweep for Static SAE Clamping. Fixed value: Sequence Length=1024. The best configurations were: WMDP-Bio (retain threshold=0.01, multiplier=200, features=5) and WMDP-Cyber (retain threshold=0.005, multiplier=500, features=10). Gradient Ascent (GA). GA (Jang et al., 2023) is finetuning-based unlearning method that directly minimizes the likelihood of correct predictions on the forget dataset using gradient ascent. In contrast to standard finetuning which employs gradient descent, GA utilizes gradient ascent to maximize the cross-entropy loss on the forget dataset, pushing parameters in directions that increase prediction error on the targeted data. We varied the learning rate and beta (β), the retain loss weight: Hyperparameter Values Tested Learning Rate (lr) Beta (Retain Loss Weight) 1e-5, 5e-5 0.01, 0.1, 1.0, 5.0, 10.0 Table 8: Hyperparameter sweep for Gradient Ascent. Fixed values: Gamma=1.0, Batch Size=8, Steps=400. We explored both with and without retain data configurations. The best setting for WMDPBio used learning rate 1e-5 with beta 1.0. For WMDP-Cyber, we used learning rate 1e-5 with beta 0.1. Negative Preference Optimization (NPO). NPO (Zhang et al., 2024) adapts preference optimization techniques to treat the forget set as negative examples. It reframes unlearning as preference learning, optimizing the model to assign lower likelihood to the forget set. The beta parameter controls the extent to which the unlearned models output distribution can diverge from the original model. To mitigate utility degradation and preserve performance on benign data, NPO can be regularized using two distinct retain loss types: Negative Log-Likelihood (NLL) and Kullback-Leibler (KL) divergence. NLL minimization directly encourages the model to maintain high probabilities for correct tokens in the retain set, calculated as the negative sum of log probabilities assigned to ground truth tokens. KL divergence minimization encourages the probability distribution of the unlearned model to remain close to that of the original model on retain set inputs, measured as the information lost when approximating the original models distribution with the unlearned models distribution. We tested NPO with various configurations: 27 Preprint. Under review. Hyperparameter Values Tested Alpha (Retain Loss Weight) Beta (Temperature Parameter) Retain Loss Type 0.01, 0.1, 1.0 0.1, 1.0 NLL, KL Table 9: Hyperparameter sweep for NPO. Fixed values: Gamma=1.0, Learning Rate=1e-5, Batch Size=8, Steps=400. The optimal settings for WMDP-Bio used alpha 0.1, beta 0.1, and KL divergence as the retain loss type and WMDP-Cyber used alpha 1.0, beta 0.1, and KL divergence as the retain loss type. For all methods, we selected configurations that minimized WMDP accuracy while maintaining at least 99% of the original models MMLU accuracy. Compute. All finetuning and inference was performed on 4 A6000 GPUs in under day. G.2 Results on WMDP-Cyber Table 10 shows the performance of various unlearning baselines on WMDP-Cyber dataset. RMU is less effective on WMDP-Cyber (88.00%), likely due to the data inefficiency of gradient-based methods on the smaller cyber forget set. Method WMDP Cyber () MMLU () Target GA NPO SSD SCRUB Farrell et al. RMU DSG (Ours) HS Hist C. Bio HS Geo H. Aging All 100.00 100.00 100. 100.00 100.00 100.00 98.91 96.36 98.91 97.82 52.73 88.00 26.74 98.15 100.0 100.00 99.07 99.07 99. 100.0 100.0 100.00 100.00 100.00 100.00 100.0 100.0 98.08 100.00 100.00 99.04 100.0 100.0 98.81 98.81 97.62 98.81 99.07 100.00 100. 100.00 99.46 100.0 99.19 99.46 99.19 99.19 99.73 MT () 7.36 7.39 7.18 7.25 6.51 7.39 7. 7.66 Table 10: Unlearning performance on WMDP-Cyber. All represents the average MMLU score. MTBench scores show 0.13 variance across 5 runs. DSG shows superior unlearning effectiveness compared to other baselines while maintaining high MMLU performance. G.3 MT-Bench Evaluation Details To measure the impact of unlearning on the models general conversational abilities and fluency, we utilized the MT-Bench benchmark (Zheng et al., 2023). Specifically, we report the average score across two conversational turns (the two-turn average score), which provides measure of multi-turn conversational quality. Following standard MT-Bench protocol, evaluations were conducted using GPT-4 (Achiam et al., 2023) as the judge to score the models responses. To ensure the robustness of these fluency assessments, each model configuration reported in Section 4.1 was evaluated 5 times using MT-Bench. The mean scores presented in Table 1 and Table 10 reflect the average performance across these runs, and the standard deviation across the 5 runs is noted in the respective table captions (0.16 for WMDP-Bio results, 0.13 for WMDP-Cyber results). Higher MT-Bench scores indicate better preservation of general conversational capabilities after the unlearning procedure. 28 Preprint. Under review."
        },
        {
            "title": "H Unlearning on MUSE",
            "content": "H.1 Hyperparameter Details and Model Descriptions for Baselines We provide implementation details for the baseline unlearning methods evaluated in our experiments. Gradient Ascent (GA). GA maximizes the loss on the forget set, directly opposing the standard training objective to push the model away from the forget datas distribution (Jang et al., 2023). While straightforward, it often leads to catastrophic forgetting of general knowledge. Gradient Difference (GradDiff). GradDiff balances competing objectives by maximizing the loss on the forget set while minimizing the loss on the retain set (Liu et al., 2022). Despite this approach, GradDiff struggles to find an optimal trade-off, resulting in either overor under-unlearning. Negative Preference Optimization (NPO). NPO reframes unlearning within preference learning framework, treating the forget set as negative preference data by adapting the Direct Preference Optimization objective (Zhang et al., 2024). We use NPO with KL Divergence Minimization that augments NPO with KL divergence term to preserve utility by minimizing distributional shift on benign data. Simplified NPO (SimNPO). computationally efficient variant of NPO that simplifies the optimization process while retaining core principles of negative preference learning (Fan et al., 2024). SimNPO trades some unlearning effectiveness for faster processing. Representation Misdirection for Unlearning (RMU). RMU injects targeted noise into specific layers to disrupt the models ability to process information related to the forget set (Li et al., 2024). Its effectiveness depends heavily on precise noise targeting and hyperparameter tuning. We injected noise in the 7th layer for both News and Books. For all finetuning-based baselines (GA, GradDiff, NPO, SimNPO, RMU), we used AdamW optimizer with learning rate of 1e-5 and batch size of 32. We finetuned all parameters in the model. The optimal checkpoint for each method was determined by selecting the first epoch (within 10 epochs) where the unlearned models utility on the retain set fell below 90% that of the target model. Table 11 summarizes the optimal epochs or α values for each method on both datasets. Unlearning Method NEWS BOOKS GA GradDiff NPO SimNPO RMU epoch 1 epoch 2 epoch 8 epoch 10 epoch 9 epoch 1 epoch 3 epoch 10 epoch 10 epoch 10 Table 11: Optimal epochs for baseline unlearning methods on MUSE benchmark, determined by utility-based stopping criteria. All finetuning and inference was performed on 4 A6000 GPUs in under day. H.2 Sequential Unlearning Strategies for DSG In real-world scenarios, unlearning requests often arrive sequentially over time. An effective unlearning method must be able to handle multiple, successive requests without significant degradation in performance or utility. In Section 3, we evaluated DSGs performance under sequential unlearning using the MUSE benchmark (Shi et al., 2024) with four disjoint folds of the NEWS corpus. We implemented and compared two strategies for adapting DSG to this sequential setting, referred to as DSGall and DSGunion. Both strategies leverage the core DSG mechanisms of feature selection and dynamic thresholding but differ in how they aggregate information across multiple unlearning requests. 29 Preprint. Under review. Setup. Let = 1, 2, . . . , index the sequential unlearning requests. Each request introduces new forget dataset DF,k. We assume the retain dataset DR remains constant throughout the process. The goal at step is to produce an unlearned model that effectively forgets the cumulative forget data Dcumul i=1DF,i while preserving utility evaluated on DR. Let nfeats be the desired number of features to select at each relevant stage. F,k = Strategy 1: DSGall (Cumulative Score Update) This strategy treats the sequential unlearning problem as equivalent to unlearning single, growing forget set Dcumul at each step k. It maintains cumulative statistics required for calculating the feature importance scores. F,k Cumulative Statistics: At step k, we need the aggregate sum of squared activaF,i(j) = t=1[ fj(hx,t)]2 be the sum of squared activations for feature on dataset DF,i, be the total number of tokens in DF,i. The cumulative sums at tions and the total number of tokens for all forget data seen so far. Let A2 xDF,i and NF,i = xDF,i step are: Σ2 F,k(j) = Ncumul F,k = i=1 i=1 A2 F,i(j) NF,i These sums can be updated incrementally as each new DF,k arrives, without needing to store all previous datasets. The retain set statistics (A2 R(j) and NR) are computed once from DR. Score Calculation: The importance scores are calculated using the cumulative statistics: forget scoreall,k(j) = retain score(j) = imp ratioall,k(j) = Σ2 A2 F,k(j) Ncumul F,k R(j) NR forget scoreall,k(j) max{retain score(j), ε} (constant across k) Feature Selection: Using imp ratioall,k(j) and forget scoreall,k(j), select the feature set Sall,k containing the top nfeats features, following the procedure in Algorithm 1 (filtering by percentile pratio and ranking by forget score). 1[j Sall,k Dynamic Threshold and Intervention: Calculate the activation statistic ρall,k(x) = 1 fj(ht) > 0]. Calibrate the dynamic threshold τall,k = Percentile({ρall,k(x)}xDR , pdyn). Apply conditional clamping using Sall,k and τall,k during inference. : DSGall aims for the most accurate representation of feature importance with respect to all forgotten data combined. Strategy 2: DSGunion (Union of Feature Sets) This strategy selects features based on each individual forget request DF,k and then uses the union of these feature sets for intervention. 30 Preprint. Under review. Independent Score Calculation: At step k, calculate importance scores using only the current forget set DF,k and the retain set DR: forget scoreindep,k(j) = retain score(j) = imp ratioindep,k(j) = A2 A2 F,k(j) NF,k R(j) NR forget scoreindep,k(j) max{retain score(j), ε} Independent Feature Selection: Select the feature set Sindep,k containing the top nfeats features based on imp ratioindep,k(j) and forget scoreindep,k(j). Union Set Formation: Maintain the cumulative union of feature sets identified at each step: Sunion,k = Sunion,k1 Sindep,k (with Sunion,0 = ) The size of Sunion,k may grow beyond nfeats. 1[j Sunion,k Dynamic Threshold and Intervention: Calculate the activation statistic ρunion,k(x) = 1 fj(ht) > 0]. Calibrate the dynamic threshold τunion,k = Percentile({ρunion,k(x)}xDR , pdyn). Apply conditional clamping using Sunion,k and τunion,k during inference. : DSGunion ensures that features deemed important for any past forget request are considered for intervention, potentially capturing broader range of forget-related concepts but possibly leading to larger intervention set over time. Result. As reported in the main text (Figure 4(b)), both DSGall and DSGunion demonstrated strong and stable performance across the four sequential unlearning requests on the MUSE benchmark, significantly outperforming gradient-based methods which showed rapid degradation."
        },
        {
            "title": "I Relearning attack",
            "content": "I.1 Superficial Alignment Hypothesis (a) (b) Figure 9: (a) Distribution of activation cosine similarity and activation magnitude ratio between Base and Finetuned models. Finetuning does not significantly change the underlying activation space. (b) Train loss when finetuning Base model and Base+SAE model on WMDP and MMLU. Loss on WMDP for the BASE+SAE model is significantly higher than on MMLU. The resistance of DSG to relearning attacks can be understood through the lens of the Superficial Alignment Hypothesis (Zhou et al., 2023), which posits that models activation geometry is established during pretraining and remains relatively stable during subsequent finetuning. We provide empirical evidence supporting this hypothesis in Figure 9a, which presents the distribution of activation cosine similarities and magnitude ratios between the base and finetuned models. Preprint. Under review. The concentration of cosine similarity values near 1.0 indicates that finetuning preserves the directional information in the activation space, with minimal rotational changes. Similarly, the activation magnitude ratios cluster tightly around 1.0, demonstrating that the scale of activations remains largely unchanged during finetuning. These findings align with previous research suggesting that while weights may change substantially during finetuning, the underlying activation patterns and geometry remain remarkably stable. This stability of activation geometry is the basis for DSGs effectiveness against relearning attacks. By operating directly on these stable activation patterns rather than weights, DSG establishes more durable defense mechanism that persists even when adversaries attempt to modify the models weights through finetuning. I.2 Train-time DSG Details Beyond applying DSG only at inference (Test-time DSG), we explore integrating it directly into the finetuning process itself to further enhance resistance against relearning attacks. This approach, termed Train-time DSG, applies the standard DSG logic during each forward pass of the finetuning/relearning phase. Specifically, during finetuning on potentially adversarial dataset (like the forget set itself in relearning attack scenario), Train-time DSG operates as follows: 1. For each input sequence in training batch, compute the hidden states ht and corresponding SAE feature activations fj(ht). 2. Calculate the statistic ρ(x) based on the pre-selected forget feature set Snfeats. 3. Classify the sequence using the dynamic threshold τ: C(x) = 1[ρ(x) > τ]. 4. Conditional Clamping: If C(x) = 1 (forget-relevant), modify the activations for features (ht) = for all tokens t. Otherwise, Snfeats by setting (ht) = fj(ht). These potentially modified activations are then used for the reconstruction ˆh and subsequent layers of the LLM. 5. The final loss (e.g., cross-entropy on the relearning task) is computed based on the LLMs output derived from these potentially clamped activations. 6. Gradient Blocking: During the backward pass, gradients flow back through the model as usual. However, for any feature activation fj(ht) that was clamped to c, the gradient of the loss with respect to the upstream components (that produced ht) through that specific feature pathway is effectively blocked. Setting the activation to constant detaches it from the upstream computations for the purpose of gradient calculation via that features contribution path. This is conceptually akin to applying stop gradient operation specifically on the clamped feature activations. During this finetuning process, the parameters of the SAE itself (encoder Wenc, benc and decoder Wdec, bdec) are kept frozen. This prevents the SAE from adapting to circumvent the clamping intervention. This dynamic gradient blocking prevents the finetuning process from easily undoing the unlearning effect by simply adjusting weights to reactivate the specific features in Snfeats that carry the forget-set information. When the model attempts to minimize loss on forget-set examples by utilizing these features, Train-time DSG clamps them and blocks the relevant gradient signal. This forces the model, if it attempts to relearn, to find potentially much less direct or alternative pathways through other features or model components. This difficulty in relearning via the original pathways contributes to the significantly higher training loss observed on WMDP-Bio when finetuning with Train-time DSG active, as seen in Figure 9b. I.3 Tamper-Resistant Safeguards DSG functions as tamper-resistant safeguard during finetuning by effectively filtering gradients that would otherwise enable the model to relearn forgotten knowledge. Figure 9b demonstrates this mechanism quantitatively, showing the training loss profiles when finetuning the base model and the base model with DSG active on both WMDP-Bio (forget set) and MMLU (retain set) datasets. 32 Preprint. Under review. When DSG is active during finetuning, we observe significantly elevated training loss values on WMDP-Bio compared to MMLU. This marked difference in loss profiles indicates that DSG selectively impedes the model from reducing loss on forget set content while allowing normal optimization on retain set content. This selective gradient filtering creates an effective barrier against relearning targeted information. The mechanism works because during finetuning, DSG constantly monitors activations and applies clamping whenever forget-relevant features are activated above the dynamic threshold. This intervention disrupts the gradient flow for targeted concepts, requiring the model to develop entirely new processing pathways rather than simply recovering previously established connections. This rewiring requirement explains the delayed recovery pattern observed in the main relearning experiments, where performance remains near random for approximately six epochs before beginning to increase. I.4 Relearning Attack at Learning Rate 1e-6 Figure 10: Relearning attack performance with reduced learning rate (1e-6). All configurations show minimal performance changes across finetuning epochs, demonstrating that relearning attack efficacy is strongly dependent on learning rate. To investigate the impact of learning rate on relearning attack efficacy, we conducted supplementary analysis using reduced learning rate of 1e-6 (compared to 1e-5 in the main experiments). Figure 10 presents WMDP-Bio accuracy across finetuning epochs for all configurations under this reduced learning rate condition. The results demonstrate minimal performance changes across all configurations throughout the finetuning process. This stability indicates that relearning attack efficacy is strongly dependent on learning rate, with lower rates substantially limiting the models ability to recover forgotten knowledge. This finding has important implications for practical deployment scenarios, suggesting that implementing learning rate constraints on model access APIs could serve as an additional defense layer against relearning attacks. I.5 Relearning Hyperparameters For the relearning experiments, we used the RMU unlearned model as described in Section 4.1, with RMU hyperparameters set to steering coefficient 400, alpha 100, monitoring layer 3, AdamW optimizer, learning rate 5e-6, batch size 8, and 400 steps. For DSG configurations, we employed the optimal parameters identified in our WMDP-Bio experiments: importance ratio percentile (pratio) of 95, feature count of 20, and clamp strength (c) of 500 for both test-time and train-time DSG interventions. The dynamic threshold percentile (pdyn) was maintained at 95, consistent with our main experiments. All finetuning and inference was performed on 2 A100 GPUs in under day. 33 Preprint. Under review. Data Efficiency and Zero-shot Capabilities J.1 Hyperparameters Data Efficiency For the data efficiency experiments, we maintain consistent hyperparameter settings across all data subsets to isolate the impact of dataset size. We use the optimal DSG configuration identified for WMDP-Bio with 100% data, as shown in Table 12. Parameter Value SAE SAE layer Importance ratio percentile (pratio) Dynamic threshold percentile (pdyn) Number of selected features Clamp strength (c) gemma-scope-2b-pt-res SAE (width 16k) Layer 3, ℓ0 142 95 95 20 Table 12: DSG hyperparameters for data efficiency experiments For RMU comparisons, we evaluate two approaches: (1) maintaining the same number of training steps (400) across all data subsets, and (2) completing one full epoch over each dataset subset. Maintaining the same number of training steps produced better Pareto front. We select the model with lower WMDP accuracy for each subset. The base RMU configuration for WMDP-Bio is presented in Table 13. Parameter Value Steering coefficient Alpha (α) Monitoring layer Learning rate Parameter subset 400 100 3 5e-6 MLP layers only Table 13: RMU hyperparameters for data efficiency experiments Zero-shot For zero-shot experiments, we vary only the dynamic threshold τ (as no retain set is available for calibration) while keeping all other hyperparameters fixed at their optimal values for each task, as shown in Table 14. Parameter SAE layer Importance ratio percentile (pratio) Feature selection Clamp strength (c) τ range tested Optimal τ WMDP-Bio Layer 3, ℓ0 59 95 20 features 500 0.1 to 0.9 (increments of 0.1) 0.6 WMDP-Cyber Layer 3, ℓ0 59 90 20 features via 500 0.1 to 0.9 (increments of 0.1) 0. Table 14: Hyperparameters for zero-shot experiments The optimal thresholds were determined to be τ = 0.6 for WMDP-Bio and τ = 0.2 for WMDP-Cyber, as shown in Figure 6B."
        },
        {
            "title": "K Ablations",
            "content": "This appendix provides comprehensive details on our ablation studies for DSG. We analyze each components contribution to overall performance and explore sensitivity to various hyperparameters. 34 Preprint. Under review. K.1 Additional Ablations DSG Clamp Strength c. The clamping parameter determines the magnitude of intervention applied to selected SAE features. As shown in Figure 11, WMDP-Bio accuracy drops significantly at modest clamp values (c = 25), reaching near-optimal unlearning performance, while MMLU accuracy remains above 99% for configurations with 10-20 features. For these optimal feature counts, performance remains remarkably stable across wide range of clamp strengths (100 500), demonstrating DSGs robustness to this parameter. By contrast Farrell et al. (2024) exhibit greater sensitivity to clamp values, as seen in Figure 7A. Figure 11: Effect of clamp strength on DSG performance across different feature counts. MMLU accuracy (solid lines) remains consistently high (> 99%) for 10-20 features across all clamp values, while WMDP-Bio accuracy (dashed lines) drops sharply even at modest clamp strengths (c = 25). This demonstrates DSGs ability to effectively remove targeted knowledge while preserving general model capabilities with minimal parameter sensitivity. DSG Number of Features. Across experiments, the number of features selected during percentile-based feature selection represents critical balance between coverage and precision. Selecting too few features may result in insufficient removal of forget-set information, as some forget-set inputs might not activate the limited feature set strongly enough to trigger intervention. Conversely, selecting too many features increases the risk of including noisy features selected using importance scoring or less discriminative features that activate on retain-set samples, potentially causing false positive detections and reducing model utility. Our experiments consistently show that 20 features provides an optimal balance for both WMDP-Bio and WMDP-Cyber domains. Configurations with 10 features occasionally show reduced unlearning effectiveness despite good utility preservation, while 30-feature configurations begin to impact retain-set performance at higher clamp strengths. The precise optimal feature count may vary by domain and dataset characteristics as well as SAE width, but the overall pattern of diminishing returns with increased feature counts remains consistent. Choice of Activation Statistic: Percentage vs. Raw Count. DSGs dynamic classification uses sequence-level statistic derived from forget-feature (Snfeats) activations. We compared two statistics: (1) Percentage-based (ρ), the fraction of tokens where any Snfeats activates ( fj(ht) > 0): ρ(x) = 1 x t= 1[j Snfeats : fj(ht) > 0] 35 Preprint. Under review. and (2) Raw count-based (ρraw), the absolute number of such tokens: ρraw(x) = t= 1[j Snfeats : fj(ht) > 0] Effective dynamic thresholding (calibrated on WikiText) requires low distributional distance (Total Variation Distance, TVD) between retain sets (WikiText vs. MMLU) for generalization, and high TVD between retain and forget sets (WikiText vs. WMDP) for discrimination (Figure 12). Empirically, ρ performs significantly better. For WMDP-Bio: (1) Retain alignment (WikiText vs. MMLU): TVD(ρ) = 0.38 0.03 vs. TVD(ρraw) = 0.88 0.01, indicating ρ generalizes better across retain sets. (2) Retain/Forget separation (WikiText vs. WMDP-Bio): TVD(ρ) = 0.90 0.02 vs. TVD(ρraw) = 0.41 0.03, showing ρ discriminates more effectively. Similar results hold for WMDP-Cyber (Figure 12). The percentage-based statistic ρ outperforms ρraw due to its inherent normalization. Raw counts (ρraw) are confounded by sequence length, whereas ρ measures activation density, providing length-invariant signal. This normalization improves both generalization across retain data and discrimination from forget data, making ρ the more robust choice for DSG. Figure 12: Total Variation Distance (TVD) between WikiText and benchmark datasets using percentagebased (ρ) vs. raw count-based (ρraw) metrics. Lower TVD between WikiText and MMLU indicates better alignment of retain sets, while higher TVD between WikiText and WMDP indicates better separation between retain and forget distributions. Percentage-based metrics consistently outperform raw counts on both measures across all benchmarks. K.2 Ablations Hyperparameter Details Our ablation studies used the following hyperparameter configurations: Clamp Strength and Feature Count. We evaluated DSG performance with feature counts of 10, 20, and 30, across clamp strengths {10, 25, 50, 100, 200, 300, 400, 500} and pratio = 95. Feature Selection Comparison. To compare our percentile-based approach with Farrell et al. (2024), we tested both methods using 20 and 30 features, with clamp values in the range [10-500]. We set pratio = 95 for DSG and used the recommended threshold of 0.01 for Farrell et al. (2024). Dynamic Threshold. We varied pdyn from 60 to 97 using 20 and 30 features with = 500 to examine the impact of threshold selection on the forget-retain trade-off. Importance Ratio Threshold. We tested pratio values from 75 to 95 using 20 and 30 features with = 500 to assess feature selection stringency effects. Preprint. Under review. Activation Metrics. For comparing percentage vs. raw count metrics, we applied bootstrap resampling with 1000 iterations, using Kernel Density Estimation to compute robust TVD estimates between WikiText and test set distributions. Computational Cost (Inference Latency) practical consideration for deploying unlearning methods is their impact on inference speed. We evaluated the latency introduced by DSG compared to the original model and static clamping baseline (Farrell et al., 2024). Interventions using SAEs inherently introduce some latency compared to the original LLM without the SAE. This overhead stems from two main sources: (1) The baseline cost of the SAEs forward pass, which involves matrix multiplications for both encoding (z = σ(Wench + benc)) and decoding ( ˆh = Wdecz + bdec), scaling with the SAEs width (dsae); and (2) The cost of the specific intervention logic applied to the SAE features. For DSG, this intervention logic involves two main steps beyond the standard SAE pass: (a) calculating the ρ(x) statistic (fraction of forget-activated tokens) across the sequences activations, and (b) conditionally applying the clamping intervention based on the ρ(x) > τ comparison. Table 15 presents the mean inference times (in seconds) and standard deviations over 100 samples (batch size 1) for processing sequences of varying lengths (256, 512, 1024 tokens). These measurements were performed using the google/gemma-2-2b-it model (Lieberum et al., 2024) and with the gemma-scope-2b-pt-res SAE (width 16k, applied at layer 3, ℓ0 142) (Lieberum et al., 2024) on single A6000 GPU. As shown in Table 15, the total combined overhead (SAE matrix multiplications + intervention logic) introduced by both static clamping and DSG is minimal. Specifically, DSG increases latency by about 5% (ranging from approximately 3.6% to 7.3%) over the original model across the tested sequence lengths. Importantly, the additional overhead incurred by DSGs dynamic classification logic (calculating ρ(x) and thresholding) compared to simple static clamping is negligible indicating that the primary source of the observed latency increase relative to the base LLM is the SAEs own forward pass. Seq Length Original Model (s) Static Clamping (s) Dynamic Clamping (DSG) (s) 256 tokens 512 tokens 1024 tokens 0.0872 0.0098 0.1618 0.0061 0.3300 0.0081 0.0933 0.0091 (+7.0%) 0.1659 0.0029 (+2.5%) 0.3403 0.0083 (+3.1%) 0.0936 0.0090 (+7.3%) 0.1676 0.0047 (+3.6%) 0.3420 0.0081 (+3.6%) Table 15: Comparison of Inference Latency Across Sequence Lengths for gemma-2-2b-it with gemma-scope-2b-pt-res SAE. Data reported as mean std over 100 samples on single A6000 GPU. Percentage increase relative to the Original Model shown in parentheses. While DSG introduces this slight inference overhead, it is important to consider the broader computational context. Gradient-based unlearning methods require computationally intensive finetuning processes involving backward passes through the model for each unlearning request. In contrast, DSGs unlearning cost primarily involves one-time computation of activation statistics (which can be amortized across many uses) and the minimal, constant inference-time overhead detailed above. Therefore, DSG offers highly efficient alternative for unlearning, achieving state-of-the-art forgetting effectiveness and utility preservation with only marginal increase in inference latency. This makes it particularly attractive for scenarios requiring frequent or sequential unlearning operations where the cost of repeated gradient-based finetuning would be prohibitive. 37 Preprint. Under review."
        },
        {
            "title": "M Feature Interpretability",
            "content": "A key strength of Dynamic SAE Guardrails (DSG) is interpretable unlearning, especially in zero-shot scenarios where domain-specific data is absent. To demonstrate this, we used Neuronpedia APIs search by SAE (Lin, 2023) to directly identify Sparse Autoencoder (SAE) features relevant to biosecurity and cybersecurity hazards. For WMDP-Bio and WMDPCyber, Biology and Cybersecurity queries retrieved the top 20 feature IDs from the gemma-scope-2b-pt-res SAE (width 16k) (Lieberum et al., 2024) applied to gemma-2-2b-it layer 3 (ℓ0 59). Table 16 shows the semantic alignment of these zero-shot features with the targeted knowledge. Listing the top 20 SAE feature IDs for both domains, alongside Neuronpedia interpretations, the table shows features for Biology consistently described with terms like biological processes, cellular functions, and geneticscore concepts of biosecurity risks. Similarly, Cybersecurity features are linked to cyber threats, digital security, and encryption, reflecting cybersecurity risks in WMDP-Cyber. This highlights SAEs ability to extract topically precise features, even without task-specific data. Figure 13 further illustrates this, visualizing activations on WMDP-Bio and WMDP-Cyber forget set sequences. Figure 13A (WMDP-Bio) shows activations for IDs 373 and 10933 clustering around biological terms like bacteria, cellular, and infection while Figure 13B (WMDP-Cyber, IDs 15286 and 2905) shows clusters around cybersecurity terms like encryption, data, and security. These examples and Table 16 show that zero-shot SAE feature selection captures semantically rich, domain-relevant concepts associated with hazardous knowledge. This interpretability is prescriptive for unlearning: by targeting these topically coherent features, DSG achieves zero-shot interpretable unlearning. This is key practical advantage over gradient-based methods, which require task-specific data and lack inherent interpretability, making DSG uniquely transparent and data-efficient solution for mitigating hazardous knowledge, especially in data-scarce or zero-shot deployment. Figure 13: Feature Activations on Example Sequences from Forget Sets. (A) WMDP-Bio sequence with words highlighted in green indicating activation values > 0 for feature ID (top) 373 and (bottom) 10933. (B) WMDP-Cyber sequence with words highlighted in green indicating activation values > 0 for feature ID (top) 15286 and (bottom) 2905. Activation magnitudes are reported above the words in grey. 38 Preprint. Under review. Biology ID 12382 9722 343 373 11 15969 12117 Concepts related to biological or cellular processes and conditions, particularly focusing Sentence Terms related to biological processes and structures in living organisms Concepts related to biological processes and systems Terms related to biological processes and laboratory techniques Scientific terminology related to biological processes and cellular functions Scientific terms and concepts related to biology Terms related to biotechnology and bio-related fields 5877 968 on requirements, limitations, and energy dynamics Terms related to biological processes and molecular interactions Terms related to biological or medical processes and conditions, especially those involving cellular or molecular biology Scientific terminology related to cellular processes and functions Specific terminology related to biological processes and gene expression Biological and genetic terms or sequences 622 5231 10546 12037 Medical terms and technical jargon related to genetic and biological research 6150 5704 14747 8786 10933 140 Elements related to scientific terminology, particularly in genetics and molecular biology Scientific terms and jargon related to biological research Technical terminology and references related to biotechnology and medical research Scientific terminology related to molecular biology and laboratory procedures Terms related to biological research and medical methodologies Technical terms and concepts related to biology and bioengineering Terms related to biological or medical research, particularly focusing on specific conditions and associated microorganisms Cybersecurity Sentence Terms related to cyber threats and cybersecurity issues Explicit mentions of digital security concerns ID 15331 2060 15286 Concepts and terms related to digital security and data integrity 11015 364 4836 2905 10931 11716 Terms related to security and the act of securing something References to security and related terms Concepts related to secure web connections and cryptocurrency surplus Terms related to data security and encryption References to national security and related governmental positions or actions Technical terms and language related to coding and software functionality, specifically focusing on vulnerabilities References to technology and its applications across various sectors Terms related to computing and data centers References to Common Weakness Enumeration (CWE) identifiers 16160 Discussions related to technology and computer systems 6309 10543 Keywords related to safety and security measures in various contexts 11513 1803 12681 Keywords related to safety and security 11520 11323 Key concepts related to digital citizenship and its implications in various contexts 10415 Key components of data processing and communication in systems, particularly focusing on the details of data packet headers and their significance for routing and interpreting data References to computing systems and technologies References to technology and tech-related topics References to information technology and IT-related concepts 3943 4686 Table 16: Top 20 SAE Features for Biology and Cybersecurity in Zero-Shot Setting. List of the top 20 SAE feature IDs identified by querying Neuronpedia with Biology and Cybersecurity, alongside their corresponding Neuronpedia-provided interpretations, showing the semantic relevance of the selected features to the targeted knowledge domains."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Leonardo Labs"
    ]
}