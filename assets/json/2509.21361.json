{
    "paper_title": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs",
    "authors": [
        "Norman Paulsen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates."
        },
        {
            "title": "Start",
            "content": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs Norman Paulsen Denver, Colorado, USA norman.paulsen@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define concept of maximum effective context window, 2) formulate testing method of context windows effectiveness over various sizes and problem types, and 3) create standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as >99%. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates."
        },
        {
            "title": "CCS Concepts",
            "content": "Computing methodologies Artificial intelligence Natural language processing Information extraction"
        },
        {
            "title": "Keywords",
            "content": "Large Language Models, Context Window, Inference Tokens, Hallucination Rates, LLM Accuracy ACM Reference Format: Norman Paulsen. 2025. Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs"
        },
        {
            "title": "1  Introduction",
            "content": "The rise of large language models (LLMs) such as ChatGPT, Claude, Gemini, and LLaMA has reshaped the landscape of natural language processing (NLP), enabling increasingly sophisticated contextual understanding, summarization, coding, and dialog capabilities. Central to the application of these advancements is the concept of the context window (or max context length), the max number of input tokens (words, punctuation, and symbols), model can consider at one time. The expansion of context windows from hundreds to tens of thousands and, more recently, to millions of tokens, represents technical triumph. Several methods have been employed to effectively extend context windows (Pawar et al., 2024) yet, an unresolved and often misunderstood question remains: how much of that context can truly be used effectively by the model? While model specifications cite maximum context window of 128k, 1 million or even as much as 10 million tokens (Meta 2025a), these numbers reflect architectural or implementation limits, not necessarily the models practical capacity for handling or retaining that full input context. Empirical evidence increasingly suggests divergence between the maximum context window (MCW) and the maximum effective context window (MECW) the point beyond which additional tokens no longer meaningfully contribute to model output quality. Understanding this discrepancy is vital for the efficient deployment of LLMs in domains that demand long-context comprehension, such as legal reasoning, scientific literature synthesis, financial document analysis, and multimodal temporal correlation in video or audio. This paper explores the emerging distinction between the MCW and the MECW in contemporary transformer-based LLMs. We propose that while LLM architecture permits long-sequence processing, practical limitations constrain the usable span of context in real-world inference tasks. We define MECW as the longest span of token input, for given problem type, for which incremental tokens degrade the model's output with measurable effect. This notion reframes the context window not as flat max input capacity, but as spectrum of values dependent on the task at hand. The appeal of longer context windows is intuitive. An LLM capable of digesting entire books, codebases, or sessions without truncation seems closer to achieving general-purpose intelligence. In enterprise settings, longer contexts allow seamless retrieval-augmented generation (RAG), more nuanced chat histories, and document-centric agents capable of reasoning over sprawling datasets. Small context windows limit the practical uses of LLMs. Yet anecdotal observations from practitioners often contradict this promise. Despite feeding entire books or lengthy transcripts into models with claimed milliontoken capacity, users frequently observe that LLMs fail to answer questions about information embedded in the input sequences. General observations seem to show performance degrades when prompts rely on large context, and models exhibit increased hallucination rates as token counts rise. However, there is lack of empirical evidence to support what weve seen anecdotally in the field. In this paper we outline testing methodology for real world applications of LLMs to find the maximum effect context window. Leveraging the proposed methodology, weve collected hundreds of thousands of data points from the most prominent LLMs on the market. We aggregate that data to show MECW values across series of different problem types and compare the MCW to the MECW. This paper makes three key contributions: Formal Definition of MECW: We offer principled definition of the Maximum Effective Context Window, grounded in informational, theoretical and behavioral criteria. This includes defining effectiveness in terms of fluctuating measurable influence on model predictions, rather than static inclusion limits like that of MCW. Empirical Analysis Across Tasks and Models: We evaluate several state-of-the-art LLMs across battery of tasks (finding Needle-in-a-Haystack, finding Needles-in-aHaystack, summarization, finding Needlesin-a-Haystack with sorting) using controlled token context intervals to chart the actual usable range of input. This includes both open-source models (e.g., Mistral, LLaMA, Deepseek) and proprietary APIs (e.g., GPT4o-mini, GPT-5, Claude 3, Gemini 2.5, Gemini 2.0). Recommendations for Design and Deployment: Based on our findings, we outline practical guidelines for model architects, prompt engineers, and application developers. These include strategies to optimize RAG pipelines, truncate or summarize distant context, and more realistically context window limit estimates based on MECW rather than MCW. Broader Implications Understanding the gap between the maximum context window and maximum effective context windows is not just technical nuanceit is fundamental to how we effectively use and leverage artificial intelligence in real world applications. Misinterpretation of context capacity can lead to inefficient system designs, overinvestment in retrieval techniques that yield diminishing returns, or misaligned user expectations. It can also skew benchmarking results, especially when models are assumed to have uniform memory over arbitrarily long sequences. Moreover, as LLMs are integrated into systems that simulate long-term memory or perform multi-session reasoning, distinguishing between architectural input size limits and real functional capacity becomes crucial. In cognitive science terms, MECW may be more analogous to \"working memory\" than to \"longterm memory\" and recognizing that distinction can lead to more robust, interpretable, and grounded model responses."
        },
        {
            "title": "2.1 Tokens Matter",
            "content": "Prior research has shown long context windows suffer in few different ways. Existing evidence shows models suffer from placement of data issue. In older models, providing the same data in different orders produces varying results of success retrieving the requested information. Successful retrieval drops from 76% to 66% by moving the key information from position 1 to position 2, and falls under general model performance (with no data in context) when the key information is moved to position 7. Patterns in attention have shown to improve data retrieval but are outside the scope of this experiment (Liu et al., 2023; Hsieh et al., 2024). Not only does key information placement impact performance, the size of that information matters. Prior research found that the relevant information token count compared to total context token count impacts the successful retrieval rate by up to 25% (Bianchi et al., 2025) Notable prior research papers show that models handle context window lengths greater than their training-time sequence length poorly. When using context lengths greater than their training-time sequence length, U-shaped performance curve emerges based on critical information placement. Additional research shows that models start to degrade at half their training length (An et al., 2024; Liu et al., 2023; Press et al., 2022). Relatively new research poses that model performance on novel tasks, like math and logic problems, suffers from the number of steps needed to complete. The addition of more steps degrades the models accuracy. We look to show that its not the number of steps but the token length that causes breakdown in performance (Xu et al., 2025). For the purposes of our experiments, we look to negate data positioning as contributing factor to performance by randomizing the data for each question proposed to the model. This guarantees an even distribution of data placement throughout the context."
        },
        {
            "title": "2.2 Settings Impacting Performance",
            "content": "Several studies have showcased how model performance varies over multitude of factors. Model performance relies on several factors including max allowed output tokens, temperature, top_p, and even the python frameworks used (Hochlehnert et al., 2025; Zhao et al., 2025). Higher temperatures, approaching 1, lead to increased model performance with tradeoff in reproducibility. For the purposes of our experiments, we use the default temperature of 1. Higher top_p values also lead to improved model accuracy but without the detriment to stability. We also leave top_p constant at their default values to reduce variability across experiments (Hochlehnert et al., 2025). Max token values have an outsized impact on long query performance. As models approach set max token limits, they begin to truncate responses and provide unfinished answers. Not only has prior research shown this but we saw similar results when we started testing while using token limits (Ding et al., 2025). As result, we set all token limits to maximum values to allow models to use as many tokens as necessary to provide an accurate answer. Reasoning and non-reasoning models work in distinctly different ways, leading to large performance gains from reasoning models. Many experiments have compared the contrasted reasoning vs nonreasoning models of the same provider to help benchmark performance differences between the two (Chen et al., 2024; Chen et al., 2025; Chua et al., 2025; Ding et al., 2025; Feng et al., 2025; Li et al., 2025b). Our research is less interested in the distinction between reasoning and non-reasoning models when it comes to the testing framework. Instead we focused on the top performing models from various providers, which were mostly reasoning models. Fine tuning models on specific tasks, like data extraction from large documents, increases performance for said tasks. Studies found 10.5% improvement in data retrieval questions on long context windows by fine tuning models on synthetic large context window tasks (Xiong et al., 2024). We did not want to focus the LLM on our tasks and were more interested in model generalization across various tasks. All settings and frameworks remain constant during our tests to remove as many outside variables as possible. We want to focus on the impact of input context token length as the only variable. To further reduce noise from outside variables, we reran all tests repeatedly until appropriate p-values were achieved."
        },
        {
            "title": "2.3 Novel Question Performance",
            "content": "Standard model performance frameworks are not built to evaluate long context windows. Furthermore, existing evaluation frameworks, like AIME24, AIME25 and GPQA Diamond, all suffer from random seeding volatility, wide fluctuations in scores due to the small number of questions, and variability across different versions of the frameworks (Hochlehnert et al., 2025). Small datasets, like those used in AIME24, can significantly misrepresent model performance when used in comparisons. The 30 record dataset used in AIME24 means one missed question impacts reported model performance by 3.3%. Many model providers now retest models multiple times on these same small datasets to provide more accurate number, however, those results are then impacted by seed parameters (Hochlehnert et al., 2025). The seed parameter, if not explicitly set, is automatically generated dynamically per inference. This was shown to vary model performance on the same dataset significantly higher than the baseline. Coupled with small datasets, this can result in large fluctuations in standardized model performance frameworks. For all of these reasons, we create new testing framework for measuring the specific impact of input token length for given task. None of the existing testing frameworks provide data in format sufficient for testing incrementally increasing context lengths for real world use cases."
        },
        {
            "title": "2.4 Other Frameworks",
            "content": "Other frameworks for testing long context windows have been developed in the last 12 months. Several have focused on the Needle-in-a-Haystack problem, demonstrating the effectiveness and limits of finding single piece of information in large context window (Gao et al., 2025; Ling et al., 2025; Nelson et al., 2024). Others focus on complex tasks on fixed dataset (Bogomolov et al., 2024; Cui et al., 2025; Jacovi et al,. 2025; Zhuang et al,. 2025). None of these focus on incrementally testing model effectiveness on various tasks as token count increases. ETHIC is designed to test long context tasks to see how well LLMs cover the provided material (Lee et al., 2024). This framework finds similar results but is focused on how to test model effectively using its long context window while we want to determine the point in which context window breaks down for given task. The DocPuzzle Benchmark provides 100 multidomain cases with verification mechanisms (Zhuang et al,. 2025). While this also focuses on long context data retrieval followed by complex reasoning tasks, it does not provide an incremental token count for the tasks. CURIE, scientific long-Context Understanding, Reasoning, and Information Extraction benchmark, also shows models underperform on long contexts (Cui et al., 2025). This benchmark focuses on scientific tasks with predetermined questions and answers which greatly differs from our approach of generating questions with variable context lengths. The FACTS Grounding Leaderboard is an ongoing benchmark continuously testing model performance with documents up to 32k tokens in length (Jacovi et al,. 2025). Similar to many other long form benchmarks, it only tests accuracy on fixed set of data with predetermined questions. Long Code Arena focuses on testing long context windows in domain specific benchmark for LLM coding (Bogomolov et al., 2024). The benchmark focuses on 6 different aspects of code processing: generation, repair, completion, summarization, processing diffs. This differs from our research which looks at generalized model performance. The LaRA Benchmark also tests large context windows with focus on RAG vs long-context windows and finds inconclusive results (Li et al., 2025a). The tests found many factors are at play including the model's parameter size, long-text capabilities, context length, task type, and the characteristics of the retrieved chunks. We narrow our focus to context length by task type to determine the relationship. U-NIAH, Unified Needle-in-a-Haystack, focuses on comparing LLM long contexts to RAG results to find tradeoffs between the two (Gao et al., 2025). The focus on single problem type, Needle-in-aHaystack, differs from our frameworks focus on context length per question type. The HELMET Benchmark tests models with various tasks and context sizes, like our approach (Yen et al., 2024). It also finds that models degrade on larger contexts, however, they do not focus on finding the point where models degrade for given task. Instead, they bucket context windows into one of 5 buckets ranging from 8k to 128k tokens. Models can be extended to effectively find facts in contexts of extraordinary size. The BABILong Benchmark tests model retrieval capacity exceeding 11 million tokens, the equivalent of 16,800 pages or 85 books (Kuratov et al, 2024). This framework focuses on breaking apart long contexts into smaller chunks referenced via recurrent memory. While this is impressive, we are interested in what are the optimal sized chunks to pass to an LLM. The LongReason framework provides set of questions and artificially adds context to the material containing the answer to test models at various context sizes (Ling et al., 2025). Per their research, some key limitations are the fixed questions and the non-complexity of the task. It is mostly Needle-in-aHaystack type problems. Our framework expands on their research by increasing question variability, complexity and specificity of token length. The NoLiMa Benchmark also tests increasing context lengths using Needle-in-a-Haystack style questions but with twist. The information requires inference, meaning there is one additional reasoning step required to find the information (Modarressi et al., 2025). They also found similar results to us, that longer contexts degrade performance, but they do not test and compare different problem types across the same model. The FLenQA data set most closely mirrors our own but with few distinctions. FLenQA also focuses on showing model performance degradation over increasingly large context windows (Levy et al., 2024). However, they focus on single type of true/false reasoning question and fill the context with autogenerated text. We diversified the question types and provided only data that could be relevant to the answer, similar to real-world RAG implementation. We also build upon their findings to show model degradation on context window size is task specific, providing guidance in real world applications. Many frameworks prior to 2024 also showed similar limitations with large context windows focusing on novel set of data and novel set of questions, like BAMBOO, L-Eval, LongBench, MuLD (Dong et al., 2023; An et al., 2023; Bai et al., 2023; Hudson et al., 2022). Our focus is building on this great corpus of research by providing practical, real world, guidance"
        },
        {
            "title": "3 Methodology",
            "content": "To answer our research questions how does MECW compare to MCW across models and task complexities and can this be leveraged to improve model performance we produced series of novel questions for LLMs to answer. This involved creating redundant questions with randomized data, asking these questions of various models repeatedly, slowly increasing the data set size in each round of questions and recording the answers."
        },
        {
            "title": "3.1 Model Selection",
            "content": "We wanted wide selection of frontier models from several different providers with open source and proprietary weights. Because of this, we primarily went with reasoning models and excluded small and mid-sized models. Our selection criteria resulted in the following 11 models: Open weight: Deepseek.r1-v1:0 (DeepSeekAI et al., 2025), Meta.llama3-3-70b-instructv1:0 (Meta 2025b) Closed weight: claude-3-5-sonnet-20241022 (Anthropic 2024), gemini-2.0-flash (Gemini 2025a), gemini-2.5-flash-preview-05-20 (Gemini 2025b), GPT-4.1 (OpenAI 2025a), GPT o4-mini (OpenAI 2025b), GPT-5 (OpenAI 2025c), Grok-3-latest (xAI 2025), mistral-medium-2505 (Mistral AI 2025), Qwen-plus (Quen Team 2025)"
        },
        {
            "title": "3.1 Framework Design",
            "content": "To collect the necessary data, we developed the following framework to test model performance over an increasing number of input token lengths. 3.1.1 Dataset: We defined our own dataset of 10,000 unique names of individuals. Each individual in the dataset was provided random number, 1-20, of random item from list of 15 possibilities. Each item for each person was then assigned random color out of 9 possibilities. Example data row: Abigail Holmes has 19 red balloons. 3.1.2 Question types: We defined four distinct questions based on this dataset. 1) Needle-in-aHaystack, search for single data point from the data set; 2) Needles-in-a-Haystack, search for multiple data points from the data set and then sum the total; 3) Summarization, full sum of all data points in the data set; 4) Find and sort, search of multiple data points from the dataset then sorted alphabetically by name. The Needle-in-a-Haystack is the simplest question on our list and, unsurprisingly, the models handled this one the best. For this question, we simply asked for the number of objects person in the context data had. While all models performed the best at this question type, none managed to effectively find objects up to their MCW. For summarization, we simply asked the model to sum up all object totals. All models performed worse than the Needles-in-a-Haystack task, which was unexpected. We assumed it would be harder for model to perform the multi-step problem (filter and sum) over just one step (sum). This further lends itself to the fact that large context windows are ineffective. The filtered list allowed for smaller context window on the final step, summarization. Figure 3: Summary The filter and sort question is the most complex one, requiring few steps to complete. We ask the model to find the objects of random type or color, then sort the object counts by owner name, then concatenate the values together in that order. Figure 1: Needle-in-a-Haystack In the Needles-in-a-Haystack question, we asked the model to find all instances of an object type or color (randomly selected) and sum up the total. Here we saw large divergence in model performance between the top performers and lowest performers. The best performers showed reasoning steps that included filtering to the needed information and then summing that shorter list. Figure 4: Sort"
        },
        {
            "title": "3.2 Study Setup",
            "content": "Figure 2: Needles-in-a-Haystack To collect our answers from each model, we connected via APIs to every model using Python. We stored our initializing dataset and model responses in Postgres database. The Python code iterates through pre-selected range of data points. For each value in that range, we would concatenate that many data points from our data set, formulate question based on that dataset and then randomize the order of the dataset. The model instructions were simply to answer the question based on the provided data in specified JSON format, for ease of retrieval. The resulting sample dataset and question were then fed into each model selected for given range of data points. The resulting answer from each model was then captured and compared to the correct answer."
        },
        {
            "title": "3.3 Analysis Procedure",
            "content": "We collected over 66k rows of data, capturing the model name, question type, input token count, and if the correct answer was achieved. For each question and model combination, we validated we captured enough data by measuring the p-value of the relationship between input token count and correct answer (1 for true 0 for false). Because of the pvalues needed for further validation steps (validation of each graphical data point for bucket/model/accuracy combination), the p-values found at this step were always extremely low <1.0e172 (Figures 5-8). us.meta.llama3-3-70b-instructv1:0 0.00E+00 Figure"
        },
        {
            "title": "Needles Question",
            "content": "p-value claude-3-5-sonnet-20241022 0.00E+00 gemini-2.0-flash 0.00E+00 gemini-2.5-flash-preview-050.00E+00 gpt-4.1 gpt-5 grok-3-latest mistral-medium-2505 o4-mini qwen-plus us.deepseek.r1-v1:0 0.00E+00 0.00E+00 0.00E+00 0.00E+ 0.00E+00 0.00E+00 0.00E+00 us.meta.llama3-3-70b-instructv1:0 0.00E+00 Figure"
        },
        {
            "title": "Needle Question",
            "content": "p-value"
        },
        {
            "title": "Summary Question",
            "content": "p-value claude-3-5-sonnet-20241022 4.05E-244 gemini-2.0-flash 0.00E+00 gemini-2.5-flash-preview-050.00E+00 gpt-4.1 grok-3-latest mistral-medium-2505 o4-mini qwen-plus 0.00E+00 0.00E+00 4.86E-298 1.61E-250 1.98E-294 claude-3-5-sonnet6.97E-183 gemini-2.0-flash 1.51E-183 gemini-2.5-flash-preview-05-20 0.00E+00 gpt-4. gpt-5 grok-3-latest mistral-medium-2505 o4-mini qwen-plus 4.44E0.00E+00 6.44E-194 8.00E-189 0.00E+00 1.51E-193 us.deepseek.r1-v1: 5.32E-250 us.deepseek.r1-v1:0 0.00E+00 us.meta.llama3-3-70b-instructv1:0 4.05E-206 Figure"
        },
        {
            "title": "Sorted Question",
            "content": "p-value claude-3-5-sonnet-20241022 2.34E-172 gemini-2.0-flash 2.37E-176 gemini-2.5-flash-preview-050.00E+00 gpt-4.1 gpt-5 grok-3-latest mistral-medium-2505 o4-mini qwen-plus us.deepseek.r1-v1:0 2.28E-182 0.00E+00 1.74E-182 4.40E0.00E+00 2.59E-180 0.00E+00 us.meta.llama3-3-70b-instructv1:0 1.41E-193 Figure To better tie token input count to correct answer rate, we bucketed input token counts into ranges and averaged the correct answers over the range for each model. For the needle in the haystack question, we used buckets of 5000 tokens because of the large level of accuracy across all models for this question type. For the remaining question types, we used buckets of 100 tokens. To clean the data for bucketing, we did remove datapoints that fell into bucket with only 1 or 2 datapoints. This usually occurred on the high end of the token counts where most tests fell in the preceding buckets but one rolled into new bucket. To prevent skewed results (dramatic swings to 0 or 1), we removed these values to eliminate that bucket from the results. With buckets we then retested p-values for statistical significance. See appendix A.3 for p-values for bucketed data."
        },
        {
            "title": "4 Findings for Q1: Does MECW differ \nfrom MCW",
            "content": "Using buckets, clear data patterns emerged. Low levels of token counts improved upon published model hallucination rates with high confidence levels (Hughes 2023). As token count increased, all models accuracy diverged from their published hallucination rates, providing increasingly erratic results. Model performance, in most cases, could be consistently forced to near 0% accuracy levels if provided too large of context. These findings indicate that there is need for MECW measure across models."
        },
        {
            "title": "5 Findings for Q2: Do different types of \nquestions change the MECW",
            "content": "Our data provided surprising and clear results in this area. Models perform vastly differently to the type of question asked, as expected. However, we expected model rankings across tasks to remain relatively stable. This was not the case, however. Some models handled the needle in the haystack question far better than their peers but well under performed their peers on other question types. This provides an avenue for further research on model performance across task types: coding, scientific research, general Q/A, mathematics, etc."
        },
        {
            "title": "6.1 Model Accuracy Using RAG",
            "content": "All models outperformed their standard hallucination rates for our questions, under certain context size. As context size increased, hallucination rates exceeded base hallucination rates for all models. For the worst performing models, hallucination rates reached near 100%. The same would likely happen to all models, if we continued to test at larger context windows. Since our line of questioning provided both the facts and the question, it was simple form of RAG, suggesting, like other research, that RAG increases model accuracy (Li et al., 2025a). Our research expands on this to show accuracy using RAG can reach near 100% levels, if utilized under the MECW. It also shows RAG can worsen model performance when exceeding the MECW."
        },
        {
            "title": "6.2 Model Selection",
            "content": "Existing production agentic frameworks tend to utilize the best model or multiple models to guarantee accuracy of the results. This comes at detriment to both cost and speed for responses. Understanding the specific use case and MECW for that use case across models allows for better weighing of cost and speed when making model selection. While OpenAI o4-mini performed at the top in the needles problem, if we are only utilizing 500 input tokens or less, we could use DeepSeek r1 at fraction of the price with no reduction in accuracy. The MECW is designed as an effective way to increase LLM accuracy by measuring, understanding and working within the limits of given model and problem. This is especially useful for agents in an agentic framework. Each agent is designed with specific task in mind and the MECW can improve each agents performance to near flawless levels. This is without any further modifications, like temperate, top_p, reverse RAG, or mixture of models to further improve accuracy. Model rankings also changed across tasks. OpenAIs o4-mini was top performer in the Needles in Haystack problem but one of the worst at the Needle in Haystack problem. This further reinforces the need for an MECW measurement to help select the correct model for the correct task."
        },
        {
            "title": "6.1 Implications for GenAI Use",
            "content": "More important than temperature, top_p, seed parameters and other settings, context window size is the most important factor for determining model accuracy. While these other factors do help model accuracy, they only contribute to overall performance by few percentage points at most (Hochlehnert et al., 2025; Zhao et al., 2025). Context window size can vary model's performance from near 100% accuracy to near 0% accuracy. Model Context Windows have grown to outsized amounts as high as 1 million and 10 million tokens. These published limits lead to false promise of model performance up to that amount. Existing platforms have changed architectures to support these large context Windows with the idea that their output performance would improve. Our research shows real world use cases for LLMs should focus on limiting token count in tasks for best results."
        },
        {
            "title": "6.2 Need for New Testing Frameworks",
            "content": "Existing testing frameworks, like AIME24, AIME25 and GPQA, provide limited value on model performance in real world use cases. Furthermore, they provide wide swings in measured accuracy because of the small sample rates. Most applications of Generative AI do not use an LLM alone and leverage some kind of context extension, like RAG. This means we need better testing frameworks for showcasing model performance with more complex use cases. This includes novel question approaches, like those performed by Apple, and context stuffing like what was performed here (Shojaee et al., 2025). Beyond static testing frameworks, we need testing framework designed for testing models MECW across various tasks that can be used by AI developers for their own tasks. Understanding when and where model performance breaks down will help developers understand the limits of given model in given context."
        },
        {
            "title": "6.3 Impact on RAG Systems",
            "content": "Our data does support the notion that RAG systems improve hallucination rates. As an example, GPT-5 did not hallucinate once in our data set, when asked question with under 500 tokens. The problem becomes that as input token amounts increase the hallucination rate increases. As input token counts reach as little as 2000 tokens, some models hallucination rates go as high as 99%. Because of the drastic decline in model performance when using larger context windows, RAG systems leveraging higher token counts decline model performance instead of improving it. Overall, this leads to cascading failure rates when LLMs are chained together, like in agentic frameworks (Meimandi et al., 2025; Xu et al., 2025). The idea of near limitless context window leads developers to believe that an agentic system chaining multiple agents with large context windows will perform reasonably well under most situations. As shown through our research, large context windows degrade model performance so agentic systems relying on large context windows for purposes of RAG will see cascading failures. More importantly, model accuracy can improve above standard hallucination rates simply by providing context windows at the correct size for the model and problem type. This shift in thought prevents cascading model failure by decreasing hallucination rates to point where chaining agents together will not fail at massively increased rates. Our research concludes anyone leveraging large context windows and/or RAG systems should be aware of the kinds of questions that they are posing to their models and the limits of context windows around those questions to prevent or reduce hallucination rates and improve overall model accuracy."
        },
        {
            "title": "6.4 Limitations",
            "content": "Multivariate testing: Our study focuses on one variable, token count. Isolating this single variable did give us rich results on its impact on LLM performance. However, further testing could be done on token counts tied to other variables, like top-p, to see if another variable allows for larger MECWs. Real world problems: Our questions and dataset are very simple. Real world problems might have more structured data input or attached documents, like pdf or excel. Testing the effects of data format could lead to more understanding in how to effectively use models context window. Our questions were quite simple. Prompt engineering techniques could be tested to see if there is an uplift in model performance on larger context windows."
        },
        {
            "title": "7 Conclusion",
            "content": "Our findings conclude that the Maximum Context Window does vary widely from the Maximum Effective Context Window (MECW) for all models tested. Additionally, MECW changes with the type of problem presented to the model. While we did not test every model, we hypothesize these statements hold true for all models currently on the market. Our results suggest effectively using models context window is the highest contributing factor to the hallucination rate of the model."
        },
        {
            "title": "Acknowledgements",
            "content": "A special thank you to all of those who inspired this research, pushed me to continue it and provided support along the way."
        },
        {
            "title": "References",
            "content": "Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu. 2023. L-Eval: Instituting Standardized Evaluation for Long Context Language Models. ArXiv:2307.11088 Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu and Lingpeng Kong. 2024. Why Does the Effective Context Length of LLMs Fall Short? ArXiv:2410.18745v1. Anthropic. 2024. Claude 3.5 Sonnet Model Card Addendum. URL: https://www-cdn.anthropic.com/ fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li. 2023. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. ArXiv:2308.14508 Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi. 2025. Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find. ArXiv:2505. Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, Timofey Bryksin. 2024. Long Code Arena: Set of Benchmarks for Long-Context Code Models. ArXiv:2406.11612 Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez. 2025. Reasoning Models Don't Always Say What They Think. ArXiv:2505.05410 Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu. 2024. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs. ArXiv:2412. James Chua and Owain Evans. 2025. Are DeepSeek R1 And Other Reasoning Models More Faithful? ArXiv:2501.08156 Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Christian Norgaard, Nayantara Mudur, Martyna Beata Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman. 2025. CURIE: Evaluating LLMs on Multitask Scientific Long-Context Understanding and Reasoning. ArXiv:2503.13517 DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. ArXiv:2501.12948 Bowen Ding, Yuhan Chen, Futing Wang, Lingfeng Ming and Tao Lin. 2025. Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model. ArXiv:2506.23840 Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen. 2023. BAMBOO: Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. ArXiv:2309.13345 Sicheng Feng, Gongfan Fang, Xinyin Ma, Xinchao Wang. 2025. Efficient Reasoning Models: Survey. ArXiv:2504.10903 Yunfan Gao, Yun Xiong, Wenlong Wu, Zijing Huang, Bohan Li, Haofen Wang. 2025. U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack. ArXiv:2503. Google Gemini. 2025a. Gemini 2.0 Flash Model Card. URL: https://storage.googleapis.com/modelcards/documents/gemini-2-flash.pdf Google Gemini. 2025b. Gemini 2.5 Flash & 2.5 Flash Image Model Card. URL: https://storage.googleapis.com/ deepmindmedia/Model-Cards/Gemini-2-5-Flash-ModelCard.pdf Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu and Matthias Bethge. 2025. Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility. ArXiv:2504. Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long T. Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister. 2024. Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization. ArXiv:2406.16008 Thomas Hudson, Noura Al Moubayed. 2022. MuLD: The Multitask Long Document Benchmark. ArXiv:2202.07362 Simon Hughes, Minseok Bae. 2023. Hughes Hallucination Evaluation Model (HHEM) Leaderboard. https://huggingface.co/ spaces/vectara/Hallucination-evaluationleaderboard Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das. 2025. The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input. ArXiv:2501.03200 Gregory Kamradt. 2023. Needle-in-a-Haystack pressure testing llms. Accessed: 2025-09-06. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev. 2024. In Search of Needles in 11M Haystack: Recurrent Memory Finds What LLMs Miss. ArXiv:2402.10790 Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang. 2025. ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage. ArXiv:2410.16848 Mosh Levy, Alon Jacoby, Yoav Goldberg. 2024. Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models. ArXiv:2402.14848 Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang. 2023. LooGLE: Can Long-Context Language Models Understand Long Contexts? ArXiv:2311.04939 Haystack for Memory Based Large Language Models. ArXiv:2407.01437 Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, OpenAI. 2025a. Introducing GPT-4.1 in the API. Fei Huang, Shuai Wang, Minhao Cheng. 2025a. LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs -- No Silver Bullet for LC or RAG Routing. ArXiv:2502. Ming Li, Zhengyuan Yang, Xiyao Wang, Dianqi Li, Kevin Lin, Tianyi Zhou, Lijuan Wang. 2025b. What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding. ArXiv:2506.06998 Zhan Ling, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen. 2025. LongReason: Synthetic Long-Context Reasoning Benchmark via Context Expansion. ArXiv:2501.15089 Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni and Percy Liang. 2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv:2307.03172. Kiana Jafari Meimandi, Gabriela Aránguiz-Dias, Grace Ra Kim, Lana Saadeddin, Mykel J. Kochenderfer. 2025. The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims. ArXiv:2506.02064 Meta. 2025a. Model Information. URL https://github.com/meta-llama/llamamodels/blob/main/models/llama4/MODEL_CARD .md Meta. 2025b. Model Information. URL https://github.com/meta-llama/llamamodels/blob/main/ models/llama3_3/ MODEL_CARD.md Mistral AI. 2025. Medium is the new large. URL: https://mistral.ai/news/mistral-medium-3 Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze. 2025. NoLiMa: Long-Context Evaluation Beyond Literal Matching. ArXiv:2502.05167 Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, Soham Dan. 2024. Needle in the URL: https://openai.com/index/gpt-4-1/ OpenAI. 2025b. Introducing OpenAI o3 and o4mini. URL: https://openai.com/index/introducingo3-and-o4-mini/ OpenAI. 2025c. GPT-5 System Card. URL: https://cdn.openai.com/gpt-5-system-card.pdf Qwen Team. 2025. Qwen3: Think Deeper, Act Faster. URL: https://qwenlm.github.io/blog/qwen3/ Saurav Pawar, S.M Towhidul Islam Tonmoy, Mehedi Zaman, Vinija Jain, Aman Chadha, Amitava Das. 2024. The What, Why, and How of Context Length Extension Techniques in Large Language Models Detailed Survey. ArXiv:2401.07872 Ofir Press, Noah A. Smith, Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. ArXiv:2108.12409 Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad Farajtabar. 2025. The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity. ArXiv:2506.06941 xAI. 2025. Grok 3 Beta The Age of Reasoning Agents. URL: https://x.ai/news/grokZheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos. 2024. From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data. ArXiv:2406.19292 Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Melroy Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou and Graham Neubig. 2025. TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks. ArXiv:2412.14161 or Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen, Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen. 2025. Survey of Large Language Models. ArXiv:2303.18223 Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen. 2024. HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly. ArXiv:2410.02694 Tianyi Zhuang, Chuqiao Kuang, Xiaoguang Li, Yihua Teng, Jihao Wu, Yasheng Wang, Lifeng Shang. 2025. DocPuzzle: Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities. ArXiv:2502. How many {object} are there? For the summary question, we simply ask: How many objects are there total? For the sorted Needles-in-a-Haystack, we ask variant of question depending on randomly selected color or object type. Find all people with {color} objects. Sort them by first and last name. Concatenate the number of objects they have into one long string value in the order they were sorted. or Find all people with {object}. Sort them by first and last name. Concatenate the number of objects they have into one long string value in the order they were sorted."
        },
        {
            "title": "A Appendix",
            "content": "A.2 Definitions A.1 Survey Questions For the Needle-in-a-Haystack question, we ask the following: How many objects does {person} have? For the Needles-in-a-Haystack, we ask variant of question depending on randomly selected color or object type. How many {color} objects are there? A.3 Graphical Data 1) Maximum Effective Context Window: The maximum token count, for given problem type, before the model performance begins to degrade in measurable fashion. 2) Cascading Failures: where an agentic framework consisting of multiple agents fails most of the time because each agent has mediocre success rate. 3 agent system with 70% success per agent results in system with 34.3% success rate. A.4 P-Value Calculation Charted P-Values for each bucket for each model for each problem set. A.4.1 Needle in Haystack Question claude-35-sonnet20241022 gemini2.0-flash gemini2.5-flashpreview05-20 gpt-4.1 grok-3latest mistralmediumo4-mini qwen-plus us.deepse ek.r1-v1:0 us.meta.lla ma3-3-70binstructv1:0 5000 1.01E-164 3.14E-184 9.35E-186 8.69E-185 8.96E-182 5.90E-188 4.51E-183 2.18E-188 2.55E-186 6.76E-187 10000 1.45E-122 4.24E-107 4.15E-104 8.06E-107 6.21E-112 1.25E-115 1.48E-107 7.22E-116 2.08E-103 1.10E15000 9.83E-93 6.11E-128 5.55E-125 7.12E-126 4.21E-132 1.99E-124 3.89E-127 1.01E-124 1.50E-125 1.42E-125 20000 1.11E-111 7.24E-125 7.18E-127 8.92E-126 4.64E-123 6.57E-149 8.33E-126 5.98E-149 2.33E-126 7.66E-126 25000 3.54E-99 1.20E-192 2.08E-184 2.77E-189 3.90E-209 3.78E-145 1.07E-189 7.72E-146 3.82E-185 3.82E-184 30000 8.55E-97 1.46E-175 6.12E-151 6.78E-142 1.21E-101 1.84E-175 6.79E-142 3.09E-182 1.98E-150 4.64E-162 35000 8.76E-152 3.68E-149 8.49E-164 2.84E-169 8.17E-194 1.07E-195 6.22E-159 7.57E-152 1.68E-159 1.60E-172 40000 7.84E-79 1.47E-162 4.31E-178 1.27E-183 1.35E5.38E-26 6.01E-206 1.25E-166 2.99E-163 8.14E-204 2.99E45000 9.55E-40 5.71E-177 6.13E-194 4.15E-183 2.01E-169 50000 5.31E-40 2.09E-211 1.02E-199 4.02E-195 1.89E-193 55000 1.05E-92 8.98E-211 2.11E-257 3.56E-255 8.64E-229 60000 65000 75000 80000 85000 90000 2.45E-249 7.64E-238 5.29E-243 7.87E-279 0.00E+00 3.42E-303 0.00E-02 0.00E+ 1.23E-41 0.00E+00 2.79E-263 3.17E-269 0.00E+00 1.52E-86 2.91E-105 2.57E-86 A.4.2 Needles in Haystack Question claude-35-sonnet20241022 gemini2.0-flash gemini2.5-flashpreview05-20 gpt-4.1 gpt-5 grok-3latest mistralmedium2505 o4-mini qwen-plus us.deepse ek.r1-v1:0 us.meta.lla ma3-3-70binstruct-v1:0 2.65E-75 2.59E-136 1.07E-113 4.53E-109 7.64E-81 3.73E-136 9.31E-118 8.10E-131 9.29E-113 1.50E-129 8.53E-105 200 3.30E-137 1.07E-177 7.62E-150 2.15E-153 2.68E-105 2.55E-181 2.37E-163 2.25E-174 3.98E-165 5.22E-174 3.93E-173 300 2.54E-143 2.35E-194 1.58E-173 4.66E-172 4.12E-119 1.37E-202 1.52E-182 1.36E-193 4.03E-183 1.93E-191 5.55E-197 400 4.98E-156 2.70E-216 1.39E-188 6.51E-192 3.02E-136 1.07E-217 8.73E-207 5.31E-216 4.48E-202 1.58E-205 5.99E-217 500 7.09E-176 1.23E-203 8.32E-192 8.38E-199 2.55E-141 7.25E-215 1.32E-197 9.69E-210 4.32E-198 2.60E-216 4.03E600 3.56E-166 4.58E-213 9.18E-191 3.97E-192 8.98E-148 8.60E-214 5.73E-196 8.16E-204 3.53E-199 7.57E-208 1.79E-212 700 2.76E-174 8.17E-198 8.19E-185 6.95E-192 4.83E-159 4.21E-200 1.87E-191 6.00E-211 1.74E-192 7.12E-205 1.06E-192 800 9.50E-173 4.07E-200 8.98E-189 4.66E-188 4.76E-159 2.77E-212 1.55E-189 2.10E-207 1.38E-191 6.51E-200 3.83E-209 900 1.24E-157 2.74E-119 1.10E-185 5.94E-132 5.95E-176 1.40E-87 4.03E-190 6.69E-216 4.63E-195 1.71E-216 7.06E-189 1000 9.19E1100 5.62E-176 1200 1.03E-146 1300 1.76E-173 1400 1.66E-171 1500 3.61E-186 1600 1.30E1700 9.83E-198 1800 1.24E-163 1900 6.88E-226 2000 3.73E-191 2100 4.75E-126 2200 2.99E2300 2.15E-193 2400 4.02E-187 2500 1.46E-222 2600 3.09E-196 2700 1.58E-190 8.00E-34 2900 3000 3100 3200 2.51E4.17E-173 4.38E-88 1.67E-286 1.42E-97 2.16E-281 2.02E-37 2.22E-221 1.59E4.35E-287 8.32E-289 1.68E-206 1.97E-182 3.24E-292 4.40E2.73E-196 1.01E-193 8.20E-275 8.42E-271 7.71E-255 1.35E0.00E+00 0.00E-02 1.43E-196 6.49E-206 2.84E-228 7.45E6.71E-178 1.94E-210 8.51E-243 1.79E-238 8.56E-260 1.30E6.54E-263 2.68E-254 1.14E-251 9.23E-217 1.44E-274 2.45E2.36E-216 3.48E-224 1.08E-271 3.37E-223 1.74E-160 1.33E3.36E-203 1.61E-161 1.65E-246 7.34E-221 6.87E-239 6.13E1.58E-202 1.06E-206 1.14E-235 7.73E-211 4.46E-218 1.16E1.57E-284 5.74E-229 A.4.3 Summarization Question claude-35-sonnet20241022 gemini2.0-flash gemini2.5-flashpreview05gpt-4.1 gpt-5 grok-3latest mistralmedium2505 o4-mini qwen-plus us.deepse ek.r1-v1:0 us.meta.lla ma3-3-70binstruct-v1:0 0 1.19E-38 1.03E-50 4.92E2.99E-50 9.51E-13 5.19E-53 7.28E-48 7.39E-50 1.43E1.31E-50 1.52E-40 100 200 1.45E-72 2.53E3.97E-95 1.59E-97 1.38E-91 2.85E-99 5.22E-91 1.54E1.48E-90 6.32E-95 9.40E-96 6.20E-92 2.96E-122 2.89E-122 3.59E-124 1.53E-113 2.66E-129 1.59E-115 3.91E-125 2.74E-118 2.24E-122 8.52E-120 300 6.10E-100 6.91E-145 1.55E-142 1.54E-143 2.27E-127 6.50E-149 2.48E-131 1.04E-143 3.53E-131 1.78E-144 1.22E-139 400 8.38E-117 1.05E-136 1.59E-148 2.64E-147 2.96E-144 5.87E-138 2.04E-144 7.40E-152 1.61E-139 6.30E-152 1.22E500 3.59E-124 6.05E-14 9.97E-153 6.19E-19 4.73E-152 1.69E-12 7.33E-58 2.18E-177 1.27E-69 6.52E2.53E-62 600 700 800 900 2.19E1.22E-17 1.38E-160 1.30E-14 4.21E-157 8.79E-19 8.32E-14 1.56E-186 1.67E-14 1.51E-183 1.73E8.96E-12 1.26E-15 1.78E-86 1.00E-15 3.77E-170 1.17E-14 3.15E-15 5.45E5.39E-15 2.73E-177 4.49E-16 3.66E-11 3.57E-15 9.49E-155 2.71E-16 3.91E-177 8.08E5.13E-15 5.02E-193 1.32E-13 4.02E-198 4.25E-17 5.88E-14 9.68E-09 1.28E-143 8.59E-12 3.41E6.83E-17 2.33E-177 5.40E-17 4.57E-175 8.49E-16 1000 1.09E-13 1.34E-09 1200 4.09E-13 1300 1400 1600 1700 1800 1900 2000 2200 2300 2400 2500 2600 2800 2900 3000 3100 1.07E-134 3.03E2.11E-159 1.36E-157 1.59E-90 1.22E-191 1.00E-154 1.79E2.57E-204 7.20E-194 1.34E-191 1.01E-193 1.22E-207 7.56E4.72E-206 5.93E-206 6.43E-202 2.28E-198 4.38E-215 8.58E5.76E-227 4.46E-230 5.61E-226 6.11E-230 8.15E-201 4.43E9.38E-208 5.29E-261 7.58E-156 6.88E-149 1.39E-172 2.05E1.20E-156 6.00E-172 2.83E-177 2.29E-159 4.26E-161 3.94E1.69E-175 1.90E-181 3.46E-177 1.60E-169 3.92E-169 6.20E1.82E-183 3.50E-178 2.71E-186 3.55E-171 7.35E-174 6.49E1.50E-189 1.89E-159 7.44E-193 8.22E-188 5.31E-188 1.05E2.71E-190 1.10E-193 1.06E-182 6.74E-222 3200 3400 3500 3600 5.74E-284 2.24E-171 3.02E1.00E-134 3.13E-38 2.12E-39 1.64E-45 A.4.4 Find and Sort Question"
        },
        {
            "title": "Sorted \nQuestio\nn",
            "content": "claude-35-sonnet20241022 gemini2.0-flash gemini2.5-flashpreview05-20 gpt-4.1 gpt-5 grok-3latest mistralmedium2505 o4-mini qwen-plus us.deepse ek.r1-v1:0 us.meta.lla ma3-3-70binstruct-v1:0 200 300 400 500 600 800 900 1000 1100 1200 1400 1500 1600 1700 1800 2000 2100 5.60E-52 3.49E-72 2.01E-70 6.31E2.94E-61 3.94E-75 6.77E-67 7.22E-72 7.22E-66 1.41E2.06E-68 1.97E-71 1.73E-91 3.17E-91 7.91E-93 4.85E-101 5.16E1.10E-85 9.84E-93 3.99E-86 4.11E-92 2.68E-92 7.01E-79 2.88E-109 8.87E-108 5.38E-107 6.88E-120 6.22E-116 2.69E-104 6.66E-107 2.89E-103 1.23E-105 5.01E1.65E-85 3.06E-115 5.04E-113 5.47E-118 9.61E-125 1.23E-118 2.97E-107 4.78E-117 8.85E-104 2.52E-115 6.10E-111 3.82E-90 1.18E-45 1.08E-129 9.22E-60 8.95E-140 3.38E-34 2.40E-88 3.20E5.67E-94 1.06E-123 2.06E-95 2.54E-96 2.72E-125 1.70E-153 4.74E5.63E-132 8.01E-15 1.21E-117 2.61E-151 2.23E-121 2.26E2.86E-110 4.61E-149 1.92E-107 3.86E-109 4.54E-116 2.56E5.20E-104 1.23E-106 1.32E-105 1.96E-168 3.61E-111 4.89E1.71E-104 7.24E-175 1.83E-112 8.40E-112 4.11E-114 8.57E8.32E-109 1.30E-113 1.67E-85 5.92E-207 5.41E-80 1.06E7.55E-59 2.71E-191 3.87E-54 5.70E-65 2.44E-169 7.79E1.05E-188 5.17E-199 4.00E-194 4.11E-191 3.57E-"
        }
    ],
    "affiliations": []
}