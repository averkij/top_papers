{
    "paper_title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource Dialogue Applications",
    "authors": [
        "Sathya Krishnan Suresh",
        "Wu Mengjun",
        "Tushar Pranav",
        "Eng Siong Chng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The scarcity of domain-specific dialogue datasets limits the development of dialogue systems across applications. Existing research is constrained by general or niche datasets that lack sufficient scale for training dialogue systems. To address this gap, we introduce DiaSynth - a synthetic dialogue generation framework capable of generating high-quality, contextually rich dialogues across a wide range of domains. Unlike existing frameworks, DiaSynth uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to generate dynamic, domain-specific dialogues with simulated personas and diverse conversational features. We perform our experiments by generating synthetic data using different LLMs and few-shot examples from DialogSum and SAMSum. The pretrained language models fine-tuned on the synthetic data outperform the base models by 16.47% on dialogue summarization, while the comparison between models fine-tuned on in-domain data and synthetic data shows that the synthetic data is able to capture 90.48% of the performance distribution of the in-domain data on dialogue summarization. The quality of the data generated also increases as we increase the size of LLM from 3B to 8B. These results validate DiaSynth's potential as a robust alternative to traditional data collection methods. We open source the code and data generated for future research."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 2 0 2 0 9 1 . 9 0 4 2 : r DIASYNTH: SYNTHETIC DIALOGUE GENERATION FRAMEWORK FOR LOW RESOURCE DIALOGUE APPLICATIONS"
        },
        {
            "title": "A PREPRINT",
            "content": "Sathya Krishnan Suresh1, Wu Mengjun1, Tushar Pranav2, Eng Siong Chng1 1Nanyang Technological University, Singapore, 2Singapore Institute of Technology, Singapore sathyakr001@e.ntu.edu.sg, mwu016@e.ntu.edu.sg, pranav.tushar@singaporetech.edu.sg, ASESChng@ntu.edu.sg October 16,"
        },
        {
            "title": "ABSTRACT",
            "content": "The scarcity of domain-specific dialogue datasets limits the development of dialogue systems across applications. Existing research is constrained by general or niche datasets that lack sufficient scale for training dialogue systems. To address this gap, we introduce DiaSynth - synthetic dialogue generation framework capable of generating high-quality, contextually rich dialogues across wide range of domains. Unlike existing frameworks, DiaSynth uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to generate dynamic, domain-specific dialogues with simulated personas and diverse conversational features. We perform our experiments by generating synthetic data using different LLMs and few-shot examples from DialogSum and SAMSum. The pretrained language models fine-tuned on the synthetic data outperform the base models by 16.47% on dialogue summarization, while the comparison between models fine-tuned on in-domain data and synthetic data shows that the synthetic data is able to capture 90.48% of the performance distribution of the in-domain data on dialogue summarization. The quality of the data generated also increases as we increase the size of LLM from 3B to 8B. These results validate DiaSynths potential as robust alternative to traditional data collection methods. We open source the code and data generated for future research."
        },
        {
            "title": "Introduction",
            "content": "Dialogue systems are crucial in natural language processing due to applications like customer service chatbots and virtual assistants. Their effectiveness depends on large, high-quality, domain-specific datasets. The lack of largescale, high-quality datasets across domains like academic discussions, healthcare, and everyday conversations poses challenge. This scarcity limits the development of dialogue systems that generalize well across domains. Prior work (Feng et al. [2020], Zeng et al. [2020], Budzianowski et al. [2018]) collects domain-specific dialogues but often lacks depth, scale, or domain diversity. On the one hand, the conversations in domain specific dataset are superficial and do not go deep into the domain. On the other hand, niche domain dialogue datasets, while contextually rich, often suffer from limited scale. This imbalance hinders dialogue system development in underrepresented domains, where data collection is costly and complex. To address these problems, we introduce DiaSynth, synthetic dialogue generation framework that produces contextually rich and realistic dialogues tailored to specific domains. DiaSynth, using Large Language Model (LLM), generates high-quality conversations by simulating personas and conversation characteristics like tone and formality. With LLMs and Chain of Thoughts (CoT) Wei et al. [2024], DiaSynth generates dialogues that mimic real-world conversations for wide range of domains. CoT plays crucial part by building different environments for different personas (refer to Appendix for examples) which influence varied conversations. This approach addresses data scarcity and offers scalable, cost-effective alternative to traditional methods."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT Figure 1: Overview of DiaSynth To validate the effectiveness of DiaSynth, we evaluated the framework on two criteria: the quality of the generated data and the usability of this data for downstream tasks. The results for the quality criterion showed that the data quality improved with the scale of the model. For usability, we tested DiaSynth on dialogue summarization. Models fine-tuned on DiaSynth data outperformed base versions by 16.47% on average. Additionally, DiaSynths synthetic data captures 90.48% of in-domain performance, highlighting its potential as strong alternative when domain-specific data is unavailable. The remainder of this paper is organized as follows: Section 2 reviews related work on dialogue datasets and synthetic data. Section 3 details the DiaSynth framework and methodology. Section 4 describes our experimental setup and evaluation. Section 5 compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section 6 concludes with summary of our findings and Section 7 discusses the limitations of DiaSynth and potential future directions for this research."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Personality in Synthetic Data Generation In recent years, there has been significant increase in research focused on synthetic dialogue generation, largely driven by advancements in Large Language Models (LLMs). To generate realistic and diverse synthetic data, researchers have incorporated personalities, profiles, and character information when prompting LLMs to generate dialogues Han et al. [2024]. By enhancing dialogue realism through the simulation of various personality profiles, utilizing the Big Five personality model, and employing structured prompts, this approach has improved task performance in models fine-tuned on these generated dialogues compared to those trained on general chit-chat datasets. Moreover, integrating personas into synthetic data generation prompts Chan et al. [2024] has demonstrated that models fine-tuned on personalized synthetic data outperform some LLMs of much larger scales. The inclusion of personas in prompts provides diversity in difficulty levels and ranges within the synthetic data, enabling the models to handle situations of varying complexity. Our approach involves persona extraction after generating subtopics related to the general topics. This enables the generated dialogues to be more in-depth and specific to those subtopics, enhancing both the scale and quality of domain-specific dialogue generation."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT LLM Few-shot examples Number of Samples Avg. number of turns Avg. number of tokens per turn Diversity (ROUGE-L) Phi-3 InternLM-2.5 LLaMA-3 GPT-4o Phi-3 InternLM-2.5 LLaMA-3 GPT-4o DialogSum DialogSum DialogSum DialogSum SAMSum SAMSum SAMSum SAMSum 1215 1035 1154 1375 1410 1135 1195 9.13 9.23 6.86 15.16 13.98 13.96 10.54 15.43 20.38 27.98 31.99 15.96 13.94 19.07 20.41 13.53 0.27 0.30 0.29 0.29 0.27 0.29 0.29 0.28 Table 1: Data Statistics 2.2 Prompting Task-Oriented Dialogue Generation Prompt-based techniques have also emerged as powerful methods for generating high-quality synthetic dialogues, particularly for task-oriented dialogue systems. Steindl et al. [2023] explore the generation of synthetic dialogues from structured prompts, focusing on enhancing task-oriented dialogue systems. Their work demonstrates that prompt engineering can produce dialogues that are contextually appropriate and improve system performance by aligning synthetic data more closely with real-world requirements. To achieve higher quantity, diversity, and creativity in human-written instruction data, Wang et al. [2022] propose inputting prompts to LLMs to generate instructions based on small set of seed human-written instructions. This approach aligns the expanded training data more closely with desired task objectives and allows for iterative improvements, producing more nuanced and effective dialogues that meet specific task demands. Similarly, our study expands topics into subtopics, ensuring that the generated dialogues provide more in-depth and high-quality conversations. By doing so, we aim to produce synthetic data that not only covers broader range of scenarios but also delves deeper into each topic, thereby enhancing the overall effectiveness of the dialogue systems trained on this data. 2.3 Existing Task-Oriented Dialogue Datasets In addition to prompt-based synthetic data generation, various large-scale dialogue datasets have been instrumental in advancing task-oriented dialogue systems. Among these, the MultiWOZ dataset Budzianowski et al. [2018] is prominent resource, providing richly annotated dialogues across multiple domains. MultiWOZ has enabled researchers to train models capable of handling complex, multi-turn interactions across diverse tasks. The nature of MultiWOZs annotations has made it benchmark for evaluating the performance of dialogue systems, though it is often complemented by synthetic data to introduce further diversity and variation in dialogue scenarios. Similarly, Doc2Dial Feng et al. [2020] is another widely used dataset designed specifically for document-grounded dialogue systems. Doc2Dial includes conversations grounded in structured documents, focusing on providing users with accurate and relevant information based on their inquiries. This dataset has been instrumental in improving the ability of dialogue systems to retrieve and generate accurate responses when interacting with complex information sources. However, much like MultiWOZ, Doc2Dials scope is limited to the predefined topics and domains covered within the dataset, which can restrict model generalizability to new or unseen situations. To overcome the domain-specific limitations of datasets, our study adopts synthetic data generation approach that expands existing topics into subtopics, thus providing broader and deeper pool of conversational data. By incorporating both task-oriented prompts and personas, our generated dialogues aim to complement these datasets by offering more personalized and contextually rich conversations, thereby enhancing the robustness and versatility of dialogue systems"
        },
        {
            "title": "3 DiaSynth",
            "content": "DiaSynth is synthetic dialogue generation framework designed to address the scarcity of high-quality, large-scale, domain-specific dialogue datasets. DiaSynth uses an LLM and CoT reasoning to simulate diverse, nuanced dialogues. DiaSynth takes list of user-provided topics to generate dialogues. The users can optionally provide few-shot examples of the format in which they want the dialogue to be generated. Directly generating dialogues from user topics would be too superficial due to their lack of specificity. To overcome this lack of specificity, we generate sub topics for each of the topics given by the user. Generating dialogues from the subtopics will have specificity but the dialogues will lack variety. This is because every dialogue is influenced implicitly by the personas of the people involved in the dialogue and, other characteristics such as the location, emotion and more. To enhance variety and depth, we generate personas per subtopic and create dialogues for all persona-subtopic combinations. To further ground the dialogues in various"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT coherent error recovery consistent diverse depth likeable understand flexible informative inquisitive Phi-3 InternLM-2.5 LLaMA-3 GPT-4o Phi-3 InternLM-2.5 LLaMA-3 GPT-4o 0.9536 0.8439 0.9684 0.9525 0.9161 0.8746 0.9829 0.9939 0.9440 0.8313 0.9522 0.9407 0.9088 0.8647 0.9677 0.9876 0.9540 0.8359 0.9570 0. 0.9199 0.8734 0.9757 0.9878 DIALOGUESUM 0.9534 0.8353 0.9596 0.9423 0.9521 0.8352 0.9592 0.9425 -0.0005 0.0048 0.0032 0.0121 SAMSUM 0.9161 0.8655 0.9712 0.9836 0.9130 0.8661 0.9731 0.9824 -0.0004 0.0033 0.0003 0.0083 Table 2: FED scores 0.9353 0.8278 0.9453 0.9368 0.9014 0.8582 0.9593 0. -3.96E-05 -0.0046 -0.0063 -0.0027 -0.0024 -0.0019 -0.0048 0.0004 0.0009 0.0042 0.0063 0.0085 0.0034 0.0106 0.0100 0.0141 -0.0033 0.0069 0.0105 0.0144 -0.0040 0.0028 0.0029 0. settings and characteristics, we employ CoT reasoning during the generation process. DiaSynth employs CoT to reason about the settings and characteristics of dialogue, which are listed in Appendix C, ensuring that the dialogues are contextually rich and realistic. This three-stage pipeline not only guarantees the quality of the generated dialogues but also allows for exponential scalability, as illustrated programmatically in Appendix B. 3.1 Subtopic Generation Subtopic generation is crucial step in DiaSynths pipeline, since it enhances the specificity and depth of the dialogues that will be generated later. For each primary topic given by the user, DiaSynth generates multiple subtopics, effectively narrowing the focus of the conversation. This breakdown is necessary because the primary topics are often too general to generate contextually rich dialogues on their own. For instance, topic like healthcare can be expanded into subtopics such as doctor-patient consultations, mental health discussions, and medical diagnostics, each of which offers more focused context for dialogue generation. To achieve this, DiaSynth prompts an LLM to generate the user specified number of subtopics for each primary topic. We also run similarity check between each of the subtopics and remove subtopics that are too similar to other subtopics using threshold. 3.2 Persona Generation Personas of the individuals involved in conversation are primary influencers in determining how conversation pans out. Using random personas from persona datasets and prompting the LLM to simulate dialogue between them about random topic often leads to superficial dialogues that lack depth and contextual richness. To address this issue, DiaSynth generates user-specified number of personas for each subtopic, ensuring that the personas are conditioned on the subtopic context. This conditioning prompts the LLM to create personas that are most likely to engage in meaningful dialogue about the subtopic, such as medical professional and patient discussing \"medical diagnostics\" or researcher and student talking about \"academic publishing.\" We also run similarity check for the personas too. The conditioned persona generation is crucial because it ensures that future dialogues will not only be contextually rich but also exhibit high level of depth. Each dialogue will be between two personas who have relevant expertise or perspectives on the given subtopic, allowing the conversation to explore nuances that would otherwise be missed in generic dialogue setting. 3.3 Dialogue Generation The final stage in DiaSynths pipeline is the generation of dialogues, where all the componentssubtopics, personas, and characteristicsconverge to create contextually rich and realistic conversations. This step uses an LLM as the backbone and CoT as the reasoning mechanism, allowing the model to simulate dialogues that incorporate various aspects of human interaction. DiaSynth generates dialogues by pairing all persona-subtopic combinations. The process also integrates predefined characteristics  (Table 11)  like emotional state, formality, and familiarity to guide the flow and style. These characteristics are defined in the CoT prompt, guiding the LLM to generate realistic, contextually appropriate dialogues. The importance of CoT and the lack of it affects the quality of the dialogues, which is shown quantitatively in Appendix E."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In this section, we detail the experimental setup used to evaluate the effectiveness of DiaSynth. Our evaluation focuses on two criteria - quality of the dialogues generated and usability of the dialogues generated for downstream task."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT coherence diversity flexibility understandability inquisitiveness consistency informativeness likeability depth error recovery Phi-3 InternLM-2.5 LLaMA-3 GPT-4o Phi-3 InternLM-2.5 LLaMA-3 GPT-4o 0.0286 0.0069 0.0189 0.0039 0.0325 0.0128 0.0288 0.0055 0.0310 0.0196 0.0430 0.0156 0.0372 0.0408 0.0655 0.0162 0.0218 0.0084 0.0186 0. 0.0260 0.0194 0.0306 0.0094 0.0193 0.0061 0.0220 0.0039 0.0270 0.0174 0.0365 0.0084 DIALOGUESUM 0.0363 0.0244 0.0415 0.018 SAMSUM 0.0395 0.0504 0.0622 0.0186 0.0369 0.0137 0.0321 0.0080 0.0415 0.0306 0.0612 0.0119 Table 3: GPTScore 0.0172 0.0148 0.0318 0.0097 0.0201 0.0328 0.0542 0. 0.0213 0.0050 0.0110 0.0026 0.0232 0.0142 0.0168 0.0038 0.0117 0.0080 0.0201 0.0053 0.0126 0.0177 0.0270 0.0079 0.0342 0.0197 0.0440 0.0135 0.0290 0.0407 0.0558 0. Quality of the dialogues is evaluated using metrics such as FED, GPTScore, and G-Eval. We evaluate the usability of DiaSynth-generated dialogues by using summarization as the downstream task. 4.1 Quality of the dialogues To evaluate the quality of the dialogues, we employ the metrics that have been developed for evaluating the quality of text generated by LLMs. We use the following metrics: FED Mehri and Eskenazi [2020] - FED evaluates dialogue quality by comparing the probabilities of appended positive and negative utterances. GPTScore Fu et al. [2024] - GPTScore assesses dialogue quality by asking an LLM to evaluate criteria like coherence and diversity, with scores based on the probability of affirmative responses. G-Eval Liu et al. [2023] - G-Eval rates dialogue on 1-3 scale across criteria, with final scores as weighted average from the LLMs probability distribution. To validate the framework across models and also domains, we generate data using three open source LLMs, one closed source LLM and also use few shot examples from two different dialogue datasets. The open sourced LLMs are - Phi-3 Abdin et al. [2024], InternLM-2.5 Cai et al. [2024], LLaMA-3 Dubey et al. [2024] and the closed source LLM used is GPT-4o. The 8-bit quantized versions of the open source LLMs were used for faster experimentation and generation. The two different dialogue datasets that were used as few-shot examples are DialogSum Chen et al. [2021] and SAMSum Gliwa et al. [2019] 4.2 Downstream Task - Summarization To evaluate the usability of the dialogues generated by DiaSynth, we choose summarization as the downstream task. Summarization, key application of dialogue systems, aims to generate concise, contextually relevant summaries. We use three established evaluation metricsQAGS, BERTScore, and ROUGE-Lto assess the performance of summarization models fine-tuned on DiaSynth-generated data. QAGS Wang et al. [2020] - QAGS evaluates factual consistency by generating questions from the summary and comparing answers to those from the source dialogue. BERTScore Zhang* et al. [2020] - BERTScore measures semantic similarity between generated and reference summaries using contextual embeddings. ROUGE-L Lin [2004] - ROUGE-L measures longest common subsequence (LCS) overlap between generated and reference summaries. We fine-tune pretrained summarization models like DistilBART, BART Lewis et al. [2020], T5 Raffel et al. [2020] and LED Beltagy et al. [2020], on DiaSynth-generated dialogues and evaluate their performance using the above metrics. We evaluate the usability of DiaSynth in two key aspects: first, by assessing the performance improvement of models fine-tuned on DiaSynth-generated data compared to the pretrained models; and second, by measuring the extent to which DiaSynth-generated data reflects real-world data distribution by comparing the performance of models fine-tuned on DiaSynth data versus those fine-tuned on in-domain data."
        },
        {
            "title": "5 Results",
            "content": "This section discusses the results of the data generated using DiaSynth (quality of the data and usability in downstream tasks) with different LLMs and varying few-shot examples. Specifically, we utilized Phi-3, InternLM-2.5, LLaMA-"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT and GPT-4o as the LLM backbones, and the few-shot examples were sourced from DialogueSum and SAMSum datasets. These combinations allow us to evaluate the robustness and adaptability of DiaSynth across different models and few shot examples. In total, eight distinct datasets were generated using DiaSynth by pairing each LLM with the two sets of few-shot examples, resulting in all possible combinations. For each combination, DiaSynth was provided with the same 16 broad topics and tasked with generating 6 subtopics for each topic, followed by creating 6 personas for each subtopic. The statistics of the datasets generated using DiaSynth, including the number of dialogues, average number of turns, and average number of tokens per turn, are summarized in Table 1. All the experiments were run on single A100 GPU with the generation time ranging from 2 hours to 4 hours. 5.1 Quality of the Dialogues The quality of the synthetic datasets produced by DiaSynth was evaluated using FED, GPTScore, and G-Eval metrics, as detailed in Tables 2, 3, and 4. The results reveal distinct variations in performance across different model and dataset configurations, reflecting the unique characteristics of each. 5.1.1 Metric Scores FED: The FED scores in Table 2 show that LLaMA-3 and GPT-4o achieve almost perfect score (+1) in most of the criteria, while Phi-3 and InternLM-2.5 also have decent performances. GPT-4o has clear advantage when it comes to generating likeable dialogues while there is not much separation on other criteria. GPTScore: Results illustrated in 3 are surprising in that GPT-4o is the worst performing model on GPTScore, which might require further research while LLaMA-3 clearly dominates the other models. G-Eval: Table 4 highlights GPT-4os dominance in engagingness and naturalness with perfect scores (3.0) for DialogSum, while InternLM-2.5 stands out in coherence (2.9990) and groundedness (2.9973) for DialogSum, and coherence (2.9983) and groundedness (2.9952) for SAMSum, suggesting it maintains high factual accuracy. Dataset-Specific Performance. The contrasting performance of GPT-4o on the DialogSum and SAMSum datasets in Table 2 can be attributed to the differing structures of the dialogues in these datasets. DialogSum consists of more formal and structured dialogues, which aligns with the typical response style of GPT-4o, leading to its stronger performance. In contrast, SAMSum contains more casual, human-like conversations, which might explain GPT-4os relatively poorer performance, as it may not adapt as well to the informal, spontaneous nature of such dialogues. Overall, while GPT-4o excels in natural and engaging dialogue, LLaMA-3 offers the most versatility, and InternLM-2.5 provides strong alternative with high coherence and groundedness. engagingness naturalness coherence groundedness Phi-3 InternLM-2.5 LLaMA-3 GPT-4o Phi-3 InternLM-2.5 LLaMA-3 GPT-4o DIALOGUESUM 2.5236 2.9995 2.9987 3 2.4623 2.9992 2.9976 2. 2.7238 2.9989 2.9988 3 SAMSUM 2.6821 2.9969 2.9971 2.9977 Table 4: G-EVAL 2.6308 2.9990 2.9972 3 2.5848 2.9983 2.9969 2. 2.5557 2.9973 2.9935 2.9975 2.5060 2.9952 2.9916 2.9944 5.1.2 Strong performance of LLaMA-3 The observed superiority of LLaMA-3 over GPT-4o is surprising because an 8 billion 8-bit quantized model not only competes with but also performs better than GPT-4o in certain metrics. We hypothesize that this could be due to the way GPT-4o was trained, which might make it more constrained in its responses, whereas LLaMA-3, being an open-source model, operates with fewer restrictions. This allows LLaMA-3 to exhibit greater flexibility, diversity, and adaptability in generating dialogues, potentially explaining its better performance in certain metrics. These characteristics can be seen in criteria like inquisitiveness and likeability in Table 3 and, depth and diverse in Table 2. These results suggest that for building human-like data generation frameworks, open-source LLMs are more suitable choice than"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT Model Before Finetuning Finetuning on In-Domain Data QAGS BERTScore ROUGE-L QAGS BERTScore ROUGE-L distillbart-cnn bart-base t5-base led-base0.6134 0.7007 0.5901 0.8261 distillbart-cnn bart-base t5-base led-base-16384 0.6627 0.7563 0.5574 0.7429 DIALOGSUM 0.5093 0.5274 0.5491 0.5471 0.5500 0.4389 0.4190 0. 0.1950 0.1375 0.1812 0.1634 SAMSUM 0.2394 0.1765 0.1237 0.1812 0.5586 0.4789 0.4766 0.4872 0.6041 0.5302 0.5460 0.5440 0.7005 0.6868 0.6953 0. 0.6849 0.6520 0.6448 0.6522 0.3367 0.2969 0.2986 0.3165 0.3578 0.3049 0.3000 0.3175 Table 5: Performance of models before and after finetuning on in-domain data Model PhiInternLM-2.5 LLaMA-3 GPT-4o QAGS BERTScore ROUGE-L QAGS BERTScore ROUGE-L QAGS BERTScore ROUGE-L QAGS BERTScore ROUGE-L distillbart-cnn bart-base t5-base led-base-16384 0.6588 0.5355 0.5937 0. distillbart-cnn bart-base t5-base led-base-16384 0.6585 0.5648 0.5905 0.5883 0.5778 0.5958 0.5949 0.6129 0.5931 0.5665 0.5397 0.5477 0.2187 0.2029 0.2047 0.2109 0.2262 0.2146 0.2085 0. 0.6420 0.5418 0.5825 0.5189 0.6388 0.5435 0.5457 0.5457 DIALOGUESUM 0.6008 0.6212 0.5941 0.6027 0.6066 0.5663 0.5193 0.5615 0.2167 0.1897 0.1878 0. SAMSUM 0.2422 0.2021 0.1854 0.2167 0.6586 0.5825 0.6034 0.5697 0.6849 0.6132 0.6412 0.5917 0.6161 0.6033 0.6172 0.6302 0.6029 0.5899 0.5054 0. 0.2040 0.1789 0.1959 0.1999 0.2374 0.2345 0.1976 0.2390 0.6713 0.5590 0.6305 0.5791 0.6757 0.5707 0.6023 0.5738 0.6242 0.6039 0.6319 0.6308 0.6029 0.5808 0.5419 0. 0.2014 0.1769 0.2044 0.1989 0.2291 0.2154 0.1979 0.2298 Table 6: Performance after finetuning on synthetic data closed-source LLMs. The minimal constraints on response formatting during the training of open-source models enable them to generate more diverse, flexible, and human-like dialogues, making them better suited for tasks requiring natural and conversational interactions. 5.2 Fine-tuning and Performance Results To validate the usability of the synthetic data generated using DiaSynth, we fine-tuned and evaluated several pretrained language models on the task of dialogue summarization. The summaries for dialogues generated by different LLMs were created using the corresponding LLMs through prompting. The pretrained models used for evaluation include DistilBART, BART, T5, and LED. The experimental setup is designed as follows: Metrics are reported on the validation and test sets of DialogSum and SAMSum. To evaluate DiaSynth-generated data, we compared models fine-tuned on DiaSynth data with their base versions (no fine-tuning). In-domain training sets were randomly sampled to match the size of the DiaSynth-generated data, enabling fair comparison. The experiment aimed to quantify performance improvement of DiaSynth-fine-tuned models and assess alignment with in-domain data distributions. Models were fine-tuned for 2 epochs with learning rate of 5e-5 and warmup of 50 steps. The results presented in Tables 5 and 6 present the performance of the base models, models finetuned on in-domain data and models finetuned on DiaSynth generated data. Models finetuned on DiaSynth data generally improves the performances from the BERTScore and ROUGE-L metrics. Surprisingly, for some models (LED and BART) the QAGS scores were higher than the models finetuned on DiaSynth. On further exploration, we found out that these models extracted multiple sentences from the given dialogue instead of generating summary which led to high QAGS scores. Comparing models finetuned on in-domain data to those finetuned on DiaSynth data reveals that DiaSynth finetuning"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT generally enhances factual accuracy, with BERTScore and ROUGE-L scores remaining comparable. The disparity in BERTScore and ROUGE-L results may be due to format variations. Models fine-tuned on in-domain data were evaluated on summaries that matched the training format closely, while DiaSynth-fine-tuned models were trained on LLM-generated summaries and evaluated on human-generated summaries, leading to minor format mismatches. Comparison between the different LLMs from Table 6, shows that GPT-4o is better at generating dialogues and summaries that are formal in nature while LLaMA-3 and open source LLMs would be better for generating dialogues that are informal and casual in nature. % Improvement = % Coverage = After - Before Before Score DiaSynth Score In-domain (1a) (1b) % Improvement % Covered % Improvement % Covered Model distilbart-cnn bart-base t5-base led-base-16384 88.81 90.6 93.67 89.68 Table 7: Summarization results on DialogSum 10.96 9.21 7.59 2.14 Model distilbart-cnn bart-base t5-base led-base-16384 87.25 94.35 87.36 90.91 Table 8: Summarization results on SAMSum 6.07 16.12 30.04 15.25 To assess the percentage improvement and percentage coverage of the distributional characteristics of the in-domain data by the synthetically generated data, we use Equations 1a and 1b respectively. We use the scores of models finetuned on LLaMA-3 generated data because of its dominance in both quality and usability. Across the 24 reported results, the overall coverage percentage of the LLaMA-3 generated data is 90.48%. Notably, the QAGS scores of models fine-tuned on synthetic data surpass those of models trained on in-domain data, suggesting that synthetic data can match or even exceed in-domain data performance in some aspects. Excluding QAGS, the coverage percentage is calculated to be 77.07%. In addition to the average percentages, we also present the model wise percentage improvement and coverage in Table 7 and 8. The results presented are with respect to the dialogues generated using LLaMA-3 and they illustrate clear improvements for every model, highlighting that even with moderate LLMs of small scale (3B - 8B), high-quality synthetic dialogue datasets can be effectively created across different domains and different dialogue formats."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced DiaSynth, synthetic dialogue generation framework capable of producing high-quality, contextually rich dialogues across wide range of domains. Our experiments demonstrated that models fine-tuned on DiaSynth-generated data exhibit significant improvements in downstream tasks, as evidenced by substantial increases in BERTScore and ROUGE-L compared to their base models. These results highlight the potential of DiaSynth as an effective tool for generating dialogue data, particularly in domains where training data is scarce. Furthermore, our analysis showed that different LLMs excel in different dialogue structures, with LLaMA-3 performing better for informal dialogues and GPT-4o for more structured settings. This insight suggests that leveraging open-source LLMs may be more advantageous for generating human-like conversational data. Despite certain limitations, such as varying LLM performance across dialogue types and knowledge gaps in zero-shot generation, DiaSynth presents promising approach to dialogue data generation and offers valuable resource for future advancements in building more sophisticated and adaptable dialogue systems."
        },
        {
            "title": "7 Limitations",
            "content": "Despite the promising results, our approach has some limitations. Firstly, different LLMs exhibit varied performance based on the dialogue structure, with certain models like LLaMA-3 performing better for more informal dialogues (e.g., SAMSum) and others like GPT-4o excelling in structured, formal dialogues (e.g., DialogSum). This indicates that there is no single model that can universally handle all types of dialogue structures, but single SOTA model can give stable and decent results like LLaMA-3. Secondly, the generation process may suffer from lack of knowledge on certain topics, especially in cases where the LLMs were not sufficiently trained on those domains. Additionally, our framework relies on zero-shot generation for personas and sub-topics, which, while flexible, can sometimes result in less coherent or less accurate persona simulation, as it is not fine-tuned for specific contexts."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT Since LLMs power DiaSynth, hallucinations and compute-need are two inherent limitations. We present detailed hallucination study in Appendix A, which indicates that though the hallucinations rates are acceptable, there is still scope for improvements. These limitations suggest directions for future work, such as combining LLMs to leverage their strengths or incorporating more topic-specific training to enhance knowledge coverage."
        },
        {
            "title": "References",
            "content": "Song Feng, Hui Wan, Chulaka Gunasekara, Siva Patel, Sachindra Joshi, and Luis Lastras. doc2dial: goal-oriented document-grounded dialogue dataset. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 81188128, Online, November 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.emnlp-main.652. URL https://aclanthology.org/2020.emnlp-main.652. Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, and Pengtao Xie. MedDialog: Large-scale medical dialogue datasets. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 92419250, Online, November 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.emnlp-main.743. URL https: //aclanthology.org/2020.emnlp-main.743. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašic. MultiWOZ - large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 50165026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:10.18653/v1/D18-1547. URL https://aclanthology. org/D18-1547. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, and Kyung-Ah Sohn. PSYDIAL: Personality-based synthetic dialogue generation using large language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1332113331, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.1166. Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000, personas. arXiv preprint arXiv:2406.20094, 2024. Sebastian Steindl, Ulrich Schäfer, and Bernd Ludwig. Generating synthetic dialogues from prompts to improve taskoriented dialogue systems. In KI 2023: Advances in Artificial Intelligence: 46th German Conference on AI, Berlin, Germany, September 2629, 2023, Proceedings, page 207214, Berlin, Heidelberg, 2023. Springer-Verlag. ISBN 9783-031-42607-0. doi:10.1007/978-3-031-42608-7_17. URL https://doi.org/10.1007/978-3-031-42608-7_ 17. Shikib Mehri and Maxine Eskenazi. Unsupervised evaluation of interactive dialog with DialoGPT. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. In Olivier Pietquin, Smaranda Muresan, Vivian Chen, Casey Kennington, David Vandyke, Nina Dethlefs, Koji Inoue, Erik Ekstedt, and Stefan Ultes, editors, Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225235, 1st virtual meeting, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.sigdial-1.28. URL https://aclanthology.org/2020.sigdial-1.28. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. GPTScore: Evaluate as you desire. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 6556 6576, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.naacllong.365. URL https://aclanthology.org/2024.naacl-long.365. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 25112522, Singapore,"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.153. URL https: //aclanthology.org/2023.emnlp-main.153. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiao wen Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhen Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kui-Jie Liu, Xiaoran Liu, Chen Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xing Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Rui Ze Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fen-Fang Zhou, Zaida Zhou, Jingming Zhuo, Yi-Ling Internlm2 technical report. ArXiv, abs/2403.17297, 2024. URL Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. https://api.semanticscholar.org/CorpusID:268691939. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. DialogSum: real-life scenario dialogue summarization dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 50625074, Online, August 2021. Association for Computational Linguistics. doi:10.18653/v1/2021.findings-acl.449. URL https://aclanthology.org/2021.findings-acl.449. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: human-annotated In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and dialogue dataset for abstractive summarization. Fei Liu, editors, Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 7079, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409. Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50085020, Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.450. URL https://aclanthology.org/2020. acl-main.450. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=SkeHuCVFDr. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-1013. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78717880, Online, July 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-main.703. URL https://aclanthology. org/2020.acl-main.703. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Potsawee Manakul, Adian Liusie, and Mark Gales. SelfCheckGPT: Zero-resource black-box hallucination detection In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings for generative large language models. of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017, Singapore,"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.557. URL https: //aclanthology.org/2023.emnlp-main.557. Robert Friel and Atindriyo Sanyal. Chainpoll: high efficacy method for llm hallucination detection. ArXiv, abs/2310.18344, 2023. URL https://api.semanticscholar.org/CorpusID:264590664."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT"
        },
        {
            "title": "A Hallucination Study",
            "content": "In addition to evaluating the quality and usability of dialogues produced by DiaSynth, we conducted study on the phenomenon of hallucinations within the generated dialogues. Hallucinations in language models refer to instances where the output contains misleading or incorrect information or situations where the model repeats the same content. To evaluate the occurrence of hallucinations, we compared the generated dialogues with their respective summaries and assessed them using two well-known hallucination benchmarks: SelfCheckGPT Manakul et al. [2023] and ChainPoll Friel and Sanyal [2023]. This analysis provides insights into the prevalence of hallucinations and informs strategies for improving dialogue quality in future iterations of DiaSynth. The results are presented in Tables 9 and 10. A.1 SelfCheckGPT SelfCheckGPT quantifies the self-consistency of LLM outputs by examining agreement across multiple outputs from the same prompt. This assessment reveals potential inaccuracies through metrics like SelfCheck-BertScore. The SelfCheck-BERTScore results for various models show that hallucination levels are at worst around 25%, which is acceptable but still indicates areas for improvement. Across both datasets, Phi-3 demonstrates the most robustness, likely due to its pretraining on structured, textbook-like data, which may contribute to greater consistency and factual accuracy. A.2 ChainPoll ChainPoll utilizes chain-of-thought prompting approach to identify hallucinations by iteratively polling the model with structured reasoning prompts. This method systematically detects both open-domain and closed-domain hallucinations, where lower scores indicate fewer hallucinations. The ChainPoll scores indicate that hallucination levels on these models are generally low, with the best performance seen by GPT-4o on SAMSum, which achieves the lowest score of 0.120, suggesting minimal hallucinations. On the other hand, LLaMA-3 scores higher at 0.237 on SAMSum, indicating more frequent hallucinations. These findings highlight different models strengths in generating accurate and reliable dialogues. A.3 Implications for DiaSynth The results from both SelfCheckGPT and ChainPoll evaluations suggest that DiaSynth, when leveraging models like Llama 3, is capable of generating dialogues with relatively low hallucination rates. However, specific models show variability in performance across datasets, indicating that further enhancements, such as fine-tuning or incorporating additional guardrails, could improve DiaSynths robustness in generating reliable dialogues across diverse domains. LLM Phi-3 InternLM-2.5 LLaMA-3 GPT-4o ChainPoll 0.198 0.199 0.205 0. SCGPT-BERTScore 0.791 0.726 0.793 0.765 Table 9: Hallucination calculation for DialogSum fewshot data LLM Phi-3 InternLM-2.5 LLaMA-3 GPT-4o ChainPoll 0.154 0.159 0.237 0.120 SCGPT-BERTScore 0.785 0.716 0.733 0.742 Table 10: Hallucination calculation for SAMSum fewshot data"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT"
        },
        {
            "title": "B Scalability of DiaSynth",
            "content": "This section illustrates the scalability of DiaSynth with python program and examples. 1 def c t _t l_ lo gs (n , , ) : 2 \"\"\" Calculate the total number of dialogs generated . 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 Parameters : ( int ) : Number of topics ( int ) : Number of subtopics per topic ( int ) : Number of personas per subtopic Returns : int : Total number of dialogs generated \"\"\" # Calculate the total number of subtopics total_subtopics = * # Calculate the number of dialogs for each subtopic using combinations of personas dialog s_p er _s ub to pi = ( * ( - 1) ) // 2 # Total dialogs generated across all subtopics total_dialogs = total_subtopics * ialogs_per_subtopic return total_dialogs Listing 1: Calculation of Total Dialogs Generated B.1 Example 1 Number of topics (n): 10 Number of subtopics per topic (m): 5 Number of personas per subtopic (p): 3 Calculation: Total subtopics = = 10 5 = 50 3 2 2 (p 1) = Dialogues per subtopic = = 3 Total dialogues = 50 3 = 150 This setup generates 150 dialogues. B.2 Example Number of topics (n): 20 Number of subtopics per topic (m): 4 Number of personas per subtopic (p): 5 Calculation: This setup generates 800 dialogues. Total subtopics = 20 4 = 80 Dialogues per subtopic = 5 4 = 10 Total dialogues = 80 10 ="
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT B.3 Example 3 Number of topics (n): 15 Number of subtopics per topic (m): 6 Number of personas per subtopic (p): 10 Calculation: Total subtopics = 15 6 = 90 Dialogues per subtopic = 10 9 2 = 45 Total dialogues = 90 45 = 4050 This setup generates 4050 dialogues. B.4 Scaling Observations Linear Scaling with Topics and Subtopics: Increasing the number of topics or subtopics results in linear increase in the total number of dialogues, making it straightforward to expand the scope of dialogue generation. Exponential Scaling with Personas: The number of dialogues scales exponentially as the number of personas increases because each additional persona allows for more combinations, making the framework highly scalable for complex scenarios. Practical Use Case: For specific domains like academic, healthcare, or business, these parameters can be adjusted to generate thousands of dialogues to fit the needs of various applications such as training chatbots, virtual assistants, or dialogue-based assessments. These examples illustrate DiaSynths potential for rapid and scalable generation of dialogues, which can be tailored to different domains by simply adjusting the input parameters."
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT"
        },
        {
            "title": "C Characteristics for the conversation",
            "content": "Table 11 shows different characteristics that we let the LLMs reason and decide using CoT. Before generating the dialogues, the LLMs are prompted to first reason about the various characteristics list for the dialogue given the topic and the personas and how the LLMs reason are illustrated in Appendix D. Characteristic Age and Gender Familiarity Level"
        },
        {
            "title": "Emotional States\nFormality Level\nDuration of the Conversation\nCommunication Medium\nTopic of the Conversation\nLocation of the Conversation\nAgreement or Disagreement\nNatural Dialogue Features",
            "content": "Description Defines demographic details, influencing style and tone. Affects formality and depth based on relationship between speakers. Impacts tone and flow based on emotions (e.g., happy, sad). Determines level of politeness or casualness. Suggests the intended length and complexity of dialogue. Defines the medium (e.g., face-to-face, phone), influencing style. Guides the content and direction of the dialogue. Adds context influencing formality and content. Drives dialogue dynamics based on agreement level. Adds authenticity with fillers, pauses, and slang. Table 11: Characteristics of the Dialogue for CoT Prompt"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT"
        },
        {
            "title": "D Example CoT environments",
            "content": "These examples illustrate how CoT sets the various dialogue characteristics defined in Appendix C. As can be seen in the examples, the characteristics for the conversation are completely changed based on the personas and the topics, resulting in more grounded conversation generation. Future works can further explore how CoT can be used to further break down to generate even more realistic dialogues. (a) CoT Example 1 (b) CoT Example 2 Figure 2: Illustration of Example CoT environments"
        },
        {
            "title": "DiaSynth",
            "content": "A PREPRINT"
        },
        {
            "title": "E Ablation study on the use of CoT",
            "content": "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables 12, 13, 14 and 15 compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth. We hypothesize that this improvement in quality is due to allowing the LLM to set diverse characteristics for the dialogue before generating the dialogue. This illustrates that either manually setting the relevant context or letting the LLM on its own to set the relevant context, we get better outputs, as adding relevant context lowers the probabilities of sequences that are not useful. The lower FED scores of CoT generated dialogues in Table 14, might be because of the CoT generated dialogues being longer in length but it needs further research. Criteria Coherent Error Recovery Consistent Diverse Depth Likeable Understand Flexible Informative Inquisitive Without CoT With CoT 0.9507 0.938 0.9424 0.9431 0.9451 0.0088 0.9317 -0.0013 0.0032 0.0095 0.9521 0.9424 0.9523 0.952 0.9506 -0.0003 0.9338 0.0001 0.0009 -0. Criteria Coherence Diversity Flexibility Understandability Inquistiveness Consistency Informativeness Likeability Depth Error Recovery Without CoT With CoT 0.0052 0.0120 0.0074 0.0056 0.0162 0.0091 0.0099 0.0029 0.0062 0.0142 0.0284 0.0303 0.0215 0.019 0.0362 0.0366 0.0168 0.0209 0.0115 0.0242 Table 12: FED scores of dialogues generated with and without CoT for DialogSum few-shot Table 13: GPTScore of dialogues generated with and without CoT for DialogSum few-shot Criteria Coherent Error Recovery Consistent Diverse Depth Likeable Understand Flexible Informative Inquisitive Without CoT With CoT 0.9667 0.9577 0.9621 0.9586 0.9589 0.0059 0.9503 -0.0022 0.0110 0.0030 0.9125 0.9051 0.9163 0.9124 0.9094 -0.0003 0.8978 -0.0023 0.0035 -0.0037 Criteria Coherence Diversity Flexibility Understandability Inquistiveness Consistency Informativeness Likeability Depth Error Recovery Without CoT With CoT 0.0076 0.0172 0.0116 0.0104 0.0201 0.0149 0.0148 0.0041 0.0078 0.0163 0.0324 0.0372 0.0259 0.0272 0.0389 0.0415 0.0200 0.023 0.0124 0.0289 Table 14: FED scores of dialogues generated with and without CoT for SAMSum few-shot Table 15: GPTScore of dialogues generated with and without CoT for SAMSum few-shot"
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore",
        "Singapore Institute of Technology, Singapore"
    ]
}