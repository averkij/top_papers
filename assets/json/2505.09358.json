{
    "paper_title": "Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis",
    "authors": [
        "Bingxin Ke",
        "Kevin Qu",
        "Tianfu Wang",
        "Nando Metzger",
        "Shengyu Huang",
        "Bo Li",
        "Anton Obukhov",
        "Konrad Schindler"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io"
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov,, Konrad Schindler 5 2 0 2 4 1 ] . [ 1 8 5 3 9 0 . 5 0 5 2 : r We present Marigold, fine-tuning protocol for various image analysis tasks, and family of associated diffusion models. Without loss of generality, these include monocular depth estimation, surface normals prediction, and intrinsic image decomposition. Its core principle is to leverage the rich visual knowledge stored in modern generative image models. As generative model derived from Stable Diffusion and fine-tuned with synthetic data, Marigold can zero-shot transfer to unseen datasets, offering state-of-the-art results. The visualizations above demonstrate the strong out-of-distribution performance: without observing single image other than synthetic rooms and dashboard views, Marigold can extract pixel-perfect depth maps, surface normals, and intrinsic decomposition of images, ready for downstream tasks. AbstractThe success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in latent space, has introduced new class of foundational models trained on massive, captioned image datasets. These models ability to generate realistic images of unseen content suggests they possess deep understanding of the visual world. In this work, we present Marigold, family of conditional generative models and fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification Work done at the Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland. * denotes equal technical contribution. denotes equal supervision. Corresponding author: Konrad Schindler (schindler@ethz.ch). of the pre-trained latent diffusion models architecture, trains with small synthetic datasets on single GPU over few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io. Index TermsDenoising diffusion, image analysis, image generation, foundational models, transfer learning. I. INTRODUCTION HE introduction of ImageNet [1] laid the foundation for training deep Convolutional Neural Networks (CNNs), such as AlexNet [2], catalyzing further advances in the computer vision field: in data acquisition, neural architectures, and training techniques. With the advent of VGG [3] and ResNet [4] architectures, transfer learning [5] became essential for training high-performance computer vision models and reducing training time of semantic segmentation [6], depth prediction [7], and other downstream tasks. In many cases, training neural network from random weight initialization is 00000000/00$00.00 2021 IEEE IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 claimed not feasible [6]. Modern deep learning frameworks [8] have since made it easy to use pretrained models by allowing practitioners to load pretrained weights with simple setting like pretrained=True during model creation. The rise of large text-to-image generative models [9] and denoising diffusion approaches [10], [11] has opened new opportunities for leveraging the rich priors embedded in foundational models. breakthrough in this area came with the introduction of Latent Diffusion Models (LDMs), class of models exemplified by the widely known Stable Diffusion (SD) [12]. These models operate in the compressed latent space of pretrained Variational Autoencoder (VAE), enabling significant resource savings in both training and inference. Trained on the internet-scale LAION-5B dataset of captioned images [13], Stable Diffusion excels in realism and diversity. Its open-source availability, low computational requirements for inference, and integration with toolkits like diffusers [14] have enabled widespread experimentation by researchers and artists. The abundance of customization recipes [15][17] has prompted many notable extensions that focus on enhancing the controllability of the original image generation task. Repurposing text-to-image LDMs from image generation to image analysis is recent development in generative imaging. The motivation is simple: if diffusion model demonstrates deep understanding of the visual world through high-quality image generation, that same understanding can be leveraged to derive versatile regression model for image analysis. To this end, in our recent work [18], we introduced MarigoldDepth, an LDM-based state-of-the-art zero-shot affine-invariant monocular depth estimator, along with simple and resourceefficient fine-tuning protocol for Stable Diffusion. Marigold-Depth proposed several key novelties unlocking the potential of LDMs for image understanding: (1) reusing the LDMs VAE to encode not just the input image but also the output modality into the latent space; (2) using only high-quality synthetic data; (3) short resource-efficient fine-tuning protocol; (4) generative modeling of conditional distribution rather than predicting its mode as end-to-end approaches do."
        },
        {
            "title": "Some of",
            "content": "these properties are organically entangled. (12): Encoding the modality into latent space is only possible when it is noise-free and pixel-complete rarely the case with the real depth ground truth. (23): short fine-tuning protocol preserves prior knowledge. It requires diverse, consistently labeled, and noise-free data to reduce noise in weight updates, which are satisfiable with synthetic data. (13): Operation in latent space ensures affordable fine-tuning and inference on single consumer Graphics Processing Unit (GPU), empowering research even outside large labs. The importance of synthetic data and strong prior for depth estimation have been subsequently confirmed in Depth Anything V2 [19]. Although their end-to-end model achieves impressive performance in zero-shot benchmarks, it involves 3-stage training procedure, teacher-student separation, and generating 62M pseudo labels; both do not fit the bill of simple and affordable transfer learning recipe. For the property (4), as shown in [18], modeling the distribution of depth conditioned on the input image with an LDM allows for multiple plausible interpretations of the input. This ability is essential for solving ill-posed problems, as there may be no single correct output due to over-exposed input, blur, ambiguities of transparent objects, etc. Obtaining samples from the conditional distribution with Marigold is as simple as starting the diffusion process from different noise samples given the same input. Multiple such predictions can be ensembled to approximate the mode of conditional distribution. Ensembling is required to evaluate prediction quality in standard benchmarks comprised of image-depth ground truth pairings, an established evaluation protocol [20], [21]. Additionally, computing predictive uncertainty becomes tractable given such an ensemble. The multi-step inference and the computational redundancy of the ensembling typically result in many function evaluations (NFEs), which slow down inference speed initially, major point of criticism of the original Marigold-Depth [18]. To this end, we explored Latent Consistency Distillation [22] to reduce the number of sampling steps arbitrarily low, even to just one. Simultaneously, several works explored other techniques to bring sampling steps down. Some preserved the generative nature of the model [23][25]. Additionally, [23] and [26] showed the possibility of scoring high in the said benchmarks by re-casting Marigold as an end-to-end network, effectively yielding just one function evaluation. In this paper, we continue focusing on the generative formulation. Notably, Garcia et al. [23] discovered sub-optimal settings in the diffusion scheduler of the original Marigold-Depth [18] and proposed correction, leading to significant improvement of that exact models performance in the same benchmarks in the few-step inference regime. We gratefully incorporated this observation and integrated this regime into our study. With single-step inference and technical enhancements such as using lightweight compatible VAE [27] and low-precision weight quantization, Marigold can now produce predictions in under 100ms on most commodity hardware. In this paper, we follow up on the initial body of work [18], recap it in Sec. III, and extend our study with several dense image analysis tasks [28] having long history in computer graphics: surface normals prediction in Sec. IV and intrinsic image decomposition in Sec. V. Additionally, we introduce protocol for distilling Marigold models into Latent Consistency Models (LCMs) in Sec. VI and the new High-Resolution (HR) model and inference strategy in Sec. VII. To summarize, our contributions are: (1) Marigold simple and resource-efficient fine-tuning protocol to convert foundational LDM image generator into zero-shot generative image analysis model with robust in-the-wild generalization capability. Training Marigold takes less than 3 GPU-days and can be accomplished with most commodity hardware; (2) comprehensive study of training diffusion models with Marigold for monocular depth estimation, surface normal prediction, and intrinsic image decomposition tasks; (3) Fast (sub-100ms) fewor single-step inference; (4) Overcoming the resolution bias of the base model, enabling high-resolution inference. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3 II. RELATED WORK A. Diffusion Models Denoising Diffusion Probabilistic Models (DDPMs) [10], [29] generate data by reversing Gaussian noise diffusion process. Denoising Diffusion Implicit Models (DDIMs) [30] extend this by introducing non-Markovian shortcut for faster sampling at inference time. Latent Consistency Models (LCMs) [22], [31] distill DDPM models into consistency functions that map points on the diffusion ODE trajectory [32] to the same output, thereby introducing different parameterization and reducing inference time. In the realm of image generation, Rombach et al. [12] have revolutionized generative modeling with their Stable Diffusion model, trained with LAION-5B [13] dataset of 2.3B text-image pairs. The cornerstone of their approach is an LDM, where the denoising process is run in an efficient latent space, drastically reducing the complexity of the learned mapping. This model holds rich visual priors, providing solid foundation for our generalizable image analysis models and base for our transfer learning approach. B. Foundation Models Vision Foundation Models (VFMs) are large neural networks trained on internet-scale data. The extreme scaling leads to the emergence of high-level visual understanding, such that the model can then be used as is [33] or fine-tuned to wide range of downstream tasks with minimal effort [34]. Prompt tuning methods [35][37] can efficiently adapt VFMs towards dedicated scenarios by designing suitable prompts. Feature adaptation methods [38][42] can further pivot VFMs towards different tasks. Direct tuning enables more flexible adaptation, especially in few-shot customization scenarios like DreamBooth [17]. As we show in this paper, Marigold can be interpreted as an instance of this type of tuning, where Stable Diffusion [12] plays the role of the foundation model for multiple image analysis tasks. Our models exhibit strong in-the-wild performance thanks to the foundational prior and efficient fine-tuning protocol (cf . the teaser figure). C. Monocular Depth Estimation The pioneering work [7] introduced an end-to-end trainable network and showed that metric depth for dataset can be recovered with single sensor. Successive improvements have come from various fronts, including various parameterizations (ordinals, bins, planar maps, CRFs, etc.) [43][50] and switching CNN backbones to vision transformers [51][54]. class of works [55][57] relies on privileged information fed alongside input, such as camera intrinsics. Estimating depth in the wild or out-of-distribution refers to methods with robustness across wide range of possibly unfamiliar settings where no privileged information is available. MegaDepth [58] and DiverseDepth [59] utilize extensive internet photo collections to train models that can adapt to unseen data, while MiDaS [60] achieves generality by training on mixture of multiple datasets. To unify representations across datasets, MiDaS estimates affine-invariant depth up to unknown shift and scale. The step from CNNs to vision transformers has further boosted performance, as evidenced by DPT (MiDaS v3) [61]. Depth Anything [62] took data scaling to the next level by relying on DINOv2 [63] foundational model prior trained on 142M images with self-supervision and subsequent training with 62M pseudo-labels, 1M real depth annotations, and 0.5M synthetic ones. Several methods [42], [42], [64], [65] proposed using DDPMs and LDMs for depth estimation. However, they either train models from scratch, use Stable Diffusion [12] as feature extractor, resort to custom latent spaces, operate in pixel space, or require extensive training. Our previous work [18] proposed fine-tuning generative text-to-image LDM such as Stable Diffusion, trained with LAION-5B [13], dataset of 2.3B image-text pairs, towards affine-invariant depth using just 74K samples from the HyperSim [66] and Virtual KITTI [67] synthetic datasets. Marigold demonstrated impressive zero-shot generalization both in benchmarks and in the wild. Since uploading [18], Depth Anything V2 [19] confirmed our findings about the role of synthetic data in the task and retired real data from their pipeline, achieving impressive performance gains. E2E-FT [23] and GenPercept [26] performed endto-end fine-tuning. The former also proposed fix for few-step DDIM scheduler inference regime. E2E-FT also demonstrated that end-to-end networks can score higher in zero-shot benchmarks than the similar generative model. Two more works kept the generative nature of the base model: Lotus [25] switched to predicting the target using exactly one step; DepthFM [24] adopted flow matching [68] at training. GeoWizard [69] proposed to estimate the depth and surface normals jointly, although using privileged information about scene type. BetterDepth [70] introduced Marigold-based refiner for coarse depth inputs. SteeredMarigold [71] used Marigold as generative prior in order to perform depth densification. Robustness under challenging conditions [72] was tackled through depth-conditioned generation of training data, an approach similar to DGInStyle [73]. ChronoDepth [74] and DepthCrafter [75] address the temporal consistency of depth prediction in the video domain by performing Marigold-like fine-tuning of video diffusion models. We stick to the generative formulation of Marigold, incorporate the DDIM fix [23], and recap the method [18] (Sec. III). D. Monocular Surface Normals Prediction Early monocular surface normals estimation methods often employed CNNs consisting of feature extractor backbone followed by prediction head [76][79]. Over time, various improvements have been proposed. Bae et al. [80] suggested estimating aleatoric uncertainty and using uncertainty-guided sampling during training to enhance prediction quality for small structures and object boundaries. Omnidata v2 [81] introduced transformer-based model trained on 12M images, applying sophisticated 3D data augmentation and enforcing cross-task consistency. DSINE [82] identified and incorporated inductive biases tailored for surface normals estimation. It leverages the per-pixel ray direction coupled with ray-based activation function and learns the relative rotation between neighboring surface normals. Since uploading [18], several papers have repurposed diffusion models to surface normals estimation. GeoWizard [69] IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 jointly estimates depth and surface normals, although it uses privileged information about scene type. Shortly after, we released Marigold-Normals v0.1, preview model for monocular surface normals estimation trained similarly to Marigold-Depth, albeit just on HyperSim. GenPercept [26] treats the denoising UNet as deterministic backbone and employs one-step inference. StableNormal [83] aims to reduce the inherent stochasticity of diffusion models by using two-stage coarse-to-fine strategy. single-step surface normals estimator first produces an initial coarse estimate, followed by refinement process that recovers finer geometric details, semantically guided by DINOv2 [63] features. Lotus [25] directly predicts annotations instead of noise and reformulates the multi-step diffusion into single-step procedure. Garcia et al. [23] proposed single-step inference fix for Marigolds DDIM scheduler to enhance inference speed. Additionally, they reframe single-step generative predictor into an end-to-end network based on the same architecture. In what follows, we continue developing the ideas behind Marigold-Normals and arrive at v1.1. We demonstrate that careful curation of synthetic fine-tuning datasets together with the vanilla Marigold protocol without any other foundational models, multi-task aggregation, privileged information, or refinement stages achieves the best performance in most evaluation datasets, compared to the other recent diffusionbased surface normals estimation methods (Sec. IV). E. Intrinsic Image Decomposition (IID) Intrinsic image decomposition aims to recover the intrinsic properties of objects in an image, including albedo (surface reflectance), shading, and Bidirectional Reflectance Distribution Function (BRDF) parameters, such as roughness and metallicity. It was introduced by Horn [28] and later studied by Barrow et al. [84]. The theory evolved from early prior-based approaches, such as the retinex theory, to modern deep learningbased methods. First, deep learning approaches typically utilized feed-forward convolutional network [85][88] or transformer [89] to predict pixel-level intrinsic decomposition channels from the input image. Careaga et al. [90] proposed multi-step approach that first predicts initial albedo and grayscale shading maps using an off-the-shelf network [91], and then progressively refines them. With the recent advent of vision foundation models, especially generative models, alternative solutions have shown success by utilizing StyleGAN [92], [93] or diffusion models [94][97]. DMP [95] learns deterministic mapping between an input and the IID task (albedo and shading) through low-rank adaptation of text-to-image models. IIDDiffusion [96] uses custom latent encoder and CLIP [98] features to guide the fine-tuned Stable Diffusion [12] on InteriorVerse [99] decomposition into albedo and material properties: roughness and metallicity. RGBX [97] learns bijection between input images and various modalities, including the ones used in IID-Diffusion. Their model, based on the pretrained Stable Diffusion, has similar architecture to Marigold, yet uses the text encoder to switch between predefined modalities. We train two Marigold-IID models: one that predicts albedo, roughness, and metallicity, and another that estimates albedo, non-diffuse shading, and residual diffuse component. We compare our models to IID-Diffusion [96], RGBX [97] and Careaga et al. [90] in their respective domains on the InteriorVerse [99] and HyperSim [66] datasets. Our simpler models achieve highly competitive performance in both quantitative and qualitative evaluations (Sec. V). F. High-Resolution Estimation Models fine-tuned from Stable Diffusion [12] usually exhibit resolution bias towards the original resolution used to train the text-to-image LDM. The same applies to all Marigold models: their processing resolution defaults to the recommended value of 768 inherited from the base model, which should correspond to the longest side of the input image. As result, large resolutions suffer major loss of details during downsampling to the processing resolution and upsampling output to the original size (see the teaser figure). This issue prompted us to investigate approaches to high-resolution inference with Marigold. Although the study is centered around MarigoldDepth, the findings apply to other modalities. BoostingDepth [100] fuses local patches into global canvas through an additional GAN network. PatchFusion [101] introduces tile-based framework that combines globally consistent coarse features with finer local features using patch-wise fusion network. Moreover, it ensures consistency across patches during training without post-processing. PatchRefiner [102] performs high-resolution depth estimation by refining predictions with pseudo-labeling strategy. MultiDiffusion [103] is an inference protocol for pre-trained diffusion models that generates high-resolution outputs by performing diffusion over overlapping tiles in an interleaved fashion. While it has been used in generative tasks like semantic segmentation [73], it has not been applied yet to enhance the resolution of image analysis tasks. Depth Pro [104] proposed concurrently multiscale vision transformer trained on tens of datasets, capable of predicting metric depth for HD images in split second. Our Marigold-HR inference strategy attains competitive or better performance compared to these other methods (Sec. VII). III. BASE MODEL: MARIGOLD-DEPTH In this section, we recap the details of our fine-tuning protocol in the context of monocular depth estimation [18]. A. Generative Formulation We pose monocular depth estimation as conditional denoising diffusion generation task and train Marigold to model the conditional distribution D(d x) over depth RW , where the condition RW H3 is an RGB image. In the forward process, which starts at d0 := from the conditional distribution, Gaussian noise is gradually added at levels {1, ..., } to obtain noisy samples dt as dt = αtd0 + 1 αtϵ (1) where ϵ (0, I), αt := (cid:81)t s=1 1βs, and {β1, . . . , βT } is the variance schedule of process with steps. In the reverse process, the denoising model ϵθ() parameterized with learned parameters θ gradually removes noise from dt to obtain dt1. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5 data range of the depth data plays significant role in enabling affine-invariance. We discuss our normalization approach in Sec. III-C. We verified that without any modification of the VAE or the latent space structure, the depth map can be reconstructed from the encoded latent code with negligible error, i.e., D(E(d)). This is the first check to be performed when following the Marigold fine-tuning protocol for new modality. At inference time, the depth latent code is decoded once at the end of diffusion, and the average of three channels is taken as the predicted depth map. Extending Marigold to another modality with different number of channels may prompt allocating new latent space for each triplet of channels. t Adapted denoising U-Net. To implement the conditioning of the latent denoiser ϵθ(z(d) , z(x), t) on input image x, we concatenate the image and depth latent codes into single input zt = cat(z(d) , z(x)) along the feature dimension. The input channels of the latent denoiser are then doubled to accommodate the expanded input zt. To prevent inflation of activations magnitude of the first layer and keep the pre-trained structure as faithfully as possible, we duplicate the weight tensor of the input layer and divide its values by two. similar conditioning mechanism, except for the zero weights initialization, was previously used in InstructPix2Pix [15]. C. Fine-Tuning for Depth Estimation Affine-invariant depth normalization. For the ground truth depth maps d, we implement linear normalization such that the depth primarily falls in the value range [1, 1], to match the designed input value range of the VAE. Such normalization serves two purposes. First, it is the convention for working with the original Stable Diffusion VAE. Second, it enforces canonical affine-invariant depth representation independent of the data statistics any scene must be bounded by near and far planes with extreme depth values. The normalization is achieved through an affine transformation computed as = (cid:18) d2 d98 (cid:19) 0.5 2, (3) where d2 and d98 correspond to the 2% and 98% percentiles of individual depth maps. This normalization allows Marigold to focus on pure affine-invariant depth estimation. Training on synthetic data. Real depth datasets suffer from missing depth values caused by the physical constraints of the capture rig and the physical properties of the sensors. Specifically, the disparity between cameras and reflective surfaces diverting LiDAR laser beams are inevitable sources of ground truth noise and missing pixels [105], [106]. In contrast to prior work that utilized diverse real datasets to achieve generalization [60], [107], we train exclusively with synthetic depth datasets. As with the depth normalization rationale, this decision has two objective reasons. First, synthetic depth is inherently dense and complete, meaning that every pixel has valid ground truth depth value, allowing us to feed such data into the VAE, which can not handle data with invalid pixels. Second, synthetic depth is the cleanest possible form of depth, which is guaranteed by the rendering pipeline. It provides the cleanest examples and reduces noise in gradient updates during Fig. 1: Overview of the Marigold fine-tuning protocol. Starting from pretrained Stable Diffusion, we encode the image and depth into the latent space using the original Stable Diffusion VAE. We fine-tune just the U-Net by optimizing the standard diffusion objective relative to the depth latent code. Image conditioning is achieved by concatenating the two latent codes before feeding them into the U-Net. The first layer of the U-Net is modified to accept concatenated latent codes. See details in Sec. III-B and Sec. III-C. At training time, parameters θ are updated by taking data pair (x, d) from the training set, noising with sampled noise ϵ at random timestep t, computing the noise estimate ˆϵ = ϵθ(dt, x, t) and minimizing one of the denoising diffusion objective functions. The canonical standard noise objective is given as follows [29]: = Ed0,ϵN (0,I),tU (T ) ϵ ˆϵ2 2 . (2) At inference time, := d0 is reconstructed starting from normally-distributed variable dT , by iteratively applying the learned denoiser ϵθ(dt, x, t). We consider the latent space formed in the bottleneck of VAE, trained independently of the denoiser, to enable latent space compression and perceptual alignment with the data space. To translate our formulation into the latent space, for given depth map d, the corresponding latent code is given by the encoder E: z(d) = E(d). Given depth latent code, depth map can be recovered with the decoder D: ˆd = D(z(d)). The conditioning image is also naturally translated into the latent space as z(x) = E(x). The denoiser is henceforth trained in the latent space: ϵθ(z(d) , z(x), t). The adapted inference procedure involves one extra step the decoder reconstructing the data ˆd from the estimated clean latent z(d) 0 : ˆd = D(z(d) 0 ). B. Network Architecture We base our model on pretrained text-to-image LDM Stable Diffusion v2 [12]. With minimal changes to the model, we turn it into conditional depth map generator  (Fig. 1)  . Depth encoder and decoder. We take the frozen VAE to encode both the image and its corresponding depth map into latent space for training our conditional denoiser. Given that the encoder, which is designed for 3-channel (RGB) inputs, receives single-channel depth map, we replicate the depth map into three channels to simulate an RGB image. At this point, the IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6 2 where the binominal coefficient = (cid:0)N (cid:1) represents the number of possible combinations of image pairs from images. After the iterative optimization for spatial alignment, the merged depth is taken as our ensembled prediction. Note that this ensembling step requires no ground truth for aligning independent predictions. This scheme enables flexible tradeoff between computation efficiency and prediction quality by choosing accordingly. E. Implementation We use Stable Diffusion v2 [12] as base LDM, following the original pre-training setup with v-objective [108]. We disable text conditioning and perform steps outlined in Sec. III-B. During training, we apply the DDPM noise scheduler [29] with 1000 diffusion steps. At inference time, we apply DDIM scheduler [30] and sample between 1 and 50 steps. To approximate the mode of conditional distribution and increase quality, we ensemble 10 predictions from different initial noise. Training takes 18K iterations using batch size of 32. To fit one GPU, we use real batch size of 2 and accumulate gradients 16 times. We use the Adam optimizer with 3 105 learning rate. Additionally, we apply random horizontal flipping augmentation to the training data. Training our method to convergence takes approximately 2.5 days on single Nvidia RTX 4090 GPU card. Unlike the 1050 zero-shot evaluation protocol, inference with one ensemble member and one diffusion step 1 1 produces sufficiently good results fast, often sharper than the ensembled prediction. Coupled with model weight quantization and smaller compatible VAEs, such as TAESD [27], the 1 1 prediction takes less than 100ms on most hardware. F. Evaluation Training datasets. We employ two synthetic datasets covering both indoor and outdoor scenes. HyperSim [66] is photorealistic dataset with 461 indoor scenes. We use the official split for training, with around 54K samples from 365 scenes, filtering out incomplete samples. RGB images and depth maps are resized to 480 640 resolution. Depth is normalized with the dataset statistics. Additionally, we transform distances relative to the focal point into depth values relative to the focal plane. The second dataset, Virtual KITTI [67], is synthetic street-scene dataset featuring 5 scenes with diverse weather and camera perspectives. We crop the images to the KITTI resolution [111] and set the far plane to 80 meters. New in Marigold-Depth v1.1: (1) the training data is augmented with flipping, blurring, and color jitter; (2) DDIM timesteps are set to trailing and zero SNR is enabled [23] before fine-tuning. Evaluation datasets. We evaluate Marigold-Depth on 5 real datasets not seen during training. NYUv2 [21] and ScanNet [112] are both indoor scene datasets captured with an RGB-D Kinect sensor. For NYUv2, we utilize the designated test split, comprising 654 images. In the case of the ScanNet dataset, we randomly sampled 800 images from the 312 official validation scenes for testing. KITTI [111] is street-scene dataset with sparse metric depth captured by LiDAR sensor. We employ the Eigen test split [7] made of 652 images. Fig. 2: Overview of the Marigold inference scheme. Given an image x, we encode it with the original Stable Diffusion VAE into the latent code z(x), and concatenate with the depth latent z(d) before giving it to the modified fine-tuned U-Net on every denoising iteration. After executing the schedule of steps, the resulting depth latent z(d) is decoded into an image whose 3 channels are averaged to get the final estimation ˆd. See Sec. III-D for details. 0 the short fine-tuning protocol. Thus, the remaining concern is the sufficient diversity or domain gaps between synthetic and real data, which sometimes limits generalization ability. As demonstrated in our experiments across modalities, our choice of synthetic datasets leads to impressive zero-shot transfer. D. Inference Latent diffusion denoising. The overall inference pipeline is presented in Fig. 2. We encode the input image into the latent space, initialize depth latent as standard Gaussian noise, and progressively denoise it with the same schedule as during fine-tuning. We use DDIM [30] to perform non-Markovian sampling with re-spaced steps for accelerated inference. The final depth map is decoded from the latent code using the VAE decoder and postprocessed by averaging channels. Test-time ensembling. The stochastic nature of the inference pipeline leads to varying predictions depending on the initialization noise in z(d) . Capitalizing on that, we propose the following test-time ensembling scheme, capable of combining multiple inference passes over the same input. For each input sample, we can run inference times. To aggregate these affine-invariant depth predictions {ˆd1, . . . , ˆdN }, we jointly estimate the corresponding scale ˆsi and shift ˆti, relative to some canonical scale and range, in an iterative manner. The proposed objective minimizes the distances between each pair of scaled and shifted predictions ( ˆd j), where ˆd = ˆd ˆs + ˆt. In each optimization step, we calculate the merged depth map by the taking pixel-wise median 1(x, y), . . . , ˆd m(x, y) = median( ˆd (x, y)). An extra regularization term = min(m) + 1 max(m), is added to prevent collapse to the trivial solution and enforce the unit scale of m. Thus, the objective function can be written as follows: i, ˆd (cid:32)(cid:118) (cid:117) (cid:117) (cid:116) 1 N 1 (cid:88) (cid:88) i=1 j=i+1 min s1,...,sN t1,...,tN ˆd ˆd j2 2 + λR (4) (cid:33) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7 TABLE I: Quantitative comparison of Marigold-Depth with SOTA affine-invariant depth estimators on several zero-shot benchmarks1. In the 1st section, we list methods citing our approach [18] as well as methods that require more than 2M samples for fine-tuning (denoted by gray). We compare methods from the 2nd section with flavors of Marigold-Depth v1.0 (ours, CVPR2024) from the 3rd section and Marigold-Depth v1.1 (ours, this paper) in the 4th section. Legend: All metrics are presented in percentage terms; bold numbers are the best, underscored second best; NFEs is the number of function evaluations required to obtain the prediction ensemble steps for diffusion models and 1 for end-to-end networks. Marigold outperforms other methods in this low-data regime on indoor and outdoor scenes without access to real depth samples. Method Omnidata [107] Metric3D [55]2 Metric3D v2 [56]2 DepthAnything [62] DepthAnything v2 [19] DepthFM [24] GeoWizard [69]2 GenPercept [26] Lotus-G [25] Lotus-D [25] E2E-FT [23] E2E-FT [23] DiverseDepth [59] MiDaS [60] LeReS [109] HDN [110] DPT [61] NFEs Prior 1 ImageNet 1 ImageNet 1 DINOv2 1 DINOv2 1 DINOv2 ? 1 SD v2.1 10 50 SD v2.0 1 SD v2.1 1 1 SD v2.0 1 SD v2.0 1 Marigold 1 SD v2.0 1 ImageNet 1 ImageNet 1 ImageNet 1 ImageNet 1 ImageNet Marigold v1.03 w/ TAESD [27] Marigold v1.0 LCM Marigold v1.03 Marigold v1.03 Marigold v1.0 1 1 SD v2.0 10 1 SD v2.0 1 1 SD v2.0 10 1 SD v2.0 10 50 SD v2.0 Marigold v1.13 w/ TAESD [27] (fp16) 1 1 SD v2.0 Marigold v1.13 (fp16) 1 1 SD v2.0 Marigold v1.13 1 1 SD v2.0 Marigold v1.13 10 1 SD v2.0 Marigold v1.13 1 4 SD v2.0 Marigold v1.13 10 4 SD v2. Data Prior Generated Real Synthetic NYUv2 AbsRel δ1 KITTI AbsRel δ1 ETH3D AbsRel δ1 ScanNet AbsRel δ1 DIODE AbsRel δ1 12M 310K 8M 14M 14M 142M 16M 91K 142M 62M 1M 524K 142M 62M 595K 63K 2.3B 51K 227K 2.3B 74K 2.3B 59K 2.3B 59K 2.3B 74K 2.3B 74K 2.3B 1M 1M 1M 14M 14M 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 2.3B 320K 2M 300K 54K 300K 1.2M 188K 74K 74K 74K 74K 74K 74K 74K 74K 74K 74K 74K 7.4 5.8 4.3 4.3 4.4 6.5 5.2 5.6 5.4 5.3 5.2 5. 11.7 11.1 9.0 6.9 9.8 5.9 5.8 5.7 5.7 5.5 6.1 5.8 5.9 5.8 5.7 5.5 94.5 96.3 98.1 98.1 97.9 95.6 96.6 96.0 96.6 96.7 96.6 96.5 87.5 88.5 91.6 94.8 90.3 96.0 96.1 96.2 96.2 96. 95.8 96.1 96.1 96.1 96.2 96.4 14.9 5.3 4.4 7.6 7.5 8.3 9.7 9.9 11.3 9.3 9.6 9.6 19.0 23.6 14.9 11.5 10.0 12.2 10.1 11.0 10.9 9.9 12.4 11.0 11.0 10.9 10.8 10.5 83.5 96.5 98.2 94.7 94.8 93.4 92.1 90.4 87.7 92.8 91.9 92. 70.4 63.0 78.4 86.7 90.1 86.2 90.9 89.1 89.2 91.6 85.0 88.8 88.8 89.0 89.6 90.2 16.6 6.4 4.2 12.7 13.2 6.4 6.2 6.2 6.8 6.2 6.4 22.8 18.4 17.1 12.1 7.8 7.5 6.6 6.9 6.8 6. 7.6 7.0 7.0 6.9 7.2 6.9 77.8 96.5 98.3 88.2 86.2 96.1 95.8 96.1 95.3 95.9 95.9 69.4 75.2 77.7 83.3 94.6 94.4 95.8 95.5 95.6 96.0 94.3 95.5 95.5 95.7 95.3 95.7 7.5 7.4 2.2 4.2 6.1 6.0 6.0 5.8 5. 10.9 12.1 9.1 8.0 8.2 6.7 6.6 6.6 6.5 6.4 6.8 6.6 6.6 6.5 6.0 5.8 93.6 94.2 99.4 98.0 95.3 96.0 96.3 96.2 96.5 88.2 84.6 91.7 93.9 93.4 95.0 95.0 95.2 95.3 95. 95.1 95.3 95.3 95.4 96.0 96.3 33.9 21.1 13.6 27.7 6.5 22.5 29.7 35.7 30.2 30.3 37.6 33.2 27.1 24.6 18.2 31.7 30.5 31.2 31.0 30.8 31.1 30.4 30.4 30.3 30.1 29.8 74.2 82.5 89.5 75.9 95.4 80.0 79.2 75.6 77.9 77. 63.1 71.5 76.6 78.0 75.8 75.6 77.2 76.6 76.7 77.3 75.9 77.3 77.3 77.3 77.9 78.2 1 Metrics in the 1st section are sourced from the respective papers. Metrics in the 2nd section are sourced from Metric3D [55], except the ScanNet benchmark. For ScanNet, Metric3D used different random split that is not publicly accessible. Therefore, we re-ran baseline methods on our split. We additionally took numbers from Metric3D for HDN [110] on ScanNet benchmark due to unavailable source code. 2 Privileged information used by methods: Metric3D and Metric3D v2 require camera intrinsics; GeoWizard requires choosing between indoor and outdoor regimes. 3 These Marigold variants are evaluated using the trailing timestamps setting of the DDIM scheduler. TABLE II: Inference time of Marigold-Depth and other methods on 768 768 image using an RTX 3090 GPU. Method DPT DepthAnything v2 Metric3D v2 Marigold v1.1 (11) Marigold v1.1 (11) (fp16) Marigold v1.1 (11) w/ TAESD (fp16) Time (sec) 0.158 0.289 0.386 0.568 0.274 0.082 ETH3D [113] and DIODE [114] are two high-resolution datasets, both featuring depth maps derived from LiDAR sensor measurements. For ETH3D, we incorporate all 454 samples with available ground truth depth maps. For DIODE, we use the entire validation split, which encompasses 325 indoor samples and 446 outdoor samples. Evaluation protocol. Following the protocol of affine-invariant depth evaluation [60], we first align the estimated merged prediction to the ground truth with the least squares fitting. This step gives us the metric depth map = + in the same units as the ground truth. Next, we apply two metrics [55], [60], [61], [109] for assessing quality of depth estimation. The first is Absolute Mean Relative Error (AbsRel ), calculated as: 1 i=1 ai di/di, where is the total number of pixels. The second, Threshold Accuracy (δ1 ), measures the (cid:80)M proportion of pixels satisfying max(ai/di, di/ai) < 1.25. Comparison with other methods. We compare MarigoldDepth to 5 zero-shot baselines in Tab. I. We filtered baselines based on the affordability of the training protocol (at most 2M training samples) and temporal relevance. Marigold-Depth, built upon the Stable Diffusion prior and small set of synthetic samples, outperforms prior work and remains competitive with methods that rely on larger training sets. Although trained exclusively on synthetic depth datasets, the model generalizes well to wide range of real-world scenes. For visual assessment, we present qualitative comparison and 3D visualizations of surface normals reconstructed from depth in Figs. 3 and 5. Marigold accurately captures both the overall scene layout, such as the spatial relationships between walls and furniture, and fine-grained details, as indicated by the arrows. In particular, the reconstruction of flat surfaces, especially walls, is significantly improved. Additionally, we report an inference speed comparison in Tab. II. G. Ablation Studies We select two zero-shot validation sets for ablation studies: the official training split of NYUv2 [21], consisting of 785 samples, and randomly selected subset of 800 images from the KITTI Eigen [7] training split. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
        },
        {
            "title": "DPT",
            "content": "DAv2 Metric3Dv2 Marigold (ours) Ground Truth 2 N I 3 t a E I Fig. 3: Qualitative comparison of monocular depth estimation methods across different datasets. Marigold excels at capturing thin structures (e.g., chair legs) and preserving overall layout of the scene (e.g., walls in ETH3D and chairs in DIODE). Fig. 4: Ablation of ensemble size. We observe monotonic improvement with the growth of ensemble size. This improvement starts to diminish after 10 predictions per sample. Fig. 6: Ablation of denoising steps. One denoising step is sufficient with DDIM-trailing [23] (default in v1.1). DDIM [18] (default in v1.0) requires at least 4-10 steps. Input RGB Image MiDaS Omnidata DPT Marigold (ours) Ground Truth 2 N N S D TABLE III: Training datasets: HyperSim [66] alone contributes the most; Virtual KITTI [67] improves outdoor results."
        },
        {
            "title": "Virtual\nKITTI",
            "content": "NYUv2 AbsRel δ"
        },
        {
            "title": "KITTI",
            "content": "AbsRel 13.9 5.7 5.6 83.4 96.3 96.5 15.4 13.7 11.3 δ1 79.3 82.5 88. Fig. 5: Qualitative comparison (unprojected from depth, colored as normals) of monocular depth estimation methods across different datasets. Marigold-Depth stands out for its superior reconstruction of flat surfaces and detailed structures. Training data domain. To better understand the impact of the synthetic data on model generalization, we conduct an ablation study using the two datasets employed during training. The results, presented in Tab. III, show that even fine-tuning on single synthetic dataset enables the pretrained LDM to adapt reasonably well to monocular depth estimation. However, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9 TABLE IV: Quantitative Comparison of Marigold-Normals with SOTA surface normals estimators on several zero-shot benchmarks1. In the 1st section, we list methods requiring more than 2M samples for fine-tuning. We compare methods from the 2nd section with flavors of Marigold-Normals (ours, this paper) from the 3rd section. Legend: The mean metric is presented as absolute angles, 11.25 metric is in percentage terms; bold numbers are the best, underscored second best; NFEs is the number of function evaluations required to obtain the prediction ensemble steps for diffusion models and 1 for end-to-end networks. Marigold outperforms other methods in this low-data regime on most indoor and outdoor scenes. Method NFEs Prior Data Prior Real Synthetic ScanNet Mean 11.25 NYUv2 Mean 11.25 iBims-1 Mean 11.25 DIODE Mean 11.25 OASIS Mean 11.25 Metric3D v2 [56]2 Omnidata v2 [81] DSINE [82]2 GeoWizard [69]2 GenPercept [26] StableNormal [83] Lotus-G [25] Lotus-D [25] E2E-FT [23] E2E-FT [23] Marigold-Normals v0.13 Marigold-Normals v1.1 Marigold-Normals v1.1 Marigold-Normals v1.1 Marigold-Normals v1.1 1 DINOv2 1 ImageNet 142M 16M 91K 15M 12M 310K 1 ImageNet 10 50 SD v2.0 1 SD v2.1 1 SD v2.0 1 1 SD v2.0 1 SD v2.0 1 Marigold 1 SD v2.0 10 50 SD v2.0 1 1 SD v2.0 10 1 SD v2.0 1 4 SD v2.0 10 4 SD v2.0 1.3M 86K 74K 2.3B 51K 227K 2.3B 44K 2.4B 51K 227K 2.3B 59K 2.3B 59K 2.3B 74K 2.3B 74K 2.3B 39K 2.3B 77K 2.3B 77K 2.3B 77K 2.3B 77K 16. 16.2 17.6 18.2 16.7 15.3 15.3 14.7 14.7 16.1 14.9 14.8 15.2 14.5 60.2 61.0 54.6 57.4 54.0 64.0 62.9 66.0 66.1 62.3 64.3 64.5 65.3 66.1 13.3 17. 16.4 19.0 18.3 17.8 16.9 16.8 16.2 16.5 17.1 16.4 16.3 17.0 16.1 66.4 55.5 59.6 50.0 56.0 54.2 59.1 58.2 61.4 60.4 58.5 58.9 59.0 59.6 60.5 19.6 18. 17.1 19.3 18.3 17.1 17.5 17.7 15.8 16.1 16.6 17.2 17.1 17.0 16.3 69.7 63.9 67.4 62.3 63.8 67.8 66.1 64.9 69.9 69.7 68.0 65.6 65.7 68.0 68.5 12.6 20. 19.9 24.7 22.3 19.3 21.2 21.0 19.2 19.0 19.6 19.5 19.5 19.4 18.8 64.9 40.8 41.8 30.1 38.1 53.8 39.7 39.7 43.8 44.4 44.7 43.2 43.3 45.0 45.5 23.4 24. 24.4 25.3 25.9 25.7 24.7 25.7 22.8 23.6 23.5 23.2 23.2 23.2 22.4 28.5 27.7 28.8 26.9 23.3 25.4 27.0 25.3 29.8 27.9 28.0 28.3 28.3 29.2 30.1 1 Metrics on ScanNet, NYUv2, and iBims-1 are sourced from the respective papers. Metrics that were not reproducible are re-computed by us (for GeoWizard, GenPercept, and StableNormal). We compute the metrics for all methods on OASIS (except for Omnidata and DSINE) and DIODE. 2 Privileged information used by methods: Metric3D v2 and DSINE require camera intrinsics; GeoWizard requires choosing between indoor and outdoor regimes. 3 The preview models (v0.1), uploaded to the Hugging Face repository shortly after releasing Marigold-Depth (v1.0) [18]. more diverse and photorealistic data yield stronger performance across both indoor and outdoor scenes. Notably, incorporating training data from different domain enhances performance not only on that domain but also on the original one. Test-time ensembling. We test the effectiveness of the proposed test-time ensembling scheme by varying the number of predictions. As shown in Fig. 4, single prediction already yields reasonably good results. Ensembling 10 predictions can reduce the absolute relative error on NYUv2 by 8%, and ensembling 20 predictions brings an improvement of 9.5%. Number of denoising steps. Like the base model, Marigold is configured to use 1000-step DDPM schedule during training. In version 1.0, inference used DDIM with leading timesteps, requiring 4 to 10 function evaluations (NFE) to reach peak performance. To report best-case results, we used 50 steps in the initial evaluations. Later, Garcia et al. [23] proposed switching to trailing timesteps for inference with DDIM. This change significantly improved efficiency of Marigold, with performance saturating at just 1 DDIM step (NFE=1). This is the default in all Marigold v1.1 models. Table reports results across model versions, NFE, ensemble sizes, alternative VAEs, and FP16 quantized weights. The effect of varying denoising steps in DDIM scheduler [30] is shown in Fig. 6. Input Image StableNormal DSINE Lotus-G Marigold-Normals Ground Truth 2 N D e c 1 - i Fig. 7: Qualitative comparison of monocular surface normals estimation methods across different datasets. Compared to baseline methods, Marigold-Normals demonstrates superior performance in handling complex scene layouts and shows greater robustness to motion blur and reflections. often struggles with the sim-to-real gap. This motivates Marigold-Normals, which aims to bridge the gap to real data through its Stable Diffusion prior. IV. SURFACE NORMALS ESTIMATION MODEL A. Method Surface normals and depth estimation are inherently related, as both aim to regress 3D geometry. While the depth of pixel is predicted as single positive scalar, the surface normal is represented as three-dimensional vector on the unit sphere. Real surface normals can hardly be collected outside of simulation or controlled environment. Instead, normals have traditionally been derived from depth measurements, which often introduce noise at flat surfaces and unrealistic smoothness at depth discontinuities. Simulated data, however, We closely follow the Marigold-Depth fine-tuning protocol and introduce Marigold-Normals, model variant for monocular surface normals estimation. Methodology adaptation for the new task to cope with the three-dimensional unit vectors of surface normals is minimal. Raw ground-truth normal maps are streamed directly into the VAE encoder during training. No range normalization or channel replication is required. VAE outputs are normalized along the channel dimension to ensure unit-length predictions. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10 Fig. 8: Ablation of ensemble size for Marigold-Normals. The performance consistently improves with increasing ensemble size. Diminishing returns begin after 10 predictions per sample. Fig. 9: Ablation of denoising steps for Marigold-Normals. The best performance in benchmarks is obtained at 4 denoising steps. Visually, one step is sufficient in most cases. Test-time ensembling. First, we run inference times with different noise initialization. We then average the normals predictions {ˆn1, . . . , ˆnN } to single mean prediction and normalize it to unit length. Lastly, for every pixel (u, v) in the final prediction, we select the nearest neighbor vector from the i-th prediction ˆn(u,v) that maximizes the cosine similarity with the corresponding vector in the mean normal map n(u,v): arg maxi{1,...,N } n(u,v) ˆn(u,v) . B. Implementation We fine-tune the model for 26K iterations using the Adam optimizer with learning rate of 6 105. The training data is augmented with horizontal flipping, blurring, and color jitter. At inference, we use the DDIM scheduler [30] in trailing mode and perform 4 steps. The final prediction is aggregated using an ensemble size of 10. All other settings are the same as for the depth model. Training Datasets. We train the model on three synthetic datasets covering both indoor and outdoor scenes. Hypersim [66] and InteriorVerse [99] are photo-realistic indoor datasets. We filter out incomplete samples and obtain 49K samples from 434 scenes for HyperSim and 27K samples from 3806 scenes for InteriorVerse. The training resolution is kept at 480 640 for both datasets. Sintel [115] consists of indoor and outdoor sequences from short animated film. We filter out low-quality samples and use 627 training images. The images are center-cropped to an aspect ratio of 3 : 4 and resized to 480640. Despite the small sample size, we observe improvement in training with Sintel due to its scene diversity and image appearance. C. Evaluation Evaluation Protocol. We evaluate our method on five unseen benchmarks. NYUv2 [21], ScanNet [112], and iBims-1 [116] are indoor depth datasets, for which we use the ground-truth normals provided by [82]. This includes 654 samples from the NYUv2 test split, all 100 samples of iBims-1, and subset of 300 samples defined by [78] of ScanNet. DIODE [114] features both indoor and outdoor scenes. We use the validation split, encompassing 325 indoor and 446 outdoor samples. OASIS [117] contains in-the-wild images sourced from the internet. We use the entire validation split of 10,000 samples. Following established methods [78], [118], [119], we report the mean angular error and the percentage of pixels with an angular error <11.25. Comparison with other methods. We compare MarigoldNormals to 8 baselines. DSINE [82] is discriminative regression-based method that relies on CNN architecture. Similar to us, GeoWizard [69], StableNormal [83], and LotusG [25] are generative diffusion-based methods that fine-tune Stable Diffusion backbone. GenPercept [26], Lotus-D [25], and E2E-FT [23] bypass the probabilistic formulation and perform end-to-end training and inference instead. The quantitative results are shown in Table IV. Our method consistently outperforms the baselines on most datasets and metrics. Notably, our straightforward fine-tuning and inference approach proves more effective than the more complex strategies employed by other diffusion-based methods. qualitative comparison is shown in Figure 7. It is apparent that Marigold-Normals produces highly accurate predictions, even in challenging scene layouts and scenarios. D. Ablations Test-time ensembling. We investigate the impact of testtime ensembling while varying the number of aggregated predictions. The ablation studies are conducted on the ScanNet and DIODE splits. As illustrated in Fig. 8, the performance with ensembling behaves very similarly to the depth model. While single prediction already delivers solid results, the performance consistently improves as more predictions are combined. However, like depth, the gains plateau when more than 10 predictions per image are aggregated. Number of denoising steps. As quantitatively shown in Fig. 9, the best performance with the DDIM trailing timesteps setting is achieved at 4 steps. We additionally visualized predictions with 1-, 4-, and 20-step inference in Fig. 10. Interestingly, the level of detail can be controlled by simply adjusting the number of denoising iterations. While 1-step inference already IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11 TABLE V: Quantitative Comparison of Marigold-IIDAppearance v1.1 with SOTA methods on the InteriorVerse test set. Marigold outperforms the two competing methods on this benchmark. Method Albedo PSNR SSIM LPIPS Material PSNR SSIM LPIPS IID-Diffusion [96] RGBX [97] 18.10 13.16 Marigold-IID-Appearance v1.1 19.50 0.863 0. 0.846 0.198 0.289 0.190 16.09 10.13 0.691 0.547 0.356 0. 17.63 0.803 0.286 TABLE VI: Quantitative Comparison of Marigold-IIDLighting v1.1 with SOTA methods on the HyperSim test set. Marigold achieves competitive performance on this benchmark. Method Albedo PSNR SSIM LPIPS Lighting PSNR SSIM LPIPS IID-in-the-wild [90] RGBX [97] 19.28 17.43 0.819 0.795 0.260 0.200 16.82 16. 0.725 0.742 0.308 0.251 Marigold-IID-Lighting v1.1 18.21 0.771 0. 17.62 0.729 0.263 B. Implementation The U-Net is adapted to handle the increased number of predicted images (P ): 2 and 3 for the IID-Appearance and IID-Lighting models, respectively. The input channels of the first convolutional layer are replicated + 1 times; the whole weight tensor is divided by the replication factor to maintain the activations statistics. The output channels of the final layer are replicated times without changing weights. The modified IID-Appearance U-Net is fine-tuned for 40K iterations on the training split of InteriorVerse, consisting of 45K samples at 480 640 resolution. Gamma correction and conversion from linear to sRGB space are applied to the input scene and target intrinsic images. The IID-Lighting U-Net is fine-tuned for 36K iterations on pre-filtered training split of HyperSim, yielding 24K samples. All samples are resized to 480 640 and converted to sRGB space while keeping the target intrinsic images in linear space. For quantitative evaluation, we perform 4 denoising steps without ensembling. Other settings remain the same as for the depth model. As with other Marigold models, 1 step is sufficient qualitatively. C. Evaluation Evaluation Protocol. The IID-Appearance model is evaluated on the test split of the InteriorVerse [99] dataset, which contains 2.6K samples. We assess the prediction performance of albedo and material. For IID-Lighting, evaluation is performed on the HyperSim [66] test split, comprising 5.2K samples, where we evaluate the quality of the predicted albedo and shading components. Albedo and material predictions are compared directly to the ground truth without any alignment. Due to their differing value ranges, shading predictions are first scalealigned to the corresponding ground truth and then normalized to the unit range before evaluation. The reported metrics are Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) [120], and Learned Perceptual Image Patch Similarity (LPIPS) [121]. Fig. 10: Prediction granularity and denoising steps. From left to right, we visualize predictions with 1, 4, and 20 denoising steps during inference. By increasing the number of steps, fine details, such as the cats fur, become more pronounced. produces reasonably good results in all cases, increasing the number of steps results in more pronounced details in highfrequency regions. However, improved details do not necessarily translate to improved performance metrics, as most evaluation benchmarks either mask out or over-smooth high-frequency regions in the ground truth. V. INTRINSIC DECOMPOSITION MODELS Adaptation of Marigold to other image analysis tasks, such as Intrinsic Image Decomposition (IID), is simple and affordable. Specifically, this task enables structured separation of images into physically meaningful properties. We introduce two models: Marigold-IID-Appearance and Marigold-IID-Lighting. Marigold-IID-Appearance represents intrinsic properties through physically-based BRDF, where material attributes are characterized by three key components: albedo, roughness, and metallicity. This model primarily focuses on estimating illumination-independent reflectance properties. Marigold-IID-Lighting decomposes an image into albedo, diffuse shading, and non-diffuse residual component. This decomposition aligns with the intrinsic residual model in linear space = + R, where the image is composed of albedo A, diffuse shading component (representing illumination color), and an additive residual term capturing non-diffuse effects. This formulation provides structured way to separate reflectance from shading while accounting for complex illumination phenomena. Similarly to the surface normals estimation task, ground truth for IID is hard to obtain without simulation or outside controlled capture environment. Therefore, we again turn to the available synthetic data to derive the Marigold-IID models. A. Method Different from depth and normals, IID requires predicting multiple images representing the decomposition of single input image. In the case of Marigold-IID-Appearance, we predict two images per input: the first is the albedo image in standard color space, with values normalized to the unit range. The second image encodes two BRDF properties (roughness and metallicity) into the red and green channels [96], also normalized to unit range. We denote this modality as material. We keep the blue channel at zero, which gives the material visualization red-green tone. For Marigold-IID-Lighting, three images are predicted per input: the albedo image, shading, and residual components, all normalized to the unit range. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12 Fig. 11: Qualitative comparison of Marigold-IID-Appearance (left) and Marigold-IID-Lighting (right). Left: albedo and material (with roughness in the red channel and metallicity in the green channel) predictions on the InteriorVerse test set. Right: albedo and diffuse shading predictions on the HyperSim test set. Predictions of Marigold-IID-Appearance and Marigold-IID-Lighting contain less baked-in shading and are more consistent with the ground truth. Fig. 12: Robustness to varying lighting conditions. The Marigold-IID models generate consistent predictions across different environmental lighting setups of the same scene. Comparison with other methods We compare our results to three state-of-the-art methods: RGBX [97] and Intrinsic Image Diffusion (IID-Diffusion) [96] for IID-Appearance, and RGBX and IID-in-the-wild [90] for IID-Lighting. For the cited methods, we adopt the inference settings reported in their respective papers: 50 denoising steps for RGBX, and 50 denoising steps with an ensemble size of 10 for IID-Diffusion. The quantitative comparisons are shown in Tab. and Tab. VI. The visual comparisons are presented in Fig. 11. Our method produces quantitatively more accurate and qualitatively cleaner decompositions of images. We further demonstrate the robustness of our method to varying environmental lighting conditions in Fig. 12. All modalities in-the-wild can be seen in Fig. 13. Evidently, image analysis tasks benefit from the rich generative prior, validating our universal fine-tuning protocol. Fig. 13: Marigold in-the-wild results all modalities. Our fine-tuning protocol enables generalization across multiple modalities. None of the fine-tuning datasets included humans, animals, food, engines, or toys, attesting to the successful carryover of the rich generative prior to downstream tasks. VI. LATENT CONSISTENCY MODEL (LCM) A. Method Latent Consistency Models [22] is latent diffusion model class that enables high-quality oneor few-step inference. Inspired by LCMs success in fast image generation, we developed Marigold-LCM, Latent Consistency Model variant of Marigold that achieves similar prediction results in one or few denoising steps. We distill Marigold-LCM from the standard Marigold using similar recipe detailed in Luo et al. [22]  (Fig. 14)  . Similarly to the base fine-tuning protocol, we only distill the U-Net part of the model while keeping the VAE frozen. The LCM distillation involves three models: frozen teacher model Φ, which is standard Marigold U-Net; student model Θ, which IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE B. Inference The consistency distillation training allows only one forward pass of the noise latent through the trained student model to generate the clean predicted depth latent. Additionally, one can improve the sample quality by taking multi-step sampling [22], [31]. This is done by adding another random noise latent to the denoised latent following timestep schedule and then denoising again through the student model. In practice, we use the identical noise schedule of the original Marigold, swap the U-Net with the trained LCM model, and change the denoising sampling method from DDIM to LCM, as introduced above. This means that the evaluation protocol of Marigold-LCM is identical to the base Marigold. C. Implementation The Marigold-LCM model is distilled from the base model for 5K iterations using the AdamW [123] optimizer with base learning rate of 3106. We use the same training set, batch size, gradient accumulation steps, and data augmentation as Marigold training. Distilling Marigold LCM takes approximately one day on single NVIDIA A100 GPU card with 40G VRAM. We apply one-step denoising at inference time to output the clean predicted depth latent. For evaluation, we follow the same ensemble method as the standard Marigold using 10 samples. D. Experiments We compared Marigold-LCM with one LCM inference step with various Marigold DDIM configurations in Tab. I. Although Marigold-LCM with one step does not outperform the original Marigold with 50 steps in most cases, it outperforms prior art on most datasets and metrics. This validates the hypothesis that Marigold is amenable to the latent consistency distillation, and the resulting model is on par with the base. It also shows that LCM distillation can be successfully adapted to modalities other than text-to-image. However, given the improved quantitative and qualitative performance of Marigold with DDIM and trailing timesteps pointed out in E2E-FT [23], the viability of LCM distillation remains an open question for future research. VII. HIGH RESOLUTION DEPTH MODEL Applying monocular depth estimation networks to highresolution images seems straightforward, but poses two inherent challenges. First, neural networks have fixed receptive field limited to the model architecture or the resolution of the training data. Second, memory consumption can become excessive when naively applying those models to larger image dimensions. The simple way to address those challenges is to downsample to its native processing resolution (768 for Marigolds fine-tuned from Stable Diffusion) and later upsample the computed depth map. We refer to this prediction as global depth map ˆd(g). Typically, this approach compromises the edge quality of the depth estimation for high-resolution predictions. An alternative approach is partitioning large image into smaller patches and processing them independently. However, even if consistency at the seams was perfect, this method suffers from global inconsistencies due to the lack of communication between neighboring patches w.r.t. global layout. Fig. 14: Overview of Marigold-LCM distillation. To train Marigold-LCM, we initialize three replicas of the base Marigold: the Teacher (Φ), the Target (Θ), and the Student (Θ). The student is updated via optimization of the consistency objective, and the target is updated via EMA of student weights. At each training step, the student learns to predict the same clean latent z(d) as produced by the teacher after applying the DDIM step of size k. Each model takes the image condition z(x) and the input timestep, similarly to Fig. 1. 0 we eventually output as Marigold-LCM; and target model Θ. Both the student and target models are initialized with the same weights as the teacher model. At each training iteration, we sample data from the training dataset and convert the sample to latent images using the VAE encoder. We then sample Gaussian noise and diffuse the depth latent to random timestep to obtain z(d) . Now the teacher model takes z(d) as input, predicts noise, and then estimates the depth latent z(d) tk at noise level tk using the DDIM solver step detailed in [22]. We set = 200 in our implementation, which yields the best distillation result. We now minimize the loss between the outputs of student Θ and target model Θ through self-consistency function [22], [31] (cid:16) (Θ, z(d) , t), (Θ, z(d) (cid:17) tk, k) . Here, the self-consistency function is defined as (θ, x, t) = cskip(t)x + cout(t)z(d,θ) , (5) (6) where cskip, cout are differentiable functions that satisfiy cskip(ϵ) = 1, cout = 0 for some small ϵ > 0, and z(d,θ) is the clean denoised depth latent predicted by model θ. We use the Pseudo-Huber metric [122] as our loss function: (cid:113) 0 L(x, y) = y2 2 + c2 c, (7) where we set = 0.001. At the end of each training iteration, the student model is updated using gradient descent according to our loss function. While the weights of the target model are updated with running average of the weights of the student model: Θ = stopgrad (cid:0)µΘ + (1 µ) Θ(cid:1) , (8) here the decay weight µ is set to be 0.95. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 half-resolution crops of size 384 512. To generate the global conditioning, we precompute Marigold-Depth results for the whole training dataset with half the dataset resolution as the processing resolution. Specifically, we generate two sets of predictions: one set of base predictions with 10 ensemble members and one set using ensemble size one. We randomly select one of these sets for each sample during training. For data augmentations, we apply Gaussian blur to 50% training samples with random radii ranging from 0 to 4 pixels. C. Evaluation Evaluation datasets. We benchmark on two high-resolution datasets that contain unmasked depth continuities. The datasets are: First, the stereo-matched Middlebury 2014 [124] contains ground truth of resolution 2016 2940. We evaluate the complete dataset with 46 samples. Second, we evaluate the Booster dataset [125], which contains stereo-generated ground-truth of size 3008 4112. We only consider the scenes where the ground truth is provided. Since the images in each scene only vary by illumination and small parallax, we use one image per scene 31 samples total. Evaluation protocol. We evaluate all depth maps with the affine-invariant protocol and provide the corresponding general-purpose metrics i.e., the Absolute Mean Relative Error (AbsRel ) and Threshold Accuracy (δ1 ) accuracy. Furthermore, four edge-based metrics evaluate the quality of the ) discontinuities: Depth Boundary Error Completeness (ϵcomp ) [126], as well as Edge Precision (ϵprc ) and Accuracy (ϵacc and Recall (ϵrec ) [127]. Since none of the methods directly outputs the exact dataset-specific resolutions, we resample the prediction using bilinear interpolation. For ours, we employ two upsampling iterations with Marigold-HR, until the output resolution approximates the target resolution. DBE DBE Comparison with other methods. We benchmark our method against other recent methods designed for high-resolution inference. BoostingDepth [100] implements version based on MiDaS [60] and another based on LeRes [109]. We note that the officially provided checkpoints are trained on mixed dataset, including our evaluation dataset, Middlebury 2014. Finally, we also compare the recent PatchFusion [101] that bootstraps ZoeDepth [54] and the concurrently proposed DepthPro [104]. In Table VII, we present the results on the high-resolution dataset. Marigold-HR achieves the best or second-best performance in all metrics. Depth Pro performs best in global depth estimation metrics (AbsRel and δ1). In terms of edge quality metrics (ϵcomp DBE, ϵprc, ϵrec), Depth Pro is also strong performer; however, on the Booster dataset, Marigold-HR achieves slightly better performance. DBE , ϵacc Furthermore, we show qualitative results in Fig. 16. These results demonstrate the visual quality attainable with diffusionbased model at high resolution. For the in-the-wild example as well, our model produces plausible results, capturing even fine-grained details such as the cats whiskers. D. Ablations Fig. 15: High-resolution Marigold Pipeline. We first create global prediction ˆd(g) with the original Marigold-Depth pipeline at the native processing resolution. This prediction is then used as an additional conditioning variable in the upsampling diffusion process, which upsamples the prediction in patch-based, MultiDiffusion forward pass. A. Method We introduce the Marigold-HR model to overcome these challenges  (Fig. 15)  . The process starts by predicting global depth map at native processing resolution, as in the original version. This global depth map serves as coarse scene representation and is the initialization for the following refinement procedure. Next, we upsample the global depth map by factor 2 and use it with the correspondingly resampled RGB image as the conditioning for another diffusion model named Φ. To keep memory usage bounded, we implement the forward pass as bundle of inferences of overlapping tiles, where we synchronize latents via closed-form equation from the MultiDiffusion approach [103]: Ψ (Jt z) = (cid:88) i=1 (Wi) 1 j=1 1 (cid:80)n (Wj) 1 (cid:0)Φ (cid:0)zi (cid:1)(cid:1) (9) Wi are the per-pixel blending weights of tile in our implementation, we use the Chamfer distance to the image border. The function 1 transforms the tile back into its location in the global canvas based on the tile index i. For this model is defined as the Fi(cat(z(d) latent variable zi , z(x))). , z(g) i B. Implementation For the Marigold-HR refiner, we resume the training from the Marigold-Depth checkpoint for 12K iterations with additional conditioning on the 2 lower resolution inference. We follow the BetterDepth protocol [70], first aligning the global prediction to the ground truth and then masking out the loss of dissimilar patches, such that the refiner is encouraged to follow the conditioning. We use the suggested setting of η = 0.1 to threshold the distance between patches. For the MultiDiffusion pipeline, we set the patch overlap to 50%. We keep the same training datasets as the base model. However, we deviate from the base protocol and train with We conduct ablation studies to evaluate the impact of our models two main methodological design choices: global IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE VII: Quantitative comparison of Marigold-HR v1.0 against SOTA depth estimators. We note the bootstraped model in the brackets after the actual method name. Marigold-HR improves upon the base model, particularly excelling in edge quality metrics, where it achieves results competitive with current state-of-the-art models."
        },
        {
            "title": "Data",
            "content": "Middlebury 2014 Real Synthetic AbsRel δ1 ϵcomp DBE ϵacc DBE ϵprc ϵrec AbsRel Boosting [100] (MiDaS [60]) Boosting [100] (LeRes [109]) PatchFusion [101] (ZoeDepth [54]) Depth Pro [104] Marigold-Depth v1.0 Marigold-HR v1. 2M 300K 54K 3.9M 1M 5.1M 2.5M 74K 74K 7.3 7.0 6.4 3.1 5.0 5.0 94.2 95.3 96.2 99. 97.5 97.8 5.9 5.4 4.5 3.6 5.0 3.5 1.9 1.9 1.9 1.7 2.4 1.8 30. 29. 29. 36. 24. 33. 21. 29. 42. 54. 29. 61. 7.5 6.9 6.7 2.0 3.9 4.3 Booster Dataset ϵacc DBE δ1 ϵcomp DBE ϵprc ϵrec 95.2 94.7 94.6 99.8 99.1 99.2 8.1 6.2 5.7 4.9 6.4 4. 2.6 2.2 2.4 2.3 3.1 1.8 43. 44. 44. 48. 33. 48. 12. 25. 28. 38. 22. 45. TABLE VIII: Ablation study for Marigolds high-resolution module. Combining both global conditioning and MultiDiffusion inference yields the best overall performance compared to its ablated versions. Method Marigold-Depth (base) Marigold-Depth + Global cond. Marigold-Depth + MultiDiffusion Marigold-HR (best) Size 768 1536 3072 1536 3072 1536 3072 1536 3072 Middlebury 2014 ϵacc DBE δ1 ϵcomp DBE ϵprc Booster Dataset ϵacc DBE δ1 ϵcomp DBE ϵprc 97.5 95.3 84. 97.4 96.7 93.3 82.9 98.0 97.8 5.0 4.4 6.3 3.7 4.0 4.0 5. 3.8 3.5 2.4 2.3 2.2 1.9 1.8 2.1 2.0 2.0 1.8 24. 23. 23. 30. 30. 25. 23. 30. 33. 99.1 92.6 79.3 98.4 98.1 88.6 82. 98.8 99.2 6.4 5.6 8.8 4.5 4.4 4.5 5.0 4.5 4.1 3.1 2.5 7. 2.1 1.9 1.6 2.0 2.2 1.8 33. 37. 16. 45. 49. 40. 23. 43. 48. denotes that inference had to be done in float16 to reduce GPU memory usage. balancing the benefits of global context and high-resolution edges, with moderate memory costs below 15GB. VIII. CONCLUSION We have presented Marigold, an affordable fine-tuning protocol for pretrained text-to-image LDMs, and family of models for state-of-the-art image analysis tasks. Our evaluation confirms the value of leveraging rich scene priors and diverse synthetic data across tasks such as monocular depth prediction, surface normals estimation, and intrinsic image decomposition. Marigold offers competitive performance across all these tasks. Additionally, we have presented LCM distillation and High-Resolution inference, which are adaptable to any modality. Marigold is trainable in under 3 GPU-days on consumer hardware, and its single-step inference has runtime comparable with other recent approaches. List of models, web apps (spaces), code, and further reading links: Space Depth Space Normals Space Intrinsics Model Depth v1.0 Model Depth v1.1 Model Normals v1.1 Model Appearance v1.1 Model Lighting v1.1 Model Depth-LCM v1.0 Model Depth-HR v1.0 Training code Inference code diffusers tutorial hf.co/spaces/prs-eth/marigold hf.co/spaces/prs-eth/marigold-normals hf.co/spaces/prs-eth/marigold-intrinsics hf.co/prs-eth/marigold-depth-v1-0 hf.co/prs-eth/marigold-depth-v1-1 hf.co/prs-eth/marigold-normals-v1-1 hf.co/prs-eth/marigold-iid-appearance-v1-1 hf.co/prs-eth/marigold-iid-lighting-v1-1 hf.co/prs-eth/marigold-depth-lcm-v1-0 hf.co/prs-eth/marigold-depth-hr-v1-0 github.com/prs-eth/Marigold hf.co/docs/diffusers/api/pipelines/marigold hf.co/docs/diffusers/using-diffusers/marigold usage Fig. 16: Quantitative comparison of Marigold-HR, Marigold, and other SOTA methods. Predictions on Middlebury 2014 and Booster are aligned to the ground truth and visualized with the same color mapping. The predictions for the in-the-wild example are per sample normalized. Marigold-HR produces fine-grained outputs while also maintaining global context. conditioning and MultiDiffusion inference. The results are presented in Table VIII. Starting with the base Marigold-Depth model, we first augment it with global conditioning, which requires retraining the model. This modification improves the edge quality metrics, while the global metrics only slightly worsen. However, we note that GPU memory consumption scales quadratically with the upsampling factor in this configuration, making it less practical for high-resolution applications. Next, we apply the MultiDiffusion inference strategy to the Marigold-Depth model without retraining. This approach also improves the edge quality metrics at higher resolutions but keeps GPU memory bounded. However, we observe degradation in the global metrics because the patches processed during inference lack global context. Finally, by combining global conditioning and MultiDiffusion inference, we improve global and edge-focused metrics as shown in the last two rows of Table VIII; effectively IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank: The Hugging Face team (and especially Ahsen Khaliq, Omar Sanseviero, Sayak Paul, Yiyi Xu) for (1) the GPU grants to host Marigold spaces and models, (2) helping us to promote Marigold among research and content creator communities on (Twitter), Posts, Spaces, and beyond, (3) guidance with integrating Marigold into the diffusers [14]; Robert Presl for creating multiple promotional 3D prints for this paper; Alexander Becker, Dominik Narnhofer, and Xiang Zhang for discussions related to Marigold-HR; Peter Kocsis for help with reproducing [96]. [22] S. Luo, Y. Tan, L. Huang, J. Li, and H. Zhao, Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023. 2, 3, 12, 13 [23] G. M. Garcia, K. A. Zeid, C. Schmidt, D. de Geus, A. Hermans, and B. Leibe, Fine-tuning image-conditional diffusion models is easier than you think, arXiv preprint arXiv:2409.11355, 2024. 2, 3, 4, 6, 7, 8, 9, 10, 13 [24] M. Gui, J. S. Fischer, U. Prestel, P. Ma, D. Kotovenko, O. Grebenkova, S. A. Baumann, V. T. Hu, and B. Ommer, Depthfm: Fast monocular depth estimation with flow matching, arXiv preprint arXiv:2403.13788, 2024. 2, 3, 7 [25] J. He, H. Li, W. Yin, Y. Liang, L. Li, K. Zhou, H. Liu, B. Liu, and Y.-C. Chen, Lotus: Diffusion-based visual foundation model for high-quality dense prediction, arXiv preprint arXiv:2409.18124, 2024. 2, 3, 4, 7, 9,"
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009. 1 [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, NeurIPS, 2012. 1 [3] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in ICLR, 2015. [4] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016. 1 [5] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, How transferable are features in deep neural networks? NeurIPS, 2014. 1 [6] J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in CVPR, 2015. 1, [7] D. Eigen, C. Puhrsch, and R. Fergus, Depth map prediction from single image using multi-scale deep network, in NeurIPS, 2014. 1, 3, 6, 7 [8] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, NeurIPS, 2019. 2 [9] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, Zero-shot text-to-image generation, OpenAI, 2021. 2 [10] Y. Song and S. Ermon, Generative modeling by estimating gradients of the data distribution, NeurIPS, vol. 32, 2019. 2, 3 [11] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen, GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models, in ICML, 2022. [12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, Highresolution image synthesis with latent diffusion models, in CVPR, 2022. 2, 3, 4, 5, 6 [13] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., LAION-5B: An open large-scale dataset for training next generation image-text models, NeurIPS, 2022. 2, 3 [14] P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul, M. Davaadorj, D. Nair, S. Paul, W. Berman, Y. Xu, S. Liu, and T. Wolf, Diffusers: State-of-the-art diffusion models, GitHub repository, 2022. 2, 16 [15] T. Brooks, A. Holynski, and A. A. Efros, Instructpix2pix: Learning to follow image editing instructions, in CVPR, 2023. 2, 5 [16] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in ICCV, 2023. 2 [17] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, DreamBooth: Fine tuning text-to-image diffusion models for subjectdriven generation, in CVPR, 2023. 2, 3 [18] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and K. Schindler, Repurposing diffusion-based image generators for monocular depth estimation, in CVPR, 2024. 2, 3, 4, 7, 8, 9 [19] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv preprint arXiv:2406.09414, 2024. 2, 3, 7 [20] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, Vision meets robotics: The KITTI dataset, International Journal of Robotics Research, 2013. [21] P. K. Nathan Silberman, Derek Hoiem and R. Fergus, Indoor segmentation and support inference from RGBD images, in ECCV, 2012. 2, 6, 7, 10 [26] G. Xu, Y. Ge, M. Liu, C. Fan, K. Xie, Z. Zhao, H. Chen, and C. Shen, Diffusion models trained with large data are transferable visual models, arXiv preprint arXiv:2403.06090, 2024. 2, 3, 4, 7, 9, 10 stable [27] O. B. Bohan, autoencoder Tiny for diffusion, accessed 15.06.2024. https://hf.co/madebyollin/taesd, 2024, 2, 6, 7 last [28] B. K. Horn, Shape from shading: method for obtaining the shape of smooth opaque object from one view, 1970. 2, 4 [29] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, NeurIPS, 2020. 3, 5, 6 [30] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, in ICLR, 2021. 3, 6, 9, 10 [31] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, Consistency models, in ICML, 2023. 3, 13 [32] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, Score-based generative modeling through stochastic differential equations, in ICLR, 2021. [33] T. Wang, M. Kanakis, K. Schindler, L. Van Gool, and A. Obukhov, Breathing new life into 3d assets with generative repainting, in BMVC, 2023. 3 [34] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson et al., On the opportunities and risks of foundation models, arXiv preprint arXiv:2108.07258, 2022. 3 [35] H. Yao, R. Zhang, and C. Xu, Visual-language prompt tuning with knowledge-guided context optimization, in CVPR, 2023. 3 [36] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, and H. Li, Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners, in CVPR, 2023. 3 [37] H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola, Exploring visual prompts for adapting large-scale models, arXiv preprint arXiv:2203.17274, 2022. [38] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, CLIP-adapter: Better vision-language models with feature adapters, ICJV, 2023. 3 [39] R. Zhang, R. Fang, W. Zhang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li, Tip-adapter: Training-free CLIP-adapter for better vision-language modeling, arXiv:2111.03930, 2021. 3 [40] O. Pantazis, G. Brostow, K. Jones, and O. Mac Aodha, SVL-adapter: Self-supervised adapter for vision-language pretrained models, in BMVC, 2022. 3 [41] C. Zhou, C. C. Loy, and B. Dai, Extract free dense labels from CLIP, in ECCV, 2022. 3 [42] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, Unleashing text-to-image diffusion models for visual perception, in ICCV, 2023. [43] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, Deep ordinal regression network for monocular depth estimation, in CVPR, 2018. 3 [44] J. H. Lee, M.-K. Han, D. W. Ko, and I. H. Suh, From big to small: Multi-scale local planar guidance for monocular depth estimation, arXiv preprint arXiv:1907.10326, 2019. 3 [45] W. Yuan, X. Gu, Z. Dai, S. Zhu, and P. Tan, NeWCRFs: Neural window fully-connected CRFs for monocular depth estimation, in CVPR, 2022. 3 [46] V. Patil, C. Sakaridis, A. Liniger, and L. Van Gool, P3depth: Monocular depth estimation with piecewise planarity prior, in CVPR, 2022. 3 [47] C. Liu, S. Kumar, S. Gu, R. Timofte, and L. Van Gool, VA-DepthNet: variational approach to single image depth prediction, in ICLR, 2023. 3 [48] S. F. Bhat, I. Alhashim, and P. Wonka, AdaBins: Depth estimation using adaptive bins, in CVPR, 2021. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17 [49] Z. Li, X. Wang, X. Liu, and J. Jiang, Binsformer: Revisiting adaptive bins for monocular depth estimation, IEEE TIP, vol. 33, pp. 39643976, 2024. 3 [50] J. Ning, C. Li, Z. Zhang, C. Wang, Z. Geng, Q. Dai, K. He, and H. Hu, All in tokens: Unifying output space of visual tasks via soft token, in ICCV, 2023. 3 [51] G. Yang, H. Tang, M. Ding, N. Sebe, and E. Ricci, Transformer-based attention networks for continuous pixel-wise prediction, in ICCV, 2021. 3 [52] Z. Li, Z. Chen, X. Liu, and J. Jiang, Depthformer: Exploiting longrange correlation and local information for accurate monocular depth estimation, Machine Intelligence Research, pp. 118, 2023. 3 [53] S. Aich, J. M. U. Vianney, M. A. Islam, M. Kaur, and B. Liu, Bidirectional attention network for monocular depth estimation, in ICRA, 2021. [54] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. Muller, ZoeDepth: Zero-shot transfer by combining relative and metric depth, arXiv preprint arXiv:2302.12288, 2023. 3, 14, 15 [55] W. Yin, C. Zhang, H. Chen, Z. Cai, G. Yu, K. Wang, X. Chen, and C. Shen, Metric3D: Towards zero-shot metric 3d prediction from single image, in ICCV, 2023. 3, 7 [56] M. Hu, W. Yin, C. Zhang, Z. Cai, X. Long, H. Chen, K. Wang, G. Yu, C. Shen, and S. Shen, Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation, arXiv preprint arXiv:2404.15506, 2024. 3, 7, 9 [57] V. Guizilini, I. Vasiljevic, D. Chen, R. Ambrus, , and A. Gaidon, Towards zero-shot scale-aware monocular depth estimation, in ICCV, 2023. 3 [58] Z. Li and N. Snavely, MegaDepth: Learning single-view depth prediction from internet photos, in CVPR, 2018. 3 [59] W. Yin, X. Wang, C. Shen, Y. Liu, Z. Tian, S. Xu, C. Sun, and D. Renyin, Diversedepth: Affine-invariant depth prediction using diverse data, arXiv preprint arXiv:2002.00569, 2020. 3, [60] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer, IEEE PAMI, 2020. 3, 5, 7, 14, 15 [61] R. Ranftl, A. Bochkovskiy, and V. Koltun, Vision transformers for dense prediction, in ICCV, 2021. 3, 7 [62] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, in CVPR, 2024. 3, 7 [63] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., DINOv2: Learning robust visual features without supervision, Transactions on Machine Learning Research, 2024. 3, 4 [64] Y. Duan, X. Guo, and Z. Zhu, DiffusionDepth: Diffusion denoising approach for monocular depth estimation, in ECCV, 2024. 3 [65] S. Saxena, A. Kar, M. Norouzi, and D. J. Fleet, Monocular depth estimation using diffusion models, arXiv preprint arXiv:2302.14816, 2023. [66] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, in ICCV, 2021. 3, 4, 6, 8, 10, 11 [67] Y. Cabon, N. Murray, and M. Humenberger, Virtual KITTI 2, arXiv preprint arXiv:2001.10773, 2020. 3, 6, 8 [68] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, in ICLR, 2023. 3 [69] X. Fu, W. Yin, M. Hu, K. Wang, Y. Ma, P. Tan, S. Shen, D. Lin, and X. Long, Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image, in ECCV, 2024. 3, 7, 9, [70] X. Zhang, B. Ke, H. Riemenschneider, N. Metzger, A. Obukhov, M. Gross, K. Schindler, and C. Schroers, Betterdepth: Plug-and-play diffusion refiner for zero-shot monocular depth estimation, NeurIPS, 2024. 3, 14 [71] J. Gregorek and L. Nalpantidis, Steeredmarigold: Steering diffusion towards depth completion of largely incomplete depth maps, arXiv preprint arXiv:2409.10202, 2024. 3 [72] F. Tosi, P. Z. Ramirez, and M. Poggi, Diffusion models for monocular depth estimation: Overcoming challenging conditions, in ECCV, 2024. 3 [73] Y. Jia, L. Hoyer, S. Huang, T. Wang, L. V. Gool, K. Schindler, and A. Obukhov, Dginstyle: Domain-generalizable semantic segmentation with image diffusion models and stylized semantic control, in ECCV, 2024. 3, 4 [74] J. Shao, Y. Yang, H. Zhou, Y. Zhang, Y. Shen, M. Poggi, and Y. Liao, Learning temporally consistent video depth from video diffusion priors, arXiv preprint arXiv:2406.01493, 2024. 3 [75] W. Hu, X. Gao, X. Li, S. Zhao, X. Cun, Y. Zhang, L. Quan, and Y. Shan, Depthcrafter: Generating consistent long depth sequences for open-world videos, arXiv preprint arXiv:2409.02095, 2024. 3 [76] L. Ladicky, B. Zeisl, and M. Pollefeys, Discriminatively trained dense surface normal estimation, in ECCV, 2014. 3 [77] X. Wang, D. Fouhey, and A. Gupta, Designing deep networks for surface normal estimation, in CVPR, 2015. 3 [78] J. Huang, Y. Zhou, T. Funkhouser, and L. Guibas, Framenet: Learning local canonical frames of 3d surfaces from single rgb image, in ICCV, 2019. 3, 10 [79] D. Eigen and R. Fergus, Predicting depth, surface normals and semantic labels with common multi-scale convolutional architecture, in ICCV, 2015. 3 [80] G. Bae, I. Budvytis, and R. Cipolla, Estimating and exploiting the aleatoric uncertainty in surface normal estimation, in ICCV, 2021. 3 [81] O. F. Kar, T. Yeo, A. Atanov, and A. Zamir, 3d common corruptions and data augmentation, in CVPR, 2022. 3, 9 [82] G. Bae and A. J. Davison, Rethinking inductive biases for surface normal estimation, in CVPR, 2024. 3, 9, 10 [83] C. Ye, L. Qiu, X. Gu, Q. Zuo, Y. Wu, Z. Dong, L. Bo, Y. Xiu, and X. Han, Stablenormal: Reducing diffusion variance for stable and sharp normal, ACM Transactions on Graphics (TOG), 2024. 4, 9, 10 [84] H. Barrow, J. Tenenbaum, A. Hanson, and E. Riseman, Recovering intrinsic scene characteristics, Comput. vis. syst, vol. 2, no. 3-26, p. 2, 1978. 4 [85] Z. Li, M. Shafiei, R. Ramamoorthi, K. Sunkavalli, and M. Chandraker, Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from single image, in CVPR, 2020. 4 [86] Z. Wang, J. Philion, S. Fidler, and J. Kautz, Learning indoor inverse rendering with 3d spatially-varying lighting, in ICCV, 2021. 4 [87] Z. Li, J. Shi, S. Bi, R. Zhu, K. Sunkavalli, M. Haˇsan, Z. Xu, R. Ramamoorthi, and M. Chandraker, Physically-based editing of indoor scene lighting from single image, in ECCV, 2022. 4 [88] J. Luo, D. Ceylan, J. S. Yoon, N. Zhao, J. Philip, A. Fruhstuck, W. Li, C. Richardt, and T. Wang, Intrinsicdiffusion: Joint intrinsic layers from latent diffusion models, in ACM SIGGRAPH, 2024, pp. 111. 4 [89] R. Zhu, Z. Li, J. Matai, F. Porikli, and M. Chandraker, Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes, in CVPR, 2022. [90] C. Careaga and Y. Aksoy, Colorful diffuse intrinsic image decomposition in the wild, ACM Trans. Graph., vol. 43, no. 6, 2024. 4, 11, 12 [91] , Intrinsic image decomposition via ordinal shading, ACM Transactions on Graphics, vol. 43, no. 1, pp. 124, 2023. 4 [92] T. Karras, S. Laine, and T. Aila, style-based generator architecture for generative adversarial networks, in CVPR, 2019. 4 [93] A. Bhattad, D. McKee, D. Hoiem, and D. Forsyth, Stylegan knows normal, depth, albedo, and more, NeurIPS, 2024. 4 [94] X. Du, N. Kolkin, G. Shakhnarovich, and A. Bhattad, Generative models: What do they know? do they know things? lets find out! arXiv preprint arXiv:2311.17137, 2023. 4 [95] H.-Y. Lee, H.-Y. Tseng, and M.-H. Yang, Exploiting diffusion prior for generalizable dense prediction, in CVPR, 2024. 4 [96] P. Kocsis, V. Sitzmann, and M. Nießner, Intrinsic image diffusion for indoor single-view material estimation, in CVPR, 2024. 4, 11, 12, 16 [97] Z. Zeng, V. Deschaintre, I. Georgiev, Y. Hold-Geoffroy, Y. Hu, F. Luan, L.-Q. Yan, and M. Haˇsan, Rgb-x: Image decomposition and synthesis using material-and lighting-aware diffusion models, in SIGGRAPH conference, 2024. 4, 11, 12 [98] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021, pp. 87488763. [99] J. Zhu, F. Luan, Y. Huo, Z. Lin, Z. Zhong, D. Xi, R. Wang, H. Bao, J. Zheng, and R. Tang, Learning-based inverse rendering of complex indoor scenes with differentiable monte carlo raytracing, in SIGGRAPH Asia conference. ACM, 2022. 4, 10, 11 [100] S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, and Y. Aksoy, Boosting monocular depth estimation models to high-resolution via contentadaptive multi-resolution merging, in CVPR, 2021. 4, 14, 15 [101] Z. Li, S. F. Bhat, and P. Wonka, Patchfusion: An end-to-end tile-based framework for high-resolution monocular metric depth estimation, in CVPR, 2024. 4, 14, 15 [102] , Patchrefiner: Leveraging synthetic data for real-domain highresolution monocular metric depth estimation, in ECCV, 2024. 4 [103] O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dekel, Multidiffusion: fusing diffusion paths for controlled image generation, in ICML, 2023. 4, 14 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 18 [104] A. Bochkovskii, A. Delaunoy, H. Germain, M. Santos, Y. Zhou, S. R. Richter, and V. Koltun, Depth pro: Sharp monocular metric depth in less than second, arXiv, 2024. 4, 14, 15 [105] W. Wagner, A. Ullrich, V. Ducic, T. Melzer, and N. Studnicka, Gaussian decomposition and calibration of novel small-footprint full-waveform digitising airborne laser scanner, ISPRS journal of Photogrammetry and Remote Sensing, vol. 60, no. 2, pp. 100112, 2006. 5 [106] S. Huang, Z. Gojcic, Z. Wang, F. Williams, Y. Kasten, S. Fidler, K. Schindler, and O. Litany, Neural lidar fields for novel view synthesis, in ICCV, 2023. 5 [107] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans, in ICCV, 2021. 5, 7 [108] T. Salimans and J. Ho, Progressive distillation for fast sampling of diffusion models, in ICLR, 2022. 6 [109] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and C. Shen, Learning to recover 3d scene shape from single image, in CVPR, 2021. 7, 14, 15 [110] C. Zhang, W. Yin, B. Wang, G. Yu, B. Fu, and C. Shen, Hierarchical normalization for robust monocular depth estimation, NeurIPS, 2022. 7 [111] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the KITTI vision benchmark suite, in CVPR, 2012. 6 [112] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, ScanNet: Richly-annotated 3d reconstructions of indoor scenes, in CVPR, 2017. 6, 10 [113] T. Schops, J. L. Schonberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys, and A. Geiger, multi-view stereo benchmark with high-resolution images and multi-camera videos, in CVPR, 2017. 7 [114] I. Vasiljevic, N. Kolkin, S. Zhang, R. Luo, H. Wang, F. Z. Dai, A. F. Daniele, M. Mostajabi, S. Basart, M. R. Walter, and G. Shakhnarovich, DIODE: Dense Indoor and Outdoor DEpth Dataset, arXiv preprint arXiv:1908.00463, 2019. 7, [115] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, naturalistic open source movie for optical flow evaluation, in ECCV, 2012. 10 [116] T. Koch, L. Liebel, F. Fraundorfer, and M. Korner, Evaluation of cnnbased single-image depth estimation methods, in ECCV workshop, 2019. 10 [117] W. Chen, S. Qian, D. Fan, N. Kojima, M. Hamilton, and J. Deng, OASIS: large-scale dataset for single image 3d in the wild, in CVPR, 2020. 10 [118] A. Bansal, B. Russell, and A. Gupta, Marr Revisited: 2D-3D model alignment via surface normal prediction, in CVPR, 2016. 10 [119] D. F. Fouhey, A. Gupta, and M. Hebert, Data-driven 3D primitives for single image understanding, in ICCV, 2013. [120] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE TIP, vol. 13, no. 4, pp. 600612, 2004. 11 [121] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, The unreasonable effectiveness of deep features as perceptual metric, in CVPR, 2018. 11 [122] Y. Song and P. Dhariwal, Improved techniques for training consistency models, in ICLR, 2024. 13 [123] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2019. [124] D. Scharstein, H. Hirschmuller, Y. Kitajima, G. Krathwohl, N. Neˇsic, X. Wang, and P. Westling, High-resolution stereo datasets with subpixelaccurate ground truth, in GCPR. Springer, 2014. 14 [125] P. Z. Ramirez, F. Tosi, L. Di Stefano, R. Timofte, A. Costanzino, M. Poggi, S. Salti, S. Mattoccia, Y. Zhang, C. Wu et al., Ntire 2024 challenge on hr depth from images of specular and transparent surfaces, in CVPR, 2024. 14 [126] T. Koch, L. Liebel, F. Fraundorfer, and M. Korner, Evaluation of cnnbased single-image depth estimation methods, in ECCV workshop, 2018. 14 [127] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries, in WACV, 2019. 14 Bingxin Ke is PhD student at the Photogrammetry and Remote Sensing Lab of ETH Zurich. His interest lies in generalizable computer vision, especially for 3D vision problems. He earned his Bachelors degree in Geomatics Engineering at Wuhan University in 2020, and Masters degree in Geomatics at ETH Zurich in 2022. Kevin Qu is Master student at the Photogrammetry and Remote Sensing Lab of ETH Zurich. He obtained his Bachelors degree in Electrical and Computer Engineering at the Technical University of Munich in 2023, and is currently pursuing his Masters degree in Robotics, Systems and Control at ETH. His research interests include generative models, 3D vision and robotic perception. Tianfu Wang is PhD student at the Intelligent Sensing Lab of the University of Maryland, College Park. Tianfu completed his Masters degree in Computer Science at ETH Zurich, where he worked in the Photogrammetry and Remote Sensing Lab. Prior to that, Tianfu earned his Bachelors degree in Computer Science and Mathematics from Northwestern University. Tianfu is interested in computational imaging, generative models, and differentiable rendering. Nando Metzger is PhD student at the Photogrammetry and Remote Sensing Lab of ETH Zurich. He is interested in weakly supervised learning and superresolution techniques and their applications to monocular depth and remote sensing. He studied Geomatics at ETH Zurich, where he received his Bachelors degree in 2019 and his Masters degree in 2021. Shengyu Huang is PhD student at the Photogrammetry and Remote Sensing Lab of ETH Zurich. He is interested in 3D vision problems and applications of neural fields beyond conventional cameras. He earned Bachelors degree in Surveying and Mapping Engineering at Tongji University in 2018, later he obtained his Masters degree in Geomatics from ETH Zurich in 2020. Bo Li is student collaborator with the Photogrammetry and Remote Sensing Lab of ETH Zurich. His research interests include computer graphics, generative models and efficient GPU computation for large-scale AI workloads. He earned dual Bachelors degrees in Biological Science and Computer Science from Peking University and Masters degree in Computer Science at ETH Zurich. Anton Obukhov is an established researcher in the Photogrammetry and Remote Sensing Lab, broadly interested in computer vision and machine learning research. He joined the Computer Vision Laboratory at ETH Zurich in 2018 and received his PhD in 2022, supervised by Prof. Luc Van Gool and funded by the Toyota TRACE-Zurich project. He also holds Diploma in Computational Mathematics and Cybernetics from Moscow State University, obtained in 2008. Between these degrees, he spent decade working in the industry: he helped NVIDIA drive the adoption of NVIDIA CUDA technology in scientific computing and later helped Ubiquiti Networks build multiple video camera products with varying degree of artificial intelligence. Konrad Schindler (Senior Member, IEEE) received the Diplomingenieur (MTech) degree from the Vienna University of Technology, Vienna, Austria, in 1999, and the PhD from the Graz University of Technology, Graz, Austria, in 2003. He was photogrammetric engineer in the private industry and held researcher positions with the Graz University of Technology, Monash University, Melbourne, VIC, Australia, and ETH Zurich, Zurich, Switzerland. He was an assistant professor of image understanding with TU Darmstadt, Darmstadt, Germany, in 2009. Since 2010, he has been tenured professor of photogrammetry and remote sensing with ETH Zurich. His research interests include computer vision, photogrammetry, and remote sensing."
        }
    ],
    "affiliations": [
        "Photogrammetry and Remote Sensing Laboratory, ETH Zurich, Switzerland"
    ]
}