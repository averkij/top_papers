{
    "paper_title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
    "authors": [
        "Vitor Guizilini",
        "Muhammad Zubair Irshad",
        "Dian Chen",
        "Greg Shakhnarovich",
        "Rares Ambrus"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 4 0 8 8 1 . 1 0 5 2 : r Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion Vitor Guizilini1 Muhammad Zubair Irshad1 Dian Chen1 Greg Shakhnarovich2 Rares Ambrus Toyota Research Institute (TRI)1 Toyota Technological Institute at Chicago (TTIC)2 Figure 1. MVGD is state-of-the-art method that generates images and scale-consistent depth maps from novel viewpoints given an arbitrary number of posed input views. In the above, red cameras are used as conditioning to directly generate RGB-D predictions from green cameras. To highlight the multi-view consistency of our method, predicted colored pointclouds from all novel viewpoints are stacked together for visualization without any post-processing. More examples and videos can be found in https://mvgd.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-ofthe-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation. 1. Introduction We investigate the task of generalizable novel view synthesis and depth estimation from sparse posed images. This task has received much attention with the advent of Neural Radiance Fields (NeRFs) [65], 3D Gaussian Splatting (3DGS) [55] and other neural methods [36, 79, 86] for constructing parameterized 3D representation of the scene observed from different viewpoints. Novel views, and by extension depth maps observed from novel viewpoints, can be generated via volumetric rendering or rasterization from the 3D model, once it is constructed from available input views. Subsequent work introduced improvements including generalization to novel scenes [14, 15, 121], better sampleefficiency [17, 76, 104], reduced memory cost [38], and rendering speedups [38, 86]. Despite these advances, the ability to generalize to novel scenes remains limited, especially with increasing numbers of input views [14, 15]. We pursue compelling alternative to creating pathe direct rendering of rameterized 3D representation: novel views and depth maps as conditional generative task. Our work is inspired by light and depth field networks [36, 38, 79, 86] and recent developments on multiview diffusion [24, 43, 106, 111]. Diffusion-based methods for novel view synthesis have so far been limited to simpler settings [60, 61] or as source of 2D priors for downstream 3D reconstruction [24, 106]. This is largely due to key challenge: the difficulty, without an intermediate 3D scene representation, to impose multi-view consistency when generating novel views. Our proposed solution to this problem is to train single diffusion model for the joint task of novel view and depth synthesis, using direct conditioning on the input views and known input camera geometry via raymap conditioning. We show that this leads to the generation of scale-aware and multi-view consistent predictions without any intermediate 3D representation or special mechanisms. To train such model that learns to implicitly represent multi-view consistent 3D scenes from sparse observations, we curated more than 60M multi-view samples (see Tab. 1) from publicly available datasets, including real-world and synthetic images from widely diverse range of driving, indoors, and robotics scenarios. To properly leverage such diverse data, we introduce novel techniques for effective training in heterogeneous conditions. First, to handle possibly sparse real-world depth, we use an efficient Transformer architecture [49] to perform pixel-level diffusion, thus removing the need for dedicated auto-encoders [39]. Second, we use learnable task embeddings to determine which output should be generated from the same conditioned latent tokens, enabling unified training on datasets with or without depth labels. Lastly, to handle non-metric datasets, we propose novel scene scale normalization (SSN) procedure, to generate scale-aware and multi-view consistent depth maps. These strategies enable the effective scaling of our method, resulting in robust generalization across diverse scenarios. Our proposed Multi-View Geometric Diffusion (MVGD) framework outperforms existing baselines on several indomain and zero-shot benchmarks, establishing new stateof-the-art in novel view synthesis with varying amounts of conditioning views (2-9). To further showcase our implicit geometric reasoning, we also evaluate MVGD for the tasks of stereo and video depth estimation, reporting state of the art results on the ScanNet benchmark. We show that MVGD can process arbitrarily large numbers of conditioning views (100+), and propose an incremental conditioning strategy designed to further improve multi-view consistency over long sequences. Finally, we report promising scaling behavior in which increasing model complexity improves results, and present strategy for incremental fine-tuning from smaller models that cuts down on training time by up to 70%. In summary, our contributions are as follows: novel multi-task diffusion architecture (MVGD) capable of multi-view consistent image and depth synthesis from an arbitrary number of posed images (up to thousands) without an intermediate 3D representation. set of techniques designed to enable training on large-scale, heterogeneous dataset with over 60M multi-view samples from diverse visual domains, different levels of 3D supervision, varying numbers of views and cameras, and broad range of scene scales. strategy for the efficient training of larger MVGD models by incrementally fine-tuning smaller ones, with promising scaling behavior in terms of performance. The combined effect of these contributions allows us to establish new state of the art in generalizable novel view synthesis, as well as multi-view stereo and video depth estimation, on number of well established benchmarks. Dataset Syn. Dyn. Met. I/O V/M # Sequences # Samples VM ArgoVerse2 [105] BlendedMVG [117] CO3Dv2 [74] DROID [56] VM LSD [37] VM LyftL5 [42] MVImgNet [122] VM NuScenes [8] Taskonomy [123] HM3D [71] Hypersim [75] P. Domain [32, 35] IO VM RealEstate10k [125] RTMV [97] ScanNet [16] VM TartanAir [102] VM VKITTI2 [7] VM Waymo [93] WildRGBD [108] 1043 502 25, 243 76, 792 17, 647 394 194, 368 850 533 900 457 5, 449 3, 909, 297 115, 142 5, 088, 873 7, 340, 712 1, 057, 920 347, 508 5, 768, 120 204, 894 4, 584, 462 9, 531, 876 74, 619 555, 000 74, 532 10, 115, 793 286, 350 2, 477, 378 613, 274 38, 268 990, 340 8, 026, 495 1, 909 1, 513 369 45 1, 000 23, 049 Total 426, 595 61, 126, 321 Table 1. Datasets used to train MVGD. Syn indicates if the dataset is synthetic, Dyn. if it contains dynamic objects, Met. if depth labels are metric, I/O if cameras are looking inwards and/or outwards, and M/V if it contains multiple cameras and/or videos. 2. Related Work Generalizable Radiance Fields. Traditionally, radiance field representations [3, 48, 55, 65, 66] are trained at per-scene basis, incurring expensive optimization. Generalizable formulations [14, 47, 121] tackle the problem of few-shot view-synthesis, hence reducing their dependency on dense captures. These methods [47, 79, 79, 98, 98, 121] utilize local features as conditioners in feed-forward neural network for effective few-view synthesis. By leveraging vast collection of multi-view posed data from various scenes, they can learn transferable priors. However, performance declines when the input views fall outside the distribution. More recently, this idea has been extended to 3D Gaussian Splatting [14, 15] for real-time few-shot rendering. Alternatively, other methods rely on regularizing the scenes geometry [68, 88, 107], ensuring semantic consistency [50], or utilizing sparse depth-supervision [17, 28, 76] to improve rendering quality given few views. Generative Models for Novel View Synthesis. Generating novel views outside the distribution of inputs views requires extrapolating unseen parts of the scene. Existing works have utilized Generative Adversarial Networks (GANs) [10, 11, 18, 67, 83] for conditional or unconditional scene generation. Owing to the impressive success of Diffusion models [40] for text-to-image generation [72, 77], recent works have utilized Diffusion Models [12, 27, 52, 60, 61, 96, 120] for the task of novel view synthesis. Existing methods [60, 103] train the conditional diffusion model with relative poses as inputs and show great generalization on object-centric scenes. Follow-up works extended this idea to real-world scenes, achieving impressive zero-shot generalization. Notably, ZeroNVS [80] finetunes diffusion model on large-scale real-world scenes, focusing on single-frame reconstruction. Reconfusion [106] trains conditional diffusion model in combination with 2 Figure 2. Diagram of our proposed Multi-View Geometric Diffusion (MVGD) framework, at inference time. input images In cameras Cn are used for scene conditioning, and different camera Ct is selected for novel view and depth synthesis. with PixelNeRF [121] objective for more accurate conditioning. CAT3D [24] trains multi-view conditional diffusion model along with feed-forward 3D reconstruction pipeline to render 3D consistent novel views. Multi-View Depth Estimation. Earlier works [22, 25, 31, 34] concentrated on in-domain depth estimation, which overfits to the specific camera geometry and requires similar training and testing domains. Most recently, zero-shot monocular depth estimation [5, 21, 37, 73] has gained significant interest, where models are trained on large-scale mixture of datasets and demonstrate impressive generalization to out-of-domain inference without finetuning. Notably, [6, 21, 44, 73, 114, 115, 119] scale up their training to large-scale real or mix of real and synthetic datasets. Recent works have also used priors from 2D diffusion models as denoisers [23, 29, 53] for monocular depth estimation, as well as explored video [45, 63, 84] and multi-view settings [43, 54, 116]. Notably, very few works have focused on the task of novel depth synthesis [36], relying on intermediate 3D representations to generate scene geometry. 3. Multi-View Geometric Diffusion Given collection IC = {I, C}N n=1 of input images In RHW 3 and corresponding cameras Cn = {K, T} with intrinsics R33 and extrinsics R44, our goal is to generate predicted image ˆIt RHW 3 and depth map ˆDt RHW for novel target camera Ct. We use diffusion model fθ p(ˆIt, ˆDtCt, IC) to learn conditional distribution from which to sample novel target images and depth maps. diagram of our proposed Multi-View Geometric Diffusion (MVGD) architecture is shown in Figure 2, and below we describe each of its components in detail. 3.1. Preliminaries"
        },
        {
            "title": "3.1.1 Diffusion Overview",
            "content": "Diffusion models [13, 40, 87] operate by learning state transition function from noise tensor ϵ to sample x0 from learned data distribution. Such function is defined as: xt = αtx0 + 1 αtϵ (1) 3 s=1 (1 βs) and {βt}T where ϵ (0, I), αt = (cid:81)t t=1 is the variance schedule for process with steps. neural network ˆϵ = fθ(xt, t, c) is trained to estimate the noise ˆϵt added to sample x0 at timestep t, given conditioning variable used to control the generative process [19, 60, 77]. At inference time, novel x0 is reconstructed from normally-distributed variable xT (0, I) by iteratively applying the learned transition function fθ over steps. key choice when designing diffusion models is the architecture used to parameterize fθ. Initial implementations relied on convolutional architectures [19, 41, 77, 78], due to their simplicity and easiness to train. At high resolutions this becomes costly, which led to the popularization of latent diffusion, in which fθ is not trained in pixel space, but in lower-dimensional latent space of an image autoencoder [100]. Although more efficient, this approach has its drawbacks: the loss of fine-grained details due to compression, and the need for dense 2D grid-like inputs."
        },
        {
            "title": "3.1.2 RIN Architecture",
            "content": "Transformer-based architectures [69] have been explored as more scalable alternative that does not require structured grid-like inputs. However, the squared complexity of attention still poses challenge at higher resolutions. Hence, in this work we use Recurrent Interface Networks (RIN) [49] as more efficient Transformer-based architecture. The key idea is the separation of computation into input tokens RN and latent tokens RLD, where the former is obtained by tokenizing input data (and thus depends on the input size ), but is fixed dimension. At each RIN block, the latents are first cross-attended with the inputs X, followed by several self-attention layers on Z, and the resulting latents are cross-attended back with X. The fact that the bulk of our computation (i.e., selfattention) operates on fixed number of latent tokens, rather than on all input tokens, makes it affordable to learn fθ directly in pixel space. It also enables the use of significantly more conditioning views to generate scene tokens (as shown in the supplementary material). Moreover, we show that RIN latent tokens can be incrementally expanded to allow the training of larger models by fine-tuning smaller ones, with promising scaling behavior in terms of performance versus complexity (Sec. 4.5). 3.2. Scene Scale Normalization Scale estimation is an inherently ambiguous problem in 3D computer vision. Single-frame methods are unable to directly estimate scale, and must rely on other sources to disambiguate [30, 33, 37, 39, 70, 119]. Multi-frame methods must inherit the scale given by cross-camera extrinsics, to preserve multi-view geometry. This is particularly challenging when training across multiple datasets, that use different calibration procedures, such as metric scale from additional sensors (e.g., LiDAR, IMUs, or stereo pairs) or arbitrary scale from self-supervision (e.g., COLMAP [82]). Several works have explored ways to address this challenge, such as scale jittering to increase robustness [36], scale conditioning [12], or test-time multi-view alignment [14]. We propose simple technique designed to automatically extract scene scale from conditioning cameras and ground-truth depth, by normalizing the input to our diffusion process. This scene scale is injected back to generated predictions, resulting in multi-view consistent depth maps. First, all conditioning extrinsics Tn are expressed relative to the novel target extrinsics Tt, so that T1 = Tn . Note that this means that the novel normalized target camera Ct = {K, T}t is always positioned at the origin. This enforces translational and rotational invariance to scene-level coordinate changes, property that has been shown to improve multi-view depth estimation [36, 112]. (cid:105) 1 n=1, where tn (cid:104)R = 0 We define the scene scale as scalar representing the largest absolute camera translation in any spatial coordinate, = [x, y, z]T is }N i.e., = max{{x, y, z}n R33 the translation component of Tn , and Rn is its rotational component. We then divide all translation = [x/s, y/s, z/s]T . During vectors by this value, such that training, if the target depth map Dt is used as ground-truth, we also divide it by to maintain the scene geometry consistent across views, such that Dt = Dt/s. If max{ Dt} > dmax (the maximum value estimated by our model), we recalculate the scene scale as = Dmax/max{Dt} so the normalized ground-truth is within range, and use this new value to n=1. During inference, once ˆDt is generrecalculate {tn ated, we multiply it by to ensure consistency with conditioning cameras. In other words, generated depth maps will have the same scale as conditioning cameras. }N 3.3. Conditioning Embeddings Image Encoder. We use EfficientViT [9] to tokenize input conditioning views, providing visual scene information for novel generation. We start from pre-trained EfficientViT-SAM-L2 model taken from the official 4 4 448DI to produce image embeddings EI,n repository, and fine-tune it end-to-end during training. input image will result in FI 4 448 features. These features are flattened and processed by linear layer LI HW 16 DI . This process is repeated for each conditioning view, resulting in sets of image embeddings. Ray Encoder. We use Fourier encoding [36, 37, 65] to tokenize input cameras, parameterized as raymap containing origin tijk = [x, y, z]T and viewing direction rijk = (KkRk)1 [uij, vij, 1]T for each pixel pij from camera k. This information is used to (a) position features extracted from conditioning views in 3D space; and (b) determine novel viewpoints for image and depth synthesis. Conditioning cameras Cn are resized to match the resolution of image embeddings, and the target camera Ct is kept the same (note that tt is always at the origin, and Rt = I). Assuming No and Nr origin and ray frequencies, the resulting ray embeddings are of dimensionality DR = 3 (No + Nr + 1) . 3.4. Multi-Task Learning Differently from other methods [14, 15, 24, 106, 121], MVGD does not maintain an intermediate 3D representation. These methods rely on such representations to produce multi-view consistent predictions, by either conditioning NeRF on available images for novel rendering, or projecting Gaussians generated from available images onto novel viewpoints. Instead, we propose to generate novel renderings directly from our implicit model, which therefore must itself be multi-view consistent. We achieve that by jointly learning novel view and depth synthesis, i.e., by directly rendering depth maps from novel viewpoints alongside images. simple approach would be to generate RGB-D predictions as single task. However, that would limit our training datasets to only those with dense ground-truth depth. An alternative would be to condition the latent tokens themselves, by appending task-specific tokens. However, that would (a) make it impossible to jointly generate predictions for both tasks, and (b) promote the separation of appearance and geometric priors, by creating dedicated latent tokens portions of the latent space. Thus, we propose to use learnable task embeddings Etask RDtask to guide each individual generation towards specific task. The models predictions are parameterized as follows, depending on the task: RGB Parameterization. Our pixel-level diffusion does not require latent auto-encoders, and therefore ground-truth images are simply normalized to [1, 1] with PRGB = (I + 1) /2. Generated predictions are converted back to images using the inverse operation ˆI = 2 ˆPRGB + 1. Depth Parameterization. To preserve multi-view consistency, our generated depth predictions must be scale-aware, and thus should cover wide range of possible values. We assume dmin = 0.1 and dmax = 200, which makes MVGD suitable for indoor and outdoor scenarios. Note that these values are not metric, since they are considered after scene scale normalization. We use log-scale parameterization [39, 81] (Equation 2), and predictions are converted back using the inverse operation (Equation 3). (cid:18) (cid:18) (cid:19) PD = log dmin (cid:18) (cid:0)2 ˆPD + 1(cid:1) log ˆD = exp / log (cid:19)(cid:19) (cid:18) dmax dmin (cid:19)(cid:19) 1 (2) dmin (3) (cid:18) dmax dmin 3.5. Novel View and Depth Synthesis ER,n The steps described above produce two different sets of inputs: scene tokens, used to contextualize the diffusion process; and prediction tokens, used to guide the diffusion process towards generating the desired predictions. Scene tokens are obtained by first concatenating image and ray embeddings from each conditioning view, producing = EI,n En , and then concatenating embeddings from all conditioning views, producing Ec = E1 ... NHW EN 16 (DI +DR). To improve training efficiency, we randomly sample Ms scene tokens as conditioning. Prediction tokens are obtained by concatenating ray embeddings ER from the target camera with the desired task embeddings Etask and state embeddings Stask . The state embeddings contain the evolving state of the diffusion models predictions, and are defined as follows: Training: State embeddings St are generated by parameterizing an input image It or depth map Dt, and adding random noise determined by noise scheduler n(t), given randomly sampled timestep [1, ]. Our diffusion model is trained to learn the transition function fθ according to Equation 1. We use L2 and L1 losses to supervise image and depth generation, respectively. For depth estimation, prediction tokens are generated only for pixels with valid ground-truth. For both tasks, to improve efficiency, we randomly sample Mp prediction tokens. Inference: State embeddings ST (0, I) are sampled as 3-dimensional vectors for image or scalars for depth generation. They are iteratively denoised for steps using fθ with scheduler n(t). At = 0 state embeddings S0 will contain the parameterized prediction, that is converted back to ˆIt or ˆDt. To mitigate stochasticity, we perform test-time ensembling [39, 53] over = 5 samples. 3.6. Incremental Multi-View Generation Due to stochasticity in the diffusion process, generating multiple predictions from unobserved areas may lead to inconsistencies, even though each one is equally valid. To address this ambiguity, we propose to maintain history of generated images IG = {ˆI, C}G g=1 to use as additional conditioning for future generations. Although this approach creates additional scene tokes, the number of RIN latent tokens remain the same, which greatly increases efficiency. 5 In fact, we show in the supplementary material that MVGD can scale to thousands of views on single GPU, leading to further improvements without any additional information. 3.7. Fine-Tuning Larger Models The fixed dimensionality of our latent tokens enables efficient training and inference in terms of the number of input tokens X. However, this bottleneck representation may also hinder the quality of generated samples, especially when considering large-scale multi-task learning. If this hypothesis is correct, increasing the number of latents tokens should lead to improvements, albeit at the cost of slower training times. To avoid training larger models from scratch, we observe that introducing more latent tokens does not change our architecture at all, i.e., the cross-attention with inputs and the self-attention between latents remains the same. Thus, after training with specific number of latent tokens, we can simply duplicate and concatenate them, resulting in structurally similar representation with twice the capacity. This larger model can then be further optimized, and we empirically demonstrate (Sec. 4.5) that this incremental fine-tuning strategy leads to substantial improvements without the need to train new models from scratch. 4. Experiments 4.1. Architecture Details Our diffusion model is based on the RIN implementation from [101], containing 6 blocks with depth 4, 16 latent and read-write heads, and R2561024. Ray embeddings were calculated using No = Nr = 8 and maximum frequency 100. Task embeddings are of dimensionality Dtask = 128, and DI = 256. At training time, we randomly select Ms = 1024 scene tokens from each conditioning view, and Mt = 4096 prediction tokens. Prediction tokens are dedicated either for only image or depth generation, or split equally between both tasks. We select between 2 5 views for conditioning, and an additional view as target. We use DDIM [89] with 1000 training and 10 evaluation timesteps, sigmoid noise schedule, and EMA [57] with β = 0.999. Images are resized to longest side of 256. We trained MVGD from scratch, using AdamW [62] with β1 = 0.9, β2 = 0.99, batch size = 512, weight decay = 102, learning rate lr = 104, and warm-up scheduler [26] with linear increase for 10k steps followed by cosine decay, for total of 300k steps (until convergence). For each fine-tuning stage, we duplicate and train for 50k steps, using the same scheduler and half the learning rate. In total, training takes around 6k A100 GPU-hours. Code and pre-trained models will be made available upon publication. 4.2. Training Datasets Rather than relying on intermediate 3D representation to generate novel predictions, MVGD samples directly from Figure 3. MVGD novel view and depth synthesis results randomly sampled from different evaluation benchmarks and in-the-wild datasets. Top images are conditioning views (colored cameras), and bottom images are the target view (black camera), showing from leftto-right: ground-truth image, predicted image, and predicted depth map. These predictions are used to produce colored 3D pointcloud observed from the target view. For more examples and additional visualizations, please refer to the supplementary material. RealEstate10K [125] ACID [59] PSNR SSIM LPIPS PSNR SSIM LPIPS PixelNeRF [121] 20.43 24.11 GPNR [91] 24.78 AttnRend [20] 26.10 MuRF [110] 0.589 0.793 0.820 0.858 0.550 0.255 0.213 0. 20.97 25.28 26.88 28.09 0.547 0.764 0.799 0.841 0.533 0.332 0.218 0.155 PixelSplat [14] 25.89 0. 0.142 28.14 0.839 0.150 MVSplat [15] 26. 0.869 0.128 28.25 0.843 0."
        },
        {
            "title": "MVGD",
            "content": "28.41 0.891 0.107 29.98 0.875 0. Table 2. Novel view synthesis results with 2 input views. an implicit latent space. To promote the learning of the priors required to achieve this, we curated more than 60M samples from publicly available multi-view datasets, focusing on diversity both in terms of appearance (e.g., indoor, outdoors, driving, robotics, objects, synthetic) and geometry (e.g., intrinsics, resolution, scale). Table 1 shows list of the datasets used to train MVGD, and more information can be found in the supplementary material. For fair comparison with similar methods on in-domain benchmarks, wherever possible we use only samples from the official training splits. We also note that some datasets contain dynamic objects, which are currently not modeled by our framework. We decided to include these to further increase the diversity and zero-shot capabilities of MVGD, and observed nontrivial improvements across all benchmarks  (Table 5)  . 4.3. Novel View Synthesis We evaluated MVGD for the task of novel view synthesis with an arbitrary number of conditioning views, from 2 to 9. The 2-view setting is dominated by 3DGS [14, 15], in which predictions from available views are scale-aligned and rendered from novel viewpoints; whereas settings with more views are dominated by methods that first generate additional conditioning images with diffusion, and then use generalizable NeRF for novel view rendering [24, 106]. Table 2 reports 2-view quantitative results, showing that MVGD significantly outperforms the previous state of the art. We attribute this improvement to combination of (a) our proposed large-scale training procedure, that leads to improvements even on in-domain benchmarks; and (b) the implicit learning of novel view and depth synthesis from such diverse data, instead of relying on indirect renderings. We also note that these methods struggle with more than 2 views, since explicit geometric alignment between cameras is required. In contrast, MVGD generates images and depth maps from novel viewpoints without any additional steps. Therefore, we also evaluated the same MVGD model on benchmarks with larger number of input views. Table 3 reports these results, and once again we outperform strong baselines that also leverage large-scale pre-training in combination with diffusion models [24, 106]. Note that MVGD was trained using only 2-5 conditioning views, and still it was able to generalize to larger numbers (additional experiments are available in the supplementary material). For completeness, in Table 4 we report novel depth synthesis results compared to sparse COLMAP [82] ground-truth, showing similar trend of improvements with more conditioning views. Qualitative results can be found in Figure 3, including reconstructed pointclouds from novel viewpoints. 4.4. Multi-View Depth Estimation To further evaluate the implicit geometric priors learned by MVGD, we evaluated our model on two different multiview depth estimation benchmarks. The Stereo benchmark involves 2-view conditioning from overlapping image pairs, and the Video benchmark involves 10-view conditioning from image sequences. Note that in these benchmarks the target image is also used as conditioning, and the goal is to generate its depth map given all available views (a setting not considered during training). Results are reported in Tables 6 and 7, where MVGD once again outperforms the pre6 FreeNeRF [113] SimpleNeRF [88] ZeroNVS[80] ReconFusion [106] CAT3D [24] MVGD FreeNeRF [113] SimpleNeRF [88] ZeroNVS[80] ReconFusion [106] CAT3D [24] 3-view 20.54 23.89 19.11 25.84 26.78 28.70 19.63 19.24 15.91 21. 21.58 PSNR 6-view 25.63 28.75 22.54 29.99 31.07 31. 23.72 23.05 18.39 24.25 24.71 9-view 3-view 27.32 29.55 23.73 31.82 32. 32.89 25.12 23.98 18.79 25.21 25.63 0.731 0.839 0.675 0.910 0.917 0. 0.613 0.623 0.359 0.724 0.731 SSIM 6-view 0.817 0.896 0.744 0.951 0. 0.960 0.773 0.737 0.449 0.815 0.833 9-view 3-view 6-view 9-view LPIPS 0.843 0.900 0.766 0.961 0.963 0.969 0.820 0.762 0.470 0. 0.860 0.394 0.292 0.422 0.144 0.132 0.128 0.347 0.375 0.512 0.203 0. 0.344 0.239 0.374 0.103 0.092 0.090 0.232 0.296 0.438 0.152 0.121 0.332 0.236 0.358 0. 0.082 0.085 0.193 0.286 0.416 0.134 0.107 MVGD 22. 24.76 25.93 0.776 0.882 0.853 0. 0.131 0.103 FreeNeRF [113] SimpleNeRF [88] ZeroNVS[80] ReconFusion [106] CAT3D [24] MVGD FreeNeRF [113] SimpleNeRF [88] ZeroNVS[80] ReconFusion [106] CAT3D [24]"
        },
        {
            "title": "MVGD",
            "content": "] FreeNeRF [113] 4 [ SimpleNeRF [88] ZeroNVS[80] ReconFusion [106] CAT3D [24] 20.46 16.25 16.71 20.74 22.02 23.79 13.28 15.40 17.13 19. 20.57 20.68 12.87 13.27 14.44 15.50 16.62 23.48 20.60 17.70 23.62 24. 25.31 15.20 18.12 19.72 21.84 22.79 23.49 13.35 13.67 15.51 16.93 17. 25.56 22.75 17.92 24.62 25.92 26.88 17.35 20.52 20.50 22.95 23.58 24. 14.59 15.15 15.99 18.19 18.67 0.826 0.751 0.716 0.875 0.844 0.893 0.461 0.553 0.581 0. 0.666 0.678 0.260 0.283 0.316 0.358 0.377 0.870 0.828 0.737 0.904 0. 0.912 0.523 0.622 0.627 0.714 0.726 0.743 0.283 0.312 0.337 0.401 0. 0.902 0.856 0.745 0.921 0.928 0.939 0.575 0.672 0.640 0.736 0.752 0. 0.319 0.354 0.350 0.432 0.460 0.173 0.249 0.223 0.124 0.121 0.115 0.634 0.612 0.566 0. 0.351 0.331 0.715 0.741 0.680 0.585 0.515 0.131 0.190 0.205 0.105 0. 0.091 0.596 0.541 0.515 0.342 0.292 0.322 0.717 0.721 0.663 0.544 0. 0.102 0.176 0.200 0.094 0.073 0.088 0.561 0.493 0.500 0.318 0.273 0. 0.695 0.676 0.655 0.511 0."
        },
        {
            "title": "MVGD",
            "content": "18.74 20.28 21.18 0.425 0.463 0. 0.499 0.512 0.488 ] 5 2 1 [ 0 1 ] 4 6 [ L ] 1 5 [ ] 4 7 [ 3 0 6 3 N - Table 3. Novel view synthesis results with 3-9 conditioning views. indicate methods fine-tuned and reported by [106]. indicate the use of image diffusion models pre-trained with internal datasets. Colored numbers represent first , second , and third best ranked metrics. RealEstate10k and CO3D results are in-domain, and results on other benchmarks are zero-shot. Dataset LLFF MIPNeRF360 CO3D Split 3-view 6-view 9-view 3-view 6-view 9-view 3-view 6-view 9-view Abs.Rel. 0.105 0.098 0.093 0.143 0.124 0.109 0.101 0.086 0.079 RMSE 8.378 8.260 8.006 2.445 2.292 2. 4.654 4.436 4.195 δ1.25 0.865 0.879 0.891 0.842 0.866 0.883 0.874 0.898 0.914 Table 4. Novel depth synthesis results with 3-9 conditioning views, relative to COLMAP [82] ground-truth. No test-time alignment was used, which means that predicted depth maps share the same scale as the provided camera extrinsics. vious state of the art by large margin, even though it does not rely on geometry-preserving 3D augmentations [36], architectural equivariance [112], or TSDF volumes [92]. 4.5. Incremental Fine-Tuning In Table 8 we analyze the impact that our proposed incremental fine-tuning strategy has on novel view and depth synthesis. The first noticeable aspect is that, even though adding more latent tokens barely changes the number of parameters, it leads to substantial improvements across all benchmarks. We attribute this to the use of latent tokens in RE10K(2) DTU(3) CO3D(3) LLFF(3) ScanNet(2)"
        },
        {
            "title": "Variation",
            "content": "PSNR PSNR PSNR Abs.Rel PSNR Abs.Rel Abs.Rel 2048 (scratch) 22.09 20.22 16.25 0.120 19.11 0.148 0. RGB Depth SSN Dynamic 21.46 20.15 23.24 0.132 0.138 19.31 16.39 17.98 14.67 0.175 16.87 0.224 19.51 17.04 0.141 19.28 0. 19.01 0.089 0.329 0.087 MVGD (short) 24.44 20.16 17.98 0.127 20.19 0.131 0.081 + RGB (B) + Depth (C) 25.15 25.68 21.11 18.04 0.120 20.21 0.120 21.31 18.39 0.124 20.52 0.127 0.075 0.079 MVGD (full) 25.89 21.32 18.48 0.121 20.58 0. 0.075 Table 5. MVGD ablation analysis. Top results obtained with 200k steps and 256 latent tokens. Bottom results obtained by finetuning top models for 100k steps. the first place: even though it dramatically increases efficiency, computation at lower-dimensional space hinders prediction quality. This is especially important in novel view synthesis, that benefits from high frequency details. However, experiments with direct self-attention [69] were not possible due to memory constraints: 256 256 target image has 65536 prediction tokens, plus 4096 scene tokens per conditioning view. In contrast, our largest model has only 2048 latent tokens, regardless of resolution or number of inputs. Moreover, we observed sub-linear increase 7 Method ScanNet [16] SUN3D [109] RGB-D [90] Abs.Rel. RMSE δ1.25 Abs.Rel. RMSE δ1.25 Abs.Rel. RMSE δ1.25 DPSNet [46] NAS [58] IIB [118] DeFiNe [36] EPIO [112] GRIN [39] 0.126 0.107 0.116 0.093 0. 0.088 0.315 0.281 0.281 0.246 0.229 - - 0.908 0.911 0.923 0.224 0.925 0.147 0.127 0.099 0.095 0. 0.097 0.449 0.378 0.293 0.287 0.260 0.781 0.829 0.902 0.914 0.912 0.274 0.908 0.151 0.131 0.095 0.095 0. 0.092 0.695 0.619 0.550 0.539 0.433 0.804 0.857 0.907 0.909 0.912 0.512 0.919 MVGD 0.065 0.202 0.946 0.088 0.247 0. 0.074 0.405 0.938 Method Abs. Rel Sq. Rel RMSE DeMoN [99] MiDas-v2 [73] BA-Net [94] CVD [63] DeFiNe [36] DeepV2D [95] NeuralRecon [92] 0.231 0.208 0.091 0.073 0.059 0.057 0.047 0.520 0.318 0.058 0.037 0. 0.010 0.024 0.761 0.742 0.223 0.217 0.184 0.168 0.164 MVGD 0.041 0.007 0.143 Table 6. Stereo depth experiments, with 2 conditioning views. Results on ScanNet are in-domain, and results on SUN3D and RGB-D are zero-shot. Table 7. Video depth experiments on ScanNet (in-domain), with 10 conditioning views. s e # . P # RE10K (2-view) CO3Dv2 (3-view) MIPNeRF360 (3-view) LLFF (3-view) PSNR SSIM PSNR SSIM Abs.Rel. RMSE PSNR SSIM Abs.Rel. RMSE PSNR SSIM Abs.Rel. RMSE 256 417.9M 25.89 512 418.2M 26.33 1024 418.7M 27.73 2048 419.7M 28.41 0.841 0.859 0.870 0.891 18.48 19.56 20.08 20.68 0.567 0.590 0.622 0. 0.121 0.109 0.104 0.101 5.347 5.048 4.766 4.654 17.71 18.15 18.49 18.74 0.415 0.429 0.444 0.465 0.156 0.149 0.147 0.143 2.789 2.581 2.499 2. 20.58 21.14 21.85 22.39 0.706 0.738 0.754 0.776 0.119 0.111 0.109 0.105 8.807 8.686 8.547 8.378 Table 8. Incremental fine-tuning experiments. The first model (256) was trained from scratch for 300k steps, and each subsequent model was obtained by duplicating the latents of the previous one and fine-tuning for 50k steps. Even though the total number of learnable parameters barely changes (+0.5%), results improve by significant margin (up to +20%) across all benchmarks and tasks. in inference time: 256 model generates predictions in roughly 0.3s, while 2048 model (8) takes 1.1s (4). The second aspect is how efficiently smaller models can be improved with our proposed incremental fine-tuning strategy. Once the first model has been trained for 300k steps, each subsequent one is fine-tuned for only 50k steps. Similar to inference, we noticed that each training step of 2048 model is roughly 4 slower compared to 256 model, which means that training our largest model from scratch would take 3 longer compared to incremental fine-tuning. Moreover, we trained 2048 model from scratch for 300k steps, and achieved worse performance than 256 model (A on Table 5). This indicates that (a) training more complex models from scratch requires longer schedules, which further increases training time; and (b) our proposed strategy can leverage priors from smaller models and quickly adapt them to fit within more complex representation. 4.6. Ablation Study Here we ablate our main design choices, with results summarized in Table 5. For computational reasons, shorter schedule of 200k steps was used, achieving performance within 10% of the full schedule. First, we analyzed single vs. multi-task learning, by training models capable of only novel (B) view or (C) depth synthesis. As noted in [36], training depth-only model led to slight degradation, in addition to limiting training datasets to only those with valid depth labels. However, an image-only model performed much worse, as evidence that geometric reasoning is key to accurate novel view synthesis. This hypothesis is reinforced when (D) training multitask model without scene scale normalization (SSN), which led to significantly worse novel view and depth synthesis. Interestingly, novel view synthesis is worse without SSN than without multi-tasking, showing that inaccurate geometry is more detrimental than no geometry at all. We also validated the benefits of data diversity by training model (E) without dynamic datasets, which led to decrease of roughly 12% in the amount of training data. Even though MVGD does not model dynamics, it still benefits from additional training data even on in-domain benchmarks. In the supplementary material we provide examples that show signs of implicit dynamics modeling. To further evaluate the multi-task capabilities of MVGD, we experimented with the introduction of new tasks to trained model, by fine-tuning (F) our depth-only model to include novel view synthesis, and (G) vice-versa. The only added parameters were task embeddings for the new task, randomly initialized. This fine-tuning stage was performed for an 100k steps, reaching our proposed full schedule for 256 latents. Interestingly, the resulting model not only reached original task performance comparable to multitask model trained from scratch, but also on the added task, as indication that our learned multi-view priors can be repurposed for other tasks with minimal fine-tuning. 5. Conclusion We introduce MVGD, diffusion-based architecture for generalizable novel view and depth synthesis from an arbitrary number of posed images. We propose to generate novel images and depth maps by directly sampling from multi-view consistent and scale-aware implicit distribution, jointly trained for both tasks. To achieve this, we curated over 60M multi-view samples from publicly available datasets, and propose key technical modifications to the standard diffusion pipeline that enables efficient training in such diverse conditions. As result, MVGD achieves stateof-the-art results in multiple novel view synthesis benchmarks, as well as stereo and video depth estimation. We also design novel incremental fine-tuning strategy to increase model complexity without retraining from scratch, showing promising scaling behavior with larger models. 8 A. Datasets Preparation within cmin < tn Each dataset was downloaded from its official source, following standard procedures. Whenever possible, we used officially provided PyTorch dataloaders to access the relevant information (i.e., images, intrinsics, extrinsics, and depth maps), and wrote our own if not available. In all cases, data was mapped into common format to enable mixed-batch training across all data sources. This includes padding images and intrinsics of different resolutions, and using empty depth maps whenever this label is not available. We treat video and multi-view datasets equally, by positioning all available cameras from the same scene on global frame of reference, and considering all possible pairs as potential source of training data. To select valid pairs for our purposes, we utilized three criteria, described below: Camera center distance. Conditioning views must have camera center distance within tn tt < cmax, where is the cameras translation vector. In the case of dynamic datasets, we also apply the same constraint in temporal sense to mitigate the impact of moving objects, such that tmin < tn tt < tmax, where is the timestep of each camera within the sequence (fractional timesteps are used in the case of datasets with multiple asynchronized cameras). Assuming cM to be the maximum distance across any two cameras in sequence, we set cmin = 0.05 cM and cmax = 0.2 cM , and tmin = 8 and tmax = 8. Camera viewpoint angle. Conditioning views must have viewing direction with cosine similarity within αmin < vn vt vt < αmax, where = R1 [0, 0, 1]T is cos1 vn vector pointing forward (positive z) relative to worldto-camera rotation matrix R. In all experiments, we set αmin = 0 and αmax = π/2 for depth generation, to avoid supervision from sparse reconstructions, and αmin = 0 and αmax = π for image generation, to promote extrapolation to novel viewpoints. Pointcloud overlapping. Whenever depth maps are available, we set threshold pmin on the percentage of how many valid pixels of each conditioning view are projected onto the target view. For each pixel pn = {u, v} with depth from conditioning view In , we can obtain and depth its projection c with depth Dn onto the target view via: v 1 (cid:16) c Tn = (cid:17)1 1 (cid:16) Tt Kt (cid:17) (4) (cid:105) (cid:104)K 0 1 0 R44 is homogeneous intrinsics mawhere = trix. projected point is considered valid if [0, H] and [0, ], i.e. if its projected coordinates are inside the target view. We set pmin = 30%, and additionally discard samples with less than 64 valid projected pixels. These criteria were used as pre-processing step, to generate list of valid training samples. We will opensource dataloaders for all training and validation datasets, to facilitate the reproduction of our work, as well as the list of valid samples used in our experiments. We use Webdataset [1] to optimize storage and training efficiency. B. Additional Qualitative Examples In Figure 4 we provide additional qualitative results of MVGD in different evaluation benchmarks, as well as inthe-wild images from different sources (complementing Figure 3 of the main paper). Conditioning images are shown in the top left, with corresponding cameras (denoted by different colors) positioned relative to the target camera (denote by black). On the bottom, from left to right, we show: ground-truth image, predicted novel image, and predicted novel depth, all from the target viewpoint. We emphasize that novel images and depth maps are generated directly as an output of our diffusion model, rather than rendered from 3D neural field or set of 3D Gaussians. To highlight the multi-view consistency of MVGD, in Figure 5 we show qualitative results obtained using the same conditioning views to generate multiple predictions from novel viewpoints, and stacking the predicted colored pointclouds together without any post-processing. Each prediction is generated independently, by setting the novel viewpoint as the origin and positioning the conditioning views relative to it. Even so, they yielded highly consistent pointclouds, both in terms of appearance as well as reconstructed 3D geometry. We attribute this consistency (and ablate it in Table 5 of the main paper) to our proposed scene scale normalization (SSN) procedure, that promotes the generation of depth maps that share the same scale as the one provided by conditioning cameras, even in very different settings (e.g., driving, indoors, object-centric). C. Implicit Dynamics Modeling Although MVGD does not explicitly model dynamic objects, we elected to include datasets with such behavior to increase the diversity of our training data, and report nontrivial improvements relative to baseline that only considers static datasets (Table 8 of the main paper). We attribute this behavior to learned robustness to the presence of dynamic objects [124], similar to other methods that rely on self-supervised multi-view consistency with static environment assumption [25, 30, 85]. However, upon further inspection we observed some degree of implicit motion understanding in our learned representation. Examples are shown in Figure 6, using the DDAD [30] dataset. In those examples, every 10th frame from 100-frame sequence was used as conditioning, and Figure 4. Zero-Shot MVGD novel view and depth synthesis results randomly sampled from different evaluation benchmarks and in-thewild datasets. Top left images are conditioning views (colored cameras), and bottom images are the target view (black camera), showing from left-to-right: ground-truth image, predicted image, and predicted depth map. These predictions are used to produce colored 3D pointcloud observed from the target viewpoint. 10 Figure 5. Accumulated MVGD pointclouds, obtained by generating novel images and depth maps from various viewpoints (black cameras), using the same conditioning views (colored cameras), and stacking them together without any post-processing. Our zero-shot architecture is capable of directly generating multi-view consistent predictions that match the scale from conditioning cameras. 11 Figure 6. Accumulated MVGD pointclouds on dynamic dataset [30]. (left) Red cameras are used as conditioning views, and novel images and depth maps are generated from green cameras. (right) Colored pointclouds are calculated from these predictions and stacked together without any post-processing. Even though MVGD does not explicitly model dynamic objects, it implicitly learns how the scene should change when interpolating between views with objects in different locations (e.g., moving cars), while keeping the remainder static. remaining cameras were used to generate novel images and depth maps. As we can observe, moving cars are correctly rendered in different locations to ensure smooth transition between frames, while static portions of the environment are rendered in the same location, taking into consideration only camera motion. D. Incremental Conditioning Here we explore how MVGD scales in terms of the number of conditioning views. Due to the use of latent tokens, computational complexity is largely independent of the number of input tokens, which enables (a) pixel-level diffusion 12 Figure 7. MVGD per-frame novel view and depth synthesis results using different numbers of fixed conditioning views, on ScanNet (scene 0086 02, with 1267 images). The same model from all experiments reported in the main paper was used. Legend numbers indicate the stride (i.e., how many target images are positioned between each conditioning view). As expected, results consistently improve as more input information is available, eventually plateauing at around 100 conditioning views. without the need for dedicated auto-encoders; and (b) the simultaneous use of more conditioning views. In fact, one target 256 256 image generates 65536 prediction tokens, while each conditioning views adds only 4096 scene tokens, since image features are produced at 1/4 the original resolution. In contrast, our largest model has 2048 latent tokens, which is only 3% of the number of prediction tokens. In Figure 7 we show the impact of using more conditioning views over 1267-frame ScanNet sequence, in terms of novel view (PSNR) and depth (AbsRel) synthesis. We take every -th frame as conditioning views (given by the legend number), and generate predictions for all remaining frames, using the same model from all experiments reported in the main paper. As expected, results degrade in areas further away from available views, and consistently improve as more conditioning views are provided, eventually plateauing at around 100 (stride 20). Interestingly, independent experiments using subsets of the sequence (5 subsets of 250 frames) yielded worse results, as evidence that large-scale conditioning (i) does not degrade local performance; and (ii) provides better global context for local predictions. As mentioned in Section 3.6 of the main paper, we also take advantage of this highly efficient architecture to investigate how images generated from novel viewpoints can be added as additional conditioning views, thus increasing the amount of available information for future generations. This incremental conditioning strategy should further improve multi-view consistency in cases where model stochasticity becomes relevant, since each novel view is generated independently and thus might come from different parts of the underlying distribution, especially in unobserved areas. Figure 8 provides quantitative evaluation of our proposed 13 Figure 8. MVGD per-frame novel view and depth synthesis results with and without our proposed incremental conditioning strategy, on ScanNet (scene 0086 02, with 1267 images). The same model from all experiments reported in the main paper was used. The blue line indicates fixed number (25) of conditioning views, evenly spaced with stride of 50. The red line indicates our proposed incremental conditioning strategy, in which each new generation uses previously generated images as additional conditioning. This strategy leads to consistently better and more stable results in novel view and depth synthesis, especially in regions further away from conditioning views. incremental conditioning strategy in terms of novel view (PSNR) and depth (AbsRel) synthesis, compared to the use of fixed number of conditioning views. As we can observe, the introduction of additional conditioning from generated views consistently improves generation quality, In Figure 9 we qualitatively show incremental conditioning results on different sequences. Red cameras serve as initial conditioning, and novel images and depth maps are generated from green cameras. After each generation, the predicted image is used as additional conditioning. Since generation order matters in this setting, each new generation is performed on the green camera closest to the initial set of conditioning cameras, that still has not been pro14 cessed. Note that all previously generated views are used as additional conditioning, which in some scenarios could lead to thousands of images. Even so, we were able to generate novel predictions on single A100 GPU with 40GB. In terms of inference speed, generations with 25 conditioning views in this setting take 0.5s, and generations with 1250 (50) conditioning views take around 20s (40). Additional heuristics, such as using only generated views close to the target view, should lead to increased efficiency while still improving generation quality. (a) 149 frames, 5 initial conditioning views. (b) 202 frames, 7 initial conditioning views. (c) 252 frames, 5 initial conditioning views. (d) 337 frames, 6 initial conditioning views. (e) 1268 frames, 25 initial conditioning views. (f) 5578 frames, 60 initial conditioning views. Figure 9. MVGD novel view and depth synthesis results using our proposed incremental conditioning strategy. Red cameras indicate initial conditioning views, used to generate predictions for green cameras (ordered from closest to furthest away from the initial conditioning views). After each generation, the predicted image is added to the set of conditioning views for future generations. Even though MVGD was trained using only 2 5 conditioning views, it can directly scale to thousands on single GPU. E. Limitations limitation of our proposed Scene Scale Normalization (SSN) procedure is its inability to simultaneously generate predictions from multiple viewpoints, since the target camera is always assumed to be at the origin. In Section we describe an incremental conditioning strategy that mitigates stochasticity when generating predictions from unobserved regions, leading to multi-view consistency over very long sequences (2000+ frames). Another current limitation of MVGD is the lack of dynamics modeling. In Section we show some evidence of implicit modeling of mov15 ing objects, however the proper handling of dynamic scenes (e.g., via temporal embeddings and motion tokens, such as [2]) could lead to improvements and spatio-temporal control over novel view and depth synthesis. Moreover, we believe the lack of large-scale dynamic datasets with accurate camera information still constitutes challenge for the generation of such spatio-temporal implicit foundation model."
        },
        {
            "title": "References",
            "content": "[1] Webdataset. https://github.com/webdataset/ webdataset, 2024. 9 [2] Anonymous. STORM: Spatio-temporal reconstruction model for large-scale outdoor scenes. In Submitted to The Thirteenth International Conference on Learning Representations, 2024. under review. 16 [3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. ICCV, 2021. 2 [4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. [5] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth, 2023. 3 [6] Reiner Birkl, Diana Wofk, and Matthias Muller. Midas v3.1 model zoo for robust monocular relative depth estimation, 2023. 3 [7] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv:2001.10773, 2020. 2 [8] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: mulIn CVPR, 2020. timodal dataset for autonomous driving. 2 [9] Han Cai, Chuang Gan, Efficientvit: Enhanced linear attention for high-resolution arXiv preprint low-computation visual arXiv:2205.14756, 2022. 4 and Song Han. recognition. [10] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In arXiv, 2020. 2 [11] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In arXiv, 2021. 2 [12] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. GeNVS: Generative novel view synthesis with 3D-aware diffusion models. In arXiv, 2023. 2, 4 [13] Ziyi Chang, George Alex Koulieris, and Hubert P. H. Shum. On the design fundamentals of diffusion models: survey, 2023. 16 [14] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In arXiv, 2023. 1, 2, 4, 6 [15] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatarXiv preprint ting from sparse multi-view images. arXiv:2403.14627, 2024. 1, 2, 4, 6 [16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 2, 8 [17] Kangle Deng, Andrew Liu, Jun-Yan Zhu, , and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. arXiv:2107.02791, 2021. 1, 2 [18] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham Taylor, and Joshua Susskind. Unconstrained scene generation with locally conditioned radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1430414313, 2021. 2 [19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 3 [20] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline In Proceedings of the IEEE/CVF Conferstereo pairs. ence on Computer Vision and Pattern Recognition (CVPR), pages 49704980, 2023. 6 [21] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multitask mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1078610796, 2021. 3 [22] Jiading Fang, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Greg Shakhnarovich, Adrien Gaidon, and Matthew Walter. Self-supervised camera self-calibration from video. In IEEE International Conference on Robotics and Automation (ICRA), 2022. 3 [23] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. arxiv, 2024. 3 [24] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv, 2024. 1, 3, 4, 6, [25] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth prediction. In ICCV, 2019. 3, 9 [26] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2018. 5 [27] Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In International Conference on Machine Learning, 2023. 2 [28] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking for few-shot novel IEEE/CVF International Conference on view synthesis. Computer Vision (ICCV), 2023. 2 [29] Ming Gui, Johannes S, Fischer, Ulrich Prestel, Pingchuan Ma, Olga Grebenkova Dmytro Kotovenko, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Depthfm: Fast monocular depth estimation with flow matching, 2024. [30] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In CVPR, 2020. 4, 9, 12 [31] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien Gaidon. Semantically-guided representation learning for self-supervised monocular depth. In ICLR, 2020. 3 [32] Vitor Guizilini, Jie Li, Rares Ambrus, and Adrien Gaidon. Geometric unsupervised domain adaptation for semantic segmentation. In ICCV, 2021. 2 [33] Vitor Guizilini, Igor Vasiljevic, Rares Ambrus, Greg Shakhnarovich, and Adrien Gaidon. Full surround monodepth from multiple cameras. arXiv:2104.00152, 2021. 4 [34] Vitor Guizilini, Rares Ambrus, Dian Chen, Sergey Zakharov, and Adrien Gaidon. Multi-frame self-supervised In Proceedings of the Internadepth with transformers. tional Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 3 [35] Vitor Guizilini, Kuan-Hui Lee, Rares Ambrus, and Adrien Gaidon. Learning optical flow, depth, and scene flow without real-world labels. IEEE Robotics and Automation Letters, 2022. 2 [36] Vitor Guizilini, Igor Vasiljevic, Jiading Fang, Rares Ambrus, Greg Shakhnarovich, Matthew Walter, and Adrien Gaidon. Depth field networks for generalizable multi-view In Computer VisionECCV 2022: scene representation. 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXII, pages 245262. Springer, 2022. 1, 3, 4, 7, [37] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares Ambrus, and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 3, 4 [38] Vitor Guizilini, Igor Vasiljevic, Jiading Fang, Rares Ambrus, Sergey Zakharov, Vincent Sitzmann, and Adrien Gaidon. Delira: Self-supervised depth, light, and radiance In International Conference on Computer Vision fields. (ICCV), 2023. 1 [39] Vitor Guizilini, Pavel Tokmakov, Achal Dave, and Rares Ambrus. Grin: Zero-shot metric depth with pixel-level diffusion, 2024. 2, 4, 5, 8 [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [41] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):22492281, 2022. 3 [42] John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one In CoRL, hours: Self-driving motion prediction dataset. pages 409418, 2020. [43] Hanzhe Hu, Zhizhuo Zhou, Varun Jampani, and Shubham Tulsiani. Mvd-fusion: Single-view 3d via depth-consistent multi-view generation. In CVPR, 2024. 1, 3 [44] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. 3 [45] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 3 [46] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. Dpsnet: End-to-end deep plane sweep stereo. arXiv:1905.00538, 2019. 8 [47] Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira, and Rares Ambrus. Neo 360: Neural fields for sparse view synthesis of outdoor scenes. In Interntaional Conference on Computer Vision (ICCV), 2023. 2 [48] Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, and Rares Ambrus. Nerf-mae: Masked autoencoders for self-supervised 3d representation learning for neural radiance fields. In European Conference on Computer Vision (ECCV), 2024. 2 [49] Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In ICML, 2023. 2, 3 [50] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 58855894, 2021. 2 [51] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 406413. IEEE, 2014. 7 [52] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy Mitra. Holodiffusion: Training 3D diffusion model In Proceedings of the IEEE/CVF conusing 2D images. ference on computer vision and pattern recognition, 2023. 2 [53] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation, 2023. 3, 5 17 [54] Tong Ke, Tien Do, Khiem Vuong, Kourosh Sartipi, and Stergios Roumeliotis. Deep multi-view depth estimation with predicted uncertainty. arXiv:2011.09594, 2020. 3 [55] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. 1, [56] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martın-Martın, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. 2 [57] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv:1412.6980, 2014. 5 [58] Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. NorIn Proceedings of mal assisted stereo depth estimation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21892199, 2020. 8 [59] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 6 [60] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 1, 2, 3 [61] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 1, 2 [62] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 5 [63] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (TOG), 39(4):711, 2020. 3, 8 [64] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 7 [65] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), pages 405421, 2020. 1, 2, 4 [66] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. 2 [67] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [68] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. [69] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 3, 7 [70] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4 [71] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, Manolis Savva, Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 In Thirtylarge-scale 3d environments for embodied AI. fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 2 [72] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [73] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 3, 8 [74] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In International Conference on Computer Vision, 2021. 2, 18 [75] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In International Conference on Computer Vision (ICCV) 2021, 2021. 2 [76] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias Nießner. Dense depth priors for neural radiance fields from sparse input views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2 [77] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 [78] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In MICCAI. Springer, 2015. 3 [79] Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. CVPR, 2022. 1, 2 [80] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. ZeroNVS: Zero-shot 360-degree view synthesis from single real image. CVPR, 2024, 2023. 2, [81] Saurabh Saxena, Junhwa Hur, Charles Herrmann, Deqing Sun, and David J. Fleet. Zero-shot metric depth with fieldof-view conditioned diffusion model, 2023. 5 [82] Johannes Schonberger and Structure-from-motion revisited. 6, 7 Jan-Michael Frahm. In CVPR, 2016. 4, [83] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis with sparse voxel grids. In Advances in Neural Information Processing Systems (NeurIPS) (NeurIPS), 2022. 2 [84] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors, 2024. [85] Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang. Feature-metric loss for self-supervised learning of depth and egomotion. In ECCV, 2020. 9 [86] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In Proc. NeurIPS, 2021. 1 [87] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 3 [88] Nagabhushan Somraj, Adithyan Karanayil, and Rajiv Soundararajan. SimpleNeRF: Regularizing sparse input neural radiance fields with simpler solutions. GRAPH Asia, 2023. 2, 7 In SIG- [89] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020. 5 [90] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. benchmark for the evaluation of rgb-d slam systems. In Proc. of the International Conference on Intelligent Robot Systems (IROS), 2012. [91] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In European Conference on Computer Vision, pages 156174. Springer, 2022. 6 [92] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1559815607, 2021. 7, 8 [93] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 2 [94] Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. arXiv preprint arXiv:1806.04807, 2018. 8 [95] Zachary Teed and Jia Deng. Deepv2d: Video to depth with differentiable structure from motion. In ICLR, 2020. 8 [96] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fredo Durand, William T. Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In arXiv, 2023. 2 [97] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Charles Loop, Nathan Morrical, Koki Nagano, Towaki Takikawa, and Stan Birchfield. Rtmv: ray-traced multi-view synthetic dataset for novel view synthesis. IEEE/CVF European Conference on Computer Vision Workshop (Learn3DG ECCVW), 2022, 2022. 2 [98] Alex Trevithick and Bo Yang. Grf: Learning general radiance field for 3d representation and rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1518215192, 2021. [99] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 50385047, 2017. 8 [100] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 3 [101] Phil Wang. Recurrent interface network (pytorch), 2022. 5 [102] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and 19 [115] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. [116] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth inference for unstructured multiIn Proceedings of the European Conference view stereo. on Computer Vision (ECCV), pages 767783, 2018. 3 [117] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo netIn Proceedings of the IEEE/CVF Conference on works. Computer Vision and Pattern Recognition, pages 1790 1799, 2020. 2 [118] Wang Yifan, Carl Doersch, Relja Arandjelovic, Joao Carreira, and Andrew Zisserman. Input-level inductive biases for 3D reconstruction. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2022. 8 [119] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In ICCV, 2023. 3, 4 [120] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane Gu. Dreamsparse: Escaping from platos cave with 2d frozen diffusion model given sparse views. arXiv preprint arXiv:2306.03414, 2023. 2 [121] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 1, 2, 3, 4, 6 [122] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet: large-scale dataset of multi-view images. In CVPR, 2023. [123] Amir Zamir, Alexander Sax, , William Shen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018. 2 [124] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arxiv:2410.03825, 2024. 9 [125] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018. 2, 6, 7 Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, 2020. 2 [103] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models, 2022. 2 [104] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In ICCV, 2021. [105] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021), 2021. 2 [106] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors. arXiv, 2023. 1, 2, 4, 6, 7 [107] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models. In CVPR, 2023. 2 [108] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos, 2024. 2 [109] Jianxiong Xiao, Andrew Owens, and Antonio Torralba. Sun3d: database of big spaces reconstructed using sfm and object labels. In 2013 IEEE International Conference on Computer Vision, pages 16251632, 2013. 8 [110] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: Multi-baseline radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2004120050, 2024. 6 [111] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multiview diffusion using 3d large reconstruction model, 2023. [112] Yinshuang Xu, Dian Chen, Katherine Liu, Sergey Zakharov, Rares Andrei Ambrus, Kostas Daniilidis, and Vitor Campagnolo Guizilini. $SE(3)$ equivariant ray embeddings for implicit multi-view depth estimation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 4, 7, 8 [113] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. 7 [114] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024."
        }
    ],
    "affiliations": [
        "Toyota Research Institute (TRI)",
        "Toyota Technological Institute at Chicago (TTIC)"
    ]
}