{
    "paper_title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
    "authors": [
        "Guozhao Mo",
        "Wenliang Zhong",
        "Jiawei Chen",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench."
        },
        {
            "title": "Start",
            "content": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools? Mo Guozhao1,2, Wenliang Zhong1,2, Jiawei Chen1,2, Xuanang Chen1, Yaojie Lu1, Hongyu Lin1, Ben He1,2, Xianpei Han1, Le Sun1 1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2University of Chinese Academy of Sciences {moguozhao2024,zhongwenliang2024,chenjiawei2024,xuanang2020,luyaojie,hongyu,xianpei,sunle}@iscas.ac.cn benhe@ucas.ac.cn 5 2 0 2 3 ] . [ 1 0 8 7 1 0 . 8 0 5 2 : r Abstract With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to singleserver settings with only few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (ClaudeSonnet-4) reaching 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBenchs complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench. Introduction Recent years have witnessed remarkable progress in tool-use agents powered by large language models (LLMs), demonstrating promising potential as pathway towards artificial general intelligence (Qu et al. 2025; Wang et al. 2024). As model capabilities advance and application scenarios expand, enabling LLMs to effectively invoke external tools has emerged as critical research direction for enhancing their generalization and real-world task execution abilities. Notably, with the widespread adoption of the Model Context Protocol (Anthropic 2024, MCP), an increasing number of real-world tools now expose their functionalities through standardized contextual interfaces, forming vast ecosystem encompassing more than 10,000 MCP servers (Hou et al. 2025). Concurrently, certain pretrained models have (a) Tool-Use Benchmarks: Simple Simulator. (b) MCP Benchmarks: Small-scale MCP servers. (c) LiveMCPBench: Real & Large-scale MCP servers. Figure 1: Comparison between LiveMCPBench and existing Tool-Use and MCP benchmarks: We focus on real-world task resolution using large-scale MCP toolset. begun to directly learn interaction patterns with MCP (Qwen 2025), further accelerating the evolution of tool-use agents. However, existing tool-use benchmarks predominantly rely on simulate API interfaces (see Figure 1), which suffer from fundamental limitation. API interfaces exhibit high instabilityFor instance, 55.6% of APIs in ToolBench (Qin et al. 2024) have become unavailable (Guo et al. 2024), forcing evaluation frameworks (e.g., API-Bank (Li et al. 2023)) to resort to simplified simulated tools, significantly compromising task authenticity and challenge. And the emergence of MCP provides stable tool call interface. For the few MCP benchmarks (e.g., MCPBench (Luo et al. 2025) and MCPEval (Liu et al. 2025)), their experimental scales remain severely limited, typically involving only small number of MCP servers (about 10), failing to reflect agents generalization and decision-making capabilities in large-scale toolset. Consequently, two pivotal questions remain underexFigure 2: LiveMCPBench comprises four parts: (1) Diverse Daily Task, collection of everyday tasks; (2) LiveMCPTool, large-scale, out-of-the-box MCP toolset; (3) MCP Copilot Agent, ReACT-based agent system capable of retrieval and multiturn tool invocation across the MCP toolset; (4) LiveMCPEval, an automated LLM-as-a-Judge evaluation system for accurately assessing online, time-varying tasks. plored: (1) How can optimal planning and retrieval be achieved in large-scale MCP toolset to accomplish realworld tasks? (2) Can an LLM-driven agent, trained for tool invocation, exhibit meta-tool-learning capabilitiesi.e., autonomously explore and compose tools from real-world toolset to complete tasks? To address these questions, we present LiveMCPBench, multi-domain, multi-step MCP tool-use benchmark. LiveMCPBench focuses on practical, everyday tasks across six domains, comprising 95 high-quality tasks solvable via MCP tools. To lower the deployment barrier and avoid complex multi-API-key dependencies, we introduce LiveMCPTool, ready-to-use, fully functional MCP toolset encompassing 70 MCP servers and 527 tools, significantly improving reproducibility and reducing research overhead. Furthermore, evaluating agent systems in real-world MCP environments presents two unique challenges: (1) Task dynamism (e.g., time-sensitive answers for news summarization) and (2) Solution diversity, multiple tool combinations can solve the same task. Consequently, traditional metrics based on tool-matching accuracy in API-Bank become inadequate. We thus design LiveMCPEval, an LLM-as-a-Judge (Zheng et al. 2023) evaluation system capable of automatically assessing multi-turn tool invocation trajectories. Finally, the server-tool architecture of MCP fundamentally differs from traditional parallel API structures, rendering existing tool-use agent methods inapplicable in MCP environments. To evaluate planning and retrieval strategies in large-scale MCP toolset and verify whether LLMs possess meta-tool-learning capabilities, we propose MCP Copilot Agentan agent integrating reasoning and acting (Yao et al. 2023, ReACT) strategy with tool retrieval and multistep invocation abilities, capable of dynamically responding to environmental changes. Our evaluation of 10 frontier models reveals that the Claude-Sonnet-4 agent achieves 78.95% task success rate, demonstrating strong meta-toollearning capabilities. We further conduct human annotation on the agent trajectory and measure the human agreement rate across different evaluator models, with DeepSeek-V3 achieving an agreement rate of 81.05%, validating the reliability of our evaluation method. In summary, our key contributions are: We propose LiveMCPBench, the first evaluation framework leveraging large-scale MCP toolset for everyday tasks. We evaluate the frontier models using LiveMCPBench, revealing their limitations in meta-tool-learning. We analyze the agent trajectories, identifying key bottlenecks in multi-tool collaboration and providing insights for future improvements."
        },
        {
            "title": "LiveMCPBench",
            "content": "We present LiveMCPBench (see Figure 2), novel benchmark designed to evaluate the capability of agent systems in retrieving appropriate tools from large-scale MCP toolset to accomplish general-purpose everyday tasks. The construction of such benchmark necessitates addressing three fundamental challenges: Tools Tasks"
        },
        {
            "title": "Type",
            "content": "Stable Servers Tools Plug & Play Type Time-Varying Evaluation MCP MCPBench (Luo et al. 2025) MCP MCP-RADAR (Gao et al. 2025) MCP-Zero (Fei, Zheng, and Feng 2025) MCP MCP MCPEval (Liu et al. 2025) LiveMCPBench (ours) MCP"
        },
        {
            "title": "MCP Benchmarks",
            "content": "(cid:33) (cid:33) (cid:33) (cid:33) (cid:33) 10 9 308 12 70 10 42 2,797"
        },
        {
            "title": "Tool Benchmarks",
            "content": "ToolAlpaca (Tang et al. 2023) ToolBench (Qin et al. 2024) API-Bank (Li et al. 2023) Simulate (cid:33) (cid:37) Simulate (cid:33) 1,000"
        },
        {
            "title": "API",
            "content": "426 16,464 2,138 (cid:37) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) Real Real - Syn. Real Syn. Syn. Real (cid:37) (cid:37) - (cid:33) (cid:33) (cid:37) (cid:33) (cid:37) Rule Rule - LLM LLM"
        },
        {
            "title": "Rule\nLLM\nRule",
            "content": "Table 1: Comparison with existing MCP and Tool Benchmarks. Plug & Play indicates whether the toolset can be used directly without additional API keys. Time-Varying denotes whether the task output changes over time. Note that MCP-Zero is not benchmark, but contains toolset. And, it is not directly usable, as many of its tools require API keys. Syn. means synthetic. How to construct representative daily tasks requiring multi-step tool use? pipeline yielded 95 high-fidelity daily tasks (see Appendix for annotation principles and distribution statistics). How to collect large, redundant yet functionally complete MCP toolset? How to automatically evaluate performance on evolving online tasks? In this section, we first elaborate on our task construction process. Then, we outline the collection process of the LiveMCPTool. Next, we introduce LiveMCPEval, an LLM-as-a-Judge evaluation framework designed for robust and scalable assessment. Finally, we describe MCP Copilot Agent as our baseline approach. Task Construction To advance the development of practical agents for realworld applications, we create diverse set of tasks grounded in everyday scenarios, spanning six key domains: Office (e.g., spreadsheet analysis), Lifestyle (e.g., news retrieval), Leisure (e.g., video game inquiries), Finance (e.g., stock price monitoring), Travel (e.g., ticket search), and Shopping (e.g., product recommendations). These scenarios were carefully selected to embody three critical characteristics. (1) Time-varying: Tasks exhibit time-sensitive outcomes; (2) Long-horizon: Tasks require multiple tools to complete; (3) Genuine utility: Tasks address authentic user needs. The annotation process employed rigorous two-stage methodology involving two groups of computer science students serving as task proposers and validators. Proposers first generated scenario-specific tasks based on personal experience, with LLM-assisted ideation permitted but strictly vetted for authenticity. Each proposer then interacted with our toolset to complete their proposed task, meticulously annotating key points to preserve the tasks compositional depth. Validators subsequently scrutinized both the task design and corresponding toolchain invocations, eliminating duplicates while enforcing quality standards. This iterative LiveMCPTool Collection While prior study (Hou et al. 2025) suggests the existence of over 10,000 MCP servers, curating practical and accessible toolset remains nontrivial due to critical usability constraints. The predominant challenge stems from dependency fragmentation: the majority of MCP servers necessitate proprietary API keys or integrations with third-party services, rendering them impractical for standardized toolset. To address this, we introduce rigorously validated methodology for constructing high-quality, dependency-free MCP toolsetprioritizing reproducibility and broad applicability. Our approach first aggregates 5,588 server configurations from mcp.so, then systematically filters out key-dependent servers to eliminate access barriers. Beyond accessibility, we ensure the toolsets representativeness through structured curation and expert annotation. Tools are taxonomically organized into five functional categories (Discovery, Visualization, File Access, Location, and Miscellaneous), followed by manual vetting to exclude lowquality implementations. This two-stage pipeline yields 70 MCP servers providing 527 tools, each verified for standalone functionality and categorical relevance. By decoupling the toolset from external dependencies, our collection establishes the first reproducible toolset for large-scale MCP performance analysis (see Appendix for distribution details). LiveMCPEval Automated evaluation of trajectories generated by the agent is essential for benchmarking task performance. However, achieving robust automated evaluation remains challenging due to several factors: (1) Time-varying nature of daily tasks, (2) Inherent instability of MCP tool outputs caused by its"
        },
        {
            "title": "Finance",
            "content": "Shopping Overall (%) Claude-Sonnet-4-20250514 Claude-Opus-4-20250514 DeepSeek-R1-0528 Qwen3-235B-A22B GPT-4.1-Mini Qwen2.5-72B-Instruct DeepSeek-V3-0324 Gemini-2.5-Pro GPT-4.1 Qwen3-32B 90.32 80.65 41.94 54.84 45.16 35.48 41.94 48.39 51.61 29.03 64.29 64.29 50.00 35.71 50.00 35.71 42.86 28.57 28.57 14.29 75.00 66.67 58.33 41.67 50.00 50.00 50.00 16.67 25.00 25.00 80.00 86.67 46.67 53.33 46.67 40.00 40.00 60.00 46.67 53. 78.57 64.29 50.00 50.00 42.86 57.14 28.57 57.14 35.71 28.57 66.67 33.33 55.56 44.44 22.22 55.56 55.56 11.11 22.22 33.33 78.95 70.53 48.42 48.42 44.21 43.16 42.11 41.05 38.95 30.53 Table 2: Task success rate results for the frontier models. Evaluation using Deepseek-V3. online dynamics, and (3) Diversity of trajectories resulting from different tool combinations that can accomplish the same task. To address these challenges, we employ an LLM-asa-Judge system, leveraging the adaptability of LLMs to dynamically assess task completion based on tool usage patterns and feedback. While dynamic tasks may exhibit variability, they often share set of critical subtasks or key points that must be fulfilled. Incorporating these key pointswhether manually annotated or automatically extracted by LLMimproves the accuracy of the LLM-as-aJudge system. In our framework, all tasks are annotated with verified set of key points to ensure reliable evaluation. Specifically, given task , set of key points , agents execution trajectory with retrieval and tool-call sequences and descriptions of all tools used D, the evaluator performs binary classification to determine the outcome as either Success or Failure: = Evaluator(T, P, A, D) (1) Therefore, we use the success rate as the primary metric. Finally, we systematically compare LiveMCPBench with existing benchmarks in Table 1, highlighting their fundamental differences. MCP Copilot Agent Due to the dynamic nature of daily tasks and the inherent uncertainty in retrieval systems, fixed pipeline cannot be effectively employed for tool retrieval and invocation in LiveMCPTool. Instead, we require agents to dynamically adapt to environmental changes. To model this dynamic tool retrieval and invocation process, we formulate it as Partially Observable Markov Decision Process (Silver and Veness 2010, POMDP), as the agent can only make decisions based on the textual descriptions of retrieved tools and feedback from tool execution. We characterize the toolset environment using the following components: (1) Hidden state space S; (2) Observation space containing the descriptions of retrieved tools and feedback from tools; (3) Language action space A, including three key actionsroute, execute, and response along with their associated descriptions; (4) State transition : St St+1; (5) Terminal reward : quantifying task completion. Our agent implementation is based on the ReACT framework. For the route tool, we adopt retrieval strategy inspired by MCP-Zero (Fei, Zheng, and Feng 2025), where tool prioritization is determined by weighted combination of server description similarity and tool description similarity."
        },
        {
            "title": "Experiments and Results",
            "content": "Setup We evaluate 10 frontier models: Claude-Opus-4 and ClaudeSonnet-4 (Anthropic 2025), GPT-4.1 and GPT-4.1-Mini (Openai 2025), Gemini-2.5-Pro (Google 2025), DeepseekV3 and Deepseek-R1 (DeepSeek-AI 2025), Qwen3-235BA22B and Qwen3-32B (Qwen 2025), and Qwen2.5-72BInstruct (Qwen et al. 2025). For assessment, we employ Deepseek-V3 as our primary evaluation model. Detailed implementation and computational resources are documented in Appendix to ensure reproducibility. Main Results We show the task success rates for different models in Table 2. We can see that: 1. Meta-Tool-Learning Capabilities in Claude Models. The Claude series demonstrates remarkable meta-toollearning proficiency, with Claude-Sonnet-4 and ClaudeOpus-4 achieving success rates of 78.95% and 70.53% respectively. These results indicate their superior ability to effectively explore and combine tools from largescale toolset to accomplish complex real-world tasks. 2. Performance Variance Among Models. We observe substantial performance gap across frontier models. While most contemporary models achieve only 30% 50% task success rates, the Claude series shows significantly superior performance. This performance gap suggests fundamental limitations in the meta-tool-learning capabilities of other models. 3. Domain-Specific Superiority of Claude Models. The Claude series exhibits particularly dominant performance in Office and Lifestyle scenarios, outperforming other models by more than 30%. This substantial advantage highlights Claude models unique strengths and adaptability in these specific domains. Figure 3: Correlation between model evaluation performance and human agreement rates across multiple evaluators: The analysis of annotated agent trajectories from Claude-Opus-4 and Claude-Sonnet-4 (color indicates evaluator variant). To validate the reliability of LiveMCPEvals automatic evaluation, we conducted human annotation of the execution trajectories for the top-performing models (Claude-Sonnet4 and Claude-Opus-4). We systematically tested all models used in the baselines and calculated human agreement rates, with results presented in Figure 3. 1. LiveMCPEval demonstrates high accuracy under appropriate model conditions. Our experiments show that Deepseek-V3 achieves an average human agreement rate of 78.95%, validating the reliability of our evaluation framework. Additionally, GPT-4.1 Mini and Qwen2.572B-Instruct exhibit comparable performance, with human agreement rates around 75%, making them viable alternative models for accurate assessment. 2. Certain models prove less suitable for evaluation tasks. Notably, advanced reasoning models such as Deepseek-R1, Claude-Opus-4, and Qwen3-32B exhibit lower human agreement rates (60%-70%). We hypothesize that this limitation stems from their reduced ability to process long trajectory inputs. To investigate the generalizability of LiveMCPEval, we conducted experiments on Claude-Sonnet-4s trajectories by evaluating how LLM-generated key points affect human agreement rates, with results visualized in Figure 4 (Detaied examples in Appendix F). Our key findings reveal: Figure 4: Comparison of human agreement rates across different models when evaluated using human-annotated versus LLM-generated key points. 1. Generalizability of LiveMCPEval. The framework demonstrates strong generalizability across models. Notably, even without human-annotated references, the majority of evaluated models achieved improved human agreement rates through automatically generated key points. 2. Optimal Utilization of Human-Annotated Key Points. Deepseek-V3 exhibits superior capability in leveraging human-annotated key points compared to other models. This observation provides important implications for model selection in scenarios where human-annotated references are available."
        },
        {
            "title": "Analysis",
            "content": "Efficiency Analysis To compare the behavioral characteristics of different models, we present the average number of dialogue turns, used tools, tool execution attempts, and retrieval calls in Table 3. Based on these metrics, we draw the following conclusions: 1. Claude series models exhibit more proactive exploration and utilization behavior. Their retrieval and execution frequencies are significantly higher than other models, accompanied by greater number of used tools. This suggests that Claude models actively engage with and adapt to the tool-augmented environment, demonstrating stronger tendency to explore and exploit available tools."
        },
        {
            "title": "Model",
            "content": "Steps Tools excute route Overall (%) Claude-Sonnet-4 20.09 2.71 25.53 3.40 Claude-Opus-4 16.76 1.59 Qwen3-235B 10.33 1.24 DeepSeek-R1 10.89 1.37 GPT-4.1-Mini 11.22 1.31 Qwen2.5-72B 1.01 8.33 DeepSeek-V3 0.99 8.08 Gemini-2.5-Pro 1.31 9.03 GPT-4.1 1.16 9.99 Qwen3-32B 5.59 6.93 5.12 2.11 2.71 2.80 1.29 1.46 1.72 2.31 2.98 4.35 1.77 2.00 1.65 1.38 1.41 1.35 1.64 1.19 78.95 70.53 48.42 48.42 44.21 43.16 42.11 41.05 38.95 30.53 Table 3: Performance efficiency metrics: Steps denotes average dialogue turns, Tools indicates average used tools, execute represents average tool executions, route refers to average retrievals, and Overall shows average task success rate. Figure 5: Log-Price vs. Performance scatter plot with pareto frontier representation. Different colors represent different model families. 2. Most models suffer from severe underutilization of tools. The average number of tools used by these models remains close to 1, indicating that once model identifies and adopts single tool, it tends to rely exclusively on it while neglecting other available tools. This behavior highlights critical limitation in their ability to dynamically leverage multiple tools during task execution. In practical applications, trade-off between model performance and cost must be carefully considered. To provide actionable insights for model selection, we plotted the relationship between logarithmic cost and performance, along with the corresponding pareto frontier (Lotov, Bushenkov, and Kamenev 2004). As illustrated in Figure 5, our analysis reveals two key findings: 1. Near-Linear Trade-off on the Pareto Frontier. The performance and logarithmic cost of models along the Figure 6: Distribution of errors for four types. We counted two agent trajectories for the advanced claude series models. pareto frontier exhibit an approximately linear relationship. This observation presents valuable opportunity for optimizing cost-performance balance in real-world toolcalling agent. 2. Optimal Cost-Performance Models. The models positioned on the pareto frontier represent the most costeffective choices for tool calling. These include Qwen332B, Qwen2.5-72B-Instruct, Deepseek-R1-0528, and Claude-Sonnet-4, each demonstrating distinct advantages in terms of cost-performance efficiency. Error Analysis We conducted detailed error analysis on the trajectories of current retrieval and invocation agents to provide insights for future development. Human annotators were employed to classify error types in the trajectories of Claude-Opus-4 and Claude-Sonnet-4. Based on the modules defined in the MCP Copilot Agent framework, we identified four distinct and easily distinguishable error categories (Figure 6). Each erroneous trajectory was uniquely classified into one error type without overlap. Detailed error examples are provided in Appendix G. Query Error. Query errors occur when the generated query either lacks semantic relevance to the required tools or exhibits granularity mismatch with tool capabilities. For instance, in the task summarize todays news and save as PDF, the agent might request single omnipotent tool despite the availability of specialized tools for news retrieval and PDF generation. Such granularity mismatches prevent the retrieval system from providing appropriate tools, and agents often fail to refine queries based on retrieval feedback. Hallucinated queries for irrelevant tools further exacerbate this issue. These errors stem from limitations in LLMs task decomposition and planning capabilities, suggesting room for improvement despite their generally competent performance. Retrieve Error. Retrieve errors arise when semantically appropriate queries fail to match available tools due to retrieval system shortcomings. For example, in the task Convert the YouTube video to MP3 format, the retrieval system may overlook the youtube downloader tool (which supports format conversion) due to unrecognized semantic equivalence between convert to MP3 and the tools documented extract audio tracks functionality. These errors highlight challenges in hierarchical retrieval (e.g., MCP server-tool structures) and semantic similarity computation. Dominating the error distribution, retrieve errors underscore the critical need for enhanced retrieval architectures and more robust similarity metrics. Tool Error. Tool errors occur when the agent retrieves the correct tool but invokes it incorrectlye.g., via error parameters or incomplete server/tool names. In the task summarize news and save to specified path, the agent might supply path name instead of the required path parameter to the save tool. Such inaccuracies reflect limitations in contextual precision and memory retention. While modern LLMs exhibit strong contextual understanding, these errors indicate need for more sophisticated memory mechanisms to ensure reliable tool usage. Other Error. This category encompasses sporadic failures beyond the above types, including network timeouts or model invocation errors. For example, in summarize todays news, network timeout during news retrieval may cause the agent to abandon the task without retries or alternative solutions. Such behavior reveals deficiencies in framework design, particularly the absence of robust errorhandling mechanisms (e.g., failure recovery, adaptive tool exploration). The prevalence of these errors suggests that while current frameworks support basic exploration, significant improvements in fault tolerance and proactive problemsolving are needed."
        },
        {
            "title": "Related Work",
            "content": "Tool-Use Benchmarks Most existing benchmarks for tool utilization rely on simulated APIs due to the inherent instability of real-world APIs. For instance, API-Bank (Li et al. 2023) and StableToolBench (Guo et al. 2024) employ artificially constructed toolsets to ensure API stability. Other works, such as ToolAlpaca (Tang et al. 2023) and Seal-Tools (Wu et al. 2025), collect real-world API interfaces but are unable to execute actual calls. third category of tool-use benchmarks, including ToolBench (Qin et al. 2024) and ShortcutsBench (SHEN et al. 2025), attempts to integrate real-world APIs but faces challenges due to rapid API changes, leading to frequent tool unavailability (Guo et al. 2024). Recent efforts, like StableToolBench-MirrorAPI (Guo et al. 2025), leverage fine-tuned LLMs to simulate API interfaces and calls. However, these prior tool-use benchmarks predominantly focus on API-based tools, which introduces instability and limits functionalityparticularly in directly manipulating local user files or enabling complex operations (e.g., interacting with local software). The emergence of MCP has shifted this paradigm by providing stable, unified interface, enabling the development of general-purpose toolsets. In this work, we construct practical MCP toolset, addressing both the instability of APIs and their functional constraints, thereby delivering comprehensive and reliable real-world tool-use benchmark. MCP Benchmarks The evaluation of MCP systems remains an emerging and rapidly evolving field. Among existing benchmarks, MCPBench (Luo et al. 2025) stands as one of the earliest efforts, primarily focusing on comparative analyses between MCP tools and traditional API-based tools. Building upon this, MCP-RADAR (Gao et al. 2025) extends the evaluation scope by introducing multi-dimensional assessment framework that examines critical aspects such as efficiency, accuracy, and robustness. More recently, MCPEval (Liu et al. 2025) has been advanced the field further by proposing fine-grained evaluation framework capable of automatically generating queries to assess the performance of MCP servers. Despite these advancements, existing benchmarks suffer from key limitation: their evaluations are predominantly conducted on small-scale MCP servers (typically around 10 servers), which inadequately reflects real-world scenarios where agents must operate in large-scale, dynamic environments. To bridge this gap, our work introduces large-scale MCP toolset and systematically investigates agent capabilities in accomplishing everyday tasks through extensive tool utilization. Recent efforts, such as RAG-MCP (Gan and Sun 2025), MCPZero (Fei, Zheng, and Feng 2025), and ScaleMCP (Lumer et al. 2025), have explored retrieval methods over large-scale MCP toolsets. However, these approaches are constrained by rigid pipelines that lack dynamic adaptability in tool invocation and error feedback. Furthermore, ScaleMCP relies on manually constructed toolset with limited functional diversity, restricting its applicability to broader, real-world use cases. Conclusion In this paper, we present LiveMCPBench, benchmark designed to evaluate the capability of agents in accomplishing daily tasks using large-scale MCP toolset. We introduce LiveMCPTool, comprehensive and readily deployable collection of MCP tools. We propose LiveMCPEval, an automated evaluation framework based on the LLM-as-a-Judge, which effectively assesses complex tool-usage tasks characterized by time-varying dynamics and diverse completion paths. Human evaluations confirm the reliability of LiveMCPEval. Furthermore, we develop MCP Copilot Agent, an agent framework capable of autonomous exploration and dynamic decision-making in large-scale MCP environments. We conduct extensive evaluations on ten frontier models, revealing limitations in widely-used LLMs when applied to large-scale tool invocation tasks. Our in-depth analysis uncovers distinct behavioral patterns across different models and identifies the most cost-effective solutions. Finally, through detailed error analysis, we highlight two critical shortcomings in current models: (1) deficiencies in task decomposition and planning, and (2) inadequate adaptation of tool retrieval systems in MCP environments. These findings provide clear directions for future improvements in the field."
        },
        {
            "title": "Claude",
            "content": "4. Accessed: DeepSeek-V3 Technical Report. Introducing the Model Context Prohttps://www.anthropic.com/news/model-contextAnthropic. 2024. tocol. protocol. Accessed: 2025-07-24. Anthropic. Introducing 2025. https://www.anthropic.com/news/claude-4. 2025-07-24. DeepSeek-AI. 2025. arXiv:2412.19437. Fei, X.; Zheng, X.; and Feng, H. 2025. MCP-Zero: Active Tool Discovery for Autonomous LLM Agents. arXiv:2506.01056. Gan, T.; and Sun, Q. 2025. RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation. arXiv:2505.03275. Gao, X.; Xie, S.; Zhai, J.; Ma, S.; and Shen, C. 2025. MCP-RADAR: Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models. arXiv:2505.16700. Google. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv:2507.06261. Guo, Z.; Cheng, S.; Niu, Y.; Wang, H.; Zhou, S.; Huang, W.; and Liu, Y. 2025. StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs. arXiv:2503.20527. Guo, Z.; Cheng, S.; Wang, H.; Liang, S.; Qin, Y.; Li, P.; Liu, Z.; Sun, M.; and Liu, Y. 2024. StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics: ACL 2024, 1114311156. Bangkok, Thailand: Association for Computational Linguistics. Hou, X.; Zhao, Y.; Wang, S.; and Wang, H. 2025. Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions. arXiv:2503.23278. Li, M.; Zhao, Y.; Yu, B.; Song, F.; Li, H.; Yu, H.; Li, Z.; Huang, F.; and Li, Y. 2023. API-Bank: Comprehensive Benchmark for Tool-Augmented LLMs. In The 2023 Conference on Empirical Methods in Natural Language Processing. Liu, Z.; Qiu, J.; Wang, S.; Zhang, J.; Liu, Z.; Ram, R.; Chen, H.; Yao, W.; Wang, H.; Heinecke, S.; Savarese, S.; and Xiong, C. 2025. MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models. arXiv:2507.12806. Lotov, A. V.; Bushenkov, V. A.; and Kamenev, G. K. 2004. Interactive decision maps: Approximation and visualization of Pareto frontier, volume 89. Springer Science & Business Media. Lumer, E.; Gulati, A.; Subbiah, V. K.; Basavaraju, P. H.; and Burke, J. A. 2025. ScaleMCP: Dynamic and AutoSynchronizing Model Context Protocol Tools for LLM Agents. arXiv:2505.06416. Luo, Z.; Shi, X.; Lin, X.; and Gao, J. 2025. Evaluation Report on MCP Servers. arXiv:2504.11094. Introducing GPT-4.1 in the API. Openai. 2025. https://openai.com/index/gpt-4-1/. Accessed: 2025-07-24. Qin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.; Cong, X.; Tang, X.; Qian, B.; Zhao, S.; Hong, L.; Tian, R.; Xie, R.; Zhou, J.; Gerstein, M.; dahai li; Liu, Z.; and Sun, M. 2024. ToolLLM: Facilitating Large Language Models to In The Twelfth InternaMaster 16000+ Real-world APIs. tional Conference on Learning Representations. Qu, C.; Dai, S.; Wei, X.; Cai, H.; Wang, S.; Yin, D.; Xu, J.; and Wen, J.-R. 2025. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8): 198343. Qwen. 2025. Qwen3 Technical Report. arXiv:2505.09388. Qwen; Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang, J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.; Dang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue, M.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.; Xia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan, Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5 Technical Report. arXiv:2412.15115. SHEN, H.; Li, Y.; Meng, D.; Cai, D.; Qi, S.; Zhang, L.; Xu, M.; and Ma, Y. 2025. ShortcutsBench: Large-Scale Realworld Benchmark for API-based Agents. In The Thirteenth International Conference on Learning Representations. Silver, D.; and Veness, J. 2010. Monte-Carlo Planning in Large POMDPs. In Lafferty, J.; Williams, C.; Shawe-Taylor, J.; Zemel, R.; and Culotta, A., eds., Advances in Neural Information Processing Systems, volume 23. Curran Associates, Inc. Tang, Q.; Deng, Z.; Lin, H.; Han, X.; Liang, Q.; Cao, B.; and Sun, L. 2023. ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases. arXiv:2306.05301. Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345. Wu, M.; Zhu, T.; Han, H.; Tan, C.; Zhang, X.; and Chen, W. 2025. Seal-Tools: Self-instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark. In Wong, D. F.; Wei, Z.; and Yang, M., eds., Natural Language Processing and Chinese Computing, 372384. Singapore: Springer Nature Singapore. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K. R.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine, S., eds., Advances in Neural Information Processing Systems, volume 36, 4659546623. Curran Associates, Inc. Limitations While LiveMCPBench represents comprehensive benchmarking framework, we acknowledge several limitations in its design and evaluation methodology: Dependence on LLM evaluation. The LiveMCPEval component relies heavily on LLM-based assessment. Although we have validated the evaluation accuracy through human experiments, potential model biases may still influence the results. To mitigate this concern, we conducted extensive case studies analyzing model judgment failure cases, which helps improve the robustness of our evaluation framework. Evaluation Assumptions. Our assessment framework operates under the assumption that agent behavior trajectories and tool descriptions sufficiently reflect task performance, without explicitly verifying the final environmental impact. While this assumption holds in most cases, expanding the toolset could introduce inconsistencies between actual tool effects and their descriptions, potentially compromising evaluation reliability. To address this, we rigorously inspect the quality of LiveMCPTool to minimize such discrepancies. Ethical Considerations The advent of large-scale multi-tool retrieving and calling agents promises to revolutionize traditional UI-based interaction paradigms by shifting from complex message retrieval or manual UI operations to automated tool invocation. This transition holds significant potential to reduce usability barriers, enhance operational efficiency, and accelerate progress toward Artificial General Intelligence (AGI). Furthermore, such systems can augment the capabilities of smaller models through automated tool construction by larger models. For instance, when faced with tasks beyond their native competence (e.g., complex code generation), smaller models can leverage tools dynamically encapsulated by larger models through MCP interfaces. However, alongside these benefits, our framework introduces potential risks that warrant careful consideration. Malicious actors could exploit the system by disguising harmful or unsafe tools through misleading descriptions, potentially inducing models to execute dangerous operations. Such misuse may lead to information security breaches or financial losses. Additionally, erroneous tool invocation by the modelsuch as unintended deletion of local filescould cause significant losses, underscoring the need for robust safeguards in tool validation and execution monitoring."
        },
        {
            "title": "C Details of Task Construction",
            "content": "C.1 Task Statistics The task statistics of LiveMCPBench are illustrated in Figure 7. LiveMCPBench comprises six categories of tasks, each designed to reflect common real-life scenarios: Figure 7: Task distribution in LiveMCPBench: comprehensive benchmark comprising 95 tasks Across 6 distinct domains. 2. Lifestyle. These tasks pertain to daily routines, such as retrieving news updates or querying the latest arXiv papers. 3. Leisure. This category encompasses entertainmentoriented tasks, including fetching gaming news, obtaining specific game-related information, or retrieving details about museums. 4. Finance. Tasks in this category focus on personal financial management, such as checking stock prices, analyzing market trends, or obtaining cryptocurrency valuations. 5. Travel. This category includes tasks related to personal travel, such as route planning, hotel searches, and ticket inquiries. 6. Shopping. These tasks revolve around personal shopping activities, including product information retrieval and recommendations. C.2 Annotation Principles LiveMCPBench focuses on leveraging large-scale MCP toolset to accomplish complex tasks. To ensure high-quality task construction, we employ two groups of annotators involving proposers and verifiers. All annotators first freely explore the MCP toolset, including tool descriptions and real-world calls, to gain familiarity with its functionalities. First, Proposers are randomly assigned scenario and instructed to formulate tasks adhering to the following principles: 1. Real-World Relevance. Tasks must reflect realistic needs within the given scenario. 1. Office. This category represents typical office-related tasks, primarily involving reading and writing documents in Word, Excel, and PowerPoint. 2. Temporal Dynamics. Tasks should be time-sensitive, requiring real-time information retrieval from tools rather than relying solely on static internal knowledge. Figure 8: Distribution of tools in LiveMCPTool, categorized into 8 distinct types (total: 527 tools). Figure 9: Distribution of servers in LiveMCPTool, categorized into 8 distinct types (total: 70 servers). 3. Tool Diversity. Tasks should necessitate the integration of multiple tools, avoiding cases where single tool suffices for completion. 5. Entertainment. This category includes recreational tools, such as those for retrieving Yu-Gi-Oh card information or League of Legends game data. After proposing task, the proposers try to complete it using the MCP toolset, documenting the required tools and key points. Once all tasks are collected, verifiers manually consolidate similar tasks to prevent redundancy. Additionally, they rigorously assess task feasibility and execution quality to maintain high standards in the benchmark."
        },
        {
            "title": "D Details of LiveMCPTool Collection",
            "content": "D.1 Toolset Statistics The statistics of LiveMCPTools tools and servers are illustrated in Figures 8-9. The collected toolset is categorized into eight distinct classes: 1. Discovery. This category encompasses tools for information gathering and retrieval, such as search engines and news aggregators. 2. Visualization. Tools in this category facilitate data or concept visualization, including bar chart plotting and mind map creation. 3. File Access. This class comprises tools for local file operations, such as reading Word, Excel, or PowerPoint files, as well as executing command-line instructions. 4. Code. These are programming-related tools, such as those providing the latest AntV documentation and sample code. 6. Finance. Financial tools fall under this class, including those for fetching real-time stock prices or cryptocurrency market data. 7. Location. This category consists of map-based services, such as navigation systems and points-of-interest discovery tools. 8. Miscellaneous. This catch-all category accommodates tools not fitting the above classifications, such as calculators and local memory utilities."
        },
        {
            "title": "E Implementation Details",
            "content": "E.1 Computing Resources and Private Models In our experiments, we deployed two models: Qwen2.572B-Instruct and Qwen3-Embedding-0.6B. The computational infrastructure consisted of Linux server (Ubuntu 22.04) with 4 NVIDIA A800-80G GPUs and 1TB of memory. We accessed the following proprietary models through their respective platforms: OpenRouter: GPT-4.1, GPT-4.1-Mini, DeepSeek-R10528, DeepSeek-V3-0324, Qwen3-235B-A22B and Qwen3-32B. Anthropic Console: Claude-Opus-4-20250514, ClaudeSonnet-4-20250514. Google AI Studio: Gemini-2.5-Pro. Figure 10: An illustrative case of evaluation failure in LiveMCPEval. The assessment was conducted using DeepSeek-V3 on the completion trajectory from Claude-Sonnet-4. The evaluator model erroneously concluded the task was successful based solely on the agents file creation action, while failing to recognize that the agent did not actually acquire the required information. To address suboptimal greedy decoding in certain reasoning models, we implemented uniform temperature parameter of 0.7 across all experiments. This configuration introduces controlled stochasticity while maintaining result reliability for long-horizon tasks, as we observed that sporadic randomness has negligible cumulative impact on aggregate performance. E.2 Tool Retrieval Configuration For consistent retrieval performance, we established standardized framework using Qwen2.5-72B-Instruct for tool summarization and Qwen3-Embedding-0.6B for embedding generation. To control for potential variance in this component, we maintained identical retrieval module parameters across all experimental conditions. Given the inherent temporal variability in tool outputs, we conducted all experiments within tightly controlled window (July 2027, 2025) to minimize fluctuations attributable to temporal factors."
        },
        {
            "title": "F Evaluation Analysis",
            "content": "F.1 Case Study: Error Evaluation Examples To illustrate cases where evaluator judgments diverge from human assessments, we conducted case study on ClaudeSonnet-4 trajectories evaluated by DeepSeek-V3, presenting representative example in Figure 10. In this instance, the evaluator failed to recognize that the agent did not actually acquire the correct information, despite successfully creating the required file. The evaluator erroneously concluded task completion based solely on the file creation process. This case highlights potential limitation of LiveMCPEval: the systems tendency to overlook critical details when processing excessively lengthy and complex trajectories. We propose that this long-range evaluation challenge could be addressed by modifying existing evaluation frameworks to incorporate dynamic agent-based assessment of each trajectory step. However, such an approach would significantly compromise evaluation efficiency. While our current evaluation method achieves satisfactory human agreement rates, this particular issue warrants further in-depth investigation. Figure 11: Comparison of key points in DeepSeek-V3 and human annotations: Similar content despite different ordering. F.2 Human and LLM Key Points Examples To analyze the differences between LLM-generated key points and human-annotated key points, we conducted comparative study between key points generated by Deepseek-V3 and those manually annotated by humans. The comparison results are presented in Figure 11. Our analysis reveals that while the ordering of key points differs between human and LLM-generated outputs, both consistently capture similar critical steps. This observation suggests the practical applicability of LLM-generated key points in evaluation tasks. Importantly, our findings indicate that LLM-generated key points can serve as reliable alternative for robust evaluation in scenarios where human annotations are unavailable. Case Study: Error Examples Figures 12-15 present concrete examples of four distinct error types: Query Error, Retrieve Error, Tool Error, and Other Error. Broadly speaking, Query and Other errors primarily highlight design flaws in the agents architecturespecifically, whether the agent incorporates sufficient mechanisms to ensure task completion. In contrast, Tool errors are more closely tied to the capabilities of the LLM itself, particularly its ability to accurately process tool parameters and descriptions while maintaining nuanced contextual understanding. Retrieve errors, on the other hand, largely reflect the limitations of the tool retrieval system, testing its effectiveness in identifying relevant tools based on the server-tool description."
        },
        {
            "title": "H Prompts",
            "content": "H.1 MCP Copilot Agent Prompt Prompt for MCP Copilot Agent You are an agent designed to assist users with daily tasks by using external tools. You have access to two tools: retrieval tool and an execution tool. The retrieval tool allows you to search large toolset for relevant tools, and the execution tool lets you invoke the tools you retrieved. Whenever possible, you should use these tools to get accurate, up-to-date information and to perform file operations. Note that you can only response to user once, so you should try to provide complete answer in your response. Task Tool: mcp-copilot (with route and excute tool)"
        },
        {
            "title": "Prompt for route tool",
            "content": "This is tool used to find MCP servers and tools that can solve user needs When to use this tool: -When faced with user needs, you (LLM) are unable to solve them on your own and do not have the tools to solve the problem. -When user proposes new task and you (LLM) are unsure which specific tool to use to complete it. -When the users request is vague or complex, and feasible tool options need to be explored first. -This is the first step in executing unknown tasks, known as the discovery phase, aimed at finding the correct tool. **Parameter Description** Query (string, required): The input query must contain <tool assistant> tag with server and tool descriptions, for example: <tool assistant> server: ... # Platform/permission domain tool: ... # Operation type + target </tool assistant> Prompt for excute tool tool for executing specific tool on specific server.Select tools only from the results obtained from the previous route each time. When to use this tool: - When using the route tool to route to specific MCP server and tool - When the execute-tool fails to execute (up to 3 repetitions). - When the users needs and previous needs require the same tool. Parameters explained: -server name: string, required. The name of the server where the target tool is located. -tool name: string, required. The name of the target tool to be executed. -params: dictionary or None, optional. dictionary containing all parameters that need to be passed to the target tool. This can be omitted if the target tool does not require parameters. Prompt for server summary You are an expert AI technical writer. Based on the following information about an MCP server, please generate concise and accurate summary of its core purpose and capabilities. **Server Name:** server name **Server Description:** server desc **Available Tools:** tool descriptions Please return only the generated summary text, without any additional titles or preambles. H.2 LiveMCPEval Prompt"
        },
        {
            "title": "Prompt for evaluation",
            "content": "You are an expert in evaluating the performance of tool-use agent. The agent is designed to help human user use multi-tools to complete task. Given the users task, the agents final response, key points for task completion, and tool call history, your goal is to determine whether the agent has completed the task and achieved all requirements. Your response must strictly follow the following evaluation criteria! *Important Evaluation Criteria*: 1. You must carefully check whether the information (e.g. the coordinates of the addresses) comes from the tool call, if the agent get it from the internal knowledge, it should be considered failed. 2: Some tasks require to create files to be considered successful. *IMPORTANT* Format your response into two lines as shown below: Thoughts: <your thoughts and reasoning process based on double-checking each key points and the evaluation criteria> Status: success or failure User Task: task Key Points: key points Final Response: response Tool Call History: tool calls Tool Descriptions: tool descriptions Prompt for identify key points You are an expert tasked with analyzing given task to identify the key points explicitly stated in the task description. **Objective**: Carefully analyze the task description and extract the critical elements explicitly mentioned in the task for achieving its goal. **Instructions**: 1. Read the task description carefully. 2. Identify and extract **key points** directly stated in the task description. - **key point** is critical element, condition, or step explicitly mentioned in the task description. - Do not infer or add any unstated elements. **Respond with**: - **Key Points**: numbered list of the explicit key points for completing this task, one per line, without explanations or additional details. task Figure 12: An illustration of Query Error: Discrepancy between the agent-generated query and the tasks required competencies. Figure 13: An illustration of Retrieve Error: The retrieve system incorrectly identifies and returns an inappropriate tool. Figure 14: An illustration of Tool Error: The LLM misapplication with improper tool parameters. Figure 15: An illustration of Other Error: The agents inadequate response to tool timeout."
        }
    ],
    "affiliations": [
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}