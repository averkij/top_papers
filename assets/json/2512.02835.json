{
    "paper_title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning",
    "authors": [
        "Yifan Li",
        "Yingda Yin",
        "Lingting Zhu",
        "Weikai Chen",
        "Shengju Qian",
        "Xin Wang",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ ."
        },
        {
            "title": "Start",
            "content": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning Yifan Li1,2 Yingda Yin3* Lingting Zhu3 Weikai Chen3 Shengju Qian3 Xin Wang3 Yanwei Fu1,2* 1Fudan University 2Shanghai Innovation Institute 3LIGHTSPEED 5 2 0 D 2 ] . [ 1 5 3 8 2 0 . 2 1 5 2 : r Figure 1. (Left) Through an explicit reasoning chain, our ReVSeg tackles reasoning-focused video object segmentation and accurately grounds objects referenced by complex, abstract real-world queries. (Right) While the base model and its RL variant struggle on the task, our method achieves strong performance, with RL post-training yielding further substantial boost. We report the &F metric on Ref-DAVIS17 (in-domain) and ReasonVOS (out-of-domain) datasets in the chart."
        },
        {
            "title": "Abstract",
            "content": "terpretable reasoning trajectories. Project Page. Reasoning-centric video object segmentation is an inherthe query often refers to dynamics, ently complex task: causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into single-step prediction, ReVSeg executes three explicit operations semantics interpretation, temporal evidence selection, and spatial grounding aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields in- * Corresponding authors. Project leads. 1. Introduction Human interpretation of videos relies on recognizing how events unfold, why actions occur, and when key moments matter. Traditional video object segmentation (VOS) methods, however, primarily exploit appearance-only or category-level cues [3, 29, 40, 45, 46]. Reasoning VOS elevates the task: the model must parse abstract, underspecified instructions, draw on commonsense and causal integrate temporal dynamics with semantic reasoning, knowledge to identify the target (e.g., the runner most likely to win or the object causing the accident). While recent vision-language models (VLMs) have reasoning segmentashown promising capabilities for tion [2, 16, 50], current systems predominantly model reasoning VOS as single-step latent prediction: inserting special token (e.g., <SEG>) and decoding it directly into mask outputs [2, 8, 20, 35, 41, 42, 50, 53]. This collapses the multi-step reasoning process interpreting abstract instructions, identifying candidates, and spatial-temporal 1 grounding into simple conclusion as well as opaque embeddings. Such compactness comes at great cost: interpretability is bounded, distribution shift arises from forcing VLMs into non-native output spaces, and substantial data is required for supervised fine-tuning. These challenges reflect deep insight: effective video reasoning unfolds through sequence of deliberate choices, not single latent inference. model must determine where to focus, when to attend, and which entity the query refers to. Motivated by this principle, we introduce ReVSeg, which eliminates the use of latent segmentation tokens and instead reformulates reasoning video segmentation as an explicit sequence of reasoning chain. In particular, ReVSeg decomposes reasoning VOS into three actions aligned with VLM-native capabilities: video understanding (interpreting the query and assessing scene dynamics), temporal grounding (identifying key frames or intervals pertinent to the query), and spatial grounding (localizing the target objects within selected frames). ReVSeg orchestrates these primitives through multi-turn actions with single VLM, ensuring the semantic context established in early reasoning steps seamlessly propagates to downstream localization. Executing reasoning through these native interfaces preserves pretraining alignment, avoids heavy re-training, and transforms an otherwise entangled problem into structured procedure. Once the reasoning chain is made explicit, the central challenge becomes how to optimize the chain itself. Supervised learning provides little leverage, as the correctness of intermediate decisions is hardly annotated. We therefore adopt reinforcement learning (RL) [9, 30], viewing reasoning VOS as behavioral policy that should be rewarded only when its full decision trajectory leads to correct segmentation outcome [17]. Yet, naıvely applying RL encounters obstacles: reasoning VOS offers extremely sparse success signals, and requires coordination across video understanding, temporal selection, and spatial localization. To address these challenges, we introduce reasoning-aligned rewards which inject signals at critical decision points, ensuring that RL enhances the models reasoning behavior itself. Together with decomposition, RL transforms the task from an opaque latent regression into structured, optimizable reasoning policy: decomposition isolates what the model must decide while RL determines how those decisions should cooperate. This synergy yields reasoning policy that is both internally consistent and empirically strong. Experiments show that ReVSeg delivers the state-of-theart (SOTA) performances on multiple standard VOS benchmarks, outperforming both latent-based VLMs [8, 16, 20, 35, 41, 42, 50, 53, 55] and the training-free explicit VLM [13], with auditable reasoning traces. As shown in Figure 1, controlled ablations demonstrate that without our decomposed reasoning chain, both the base VLM and its RL posttrained variant tend to fail on the complex reasoning VOS task. In contrast, our framework attains strong performance even before RL, and the RL post-training further delivers substantial improvement. In summary, our contributions are as follows: principled decomposition of reasoning VOS. We introduce ReVSeg, which reformulates reasoning video segmentation as an explicit multi-step reasoning chain built from native VLM primitives. An RL framework that optimizes the reasoning chain itself. We develop novel RL-based formulation that directly optimizes the reasoning chain, allowing the model to refine decision quality without requiring dense supervision. We set the new state of the art on standard VOS benchmarks while providing interpretable reasoning traces. 2. Related Works 2.1. Reasoning Video Object Segmentation Fine-grained text-guided VOS has advanced through specialized segmentation architectures and referring-video pipelines [3, 5, 29, 45]. For the emerging and more complex setting of reasoning VOS, recent methods fine-tune VLMs to emit implicit mask tokens [2, 8, 20, 41, 42, 50, 53], often paired with strong decoders such as SAM/SAM2 [15, 26]. Yet robust reasoning in complex videosmultiple similar objects, occlusion, fast motion, and long-range contextremains challenging. Explicit textual reasoning via CoT has begun to appear, e.g., the training-free framework CoT-RVS [13], which adopts two independent VLM systems for frame-by-frame segmentation bridged via text. While effective, such designs are bounded by the separated information flow and module interoperability and is In this work, we formuconstrained to be training-free. late reasoning VOS as an explicit reasoning chain within unified VLM, and employ reinforcement learning for selfimprovement. 2.2. VLMs with Reinforcement Learning Reinforcement learning has emerged as practical route to strengthen the reasoning capabilities of large models when supervised annotated data are scarce. Test-time scaling via Chain-of-Thought [43] improves VLM reasoning, while rewards further refine solution quality. Notably, Group Relative Policy Optimization (GRPO) [30] enables efficient critic-free updates and strong gains with modest training budgets, as demonstrated by DeepSeek-Zero [9]. Recent efforts [11, 24, 32, 37, 47] leverage VLMs for high-level reasoning, and reasoning image grounding [21, 22, 27, 48, 54] has been actively studied. However, in the video modality, most work [4, 7, 17] still targets high-level understanding; fine-grained spatio-temporal grounding remains limited due to task complexity. Our results show that reinforcement 2 Figure 2. Overview of ReVSeg. The model runs two-turn reasoning chain over the input video and query. Round one analyzes the scene and selects an informative keyframe with concise object description. Round two grounds the target on that keyframe by predicting bounding box. The keyframe-bbox pair conditions video tracker to produce full segmentation sequence. reward manager provides concise signals to post-train the VLM via reinforcement learning, improving keyframe selection, grounding accuracy, and overall robustness. learning can effectively boost reasoning for video grounding. 3. Method 3.1. Overview Task Formulation. Given natural language query and video sequence with frames = {It}T t=1 RT HW 3, where and denote the height and width of each frame, the goal of reasoning VOS is to segment the query-referred objects throughout the video. The model predicts sequence of binary masks = {mt}T t=1 RT HW , where mt {0, 1}HW represents the foreground region in frame t. Let = {m t=1 denote the ground-truth masks. The task is to learn mapping }T (V, x) M, (1) that maximizes agreement with . This mapping must correctly resolve the semantic correspondence specified in query and maintain temporal consistency across the video. Pipeline Overview. ReVSeg formulates reasoning-centric video object segmentation as an explicit sequence of reasoning steps. The complex task is decomposed into three VLM-native capabilities video understanding, temporal grounding, and spatial grounding and executed through multi-turn dialogue with single VLM. As illustrated in Fig. 2, the first-round dialogue takes the video and query (V, x) as input. The VLM performs video understanding and temporal grounding: it interprets the abstract query, analyzes scene dynamics, produces concise spatial description of the target, and identifies the keyframe that best captures the entity of interest. Within this round, the raw, cluttered video sequence and the vague high-level query (e.g., the best choice for the family outing) are distilled into well-specified keyframe and concrete textual description (e.g., the silver minivan). These intermediate outputs encapsulate the VLMs commonsense and causal reasoning, effectively transforming complex videotext reasoning problem into substantially simpler image-level segmentation task. Given these intermediate results, the second-round dialogue processes the selected keyframe and the concrete object description. The VLM then performs spatial grounding and outputs tight bounding box for the specified object, completing the two-round reasoning chain. With the localized keyframe target, an off-the-shelf video tracker (e.g., SAM2) is readily applied to produce the final video mask predictions. Because both rounds occur within single VLM, the semantic context established during early reasoning is seamlessly propagated to subsequent steps, ensuring consistency throughout the chain. Finally, we optimize the reasoning process through reinforcement learning. Rather than rewarding only the final outcome of the complex task, the decomposition of the pipeline and the availability of meaningful intermediate products allow for richer, reasoning-aligned reward signals that better guide policy improvement. 3.2. Decomposed Generation with Reasoning Chain Directly producing spatio-temporal grounding from video input remains challenging for current VLMs [1, 11, 31, 32, 36, 38]. Owing to the limited availability of high-quality annotated datasets, these models have not been extensively pretrained for VOS. Even with test-time scaling strategies such as Chain-of-Thought prompting [43], multirollout sampling with self-consistency [39], or searchbased inference methods including Tree Search and Beam Search [33, 51] VLMs still struggle to generate reliable spatio-temporal grounding results directly from raw video. These limitations highlight key observation that complex video reasoning inherently unfolds as sequence of interdependent reasoning actions, rather than single-step prediction. This insight motivates us to decompose the reasoning-based VOS task into set of primitive capabilities, orchestrated through multi-turn reasoning chain, as described below. First Round Rollout. In the first reasoning round, the VLM receives videoquery pair (V, x) and generates text response y1 under an instruction-guided prompt. During this step, the model interprets the user query, analyzes the video content, infers the target entity, identifies its temporal occurrence within the sequence, and produces concise textual description of the target object grounded in keyframe. The output of this reasoning round can be formalized as: y1 F( V, x). (2) parser then processes the structured response y1, extracting: the keyframe index {0, 1, , 1}, concise spatial description of target objects, and status flag S1 {succ, fail} indicating the extraction success: G(y1) = (cid:40) S1 = succ (S1, k, d), (S1, null, null), S1 = fail. (3) Based on the selected keyframe index k, the corresponding frame Ik is retrieved from the video sequence . Second Round Rollout. If the status flag from the previous round is S1 = succ, the rollout generation proceeds In this round, the VLM receives to the second stage. the selected keyframe Ik and the concise object description d, together with the generation history (V, x, y1) from the first round. Conditioned on this context and an instructionguided prompt, the model generates text response y2. At this stage, is responsible for spatial grounding: it localizes the target object in keyframe Ik using both the visual input and the accumulated dialogue history. The output y2 follows structured format, formalized as: y2 F(V, x, y1, Ik, d). (4) Similarly, the parser processes the structured response y2, extracting bounding box Bk R4 0 and new status flag S2: G(y2) = (cid:40) S2 = succ (S2, Bk), (S2, null), S2 = fail. (5) The final rollout output is obtained by concatenating the two-stage responses: = y1 y2. (6) This two-round decomposition cleanly separates video understanding and temporal selection from spatial localization, facilitates well-defined task at each stage, and provides stable, modular interface for the RL optimization. 3.3. Reasoning VOS with GRPO Reasoning capability is the key factor determining the upper bound of reasoning-based VOS. However, the supervised reasoning data required by Supervised Fine-tuning (SFT) is scarce and costly. Inspired by outcome-driven training in recent reasoning systems [9, 30], we employ reinforcement learning to enable the policy model to self-improve under task-specific rewards, thereby enhancing its reasoning ability. Reward Modeling. Rewards shape the optimization dynamics and are therefore crucial in RL. Following the minimalist design philosophy of DeepSeek-R1-Zero [9], we adopt rule-based reward system consisting of three components: Format Reward rf : Correct output formatting is essential for interacting with the environment. The model must place its reasoning process between <think> and </think> tags and the final answer between <answer> and </answer> tags. In addition, the firstturn output y1 must include the keyframe index and object description in JSON format, while the second-turn output y2 must provide the bounding box Bk in JSON. Based on the degree to which the output satisfies these rules, the format reward rf is assigned value in [0, 1]. Temporal Reward rt: The keyframe Ik selected by in y1 critically affects subsequent spatial grounding. Beyond merely containing the target object, we encourage selecting frames where the object is clearly visible, minimally occluded, and sufficiently large. We experiment with several temporal reward choices and finally choose the normalized area of the ground-truth bounding box at Ik: rt = 1(m k1>0) S(m maxt S(m k) mint S(m ) ) mint S(m ) . (7) where S(m ) denotes the pixel area of the ground truth bbox at frame It and 1() denotes the indicator function. See Sec. 4.3 for ablations. Spatial Reward rs: This reward measures final detection quality of the predicted Bk. Following prior works [21, 22], we use Intersection-over-Union (IoU) between the If IoU > 0.5, the predicted and ground-truth boxes. prediction is considered correct and rs = 1; otherwise rs = 0. 4 Table 1. Reasoning video object segmentation performance comparison on ReasonVOS [2] dataset. Method MTTR [3] ReferFormer [46] SOC [23] OnlineRefer [44] SgMg [25] LISA [16] VideoLISA [2] GLUS [20] RGA-3B [35] RGA-7B [35] CoT-RVS-online-7B [13] CoT-RVS-offline-13B [13] ReVSeg-7B (Ours) [CVPR'22] [CVPR'22] [NeurIPS'24] [CVPR'23] [ICCV'23] [CVPR'24] [NeurIPS'24] [CVPR'25] [ICCV'25] [ICCV'25] [arXiv'25] [arXiv'25] - 29.1 30.2 33.3 34.6 33.7 29.1 45.1 47.5 49.1 51.3 49.5 47.5 61.8 33.1 35.6 38.5 42.9 38.7 33.1 49.9 52.4 54.3 56.0 54.5 54.0 67.7 &F 31.1 32.9 35.9 38.7 36.2 31.1 47.5 49.9 51.7 53.6 52.0 50.7 64.8 The total reward for an output combines these components with status flags S1 and S2 indicating whether each step succeeds: = rf + 1(S1=succ)rt + 1(S1=succ & S2=succ)rs. (8) Objective. We adopt Group Relative Policy Optimization (GRPO) [30], critic-free variant of Proximal Policy Optimization (PPO) [28] tailored for sequence models. Given an input query = {V, x}, GRPO samples group of candidate outputs {oi}n i=1 from the current policy πold, evaluates their rewards {ri}, and computes normalized withingroup advantage: Ai = ri mean(r1, . . . , rn) std(r1, . . . , rn) . (9) Since we use on-policy updates (πold = πθ) in practice, importance sampling ratios are remains at one. The policy loss, combined with KL regularization to reference model πref, yields the final training objective: L(θ) = EqP (Q), {oi}πθ(q) (cid:34) 1 (cid:88) i=1 (cid:16) Ai β DKL(πθ πref) (cid:17) (cid:35) . (10) 4. Experiments 4.1. Experiment Settings Training Datasets. For our efficient RL post-training, we rely solely on the video object segmentation (VOS) data, in contrast to previous works [2, 8, 41, 42, 50] that jointly fine-tunes on large, heterogeneous corpora spanning video segmentation, image segmentation, and VQA datasets. Specifically, we curate training data from five benchmarks: Ref-YouTube-VOS [29], MeViS [5], RefDAVIS17 [14], ReVOS [50] and LV-VIS [34]. For each annotated sequence, we first convert per-frame masks to bounding boxes, which serve as ground-truth signals for post-training rewards. To ensure label quality, we run SAM2 [26] on every frame conditioned on its ground-truth box to obtain predicted masks, compute IoU against the annotated masks, and discard all queries from any video whose mean IoU falls below 0.6. This filtering yields approximately 67k data pairs. Benchmarks. We evaluate on five standard VOS benchmarks: two reasoning datasets including ReVOS [50] and ReasonVOS [2] and three referring datasets including RefDAVIS17 [14], Ref-YouTube-VOS [29], MeViS [5]. Notably, ReasonVOS has no training split, thus its evaluation is zero-shot, providing clearer measure of the models generalization ability. Baselines. We benchmark against three families of methods (1) Segmentation Specialists: to contextualize our gains. strong VOS/Ref-VOS systems [3, 5, 6, 10, 18, 23, 25, 29, 44, 46] trained with dense supervision, optimized for mask quality and temporal consistency. (2) VLM-Based with Latent Tokens Methods: methods [2, 8, 20, 35, 41, 42, 50, 53] that fine-tune VLMs to emit task-specific control tokens or logits that drive downstream mask head, which are the current mainstream for reasoning VOS. (3) VLM-Based with Explicit Reasoning Methods: methods that perform reasoning to explicitly ground targets via boxes/masks, an underexplored paradigm where CoT-RVS [13] and our method fall. We report results across all three to isolate the advantage of our proposed ReVSeg. Evaluation Metrics. Following previous works [2, 8, 13, 20, 41, 42, 50, 53] on reasoning video object segmentation, we report region similarity (J ), contour accuracy (F) and their mean (J &F) as the primary video-level metrics. Implementation Details. We adopt Qwen2.5-VL-7B [1] as the default reasoning model and SAM2 (Hiera-L) [26] as the default video tracker model . For post-training with GRPO, each optimizer step processes 128 input data, and we sample = 8 rollouts per prompt, yielding an effective batch of 1024 sequences per optimizer step. The learning rate is set to 1e 6, and the KL regularization coefficient β = 1e 3. For each video, we uniformly sample 16 frames as input to F. All input frames are resized to 448 448 before the first round generation. In the second round, the selected keyframe Ik is resized to 840 840 for spatial grounding. The tracker operates on the full video at its original resolution. 4.2. Experimental Results Reasoning Video Object Segmentation. We first evaluate on reasoning VOS benchmarks ReasonVOS dataset in Tab. 1 and ReVOS dataset in Tab. 2. On ReasonVOS, ReVSeg-7B achieves decisive margin over the previous state-of-the-art (SOTA) method RGA-7B [35], improving by +10.5 points, by +11.7 points, and &F by 5 Table 2. Reasoning video object segmentation performance comparison on ReVOS [50] dataset. Method Type MTTR [3] ReferFormer [46] LMPM [5] LLaMA-VID [18] + LMPM [5] LISA-7B [16] LISA-13B [16] TrackGPT(IT)-7B [55] TrackGPT(IT)-13B [55] VISA-7B [50] VISA-13B [50] VISA(IT)-7B [50] VISA(IT)-13B [50] VRS-HQ-7B [8] GLUS [20] HyperSeg [41] RGA3-3B [35] RGA3-7B [35] ViLLa [53] InstructSeg [42] CoT-RVS-online-7B [13] CoT-RVS-offline-12B [13] ReVSeg-7B (Ours) [CVPR'22] [CVPR'22] [ICCV'23] [ECCV'24] [CVPR'24] [CVPR'24] [arXiv'24] [arXiv'24] [ECCV'24] [ECCV'24] [ECCV'24] [ECCV'24] [CVPR'25] [CVPR'25] [CVPR'25] [ICCV'25] [ICCV'25] [ICCV'25] [ICCV'25] [arXiv'25] [arXiv'25] - Segmentation Specialists VLM-Based w/ Latent Tokens VLM-Based w/ Explicit Reasoning referring 30.2 34.3 39.1 39.1 47.1 47.9 49.7 50.6 54.7 55.8 52.6 59.1 64.5 56.0 60.9 61.0 62.3 - 59.2 - - 68.1 &F 30.0 32.7 34.1 34.1 45.7 46.6 48.2 49.5 52.9 54.1 50.9 57.4 62.1 60.7 58.5 59.3 60.5 - 57.0 - - 65. 29.8 31.2 29.0 29.0 44.3 45.2 46.7 48.3 51.1 52.3 49.2 55.6 59.8 58.3 56.0 57.6 58.7 - 54.8 - - 63.3 reasoning 21.5 25.6 24.3 23.7 38.4 39.1 41.2 42.9 41.7 43.5 45.4 46.7 58.7 53.9 55.8 55.0 57.7 - 54.7 - - 61.8 &F 21.0 23.4 18.8 18.2 36.1 36.7 39.0 40.5 39.2 40.9 43.0 44.3 56.1 51.4 53.0 52.8 55.4 - 51.9 - - 58.6 20.4 21.3 13.3 12.8 33.8 34.3 36.8 38.1 36.7 38.3 40.6 42.0 53.5 48.8 50.2 50.6 53.1 - 49.2 - - 55.4 overall 25.9 29.9 31.7 31.4 42.7 43.5 45.5 46.8 48.2 49.7 49.0 52.9 61.6 - 58.4 58.0 60.0 59.1 56.9 48.8 50.9 65.0 &F 25.5 28.1 26.4 26.1 40.9 41.6 43.6 45.0 46.1 47.5 46.9 50.9 59.1 - 55.7 56.1 58.0 57.0 54.5 46.2 47.1 62. 25.1 26.2 21.2 20.9 39.1 39.8 41.8 43.2 43.9 45.3 44.9 48.8 56.6 - 53.1 54.1 55.9 54.9 52.0 43.5 43.4 59.3 +11.2 points. The significant performance improvement demonstrates the effectiveness of ReVSeg with the proposed framework. Furthermore, given the zero-shot nature of ReasonVOS, these gains highlight our strong generalization and robustness under challenging open-world queries. On ReVOS, we conduct comprehensive comparison across nine metrics. Our ReVSeg-7B consistently ranks first, surpassing previous SOTAs, including several larger parameter systems, by obvious margin. The across-theboard improvements on these reasoning VOS benchmarks substantiate the effectiveness of our explicit reasoning chain and the efficiency of the proposed training recipe. Referring Video Object Segmentation. As previous practice [2, 5, 14, 29, 35, 50, 53], we report the experiment results on three Ref-VOS benchmarks, i.e., Ref-YouTubeVOS, Ref-DAVIS17, and MeViS. As in Tab. 4, consistently, our ReVSeg-7B sets new state of the art, improving &F by +2.7 points on Ref-YouTube-VOS, +4.8 points on RefDAVIS17, and +8.5 points on MeViS against the previous SOTAs. Notably, MeViS is motion-guided benchmark and is regarded as the most challenging Ref-VOS benchmark in GLUS [20]. Our substantial gains on MeViS indicate strong adaptability on complex video scenarios with intricate motion patterns. Although referring queries require less semantic reasoning than reasoning queries, our performance gains are still obvious and consistent. These results indicate ReVSeg offers stronger cross-modal video understanding, better temporal aggregation, and more accurate target object detection than prior art. Zero-shot Reasoning Image Segmentation. Besides the main results, we ask whether post-training on video tasks Table 3. Zero-shot reasoning image segmentation results on ReasonSeg [16] dataset. Method Qwen2.5VL-7B ReVSeg-7B (Ours) test val gIoU cIoU gIoU cIoU 54.0 55.9 59.9 59.7 59.5 63.7 44.3 47.4 improves spatial grounding that transfers to image reasoning segmentation. In other words, does our model truly learn better spatial grounding capabilities which is generalizable to other tasks? To probe this, we conduct zeroshot evaluation on the reasoning image segmentation task. Specifically, we evaluate both the base model (Qwen2.5VL-7B [1]) and our post-training ReVSeg-7B on ReasonSeg dataset [16], and report the mean per-image IoU (gIoU) and the cumulative IoU (cIoU) as in LISA [16]. As shown in Tab. 3, despite no image-specific training, ReVSeg yields delivers consistent gains on both test and validation splits compared to the base model. These improvements indicate that our pipeline no only simply couples spatial grounding with video understanding and temporal cues, but also upgrades its intrinsic spatial grounding ability. Qualitative Results. Fig. 3 presents detailed case studies highlighting the role of explicit reasoning chains in VOS. In the first example, the video depicts typical road scene without visually salient target. The query asks what could triggered the drivers honk. The model parses the scene, integrates visual cues with traffic commonsense, and infers plausible causes, ultimately identifying the jaywalking pedestrian as the most likely trigger. Notably, the target object occupies only small portion of the frame, making Table 4. Video referring segmentation results on Ref-Youtube-VOS [29], Ref-DAVIS17 [14], MeViS [5] datasets. Method Type URVOS [29] LBDT [6] MTTR [3] LMPM [5] ReferFormer [46] OnlineRefer [44] DsHmp [10] LISA-7B [16] LISA-13B [16] TrackGPT-7B [55] TrackGPT-13B [55] VISA-7B [50] VISA-13B [50] VideoLISA [2] VRS-HQ-7B [8] GLUS [20] HyperSeg [41] RGA3-3B [35] RGA3-7B [35] ViLLa [53] InstructSeg [42] CoT-RVS-online-7B [13] CoT-RVS-offline-13B [13] ReVSeg-7B (Ours) [ECCV'20] [CVPR'22] [CVPR'22] [ICCV'23] [CVPR'22] [CVPR'23] [CVPR'24] [CVPR'24] [CVPR'24] [arXiv'24] [arXiv'24] [ECCV'24] [ECCV'24] [NeurIPS'24] [CVPR'25] [CVPR'25] [CVPR'25] [ICCV'25] [ICCV'25] [ICCV'25] [ICCV'25] [arXiv'25] [arXiv'25] - Segmentation Specialists VLM-Based w/ Latent Tokens VLM-Based w/ Explicit Reasoning Ref-YouTube-VOS 49.2 45.3 50.6 48.2 56.6 54.0 - - 64.6 61.3 65.5 61.6 69.1 65.0 54.3 53.4 54.8 54.0 57.4 55.3 60.8 58.1 63.2 59.8 64.7 61.4 65.7 61.7 72.5 68.3 69.0 65.5 - - 69.1 65.8 70.1 66.8 70.4 64.6 69.5 65.4 - - - - 75.2 71.1 &F 47.2 49.4 55.3 - 62.9 63.5 67.1 53.9 54.5 56.4 59.5 61.5 63.0 63.7 70.4 67.3 68.5 67.4 68.5 67.5 67.5 - - 73.1 Ref-DAVIS17 56.0 - - - 64.1 67.7 68.1 67.3 68.8 67.0 70.4 72.5 73.8 72.7 79.4 - - 76.6 77.3 78.0 74.9 77.5 78.3 84.1 &F 51.6 54.1 - - 61.1 64.8 64.9 64.8 66.9 63.2 66.5 69.4 70.4 68.8 76.0 - 71.2 72.1 72.8 74.3 71.1 73.9 74.6 80.8 47.3 - - - 58.1 61.6 61.7 62.2 63.2 59.4 62.7 66.3 67.0 64.9 72.6 - - 67.6 68.3 70.6 67.3 70.4 70.9 77. MeViS 29.9 30.8 31.2 40.2 32.2 - 49.8 39.4 40.0 42.6 43.1 46.3 47.1 47.6 53.7 54.2 - 51.5 52.8 52.3 - 49.1 48.1 63.4 &F 27.8 29.3 30.0 37.2 31.0 - 46.4 37.2 37.9 40.1 41.2 43.5 44.5 44.4 50.6 51.3 - 48.8 50.1 49.4 - 45.9 44.2 59.8 25.7 27.8 28.8 34.2 29.8 - 43.0 35.1 35.8 37.6 39.2 40.7 41.8 41.3 47.6 48.5 - 46.2 47.4 46.5 - 42.7 40.3 56.1 Table 5. Ablation experiments for the proposed decoupling framework and RL post-training. The referring VOS results were evaluated on the Ref-DAVIS17 dataset, while the reasoning VOS results were evaluated on the ReasonVOS dataset. Decom. Pipeline indicates whether to use the decomposed pipeline, RL denotes the usage of Reinforcement Learning. Decom. Pipeline RL Ref-DAVIS17 5.6 6.0 66.9 84.1 &F 4.4 5.3 63.0 80.7 3.2 4.0 59.2 77. ReasonVOS 8.8 8.1 43.4 67.7 &F 8.1 7.6 41.0 64.8 7.4 7.1 38.7 61.8 this case particularly challenging. In the second example, the model is queried about the creature posing the greatest threat. It first recognizes the most menacing species (elephant) and, guided by world knowledge that elder elephants protect calves and the herd, pinpoints the specific individual with high precision. Across cases, the reasoning chain consistently selects well-segmentable keyframes, stabilizing grounding and mitigating error accumulation, which yields cleaner masks and more consistent trajectories. Additional visualizations are provided in the supplementary. 4.3. Ablation Studies Key Pipeline Design. We isolate the contributions of decomposed reasoning chain and RL post-training via four (1) Base Model, which directly predicts spatiovariants. (2) Base temporal grounding from the video and query. 7 Model + RL, which applies GRPO post-training to the (3) Decomposed, which restructures inferbase model. ence into our multi-turn decomposed reasoning chain. (4) Decomposed + RL, our final model ReVSeg with GRPOenhanced reasoning. We evaluate them on Ref-DAVIS17 (in-domain Ref-VOS) and ReasonVOS (out-of-domain reasoning VOS), the results are showed in Tab. 5. The base model exhibits very poor video grounding ability, and RL alone fails to lift performance due to sparse effective rollIn outs and credit assignment in end-to-end prediction. contrast, decomposed pipeline yields clear jump by guiding the model to compose its primitive skills (video understanding, temporal grounding, spatial grounding) into coherent procedure. Adding RL further drives self-evolution, tightening the interplay of these skills for VOS reasoning and delivering substantial additional gains across both datasets. Together, decomposed reasoning and RL posttraining are necessary and complementary, culminating in unprecedented performance. Frame Sampling Strategy. We examine the effect of input frame count during training and inference. As shown in Tab. 6, we train models with 12 / 16 / 20 uniformly sampled frames and observe the diminishing returns beyond 16. Accuracy improves from 12 to 16, while additional frames bring only marginal gains but increase training cost roughly. Balancing efficiency and performance, we adopt 16 frames as the default for both training and evaluation. At test time, performance remains stable across different frame counts, indicating that the reasoning pipeline is robust to temporal Figure 3. Qualitative cases of ReVSeg on ReasonVOS [2]. The frame highlighted in red indicates the selected keyframe. The green bounding box within the enlarged keyframe on the right size represents the grounding result. Zoom in to view visual details. Table 6. Results obtained by employing different frame sampling rates during both the training and testing phases separately. #Training Frames 12 16 20 16 16 #Testing Frames 12 16 20 12 16 20 Ref-DAVIS17 83.8 84.1 84.1 83.9 84.1 84.1 &F 80.3 80.7 80.5 80.3 80.7 80.8 76.8 77.4 77.0 76.6 77.4 77.5 Training Time per Step (s) 603 725 831 Table 7. Model performance on VOS with three different temporal reward types. The referring VOS results were evaluated on the MeViS dataset, while the reasoning VOS results were evaluated on the ReasonVOS dataset. Temporal Reward Type No Reward 0/1 Reward Soft Reward MeViS 58.7 61.2 63.4 &F 54.7 57.4 59.8 ReasonVOS 61.0 62.7 67.7 &F 58.0 59.7 64. 55.0 56.7 61.8 50.7 53.6 56.1 sampling density. Designs of Reward. Temporal rewards are crucial for teaching the model to select keyframes that genuinely fa8 cilitate downstream spatial grounding and mask decoding. Prior keyframe selection works [12, 19, 49, 52] primarily score semantic relevance, which is insufficient for VOS: good frame should also reveal the target with minimal occlusion and adequate scale. We therefore ablate three temporal reward schemes in RL: (1) no temporal reward, (2) binary 0/1 reward that only checks whether the target appears in the selected frame(s), and (3) our soft temporal reward that scores object visibility via normalized bounding box area. As shown in Tab. 7, the soft variant provides graded learning signal aligned with object visibility and scale, alleviates sparse credit assignment, and discourages degenerate selections (e.g., heavily occluded or tiny targets). 5. Conclusion In this work, we reformulate reasoning-centric VOS as sequential decision problem and introduce ReVSeg, which decomposes the task into three pretrained primitive capabilities. We further develop reinforcement learning framework that directly optimizes the reasoning chain, enabling the model to refine its decision quality without relying on dense supervision. Experiments demonstrate state-of-the-art performance across multiple VOS benchstepwise reasoning interpretable, marks and uncover trajectories. We believe that this explicit-chain, outcomedriven formulation provides general paradigm for advancing reasoning-aligned video understanding models."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 5, 6 [2] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2024. 1, 2, 5, 6, 7, 8 [3] Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin. End-to-end referring video object segmentation with multimodal transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49854995, 2022. 1, 2, 5, 6, 7 [4] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 2 [5] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for In Proceedvideo segmentation with motion expressions. ings of the IEEE/CVF international conference on computer vision, pages 26942703, 2023. 2, 5, 6, 7 [6] Zihan Ding, Tianrui Hui, Junshi Huang, Xiaoming Wei, Jizhong Han, and Si Liu. Language-bridged spatial-temporal interaction for referring video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 49644973, 2022. 5, 7 [7] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [8] Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, and Huchuan Lu. The devil is in temporal token: High quality video reasoning segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2918329192, 2025. 1, 2, 5, 6, 7 [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 4 [10] Shuting He and Henghui Ding. Decoupling static and hierarchical motion perception for referring video segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1333213341, 2024. 5, 7 [11] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 2, 3 [12] Kai Hu, Feng Gao, Xiaohan Nie, Peng Zhou, Son Tran, Tal Neiman, Lingyun Wang, Mubarak Shah, Raffay Hamid, Bing Yin, et al. M-llm based video frame selection for efficient video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13702 13712, 2025. 8 [13] Shiu-hong Kao, Yu-Wing Tai, and Chi-Keung Tang. Cotrvs: Zero-shot chain-of-thought reasoning segmentation for videos. arXiv preprint arXiv:2505.18561, 2025. 2, 5, 6, [14] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Asian conference on computer vision, pages 123141. Springer, 2018. 5, 6, 7 [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2 [16] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 1, 2, 5, 6, 7 [17] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 2 [18] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2024. 5, 6 [19] Hao Liang, Jiapeng Li, Tianyi Bai, Xijie Huang, Linzhuang Sun, Zhengren Wang, Conghui He, Bin Cui, Chong Chen, and Wentao Zhang. Keyvideollm: Towards large-scale video keyframe selection. arXiv preprint arXiv:2407.03104, 2024. 8 [20] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into single large language model for video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 86588667, 2025. 1, 2, 5, 6, [21] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 2, 4 [22] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025. 2, 4 [23] Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, and Yujiu Yang. Soc: 9 Semantic-assisted object cluster for referring video object segmentation. Advances in Neural Information Processing Systems, 36:2642526437, 2023. 5 [24] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [25] Bo Miao, Mohammed Bennamoun, Yongsheng Gao, and Ajmal Mian. Spectrum-guided multi-granularity referring video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 920 930, 2023. 5 [26] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 2, 5 [27] Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025. 2 [28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5 [29] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In European conference on computer vision, pages 208223. Springer, 2020. 1, 2, 5, 6, 7 [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4, [31] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 3 Gemma 3 technical report. [32] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 2, 3 [33] Ashwin Vijayakumar, Michael Cogswell, Ramprasath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse arXiv preprint solutions from neural sequence models. arXiv:1610.02424, 2016. 4 [34] Haochen Wang, Cilin Yan, Shuai Wang, Xiaolong Jiang, XU Tang, Yao Hu, Weidi Xie, and Efstratios Gavves. Towards open-vocabulary video instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [35] Haochen Wang, Qirui Chen, Cilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, Weidi Xie, and Stratis Gavves. Object-centric video question answering with visual grounding and referring. In Proceedings of the IEEE/CVF International Confer10 ence on Computer Vision, pages 2227422284, 2025. 1, 2, 5, 6, 7 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [37] Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025. 2 [38] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 4 [40] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87418750, 2021. 1 [41] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Jie Hu, Dengjie Li, Zheng Zhao, and Yujiu Yang. Hyperseg: Hybrid segmentation assistant with fine-grained visual perceiver. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 89318941, 2025. 1, 2, 5, 6, 7 [42] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Hongfa Wang, and Yujiu Yang. Instructseg: Unifying instructed visual segmentation with multi-modal large lanIn Proceedings of the IEEE/CVF Internaguage models. tional Conference on Computer Vision, pages 2019320203, 2025. 1, 2, 5, 6, 7 [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2, 4 [44] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: simple online baseline for referring video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27612770, 2023. 5, [45] Junfeng Wu, Yi Jiang, Wenqing Zhang, Xiang Bai, and Song Bai. Seqformer: frustratingly simple model for video instance segmentation. arXiv preprint arXiv:2112.08275, 2(3): 4, 2021. 1, 2 [46] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segIn Proceedings of the IEEE/CVF Conference mentation. on Computer Vision and Pattern Recognition, pages 4974 4984, 2022. 1, 5, 6, 7 [47] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. 2 [48] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. 2 [49] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and Larry Davis. Adaframe: Adaptive frame selection for In Proceedings of the IEEE/CVF fast video recognition. Conference on Computer Vision and Pattern Recognition, pages 12781287, 2019. 8 [50] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In European Conference on Computer Vision, pages 98115. Springer, 2024. 1, 2, 5, 6, [51] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv. org/abs/2305.10601, 3:1, 2023. 4 [52] Shaojie Zhang, Jiahui Yang, Jianqin Yin, Zhenbo Luo, and Jian Luan. Q-frame: Query-aware frame selection and multi-resolution adaptation for video-llms. arXiv preprint arXiv:2506.22139, 2025. 8 [53] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, and Hengshuang Zhao. Villa: Video reasoning segmentation with large language model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23667 23677, 2025. 1, 2, 5, 6, 7 [54] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [55] Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie. arXiv preprint Tracking with human-intent reasoning. arXiv:2312.17448, 2023. 2, 6,"
        },
        {
            "title": "Appendix",
            "content": "A. Training Curves In Fig. 4, we summarize the dynamics of ReVSeg training. The first row plots the three reward components. Format reward rf in (a) rises sharply to near-perfect score within the first few steps and remains saturated thereafter, indicating that the policy quickly masters the instructed output format and consistently completes the two-round rollout generations. Temporal reward rt and spatial reward rs in (b) and (c) follow similar growth pattern early on, suggesting that initial gains are driven primarily by producing well-formed, parseable responses that expose the temporal-spatial grounding results. As the format reward plateaus, the growth of temporal and spatial rewards decelerates improvements, steming from stronger reasoning rather than formatting, reflecting better temporal selection and tighter localization. The second row reports response length, total reward and rollout turns. Response length in (d) increases during the early phase as the proportion of complete two round rollouts rises, and shows pronounced oscillations between roughly 100 and 250 steps. The analysis of training output reveals several shifts in response policy before settling into to stable reasoning pattern. Total reward in (e) exhibits steady increase over training. Rollout turns in (f), defined as the average number of rounds per sample within batch, quickly converge to two, consistent with the observed trends of the reward components. Overall, these curves indicate well-behaved optimization, ReVSeg promptly adopts the intend two-round policy and continues to improve its reasoning quality in stable manner. (a) (d) (b) (e) (c) (f) Figure 4. Training curves of ReVSeg. (a) Format reward rf rapidly converges to full score and remains saturated. (b) Temporal reward rt and (c) Spatial reward rs increase steadily with training. (d) Response length remains stable overall without collapse. (e) Total reward rises consistently over time. (f) Average number of rollout turns quickly converge to 2. B. Qualitative Results In Fig. 5, we present more visualization examples highlighting the reasoning-centric segmentation capability of ReVSeg. Across diverse scenes, the model analyzes temporal dynamics, including object interactions (e.g., the kittens interact with orange ballon, person holding green towel) and motion cues (e.g., car moving in the opposite direction compared to the motorcycles), while aligning them with the abstract semantics of the user query. It integrates the video evidence with commonsense knowledge (e.g., the concrete mixer truck designed for construction sites) to identify the target entity, selects frames that are favorable for downstream localization (e.g., bright color, centrally located in the frames, visually against the background, size), and converts this selection into keyframe index and concrete object description. Conditioned on this description, ReVSeg produces tight spatial grounding and propagates masks across the video sequence, yielding the fine-grained segmentation masks. 12 Figure 5. Additional qualitative cases of ReVSeg. The frame highlighted in red indicates the selected keyframe. The green bounding box within the enlarged keyframe on the right side represents the grounding result. Zoom in to view visual details. 13 (a) User prompt for first round generation, where {second}, {nf rames} and {question} refer to the video duration /f ps, the frame number of input video sequence and input query x, respectively. (b) User prompt for second round generation, where {object description} refers to the extracted object description from the first round response y1. Figure 6. User prompt templates for two-round rollout. C. Implementation Details In this section, we supplement some implementation details, including user prompts in Sec. C.1 and pseudocode for our proposed decomposed generation with reasoning chain in Sec. C.2. C.1. ReVSeg User Prompt Details To elicit reliable reasoning for video object segmentation, we design task-specific prompting scheme tailored to our tworound rollout of the policy VLM F, as shown in Fig. 6. Each round uses distinct prompt that mirrors the decomposition of the task. Round one: user prompt for video understanding and temporal grounding. The first prompt follows chain-of-thought style to encourage deep analysis. It asks the model to compare salient objects across the video, examine their occurrences over frames, and select keyframe where the target is most suitable for localization. The prompt instructs to first output reasoning trace, then structured result containing the keyframe index and concise object description. Round two: user prompt for spatial grounding. The second prompt provides the keyframe and object description extracted from round one response and requests precise localization on the selected frame. The output must follow JSON format containing tight bounding box. Both prompts request reasoning separated from the final answer, standardized JSON fields to stabilize generation and reduce parsing errors. In practice, this design yields interpretable responses, that serve as robust visual prompts for subsequent mask propagation. C.2. Algorithm We distill the key generation component of ReVSeg into compact pseudocode summarized in Algorithm 1. For each input video-query input, the model performs times two-round rollout sampling and ultimately returns the resulting trajectories set {oi}n i=1. 14 Algorithm 1: Decomposed Generation of ReVSeg with Two-Round Rollout Require: Video = {It}T Ensure : Rollout sequences {oi}n i=1 t=1, query x, policy VLM F, parser G, group size n, prompt template P1 and P2 1 for = 1 to do 2 video understanding + temporal grounding Initialize current VLM input sequence P1(V, x) Initialize current VLM rollout sequence oi // Round-1: Generate the first round response sequence y1 F( z) Extract status flag S1 {succ,fail}, keyframe index and object description through parser (S1, k, d) G(y1) Append y1 to input sequence y1 Append y1 to rollout sequence oi oi y1 if S1 = fail then Terminate rollout generation early and flag status S2 fail, set keyframe Ik and bounding box Bk to Null Ik, Bk null Continue end // Round-2: spatial grounding Select the keyframe Ik from video Ik [k] Append (Ik, d) to the ongoing input sequence P2(Ik, d) Generate the second response sequence y2 F( z) Extract status flag S2 {succ,fail} and spatial grounding result Bk R4 Append y2 to rollout sequence oi oi y2 16 17 end 18 return final generated rollouts {oi}n i=1 0 through parser (S2, Bk) G(y2) 3 4 5 7 8 9 10 11 13 14"
        }
    ],
    "affiliations": [
        "Fudan University",
        "LIGHTSPEED",
        "Shanghai Innovation Institute"
    ]
}