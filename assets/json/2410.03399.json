{
    "paper_title": "EBES: Easy Benchmarking for Event Sequences",
    "authors": [
        "Dmitry Osin",
        "Igor Udovichenko",
        "Viktor Moskvoretskii",
        "Egor Shvetsov",
        "Evgeny Burnaev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 9 9 3 3 0 . 0 1 4 2 : r EBES: EASY BENCHMARKING FOR EVENT SEQUENCES Dmitry Osin, Igor Udovichenko Skolkovo Institute of Science and Technology Moscow, Russia {d.osin, i.udovichenko}@skoltech.ru Egor Shvetsov Skolkovo Institute of Science and Technology Moscow, Russia e.shvetsov@skoltech.ru Viktor Moskvoretskii Skolkovo Institute of Science and Technology HSE University Moscow, Russia v.moskvoretskii@skoltech.ru Evgeny Burnaev Skolkovo Institute of Science and Technology Artificial Intelligence Research Institute Moscow, Russia e.burnaev@skoltech.ru"
        },
        {
            "title": "ABSTRACT",
            "content": "Event sequences, characterized by irregular sampling intervals and mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library 1 simplifies benchmarking, dataset addition, and method integration through unified interface. It includes novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts."
        },
        {
            "title": "Introduction",
            "content": "The world we live in is constantly changing [26]. We continuously collect and analyze data to understand and navigate this dynamic environment. This ongoing data collection helps capture the evolving nature of reality and can be captured in sequential datasets, which can be further analyzed or used for modeling. Various types of sequential data are usually approached differently based on their characteristics. One prevalent form of sequential data is time series, regular measurements of some processes. The uniformity of these intervals enables researchers to apply wide range of developed techniques [16]. Measurements of some processes that are taken or observed at non-uniform time intervals lead to irregularly sampled time series (ISTS). Fewer methods exist specifically for ISTS [16], and modeling them brings new challenges [27]. However, modeling them has considerable importance since they naturally occur in many real-world areas: ecology [10], astronomy [37], climate [38], biology [16], medicine [18, 25, 35], geology [17] and finance [5]. Another widely explored temporal data type is stream of discrete events. Intervals between events are random, and modeling the distribution of inter-event intervals is an essential task with many applications. Temporal point process (TPP) model is commonly employed to model streams of discrete events [14, 30, 33, 24, 39, 51, 54, 53, 42]. *Equal contribution 1The code is available at https://github.com/On-Point-RND/EBES (a) Regularly Sampled Time Series (RSTS). (b) Irregularly Sampled Time Series (ISTS) with missing data. (c) stream of discrete events, usually, modeled by Temporal Point Process (TPP). (d) Event sequence with 2 numerical and 2 categorical features. Figure 1: Sequential data taxonomy. Event sequences (EvS) generalize both irregularly sampled time series and streams of discrete events. In this work, we focus on another type of sequential data, event sequences (EvS), which are sequences of observations made at irregular times characterized by numerical and categorical features. EvS can be viewed as generalization of both ISTS and streams of discrete events. Examples of various types of EvS are illustrated in Figure 1. Many modeling tasks naturally arise when dealing with sequential data, including whole sequence classification and regression [40], extrapolation or forecasting [13], missing data imputation [36], point-wise classification [21], and predicting the next events time and type [48]. Some of these tasks assume either continuous or discrete nature of the data, which may not be known given raw dataset. For instance, predicting the time of the next event is not reasonable when dealing with measurements from continuous process. However, we can perform an assessment of the entire sequence regardless of the assumptions about the nature of the data. As task we consider the whole EvS classification and regression task, which we refer to EvS assessment. We emphasize the crucial role of EvS classification and regression in medicine [40], churn prediction [23], e-commerce [52], fraud detection [46] and more. Our contributions are as follows: We introduce EBES, comprehensive benchmarking framework designed for EvS assessment. EBES features unified interfaces for datasets, models, and experimental protocols, facilitating future research in EvS assessment. Our library is publicly available. We design benchmark protocol that considers both model and dataset analysis. Our evaluation includes various scenarios, including some specific to EvS, highlighting important properties of both the datasets and models. Using EBES, we evaluated various methods on established datasets through multi-phase evaluation protocol. This approach ensures fair and consistent comparison across different methods. All results are tested for statistical significance. As result of our analysis, we provide recommendations for future research. These recommendations include possible pitfalls related to dataset usage and model evaluation."
        },
        {
            "title": "2 Benchmark goals and approaches",
            "content": "Numerous methods have been proposed for EvS modeling and related problems. However, most of these methods lack rigorous evaluation, and there is currently no established benchmark for this domain. Benchmarking machine learning algorithms involves two main components: benchmark design and datasets, each presenting its challenges and goals. Below, we describe how we address each challenge in the context of EvS. 2.1 Datasets We have chosen three commonly used datasets based on previous studies [41, 44, 4, 32], one recent and one of the largest event sequence datasets MBD [15], two medical datasets, and one synthetic pendulum dataset to validate the importance of time and how models capture the sequential properties of the data. We present statistics for each dataset in Table 1, and detailed description of each dataset can be found in Appendix C. 2 Table 1: Statistics of sequential datasets used in our benchmark. The statistics are calculated on the train set if not specified otherwise. We use the following tasks notation: classification (C), regression (R) or multi-label classification (MLC). For MLC we report the average class balance. Dataset PhysioNet 2012 MIMIC-III Pendulum AGE Retail MBD Taobao PhysioNet 2012 MIMIC-III Pendulum AGE Retail MBD Taobao Task C MLC # classes Class balance, % Target Category 2 2 NA 4 4 4 2 2 86 / 14 90 / 10 NA 25 / 25 / 25 / 25 27 / 21 / 27 / 24 99.7 0.2 / 0.3 0.2 43 / 57 Mortality Mortality Air resistance Age group Age group Purchase items Purchase event Medical Medical Physical (synth.) Transactions Transactions Transactions E-commerce # seq. (train / test) # events (train / test) # events per seq. (mean std) # cat. features # num. features 4k / 4k 45k / 11k 80k / 20k 24k / 6k 319k / 80k 7.4m / 1.8m 18k / 9k 299k / 299k 2.7m / 657k 2.5m / 631k 21m / 5.3m 37m / 9.1m 156m / 39m 5.1m / 2.8m 75 23 58 93 32 9 881 125 114 103 21 435 280 387 3 1 0 1 7 11 2 38 10 2 1 9 1 0 Data Quality One of the primary challenges in benchmarking is ensuring that the datasets used are high quality and accurately represent the problem domain. Poor data quality can lead to misleading benchmark results. To address data quality, we employ two strategies: Synthetic Dataset Development: We create synthetic Pendulum dataset, particularly useful for evaluating timesensitive methods; dataset creation is described in Appendix C. Dataset Analysis: We analyze the correlation of model performance with Monte Carlo cross-validation. Specifically, we consider the relationship between metrics across various folds and the holdout test set. Diversity of Datasets. Datasets with similar structures but different domains can vary greatly. For example, financial transactions differ significantly from medical records. Additionally, datasets can vary in complexity and difficulty. Our work includes diverse range: two medical, three banking, one retail, and one synthetic dataset. Volume of Data. Large datasets enable models to capture the complexity and nuances of real-world phenomena, leading to more accurate and reliable predictions. Moreover, different algorithms scale differently as the data grows. To address this challenge, we included datasets of various sizes. Open Access to Data. It is crucial that data is available to researchers worldwide for reproducibility, collaboration, and innovation. While many event-sequence datasets exist, we focus on open-access ones and welcome contributions from other domains to enhance our collection. For example, astronomical observations [6] are event sequences but are not openly accessible. 2.2 Benchmark Design Creating effective benchmarks is complex task, which involves designing tests that accurately reflect the capabilities of machine learning models across different scenarios: Model evaluation. Hyperparameters are fundamental aspect of machine learning that directly impacts model performance. However, the procedure of hyperparameters tuning is rarely described. Therefore, this becomes source of non-reproducibility [3, 20]. Moreover, manual hyperparameter tuning can lead to the leakage of the test set into the training procedure and performance [28], and testing different hyperparameter values is necessary to find model that generalizes well [20]. In our procedure, we first conduct an extensive hyperparameter search. Randomness can destabilize models, causing large variances in results across training runs. Ignoring this sensitivity can create false perception of research progress [34]. Therefore, after determining the optimal hyperparameters, we perform Monte Carlo cross-validation [47] with 20 seeds to evaluate the final model. Scalability As datasets grow larger, machine learning algorithms scale differently. Large datasets enable models to capture real-world nuances, improving prediction accuracy. To address this, we study the scaling properties of various event sequence assessment algorithms. Importance of Time and Sequence Order It is possible to perform EvS assessment while disregarding the temporal and sequential nature of the data. To evaluate the importance of each component, we designed two stress tests for event sequences: rearranging the sequence order and replacing time components with noise. This analysis provides significant insights and highlights future research directions. Model Granularity As AI systems grow more complex, assessing which components contribute to success becomes challenging. In our work, we evaluate different components, such as various aggregation approaches along the temporal dimension, batch normalization, and the impact of adding time as separate feature on overall model performance. 2.3 Benchmark Accessibility and Maintenance The rapid evolution of machine learning makes keeping benchmarks up-to-date challenging. Benchmarks must reflect the latest advancements, incorporate new data and algorithms, and be maintained over time. Our work focuses on developing an easy-to-use plug-and-play codebase to facilitate collaboration and research. The librarys interface structure enforces the independence of implementing new datasets, methods, and experiments, making adding and testing new components easy. We are committed to maintaining this benchmark and encourage contributions from researchers and practitioners to support reproducible research. 2.4 Models We have carefully selected diverse set of popular models and approaches that have been previously applied for EvS assessment. Some of the models, such as MLP, are included as baseline solutions, some are commonly used for sequential data GRU [9], Mamba [19], Transformer [45]. The following models were explicitly designed to handle the unique challenges associated with EvS: mTAND [41], PrimeNet [8] and CoLES [4]. Appendix provides detailed description of each model."
        },
        {
            "title": "3 Benchmarking Methods",
            "content": "3.1 Dataset prepossessing In our work we aim to perform as little preprocessing as possible to preserve the originality of the data in order to prevent data preproccessing from affecting model evaluation. For ease of extensibility, we convert all datasets into single format and release scripts that perform the conversion. Our data preprocessing includes: Applying logarithm to fat-tailed variables, which are selected manually; Rescaling time points to make the time range of all sequences to fall in [0, 1]; For missing values, we propagate them forward for the PhysioNet, MIMIC-III, and Pendulum datasets based on results in [7], and impute with constants for others. We encode categorical features using embedding layer and treat missing values as additional categories. 3.2 Model evaluation and HPO Hyperparameter optimization (HPO) and Monte Carlo cross-validation are at the core of our benchmark design, as they enable us to evaluate numerous design choices and hyperparameters, and to fairly compare models. Furthermore, we derive important insights from multiple HPO runs. Our evaluation procedure is twofold: HPO step, here we perform hyperparameter optimization for all the models for each dataset. After obtaining the set of best hyperparameters (BHP), we use them for the next step. Final evaluation, during this step we train models with BHP 20 times using different seeds and random train and train-val splits. Final metrics are reported as average with standard deviation over 20 runs on test sets after models were trained from scratch. detailed algorithm with all the steps is outlined in Appendix D. 4 Figure 2: Data splits and their usage in our evaluation procedure. Table 2: Model performance obtained using EBES. Results are averaged over 20 runs with the best hyperparameters determined through HPO. Statistically indistinguishable (p > 0.01) results share the same superscripts, indicating the methods rank for each dataset. The best-performing methods for each dataset are highlighted. Methods are sorted according to their average rank across all datasets. Dataset Metric CoLES GRU Age Accuracy 0.634 0.0051 0.626 0.0042 MLEM 0.634 0.0031 0.609 0.0063 Mamba 0.621 0.0062 Transformer 0.582 0.0094 mTAND 0.581 0.0074 MLP 0.583 0.0114 PrimeNet MBD Mean ROC AUC 0.826 0.0012 0.827 0.0011 0.824 0.0013 0.820 0.0034 0.821 0.0024 0.798 0.0026 0.809 0.0015 0.780 0.0067 MIMIC-III ROC AUC 0.902 0.0011 0.901 0.0021 0.899 0.0022 0.895 0.0023 0.894 0.0023 0.888 0.0034 0.881 0.0015 0.887 0.0044 Pendulum R2 0.916 0.0042 0.896 0.0104 0.890 0.0074 0.908 0.0053 0.891 0.0154 0.941 0.0091 0.165 0.0056 0.842 0.0175 PhysioNet2012 ROC AUC 0.840 0.0043,4 0.846 0.0041,2 0.846 0.0071 0.835 0.0064 0.838 0.0083,4 0.841 0.0052,3 0.835 0.0044 0.839 0.0043,4 Retail Accuracy 0.553 0.0021 0.543 0.0022 0.544 0.0022 0.538 0.0033 0.536 0.0063 0.519 0.0035 0.526 0.0024 0.521 0. Taobao ROC AUC 0.713 0.0021 0.713 0.0041 0.713 0.0041 0.693 0.0232 0.692 0.0132,3 0.672 0.0104 0.659 0.0354 0.681 0.0103,4 Train-Val-Test splits For both steps we utilize data splits as follows: train - for training models, train-val - for early stopping procedure, we stop training if the model performance does not improve after several epochs and exceeds patience limit, hpo-val - subset to evaluate the model to update HPO sampler, it does not present in Final evaluation step. Both train-val and hpo-val take 15% from the initial train dataset. See Figure 2 for clarification. For each split we apply on-target stratification. The number of patience steps is different for each dataset due to computational constrains. For datasets, which do not have commonly accepted test sets, we cut 20% as our fixed test set. For HPO we use Optuna [2] Tree-structured Parzen Estimator (TPE). For the main performance metrics of our benchmark, see Section 4.1."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Assessment performance In this section, we address the main question of the benchmark: Which model performs the best? The results are presented in Table 2, where methods are sorted from top to least performing. Along with the mean performance we report methods rank as superscript. We performed pairwise MannWhitney test [29] with HolmBonferroni correction [22], methods with no significant performance difference (p > 0.01) share the same superscript. All top three performing methods are based on GRU with different pre-training strategies. Mamba and Transformer comes next in rating, suggesting that this architectures are less suitable for EvS. mTAND [41] demonstrated best performance on Pendulum dataset, this result can be attributed to the mTAND architecture, which is specifically designed to model the time component. The MLP performs relatively well, typically within 5% of the top-performing method on all real-world datasets. This suggests that EvS assessment can be effectively carried out using aggregated statistics along temporal dimensions, practice commonly employed in industrial applications with boosting models [1]. The difference in performance between MLP and mTAND on the Pendulum dataset further supports this idea, since we can not apply such aggregation approach to this dataset. We see that, all methods show close results on the PhysioNet2012 dataset, based on ranks. This raises questions about its suitability for evaluating models for EvS assessment task. 5 Figure 3: Performance metric relationships and correlations of different subsets among all methods on PhysioNet2012 (top row) and Taobao (bottom row) are presented. We do not observe correlation between the test metric and train-val on PhysioNet2012, as seen in the right upper corner. For the Taobao dataset, we do not observe clear linear trend between hpo-val and the test metric suggesting the presence of distribution shift. 4.2 Dataset Analysis In this section, we analyze datasets based on data from the HPO step and Final evaluation phases, exploring relationships between metrics from different data subsets. Correlations between different subsets for the PhysioNet2012 and Taobao datasets are depicted in Figure 3, with other datasets presented in Appendix E. During the HPO step, we observe overfitting for most datasets, as train metrics increase while train-val metrics plateau, as seen in Figure 3 on the left. This supports the use of early stopping. Metrics of hpo-val and test subsets (third column in Figure 3) are strongly correlated unless the test set is sampled out-of-time, as seen for the Taobao dataset. Here, hpo-val and test metrics lack clear linear trend, but train-val and hpo-val metrics do, suggesting distribution shift in the test set. For most datasets, in the Final evaluation phase (fourth column in Figure 3), validation and test set metrics exhibit linear trend, except for PhysioNet2012, where different validation metrics attribute to similar test metrics. This supports our observations in Section 4.1, where results for most models are not statistically distinguishable for most methods on PhysioNet2012. 4.3 Data Scaling Results To study the scaling properties of various models, we evaluated each model trained with different numbers of sequences. We focused on two biggest real-world dataset in our benchmark: Retail and MBD. We sampled different subsets, each containing progressively more data. Each model was trained from scratch on different-sized subsets with Monte Carlo cross-validation using three random seeds. common approach is to estimate model performance with fixed data size. However, as seen in Figure 4, while all models improve with the growth of the data, their ranking does not stay the same, except for CoLES on the Retail Dataset, where it demonstrates superior performance. With some data size, even MLP becomes top performer. Most models, except for MLP, mTAND, and PrimeNet, converge to similar performance on the MBD dataset given large data size. It is worth noting that for each dataset, we used the BHP found for each model when the dataset was at its full size. 6 (a) MBD dataset (b) Retail dataset Figure 4: Performance of various models as function of number of sequences. Metrics from Table 1 are reported. Number of sequences is presented in log scale. Standard deviation across 3 runs is depicted as vertical lines. The standard deviation, decreases as the data size increases and models perform very differently with smaller subsets. This makes evaluating model performance on relatively small datasets more prone to misleading results. 4.4 Assessing Architecture Design Choices Although our models exhibit diverse range of architectures, there are several common design choices among them. We evaluated the impact of these choices as part of our HPO procedure. We observe that some design choices depend more on the dataset than on the method, highlighting the importance of HPO for fair evaluation. First, we study the effect of different aggregation approaches along the temporal dimension on overall performance. We focus on two approaches: mean across all hidden states and the last sequence state. The best aggregation strategy depends more on the dataset than on the method. Similarly, batch normalization for numerical features improves performance for almost all methods and datasets, except for Pendulum. Finally, we evaluate the importance of hyperparameters according to HPO. There is no clear winner except for the learning rate, which is often the most important hyperparameter across all HPO runs. Results are presented in Tables 9, 8 and 7 in the Appendix. 4.5 Importance of Sequence Order One aspect of EvS is the order of events in sequence. To examine its importance, we conducted two experiments: 1) We took models trained on regular data and evaluated them on test sequences with permuted order, keeping the time component unchanged. 2) We removed the time component and retrained the models on sequences with permuted order, then evaluated them on permuted test sequences. Testing on Permuted Sequences We evaluated pre-trained models from the Final evaluation step on perturbed sequences. Missing values were filled prior to shuffling, and time was added as numerical feature before shuffling. For all runs, the last events were kept in their original positions, as some models use the last hidden state in the aggregation step. Results are presented in Table 3. The Transformer model experienced significantly small drop due to its attention mechanism. The MLP model did not experience any drop at all because sequence order is inherently not important for aggregation. We observed that while performance dropped for other models, the drop was statistically significant (p < 0.01) but less than expected for all real-world datasets. Additionally, the MBD dataset did not experience significant drop with most methods, suggesting that models do not rely on the order of sequences to make predictions. This indicates that while sequence order is important, it is not as critical for EvS assessment of real-world datasets as initially thought. However, we observed that models performance degraded on the pendulum dataset, indicating that the evaluated models can capture the sequential nature of the data. Training on Permuted Sequences The second experiment further analyzed datasets to determine if sequential order is important or if sequences can be treated as \"bag of words.\" We selected the GRU with BHP for each dataset, removed the time component from its architecture, and trained it from scratch with both training and test sequences permuted. The results are in Table 4. We observed that for some real-world datasets, the performance drop was not statistically significant. We speculate that such permutation could Age MBD Dataset Metric Accuracy Mean ROC AUC Table 3: Robustness to sequence permutation results. We report performance difference relative to metrics obtained on not permuted sequences. Models were train on non-permuted data; only the test set was permuted. Values with statistically significant difference (p < 0.01) in performance are highlighted and marked with asterisk. MIMIC-III ROC AUC 1.86% 4.24% 1.43% 0.00% 3.04% 3.72% 0.00% 5.05% Retail Taobao Accuracy ROC AUC 1.57% 0.49% 2.25% 0.67% 2.57% 0.89% 0.00% 0.00% 2.44% 0.00% 26.41% 2.12% 0.09% 0.05% 28.09% 4.13% CoLES 1.63% GRU 1.15% MLEM 1.52% MLP 0.00% Mamba 1.20% PrimeNet 7.82% Transformer 0.00% mTAND 8.95% R2 219.60% 227.58% 242.09% 0.00% 351.14% 128.39% 5.20% 133.66% ROC AUC 2.36% 1.49% 1.71% 0.00% 0.65% 3.95% 0.03% 4.13% 0.09% 0.10% 0.30% 0.00% 0.06% 4.08% 0.00% 5.05% Pendulum PhysioNet2012 Table 4: Comparison of GRU with BHP and the same GRU with the time component removed, retrained on the permuted training set. Statistically significant differences are highlighted and marked with asterisk. Retail Accuracy PhysioNet2012 ROC AUC MBD Mean ROC AUC MIMIC-III ROC AUC Pendulum R2 Taobao ROC AUC Age Accuracy Dataset Metric GRU w/o time w/ perm. Vanilla GRU 0.626 0.004 0.630 0.004 0.827 0.001 0.819 0.001 0.901 0.002 0.890 0.002 0.896 0.010 0.581 0.003 0.846 0.004 0.844 0.005 0.543 0.002 0.546 0.003 0.713 0.004 0.702 0.006 even serve as form of data augmentation, since in some cases mean metrics increased with permutation. Notably, after retraining on permuted sequences, we observed significant drop on the MBD dataset. At first, this seems to contradict the results from the previous section. However, upon considering that the time component was also removed, we conclude that in the MBD dataset, time component is crucial while the order is not. From both experiments, we conclude that sequence order is important for EvS assessment, but it is less critical than expected for real-world datasets and varies from dataset to dataset. 4. Importance of Time Next, we evaluate the role of time in EvS. Similarly to the previous section, we perform two experiments: 1) using random time-steps on pre-trained models during testing, and 2) adding or removing time as an extra feature to train the models. Incorporation of Event Time Information into Models To evaluate the importance of time, we follow simple procedure. First, we note that time is rescaled during preprocessing. After that, there are three options to incorporate it into the model, all of which are searchable during hyperparameter optimization (HPO): No time - Do not use time at all; Time delta - Compute the time difference from the previous step and concatenate it as feature; Absolute time - Concatenate the rescaled time as feature. The results in Table 5 indicate that time significantly improves performance, if added, to three datasets: MBD, MIMICIII, and Pendulum. Surprisingly, it is important for almost all datasets if we use the Transformer. However, we cannot make the claim for other methods and datasets that the time is not important, as there are various other ways to incorporate it into models that may show statistically significant improvements, but we did not explore them. Random Timestamps In our work, two methods are specifically designed to model the time component: mTAND [41] and PrimeNet [8]. We evaluated them on test data with noisy timestamps, where the original timestamps were replaced with random values sorted in ascending order. The results are presented in Table 6. While time is important for these models on the synthetic Pendulum dataset, it did not contribute significantly to the other datasets. From the observations above, we first see that time is important and contributes to EvS assessment. Secondly, we observe that methods specifically designed to work with time do not effectively capture temporal dependencies on real-world datasets. This emphasizes the importance of developing or testing new methods on EvS that can model the time component on real-world datasets. 8 Table 5: Including vs. Excluding time as feature. We take top 3 sets of hyperparameters from HPO step for each option and report test metrics. Highlighted bold if adding time significantly improves performance. MLEM not included since it has fixed time process option - copied from best CoLES CoLES Transformer PrimeNet mTAND Mamba GRU MLP Dataset Time process Age MBD MIMIC-III Pendulum w/o time with time w/o time with time w/o time with time w/o time with time 0.632 0.002 0.633 0. 0.622 0.005 0.629 0.005 0.612 0.002 0.616 0.005 0.587 0.006 0.588 0.005 0.583 0.005 0.588 0.004 0.582 0.006 0.594 0.005 0.609 0.005 0.620 0. 0.817 0.002 0.825 0.000 0.818 0.001 0.826 0.001 0.815 0.001 0.822 0.001 0.801 0.002 0.808 0.000 0.777 0.013 0.797 0.000 0.757 0.011 0.781 0. 0.813 0.001 0.823 0.000 0.902 0.002 0.904 0.001 0.896 0.003 0.897 0.002 0.892 0.001 0.896 0.001 0.869 0.002 0.879 0.001 0.882 0.001 0.890 0. 0.885 0.002 0.888 0.002 0.886 0.001 0.895 0.001 0.621 0.003 0.905 0.002 0.622 0.007 0.895 0.000 0.626 0.004 0.908 0.002 0.160 0.000 0.170 0. 0.893 0.019 0.942 0.002 0.792 0.010 0.852 0.004 0.598 0.003 0.864 0.003 PhysioNet2012 w/o time with time 0.839 0.002 0.843 0. 0.840 0.003 0.841 0.006 0.835 0.001 0.840 0.004 0.841 0.002 0.837 0.004 0.842 0.002 0.845 0.001 0.844 0.001 0.842 0.003 0.834 0.004 0.838 0. Taobao Retail w/o time with time w/o time with time 0.705 0.005 0.712 0.004 0.685 0.014 0.705 0. 0.693 0.004 0.666 0.058 0.637 0.042 0.666 0.018 0.664 0.004 0.679 0.003 0.653 0.007 0.665 0.040 0.702 0.007 0.711 0.002 0.551 0.001 0.551 0. 0.543 0.001 0.543 0.001 0.539 0.001 0.539 0.001 0.525 0.000 0.525 0.002 0.518 0.000 0.519 0.001 0.518 0.001 0.524 0.004 0.530 0.002 0.541 0. Table 6: Trained models evaluation with random timestamps. Values with statistically significant difference (p-value < 0.01) in performance are highlighted and marked with asterisk. Method mTAND Dataset Metric Time Age Accuracy MBD Mean ROC AUC MIMIC-III ROC AUC Pendulum R2 PhysioNet2012 ROC AUC Retail Accuracy Taobao ROC AUC Real 0.582 0.009 Random 0.581 0.009 0.798 0.002 0.795 0.002 0.888 0.003 0.886 0.003 0.941 0.009 0.580 0.067 0.841 0.005 0.840 0.005 0.519 0.003 0.519 0. 0.672 0.010 0.666 0.010 PrimeNet Real 0.583 0.011 Random 0.582 0.010 0.780 0.006 0.775 0.006 0.887 0.004 0.884 0. 0.842 0.017 0.260 0.108 0.839 0.004 0.840 0.004 0.521 0.003 0.521 0.003 0.681 0.010 0.680 0."
        },
        {
            "title": "5 Related Work",
            "content": "The UCR Time Series Archive, widely used for time-series classification, is limited to univariate time series, offering 128 datasets for algorithm evaluation [12]. Despite its extensive use, this benchmark does not address the complexity of event sequence data, crucial for many real-world applications. The torchtime package [11] extends the utility of UEA & UCR datasets by providing reproducible implementations for PyTorch, simplifying data access and ensuring fair model comparisons, it is still primarily focuses on time series classification. EasyTPP [48] is new benchmark targeting streams of discrete events, offering centralized repository for evaluating TPP models. It emphasizes reproducible research through standardized benchmarking framework and provides various research assets. However, EasyTPP cannot be extended to handle general EvS, as event sequences generally cannot be modeled using TPP. The sequence of card transactions made by client is good example of EvS. Each transaction is characterized by attributes such as transaction amount and merchant category code, making them unfit for time series or discrete event streams categories. Authors in [5, 50, 4] evaluate several representation learning approaches on event sequences. In [5], the authors propose protocol for evaluating obtained representations on set of downstream tasks."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we presented EBES, an open and comprehensive benchmark for the standardized and transparent comparison of event sequence models. The benchmark includes diverse range of datasets and models. Additionally, it provides user-friendly interface and rich library, allowing for the easy integration of new datasets and the implementation of new models. With these features, EBES has the potential to facilitate future research in event sequence modeling significantly. We emphasize the importance of HPO and cross-validation for fair model evaluation. Moreover, we recommend performing several runs to validate if the model performance is statistically significant, especially on small datasets. This is also supported by scaling experiments, where model rankings tend to change significantly on smaller data sizes and slowly converge to the same point as the data size grows while the standard deviation decreases. 9 Our analysis of datasets highlights two crucial points. We found that results on the PhysioNet2012 dataset are not statistically distinguishable. Therefore, future researchers should be cautious when deriving conclusions for EvS assessment based on results obtained with this dataset. Another observation is that out-of-time data splits naturally tend to have distribution shift, and one should account for it during model validation and HPO. For example, this appears in the low correlation between the Taobao datasets validation and test metric values. We demonstrate that the importance of time and the sequential nature of the data varies for real-world datasets concerning EvS assessment. Similarly, different models capture these properties differently. Developing or testing models that inherently account for the time component on real-world data could be promising direction for future research."
        },
        {
            "title": "7 Limitations",
            "content": "We acknowledge that conducting full HPO (Hyperparameter Optimization) process requires substantial computational resources, which may not be available to all users. The development of more efficient strategies for proper model evaluation could be promising direction for future research. Our work focuses solely on one taskEvS assessment while there are various tasks applied to EvS. We leave this for future work."
        },
        {
            "title": "8 Reproducibility Statement",
            "content": "We made available all the code necessary to run our experiments and generate the corresponding figures. Additionally, we include raw logs from all experiments, including valuable data obtained during the HPO process. Each experiment was conducted with fixed random seeds, ensuring that model training yields consistent results when the same seeds are used. Our code repository includes: Configuration files with specifications for the HPO process. Implementations of all the methods mentioned in this paper, along with their best hyperparameters. complete data preprocessing pipeline for each dataset used in our study. By following the instructions provided in our repository, you should be able to reproduce our results accurately."
        },
        {
            "title": "References",
            "content": "[1] Lahcen ABIDAR, Dounia ZAIDOUNI, EL Ikram, and Abdeslam ENNOUAARY. Predicting customer segment changes to enhance customer retention: case study for online retail using machine learning. International Journal of Advanced Computer Science and Applications, 14(7), 2023. [2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 26232631, 2019. [3] Christian Arnold, Luka Biedebach, Andreas Küpfer, and Marcel Neunhoeffer. The role of hyperparameters in machine learning models and how to tune them. Political Science Research and Methods, pp. 18, 2023. [4] Dmitrii Babaev, Nikita Ovsov, Ivan Kireev, Maria Ivanova, Gleb Gusev, Ivan Nazarov, and Alexander Tuzhilin. Coles: contrastive learning for event sequences with self-supervision. In Proceedings of the 2022 International Conference on Management of Data, pp. 11901199, 2022. [5] Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan Kireev, Andrey Savchenko, and Alexey Zaytsev. Universal representations for financial transactional data: embracing local, global, and external contexts, 2024. [6] Rodrigo Carrasco-Davis, Guillermo Cabrera-Vives, Francisco Förster, Pablo Estévez, Pablo Huijse, Pavlos Protopapas, Ignacio Reyes, Jorge Martínez-Palomera, and Cristóbal Donoso. Deep learning for image sequence classification of astronomical events. Publications of the Astronomical Society of the Pacific, 131(1004):108006, 2019. [7] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):6085, 2018. 10 [8] Ranak Roy Chowdhury, Jiacheng Li, Xiyuan Zhang, Dezhi Hong, Rajesh K. Gupta, and Jingbo Shang. Primenet: Pre-training for irregular multivariate time series. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):71847192, Jun. 2023. doi: 10.1609/aaai.v37i6.25876. URL https://ojs.aaai.org/index.php/ AAAI/article/view/25876. [9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. [10] James Clark and Ottar Bjørnstad. Population time series: process variability, observation errors, missing values, lags, and hidden states. Ecology, 85(11):31403150, 2004. [11] Philip Darke, Paolo Missier, and Jaume Bacardit. Benchmark time series data sets for pytorchthe torchtime package. arXiv preprint arXiv:2207.12503, 2022. [12] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):12931305, 2019. [13] Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous modeling of sporadically-observed time series. Advances in neural information processing systems, 32, 2019. [14] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 15551564, 2016. [15] Mollaev Dzhambulat, Alexander Kostin, Postnova Maria, Ivan Karpukhin, Ivan Kireev, Gleb Gusev, and Andrey Savchenko. Multimodal banking dataset: Understanding client needs through event sequences, 2024. URL https://arxiv.org/abs/2409.17587. [16] Andreas Eckner. framework for the analysis of unevenly spaced time series data. Preprint. Available at: http://www. eckner. com/papers/unevenly_spaced_time_series_analysis, pp. 93, 2012. [17] Wenqian Fang, Lihua Fu, Mengyi Wu, Jingnan Yue, and Hongwei Li. Irregularly sampled seismic data interpolation with self-supervised learning. Geophysics, 88(3):V175V185, 2023. [18] Ary Goldberger, Luis AN Amaral, Leon Glass, Jeffrey Hausdorff, Plamen Ch Ivanov, Roger Mark, Joseph Mietus, George Moody, Chung-Kang Peng, and Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of new research resource for complex physiologic signals. circulation, 101(23):e215e220, 2000. [19] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [20] Odd Erik Gundersen, Kevin Coakley, Christine Kirkpatrick, and Yolanda Gil. Sources of irreproducibility in machine learning: review. arXiv preprint arXiv:2204.07610, 2022. [21] Ramin Hasani, Mathias Lechner, Alexander Amini, Lucas Liebenwein, Aaron Ray, Max Tschaikowski, Gerald Teschl, and Daniela Rus. Closed-form continuous-time neural networks. Nature Machine Intelligence, 4(11): 9921003, 2022. [22] Sture Holm. simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pp. 6570, 1979. [23] Hemlata Jain, Ajay Khunteta, and Sumit Srivastava. Telecom churn prediction and used techniques, datasets and performance measures: review. Telecommunication Systems, 76:613630, 2021. [24] Junteng Jia and Austin Benson. Neural jump stochastic differential equations. Advances in Neural Information Processing Systems, 32, 2019. [25] Alistair EW Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger Mark. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19, 2016. [26] Diogenes Laertius. Lives of eminent philosophers, translated by RD Hicks. Loeb Classical Library, 2:408423, 1925. [27] Steven Cheng-Xian Li and Benjamin Marlin. Learning from irregularly-sampled time series: missing data perspective. In International Conference on Machine Learning, pp. 59375946. PMLR, 2020. [28] Michael Lones. How to avoid machine learning pitfalls: guide for academic researchers. arXiv preprint arXiv:2108.02497, 2021. [29] Henry Mann and Donald Whitney. On test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, pp. 5060, 1947. 11 [30] Hongyuan Mei and Jason Eisner. The neural hawkes process: neurally self-modulating multivariate point process. Advances in neural information processing systems, 30, 2017. [31] Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko, Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, and Evgeny Burnaev. Self-supervised learning in event sequences: comparative study and hybrid approach of generative modeling and contrastive learning. arXiv preprint arXiv:2401.15935, 2024. [32] Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko, Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, and Evgeny Burnaev. Self-supervised learning in event sequences: comparative study and hybrid approach of generative modeling and contrastive learning, 2024. [33] Takahiro Omi, Kazuyuki Aihara, et al. Fully neural network based model for general temporal point processes. Advances in neural information processing systems, 32, 2019. [34] Branislav Pecher, Ivan Srba, and Maria Bielikova. survey on stability of learning with limited labelled data and its sensitivity to the effects of randomness. ACM Computing Surveys, 2024. [35] Matthew Reyna, Christopher Josef, Russell Jeter, Supreeth Shashikumar, Brandon Westover, Shamim Nemati, Gari Clifford, and Ashish Sharma. Early prediction of sepsis from clinical data: the physionet/computing in cardiology challenge 2019. Critical care medicine, 48(2):210217, 2020. [36] Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent ordinary differential equations for irregularlysampled time series. Advances in neural information processing systems, 32, 2019. [37] Jeffrey Scargle. Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of unevenly spaced data. Astrophysical Journal, Part 1, vol. 263, Dec. 15, 1982, p. 835-853., 263:835853, 1982. [38] Michael Schulz and Karl Stattegger. Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series. Computers & Geosciences, 23(9):929945, 1997. [39] Oleksandr Shchur, Marin Biloš, and Stephan Günnemann. Intensity-free learning of temporal point processes. In International Conference on Learning Representations, 2019. [40] Satya Narayan Shukla and Benjamin Marlin. Interpolation-prediction networks for irregularly sampled time series. In International Conference on Learning Representations, 2018. [41] Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled time series. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= 4c0J6lwQ4_. [42] Yujee Song, LEE Donghyun, Rui Meng, and Won Hwa Kim. Decoupled marked temporal point process using neural ordinary differential equations. In The Twelfth International Conference on Learning Representations, 2024. [43] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding, 2021. [44] Igor Udovichenko, Egor Shvetsov, Denis Divitsky, Dmitry Osin, Ilya Trofimov, Ivan Sukharev, Anatoliy Glushenko, Dmitry Berestnev, and Evgeny Burnaev. Seqnas: Neural architecture search for event sequence classification. IEEE Access, 12:38983909, 2024. doi: 10.1109/ACCESS.2024.3349497. [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [46] Yu Xie, Guanjun Liu, Chungang Yan, Changjun Jiang, and MengChu Zhou. Time-aware attention-based gated network for credit card fraud detection by extracting transactional behaviors. IEEE Transactions on Computational Social Systems, 2022. [47] Qing-Song Xu and Yi-Zeng Liang. Monte carlo cross validation. Chemometrics and Intelligent Laboratory Systems, 56(1):111, 2001. [48] Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Hongyan Hao, Fan Zhou, Caigao Jiang, Chen Pan, James Y. Zhang, Qingsong Wen, Jun Zhou, and Hongyuan Mei. EasyTPP: Towards open benchmarking temporal point processes. In International Conference on Learning Representations (ICLR), 2024. URL https://arxiv.org/ abs/2307.08097. [49] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative adversarial networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings. neurips.cc/paper_files/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf. [50] Aleksandr Yugay and Alexey Zaytsev. Uniting contrastive and generative learning for event sequences models. arXiv preprint arXiv:2408.09995, 2024. [51] Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. Self-attentive hawkes process. In International conference on machine learning, pp. 1118311193. PMLR, 2020. [52] Shiwei Zhao, Runze Wu, Jianrong Tao, Manhu Qu, Minghao Zhao, Changjie Fan, and Hongke Zhao. percltv: general system for personalized customer lifetime value prediction in online games. ACM Transactions on Information Systems, 41(1):129, 2023. [53] Vladislav Zhuzhel, Vsevolod Grabar, Galina Boeva, Artem Zabolotnyi, Alexander Stepikin, Vladimir Zholobov, Maria Ivanova, Mikhail Orlov, Ivan Kireev, Evgeny Burnaev, et al. Continuous-time convolutions model of event sequences. arXiv preprint arXiv:2302.06247, 2023. [54] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer hawkes process. In International conference on machine learning, pp. 1169211702. PMLR, 2020."
        },
        {
            "title": "B Models description",
            "content": "GRU We have chosen to use the GRU as one of our base models due to its proven effectiveness in encoding timeordered sequences [4, 36, 43, 49, 44]. In recent study on neural architecture search [44]), authors demonstrated that architectures with RNN blocks tend to exhibit higher performance on average on EvS assesment task. MLP The models applied 3 linear layers with the ReLU nonlinearity and dropouts in between to the aggregated embeddings obtained right after the preprocessing block. So effectively the model is just basic MLP applied to aggregations. Models for EvS handles the sequential nature of data in special way, ofthen considering the exact time intervals between the events, so we were interested in the performance of the model, that consciously discards the sequential nature of data. Mamba Mamba [19] is recent state-space model (SSM) that has been designed for efficient handling of complex, long sequences. It incorporates selective state spaces to deliver top-notch performance across different modalities, including language, audio, and genomics, outperforming Transformers in some scenarios. For the best of our knowledge Mamba has not been applied to EvS assessment previously, however, we believe that type of models worth of investigating. mTAND Authors in [41] proposed an architecture which learns an embedding of continuous-time values and utilizes an attention mechanism to produce fixed-length representation of time series. This procedure is specifically designed to deal with ISTS and has been shown to outperform numerous ordinary differential equations-based models such as Latent ODE and ODE-RNN [36]. CoLES The contrastive pretraining method for sequential data was proposed by [4]. We specifically focus on this method due to its superior performance compared to other contrastive approaches demonstrated in the work. CoLES learns to encode sequence into latent vector by bringing sub-sequences of the same sequence closer in the embedding space while pushing sub-sequences from different sequences further apart. PrimeNet The method proposed in [8] also, falls under the category of self-supervised. It utilizes time-sensitive contrastive pretraining and enhances pretraining procedure with data reconstruction tasks to facilitate the usage of unlabeled data. Authors modify mTAN architecture by replacing an RNN block with Feature-Feature Attention. MLEM The Multimodal Learning Event Model [31] is recently proposed method for Event Sequences that unifies contrastive learning with generative modeling. It treats generative pre-training and contrastive learning as distinct modalities. First, contrastive encoder is trained, followed by an encoder-decoder that learns latent states using reconstruction loss while aligning with contrastive embeddings to enhance the embedding information."
        },
        {
            "title": "C Datasets Description",
            "content": "PhysioNet2012 dataset2 was first intruduced in [18]. It includes multivariate time series data with 37 variables gathered from intensive-care unit (ICU) records. Each record contains measurements taken at irregular intervals during the first 48 hours of ICU admission. We used set-a as train set and set-b as test set. Both sets contain 4000 labeled sequences. MIMIC-III dataset3 [25] consists of multivariate time series data featuring sparse and irregularly sampled physiological signals, collected at Beth Israel Deaconess Medical Center from 2001 to 2012. While we aimed to follow the general pipeline outlined in [40], we made several modifications to enhance the accuracy and reproducibility of our approach. Importantly, we did not alter the original problem statement: we excluded series that last less than 48 hours and used the first 48 hours of observations from the remaining series to predict in-hospital mortality. These adjustments were necessary to address certain issues and improve the overall robustness of our analysis. Age dataset4 consists of 44M anonymized credit card transactions representing 50K individuals. The target is to predict the age group of cardholder that made the transactions. The multiclass target label is known only for 30K records, and within this subset the labels are balanced. Each transaction includes the date, type, and amount being charged. The dataset was first introduced in scientific literature in work [4]. Retail dataset5 comprises 45.8M retail purchases from 400K clients, with the aim of predicting clients age group based on their purchase history. Each purchase record includes details such as time, item category, the cose, and loyalty program points received. The age group information is available for all clients, and the distribution of these groups is balanced across the dataset. The dataset was first introduced in scientific literature in work [4]. MBD is multimodal banking dataset introduced in [15]. The dataset contains an industrial-scale number of sequences, with data from more than 1.5 million clients. Each client corresponds to sequence of events. This multi-modal dataset includes card transactions, geo-position events, and embeddings of dialogs with technical support. The goal is to predict the purchases of four banking products in each month, given the historical data from the previous month. For our analysis, we use only card transactions. Since we focused on the event sequence assessment task, we restricted our setup as follows. To predict the purchases, we use transactions from the preceding month. For example, we use sequence from June to predict label by the last day of July. We did not use out-of-time validation, as the labeled time span of the data is less than year. The authors of the dataset split the data into 5 folds (04), we use fold 4 as the test fold. Pendulum Inspired by [32] we created pendulum dataset to evaluate time-dependent models. The dataset simulates damped pendulum motion with varying lengths. Observation times are sampled irregularly using Hawkes process, emphasizing the importance of accurate event timing for real-world applications. Each sequence in the dataset consists of events represented by time and two normalized coordinates (x, y), with some values randomly dropped. The goal is to predict the damping factor. We publish the reproducible code to generate the dataset. To model the Hawkes process, we consider the following intensity function λ(t) that is given by (1). λ(t) = µ + αeβ(tti) (cid:88) ti<t (1) We used following parameters for the Hawkes process: µ is the base intensity; α is the excitation factor, was chosen to be 0.5; β is the decay factor, was set to 1. ti are the times of previous events before time t. 2https://physionet.org/content/challenge-2012/1.0.0/ 3https://physionet.org/content/mimiciii/1.4/ 4https://ods.ai/competitions/sberbank-sirius-lesson 5https://ods.ai/competitions/x5-retailhero-uplift-modeling 14 The time points are sampled within the interval [0, end time], where the end time is sampled from uniform distribution (3, 5). To maintain an approximately constant number of points (30) per sequence, we adjust the base intensity µ as follows: µ = 30 1 α end time 1 This ensures each sequence has dynamic time interval but approximately the same number of points, preventing the model from learning the timestamp distribution without using timestamp data. To model the pendulum we consider the second-order differential equation: θ + (cid:19) (cid:18) θ + (cid:17) (cid:16) sin(θ) = 0 (2) where, θ is the Angular Acceleration, θ is the Angular Velocity, θ is the Angular Displacement, is the Damping Factor, = 9.81 m/s2 is the acceleration due to gravity, is the Length of pendulum, is the Mass of bob in kg. To convert this second-order differential equation into two first-order differential equations, we let θ1 = θ and θ2 = θ, which gives us: 2 = θ = θ θ2 (cid:19) (cid:18) θ 1 = θ2 (cid:17) (cid:16) sin(θ1) Thus, the first-order differential equations for the pendulum simulation are: (cid:19) (cid:18) θ2 (cid:17) (cid:16) sin(θ1) θ 2 = θ 1 = θ2 (3) (4) (5) (6) In our simulations, the damping factor is sampled from uniform distribution (1, 3), and the mass of the bob = 1. The length of the pendulum is taken from uniform distribution (0.5, 10), representing range of possible lengths from 0.5 to 10 meters. The initial angular displacement θ is sampled from uniform distribution (0, 2π), and the initial angular velocity θ is sampled from uniform distribution (π, π), providing range of initial conditions in radians and radians per second, respectively. Our primary objective is to predict the damping factor b, using the normalized coordinates and on the plane. These coordinates are scaled with respect to the pendulums length, such that the trajectory of the pendulum is represented in unitless fashion. This normalization allows us to abstract the pendulums motion from its actual physical dimensions and instead focus on the pattern of movement. Additionally, we randomly drop 10% of values for both coordinates. An illustrative example of this motion is presented in Figure 5."
        },
        {
            "title": "D HPO details",
            "content": "Hyperparameter Optimization (HPO) is critical step in the development and evaluation of machine learning models. It involves systematically searching for the optimal set of hyperparameters that maximize model performance. In this section, we outline our main evaluation methodology and HPO process, which is detailed in Algorithm 1. Our approach includes two main steps: the HPO step and the final evaluation step. In the HPO step, we use the Tree-structured Parzen Estimator (TPE) to efficiently search the hyperparameter space. We split the training dataset into 15 Figure 5: Pendulum motion at various instances, with time steps determined by Hawkes process. three subsets: train (70%), train-val (15%), and hpo-val (15%). The model is trained on the train set, and its performance is evaluated on the train-val set to determine when to stop training. The hpo-val set is used to update the TPE sampler and guide the selection of hyperparameters. After the HPO step, we proceed to the final evaluation step. Here, we use the best hyperparameters (BHP) identified in the HPO step to train and evaluate the model multiple times with different random seeds. This ensures that our results are robust and not dependent on particular random initialization. The training dataset is split into train (85%) and train-val (15%) sets, and the model is trained until performance on the train-val set stops improving or until the training budget is exhausted. Finally, we evaluate the model on the test set and report the mean and standard deviation of the test metrics. For more details about the HPO process, we refer to our Algorithm 1. Algorithm 1 Our main evaluation methodolgy and HPO, here Nhpo - is HPO budget, axIters - training budget, Nseeds - number of iterations for random seed runs. 1: axIters = 105 2: Nseeds = 20 3: start HPO step 4: split train dataset randomly into three subsets train (70%), train-val (15%) and hpo-val (15%) 5: initalize TPE 6: for = 1, 2, . . . , Nhpo do 7: 8: set model hyper parameters with TPE train model until performance on train-val set stops improving or until we run out from the budget axIters. update TPE sampler using metrics obtained on hpo-val 9: 10: end for 11: select best hyper parameters (BHP) according to hpo-val metrics 12: Start Final evaluation step 13: for seed = 1, 2, . . . , Nseeds do set new random seed 14: randomly split train dataset into train (85%) and train-val (15%) sets 15: train model with BHP until performance on train-val set stops improving or until we run out from the 16: budget axIters. evaluate the model on test set 17: 18: end for 19: Report mean and std of test metrics from Final evaluation step"
        },
        {
            "title": "E Subsets metric relationships",
            "content": "Figure 6: Performance metric relationships and correlations of different subsets among all methods on Age dataset Figure 7: Performance metric relationships and correlations of different subsets among all methods on MBD dataset Figure 8: Performance metric relationships and correlations of different subsets among all methods on MIMIC-III dataset 17 Figure 9: Performance metric relationships and correlations of different subsets among all methods on Pendulum dataset Figure 10: Performance metric relationships and correlations of different subsets among all methods on PhysioNet2012 dataset Figure 11: Performance metric relationships and correlations of different subsets among all methods on Taobao dataset 18 Figure 12: Performance metric relationships and correlations of different subsets among all methods on Retail dataset"
        },
        {
            "title": "F HPO analisys",
            "content": "This section presents comprehensive evaluation of different aggregation and normalization approaches, as well as the importance of learning rates, for various models across multiple datasets. Table 7 compares two aggregation methods: using the last hidden state and the mean of all hidden states. The results indicate that the choice of aggregation method can significantly impact model performance. For instance, in the Age dataset, the mean hidden state approach improves performance for models like GRU and Mamba, while the last hidden state approach is more effective for mTAND. Similarly, Table 8 evaluates the impact of batch normalization on input features. The results show that batch normalization can enhance model performance in many cases. Additionally, Table 9 ranks the importance of learning rate hyperparameter for different models and datasets using Optuna. The rankings highlight that the learning rate is critical hyperparameter, with its importance varying across different dataset and model combinations. For example, the learning rate is ranked highest for Mamba across all datasets, indicating its significant impact on model performance. These findings provide valuable insights into the optimal configuration of models for different datasets and can guide future research in hyperparameter optimization. Table 7: Different aggregation approaches: mean across all hidden states or last hidden state. We take top 3 sets of hyperparameters from HPO step for each option and report test metrics. Highlighted bold if adding time significantly improves performance. CoLES GRU Mamba MLEM MLP mTAND Transformer Dataset Aggregation Age MBD MIMIC-III Pendulum PhysioNet Taobao Retail Last hidden Mean hidden Last hidden Mean hidden Last hidden Mean hidden Last hidden Mean hidden Last hidden Mean hidden Last hidden Mean hidden Last hidden Mean hidden 0.631 0.001 0.630 0.004 0.825 0.000 0.821 0.002 0.904 0.001 0.897 0.002 0.905 0.002 0.903 0.002 0.843 0.002 0.827 0.002 0.712 0.004 0.698 0.017 0.551 0.001 0.546 0.001 0.616 0.004 0.629 0.005 0.826 0.001 0.822 0.001 0.897 0.002 0.894 0.002 0.884 0.003 0.895 0.000 0.841 0.006 0.803 0.011 0.705 0.010 0.696 0.026 0.543 0.000 0.541 0.001 0.593 0.003 0.616 0.005 0.822 0.001 0.822 0.001 0.889 0.001 0.896 0.001 0.886 0.007 0.908 0.002 0.840 0.004 0.806 0.009 0.671 0.038 0.666 0.058 0.528 0.000 0.539 0. 0.637 0.004 0.628 0.008 0.823 0.001 0.820 0.003 0.897 0.001 0.895 0.001 0.889 0.001 0.890 0.004 0.844 0.004 0.826 0.013 0.714 0.002 0.707 0.003 0.545 0.002 0.540 0.002 0.340 0.002 0.588 0.005 0.756 0.000 0.808 0.000 0.879 0.001 0.875 0.001 0.130 0.001 0.170 0.000 0.837 0.004 0.808 0.001 0.611 0.028 0.666 0.018 0.342 0.001 0.525 0.002 0.588 0.004 0.579 0.001 0.797 0.000 0.787 0.001 0.890 0.005 0.885 0.003 0.942 0.002 0.899 0.025 0.844 0.000 0.844 0.001 0.679 0.003 0.675 0.002 0.518 0.001 0.519 0.001 0.600 0.008 0.617 0.004 0.819 0.000 0.823 0.000 0.895 0.001 0.888 0.003 0.848 0.006 0.864 0.003 0.838 0.003 0.831 0.011 0.711 0.001 0.711 0.002 0.537 0.001 0.541 0.003 19 Table 8: Different normalization approaches: with vs without Batch Normalization for input features. We take top 3 sets of hyperparameters from HPO step for each option and report test metrics. Highlighted bold if adding time significantly improves performance. CoLES GRU Mamba MLEM MLP mTAND PrimeNet Transformer Dataset Normalization Age MBD MIMIC-III Pendulum PhysioNet2012 Taobao Retail with norm 0.629 0.002 w/o norm 0.636 0.005 with norm 0.825 0.000 w/o norm 0.823 0.000 with norm 0.904 0.001 w/o norm 0.884 0.004 with norm 0.872 0.002 w/o norm 0.905 0.002 with norm 0.843 0.002 w/o norm 0.775 0.009 with norm 0.712 0.004 w/o norm 0.706 0.001 with norm 0.551 0.001 w/o norm 0.539 0.004 0.628 0.006 0.624 0.003 0.826 0.001 0.822 0.001 0.897 0.002 0.882 0.006 0.853 0.006 0.895 0.000 0.841 0.006 0.781 0.012 0.705 0.010 0.703 0.006 0.543 0.000 0.523 0. 0.616 0.005 0.614 0.007 0.822 0.001 0.819 0.000 0.896 0.001 0.880 0.005 0.884 0.003 0.908 0.002 0.840 0.004 0.832 0.003 0.666 0.058 0.685 0.019 0.539 0.001 0.521 0.003 0.636 0.004 0.639 0.006 0.811 0.010 0.823 0.001 0.897 0.001 0.880 0.006 0.844 0.004 0.892 0.002 0.844 0.004 0.749 0.011 0.714 0.002 0.709 0.000 0.545 0.002 0.530 0.003 0.588 0.005 0.582 0.002 0.808 0.000 0.806 0.001 0.879 0.001 0.849 0.008 0.144 0.000 0.170 0.000 0.837 0.004 0.814 0.009 0.666 0.018 0.568 0.068 0.525 0.002 0.511 0.001 0.588 0.004 0.585 0.003 0.787 0.002 0.797 0.000 0.890 0.005 0.877 0.001 0.921 0.001 0.942 0.002 0.845 0.001 0.808 0.010 0.679 0.003 0.654 0.015 0.519 0.001 0.515 0.003 0.584 0.008 0.594 0.005 0.778 0.003 0.778 0.005 0.886 0.000 0.888 0.002 0.829 0.013 0.852 0.004 0.844 0.001 0.835 0.009 0.665 0.040 0.655 0.009 0.524 0.004 0.518 0.001 0.616 0.010 0.617 0.004 0.822 0.001 0.822 0.001 0.895 0.001 0.874 0.001 0.864 0.004 0.859 0.002 0.838 0.003 0.787 0.006 0.711 0.002 0.708 0.001 0.541 0.003 0.435 0. Table 9: Learning Rate Importance by Optuna Ranking (Smaller Rank = Higher Importance). There is unique best Learning Rate for each Dataset/Method combination Age MBD MIMIC-III Pendulum PhysioNet2012 Taobao Retail"
        },
        {
            "title": "CoLES\nGRU\nMamba\nMLEM\nMLP\nmTAND\nPrimeNet\nTransformer",
            "content": "1 2 1 2 2 1 11 1 10 1 1 3 1 1 1 4 2 2 1 6 3 1 1 7 3 4 1 4 1 3 1 10 1 1 1 1 2 1 1 3 4 1 1 1 1 1 1 7 3 1 11 5 1 2"
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute",
        "HSE University",
        "Skolkovo Institute of Science and Technology"
    ]
}