{
    "paper_title": "I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models",
    "authors": [
        "Zhenxing Mi",
        "Kuan-Chieh Wang",
        "Guocheng Qian",
        "Hanrong Ye",
        "Runtao Liu",
        "Sergey Tulyakov",
        "Kfir Aberman",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents ThinkDiff, a novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as a proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of a diffusion decoder. This proxy task builds on the observation that the $\\textbf{LLM decoder}$ shares the same input feature space with $\\textbf{diffusion decoders}$ that use the corresponding $\\textbf{LLM encoder}$ for prompt embedding. As a result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff."
        },
        {
            "title": "Start",
            "content": "I Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Zhenxing Mi 1 Kuan-Chieh Wang 2 Guocheng Qian 2 Hanrong Ye 1 Runtao Liu 1 Sergey Tulyakov 2 Kfir Aberman 2 Dan Xu 1 5 2 0 2 2 1 ] . [ 1 8 5 4 0 1 . 2 0 5 2 : r Figure 1. (a) Our ThinkDiff reasons over interleaved images (a flying monkey and flying cat) and text prompts (monkey, cat, and zebra) to generate logically correct and high-quality image (a flying zebra). The ground truth reasoning answer is provided as reference for readers. (b) ThinkDiff composes images and texts into coherent and reasonable image. Abstract This paper presents ThinkDiff, novel alignment paradigm that empowers text-to-image diffusion models with multimodal in-context understanding and reasoning capabilities by integrating the strengths of vision-language models (VLMs). Existing multimodal diffusion finetuning methods largely focus on pixel-level reconstruction rather than in-context reasoning, and are constrained by the complexity and limited availability of reasoning-based datasets. ThinkDiff addresses these challenges by leveraging vision-language training as proxy task, aligning VLMs with the decoder of an encoder-decoder large language model (LLM) instead of diffusion decoder. This proxy task builds on the observation that the LLM decoder shares the same input feature space with diffusion decoders that use the corresponding LLM encoder for prompt embedding. As result, aligning VLMs with diffusion decoders can be simplified through alignment with the LLM decoder. Without complex training and datasets, ThinkDiff effectively unleashes understanding, reasoning, and composing capabilities in diffusion models. Experiments demonstrate that ThinkDiff 1Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST). 2Snap Inc. Correspondence to: Dan Xu <danxu@cse.ust.hk>. significantly improves accuracy from 19.2% to 46.3% on the challenging CoBSAT benchmark for multimodal in-context reasoning generation, with only 5 hours of training on 4 A100 GPUs. Additionally, ThinkDiff demonstrates exceptional performance in composing multiple images and texts into logically coherent images. Project page: https://mizhenxing.github.io/ThinkDiff . 1. Introduction Can diffusion models take IQ tests? Figure 1a presents an example of visual analogy IQ test. The model is provided with images of flying monkey and flying cat, along with text prompts of monkey, cat, and zebra, and asked to generate the next image. reasonable output image should be an image of flying zebra, requiring the models ability to reason and recognize implicit patterns in context, such as the shared attribute of the flying action in this example. The concept of enabling diffusion models to think and then generate is compelling yet underexplored. Current text-toimage diffusion models (AI, 2024c; Forest, 2024a) excel at generating high-quality images by strictly following explicit prompts, while typically lacking multimodal in-context reasoning. Unlocking reasoning capabilities in them can enable them to handle more sophisticated tasks, such as interpreting complex instructions, solving visual analogy problems that require inferring implicit logic relationships, and composing multiple images and text in logically consistent manner. 1 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 3. Several diffusion models share language encoder with encoder-decoder LLMs, allowing aligning with diffusion decoders through aligning with LLM decoders. Pix2Pix (Brooks et al., 2023) dataset primarily focus on image editing tasks, lacking the diversity needed for reasoningbased generation tasks. Finally, finetuning diffusion models for reasoning from scratch using limited datasets constrains their performance across broad range of reasoning tasks. To tackle these challenges, we propose ThinkDiff, novel alignment paradigm to transfer multimodal in-context reasoning capabilities from VLMs to diffusion models. Instead of directly aligning VLMs with diffusion decoder, we design proxy task to align VLMs with large language model (LLM) decoder by vision-language training. The foundation of this proxy task is depicted in Figure 3. Recent diffusion models (AI, 2024b; Chen et al., 2024; Forest, 2024a; AI, 2024c) have adopted the encoder of an encoder-decoder LLM (Raffel et al., 2020) as diffusion models prompt encoder. This shared text encoder establishes shared input feature space for both the diffusion decoder and LLM decoder. Therefore, aligning VLM with diffusion decoder can be achieved by the proxy task of aligning VLM with the LLM decoder by vision-language training. Figure 2b depicts the vision-language training in ThinkDiff. The input images and text prompts are processed by VLM and an aligner network, after which they are fed into an LLM decoder. The LLM decoder generates text autoregressively, supervised by cross-entropy loss against ground truth texts. After training, the VLM is aligned to the LLM decoder, and inherently to the diffusion decoder. Our method offers several advantages. First, it fully leverages the multimodal in-context understanding and reasoning capabilities of VLMs without requiring expensive training from scratch. Second, by aligning multimodal features to the input space of the LLM decoder through fine-grained text supervision, the model effectively captures rich semantic details from multimodal inputs, enabling seamless collaboration between vision and text modalities. Finally, ThinkDiff is lightweight, efficient and highly versatile. The vision-language training in it only requires readily available image-caption pairs, eliminating the need for complex reasoning-based datasets while achieving remarkable incontext reasoning capabilities. This paper introduces two variants of ThinkDiff, each using different VLM. ThinkDiff-LVLM aligns generated Figure 2. (a) Reconstruction-based diffusion finetuning integrates image features using diffusion loss, focusing on pixel-level image reconstruction without reasoning. (b) ThinkDiff aligns VLM to an LLM decoder by visionlanguage training on image-caption datasets. In inference (dotted lines), it transfers multimodal in-context reasoning capabilities from the VLM to diffusion decoder. With rapid advancements in vision-language models (VLMs) such as CLIP (Radford et al., 2021) and GPT-like models (Radford et al., 2018), we now have powerful tools for advanced multimodal understanding and reasoning. This leads us to question: can we equip diffusion models with the reasoning capabilities of VLMs? Existing multimodal diffusion adapters (Zhang et al., 2023; Ye et al., 2023; Mou et al., 2024) primarily rely on reconstruction-based diffusion finetuning to incorporate visual conditions into text-to-image diffusion models. Figure 2a illustrates the typical training pipeline of IP-Adapter (Ye et al., 2023), where the model is finetuned to replicate input images at the pixel level. While effective for pixel-level control and high-fidelity image generation, adapting this finetuning paradigm to support in-context reasoning introduces several challenges. First, this multimodal finetuning primarily focuses on pixel-level reconstruction of explicit image inputs rather than performing multimodal reasoning based on input context. Second, the pixel-level reconstruction training does not focus on aligning vision representations with the textual feature space, limiting the models ability to reason effectively across modalities. Third, instead of readily available image-caption pairs, it requires multimodal reasoning datasets that pair multimodal inputs with logically consistent output images and cover different reasoning tasks. Collecting such datasets is significantly more complex than captioning images. Existing instruction-guided datasets such as the synthetic Instruct2 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models tokens of large vision-language model (LVLM) to diffusion models. ThinkDiff-CLIP aligns image tokens from CLIP vision encoder (Radford et al., 2021) to diffusion models. Our contributions are summarized as follows: We propose ThinkDiff, novel alignment paradigm that equips diffusion models with multimodal incontext reasoning capabilities from VLMs. ThinkDiff designs proxy task to align VLMs into shared feature space of both an LLM decoder and diffusion decoder by vision-language training, fully transferring VLMs reasoning capabilities to diffusion models with efficient training and simple datasets. We address the poor convergence problem in ThinkDiff for robust feature alignment. After training for only 5 hours on 4 A100 GPUs, ThinkDiff improves state-ofthe-art accuracy on the major visual in-context learning benchmark (Zeng et al., 2024) from 19.2% to 46.3%. It also demonstrates powerful abilities to compose multiple images and texts into logically coherent images. 2. Related Work 2.1. Diffusion models Diffusion models have become powerful tools for text-toimage generation (Ho et al., 2020; Rombach et al., 2022; Forest, 2024a). Early models, e.g. Stable Diffusion (Rombach et al., 2022), use CLIP (Radford et al., 2021) for prompt embedding, while recent works integrate large language models (LLMs) (Saharia et al., 2022; Chen et al., 2024; AI, 2024c) for complex prompts. Methods such as ControlNet (Zhang et al., 2023), T2I-Adapter (Mou et al., 2024), and IP-Adapter (Ye et al., 2023) introduce structural and image-level controls by reconstruction-based fine-tuning. Personalized generation has been enhanced by methods like DreamBooth (Ruiz et al., 2023), and other methods (Gal et al., 2023; Wang et al., 2024a; Li et al., 2024; Wang et al., 2024c; Qian et al., 2024; Wang et al., 2024d), some of which use interleaved image-text inputs (Pan et al., 2023; Berman & Peysakhovich, 2024). However, these methods focus on reconstruction fidelity rather than in-context reasoning. In contrast, our method equips diffusion models with the multimodal in-context reasoning capabilities of VLMs. 2.2. Unified understanding and generation Recent work on large language models (LLMs) and diffusion transformers (Peebles & Xie, 2023; Forest, 2024a) has inspired unified models for multimodal understanding and generation. These models either finetune LLMs to generate image tokens, which are then decoded into images via diffusion decoders (Ge et al., 2024; Pan et al., 2023; Sun et al., 2023; Koh et al., 2024; Wu et al., 2023; Ye et al., 2024), or integrate text, image, and noise tokens within transformer architecture (Xiao et al., 2024; Shi et al., 2024). They are typically trained end-to-end with diffusion losses or align output image tokens with CLIP text features using cosine similarity losses (Wu et al., 2023; Ye et al., 2024; Tong et al., 2024). While some methods exhibit preliminary reasoning capabilities, these capabilities remain constrained by the limits of diffusion training paradigms, the availability of reasoning datasets, and the representational limits of CLIP embeddings. In contrast, our method leverages vision-language training to transfer advanced multimodal reasoning capabilities in VLMs to diffusion models. 2.3. Vision-language training Vision-language training has proven effective in developing powerful multimodal models. CLIP-like models (Radford et al., 2021; Fang et al., 2023) use contrastive learning to align image and text embeddings. Recent large visionlanguage models (LVLMs)(Li et al., 2023; Liu et al., 2023; Zhu et al., 2023; AI, 2024a; Wang et al., 2024b) align CLIP visual features with advanced large language models (LLMs)(Brown et al., 2020; Achiam et al., 2024; AI, 2024a; Yang et al., 2024a) by fine-grained text prediction. This vision-language training enables robust multimodal feature alignment, developing multimodal understanding and reasoning by leveraging powerful LLMs. Inspired by these advancements, our method employs vision-language training as proxy task to bridge VLMs with diffusion models, inheriting their advanced multimodal reasoning capabilities. 3. Method 3.1. Overview ThinkDiff employs VLMs to enable diffusion decoders to perform multimodal in-context reasoning. This is achieved by an aligner network that bridges VLM and diffusion decoder. As described in Section 1, ThinkDiff simplifies the alignment process by introducing proxy task that aligns the VLM with an LLM decoder using text supervision. This task is based on the shared input feature space between the LLM decoder and diffusion decoder. Figure 2b and Figure 4 illustrate the overall network structure and two model variants, respectively. The multimodal input comprises . The aligner netset of images Ii} Ti} { { work processes its input token features into its output xi} { . In training, ThinkDiff generates text token features i} { . In tokens inference, it generates an image . , supervised by ground truth text tokens and text tokens yi} { i} { Module Overview. ThinkDiff comprises three submodules: AN), and source VLM ( VLM), an aligner network ( LLMD) in decoder. The decoder is LLM decoder ( training and diffusion decoder ( DiffD) in inference. M Source VLM. The source VLM generates multimodal toI Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 4. (a) In ThinkDiff-LVLM training, the LVLM processes an image and text to generate text tokens and token features, with some token features randomly masked. Unmasked token features are passed to trainable aligner network and an LLM decoder, predicting masked text tokens supervised by cross-entropy loss. In inference, the LLM decoder is replaced by diffusion decoder, enabling in-context reasoning image generation from interleaved images and texts. (b) In ThinkDiff-CLIP training, CLIP vision model extracts image token features which are then mapped by trainable aligner network. part of the image caption is encoded by the LLM encoder and concatenated with image tokens. These combined tokens are passed to the LLM decoder to predict the next part of the caption supervised by cross-entropy loss. In inference, the LLM decoder is replaced by diffusion encoder, allowing coherent image generation based on multimodal context. = xi} { xi} { ken features , capturing the reasoning and understanding derived from multimodal inputs and transferring this information to diffusion decoders. The generation is ex- ). This paper introVLM( pressed as: , Ii} { duces two variants of ThinkDiff, each utilizing different VLM. ThinkDiff-LVLM uses large vision-language model (LVLM) to deliver advanced multimodal reasoning capabilities while ThinkDiff-CLIP leverages the semantically rich image embeddings provided by CLIP vision encoder for image understanding. Detailed descriptions of these variants can be found in Sections 3.3 and 3.4. Ti} { Aligner network. The aligner network bridges the source VLM with the LLM and diffusion decoder. It transforms , which encapsulate rich reasoning infortoken features xi} { , making them interpretable by the LLM mation, into i} { and diffusion decoder. This transformation is represented as: = AN( i} { ). xi} { Decoder. The decoder operates differently during training LLMD) is central to and inference. The LLM decoder ( ThinkDiffs vision-language training. It is derived from an encoder-decoder LLM. In this LLM, the LLM encoder encodes token features and the LLM decoder generates text autoregressively from these token features. In ThinkDiff training, the VLM token features are mapped to by the aligner network. The LLM decoder then treats i} { as if they were outputs from the LLM encoder and i} { . This process autoregressively decodes them into text i} { ). By this training, is expressed as: i} { VLM token features are aligned with the decoders input space, transferring reasoning capabilities from the VLM to i} { xi} { LLMD( = LLMD in training and to DiffD in inference. In inference, the LLM decoder is replaced by diffusion DiffD), which can interpret VLMs outputs and decoder ( leverage the VLMs multimodal reasoning abilities for image generation. ThinkDiff can handle multiple images, texts, or interleaved sequences of images and texts during inference, thanks to their shared feature space. The generated image is given by = ). DiffD( { i} yi} { Loss. We employ cross-entropy loss between the LLM and the ground truth text decoders generated tokens i} tokens , the in training. Let be the length of (cid:80)N loss is defined as: Ltext = 1 In the following sections, we detail the design of the aligner network and two variants of ThinkDiff. i} { = yi). i=1 log p(y { 3.2. Aligner network AN is lightweight module comprisThe aligner network GELU) ing two linear layers ( Linear), GELU activation ( and an RMSNorm layer (Zhang & Sennrich, 2019) ( Norm). AN is: Given the VLMs output )))) In training, only AN is updated. Despite its simplicity, AN can effectively aligns feature spaces of the powerful of i} { Linear( xi} { xi} { Linear( , the output i} { GELU( Norm( (1) = VLM and the LLM decoder in the training. Stable training. Our experiments revealed that without carefully initialized RMSNorm layer, ThinkDiff encounters convergence issues due to scale mismatch between the VLM output space and the LLM decoder input space. To address this, we incorporate an RMSNorm (Zhang & Sennrich, 2019) layer into AN, initialized with parameters from the LLM encoders final RMSNorm layer. Since the LLM encoder output space aligns naturally with the LLM decoder input space, this initialization ensures consistent scale alignment at the start of training, significantly improving training stability and convergence. 4 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models 3.3. ThinkDiff-LVLM ThinkDiff-LVLM incorporates decoder-only large visionlanguage model (LVLM) that excels at advanced in-context reasoning tasks, as its VLM. It aligns the deep features of the LVLMs generated tokens to both DiffD. LLMD and M AN and yi} { Training. The training framework is illustrated in Figure 4a. The LVLM autoregressively generates text tokens from an input image and text prompt . The corresponding are extracted from the LVLMs final token features xi} { are then passed to RMSNorm layer. These features xi} { LLMD, where they are decoded into text tokens , supervised by LVLMs generated tokens . This yi} i} { setup is self-supervised, as both the token features xi} { and the supervision are all generated by the LVLM itself. This enables the aligner network to accurately transfer information from the LVLM to have one-toHowever, in this setup, token features one correspondence with the supervision text tokens . yi} This may cause the aligner to learn trivial mapping between without truly aligning features. We yi} { refer to this issue as shortcut mapping. LLMD and xi} { yi} { xi} DiffD. and { { { Random masked training. To address the shortcut mapping issue, we introduce random masked training strategy. are ranand features In this strategy, text tokens xi} yi} { { x2 x1 y2 y1 , , and , domly split into two parts: } } } } { { { y2 x1 where correspond to and correspond to } } { { x1 x2 is passed to the aligner and } } { { LLM decoder, generating text tokens supervised by the i} { . This breaks the one-to-one corsecond part of tokens respondence, encouraging more robust feature alignment. The generated tokens . Only the first part are computed as: y1 } { { { y2 } i} AN(fmask( { = i} LLMD( LVLMG(I, )))), (2) { where fmask is the random masking and LVLMG is the LVLMs generation process. The cross-entropy loss is: LLVLM = i=1 log p(y = y2 (cid:80)N ). 1 Why use LVLMs generated tokens. Some diffusion models (Liu et al., 2024; Xie et al., 2024) incorporate decoderonly LLMs for prompt encoding but actually treat them as encoders by using the deep features of input tokens. In contrast, ThinkDiff-LVLM uses the deep features of the generated tokens from the LVLM decoder as input to the aligner. This design is motivated by the insight that, in autoregressive models, reasoning is embedded in the generation process. Tokens are generated sequentially, conditioned on both the input context and the prior generated tokens. As result, the full sequence of generated tokens captures the models logical reasoning about the input context. By aligning these generated token features with diffusion models, ThinkDiff-LVLM ensures that the diffusion models inherit the LVLMs advanced multimodal reasoning capabilities. 5 Inference for in-context reasoning. In inference, as shown in Figure 4a, the LLM decoder is replaced by diffusion decoder for image generation. As shown in Figure 1a and 5, ThinkDiff-LVLM effectively leverages the LVLMs multimodal in-context reasoning capability, using the context to generate highof interleaved images quality, logically coherent images that go beyond simply reconstructing the input content. The generated image is: and texts Ti} { Ii} { = DiffD( AN( LVLMG( { , Ii} ))) Ti} { (3) 3.4. ThinkDiff-CLIP ThinkDiff-CLIP employs the vision encoder of CLIP vision-language model (Radford et al., 2021) pretrained on contrastive vision-language tasks, as its VLM. This encoder produces semantically rich image features, enabling aligned diffusion decoders to generate images based on the semantic understanding of input images. { { to ti} xi} i} xi} { Training. Figure 4b illustrates the training framework. The model is trained to predict partial captions for an input image. The CLIP vision encoder encodes the input image into image tokens , which are downsampled via 2D pooling to reduce token count. The aligner network then maps . Meanwhile, the image caption is randomly split into two parts: T1 and T2. The first part, T1, by the LLM encoder. is encoded into text token features The aligned image tokens , ti} i} { { and fed to the LLM decoder, which autoregressively predicts text supervised by the second caption part T2 (tokens i} { y2 ). The text generation process is formulated as: } i} { (4) LLME is the where fcat denotes concatenation, and LLM encoder. The cross-entropy loss is: LCLIP = ). After training, the aligned imcapture semantic details of the input image LLMD and i=1 log p(y age tokens i} { and can be interpreted by both are concatenated with LLME(T1))), LLMD(fcat( CLIP(I)), = y2 DiffD. (cid:80)N AN( M 1 = { { Inference. In inference, as shown in Figure 4b, the LLM decoder is replaced by diffusion decoder for image generation. As shown in Figure 1b, 6, 8, and 13, with an image as input, ThinkDiff-CLIP preserves semantic details of this image in the generated image. With multiple input images and text prompts, it seamlessly combines them into semantically coherent image, as both image and text features are well-aligned within shared feature space. These results highlight ThinkDiff-CLIPs ability to understand and compose multimodal context. In contrast, reconstructionbased diffusion finetuning methods like FLUX Ultra (Forest, 2024a), often struggle to simultaneously adhere to image and text prompts. The generation of ThinkDiff-CLIP is: = Ti} { DiffD(fcat( Ii} { LLME( CLIP( AN( M ))) )), (5) Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 5. 2-shot evaluation results on CoBSAT. The input structure is similar to Figure 1a. Given multimodal inputs, ThinkDiff-LVLM accurately captures both implicit attributes (e.g., wicker material) and explicit attributes (e.g. car), and generates logically correct image (wicker car). In contrast, methods such as SEED-LLaMA (Ge et al., 2024), Emu (Sun et al., 2023) and GILL (Koh et al., 2024) produce inaccurate and lower-quality images. The ground truth implicit attribute is highlighted in red for readers reference. See more results in Appendix Figure 9 and 10. Table 1. 2-shot CoBSAT accuracy of ThinkDiff-LVLM. It achieves SoTA accuracy on 9 of 10 tasks by large margins, increasing accuracy by more than 20% on Action-I, Color-II, Action-II tasks which are particularly hard for other methods. Style-II Action-II Texture-II Style-I Action-I Texture-I Color-II Background-II Color-I Background-I 0.680 SEED-LLaMA 0.065 Emu GILL 0.171 ThinkDiff-LVLM 0. 0.348 0.051 0.054 0.349 0.203 0.057 0.069 0.237 0.182 0.052 0.063 0.459 0.196 0.078 0.074 0.290 0.287 0.062 0.010 0.511 0.467 0.109 0.043 0. 0.297 0.081 0.024 0.340 0.261 0.092 0.022 0.534 0.163 0.074 0.040 0.292 4. Experiments 4.1. Implement details Base models. We use publicly available FLUX.1-dev (Forest, 2024a) as the diffusion decoder as it employs T5 (Raffel et al., 2020), an LLM, as its prompt encoder. We use the LLMD. ThinkDiff-LVLM corresponding T5 decoder as uses Qwen2-VL (Wang et al., 2024b) as the VLM, which excels at vision-language reasoning on interleaved images and texts. ThinkDiff-CLIP employs the vision encoder from the ViT-G/14 model of EVA-CLIP (Fang et al., 2023). Training and evaluation resources. We use public imagecaption datasets for training. ThinkDiff-LVLM is trained for 25,000 steps on 4 A100 GPUs for 5 hours, with total batch size of 96. ThinkDiff-CLIP is trained for 100,000 steps on 4 A100 GPUs by one day, with total batch size of 168. See Appendix for detailed dataset settings. The multimodal in-context reasoning capabilities of ThinkDiff-LVLM are evaluated on the challenging CoBSAT benchmark (Zeng et al., 2024) and measured by prediction accuracy. More details are in its paper. We assess ThinkDiff-CLIPs reasoning and composition abilities on various prompts and images from (Ruiz et al., 2023; Peng et al., 2024; Ye et al., 2023). Baselines. We compare ThinkDiff-LVLM with SEEDLLaMA (Ge et al., 2024), Emu (Sun et al., 2023) and GILL (Koh et al., 2024) that can generate images based on image and text inputs. SEED-LLaMA is the previous state-of-the-art (SoTA) model on the CoBSAT benchmark. We compare ThinkDiff-CLIP with FLUX1.1-proUltra API (Forest, 2024b), which supports image generation from image and text inputs. FLUX1.1-pro-Ultra is possibly finetuned by diffusion training and image reconstruction supervision, which differs fundamentally from our method. 4.2. Evaluation results of ThinkDiff-LVLM We evaluate ThinkDiff-LVLM on the 10 multimodal incontext reasoning generation tasks in the CoBSAT, in both 2-shot and 4-shot settings. In each setting, 2 or 4 input images and corresponding texts are provided as input, with an additional instruction prompt to make the model generate the next image that contains the correct object and attribute, based on in-context reasoning, (see Appendix Section B). Tables 1 and 2 report the accuracy for 2-shot and 4-shot evaluations, respectively. Results of SEED-LLaMA (Ge et al., 2024), Emu (Sun et al., 2023) and GILL (Koh et al., 2024) are token from the CoBSAT (Zeng et al., 2024) paper. As shown in Table 1 for 2-shot evaluation, ThinkDiff-LVLM achieves SoTA performance on 9 out of 10 tasks, outperforming other methods by large margin. Baselines like Emu and GILL perform poorly on most tasks with accuracy below 10%, reflecting the difficulty of these tasks. While SEED-LLaMA performs well on task Color-I, it underperforms ThinkDiff-LVLM on other tasks. Notably, ThinkDiffLVLM exceeds the previous SoTA by over 20% in accuracy on Action-I, Color-II, and Action-II tasks, showcasing its superior in-context reasoning generation capabilities. More importantly, in the more complex 4-shot evaluation  (Table 2)  , ThinkDiff-LVLM further demonstrates its supe6 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 6. Generation results for single image (I) and single image with text prompt (I + T) inputs. Our method effectively integrates semantic details of both image and text modalities to produce coherent images. FLUX excels at replicating the input image but struggles to maintain consistency with additional text prompts. See more results in Figure 11. Table 2. 4-shot CoBSAT accuracy of ThinkDiff-LVLM shows 27% average improvement over other methods and 4.7% increase over its 2-shot results, highlighting its ability to handle complex in-context reasoning. In contrast, SEEDLLaMA (Ge et al., 2024), Emu (Sun et al., 2023), and GILL (Koh et al., 2024) exhibit reduced performance in 4-shot evaluations, indicating their struggle with increased input complexity. Improvement ratios over SoTA are also provided. Color-I Background-I Style-I Action-I Texture-I Color-II Background-II Style-II Action-II Texture-II SEED-LLaMA Emu GILL Ours 0.482 0.063 0.106 0.638 0.211 0.018 0.044 0. 0.141 0.045 0.041 0.254 0.053 0.048 0.073 0.434 0.122 0.097 0.087 0.317 0.252 0.037 0.022 0.610 0.076 0.122 0.059 0.590 0.268 0.109 0.044 0. 0.207 0.077 0.032 0.664 0.105 0.088 0.067 0.332 Improvement (%) 32.4% 71.6% 80.1% 718.9% 159.8% 142.1% 676.3% 61.2% 220.8% 216.2% ThinkDiff-LVLM generates both correct and significantly higher-quality images compared to other methods. 4.3. Evaluation results of ThinkDiff-CLIP We evaluate ThinkDiff-CLIP on various test cases to demonstrate its ability to semantically understand images and enable coherent composing of image and text modalities. Single image + text prompt. Figure 6 and Appendix Figure 11 show results with single image as input. FLUX Ultra (Forest, 2024b), possibly finetuned by reconstructionbased diffusion training, performs well in copy-pasting the input image (FLUX Ultra + I), but struggles to maintain coherence when an additional text prompt is included (FLUX Ultra + + T). In contrast, ThinkDiff-CLIP excels at understanding the semantic details of the input image and effectively integrates both image and text to generate logically coherent outputs (Ours + and Ours + + T). Multiple images + text prompt. ThinkDiff-CLIP is flexible and can handle multiple images and text prompts. As shown in Figure 8 and Appendix Figure 13, it can combine semantic details from two images in reasonable and coherent manner. Figure 13 further demonstrates that with an additional text prompt (Ours + 2I + T), ThinkDiff-CLIP Figure 7. Training losses (log scale) of ThinkDiff-LVLM comparing different RMSNorm designs. Disabling RMSNorm (w/o RMSNorm) or using the default RMSNorm initialization (RMSNorm w/ Default init.) results in significantly unstable training. rior performance, outperforming all methods across every task, with an average accuracy improvement of 27%. Notably, it also shows consistent 4.7% accuracy increase over its 2-shot performance, highlighting its ability to effectively leverage additional complex information. In contrast, the accuracy of baselines drops significantly with 4-shot inputs, indicating their difficulties with the increased complexity of multimodal inputs. This underscores that ThinkDiff-LVLM not only excels in advanced in-context reasoning but also adapts more effectively to complex multimodal inputs. Figures 5, 9, and 10 present the qualitative comparison, where 7 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Table 3. 2-shot results on CoBSAT ablating models with and without masking, and using deep features of input tokens. Color-I Background-I Style-I Action-I Texture-I Color-II Background-II Style-II Action-II Texture-II Ours using input tokens Ours w/o masked training Ours 0.024 0. 0.622 0.004 0.215 0.349 0.03 0.105 0.237 0.011 0. 0.459 0.032 0.187 0.290 0.007 0.510 0.511 0.008 0. 0.534 0.012 0.156 0.340 0.019 0.325 0.534 0.011 0. 0.292 Table 4. Training resources and 4-shot accuracy. ThinkDiffLVLM drastically reduces GPU usage and training time and improves accuracy from 0.192, 0.07, and 0.058 to 0.463. GPU No. Time / Average Acc. SEED-LLaMA Emu GILL 64 A100 128 A100 2 A6000 ThinkDiff-LVLM 4 216 48 48 5 0.192 0.070 0.058 0.463 Figure 8. Results of ThinkDiff-CLIP composing two images. It creatively merge semantic details of both images. See more results in Appendix Figure 12. effectively incorporates the prompt into the generation. These multimodal generation results highlight the advantage of our vision-language training, which aligns multimodal features into shared space, enabling flexible handling of complex multimodal understanding and composing tasks. Video generation. ThinkDiff-CLIP is agnostic to diffusion decoders, and is versatile for integrating models like CogVideoX-5B (Yang et al., 2024b), text-to-video diffusion model. As shown in Appendix Figure 14, background image is fed to the vision encoder and aligner network, along with text prompt, and then to CogVideoX decoder. The model generates coherent video by seamlessly integrating images and text. This shows ThinkDiff-CLIPs flexibility and broad applicability for multimodal generation tasks. 4.4. Ablation study RMSNorm in the aligner network. As discussed in Section 3.2, the RMSNorm layer and its initialization are critical for training convergence. Figure 7 compares training losses of three setups: without RMSNorm layer, with default ini8 tialization, and with our final design. Without RMSNorm layer or using default initialization, the training loss fails to converge while with our design, the loss converges to reasonable value, leading to strong evaluation performance. This comparison validates the effectiveness of our design. Random masked training strategy. As discussed in Section 3.3, we introduce masked training strategy to address the shortcut mapping problem in ThinkDiff-LVLM training. In Table 3, we compare the 2-shot accuracy on CoBSAT benchmark for models trained with and without this strategy. Without the random masked training, ThinkDiff-LVLM converges quickly but achieves inferior evaluation accuracy, indicating incomplete feature space alignment. In contrast, with the random masked training, the model achieves SoTA accuracy on the evaluation tasks. This validates the critical role of the random masked training for proper feature alignment in ThinkDiff-LVLM. Using generated tokens of LVLM. As discussed in Section 3.3, ThinkDiff-LVLM uses deep features of generated tokens from the LVLM to effectively transfer reasoning information to diffusion decoders. In this study, we train model using the deep features of input tokens of LVLM for alignment, with these features extracted from the final normalization layer of the LVLM. As shown in Table 3, using input token features for alignment leads to significant performance drop, underscoring the critical role of generated tokens in successfully transferring reasoning capabilities. Training time and GPU usage. Table 4 summarizes the training time, GPU requirements, and 4-shot average accuracy on CoBSAT for different methods. Our method drastically reduces GPU usage from 128 A100 GPUs to just 4 and cuts training time from 216 hours to only 5 hours. Meanwhile, it achieves significant improvement in average accuracy, increasing from 0.192, 0.070, and 0.058 to an impressive 0.463. These results highlight the efficiency and effectiveness of our novel alignment paradigm. 5. Conclusion We introduced ThinkDiff, novel alignment paradigm equipping diffusion models with multimodal in-context reasoning of VLMs by vision-language training. ThinkDiff sets new SoTA on the CoBSAT benchmark and excels in various reasoning tasks. Future work will address its limitations (Appendix A), and extend its capabilities to modalities like audio and video to develop any-to-any foundation models. Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models 6. Impact Statements This paper proposed ThinkDiff, novel alignment method that enhances text-to-image diffusion models by integrating multimodal in-context reasoning capabilities from visionlanguage models. By simplifying the alignment process between the VLM and diffusion decoder, ThinkDiff democratizes complex multimodal reasoning generation tasks and make them more accessible and efficient to train. ThinkDiff has potential applications across different fields, such as education, design, and creative industries. However, similar to other text-to-image diffusion models and large visionlanguage models, ThinkDiff could be potentially misused for generating misleading and harmful content. To mitigate these problems, it is essential to deploy the model responsibly and implement robust safeguards to prevent misuse."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., and et. al., S. A. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2024. Llama 3: Vision and edge ai AI, M. bile blog/llama-3-2-connect-2024-vision -edge-mobile-devices/, 2024a. for mohttps://ai.meta.com/ devices. AI, Text-to-image Deepfloyd https://stability.ai/news/ S. model. deepfloyd-if-text-to-image-model, 2024b. if: AI, S. Stable diffusion 3.5. https://github.com/ Stability-AI/sd3.5, 2024c. GitHub repository. Berman, W. and Peysakhovich, A. Mumu: Bootstrapping multimodal image generation from text-to-image data. arXiv preprint arXiv:2406.18790, 2024. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. NeurIPS, 2020. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pp. 3558 3568, 2021. Chen, J., Jincheng, Y., Chongjian, G., Yao, L., Xie, E., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. 9 Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., and Cao, Y. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR, pp. 1935819369, 2023. Forest, B. Flux. black-forest-labs/flux, 2024a. repository. https://github.com/ GitHub Forest, B. Flux ultra. https://blackforestlabs. ai/ultra-home, 2024b. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. Ge, Y., Zhao, S., Zeng, Z., Ge, Y., Li, C., Wang, X., and Shan, Y. Making llama see and draw with seed tokenizer. In ICLR, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. Koh, J. Y., Fried, D., and Salakhutdinov, R. R. Generating images with multimodal language models. NeurIPS, 36, 2024. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In SOSP, pp. 611626, 2023. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pp. 1973019742. PMLR, 2023. Li, Z., Cao, M., Wang, X., Qi, Z., Cheng, M.-M., and Shan, Y. Photomaker: Customizing realistic human photos via stacked id embedding. In CVPR, pp. 86408650, 2024. Liu, B., Akhgari, E., Visheratin, A., Kamko, A., Xu, L., Shrirao, S., Souza, J., Doshi, S., and Li, D. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, 2023. Mou, C., Wang, X., Xie, L., Wu, Y., Zhang, J., Qi, Z., and Shan, Y. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, volume 38, pp. 42964304, 2024. Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned photographs. NeurIPS, 24, 2011. Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Pan, X., Dong, L., Huang, S., Peng, Z., Chen, W., and Wei, F. Kosmos-g: Generating images in context with arXiv preprint multimodal large language models. arXiv:2310.02992, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, pp. 41954205, 2023. language models for multimodal generation. preprint arXiv:2412.15188, 2024. arXiv Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. GenarXiv preprint erative pretraining in multimodality. arXiv:2307.05222, 2023. Peng, Y., Cui, Y., Tang, H., Qi, Z., Dong, R., Bai, J., Han, C., Ge, Z., Zhang, X., and Xia, S.-T. Dreambench++: human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. Tong, S., Fan, D., Zhu, J., Xiong, Y., Chen, X., Sinha, K., Rabbat, M., LeCun, Y., Xie, S., and Liu, Z. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Qian, G., Wang, K.-C., Patashnik, O., Heravi, N., Ostashev, D., Tulyakov, S., Cohen-Or, D., and Aberman, K. Omni-id: Holistic identity representation designed for generative tasks. arXiv preprint, 2024. Wang, K.-C., Ostashev, D., Fang, Y., Tulyakov, S., and Aberman, K. Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation. In SIGGRAPH Asia, pp. 112, 2024a. Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding by generative pretraining. OpenAI, 2018. URL https://openai. com/research/language-unsupervised. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Radford, A., Kim, J. W., Hallacy, C., Ramesh, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Wang, Q., Bai, X., Wang, H., Qin, Z., Chen, A., Li, H., Tang, X., and Hu, Y. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024c. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494, 2022. Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, pp. 2556 2565, 2018. Wang, X., Zhou, X., Fathi, A., Darrell, T., and Schmid, C. Visual lexicon: Rich image features in language space. arXiv preprint arXiv:2412.06774, 2024d. Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S. NextarXiv preprint llm. gpt: Any-to-any multimodal arXiv:2309.05519, 2023. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Xie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang, Z., Li, M., Zhu, L., Lu, Y., and Han, S. Sana: Efficient high-resolution image synthesis with linear diffusion transformer, 2024. URL https://arxiv.org/ abs/2410.10629. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, arXiv preprint et al. Qwen2.5 technical report. arXiv:2412.15115, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Shi, W., Han, X., Zhou, C., Liang, W., Lin, X. V., Zettlemoyer, L., and Yu, L. Llamafusion: Adapting pretrained Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: Text compatible image prompt adapter for text-to-image 10 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models diffusion models. 2023. arXiv preprint arXiv:2308.06721, Ye, H., Huang, D.-A., Lu, Y., Yu, Z., Ping, W., Tao, A., Kautz, J., Han, S., Xu, D., Molchanov, P., et al. Xvila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. Zeng, Y., Kang, W., Chen, Y., Koo, H. I., and Lee, K. Can mllms perform text-to-image in-context learning? COLM, 2024. Zhang, B. and Sennrich, R. Root mean square layer normalization. NeurIPS, 2019. Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models. In ICCV, pp. 38363847, 2023. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 11 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models"
        },
        {
            "title": "APPENDIX",
            "content": "A. Limitation Despite ThinkDiffs strong performance in reasoning generation tasks, several limitations remain for future work. First, while it substantially outperforms existing methods, ThinkDiff still encounters difficulties with certain complex cases. Enhancing reasoning accuracy may require stronger VLMs, better data quality, advanced diffusion models, and improved training strategies. Second, although this work primarily focuses on logical reasoning rather than preserving image fidelity, improving fidelity could expand its applications in tasks like image editing. Finally, more diverse evaluation tasks are needed to better assess reasoning performance and advance research in this area. B. Dataset details For ThinkDiff-LVLM, the training process requires images and their corresponding VLM-generated tokens. We randomly sample 1.7 million images from the CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), and SBU (Ordonez et al., 2011) datasets. These images are preprocessed using Qwen2-VL, which generates detailed descriptions based on randomly selected text prompts from predefined set. The generated text tokens and token features are stored for training the alignment. We generate 64 tokens for each data sample. Data processing is accelerated using the vLLM framework (Kwon et al., 2023). For ThinkDiff-CLIP, the training utilizes images and their corresponding captions, sampled from combination of CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011). The predefined prompts for ThinkDiff-LVLM are designed to encourage the VLM to generate detailed descriptions of the image. Below is list of the prompts we use, some of which are adapted from LLaVA (Liu et al., 2023). Describe the image concisely. Provide brief description of the given image. Offer succinct explanation of the picture presented. Summarize the visual content of the image. Give short and clear explanation of the subsequent image. Share concise interpretation of the image provided. Present compact description of the photos key features. Relay brief, clear account of the picture shown. Render clear and concise summary of the photo. Write terse but informative summary of the picture. Create compact narrative representing the image presented. Generate prompt that can recreate the image in 2D diffusion model. Provide descriptive prompt to reproduce the given image using diffusion model. Create prompt suitable for 2D diffusion model to generate the same image. Summarize the visual details as prompt for 2D diffusion model. Write clear prompt to guide 2D diffusion model in recreating the image. 12 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Evaluation on CoBSAT. As described in Section 4.2, when evaluating ThinkDiff-LVLM on the CoBSAT dataset, we use an instruction prompt to guide Qwen2-VL to generate the next image based on multimodal inputs. Qwen2-VL is vision-language model primarily designed to answer questions by text. It does not automatically know that we want it to generate the next image and we also do not finetune it for this specific task. Therefore, the instruction prompt is necessary. The instruction prompt used in our evaluation is: give you several words and pictures. First, please analyse what the next picture is. Then give me detailed diffusion prompt to describe the next picture. Please only provide me the detailed prompt and start the answer with Create an image. C. More high-quality results C.1. ThinkDiff-LVLM Figure 9 and 10 demonstrate more high-quality results of ThinkDiff-LVLM on 2-shot evaluation in CoBSAT benchmark. ThinkDiff-LVLM can not only generate images with logically correct objects and attributes based on advanced reasoning, but also generate much higher-quality images than SEED-LLaMA (Ge et al., 2024), Emu (Sun et al., 2023), and GILL (Koh et al., 2024). These compared methods typically generate wrong images of lower quality. C.2. ThinkDiff-CLIP Figure 11 shows more results with single image (I) or single image with text prompt (I + T) as input. FLUX Ultra (Forest, 2024b) struggles to maintain coherence when an additional text prompt is included (FLUX Ultra + + T) while ThinkDiff-CLIP excels at integrating both image and text to generate logically coherent images (Ours + and Ours + + T). Figure 12 and 13 shows more results of our ThinkDiff-CLIP handling multiple images and text prompts. ThinkDiff-CLIP effectively combines semantic details from two input images in coherent manner and seamlessly integrates text prompts to guide the generation, showcasing its flexibility and capability for complex multimodal tasks. D. Video results of ThinkDiff-CLIP As discussed in Section 4.3, ThinkDiff-CLIP can integrate CogVideoX (Yang et al., 2024b) model for text-to-video generation. Figure 14 demonstrates frames of video generation results, validating ThinkDiff-CLIPs flexibility and broad applicability for multimodal generation tasks. 13 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 9. More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark. 14 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 10. More 2-shot reasoning results of ThinkDiff-LVLM on CoBSAT benchmark. 15 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 11. Generation results of single image and text prompt of ThinkDiff-CLIP. 16 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 12. Multiple input image generation results of ThinkDiff-CLIP. 17 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 13. Generation results for multiple images (2I) and multiple images with text prompt (2I + T) of ThinkDiff-CLIP. 18 Think, Therefore Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models Figure 14. Image + text to video generation results of ThinkDiff-CLIP."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering (CSE), The Hong Kong University of Science and Technology (HKUST)",
        "Snap Inc."
    ]
}