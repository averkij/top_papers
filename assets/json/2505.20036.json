{
    "paper_title": "Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction",
    "authors": [
        "Hazem Alsamkary",
        "Mohamed Elshaffei",
        "Mohamed Soudy",
        "Sara Ossman",
        "Abdallah Amr",
        "Nehal Adel Abdelsalam",
        "Mohamed Elkerdawy",
        "Ahmed Elnaggar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 3 0 0 2 . 5 0 5 2 : r Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction Hazem Alsamkary Mohamed Elshaffei Mohamed Soudy"
        },
        {
            "title": "Sara Ossman",
            "content": "Abdallah Amr Nehal Adel Abdelsalam Mohamed Elkerdawy Ahmed Elnaggar Proteinea Inc"
        },
        {
            "title": "Abstract",
            "content": "Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce meticulously curated version of the PPB-Affinity dataset of total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates stringent 30% sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction. The code and dataset are publicly available at https://github.com/Proteinea/ppiseq and https: //huggingface.co/datasets/proteinea/ppb_affinity, respectively."
        },
        {
            "title": "Introduction",
            "content": "Protein-protein interactions (PPIs) are fundamental to major biological processes, underpinning complex protein networks, the assembly of functional protein structures, the operation of cellular machinery, enzymatic catalysis, the modulation of protein specificity, and signal transduction pathways [1]. In translational research, the study of PPIs is crucial for identifying novel therapeutic targets and developing protein-based drugs for complex multifactorial diseases [2]. The efficacy of such protein-based therapeutics is often directly correlated with their binding affinity to their targets; consequently, achieving high binding affinity is primary objective in drug screening and Equal contribution Corresponding author: publications@proteinea.com Preprint. Under review. lead optimization [3]. Experimental techniques are employed to quantify these interaction strengths, typically by measuring the dissociation constant (pKd), direct indicator of binding affinity. However, despite their accuracy, these experimental methods are often time-consuming, labor-intensive, resource-heavy, and characterized by low throughput [4]. This has spurred the development of computational approaches, including molecular docking and molecular dynamics simulations, aimed at predicting PPIs and their binding affinities [5]. Nevertheless, many existing computational methods for PPI detection and affinity quantification face challenges, including high computational costs, as well as limitations in accuracy and reliability [6]. To overcome these limitations, machine learning (ML) techniques, including learnable scoring functions, have been implemented for PPI prediction [6]. More recently, the advent of deep learning, particularly protein language models (PLMs), has offered promising alternatives to traditional statistical learning methods. PLMs, pre-trained on vast sequence datasets [7, 8] using self-supervised learning, have shown remarkable success in diverse proteinrelated downstream tasks, including structure and function prediction [9, 10, 11, 12, 13, 14, 15]. This success has spurred interest in applying PLMs to the more complex task of PPI prediction, especially for quantifying binding affinity [16, 17, 18, 19]. However, key challenge arises because PLMs are typically pre-trained on individual protein sequences (e.g., through masked language modeling). This design makes their native application to tasks requiring the simultaneous modeling of multiple interacting protein chains non-trivial. Consequently, current approaches encode each interacting protein independently using PLM, then aggregate (via concatenation or addition) the fixed-length representations of all proteins before these are fed into predictive neural network head [16, 19]. While straightforward to implement, these simple aggregation methods may underutilize the rich contextual information captured by PLMs. This highlights several critical gaps in current research. First, there has been limited exploration of more sophisticated architectures tailored to adapt PLMs effectively for PPI prediction. Second, the field lacks standardized techniques for handling multi-chain protein complexes, where ligands, receptors, or both may consist of multiple polypeptide chains. For instance, existing methods may concatenate all constituent chains into single sequence, often without explicit boundary information [20, 16], or entirely omit multi-chain interactions from the analysis [21]. Furthermore, significant impediment to progress is the absence of robust, standardized benchmarking datasets and evaluation protocols for sequence-based, multi-chain PPI binding affinity prediction. Current evaluation strategies vary widely from 10-fold cross-validation [20] and mutation-based splits [16] to complex-level or binding-site-level leave-one-out schemes [22]. This methodological heterogeneity in data pre-processing and splitting makes direct and fair comparison of different approaches difficult, thereby slowing research advancement in this domain. To address these gaps, our research makes two primary contributions. First, we introduce rigorously curated and standardized benchmark dataset specifically for multi-chain PPI binding affinity prediction. Derived from the recently published PPB Affinity dataset [23], our version features meticulous pre-processing to minimize errors. Importantly, it incorporates strict 30% sequence identity cutoff for creating training, validation, and test splits. This rigorous splitting is designed to ensure robust assessment of model generalization capabilities and minimize information leakage. The resulting dataset is made readily accessible via the Huggingface Datasets library [24], fostering standardized evaluation and promoting reproducible research. Second, building upon this standardized benchmark, we propose and systematically evaluate novel architectures for adapting PLMs to the task of PPI binding affinity prediction. We conducted comprehensive comparison of these proposed architectures against commonly used simple concatenation techniques. Our empirical study utilized several state-of-the-art PLMs, providing insights into both architectural efficacy and PLM-specific performance, aiming to guide future research in developing more effective computational models for PPI analysis."
        },
        {
            "title": "2.1 Dataset",
            "content": "To train and evaluate our proposed architectures for predicting PPI binding affinity, we utilized the PPB-Affinity dataset [23]. This dataset is comprehensive aggregation of PPI data from multiple sources, notably including the widely recognized SKEMPI2 database [25]. PPB-Affinity encompasses diverse interaction types, such as antibody-antigen, TCR-pMHC, and general protein-protein interactions. Its inherent diversity provides rich training corpus, expected to foster the development of machine learning models that generalize effectively across broad spectrum of PPIs. Our data curation pipeline for this dataset comprised three distinct phases: 2 1. Preprocessing: This initial stage focused on identifying and correcting erroneous entries within the raw data before the extraction of protein sequences from Protein Data Bank (PDB) files. 2. Postprocessing: Following sequence extraction, this stage involved filtering and refining the protein sequences to enhance the overall data quality. 3. Splitting: The final phase partitioned the curated and filtered data into distinct training, validation, and test sets for model development and robust evaluation."
        },
        {
            "title": "2.1.1 Data preprocessing",
            "content": "The preprocessing phase focused on enhancing data integrity by systematically identifying and rectifying or removing erroneous entries from the initial 12,062 records in the PPB-Affinity dataset. First, we performed consistency checks for mutation annotations. Entries with incorrect mutation specifications were identified by comparing the provided mutation information against the residue in the corresponding PDB file chain. Such discrepancies can lead to incorrectly generated mutated sequences, creating mismatch between the input protein sequences and the target binding affinity label, thereby potentially hampering model training. We identified and removed three entries due to such errors. For instance, the entry with ID 3QIW:A, B, C, D, E:C_K8E:PMID=21490152 specified K8E mutation in chain C; however, inspection of the PDB file revealed an at this position, not K. Second, we screened for entries referencing non-existent ligand or receptor chains in their associated PDB files. As complete protein sequence construction was impossible for these cases, such entries were excluded. We identified and removed 11 such entries. An example is the entry 5KVE:E, H, L::PMID=27475895, which specified chain as part of the ligand, although this chain was absent in the corresponding PDB file. In related issue, several entries, predominantly associated with the SAbDab database, listed duplicate chain identifiers (e.g., and a) where the lowercase version did not exist in the PDB file. These entries were corrected by removing the reference to the non-existent lowercase chain. As the primary uppercase chain provided all necessary information for sequence reconstruction, these entries were therefore retained. Finally, specific systematic annotation error was identified: all entries associated with the PDB ID 3QIB and originating from the ATLAS database exhibited an off-by-one error in the reported mutation site numbering for chain C. We programmatically corrected these mutation locations by incrementing the residue position by one. This ensured accurate application of mutations and maintained consistency between the derived protein sequence data and the associated binding affinity labels. Following these preprocessing steps, the curated dataset was ready for the subsequent stage of parsing PDB files to extract protein sequences and apply the validated mutation information."
        },
        {
            "title": "2.1.2 Data postprocessing",
            "content": "Following the initial preprocessing, the postprocessing phase further refined the dataset through several key steps to ensure data quality and suitability for training protein language models. First, we addressed missing residues within the protein sequences derived from PDB ATOM records. The presence of such gaps, particularly non-terminal missing residues which can be integral to the protein core and thus protein stability [26], can significantly impact the representations generated by PLMs and potentially degrade model performance, an aspect often overlooked in literature. To ensure sequence completeness, we recovered these missing residues by referencing the corresponding sequence information available in the SEQRES records of the respective PDB files, incorporating the missing amino acids into their correct positions within the sequence. This procedure resulted in the recovery of missing residues for total of 4,946 protein chains within the dataset. Subsequently, we filtered out entries involving short protein chains. Specifically, any PPI entry where at least one interacting partner (ligand or receptor chain) comprised fewer than 40 residues was removed [27, 28]. This filtering criterion was applied to ensure that our dataset predominantly contained interactions between well-defined protein domains or complete proteins, rather than protein-peptide interactions. This step resulted in the exclusion of 2,753 entries. Finally, we addressed duplicate entries, defined as those having identical ligand and receptor sequences. Such duplicates could possess varying reported binding affinity values or be associated with different experimental measurement methods. Our consolidation strategy for these cases was as follows: if multiple affinity measurement methods were documented for group of identical sequence pairs, entries associated with Unknown or Other methods were omitted to prioritize more reliable data. The binding affinity values from the remaining entries (i.e., those with explicitly known measurement methods) were then averaged to create single, 3 representative entry for that unique interacting pair. This de-duplication process, aimed at enhancing data quality and consistency, led to the consolidation and effective removal of 1,088 entries. After these postprocessing steps, the final dataset prepared for model training and evaluation consisted of 8,207 unique protein-protein interaction entries."
        },
        {
            "title": "2.1.3 Data splitting",
            "content": "Standard practices in PPI modeling often involve splitting data by randomly assigning PDB IDs to training and validation sets [29, 30]. Such approaches can lead to severely overestimated performance metrics due to significant information leakage between splits; for instance, random PDB ID-based splitting can result in up to 65% leakage of training PPI interfaces into evaluation sets [31, 32]. This leakage means models are often evaluated on their ability to memorize training data rather than their capacity to generalize to genuinely unseen data. To address this critical issue, we implemented rigorous, two-stage data splitting strategy designed to minimize information leakage and enable fair assessment of model generalization. The first stage involved an initial splitting of the dataset based on sequence clustering. We first created single representative sequence for each PPI entry by concatenating all its constituent protein chains. These concatenated sequences were then clustered using the MMseqs2 [33] cluster algorithm, adopting commonly used minimum sequence identity threshold of 30% for cluster formation (i.e., between cluster representative and any member sequence) [34, 27, 28, 35, 16, 19, 36]. This process yielded 1,572 distinct clusters. An initial training split was formed by assigning the largest clusters to it until this split constituted approximately 75% of the total dataset. This initial target of approximately 75% was set because the subsequent refinement stage (detailed in the next paragraph) was designed to identify and reassign further qualifying sequences into the training set, thereby increasing its final proportion. Assigning larger clusters first aimed to prevent the bias of the smaller, preliminary validation and test splits towards any specific protein families. Throughout this process, we enforced PDB ID coherency, ensuring that all sequences originating from the same PDB ID were assigned to the same split. The second stage focused on refining these initial splits to stringently control for sequence similarity across them. We iteratively used the MMseqs2 search command to identify any sequences remaining in the preliminary validation or test splits that exhibited sequence identity > 30% with any sequence in the formulated training split. All such identified sequences were then reassigned to the training split. During each reassignment, PDB ID coherency was maintained: if sequence was moved, all other sequences from its parent PDB ID were also moved to the training split. This iterative refinement continued until no more sequences in the validation or test splits met the > 30% similarity criterion with the training set. In total, 328 sequences were reassigned to the training data through this procedure, expanding the training split to approximately 79% of the total dataset. Finally, the remaining 21% of the data was partitioned into validation and test sets. This subset was re-clustered using MMseqs2 with the identical parameters as described above, resulting in 1,071 new clusters. These clusters were then distributed as equitably as possible between the validation and test splits, ensuring that all sequences derived from the same PDB ID were allocated to only one of these final splits. This meticulous process resulted in final dataset splits of approximately 79% for training, 12% for validation, and 9% for testing."
        },
        {
            "title": "2.2 Architecture",
            "content": "Adapting PLMs for PPI binding affinity prediction involves processing multiple protein sequences, evolving from simple ligand-receptor pairs (l, r) to complex multi-chain entities (Lig = (l1, . . . , lL), Rec = (r1, . . . , rR)). Since PLMs are typically pre-trained on single protein sequences [10, 11, 12], they cannot natively handle such multi-entity, variable-number inputs. Therefore, effectively leveraging PLMs for this task requires specialized architectures, which are detailed in the subsequent sections."
        },
        {
            "title": "2.2.1 Embeddings concatenation",
            "content": "The embeddings concatenation (EC) architecture (Figure 1a) adapts PLMs to PPI tasks [16, 19]. For each interacting partner (ligand or receptor), its constituent chains are concatenated with interchain End-of-Sequence (EOS) tokens for boundary demarcation. This composite sequence is then encoded by PLM (outputting embeddings of dimension Edim) into sequence of hidden states with dimensions (lenpartner Edim). These hidden states are subsequently condensed by 1D global 4 R1 MLP (2 Edim) Concatenate R1 MLP (2 Edim) Concatenate (Edim) Attention Pooler (L Edim) Attention Pooler (Edim) Attention Pooler (R Edim) Attention Pooler (Edim) Attention Pooler (Edim) Attention Pooler (lenlig Edim) PLM (lenrec Edim) PLM {(len_l1 Edim), . . ., (len_lL Edim)} PLM {(len_r1 Edim), . . . ,(len_rR Edim)} PLM l1[EOS]l2 . . . lL[EOS] Ligand Chains r1[EOS]r2 . . . rR[EOS] Receptor Chains (l1, l2, . . . , lL) Ligand Chains (r1, r2, . . . , rR) Receptor Chains (a) Embeddings Concatenation (EC) Architecture (b) Hierarchical Pooling (HP) Architecture R1 MLP + (Edim) Attention Pooler (Edim) Attention Pooler + + (lenlig Edim) Multi-Head Cross-Attention (lenrec Edim) Multi-Head Cross-Attention (lenlig Edim) PLM (lenrec Edim) PLM l1[EOS]l2 . . . lL[EOS] Ligand Chains r1[EOS]r2 . . . rR[EOS] Receptor Chains (c) Pooled Attention Addition (PAD) Architecture Figure 1: Architectures used to adapt protein language models to the binding-affinity prediction task. Dimensions displayed on each block denote component output dimensions; all weights were shared across parallel ligand and receptor processing pathways attention pooling layer (supplementary Figure 4) to produce fixed-dimension Edim embedding for that partner. Both the PLM and the attention pooler share trainable weights across these parallel ligand and receptor processing pathways. Finally, the resulting Edim ligand and receptor embeddings are concatenated (forming combined vector of dimension 2Edim) and input to multi-layer perceptron (MLP) for binding affinity prediction."
        },
        {
            "title": "2.2.2 Sequences concatenation",
            "content": "The sequences concatenation (SC) architecture (supplementary Figure 3) adapts PLM to PPI task by concatenating the input protein sequences before they are processed by the PLM, in contrast to EC architecture which concatenates PLM output representations. In the SC approach, all individual protein chains from both the ligand and the receptor are combined to form single, unified input 5 sequence. EOS tokens are inserted between each original chain sequence within this composite input to mark their respective boundaries. This entire concatenated sequence is then fed into the PLM. The PLM processes this sequence to produce corresponding sequence of hidden state representations, capturing contextual information across all constituent chains. These representations typically have dimensions of (lenconcat Edim), where lenconcat is the total length of the concatenated input sequence and Edim is the embedding dimension of the PLM. Subsequently, this sequence of hidden state representations is passed to 1D global attention pooling layer. This pooling layer aggregates information across the entire sequence of hidden states, outputting single, fixeddimension embedding of Edim. Finally, this comprehensive embedding is fed into MLP, which serves as the prediction head, to map it to single binding affinity value."
        },
        {
            "title": "2.2.3 Hierarchical pooling",
            "content": "Concatenating multiple input protein chains, as performed in EC and SC architectures, can lead to significant GPU memory footprint, particularly when dealing with long sequences or complexes with many chains. To address this limitation, we propose the hierarchical pooling (HP) architecture (Figure 1b). Unlike EC and SC, HP avoids the concatenation of input sequences to the PLM. The HP architecture processes interacting partners (ligand and receptor) as follows. Considering the ligand, which may consist of individual chains (l1, l2, . . . , lL): 1. Individual chain encoding: Each of the ligand chains is fed independently into the PLM. This produces distinct sequences of hidden state representations, where each sequence corresponds to an individual chain and its dimensions are (sequence length Edim). 2. Intra-chain pooling (First-level pooling): Each of these hidden state sequences is then independently processed by 1D global attention pooling layer. This step generates fixed-dimension chain embeddings with output shape of (L, Edim), effectively summarizing the information from each individual ligand chain. 3. Inter-chain pooling (Second-level pooling): The resulting chain embeddings, which form an input tensor of shape (L, Edim), are subsequently pooled together using another 1D global attention pooling layer. This \"pooling across chains\" step aggregates information across the chain dimension (i.e., across the chains), collapsing the input into single, comprehensive ligand-level representation of dimension Edim. An identical hierarchical procedure is carried out for the receptor chains to compute corresponding receptor-level representation of dimension Edim. All trainable weights within the PLM and both levels of attention pooling layers are shared between the parallel ligand and receptor processing pathways. Finally, the computed ligand-level and receptor-level embeddings are concatenated, forming single feature vector of dimension 2Edim. This combined vector is then fed into MLP prediction head, which maps it to the final binding affinity value."
        },
        {
            "title": "2.2.4 Pooled attention addition",
            "content": "To introduce more explicit inductive bias for learning direct ligand-receptor interactions, which may be less emphasized in architectures relying solely on various forms of concatenation, we propose the pooled attention addition (PAD) architecture (Figure 1c). The initial processing stages of the PAD architecture are similar to those in the EC approach. For the ligand, all its constituent chains are concatenated into single sequence, with EOS tokens inserted between chains to demarcate their boundaries. This composite sequence is then processed by PLM to generate sequence of hidden states, Elig, with dimensions (lenlig Edim), where lenlig is the length of the concatenated ligand sequence and Edim is the PLMs embedding dimension. An analogous procedure, employing PLM with shared weights, is used for the receptor chains to compute the receptors hidden state sequence, Erec, with dimensions (lenrec Edim). The core of the PAD architecture lies in its interaction modeling through Multi-head attention (MHA) block [37], which computes cross-attention between the ligand and receptor hidden state sequences. Specifically, the receptor-contextualized ligand representation, alig, is computed by feeding Elig as the Query (Q) sequence, and Erec as both the Key (K) and Value (V) sequences to the MHA block. Symmetrically, the ligand-contextualized receptor representation, arec, is then computed by using Erec as the sequence, and Elig as both and sequences. This cross-attention mechanism is designed to explicitly model the interdependencies between the ligand and receptor representations. The resulting attention outputs are 6 lig and lig = Elig + alig and then integrated back into their respective original sequences via element-wise addition using residual connections: rec = Erec + arec. Subsequently, these updated hidden state sequences, rec, are independently processed by 1D global attention pooling layers. This step yields fixed-dimension embedding for the ligand and fixed-dimension embedding for the receptor, each of dimension Edim. Unlike other architectures that concatenate these embeddings, the PAD architecture combines them through element-wise vector addition. The resulting sum vector, also of dimension Edim, is then passed to MLP prediction head to output the final binding affinity value. All trainable weights in the parallel PLM encoding, MHA block, and 1D global attention pooling stages for the ligand and receptor paths are shared."
        },
        {
            "title": "2.3 Training details",
            "content": "We evaluated two primary training paradigms: (1) full fine-tuning of the PLM, adapting all its parameters, which is computationally intensive but may yield higher performance; and (2) lightweight approach training downstream head on features extracted from frozen PLM. For the latter, we employed ConvBERT model [38] as the downstream head, selected for its previously reported state-of-the-art efficacy [12]. This feature-extraction strategy significantly reduces memory demands, crucial factor for architectures like EC, SC, and PAD that involve input sequences concatenation, leading to longer sequence lengths for the PLM. These training paradigms were applied across suite of PLMs: ProtT5 [10], ESM2 (650M and 3B variants) [11], Ankh (Base and Large variants) [12], Ankh2 (Ext1 and Ext2 variants) [13, 14], and ESM3-SM-Open [15]. All models were trained by minimizing the mean squared error (MSE) loss between the predicted and ground-truth binding affinities (measured in pKd). Full PLM fine-tuning utilized brain float16 (bf16) mixed precision, whereas the ConvBERT head was trained with float32 precision. The ConvBERT head consisted of single layer with an intermediate size of 0.5 Edim (where Edim is the PLMs embedding dimension), kernel size of 7, hidden dropout probability of 0.2, and an attention dropout probability of 0.1. For the EC, HP, and PAD architectures, ConvBERT weights were shared between the ligand and receptor processing pathways. Training proceeded for maximum of 30 epochs, with early stopping if the Spearman correlation coefficient (ρ) on the validation set did not improve for five consecutive epochs. The checkpoint achieving the highest validation ρ was selected for final evaluation. All reported metrics were the mean values from three independent runs using different random seeds. Standard hyperparameters (detailed in Table 1) were used for most experiments. Exceptions were made for full fine-tuning of ESM2-3B and ESM3-SM-Open, which required reduced learning rate for stable convergence. Additionally, for ESM3-SM-Open under full fine-tuning, per-residue representations from the PLM were L2-normalized to mitigate training instability caused by large magnitude outputs. All models were trained using PyTorch [39] Distributed Data Parallel across two GPUs (either NVIDIA H100 or A6000, yielding identical results), leveraging available hardware to expedite experimentation. Table 1: Training hyperparameters for full finetuning and ConvBERT training across all architectures"
        },
        {
            "title": "Learning Rate Scheduler\nWarmup Steps\nNumber of GPUs\nPer Device Batch Size\nGradient Accumulation Steps\nTotal Effective Batch Size\nRandom Seeds",
            "content": "Adam 5 104 (5 105 fine-tuning ESM2-3B and ESM3) Linear with warmup 1000 2 1 32 64 (1 32 2) 7, 8,"
        },
        {
            "title": "3 Results and discussion",
            "content": "Analysis of the top 10 performing runs, ranked by Spearman correlation coefficient on the test set (Table 2 highlights key trends in adapting PLMs for binding affinity prediction (The results of all 7 Table 2: Top 10 runs ranked by test set Spearman correlation (ρ). Metrics (mean standard deviation, 3 seeds): Spearman ρ, Pearson r, and RMSE (pKd). PAD: Pooled attention addition; HP: Hierarchical pooling PLM Setup Validation Split Test Split Spearman Pearson RMSE Spearman Pearson RMSE ConvBERT-PAD 0.48 0.02 Prot-T5 0.48 0.01 Finetuning-HP Ankh2-Ext1 0.44 0.03 ESM2-650M ConvBERT-HP 0.47 0.01 Ankh2-Ext2 Finetuning-HP 0.45 0.02 ESM2-650M Finetuning-HP 0.47 0.01 Finetuning-HP Ankh-Base 0.42 0.02 ConvBERT-HP Prot-T5 Finetuning-PAD 0.44 0.00 Prot-T5 0.47 0.02 Finetuning-HP Prot-T5 Finetuning-PAD 0.45 0.01 ESM2-3B 0.48 0.01 0.49 0.01 0.43 0.03 0.47 0.01 0.44 0.02 0.47 0.00 0.42 0.02 0.44 0.00 0.46 0.02 0.45 0.01 1.52 0.09 1.50 0.10 1.86 0.29 1.51 0.07 1.75 0.29 1.47 0.03 1.66 0.23 1.65 0.13 1.51 0.05 1.49 0.07 0.48 0.03 0.47 0.01 0.47 0.02 0.45 0.01 0.44 0.02 0.44 0.01 0.44 0.01 0.44 0.01 0.44 0.01 0.44 0.01 0.51 0.02 0.48 0.01 0.48 0.02 0.46 0.02 0.45 0.01 0.45 0.01 0.44 0.01 0.45 0.01 0.45 0.01 0.46 0. 1.42 0.10 1.45 0.13 1.74 0.33 1.43 0.06 1.68 0.29 1.41 0.03 1.59 0.25 1.57 0.14 1.47 0.08 1.43 0.08 the runs are available in Supplementary Table 3). While the single highest correlation was achieved by ProtT5 using the PAD architecture with ConvBERT head, the top three entries featured distinct combinations of PLMs, training methods, and architectures. This diversity suggests that optimal performance is not confined to singular configuration. Notably, the HP architecture prevailed in 7 of the top 10 runs, and PAD also demonstrated consistently strong performance. On the other hand, the simpler EC and SC architectures were conspicuously absent from this top tier, indicating their limitations for this complex task. The overall performance (Figure 2) indicated by test Spearman correlations for all PLMs across all setups (defined as training method-architecture combination), underscores the critical role of architectural design. The HP and PAD architectures significantly outperformed EC and SC, particularly when employing full PLM fine-tuning. The marked underperformance of SC may stem from its approach of concatenating all ligand and receptor chains into one input sequence. This strategy can obscure the distinction between ligand and receptor entities for the PLM, as the same EOS token separates all chains. Furthermore, the PLMs self-attention mechanism, when applied to such long, composite sequence, might not effectively differentiate crucial inter-chain interactions from intra-chain relationships, leading to dilution of the interaction signal that is potentially aggravated by the final global pooling step. Figure 2: Heatmap of test set Spearman ρ (each value averaged over 3 seeds): PLMs vs. setups for binding affinity prediction. Marginal means show average ρ per PLM (last column) and per setup (last row). PAD: Pooled Attention Addition; HP: Hierarchical Pooling; SC: Sequences Concatenation; EC: Embeddings Concatenation 8 The superiority of HP over EC, despite their identical behavior for single-chain ligand/receptor pairs, is particularly instructive and points to more effective handling of multi-chain complexes by HP. In the EC architecture, concatenating all chains of partner (e.g., ligand) before PLM encoding and subsequently pooling this combined, lengthy representation can dilute chain-specific features and lead to information loss during the compression to fixed-size vector. In contrast, the hierarchical strategy of HP processes each chain individually through the PLM, performs chain-level attention pooling to capture distinct characteristics, and only then aggregates these rich chain embeddings into partner-level representation. This preserves vital chain-level information more effectively, providing more robust foundation for predicting interactions in multi-chain scenarios. The PAD architecture also demonstrated superior performance compared to EC, despite sharing similar initial chain concatenation strategy for forming ligand and receptor representations. This advantage can be attributed to its integrated cross-attention block. This mechanism introduces strong inductive bias for modeling direct ligand-receptor interdependencies, generating refined, context-aware representations for both partners before pooling. These refined representations are likely less susceptible to information loss during the subsequent pooling and final aggregation steps (element-wise addition in PAD), contributing to its enhanced predictive power over EC and SC. No single PLM consistently outperformed others across all setups; rather, PLM efficacy was highly dependent on the chosen architecture and training strategy (Figure 2). ProtT5, for instance, achieved the highest average Spearman correlation, demonstrating robust performance across various configurations. However, in specific setups, other models excelled; for example, ESM2-650M and Ankh2-Ext1 showed superior results when combined with the HP architecture. Conversely, ESM3-SM-Open generally underperformed. During full fine-tuning, it exhibited training instability, characterized by large hidden state values, which persisted to some degree despite mitigation like reduced learning rates and L2 normalization of representations. While training was stable when using ESM3-SM-Open with ConvBERT heads (frozen PLM), its performance remained comparatively low. This suggests that ESM3-SM-Open might require more extensive, task-specific hyperparameter tuning or possess inherent characteristics less suited to this PPI binding affinity task within the evaluated configurations. Notably, model size was not definitive predictor of performance, as exemplified by ESM2-650M outperforming the larger ESM2-3B model both overall and in multiple specific setups. The choice between full PLM fine-tuning and using lightweight ConvBERT head with frozen PLM was also dependent on the specific PLM (Figure 2). For ESM2 models, ESM3-SM-Open, and ProtT5, full fine-tuning offered minimal, if any, performance gains over the ConvBERT approach. In such cases, the substantially lower computational and memory requirements of the ConvBERT strategy present more efficient alternative. In contrast, for the Ankh and Ankh2 PLMs, full fine-tuning, particularly when paired with the HP or PAD architectures, generally yielded significantly better results than the ConvBERT-based feature extraction approach. This indicates that for certain PLMs, allowing the models deeper layers to adapt to the downstream task is crucial for unlocking their full potential in PPI binding affinity prediction, especially with sophisticated interaction architectures."
        },
        {
            "title": "4 Conclusion and future work",
            "content": "This work underscores the necessity of meticulously processed datasets with stringent, low-leakage splits for the reliable evaluation of PLMs in PPI binding affinity prediction. We demonstrated that optimal model performance is not solely dependent on PLM choice but is also determined by the interplay between the PLM, the chosen training strategy, and the architecture used. Our findings reveal that the proposed hierarchical pooling and pooled attention addition architectures significantly surpass commonly used concatenation-based approaches, offering more effective strategies for leveraging PLMs in this vital domain and expanding the horizons for their application. We acknowledge limitations stemming from computational constraints, which prevented extensive, PLM-specific hyperparameter tuning that may potentially boost performance. Our analysis was also limited to sequence-based prediction. Incorporating protein structure, particularly relevant for models like ESM3, was not feasible due to inconsistencies created during missing residue recovery in sequences compared to original PDB structures, and the prohibitive computational cost of predicting updated structures for all affected entries. Future directions include dedicated hyperparameter optimization for individual PLMs and integrating predicted 3D structural information. Using tools like AlphaFold [40] to generate structures for processed sequences would facilitate developing and testing multi-modal models, potentially enhancing PLMs capable of leveraging structural features."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The authors gratefully acknowledge the collaborative efforts and significant contributions of the Proteinea team. We especially thank Ahmed Mansour for conducting an in-depth review of the paper. Our gratitude also extends to Proteineas deep learning and bioinformatics teams, whose provision of hardware, software, and broad project support was invaluable. It is important to note that this research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors."
        },
        {
            "title": "References",
            "content": "[1] Pascal Braun and Anne-Claude Gingras. History of proteinprotein interactions: From eggwhite to complex networks. Proteomics, 12(10):14781498, 2012. [2] Haiying Lu, Qiaodan Zhou, Jun He, Zhongliang Jiang, Cheng Peng, Rongsheng Tong, and Jianyou Shi. Recent advances in the development of proteinprotein interactions modulators: mechanisms and clinical trials. Signal transduction and targeted therapy, 5(1):213, 2020. [3] Shuyan Li, Lili Xi, Chengqi Wang, Jiazhong Li, Beilei Lei, Huanxiang Liu, and Xiaojun Yao. novel method for protein-ligand binding affinity prediction and the related descriptors exploration. Journal of computational chemistry, 30(6):900909, 2009. [4] Jiffriya Mohamed Abdul Cader, MA Hakim Newton, Julia Rahman, Akmal Jahan Mohamed Abdul Cader, and Abdul Sattar. Ensembling methods for protein-ligand binding affinity prediction. Scientific Reports, 14(1):24447, 2024. [5] Jingtian Zhao, Yang Cao, and Le Zhang. Exploring the computational methods for proteinligand binding site prediction. Computational and structural biotechnology journal, 18:417426, 2020. [6] Sangmin Seo, Jonghwan Choi, Sanghyun Park, and Jaegyoon Ahn. Binding affinity prediction for proteinligand complex using deep attention mechanism based on intermolecular interactions. BMC bioinformatics, 22:115, 2021. [7] Baris Suzek, Yuqi Wang, Hongzhan Huang, Peter McGarvey, Cathy Wu, and UniProt Consortium. Uniref clusters: comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926932, 2015. [8] Martin Steinegger, Milot Mirdita, and Johannes Söding. Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Nature methods, 16(7):603606, 2019. [9] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021. [10] Ahmed Elnaggar, Heinzinger, Dallago, et al. Prottrans: Towards cracking the language of lifes code through 500 self-supervised deep learning and high performance computing [j]. IEEE Trans, 685, 2020. [11] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. [12] Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, and Burkhard Rost. Ankh: Optimized protein language model unlocks general-purpose modelling. arXiv preprint arXiv:2301.06568, 2023. [13] Elnaggar Lab. ankh2-ext1 (revision 286cb6e), 2025. URL https://huggingface.co/ ElnaggarLab/ankh2-ext1. 10 [14] Elnaggar Lab. ankh2-ext2 (revision 4c155ee), 2025. URL https://huggingface.co/ ElnaggarLab/ankh2-ext2. [15] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J. Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q. Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul S. Molina, Neil Thomas, Yousuf A. Khan, Chetan Mishra, Carolyn Kim, Liam J. Bartie, Matthew Nemeth, Patrick D. Hsu, Tom Sercu, Salvatore Candido, and Alexander Rives. Simulating 500 million years of evolution with language model. bioRxiv, 2024. [16] Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng Liu, and Jian Tang. Peer: comprehensive and multi-task benchmark for protein sequence understanding. Advances in Neural Information Processing Systems, 35:3515635173, 2022. [17] Samuel Sledzieski, Rohit Singh, Lenore Cowen, and Bonnie Berger. D-script translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein-protein interactions. Cell Systems, 12(10):969982, 2021. [18] Rohit Singh, Kapil Devkota, Samuel Sledzieski, Bonnie Berger, and Lenore Cowen. Topsyintegrating global view into sequence-based ppi prediction. Bioinformatics, 38 turvy: (Supplement_1):i264i272, 2022. [19] Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan. Saprot: Protein language modeling with structure-aware vocabulary. bioRxiv, pages 202310, 2023. [20] Muhao Chen, Chelsea J-T Ju, Guangyu Zhou, Xuelu Chen, Tianran Zhang, Kai-Wei Chang, Carlo Zaniolo, and Wei Wang. Multifaceted proteinprotein interaction prediction based on siamese residual rcnn. Bioinformatics, 35(14):i305i314, 2019. [21] Guangyu Zhou, Muhao Chen, Chelsea JT Ju, Zheng Wang, Jyun-Yu Jiang, and Wei Wang. Mutation effect estimation on proteinprotein interactions using deep contextualized representation learning. NAR genomics and bioinformatics, 2(2):lqaa015, 2020. [22] Yunzhuo Zhou, YooChan Myung, Carlos HM Rodrigues, and David Ascher. Ddmut-ppi: predicting effects of mutations on proteinprotein interactions using graph-based deep learning. Nucleic Acids Research, 52(W1):W207W214, 2024. [23] Huaqing Liu, Peiyi Chen, Xiaochen Zhai, Ku-Geng Huo, Shuxian Zhou, Lanqing Han, and Guoxin Fan. Ppb-affinity: Protein-protein binding affinity dataset for ai-based protein drug discovery. Scientific Data, 11(1):111, 2024. [24] Quentin Lhoest, Albert Villanova Del Moral, Yacine Jernite, Abhishek Thakur, Patrick Von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. Datasets: community library for natural language processing. arXiv preprint arXiv:2109.02846, 2021. [25] Justina Jankauskaite, Brian Jiménez-García, Justas Dapkunas, Juan Fernández-Recio, and Iain Moal. Skempi 2.0: an updated benchmark of changes in proteinprotein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics, 35(3):462469, 2019. [26] Elizaveta Kozlova, Arthur Valentin, Aous Khadhraoui, and Daniel Nakhaee-Zadeh Gutierrez. Proteinflow: python library to pre-process protein structure data for deep learning applications. bioRxiv, pages 202309, 2023. [27] José Juan Almagro Armenteros, Casper Kaae Sønderby, Søren Kaae Sønderby, Henrik Nielsen, and Ole Winther. Deeploc: prediction of protein subcellular localization using deep learning. Bioinformatics, 33(21):33873395, 2017. [28] Vineet Thumuluri, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Henrik Nielsen, and Ole Winther. Deeploc 2.0: multi-label subcellular localization prediction using protein language models. Nucleic acids research, 50(W1):W228W234, 2022. 11 [29] Ning Zhang, Yuting Chen, Haoyu Lu, Feiyang Zhao, Roberto Vera Alvarez, Alexander Goncearenco, Anna Panchenko, and Minghui Li. Mutabind2: predicting the impacts of single and multiple mutations on protein-protein interactions. Iscience, 23(3), 2020. [30] Swagata Pahari, Gen Li, Adithya Krishna Murthy, Siqi Liang, Robert Fragoza, Haiyuan Yu, and Emil Alexov. Saambe-3d: predicting effect of mutations on proteinprotein interactions. International journal of molecular sciences, 21(7):2563, 2020. [31] Anton Bushuiev, Roman Bushuiev, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, and Josef Sivic. Revealing data leakage in protein interaction benchmarks. arXiv preprint arXiv:2404.10457, 2024. [32] Matsvei Tsishyn, Fabrizio Pucci, and Marianne Rooman. Quantification of biases in predictions of proteinprotein binding affinity changes upon mutations. Briefings in bioinformatics, 25(1): bbad491, 2024. [33] Martin Steinegger and Johannes Söding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):10261028, 2017. [34] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert Ragotte, Lukas Milles, Basile IM Wicky, Alexis Courbet, Rob de Haas, Neville Bethel, et al. Robust deep learningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. [35] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. Advances in neural information processing systems, 32, 2019. [36] Sameer Khurana, Reda Rawi, Khalid Kunji, Gwo-Yu Chuang, Halima Bensmail, and Raghvendra Mall. Deepsol: deep learning framework for sequence-based protein solubility prediction. Bioinformatics, 34(15):26052613, 2018. [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [38] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33:1283712848, 2020. [39] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024. doi: 10.1145/3620665.3640366. [40] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630(8016):493500, 2024."
        },
        {
            "title": "A Technical appendices and supplementary material",
            "content": "A.1 Full results Table 3: Table presents Spearman (ρ), Pearson (r) correlations, and Root Mean Squared Error (RMSE) in pKd units for validation and test sets. Metrics are derived from the best performing checkpoint (selected based on highest validation ρ) for each setup (PLM/method/architecture combination) and are reported as mean standard deviation over three independent runs with different seeds. Setup column abbreviations: CV: ConvBERT; FT: Finetuning; PAD: Pooled attention addition; HP: Hierarchical pooling; SC: Sequences concatenation; EC: Embeddings concatenation PLM Setup Validation Split Test Split Spearman Pearson RMSE Spearman Pearson RMSE ProtT5 Ankh2-Ext1 ESM2-650M Ankh2-Ext2 ESM2-3B ESM2-650M Ankh-Base ESM2-650M ESM2-3B ProtT5 ProtT5 ProtT5 Ankh2-Ext1 Ankh2-Ext2 ESM2-650M Ankh-Large ProtT5 ESM2-3B Ankh-Base Ankh-Base ESM2-3B ProtT5 ESM2-3B ESM2-650M Ankh-Large Ankh2-Ext1 Ankh2-Ext2 Ankh2-Ext1 Ankh-Base ProtT5 Ankh-Large Ankh2-Ext2 ProtT5 ESM2-3B Ankh-Base ESM2-3B ESM2-650M ESM2-650M ESM2-650M Ankh2-Ext2 Ankh2-Ext2 Ankh2-Ext1 Ankh-Large Ankh2-Ext2 Ankh2-Ext1 Ankh-Large Ankh-Base ESM2-3B CV-PAD 0.48 0.02 0.48 0.01 1.52 0.09 0.48 0.03 0.51 0.02 1.42 0.10 0.48 0.01 0.49 0.01 1.50 0.10 0.47 0.01 0.48 0.01 1.45 0.13 FT-HP 0.44 0.03 0.43 0.03 1.86 0.29 0.47 0.02 0.48 0.02 1.74 0.33 CV-HP 0.47 0.01 0.47 0.01 1.51 0.07 0.45 0.01 0.46 0.02 1.43 0.06 FT-HP FT-PAD 0.45 0.01 0.45 0.01 1.49 0.07 0.44 0.01 0.46 0.00 1.43 0.08 CV-PAD 0.44 0.00 0.44 0.00 1.57 0.12 0.44 0.01 0.46 0.01 1.46 0.10 0.47 0.01 0.47 0.00 1.47 0.03 0.44 0.01 0.45 0.01 1.41 0.03 FT-HP 0.45 0.02 0.44 0.02 1.75 0.29 0.44 0.02 0.45 0.01 1.68 0.29 FT-HP 0.48 0.01 0.47 0.01 1.42 0.01 0.44 0.01 0.47 0.01 1.34 0.00 FT-SC 0.47 0.02 0.46 0.02 1.51 0.05 0.44 0.01 0.45 0.01 1.47 0.08 FT-HP 0.42 0.02 0.42 0.02 1.66 0.23 0.44 0.01 0.44 0.01 1.59 0.25 CV-HP FT-PAD 0.44 0.00 0.44 0.00 1.65 0.13 0.44 0.01 0.45 0.01 1.57 0.14 FT-PAD 0.46 0.02 0.46 0.02 1.48 0.01 0.43 0.00 0.44 0.00 1.40 0.02 FT-PAD 0.47 0.01 0.47 0.01 1.49 0.08 0.43 0.01 0.44 0.01 1.43 0.09 0.47 0.03 0.47 0.03 1.49 0.01 0.43 0.03 0.46 0.02 1.43 0.03 FT-SC FT-PAD 0.45 0.00 0.45 0.01 1.56 0.12 0.43 0.04 0.45 0.04 1.46 0.13 CV-EC 0.40 0.01 0.40 0.01 1.56 0.00 0.43 0.03 0.44 0.03 1.46 0.00 0.44 0.02 0.43 0.02 1.58 0.03 0.43 0.03 0.44 0.03 1.50 0.01 CV-HP FT-PAD 0.47 0.01 0.47 0.01 1.48 0.04 0.42 0.02 0.44 0.01 1.43 0.03 CV-PAD 0.46 0.01 0.45 0.01 1.54 0.12 0.42 0.01 0.44 0.01 1.47 0.12 0.44 0.01 0.43 0.01 1.51 0.07 0.41 0.02 0.44 0.02 1.46 0.09 FT-HP 0.44 0.01 0.44 0.01 1.51 0.03 0.41 0.00 0.43 0.00 1.44 0.03 CV-SC 0.45 0.02 0.45 0.03 1.50 0.08 0.41 0.02 0.44 0.02 1.44 0.05 FT-EC CV-EC 0.47 0.02 0.46 0.02 1.52 0.02 0.41 0.01 0.43 0.00 1.48 0.02 CV-PAD 0.44 0.03 0.43 0.03 1.75 0.30 0.41 0.02 0.44 0.02 1.68 0.36 CV-PAD 0.46 0.01 0.45 0.01 1.73 0.43 0.41 0.02 0.43 0.02 1.67 0.41 0.44 0.03 0.44 0.03 1.54 0.04 0.41 0.02 0.41 0.02 1.49 0.02 CV-SC 0.44 0.02 0.43 0.02 1.59 0.13 0.41 0.02 0.41 0.02 1.54 0.15 CV-SC 0.40 0.01 0.40 0.00 1.67 0.14 0.41 0.03 0.43 0.04 1.61 0.16 CV-HP 0.45 0.01 0.44 0.01 1.60 0.17 0.41 0.03 0.41 0.02 1.58 0.20 FT-EC 0.42 0.01 0.41 0.01 1.80 0.23 0.40 0.03 0.40 0.03 1.78 0.30 CV-HP CV-PAD 0.44 0.01 0.44 0.02 1.57 0.06 0.40 0.01 0.43 0.02 1.51 0.05 0.46 0.03 0.46 0.03 1.53 0.02 0.40 0.01 0.43 0.00 1.51 0.01 FT-SC CV-EC 0.45 0.00 0.45 0.00 1.69 0.09 0.40 0.01 0.42 0.01 1.65 0.11 0.42 0.03 0.43 0.03 1.56 0.07 0.39 0.04 0.41 0.04 1.55 0.09 CV-SC 0.42 0.00 0.42 0.00 1.84 0.15 0.39 0.03 0.42 0.03 1.75 0.12 CV-SC 0.42 0.03 0.42 0.02 1.56 0.04 0.39 0.01 0.41 0.01 1.51 0.04 CV-SC FT-PAD 0.43 0.01 0.43 0.02 1.56 0.08 0.39 0.02 0.39 0.01 1.49 0.04 0.45 0.03 0.44 0.03 1.51 0.02 0.39 0.03 0.41 0.04 1.46 0.06 FT-EC 0.41 0.00 0.41 0.00 1.51 0.01 0.39 0.06 0.40 0.05 1.45 0.05 FT-EC 0.43 0.03 0.41 0.03 1.86 0.47 0.39 0.03 0.39 0.03 1.82 0.51 CV-HP 0.42 0.02 0.41 0.02 1.70 0.03 0.39 0.02 0.40 0.02 1.66 0.07 CV-HP 0.42 0.01 0.42 0.01 1.52 0.04 0.38 0.02 0.40 0.03 1.48 0.06 FT-HP CV-EC 0.44 0.02 0.44 0.02 1.56 0.05 0.38 0.02 0.39 0.02 1.53 0.00 0.41 0.02 0.42 0.02 1.56 0.05 0.38 0.03 0.40 0.03 1.50 0.05 FT-SC 0.43 0.01 0.43 0.01 1.72 0.19 0.38 0.03 0.38 0.03 1.69 0.17 CV-SC CV-EC 0.42 0.01 0.42 0.01 1.59 0.11 0.38 0.04 0.40 0.04 1.58 0.19 CV-PAD 0.44 0.03 0.43 0.03 2.82 1.18 0.38 0.04 0.40 0.05 2.75 1.15 Continued on next page 13 Table 3: Table presents Spearman (ρ), Pearson (r) correlations, and Root Mean Squared Error (RMSE) in pKd units for validation and test sets. Metrics are derived from the best performing checkpoint (selected based on highest validation ρ) for each setup (PLM/method/architecture combination) and are reported as mean standard deviation over three independent runs with different seeds. Setup column abbreviations: CV: ConvBERT; FT: Finetuning; PAD: Pooled attention addition; HP: Hierarchical pooling; SC: Sequences concatenation; EC: Embeddings concatenation PLM Setup Validation Split Test Split Spearman Pearson RMSE Spearman Pearson RMSE ESM3-SM-Open FT-PAD 0.42 0.03 0.42 0.02 1.54 0.10 0.37 0.01 0.38 0.01 1.49 0.08 CV-EC 0.44 0.01 0.43 0.01 1.53 0.06 0.37 0.02 0.38 0.01 1.50 0.07 Ankh-Large ESM3-SM-Open CV-EC 0.44 0.01 0.44 0.01 1.73 0.23 0.36 0.01 0.37 0.02 1.77 0.23 ESM3-SM-Open CV-PAD 0.43 0.02 0.42 0.02 1.81 0.19 0.36 0.04 0.38 0.03 1.80 0.25 0.41 0.03 0.41 0.03 1.56 0.02 0.36 0.03 0.36 0.03 1.53 0.01 ESM3-SM-Open FT-HP 0.37 0.04 0.38 0.04 1.57 0.07 0.36 0.05 0.38 0.05 1.48 0.05 ESM3-SM-Open FT-EC CV-EC 0.44 0.02 0.43 0.01 1.69 0.08 0.36 0.05 0.38 0.05 1.69 0.10 Ankh2-Ext1 0.42 0.02 0.43 0.02 1.50 0.03 0.35 0.02 0.39 0.03 1.47 0.05 FT-SC Ankh2-Ext2 0.39 0.00 0.40 0.01 1.52 0.02 0.35 0.05 0.39 0.04 1.46 0.06 Ankh-Large FT-EC 0.38 0.00 0.38 0.00 1.54 0.02 0.35 0.01 0.36 0.00 1.49 0.03 ESM3-SM-Open CV-HP 0.44 0.03 0.42 0.03 1.81 0.27 0.35 0.02 0.36 0.02 1.82 0.27 ESM3-SM-Open CV-SC 0.42 0.01 0.43 0.01 1.51 0.05 0.34 0.01 0.39 0.02 1.47 0.05 FT-SC Ankh-Large 0.40 0.01 0.41 0.01 1.52 0.03 0.33 0.08 0.34 0.06 1.52 0.06 FT-EC Ankh-Base 0.38 0.05 0.39 0.05 1.54 0.03 0.32 0.01 0.37 0.01 1.51 0.05 FT-SC Ankh-Base 0.40 0.01 0.40 0.01 1.55 0.01 0.32 0.06 0.36 0.05 1.54 0.04 Ankh2-Ext1 FT-EC 0.27 0.01 0.27 0.01 6.50 0.08 0.28 0.01 0.28 0.02 6.31 0.09 ESM3-SM-Open FT-SC 14 A.2 Additional Methods Figures R1 MLP (Edim) Sequence Attention Pooler ((lenlig + lenrec) Edim) PLM l1[EOS]l2 . . . lL[EOS]r1[EOS]r2 . . . rL[EOS] Concat. Ligand and Receptor Chains Figure 3: Sequences Concatenation Architecture (Edim) Sum Along Sequence dim. (seq_len Edim) Weigh Each Embedding Vector (seq_len 1) Softmax (seq_len 1) Linear (Edim, 1) (seq_len Edim) Sequence Hidden Representation Figure 4: Global 1D Attention Pooler Architecture. linear layer transforms the input sequence of hidden states (each of dimension Edim) into vector of scalar attention scores, one per hidden state. These scores are subsequently normalized via softmax function to produce attention weights. The final pooled output, single vector of dimension Edim, is computed as the weighted sum of the original hidden states using these attention weights. Dimensions displayed on each block denote the output dimensions of that component"
        }
    ],
    "affiliations": [
        "Proteinea Inc"
    ]
}