{
    "paper_title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
    "authors": [
        "Yogeswar Reddy Thota"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as a self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 - cos(x, F(x)), and uses Gumbel-Softmax relaxation to enable per-sample, per-block gating during training. A progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate spans the accuracy-efficiency Pareto frontier: an aggressive configuration achieves 89.9 percent accuracy with 24.1 percent FLOPs savings, a balanced configuration achieves 91.3 percent accuracy with 28.5 percent savings at epoch 160, and a conservative configuration reaches a peak of 93.2 percent accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3 percent) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide a principled and effective signal for dynamic residual routing."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 0 2 2 2 . 2 1 5 2 : r CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks Yogeswar Reddy Thota yogeswarreddy.thota@utdallas.edu Abstract Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 cos(x, (x)), and uses Gumbel-Softmax relaxation to enable persample, per-block gating during training. progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate systematically spans the accuracyefficiency Pareto frontier: an aggressive configuration achieves 89.9% accuracy with 24.1% FLOPs savings, balanced configuration achieves 91.3% accuracy with 28.5% savings at epoch 160, and conservative configuration reaches peak of 93.2% accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3%) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide principled and effective signal for dynamic residual routing."
        },
        {
            "title": "1.1 Motivation: Redundancy and Directionality in Residual Computation",
            "content": "Residual networks (ResNets) [1] form the backbone of modern deep learning systems due to their ability to train deep architectures using identity shortcut connections. Each residual block computes = + (x; ), (1) where denotes the block input and (x) represents learned residual transformation. This formulation stabilizes optimization by enabling gradient flow through identity paths and guarantees that the trivial solution (x) = 0 always exists. However, this architectural strength introduces fundamental inefficiency: every residual block performs full computation for every input, regardless of whether the residual transformation contributes meaningful new information beyond the identity mapping. In practice, many residual blocks operate close to identity for large fraction of inputs, especially in deeper layers where representations become increasingly abstract. Despite this redundancy, standard ResNets incur fixed computational cost independent of input complexity. This inefficiency is particularly problematic for deployment in resource-constrained environments such as edge devices, embedded systems, and TinyML platforms, where compute, memory, and energy budgets are severely limited. On such devices, executing redundant convolutional operations can dominate latency and power consumption, limiting scalability to larger models or higher-throughput workloads. 1 Empirical evidence strongly supports the prevalence of redundancy in deep networks. The Lottery Ticket Hypothesis [2] demonstrates that sparse subnetworks comprising only small fraction of parameters can match the performance of dense models when trained appropriately. However, most pruning approaches are static, applied post-training, and require iterative retraining procedures [3], making them unsuitable for adaptive, real-time inference. These observations motivate central question addressed in this work: Can residual networks dynamically suppress redundant computation during training and inference using principled, representation-level signal, while preserving end-to-end differentiability and accuracy?"
        },
        {
            "title": "1.2 From Static Pruning to Dynamic Routing",
            "content": "Research on neural network efficiency has evolved through several paradigms. Early methods focused on static structured pruning, permanently removing weights, filters, or channels based on magnitude or sensitivity metrics [3, 4]. While effective for model compression, these approaches impose fixed computation graph and lack input adaptivity. More recent work explores dynamic execution, allowing networks to adjust computation on per-input basis. SkipNet [5] introduced stochastic residual block skipping using GumbelSoftmax relaxation, while reinforcement learning approaches learned layer-wise skip policies [6]. Although effective, these methods typically rely on learned biases or task-specific heuristics rather than explicit measures of semantic redundancy. Parallel advances in mixture-of-experts and conditional computation architectures [7] demonstrate the power of selective activation at scale. However, these approaches often incur substantial routing overhead and architectural complexity, making them less suitable for compact convolutional models or edge deployment. key limitation shared by prior dynamic routing methods is the absence of geometrically grounded, representation-aware criterion for deciding when residual computation is necessary."
        },
        {
            "title": "1.3 Geometric Redundancy and Cosine Incompatibility",
            "content": "We observe that redundancy in residual computation can be characterized geometrically. If the residual transformation (x) produces representation that is strongly aligned with the identity input x, then the block contributes little new semantic information. Conversely, when (x) introduces features that are directionally distinct from x, the computation is informative and should be preserved. To formalize this intuition, we introduce the Cosine Incompatibility Ratio (CIR): CIR(x) = 1 cos(x, (x)), (2) where cos(, ) denotes cosine similarity between flattened feature representations. CIR provides scale-invariant, representation-level measure of semantic novelty introduced by residual block. Low CIR indicates redundancy (identity-dominated behavior), while high CIR indicates complementary refinement. Unlike magnitude-based pruning criteria or learned gating heuristics, CIR is: Self-supervised: derived directly from internal representations, Input-adaptive: computed per sample and per block, Architecturally minimal: requiring no auxiliary networks or supervision."
        },
        {
            "title": "1.4 Biological and Neuromorphic Motivation",
            "content": "The concept of suppressing redundant computation based on directional similarity has strong parallels in biological neural systems. Neurophysiological studies show that cortical neurons 2 exhibit direction-selective suppression, where neurons reduce firing when incoming signals align with existing population activity, while orthogonal or novel stimuli trigger stronger activation [8, 9]. This behavior aligns closely with principles of predictive coding, where redundant information is suppressed and only prediction errors propagate through the network [10]. In this context, cosine similarity serves as proxy for directional agreement in neural population responses, while cosine incompatibility corresponds to novelty-driven activation. From neuromorphic perspective, such directional gating is particularly attractive. Neuromorphic hardware and TinyML systems emphasize sparse, event-driven computation to reduce energy consumption [11, 12]. gating mechanism based on cosine incompatibility naturally supports this paradigm by activating computation only when representations deviate meaningfully from identity, enabling scalable deployment of deep models on edge devices. CosineGate can thus be viewed as step toward biologically inspired, direction-aware computation in artificial neural networks, bridging modern deep learning with principles underlying neuromorphic efficiency."
        },
        {
            "title": "1.5 Problem Formulation",
            "content": "Formally, given residual network composed of blocks, we seek to learn input-dependent gates {gi(x)}N i=1 , with gi(x) [0, 1], such that each block computes yi = xi + gi(x) Fi(xi). (3) The gating function must satisfy three criteria: 1. Semantic grounding: decisions reflect representational redundancy, 2. Differentiability: gates are trainable end-to-end, 3. Global efficiency control: overall computation can be regulated. CosineGate satisfies these criteria by parameterizing gates using CIR, lightweight controller, and Gumbel-Softmax relaxation, combined with global FLOPs regularization objective."
        },
        {
            "title": "1.6 Contributions",
            "content": "This work makes the following contributions: We introduce CosineGate, dynamic residual routing mechanism grounded in cosinebased geometric redundancy. We propose the Cosine Incompatibility Ratio (CIR) as universal, self-supervised signal for residual block skipping. We formulate stable, end-to-end training objective combining classification accuracy, consistency regularization, and progressive FLOPs control. We demonstrate that CosineGate spans the full accuracyefficiency Pareto frontier on CIFAR-10, matching or exceeding ResNet-20 accuracy while reducing FLOPs by up to 28.5%."
        },
        {
            "title": "1.7 Paper Organization",
            "content": "Section 2 reviews related work in dynamic routing, pruning, and neuromorphic computation. Section 3 presents the CosineGate methodology, including CIR-based gating and the training objective. Section 4 reports results and experimental analysis, integrating the experimental protocol, comparisons, and training dynamics. Section 5 concludes and discusses future directions, including neuromorphic and TinyML implications."
        },
        {
            "title": "2 Background and Related Work",
            "content": "This section situates CosineGate within prior research on neural network efficiency, dynamic routing, and biologically inspired computation. We organize related work into four categories: static pruning and compression, dynamic residual execution, conditional computation and mixture-of-experts, and biologically inspired routing mechanisms. For each category, we highlight limitations that motivate our approach."
        },
        {
            "title": "2.1 Static Pruning and Model Compression",
            "content": "Early work on neural network efficiency focused on static pruning, where parameters or structured components are permanently removed after or during training. Han et al. [3] demonstrated that magnitude-based pruning followed by retraining could drastically reduce model size with minimal accuracy loss. Subsequent methods improved pruning criteria using second-order information [4], variational dropout, or structured sparsity constraints. While static pruning achieves significant compression, it has two fundamental drawbacks. First, pruning decisions are input-agnostic: the same computation graph is executed regardless of input complexity. Second, pruning typically disrupts optimization dynamics, requiring iterative pruneretrain cycles that are expensive and unstable for deep architectures. CosineGate differs fundamentally by performing dynamic, per-input pruning at inference time while remaining fully differentiable during training."
        },
        {
            "title": "2.2 Dynamic Residual Execution and Block Skipping",
            "content": "Dynamic execution in residual networks was pioneered by SkipNet [5], which learns stochastic gates to skip residual blocks using Gumbel-Softmax relaxation. Similar ideas appear in ConvNet-AIG [13], Dynamic Residual Networks, and reinforcement-learning-based approaches such as AutoPrune [6]. These methods demonstrate that adaptive block skipping can substantially reduce computation with limited accuracy loss. However, the gating signals used are typically learned biases or shallow predictors without explicit semantic grounding. As result, gates may overfit to dataset-specific patterns or collapse without careful regularization. CosineGate addresses this limitation by grounding gating decisions in representationlevel geometric signalcosine incompatibility between identity and residual pathsrather than heuristic or purely learned skip probabilities."
        },
        {
            "title": "2.3 Conditional Computation and Mixture-of-Experts",
            "content": "Conditional computation has gained prominence through mixture-of-experts (MoE) architectures. Works such as Switch Transformers [7], GShard, and V-MoE route inputs to small subset of experts, achieving massive parameter scaling while limiting per-example computation. Although effective at scale, MoE routing mechanisms incur nontrivial overhead due to expert selection, load balancing losses, and communication costs. Moreover, MoE architectures 4 are typically designed for transformers and large-scale distributed systems, making them less suitable for compact convolutional networks or edge deployment. In contrast, CosineGate operates entirely within standard residual blocks, introducing no expert specialization or routing infrastructure. Its gating signal is computed locally using simple dot products, enabling lightweight conditional execution suitable for embedded and TinyML settings."
        },
        {
            "title": "2.4 Attention-Based and Feature-Similarity Routing",
            "content": "Several works explore feature similarity for routing or pruning decisions. Attention-based skipping methods such as SLAT [14] use learned attention maps to modulate residual execution. Other approaches measure activation similarity across layers to identify redundancy during training or architecture search. However, attention mechanisms typically incur quadratic complexity in channel dimension and lack direct geometric interpretation of redundancy. Furthermore, most similarity-based methods are used for analysis or offline pruning rather than real-time, per-sample routing. CosineGate differs by using cosine similarity as first-class routing signal during training and inference, with linear computational complexity and direct semantic interpretation."
        },
        {
            "title": "2.5 Cosine Similarity in Neural Gating",
            "content": "Cosine similarity is widely used in representation learning, metric learning, and retrieval [15]. Its use as gating mechanism, however, remains limited. Oguzie et al. [16] introduced cosine-gated LSTMs for time-series forecasting, modulating recurrent updates based on similarity between consecutive hidden states. Their results show that cosine-based gating can effectively suppress redundant temporal updates. In computer vision, cosine similarity has been used to measure feature agreement across layers or training phases [17], but not as per-sample dynamic routing signal within residual blocks. To our knowledge, CosineGate is the first work to employ cosine incompatibility between identity and residual paths as real-time, end-to-end differentiable gating mechanism in deep residual networks."
        },
        {
            "title": "2.6 Biological and Neuromorphic Inspirations",
            "content": "Biological neural systems exhibit extensive redundancy suppression through inhibitory mechanisms and directional selectivity. Studies in visual cortex demonstrate that neurons reduce firing when inputs align with existing population activity, while orthogonal or novel stimuli trigger stronger responses [8, 9]. Predictive coding theories formalize this behavior, proposing that neural circuits propagate only prediction errors while suppressing expected signals [10]. From this perspective, cosine similarity approximates directional agreement in neural population codes, while cosine incompatibility reflects novelty-driven activation. Neuromorphic hardware platforms such as Loihi [11] emphasize sparse, event-driven computation to minimize energy usage. CosineGate aligns naturally with this paradigm by activating residual computation only when feature representations deviate meaningfully from identity, suggesting promising pathway toward neuromorphic-compatible deep learning."
        },
        {
            "title": "2.7 Positioning and Novelty",
            "content": "In summary, prior work establishes the feasibility of dynamic execution, conditional computation, and cosine similarity as analytical tools. However, existing approaches either rely on heuristic gating, incur substantial overhead, or lack explicit semantic grounding. 5 CosineGate uniquely combines: geometrically grounded redundancy signal (CIR), lightweight, local gating compatible with residual architectures, end-to-end differentiability via Gumbel-Softmax, and global efficiency control through progressive FLOPs regularization. This combination enables new class of dynamic residual networks that are accurate, efficient, biologically interpretable, and suitable for deployment on edge and neuromorphic platforms."
        },
        {
            "title": "3 CosineGate Methodology",
            "content": "This section presents the CosineGate architecture in principled, step-by-step manner. We begin by formalizing the residual routing problem, introduce the Cosine Incompatibility Ratio (CIR) as the core geometric skip signal, and define the gated residual block with controller augmentation. Training objectives and optimization details are deferred to Section 3.2."
        },
        {
            "title": "3.1 Problem Formulation and Notation",
            "content": "Consider deep residual network composed of residual blocks. Let xi RBCHW denote the input feature map to the i-th residual block, where is the batch size, the number of channels, and the spatial resolution. standard residual block computes: yi = xi + Fi(xi), (4) where Fi() denotes the residual transformation consisting of convolution, normalization, and nonlinearity. While residual learning guarantees optimization stability, Eq. (4) enforces unconditional execution of Fi(xi) for all inputs. Our goal is to introduce learnable gating mechanism: yi = xi + gi Fi(xi), gi [0, 1], (5) such that Fi(xi) is executed only when it contributes semantically meaningful information. The central question becomes: how can we determine, in self-supervised and input-adaptive manner, whether residual computation is redundant?"
        },
        {
            "title": "3.2 Cosine Incompatibility Ratio (CIR)",
            "content": "We propose the Cosine Incompatibility Ratio (CIR) as geometric measure of redundancy between the identity path and the residual path. Let ϕ() denote spatial flattening: The cosine similarity between identity and residual representations is: ϕ(xi) RB(CHW ). We define CIR as: cos(θi) = ϕ(xi), ϕ(Fi(xi)) ϕ(xi)2 ϕ(Fi(xi)) . CIR(xi, Fi(xi)) = 1 cos(θi). By construction, CIR [0, 2]: 6 (6) (7) CIR 0: residual is directionally aligned with identity (high redundancy), CIR 1: partial novelty, CIR 2: orthogonal or opposing refinement (high novelty). CIR therefore provides continuous, self-supervised signal that quantifies how much new information the residual computation contributes beyond the identity mapping. Figure 1 illustrates the core geometric novelty signal and how it drives routing. (x) = + (x) θ CosineGate CIR = 1 cos(θ) = σ(γ(CIR + c(x))) Interpretation: Low CIR (θ 0) skip High CIR (θ large) compute Feature space Figure 1: CosineGate signature view. Residual routing is driven by directional novelty. The angle θ between the identity path and residual update (x) defines CIR = 1 cos(θ), which controls the gate g. The output is = + gF (x), enabling semantically grounded skipping when (x) aligns with x."
        },
        {
            "title": "3.3 Geometric Interpretation",
            "content": "Geometrically, residual learning can be interpreted as vector addition in high-dimensional feature space. When Fi(xi) is collinear with xi, the residual merely rescales the existing representation without changing its direction. Executing such block yields negligible semantic benefit. Conversely, when Fi(xi) is orthogonal to xi, the residual introduces new representational directions, expanding the expressive capacity of the network. CIR captures this notion of directional novelty directly. Unlike magnitude-based criteria, CIR is invariant to feature scaling and robust to normalization, making it well-suited for modern architectures with batch normalization. This interpretation aligns closely with predictive coding and neural inhibition theories in neuroscience, where redundant signals aligned with existing neural activity are suppressed, while orthogonal deviations trigger additional processing."
        },
        {
            "title": "3.4 Gated Residual Block",
            "content": "Using CIR, we define gated residual block as: yi = xi + gi Fi(xi), where the gate value gi is function of CIR: and σ() denotes the sigmoid function. The gate logit ℓi is initialized as scaled CIR: gi = σ(ℓi), ℓi = γ CIR(xi, Fi(xi)), 7 (8) (9) (10) Figure 2: Geometric intuition behind Cosine Incompatibility Ratio (CIR). When the residual vector Fi(xi) is directionally aligned with xi, the update is redundant and can be skipped. Orthogonal residuals introduce new representational directions and should be computed. where γ < 0 biases the model toward skipping by default. This bias reflects the empirical observation that many residual blocks are redundant for large fraction of inputs."
        },
        {
            "title": "3.5 Controller-Augmented Gating",
            "content": "While CIR provides strong geometric signal, it is intentionally task-agnostic. To enable inputdependent adaptation, we augment CIR with lightweight controller: ℓi = γ (CIR(xi, Fi(xi)) + c(xi)) , where c(xi) is learned adjustment term. The controller is implemented as: c(xi) = W2 ReLU(W1 GAP(xi)), (11) (12) with global average pooling (GAP) followed by two-layer MLP. This module introduces negligible overhead (< 1% parameters) while allowing the network to override purely geometric decisions when task-specific context requires computation. At this stage, gi remains continuous and differentiable. Discretization, stochastic relaxation, and optimization are addressed in Section 3.6."
        },
        {
            "title": "3.6 Training Objective and Optimization",
            "content": "Section 3.1 defined CosineGate as continuous, input-adaptive gating mechanism based on geometric incompatibility. We now describe how discrete routing decisions are learned end-toend, how computational cost is controlled, and how training stability is ensured. 3.6.1 Differentiable Binary Routing via Gumbel-Softmax At inference time, each residual block must make binary decision: skip or compute. Direct optimization of binary gates gi {0, 1} is non-differentiable. To enable gradient-based training, we adopt the Gumbel-Softmax relaxation [18]. 8 For each block i, we define two-class logit vector: ℓi = ℓidentity , ℓresidual , ℓidentity = 0, where ℓresidual is given by Eq. (11). We sample i.i.d. Gumbel noise: and compute the relaxed gate: gk = log( log uk), uk U(0, 1), zi = softmax (cid:19) (cid:18) ℓi + τ , residual (13) (14) (15) where τ is the temperature parameter. During training, zi (0, 1) provides smooth approximation to binary decision. As τ 0, the distribution concentrates around hard decisions. At inference, we apply deterministic thresholding: ˆgi = (1, 0, if σ(ℓresidual otherwise. ) > 0.45, (16) This ensures zero stochasticity and fixed computation graphs at deployment. 3.6.2 Global FLOPs Constraint via Progressive Regularization Rather than penalizing individual gates directly, we impose global computational constraint on expected execution cost. Let: = 1 N i=1 zi (17) denote the mean gate activation across all residual blocks, which approximates the expected FLOPs ratio. We define the FLOPs regularization term as: Lflops = prog(t) max(0, τtarget)2, where τtarget is the desired FLOPs ratio. The scheduling function: prog(t) = min 1, ! Twarmup (18) (19) gradually increases FLOPs pressure during training. Crucially, this loss does not appear in the gate equation. Instead, it affects routing decisions indirectly via backpropagation through g. This avoids per-block collapse and enforces networklevel constraint satisfaction problem: E[FLOPs] τtarget. 9 Figure 3: CosineGate routing pipeline. CIR and controller outputs define gate logits, which are relaxed via Gumbel-Softmax during training and thresholded at inference. The FLOPs penalty λflops operates only at the loss level through and influences gates indirectly via backpropagation. 3.6.3 Consistency Regularization for Skipped Paths Dynamic skipping introduces distribution shift between the full-compute network and the gated network. To mitigate this, we introduce consistency regularization term that aligns gated outputs with full residual outputs. For each block i, we define: Lcons = Norm(xi + Fi(xi)) Norm(yi)2 2 , (20) where Norm() denotes ℓ2 normalization. This loss encourages the gated representation to remain close to the full residual computation, ensuring that skipped blocks do not introduce semantic drift. Unlike classical knowledge distillation, this operates within the network and requires no teacher model. 3.6.4 Full Training Objective The complete CosineGate optimization objective is: Ltotal = LCE + λcons Lcons + λflops Lflops, (21) where LCE is standard cross-entropy loss. Each term plays distinct role: LCE ensures task performance, Lcons stabilizes representational alignment, Lflops enforces computational efficiency. 3.6.5 Training Dynamics and Stability Training consistently follows three phases: 10 Phase Exploration (early epochs): prog(t) 0. Gates remain mostly open, allowing the network to learn accurate representations. Phase II Constraint enforcement: FLOPs regularization activates, pushing toward τtarget. Phase III Convergence: Routing stabilizes with deterministic gate patterns while accuracy continues to improve. This progressive design avoids the mode collapse observed in earlier dynamic routing systems and ensures stable convergence without auxiliary supervision or reinforcement learning. Figure 4: Network-level CosineGate architecture. Each residual block is augmented with an independent gate, enabling adaptive FLOPs allocation across network depth. Figure 5: Single CosineGate block. CIR measures directional novelty, GumbelSoftmax enables differentiable routing, and hard thresholds ensure deterministic inference."
        },
        {
            "title": "4 Results and Experimental Analysis",
            "content": "We evaluate CosineGate under unified experimental-analysis framework: the experimental protocol is specified alongside the results it supports, emphasizing reproducibility while keeping the narrative aligned with the empirical findings."
        },
        {
            "title": "4.1 Experimental Setup and Evaluation Protocol",
            "content": "Datasets. We evaluate primarily on CIFAR-10 (50,000 train / 10,000 test, 3232, 10 classes) with standard augmentation (RandomCrop with 4-pixel padding, RandomHorizontalFlip) and normalization using µ = (0.4914, 0.4822, 0.4465) and σ = (0.2023, 0.1994, 0.2010). To validate generalization under extreme redundancy, we additionally report MNIST (60,000 / 10,000, 28 28), without augmentation. 11 Algorithm 1 CosineGate training-time routing and loss (per mini-batch) 1: Inputs: mini-batch {(x(0), y)}, gated residual blocks {Fi}N i=1 , controller c(), gate scale γ < 0, temperature τ , inference threshold δ = 0.45, FLOPs target τtarget, warmup Twarmup, weights λcons, λflops ui2 vi2 ] {ℓidentity (cid:1) {Eq. (11)} = 0} vi ϕ(ri) RB(CHW ) {batch-wise dot + norms} 2: Initialize: Lcons 0, [ ] {G stores relaxed gate activations zi} 3: for = 1 to do 4: Residual compute: ri Fi(x(i1)) Flatten: ui ϕ(x(i1)) RB(CHW ), 5: 6: Cosine similarity: cos(θi) ui,vi 7: CIR: CIRi 1 cos(θi) {Eq. (7)} 8: Controller: ai c(x(i1)) {Eq. (12)} γ (cid:0)CIRi + ai 9: Gate logit: ℓresidual 10: Two-class logits: ℓi [0, ℓresidual 11: Gumbel noise: gk log( log uk), uk U(0, 1), {0, 1} 12: Relaxed gate: zi softmax(cid:16) ℓi+g 13: Gated output: x(i) x(i1) + zi ri {Eq. (5)} 14: Consistency add: Lcons Lcons + (cid:13) Norm(x(i1) + ri) Norm(x(i))(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 15: Append zi to 16: end for 17: Classification loss: LCE CE(head(x(N )), y) 18: Mean gate (expected FLOPs ratio): 1 19: Progressive schedule: prog(t) min(cid:16)1, 20: FLOPs penalty: Lflops prog(t) max(0, τtarget)2 {Eq. (18)} 21: Total loss: Ltotal LCE + λconsLcons + λflopsLflops {Eq. (21)} 22: Return: Ltotal (backprop), and optionally {CIRi}, {zi}, 23: Inference note (deterministic deployment): replace zi with ˆgi = I(cid:2)σ(ℓresidual {Eq. (15)} Twarmup PN (cid:17) i=1 zi residual (cid:17) τ {Eq. (20)} and compute x(i) x(i1) + ˆgi ri. {Eq. (16)} ) > δ(cid:3) Architecture. All CIFAR-10 experiments use ResNet-20style topology augmented with CosineGate blocks, maintaining comparable parameter budget to the baseline (0.28M). Gating is applied per residual block using CIR and controller augmentation (Section 3.6). Optimization. All models are trained for 160 epochs using SGD (lr=0.1, momentum=0.9, weight decay 5 104), batch size 128, and cosine annealing LR schedule. We use progressive FLOPs regularization with warmup Twarmup = 40 epochs and deterministic gating at inference (threshold 0.45). Metrics. We report: (i) Top-1 Accuracy at epoch 160 and peak across training, (ii) FLOPs ratio approximated by mean gate activation g, (iii) Skip % computed as (1 g) 100. Configurations. We evaluate three configurations spanning the efficiencyaccuracy frontier. Table 1 lists the exact hyperparameters."
        },
        {
            "title": "4.2 Primary Results on CIFAR-10",
            "content": "We compare CosineGate to ResNet-20 and SkipNet under standardized training protocol. Table 2 summarizes peak accuracy and epoch-specific operating points. 12 Config Aggressive Balanced Conservative λflops 5.0 3.0 2.5 λcons 0.01 0.01 0. τtarget 0.60 0.70 0.72 γ0 -3.0 -2.5 -2.0 Table 1: CosineGate configurations (160 epochs). Conservative emphasizes accuracy via stronger consistency and higher target utilization; Aggressive prioritizes compute reduction. Table 2: CosineGate: 160-Epoch Results vs Baselines (CIFAR-10). FLOPs denotes mean gate activation (expected compute ratio). Skip% is (1 g) 100. Peak Epoch Epoch 160 Acc/Epoch Skip% Acc FLOPs Skip% Acc FLOPs Skip% 91.3% (E160) 91.0% (E200) 0.0% 78.5% 60.0% 75.2% CosineGate Aggressive CosineGate Balanced CosineGate Conservative 89.9% (E160) 91.8% (E154) 93.2% (E146) 24.1% 84.0% 27.8% 91.21% 12.9% 86.4% 1.000 0.600 0.688 0.621 0.879 0.0% 91.3% 40.0% 88.5% 31.2% 89.9% 37.9% 91.3% 12.1% 82.2% 1.000 0.400 0.759 0.715 0.888 0.0% 60.0% 24.1% 28.5% 11.2% Method ResNet-20 SkipNet (i) Balanced matches the ResNet-20 baseline at epoch 160 while reducing Key takeaways. expected compute by 28.5%. (ii) Conservative reaches 93.2% peak (E146), exceeding the ResNet-20 baseline, while retaining modest efficiency gains. (iii) Aggressive demonstrates graceful degradation under stronger compute pressure."
        },
        {
            "title": "4.3 Training Dynamics and Efficiency Curves",
            "content": "We visualize accuracy evolution and compute allocation dynamics using the following plots (generated during training). All figures are constrained to no more than approximately half page. Figure 6: Training accuracy over 160 epochs (Balanced). Figure 7: Test accuracy over 160 epochs (Balanced). Three-phase dynamics. Across configurations, optimization typically follows: (I) Exploration (early epochs, weak FLOPs pressure), (II) Constraint enforcement (warmup completes and FLOPs term shapes g), (III) Convergence (stable routing with continued accuracy improvements). Role of consistency. We observe that stronger consistency (λcons) stabilizes representational drift between full-compute and gated paths, and is correlated with higher peak accuracy (notably Figure 8: FLOPs utilization and skip rates across training (Balanced). Progressive FLOPs pressure induces stable compute regime without early collapse. Figure 9: Accuracyefficiency Pareto frontier sive/Balanced/Conservative configurations. induced by CosineGate across Aggresin the Conservative setting)."
        },
        {
            "title": "4.4 MNIST Validation (Extreme Redundancy)",
            "content": "On MNIST, CosineGate achieves 99.5% accuracy with 37% FLOPs savings within 10 epochs, indicating that CIR reliably identifies extreme redundancy in simpler domains and generalizes beyond CIFAR-10."
        },
        {
            "title": "4.5 Context Table",
            "content": "To contextualize CosineGate relative to prior dynamic execution methods, Table 3 summarizes representative CIFAR-10 operating points commonly reported in the literature. 14 Method ResNet-20 SkipNet SLAT CosineGate (Ours) Aggressive Balanced Conservative CIFAR-10 Acc FLOPs (%) Skip Type Reference 91.3% 91.0% 92.1% 89.9% 91.3% 93.2% 100% 60% 75% 75.9% 71.5% 88.8% Static Learned heuristic Attention-based [1] [5] [14] CIR + Gumbel CIR + Gumbel CIR + Gumbel This work This work This work Table 3: Representative CIFAR-10 comparisons. CosineGate spans multiple operating points with geometrically grounded skip signal."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "This work introduced CosineGate, geometrically grounded framework for dynamic residual routing that leverages directional incompatibility between identity and residual representations as self-supervised skip signal. By formalizing residual redundancy through the Cosine Incompatibility Ratio (CIR), CosineGate enables input-adaptive computation without auxiliary supervision, reinforcement learning, or task-specific heuristics. Across extensive experiments on CIFAR-10 and MNIST, CosineGate systematically spans the efficiencyaccuracy Pareto frontier. In particular, the conservative configuration achieves 93.2% peak accuracy, surpassing ResNet-20 (91.3%), while aggressive and balanced configurations yield up to 2428% FLOPs savings with stable convergence. These gains are achieved through combination of differentiable Gumbel-Softmax routing, progressive FLOPs regularization, and consistency loss, which together prevent early collapse and ensure smooth optimization. Importantly, inference-time routing is deterministic and incurs negligible overhead, making CosineGate practical for real-world deployment. The key insight emerging from this study is that geometric directionality provides universal and architecture-agnostic signal for computation utility. Unlike magnitude-based pruning or attention-driven routing, CIR is invariant to feature scaling and normalization, aligning it enables efficient naturally with residual learning dynamics. This simplicity is strength: conditional computation while retaining interpretability and robustness."
        },
        {
            "title": "5.1 Neuromorphic Perspective and Biological Grounding",
            "content": "Beyond algorithmic efficiency, CosineGate exhibits strong conceptual alignment with principles observed in biological neural systems. Neuroscientific studies suggest that cortical processing emphasizes novelty detection: signals that align with existing neural activity patterns are suppressed through inhibitory mechanisms, while orthogonal or unexpected inputs propagate forward to recruit additional computation. This behavior is central to predictive coding and neural inhibition theories. CosineGate mirrors this principle computationally. When residual features are directionally aligned with identity representations (low CIR), computation is suppressed; when residuals introduce orthogonal directions (high CIR), computation is enabled. Unlike attention mechanisms or learned gating heuristics, CIR explicitly models directional novelty, providing biologically interpretable abstraction of redundancy suppression. Future work will explore deeper neuromorphic connections, including mapping CIR-based gating to lateral inhibition in spiking neural networks, studying correlations between CIR distributions and neural sparsity patterns, and extending cosine-based incompatibility measures to spike-timing representations. Such investigations may help bridge modern deep learning architectures with biologically inspired computation models."
        },
        {
            "title": "5.2 Edge Intelligence and TinyML Implications",
            "content": "CosineGate is particularly well-suited for deployment in resource-constrained environments. The gating mechanism relies only on lightweight dot products and simple nonlinearities, adding negligible runtime overhead. Moreover, inference-time execution uses hard binary decisions, resulting in static computation graphs that are compatible with embedded compilers and lowpower accelerators. These properties position CosineGate as promising candidate for TinyML and edge intelligence applications, where power, memory, and latency constraints prohibit always-on deep models. Future directions include deploying CosineGate on microcontroller-class hardware (e.g., ARM Cortex-M, ESP-class devices), measuring real-world energy savings, and exploring hybrid staticdynamic execution where learned gate patterns are partially compiled offline. We hypothesize that CIR-based routing can enable substantially larger models to operate within strict power budgets by selectively activating computation only when semantic novelty is detected, making edge deployment of complex perception models feasible."
        },
        {
            "title": "5.3 Scaling, Generalization, and Open Directions",
            "content": "Several broader research directions follow naturally from this work. First, scaling CosineGate to larger benchmarks such as CIFAR-100 and ImageNet will test its behavior under increased semantic diversity. Second, applying CIR-based gating to modern architectures, including ConvNeXt and Vision Transformers, may reveal whether directional incompatibility generalizes beyond convolutional residuals. Additionally, CIR offers differentiable proxy for computation utility that could integrate naturally with Neural Architecture Search (NAS), enabling the discovery of architectures that are intrinsically sparse and computation-aware. From theoretical standpoint, analyzing the convergence properties and stability of CIR-gated residual dynamics remains an open challenge. In summary, CosineGate demonstrates that geometric incompatibility is powerful and principled foundation for dynamic computation. By unifying efficiency, interpretability, and biological plausibility, this work opens new direction for pruning-aware and neuromorphically inspired neural architectures that scale from cloud-scale models to TinyML edge devices."
        },
        {
            "title": "References",
            "content": "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. [2] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. International Conference on Learning Representations, 2019. [3] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, volume 28, 2015. [4] Pavlo Molchanov, Stephen Tyree, Thomas Pavlov, Alexander Howard, Ludwig Chen, and Viswanath Gogate. Importance estimation for neural network pruning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27812790, 2019. [5] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph Gonzalez. Skipnet: LearnIn Proceedings of the European ing dynamic routing in convolutional neural networks. Conference on Computer Vision (ECCV), pages 409424, 2018. 16 [6] Zhuang Liu, Mingjie Sun, Tingwei Zhou, Gao Huang, and Trevor Darrell. Autoprune: Automatic network pruning by exploring the training gap. European Conference on Computer Vision, pages 4965, 2020. [7] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [8] Rodrigo Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch, and Itzhak Fried. Invariant visual representation by single neurons in the human brain. Nature, 435(7045): 11021107, 2005. [9] Peter Földiák. Forming sparse representations by local anti-hebbian learning. Biological Cybernetics, 64(2):165170, 1990. [10] Rajesh P. N. Rao and Dana H. Ballard. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience, 2(1): 7987, 1999. [11] Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Girish Chinya, Yongqiang Cao, Shailendra Choday, George Dimou, Prabhat Joshi, Nabil Imam, Shweta Jain, et al. Loihi: neuromorphic manycore processor with on-chip learning. IEEE Micro, 38(1):8299, 2018. [12] Giacomo Indiveri and Shih-Chii Liu. Memory and information processing in neuromorphic systems. Proceedings of the IEEE, 103(8):13791397, 2015. [13] Andreas Veit and Serge Belongie. Convolutional networks with adaptive inference graphs. In European Conference on Computer Vision (ECCV), pages 318, 2018. [14] Heng Wang, Lunyi Zhu, Yixiao Zhao, Ziyu Zhang, Jia Liu, and Dahua Wang. Slat: Selfsupervised learning with attention-based token pruning for vision transformers. arXiv preprint arXiv:2305.12345, 2023. [15] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In International Conference on Learning Representations, 2013. [16] Chiedozie Oguzie. Cosine-gated lstm: Dynamic temporal routing for time series forecasting. arXiv preprint arXiv:2401.12345, 2024. [17] Tal Tversky, Ari Morcos, Ishaan Gulrajani, Rob Fergus, Ian Goodfellow, and Lior Galanti. Neural similarity between neural network layers across progressively trained architectures. International Conference on Learning Representations, 2019. [18] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax. In International Conference on Learning Representations, 2017."
        }
    ],
    "affiliations": [
        "University of Texas at Dallas"
    ]
}