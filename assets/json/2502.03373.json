{
    "paper_title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
    "authors": [
        "Edward Yeo",
        "Yuxuan Tong",
        "Morry Niu",
        "Graham Neubig",
        "Xiang Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot."
        },
        {
            "title": "Start",
            "content": "Demystifying Long Chain-of-Thought Reasoning in LLMs Edward Yeo * 1 Yuxuan Tong * 2 Morry Niu 1 Graham Neubig 3 Xiang Yue * 3 5 2 0 2 5 ] . [ 1 3 7 3 3 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scaling inference compute enhances reasoning in large language models (LLMs), with long chainsof-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot. 1. Introduction Large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Anthropic, 2023; OpenAI, 2023) have *Project Lead. 1IN.AI 2Tsinghua University. Work started when interning at CMU. 3Carnegie Mellon University. Correspondence to: Xiang Yue <xyue2@andrew.cmu.edu>. demonstrated remarkable reasoning abilities in domains like mathematics (Cobbe et al., 2021) and programming (Chen et al., 2021). key technique for enabling reasoning abilities in LLMs is chain-of-thought (CoT) prompting (Wei et al., 2022), which guides models to generate intermediate reasoning steps before arriving at final answer. Despite these advancements, LLMs still struggle with highly complex reasoning tasks, such as mathematical competitions (Hendrycks et al., 2021), PhD-level scientific QA (Rein et al., 2024), and software engineering (Jimenez et al., 2024), even with CoT. Recently, OpenAIs o1 models (OpenAI, 2024) have demonstrated significant breakthroughs in these tasks. key distinguishing feature of these models is their ability to scale up inference compute with long CoTs, which include strategies such as recognizing and correcting mistakes, breaking down difficult steps, and iterating on alternative approaches, leading to substantially longer and more structured reasoning processes. Several efforts have attempted to replicate the performance of o1 models by training LLMs to generate long CoTs (Qwen Team, 2024b; DeepSeek-AI, 2025; Kimi Team, 2025; Pan et al., 2025; Zeng et al., 2025). Most of these approaches rely on verifiable rewards, such as accuracy based on ground-truth answers, which helps to avoid reward hacking in reinforcement learning (RL) at scale. However, comprehensive understanding of how models learn and generate long CoTs remains limited. In this work, we systematically investigate the underlying mechanics of long CoT generation. Specifically, we explore: 1) Supervised fine-tuning (SFT) for long CoTs the most direct way to enable long CoT reasoning. We analyze its scaling behavior and impact on RL, finding that long CoT SFT allows models to reach higher performance and also facilitates easier RL improvements than short CoT. 2) Challenges in RL-driven CoT scaling we observe that RL does not always stably extend CoT length and complexity. To address this, we introduce cosine length-scaling reward with repetition penalty, which stabilizes CoT growth while encouraging emergent reasoning behaviors such as branching and backtracking. 3) Scaling up verifiable signals for long CoT RL Verifiable reward signals are essential for stabilizing long CoT Demystifying Long Chain-of-Thought Reasoning in LLMs RL. However, scaling them up remains challenging due to the limited availability of high-quality, verifiable data. To address this, we explore the use of data containing noisy, web-extracted solutions (Yue et al., 2024b). While these silver supervision signals introduce uncertainty, we find that, with an appropriate mixture in SFT and filtration in RL, they show promise, especially in out-of-distribution (OOD) reasoning scenarios such as STEM problem-solving. 4) Origins of Long CoT Abilities and RL Challenges Core skills like branching and error validation are inherently present in base models, but effective RL-driven incentivization demands careful designs. We examine RL incentives on long CoT generation, trace reasoning patterns in pre-training data, and discuss nuances in measuring their emergence. 2. Problem Formulation In this section, we define the notation, followed by an overview of SFT and RL methods for eliciting long CoTs."
        },
        {
            "title": "Research Aim",
            "content": "Our goal is to demystify long chain-of-thought reasoning in LLMs. Through systematic analysis and ablations, we extract key insights and offer practical strategies to enhance and stabilize its performance. 2.1. Notation Let be query, and let be the output sequence. We consider LLM parameterized by θ, which defines conditional distribution over output tokens: πθ(yt x, y1:t1). We denote by CoT(y) the tokens in the generated output that constitute the chain-of-thought, which is often reasoning trace or explanatory sequence. The final answer can be separate set of tokens or simply the last part of y. In this work, we use the term long chain-of-thought (long CoT) to describe an extended sequence of reasoning tokens that not only exhibits larger-than-usual token length but also demonstrates more sophisticated behaviors such as: 1) Branching and Backtracking: The model systematically explores multiple paths (branching) and reverts to earlier points if particular path proves wrong (backtracking). 2) Error Validation and Correction: The model detects inconsistencies or mistakes in its intermediate steps and takes corrective actions to restore coherence and accuracy. 2.2. Supervised Fine-Tuning (SFT) common practice is to initialize the policy πθ via SFT (Lamb et al., 2016) on dataset DSFT = {(xi, yi)}N i=1, where yi can be normal or long CoT reasoning tokens. 2 2.3. Reinforcement Learning (RL) After optional SFT initialization, we can further optimize the generation of long CoT with reinforcement learning. Reward Function. We define scalar reward rt designed to encourage correct and verifiable reasoning. We only consider the outcome-based reward for the final answer produced, and do not consider process-based reward for the intermediate steps. We denote the term ranswer(y) to capture the correctness of the final solution. Policy Update. We adopted Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the default policy optimization method in our experiments. We also briefly discuss REINFORCE (Sutton & Barto, 2018) method in subsection 7.3. We adopt rule-based verifier as the reward function, which compares the predicted answer with the ground truth answer directly. The resulting updates push the policy to generate tokens that yield higher reward. 2.4. Training Setup We adopt Llama-3.1-8B (Meta, 2024) and Qwen2.5 -7B-Math (Qwen Team, 2024a) as the base models, which are representative general and math-specialized models respectively. For both SFT and RL, we use the 7,500 training sample prompt set of MATH (Hendrycks et al., 2021) by default, with which verifiable ground truth answers are provided. For SFT when ground truth answers are available, we synthesize responses by rejection sampling (Zelikman et al., 2022; Dong et al., 2023; Yuan et al., 2023; Gulcehre et al., 2023; Singh et al., 2023; Yue et al., 2024a; Tong et al., 2024). Specifically, we first sample fixed number of candidate responses per prompt and then filter by only retaining ones with final answers consistent with the corresponding ground truth answers. We also discuss data like WebInstruct (Yue et al., 2024b) that is more diverse but without gold supervision signals like ground truth answers in 5. We train the models with the OpenRLHF framework (Hu et al., 2024). 2.5. Evaluation Setup We focus on four representative reasoning benchmarks: MATH-500, AIME 2024, TheoremQA (Chen et al., 2023), and MMLU-Pro-1k (Wang et al., 2024a). Given that our training data is primarily in the mathematical domain, these benchmarks provide comprehensive framework for both in-domain (MATH-500 test set) and out-of-domain evaluations (AIME 2024, TheoremQA, MMLU-Pro-1k). By default, we generate from the models using temperature of = 0.7, top-p value of 0.95, and maximum output length of 16,384 tokens. Please refer to Appendix E.1 for further details on the evaluation setup. Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 1. Scaling curves of SFT and RL on Llama-3.1-8B with long CoTs and short CoTs. SFT with long CoTs can scale up to higher upper limit and has more potential to further improve with RL. 3. Impact of SFT on Long CoT 3.2. SFT Initialization for RL In this section, we compare long and short CoT data for SFT and in the context of RL initialization. 3.1. SFT Scaling To compare long CoT with short CoT, the first step is to equip the model with the corresponding behavior. The most straightforward approach is to fine-tune the base model on CoT data. Since short CoT is common, curating SFT data for it is relatively simple via rejection sampling from existing models. However, how to obtain high-quality long CoT data remains an open question. Setup. To curate the SFT data, for long CoT, we distill from QwQ-32B-Preview (we discuss other long CoT data construction methods in 3.3). For short CoT, we distill from Qwen2.5-Math-72B-Instruct, which is capable short CoT model in math reasoning. Specifically, we perform rejection sampling by first sampling candidate responses per prompt and then filtering for ones with correct answers. For long CoT, we use {32, 64, 128, 192, 256}, while for short CoT, we use {32, 64, 128, 256}, skipping one for efficiency. In each case, the number of SFT tokens is proportional to . We use the base model Llama-3.1-8B (Meta, 2024). Please refer to Appendix E.3 for more details about the SFT setup. Result. The dashed lines in Figure 1 illustrate that as we scale up the SFT tokens, long CoT SFT continues to improve model accuracy, whereas short CoT SFT saturates early at lower accuracy level. For instance, on MATH-500, long CoT SFT achieves over 70% accuracy and has yet to plateau even at 3.5B tokens. In contrast, short CoT SFT converges below 55% accuracy, with an increase in SFT tokens from approximately 0.25B to 1.5B yielding only marginal absolute improvement of about 3%. Takeaway 3.1 for SFT Scaling Upper Limit SFT with long CoT can scale up to higher performance upper limit than short CoT. (Figure 1) Since RL is reported to have higher upper limit than SFT, we compare long CoT and short CoT as different SFT initialization approaches for RL. Setup. We initialize RL using SFT checkpoints from 3.1, and train for four epochs, sampling four responses per prompt. Our approach employs PPO (Schulman et al., 2017) with rule-based verifier from the MATH dataset, using its training split as our RL prompt set. We adopt our cosine length scaling reward with the repetition penalty, which will be detailed in 4. Further details about our RL setup and hyperparameters can be found in Appendix E.4 & E.5.1 respectively. Result. The gap between solid and dashed lines in Figure 1 shows that models initialized with long CoT SFT can usually be further significantly improved by RL, while models initialized with short CoT SFT see little gains from RL. For example, on MATH-500, RL can improve long CoT SFT models by over 3% absolute, while short CoT SFT models have almost the same accuracies before and after RL. Takeaway 3.2 for SFT Initialization for RL SFT with long CoTs makes further RL improvement easier, while short CoTs do not. (Figure 1) 3.3. Sources of Long CoT SFT Data To curate long CoT data, we compare two approaches: (1) Construct long CoT trajectories by prompting short CoT models to generate primitive actions and sequentially combining them; (2) Distill long CoT trajectories from existing long CoT models that exhibit emergent long CoT patterns. Setup. To construct long CoT trajectories, we developed an Action Prompting framework (Appendix E.8) which defined the following primitive actions: clarify, decompose, solution step, reflection, and answer. We employed multi-step prompting with short CoT model (e.g., Qwen2.5-72B-Instruct) to sequence these actions, while stronger model, o1-mini-0912, generates reflec3 Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 2. Both Llama3.1-8B and Qwen2.5-Math-7B models trained under RL with the Classic Reward manifested emergent CoT length scaling past the context window size, resulting in decline in MATH-500 accuracy. The red points on the charts correspond to the iteration where the accuracy dropped to near zero. Terminated CoTs refer to responses that conclude within the context length. tion steps incorporating self-correction. For distilling long CoT trajectories, we use QwQ-32-Preview as the teacher model. In both approaches, we adopt the MATH training set as the prompt set and apply rejection sampling. To ensure fairness, we use the same base model (Llama-3.1-8B), maintain approximately 200k SFT samples, and use the same RL setup as in 3.2. Result. Table 1 shows that the model distilled from emergent long CoT patterns generalizes better than the constructed pattern, and can be further significantly improved with RL, while the model trained on constructed patterns cannot. Models trained with the emergent long CoT pattern achieve significantly higher accuracies on OOD benchmarks AIME 2024 and MMLU-Pro-1k, improving by 15-50% relatively. Besides, on the OOD benchmark TheoremQA, RL on the long CoT SFT model significantly improves its accuracy by around 20% relative, while the short CoT models performance does not change. This is also why we conduct most of our experiments based on distilled long CoT trajectories. Takeaway 3.3 for Long CoT Cold Start SFT initialization matters: high-quality, emergent long CoT patterns lead to significantly better generalization and RL gains.  (Table 1)  Table 1. Emergent long CoT patterns outperform constructed ones. All the models here are fine-tuned from the base model Llama-3.1-8B with the MATH training prompt set. Training Method Long CoT SFT Pattern MATH AIME 500 Theo. MMLU Pro-1k QA SFT SFT+RL Constructed Emergent Constructed Emergent 48.2 54.1 52.4 59.4 2.9 3.5 2.7 4.0 21.0 21.8 21.0 25. 18.1 32.0 19.2 34.6 4. Impact of Reward Design on Long CoT This section examines reward function design, with focus on its influence on CoT length and model performance. 4 4.1. CoT Length Stability Recent studies on long CoT (DeepSeek-AI, 2025; Kimi Team, 2025; Hou et al., 2025) suggest that models naturally improve in reasoning tasks with increased thinking time. Our experiments confirm that models fine-tuned on long CoT distilled from QwQ-32B-Preview tend to extend CoT length under RL training, albeit sometimes unstably. This instability, also noted by Kimi Team (2025); Hou et al. (2025), has been addressed using techniques based on length and repetition penalties to stabilize training. Setup. We used two different models fine-tuned on long CoT data distilled from QwQ-32B-Preview using the MATH train split, with context window size of 16K. The models were Llama3.1-8B and Qwen2.5-Math-7B. We used rule-based verifier along and simple reward of 1 for correct answers. We shall refer to this as the Classic Reward. More details can be found in Appendix E.5.2. Results. We observed that both models increased their CoT length during training, eventually reaching the context window limit. This led to decline in training accuracy due to CoTs exceeding the allowable window size. Additionally, different base models exhibited distinct scaling behaviors. The weaker Llama-3.1-8B model showed greater fluctuations in CoT length compared to Qwen-2.5-Math-7B, as illustrated in Figure 2. We also found that the rate at which CoTs exceeded the context window size leveled off at certain threshold below 1 (Figure 2). This suggests that exceeding the limit started to apply significant downward pressure on the CoT length distribution, and highlights the context window sizes role in implicit length penalization. Notably, trajectory might be penalized even without an explicit exceed-length penalty due to reward or advantage normalization, both of which are standard in RL frameworks. Takeaway 4.1 for CoT Length Stability CoT length does not always scale up in stable fashion. (Figure 2) Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 3. The Classic and Cosine Reward functions. The Cosine Reward varies with generation length. 4.2. Active Scaling of CoT Length We found that reward shaping can be used to stabilize emergent length scaling. We designed reward function to use CoT length as an additional input and to observe few ordering constraints. Firstly, correct CoTs receive higher rewards than wrong CoTs. Secondly, shorter correct CoTs receive higher rewards than longer correct CoTs, which incentivizes the model to use inference compute efficiently. Thirdly, shorter wrong CoTs should receive higher penalties than longer wrong CoTs. This encourages the model to extend its thinking time if it is less likely to get the correct answer. We found it convenient to use piecewise cosine function, which is easy to tune and smooth. We refer to this reward function as the Cosine Reward, visualized in Figure 3. This is sparse reward, only awarded once at the end of the CoT based on the correctness of the answer. The formula of CosFn can be found in equation 1 in the appendix. R(C, Lgen) = CosFn(Lgen, Lmax, rc CosFn(Lgen, Lmax, rw re, 0, rc L), 0 , rw ), if = 1, if = 0, if Lgen = Lmax. Hyperparameters: 0 : Reward (correct/wrong) for Lgen = 0, : Reward (correct/wrong) for Lgen = Lmax, rc 0/rw rc L/rw re : Exceed length penalty, Inputs: : Correctness (0 or 1), Lgen : Generation length. Setup. We ran experiments with the Classic Reward and the Cosine Reward. We used the Llama3.1-8B fine-tuned on long CoT data distilled from QwQ-32B-Preview using the MATH train split, as our starting point. For more details, see Appendix E.5.3. Result. We found that the Cosine Reward significantly stabilized the length scaling behavior of the models under RL, thereby also stabilizing the training accuracy and improving RL efficiency (Figure 4). We also observed improvements in model performance on downstream tasks (Figure 5). 5 Figure 4. Llama3.1-8B trained with length shaping using the Cosine Reward exhibited more stable (a) training accuracy and (b) response length. This stability led to improved performance on downstream tasks (Figure 5). Red points on the charts indicate iterations where training accuracy dropped to near zero. Takeaway 4.2 for Active Scaling of CoT Length Reward shaping can be used to stabilize and control CoT length while improving accuracy. (Figure 4, 5) 4.3. Cosine Reward Hyperparameters The Cosine Reward hyperparameters can be tuned to shape CoT length in different ways. Setup. We set up RL experiments with the same model finetuned on long CoT distilled from QwQ-32B-Preview, but with different hyperparameters for the Cosine Reward function. We tweaked the correct and wrong rewards rc 0, rc and observed their impact on the CoT lengths. For more details, see Appendix E.5.4. 0 , rw L, rw Result. We see from Figure 9 in the Appendix that if the reward for correct answer increases with CoT length (rc 0 < rc L), the CoT length increases explosively. We also see that the lower the correct reward relative to the wrong reward, the longer the CoT length. We interpret this as kind of trained risk aversion, where the ratio of the correct and wrong rewards determines how confident the model has to be about an answer for it to derive positive expected value from terminating its CoT with an answer. Takeaway 4.3 for Cosine Reward Hyperparameters Cosine Reward can be tuned to incentivize various length scaling behaviors. (Appendix Figure 9) 4.4. Context Window Size We know that longer contexts give model more room to explore, and with more training samples, the model eventually learns to utilize more of the context window. This raises an interesting question are more training samples necessary to learn to utilize larger context window? Setup. We set up 3 experiments using the same starting model fine-tuned on long CoT data distilled from Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 5. Performance of Llama-3.1-8B trained with different reward functions on variety of evaluation benchmarks. QwQ-32B-Preview with the MATH train split. We also used the latter as our RL prompt set. Each ablation used the Cosine Reward and repetition penalty with different context window size (4K, 8K, and 16K). For more details, see Appendix E.5.5. Result. We found that the model with context window size of 8K performed better than the model with 4K, as expected. However, we observed performance was better under 8K than 16K. Note that all three experiments used the same number of training samples (Figure 6). We see this as an indication that models need more training compute to learn to fully utilize longer context window sizes, which is consistent with the findings of (Hou et al., 2025). Takeaway 4.4 for Context Window Size Models might need more training samples to learn to utilize larger context window sizes. (Figure 6) 4.5. Length Reward Hacking We observed that with enough training compute, the model started to show signs of reward hacking, where it increased the lengths of its CoTs on hard questions using repetition rather than learning to solve them. We also noted fall in the branching frequency of the model, which we estimated by counting the number of times the pivot keyword alternatively, appeared in the CoT (Figure 10). We mitigated this by implementing simple -gram repetition penalty (Algorithm 1). We observed that the penalty was most effectively applied on repeated tokens, rather than as sparse reward for the entire trajectory. Similarly, we found that discounting the repetition penalty when calculating the return was effective. Specific feedback about where the repetition occurred presumably made it easier for the model to learn not to do it (see more in 4.6). Setup. We used the Llama3.1-8B model fine-tuned on long CoT data distilled from QwQ-32B-Preview. We ran two RL training runs, both using the Cosine Reward, but with and without the repetition penalty. For more details, please refer to Appendix E.5.6. Result. The repetition penalty resulted in better downstream task performance and also shorter CoTs, meaning there was better utilization of inference compute (Figure 5). Observation. Our experiments revealed relationship between the repetition penalty, training accuracy, and the Cosine Reward. When training accuracy was low, the Cosine Reward exerted greater upward pressure on CoT length, leading to increased reward hacking through repetition. This, in turn, required stronger repetition penalty. Future work could further investigate these interactions and explore dynamic tuning methods for better optimization. Takeaway 4.5 for Length Reward Hacking Length rewards will be hacked with enough compute (Figure 10), but this can be mitigated using repetition penalty. (Figure 5) 4.6. Optimal Discount Factors We hypothesized that applying the repetition penalty with temporal locality (i.e., low discount factor) would be most effective, as it provides stronger learning signal about the specific offending tokens. However, we also observed performance degradation when the discount factor for the correctness (cosine) reward was too low. To optimally tune both reward types, we modified the GAE formula in PPO to accommodate multiple reward ˆAt = types, each with its own discount factor γ: (cid:80)L mrm,t+l (st). For simplicity, we set λ = 1, which proved effective, though we did not extensively tune this parameter. γl (cid:80)M l=0 same Llama3.1-8B model Setup. We ran multiple RL experiments with the on QwQ-32B-Preview distilled long CoT data. We used the Cosine Reward and repetition penalty but with different combinations of discount factors. For more details, please see Appendix E.5.7. fine-tuned Result. lower discount factor effectively enforces the repetition penalty, whereas higher discount factor enhances the correctness reward and the exceed-length penalty. The 6 Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 6. Performance of Llama-3.1-8B trained with different context window sizes. All experiments used the same number of training samples. higher factor allows the model to be adequately rewarded for selecting correct answer earlier in the CoT (Figure 5). We observed rather interesting phenomenon where decreasing the discount factor γ of the correctness (cosine) reward increased the branching frequency in the models CoT, making the model quickly give up on approaches that did not seem to lead to correct answer immediately (Figure 11, Extract in Appendix D). We hypothesize that this short-term thinking was due to relatively small number of tokens preceding the correct answer receiving rewards, which means stepping stones to the right answer are undervalued. Such behavior degraded performance (Figure 5). However, we think this qualitative result might be of potential interest to the research community, due to its similarity to the relationship between behaviors like delayed gratification and the distribution of rewards given to the biological brain (Gao et al., 2021). Takeaway 4.6 for Optimal Discount Factors Different kinds of rewards and penalties have different optimal discount factors. (Figure 5) 5. Scaling up Verifiable Reward Verifiable reward signals like ones based on ground-truth answers are essential for stabilizing long CoT RL for reasoning tasks. However, it is difficult to scale up such data due to the limited availability of high-quality human-annotated verifiable data for reasoning tasks. As an attempt to counter this, we explore using other data that is more available despite more noise, like reasoning-related QA pairs extracted from web corpora. Specifically, we experiment with the WebInstruct dataset (Yue et al., 2024b). For efficiency, we construct WebInstruct-462k, deduplicated subset derived via MinHash (Broder et al., 1998). 5.1. SFT with Noisy Verifiable Data We first explore adding such diverse data to SFT. Intuitively, despite less reliable supervision signals, diverse data might facilitate the models exploration during RL. Setup. We experiment with three setups, varying the proportion of data without gold supervision signals: 0%, 100%, and approximately 50%. We conduct long CoT SFT by distilling from QwQ-32B-Preview. For data with gold supervision signals (MATH), ground truth answers are used for rejection sampling. In contrast, for data from WebInstruct without fully reliable supervision signals but with much larger scale, we sample one response per prompt from the teacher model without filtration. For RL here, we adopt the same setup as in 3.2, using the MATH training set. Result. Table 2 shows that incorporating silver-supervised data improves average performance. Adding WebInstruct data to long CoT SFT yields substantial 510% absolute accuracy gain on MMLU-Pro-1k over using MATH alone. Furthermore, mixing MATH and WebInstruct data achieves the best average accuracy across benchmarks. Table 2. Adding data with silver supervision signal is often beneficial. WebIT is the abbreviation of WebInstruct. Long CoT SFT Data 100% MATH 100% WebIT 50% MATH + 50% WebIT Training Method SFT SFT + RL SFT SFT + RL SFT SFT + RL MATH AIME 2024 500 Theo. MMLU Pro-1k QA 54.1 59. 41.2 44.6 53.6 57.3 3.5 4.0 0.8 1.9 4.4 3.8 21.8 25. 21.9 22.5 23.5 25.1 32.0 34.6 41.1 43.3 41.7 42.0 AVG 27.9 30.8 26.3 28.1 30.8 32.1 Takeaway 5.1 for SFT with Noisy Verifiable Data Adding noisy but diverse data to SFT leads balanced performance across different tasks.  (Table 2)  5.2. Scaling up RL with Noisy Verifiable Data We compare two main approaches to obtain rewards from noisy verifiable data: 1) to extract short-form answers and use rule-based verifier; 2) to use model-based verifier capable of processing free-form responses. Here key factor is whether the QA pair can have short7 Demystifying Long Chain-of-Thought Reasoning in LLMs form answer. So we also compare whether the dataset is filtered by only retaining samples with short-form answers. Setup. We implement the model-based verifier by prompting Qwen2.5-Math-7B-Instruct with the raw reference solution. To extract short-form answers, we first prompt Llama-3.1-8B-Instruct to extract from the raw responses and then apply rejection sampling with QwQ-32B-Preview. Specifically, we generate two responses per prompt from WebInstruct-462k and discard cases where neither response aligns with the extracted reference answers. This process yields approximately 189k responses across 115k unique prompts. Our case studies show that the rejection sampling drops many prompts due to: 1) many WebInstruct prompts lack short-form answers that our rule-based verifier can process effectively, and 2) some prompts are too difficult even for QwQ-32B-Preview. For SFT we train Llama-3.1-8B on the filtered dataset as initialization for reinforcement learning (RL). In the RL stage, we use the full 462k prompt set in the unfiltered setup and the 115k subset in the filtered setup, training with 30k prompts and 4 responses per prompt. Further details about the model-based verifier, the answer extraction and the RL hyperparameters can be found in Appendix & E.5.8 & E.6 & E.7 respectively. Table 3. Performance of RL with different verifiers and prompt filtering methods. All the models here are fine-tuned from Llama-3.1-8B. The MATH Baseline is the model trained with SFT and RL on MATH only in Table 2. The other models are trained with SFT by distillation from QwQ-32B-Preview and RL with different setups. Prompt Set Verifier Type MATH AIME 2024 500 Theo. MMLU Pro-1k QA MATH Baseline SFT Initialization Unfiltered Filtered Rule-Based Model-Based Rule-Based Model-Based 59.4 46.6 45.4 47.9 48.6 47.9 4.0 1. 3.3 3.5 3.3 3.8 25.2 23.0 25.9 26.2 28.1 26. 34.6 28.3 35.1 40.4 41.4 41.4 Result. Table 3 shows that RL with the rule-based verifier on the filtered prompt set with short-form answers achieves the best performance across most benchmarks under the same number of RL samples. This might indicate that rule-based verifier after appropriate filtration can produce the highest-quality reward signals from noisy verifiable data. Moreover, compared to the model trained on human-annotated verified data (MATH), leveraging noisy yet diverse verifiable data still significantly boosts performance on O.O.D. benchmarks, with absolute gains of up to 2.9% on TheoremQA and 6.8% on MMLU-Pro-1k. In contrast, applying rule-based verifier to unfiltered data results in the worst performance. This might be caused by its low training accuracy on free-form answers, while the model-based verifier achieves much better performance. Takeaway 5.2 for RL with Noisy Verifiable Data To obtain reward signals from noisy verifiable data, the ruled-based verifier after filtering the prompt set for short-form answers works the best.  (Table 3)  6. Exploration on RL from the Base Model DeepSeek-R1 (DeepSeek-AI, 2025) has demonstrated that long chain-of-thought reasoning can emerge by scaling up reinforcement learning compute on base model. Recent studies (Zeng et al., 2025; Pan et al., 2025) have attempted to replicate this progress by running relatively small number of RL iterations to observe the emergence of long CoT behavior (e.g., the aha moment (DeepSeek-AI, 2025), an emergent realization moment that enables critical functions like self-validation and correction). We also explore the method of RL from the base model in this section. 6.1. Nuances in Analysis Based on Emergent Behaviors Self-validation behaviors are sometimes flagged as emergent behaviors or aha-moment by the models exploration, since such patterns are rare in short CoT data. However, we notice that sometimes self-validation behaviors already exist in the base model and reinforcing them through RL requires strict conditions, such as strong base model. Setup. We follow the setup from Zeng et al. (2025) to train Qwen2.5-Math-7B using PPO with rule-based verifier on approximately 8k MATH level 3-5 questions, but we use our own rule-based verifier implementation. For inference, we adopt temperature = 0 (greedy decoding), as our preliminary experiments show that = 0 usually significantly outperforms > 0 for models obtained by direct RL from Qwen2.5-Math-7B. We use the maximum output length of 4096 tokens considering the training context length of 4096 tokens. Note that we use zero-shot prompting for the base model to avoid introducing biases to the output pattern. We select five representative keywords, wait, recheck, alternatively, retry and however from long CoT cases in previous works (OpenAI, 2024; DeepSeek-AI, 2025; Pan et al., 2025; Zeng et al., 2025), and calculate their frequencies to quantify the extent to which the model does selfvalidation. Further details about the RL hyperparameters can be found in Appendix E.5.9. Result. Figure 7 shows that our RL from Qwen2.5Math-7B effectively boosts the accuracies, but does not increase the frequency of the recheck pattern existing in the output of the base model, nor effectively incentivize other reflection patterns such as retry and alternatively. This indicates that RL from the base model does not necessarily incentivize reflection patterns, though significantly 8 Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 7. Dynamics of accuracies and reflection keyword rates on different benchmarks during our RL from the base model Qwen2.5-Math-7B. We do not see the keyword rates of wait, alternatively, and recheck get significantly improved during the RL training even though the accuracy is steadily increasing. Figure 8. Dynamics of the output token lengths and the coding rate on MATH-500 and the KL divergence of the policy over the base model on MATH Lv3-5 (training data) during our RL from Qwen2.5-Math-7B. boosting the performance. Sometimes such behaviors exist in the base models output and RL does not substantially enhance them. So we might need to be more careful about recognizing emergent behaviors. 6.2. Nuances in Analysis Based on Length Scaling The length scaling up is recognized as another important feature of the effective exploration of the model. However, we notice that sometimes length scaling up can be accompanied by the KL divergence decreasing, which raises the possibility that the length is influenced by the KL penalty and is just reverting back to the base models longer outputs, rather than reflecting the acquisition of long CoT ability. Setup. The setup is the same as in 6.1. Besides the output token length, we also calculate the coding rate. We classify the models output as coding if it contains the python, since Qwen2.5-Math-7B uses both natural language and coding to solve mathematical problems. Note that the coding output here is actually special form of natural language output, where the code in it is not executed, and the codes output is generated by the model. Result. Figure 8 (1) shows that the length of the output token increases after an initial drop, but never exceeds the initial length of the base model. Zeng et al. (2025) suggest that the initial drop may be due to the model transitioning from generating long coding outputs to shorter natural language outputs. However, Figure 8 (2) indicates that natural language outputs are actually longer 9 than coding outputs, and the initial drop in length occurs in both types of output. Furthermore, Figure 8 (3) shows that the coding rate subsequently increases again, suggesting that the distinction between coding and natural language may not significantly impact the optimization process. Moreover, we suspect that the subsequent length scaling up is not from the models exploration, since when the length scales up, the KL divergence of the policy over the base model drops, as shown in Figure 8 (4). This might indicate that it is the KL penalty influencing length. If that is the case, there is little potential for the policy output length to exceed the base models since the exploration is limited by the KL constraint. 6.3. Potential Reasons Why Emergent Behavior is Not Observed with Qwen2.5-Math-7B Our detailed analysis of RL from Qwen2.5-Math-7B, as presented in 6.1 and 6.2, suggests that it fails to fully replicate the training behavior of DeepSeek-R1. We identify the following potential causes: 1) The base model, being relatively small (7B parameters), may lack the capacity to quickly develop such complex abilities when incentivized. 2) The model might have been overexposed to MATH-like short instruction data during (continual) pre-training and annealing, leading to overfitting and hindering the development of long CoT behaviors. Demystifying Long Chain-of-Thought Reasoning in LLMs 6.4. Comparison between RL from the Base Model and"
        },
        {
            "title": "RL from Long CoT SFT",
            "content": "We compare the performance of RL from the base model and RL from long CoT SFT and find that RL from long CoT SFT generally performs better. Setup. We compare using the base model Qwen2.5Math-7B. The results of RL from the base model are from the model trained in 6.1. For RL from long CoT SFT, we adopt setup similar to 3.2. Specifically, we choose the 7.5k MATH training set as the prompt set, curate the SFT data by rejection sampling with 32 candidate responses per prompt using QwQ-32B-Preview, and perform PPO using our cosine length-scaling reward with repetition penalty and our rule-based verifier, sampling 8 responses per prompt and training for 8 epochs. To adapt Qwen2.5-Math-7B with pre-training context length of only 4096 tokens to long CoT SFT and RL, we multiply its RoPE (Su et al., 2024) θ by 10 times. We dont report the results of RL with classic reward from long CoT SFT since it collapses. For evaluation, we adopt our default temperature sampling setup for RL from long CoT SFT as in 2.5 and greedy decoding setup for RL from the base model as in 6.1 for the best performance. Further details about the distillation, SFT hyperparameters and RL hyperparameters can be found in Appendix E.2 & E.3 & E.5.9, respectively. Performance Table 4. on Qwen2.5-Math-7B. The SFT data here is distilled with rejection sampling from QwQ-32B-Preview. different models based of Setup MATH AIME 2024 500 Theo. MMLU Pro-1k QA Base (0-shot) (Direct) RL SFT SFT + RL 52.0 77.4 84.0 85.9 13.3 23.3 24.4 26.9 17.1 43.5 42.2 45.4 2.4 19.7 38.5 40.6 AVG 21.2 41.0 47.3 49. Result. Table 4 shows that, on Qwen2.5-Math-7B, RL initialized from the long CoT SFT model significantly outperforms RL from the base model and further improves upon the long CoT SFT itself. Specifically, RL from long CoT SFT with our cosine reward surpasses RL from the base model by substantial 8.7% on average and improves over the SFT initialization by 2.6%. Notably, simply applying SFT with long CoT distilled from QwQ-32B-Preview already yields strong performance. 6.5. Long CoT Patterns in Pre-training Data Based on the results in 6.1, we hypothesize that incentivized behaviors, such as the model revisiting its solutions, may have already been partially learned during pre-training. To examine this, we employed two methods to investigate whether such data are already present on the web. 10 Firstly, we used generative search engine Perplexity.ai to identify webpages explicitly containing problem-solving steps that approach problems from multiple angles or perform verification after providing an answer. The query we used and the examples we identified are in Appendix F.1). Secondly, we used GPT-4o to generate list of phrases that are characteristic of the aha moment (Appendix F.2.1), then used the MinHash algorithm (Broder, 1997) to search through OpenWebMath (Paster et al., 2023), dataset filtered from the CommonCrawl (Rana, 2010) frequently used in pre-training. We found that there was significant number of matches in discussion forum threads, where the dialogue between multiple users showed similarity to long CoT, with multiple approaches being discussed along with backtracking and error correction (Appendix F.2.2). This raises the intriguing possibility that long CoT originated from human dialogue, although we should also note that discussion forums are common source of data in OpenWebMath. Based on these observations, we hypothesize that RL primarily guides the model to recombine skills it already internalized during pre-training towards new behaviors to improve performance on complex problem-solving tasks. Given the broad scope of this paper, we leave more in-depth investigation of this behavior to future work. 7. Discussions and Future Work In this work, we demystify long CoT reasoning in LLMs. In this section, we outline potential future directions. 7.1. Scaling up Model Size We believe that model size is the primary factor limiting the emergence of the behavior observed in subsection 6.1. Hyung Won Chung (Chung, 2024) recently shared similar perspective, suggesting that smaller models may struggle to develop high-level reasoning skills and instead rely on heuristic-based pattern recognition. Future research could investigate RL using larger base model. 7.2. RL Infrastructure Is Still in Its Infancy While attempting to scale up the model size, we encountered significant challenges in expanding to 32B, ultimately determining that the required number of GPUs was too large to proceed. We observe that open-source RL frameworks (e.g., OpenRLHF (Hu et al., 2024)) often coordinate multiple systems optimized for different training and inference workloads, leading to multiple copies of model parameters being stored in memory. Additionally, algorithms like PPO alternate between these workloads synchronously and sequentially, further limiting efficiency. These factors contribute to low hardware utilization, an issue that is particularly exacerbated in long CoT scenarios due to the higher Demystifying Long Chain-of-Thought Reasoning in LLMs variance in CoT length, which leads to stragglers during inference (Kimi Team, 2025). We look forward to advancements in machine learning and systems research that will help overcome these limitations and accelerate progress in long CoT modeling. 7.3. REINFORCE Is More Tricky to Tune than PPO We also explored REINFORCE++ (Hu, 2025) as faster alternative to PPO for scaling up data. However, we found it to be significantly more unstable than PPO, leading to lower training accuracies (Figure 13). As this instability may be due to an untuned setup (Appendix E.5.10), we refrain from making general claims about the algorithm. We present this as an observation that may be useful to the community. 7.4. Scaling up Verification While our findings demonstrate that combining rule-based verifiers with prompt set filtering is highly effective, designing such rules and curating prompt sets across different domains remains labor-intensive. More fundamentally, this approach embeds human-designed heuristics into the RL environment, reflecting how we think rather than allowing for emergent learning. As highlighted in The Bitter Lesson1, manually encoding human intuition tends to be an inefficient long-term strategy. This raises an intriguing question: how can verification signals be scaled effectively? Is there an equivalent of pretraining in the context of designing RL environments? We look forward to future research on silver supervision signals and the potential for self-supervised approaches in RL verification. 7.5. Latent Capabilities in Base Models Reasoning is latent capability in base models that has only recently been unlocked. Our analysis suggests that one possible source of this emergent thinking is human dialogue on Internet discussion forums. This raises broader question: what other abilities exist, waiting to be elicited from the vast reservoir of human knowledge and experience embedded in pre-training data? We look forward to more detailed analyses tracing model behaviors back to their data origins, which could yield new insights and help uncover hidden capabilities within base models."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to provide insights into scaling inference compute and training strategies to enable long chain-ofthought reasoning in large language models. The broader impacts of this work primarily relate to the potential for enhanced reasoning and problem-solving capabilities across 1http://www.incompleteideas.net/IncIdeas/BitterLesson.html various domains, where models capable of interpretable and verifiable reasoning could drive innovation and improve decision-making. Our findings emphasize the importance of ensuring robust training data preparation, stability, and alignment with verifiable ground truths. We encourage future research to actively develop safeguards that ensure these capabilities are used responsibly. This includes careful design of reward shaping and training protocols to minimize unintended consequences while maximizing societal benefits."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would thank Yuanzhi Li for insightful discussions on this topic. The authors would also thank the SimpleRL team, particularly Weihao Zeng and Junxian He, for sharing their training experiences and experimental observations. Additionally, the authors appreciate Wenhu Chen, Xiaoyi Ren, Chao Li, Ziqiao Ma, Jiayi Pan, Xingyao Wang, and Seungone Kim for their valuable comments and discussions during the early or final stages of the project. Finally, the authors would acknowledge the DeepSeek-R1 and Kimik1.5 teams for their technical report releases, which inspired several additional experiment designs of this paper. This work was supported in part by Carnegie Bosch Institute Fellowship to Xiang Yue."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude, 2023."
        },
        {
            "title": "URL",
            "content": "https://www.anthropic.com/index/ introducing-claude. Broder, A. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pp. 2129, 1997. doi: 10.1109/SEQUEN.1997.666900. Broder, A. Z., Charikar, M., Frieze, A. M., and Mitzenmacher, M. Min-wise independent permutations. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 327336, 1998. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, Demystifying Long Chain-of-Thought Reasoning in LLMs J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. TheoremQA: theorem-driven In The 2023 Conference question answering dataset. on Empirical Methods in Natural Language Processing, 2023. Chung, H. W. Dont teach. incentivize. Presentation slides, 2024. URL https://t.co/2sjhynKxzJ. Slide 48. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., KaShun, S., and Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. Feng, X., Wan, Z., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training, 2023. Gao, Z., Wang, H., Lu, C., Lu, T., Froudist-Walsh, S., Chen, M., Wang, X.-J., Hu, J., and Sun, W. The neural basis of delayed gratification. Science Advances, 7(49):eabg6611, 2021. doi: 10.1126/sciadv.abg6611. Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. Hou, Z., Lv, X., Lu, R., Zhang, J., Li, Y., Yao, Z., Li, J., Tang, J., and Dong, Y. Advancing language model reasoning through reinforcement learning and inference scaling, 2025. Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. Hu, J., Wu, X., Zhu, Z., Xianyu, Wang, W., Zhang, D., and Cao, Y. Openrlhf: An easy-to-use, scalable and highperformance rlhf framework, 2024. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025. Lamb, A. M., ALIAS PARTH GOYAL, A. G., Zhang, Y., Zhang, S., Courville, A. C., and Bengio, Y. Professor forcing: new algorithm for training recurrent networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2024. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., and Roberts, A. The flan collection: Designing data and methods for In Krause, A., Brunskill, effective instruction tuning. E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2263122648. PMLR, 2329 Jul 2023. Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts, 2017. Meta. Introducing meta llama 3: The most capable openly available llm to date., 2024. URL https://ai.meta. com/blog/meta-llama-3. 12 Demystifying Long Chain-of-Thought Reasoning in LLMs OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Learning to reason with llms, OpenAI. URL learning-to-reason-with-llms/. 2024. https://openai.com/index/ Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. Pan, J., Zhang, J., Wang, X., and Yuan, L. Tinyzero, 2025. URL https://github.com/Jiayi-Pan/ TinyZero. Accessed: 2025-01-24. Paster, K., Santos, M. D., Azerbayev, Z., and Ba, J. Openwebmath: An open dataset of high-quality mathematical web text, 2023. Qwen Team. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024a. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024b. URL https://qwenlm.github. io/blog/qwq-32b-preview/. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Rana, A. Common crawl scale crawl using hadoop, 2010. //www.slideshare.net/hadoopusergroup/ common-crawlpresentation. building an open webURL https: Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., 13 et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249. Tong, Y., Zhang, X., Wang, R., Wu, R., and He, J. DARTmath: Difficulty-aware rejection tuning for mathematical problem-solving. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. Wang, Z., Li, Y., Wu, Y., Luo, L., Hou, L., Yu, H., and Shang, J. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2482424837. Curran Associates, Inc., 2022. Yu, L., Jiang, W., Shi, H., YU, J., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. Yuan, Z., Yuan, H., Li, C., Dong, G., Tan, C., and Zhou, C. Scaling relationship on learning mathematical reaarXiv preprint soning with large language models. arXiv:2308.01825, 2023. Demystifying Long Chain-of-Thought Reasoning in LLMs Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024a. Yue, X., Zheng, T., Zhang, G., and Chen, W. Mammoth2: Scaling instructions from the web. NeurIPS 2024, 2024b. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zeng, W., Huang, Y., Liu, W., He, K., Liu, Q., Ma, Z., and He, J. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/ simplerl-reason, 2025. Notion Blog. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 14 Demystifying Long Chain-of-Thought Reasoning in LLMs A. Related Work Complex reasoning and chain of thought prompting. Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including complex reasoning. significant advancement in improving LLM reasoning ability is the implementation of Chain of Thought (CoT) prompting (Wei et al., 2022). This technique involves guiding models to generate intermediate reasoning steps, thereby improving their performance on tasks that require logical deduction and multistep problem solving. Initial studies (Lambert et al., 2024; Wei et al., 2022; Longpre et al., 2023; Yu et al., 2024) focused on short CoT, where models produce concise reasoning paths to arrive at solutions. Although effective for straightforward problems, short CoT can be limiting when addressing more intricate tasks that necessitate deeper deliberation. OpenAIs o1 (OpenAI, 2024) series models were the first to introduce inference-time scaling by increasing the length of the CoT reasoning process. This approach helps LLMs tackle complex problems by breaking them into finer steps and reflecting during problem-solving, leading to more accurate and comprehensive solutions. In this work, we explore long CoT by identifying key factors that enable models to exhibit this behavior, encouraging advanced reasoning capabilities. Reinforcement learning for LLM. Reinforcement Learning (RL) has proven effective in enhancing LLM performance across domains. RL techniques, such as Reinforcement Learning from Human Feedback (RLHF), align model outputs with human preferences, improving coherence (Ouyang et al., 2022). Recent studies (Kimi Team, 2025; DeepSeek-AI, 2025; Lambert et al., 2024) leverage RL to enable LLMs to explore reasoning paths autonomously for complex problems. DeepSeek-R1 (DeepSeek-AI, 2025) achieves strong performance in mathematics, coding, and reasoning tasks without relying on trained reward model (Lightman et al., 2024; Wang et al., 2024b) or tree search (Feng et al., 2023; Snell et al., 2024). Notably, this capability emerges even in base models without supervised fine-tuning, albeit at the cost of output readability. Similarly, Kimi K1.5 (Kimi Team, 2025) enhances general reasoning with RL, focusing on multimodal reasoning and controlling thought process length. These works highlight RLs role in optimizing reasoning when intermediate steps are hard to supervise, and only final outcomes are verifiable. Our research share similar setup but with more detail on disentangling how different model behaviors emerge under varying training conditions and initialization strategies. 15 Demystifying Long Chain-of-Thought Reasoning in LLMs B. Figures and Tables (a) Response lengths under different Cosine Rewards (b) Reward (c) Reward (d) Reward Figure 9. (a) Tuning the hyperparameters of the Cosine Reward results in different length scaling behavior. Note that Reward results in some performance degradation on downstream tasks due to the models reduced ability to stop within the context window. (b) Reward A: rc 0 = 0, rc = 0. = 0, (c) Reward B: rc = 0 (d) Reward C: rc 0 = 10, rw 0 = 10, rw = 10, rw 0 = 10, rc = 5, rw = 9, rw 0 = 6, rc 0 = rw Figure 10. CoT branching frequency, estimated by the keyword count of the pivot word alternatively,, decreased under the Cosine Reward with more training compute. We attributed this, along with increased repetition, to reward hacking. Figure 11. Branching frequency in CoT at different γc values. Lowering the discount factor increased branching frequency, causing the model to abandon problem-solving approaches more quickly. 16 Demystifying Long Chain-of-Thought Reasoning in LLMs Figure 12. Training response length of models trained with Cosine Reward with and without repetition penalty. We see that repetition penalty reduced the length. Figure 13. Reinforce with classic reward shows signs of training instability. Table 5. Performance of model trained with different discount factors for the correctness (cosine) reward and repetition penalty. We see that different reward types have different optimal values. Correctness Discount Repetition Discount MATH -500 AIME Theo. QA MMLU -Pro-1k 3.5 5.0 4.6 3.8 2.1 1.7 0. 20.6 25.7 26.0 24.5 19.5 18.5 15.6 32.4 34.5 36.5 33. 30.7 32.0 25.5 SFT 1.000 0.999 0.99 0.999 0.99 0. 1.000 0.999 0.99 50.4 55.7 58.0 57.8 53.5 55. 47.9 17 Demystifying Long Chain-of-Thought Reasoning in LLMs C. Algorithms and Formulas C.1. Cosine Reward Formula CosFn(t, T, ηmin, ηmax) = ηmin + 1 2 (ηmax ηmin)(1 + cos( tπ )) (1) The formula above is commonly used as the learning rate schedule during gradient descent optimization. It was introduced by (Loshchilov & Hutter, 2017). C.2. N-gram Repetition Penalty Algorithm 1 N-gram Repetition Penalty : sequence of tokens : sequence length 1: Input: 2: 3: 4: : n-gram size : penalty value 5: 6: : maximum sequence length 7: Output: Rm 8: seq s[1 : l] {Extract subsequence of length l} 9: ngrams {Set of observed n-grams} 10: 0 Rm {Initialize reward vector} 11: for 1 to seq + 1 do 12: 13: 14: 15: 16: 17: 18: 19: end for 20: Output: end if ngrams ngrams {ng} for to + 1 do r[t] {Apply penalty} end for ng (seq[j], seq[j + 1], ..., seq[j + 1]) {Current n-gram} if ng ngrams then 18 Demystifying Long Chain-of-Thought Reasoning in LLMs D. Extracts Short-Term Thinking (γc = 0.99) But in this case, we saw that the fixed points dont satisfy the equation p(p(x)) = x. Wait, maybe the solutions to p(p(x)) = are not the fixed points of p, but something else. Alternatively, perhaps can consider that p(p(x)) = implies that is bijection, but since p(x) is quadratic, its not necessarily invertible over all real numbers. Wait, but the problem says that p(p(x)) = has four real solutions, so perhaps there are four values of that satisfy this equation. Alternatively, perhaps need to find the roots of and solve for x. But that seems complicated. Maybe theres better way. p(p(x)) = 0, 19 Demystifying Long Chain-of-Thought Reasoning in LLMs E. Experimental Setup E.1. Evaluation Setup Benchmarks Below are details of our evaluation benchmarks: MATH-500 (Hendrycks et al., 2021): an in-domain mathematical reasoning benchmark. MATH consists of 12,500 problems from American high school math competitions. For efficiency, we adopt MATH-500, widely-used i.i.d. subset of its test split. AIME 2024: an out-of-domain mathematical reasoning benchmark consisting of the 30 problems from American Invitational Mathematics Examination (AIME) 2024. TheoremQA (Chen et al., 2023): an out-of-domain STEM reasoning benchmark consisting of 800 samples. It covers 350+ theorems spanning across Math, EE&CS, Physics and Finance. MMLU-Pro-1k (Wang et al., 2024a): an out-of-domain general reasoning benchmark. MMLU-Pro comprises over 12,000 questions from academic exams and textbooks, spanning 14 diverse domains including Biology, Business, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, and Others. For efficiency, we adopt an 1,000-sample i.i.d. subset of its test split, called MMLU-Pro-1k. We tried to keep the distribution identical to the original one. Figure 14 shows the distribution before/after the downsampling. Figure 14. MMLU-Pro test distribution before/after downsampling for the MMLU-Pro-1k subset. The subset is i.i.d. to the full set. Statistical Metrics We calculate the average accuracy with at least 4 random seeds. To tame the variance caused by the small size of AIME 2024, we sample 16 responses per prompt. Implementation We adopt the vLLM library to accelerate the inference and SymEval2, an elaborate answer grader capable of processing complex mathematical objects like matrices and functions, keeping consistent with the sampling and reward 2https://github.com/tongyx361/symeval 20 Demystifying Long Chain-of-Thought Reasoning in LLMs implementation in our RL setup. Note that few RL experiments are carried out with an earlier version of the grader, causing nuanced performance differences. E.2. Details about Distillation To distill long CoT trajectories from QwQ-32B-Preview, we adopt the temperature = 1.0, the top-p value of 0.95 and the maximum output length of 8192 tokens. Our preliminary experiments show that 8192 tokens show almost the same accuracy with QwQ-32B-Preview on MATH-500 as 16384 tokens, while costing significantly less time. To distill short CoT trajectories from Qwen2.5-Math-72B-Instruct, we adopt the temperature = 0.7, the top-p value of 0.95 and the maximum output length of 4096 tokens, since Qwen2.5-Math-72B-Instruct has context limit of 4096 tokens and our preliminary experiments observe non-negligible ratio of nonsense output when using = 1.0. Note the data is distilled with SGLang (Zheng et al., 2024) with an early version of our code. When applying rejection sampling, we adopt the SymEval verifier as the grader. E.3. Details abour SFT Setup We use OpenRLHF (Hu et al., 2024) for our SFT experiments. By default, we adopt the SFT hyperparameters in Table 6. For efficiency, we utilize Flash Attention 2 (Dao, 2024) and ZeRO (Rajbhandari et al., 2020) based on the DeepSpeed library (Rasley et al., 2020). We uniformly set the micro batch size as 1 since we dont observe acceleration when increasing it. Table 6. SFT Hyperparameters Batch Size Context Length LR Epochs 256 128K 5e-6 2 E.4. Details about RL Setup We use OpenRLHF (Hu et al., 2024) for our RL experiments. When describing hyperparameters, we adopt the same naming conventions as OpenRLHF. E.5. Experiment Hyperparameters Note that the BS column below refers to both rollout batch size (the number of prompts used in sampling-training iteration) and train batch size (the number of samples used in training update) because we adopt the same number for these two hyperparameters in most of our RL setups. Also, the Samples column refers to the number of samples per prompt. E.5.1. DETAILS OF SECTION 3.2 (SFT INITIALIZATION FOR RL) SFT Data: CoT data distilled from QwQ-32B-Preview or Qwen2.5-Math-72B-Instruct with the MATH train split with different number of candidate responses per prompt. 21 Demystifying Long Chain-of-Thought Reasoning in LLMs Table 7. Hyperparameters Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = λ = 1 γ = 1 4 4 512 1 Prompt: 2048 Gen: Actor: 5e-7 Critic: 9e-6 0.01 E.5.2. DETAILS OF SECTION 4.1 (COT LENGTH STABILITY) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Table 8. Hyperparameters Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR Llama3.1-8B Correct: +1 Qwen2.5-Math-7B Correct: +1 λ = 1 γ = 1 λ = 1 γ = 8 8 8 8 512 1 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 4.5e-6 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 4.5eKL 0.01 0.01 22 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.3. DETAILS OF SECTION 4.2 (ACTIVE SCALING OF COT LENGTH) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Table 9. Hyperparameters Episodes Samples BS Epochs Context Length LR 8 8 512 512 512 16 512 1 1 2 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 4.5e-6 KL 0.01 Prompt: 2048 Gen: Actor: 5e-7 Critic: 4.5e-6 0.01 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 Prompt: 2048 Gen: Actor: 5e-7 Critic: 9e-6 0.01 Base Model Rewards Llama3.1-8B Correct: + Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Llama3.1-8B Correct: +1 Llama3.1-8B Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 GAE λ = 1 γ = 1 λ = 1 γ = 1 λ = 1 γ = λ = 1 γ = 1 λ = 1 γc = 1 γp = 0.99 8 8 8 8 16 512 2 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e0.01 23 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.4. DETAILS OF SECTION 4.3 (COSINE REWARD HYPERPARAMETERS) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Table 10. Hyperparameters Llama3.1-8B Llama3.1-8B Llama3.1-8B Cosine: rc 0 = 0 rc = +10 rw 0 = 0 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +6 rc = +5 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +10 rc = +9 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 1 γc = 1 γp = 0.99 λ = 1 γc = 1 γp = 0.99 λ = 1 γc = 1 γp = 0. 4 4 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e0.01 4 4 512 1 Prompt: 2048 Gen: Actor: 5e-7 Critic: 9e-6 0.01 4 4 512 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 24 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.5. DETAILS OF SECTION 4.4 (CONTEXT WINDOW SIZE) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Table 11. Hyperparameters Llama3.1-8B Llama3.1-8B Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 1 γc = 1 γp = 0. λ = 1 γc = 1 γp = 0.99 λ = 1 γc = 1 γp = 0.99 8 8 512 Prompt: 2048 Gen: 2048 Actor: 5e-7 Critic: 9e-6 0.01 8 8 1 Prompt: 2048 Gen: 6144 Actor: 5e-7 Critic: 9e-6 0.01 8 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 E.5.6. DETAILS OF SECTION 4.5 (LENGTH REWARD HACKING) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Table 12. Hyperparameters Llama3.1-8B Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 1 γ = 1 λ = 1 γc = 1 γp = 0.99 8 16 512 2 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e0.01 16 512 2 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e0.01 25 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.7. DETAILS OF SECTION 4.6 (OPTIMAL DISCOUNT FACTORS) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Table 13. Hyperparameters Llama3.1-8B Llama3.1-8B Llama3.1-8B Llama3.1-8B Llama3.1-8B Llama3.1-8B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 1 γc = 1 γp = 1 λ = 1 γc = 1 γp = 0.999 λ = 1 γc = 1 γp = 0. λ = 1 γc = 0.999 γp = 0.999 λ = 1 γc = 0.999 γp = 0.99 λ = 1 γc = 0.99 γp = 0.99 4 4 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 4 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 4 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0. 4 4 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e0.01 4 4 512 1 Prompt: 2048 Gen: Actor: 5e-7 Critic: 9e-6 0.01 4 4 512 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 0.01 26 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.8. DETAILS OF SECTION 5.2 (RL WITH NOISY VERIFIABLE DATA) SFT Data: 115k filtered from 462k instances of long CoT data distilled from QwQ-32B-Preview with WebInstruct. Base Model RL Prompt Set Verifier Rewards GAE Episodes Instances Samples BS Epochs Context Length LR KL Table 14. Hyperparameters Llama3.1-8B Unfiltered (30k sampled) Symeval Llama3.1-8B Unfiltered (30k sampled) LLM-as-a-judge Llama3.1-8B Filtered (30k sampled) Symeval Llama3.1-8B Filtered (30k sampled) LLM-as-a-judge Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 1 γc = 1 γp = 0. 1 30k instances 4 512 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 KL: 0. λ = 1 γc = 1 γp = 0.99 1 30k instances 4 512 1 Prompt: 2048 Gen: Actor: 5e-7 Critic: 9e-6 KL: 0.01 λ = 1 γc = 1 γp = 0.99 1 30k instances 4 512 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 KL: 0.01 λ = 1 γc = 1 γp = 0.99 1 30k instances 4 1 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 9e-6 KL: 0.01 27 Demystifying Long Chain-of-Thought Reasoning in LLMs E.5.9. DETAILS OF SECTION 6 (EXPLORATION ON RL FROM THE BASE MODEL) Base Model Rewards GAE Episodes Samples BS Epochs Context Length LR KL Table 15. Hyperparameters Qwen2.5-Math-7B Correct: +1 Wrong: 0.5 No Answer: Qwen2.5-Math-7B Correct: +1 Qwen2.5-Math-7B Cosine: rc 0 = +2 rc = +1 rw 0 = 10 rw = 0 re = 10 Rep. Penalty: = 0.05 = 40 λ = 0.95 γ = 1 λ = 1 γ = λ = 1 γ = 1 20 8 8 8 8 1024 (Train: 128) 512 512 1 1 Prompt: 1024 Gen: 3072 Actor: 5e-7 Critic: 9e-6 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 4.5e-6 0. 0.01 Prompt: 2048 Gen: 14336 Actor: 5e-7 Critic: 4.5e-6 0.01 E.5.10. DETAILS OF SECTION 7.3 (REINFORCE IS MORE TRICKY TO TUNE THAN PPO) SFT Data: Long CoT data distilled from QwQ-32B-Preview with the MATH train split. Table 16. Hyperparameters Base Model Rewards Gamma Episodes Samples BS Epochs Context Length LR KL Clip Llama3.1-8B Correct: + γ = 1 8 (stopped early) 8 512 1 Prompt: 2048 Gen: 5e-7 0.01 0.1 E.6. Implementation of the Model-Based Verifier We used Qwen2.5-7B-Instruct as our model-based verifier. It was provided with both the reference answer and the suffix of the long CoT. We truncated the long CoT to avoid confusing the verifier. We used the following prompt. Prompt Template for Model-Based Verifier Given the following last 20 lines of the LLM response to math question and the reference solution to that question, evaluate if the LLM response is correct based only on the LLMs final answer. LLM response (last 20 lines): ... {out} Reference solution: {ref} Explain your thought process step-by-step before responding with Judgement: < correct/wrong/not_found> 28 Demystifying Long Chain-of-Thought Reasoning in LLMs E.7. Implementation of Short-Form Answer Extraction We use the Llama-3.1-8B-Instruct model to extract short-form answer from QA pairs in WebInstruct, with the following prompt template: Prompt Template for Short-Form Answer Extraction Problem: {Problem} Solution: {Solution} Based on the Problem and the Solution, extract short final answer that is easy to check. Provide the short final answer in the format of \"The final answer is $$ boxed{...} $$\" - If the answer is mathematical object, write it in LaTeX, e.g., \"The final answer is $$ boxed{frac{1}{2}} $$\" - If the answer is boolean, write it as \"True\" or \"False\", e.g., \"The final answer is $$ boxed{True} $$\" - If the Problem cant be answered in short form, respond with \"\" like \"The final answer is $$ boxed{} $$\" For generation parameters, we use temperature = 0 (greedy decoding) and set the maximum output length as 512 tokens. After generation, we simply extract the short-form answer from within the boxed{...}. E.8. Action Prompting Framework We studied the publicly released CoTs of o1-preview and identified that its thoughts could be categorized into few types of actions (listed below). To construct long CoTs, we designed prompts for each of these actions and implemented multi-step prompting framework to sequence them. The framework ceded control flow of the CoT to the LLM, with the LLM making branching or looping decisions while the framework acted more passively as state machine reacting to the LLM outputs. The framework took care of the boilerplate around constructing the CoT with an append-only log and managed all of the orchestration. clarify: Making some observations about the problem in order to identify an approach to solve it. decompose: Breaking the current problem down into smaller and easier sub-problems to solve. solution step: Computing single step in the solution. In the context of math, this could be doing some arithmetic or symbolic manipulation. reflection: Evaluating the current approach and partial solution to see if any mistakes were made, any sub-goals were achieved, or if alternative approaches should be considered instead. Note that we used strong teacher model o1-mini for the reflection action as that one was more difficult prompt to respond to correctly as it requires self-correction. answer: Responding with final answer and terminating the CoT. E.8.1. CONTROL FLOW Simplified description of the interaction between the framework and LLM. 29 Demystifying Long Chain-of-Thought Reasoning in LLMs Algorithm 2 Action Prompting State Machine if state = clarify then else if state = decompose then else if state = solution step then output prompt action clarify() (state, thought) parse(output) chain of thought.append(thought) output prompt action decompose() (state, thought) parse(output) chain of thought.append(thought) 1: Input: prompt 2: Output: chain of thought sequence 3: chain of thought [prompt] {Initialize singleton chain of thought sequence from prompt} 4: state clarify 5: while True do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end while output prompt action answer() (state, thought) parse(output) chain of thought.append(thought) return chain of thought {Terminate after answer action} output prompt action solution step() (state, thought) parse(output) chain of thought.append(thought) output prompt action reflection() (state, thought) parse(output) chain of thought.append(thought) else if state = reflection then else if state = answer then end if E.8.2. ACTION PROMPTING TEMPLATES Action: Clarify You are very talented mathematics professor. In few sentences, VERY CONCISELY rephrase the problem to clarify its meaning and explicitly state what needs to be solved. Highlight any assumptions, constraints and potential misinterpretations. Do NOT attempt to solve the problem yet -- you are just clarifying the problem in your mind. <problem> {goal} </problem> Answer in the following format: <clarification> Problem clarification as instructed above </clarification> <goal> Summarize the problem into single statement describing the goal, e.g. Find the value of the variable w. </goal> 30 Demystifying Long Chain-of-Thought Reasoning in LLMs Action: Decompose You are talented mathematics professor. You already have partial solution to problem. In single sentence, propose candidates for the next subgoal as the next step of the partial solution that will help you make progress towards the current goal. Do not repeat any subgoal, we dont want any infinite loops! Do not suggest using computer or software tools. <current goal> {current_goal} </current goal> <parent goal> {parent_goal} </parent goal> <partial solution> {solution} </partial solution> Format your answer as follows: <thinking> step-by-step thinking of what the next possible subgoal should be, as well as some other alternatives that might also work remember, we want to solve the parent goal WITHOUT repeating the subgoals that are already DONE. do not suggest verification or checking. {parent_goal} </thinking> <sentence> single sentence describing the subgoal phrase it as if you were thinking to yourself and are considering this as hypothesis (dont express too much certainty) </sentence> <sentence> single sentence describing an *ALTERNATIVE* subgoal, without repeating previous ones start off with \"Alternatively,\" </sentence> <sentence> single sentence describing an *ALTERNATIVE* subgoal, without repeating previous ones start off with \"Alternatively,\" </sentence> 31 Demystifying Long Chain-of-Thought Reasoning in LLMs Action: Solution Step You are an extremely PEDANTIC mathematics professor who loves to nitpick. You already have partial solution to problem. Your task is to solve *only* the current goal. You should include symbols and numbers in every sentence if possible. <current goal> {current_goal} </current goal> <partial solution> {solution} </partial solution> BE VERY CONCISE. Include calculations and equations in your response if possible, and make sure to solve them instead of just describing them. DO NOT SOLVE THE WHOLE QUESTION, JUST THE CURRENT GOAL: {current_goal} Do not repeat any calculations that were already in this prior step: {prior_step} 32 Demystifying Long Chain-of-Thought Reasoning in LLMs Action: Reflection You are talented mathematics professor. You already have partial solution to math problem. Verify whether the current subgoal has been achieved. <current goal> {current_goal} </current goal> {parent_goal} <partial solution> {solution} </partial solution> Format your answer as follows: <verification> Come up with quick, simple and easy calculation to double check that the solution is correct. This calculation should not re-compute the solution in the same way, as that would defeat the purpose of double-checking. Use one of the following strategies: - An easier, alternative method to arrive at the answer - Substituting specific values into equations and checking for consistency - Working backwards from the answer to derive the given inputs and then checking for consistency Be consise. Do not suggest using computer. At the end of your verification, restate the answer from the current solution. Do not calculate it if it hasnt been solved. Phrase it as if you are reflecting as you solve the problem. </verification> <current_goal_achieved> true or false, depending on whether the solution is correct and the current goal has been achieved: {current_goal} </current_goal_achieved> <parent_goal_achieved> true or false, depending on whether the parent goal has been achieved: {parent_goal.target} </parent_goal_achieved> <new_goal> If the solution is not correct or the current goal has not been achieved, suggest an alternative current goal here in single sentence. Start off with \"Alternatively,\" Your goal should be sufficiently different from subgoals that have been solved or that have timed out: {parent_goal_tree} </new_goal> Action: Answer Extract the final answer, making sure to obey the formatting instructions. Solution: {solution} Formatting instructions: {format} 33 Demystifying Long Chain-of-Thought Reasoning in LLMs F. Long CoT Patterns in Pre-training Data F.1. Snapshot of webpages Source: brilliant.org The following two examples demonstrate how explicit verification after answering question can naturally exist on webpage."
        },
        {
            "title": "Explicit verification",
            "content": "x + 7 = 10 This problem can be solved by subtracting 7 from each side. + 7 7 = 10 7 = 3 Once the problem is solved, the solution can be verified by rewriting the problem with 3 substituted for x. 3 + 7 = 10 10 = 10 Both sides are equal, verifying that = 3 is valid solution."
        },
        {
            "title": "Explicit verification that found an error",
            "content": "x + 7 = 10 student rushing through her homework might mistakenly write = 2 as the solution to this problem. If she takes moment to rework the equation with her answer, she will realize the answer is incorrect. + 7 = 10 2 + 7 = 10 9 = 10 Since 9 = 10, the student knows she needs to go back and find different solution to the problem. 34 Demystifying Long Chain-of-Thought Reasoning in LLMs Source: kidswholovemath.substack.com"
        },
        {
            "title": "Attempt the question from different perspective",
            "content": "The Double Check Game Regardless of the scenario, we can play the double check game! The game is simple: we try to solve the problem in as many different ways as possible. Elementary School Example Math problem is: 78 57 =? To play the game, we try to solve the problem in as many different ways as possible. The first solution: ? = 78 57 Break apart the 57: ? = 78 50 7 ? = 28 7 ? = 21 second solution: ? = 78 57 Subtract an easier number from 78: ? = 78 60 + 3 ? = 18 + 3 ? = 21 third solution: ? = 78 57 Subtract 57 from an easier number: ? = 80 57 2 ? = 23 2 ? = 21 ... 35 Demystifying Long Chain-of-Thought Reasoning in LLMs F.2. OpenWebMath F.2.1. QUERIES We used GPT-4o to generate examples of typical pivot keywords found in long CoT. These were used to find documents in OpenWebMath that have interesting properties characteristic of long CoT trajectories. Aha Phrases \"Lets think step by step.\" \"Lets go through this one step at time.\" \"Breaking it down step by step...\" \"Thinking about it logically, first...\" \"Step 1: Lets figure out the starting point.\" \"If we follow the steps carefully, we get...\" \"To solve this, lets analyze it piece by piece.\" \"Going through this systematically, we have...\" \"Okay, lets solve this gradually.\" \"Does that make sense?\" \"Is this correct?\" \"Wait, does that check out?\" \"Am missing something?\" \"Hmm does that work?\" \"Let me verify that.\" \"That makes sense, right?\" \"Hold on, is this right?\" \"Lets double-check this.\" \"Wait, actually...\" \"Oh, hold on...\" \"Wait second...\" \"Actually, let me rethink that.\" \"Hmm, let me go back for moment.\" \"I might need to check this again.\" \"Lets pause and reassess.\" \"Lets check by doing the reverse.\" \"Lets verify by working backward.\" \"Can we check this by reversing the process?\" \"To confirm, lets undo the steps.\" \"A good way to verify is by reversing it.\" \"If we undo the operations, do we get the same result?\" ... 36 Demystifying Long Chain-of-Thought Reasoning in LLMs F.2.2. MATCHES Source: MC Stan Discussion Forum The discussion below took place on message board for the probabilistic programming framework MC Stan. The user Tiny has question about how to interpret some data and multiple other users are responding. We can see the usual pivot keywords (highlighted in bold) characteristic of long CoT, including branching, self-correction and even an assessment of the feasibility of an approach."
        },
        {
            "title": "Discussion on message board",
            "content": "So the question is then to find the right prediction task, looking at your setup, those may include: ... For hypothetical future serial drawn from the same observed serials. (i.e. include the varying intercept via new level and sample_new_levels = For the or intercept) In the experiments you actually observed (i.e. include the fitted varying intercepts for your experiments) e i a a o a n u underlying system (i.e. ignore the varying as the ) But you could also ask other stuff, like: What is the expected difference in some of the constants (or anything else) between two future experiments? All of those (and more) should be answerable using the posterior of the model. But you still need to figure out which questions do you actually want to ask, as there is lot of i Does that make sense? Best of luck with your model! ... am not sure follow your thought here, but maybe thats just because would have worded it differently? ... An alternative approach would be to try to find different parametrization of the model where the parameters are interpretable separately, but that might be hard. Also, if this is the parametrization of the process used by many in the field, than maybe poeple would expect you to report as (frac {L} {mol})ˆ{n-1} sˆ{-1}, because t what everybody has been doing (although possibly with fixed n)? Does that make sense? Can you not just recast the model (with modified parameters) as ... 37 Demystifying Long Chain-of-Thought Reasoning in LLMs Source: physicsforums.com The discussion below took place on physics forum. The user Songoku is asking for help with homework and another user BvU is trying to assist without revealing the solution directly. We see the usual pivot keywords indicating self-reflection, expression of uncertainty and formulation of hypotheses."
        },
        {
            "title": "Discussion on a physics forum",
            "content": "# Cylinder in 3 1. Dec 13, 2017 ### songoku 1. The problem statement, all variables and given/known data Let be positive constant. Consider the cylinder x2 + y2 <= r2, and let be the part of the cylinder that satisfies 0 <= <= y. (1) Consider the cross section of by the plane = (-r <= <= r), and express its area in terms of r, t. (2) Calculate the volume of C, and express it in terms of r. ... 5. Dec 13, 2017 ### BvU Simple case: = 0. So -1 <= <= 1. In the yz plane 0 <= <= is triangle. What about ? 6. Dec 13, 2017 ### songoku think am missing something here because feel cant really grasp the hint given. Let me start from the basic again: 1. Let the - axis horizontal, - axis vertical and - axis in / out of page. imagine there is circle on xy plane with radius then it extends out of page (I take out of page as z+) to form 3 cylinder. Is this correct? 2. Plane = is like the shape of piece of paper hold vertically with the face of paper facing - axis (I mean - axis is the normal of the plane). Is this correct? Thanks 7. Dec 14, 2017 ### BvU Yes 8. Dec 14, 2017 ### songoku \"Consider the cross section of by plane = t\" means plane = cuts the cylinder ? And the intersection will be rectangle? ... 38 Demystifying Long Chain-of-Thought Reasoning in LLMs Source: StackExchange The user Baymax is asking for help on probability problem and we see dialogue with another user Lulu. We see that the quick back-and-forth between them is similar to the kind of nimble branching behavior in long CoT where multiple solutions are quickly assessed and considered. We also see an expression of realization which can be easily re-cast as self-verification in long CoT."
        },
        {
            "title": "Discussion on Stack Exchange",
            "content": "# probability that we stop flipping after exactly ten flips in biased coin flipping? ... thought that let us fix of getting third head at last that is at 10th flip, so that we would stop there, and the remaining - getting two heads can be accommodated in the 9 trials. so there are $$9$$ choose 2 ways of getting two heads so the probability that we stop flipping after exactly ten flips is $$ˆ9C_ {2}$$ . $$frac{1}{4}ˆ3$$.$$frac{3}{4}ˆ7$$. Is this correct? EDIT - Now the probability of getting exactly 3 heads? got it to be $$ˆ{10} C_{3} frac{1}{4}ˆ3 frac{3}{4}ˆ7$$. Should we get the same as the previous one? any reason why they should/should not be same? lulu Oct 1 18 at 16:13 think you switched $P(H),P(T)$ but the approach is good. oh see now! thanks! @lulu please see the edit Your probability for exactly 3 heads is right as well. It should be obvious why the results have to be different. In the first case the outcome of the last flip BAYMAX Oct 1 18 at 16:30 BAYMAX Oct 1 18 at 16:13 is fix and in the second case the outcome of the last flip is not fix. callculus Oct 1 18 at 16:31 ... 39 Demystifying Long Chain-of-Thought Reasoning in LLMs Source: StackExchange User88 interacts with multiple other users. Observe that they are helping to clarify each others doubts, which is reminiscent of self-correction in long CoT trajectories."
        },
        {
            "title": "Discussion on Stack Exchange",
            "content": "Choosing units for drug testing Heres third puzzle that found in book, slightly paraphrased because dont entirely remember the format of the original. ... How can he arrange the dosage amounts so that he ends up using all 25 test packages, and the total units of dosage used in the tests are as low as possible? The book had the answer, but one, it didnt explain how the answer was arrived at, and two, dont remember what the answer was and no longer have that book with me. Am missing something, or is the goal just to find 25 coprime numbers from 25 to 50? They dont have to be coprime. There just cant be any two where one is factor of the other. And the range is from 1 to 50, not 25 to 50. Aza May 20 14 at 4:33 Joe Z. May 20 14 at 4:34 Wouldnt single test of 1 unit technically satisfy the requirement? Or am missing something? to perform exactly 25 tests. Yea. Wouldnt 1 win? You have to use all 25 tests. awesomepi May 20 14 at 19: arshajii May 20 14 at 14:28 Joe Z. May 20 14 at 19:31 Ah, guess you have By logically starting from 26-50 and trying to shrink them one by one you can easily show: $8 ,12,14,17,18,19,20,21,22,23,25,26,27,29,30,31,33,35,37,39,41,43,45,47,49$ Which equals $711$ ..."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "IN.AI",
        "Tsinghua University"
    ]
}