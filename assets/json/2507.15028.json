{
    "paper_title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding",
    "authors": [
        "Yuanhan Zhang",
        "Yunice Chew",
        "Yuhao Dong",
        "Aria Leo",
        "Bo Hu",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance."
        },
        {
            "title": "Start",
            "content": "Towards Video Thinking Test: Holistic Benchmark for Advanced Video Reasoning and Understanding Yuanhan Zhang*1 Yunice Chew*2 Yuhao Dong1 Aria Leo2 Bo Hu2 Ziwei Liu1(cid:66) 1S-Lab, Nanyang Technological University {yuanhan002, ziwei.liu}@ntu.edu.sg 2Independent Researcher yunicechew1119@gmail.com 5 2 0 2 0 2 ] . [ 1 8 2 0 5 1 . 7 0 5 2 : r Figure 1. Overview of the Video Thinking Test. Video-TT introduces two challenges: (1) ensuring correctness in understanding complex visual stories; (2) maintaining robustness against natural adversarial conditions."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows significant gap between video LLMs and human performance. *Equal Contribution. Project page: https : / / zhangyuanhan - ai . github . io / video-tt/ Human intelligence fundamentally depends on two key aspects: correctness and robustness, with correctness being necessary condition for robustness [13, 47]. Correctness ensures that systems outputs align with the expected standards or truths. In video understanding, this translates to making accurate judgments about the visual content. Building on this foundation, robustness is essential for ensuring that these interpretations remain reliable and consistent under various conditions, including ambiguity and conflicting information. These attributes are vital for providing dependable insights and managing complex situations. The development of video large language models (video LLMs) [4, 18, 20, 34, 46, 50, 51] have brought their capabilities closer to human intelligence. Developing benchmarks that accurately highlight current shortcomings is crucial for further improving video LLMs performance. However, current benchmarks fail to accurately reflect the differences between video LLMs and human intelligence. Regarding correctness, existing benchmarks [2, 6, 7, 11, 15, 37, 52] do not clearly distinguish between errors caused by insufficient frame sampling and errors caused by Figure 2. Dataset Comparisons. Left: We present Video thinking Test (Video-TT) for the following features: ensure the questions provided rationale for each answer. Middle: In Video-TT, the are complex. top-performing model reaches only half of human performance. Right: The lower performance of GPT-4o in the VideoMME-Long track may be due to the selection of sparse frames rather than genuine gap in understanding between humans and models. addresses the issue of selecting frames from the video. Dataset NExT-QA [38] Social-IQ [45] MVBench [15] EgoSchema [23] TempCompass [17] Video-MMMU [11] WorldQA [52] VideoMME [7] TemperalBench [2] Video-TT #Clips/#QA 1,000/8,564 1,250/7,500 3,641/4,000 5,063/5,063 504/13,157 300/900 303/1,007 900/2,700 410/7,540 1,000/5, failures in actual video understanding. As result, the large performance gap between models and humans might reflect the limitations of frame sampling rather than true understanding gap (Fig.2 right). In cases where models can sample enough framesespecially for shorter videos advanced models can perform at levels comparable to humans (Fig.2 middle). This can lead to the mistaken impression that current models have reached human-level video understanding. Therefore, it is crucial to develop benchmarks that challenge video LLMs in areas where they underperform, clearly separating issues with frame sampling from genuine limitations in understanding. Regarding robustness, recent studies [26] investigate how video LLMs respond to adversarial changes, such as visual pixel alterations or distorted words in instructions. However, these scenarios are often artificial and do not reflect the complexities of real-world conditions, making the true impact of natural adversarial conditions [10] less clear. To address these problems, we introduce the Video Thinking Test (Video-TT), new benchmark highlight current shortcomings in video LLM. This test focuses on: (i) Correctness toward complex visual narratives: We measure this by evaluating the accuracy of model responses to complex questions, highlighting differences in video understanding between models and humans. We define visual complexity and narrative complexity as guidelines for creating complex questions. Each question is created after examining manageable set of video frames. Therefore, the questions are complex yet answerable within reasonable number of frames. (ii) Robustness toward natuCurrent video LLMs typically follow two-step process: first, they sample limited number of frames, and then they understand the content within these frames. For example, changing the query from Which players head did the man tap? to Which players heed did the man tap? ral adversarial questions: We assess model performance against natural adversarial questions crafted to view query from different angles. For instance, if the query is Which players head did the man tap? and the correct answer is Number 8, the model should also handle misleading version like Did the man tap the head of the player wearing number 9? These questions simulate real-world adversarial conditions. In Video-TT, we selected 1,000 YouTube Shorts videos and created one primary open-ended question and four related adversarial questions for each, based on eight visual or narrative complexity factors. We evaluated both top opensource video LLMs and proprietary models. Our comparison of these models with human revealed significant insights for enhancing video understanding. Our key findings are summarized as follows: We introduce the Video Thinking Test, crucial benchmark for assessing the correctness and robustness of large video language models in understanding videos. This benchmark is crafted to ensure that any mistakes in the models responses are due to its lack of understanding rather than errors in selecting key frames. Our results reveal significant gap in performance between humans and the top-performing video model. Humans achieve an accuracy of 84.3% and robustness of 64.3%, while the model only reaches 36.6% accuracy and 36.0% robustness, indicating major areas for improvement. This study is the first to demonstrate that current open-source models significantly lag behind GPT-4o in terms of natural adversarial robustness. While they show comparable performance in the correctness aspect of the Video Thinking Test, in the robustness track, the top open-source modelQwen2.5-VL72Bscores 13.8 points lower than GPT-4o. Our error analysis of all errors made by GPT-4o shows that for recognizing content, GPT-4o struggles with unclear or unusual content, often guessing the most likely scenario rather than accurately representing the video. It also faces challenges in distinguishing different scenes, which impacts its ability to track actions and identify participants in multiple scenes. For cognitive ability, it lacks the integration of world knowledge needed to think about likely intentions, goals, and social dynamics in videos, and it struggles to use correctly recognized cues to deduce hidden information. 2. Related Works Our work lies within in the fild of evaluating the video understanding of video large langauge model through visual question-answering (QA) [1]. VQA [3, 8, 11, 12, 14, 36, 3844, 52] is key task in video-language research in diverse visual domains. Correctness in Video Understanding Recently, several benchmarks [7, 15] have been proposed to evaluate video large language models (Video LLM) correctness in opendomain video understanding. MVBench [15] integrates 11 public video benchmarks using static-to-dynamic method. However, this design has issues because these academic datasets are already well known and widely used in the research community. This means that many models may already be trained or fine-tuned on these videos. To address this, VideoMME [7] collects new videos by sourcing them from YouTube. This benchmark largely advances the development of Video LLM. In VideoMME, As illustrated in Fig. 2 (right), the maximum number of frames able to be sampled by GPT-4o is 384. As video duration increases, it becomes more challenging to sample key frames in the VideoMME-Long track, which is major hurdle in improving performance. This issue also occurs in other long video understanding datasets [6, 37, 53]. While handling long videos is crucial aspect of video research, our work focuses on the understanding capability of Video LLMs. We meticulously ensure each question is answerable with manageable video frames. On the other hand, in the datasets, such as VideoMME-short track, where most video frames can be sampled, the models performance has reached limit. Thus, which scenarios in short videos still challenge current Video LLM is an open question. This motivates the creation of Video-TT. Meanwhile, several benchmarks also try to find scenarios that current Video LLM cannot handle. For example, FunQA [39] tests video reasoning limits with counter-intuitive and humorous content. TemporalBench [2, 17] examines the models grasp of fine-grained temporal dynamics. Unlike these benchmarks, Video-TT covers diverse scenarios without being limited to specific video domain or type of question. We aim to build complex and comprehensive video Q&A benchmark. Robustness in Video Understanding Recent studies [26] evaluate the robustness of multimodal models by testing their performance under artificial distortions of instructions or video pixels. In this work, we focus on the significance of assessing natural adversarial robustness. This is crucial to determine whether models genuinely comprehend video content. 3. Dataset In the Video Thinking Test, we aim to present challenge that underlines the differences in accuracy and robustness In of video understanding between models and humans. Sec.3.1, we explore methods to pose complex questions that test the models ability to accurately interpret video content. In Sec.3.2, we investigate how to ask natural adversarial questions to ensure that these interpretations remain reliable. 3.1. How to Ask Complex Question? One question that guides this benchmark is: What factors make question complex? We propose that the complexity of question does not solely depend on its type (e.g. object color vs. plot understanding), but also on the context, reasons, or scenarios under which the question is asked. For example, the question What is the color of the second car in the video? might appear simple, but it becomes difficult if the car is moving fast, partially obscured, or viewed from an unusual angle. To explore how complex questions are formed, we start by identifying components within video that could be questioned. We analyze the video content hierarchy based on [54], which categorizes it from bottom to top as: element, event, plot . Each level can be the focus of question. We then consider which factors make these contents hard for viewers to grasp, leading to complex questions. First, from the perspective of video content, following [9, 24, 29, 31], we introduce visual complexity. This idea from cognitive science shows how complex visual content is. It is defined by the number of elements, the range of shapes, the variety of colors, the amount of texture, and the way items are arranged. We identify the following factors that affect visual complexity: (1) Unclear & Unusual Content: Does the content differ from what we normally see? Does the video have noise, blur, occlusion, or other issues that hide its content? (2) Movement Speed: Is any part of the video or the camera moving too fast, making it hard to identify or track objects? (3) Spatial-temporal Arrangement: How are objects arranged and interacting within the scene? Is there an abundance of spatial or temporal information that increases the cognitive load required to identify specific elements? (4) Illusions: Are there any techniques Figure 3. Benchmark Curation Pipeline. Our annotation pipeline ensures that each question: (1) is complex enough to differentiate between human and model video understanding capabilities; (2) can be understood with limited number of sampled frames; (3) also assesses the models robustness against natural adversarial conditions. Figure 4. Eight Complex Factors in Our Datasets. Video links of each case: Q-1, Q-2, Q-3, Q-4, Q-5, Q-6, Q-7, Q-8 that create illusions and make it hard to recognize the content? Second, from the perspective of the video producer, referring to [28, 32], we discuss narrative complexity, which includes special design choices that go beyond linear storytelling and require more active engagement from viewers. We define four elements of narrative complexity: (1) Complex Plot: Does the plot include twists or an unexpected conclusion? (2) Narrative Editing: Are there convoluted combined shots, such as montage methods, to present story? (3) Technical Editing: Are there special filming techniques or post-production manipulations that are seamlessly integrated and hard to detect? (4) World Knowledge: Does the video require world knowledge or assumptions for full understanding?"
        },
        {
            "title": "These complexities at various levels require viewers to",
            "content": "engage more deeply with the video content. Figure 5. VQA Question Prototypes. We present our five question prototypes. Man highlights the man framed by bounding box. 3.2. How to Ask Natural Adversarial Question? To reach human-level understanding of videos, it is not enough to just answer questions correctly; we must also explore how changing the wording of question affects model performance. These natural adversarial questions broaden our study and help users gauge the reliability of the model. Consider the primary open-ended question: Which players head did the man in the gray coat next to red pole tap?, based on which we derived four natural adversarial questions, as shown in Fig. 5. Specifically, these questions include: (1) Rephrased Open-ended Question, which rewords the primary question with minor semantic alterations. (2) Correctly-led Open-ended Question, which provides accurate cues about key points, helping guide the model to- (3) Wrongly-led Openward the correct understanding. ended Question, which gives misleading cues about key points, directing the model towards an incorrect understanding, and (4) Multiple-choice Question, where the combination of correct/wrong-led options are designed to test the models comprehension of the video. 3.3. Data Curation Process Primary Question Annotation Based on the understanding of visual complexity and narrative complexity, we asked the annotators to select videos and annotate them with question-answer pairs. The selected video and the all ques- (1) Ensuring tions should meet the following standard: Complexity for Human: Each question must involve at least one complex factor as previously discussed. In Fig. 5, identifying man in gray coat next to red pole in video (an example of visual complexityunclear element) lead to question like: Which player did the man in the gray coat next to the red pole tap on the head? (2) Ensuring Complexity for Model: Questions tested against GPT-4o [25], LLaVA-Video-7B [51], and Qwen2.5-VL-7B [34]. If any of these models fail to provide correct answer in at least one out of three attempts, the question is considered sufficiently complex and kept for further use. Answer and Rationale Annotation Besides providing an answer, annotators must explain their reasoning process in answering the primary open-ended question. This includes detailed explanation of how they arrived at the correct answer and discussion of the flaws in an incorrect answer provided by prior models. Please see the example in Fig. 7. Sampling Check Annotators are instructed to formulate questions answerable by viewing only 80 uniformly sampled frames. This criterion ensures the frame sampling does not hinder video understanding, addressing common issue in recent video understanding benchmarks [7, 37]. Additionally, it establishes that our dataset emphasizes visual rather than auditory information. Adversarial Question Expansion The same individual who developed the initial primary open-ended questions also crafted four adversarial variants. Specifically, the annotator constructed misleading open-ended and multiplechoice questions by referring to incorrect responses from GPT-4o, LLaVA-Video-7B, and Qwen2.5-VL-7B. Annotators should adjust the answer and rationale to the primary open-ended question minimally to as the answer and rationale to any related adversarial questions. Alignment Check As illustrated in Fig. 3, during the stages of Ensuring Complexity, Primary Question Annotation, Answer Annotation, Sampling Check and Adversarial Question Annotation, we involve two additional annotators to maintain consistency among three annotators. Any question displaying inconsistent annotations is excluded. Specifically, during the Answer and Rational Annotation stage, questions addressing the cause of an event with several potential explanations are omitted unless there is unanimous agreement among the annotators. Videos and their associated question-answer pairs that do not meet these standards are eliminated by the verifier. Figure 6. Benchmark Statistics. Left: We show our 18 question by category alongside the number of questions. Right: The histogram of the video length. 3.4. Dataset Statistics Overall, we collected 5,000 questions and answers for 1,000 videos. Initially, questions were classified into one of three levels based on video content, i.e., element, event, and plot. Additionally, questions were organized into categories reflecting the nature of the inquiry. For instance, Attributes typically involve what/who questions, while Localization questions often concern after/before/when. Furthermore, within specific question category, such as Element Attributes, new sub-category may emerge if particular complex factor becomes prominent. For example, if over 50 questions are driven by the same factor, this significance leads to the creation of sub-category like Element Attributes-Illusion. In total, Fig. 6 shows that there are 18 types of questions in Video-TT. The top three content categories in our dataset include comedy, sports, and daily life. To ensure quality and safety, we filtered out videos containing violent or explicit material, as well as suspected AIgenerated videos. All videos are shorter than 65 seconds. We also report the human hours for the whole curation process in Appendix 1. 4. Experiments 4.1. Experimental Setup Benchmark Models. We thoroughly evaluate eight video LLM from various model families, covering different sizes and training methods. For proprietary models, we include Gemini1.5 [33] and GPT-4o [25]. For open-source models, we assess Ola [20], Oryx [18], InternVL2.5 [4], Qwen2.5VL [34], and LLaVA-Video [51], using the lmms-eval codebase [48]. Unless specified, we default to 7B and 70(+)B model in each family. However, since the InternVL-2.578B model could not be implemented on eight H100 GPUs, we utilized its 38B version instead. All evaluations are conducted under zero-shot settings and using each models default prompts. The number of input frames is 80. Blind Baseline and Human Level Performance. BeTable 1. Correctness score (accuracy) are reported for each question type and their average across types for each model. The Robustness (RB) score is derived from further statistical analysis of these accuracies. Model Primary Rephrased Correctly-Led Wrongly-Led Multi-Choice Blind - Language Only Gemini Pro GPT-4o Video-Language Models Open-source models Qwen2.5-VL-7B LLaVA-Video-7B Ola-7B InternVL-2.5-8B Oryx-1.5-7B InternVL-2.5-38B Qwen2.5-VL-72B LLaVA-Video-72B 9.1 8.5 20.9 21.4 21.2 20.6 23.0 24.6 26.6 24.4 Proprietary models Gemini Pro GPT-4o Human Baseline 28.8 36. 84.3 8.3 9.3 22.5 22.5 22.7 22.7 23.6 27.5 25.7 25.7 29.7 35.4 83.9 22.4 58. 45.3 49.2 57.5 65.7 67.9 53.5 31.1 57.7 50.2 67.5 83.9 5.4 14.7 39.3 37.2 29.1 24.5 26.0 22.6 49.8 32.6 29.2 39. 76.2 5.3 15.3 39.9 41.8 45.5 44.7 44.8 47.1 45.6 47.5 42.3 46.6 87.5 Avg RB 9.1 21.3 2.9 12.9 33.6 34.4 35.2 35.6 37.1 35.1 35.8 37.6 38.2 45.2 83. 14.4 13.7 17.0 10.9 14.8 11.1 22.2 19.7 20.5 36.0 64.4 yond video LMM model, we also introduce two baselines. First, we introduce the blind baseline based on the GPT4o and Gemini-Pro. Specifically, such baseline indicates we prompt models with video question only without using video frames as input. Second, we also ask human evaluators independently answer each question. Metric. For assessing correctness score (accuracy), we use the Qwen2.5-72B model to score open-ended responses. Answers are scored on scale from 0 to 5, detailed in the Appendix 3. An answer scoring above three is considered correct. For multiple-choice questions, we compare the selected option from the models response to the correct answer. match confirms the response as correct. Correctness is essential for robustness. In videos where the model accurately answers the primary open question, we aim to assess how the model handles naturally adversarial scenario questions. We define the robustness score as the ratio of videos where all five questions are answered correctly to those where only the primary open-ended question is correctly answered. This measure helps identify and address any inconsistencies in responses to different versions of the same question. 4.2. Main Results: Accuracy Across Question Types We present the evaluation results at Table. 1. Among the Video-Language Models, open-source systems vary greatly in performance. For instance, InternVL-2.5-8B scores high on Correctly-Led questions (65.7%), outperforming both LLaVA-Video-7B and Qwen2.5-VL-7B. However, its accuracy drops to 24.5% on Wrongly-Led questions, suggesting it struggles with misleading prompts. The LLaVA-Video72B stands out as the best performing open-source model. Examining proprietary Models, Gemini Pro and GPT-4o achieve higher overall accuracies than many open-source alThe mathematical definition of the robustness score is provided in the Appendix-Sec 2. ternatives but still fall short of human performance. GPT4o shows strong results for Correctly-Led (67.5%) and Wrongly-Led (39.8%) questions, indicating better resistance to misleading prompts compared to most open-source models. However, even the best proprietary models reach only about half the accuracy of human annotators on openended tasks, emphasizing the ongoing challenges in complex video reasoning. In addtion, although the best open-source model, LLaVA-Video-72B, performs similarly to GPT-4o in the multiple-choice setting (47.5 vs. 46.6), it lags significantly behind in primary open-ended questions. Primary openended questions better reflect realistic user interactions, where questions are often naturally phrased and less constrained than pre-defined options. This gap highlights an important improvement area for open-source models. Moreover, this observation also reveals limitation of current video-language benchmarks, which tend to focus heavily on multiple-choice questions. Such benchmarks may overestimate model performance, failing to capture the true challenges presented by open-ended reasoning in real-world scenarios. 4.3. Natural Adversarial Robustness Table 1 shows clear ranking in robustness performance among various models. Human annotators achieve the highest score at 64.4%, followed by GPT-4o at 36.0%. This significant difference between human performance and the top-performing model highlights the ongoing challenge of reaching human-level robustness in complex tasks."
        },
        {
            "title": "For",
            "content": "the group of open-source models including InternVL-2.5, LLaVA-Video, and Qwen2.5-VL, the performance of InternVL-2.5-8B is notably lower at 10.9%, while its 38B-Instruct version shows almost no improvement at 11.1%. However, LLaVA-Video, and Qwen2.5-VL underscore the potential improvements from larger model configurations. 5. How far is Video LMM from Humans? In this section, we aim to understand the differences between humans and models in processing video. We examine the errors made by GPT-4o in primary open-ended question. As we directed annotators to create questions reflecting visual and narrative complexity, our findings suggest strong correlation between these complexities and the observed errors. Notably, among the 18 types of questions, five categoriesPlot Attributes (technique editing), objective causality (narrative editing), elements attributes (illusion), element duration & speed, and professional knowledge are directly linked to specific complex factors: technique editing, narrative editing, illusions, movement speeds, and world knowledge, respectively. In this section, we examined the errors across the other 13 question types, illustratFigure 7. Error cases in typical question types. We mark rationale answers with grey background. Video links of each case : Q-1, Q-2, Q-3, Q-4. appears, disappears, and then reappears in different frames. This error highlights the models difficulty in keeping track of elements consistently across multiple frames. Moreover, the model has trouble following sequences of actions and pinpointing who is doing what, particularly when it involves terms that specify order, like second, third,etc., as seen in Q2. World Knowledge Deficiency in Character Reaction and Motivation: For the Character Reaction and Motivation category, 44% of errors arise from lack of world knowledge. This indicates that the model frequently misinterprets why characters respond as they do. Many of these errors happen because the model lacks the commonsense or cultural knowledge needed to understand character actions. To better its performance, the model requires stronger foundation in social norms, emotions, and contextual expectations. For example, as shown in Fig.7-Q3, person might look calm or relaxed. Yet, recognizing this expression as disappointment depends on understanding the context provided by world knowledge. Complex Plot Confusion in Plot Attributes and Objective Causality: In the Plot Attributes and Objective Causality category, significant 55% of errors stem from misunderstanding of complex plots. This shows that the model struggles to keep coherent cause-and-effect relationship across multiple events. When the storyline requires linking different elements to create logical sequence, the model often fails. Enhancing the models capability to track extended causal relationships is crucial for improving its performance in this reasoning type. An example is shown in Figure 8. Human-conducted analysis of errors by question type. ing how these complexities lead to the models errors, as depicted in Fig. 8. We highlight three main errors in this section, with additional analysis provided in the Appendix 4. Spatial-Temporal Confusion in Physical and Counting Tasks: In tasks that involve understanding temporal and spatial relationship, such as Element/Event Localization and Event Counting, the most frequent errors (79% and 88%, respectively) arise from confusion in how events and objects are arranged over time and space. This confusion indicates that the model struggles to maintain clear and consistent understanding of where and when events occur, which leads to mistakes in recognizing the sequence and location of these events and objects. For example, as shown in Fig. 7-Q1, the model can correctly count the number of photo frames in single frame, but it fails when frame Figure 9. (a-b) Comparison of human and average model performance based on correctness and robustness across question types. (c) Comparison of human performance and that of two models across different numbers of frames. (d) Relative performance change (%) when adding Chain-of-Thought (CoT) reasoning and audio transcript information. Fig.7-Q4. 6. Further Analysis Human-Model Behavior Correlation We compare the performance of humans and models in terms of accuracy and robustness across different types of questions. Figure 9 (a) reveals moderate positive correlation (r = 0.49), suggesting that question types where humans excel are generally easier for models as well. Despite this, models consistently fall short of human performance, even on simpler tasks. Figure 9 (b) displays negative correlation (r = -0.50) between the model and human scores. The models performance decreases significantly in question types involving Element Counting or Displacement. These types are often linked to visual complexity, suggesting the models reduced effectiveness in complex visual situations. In contrast, humans show robust performance in these scenarios. This negative correlation merits further investigation. Impact of Frame Numbers Figure 9 (c) shows how performance changes with more input frames. Human performance improves steadily with an increase in frames, reaching nearly perfect accuracy at 64 frames. On the other hand, model performance saturates after about 8 frames. This pattern differs from other datasets [6, 7, 49], where more frames significantly boost performance. This observation supports the core design of our benchmark: Annotators are asked to create questions that can be answered with just 80 uniformly sampled frames. Impact of Chain-of-Thought Prompting Chain-ofThought Prompting (CoT) techniques improve the reasoning skills of large models in various tasks [5, 19, 27, 30, 35]. Given these successes, we investigate CoT could also enhance performance in Video-TT. We assess the CoT, which adds Lets think step by step to the prompts. We present results at Figure 9 (d). Performance on Wrongly-led shows relative increase of about 6.8%, which indicates that structured thinking helps the model spot and bypass misleading hints more effectively. For Multiple-choice Questions, however, CoT shows no noticeable advantage, with similar performance between models. This result suggests that CoT aids tasks requiring unstructured thinking (like open-ended questions), whereas tasks with structured formats such as multiple-choice benefit less. Impact of Audio Transcript Audio transcripts shows impact on model performance in recent multi-choice video LLM benchmarks [21, 49], leading us to examine their influence. In Multiple-choice Questions, transcripts do not improve performance over the GPT-4o baseline. This contrasts with other benchmarks where audio has enhanced performance. Nevertheless, our results align with our datasets focus on visual content. Furthermore, transcripts significantly boost Robustness Performancean almost 15% relative gain. This improvement underscores the value of including spoken input to increase robustness. 7. Conclusion We introduced the Video Thinking Test (Video-TT), new benchmark designed to assess the correctness and robustness of video large language models (video LLMs) in understanding complex real-world videos. Video-TT separates errors due to not enough frame sampling from those due to genuine comprehension issues, offering more reliable way to test these models. In terms of correctness, opensource models perform well on the multi-choice track but fall short in the open-ended track compared to GPT-4o. The open-ended models also show less resilience against naturally tricky questions compared to GPT-4o. However, both GPT-4o and open-source models are still far behind human performance. Error analysis shows that video LLMs have difficulties with understanding space and time together, integrating world knowledge, and linking different elements in video to create logical response. These results highlight the urgent need to improve reasoning, resilience, and real-world comprehension in video LLMs, providing clear direction for future research in video intelligence."
        },
        {
            "title": "Acknowledgement",
            "content": "This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP202210012, MOE-T2EP20223-0002), and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s)."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 3 [2] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, Yao Dou, Jaden Park, Jianfeng Gao, Yong Jae Lee, and Jianwei Yang. Temporalbench: Towards fine-grained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. 1, 2, 3 [3] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190200, 2011. 3 [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 1, 5 [5] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. 8 [6] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 1, 3, 8 [7] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2, 3, 5, 8 [8] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1128711297, 2021. 3 [9] Christopher Heaps and Stephen Handel. Similarity and features of natural textures. Journal of Experimental Psychology: Human Perception and Performance, 25(2):299, 1999. 3 [10] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1526215271, 2021. [11] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, 2025. 1, 2, 3 [12] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766, 2017. 3 [13] Michael Lee, Amy Criss, Berna Devezer, Christopher Donkin, Alexander Etz, Fabio Leite, Dora Matzke, Jeffrey Rouder, Jennifer Trueblood, Corey White, et al. Robust modeling in cognitive science. Computational Brain & Behavior, 2:141153, 2019. 1 [14] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. 3 [15] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark, 2023. 1, 2, 3 [16] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. In Proceedings of [17] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2, 3 [18] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 5 [19] Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. 8 [20] Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025. 1, 5 [21] LMMs-Lab. Video detail caption, 2024. 8 [22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. [23] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36, 2024. 2 [24] Aude Olivia, Michael Mack, Mochan Shrestha, and Angela Peeper. Identifying the perceptual dimensions of visual complexity of scenes. In Proceedings of the annual meeting of the cognitive science society, 2004. 3 [25] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 5 [26] Madeline Schiappa, Shruti Vyas, Hamid Palangi, Yogesh Rawat, and Vibhav Vineet. Robustness analysis of videolanguage models against visual and language perturbations. Advances in Neural Information Processing Systems, 35: 3440534420, 2022. 2, [27] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 8 [28] Jan Simons. Complex narratives. In Hollywood puzzle films, pages 1734. Routledge, 2014. 4 [29] Joan Snodgrass and Mary Vanderwart. standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity. Journal of experimental psychology: Human learning and memory, 6(2):174, 1980. 3 [30] Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. Visual agents as fast and slow thinkers. arXiv preprint arXiv:2408.08862, 2024. 8 [31] Zekun Sun and Chaz Firestone. Curious objects: How visual complexity guides attention and engagement. Cognitive Science, 45(4):e12933, 2021. [32] John Sweller. Cognitive load during problem solving: Effects on learning. Cognitive science, 12(2):257285, 1988. 4 [33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 5 [34] Qwen Team. Qwen2.5-vl, 2025. 1, 5 [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. 8 [36] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning In Thirty-fifth Conference on Neural in real-world videos. Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 3 [37] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. 1, 3, 5 [38] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 2, [39] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. arXiv preprint arXiv:2306.14899, 2023. 3 [40] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. [41] Li Xu, He Huang, and Jun Liu. Sutd-trafficqa: question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98789888, 2021. [42] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. [43] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. [44] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134, 2019. [45] Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. Social-iq: question answering benchmark for artificial social intelligence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88078817, 2019. 2 [46] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 1 [47] Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Nanning Zheng, and Kaipeng Zhang. Bavibench: Towards evaluating the robustness of large visionlanguage model on black-box adversarial visual-instructions, 2024. 1 [48] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 5 [49] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 8 [50] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 1 [51] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 1, [52] Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, and Ziwei Liu. Worldqa: Multimodal world knowledge in videos through long-chain reasoning, 2024. 1, 2, 3 [53] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 3 [54] Xingquan Zhu, Jianping Fan, Ahmed Elmagarmid, and Xindong Wu. Hierarchical video content description and summarization using unified semantic and visual similarity. Multimedia Systems, 9:3153, 2003. 3 Towards Video Thinking Test: Holistic Benchmark for Advanced Video Reasoning and Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Annotation Detail We present the number of human hours at each stage in the Data Curation Process as follows. In total, the annotation process cost 8227.32 human hours. Table 2. Time Estimation for Dataset Curation Process. Notes: *2 indicates that two people are required for this stage; *4 refers to four natural adversarial questions per video; *5 covers all questions for human baseline annotation. stands for Question; stands for Answer; stands for Rationale. Dataset Curation Stage #Data Hour/Data Total (hour) 226 Trial Data Annotation 226 Trial Data Alignment 2,977 Complex Video Collection 2,977 Complex Video Alignment 2,338 Primary Q&A&R Annotation 2,338 Primary Q&A&R Alignment Sampling Check 1,344 Adversarial Question Annotation 1,300*4 Adversarial Question Alignment 1,300*4 1,300*5 Human Baseline Annotation 0.5 0.25 0.16 0.05*2 0.5 0.3*2 0.25*2 0.16 0.08*2 0.16 Total 113 56.5 496.17 297.7 1,169 1,402.8 672 832 832 1,040 8227.32 2. Mathematical Definition of the Robustness"
        },
        {
            "title": "Score",
            "content": "Aprimary correct be the set of videos where the primary openended question is answered correctly. Aparaphrased correct be the set of videos where the paraphrased open-ended question is answered correctly. Acorrectly led correct be the set of videos where the correctlyled open-ended question is answered correctly. Awrongly led correct be the set of videos where the wronglyled open-ended question is answered correctly. Amultiple choice correct be the set of videos where the multiple-choice question is answered correctly. The set of videos where all five questions are answered correctly, denoted as Afull correct, is the intersection of all these sets: = Afull correct Aprimary correct Where denotes the cardinality (size) of the set A, representing the number of videos in that set. 3. Prompt for Evaluating Open-ended Answer Table. 3 shows the prompt for evaluating open-ended answers. score of 3 or higher is considered correct, while scores below 3 are deemed incorrect. We refer to the prompt introduced in VideoChatGPT [22]. System Message You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs. Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Heres how you can accomplish the task: INSTRUCTIONS: - Focus on the meaningful match between the predicted answer and the correct answer. - Consider synonyms or paraphrases as valid matches. - Evaluate the correctness of the prediction compared to the answer. Please evaluate the following video-based question-answer pair: Question: question Correct Answer: answer Predicted Answer: pred Provide your evaluation only as yes/no and score where the score is an integer value between 0 and 5, with 5 indicating the highest meaningful match. Please generate the response in the form of Python dictionary string with keys pred and score, where value of pred is string of yes or no and value of score is in INTEGER, not STRING. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. For example, your response should look like this: pred: yes, score: 4. Table 3. System message for evaluating the open-ended answer. Afull correct = Aprimary correct Aparaphrased correct Acorrectly led correct Awrongly led correct Amultiple choice correct Thus, the Robustness Score (RB) becomes: 4. Error Analysis In this section, we give more analysis about the errors made by GPT-4o. Figure 10. The data annoation flow of Video Turing Test. stands for Question; stands for Answer; stands for Rationale. Figure 11. Error cases in typical question types. We mark rationale answers with grey background. Video links of each case : Q-1 Q-2, Q-3, Q-4, Q-5, Q-6 & Q-7. 4.1. Recognition: Detecting objects and their attributes In this subsection, we analyze errors in six question types focused on the element and event categories. These errors typically stem from visual complexity, challenging the recognition capabilities of the model. Element Attributes and Event Attributes. In this category, 80% of errors involve unclear or unusual subjects in the questions, which relate to elements or events. These errors are linked to issues of unclear and the presence of unusual content in visual complexity. For instance, as depicted in Fig. 11-Q1, when confronted with unusual content, the model often defaults to the most common outcome rather than what is actually depicted in the video. For clarity issues, as shown in Fig. 11-Q3, the model struggles to accurately identify the color of small Rubiks Cube in the video frames. Event Counting. In this category, one specific errors arise from the models difficulty in accurately identifying the start and end points of repeated events, despite correctly classifying the event type (Fig. 11-Q4). Element Localization and Event Localization. Errors in this category, which make up 79%, are related to spatiotemporal challenges. In spatial terms, common error occurs when multiple individuals are present in scene, and the model incorrectly assigns actions to the wrong person. This issue is particularly prevalent in interactions involving two people, leading to confusion over who is performing and who is receiving the action (Fig. 11-Q2). Positional Relationship. Understanding the relative positions of elements is fundamental human skill. Yet, we observed that models struggle with this task. For instance, when asked whether element is on the left or right side of B, the model typically responds left if visually appears on the left side of the video frame. This response disregards their actual spatial relationship within the context of the video. Such findings indicate significant limitation in the models ability to accurately interpret positional relationships. Displacement. For frame-based model, these questions challenge the models ability to track the development of the event across consecutive frames. For instance, considering the displacement of an object from the previous frame to the current one poses significant challenge if the models vision encoder struggles with fine-grained spatial localization grounding [16]. 4.2. Cognition: Reasoning the likely intents, goals, and social dynamics of people In this subsection, we analyze errors in question types associated with the plot. These errors are typically due to narrative complexity. When prompted, the model demonstrates recognition-level perception abilities; however, the narrative complexity challenges the model in addressing cognition level questions. Character Reaction and Character Motivation. As discussed in Sec.3.1, world knowledge significantly contributes to narrative complexity. Fully understanding characters reactions and motivations requires applying this knowledge. Commonly, this involves grasping psychological activities, which are subjective by nature. To answer relevant questions effectively, the model must do more than just describe the video; it needs to link these descriptions to broader world knowledge. Plot Attributes and Objective Causality. The typical errors in plot attributes and objective causality stem from lack of world knowledge and in-context reasoning ability. An interesting aspect of necessary world knowledge is its multi-modal nature, essential for correct responses in this category. For example, as shown in Fig. 11-Q5, while the model can accurately describe mans actions in the video, understanding what these actions implysuch as imitating battle scene in trenchrequires linking the video content with relevant world scenes. Moreover, the models limited in-context reasoning is evident as it struggles to integrate diverse perceptual inputs into cohesive understanding of social dynamics, despite accurately answering recognition-level questions about actions observed in the video."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "S-Lab, Nanyang Technological University"
    ]
}