{
    "paper_title": "Agents of Change: Self-Evolving LLM Agents for Strategic Planning",
    "authors": [
        "Nikolas Belle",
        "Dakota Barnes",
        "Alfonso Amayuelas",
        "Ivan Bercovich",
        "Xin Eric Wang",
        "William Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in LLMs have enabled their use as autonomous agents across a range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark a progression of LLM-based agents, from a simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agent's code. We introduce a multi-agent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agent's logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to game-playing agents, and demonstrating adaptive reasoning over multiple iterations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 1 5 6 4 0 . 6 0 5 2 : r Agents of Change: Self-Evolving LLM Agents for Strategic Planning Nikolas Belle, Dakota Barnes, Alfonso Amayuelas, Ivan Bercovich, Xin Eric Wang, William Wang University of California, Santa Barbara {nbelle, dakotabarnes, amayuelas}@ucsb.edu, ibercovich@gmail.com, ericxwang@ucsb.edu, william@cs.ucsb.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in LLMs have enabled their use as autonomous agents across range of tasks, yet they continue to struggle with formulating and adhering to coherent long-term strategies. In this paper, we investigate whether LLM agents can self-improve when placed in environments that explicitly challenge their strategic planning abilities. Using the board game Settlers of Catan, accessed through the open-source Catanatron framework, we benchmark progression of LLM-based agents, from simple game-playing agent to systems capable of autonomously rewriting their own prompts and their player agents code. We introduce multiagent architecture in which specialized roles (Analyzer, Researcher, Coder, and Player) collaborate to iteratively analyze gameplay, research new strategies, and modify the agents logic or prompt. By comparing manually crafted agents to those evolved entirely by LLMs, we evaluate how effectively these systems can diagnose failure and adapt over time. Our results show that self-evolving agents, particularly when powered by models like Claude 3.7 and GPT-4o, outperform static baselines by autonomously adopting their strategies, passing along sample behavior to gameplaying agents, and demonstrating adaptive reasoning over multiple iterations."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved impressive results across wide range of language and reasoning tasks. However, when it comes to long-term planning and strategic decision-making, current LLMs still face serious limitations [1]. LLMs are inherently trained for local coherence in text generation rather than long-horizon utility maximization [19]. This makes it challenging for an LLM agent to exhibit strategic, long-term planning [28, 4] in multi-step domains like board games. Settlers of Catan (Catan) is prime example of multi-agent strategy game requiring players to plan resource management, expansion, and negotiation over many turns. Traditional game AI methods (e.g. search or reinforcement learning) have achieved superhuman performance in perfect-information games such as Chess [15] and Go [24], yet Catan poses unique challenges with its stochastic dice rolls and partial observability. This gap motivates exploring how LLM-based agents can autonomously improve their long-horizon strategic planning in such environments. We investigate how LLM agents can self-modify and evolve to enhance strategic planning over long horizons. Using the open-source Catanatron framework [5], we design four agent architectures with progressively greater self-improvement abilities: (1) BaseAgent that maps an unstructured game state description directly to an action, (2) StructuredAgent that receives representation of the game state, available actions, and basic strategy in natural language for better parsing and guidance, (3) *Equal contribution Preprint. Under review. Figure 1: Overview of Catan gameplay and LLM-agent interaction. Left: Settlers of Catan Players take turns to gather, trade, and spend resources to build on modular board in stochastic, partially observable strategy game. The objective is to reach 10 victory points by constructing settlements, roads, and cities [3, 2]. Right: Our LLM-based framework interacts with the Catanatron API, leveraging game state information and strategic reasoning to decide actions. Through repeated play and self-modification, agents evolve more coherent long-term strategies [25, 16, 17, 13, 33]. PromptEvolver where an Evolver agent and Player agent interact for up to 10 iterations to refine, test, and evaluate prompt for an LLM to play Catan, and (4) AgentEvolver comprising of Evolver, Analyzer, Researcher, Coder, and Player roles, which can autonomously rewrite its game-playing code between games. Approaches (3) and (4) draw inspiration from frameworks like AutoGPT that coordinate multiple specialist AI roles [34] and from LLM self-reflection techniques [23], but we apply these ideas to competitive, partially observable game environment. We evaluate these agents against Catanatrons strongest heuristic-based bot (an alphabeta search AI) in head-to-head games. Each game-playing agent (using different models such as GPT-4o, Claude 3.7, or Mistral as backends) plays series of games against the AlphaBeta bot, and we track relevant metrics such as average victory points, number of settlements/cities, largest army, and other development indicators. Our experiments reveal whether our multi-agent architecture can create agents that achieve higher scores or more effective long-term plans (e.g. building cities earlier, securing Largest Army) compared to simpler LLM agents and to the fixed strategy of the AlphaBeta bot. Additionally, they showcase how LLMs from different providers strategize in developing successful game-playing agents, how they use tools to diagnose and fix the core issues leading to poor performance, and implement strategies into the game-playing agents they design. Main Contributions. We highlight the following key contributions from our work: LLM Self-Evolving Agent Framework: We propose novel framework that enables autonomous prompt and code evolution in game-playing agents. This framework allows agents to iteratively self-improve in long-horizon complex board game with no human intervention, refining their strategies and rewriting their own decision-making code. This extends the role of LLMs from passive solvers to active, self-improving designers. Benchmarking strategic reasoning and planning in complex game Settlers of Catan: Using Catanatron [5], we conduct extensive experiments with LLM agents in head-to-head Catan games. We benchmark their performance against strong search-based bot. Agents with iterative self-improvement demonstrate more coherent strategies and higher average points than those without, narrowing the gap with the expert baseline. Empirical evidence of performance gains from evolution: Through extensive experiments, we show that agents capable of prompt and code evolution achieve consistently higher performance than static baselines. The PromptEvolver, in particular, outperforms fixed agents across key metrics, and its gains are amplified when paired with stronger base models, seen in Claude 3.7s 95% improvement from the BaseAgent  (Table 1)  . Insights from Evolved Strategies: Through qualitative analysis of game logs and agentgenerated code revisions, we examine how the LLMs evolved strategies differ between SOTA LLMs and from human-crafted agents. Our findings reveal how LLM agents learn to prioritize long-horizon objectives (e.g. balanced growth vs. opportunistic plays), adapt to past failures, and implement novel tactics through self-directed refinement."
        },
        {
            "title": "2 Related Work",
            "content": "Game-Playing AI and Strategy Games Games have long served as benchmarks for AI research [11, 6, 18]. While significant progress has been made in perfect-information games like Chess and Go [22, 24], strategic board games such as Settlers of Catan, Diplomacy [8] or Civilization [21] introduce elements of randomness, partial observability, and multi-agent interaction, posing unique challenges to an AI system [27]. Previous works approached Catan using specialized neural network architecture to handle its mixed data types, enabling an RL agent to outperform traditional rule-based bots [12]. In contrast, our approach leverages LLMs natural language understanding to navigate Catans complexities, focusing on autonomous strategy refinement without relying on extensive training data. LLM-Based Autonomous Agents The concept of employing LLMs as autonomous agents has gained traction, particularly through frameworks that leverage environment feedback to drive reasoning and action [35, 32]. Sophisticated frameworks like CAMEL [14] and AutoGen [31] facilitating multi-agent collaboration through role-based interactions. These systems assign specific roles to LLM instances, such as task proposer, solver, or critic, enabling complex problem-solving through dialogue and coordination. Our methodology extends this paradigm by introducing self-evolving multi-agent system where LLMs assume roles like Analyzer, Researcher, Coder, and Player. This architecture allows the agent to autonomously analyze performance, conduct research, modify its codebase, and iteratively enhance its strategy for gameplay. Self-Improvement and Policy Evolution Recent studies have explored mechanisms for LLMs to self-improve without gradient-based training [26, 7]. Reflexion [23] introduced framework where LLM agents use verbal feedback to refine their decision-making processes, maintaining an episodic memory of successes and failures. Similarly, PromptBreeder [10] or PromptAgent [30] treats design as an evolutionary process, allowing LLMs to generate and evaluate variations to optimize performance. AlphaEvolve [20] presents an evolutionary coding agent to tackle open scientific problems and algorithm improvement. Voyager [29] demonstrated an LLM-powered agent in Minecraft that iteratively writes and refines code to acquire new skills autonomously. Our work builds upon these concepts by integrating both and code evolution within competitive, multi-agent environment, enabling the LLM agent to adapt its strategies based on performance feedback over successive game iterations. While prior research has laid the foundations for LLM-based agents and self-improvement mechanisms, our study uniquely combines these elements within the context of complex strategy game. By embedding an LLM game-playing agent within multi-agent system capable of autonomous and code evolution, we investigate the extent to which LLMs can overcome inherent limitations in long-term strategic planning. This approach not only evaluates the agents performance against strong heuristic-based bot but also provides insights into the emergent behaviors and strategies developed through self-directed refinement."
        },
        {
            "title": "3 Background",
            "content": "Settlers of Catan as Strategic Benchmark Settlers of Catan is 34 player board game where players collect and trade resources to build settlements and roads, racing to earn 10 victory points on modular island map. The game emphasizes resource management, planning, and negotiation, with mechanics like the robber (which blocks resources) adding tactical depth. Catan is known for its balance of luck and skill. Victory goes to the first player to reach 10 points, earned by building and upgrading settlements into cities, buying development cards, and achieving goals like the longest road or largest army. Each settlement is worth 1 point, each city 2, and some development cards grant hidden points or knight bonuses. Every turn starts with dice roll that produces resources for 3 Figure 2: Diagrams of the LLM-based Agent Architectures. Baseline agents use LLMs to map Catan game states to actions by direct prompting (BaseAgent) or structured formatting (StructuredAgent). PromptEvolver adds multi-agent loop where prompts are iteratively refined via analysis and summarization. AgentEvolver enables autonomous code evolution, with the Analyzer, Researcher, Coder, and Strategizer agents collaboratively redesigning player logic from gameplay feedback. players with adjacent settlements. The active player may then trade and build. If 7 is rolled, the robber is activated, blocking tile and stealing resource. Players must plan expansions, balance upgrades, and trade strategically to manage luck. This need for adaptation and foresight makes Catan strong benchmark for evaluating strategic reasoning in agents. The Catanatron Framework We use the open-source Python-based simulator Catanatron as our evaluation environment. Designed for automated gameplay of Settlers of Catan, Catanatron offers programmatic interface for integrating custom agents and supports rapid simulation at scale. It faithfully implements the games rules and dynamics, capturing key strategic elements such as resource management, trade negotiation via structured proposals, and randomness introduced by dice rolls. Each game consists of multiple players competing to reach ten victory points, with players interacting through well-defined game states that include current resources, board positions, available actions, and observable opponent statuses. Games typically span 40 to 100 turns, allowing for extended observation of agents long-term planning capabilities. We benchmark our LLM-driven agents against AlphaBeta, the best-performing heuristic agent provided through the API which uses depth-2 alpha-beta pruning algorithm with heuristic evaluation to select actions."
        },
        {
            "title": "4 Method",
            "content": "To investigate the capabilities of LLMs in autonomously developing strategic planning abilities, we make them interact with the complex strategic board game: Settlers of Catan. We systematically evaluate different agent architectures, each leveraging LLMs in distinct roles: player, refiner, and creator. All agents share the common objective of winning the game. By doing so, we aim to analyze how these agents adapt and evolve their strategic reasoning through iterative self-improvement, particularly in comparison to traditional heuristic-based approaches. 4.1 Agent Architectures We develop series of LLM-based agents using the LangChain and LangGraph frameworks. LangChain facilitates structured interactions among multiple LLMs, managing iterative conversations, chaining, and message passing between agents. LangGraph complements this by offering visual and computational tools for modeling agent interactions as directed graphs, enabling transparent orchestration and robust management of complex agent workflows. 4 BaseAgent Our baseline LLM agent, named the BaseAgent, receives raw structured input describing the complete current game state from the Catanatron API, including available actions at each turn. The LLM directly outputs selected action without additional engineering or textual refinements. This baseline measures the inherent strategic reasoning capability of an LLM when minimally guided and provides foundational performance metrics against the AlphaBeta agent. StructuredAgent To establish more informed performance baseline, we introduce the StructuredAgent. This agent utilizes human-developed prompts, based on the public GitHub repository [9]. These structured prompts explicitly guide the LLM in interpreting the state representation, highlighting key considerations such as resource prioritization, trade heuristics, and long-term strategic planning. By incorporating human expertise into the design, we establish practical performance baseline for single-agent, static-LLM. PromptEvolver Next, the PromptEvolver is multi-agent system designed to iteratively enhance the players based on experiential feedback and strategic analysis. This architecture consists of the following specialized agents: Evolver Agent: Provided with access to game results, evolution history, and tools to search the web, view local files, and edit the Player Agents prompt. Player Agent: Utilizes the evolved prompt to choose an action on turn-by-turn basis. We implement the PromptEvolver to evaluate whether LLM-based multi-agent systems can autonomously surpass human-level prompt engineering to elicit strategic planning in LLMs. The iterative refinement loop enables the system to learn strategically effective prompts based on previous game outcomes, analyses, and external knowledge. This helps us assess the capacity of LLMs to learn prompts that provide plan-following guidance to other LLM agents, despite their inherent struggle with following long-term plans themselves. AgentEvolver Finally, the AgentEvolver represents multi-agent framework that is autonomously capable of modifying or rewriting the player agents underlying decision-making logic, starting with blank template. This AgentEvolver architecture consists of the following specialized agents: Evolver Agent: The central coordinator. After each game, it digests Analyzer reports, then orchestrates new iteration by querying the appropriate supporting agents. Analyzer Agent: Invoked after every game, and can be called by Evolver. It evaluates the players gameplay, identifies weaknesses, and summarizes areas for improvement. Researcher Agent: Handles domain-specific queries about the Catanatron codebase or broader Catan strategy, utilizing both local file access and web search via the Tavily API. Strategizer Agent: Suggests high-level gameplay strategies or critiques past strategic choices. It uses both current and historical data as reference, as well as the web. Coder Agent: Translates proposed changes into concrete code modifications. It receives the latest player code and outputs new version, along with summary of changes made. Player Agent: The iteratively refined agent that plays the game. We deploy the AgentEvolver architecture to investigate the extent to which LLM-based systems can autonomously design and implement complex strategic behaviors from first principles. Each of these agents operates within persistent message-passing framework, maintaining memory across iterations to accumulate experience in their respective domains. This design allows, for example, the Analyzer to become more proficient at diagnosing recurring failures, or the Coder to learn stylistic patterns that align with successful strategies. By empowering the Evolver agent to independently analyze gameplay, conduct research, build strategy, and generate executable code, we aim to assess the capability of LLMs not just to refine but fundamentally create strategically competent agents. This evaluation provides insights into the potential of LLMs for advanced autonomous development and optimization of complex reasoning processes."
        },
        {
            "title": "5 Experimental Setup",
            "content": "To rigorously evaluate the performance of our LLM-based agents, we conduct controlled experiments using the open-source Catanatron environment. The data was collected on MacBook Pro 2019 16GB, and MacBook M1 Max 2021 32GB over total time of 60 hours. Our evaluation focuses on measuring each agents strategic competence in competitive 2-player Catan games by comparing them head to head against the strongest built-in heuristic bot: AlphaBeta. Language Models All LLM-based agents were instantiated using one of the following models: GPT-4o (via OpenAI API), Claude 3.7 (via AWS Anthropic Bedrock API), and Mistral-large-latest (via Mistral AI API). These models were chosen for their advanced reasoning capabilities, availability, and diversity in architecture. There was no limit to input and output size. Test Procedure We evaluate each of our four agents (BaseAgent, StructuredAgent, PromptEvolver, AgentEvolver) against the AlphaBeta bot in series of head-to-head games. Each game is played under identical conditions to ensure fair comparison. The specific setup is as follows: Baseline Agents (BaseAgent & StructuredAgent): Each agent plays 10 full games against AlphaBeta. Random seeds are fixed for initial board state generation to ensure consistency across runs. PromptEvolver: This multi-agent system iteratively evolves its prompt across 10 evolution cycles with the mini-map. In each evolution, the evolver gathers information to update the prompt, and the updated player plays 5 games against AlphaBeta. We return the average victory points across games per turn. 10 game trial is then performed with the highest scoring prompt using the full-map. AgentEvolver: Similar to the PromptEvolver setup, this system undergoes 10 full-code evolution cycles using the mini-map. Each cycle includes the Evolver decision loop ending with writing new code, followed by 10 games played using the newly generated agent. We report the average victory points across 10 games for the highest scoring generated agent  (Table 3)  . All experiments track average victory points (VP), win rate, average number of settlements, cities, number of longest road achievements, largest army achievements, and victory points from development cards, turn count, and logs from each agent used in the respective architectures. Evaluation We compare the best generated player from the PromptEvolver and the StructuredAgent to the BaseAgent as baseline since they all utilize an LLM to make decisions. The best generated player from the AgentEvolver is compared against the RandomAgent as it is evolved from blank file, without any explicit instruction to use an LLM as the decision making unit. Randomness Control All games were initialized using randomized board layouts and due to inherent randomness in Catan (dice rolls, opponent behavior), we average results over multiple games to ensure statistical robustness. We also note that while AlphaBetas behavior is deterministic, LLMbased agents may produce slight variability even with temperature control. We use temperature value of 0.7 for our experiments."
        },
        {
            "title": "6 Results",
            "content": "This section presents the empirical results of our experiments, evaluating how effectively various LLM-driven agent architectures performed against the heuristic-based AlphaBeta baseline in the Settlers of Catan game. The results are gathered with all agents against AlphaBeta across 10 games on the default map. We showcase the best performing generated agents from the PromptEvolver and AgentEvolver from their 10 evolutions. 6 Table 1: Agent performance across models (avg score and avg turns). Agent GPT-4o Claude 3."
        },
        {
            "title": "Turns",
            "content": "Mistral Large Score"
        },
        {
            "title": "BaseAgent\nStructuredAgent\nPromptEvolver",
            "content": "3.60 72.20 3.80 6% 73.10 4.40 22% 95.50 3.70 80.80 4.10 11% 76.40 7.20 95% 135.50 3.60 67.80 2.50 31% 82.10 3.70 3% 81."
        },
        {
            "title": "RandomPlayer\nAgentEvolver",
            "content": "2.34 68.12 2.73 36% 70.00 2.34 68.12 2.80 40% 70.50 2.34 68.12 2.67 34% 68. (a) Average Game Length (b) Average Victory Points Figure 3: Performance comparison across LLM-agent architectures and model backends. Left:Figure 3a. Average number of turns taken to complete the game across architectures. Higher values such as Claude-3.7 PromptEvolver indicate more competitive game against the AlphaBeta opponent. Right: Figure 3b Average Victory Points (VP) achieved, showing that self-improving agents (PromptEvolver and AgentEvolver) outperform baseline agents, especially when paired with more capable LLMs like GPT-4o and Claude 3.7."
        },
        {
            "title": "7 Discussion",
            "content": "PromptEvolver Evolution Analysis The PromptEvolver framework demonstrated considerable variability in effectiveness among the different language models (Claude 3.7, GPT-4o, Mistral Large). In general, the system successfully improved agent performance by autonomously iterating on strategic prompts, although the degree of success heavily depended on the model used. Claude 3.7 exhibited the most significant strategic advancements, systematically developing detailed strategic prompts that outlined clear short-term and long-term plans, including precise settlement placement, resource prioritization, development card usage, and robust response strategies against opponent actions. This approach resulted in substantial performance gains (up to 95% improvement), notably surpassing the baselines. GPT-4o achieved moderate performance gains through incremental enhancements primarily centered around improving mid-game strategies, robber placement, and targeted trading tactics. However, the improvements, while steady, were less aggressive and lacked the comprehensive strategic vision developed by Claude. Mistral Large showed the least effectiveness, achieving minimal performance gains due to superficial revisions and limited strategic horizon. Mistral primarily focused on reactionary updates based on immediate past performance without deeper strategic planning or addressing fundamental gameplay weaknesses, evident by the low percent change in tokens across iterations in Table 2. Consequently, performance metrics saw marginal improvements only. key limitation observed across the PromptEvolver was its dependence on the inherent strategic reasoning capabilities of the underlying LLM. Models with less sophisticated strategic reasoning (such as Mistral) faced significant challenges in generating meaningful prompt refinements. Additionally, the associated randomness and complexity of the game made it difficult for the PromptEvolver to 7 Table 2: PromptEvolver Evaluation Metrics Across All Iterations Model % Improvement Length (Tokens) % Change (Tokens) GPT-4o Claude-3.7 Mistral-Large 22.2% 94.6% 2.7% 440 754 337 53.4% 52.7% 13.9% recognize the Player Agents reaction to the new prompt. Despite these limitations, the PromptEvolver demonstrated significant strengths. Notably, the models were able to identity through self learning what kind of input would be best for their own decision process. The approach effectively demonstrated the capacity of LLM-driven agents to improve not only their gameplay strategy, but also the structure of their own prompts. (a) Mistral Large (b) Claude 3.7 (c) GPT 4o Figure 4: Evolution performance of LLM agents over time. Average Victory Points (VP) across evolution steps for each model (Mistral Large, Claude 3.7, GPT-4o). PromptEvolver consistently improves with more iterations, especially when paired with stronger models like Claude 3.7 and GPT-4o. AgentEvolver shows variable performance gains, while static baselines (BaseAgent, StructuredAgent) remain flat. AgentEvolver Evolution Analysis The AgentEvolver architecture showed promising yet mixed results in its capability to autonomously design and iteratively enhance player agents through direct code generation and refinement. The results highlighted significant variations in strategic depth and implementation robustness among the tested models (Claude 3.7, GPT-4o, Mistral Large). As shown in Table 3, Claude 3.7 emerged as the top-performing model, showing pronounced strategic evolution with 40.0% improvement. It consistently implemented effective adaptations such as improved settlement and road planning, strategic use of development cards, and context-sensitive resource management. Claudes helper agents (Analyzer, Researcher, Strategizer, Coder) were notably proficient at providing sophisticated strategies, accurate analytical feedback, and reliable code implementations, resulting in steady incremental performance improvements. GPT-4o provided modest strategic and implementation advancements, excelling primarily in technical debugging and reliable code refinements. Its evolutionary process was stable but conservative, focusing more on resolving immediate functional issues rather than pursuing ambitious strategic enhancements. As result, GPT-4o delivered consistent but limited improvements relative to Claude. Mistral Large exhibited significant difficulties with strategic coherence and adaptability, typically repeating basic strategies without adequately responding to dynamic game conditions. Its helper agents frequently produced superficial analysis and unstable implementations, substantially limiting evolutionary improvements and overall performance gains. Model GPT-4o Claude-3.7 Mistral-Large Table 3: AgentEvolver Evaluation Metrics Across All Iterations % Improvement Length (Tokens) % Change (Tokens) % No Errors 36.5% 40.0% 33.5% 434 598 431 8 27.4% 53.5% 5.6% 35% 50.0% 50.0% However, the AgentEvolver system struggled to surpass simpler LLM agents and the AlphaBeta. While individual agents demonstrated potential, the absence of robust mechanism for long-term memory and advanced strategy integration limited consistent strategic refinement, ultimately reducing competitive effectiveness of the Player relative to both static LLM-based agents and the more strategically consistent AlphaBeta heuristic. Nevertheless, the project constitutes significant success in demonstrating the feasibility of constructing fully autonomous framework guided by large language model, capable of generating and refining executable code entirely from scratch. Notably, the system operated without prior access to Catanatron documentation and still succeeded in discovering, adapting to, and leveraging the games API and strategic mechanics through self-guided exploration and debugging. This highlights the models capacity to autonomously learn complex domain-specific logic and programming interfaces without human intervention. Moreover, the Player Agent achieved consistent improvements over the Random baseline and lays promising foundation for applying such autonomous, evolution-driven LLM architectures to other strategic environments and code synthesis tasks."
        },
        {
            "title": "8 Limitations",
            "content": "While our approach demonstrates the potential of self-improving LLM agents, several limitations should be noted. First, the system is computationally expensive: each evolution cycle involves multiple agent roles, prompt/code generation, and full-game simulations, which limits scalability and rapid experimentation. Second, our evaluation is restricted to Settlers of Catan and it remains unclear how well the framework generalizes to other strategic or real-world environments. Additionally, performance is strongly tied to the base models capabilities. Less capable LLMs, such as Mistral, struggled to meaningfully improve, highlighting bottleneck in the underlying reasoning abilities. We also evaluate primarily against fixed heuristic-based bot (AlphaBeta) and do not benchmark against learning-based baselines such as reinforcement learning agents."
        },
        {
            "title": "9 Ethical Statement & Broader Impact",
            "content": "We study self-improving LLM agents in closed, low-risk environment (Settlers of Catan), where agents iteratively revise prompts and code to enhance strategic play. To ensure safety, all generated code is sandboxed and manually reviewed, with no access to external systems. We avoid anthropomorphic framing, clarifying that self-improvement reflects architectural processes, not autonomy. Our framework emphasizes transparency and interpretability, producing auditable artifacts unlike black-box methods. We view this work as step toward safer, more accountable AI systems and will open-source our code with clear safeguards and usage guidelines."
        },
        {
            "title": "10 Conclusion",
            "content": "This work demonstrates the viability and promise of self-evolving LLM agents in complex, longhorizon decision-making tasks. By embedding language models within multi-agent systems capable of autonomously refining prompts and rewriting executable code, we show that LLMs can meaningfully improve their strategic behavior in competitive game environment. Our experiments in Settlers of Catan reveal that agents using iterative evolution consistently outperform static baselines, adopting more coherent and long-term strategies over time. Beyond performance gains, our analysis highlights distinct behavioral patterns across LLMs in how they interpret failure, integrate feedback, and adapt strategy. These findings suggest growing potential for LLMs not just as players, but as designers of themselves. Future work will explore extensions to multi-agent negotiation, broader generalization across games, and tighter integration of symbolic and neural reasoning for even more sophisticated autonomous improvement."
        },
        {
            "title": "References",
            "content": "[1] Mohamed Aghzal, Erion Plaku, and Ziyu Yao. Can large language models be good path planners? benchmark and investigation on spatial-temporal reasoning, 2025. [2] Catan Collector. How to Identify Your Version of Catan. https://catancollector.com/ catan-links/how-to-identify-your-version-of-catan. Accessed: 2025-05-15. [3] Catan Fusion. 3 Royal Weddings in Catan. http://catanfusion.com/index.php/blog/ entry-3-royal-weddings-in-catan. Accessed: 2025-05-15. [4] Yanan Chen, Ali Pesaranghader, Tanmana Sadhu, and Dong Hoon Yi. Can we rely on llm agents to draft long-horizon plans? lets take travelplanner as an example. arXiv preprint arXiv:2408.06318, 2024. [5] B. Collazo. Catanatron: Settlers of catan bot simulator and strong ai player. https://github. com/bcollazo/catanatron, 2025. [6] Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Joshua Clymer, and Arjun Yadav. Gamebench: Evaluating strategic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613, 2024. [7] Xiangjue Dong, Maria Teleki, and James Caverlee. survey on llm inference-time selfimprovement. arXiv preprint arXiv:2412.14352, 2024. [8] Meta Fundamental AI Research Diplomacy Team (FAIR), Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):10671074, 2022. [9] Same Farrar. Catanatron-llm. https://github.com/samefarrar/catanatron_llm, 2024. Accessed: 2025-05-16. [10] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023. [11] Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, and Georgios Yannakakis. Large language models and games: survey and roadmap. IEEE Transactions on Games, 2024. [12] Quentin Gendre and Tomoyuki Kaneko. Playing catan with cross-dimensional neural network. In Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 2327, 2020, Proceedings, Part II 27, pages 580592. Springer, 2020. [13] Hilmy Abiyyu A. Robot icons created by Hilmy Abiyyu A. - Flaticon. https://www. flaticon.com/free-icons/robot. Accessed: 2025-05-15. [14] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society, 2023. [15] Daniel Monroe and Philip A. Chalmers. Mastering chess with transformer model, 2024. [16] murmur. Conversation icons created by murmur - Flaticon. https://www.flaticon.com/ free-icons/conversation. Accessed: 2025-05-15. [17] murmur. Thought bubble icons created by murmur - Flaticon. https://www.flaticon.com/ free-icons/thought-bubble. Accessed: 2025-05-15. [18] Muhammad Umair Nasir, Steven James, and Julian Togelius. Gametraversalbenchmark: Evaluating planning abilities of large language models through traversing 2d game maps. arXiv preprint arXiv:2410.07765, 2024. 10 [19] Siddharth Nayak, Adelmo Morrison Orozco, Marina Ten Have, Vittal Thirumalai, Jackson Zhang, Darren Chen, Aditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, James Harrison, Brian Ichter, Anuj Mahajan, and Hamsa Balakrishnan. Llamar: Long-horizon planning for multi-agent robots in partially observable environments, 2025. [20] Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. Alphaevolve: coding agent for scientific and algorithmic discovery. Technical report, Google DeepMind, 2025. White paper. [21] Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, et al. Civrealm: learning and reasoning odyssey in civilization for decision-making agents. arXiv preprint arXiv:2401.10568, 2024. [22] John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, et al. Mastering board games by external and internal planning with language models. arXiv preprint arXiv:2412.12119, 2024. [23] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. [24] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484489, 2016. [25] Smashicons. Trade icons created by Smashicons - Flaticon. https://www.flaticon.com/ free-icons/trade. Accessed: 2025-05-15. [26] Yuda Song, Hanlin Zhang, Carson Eisenach, Sham Kakade, Dean Foster, and Udaya Ghai. Mind the gap: Examining the self-improvement capabilities of large language models. arXiv preprint arXiv:2412.02674, 2024. [27] István Szita, Guillaume Chaslot, and Pieter Spronck. Monte-carlo tree search in settlers of catan. In Advances in computer games, pages 2132. Springer, 2009. [28] Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. On the planning abilities of large language models (a critical investigation with proposed benchmark), 2023. [29] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, 2023. [30] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427, 2023. [31] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. [32] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. [33] yaicon. Build icons created by yaicon - Flaticon. https://www.flaticon.com/ free-icons/build. Accessed: 2025-05-15. [34] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions, 2023. 11 [35] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023."
        },
        {
            "title": "A Performance metrics for each agent with various models",
            "content": "Agent Win Rate Avg VP Avg Turns Avg Settles Avg Cities Avg Roads Avg Army Avg Dev VP BaseAgent StructuredAgent PromptEvolver RandomPlayer AgentEvolver 0.00 0.00 0.10 3.60 3.80 4. 72.20 73.10 95.50 1.80 2.00 2.10 0.50 0.30 0.60 0.00 0.00 0.00 0.00 0.20 0.30 0.00 0. 2.00 0.04 1.70 0.20 Table 4: Detailed performance metrics across agent types for GPT-4o. 68.12 125.4 2.34 4.50 0.00 0.00 0.03 0.90 0.80 0.80 0. 0.20 0.60 Agent Win Rate Avg VP Avg Turns Avg Settles Avg Cities Avg Roads Avg Army Avg Dev VP BaseAgent StructuredAgent PromptEvolver 0.00 0.00 0.10 3.70 4.10 7. 80.80 76.40 135.5 2.10 1.80 0.90 0.30 0.20 1.30 0.00 0.00 0.10 0.20 0.40 0.80 RandomPlayer AgentEvolver 0.00 0.10 2.00 1.80 Table 5: Detailed performance metrics across agent types for Claude 3.7. 68.12 120.5 0.03 0.70 0.00 0.20 0.04 0. 2.34 6.10 0.60 1.10 1.90 0.20 1.50 Agent Win Rate Avg VP Avg Turns Avg Settles Avg Cities Avg Roads Avg Army Avg Dev VP BaseAgent StructuredAgent PromptEvolver 0.00 0.00 0.00 3.60 2.50 3.70 67.80 82.10 81.00 2.50 2.30 1.90 0.10 0.00 0.30 0.00 0.10 0. 0.20 0.00 0.10 RandomPlayer AgentEvolver 0.00 0.10 2.00 0.60 Table 6: Detailed performance metrics across agent types for Mistral Large. 68.12 137.1 0.03 1. 0.04 0.00 2.34 4.00 0.00 0.10 0.50 0.00 0.40 0.20 0."
        },
        {
            "title": "B Prompt Example",
            "content": "Figure 5: Sample StructuredAgent LLM prompt."
        }
    ],
    "affiliations": [
        "University of California, Santa Barbara"
    ]
}