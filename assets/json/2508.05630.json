{
    "paper_title": "MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes",
    "authors": [
        "Henghui Ding",
        "Kaining Ying",
        "Chang Liu",
        "Shuting He",
        "Xudong Jiang",
        "Yu-Gang Jiang",
        "Philip H. S. Torr",
        "Song Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video."
        },
        {
            "title": "Start",
            "content": "MOSE DATASET PAGE: MOSE.VIDEO 1 MOSEv2: More Challenging Dataset for Video Object Segmentation in Complex Scenes Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Fellow, IEEE, Yu-Gang Jiang, Fellow, IEEE, Philip H.S. Torr, Song Bai AbstractVideo object segmentation (VOS) aims to segment specified target objects throughout video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% & ) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 highquality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video. Index TermsComplex Video Object Segmentation, MOSEv2 Dataset, Complex Scenes."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "V IDEO object segmentation (VOS) [1], [2], [3], [4] aims to segment specified target objects throughout the entire video. It is one of the most fundamental and challenging computer vision tasks, playing crucial role in various practical applications involving video analysis and understanding, e.g., autonomous vehicle, augmented reality, and video editing. There are different settings for VOS, for example, semi-supervised VOS [5], [6] that gives the first-frame mask, bounding box, or points of the target object, unsupervised VOS [7], [8] that automatically finds primary or salient objects, and interactive VOS [9], [10] that relies on the users interactions of the target object. VOS has been extensively studied in the past using traditional techniques [11], [12], [13] or deep learning methods [14], [15]. The deep-learningbased methods have greatly improved the VOS performance and surpassed the traditional techniques by large margin. Current state-of-the-art VOS methods [14], [15], [16] have achieved very high performance on two of the most commonlyused VOS datasets DAVIS [2], [3] and YouTube-VOS [4]. For example, XMem [16] achieves 92.0% & on DAVIS 2016 [2], 87.7% & on DAVIS 2017 [3], and 86.1% on YouTubeVOS [4]. With such high performance, it seems that the video Henghui Ding, Kaining Ying, and Yu-Gang Jiang are with Fudan University, Shanghai, China. (e-mail: henghui.ding@gmail.com) Shuting He is with Shanghai University of Finance and Economics, China. Chang Liu and Song Bai are with ByteDance Inc. Xudong Jiang is with Nanyang Technological University, Singapore. Philip H.S. Torr is with University of Oxford, United Kingdom. Henghui Ding and Kaining Ying are co-first authors. object segmentation has been well resolved. However, do we really perceive objects in realistic scenarios? To answer this question, we introduced the coMplex video Object SEgmentation (MOSEv1) dataset in [1], which revisits VOS under more realistic and complex scenes where traditional datasets fall short. In contrast to DAVIS [2], [3] and YouTubeVOS [4], where the target objects are typically salient and isolated, MOSEv1 focuses on challenging cases such as frequent object disappearance and reappearance, small or inconspicuous objects, heavy occlusions, and crowded environments. These real-world conditions significantly affect segmentation performance, with XMem only achieving 57.6% & on MOSEv1. Following its release in 2023, MOSEv1 has quickly attracted attention from the research community. We successfully organized several competitions based on this dataset, including PVUW [17], [18] and LSVOS [19], which have greatly promoted research in this field. series of strong VOS methods such as SAM2 [15] have pushed the performance from initial baselines to 76.4% & , highlighting both the difficulty and the value of the dataset while demonstrating substantial progress in handling complex scenes. In this work, building on the success and limitations of MOSEv1, we present MOSEv2, more challenging dataset that further pushes the boundaries of VOS in real-world environments. MOSEv2 significantly increases the complexity across multiple dimensions. Core challenges from MOSEv1, such as disappearancereappearance dynamics, small or occluded objects, and crowded scenes, are preserved but made more frequent, more challenging, and more realistic. Beyond that, MOSEv2 introduces range of new challenges rarely covered in previous datasets, including adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged ob5 2 0 2 ] . [ 1 0 3 6 5 0 . 8 0 5 2 : r MOSE DATASET PAGE: MOSE.VIDEO 2 Fig. 1: Example videos from the proposed MOSEv2 dataset. The selected target objects are masked in orange. The target object in case â‘  is enlarged for better visualization. The most notable features of MOSEv2 include both challenges inherited from MOSEv1 [1] such as disappearance-reappearance of objects (â‘ -â‘©), small/inconspicuous objects (â‘ ,â‘¢,â‘¥), heavy occlusions, and crowded scenarios (â‘ ,â‘¡), as well as newly introduced complexities including adverse weather conditions (â‘¥), low-light environments (â‘¤-â‘¦), multi-shots (â‘§), camouflaged objects (â‘¤), non-physical objects like shadows (â‘£), and knowledge dependency (â‘¨,â‘©). The goal of MOSEv2 dataset is to provide platform that promotes the development of more comprehensive and robust video object segmentation algorithms. jects, non-physical targets (e.g., shadows, reflections), knowledgedependent scenarios, etc. These additions aim to bridge the gap between current VOS benchmarks and the diverse, unconstrained nature of real-world scenes. With these multifaceted complexities, the introduced MOSEv2 serves as next-generation benchmark to assess and drive progress in complex video object segmentation under realistic, dynamic, and highly unconstrained environments. MOSEv2 consists of 5,024 videos and 10,074 annotated object instances spanning 200 diverse categories, resulting in over 701,976 high-quality segmentation masks. Representative examples are shown in Fig. 1, illustrating the intensified and newly introduced challenges. common pattern is object disappearance and reappearance, as seen in the 3rd example where vehicle repeatedly disappears and reappears under overpasses, requiring robust temporal association. Challenges like small or inconspicuous objects, crowded scenes, and severe occlusions are also more prominent. For example, in the 1st example, tiny person moves through dense crowd, frequently occluded by others. Examples 4-10 highlight some new challenges in MOSEv2. Adverse weather (e.g., fog in the 6th), low-light conditions (e.g., underwater in the MOSE DATASET PAGE: MOSE.VIDEO 3 5th, nighttime in the 7th), and multi-shot sequences (e.g., 8th) introduce appearance instability, motion ambiguity, and temporal discontinuities. These demand strong generalization and longrange association. In addition, MOSEv2 includes novel object categories that are difficult for existing methods. For example, camouflaged objects (5th) blend into backgrounds, while nonphysical targets like shadows (4th) lack stable visual cues and change shape based on external factors. Besides, MOSEv2 further introduces knowledge-dependent scenarios (e.g., 9th and 10th examples) that require higher-level reasoning. For example, the 9th example requires optical character recognition to differentiate similar-looking blocks, while the 10th tests understanding of physics-based causality, as the target must be inferred from surrounding motion despite being invisible. These diverse and fine-grained challenges make MOSEv2 an ideal benchmark for evaluating and advancing the VOS robustness in open-world complex scenes. We believe MOSEv2 will drive meaningful progress in advancing video understanding toward real-world deployment. To thoroughly analyze the proposed MOSEv2 dataset, we retrain and benchmark 20 representative VOS methods under different settings. Experimental results demonstrate that the complexity of real-world videos in MOSEv2 significantly degrades the performance of current state-of-the-art VOS methods. For example, the & score of SAM2 [15] reaches 90.7% on DAVIS 2017 [3] and 76.4% on MOSEv1 [1], but notably drops to 50.9% on MOSEv2. Similarly, Cutie [14] achieves 87.9% on DAVIS 2017 and 69.9% on MOSEv1, but markedly declines to 43.9% on MOSEv2. These consistent performance drops highlight the significant challenges posed by the more realistic and complex scenarios in MOSEv2. Beyond VOS, MOSEv2 is applicable to wide range of video perception tasks that require fine-grained understanding in complex scenes. In particular, we evaluate its applicability for video object tracking (VOT) by benchmarking 9 state-of-the-art VOT methods [15], [20], [21], [22], [23], [24], [25], [26], [27] on MOSEv2. While these methods perform well on standard VOT benchmarks such as LaSOT [28] and GOT-10k [29], we observe consistent and notable performance drop on MOSEv2, suggesting that MOSEv2 introduces new and significant challenges not only for VOS but also for VOT. This highlights the broader applicability of MOSEv2 as comprehensive benchmark for evaluating the robustness and generalization capabilities of diverse video understanding algorithms under realistic and complex scenes. In summary, our main contributions are as follows: We present MOSEv2 (coMplex video Object SEgmentation), more challenging dataset for video object segmentation in complex scenes. Compared to MOSEv1, MOSEv2 includes not only more frequent object disappearance and reappearance, heavier occlusions, crowding, and smaller targets, but also introduces additional real-world complexities, such as adverse weather (e.g., rain, fog), low-light scenes (e.g., nighttime, underwater), multi-shot videos, camouflaged objects, non-physical targets (e.g., shadows, reflections), and knowledge-dependent scenarios. We provide detailed comparative analysis between MOSEv2 and existing datasets, highlighting its unique challenges and greater complexity that better represents real-world video understanding scenarios. We conduct comprehensive benchmarks of state-of-the-art methods on MOSEv2 across various VOS and VOT task settings, including semi-supervised VOS with mask, box, and point initialization, as well as unsupervised VOS, interactive VOS, and video object tracking. We perform in-depth analysis of model performance and failure cases on MOSEv2, highlighting the key challenges posed by MOSEv2 and outlining potential directions for advancing robust video understanding in the wild."
        },
        {
            "title": "2 RELATED WORK\n2.1 Video Object Segmentation",
            "content": "Video object segmentation (VOS) aims to segment specific object throughout video. Based on how the target object is specified, VOS can be categorized into four main settings: 1) semisupervised VOS (also known as semi-automatic VOS [30] or oneshot VOS), 2) unsupervised VOS (also called automatic VOS or zero-shot VOS), 3) interactive VOS, and 4) referring VOS. Semi-supervised VOS. Semi-supervised VOS [5] aims to segment the target object throughout video, given its mask in the first frame. Most existing works can be categorized into propagation-based methods [4], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44] and matching-based methods [14], [16], [37], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55]. Propagation-based methods leverage the predicted mask from the previous frame to guide the segmentation of the current frame, thereby propagating object cues in frameby-frame manner. Matching-based methods, on the other hand, first encode the target object into an embedding space and then perform per-pixel classification by comparing the similarity between each pixels feature and the stored object embedding. Since obtaining pixel-level annotations is often expensive and timeconsuming, some methods employ bounding box as the first-frame reference [56], [57], [58]. For example, SiamMask [56] integrates mask prediction branch into fully convolutional Siamese object tracker to generate binary segmentation masks. Recently, SAM2 [15] adopts promptable visual segmentation (PVS), which allows the model to accept prompts in the form of positive/negative clicks, bounding boxes, or masks on any frame of video. This flexible interaction mechanism significantly improves the models adaptability and generalization across diverse scenarios. Following SAM2, several efficient extensions [25], [26], [27], [59] have been proposed to improve its performance. For example, SAM2Long [27] addresses error accumulation by exploring multiple segmentation pathways via constrained tree search. DAM4SAM [26] introduces distractor-aware memory and an introspection-based update strategy to mitigate ambiguity from visual distractors. To better handle dynamic scenes, recent works [25], [59] incorporate motion modeling into promptable segmentation. SAMURAI [25] integrates Kalman filtering [60] for adaptive memory selection, while MoSAM [59] enhances robustness through motion-aware sparse and dense prompts combined with spatiotemporal memory mechanisms. These SAM2 variant methods achieved impressive performance on the previous VOS datasets [1], [2], [3], [4], [15], [61], [62]. Interactive VOS. This task aims at segmenting the target object in video indicated by users interaction (e.g., clicks or scribbles) [9], [10], [37], [63], [64], [65], [66], [67], it is special form of semi-supervised VOS. Existing methods mainly follow paradigm of interaction-propagation way. Besides the feature encoder that extracts pixel features, there are other two modules placed on the feature encoder to achieve interactive video object segmentation, i.e., interactive segmentation module that corrects prediction based MOSE DATASET PAGE: MOSE.VIDEO 4 on users interaction and mask propagation module that propagates user-corrected masks to other frames. SAM2 [15] has also demonstrated strong capabilities in this task, offering superior performance with flexible interaction mechanisms, significantly enhancing both segmentation quality and user experience. Referring VOS. Referring video object segmentation [68], [69], [70], [71], [72], [73] is an emerging setting that aims to segment the target object in video given text expression. Early methods can be broadly classified as bottom-up methods and topdown methods. Bottom-up methods [70], [74], [75] perform firstframe segmentation followed by mask propagation or per-frame segmentation with post-hoc association. Top-down methods [76], [77] first generate candidate tracklets and then select the one best aligned with the expression. The introduction of motion-centric datasets in MeViS [68] and MeViSv2 [69] has drawn increased attention to the importance of temporal dynamics for accurate language grounding. Subsequent works [78] highlight that temporal modeling is essential for accurate language grounding. Recent works [79], [80] also leverage multimodal large language models [81], [82] to handle expressions requiring complex reasoning, which enables human-like understanding and generalization ability across diverse language descriptions. With the latest datasets such as MeViSv2 [69] and OmniAVS [83] supporting expressions across multiple modalities, omnimodal referring VOS is expected to gain increasing attention in future research [84]. Unsupervised VOS. This setting requires no manual clues but aims to automatically segment the primary objects in video [85], [86], [87], [88], [89], [90], [91], [92], [93], [94]. However, it typically focuses on objects from set of pre-defined categories. Early methods relied heavily on post-processing techniques [85]. Then end-to-end training methods become the mainstream, which can be categorized into local content encoding and contextual content encoding paradigms. Local content encoding methods [7], [8], [91], [93], [95], [96], [97] often employ two-stream architectures to separately process optical flow and RGB information. Contextual content encoding methods [98], [99], [100] aim to capture long-range dependencies and global context. Recent methods have adapted propagation frameworks for this task, DEVA [101] proposes decoupled framework combining image-level segmentation with class-agnostic temporal propagation, eliminating the need for taskspecific video training data. EntitySAM [102] extends SAM2 for zero-shot video entity segmentation by automatically discovering and tracking all entities without explicit prompts."
        },
        {
            "title": "2.2 Related Video Segmentation and Tracking Tasks\nThere are other video segmentation and tracking tasks related to\nVOS, e.g., video instance segmentation, video semantic segmen-\ntation, video panoptic segmentation, and video object tracking.\nâˆ™ Video Instance Segmentation (VIS). Video instance segmen-\ntation is extended from image instance segmentation by Yang et\nal. [103], it simultaneously conducts detection, segmentation, and\ntracking of instances of predefined categories in videos. Thanks\nto the large-scale VIS dataset YouTube-VIS [103], a series of\nlearning methods have been developed and greatly advanced the\nperformance of VIS [104], [105], [106], [107], [108], [109], [110].\nThen, occluded video instance segmentation is proposed by [111]\nto study the VIS under occluded scenes. Similar to [111], we study\nvideo segmentation under complex scenarios like occlusions, but\ndifferent from [111], we focus on video object segmentation, and\nthe proposed MOSEv2 dataset contains more videos and covers a\nbroader range of real-world challenges beyond occlusion.",
            "content": "Video Semantic Segmentation (VSS). Driven by the success in image semantic segmentation [112], [113], [114] and large-scale video semantic segmentation datasets [115], [116], [117], video semantic segmentation has drawn lots of attention and achieved significant achievements. Compared to image domain, temporal consistency and model efficiency are the new efforts in the video domain. For example, Sun et al. [118], [119] propose Coarse-toFine Feature Mining to capture both static context and motional context. Syed Hesham et al. [120] propose state space modelbased [121] architecture for efficient temporal feature sharing. Video Panoptic Segmentation (VPS). Kim et al. [122] introduce panoptic segmentation to the video domain to simultaneously segment and track both the foreground instance objects and background stuff. They also build Cityscapes-VPS dataset with 500 videos. Then, Miao et al. [123] build larger VPS dataset called VIPSeg with 3,536 videos. Existing methods [124], [125] mainly add temporal refinement or cross-frame association modules upon image panoptic segmentation models [126] to enhance temporal conformity and instance tracking performance. Li et al. [127] propose OMG-Seg, unified transformer-based model that supports video panoptic segmentation along with over ten other segmentation tasks via task-specific queries and outputs. Video Object Tracking (VOT). Different from VOS that focuses on pixel-level mask segmentation, VOT [128] aims to locate target object with bounding boxes in subsequent frames given its initial bounding box annotation. VOT has seen significant progress in recent years, with methods designed to handle challenging scenarios such as scale variations, occlusions, distractors, and complex backgrounds. The dominant approaches can be broadly categorized into Siamese-based methods [129], [130], [131] that learn discriminative feature embeddings through twin networks, and transformer-based methods [132], [133], [134], [135], [136] that leverage self-attention mechanisms to model long-range dependencies for robust tracking. These methods have achieved impressive performance on existing VOT benchmarks like VOT [137], LaSOT [28], and GOT-10k [29]. The proposed MOSEv2 dataset also supports VOT task while introducing more complex real-world scenarios like dense crowds, occlusions, and frequent disappearance-reappearance that pose significant challenges to existing tracking methods."
        },
        {
            "title": "2.3 Complex Scene Understanding\nComplex scene understanding has become a research focus in the\nimage understanding domain [138], [139], [140], [141], [142],\n[143], [144], [145], [146], [147], [148], [149]. For example, Ke\net al. [150] propose Bilayer Convolutional Network (BCNet) to\ndecouple overlapping objects into occluder and occludee layers.\nZhang et al. [140] propose a self-supervised approach to con-\nduct de-occlusion by ordering recovery, amodal completion, and\ncontent completion. On the video domain, however, occlusion\nunderstanding is still underexplored with only several multi-object\ntracking works [151], [152], [153], [154]. For example, Chu et\nal. [151] propose a spatial temporal attention mechanism (STAM)\nto capture the visible parts of targets and deal with the drift\nbrought by occlusion. Zhu et al. [152] propose dual matching\nattention networks (DMAN) to deal with the noisy occlusions\nin multi-object tracking. Li et al. [155] propose to track every\nthing in the open world by performing class-agnostic association.\nIn this work, we build a new complex video object segmentation\ndataset, MOSEv2, to facilitate future research on complex scene\nunderstanding in VOS and other related video understanding tasks.",
            "content": "MOSE DATASET PAGE: MOSE.VIDEO 5 Fig. 2: Category distribution of MOSEv1 and MOSEv2."
        },
        {
            "title": "3.1 Video Collection and Annotation",
            "content": "Video Collection. The videos in MOSEv2 dataset are obtained from two sources. The first source is inherited from MOSEv1 [1] with 2,149 videos. The second source consists of newly selfcaptured videos from real-world scenarios and copyright-free videos from the internet that have not appeared in any existing dataset. MOSE is specifically designed for video object segmentation in complex scenes. To ensure the complexity and diversity of the collected videos, we follow set of strict selection rules: R1. Each video must contain several objects, while videos with only single object are excluded. Specifically, videos with crowded objects of similar appearance are highly valued. R2. Occlusions should be present in the video. Videos that do not have any occlusions throughout the frames are discarded. We encourage occlusions caused by other moving objects. R3. Great emphasis should be placed on scenarios where objects disappear and then reappear due to occlusions or out-of-view. R4. The target objects should encompass diverse range of scales (e.g., small-scale vs. large-scale) and visibility conditions (e.g., conspicuous, partially visible). R5. The video must exhibit clear motion, either from object movement or camera motion. Videos with static objects and stationary camera should be discarded. Besides the points mentioned above, we further emphasize the following rules in the design of MOSEv2: R6. Target object categories should be diversified, including novel classes not present in MOSEv1, such as camouflaged objects, shadows, and reflections. R7. Longer videos are preferred when they encompass more challenging patterns, such as long-term occlusions, complex motion dynamics, and repeated object disappearance and reappearance, rather than merely for their duration. R8. wide range of challenging environments are given priority during collection, such as low-light scenes, cluttered scenes, and varying weather conditions (e.g., rain, fog, snow). R9. Multi-shot videos are encouraged, where objects undergo significant spatial or appearance changes across shots. R10. Videos requiring specific knowledge, such as optical character recognition, spatial reasoning, physical principles, and multi-view understanding are deliberately included. Video Annotation. Having all videos for MOSEv2 been collected, our research team looks through them and figure out set of targets-of-interest for each of the videos. Then we slightly clip the start and the end of videos, to reduce the number of less motional or simple frames in the video. Next we annotate the first-frame mask of the target objects, as VOS input. Following this, the videos are sent to the annotation team along with the first-frame masks for annotation of the subsequent video frames. Using the given first-frame mask as reference, the annotation team is required to identify the target object in the given firstframe mask, then track and annotate the segmentation mask of the target object in all frames following the first frame. The process of annotating videos has been made easier with the help of an interactive annotation tool that we developed. The annotation tool automatically loads videos and all target objects. Annotators use the tool to load and preview videos and first-frame masks, annotate and visualize the segmentation masks in the subsequent frames, and save them. The annotation tool also has built-in interactive object segmentation network SAM2 [15], to assist annotations in producing high-quality masks. To ensure the annotation quality under complex scenes, the annotators are required to clearly track and precisely segment the object. For frames in which the target object is disappeared or is fully occluded, annotators also need to confirm that the output masks of such frames shall be blank. It is requirement that all of our videos be annotated every five frames at the very least. For the purpose of testing the frame-rate robustness of the models, some videos are annotated every frame. After annotation, the videos are reviewed by our verification team to ensure high-quality video mask annotations."
        },
        {
            "title": "3.2 Dataset Statistics",
            "content": "In TABLE 1, we analyze the data statistics of MOSEv2 in comparison with existing VOS datasets, such as DAVIS [2], [3], YouTube-VOS [4], LVOS [61], [62], SA-V [15], MOSEv1 [1], as well as VOT datasets, including GOT-10k [29], LaSOT [28], VOT [137], and DiDi [26]. As shown in TABLE 1, MOSEv2 expands upon MOSEv1 by adding 2,875 additional videos, reaching total of 5,024 videos and 701,976 mask annotations for 10,074 objects. Among existing VOS datasets except for SAV [15], MOSEv2 achieves the largest scale, surpassing others in video count, annotation volume, and total duration. Categories. As shown in TABLE 1, MOSEv2 contains 200 object categories, the most among existing VOS datasets. Fig. 2 presents the detailed category distribution of MOSEv2. Building on the 36 categories in MOSEv1, MOSEv2 significantly expands the set to 200, covering not only common categories such as squirrels, footballs, and otters, but also rare ones like Newtons cradle and camouflaged objects (98 instances), as well as non-physical targets like shadows (125 instances). This broad coverage supports more comprehensive and robust evaluation of VOS methods. MOSE DATASET PAGE: MOSE.VIDEO 6 TABLE 1: Statistical comparison between MOSEv2 and existing video object segmentation and tracking datasets. Annotations denotes the number of annotated masks or boxes. Duration denotes the total duration of annotated videos, in minutes by default unless noted. Disapp. Rate measures the frequency of objects disappearing in at least one frame, while Reapp. Rate measures the frequency of objects that previously disappeared and later reappear. Distractors quantifies scene crowding as the average number of visually similar objects per target in the first frame. * Unless otherwise specified, SA-V uses the combination of manual and auto annotations. Dataset Year Videos Categories Objects Annotations Duration Frames Disapp. Rate Reapp. Rate Distractors VOT GOT-10k [29] LaSOT [28] VOT [137] DiDi [26] SegTrack-v2 [156] YouTube-Objects [157] VOS FBMS [158] JumpCut [13] DAVIS16 [2] DAVIS17 [3] YouTube-VOS [4] VOTS [159] VOST [160] LVOSv1 [61] LVOSv2 [62] SA-V* [15] MOSEv1 [1] MOSEv2 (ours) 2019 2019 2025 2013 2014 2014 2015 2017 2018 2023 2023 2023 2024 2023 2025 9,695 1,500 180 14 126 59 22 90 4,453 144 713 220 50,900 2,149 5,024 563 85 - - 11 10 16 14 - - 94 - 154 27 - 36 200 10,200 2,148 180 24 124 139 22 205 7,755 341 1,726 282 1, 642,600 5,200 10,074 1.5M 3.9M 19, 268,084 1,475 2,092 1,465 6,331 3, 13,543 197,272 - 173,758 156,432 407, 35.5M 431,725 701,976 40.0 hr 35.8 hr 1.5M 3.9M 11.10 19,903 152.71 274,882 0. 9.01 7.70 3.52 2.88 5.17 334. 166.00 251.92 351.00 823.00 196.0 hr 443. 1,570.63 947 2,127 13,860 5,315 3, 6,208 120,532 298,640 75,547 126,280 296, 4.2M 130,149 468,251 2.1% 17.1% 19.4% 40.0% 8.3% 6.5% 11.2% 0.0% 11.1% 16.1% 13.0% - 46.5% 50.0% 36.1% 58.7% 41.5% 61.8% 2.1% 16.9% 17.7% 40.0% 0.0% 1.6% - 0.0% 4.9% 10.7% 8.0% - 44.4% 46.7% 32.5% 27.7% 23.9% 50.3% 3.1 3.4 5. 10.6 5.4 - - - 2. 3.7 3.0 - 5.3 3.7 4. 6.2 6.5 13.6 Disappearance-Reappearance. MOSEv2 significantly surpasses its predecessor MOSEv1 regarding object disappearance and reappearance phenomena. The Disapp. Rate raise from 41.5% to 61.8%, while the Reapp. Rate more than doubles from 23.9% to 50.3%. This makes MOSEv2 the most challenging benchmark for evaluating disappearance-reappearance scenarios, surpassing all existing datasets. For example, its Disapp. Rate exceeds those of VOS datasets such as SA-V (58.7%), as well as VOT datasets like DiDi (40.0%). Similarly, its 50.3% Reapp. Rate outperforms LVOSv1 (46.7%) and SA-V (27.7%). Crowding. To assess crowding complexity, we compute the Distractors metric, which quantifies the average number of visually similar objects per target in the first frame using the state-of-theart object counting method T-Rex2 [161]. MOSEv2 significantly increases the distractor count to 13.6 per object, more than twice the 6.5 distractors in MOSEv1. In addition, MOSEv2 outperforms other benchmarks such as SA-V (6.2) and LVOSv2 (4.6). Notably, it even surpasses DiDi [26] (10.6), which is specifically designed to emphasize distractors in video object tracking, establishing MOSEv2 as the most challenging benchmark for evaluating segmentation robustness in densely crowded scenes. Occlusion. We compare the occlusion levels of MOSEv2 against other datasets in TABLE 2. While MOSEv2 achieves the highest mBOR score [111] of 28.3, this metric offers only coarse estimation of occlusion [1]. As shown in Fig. 3 (a), an object may be heavily occluded while still yielding BOR close to zero. To address this limitation, we introduce two complementary metrics: Amodal-mask Occlusion Rate (AOR) and MLLM-assisted Occlusion Rate (MLLMOR), shown in Fig. 3 (b) and (c), respectively. AOR measures the ratio between visible and amodal mask areas, where amodal masks are generated by the amodal segmentation model DiffVAS [162]. MLLMOR leverages multimodal large language model (we use QWenVL-2.5-32B [163]) to assess ocTABLE 2: Comparison of the occlusion rate of different datasets."
        },
        {
            "title": "Mean",
            "content": "mBOR mAOR mMLLMOR DAVIS17 [3] YouTube-VOS [4] LVOSv2 [62] SA-V [15] MOSEv1 [1] MOSEv2 (ours) 20.6 23.2 25.4 36.1 36.4 47.0 3.4 5.7 8.4 27.4 23.7 28. 23.7 26.0 30.6 37.2 41.2 54.8 34.6 38.0 37.2 43.6 44.2 57.8 Fig. 3: Occlusion evaluation protocol. (a) BOR: Bounding-box Occlusion Rate [111], (b) AOR: Amodal-mask Occlusion Rate, (c) MLLMOR: MLLM-assisted Occlusion Rate. clusion severity. We compute the final occlusion estimate as the average of all three metrics. As shown in TABLE 2, MOSEv2 achieves mean occlusion rate of 47.0 (mBOR: 28.3, mAOR: 54.8, mMLLMOR: 57.8), substantially exceeding MOSEv1 (36.4) and SA-V (36.1), establishing MOSEv2 as the most challenging benchmark for occlusion robustness evaluation. Mask Size. Fig. 4 compares the distribution of mask sizes (normalized by video resolution) across datasets. MOSEv2 contains substantially higher proportion of small masks (size < 0.01), reaching 50.2%, significantly above DAVIS (25.3%), YouTubeVOS (18.4%), LVOSv2 (34.8%), SA-V (40.7%), and MOSEv1 (39.5%). This high prevalence of small objects poses greater challenges for fine-grained perception and accurate segmentation. MOSE DATASET PAGE: MOSE.VIDEO 7 Fig. 4: Mask size distribution, normalized by video resolution. Fig. 5: Video length distributions. Compared to MOSEv1, MOSEv2 includes more long videos, with the longest reaching 7,825 frames. Fig. 6: Challenging environment distribution. Fig. 7: (Left) Distribution of instance sequences attributes. (Right) Attribute correlations in MOSEv2. Video Length. Fig. 5 presents the video length distribution in MOSEv2. Compared to only 11 videos exceeding 300 frames (around 1 minute) in MOSEv1, MOSEv2 introduces 183 such long videos, with the longest reaching 7,825 frames (around 26 minutes). The average video length increases from 60.6 to 93.2 frames, enabling more comprehensive evaluations of long-term temporal consistency and tracking robustness. While LVOSv2 [62] includes 362 videos over 300 frames with an average length of 590.9 frames, our 183 long videos average 598.4 frames and extend up to 7,825 frames, far beyond LVOSv2s maximum of 2,280. Importantly, video length alone does not imply difficulty. In MOSEv2, long videos are not included merely for their duration, but intentionally designed to include richer dynamics and more complex scenarios, such as object disappearance, occlusion, scene transitions, and multi-shot clips. For example, LVOSv2s reappearance rate is only 32.5%, substantially lower than our 50.3%, highlighting the increased complexity in MOSEv2. Complex Environments. Fig. 6 shows the distribution of challenging environmental conditions in MOSEv2. Compared to MOSEv1, we significantly expand the coverage of adverse scenarios. For example, rainy videos increased from 20 to 159, and underwater scenes from 29 to 280. MOSEv2 also introduces new conditions not present in MOSEv1, including 142 heavy rain, 73 snow, 60 fog, and 50 disaster scenarios (e.g., earthquake, flood). In total, MOSEv2 provides 443 cloudy, 159 rainy, 142 heavy rain, 73 snowy, 60 foggy, 280 underwater, 255 nighttime (vs. 75 in MOSEv1), and 50 disaster videos. This substantial expansion establishes MOSEv2 as more comprehensive benchmark for evaluating model robustness under diverse complex environments. Attribute Analysis. Following DAVIS [3], we define 15 instance sequence attributes of MOSEv2 in TABLE 3. As shown in Fig. 7 (left), MOSEv2 substantially expands the coverage of challenging scenarios compared to MOSEv1. For example, videos with occlusion (OCC) increase from 2,100 to 4,931, disappearancereappearance (DR) from 1,243 to 5,076, complex environments (CE) from 330 to 1,462, and long duration (LD) from 23 to 224. In addition, MOSEv2 introduces new attributes such as novel categories (NC, 609 instances), multi-shot sequences (MS, 277), and knowledge dependency (KD, 256). As visualized in Fig. 7 (right), we provide chord diagram showing the coTABLE 3: Definitions of sequences attributes in MOSEv2. We follow part attributes defined in DAVIS [3] (top) and expand with complementary set of complex video attributes (bottom). Attr. Definition BC Background Clutter. The background and the target object exhibit similar visual appearances. FM Fast Motion. The average, per-frame object motion, computed as centroids Euclidean distance, is larger than ðœð‘“ ð‘š = 20 pixels. OCC Occlusion. The target is partially or fully occluded in video. OV SV Out-of-view. The target leaves the video frame completely. Scale Variation. The ratio of any pair of bounding-box is outside of range [0.5,2.0]. Shape Complexity. The target exhibits complex boundary structures. Appearance Change. Significant appearance change, due to rotations and illumination changes. Disappearance-Reappearance. Target object reappears after disappearing in the video. Diverse Visibility. Target object is small, inconspicuous, or camouflaged in the scene."
        },
        {
            "title": "SC\nAC",
            "content": "DR DV NC CRO Crowding. Multiple similar objects appear in close proximity. CE Complex Environment. Videos with challenging conditions such as underwater, adverse weather (rain, snow), and nighttime. Novel Categories. Novel object categories, especially camouflaged targets and non-physical objects. MS Multi-Shots. Videos containing multiple camera shots. LD KD Knowledge Dependency. Videos requiring specific knowledge like Long Duration. Video duration exceeds 1 minute (300 frames). OCR or spatial reasoning capability to segment the target object. occurrence patterns between attributes, offering insights into the interplay of real-world challenges. This comprehensive attribute set makes MOSEv2 more rigorous benchmark for evaluating model robustness under diverse and complex conditions."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "Following previous VOS works [2], [3], we compute the region similarity and the contour accuracy as evaluation metrics. ð‘€ {0, 1}ð»ð‘Š and ground-truth Given predicted mask mask ð‘€ {0, 1}ð»ð‘Š , region similarity is computed as the ð‘€ and ð‘€, Intersection-over-Union of ð‘€ ð‘€ ð‘€ ð‘€ = (1) . ð‘€, contour recall Rð‘ and To measure the contour quality of precision Pð‘ are calculated via bipartite graph matching [164]. Then, the contour accuracy is the harmonic mean of the contour recall Rð‘ and precision Pð‘, i.e., = 2Pð‘Rð‘ Pð‘ + Rð‘ , (2) MOSE DATASET PAGE: MOSE.VIDEO 8 Fig. 8: Comparison of and on Small and Large objects. For small objects (e.g., chopstick with only 955 pixels), yields exaggerated scores due to fixed resolution-based thresholds, while offers more reliable measure by accounting for object scale. which represents how closely the contours of predicted masks resemble the contours of ground-truth masks. Then, the average region similarity ð‘šð‘’ð‘Žð‘› and contour accuracy ð‘šð‘’ð‘Žð‘› over all objects are calculated as the final results. For brevity, we use and to ð‘šð‘’ð‘Žð‘› and represent ð‘šð‘’ð‘Žð‘›, respectively. The overall performance is measured by & = ( + )2. Revisiting the Score. We observe that the standard score has limitations when evaluating small objects, which is particularly problematic for the proposed MOSEv2 dataset containing large number of small targets. Previous works [1], [2] adopt fixed boundary threshold ð‘¤ = 0.008 ð· for images of the same resolution, regardless of object size, where ð· is the length of image diagonal. To address this limitation, we propose an adaptive boundary threshold: ð‘¤ = min(0.008 ð·, ð›¼ ð´), (3) where ð´ is the objects area in pixels and ð›¼ is scaling factor. Based on analysis of boundary ratios in the DAVIS dataset, we set ð›¼ = 0.1 to maintain comparable boundary widths for averagesized objects while providing appropriate thresholds for smaller ones. We denote this modified, adaptive-threshold-based metric as , which offers fairer assessment of boundary accuracy across objects of varying sizes. As illustrated in Fig. 8, we analyze the boundary evaluation for both small and large objects in the same image. The predicted mask and ground truth for small object (e.g., chopstick, only 955 pixels) do not overlap. However, the widely used fixed threshold leads to excessive boundary dilation, resulting in an inflated score of 0.91, while the improved correctly assigns score of 0. For large object (e.g., person with maintains consistency with the previous evaluation. 38k pixels), Disappearance and Reappearance Metrics. Given the very frequent occurrence of object disappearance and reappearance in MOSEv2, we compute dedicated region similarity and contour accuracy scores: ð‘‘ for disappearance clips where the target object is absent, and ð‘Ÿ for reappearance clips where the target ð‘Ÿ, object reappears. We report the averaged scores & ð‘‘ and & ð‘Ÿ to represent overall performance in these respective scenarios. As shown in Fig. 9, we first compute these metrics per disappearance or reappearance clip, average them to obtain sequence-level scores. These metrics are designed to address limitations of standard and scores, which are computed over all frames in video and can be biased by the proportion of empty-target frames. For example, if video contains many disappearance frames, models that always predict empty masks may appear to perform well, even ð‘‘, Fig. 9: Illustration of & , & ð‘Ÿ computation. The slashed boxes denote frames with empty masks. Predicted masks are assumed to perfectly match the ground truth. ð‘‘, and & if they fail to recover the target later. Conversely, in videos with few disappearance frames, models that consistently predict masks can achieve higher scores, as the penalty for incorrect predictions on empty-target frames becomes negligible. By isolating evaluation to the relevant clips, & ð‘‘ and & ð‘Ÿ offer clearer insights. Models that fail to suppress predictions during disappearances will score poorly on & ð‘‘, while those unable to re-identify the object after its return will be penalized under & ð‘Ÿ. Only models that correctly handle both disappearance and reappearance can perform well on both metrics. Notably, for & ð‘Ÿ, we exclude the initial continuous presence of the target, where the reference information is strongest, and focus only on reappearance after disappearance, which better reflects the models ability to recover the target under ambiguity."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct comprehensive experiments and benchmarks on the newly built MOSEv2 dataset across multiple video object segmentation and tracking tasks. We evaluate five VOS settings, including semi-supervised VOS with mask-, box-, and point-initialization, as well as unsupervised VOS and interactive VOS. In addition, we benchmark video object tracking methods on MOSEv2 to demonstrate its broad applicability. Implementation Details. The proposed MOSEv2 follows the same data format as MOSEv1 [1] and YouTube-VOS [4]. To ensure fair comparisons, we replace the YouTube-VOS [4] training set with MOSEv2 while strictly adhering to the original training configurations used for YouTube-VOS. Methods are trained with image-pretrained backbones and do not use any additional video datasets. For SAM2-based models, we adopt SAM2.1 and finetune exclusively on MOSEv2. We evaluate model performance using standard metrics ( , , and & ) on MOSEv2 validation set, following the DAVIS protocol [2], [3]. To better capture the , & , complex challenges in MOSEv2, we additionally report & ð‘Ÿ as described in Section 3.3. Among them, & is selected as the primary evaluation metric. Dataset Splits. These videos are split into 3,666 training, 433 validation, and 614 testing videos, for model training, daily evaluation, and competition period evaluation1, respectively. An additional 311 videos, originally used as the validation set in MOSEv1, are temporarily retained for compatibility and may later serve as local validation set when MOSEv2 becomes the standard. ð‘‘, and & 1. The testing set is used for evaluation during the competition periods, such as https://pvuw.github.io/ and https://lsvos.github.io/. MOSE DATASET PAGE: MOSE.VIDEO 9 TABLE 4: Benchmark results of mask-initialization semi-supervised VOS methods on MOSEv2 validation set. To ensure fairness, BL30K [63] is not used in training. Inference speed (FPS) and GPU memory usage (GiB) are measured on single A6000 GPU. For SAM2 and its variants, video frames are offloaded to CPU memory to balance memory consumption and speed. Method Pub. FPS Mem. & MOSEv MOSEv1 & ð‘‘ & ð‘Ÿ & & SA-Vð‘¡ð‘’ð‘ ð‘¡ LVOSv2 DAVIS17 YT-VOS19 & & & AOT-L [165] STCN [166] RDE [167] XMem [16] DeAOT-L [168] DEVA [101] XMem++ [55] Cutie-B [14] JointFormer [169] SAM2-B+ (ZS) [15] SAM2-B+ [15] SAMURAI-B+ [25] DAM4SAM-B+ [26] SAM2Long-B+ [27] SAM2-L (ZS) [15] SAM2-L [15] SAMURAI-L [25] DAM4SAM-L [26] SAM2Long-L [27] [NeurIPS21] 19.7 [NeurIPS21] 45.1 32.7 [CVPR24] 49.8 [ECCV22] [NeurIPS22] 21.2 43.0 [ICCV23] 30.1 [ICCV23] 44.1 [CVPR24] 7.2 [PAMI25] 23.4 [ICLR25] 23.4 [ICLR25] [Preprint24] 17.7 17.3 [CVPR25] 9.4 [ICCV25] 14.4 [ICLR25] 14.4 [ICLR25] [Preprint24] 12.1 12.3 [CVPR25] 7.1 [ICCV25] 3.8 6.2 1.4 1.6 3.7 1.0 1.4 0.9 3.6 2.7 2.7 2.7 2.7 6.0 3.6 3.6 3.5 3.5 6.8 30.2 29.0 31.4 29.7 28.9 30.5 32.0 30.7 33.3 36.3 34.7 37.9 32.6 30.7 34.5 38.3 36.6 40.0 34.2 32.5 35.9 42.8 41.1 44.4 37.7 36.0 39.4 42.4 40.7 44.2 46.0 44.2 47.8 47.4 45.3 49.5 47.9 45.8 50.0 48.6 46.7 50.5 49.5 47.7 51.3 49.7 47.9 51.5 51.1 49.0 53.2 51.2 49.2 53.2 51.5 49.6 53.4 67.8 79.4 62.7 56.6 33.5 55.1 51.6 64.5 57.3 58.5 61.6 45.9 51.3 58.4 62.9 64.5 52.4 57.2 62.5 7.8 8.1 12.6 14.8 18.3 18.5 15.5 18.3 18.3 20.0 23.2 33.6 32.0 29.2 27.3 27.1 34.9 34.2 30.6 32.9 31.0 31.4 30.2 35.0 32.8 40.0 37.4 37.2 33.9 42.2 39.4 37.9 35.2 46.8 43.9 41.1 38.6 46.3 43.5 50.0 47.1 52.2 48.8 52.6 49.2 52.8 49.7 53.6 50.7 53.8 50.9 55.8 52.4 55.6 52.4 55.8 52. 57.2 53.1 61.3 50.8 46.6 55.0 48.8 44.6 52.9 57.6 53.3 62.0 59.4 55.1 63.8 60.0 55.8 64.3 56.0 51.5 60.6 69.9 65.9 74.1 70.2 66.3 74.0 73.6 69.5 77.6 74.7 70.6 78.8 73.3 69.0 77.5 73.8 69.5 78.0 74.7 70.6 78.8 74.5 70.5 78.4 76.4 72.3 80.5 75.6 71.4 79.8 75.6 71.5 79.8 77.1 73.0 81.2 50.3 62.5 53.9 62.3 61.8 56.2 - 60.7 - 77.0 - - - 80.8 78.4 - - - 81.2 63.9 60.6 62.2 64.5 63.9 - - - - 83.1 - - - 85.2 84.0 - - - 85.3 84.9 85.4 84.2 86.2 85.2 87.0 - 87.9 90.6 90.2 - - - - 90.7 - - - 88.8 84.1 82.7 81.9 85.6 86.0 85.4 - 87.0 87.5 88.6 - - - - 89.3 - - - 90."
        },
        {
            "title": "4.1 Semi-supervised Video Object Segmentation",
            "content": "Semi-supervised (or semi-automatic, one-shot) VOS offers the targets mask, bounding box, or points on the first frame as reference for segmenting the entire video. Mask-initialization. This is the most common and actively studied setting in VOS. As shown in TABLE 4, we benchmark two groups of mask-initialization semi-supervised VOS methods on our MOSEv2 dataset. The first group consists of traditional VOS methods, typically using ResNet-50 as their backbone. The second group comprises SAM2-based variants, including models of both SAM2-B+ and SAM2-L scales. Existing methods exhibit substantially lower performance on MOSEv2 compared to previous benchmarks such as DAVIS17 and YouTube-VOS19. For example, SAM2-B+ [15] achieves only 47.1% & on MOSEv2, which is notably lower than its performance of 74.7% & on MOSEv1, 83.1% & on LVOSv2, and 90.2% & on DAVIS17. Among traditional methods, Cutie [14] achieves the best performance with 42.8% & , only 3.2% behind SAM2-B+, benefiting from its effective instance-level modeling. However, its strong overall performance mainly comes from high & ð‘‘ scores (64.5%) in handling disappearance, while struggling with reappearance as evidenced by much lower & ð‘Ÿ score (18.3%), lagging SAM2-B+ by 4.9 points. Taking close look at the detailed metrics, we observe that all methods face significant challenges in reappearance scenarios, as indicated by the low & ð‘Ÿ scores (ranging from 7.8% to 34.9%). This underscores the difficulty of re-identifying objects after they disappear and reappear in complex scenes. The adaptive boundary metric consistently shows lower scores than across all methods, demonstrating that the proposed adaptive boundary threshold provides more rigorous evaluation criterion that better reflects the quality of boundary predictions for objects of varying sizes. SAM2-based methods [15] demonstrate superior performance, even the zero-shot versions of SAM2 models outperform most finetuned traditional methods, demonstrating the effectiveness of foundation models on challenging video segmentation tasks. Moreover, all SAM2-based variants demonstrate enhanced performance on MOSEv2 compared to the base SAM2 model. These methods are specifically designed to address complex scenarios: SAMURAI [25] incorporates Kalman filtering for motion modeling to handle occlusions; DAM4SAM [26] introduces robust memory mechanisms to mitigate the influence of distractors in crowded scenes, and SAM2Long [27] employs memory tree to mitigate error accumulation in long videos with disappearance and reappearance. While these designs are tailored for specific challenges, such as occlusions and long-term tracking, these issues become significantly more severe in MOSEv2, and the existing methods still fall short in addressing them effectively. Furthermore, MOSEv2 presents new challenges, such as adverse environments, multi-shot transitions, and knowledge-dependent scenarios, which are not explicitly considered by current designs, leading to further performance degradation. For example, SAM2Long-L, which achieves the best overall performance among all methods, only reaches 51.5% & , indicating substantial room for improvement in addressing complex real-world scenarios. Notably, most SAM2 variants show decreased & ð‘‘ but improved & ð‘Ÿ scores, reflecting tendency towards more aggressive re-identification. Among these methods, SAMURAI achieves the highest & ð‘Ÿ score but sacrifices the most in terms of & ð‘‘, while SAM2Long strikes better balance and delivers the highest overall performance. The frequent disappearancereappearance patterns and diverse complex scenarios in MOSEv2 impose dual demands on both recall and precision. Future models must not only accurately suppress predictions when targets are absent, but also reliably re-identify them upon reappearance. Achieving strong performance thus requires effectively balancing these competing objectives, which remains crucial challenge. In terms of computational efficiency, there exists clear tradeoff between accuracy and speed. Traditional methods such as XMem [16] and STCN [166] offer faster inference speeds (49.8 and 45.1 FPS, respectively) but lower performance. In contrast, SAM2-based methods yield higher-quality results but at the cost of slower inference (e.g., 7.1 FPS for SAM2Long-L) and increased memory usage (6.8 GiB compared to 0.9 GiB for Cutie-B [14]). MOSE DATASET PAGE: MOSE.VIDEO TABLE 5: Benchmarking box-initialization semi-supervised VOS methods on MOSEv2 validation set. TABLE 7: Benchmark results of unsupervised video object segmentation methods on the validation set of MOSEv2. Method Pub. & & MOSEv2 & ð‘‘ ð‘Ÿ MOSEv1 DAVIS17 & & & Method Pub. & & MOSEv2 & ð‘‘ ð‘Ÿ MOSEv1 DAVIS & & & [ICLR25] [CVPR24] [CVPR24] 16.4 UniVS [170] 42.3 Cutie+SAM SAM2-B+ 46.0 SAMURAI-B+ [Preprint24] 46.5 46.2 DAM4SAM-B+ [CVPR25] SAM2Long-B+ [ICCV25] 47.7 SAM2-L 49.0 [ICLR25] [Preprint24] 49.2 SAMURAI-L DAM4SAM-L 47.5 [CVPR25] 50.2 SAM2Long-L [ICCV25] 22.3 64.4 61.9 45.7 49.9 57.4 61.9 49.9 51.5 60.6 8.6 18.0 22.1 32.7 31.3 28.3 26.2 33.8 32.2 29.8 17.3 43.5 47.2 48.0 47.6 49.0 50.3 50.7 48.8 51.5 38.0 63.0 73.7 71.8 70.1 72.9 75.4 74.9 73.3 75.9 61.8 82.3 85.3 86.1 86.6 85.5 89.0 88.9 86.6 88. [ICLR25] [ICCV23] DEVA [101] 34.9 EntitySAM [102] [CVPR25] 28.2 SAM2-B+ 28.3 SAMURAI-B+ [Preprint24] 27.5 DAM4SAM-B+ [CVPR25] 25.9 28.9 SAM2Long-B+ [ICCV25] SAM2-L 28.2 [ICLR25] SAMURAI-L [Preprint24] 29.2 [CVPR25] 28.7 DAM4SAM-L 29.1 SAM2Long-L [ICCV25] 80.4 96.7 77.3 52.5 52.5 61.0 73.5 53.9 52.6 52.6 7.5 4.1 6.3 12.0 6.9 7.5 6.3 12.6 10.6 8.8 36.0 28.4 28.8 28.4 26.7 29.7 28.6 30.1 29.6 29. 57.0 42.2 47.2 46.9 46.4 47.6 48.1 46.5 47.8 48.3 73.4 72.6 57.3 57.4 57.7 57.4 57.9 57.7 58.0 58.0 TABLE 6: Benchmarking point-initialization semi-supervised VOS methods on MOSEv2 validation set. We use & as the evaluation metric. n-clk: using positive clicks for initialization. TABLE 8: Benchmark results of interactive VOS methods on the validation set of MOSEv2. & @60s measures how well the model performs given 60 seconds of interactive processing time."
        },
        {
            "title": "Method",
            "content": "Pub. MOSEv2 MOSEv1 1-clk 3-clk 5-clk 1-clk 3-clk 5-clk DAVIS17 5-clk Method Pub. MOSEv2 MOSEv1 DAVIS17 AUC & & @60ð‘  & @60ð‘  & @60ð‘  Cutie+SAM [CVPR24] 35.2 38.2 36.7 54.2 58.5 58.3 [ICLR25] 43.6 44.1 44.4 66.8 66.8 70.6 SAM2-B+ SAMURAI-B+ [Preprint] 44.7 45.8 45.9 65.7 65.7 68.6 DAM4SAM-B+ [CVPR25] 43.8 45.6 45.8 66.3 66.3 69.3 SAM2Long-B+ [ICCV25] 45.3 45.3 45.1 66.4 66.4 70.3 [ICLR25] 47.6 48.0 47.2 69.6 69.6 74.8 SAM2-L SAMURAI-L [Preprint] 47.9 48.2 48.6 69.3 69.3 74.1 DAM4SAM-L [CVPR25] 47.7 47.7 48.2 69.4 69.4 74.4 [ICCV25] 48.5 48.3 48.7 69.7 69.7 75.2 SAM2Long-L 62.7 80.4 78.9 80.3 80.5 86.0 84.8 85.5 86.1 Box-initialization. We benchmark box-initialization semisupervised VOS methods on MOSEv2 validation set in TABLE 5. Similar to the mask-initialization setting, we evaluate both traditional (UniVS [170] and Cutie+SAM) and SAM2-based methods. The results show that SAM2-based methods significantly outperform traditional ones, with SAM2Long-L [27] achieving the best performance of 51.5% & . However, all methods struggle with reappearance scenarios evaluated by & ð‘Ÿ, while showing relatively better performance on disappearance cases, evaluated by & ð‘‘. The performance gap between MOSEv2 and other benchmarks like DAVIS17 underscores the increase difficulty of the diverse and complex scenarios in MOSEv2. Point-initialization. As shown in TABLE 6, we benchmark point-initialization semi-supervised VOS methods on MOSEv2, including traditional methods (Cutie+SAM) and SAM2-based variants. The results show that SAM2-based methods significantly outperform traditional ones, with SAM2Long-L achieving the best performance of 48.5% & using only single click. Increasing the number of clicks from 1 to 5 does not consistently improve performance. Some methods even degrade with more clicks. This sensitivity to point initialization suggests that the ambiguity introduced by point prompts, combined with the complex scenes in MOSEv2, makes it challenging for models to maintain consistent segmentation even with additional user input. Compared to DAVIS17, where methods achieve much higher scores, e.g., 86.1% & for SAM2Long-L, the performance gap highlights the challenges posed by MOSEv2 in point-based settings. MANet [65] CiVOS [171] MiVOS [63] STCN [166] [CVPR20] [CVPR21] [CVPR21] [NeurIPS21] 28.9 32.7 36.7 39.8 41.2 46.1 48.9 54.1 46.0 51.7 53.9 59.5 79.5 84.0 88.5 88.8 any manual guidance. Following DAVIS [2], [3], we limit the number of proposals to 20 for fair comparison. In TABLE 7, we benchmark unsupervised VOS methods on MOSEv2. The results show that all methods perform poorly on MOSEv2, especially in reappearance cases, where & ð‘Ÿ scores drop to as low as 4.1%12.6%. Although DEVA [101] achieves the highest & of 36.0%, this remains far below its 73.4% performance on DAVIS17. For SAM2-based methods, we use grid prompts on the first frame to generate candidate masks, which are then propagated to subsequent frames. However, incomplete initial masks limit their effectiveness, with SAM2Long-L reaching only 29.1% & . The substantial performance gap between MOSEv2 and other benchmarks highlights the challenging nature of our dataset for unsupervised VOS methods, which must handle complex scenes without any manual guidance."
        },
        {
            "title": "4.3 Interactive Video Object Segmentation",
            "content": "Following the interactive track of the DAVIS 2019 Challenge on VOS [172], we provide initial scribbles for the target object in given video as the first interaction. Interactive video object segmentation methods must predict the full-video segmentation based on this input. After comparing predictions with ground truth, corrective scribbles on the worst-performing frame are provided for refinement. This process can be repeated up to 8 times, with 30-second time limit per object. We report & @60s to reflect the trade-off between accuracy and speed. As shown in TABLE 8, we evaluate four recent interactive VOS methods on the validation set of MOSEv2. All methods show substantial performance drops compared to DAVIS17. STCN [166] achieves the best performance of 54.1% & @60s, which is far below its 88.8% on DAVIS17. This significant performance gap highlights the increased difficulty of the complex scenarios in MOSEv2."
        },
        {
            "title": "4.4 Video Object Tracking",
            "content": "Unsupervised (or automatic, zero-shot) VOS aims to automatically identify and segment primary objects in videos without Video object tracking (VOT) aims to track target object throughout video given an initial bounding box. Unlike VOS, VOT MOSE DATASET PAGE: MOSE.VIDEO 11 TABLE 9: Attribute-based performance analysis on MOSEv2 validation set, with attribute definitions detailed in TABLE 3. The overall metric represents the average value across all attributes. The best score in each metric is highlighted in bold. Method XMem Cutie Overall OCC DR CRO DV CE NC LD MS KD & & ð‘‘ & ð‘Ÿ & & ð‘‘ & ð‘Ÿ & & & ð‘Ÿ ð‘‘ & & & ð‘Ÿ & & ð‘‘ & ð‘Ÿ ð‘‘ & & & ð‘Ÿ & & & ð‘Ÿ & & ð‘‘ & ð‘Ÿ & & & ð‘Ÿ ð‘‘ & & & ð‘Ÿ ð‘‘ ð‘‘ ð‘‘ 31.7 55.5 12.6 36.8 56.9 14.9 30.8 57.8 13.6 30.8 52.9 9.2 24.3 54.5 8.3 34.0 52.2 12.7 34.6 50.4 16.2 30.7 77.1 10.7 33.2 57.2 16.2 30.2 40.3 11.3 35.8 61.9 15.7 43.4 64.7 18.4 35.7 59.8 17.3 36.8 60.1 14.2 26.8 67. 9.6 42.0 68.7 15.2 39.9 55.1 20.7 35.4 81.8 13.5 31.8 51.2 19.6 30.5 48.3 12.7 SAM2 (ZS) 36.8 56.0 17.0 43.5 59.7 20.8 38.6 53.4 23.5 36.6 52.5 14.3 28.5 49.9 10.5 49.2 67.3 24.4 37.3 52.1 17.5 40.5 65.9 18.5 30.1 45.9 14.7 26.9 57.5 SAM2 40.7 57.0 21.4 47.1 61.6 23.7 41.5 53.8 26.4 42.5 56.2 21.5 35.1 48.8 18.5 52.6 66.9 28.5 43.1 55.2 22.4 42.5 72.4 22.7 34.0 46.9 18.5 27.8 51.3 9.0 9. SAMURAI 42.6 40.7 30.1 48.9 46.6 33.9 44.2 38.7 33.4 41.0 40.6 26.1 37.5 32.5 29.6 55.6 52.7 39.8 43.6 39.1 32.5 51.6 50.8 39.8 32.7 28.4 20.1 28.2 37.1 15.4 DAM4SAM 42.4 46.4 27.9 48.7 52.1 32.2 44.5 47.2 31.5 40.9 45.5 24.7 39.4 43.2 29.7 52.9 56.6 32.8 43.8 44.5 30.6 51.1 56.9 35.5 34.1 31.5 19.3 25.9 40.4 14.5 SAM2Long 42.9 52.8 26.2 49.4 59.7 29.9 42.9 50.1 28.9 44.6 53.0 24.7 37.5 50.1 23.2 56.7 65.4 35.5 43.9 53.4 28.2 52.9 65.9 35.4 32.0 33.4 20.3 25.7 44.1 9.9 TABLE 10: Benchmark results of video object tracking methods on MOSEv2 validation set. AUC is the area under the success plot curve. and Pnorm represent precision metrics measuring center location error, with Pnorm normalized by target size. AO is the average overlap ratio. MOSEv2 MOSEv1 LaSOT GOT-10k"
        },
        {
            "title": "Method",
            "content": "Pub. SeqTrack-B [20] [CVPR23] 21.3 24.8 23.7 AQATrack-B [21] [CVPR24] 22.6 25.6 24.6 [AAAI24] 21.3 23.8 23.5 ODTrack-B [22] [ECCV24] 20.8 24.1 23.3 LORAT-B [23] [AAAI25] 24.3 26.4 26.0 SUTrack-B [24] [ICLR25] 29.2 30.0 29.1 SAM2-B+ SAMURAI-B+ [Preprint24] 35.2 35.5 34.3 DAM4SAM-B+ [CVPR25] 35.0 35.4 33.9 [ICCV25] 32.0 32.6 31.4 SAM2Long-B+ [CVPR23] 23.5 26.3 25.3 SeqTrack-L [20] [AAAI24] 24.4 26.7 25.9 ODTrack-L [22] [ECCV24] 23.6 26.7 25.5 LORAT-L [23] [AAAI25] 26.9 28.4 27.8 SUTrack-L [24] [ICLR25] 33.1 33.6 32.1 SAM2-L [Preprint24] 37.4 37.8 36.1 SAMURAI-L [CVPR25] 36.8 37.3 35.6 DAM4SAM-L [ICCV25] 34.2 34.8 33.1 SAM2Long-L Pnorm AUC AUC 42.9 44.7 47.2 43.8 46.9 58.3 59.5 59.5 58.3 45.7 49.1 46.0 48.6 59.6 60.9 60.9 60."
        },
        {
            "title": "AUC",
            "content": "71.5 72.7 73.2 71.7 74.4 66.0 70.7 - - 72.5 74.0 75.1 75.2 70.0 74.2 75.1 73.9 AO 74.5 76.0 77.0 72.1 79.3 - 79.6 - - 74.8 78.2 77.5 81.5 80.7 81.7 - 81.1 emphasizes object localization rather than segmentation. To adapt MOSEv2 for VOT evaluation, we convert segmentation masks to bounding boxes by using the minimal enclosing rectangle. As shown in TABLE 10, we benchmark 9 state-of-the-art VOT methods on MOSEv2, including both traditional trackers and SAM2-based variants. Following LaSOT [28], we adopt P, Pnorm, and AUC as evaluation metrics. The results show that all methods undergo significant performance drop on MOSEv2 compared to existing VOT benchmarks. Among traditional trackers with Large scale, SUTrack-L [24] achieves the best performance with 27.8% AUC, while LORAT-L [23] performs the worst with only 25.5% AUC on MOSEv2. Overall, all traditional methods show relatively low performance, with scores ranging from 23.3% to 27.8% AUC. SAM2-based methods generally outperform traditional ones, with SAMURAI-L [25] achieving the highest AUC of 36.1%, followed by DAM4SAM-L (35.6%) and SAM2Long-L (33.1%). However, these results still show substantial gap compared to their performance on other benchmarks. For example, SAMURAI-L achieves 60.9% AUC on MOSEv1 [1], 74.2% on LaSOT [28], and 81.7% on GOT-10k [29], but only 36.1% on MOSEv2. Although SAMURAI lags behind other SAM2 variants like SAM2Long in VOS tasks, it shows superior tracking performance in the VOT setting. This can be attributed to two key factors. First, VOT metrics do not penalize false positives when the ground truth is empty, which aligns Method TABLE 11: Comparison on long videos (>300 frames) in MOð‘‘ and & SEv2 and LVOSv2. Î”: the difference between & ð‘Ÿ. MOSEv2 (LD) & 22.7 +49.7 39.8 +11.0 35.5 +21.4 35.4 +30. & & 72.4 42.5 SAM2 50.8 SAMURAI 51.6 56.9 DAM4SAM 51.1 65.9 52.9 SAM2Long & 62.6 +3.8 -14.5 71.3 -5.7 71.4 -1.7 68.5 & & 69.4 82.3 56.8 81.5 65.7 81.4 66.8 84.3 LVOSv2 ð‘Ÿ Î” Î” ð‘‘ ð‘‘ ð‘Ÿ with SAMURAIs higher & ð‘Ÿ scores as shown in TABLE 4. Second, the integration of Kalman filtering effectively captures temporal motion, enhancing localization and trajectory prediction larger models (L in complex tracking scenarios. In addition, variants) consistently outperform their base counterparts (B+ variants) across all methods. This observation suggests that increased model capacity contributes to better handling of the diverse and challenging tracking conditions present in MOSEv2."
        },
        {
            "title": "4.5 Attribution Evaluation",
            "content": "To better understand how different methods perform under specific challenges, TABLE 9 presents an attribute-based analysis on the validation set of MOSEv2. We evaluate mask-initialization semisupervised VOS methods across nine representative attributes defined in TABLE 3, including occlusion (OCC), disappearancereappearance (DR), crowding (CRO), diverse visibility (DV), complex environment (CE), novel categories (NC), long duration (LD), multi-shots (MS), and knowledge dependency (KD). The results reveal several key insights about model performance across different challenges. (i) SAM2Long [27] achieves the best overall performance with 42.9% & , consistent with its strong results in previous experiments. This suggests that robustness to MOSEv2s challenging scenarios translates into better general effectiveness. (ii) Fine-tuning significantly improves SAM2s performance, raising its & from 36.8% to 40.7%, which highlights the importance of adaptation to complex video scenarios. (iii) Traditional methods like Cutie [14] and XMem [16] excel in frames where objects are disappearing ( & ð‘‘), with Cutie achieving the highest scores across most attributes (up to 81.8% for LD). However, they struggle significantly on reappearance scenarios ( & ð‘Ÿ), often failing to re-identify targets. For example, Cutie scores 81.8% & ð‘Ÿ on LD videos, indicating tendency toward false negatives when objects reappear. (iv) comparison with LVOSv2 [62], which specifically focuses on long videos, highlights that the longduration sequences in MOSEv2 involve not only extended frame counts but also greater scene complexity. As shown in TABLE 11, LVOSv2 exhibits small Î” values, i.e., the gap between & ð‘‘ and & ð‘Ÿ, indicating minimal difficulty in reappearance cases. ð‘‘ but only 13.5% & MOSE DATASET PAGE: MOSE.VIDEO 12 In contrast, the LD subset of MOSEv2 shows much larger Î” values (+11.0 to +49.7), indicating severe reappearance difficulty. These challenges arise from frequent occlusions, camera shot transitions, background clutter, ambiguous reappearance cases, etc. For example, SAM2Long achieves 84.3% & on LVOSv2 but only 52.9% on MOSEv2s LD subset, underscoring the substantially more challenging nature of our dataset. (v) In knowledgedependent (KD) scenarios, all methods demonstrate significantly degraded performance, with Cutie [14] achieving only 30.5% & , underscoring the complexity of KD challenges. Traditional methods such as Cutie and XMem outperform SAM2 variants in KD scenarios, likely because they incorporate instance-level memory mechanisms that offer stronger semantic representation. SAM2 [15], is not pretrained on such scenarios and lacks heuristic design for knowledge-intensive tasks. Among SAM2-based methods, SAMURAI [25] performs best in KD scenarios (15.4% & ð‘Ÿ), possibly due to its Kalman filter-based motion modeling, which introduces spatial reasoning capabilities helpful for handling knowledge-based challenges. in contrast,"
        },
        {
            "title": "4.6 Qualitative Analysis",
            "content": "Fig. 10 presents eight challenging cases that reveal key limitations of existing VOS methods. 1) Models struggle with re-identifying objects after disappearance and occlusion. While SAM2Long [27], which maintains multiple segmentation paths, successfully tracks car undergoing simple linear motion (case a), it fails in more complex motion patterns such as person walking around crowd before reappearing (case b), indicating limitations in modeling long-term and nonlinear trajectories. 2) Densely crowded scenes containing small and heavily occluded targets (case c) remain extremely challenging, none of existing models succeed under such complexity. 3) In cases involving camouflaged objects or non-physical targets like shadows (cases and e), Cutie [14] outperforms SAM2 [15] and SAM2Long [27], especially in boundary quality. This advantage may stem from Cuties compact instancelevel memory, which explicitly models foreground objects and enables better separation from background distractions, while SAM2 relies on global image features lacking instance-specific cues. 4) Under adverse environmental conditions such as heavy snow (case f), the combination of low contrast and occlusion causes all models to fail, with Cutie producing inaccurate masks and SAM2 variants completely losing the target. 5) When faced with dramatic changes in viewpoint and object pose across multiple camera shots (case g), all models fail to maintain consistent tracking, as exemplified by the shifting appearance of Coke bottle. 6) In scenarios that require understanding physical object relationships and transformation rules (case h), such as tracking rotating Rubiks cube, the models fail to re-identify the correct block after disappearance, often incorrectly assigning it to adjacent blocks. These observations suggest several key directions for future research, including enhancing instance-level feature representations, improving cross-view and long-term consistency, and incorporating physical and spatial reasoning to better handle complex object dynamics in real-world videos."
        },
        {
            "title": "5 DISCUSSION AND FUTURE DIRECTIONS\nBased on the comprehensive analysis of MOSEv2 and the per-\nformance of current state-of-the-art methods, we identify several\nkey challenges and future research directions for video object\nsegmentation in complex scenes.",
            "content": "Robust Re-identification for Disappearance-Reappearance. The significant drop in & ð‘Ÿ scores reveals key challenge in handling object disappearance-reappearance, especially when objects reappear with different viewpoints, deformations, or require specific knowledge for re-identification, e.g., Fig. 1â‘©. While improving re-identification is essential for boosting & ð‘Ÿ, overly aggressive matching can harm & ð‘‘ performance by producing false positives during disappearance periods. Future research should develop more adaptive re-identification strategies that integrate appearance cues, motion modeling, and high-level semantics to robustly handle complex disappearance-reappearance scenarios. Occlusion Handling. MOSEv2 presents frequent and complex occlusion scenarios, especially in crowded environments. Current methods often fail to maintain object representations under partial or full occlusions. Future work should explore occlusionaware architectures, including attention mechanisms sensitive to occluded regions, multi-scale feature fusion, and temporal models that preserve identity continuity through occlusion. Tracking in Crowded and Small-Target Scenarios. Small objects and crowded scenes frequently co-occur in MOSEv2, posing significant challenges for existing methods. Limited input resolutions (e.g., 480p in Cutie, 1024p in SAM2-B+) often result in the loss of fine-grained details, hindering accurate tracking of small targets. Future research should explore efficient strategies for high-resolution processing to better preserve spatial detail, as well as enhanced feature learning tailored to small objects. Promising directions include multi-scale architectures balancing detail and efficiency, attention mechanisms focused on small-object discrimination, and contrastive learning techniques to separate targets from visually similar distractors in crowded contexts. Generalization to Rare Categories. Although VOS methods are generally designed to be class-agnostic, achieving robust generalization to rare or unseen categories remains significant challenge. MOSEv2 includes 200 categories with pronounced long-tail distribution, featuring uncommon targets such as shadows and camouflaged objects. Despite their class-agnostic nature, current methods often fail to segment these rare categories effectively due to insufficient training data and inherent domain gaps. To address this, future research could investigate test-time adaptation techniques that exploit first-frame cues more effectively, or develop more robust instance-level representations that generalize better to uncommon and visually ambiguous objects. Environmental Robustness. MOSEv2 includes diverse adverse environments, such as rain, snow, fog, nighttime, and underwater conditions, which significantly degrade the performance of existing VOS methods. Under such conditions, object appearance becomes unreliable due to low visibility, while temporal consistency is often disrupted by illumination changes and environmental occlusions. Future research should explore adaptive enhancement techniques, such as weather-invariant feature extraction, illumination-robust representations, or synthetic data augmentation for adverse conditions. Incorporating auxiliary signals or learned priors from large-scale adverse-scene datasets, may further improve robustness in these real-world scenarios. Multi-Shot Video Handling. Existing methods heavily rely on appearance matching and position estimation under the assumption of temporal continuity. However, this assumption breaks down in multi-shot videos, where abrupt scene transitions can lead to dramatic changes in object appearance and position. Multi-shot structures are common in real-world content, especially in edited MOSE DATASET PAGE: MOSE.VIDEO 13 Fig. 10: Qualitative results on MOSEv2. We compare Cutie [14], SAM2 [15], and SAM2Long [27] on 8 challenging cases that assess model performance under various complex conditions. These include object disappearance and reappearance (a, b, e, g, h), small/inconspicuous objects (c), heavy occlusions (c, f), crowded scenes (c), adverse weather (f), low-light environments (a, d), multishot sequences (g), camouflaged targets (d), non-physical objects such as shadows (e), and knowledge-dependent scenarios (h). or narrative-driven videos. Future research should explore shotaware tracking strategies that can effectively handle discontinuities while maintaining object identity across discontinuous shots. Knowledge-Dependent Tracking. While existing methods have made notable progress in many VOS scenarios, they still struggle in cases requiring external knowledge such as OCR, spatial reasoning, or common sense understanding, as shown in TABLE 9. These challenges stem from the limited reasoning capabilities of MOSE DATASET PAGE: MOSE.VIDEO 14 current models, which primarily rely on appearance and positional cues. To address this limitation, future work may explore the integration of Multimodal Large Language Models [81], [82] to enhance semantic understanding and high-level reasoning. The key challenge lies in incorporating such models while maintaining computational efficiency and real-time performance. including adverse weather conditions,"
        },
        {
            "title": "REFERENCES",
            "content": "[1] [2] [3] [4] [5] [6] [7] [8] [9] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai, MOSE: new dataset for video object segmentation in complex scenes, in ICCV, 2023. 1, 2, 3, 5, 6, 8, 11 F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung, benchmark dataset and evaluation methodology for video object segmentation, in CVPR, 2016. 1, 3, 5, 6, 7, 8, 10 J. Pont-Tuset, F. Perazzi, S. Caelles, P. ArbelÃ¡ez, A. Sorkine-Hornung, and L. Van Gool, The 2017 davis challenge on video object segmentation, arXiv preprint arXiv:1704.00675, 2017. 1, 3, 5, 6, 7, 8, 10 N. Xu, L. Yang, Y. Fan, J. Yang, D. Yue, Y. Liang, B. Price, S. Cohen, and T. Huang, Youtube-vos: Sequence-to-sequence video object segmentation, in ECCV, 2018. 1, 3, 5, 6, 8 S. Caelles, K. Maninis, J. Pont-Tuset, L. Leal-TaixÃ©, D. Cremers, and L. V. Gool, One-shot video object segmentation, in CVPR, 2017. 1, 3 H. Park, J. Yoo, S. Jeong, G. Venkatesh, and N. Kwak, Learning dynamic network using reuse gate function in semi-supervised video object segmentation, in CVPR, 2021. 1 S. D. Jain, B. Xiong, and K. Grauman, Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos, in CVPR, 2017. 1, 4 J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang, Segflow: Joint learning for video object segmentation and optical flow, in ICCV, 2017. 1, 4 Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool, Blazingly fast video object segmentation with pixel-wise metric learning, in CVPR, 2018. 1, 3 [10] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, Fast user-guided video object segmentation by interaction-and-propagation networks, in CVPR, 2019. 1, 3 [11] T. Brox and J. Malik, Object segmentation by long term analysis of point trajectories, in ECCV, 2010. 1 [12] Y. J. Lee, J. Kim, and K. Grauman, Key-segments for video object segmentation, in ICCV, 2011. 1 [13] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen, JumpCut: non-successive mask transfer and interpolation for video cutout. ACM Tran. Graphics, vol. 34, no. 6, 2015. 1, 6 [14] H. K. Cheng, S. W. Oh, B. Price, J.-Y. Lee, and A. Schwing, Putting the object back into video object segmentation, in CVPR, 2024. 1, 3, 9, 11, 12, 13 [15] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson et al., SAM 2: Segment anything in images and videos, in ICLR, 2025. 1, 3, 4, 5, 6, 9, 12, [16] H. K. Cheng and A. G. Schwing, XMem: long-term video object segmentation with an atkinson-shiffrin memory model, in ECCV, 2022. 1, 3, 9, 11 [17] H. Ding, C. Liu, N. Ravi, S. He, Y. Wei, S. Bai, and P. Torr, PVUW 2025 challenge report: Advances in pixel-level understanding of complex videos in the wild, in CVPR Workshop, 2025. 1 [18] H. Ding, C. Liu, Y. Wei, N. Ravi, S. He, S. Bai, P. Torr, D. Miao, X. Li, Z. He et al., PVUW 2024 challenge on complex video understanding: Methods and results, in ECCV Workshop, 2025. 1 [19] H. Ding, L. Hong, C. Liu, N. Xu, L. Yang, Y. Fan, D. Miao, Y. Gu, X. Li, Z. He et al., LSVOS challenge report: Large-scale complex and long video object segmentation, in ECCV Workshop, 2025. 1 [20] X. Chen, H. Peng, D. Wang, H. Lu, and H. Hu, Seqtrack: Sequence to [21] sequence learning for visual object tracking, in CVPR, 2023. 3, 11 J. Xie, B. Zhong, Z. Mo, S. Zhang, L. Shi, S. Song, and R. Ji, Autoregressive queries for adaptive tracking with spatio-temporal transformers, in CVPR, 2024. 3, [22] Y. Zheng, B. Zhong, Q. Liang, Z. Mo, S. Zhang, and X. Li, Odtrack: Online dense temporal token learning for visual tracking, in AAAI, 2024. 3, 11 [23] L. Lin, H. Fan, Z. Zhang, Y. Wang, Y. Xu, and H. Ling, Tracking meets lora: Faster training, larger model, stronger performance, in ECCV, 2024. 3, 11 [24] X. Chen, B. Kang, W. Geng, J. Zhu, Y. Liu, D. Wang, and H. Lu, Sutrack: Towards simple and unified single object tracking, in AAAI, 2025. 3, 11 [26] [25] C.-Y. Yang, H.-W. Huang, W. Chai, Z. Jiang, and J.-N. Hwang, SAMURAI: Adapting segment anything model for zero-shot visual tracking with motion-aware memory, arXiv preprint arXiv:2411.11922, 2024. 3, 9, 11, 12 J. Videnovic, A. Lukezic, and M. Kristan, distractor-aware memory for visual object tracking with SAM2, in CVPR, 2025. 3, 5, 6, 9 [27] S. Ding, R. Qian, X. Dong, P. Zhang, Y. Zang, Y. Cao, Y. Guo, D. Lin, and J. Wang, SAM2Long: Enhancing sam 2 for long video segmentation with training-free memory tree, in ICCV, 2025. 3, 9, 10, 11, 12, 13 [28] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao, and H. Ling, LaSOT: high-quality benchmark for large-scale single object tracking, in CVPR, 2019. 3, 4, 5, 6, [29] L. Huang, X. Zhao, and K. Huang, GOT-10k: large high-diversity benchmark for generic object tracking in the wild, IEEE TPAMI, vol. 43, no. 5, 2019. 3, 4, 5, 6, 11 [30] T. Zhou, F. Porikli, D. J. Crandall, L. Van Gool, and W. Wang, survey on deep learning technique for video segmentation, IEEE TPAMI, 2023. 3 [31] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. SorkineHornung, Learning video object segmentation from static images, in CVPR, 2017. 3 [32] W.-D. Jang and C.-S. Kim, Online video object segmentation via convolutional trident network, in CVPR, 2017. 3 [33] V. Jampani, R. Gadde, and P. V. Gehler, Video propagation networks, in CVPR, 2017. 3 [34] H. Xiao, J. Feng, G. Lin, Y. Liu, and M. Zhang, Monet: Deep motion exploitation for video object segmentation, in CVPR, 2018. 3 [35] P. Hu, G. Wang, X. Kong, J. Kuen, and Y.-P. Tan, Motion-guided cascaded refinement network for video object segmentation, in CVPR, 2018. 3 J. Han, L. Yang, D. Zhang, X. Chang, and X. Liang, Reinforcement cutting-agent learning for video object segmentation, in CVPR, 2018. 3 J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang, Fast and accurate online video object segmentation via tracking parts, in CVPR, 2018. 3 [37] [36] [38] S. Xu, D. Liu, L. Bao, W. Liu, and P. Zhou, Mhp-vos: Multiple hypotheses propagation for video object segmentation, in CVPR, 2019. 3 [39] X. Chen, Z. Li, Y. Yuan, G. Yu, J. Shen, and D. Qi, State-aware tracker for real-time video object segmentation, in CVPR, 2020. 3 [40] X. Huang, J. Xu, Y.-W. Tai, and C.-K. Tang, Fast video object segmentation with temporal aggregation network and dynamic template matching, in CVPR, 2020. 3 [41] S. Wug Oh, J.-Y. Lee, K. Sunkavalli, and S. Joo Kim, Fast video object segmentation by reference-guided mask propagation, in CVPR, 2018. 3 MOSE DATASET PAGE: MOSE.VIDEO 15 [42] A. Jabri, A. Owens, and A. Efros, Space-time correspondence as contrastive random walk, in NeurIPS, 2020. 3 [43] H. Lin, X. Qi, and J. Jia, Agss-vos: Attention guided single-shot video object segmentation, in CVPR, 2019. 3 [44] L. Zhang, Z. Lin, J. Zhang, H. Lu, and Y. He, Fast video object segmentation via dynamic targeting network, in ICCV, 2019. 3 J. S. Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and I. S. Kweon, Pixellevel matching for video object segmentation using convolutional neural networks, in ICCV, 2017. 3 [45] [46] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, and L.-C. Chen, Feelvos: Fast end-to-end embedding learning for video object segmentation, in CVPR, 2019. [47] Z. Wang, J. Xu, L. Liu, F. Zhu, and L. Shao, Ranet: Ranking attention network for fast video object segmentation, in ICCV, 2019. 3 [48] K. Duarte, Y. S. Rawat, and M. Shah, Capsulevos: Semi-supervised video object segmentation using capsule routing, in ICCV, 2019. 3 [49] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim, Video object segmentation using space-time memory networks, in ICCV, 2019. 3 [50] Y. Zhang, Z. Wu, H. Peng, and S. Lin, transductive approach for video object segmentation, in CVPR, 2020. [51] Z. Lai, E. Lu, and W. Xie, MAST: memory-augmented selfsupervised tracker, in CVPR, 2020. 3 [52] Z. Yang, Y. Wei, and Y. Yang, Collaborative video object segmentation by foreground-background integration, in ECCV, 2020. 3 [53] L. Hu, P. Zhang, B. Zhang, P. Pan, Y. Xu, and R. Jin, Learning position and target consistency for memory-based video object segmentation, in CVPR, 2021. 3 [54] B. Duke, A. Ahmed, C. Wolf, P. Aarabi, and G. W. Taylor, Sstvos: Sparse spatiotemporal transformers for video object segmentation, in CVPR, 2021. [55] M. Bekuzarov, A. Bermudez, J.-Y. Lee, and H. Li, Xmem++: Production-level video segmentation from few annotated frames, in ICCV, 2023. 3, 9 [56] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. Torr, Fast online object tracking and segmentation: unifying approach, in CVPR, 2019. 3 [57] M. Sun, J. Xiao, E. G. Lim, B. Zhang, and Y. Zhao, Fast template matching and update for video object tracking and segmentation, in CVPR, 2020. 3 [58] F. Lin, H. Xie, Y. Li, and Y. Zhang, Query-memory re-aggregation for weakly-supervised video object segmentation, in AAAI, 2021. 3 [59] Q. Yang, Y. Yao, M. Cui, and L. Bo, Mosam: Motion-guided segment anything model with spatial-temporal memory selection, arXiv preprint arXiv:2505.00739, 2025. 3 [60] R. E. Kalman, new approach to linear filtering and prediction problems, Journal of Basic Engineering, 1960. [61] L. Hong, W. Chen, Z. Liu, W. Zhang, P. Guo, Z. Chen, and W. Zhang, LVOS: benchmark for long-term video object segmentation, in ICCV, 2023. 3, 5, 6 [62] L. Hong, Z. Liu, W. Chen, C. Tan, Y. Feng, X. Zhou, P. Guo, J. Li, Z. Chen, S. Gao et al., LVOS: benchmark for large-scale long-term video object segmentation, arXiv preprint arXiv:2404.19326, 2024. 3, 5, 6, 7, 11 [63] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, Modular interactive video object segmentation: Interaction-to-mask, propagation and differenceaware fusion, in CVPR, 2021. 3, 9, 10 [64] Y. Heo, Y. J. Koh, and C.-S. Kim, Guided interactive video object segmentation using reliability-based attention maps, in CVPR, 2021. 3 J. Miao, Y. Wei, and Y. Yang, Memory aggregation networks for efficient interactive video object segmentation, in CVPR, 2020. 3, 10 [65] [66] B. Chen, H. Ling, X. Zeng, G. Jun, Z. Xu, and S. Fidler, Scribblebox: Interactive annotation framework for video object segmentation, in ECCV, 2020. [67] Z. Yin, J. Zheng, W. Luo, S. Qian, H. Zhang, and S. Gao, Learning to recommend frame for interactive video object segmentation in the wild, in CVPR, 2021. 3 [68] H. Ding, C. Liu, S. He, X. Jiang, and C. C. Loy, MeViS: large-scale benchmark for video segmentation with motion expressions, in ICCV, 2023. 4 [69] H. Ding, C. Liu, S. He, K. Ying, X. Jiang, C. C. Loy, and Y.-G. Jiang, MeViS: multi-modal dataset for referring motion expression video segmentation, IEEE TPAMI, 2025. 4 [70] S. Seo, J.-Y. Lee, and B. Han, Urvos: Unified referring video object segmentation network with large-scale benchmark, in ECCV, 2020. 4 [72] L. Ye, M. Rochan, Z. Liu, X. Zhang, and Y. Wang, Referring segmentation in images and videos with cross-modal self-attention network, IEEE TPAMI, 2021. 4 [73] K. Ying, H. Hu, and H. Ding, MOVE: Motion-guided few-shot video object segmentation, in ICCV, 2025. 4 [74] H. Ding, C. Liu, S. Wang, and X. Jiang, VLT: Vision-language transformer and query generation for referring segmentation, IEEE TPAMI, 2023. 4 [75] S. Liu, T. Hui, S. Huang, Y. Wei, B. Li, and G. Li, Cross-modal progressive comprehension for referring segmentation, IEEE TPAMI, 2021. 4 [76] A. Botach, E. Zheltonozhskii, and C. Baskin, End-to-end referring video object segmentation with multimodal transformers, in CVPR, 2022. 4 [77] D. Wu, X. Dong, L. Shao, and J. Shen, Multi-level representation learning with semantic alignment for referring video object segmentation, in CVPR, 2022. 4 [78] S. He and H. Ding, Decoupling static and hierarchical motion perception for referring video segmentation, in CVPR, 2024. 4 [79] C. Yan, H. Wang, S. Yan, X. Jiang, Y. Hu, G. Kang, W. Xie, and E. Gavves, Visa: Reasoning video object segmentation via large language models, in ECCV, 2024. 4 [80] Z. Bai, T. He, H. Mei, P. Wang, Z. Gao, J. Chen, Z. Zhang, and M. Z. Shou, One token to seg them all: Language instructed reasoning segmentation in videos, in NeurIPS, 2024. 4 [81] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. 4, 14 [82] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in CVPR, 2024. 4, [83] K. Ying, H. Ding, G. Jie, and Y.-G. Jiang, Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation, in ICCV, 2025. 4 [84] H. Ding, S. Tang, S. He, C. Liu, Z. Wu, and Y.-G. Jiang, Multimodal referring segmentation: survey, arXiv, 2025. 4 [85] K. Fragkiadaki, P. Arbelaez, P. Felsen, and J. Malik, Learning to segment moving objects in videos, in CVPR, 2015. 4 [86] Y. Yang, B. Lai, and S. Soatto, Dystab: Unsupervised object segmentation via dynamic-static bootstrapping, in CVPR, 2021. 4 [87] S. Ren, W. Liu, Y. Liu, H. Chen, G. Han, and S. He, Reciprocal transformations for unsupervised video object segmentation, in CVPR, 2021. 4 [88] D. Liu, D. Yu, C. Wang, and P. Zhou, F2net: Learning to focus on the foreground for unsupervised video object segmentation, in AAAI, 2021. 4 [89] X. Lu, W. Wang, M. Danelljan, T. Zhou, J. Shen, and L. Van Gool, Video object segmentation with episodic graph memory networks, in ECCV, 2020. 4 [90] X. Lu, W. Wang, J. Shen, Y.-W. Tai, D. J. Crandall, and S. C. Hoi, Learning video object segmentation from unlabeled videos, in CVPR, 2020. 4 [91] P. Tokmakov, C. Schmid, and K. Alahari, Learning to segment moving objects, IJCV, vol. 127, no. 3, 2019. 4 [92] Z. Yang, Q. Wang, L. Bertinetto, W. Hu, S. Bai, and P. H. Torr, Anchor diffusion for unsupervised video object segmentation, in ICCV, 2019. 4 [93] H. Li, G. Chen, G. Li, and Y. Yu, Motion guided attention for video salient object detection, in ICCV, 2019. 4 [94] W. Wang, H. Song, S. Zhao, J. Shen, S. Zhao, S. C. Hoi, and H. Ling, Learning unsupervised video object segmentation through visual attention, in CVPR, 2019. 4 [95] G. Li, Y. Xie, T. Wei, K. Wang, and L. Lin, Flow guided recurrent neural encoder for video salient object detection, in CVPR, 2018. 4 [96] P. Tokmakov, K. Alahari, and C. Schmid, Learning video object segmentation with visual memory, in ICCV, 2017. 4 [97] T. Zhou, S. Wang, Y. Zhou, Y. Yao, J. Li, and L. Shao, Motion-attentive transition for zero-shot video object segmentation, in AAAI, 2020. 4 [98] X. Lu, W. Wang, C. Ma, J. Shen, L. Shao, and F. Porikli, See more, know more: Unsupervised video object segmentation with co-attention siamese networks, in CVPR, 2019. [99] W. Wang, X. Lu, J. Shen, D. J. Crandall, and L. Shao, Zero-shot video object segmentation via attentive graph neural networks, in ICCV, 2019. 4 [71] H. Ding, C. Liu, S. Wang, and X. Jiang, Vision-language transformer [100] X. Lu, W. Wang, J. Shen, D. Crandall, and L. Van Gool, Segmenting and query generation for referring segmentation, in ICCV, 2021. 4 objects from relational visual data, IEEE TPAMI, 2021. 4 MOSE DATASET PAGE: MOSE.VIDEO 16 [101] H. K. Cheng, S. W. Oh, B. Price, A. Schwing, and J.-Y. Lee, Tracking anything with decoupled video segmentation, in ICCV, 2023. 4, 9, 10 [102] M. Ye, S. W. Oh, L. Ke, and J.-Y. Lee, Entitysam: Segment everything in video, in CVPR, 2025. 4, 10 [132] Y. Cui, C. Jiang, L. Wang, and G. Wu, Mixformer: End-to-end tracking with iterative mixed attention, in CVPR, 2022. 4 [133] B. Yan, H. Peng, J. Fu, D. Wang, and H. Lu, Learning spatio-temporal transformer for visual tracking, in ICCV, 2021. 4 [103] L. Yang, Y. Fan, and N. Xu, Video instance segmentation, in ICCV, [134] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, Transformer 2019. 4 tracking, in CVPR, 2021. 4 [104] X. Li, H. He, Y. Yang, H. Ding, K. Yang, G. Cheng, Y. Tong, and D. Tao, Improving video instance segmentation via temporal pyramid routing, IEEE TPAMI, 2022. [105] L. Ke, H. Ding, M. Danelljan, Y.-W. Tai, C.-K. Tang, and F. Yu, Video mask transfiner for high-quality video instance segmentation, in ECCV, 2022. 4 [106] L. Ke, M. Danelljan, H. Ding, Y.-W. Tai, C.-K. Tang, and F. Yu, Maskfree video instance segmentation, in CVPR, 2023. 4 [107] K. Ying, Q. Zhong, W. Mao, Z. Wang, H. Chen, L. Y. Wu, Y. Liu, C. Fan, Y. Zhuge, and C. Shen, CTVIS: Consistent Training for Online Video Instance Segmentation, in ICCV, 2023. 4 [108] T. Zhang, X. Tian, Y. Wu, S. Ji, X. Wang, Y. Zhang, and P. Wan, Dvis: Decoupled video instance segmentation framework, in ICCV, 2023. 4 [109] Y. Zhou, T. Zhang, S. Ji, S. Yan, and X. Li, Improving video [135] L. Lin, H. Fan, Z. Zhang, Y. Xu, and H. Ling, Swintrack: simple and strong baseline for transformer tracking, in NeurIPS, 2022. 4 [136] D. Guo, Y. Shao, Y. Cui, Z. Wang, L. Zhang, and C. Shen, Graph attention tracking, in CVPR, 2021. 4 [137] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, J.-K. KÃ¤mÃ¤rÃ¤inen, H. J. Chang, M. Danelljan, L. ÄŒ. Zajc, A. LukeÅ¾iÄ et al., The tenth visual object tracking vot2022 challenge results, in ECCV, 2022. 4, 5, 6 [138] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, Semantic segmentation with context encoding and multi-path decoding, IEEE TIP, vol. 29, 2020. 4 [139] J. Lazarow, K. Lee, K. Shi, and Z. Tu, Learning instance occlusion for panoptic segmentation, in CVPR, 2020. 4 [140] X. Zhan, X. Pan, B. Dai, Z. Liu, D. Lin, and C. C. Loy, Self-supervised segmentation via dynamic anchor queries, in ECCV, 2024. 4 scene de-occlusion, in CVPR, 2020. 4 [110] T. Zhang, X. Tian, Y. Zhou, S. Ji, X. Wang, X. Tao, Y. Zhang, P. Wan, Z. Wang, and Y. Wu, Dvis++: Improved decoupled framework for universal video segmentation, IEEE TPAMI, 2025. 4 [111] J. Qi, Y. Gao, Y. Hu, X. Wang, X. Liu, X. Bai, S. Belongie, A. Yuille, P. H. Torr, and S. Bai, Occluded video instance segmentation: benchmark, IJCV, vol. 130, no. 8, 2022. 4, 6 [112] J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in CVPR, 2015. [113] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs, IEEE TPAMI, 2017. 4 [114] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, Context contrasted feature and gated multi-scale aggregation for scene segmentation, in CVPR, 2018. 4 [115] G. J. Brostow, J. Fauqueur, and R. Cipolla, Semantic object classes in video: high-definition ground truth database, Pattern Recognition Letters, vol. 30, no. 2, 2009. 4 [116] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, The cityscapes dataset for semantic urban scene understanding, in CVPR, 2016. 4 [117] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li, and Y. Yang, Vspw: large- [141] A. Kortylewski, Q. Liu, A. Wang, Y. Sun, and A. Yuille, Compositional convolutional neural networks: robust and interpretable model for object recognition under occlusion, IJCV, vol. 129, no. 3, 2021. 4 [142] H. Zhang and H. Ding, Prototypical matching and open set rejection for zero-shot semantic segmentation, in ICCV, 2021. 4 [143] X. Wang, T. Xiao, Y. Jiang, S. Shao, J. Sun, and C. Shen, Repulsion loss: Detecting pedestrians in crowd, in CVPR, 2018. 4 [144] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, Occlusion-aware r-cnn: Detecting pedestrians in crowd, in ECCV, 2018. 4 [145] M.-J. Chiou, H. Ding, H. Yan, C. Wang, R. Zimmermann, and J. Feng, Recovering the unbiased scene graphs from the biased ones, in ACM MM, 2021. [146] X. Li, H. Ding, W. Zhang, H. Yuan, J. Pang, G. Cheng, K. Chen, Z. Liu, and C. C. Loy, Transformer-based visual segmentation: survey, IEEE TPAMI, 2024. 4 [147] J. Wu, X. Li, S. Xu, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong, X. Jiang, B. Ghanem, and D. Tao, Towards open vocabulary learning: survey, IEEE TPAMI, 2024. 4 [148] B. Miao, M. Bennamoun, Y. Gao, and A. Mian, Region aware video object segmentation with deep motion modeling, arXiv preprint arXiv:2207.10258, 2022. 4 scale dataset for video scene parsing in the wild, in CVPR, 2021. 4 [149] G. Zhan, W. Xie, and A. Zisserman, tri-layer plugin to improve [118] G. Sun, Y. Liu, H. Ding, T. Probst, and L. Van Gool, Coarse-to-fine occluded detection, in BMVC, 2022. 4 feature mining for video semantic segmentation, in CVPR, 2022. 4 [150] L. Ke, Y.-W. Tai, and C.-K. Tang, Deep occlusion-aware instance [119] G. Sun, Y. Liu, H. Ding, M. Wu, and L. Van Gool, Learning local and global temporal contexts for video semantic segmentation, IEEE TPAMI, 2024. 4 [120] S. A. S. Hesham, Y. Liu, G. Sun, H. Ding, J. Yang, E. Konukoglu, X. Geng, and X. Jiang, Exploiting temporal state space sharing for video semantic segmentation, in CVPR, 2025. 4 [121] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, in COLM, 2024. 4 segmentation with overlapping bilayers, in CVPR, 2021. 4 [151] Q. Chu, W. Ouyang, H. Li, X. Wang, B. Liu, and N. Yu, Online multi-object tracking using cnn-based single object tracker with spatialtemporal attention mechanism, in ICCV, 2017. 4 [152] J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, Online multi-object tracking with dual matching attention networks, in ECCV, 2018. 4 [153] J. Xu, Y. Cao, Z. Zhang, and H. Hu, Spatial-temporal relation networks [122] D. Kim, S. Woo, J.-Y. Lee, and I. S. Kweon, Video panoptic segmenfor multi-object tracking, in ICCV, 2019. 4 tation, in CVPR, 2020. 4 [123] J. Miao, X. Wang, Y. Wu, W. Li, X. Zhang, Y. Wei, and Y. Yang, Large-scale video panoptic segmentation in the wild: benchmark, in CVPR, 2022. 4 [124] S. Woo, D. Kim, J.-Y. Lee, and I. S. Kweon, Learning to associate every segment for video panoptic segmentation, in CVPR, 2021. 4 [125] S. Qiao, Y. Zhu, H. Adam, A. Yuille, and L.-C. Chen, Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation, in CVPR, 2021. [126] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun, Upsnet: unified panoptic segmentation network, in CVPR, 2019. 4 [127] X. Li, H. Yuan, W. Li, H. Ding, S. Wu, W. Zhang, Y. Li, K. Chen, and C. C. Loy, OMG-Seg: Is one model good enough for all segmentation? in CVPR, 2024. 4 [154] Q. Liu, Q. Chu, B. Liu, and N. Yu, Gsm: Graph similarity model for multi-object tracking. in IJCAI, 2020. 4 [155] S. Li, M. Danelljan, H. Ding, T. E. Huang, and F. Yu, Tracking every thing in the wild, in ECCV, 2022. 4 [156] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg, Video segmentation by tracking many figure-ground segments, in ICCV, 2013. 6 [157] S. D. Jain and K. Grauman, Supervoxel-consistent foreground propagation in video, in ECCV, 2014. 6 [158] P. Ochs, J. Malik, and T. Brox, Segmentation of moving objects by long term video analysis, IEEE TPAMI, vol. 36, no. 6, 2014. 6 [159] M. Kristan, J. Matas, M. Danelljan, M. Felsberg, H. J. Chang, L. ÄŒ. Zajc, A. LukeÅ¾iÄ, O. Drbohlav, Z. Zhang, K.-T. Tran et al., The first visual object tracking segmentation vots2023 challenge results, in ICCV Workshop, 2023. 6 [128] A. Yilmaz, O. Javed, and M. Shah, Object tracking: survey, ACM [160] P. Tokmakov, J. Li, and A. Gaidon, Breaking the object in video Comput. Surv., 2006. 4 object segmentation, in CVPR, 2023. [129] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji, Siamese box adaptive network for visual tracking, in CVPR, 2020. 4 [130] Z. Zhang and H. Peng, Deeper and wider siamese networks for realtime visual tracking, in CVPR, 2019. 4 [131] D. Guo, J. Wang, Y. Cui, Z. Wang, and S. Chen, Siamcar: Siamese fully convolutional classification and regression for visual tracking, in CVPR, 2020. 4 [161] Q. Jiang, F. Li, Z. Zeng, T. Ren, S. Liu, and L. Zhang, T-rex2: Towards generic object detection via text-visual prompt synergy, in ECCV, 2024. [162] K. Chen, D. Ramanan, and T. Khurana, Using diffusion priors for video amodal segmentation, in CVPR, 2025. 6 [163] S. Bai, K. Chen, X. Liu, J. Wang, and et al, Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. 6 MOSE DATASET PAGE: MOSE.VIDEO [164] D. R. Martin, C. C. Fowlkes, and J. Malik, Learning to detect natural image boundaries using local brightness, color, and texture cues, IEEE TPAMI, vol. 26, no. 5, 2004. 7 [165] Z. Yang, Y. Wei, and Y. Yang, Associating objects with transformers for video object segmentation, in NeurIPS, 2021. 9 [166] H. K. Cheng, Y.-W. Tai, and C.-K. Tang, Rethinking space-time networks with improved memory coverage for efficient video object segmentation, in NeurIPS, 2021. 9, 10 [167] M. Li, L. Hu, Z. Xiong, B. Zhang, P. Pan, and D. Liu, Recurrent dynamic embedding for video object segmentation, in CVPR, 2022. 9 [168] Z. Yang and Y. Yang, Decoupling features in hierarchical propagation for video object segmentation, in NeurIPS, 2022. [169] J. Zhang, Y. Cui, G. Wu, and L. Wang, Jointformer: unified framework with joint modeling for video object segmentation, IEEE TPAMI, 2025. 9 [170] M. Li, S. Li, X. Zhang, and L. Zhang, Univs: Unified and universal video segmentation with prompts as queries, in CVPR, 2024. 10 [171] S. VujasinoviÄ‡, S. Bullinger, S. Becker, N. Scherer-Negenborn, M. Arens, and R. Stiefelhagen, Revisiting click-based interactive video object segmentation, in ICIP, 2022. 10 [172] S. Caelles, J. Pont-Tuset, F. Perazzi, A. Montes, K.-K. Maninis, and L. Van Gool, The 2019 davis challenge on vos: Unsupervised multiobject segmentation, arXiv:1905.00737, 2019."
        }
    ],
    "affiliations": [
        "ByteDance Inc",
        "Fudan University, Shanghai, China",
        "Nanyang Technological University, Singapore",
        "Shanghai University of Finance and Economics, China",
        "University of Oxford, United Kingdom"
    ]
}