{
    "paper_title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
    "authors": [
        "Enshen Zhou",
        "Qi Su",
        "Cheng Chi",
        "Zhizheng Zhang",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Lu Sheng",
        "He Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments."
        },
        {
            "title": "Start",
            "content": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection Zhizheng Zhang4, Enshen Zhou1* , Qi Su2* , Cheng Chi3* , Zhongyuan Wang3, Tiejun Huang2, Lu Sheng1, He Wang2,3,4 4 2 0 2 ] . [ 1 5 5 4 4 0 . 2 1 4 2 : r 1School of Software, Beihang University 2School of Computer Science, Peking University 3Beijing Academy of Artificial Intelligence 4Galbot zhouenshen@buaa.edu.cn qiisuu@stu.pku.edu.cn chicheng15@mails.ucas.ac.cn lsheng@buaa.edu.cn hewang@pku.edu.cn Figure 1. For the task Move the pan with lobster to the stove without losing the lobster, (a) reactive failure detection identifies failures after they occur, and (b) proactive failure detection prevents foreseeable failures. In (a), at tR 4 , the robot detects the failure after the lobster unpredictably jumps out due to the heat. In (b), pan tilting is detected at tP 3 , requiring real-time precision. We formulate both tasks as spatio-temporal constraint satisfaction problems, leveraging our proposed constraint elements for precise, real-time checking. For example, in (a), large relative distance between pan and lobster indicates failure; in (b), large angle between the pan and the horizontal plane needs correction. (c) shows that our method combined with an open-loop policy forms closed-loop system, enabling proactive (e.g., detecting moving glass during grasping) and reactive (e.g., removing toy after grasping) failure detection in cluttered scenes. 3 and corrected it at tP"
        },
        {
            "title": "Abstract",
            "content": "Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuEqual contribution Corresponding author. Work done during internship at Galbot. racy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments. Please see the project page at https://zhoues.github. io/Code-as-Monitor/. 1 1. Introduction As expectations grow for robots to handle long-horizon tasks within intricate environments, failures are unavoidable. Therefore, automatically detecting and preventing those failures plays vital role in ensuring the tasks can eventually be solved, especially for closed-loop robotic systems. There are two modes of failure detection [30], reactive and proactive. As depicted in Fig. 1, reactive failure detection aims to identify failures after they occur (e.g., recognizing that lobster lands on the table, indicating delivery failure). In contrast, proactive failure detection aims to prevent foreseeable failures (e.g., recognizing that tilted pan could cause the lobster to fall out, leading to delivery failure). Both detection modes are even more challenging in open-set scenarios, where the failures are not predefined. With the help of large language models (LLMs) [14, 59] and vision-language models (VLMs) [1, 39], recent studies can achieve open-set reactive failure detection [9, 12, 13, 16, 22, 40, 57, 58, 68, 75] as special case of visual question answering (VQA) tasks. However, these methods often bear compromised execution speeds and coarse-grained detection accuracy, due to the high computational costs and inadequate 3D spatio-temporal perception capability of recent LLMs/VLMs. Moreover, open-set proactive failure detection, which has been less explored in the literature, presents even severer challenges as it is required to foresee potential causes of failure and monitor them in real-time with high precision to anticipate and prevent imminent failure. Simply adapting LLMs/VLMs cannot meet such expectations. In this work, we aim to develop an open-set failure detection framework that achieves reactive and proactive detection simultaneously, not only benefiting from the generalization power of VLMs but also enjoying high precision in monitoring failure characteristics with real-time efficiency. We address this by formulating both tasks as unified set of spatio-temporal constraint satisfaction problems, which can be precisely translated by VLMs into executable programs. Such visual programs can efficiently verify whether entities (e.g., robots, objects) or their parts in the captured environment maintain or achieve required states during or after execution (i.e., satisfying constraints), so as to immediately prevent or detect failures. To the best of our knowledge, this is the first attempt to integrate both detection modes within single framework. We name this constraint-aware visual programming framework as Code-as-Monitor (CaM ). To be specific, the proposed spatio-temporal constraint satisfaction scheme abstracts the constraint-related entity or part segments from the observed images into compact geometric elements (e.g., points, lines, and surfaces), as shown in Fig. 1. It simplifies the monitoring of constraint satisfaction by tracking and evaluating the spatio-temporal combinational dynamics of these elements, eliminating the most irrelevant geometric and visual details of the raw entities/parts. The constraint element detection and tracking are grounded by our trained constraint-aware segmentation and off-the-shelf tracking models, ensuring speed, accuracy, and certain open-set adaptation capabilities. The evaluation protocol is in the form of VLM-generated code, i.e., monitor code, which is visually prompted by the starting frames of sub-goal and its associated constraint elements, in addition to the textual constraints that must be fulfilled. Once generated, this monitor code could detect reactive or proactive failures just by being executed according to the tracked constraint elements, without needing to call the VLMs again. Therefore, this minimalist scheme is generalizable to openset failure detection for unseen entities and scenes (enabled by the potential diversity of the structured associations of constraint elements) as well as common skills (powered by the rich prior knowledge offered by VLMs), maintaining sufficient detection accuracy and real-time execution speed. We conduct extensive experiments in three simulators (i.e., CLIPort [56] , Omnigibson [32], and RLBench [24]) and one real-world setting, spanning diverse manipulation tasks (e.g., pick & place, articulated objects, tool-use), robot platforms (e.g., UR5, Fetch, Franka) and end-effectors (e.g., suction cup, gripper, dexterous hand). The results show that CaM is generalizable and achieves both reactive and proactive failure detection in real-time, resulting in 28.7% higher success rates and reduced execution time by 31.8% under severe disturbances compared to the baselines. Moreover, in Sec. 4.3, CaM can be integrated with the existing open-loop control policy to form closedloop system, enabling long-horizon tasks in cluttered scenes with environment dynamics and human disturbances. Our contributions are summarized as follows: We introduce Code-as-Monitor (CaM ), novel paradigm that leverages VLMs for both reactive and proactive failure detection via constraint-aware visual programming. We propose the constraint elements to enhance the accuracy and efficiency of constraint satisfaction monitoring. Extensive experiments show that CaM is generalizable and achieves more real-time and precise failure detection than baselines across simulators and real-world settings. 2. Related work Robotic Failure Detection. Recent advances in LLMs [4, 14, 59, 60] and VLMs [1, 7, 20, 39, 47, 48, 54, 66, 73, 77] greatly improve open-set reactive failure detection. Current LLM-based methods either convert visual inputs into text [41, 55, 65], potentially losing visual details, or rely on ground-truth feedback [18, 22, 49, 57, 58], which is impractical in real-world scenarios. Recent studies employ VLMs as failure detectors, offering binary success indicators [12, 40, 68, 75] or textual explanations [9, 13] through visual question answering (VQA), such as DoReMi [16]. However, they often bear compromised execution speeds 2 Figure 2. Overview of Code-as-Monitor. Given task instructions and prior information, the Constraint Generator derives the next subgoal and corresponding textual constraints based on multi-view observations. The Painter maps these constraints onto images as constraint elements. The Monitor generates monitor code from these images and tracks them for real-time monitoring. If any constraint is violated, it outputs the reason for failure and triggers re-planning. This framework unifies reactive and proactive failure detection via constraints, more generally abstracts relevant entities/parts through constraint elements, and ensures precise and real-time monitoring via code evaluation. and coarse-grained detection accuracy. Meanwhile, openset proactive failure detection is rarely explored, as previous methods [2, 10] are confined to predefined failures. This detection mode requires foreseeing potential failure causes and monitoring them in real-time with high precision. In this work, we formulate both reactive and proactive failure detection as constraint satisfaction problems and use VLMgenerated code to evaluate them, meeting the expectations of both modes above. Visual Prompting. Visual prompts enhance the visual reasoning abilities of VLMs and are categorized into maskbased, point-based, and element-based methods. Maskbased approaches like SoM [64] apply numbered segmentation masks on images without considering instructions, whereas instruction-guided methods like IVM [74] and API [67] generate masks to block irrelevant regions. Pointbased prompting encodes functionalities via points, including affordances [29, 38, 69], motion [38, 45], and constraints like ReKep [23]. However, ReKep [23] extracts semantic keypoints using DINOv2 [46] and struggles to capture precise keypoints that accurately represent desired constraints. Element-based methods like CoPa [21], represent robotic control using basic elements (e.g., points, lines) via pre-trained models like SAM [27] and GPT-4V [1] simply. In contrast, this work explores constraint satisfaction monitoring and introduces constraint elements that more precisely represent relevant entities/parts. These elements are tracked and evaluated in real-time to simplify monitoring. Visual Programming. Visual programming requires strong visual concept understanding and reasoning. Prior works show strong generalization (e.g., zero-shot) across various tasks, such as image editing [8, 17], 3D visual grounding [70], and robotics control [36, 61], by integrating LLMs with visual modules but lose fine-grained details. Recent studies [44, 62] explore visual programming using VLMs, but these methods take raw RGB images as inputs with predefined primitive libraries. In contrast, this work leverages constraint elements for visual programming and uses arithmetic operations in the code to encode their spatio-temporal combinational dynamics, presenting greater challenges. 3. Method We first give an overview of the proposed Code-as-Monitor (CaM ) (Sec. 3.1). Then, we elaborate on the constraint element in Constraint Painter, especially constraint-aware segmentation (Sec. 3.2). Finally, we present Constraint Monitor for real-time detection (Sec. 3.3). 3.1. Overview The proposed CaM comprises three key modules: the Constraint Generator, Painter, and Monitor. We focus on longhorizon manipulation task instructions Lglobal (e.g., Move the pan with the bread to the stove, and be careful not to let the bread fall out), using RGB-D observations from two camera views (front and top). As shown in Fig. 2, the RGB images O, along with instructions Lglobal, previous subgoal lpre, and failure feedback from the Constraint Monitor fpre (e.g., subgoal success or failure reason), are fed 3 Figure 3. Constraint Element Pipeline. Given constraint, our model ConSeg generates instance-level and part-level masks across multiple views, which are projected into 3D space. Through series of heuristics, the desired elements are produced. Once all elements are obtained, they are annotated onto the original multi-view images. Here we display the annotation result of one element. into the Constraint Generator FVLM (i.e., GPT-4o [1]) to generate the next subgoal lnext and associated textual constraints C. This process can be formulated as follows: lnext, Cd, Cu = FVLM(O, Lglobal, lpre, fpre) (1) d, c1 d, . . . , cn where Cd = {c0 } denotes the constraints that must be maintained during subgoal execution (e.g., pan handle must be grasped, bread must remain in the pan, pan should remain horizontal during transfer), and Cu = {c0 u} refers to the constraints that must be met upon subgoal completion (e.g., pan should be directly above the stove). We successfully unify reactive and proactive failure detection as these task-specific, situation-aware constraint satisfaction problems. u, . . . , ck u, In Painter, for each textual constraint from Cd or Cu, we generate corresponding constraint elements (detailed in Sec. 3.2) from observations O. These elements, which are composed of 3D points, abstract the constraint-related entities or their parts to represent the desired textual constraint more easily (e.g., the constant distance between green points on bread and pan determines if bread remains in pan, as shown in Fig. 2.) These generated elements are then aggregated into the final set = {e0, e1, . . . , en+k}, and numerically annotated across all views to produce the final visual prompted images OE . In Monitor, we provide GPT-4o [1] with the next subgoal lnext, textual constraints C, and annotated observations OE for constraint-aware visual programming to generate the evaluation protocol, i.e., monitor code. This code inputs the elements 3D positions, calculates arithmetic operations within it, and returns boolean to indicate potential or actual failure and string to describe its reason. During subgoal execution, Monitor tracks the elements and evaluates the spatio-temporal combinational dynamics of these elements. If the code returns False, the policy execution halts immediately, and the accompanying string is used as feedback fpre for re-planning. Otherwise, the subgoal is considered completed. In either case, the cycle is repeated. 4 3.2. Constraint Element To simplify the monitoring of constraint satisfaction, we introduce constraint elements by abstracting constraintrelated entities/parts into compact geometric elements (e.g., points, lines) to get rid of the most irrelevant geometric and visual details, making them easier to track and generalize. Pipeline. The constraint element generation pipeline is shown in Fig. 3. Our trained multi-granularity constraintaware model ConSeg performs two inference steps for each and each RGB image from the set of views O. First, instance-level masks Mi are generated to capture constraint-related entities. Then, fine-grained part-level masks Mp and corresponding element type descriptions le are produced, as shown in Fig. 4. Using corresponding depth data, we project Mp from all views into 3D space, fusing them into point cloud. However, directly tracking and evaluating the spatio-temporal combinational dynamics of these constraint-related entities/parts is challenging. Therefore, we convert these entities into proposed constraint elements. We first apply voxelization to the point cloud with voxel size determined by element type le (e.g., the surface needs at least 3 points, we divide the occupied space into 2 2 voxels.). Then we cluster one representative point within each voxel and filter them to specified number, also determined by le to extract the final desired 3D points. We connect points within each instance-level mask Mi to form the constraint element associated with constraint c. Notably, points modeled as end-effectors (e.g., the points of the fingertips and hands center represent dexterous hand) can be directly obtained from forward kinematics, bypassing the process above. Additionally, we perform parallel inference of ConSeg across all views to expedite the acquisition of the final set and their annotations onto the corresponding views OE . Moreover, this minimalist approach, i.e., constraint elements, emphasizes the most relevant entities/parts, enabling generalization to unseen scenes and entities, which is critical for open-set failure detection. For more details, please check Supp. A.3. Constraint-aware Segmentation. Since the textual constraint does not explicitly specify relevant entities/parts, we constraints and the next subgoal lnext. The generated code accepts the elements 3D positions as input and performs arithmetic operations (e.g., NumPy-supported calculations) to evaluate spatio-temporal constraints at various stages (i.e., during execution or upon completion), as shown in Fig. 2 code block. It returns boolean flag indicating potential or actual failure if the computed result exceeds predefined tolerance (e.g., accepting deviations of the pans surface normal from the z-axis within 15), along with string explaining the cause. By tracking the elements using CoTracker [25], we achieve real-time detection via VLMs without frequent calls, reducing the computational cost. Notably, including both current positions and historical trajectories of the elements as input allows us to monitor tasks needing high precision, such as the requirement of 2cm movement or 180 rotation, which previous reactive detection methods could not address through direct VLM queries using raw RGB images. Moreover, the potential diversity in the structured associations of constraint elements within the code (e.g., calculating the angle between the normal vector of the pans surface element and the z-axis determines if the pan is level, as shown in Fig. 2 code block) and rich prior knowledge offered by VLMs enable generalization to certain unseen tasks requiring common skills. 4. Experiments Our experiments aim to address the following questions: (1) Can CaM achieve open-set reactive and proactive failure detection across diverse robots, end-effectors, and objects, both in simulator (Sec. 4.2) and on real robots (Sec. 4.3)? (2) Can ConSeg infer multi-granularity constraint-aware masks for unseen scenes and objects (Sec. 4.4)? (3) Which design choices greatly enhance the performance (Sec. 4.5)? 4.1. Experimental Setup Environment Settings. We evaluate CaM across three simulators (i.e., CLIPort [56], Omnigibson [32], and RLBench [24]) and real-world setting. In CLIPort, we employ UR5 arm equipped with suction cup for pick-andplace and spatula for pushing, controlled by pre-trained CLIPort policy [56]. Omnigibson features Fetch robot with gripper, controlled by ReKep [23]. RLBench uses Franka arm with gripper, controlled by ARP [72]. We use UR5 arm with Leap Hand [53] for real-world experiments, controlled by an open-loop policy named DexGraspNet 2.0 [71]. More details are provided in Supp. B. ConSeg Configuration. Given the significant gap between simulation and real-world data, we fine-tune the ConSeg model with 100 trajectories collected from each simulation environment before deploying it. To share the same data collection and auto-label pipeline, this fine-tuning dataset is also limited to pick-and-place tasks. We manuFigure 4. ConSeg architecture. Here we display the part-level segmentation, which will output the desired element type and mask. require model capable of logical reasoning and certain open-set adaptation, enabling precise constraint-aware instance and part-level segmentation. As depicted in Fig. 4, we propose ConSeg, which builds upon LISA [28] comprising VLM FVLM (i.e., LLaVA [39]) with the vision encoder Fenc and decoder Fdec from SAM [27]. The textual constraint and the image are input into the VLM FVLM to generate an embedding for the <SEG> token. The VLM outputs textual description le of the desired element type additionally for part-level segmentation. The decoder Fdec then generates the segmentation mask by leveraging visual features from the vision encoder Fenc based on the current observation o, and features derived from the final-layer embedding h<SEG> of the <SEG> token, transformed via an MLP (FMLP). The process can be formulated as follows: le, h<SEG> = FVLM(c, o), = Fdec(FMLP(h<SEG>), Fenc(o)) (2) The entire pipeline, including additional LoRA [19] parameters, is optimized end-to-end while keeping the parameters of the VLM FVLM and vision encoder Fenc frozen. Dataset and Collection Pipeline. Our dataset utilizes images from BridgeData V2 [63]. While BridgeData V2 provides trajectory-level instructions, we require frame-level annotations for per-frame subgoals and constraints. To address this, we sample pick-and-place data, using external references (e.g., gripper open/close states) to generate 10, 181 trajectories with 219, 356 images. We employ GPT4o to decompose trajectory instructions into subgoals, constraints, and object-part associations to generate groundtruth annotations. Instance-level and part-level segmentations are performed using Grounded SAM [51] and Semantic SAM [33], respectively. These outputs are integrated and refined through manual inspection to produce the multigranularity dataset, which is combined with LISAs training data to fine-tune our model. More details are in Supp. A. 3.3. Real-time Monitor Module After annotating constraint elements with unique colors and numerical labels across all views, these images serve as visual prompts to generate monitor code via GPT-4o [1] with 5 Table 1. Performance in CLIPort. We report the success rate and execution time, compared to CLIPort (CP) [56], with Inner Monologue (IM) [22] and DoReMi (DRM) [16]. Tasks with disturbance CP Stack in order with drop Stack in order with noise Sweep Half the Blocks p=0.0 p=0.15 p=0.3 q=1 q=2 q=3 Success Rate(%) Execution Time(s) +IM +DRM +Ours +IM +DRM +Ours 13.4 21.0 25.4 24.2 29.2 36.8 16.4 100.0 95.00 88.33 98.33 83.33 63.33 75.00 100.0 83.33 76.67 96.67 75.00 40.00 16.67 13.4 26.00 34.20 24.6 37.0 54.2 16. 13.4 34.8 42.8 24.8 39.4 58.2 22.0 100.0 100.0 56.67 81.67 21.67 75.00 90.00 90.00 41.67 71.67 15.00 40.00 18.33 0.00 ally annotate trajectory-level instructions, and image frames are captured at frequency of 1 Hz. Using only pick-andplace task data helps mitigate the data distribution gap while allowing us to validate the models generalization to unseen tasks, including tool-use, articulated objects, and longhorizon complex tasks. Notably, the ConSeg model used in real-world experiments is not fine-tuned, ensuring rigorous evaluation of ConSegs generalization capability. Task Settings. We introduce disturbances across all environments to induce failures, classified by the affected constraints (e.g., points, lines, surfaces). We evaluate task success rate, execution time, and additional token usage in Omnigibson. Notably, we dont assess failure judgment success rate because our code operates continuously in real-time, rendering this metric inapplicable. Moreover, there is no standard for evaluating proactive failure detection accuracy. Detailed evaluation settings are provided in each section. Baselines. We compare DoReMi (DRM) [16] as the baseline for all settings, which uses VLMs to detect intermediate failures during execution via frequent VQA-based queries. In CLIPort, we include Inner Monologue [22] baseline, which detects failures only upon each subgoal completion. 4.2. Main Results in Simulator"
        },
        {
            "title": "4.2.1 Results in CLIPort",
            "content": "We evaluate two tasks in CLIPort: (1) Stack in Order: The robot must stack blocks in specified order, including two point-level disturbances: (a) with per-step probability p, the suction cup may release block, causing it to drop; (b) placement positions are perturbed by uniform noise in [0, q] cm, potentially leading to tower collapse. Success is defined as correctly stacking the blocks within 70s. (2) Sweep Half the Blocks: To address the pre-trained policys tendency to sweep all blocks, the robot must determine when to halt, sweeping half of the blocks (10%) into specified colored area within 30s. Success is achieved by meeting these criteria. Tab. 1 shows the mean results over 5 different seeds, each with 12 episodes. For more details, please check Supp. C.1. The following paragraphs present our analyses. Code better monitors 3D space relations. As shown in the Tab. 1, Under the most severe disturbances (p=0.3, q=3) in Stack in Order, CaM achieves 17.5% higher success 6 rate than DoReMi. Frequent VLM queries lead to increased incorrect failure judgments due to limited 3D spatial understanding from single images, compared to code-based evaluation of block positions. For example, after placing the green block on the red and preparing to pick up the blue, DoReMi mistakenly thinks that the green block is no longer atop the red, causing it to re-execute the previous subgoal. Code with elements achieve reactive and proactive failure detetction. As shown in Tab. 1, under severe disturbances in Stack in Order, CaM reduces execution time by 38.7% and 14.4% compared to Inner Monologue [22], which only detects failures upon subgoal completion, and DoReMi [16], which suffers from misjudgments due to repeated VLM queries, respectively. In contrast, CaM unifies both reactive and proactive failure detection with high precision in real-time, especially in preventing foreseeable failures. For example, if the green block is placed on the red block with heavy placement position disturbance, CaM anticipates that further stacking the blue may lead to collapse and then stabilizes the green block before proceeding. Code with elements leads to more accurate counting. in Sweep Half the Blocks, As shown in the Tab. 1, CaM achieves average success rates 4.5 higher than DoReMi [16]. Calculating the block points in the designated surface region provides more accurate results than directly using the VLM to count, enabling more precise halting of the policy to complete the task."
        },
        {
            "title": "4.2.2 Results in Omnigibson",
            "content": "We conduct experiments on three tasks in Omnigibson, each involving distinct type of constraint-element disturbance: (1) Slot Pen: Insert pen into holder, facing point-level disturbances wherein (a) pen is moved during grasping; (b) pen is dropped during transport; (c) holder is moved during insertion. (2) Stow Book: Place book to bookshelf vertically, with line-level disturbances where (a) book is randomly rotated during grasping; (b) end-effector joint is randomly actuated to alter the book pose; (c) book is reoriented (3) Pour Tea: Pours from horizontally after placement. teapot into teacup, encountering surface-level disturbances wherein (a) teapot is tilted forward/backward during movement; (b) end-effector joint is actuated to induce lateral tilt of the teapot during movement; (c) teapot is returned to horizontal position during pouring. Tab. 2 reports the results across three tasks, each including one no-disturbance trial and three specific-disturbance trials, with 10 runs for each setting. More details are provided in Supp. C.2. Code with elements detects richer failures. In Tab. 2, only CaM can detect failures caused by surface-level disturbances in Pour Tea compared to DoReMi [16]. The reason is that changes in the teapots pitch and roll angles are hard to detect by querying VLM via VQA using one Table 2. Performance in Omnigibson. We report the success rate (SR), execution time, and token usage, compared to DoReMi (DRM) [16]. Solt Pen (Point-level Disturbances) Stow Book (Line-level Disturbances) Pour Tea (Surface-level Disturbances) Method SR with Disturbance(%) None Dist.(a) Dist.(b) Dist.(c) ReKep [23] +DRM +Ours 30 40 60 20 10 50 10 20 40 10 20 Time Token (s) - - 177.84 54.54 101.85 25.82 SR with Disturbance(%) SR with Disturbance(%) (k) None Dist.(a) Dist.(b) Dist.(c) (k) None Dist.(a) Dist.(b) Dist.(c) 40 50 70 30 40 60 30 20 70 20 40 60 20 0 20 0 40 20 0 40 10 0 30 Time Token (s) - - 127.17 38.67 18.67 93. Time Token (k) (s) - - - - 174.55 44.19 Figure 5. Example of Real-world Evaluation. The red bounding box shows the current grasp target, which may shift due to environmental changes. CaM monitors and adapts to these changes in real-time, resulting in closed-loop system with an open-loop policy. Table 3. Performance of Single Pick & Place. We report the success rate and execution time. DGN donates DexGraspNet 2.0 [71]. Table 5. Performance of reasoning and constraint-aware segmentation. FMC denotes the foundation model combination baseline. Object types Tasks with disturbance Pick & Place Deformable with the Transparent objects being Small Rigid moved Pick & Place Deformable with the Transparent objects being Small Rigid removed Large Geometric Large Geometric Success Rate(%) Execution Time(s) DGN +DRM +Ours +DRM +Ours 46.3 0.00 48.1 0.00 45.4 0.00 45.3 0.00 62.5 0.00 62.7 0.00 60.5 0.00 60.3 0. 96.67 93.33 96.67 96.67 93.33 90.00 93.33 96.67 83.33 66.67 80.0 86.67 76.67 60.00 63.33 76.67 61.8 72.6 65.7 68.9 68.7 77.7 69.8 72.3 Table 4. Performance of Reasoning Pick & Place in cluttered scene. We report the success rate. The robot is controlled by an open-loop policy named DexGraspNet 2.0 (DGN) [71]. w/ CE with indicates using constraint elements; otherwise, constraintaware entities or parts are used for tracking and code computation. Tasks Clear all objects on table except for animals Grasp the animals according to their distances to fruits, from nearest to farthest w/ CE Success Rate(%) DGN +DRM +Ours 60.00 10.00 0.00 20.00 10.00 0.00 0. 0.00 90.00 current image. The misjudgment leads to 0% success rate of DoReMi in this task. Notably, DoReMis success rate is sometimes lower than that of ReKep alone due to VLMs limited spatio-temporal understanding of single images. Code with elements detects failure with lower computational cost. As shown in Tab. 2, we achieve 34.8% reduction in execution time and 52.2% decrease in token count compared to DoReMi [16]. This improvement stems from proactive failure detection, which prevents more severe failures ahead in real-time for timely re-planning while generating code only once per subgoal. For example, in Pour Tea, CaM detects failure by monitoring the teapots tilt angle in real-time, avoiding frequent VLM checks."
        },
        {
            "title": "4.2.3 Results in RLBench",
            "content": "We further evaluate CaM on RLbench and demonstrate its superior generalization across diverse manipulation tasks, 7 Method OVSeg [35] GRES [37] X-Decoder [78] SEEM [79] PixelLM [52] LISA-13B [28] FMC ConSeg-13B ReasonSeg Instance-level cIoU gIoU 18.6 28.5 19.9 22.4 17.9 22.6 21.2 25.5 61.4 56.0 65.1 56.6 53.3 51.2 63.9 55.7 ConstraintSeg Instance-level cIoU gIoU 31.4 32.9 26.4 28.6 28.0 27.8 28.0 29.4 43.2 44.4 38.9 42.1 49.6 49.3 68.7 62.1 Part-level gIoU 20.2 22.7 23.5 23.1 24.1 23.4 40.8 60.2 cIoU 21.5 22.6 25.1 24.8 22.6 24.3 39.3 65.3 including articulated objects, rotational manipulation, and tool use. Experimental details can be found in Supp. C.3. 4.3. Main Results in Real World We conduct real-world evaluations on two tasks: (1) Simple Pick & Place: The robot has 70s to pick up objects and place them at specified locations, facing two disturbances: (a) moving the object during grasping, and (b) removing the object from hand during movement. We test four object types (e.g., deformable, transparent), selecting 3 examples per type and conducting 10 trials for each (see Tab. 3). (2) Reasoning Pick & Place: The robot executes long-horizon tasks, involving ambiguous terms (e.g., fruit, animal), under the same disturbances. We evaluate 2 long-horizon tasks in cluttered scenes, performing 10 trials each (see Tab. 4). For more details, please check Supp. C.4. The following paragraphs present our analyses. Elements generally abstract constraints and relevant entities. As shown in Tab. 3, using different end-effector (i.e., Leap Hand [53]) in Simple Pick & Place, CaM also achieves success rates surpassing DoReMi [16] by 20.4% when handling different kinds of objects (e.g., deformable). We find that abstracting constraint-related entities/parts removes the irrelevant visual details, enabling generalization to different kinds of entities in unseen scenes (as discussed in Sec. 3.2), leading to easier tracking and code evaluation. Figure 6. Visual comparison between our ConSeg and LISA [28] at instance and part level. The red masks are the segmentation results. Table 6. Ablation studies in Omnigibsons Stow Book, assessing the impact of Multi-View (MV), Constraint-aware Segmentation (CS) for elements, and Connect Points (CP) for element formation. MV CS CP None 40.0 50.0 60.0 70.0 Success Rate with Disturbance(%) Dist.(b) 30.0 40.0 60.0 70. Dist.(c) 50.0 40.0 50.0 60.0 Dist.(a) 40.0 40.0 50.0 60.0 Avg 40.0 42.5 55.0 65.0 4.4. Main Results of Segmentation Reactive and Proactive failure detection combined with an open-loop policy forms close-loop system. Tab. 4 shows that only CaM successfully handles long-horizon tasks in cluttered scenes, while all baselines fail. These tasks are challenging as the robot is controlled by an openloop policy that cannot handle environment dynamics and human disturbances in closed-loop manner. By incorporating both reactive and proactive failure detection with the open-loop policy, as shown in Fig. 5, the robot can dynamically adjust its target object in real-time. For example, when human moves the horse or pear during the task, the robot adapts by grasping the animal closest to the fruit, effectively forming closed-loop system. Tab. 5 presents segmentation results comparing our ConSeg, with SOTA models and Foundation Model Combination (FMC) baseline. FMC integrates GPT-4o for reasoning over tasks and constraints to identify relevant instances and parts, along with Grounded SAM [51] and Semantic SAM [33] for instance and part-level segmentation, respectively, similar to our data collection pipeline. We evaluate performance on the ReasonSeg [28] benchmark and our proposed Constraint-Aware Segmentation (ConstraintSeg) benchmark. Our benchmark evaluates performance using gIoU and cIoU, following ReasonSegs evaluation setting. ConSeg performs both reasoning and multi-granularity constraint-aware segmentation. As shown in Tab. 5, ConSeg performs comparably to LISA and PixelLM on the Rea8 Table 7. Ablation study on training data. SemanticSeg includes ADE20K [76], COCO-Stuff [5], PACO-LVIS [50] and PASCALPart [6]. ReferSeg includes refCLEF, refCOCO, refCOCO+ [26] and refCOCOg [42]. VQA indicates LLaVAInstruct-150k [39]. SemanticSeg ReferSeg VQA ReasonSeg ContraintSeg Ins-level Part Ins Part-level gIoU cIoU gIoU cIoU 42.1 38.9 23.4 24.3 60.7 65.9 40.4 45.6 51.5 50.6 56.5 61.7 62.1 68.7 60.2 65.3 sonSeg but significantly surpasses them on ConstraintSeg, achieving nearly 40% improvement at the part level. Visual comparisons in Fig. 6 further demonstrate ConSegs strong generalization to unseen objects, scenes, and tasks. 4.5. Ablation Study We conduct ablation studies following the corresponding environment settings and present analyses below. Multi views are critical for visual programming. As shown in Tab. 6, using only front-view images with constraint elements reduces the average success rate from 65% to 40%. This is primarily due to (1) occlusion leading to suboptimal element generation and (2) errors in visual programming, where dimensional reduction causes surfaces to be misinterpreted as lines or lines as points. Constraint-aware segmentation enhances element. As shown in Tab. 6, using DINOv2 [46] to generate semantic points to form elements, rather than through constraintaware segmentation, reduces the success rate from 65% to 42.5%, because these constructed elements fail to represent the desired constraints accurately. For example, capturing books vertical orientation requires two precise points on its edge, which DINOv2 can not provide. Forming elements improves code generation. As shown in Tab. 6, using unconnected 3D points as final elements reduces the success rate from 65% to 55%. Pre-formed elements contain more prior constraint information, serving as visual cues better encoded in code, whereas standalone points require recombination during visual programming. Elements simplify constraints computation. As shown in Tab. 4, the success rate decreases from 60% to 20% when entities or parts are not transformed into proposed elements in real-world evaluation. We find two key issues: (1) inaccurate 3D position and slow pose tracking of entities or parts; and (2) difficulties in performing arithmetic operations on their 3D positions or poses in code, hindering the encoding of their spatio-temporal constraints. Both instance-level and part-level data improve performance. We conduct ablation studies to assess the impact of our proposed dataset on the ConstraintSeg benchmark. The results are shown in Tab. 7. The results indicate that both the instance-level and part-level subsets contribute to performance improvements and enhance each others effects. 5. Conclusion In this paper, we present novel paradigm termed Codeas-Monitor, leveraging the VLMs for both open-set reactive and proactive failure detection. In detail, We formulate both detection modes as spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. We further propose constraint elements, which abstract constraints-related entities or their parts into compact geometric elements, to improve the precision and efficiency of monitoring. Extensive experiments demonstrate the superiority of the proposed approach and highlight its potential to advance closed-loop robot systems."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv, 2023. 2, 3, 4, 5, 9, 22 [2] Aneseh Alvanpour, Sumit Kumar Das, Christopher Kevin Robinson, Olfa Nasraoui, and Dan Popa. Robot failure mode prediction with explainable machine learning. CASE, 2020. 3 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv, 2022. 9, 20 [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 2 [5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Cocostuff: Thing and stuff classes in context. CVPR, 2018. 8 [6] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. CVPR, 2014. 8 [7] Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Hao Shu Fang, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, et al. Rh20t-p: primitive-level robotic dataset towards composable generalization agents. arXiv, 2024. 2 [8] Jaemin Cho, Abhay Zala, and Mohit Bansal. Visual programming for step-by-step text-to-image generation and evaluation. NeurIPS, 2024. 3 [9] Yinpei Dai, Jayjun Lee, Nima Fazeli, and Joyce Chai. Racer: Rich language-guided failure recovery policies for imitation learning. arXiv, 2024. [10] Maximilian Diehl and Karinne Ramirez-Amaro. causalbased approach to explain, predict and prevent failures in robotic tasks. RAS, 2023. 3 [11] Yufei Ding, Haoran Geng, Chaoyi Xu, Xiaomeng Fang, Jiazhao Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, and He Wang. Open6dor: Benchmarking open-instruction 6dof object rearrangement and vlm-based approach. IROS, 2024. 9, 19 [12] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. Vision-language models as success detectors. arXiv, 2023. 2 [13] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: visionlanguage-model for detecting and reasoning over failures in robotic manipulation. arXiv, 2024. 2 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv, 2024. 2 [15] Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, and Dieter Fox. Rvt2: Learning precise manipulation from few demonstrations. RSS, 2024. [16] Yanjiang Guo, Yen-Jen Wang, Lihan Zha, Zheyuan Jiang, and Jianyu Chen. Doremi: Grounding language model by detecting and recovering from plan-execution misalignment. arXiv, 2023. 2, 6, 7, 4, 5, 9, 12 [17] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. CVPR, 2023. 3 [18] Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided robot skill acquisition. CoRL, 2023. 2 [19] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2021. 5 [20] Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, and Jing Shao. Vlsbench: Unveiling visual leakage in multimodal safety. arXiv preprint arXiv:2411.19939, 2024. 2 [21] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. arXiv, 2024. [22] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor 9 Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv, 2022. 2, 6, 4, 5 [23] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv, 2024. 3, 5, 7, 4 [24] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment. RAL, 2020. 2, 5, 3 [25] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. ECCV, 2024. 5 [26] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. EMNLP, 2014. [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. ICCV, 2023. 3, 5 [28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. CVPR, 2024. 5, 7, 8, 1 [29] Olivia Lee, Annie Xie, Kuan Fang, Karl Pertsch, and Chelsea Finn. Affordance-guided reinforcement learning via visual prompting. arXiv, 2024. 3 [30] Gregory LeMasurier, Alvika Gautam, Zhao Han, Jacob Crandall, and Holly Yanco. Reactive or proactive? how robots should explain failures. HRI, 2024. 2 [31] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. NeurIPS, 2020. 8 [32] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Mona Anvari, Minjune Hwang, Manasi Sharma, Arman Aydin, Dhruva Bansal, Samuel Hunter, Kyu-Young Kim, Alan Lou, Caleb Matthews, Ivan Villa-Renteria, Jerry Huayang Tang, Claire Tang, Fei Xia, Silvio Savarese, Hyowon Gweon, Karen Liu, Jiajun Wu, and Li Fei-Fei. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. CoRL, 2022. 2, 5, 3 [33] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv, 2023. 5, 8, 1, [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with ICML, frozen image encoders and large language models. 2023. 4 [35] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. CVPR, 2023. 7 [36] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. ICRA, 2023. 3 [37] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. CVPR, 2023. [38] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. arXiv, 2024. 3 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2024. 2, 5, 8 [40] Jiaming Liu, Chenxuan Li, Guanqun Wang, Lily Lee, Kaichen Zhou, Sixiang Chen, Chuyan Xiong, Jiaxin Ge, Renrui Zhang, and Shanghang Zhang. Self-corrected multimodal large language model for end-to-end robot manipulation. arXiv, 2024. 2 [41] Zeyi Liu, Arpit Bahety, and Shuran Song. Reflect: Summarizing robot experiences for failure explanation and correction. CoRL, 2023. 2, 9, 18 [42] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. CVPR, 2016. 8 [43] Pasquale Minervini et al. awesome-hallucination-detection. https://github.com/EdinburghNLP/awesomehallucination-detection, 2024. [44] Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et al. Robocodex: Multimodal code generation for robotic behavior synthesis. arXiv, 2024. 3 [45] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv, 2024. 3 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv, 2023. 3, 8 [47] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. 2 [48] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: multi-modal open-ended embodied system in minecraft via active perception. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16307 16316. IEEE, 2024. 2 [49] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. NeurIPS, 2022. 2 [50] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. CVPR, 2023. 8 [51] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv, 2024. 5, 8, 1, 2 [52] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. CVPR, 2024. 7 [53] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning. arXiv, 2023. 5, 7, 3 [54] Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao. Assessment of multimodal large language models in alignment with human values. arXiv, 2024. 2 [55] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and selfreflection. arXiv, 2023. 2 [56] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. CoRL, 2021. 2, 5, 6, 3, 4 [57] Tom Silver, Soham Dan, Kavitha Srinivas, Joshua Tenenbaum, Leslie Kaelbling, and Michael Katz. Generalized planning in pddl domains with pretrained large language models. AAAI, 2024. 2 [58] Marta Skreta, Naruki Yoshikawa, Sebastian ArellanoRubach, Zhi Ji, Lasse Bjørn Kristensen, Kourosh Darvish, Alan Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv, 2023. [59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv, 2023. 2 [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv, 2023. 2 [61] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. IEEE Access, 2024. 3 [62] David Venuto, Sami Nur Islam, Martin Klissarov, Doina Precup, Sherry Yang, and Ankit Anand. Code as reward: Empowering reinforcement learning with vlms. arXiv, 2024. 3 [63] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. CoRL, 2023. 5, 1 [64] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv, 2023. 3 [65] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv, 2022. 2 [66] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multimodal instruction-tuning dataset, framework, and benchmark. NeurIPS, 2024. [67] Runpeng Yu, Weihao Yu, and Xinchao Wang. Attention prompting on image for large vision-language models. arXiv, 2024. 3 [68] Zhouliang Yu, Jie Fu, Yao Mu, Chenguang Wang, Lin Shao, and Yaodong Yang. Multireact: Multimodal tools augmented reasoning-acting traces for embodied agent planning. ICLR, 2024. 2 [69] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv, 2024. 3 [70] Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, and Zhen Li. Visual programming for zero-shot open-vocabulary 3d visual grounding. CVPR, 2024. 3 [71] Jialiang Zhang, Haoran Liu, Danshi Li, XinQiang Yu, Haoran Geng, Yufei Ding, Jiayi Chen, and He Wang. Dexgraspnet 2.0: Learning generative dexterous grasping in largescale synthetic cluttered scenes. CoRL, 2024. 5, 7, 4, 8 [72] Xinyu Zhang, Yuhan Liu, Haonan Chang, Liam Schramm, and Abdeslam Boularias. Autoregressive action sequence learning for robotic manipulation. arXiv, 2024. 5, 3, 7 [73] Zaibin Zhang, Shiyu Tang, Yuanhang Zhang, Talas Fu, Yifan Wang, Yang Liu, Dong Wang, Jing Shao, Lijun Wang, and Huchuan Lu. Ad-h: Autonomous driving with hierarchical agents. arXiv, 2024. 2 [74] Jinliang Zheng, Jianxiong Li, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, and Xianyuan Zhan. Instruction-guided visual masking. arXiv, 2024. 3 [75] Zhi Zheng, Qian Feng, Hang Li, Alois Knoll, and Jianxiang Feng. Evaluating uncertainty-based failure detection for closed-loop llm planners. arXiv, 2024. 2 [76] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. CVPR, 2017. 8 [77] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-ofarXiv preprint imagination for simulated-world control. arXiv:2403.12037, 2024. 2 [78] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. CVPR, 2023. 7 [79] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. NeurIPS, 2024. 7 11 Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary document is organized as follows: Sec. A: Implementation Details of Painter, including data collection, training, and element pipeline details. Sec. B: Environment Configuration, including environmental setups, control policies, and baseline details. Sec. C: Evaluation Details, including detailed task definition, disturbances, and experimental results. Sec. D: Limitations and Future Work. Sec. E: More Demonstrations of CaM . A. Constraint Painter In this section, we first describe the data collection and annotation process of the Constraint-aware Segmentation Dataset. Next, we present the training details of the proposed ConSeg model. Finally, it is followed by detailed explanation of the Element Pipeline. A.1. ConSeg Data Collection To ensure broader coverage of scenarios and objects in our dataset, along with text-based instructions, we utilized the BridgeData V2 dataset [63]. This large and diverse dataset of robotic manipulation behaviors comprises 60, 096 trajectories collected across 24 environments, encompassing 13 distinct skills. The dataset collection pipeline is shown in Fig. 7. The entire process is divided into three stages, i.e., trajectory decomposition, assigning textual information, and dataset collection. First, we decompose each trajectorys instruction and initial observation from BridgeData V2 into subgoals for each stage, along with the constraints upon completion, constraints during execution, the corresponding object-part associations and element type for each constraint. Subfigure 1 in Fig. 7 illustrates specific example, demonstrating the decomposition of the task Take cup off plate. Notably, the third subgoal, Place the cup on the stove, indicates that the initial observation ensures the decomposition process fully understands the tasks contextual environment. During this process, we also obtain the constraint element type, which serves as the ground-truth text response for part-level constraint-aware segmentation. This decomposition is performed using the off-the-shelf GPT-4o API. After obtaining the subgoals, constraints, and objectpart associations for each stage, we need to assign each frame to its corresponding stage. Since BridgeData V2 does not provide per-frame annotations, we addressed this issue by sampling pick-and-place data and leveraging the additional information (e.g., gripper open/close states) provided by BridgeData V2 for assignment. The pick-andplace task is typically divided into three stages: Approach, Grasp and Transfer, and Place, corresponding to the gripper states of open, closed, and open, respectively. Leveraging the characteristics of the pick-and-place task, we complete the frame-level assignment. Subfigure 2 in Fig. 7 illustrates specific example of the assignment process. Using the obtained frame-level constraint-aware object and part information, Instance-level and part-level segmentations are performed using Grounded SAM [51] and Semantic SAM [33], respectively. We conducted sampled manual inspection of the final annotations to filter out errors and low-quality labels. Our final dataset is composed of 10, 181 trajectories with 219, 356 images. A.2. ConSeg Training Details We adopt LISAs [28] loss function, including the nexttoken prediction loss for text output, and the combination of per-pixel BCE loss with DICE loss for mask output. Our ConSeg-13B model is trained on an 8 NVIDIA 80G H800 GPU for two days with batch size of 4. Our training data comprises multiple components: SemanticSeg, ReferSeg, VQA, ReasonSeg, and ConstraintSeg, to ensure our model retains dialogue and reasoning segmentation capabilities while achieving constraint-aware segmentation. LISA inspires this training data setting. SemanticSeg includes ADE20K, COCO-Stuff, PACO-LVIS, and PASCALPart. ReferSeg includes refCLEF, refCOCO, refCOCO+, and refCOCOg. VQA includes LLaVAInstruct-150k. ConstraintSeg includes instance and part-level data. The training setting described above is for the ConSeg-base model. Since the training data consists entirely of real-world scenarios, there is significant gap between the simulation and real-world environments. To address the ConSeg model used in the simulation experithis, ments is fine-tuned version, called ConSeg-ft, finetuned on small amount of data collected from the simulator. Specifically, we collect 100 trajectories from each simulator, sampled frames at 1 Hz, and utilize either ground truth masks from the simulator or annotations generated using Grounded SAM and Semantic SAM. Notably, we use the 1 Figure 7. Dataset Collection Pipeline. Our data is sourced from BridgeData V2 [32]. The data collection process consists of three steps: (1) Using GPT-4o [1] to decompose the task instruction based on the initial observation from the first frame of the trajectory, generating subgoals along with two types of constraints for each subgoal (i.e., constraints during execution and upon completion) and object-part associations. (2) Utilizing external references (e.g., gripper open/close states) to assign subgoals, constraints, and object-part associations to each frame. (3) Leveraging off-the-shelf models (e.g., Grounded SAM [51], Semantic SAM [33]) to generate instanceand part-level masks (blue mask in this figure) automatically, followed by manual filtering to curate the final dataset. ConSeg-base model in real-world experiments, demonstrating the models generalization capability across different scenarios. A.3. Element Pipeline Details Here, we provide additional details about the Constraint Element Pipeline. We first filter out outliers after obtaining the constraint-aware object/part point clouds. Next, we calculate the 3D spatial bounds occupied by the remaining point cloud and determine the voxel size for voxelization based on the element type. We then perform point cloud clustering using the DBSCAN algorithm, which has advantages over other methods, including identifying clusters of arbitrary shapes, eliminating the need to predefine the number of clusters, and its effectiveness in high-density regions. B. Environment Configuration We first provide detailed descriptions of the simulators (CLIPort [56], Omnigibson [32], RLBench [24]) and realworld setups used in our study. We then discuss the lowlevel control policies implemented in these environments. Finally, we present the baselines and their implementation specific to each environment. Notably, our framework, CaM , is policy agnostic, meaning it can be adapted to any control policy without requiring any modification. B.1. Environmental Setup The CLIPort [56] simulator is robotic manipulation benchmark to gather extensive data for imitation learning https://github.com/cliport/cliport 2 Figure 8. Environmental Setup of three simulators and one real-world setting. For CLIPort [56] and OmniGibson [32], we provide thirdperson front and top views and the robot platforms are the UR5 arm and Fetch, respectively. RLBench [24] offers four camera views, including front left shoulder, right shoulder, and wrist views, with the robot platform being Franka equipped with gripper. We provide wrist and third-person front view for the real-world setting, utilizing UR5 robot equipped with Leap Hand [53]. and train language-conditioned multi-task low-level control policy. The environment features UR5 robotic arm with suction cup as the end effector for pick-and-place tasks and spatula as the end effector for pushing tasks, both operating on black tabletop. We use two cameras: third-person front view and top view to provide comprehensive perspectives of the tabletop, as shown in Fig. 8 (a). The OmniGibson [32] simulator offers realistic setting including physics engine capable of supporting features such as lighting rendering, gravity effects, and temperature variations impacting objects within the environment. This platform also provides an extensive array of pre-configured scenes and objects, enabling researchers to customize setups and train mobile manipulation robots. We select version 1.1.0 for our study. This simulator involves Fetch robot equipped with gripper as the end effector, operating on white tabletop. We utilize two cameras: third-person front view and top view to provide comprehensive perspectives of the tabletop, as shown in Fig. 8 (b). RLBench [24] simulator is widely used benchmark for robot manipulation, featuring tasks such as articulated objects and tool use. Researchers can gather data and train low-level control policies using imitation learning or reinforcement learning within this environment. RLBench features Franka robotic arm with gripper as its end effector, operating on brown tabletop. Four cameras provide comprehensive tabletop coverage, as shown in Fig. 8 (c). Additionally, RLBench uses sampling-based motion planner for motion planning given the next predicted action/pose. In the real-world setup depicted in Fig. 8 (d), we utilize https://github.com/StanfordVL/OmniGibson https://github.com/stepjam/RLBench fixed UR5 robotic arm with Leap Hand [53] as the end effector. Two RealSense D415 RGB-D cameras capture the scene, one mounted on the wrist and the other positioned for third-person front view. B.2. Control Policy In CLIPort, we use pre-trained low-level policy the CLIPort [56] simulator provides to control the robotic arm and end effector. This policy can execute multi-task operations based on language instructions with RGB observations and its performance approaches perfection due to extensive imitation learning training. Notably, the policy is open-loop, meaning it does not adjust its actions in response to dynamic environmental changes (e.g., it will not immediately pick up dropped block during movement but will continue with the previously planned actions). In Omnigibson, We utilize ReKep [23] as our lowlevel control policy, transforming long-horizon tasks into set of relationships between fixed keypoints at different stages. At each stage, an optimization algorithm computes these relationships to generate actions, enabling languageconditioned closed-loop control. Notably, ReKep employs pre-trained large vision model (i.e., DINOv2 [46]) to process raw RGB data, extracting semantically relevant keypoints. This approach also serves as the compared method in our ablation study for extracting 3D points through constraint-aware segmentation, showing our superiority. In RLBench, we employ the Autoregressive Policy (ARP) [72] as the control policy, which generates the next action based on historical observations and action sequences through an autoregressive process. This method achieves state-of-the-art performance in the RLBench. Figure 9. CLIPort task demonstration. we present three types of tasks in our experiments, including the starting and ending frames. Table 8. Detailed Performance in CLIPort. We report the success rate and execution time for three tasks, compared to baseline methods."
        },
        {
            "title": "Stack in\norder with\ndrop p",
            "content": "CLIPort 100.00 0.00 56.67 6.11 21.67 8.33 90.00 6.11 Stack in 41.67 7.30 order with noise 15.00 6.11 Sweep Half the Blocks 0.00 0.00 p=0.0 p=0.15 p=0.3 q=1 q=2 q=3 Success Rate(%) Execution Time(s) +Inner Monologue 100.00 0.00 81.67 6.11 75.00 8.95 90.00 6.11 71.67 8.33 40.00 8.00 18.33 6.11 +DoReMi +Ours +Inner Monologue +DoReMi +Ours 100.00 0.00 100.00 0.00 83.33 5.17 95.00 4.00 76.67 9.52 88.33 6.53 96.67 4.00 98.33 3.27 75.00 5.17 83.33 5.17 40.00 6.11 63.33 8.33 16.67 8.95 75.00 11.55 13.40 1.82 34.80 3.12 42.80 3.18 24.80 4.08 39.40 5.87 58.20 4.74 22.00 2.91 13.40 1.82 13.40 1.82 26.00 2.77 21.00 1.75 34.20 2.73 25.40 2.95 24.60 4.66 24.20 4.65 37.00 6.29 29.20 4.61 54.20 6.02 36.80 4.61 16.60 1.33 16.40 1."
        },
        {
            "title": "Use Rope to Close\nthe Opening Square",
            "content": "0.00 0.00 68.33 9.52 58.33 18.62 76.67 6.11 41.60 6.34 65.80 7.40 34.60 2.81 In the real-world setting, We employ DexGraspNet 2.0 [71] as our low-level policy, which predicts the dexterous hands grasping pose based on the scenes point cloud and facilitates the robotic arms action trajectory through motion planning to achieve robust generalized grasping. Notably, DexGraspNet 2.0 is an open-loop policy, which means it does not adjust to environmental changes during action execution. For example, if the target objects position shifts while the arm moves, the system does not modify its motion plan. Therefore, it continues to execute toward the originally intended location to complete the grasp, failing. B.3. Baseline Details In CLIPort, we compare three baselines: CLIPort [56], CLIPort with Inner Monologue [22], and CLIPort with DoReMi [16]. We slightly modify the original implementations of these three baselines to suit our task requirements. (1) For CLIPort [56], the sole modification involves substituting the original oracle success detector with an offthe-shelf VLM (i.e., GPT-4o [1]) used as failure detector. This change enables the robot system to determine whether to transition to the next subgoal using image-based vision question answering (VQA). Notably, CLIPort decomposes the instructions into list of subgoals before the task begins and does not dynamically adjust or revert to previous (2) For Inner Monosubgoals upon detecting failure. logue [22], we replicate the implementation detailed in the original literature, by employing CLIPort for the low-level policy and an off-the-shelf VLM (i.e., GPT-4o [1]) as the planner. This pipeline determines the next subgoal based on the completed subgoals and current observations after each subgoal concludes. Notably, Inner Monologue queries the VLM only at the end of each subgoal, without considering events that occur during execution. (3) For DoReMi [16], we reproduce the implementation according to the original DoReMi paper and enhance it by replacing the VLM initially (i.e., BLIP2 [34]) used for repeated VQA-style queries during robotic execution with more powerful VLM (i.e., GPT-4o [1]). Additionally, we substitute its LLM, which lacks environmental awareness, with the same GPT-4o to serve as the task planner. In Omnigibson, we compare two baselines: ReKep [23] and ReKep with DoReMi [16]. (1) For ReKep, we directly implement it using the official codebase. (2) For DoReMi, it is implemented as described above. In RLBench, we compare two baselines: ReKep [23] and ReKep with DoReMi [16]. (1) For ReKep, we directly implement it using the official codebase. (2) For DoReMi, it is implemented as described above. For the real-world evaluation, we compare two baselines: DexGraspNet 2.0 [71] and DexGraspNet 2.0 with DoReMi [16]. (1) For DexGraspNet 2.0, we directly im4 Figure 10. Omnigibson task demonstration. we present three types of tasks in our experiments, including the starting and ending frames. plement it using the official codebase. (2) For DoReMi, it is implemented as described above. C. Evaluation Details In this section, we first detail the task specifications within the simulator and real-world evaluations. Then, we introduce the disturbances introduced in each task and the evaluation metrics used. Finally, we report the detailed experimental results and our analyses, with additional results not included in the main text due to space constraints. C.1. CLIPort C.1.1 Task, Disturbance and Metric Details As shown in Fig. 9, we evaluate three tasks in CLIPort: (1) Stack in Order: Given blocks in red, green, and blue on table, the robot must stack them with red at the bottom, green in the middle, and blue on top. (2) Sweep Half the Blocks With 40 blocks on the table, the robot must sweep approximately half of them (with permissible error margin of 10%, i.e., 16 to 24 blocks) to designated area. (3) Use Rope to Close the Opening Square: The robot should use rope to enclose an open square, to enclose the area sufficiently, rather than form perfectly closed square. We introduce two types of disturbances to the Stack in Order task: (1) after the suction cup grasps block, there is probability at each step that the block will be released, causing it to drop; (2) The predicted placement position by the policy is perturbed by uniform noise in the range [0, q] cm, potentially leading to block tower collapse. For each task and disturbance type, we conduct 5 trials using different seeds, each comprising 12 episodes. We assess performance based on success rate and execution time, excluding the computational time for invoking the VLM. Results are reported as mean values with 95% confidence intervals. In the Stack in Order task, the robot must successfully stack the blocks in the specified order into tower within 70 seconds, despite the two perturbations above. For the Sweep Half the Blocks task, the pre-trained policy aims to sweep blocks into designated area. The robot must stop the policy once half of the blocks are in the target region. If, after 30 seconds, the number of blocks in the area falls within the required range (1624), the task is considered successful. For the Use Rope to Close the Opening Square task, the pre-trained policy attempts to close an open rectangle into perfect square using rope. The robot must detect when the rectangle is sufficiently enclosed and immediately stop execution. Success is achieved if the robot halts within 70 seconds, and the enclosure is complete. C.1.2 Detailed Experiment Results In Tab. 8, we present detailed results in CLIPort, including those discussed in the main text and additional results. In the Stack in Order task under severe interference conditions, our CaM shows an improvement of 18.33% and 17.5% in success rate over Inner Monologue [22] and DoReMi [16], respectively, while also reducing execution times by 38.7% and 14.4% compared to Inner Monologue and DoReMi, respectively. The failure detection and recovery processes are shown in Fig. 12 and Fig. 13. In the Sweep Half the Blocks task, our CaM achieved success rates that are 4.1 and 4.5 times higher than those of Inner Monologue [22] and DoReMi [16], respectively. However, the success rate is not very high even in distraction-free scenarios. This is attributed to the high density of tracking points in the scene, which increases the likelihood of confusion and tracking errors, leading to incon5 Figure 11. RLBench task demonstration. we present six types of tasks in our experiments, including the starting and ending frames. sistent block counts within the target area. The completion process of the task is also illustrated in Fig. 14. In the Use Rope to Close the Opening Square task, our approach outperforms Inner Monologue [22] and DoReMi [16] by 8.34% and 18.34% in success rates, respectively, while also reducing execution times by 16.82% and 47.43% compared to Inner Monologue and DoReMi, respectively. We find that calculating the distance between the rope ends and the openings edges to determine closure is more accurate than directly querying VLM with an image, allowing for earlier termination of the policy execution. The complete processes of the task are illustrated in Fig. 15. C.2. OmniGibson C.2.1 Task, Disturbance and Metric Details As shown in Fig. 10, in the OmniGibson environment, we evaluated three distinct tasks: (1) Slot Pen: pen placed on desk is picked up, rotated to near-vertical position, moved above pen holder, and then inserted into the holder. (2) Stow Book: book located on desk is picked up and vertically positioned on bookshelf. (3) Pour Tea: teapot on the desk is lifted, horizontally moved above teacup, and then tilted to pour tea into the cup. We introduce three types of disturbances with varying constraint elements for each task: (1) Slot Pen Task: Pointlevel disturbances are applied as follows: (a) moving the pen while the robot is grasping it, (b) forcing the robot to release the pen mid-transfer, causing it to drop onto the table, and (c) moving the pen holder while the robot attempts to insert the pen. Despite these disturbances, the task is considered successful only if the robot can insert the pen into the holder. (2) Stow Book Task: Line-level disturbances include: (a) rotating the book during the robots grasping process, (b) altering the books pose during transfer to disrupt its vertical alignment, and (c) reorienting the book horizontally after it has been placed vertically on the shelf. Success requires the robot to place the book vertically on the shelf despite these disturbances. (3) Pour Tea Task: Surface-level disturbances involve: (a) tilting the container forward or backward during transfer, (b) inducing lateral tilts during movement, and (c) restoring the container to level position during pouring. To succeed, the robot must prevent spillage and complete the pouring task under these disturbances. We conduct experiments on three tasks, each consisting of one no-disturbance trial and three specific-disturbance trials. Each trial was repeated 10 times, and the performance was evaluated based on success rate, execution time (including the computation time for invoking the VLM), and the number of tokens used. C.2.2 Detailed Experiment Results Specific experimental results are detailed in the main text; here, we present additional demonstrations in Sec.E.2. C.3. RLBench C.3.1 Task, Disturbance and Metric Details As shown in Fig. 11, in RLBench, we evaluate six tasks (1) Articulated across three categories of manipulation: Object Interaction: (a) Open DrawerOpen the top drawer (b) Put in DrawerOpen the drawer and place (2) Rotational Manipulaan item into the open drawer. tion: (a) Screw BulbScrew in the red light bulb. (b) (3) Tool Use: (a) Drag Turn TapTurn the left tap. StickUse stick to drag the cube onto the red target. (b) Sweep to DustpanSweep dirt into the tall dustpan. In RLBench, we avoid introducing additional disturbances, as its control policy naturally generates diverse 6 Table 9. Performance in RLBench. We report the success rate, compared to baseline methods."
        },
        {
            "title": "Method",
            "content": "RVT2 [15] ARP [72] +DRM [16] +Ours Avg."
        },
        {
            "title": "Articulated Object",
            "content": "Tool-Use Tool-Use Success Rate (%) Open Drawer 89.83 91.27 87.97 97.08 90.3 93.9 90.6 98.1 Put in Drawer 97.6 91.0 87.7 98. Screw Bulb 86.6 86.4 83.1 97."
        },
        {
            "title": "Turn Tap Drag Stick",
            "content": "91.0 96.6 93.3 97.9 93.8 88.1 84.8 95.6 Sweep to Dustpan 79.7 91.6 88.3 94.0 failures to validate the effectiveness of our framework. The RLBench-trained policy lacks failure recovery mechanisms; thus, any episode flagged as failure by the detection framework is deemed invalid, and new episode is initiated. For each task, we evaluate performance over 1, 000 valid episodes (maximum 25 steps each), measured by the average success rate. C.3.2 Detailed Experiment Results Code with elements can generalize better to monitor diverse tasks. Notably, despite our training data lacking information on articulated objects at both the instance and part levels, our method effectively handles them, accurately segmenting parts such as drawer handles. In the Open Drawer task, CaM achieves 98.1% success rate, significantly outperforming DRMs 90.6%. For Put in Drawer, CaM reaches 98.3%, surpassing DRMs 87.7% by 10.6 percentage points. We attribute this generalization to two factors: (1) the inherent prior world knowledge of extensively pre-trained VLMs (e.g., SAM, LLaVA), which enables generalization to unseen tasks; and (2) our minimalist scheme that abstracts articulated objects into geometric components via constraint elements, ignoring irrelevant details, enhancing the generalizability. We also demonstrate our method on additional unseen tasks, such as rotational manipulation and tool use, where it consistently outperforms baseline methods. C.4. Real-world Evaluation C.4.1 Task, Disturbance and Metric Details We conduct real-world evaluations on two tasks: (1) Simple Pick & Place: The robot should pick and place objects within 70 seconds. We include four kinds of objects: Deformable, Transparent, Small Rigid, and Large Geometric, with three objects in each category. The deformable objects are Loopy, Dog, and Rabbit toys, which undergo deformation when grasped by the dexterous hand. The transparent objects are clear beverage bottle, transparent glass, and clear shampoo bottle. The small rigid objects consist of apple, pear, and peach models, while the large geometric objects include large plate, ball, and pyramid. (2) Reasoning Pick & Place: In cluttered scenes, the robot must perform long-horizon tasks where the instructions contain ambiguous terms (e.g., animals or fruits without specifying particular types). Specifically, the tasks are: (a) Clear all objects on the table except for animals, and (b) Grasp the animals according to their distances to fruits, from nearest to farthest, with ambiguous terms underlined. For both tasks, we introduce two identical disturbances: (1) Moving the object during the robots grasping. (2) Removing the object from the robots dexterous hand during movement after grasping. For each task, and each object involved in Simple Pick & Place, we conduct 10 trials. For each long-horizon task in Reasoning Pick & Place, we also conduct 10 trials. We evaluate performance based on success rate and execution time, including the computational time invoking the VLM. Results are reported as mean values with 95% confidence intervals. For the Simple Pick & Place, the robot has only one opportunity to autonomously release the object held in its gripper at designated location. Any disturbance the robot encounters allows for return and reattempt at grasping if the robot successfully detects it. Success is defined as meeting these conditions within 90 seconds. For the Reasoning Pick & Place task, the robot must clear all objects on the table except for animals within 4 minutes for Clear all objects on the table except for animals. In the task Grasp the animals according to their distances to fruits, from nearest to farthest, the robot must sequentially grasp the animals in the correct order within 2 minutes, despite humaninduced distractions such as moving animals or fruits. Notably, this task is particularly challenging because the robot operates under an open-loop policy, preventing it from using closed-loop feedback to handle the dynamic distances between fruits and animals caused by external disturbances. Therefore, failure detection framework is necessary to enable both reactive and proactive real-time detection with high precision, monitoring the distance changes and adjusting the grasping sequence accordingly. C.4.2 Detailed Experiment Results In Tab. 10, we present detailed results of Simple Pick & Place. CaM achieves success rates surpassing DoReMi by 20.4% when handling different kinds of objects. We show 7 Table 10. Detailed Performance of Single Pick & Place. We report the success rate and execution time. DGN is DexGraspNet 2.0 [71]. Tasks with disturbance Pick & Place with the objects being moved during grasping Pick & Place with the objects being removed during movement"
        },
        {
            "title": "Object\nName\nToy Loopy\nToy Dog\nToy Rabbit\nBeverage Bottle\nGlass Cup\nShampoo Bottle\nApple Model\nPear Model\nPeach Model\nPlate\nBall\nPyramid\nToy Loopy\nToy Dog\nToy Rabbit\nBeverage Bottle\nGlass Cup\nShampoo Bottle\nApple Model\nPear Model\nPeach Model\nPlate\nBall\nPyramid",
            "content": "Success Rate(%) Execution Time(s) DGN +DoReMi 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 80.00 80.00 90.00 60.00 70.00 70.00 80.00 90.00 70.00 80.00 90.00 90.00 80.00 70.00 80.00 50.00 70.00 60.00 70.00 60.00 60.00 90.00 70.00 70.00 +Ours 100.00 100.00 90.00 100.00 90.00 90.00 100.00 90.00 90.00 100.00 100.00 90.00 90.00 100.00 90.00 90.00 90.00 90.00 90.00 100.00 90.00 100.00 100.00 90.00 +DoReMi 64.91 2.83 60.68 4.00 59.83 1.82 69.97 7.89 76.99 4.60 70.91 5.68 64.65 4.34 67.11 1.10 65.48 2.90 69.86 2.64 67.43 2.63 69.14 3.32 69.29 4.87 66.09 2.99 70.86 4.56 77.90 2.89 70.00 3.46 60 4.28 70.21 4.30 72.70 4.84 66.48 3.32 72.00 2.77 70.92 3.37 73.83 2. +Ours 46.02 3.11 47.06 3.24 45.77 2.03 47.61 2.58 48.32 3.22 48.31 3.08 45.39 0.71 45.48 1.01 45.37 0.64 45.18 0.65 45.37 0.70 45.42 0.72 60.86 3.41 63.12 3.75 63.40 3.88 61.97 3.90 63.22 4.35 63.00 3.81 63.71 3.91 58.61 2.41 59.19 2.59 59.21 2.61 61.57 3.80 60.25 3.25 real-world demonstrations of Simple Pick & Place and Reasoning Pick & Place in Sec. E.3. D. Limitations and Future Work Despite promising results, our approach has several limitations. First, using Visual Language Models (VLMs)such as off-the-shelf GPT for code generation and constraintaware segmentation modelsinevitably leads to hallucination issues. Even with minimized VLM usage, inaccuracies in code generation and biases in segmentation may persist. Integrating more relevant knowledge via methods like Retrieval-Augmented Generation (RAG) [31] or reducing multimodal large language model (MLLM) hallucinations [43] could alleviate these problems. Second, while we unify reactive and proactive failure detection as spatio-temporal constraint satisfaction problems and propose constraint elements for simplified real-time, highprecision monitoring, the constraint element representation (1) It primarily focuses on failures dehas limitations: tectable through explicit displacement and rotation, rendering it less effective for force-direction-related failures without noticeable displacementfor example, robotic gripper failing to open drawer due to incorrect force application. (2) The simplified representation abstracts objects and minimizes irrelevant visual details for efficient failure detection but may overlook critical visual cues and multimodal inputs, such as flowing water or audible sounds from partially closed faucet, which are ignored in our current framework. Thus, exploring more robust representations that balance real-time precision with minimal information loss or integrate richer multimodal inputs is promising direction for future research. E. More Demonstrations and Prompts This section presents additional demonstrations, including simulations and real-world scenarios of failure detection and recovery, as well as constraint-aware segmentation results and prompts. E.1. CLIPort Here, we present demonstrations of three tasks in CLIPort: Stack in Order, Sweep Half the Blocks, and Use Rope to Close the Opening Square. Fig. 12 demonstrates how our framework detects failures and assists in recovery when the placement positions predicted by the policy for the Stack in Order task are subject to uniform [0, q] cm interference. Fig. 13 illustrates how our framework performs failure detection and aids in recovery when, in the Stack in Order task, there is probability that blocks will fall due to being released by the robots suction cup at each step. Fig. 14 shows the Sweep Half the Blocks task, where our framework precisely counts the blocks within specified area and timely halts the policy execution to complete the task. In contrast, DoReMi [16] fails to stop the policy execution in time, leading to task failure. Fig. 15 depicts the Use Rope to Close the Opening Square task. Our framework effectively detects when the rope closes the opening square and promptly stops the policy execution to complete the task successfully. Conversely, DoReMi fails to halt the policy execution on time; although it eventually succeeds in closing the opening, the excessive execution time results in task failure. E.2. OmniGibson As shown in Fig. 16, Fig. 17 and Fig. 18, we show how our framework detects failures and assists in recovery when facing point-, lineand surface -level disturbances. E.3. Real-world Evaluation Fig. 19 demonstrates the task Clear all objects on the table except for animals, where our framework achieves both reactive failure detection (e.g., detecting unexpected failures when humans remove objects from the robots grasp) and proactive failure detection (e.g., identifying target object movement during grasping to prevent foreseeable failures). This effectively enhances the task success rate and reduces the execution time. E.4. Constraint-aware segmentation As shown in Fig. 20, Fig. 21, Fig. 22, and Fig. 23 we present additional results on constraint-aware segmentation, including instance and part-level results. To demonstrate generalizability, we utilize out-of-distribution (OOD) data, including the RoboFail Dataset from REFLECT [41], datasets from the Open6DOF benchmark [11], and the RT-1 dataset [3]. Additionally, we showcase segmentation results from the OmniGibson. E.5. Prompts As illustrated in Fig. 24, we detail the prompt used to invoke an off-the-shelf VLM, i.e., GPT-4o [1], to generate Python code for monitoring. Figure 12. Demonstration of Stack in Order. We show how our framework detects failures and assists in recovery when the placement positions predicted by the policy for the Stack in Order task are subject to uniform [0, q] cm interference. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution. 10 Figure 13. Demonstration of Stack in Order. We show how our framework performs failure detection and aids in recovery when, in the Stack in Order task, there is probability that blocks will fall due to being released by the robots suction cup at each step. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution. 11 Figure 14. Demonstration of Sweep Half the Blocks and comparison to baseline. We show our framework can precisely count the blocks within specified area and timely halts the policy execution to complete the task. In contrast, DoReMi [16] fails to stop the policy execution in time, leading to task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution. Figure 15. Demonstration of Use Rope to Close the Opening Square and comparison to baseline. We show that our framework effectively detects when the rope closes the opening square and promptly stops the policy execution to complete the task successfully. Conversely, DoReMi fails to halt the policy execution on time; although it eventually succeeds in closing the opening, the excessive execution time results in task failure. Red boxes indicate the occurrence of failures, while green boxes signify successful task execution. 13 Figure 16. Demonstration of Slot Pen. We show how our framework detects failures and assists in recovery when facing point-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution. 14 Figure 17. Demonstration of Stow Book. We show how our framework detects failures and assists in recovery when facing line-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution. Figure 18. Demonstration of Pour Tea. We show how our framework detects failures and assists in recovery when facing surface-level disturbances. Red boxes indicate the occurrence of failures, light green indicates the recovery with subgoal success and dark green boxes signify successful task execution. 16 Figure 19. Demonstration of Clear all objects on the table except for animals. We show that our framework achieves both reactive failure detection (e.g., detecting unexpected failures when humans remove objects from the robots grasp) and proactive failure detection (e.g., identifying target object movement during grasping to prevent foreseeable failures). This effectively enhances the task success rate and reduces the execution time. 17 Figure 20. Visualization of constraint-aware segmentation for the RoboFail Dataset [41]. This dataset is not included in the training data. Figure 21. Visualization of constraint-aware segmentation for the Open6DOF [11]. This dataset is not included in the training data. 19 Figure 22. Visualization of constraint-aware segmentation for the RT-1 dataset [3]. This dataset is not included in the training data. 20 Figure 23. Visualization of constraint-aware segmentation for the Omnigibsom simulator. Figure 24. Prompt of monitor code generation. We use this prompt, combined with additional task instructions, the current subgoal, and images from two perspectives, to enable an off-the-shelf VLM, i.e., GPT-4o [1], to generate Python code for monitoring."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "School of Computer Science, Peking University",
        "School of Software, Beihang University"
    ]
}