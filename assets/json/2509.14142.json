{
    "paper_title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook",
    "authors": [
        "Peng Xu",
        "Shengwu Xiong",
        "Jiajun Zhang",
        "Yaxiong Chen",
        "Bowen Zhou",
        "Chen Change Loy",
        "David A. Clifton",
        "Kyoung Mu Lee",
        "Luc Van Gool",
        "Ruiming He",
        "Ruilin Yao",
        "Xinwei Long",
        "Jirui Huang",
        "Kai Tian",
        "Sa Yang",
        "Yihua Shao",
        "Jin Feng",
        "Yue Zhong",
        "Jiakai Zhou",
        "Cheng Tang",
        "Tianyu Zou",
        "Yifang Zhang",
        "Junming Liang",
        "Guoyou Li",
        "Zhaoxiang Wang",
        "Qiang Zhou",
        "Yichen Zhao",
        "Shili Xiong",
        "Hyeongjin Nam",
        "Jaerin Lee",
        "Jaeyoung Chung",
        "JoonKyu Park",
        "Junghun Oh",
        "Kanggeon Lee",
        "Wooseok Lee",
        "Juneyoung Ro",
        "Turghun Osman",
        "Can Hu",
        "Chaoyang Liao",
        "Cheng Chen",
        "Chengcheng Han",
        "Chenhao Qiu",
        "Chong Peng",
        "Cong Xu",
        "Dailin Li",
        "Feiyu Wang",
        "Feng Gao",
        "Guibo Zhu",
        "Guopeng Tang",
        "Haibo Lu",
        "Han Fang",
        "Han Qi",
        "Hanxiao Wu",
        "Haobo Cheng",
        "Hongbo Sun",
        "Hongyao Chen",
        "Huayong Hu",
        "Hui Li",
        "Jiaheng Ma",
        "Jiang Yu",
        "Jianing Wang",
        "Jie Yang",
        "Jing He",
        "Jinglin Zhou",
        "Jingxuan Li",
        "Josef Kittler",
        "Lihao Zheng",
        "Linnan Zhao",
        "Mengxi Jia",
        "Muyang Yan",
        "Nguyen Thanh Thien",
        "Pu Luo",
        "Qi Li",
        "Shien Song",
        "Shijie Dong",
        "Shuai Shao",
        "Shutao Li",
        "Taofeng Xue",
        "Tianyang Xu",
        "Tianyi Gao",
        "Tingting Li",
        "Wei Zhang",
        "Weiyang Su",
        "Xiaodong Dong",
        "Xiao-Jun Wu",
        "Xiaopeng Zhou",
        "Xin Chen",
        "Xin Wei",
        "Xinyi You",
        "Xudong Kang",
        "Xujie Zhou",
        "Xusheng Liu",
        "Yanan Wang",
        "Yanbin Huang",
        "Yang Liu",
        "Yang Yang",
        "Yanglin Deng",
        "Yashu Kang",
        "Ye Yuan",
        "Yi Wen",
        "Yicen Tian",
        "Yilin Tao",
        "Yin Tang",
        "Yipeng Lin",
        "Yiqing Wang",
        "Yiting Xi",
        "Yongkang Yu",
        "Yumei Li",
        "Yuxin Qin",
        "Yuying Chen",
        "Yuzhe Cen",
        "Zhaofan Zou",
        "Zhaohong Liu",
        "Zhehao Shen",
        "Zhenglin Du",
        "Zhengyang Li",
        "Zhenni Huang",
        "Zhenwei Shao",
        "Zhilong Song",
        "Zhiyong Feng",
        "Zhiyu Wang",
        "Zhou Yu",
        "Ziang Li",
        "Zihan Zhai",
        "Zijian Zhang",
        "Ziyang Peng",
        "Ziyun Xiao",
        "Zongshu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 2 4 1 4 1 . 9 0 5 2 : r MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook Peng Xu Shengwu Xiong Jiajun Zhang Yaxiong Chen Bowen Zhou Chen Change Loy David A. Clifton Kyoung Mu Lee Luc Van Gool Ruiming Hea,b,c Ruilin Yaoa,b,c Xinwei Longa,b,c Jirui Huanga,b Kai Tianb,c,d Sa Yangb,c,d Yihua Shaob,c"
        },
        {
            "title": "Yue Zhonga",
            "content": "Jiakai Zhoua,c Cheng Tanga,c Tianyu Zoub,c Yifang Zhangb,c Junming Liangb,c Guoyou Lib,c Zhaoxiang Wangb,c"
        },
        {
            "title": "Xiaodong Dongd",
            "content": "Xiao-Jun Wud"
        },
        {
            "title": "Ziyang Pengd",
            "content": "Ziyun Xiaod Zongshu Lid competition organizers; steering committee; organizing contributors; dataset contributors; baseline implementors; competition participants; The authors teams and affiliations are in the appendix. ICCV 2025 MARS2 workshop & challenge website: https://mars2workshop.github.io/iccv2025/ MARS2 GitHub organization: https://github.com/mars2workshop"
        },
        {
            "title": "Abstract",
            "content": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this years MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VGRS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/ mars2workshop/, where our updates and announcements of upcoming events will be continuously provided. 1. Introduction Large language models (LLMs) [91, 93] represent major advance and may mark crucial step towards Artificial General Intelligence (AGI). LLMs exhibit various characteristics that push the envelope of traditional deep learning, including generalization and emergent abilities [9], chainof-thought (CoT) [122], instruction understanding and following [166], tool calling [102], etc. Specifically, LLMs equipped with long chain reasoning capabilities such as OpenAI o1 [49] and DeepSeek-R1 [41] have opened up new avenue of large reasoning models. Furthermore, multimodal large language models (MLLMs) have expanded the application boundaries of language models and further advanced multimodal machine learning [132, 133]. Given the remarkable progress of LLMs and multimodal machine learning, natural question for researchers is what to focus on next. Simply deploying LLMs on tasks that conventional deep learning already solves well is unsurprising and adds little novelty. Instead, we should pursue more challenging [10] and specialized [106] tasks that require complex multimodal reasoning and slower, System 2 thinking [32], borrowing from Kahnemans dual-process view of mind [56]). Thus, we have organized the MARS2 Workshop and Challenge Multimodal Reasoning and Slow Thinking in the Large Model Era: Towards System 2 and Beyond. The goal is to bring together perspectives from multiple disciplines and to help researchers follow the stateof-the-art in both multimodal machine learning and multimodal large language models. Motivation and objectives. Domain-specific problems highlight the challenge of equipping general-purpose models with specialized expertise. Accordingly, this years MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. In particular, we explore the following challenges: Synergistic effects among reasoning tasks. Existing reasoning benchmarks fail to guarantee that different task samples come from the same data distribution, thus falling short of evaluating the synergistic effects among the different reasoning tasks [142]. Non-stepwise complex reasoning. It has been observed [39, 152] that LLMs already work well for stepwise problems such as math and programming that are based on explicit theorems and programming syntax, compatible with the chain-style reasoning approach if A, then B. To further advance LLM reasoning, we should study complex reasoning that goes beyond step-wise approaches. Therefore, we released two tailored datasets Lens [142] and AdsQA [84] as our competition test sets to draw the attention of the community to these two issues. For details, we refer to Section 2.1. Overview and features. This challenge was organized in conjunction with the ICCV 2025 MARS2 Workshop Multimodal Reasoning and Slow Thinking in the Large Model Era: Towards System 2 and Beyond. Our committee organized team consisting of 80+ data contributors and baseline model testers. Two large-scale and tailored datasets Lens [142] and AdsQA [84] were released as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated over 40 baselines that include both generalist MLLMs and task-specific models, covering open-source and commercial models. This year, we provided three competition tracks: Track #1 Visual Grounding in Real-world Scenarios (VG-RS), Track #2 Visual Question Answering with Spatial Awareness (VQASA), and Track #3 Visual Reasoning in Creative Advertisement Videos (VR-Ads). The challenge was divided into two stages and lasted for more than two months. 76 teams from renowned academic and industrial institutions (e.g., ByteDance, Meituan, NVIDIA, and Samsung) have registered and submitted 1200+ entries in total, of which 40+ valid submissions have been included in our final ranking lists. Our datasets, codes (40+ baselines and 15+ participants methods), and rankings are publicly available at the MARS2 workshop website and our GitHub repositories. Thanks to the significant efforts of all contributors and participants, the MARS2 workshop met our expectations. This years challenge boasts: Tailored datasets. Our datasets Lens and AdsQA come with distinct characteristics. For example, the samples of Lens were manually collected from social media, with 53% posted after January 2025. Each sample supports Track #1 and Track #2, enabling the study of synergistic effects of multimodal reasoning tasks. On the other hand, AdsQA is challenging advertisement Video QA test set, and to the best of our knowledge, is the first attempt to use ad videos for tasks that evaluate LLMs. For more details of Lens and AdsQA, the reader is referred to Section 2.1. Comprehensive benchmarks. Our contributors evaluated 40+ baselines involving both generalist MLLMs and task-specific models. The participants contributed 40+ valid solutions, by using not only open-source models like Qwen [7] but also some commercial LLMs like TeleMM1. The sizes of the baselines and the participants models range from 3B to 72B. This benchmark provides comprehensive comparison covering generalist models vs. specialist models, large models vs. small models, open-source models vs. closed-source models, etc. Open-source reproducibility. Our committee has set up GitHub organization https://github.com/ mars2workshop and dedicated webpage https:// mars2workshop.github.io/iccv2025/, where our datasets and codes (40+ baselines and 15+ participants methods) are publicly available. This should ensure reproducibility. Post-competition discussion. After the submission deadline, the MARS2 committee has initiated multiple rounds of discussion, inviting additional contributors and covering interesting issues like hard samples, corIn particular, we have rener cases, and valid tricks. opened submissions to allow participants to conduct ablation studies. Organization of this report. The rest of this report is organized as follows. Section 2 provides the overview of the MARS2 2025 challenge, introducing tailored datasets (Section 2.1), competition tracks (Section 2.2), evaluation protocols (Section 2.3), and other details (Section 2.4). Section 3 presents the solutions and baselines, including methods, details, and rankings. In Section 4, we discuss open problems and potential research directions before this report concludes in Section 5. 1https://www.teleai.com/ 2. Challenge 2.1. Tailored Datasets Based on the motivation introduced in Section 1, our committee collected two large-scale multimodal datasets Lens [142] and AdsQA [84] from real-world scenarios to explore the synergistic effects among the reasoning tasks and non-stepwise complex reasoning, respectively. 2.1.1. Lens Dataset Statistics. Lens is created to support the multi-level evaluation of multimodal reasoning. It is multi-level benchmark that provides three progressive task tiers, i.e., perception, understanding, and reasoning. It has 3.4K images and 60K+ human-authored questions covering eight tasks (e.g., described object counting, region-wise OCR, and scene knowledge inference) and 12 daily scenarios (e.g., streets, stations, schools, and homes). Features. Lens has three main features. (1) Each image is annotated with rich text for all eight tasks. Thus, Lens supports to evaluate MLLMs to handle image-invariable prompts, from basic perception to compositional reasoning. This dataset provides the comprehensive evaluations that help the study of the synergistic effects of different reasoning tasks. (2) The images of Lens are collected from the social media, in which 53% are published later than January 2025. This feature ensures that when evaluating some recently released models, the influence of the models inherent knowledge on the reasoning performance can be minimized as much as possible. (3) The diversity of object categories, scene types, multi-scale images, and bounding box annotations further facilitates numerous downstream tasks. See word cloud in Figure 1. Difficulty. As reported in [142], we evaluate 20+ frontier MLLMs (e.g., Qwen2.5-VL-72B [8], InternVL378B [168], Deepseek-VL2 [124], Gemma3 [117], GPT4o [1], and Gemini2.5 Pro [116]) and two reasoning models (i.e., QVQ-72B-preview and Kimi-VL) that are released later than December 2024. None of them achieve an accuracy greater than 60% in the reasoning tasks of Lens. Specifically, samples from the Lens are particularly challenging for the visual grounding task. This difficulty is attributed to combination of factors, including complex and variable queries, wide range of image resolutions, and large number of small ground-truth bounding boxes, etc. As demonstrated in Table 2, even Qwen2.5-VL-32B only achieves 48.47% for Acc.@0.5. 2.1.2. AdsQA Dataset Statistics. AdsQA is an advertising video question sourced from 1,544 advertisement answering dataset Figure 1. word cloud illustrating the textual content of the Lens [142] dataset. The frequent scene-specific terms suggest diversity of daily scenarios, while directional terms (e.g., turn right) highlight its capacity for evaluating spatial reasoning. Figure 2. word cloud of the ground truth answers of AdsQA [84], where the domain-specific words such as narrative, engagement, and storytelling showcase the specialized reasoning in the advertising videos. videos, providing 10,962 clips totaling 22.7 hours. Compared with image-based understanding tasks, VideoQA requires more computational resources. To enable the participants to rapidly update their solutions, we thus sampled subset from the AdsQA dataset to form the test set for Track #3, consisting of 555 videos and over 3,000 questions. AdsQA comprises five open-ended QA tasks, each requiring distinct reasoning skills. Visual concept understanding: identifying and analyzing ad visuals. Emotion recognition: detecting emotions and inferring character roles. Theme and core message extraction: summarizing central theme and key messages. Persuasion strategy mining: strategies. analyzing ad persuasion Potential audience modeling: identifying and characterizing target audience. Features. Ad videos are characterized by their domainspecific traits of being clue-rich and information-dense, owing to their inherent marketing logic, persuasive strategies, audience engagement, etc. Thus, Lens serves to assess the capability of MLLMs to perceive beyond the objective physical content of conventional visual domains. Furthermore, understanding ad videos requires complex and specialized reasoning that is not compatible with the chainstyle reasoning approach if A, then B. Lens brings novel domain that further extends the diversity of specialized reasoning in LLMs. Figure 2 illustrates the domain-specific words such as narrative, engagement, and storytelling. To our knowledge, AdsQA is the first video QA benchmark for advertisement domain, which is also the first ad benchmark for LLMs, with domain-unique features implicit, non-physical, mental, heuristic, etc. It presents new challenge to the current mainstream if A, then LLM thinking approach, hence further extending the domain specialization reasoning scenarios for LLMs. Moreover, it advances ad video understanding beyond physical-content dominated shallow perception towards deeper cognitive reasoning. Difficulty. Subtle visual details in ads pose significant challenge for models, which often overlook them and fail to establish the connection between these elements, the videos theme, and the underlying creative intent. We evaluated over 14 models, such as GPT-4o, Gemini 2.5 Pro, Qwen2.5-VL series, and LLaVA series [65, 158], as reported in [84]. Our evaluations reveal that even the stateof-the-art Gemini 2.5 Pro attained mere 60.7% accuracy. We have also conducted human evaluation by recruiting five non-expert evaluators who assessed the same randomly sampled subset (200 QA pairs). Each evaluator was required to answer the questions based solely on the provided videos, without any additional background information. The evaluators achieved an average accuracy of 71.4%, demonstrating clear gap between the state-of-theart MLLMs and human performance. 2.2. Competition Tracks As stated in Section 1, through the challenge of this year, we would like to make efforts to explore two challenging and understudied issues, i.e., synergistic effects among reasoning tasks and non-stepwise complex reasoning. Following this motivation, we define three competition tracks, where Track #1 and Track #2 share the same samples of Lens dataset to study the synergistic effects among reasoning tasks on the shared data distribution. AdsQA dataset supports Track #3 to evaluate non-stepwise complex reasoning in advertising. In particular, the three tracks are defined as open-ended QA tasks. Track #1 Visual Grounding in Real-world Scenarios (VG-RS) evaluates the models scene perception, object localization, and spatial reasoning abilities in complex scenarios. Track #2 Visual Question Answering with Spatial Awareness (VQA-SA) assesses how well the model performs spatial, commonsense, and counterfactual reasoning based on concrete physical content following user instructions. Track #3 Visual Reasoning in Creative Advertisement Videos (VR-Ads) probes the models cognitive reasoning abilities in understanding implicit, non-physical, and abstract visual concepts in advertisement videos. 2.3. Evaluation Protocols All our tracks take quantitative measures, and the higher score indicates the better outcome. The participants submissions are evaluated on the open source platform EvalAI. The evaluation protocols are as the follows. Track #1 VG-RS: To ensure the consistency with existing evaluation protocols, we follow the standard Acc.@0.5 [26, 28, 141] to evaluate the detection performance for the visual grounding task. Specifically, given an input, the predicted bounding box is considered as correct only if the Intersection-over-Union (IoU) [74, 140, 153] between the prediction and the ground truth exceeds 0.5. Track #2 VQA-SA: We follow the prior work [15] and employ large language model GLM4-flash2 as the automatic evaluator. For each predicted result and human annotation pair, our evaluation model generates multiple candidate responses and determines the correctness through majority voting. The final accuracy is calculated based on the number of correct predictions identified by the evaluator. Track #3 VR-Ads: Following the recent practice [84], we employ gpt-4o-2024-08-06 to assist in evaluating text similarity between prediction and ground truth. We provide each sample with the ground truth annotation and metainformation, which includes relevant details such as creative elements, storyline, and themes in the advertising video. Moreover, we apply inclusion and exclusion rules to guide the scoring of text generated by the large language model. The inclusion rule requires the generated answer to incorporate as many elements of the ground truth as possible. score of 1.0 is awarded for fully satisfactory answer, 0.5 for partially matched one, and 0.0 otherwise. The exclusion rule specifies that if the generated content includes elements that are not present in the ground truth and these elements fail to be inferred from the meta-information, then the content will be scored as 0. We adopt the average score as the final evaluation metric. 2.4. Terms, Phases, and Other Details This challenge began on June 1st and ended on August 6th. Once the participants complete the registration, they will immediately obtain the test data for the competition tracks they are participating in and can start the submissions to test their solutions. All competition tracks use the EvalAI evaluation server3 to provide immediate feedbacks to the participants. To offer participants the greatest freedom for innovation, the MARS2 2025 committee did not set any additional rules. Both open-source and commercial models are allowed. Meanwhile, we have no restrictions on the size of the model. Based on the access rules for the leaderboard, it can be divided into two stages. Stage #1: completely public (01/06/2025 - 02/08/2025). To fully motivate the participants, the committee makes the real-time leaderboard public every day, including private submissions. In this stage, we received 900 submissions. Stage #2: priviate allowed (03/08/2025 - 06/08/2025). In the final stage, we discontinued the public leaderboard to increase competitive suspense and tension. 300 submissions are received in Stage #2. Finally, we received over 1200 submissions. 3. Methods and Results Overview. The MARS2 2025 Challenge has received widespread attention that 76 teams from the renowned academic and industrial institutions have registered and submitted 1200+ entries in total. The MARS2 committee has performed 40+ state-of-the-art models as the baselines including both generalist MLLMs and task-oriented specialist models. After the submission deadline, 40+ valid submissions have been included in our final ranking lists, in which few teams contribute the commercial closed-source model based solutions. The final scoring results are reported in Table 1. Finally, we have organized large and comprehensive benchmark by combining our baselines and the participants solutions, where the model sizes range from 3B to 72B. This benchmark provides the comprehensive comparison covering generalist models vs. specialist models, large models vs. small models, open-source models vs. closed-source models, etc. Baselines. To offer the participants some reference and make our benchmark more comprehensive and diverse, the MARS2 committee provides 40+ results as baselines, by using the state-of-the-art MLLMs such as Qwen2.5-VL and InternVL3. In addition to generalist models, we further assessed more than 20 state-of-the-art specialist models proposed for Visual Grounding (VG) and Visual Question An3https://eval.ai/web/challenges/challenge-page/ 2https://github.com/zai-org/GLM-4 2552/overview Track #1 Visual Grounding in Real-world Scenarios (VG-RS) on Lens Rank Team Method Star 1 ActiveAlphaAgent 2 3 4 5 BL BL BL Location depends on guessing SRCN-AIVL SUP MARS2 MARS2 MARS2 Qwen2.5-VL-Distilled Qwen2.5-VL & G-Dino closed-source ensemble ensemble Qwen2.5-VL Qwen2.5-VL Qwen2.5-VL Accuracy Verified Generalist - 66.70 64.83 64.30 63.01 61.90 48.47 46.94 45.03 - - Size Other Details 7B 72B training-free, multi-scale input, prompt engineering GRPO training, multi-turn tool usage - - - 32B 7B 3B GRPO training, prompt engineering ensemble methods, GRPO training ensemble methods, scene knowledge enhancement state-of-the-art open-source model state-of-the-art open-source model state-of-the-art open-source model Track #2 Visual Question Answering with Spatial Awareness (VQA-SA) on Lens Rank Team Method Echoch Tele AI ensemble closed-source 1 2 3 ActiveAlphaAgent Qwen2.5-VL-Distilled 4 5 BL BL BL o3-2025-04-16 ensemble Qwen2.5-VL GLM-4.1V-Thinking InternVL3 MILVLG HDU SRCN-AIVL MARS2 MARS2 MARS Accuracy Verified Generalist - 79.03 72.60 69.72 62.20 54.60 54.10 51.32 49.98 Size - - 7B - - 32B 9B 38B Other Details ensemble methods, distillation, majority voting GRPO training GRPO training, multi-turn tool usage prompt engineering ensemble methods, scene knowledge enhancement state-of-the-art open-source model open-source reasoning model state-of-the-art open-source model Track #3 Visual Reasoning in Creative Advertisement Videos (VR-Ads) on AdsQA Rank Team Method gogogo truefaler HNU-VPAI 1 2 3 ActiveAlphaAgent 4 5 BL BL BL rookiesllm mm618 MARS2 MARS2 MARS2 Qwen2.5-VL Qwen2.5-VL Qwen2.5-VL Qwen2.5-VL GLM-4.1V-Thinking Qwen2-VL MiniCPM-o Qwen2.5-VL Accuracy Verified Generalist 56.35 54.62 53.13 52.77 52.74 41.41 43.23 48.04 Size 72B 72B 72B 72B 9B 7B 7B 7B Other Details feature engineering, CoT reasoning prompt engineering feature engineering, CoT reasoning feature engineering SFT, multi-hop reasoning state-of-the-art open-source model state-of-the-art open-source model state-of-the-art open-source model Table 1. Final leaderboard of MARS2 2025. Only the Top5 teams of each track evaluated by the MARS2 committee are listed. See our website for full ranking lists. BL: baseline methods implemented by the MARS2 committee. Generalist: generalist model. Verified: the method has been verified by the committee. Lens and AdsQA can be downloaded from our GitHub organization page https://github.com/mars2workshop. swering (VQA). These models are included in our benchmarks, as summarized in Table 2 and Table 3. Observations and discussion. The committee has carefully reviewed the solutions and code implementations. Our observations are summarized as follows. (1) Ideas. Given the current prosperity of LLMs, some shared ideas are summarized from the solutions, which mainly involve ensemble, data augmentation, prompt engineering, alignment training, etc. To tackle complex reasoning tasks, most teams employ leading MLLMs as their base models and perform multi-step alignment through supervised fine-tuning (SFT) and reinforcement learning (RL). Due to the recent success of the DeepSeek models, Group Relative Policy Optimization (GRPO) [104] is widely used by the teams, e.g., ActiveAlphaAgent, SRCN-AIVL, and Tele AI. (2) Data augmentation. In the participants solutions, domain-oriented data augmentation brings clear gains over the generalist base model (e.g., Location depends on guessing Team), demonstrating the pattern novelty of our datasets Lens and AdsQA for the general-purpose models. (3) Reinforcement learning. RL training works well for the complex and specialized reasoning as kind of useful and promising alignment technique. The teams use the RL algorithm of GRPO for the post-SFT alignment training, achieving clear performance improvement. (4) Prompt engineering. Various prompts are designed carefully by the teams (e.g., Star and HNU-VPAI), with goal of using the detailed instructions to activate the reasoning of MLLMs. As evidenced by their prompt details, the samples of both Lens and AdsQA exhibit complex patterns and high density of information. (5) Model collaboration. The model collaboration is promising direction, e.g., generalist and specialist collaboration, and this kind of solution achieves good performance in this challenge. For instance, the team Star employs Grounding DINO [77] as task expert to verify and filter the bboxes generated by Qwen2.5-VL [8], earning second place in Track #1. Method Visual Backbone Linguistic Backbone @0.5 @0.6 @0.7 @0.8 @0.9 ACCs ACCm ACCl Scale-wise Accuracy Accuracy @ IoU Methods based on predictive multimodal models: TransVG [28] VLTVG [137] CLIP-VG [127] MMCA [141] EEVG [20] SimVG [26] G-DINO [77] RN101 RN101 CLIP-B RN101 ViT-B/16 BEIT-3 Swin-L BERT-B BERT-B CLIP-B BERT-B BERT-B BEIT-3 BERT-B 8.73 11.04 8.73 10.92 9.27 16.46 37. 7.57 9.60 7.57 9.45 5.78 13.90 33.92 Methods based on generative multimodal large language models: Groma-7B [87] Mova-7B [170] Ferret-7B [145] Ferret-13B [145] InternVL3-2B [168] InternVL3-8B [168] InternVL3-14B [168] InternVL3-38B [168] LLM-wrapper [12] VLM-R1-3B [105] Qwen2.5-VL-3B [8] Qwen2.5-VL-7B [8] Qwen2.5-VL-32B [8] DINOv2-L Multi-expert CLIP-L CLIP-L InternViT-0.3B InternViT-0.3B InternViT-0.3B InternViT-6B Florence-2-L FE-ViT FE-ViT FE-ViT FE-ViT Methods in MARS2 competition: ActiveAlphaAgent Star Location depends on guessing SRCN-AIVL SUP - - - - - Vicuna Vicuna Vicuna Vicuna Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Llama-3 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 - - - - - 33.59 20.44 23.26 24.20 7.89 17.54 29.53 27.85 42.24 23.79 45.03 46.94 48.47 66.71 64.83 64.30 63.01 61.90 29.95 13.10 18.97 19.81 5.10 13.23 23.98 21.42 38.78 19.91 37.92 39.39 40. 59.18 57.74 56.02 56.83 55.84 6.29 7.75 6.29 7.90 2.51 11.12 29.20 25.47 5.98 13.95 14.41 2.85 8.89 17.25 15.00 33.64 15.65 29.33 29.94 30.78 48.07 47.76 44.65 47.85 47.53 4.40 5.33 4.40 5.64 0.48 7.44 22.57 18.73 1.09 7.61 8.12 1.36 4.94 10.05 8.23 26.10 10.53 18.48 18.38 19. 31.93 34.43 29.76 35.14 34.74 1.69 1.99 1.69 2.22 0.05 2.70 11.36 8.52 0.13 1.84 2.05 0.33 1.60 3.00 2.37 13.00 4.31 6.51 6.26 6.63 12.28 15.76 11.87 16.59 15.97 0.01 0.00 0.01 0.03 0.01 0.01 24.87 11.58 5.06 1.85 2.26 0.61 3.23 4.58 4.91 21.99 8.15 29.14 28.87 30. 54.31 49.86 50.99 47.57 44.82 Table 2. comprehensive benchmark for MARS2 2025 Track #1. 2.01 2.80 2.01 2.79 0.98 3.10 37.54 33.91 15.97 19.49 20.42 3.46 15.36 27.80 25.81 48.38 22.84 50.54 54.12 55.04 68.07 67.73 67.35 66.22 64.79 23.64 29.70 23.64 29.31 26.12 45.20 52. 58.59 40.36 54.64 56.31 19.34 35.21 57.07 53.56 58.39 40.94 57.15 60.24 61.48 78.48 78.05 75.57 76.37 77.23 Method Visual Backbone Linguistic Backbone 640 Accuracy @ Input Resolution 960 960 1280 1280 1600 1600 InternVL3-2B [168] InternVL3-9B [168] InternVL3-14B [168] InternVL3-38B [168] Qwen2.5-VL-3B [8] Qwen2.5-VL-7B [8] Qwen2.5-VL-32B [8] GLM-4.1V-Base-9B [44] GLM-4.1V-Thinking-9B [44] InternViT-0.3B InternViT-0.3B InternViT-0.3B InternViT-6B FE-ViT FE-ViT FE-ViT AlMv2-Huge AlMv2-Huge Qwen2.5 InternLM3-8B Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 Qwen2.5 GLM-4-0414 GLM-440.97 46.95 50.28 50.88 40.08 46.52 53.72 42.35 48.77 41.53 46.54 51.15 50.97 40.44 47.41 54.37 42.86 50.78 41.90 46.65 51.58 49.98 40.48 48.13 54.10 43.28 51.32 40.60 46.63 51.17 51.08 40.58 48.11 54.09 43.73 51.13 Table 3. Benchmark results across varying input resolutions for MARS2 2025 Track #2. (6) Performance. The results demonstrate that multimodal reasoning in complex scenarios and specialized domains is challenging, even using the strong LLMs as base models. For example, it is observed that in Table 1 the score of the winning solutions for VG-RS task does not exceed 70%. In Track #3, the best accuracy (56%) still has clear gap if compared with the human performance (70% [84]). These results highlight the challenging nature of reasoning on the Lens and AdsQA datasets. The top-performing methods for each track are described in the following parts. For further discussion, please refer to Section 4. 3.1. Track #1 Visual Grounding in Real-world Scenarios (VG-RS) 3.1.1. VG-SMART (submitted by ActiveAlphaAgent Team) The champion solution of Track #1 is multi-stage post-training Visual Grounding method: SNR-Driven Data Synthesis based Multi-Stage Alignment Combining Supervised Fine-Tuning and Reinforcement Training, termed VG-SMART. This method begins with synthesizing highquality dataset via ensembling the state-of-the-art models and using filtering process based on custom signal-tonoise ratio (SNR) metric. The key idea of this method is the multi-stage training process built upon the Qwen2.5-VL72B-Instruct model [8], which involves grounding-based supervised fine-tuning (SFT) followed by reinforcement learning (RL) stage with an IoU-centric reward function. The final step is distilling the knowledge from 72B model to Qwen2.5-VL-7B to optimize inference speed. Data synthesis and filtering. The data augmentation mainly includes two steps: (1) Synthesis of domain-specific data. They perform domain-based classification of the Track #1s test set, establishing four main categories. Transportation hubs: 20.0% Urban & life scenes: 21.0% Indoor spaces: 44% Educational & public institutions: 15% In addition to visual data analysis, fine-grained examination of the questions is conducted to understand the types of reasoning demanded by the test set. The queries are analyzed from multiple perspectives, including: object categories, object attributes, and spatial relationships. The results reveal that each domain poses distinct grounding challenges. For example, transportation and urban scenes frequently exhibit occlusion or contain minuscule objects. To capture these complex patterns, the team synthesizes dataset using an ensemble of the state-of-the-art MLLMs [8, 128, 168]. The approach leverages multi-model voting to identify high-confidence responses and employs difference analysis to create challenging and diverse samples. This process yields an initial dataset of 11k samples. (2) Data filtering by signal-to-noise ratio and pass rate. The synthesized dataset encompasses samples of varying difficulty (simple, moderate, and difficult) but may contain noise. To enhance its quality, signal-to-noise ratio (SNR) metric is defined as Accuracysynthetic/Accuracybase and employed for filtering. The accuracy values are derived from the committees online evaluation system. Only samples with an SNR greater than 1 are retained. To quantify the difficulty distribution of the refined samples, the team uses the Qwen2.5-VL-7B-Instruct [8] with temperature setting of 1.0 to run eight inference passes per sample. The pass rate@8 (the number of correct predictions out of eight attempts) serves as the metric to categorize each sample as easy, medium, or hard. As result, the difficulty distribution of the refined samples becomes balanced, forming high-quality training set that provides more comprehensive learning signals for the subsequent stages of SFT and RL. Supervised fine-tuning. The first stage is cold-start [41] supervised fine-tuning phase. The primary goal of this stage is to adapt the Qwen-VL-72B-Instruct model to the specific domains, question styles, and output formats of this competition track, meanwhile establishing robust baseline for the reinforcement learning phase. Thus, one necessary step is aligning the training samples with the Qwen-VL instruction format. For image preprocessing, they use the standard Qwen-VL-Utils, setting max pixels to 12802828 and applying smart resize transformation to both the images and their corresponding ground truth bounding boxes. The team employs full-parameter fine-tuning strategy for both the adapter and the large language model, while the ViT [30] backbone is frozen. Reinforcement learning. Motivated by [40, 149], the team deliberately selects only the medium and hard samples from their training set, as identified by the pass rate@8based analysis. They also use modified IoU reward function aimed at amplifying the feedback signal around the critical Intersection over Union (IoU) decision boundary of IoU=0.5. For predictions with an IoU greater than 0.5, non-linear function is applied to attenuate the reward difference, preventing the model from becoming overly conservative on already correct samples. Meanwhile, for predictions in the range of 0.3-0.5, the reward function magnifies the negative feedback. This approach encourages more aggressive learning from critical errors and significantly improves fine-grained decision-making. This team uses verl framework [107] and Group Relative Policy Optimization (GRPO) [104] with rollout parameter of 8. The model is trained for three epochs. For validation during training, they use custom metric defined as the pass rate for predictions achieving an IoU greater than 0.5. In addition to the aforementioned approach, this team reports that they explore thinking-based agent [165] that uses multi-turn tools, such as zoom-in and grid overlays. Distillation. To balance performance and efficiency, this team creates final model by distilling the knowledge from into Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct [160, 161]. Team ActiveAlphaAgent secured the first place on the Track #1s final leaderboard, with score of 0.6671. 3.1.2. DCM-VG (submitted by Star Team) This team proposes solution, Detector Consensus Guided Multi-query Visual Grounding (DCM-VG). The motivation is to combine the generalist model and the specialist model. Based on tailored prompting strategy, the generalist model Qwen2.5-VL [8] generates bounding box candidates, which are then verified and refined by the taskspecific model Grounding DINO [77] to select the optimal result. Prompting strategy. The team implements the following prompt to consolidate all detection targets within an image into single prompt, which is then fed into Qwen2.5-VL to stimulate the models ability to leverage inter-object associations. will provide you with an image and several questions. . . . . . . Return the accurate 2D bounding box coordinates for all main objects mentioned in the questions. . . . . . . If an object mentioned in question is not found in the image, set its bbox 2d value to [0, 0, 0, 0]. Despite using multi-target prompt, Qwen2.5-VL still regularly fails to identify small or obscured objects. Therefore, the team proposes an enhanced prompt for such failure cases. The enhanced prompt template is presented below. will provide you with an image and several questions. You need to note that these targets are usually obscured or small. You must select objects that strictly meet the textual description. . . . . . . Return the accurate 2D bounding box coordinates for all main objects mentioned in the questions. . . . . . . You must provide the most likely one bbox 2d. Moreover, Qwen2.5-VL is sensitive to variations in image scale. To improve its robustness, the team employs multi-resolution inference strategy, processing each image at multiple resolutions. The task expert model then selects the most confident results, leading to more consistent and reliable grounding performance. Selection strategy. The team employs Grounding DINO as task expert to verify and filter the candidate boxes generated by Qwen2.5-VL, solution which earned second place in Track #1 with final score of 0.65. 3.1.3. VR-VG (submitted by Location depends on guessing Team) This team implements multi-stage alignment solution involving data augmentation, SFT, and RL. The main idea is to design verifiable reward for visual grounding (VR-VG). To alleviate data scarcity, five-step automated pipeline is proposed, including global captioning, entity extraction, instance grounding, region refining, and question synthesis. Leveraging the strong tools such as Qwen-2.5-VL72B and Grounding DINO, the team generates large-scale synthetic dataset comprising 2.7 million diverse questionanswer pairs. RL with GRPO and verifiable rewards. The proposed model is designed to jointly leverage fine-grained local details (high-resolution) and global scene context (lowresolution) via the lightweight cross-attention. Following the practice of the two-stage training strategy, they first perform supervised fine-tuning on the synthetic dataset, and then apply reinforcement learning with verifiable rewards to further enhance grounding accuracy. The reward is weighted sum of Rf mt and Riou defined as follows. (1) Verifiable format reward Rf mt: binary validation signal that verifies whether the output strictly adheres to the specified schema: <think>...</think><answer>(x1, y1), (x2, y2)</answer>, ensuring both correct tag structure and proper coordinate tuple format. (2) Dense IoU reward Riou: continuous score that offers fine-grained spatial supervision proportional to the IoU between the predicted and ground truth boxes. This team reports that this combination of rewards improves both semantic correctness and spatial precision, producing an additional +3.2 mIoU. More details can be found in their code implementation. 3.1.4. RVG-CVME (submitted by SRCN-AIVL Team) This solution, FVG-CVME, is reinforced visual-spatial grounding model that integrates candidate voting and model ensembling. It employs two-stage training strategy (SFT + RL) combined with training-free ensemble techniques. Two-stage training. Before training-free ensembling, the team optimizes 3B base model by two-stage training of SFT and RL. (1) Cold-start supervised fine-tuning. The team observes that Lens dataset contains diverse scenarios like in/outdoor, city street, shopping mall, etc., in which there are more than one-third referring cases are person-related. Thus, this team uses HumanRef-CoT [53] as their SFT dataset. HumanRef-CoT is large-scale human-centric referring expression dataset designed for multi-instance human referring in natural scenes. The base model selected for this stage is Qwen2.5-VL-3B, with its vision encoder and connector frozen during SFT. (2) Reinforcement learning training. The team adopts the GRPO algorithm to conduct reinforcement learning training, aiming to boost the models performance. Prior to this, Qwen2.5 is utilized to extract all entity categories. For example, given referring expression relatively the middle socket, its entity category will be extracted as socket. Then, Grounding DINO extracts all candidate boxes related to socket in the image. The target is to drive the model to select the bbox to match the description. An option for the reward function is F1 score. Moreover, this team also expects that the model has zero-shot regression ability when the given candidate boxes fail to match the ground truth. Thus, they additionally add bbox IoU reward function working in conjunction with the F1 score. Ensemble. This team employs VLM-R1-3B [105], Qwen2.5-VL-32B, and Qwen2.5-VL-72B as their base models for result aggregation. They then apply weighted boxes fusion (WBF) [110] incorporating two domainspecific experts (OV-DINO [119] and LLMDet [34]) along with reasoning model (GLM-4.1V-Thinking [44]) as an assistant model to address unprocessed parts. 3.1.5. KEME (submitted by SUP Team) The team proposes method named KEME (knowledge enhancement with multistep ensemble), which consists of three-stage process. Input preprocessing. Both visual and textual modalities undergo preprocessing. To address the inherent resolution limitations of vision transformers, they employ two distinct resizing strategies: (a) resizing all images to fixed resolution of 11201120 pixels, and (b) resizing the longest edge of each input image to 1400 pixels to preserve the original aspect ratio and padding the shorter edge. To reduce semantic ambiguity in queries, the team employs bilingual prompting strategy. Specifically, each question is translated into Chinese, and both the original English version and its Chinese translation are provided to the model. This dual-language approach improves the robustness to phrasing variations and mitigates misgrounding caused by ambiguous expressions. Contextual reasoning enhancement. For each input image, Qwen2.5-VL-72B [8] is prompted to generate rich contextual information, including scene category, global For evcaption, and description of spatial relation. ery imagequestion pair, the model identifies the key subjects mentioned in the question, which are subsequently grounded to the corresponding regions using Florence2 [125]. This assembled context including scene tag, caption, and relevant bounding boxes is processed by Qwen2.5VL under varied prompting strategies to produce multiple outputs for subsequent stages. Model ensemble. This method employs Qwen2.5-VL as an evaluator to assess detector candidates such as FlorenceFor each query, 2, Grounding DINO, and LLMDet. Qwen2.5-VL evaluates the proposed bounding boxes and selects the best-matching candidate. If no suitable candidate is identified, deterministic fallback strategy is applied. 3.1.6. VLM-R1-based Visual Grounding Method (submitted by CV and RL Team) This team participated in Track #1, achieving 8th place. They use Qwen2.5-VL-7B [8] and RefCOCOg [146] as the base model and the training set, respectively. Within the VLM-R1 [105] framework, they implement the GRPO algorithm [104] by designing custom reward function. The models output is passed through regularization function to extract bounding box coordinates. The reward is determined by the Intersection over Union (IoU) between the predicted bounding box and the ground truth: reward of 1 is given if IoU 0.5, and 0 otherwise. 3.2. Track #2 Visual Question Answering with Spatial Awareness (VQA-SA) 3.2.1. RSVT (submitted by Echoch Team) The champion team of this track introduces twostage fine-tuning method, rejection sampling and viewpoint transformation, termed RSVT. In the first stage, bilingual cold-start corpus is built using multimodal benchmarks and high-quality translations for linguistic consistency. In the second stage, self-consistency generation and cross-view augmentation enrich multi-view samples, improving spatial reasoning generalization. Data preparation. The data preparation is divided into two stages, drawing from both publicly available datasets and synthetic data sources to ensure diversity. (1) First stage. Multiple multimodal spatial reasoning benchmarks (e.g., 3DSRBench [88], DORI-Benchmark [94], OpenSpaces [3], SpaceOm [2], Spatial MM CoT [109], SpatialMQA [76], and ViewSpatial-Bench [66]) are adopted to broaden the sample and task diversity. They use GPT-4o for high-quality translation and rewriting to ensure linguistic consistency. In addition, the VQASynth [3] toolkit generates around 10k synthetic spatial-relation QA pairs, which are combined with benchmark datasets to yield about 170k samples as the initial training set. (2) Second stage. Advanced data augmentation is applied to improve diversity. Multiple MLLMs (e.g., OpenAI o3 [95] and Gemini 2.5 Pro [25]) are used to incorporate scene and viewpoint cues. self-consistency strategy then generates multiple QA candidates for each sample and selects the most representative ones. Further expansion is achieved with open-source models (e.g., InternVL3-38B [168] and Qwen2.5-VL-32B [8]), alongside viewpoint transformation augmentation to enhance generalization in VQA-SA reasoning. Training details. This team trains three MLLMs, i.e., InternVL3-14B, InternVL3-38B, and Qwen2.5-VL-32B. (1) First stage. For cold start, the team fine-tunes the InternVL3-14B model on 170k bilingual samples using low-rank adaptation (LoRA) [45]. The model is trained at resolution of 448448 to capture essential spatial features. Owing to constraints in model size and training time, they train InternVL3-38B and Qwen2.5-VL-32B exclusively on the Chinese samples for only three epochs. (2) Second stage. The three models undergo additional training for five epochs using second-stage dataset, which is augmented via viewpoint transformation and selfconsistency strategies. In this stage, the image resolution is enhanced to 10241024 pixels to capture more fine-grained spatial details. Reasoning enhancement. Each model independently generates predictions, and the final output is determined via majority voting among the three models. To improve the recall of Chinese spatial expressions, post-processing heuristics are applied, for instance, mapping front-right to right-front or front-right side. For multi-turn dialogue scenarios, they append each subsequent question under the same image with the camera perspective established in the first turn. This approach ensures that the viewpoint remains consistent and unambiguous throughout the interaction. Tricks. The committee has observed that this team employs two effective strategies. First, they provide deliberately ambiguous responses, such as offering multiple possible answers to location-based questions. This increases the probability of matching the ground truth. Second, they present the same answer in various grammatical forms or phrasings, reducing false negatives in large-scale automated evaluation. 3.2.2. SMART (submitted by Tele AI Team) The team introduces the SMART framework, comprising modules for synthesis, model adaptation, reasoning, and testing. They construct multimodal reasoning and thinking dataset (MRTD) and employ two-stage training pipeline. In the final evaluation, the team secured second place with an accuracy score of 0.7261. Training data synthesis. The team designs an automated workflow for generating diverse spatial reasoning training samples as follows. (1) Spatial scene rendering. The team leverages Blenders programmable API to build multi-object scenes with predefined spatial relationships. An automated filtering process removes invalid cases, while collision and occlusion checks ensure visibility. The 3D coordinates and orientations of all objects are then recorded for subsequent use. (2) QA pair generation. The team constructs question templates across three difficulty levels, embedding object attributes such as category, color, and spatial relations. Answers are derived through geometric analysis of recorded object coordinates to capture relations like left of, frontback, or behind. An example of the hard question is illustrated below. In the picture, in which direction is the sky-blue road bicycle in relation to the red car? Please choose from front, back, left, right, above, below, front left, front right, left behind, right behind. Model fine-tuning. The team implements two-stage fine-tuning pipeline combining supervised learning and reinforcement learning. (1) Location-aware data preparation. The team employs Qwen2.5-VL-32B [8] to generate QA samples containing explicitly position-aware reasoning chains. The synthetic data is integrated with the GRiD-3D [63] and GQA [47] datasets in ratio of 2:1:1 and augmented with original QA pairs to enhance dataset diversity. An example of locationaware reasoning is provided below. Question: In which direction is the sky-blue road bicycle in relation to the red car? <think>The sky-blue bike is positioned at the bottom left of the image, ... To determine the direction of the blue bike relative to the car, we need to evaluate its location from the cars view. From the cars viewpoint, the sky-blue bike is in front right of it. </think> (2) Cold-start training and reinforcement learning. In the first stage, cold-start training strategy [41] is performed on their MRTD dataset, aiming to minimize the average negative log-likelihood over reasoning tokens and answer tokens. In the second stage, reinforcement learning is performed using the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) [147], in which reward signals are computed via normalized advantage functions to ensure stable optimization guidance. Inference strategy. The teams reasoning strategy centers on spatial relationship modeling and contextual dependency. They leverage GPT-4o [1] to refine contextdependent questions, enhance image resolution to retain fine-grained spatial cues, and generate multiple reasoning trajectories. process-level reward model, VisualPRM-8B [121], evaluates these trajectories to select the answer most consistent with semantic logic, thereby improving stability and reliability in spatial reasoning. 3.2.3. STAGES (submitted by ActiveAlphaAgent Team) Based on the analysis of the Track #2 dataset, the team proposes the synthetic training and adaptive graduated enhancement strategy (STAGES). The method first optimizes the prompts and then adopts three-stage training strategy that involves large-scale SFT on synthetic data, indomain SFT on high-confidence samples, and knowledge distillation for model efficiency. Prompt optimization. Based on preliminary analysis of the Track #2 dataset, the team optimizes prompts before training. key strategy is to enrich the input prompt for given question with the complete list of all VQASA and VG-RS questions that share the same image. This method results in an accuracy improvement of 27 percentage points in multiple models, with stronger reasoning models exhibiting greater gains. Further performance gains are achieved by customizing the prompts according to model type, image category, and question type. Three-stage training strategy. With the optimized prompts established, STAGES proceeds with three-stage training strategy, preparing distinct dataset for each stage. (1) Large-scale task-oriented SFT. This team performs large-scale supervised fine-tuning [149] using synthetic data generated by Qwen2.5-VL-72B-Instruct [8]. The data construction process begins by sourcing images from public datasets and the internet, then filters them for visual similarity to the Track #2 samples, yielding approximately 150k valid images. Based on these images and MLLMs, 1.5 million new questions are generated, for which the team uses their optimized prompts to generate answers. Then strict filtering approach of ensemble consistency and model scoring is used, producing 800k high-quality QA pairs. (2) In-domain SFT. In this stage, to align with the distribution of the Track #2 dataset, optimized prompts are applied across multiple MLLMs to generate answers for the Track #2s 6.5k VQA-SA questions. An ensemble and modelbased scoring strategy yields dateset of high-confidence samples with an accuracy of 79.47%. The stage-one model is then fine-tuned on these in-domain samples, leading to further performance gains. (3) Knowledge distillation. To reduce the inference cost of the 72B model, the final stage applies knowledge distillation [160, 161], transferring knowledge from Qwen2.5-VL 72B to smaller Qwen2.5-VL 7B model. Notably, the 7B student achieves precision of 69.72% in VQA-SA, matching the performance of its 72B teacher. 3.2.4. o3-MQP (submitted by MILVLG HDU Team) The team develops multi-query pipeline based on OpenAI o3 (o3-MQP) [95] that is designed to deeply analyze fine-grained spatial relationships between objects within an image and to execute complex visual reasoning. This solution comprises two primary stages. Preprocessing stage. The team observes that in the Track #2s dataset, questions associated with the same image often exhibit logical dependencies and semantic correlations. Consequently, during the preprocessing stage, they perform difficulty-based grouping of questions for each image. This approach prioritizes addressing simpler and more fundamental questions first, thus supplying contextual cues for answering more complex questions in subsequent steps. Prompting stage. During the prompting stage, the team designs multi-query prompting template to query the OpenAI o3 model. The template batches multiple questions about the same image into single prompt, and the model returns answers to all queries in single-pass response. Coupled with o3s tool-augmented reasoning, the template enables calls to external tools, such as bounding-box annotation, image cropping, and object classification, to tackle complex visual reasoning. By leveraging shared crossquestion context within the prompt, the approach improves the consistency and accuracy of the multi-query answers. 3.2.5. CCP4SR (submitted by SRCN-AIVL Team) The team proposes method of coreference and causality prompting for spatial reasoning (CCP4SR), which enhances spatial reasoning in multi-turn VQA-SA tasks by integrating coreference resolution, causal prompting, and deep learning assisted spatial reasoning. This approach is designed to mitigate ambiguities from omitted referents and to improve the models ability to interpret directional relationships between entities. Multi-turn coreference resolution with causal prompting. After reviewing the test set, the team observes that answering questions individually introduces coreference ambiguity, and prevents the effective use of causal relationships across consecutive questions. When isolated, these questions often omit explicit subject or object descriptions, making it challenging for MLLMs to recover the missing information from the query alone. Such omissions include not only the reference subject but also the perspective and answer options established in earlier rounds. Simply concatenating the QA pairs from previous questions as input for the current one further risks ambiguity, as the coreference chain becomes unstable with increasing context length. To address this issue, the team designs preprocessing step that treats each image as unit encompassing all its associated questions while preserving their original order. This process aims to capture as many inter-referential dependencies and causal connections across multiple rounds as possible. Depth-augmented spatial reasoning. The team incorporates depth information into spatial reasoning to improve performance on direction-related questions. Drawing on prior research in VQA spatial reasoning, they select Ovis234B [86] as the primary model, balancing parameter scale and computational constraints. For the SpaceOm dataset [14], despite being fine-tuned on synthetic spatial reasoning data, its low training resolution and small model size (3B) limit its ability to identify small entities and fully comprehend questions. Larger models (32B, 34B) demonstrate improved question understanding but still exhibit frequent errors on direction-related tasks requiring precise attribute determination. To address this issue, the team adapts the SpatialRGPT [22] pipeline. Direction-related questions are first filtered from the test set, and relevant entities and reference objects are extracted using an LLM. Depth maps from Depth Pro [11] are then incorporated to provide geometric cues, guiding the model towards depth-informed reasoning processes. However, in practice, models without task-specific training struggle to leverage bounding boxes and depth for complex reference transformations through prompt engineering. While this approach helps address basic depth-related questions, its overall performance remains slightly inferior. 3.2.6. Prompt4SA (submitted by Tang TUTE Team) The team (ranked #8) develops the Prompt4SA system to address spatial reasoning challenges such as coreference ambiguity, compositional attributes, occlusion, and viewpoint variation. Built on the MS-SWIFT LLM [164] with vLLM [62] as the inference backend, it employs cognition-guided structured prompting strategy that decomposes reasoning into five stages. By defining the reference frame before computing relations, the method reduces answer drift and improves consistency, achieving high accuracy and stable performance in the official evaluation of Track #2 without task-specific fine-tuning. Structured prompt for spatial reasoning. This team introduces structured prompt strategy to stabilize spatial reasoning and reduce ambiguity through five-stage process. It begins by classifying the question to determine cognitive complexity, then performs feature binding and object recognition to resolve composite descriptors and anchor objects. Next, it establishes the proper reference frame, objective, subjective, or ego-centric, before computing spatial relations such as directions, distances, and occlusions. Finally, the output is verified for consistency and formatted as XML-style tags to support reliable post-processing. Training and inference details. This approach is implemented on top of the VllmEngine in MS-SWIFT, employing MLLMs such as InternVL3 [168] as the base model. During post-processing, the final predictions are extracted from the <answer>...</answer> tags in the model output, and all QA pairs for each image are aggregated into single visualization panel. 3.3. Track #3 Visual Reasoning in Creative Advertisement Videos (VR-Ads) 3.3.1. Hi-CoT (submitted by gogogo truefaler Team) The champion of this track proposes hierarchical chain-of-thought based method for VR-Ads. Their idea is to guide multimodal large language model to perform intention-driven reasoning through structured prompts, emulating human cognitive processes that progress from perceptive understanding to higher-order reasoning. Method. The pipeline comprises four sequential steps: (1) Global understanding via audio-visual integration. The audio in the video is transcribed by automatic speech recognition tools [29, 99]. custom-designed prompt is then employed to guide the MLLM in conducting global analysis of the video. This initial step aims to extract highlevel semantic information, including the advertisements core theme, emotional tone, persuasive intent, and overall narrative structure. (2) Segment-level reasoning with contextual alignment. After extracting the global context, fine-grained shot-level analysis (segmented by scene changes) is conducted to address detailed questions. The video is partitioned into discrete shots based on scene transitions, and for each segment, the MLLM generates time-stamped visual-semantic descriptions. Subsequently, question-driven prompts are employed to align user queries with relevant segments, improving query-vision alignment. (3) Hierarchical reasoning over global and local observations. With key observations identified, the model proceeds to perform higher-level reasoning. This stage involves causal and commonsense inferences based on evidence collected in the previous steps. The model connects the what (observations) with the why (underlying reasons), bridging the gap between perception and cognition. (4) Answer generation. Finally, the model integrates all intermediate reasoning steps, including global context, localized observations, and causal inferences, to generate the final answer. This structured approach ensures that the output is not merely direct retrieval of facts but well-reasoned conclusion grounded in logical chain of thought. Implementation details. The Qwen2.5-VL-72B [8] serves as the base model. custom preprocessing pipeline is developed to enhance video understanding performance and address the data scarcity. The audio processing stage utilizes FFmpeg v5.1 for audio stream extraction, with subsequent transcription performed by the Whisper Large-v3 model [99] to facilitate multimodal analysis. The visual processing pipeline performs temporal segmentation using PySceneDetect v0.6.1 [13], with scene change threshold of 30. Each segmented video shot then undergoes detailed visual description generation via Qwen-VL, using well-designed prompting strategies. 3.3.2. TFA (submitted by HNU-VPAI Team) This teams solution adopts training-free approach (TFA). Their experiments are conducted on Qwen2.5Omni [131], Qwen2.5-VL [8], and GPT-4o [1], and results indicate that Qwen2.5-VL outperforms other models in terms of both accuracy and interpretability. The prompt is designed to guide the model towards concise and reasoningoriented answering. The template is as follows. You are provided with creative advertisement video. Please watch the video carefully and answer the given question based on its visual content and inferential reasoning. Limit your response for each question to no more than 30 words. Question: This prompt encourages the model to perform visual understanding and inference while maintaining concise response format. The video frames are extracted at rate of 1 FPS. Processing long videos at 1 FPS will exceed the token limit of the GPT API. Thus, for GPT-4o, 30 frames per video are instead uniformly sampled. Three observations are made: (1) The Qwen2.5-VL model (score=0.55 on the 72B model, 0.51 on 32B, 0.46 on 7B) outperforms other models, including GPT4o (score=0.50), demonstrating superior capability in reasoning tasks. (2) The expansion of parameter count in the Qwen2.5-VL model family demonstrates positive correlation with improvements in both video understanding and reasoning capabilities. (3) The audio information in Qwen2.5-Omni fails to yield measurable improvements (score=0.44 on 7B). This is likely because the audio in advertisement videos consists primarily of background music, which offers limited semantic cues for reasoning. 3.3.3. T-STAR (submitted by ActiveAlphaAgent Team) This solution T-STAR (test-time strategy tuning and adaptive reasoning) focuses on optimizing the inference strategy of the base model Qwen2.5-VL-72B-Instruct. Inference strategy. This efficient approach directly enhances the models prompt perception and response through three key hyper-parameters. (1) Video sampling rate. Multiple frame rates (2, 3, and 5 FPS) are systematically evaluated to identify the optimal balance between motion detail and processing efficiency. (2) Frame resolution. Various resolutions are assessed to achieve balance between providing adequate visual detail for the visual encoder and maintaining efficiency. (3) Rejection sampling. Any response exceeding 30 words is discarded and regenerated. To enforce this, strict instruction is prepended to the prompt as REMEMBER: Your answer MUST be 30 words or fewer. Question: {original question}. The teams top-performing leaderboard submission uses configuration of FPS=2 while maximizing frame resolution. key insight is that for the abstract reasoning in VRAds, prioritizing high spatial resolution, even at the cost of lower frame rate, proves more effectively than increasing temporal sampling. This approach yields better performance under inference constraints, suggesting that critical visual cues in ads depend more on spatial detail than on high-frequency temporal information. 3.3.4. QAAF (submitted by rookiesllm Team) The team presents Qwen-based advertising analysis framework (QAAF). To perceive rich information in advertising videos [38], the framework incorporates dedicated preprocessing pipeline where an audio extraction module isolates and extracts spoken audio content from raw videos. The extracted speech is translated into English to standardize the language for subsequent analysis, ensuring robustness across multilingual advertisements. The Qwen2.5-VL72B-Instruct model processes the multimodal input, which combines original video frames with standardized English text derived from the audio, integrating these synchronized visual and auditory cues to produce detailed, accurate, and contextually relevant responses to complex queries. Figure 3. Failure cases of Track #1 winner on Lens. Images are shown in their original proportions. Implementation details. Key parameters are set to video sampling rate of 2 FPS and maximum generation length of 2,048 new tokens. The results of automatic speech recognition are integrated via structured prompts. The voiceover content is explicitly prefixed with Voiceover: to disambiguate it from the visual information, thus enabling the correlation between auditory and visual features. tailored prompt template drives the model to advertisingspecific analysis. You are an expert in video analysis and advertising content understanding. Analyze the provided video frames and answer accurately based on visual content, focusing on key elements, actions, and messages conveyed. 3.3.5. GLM4ads (submitted by mm618 Team) The team finetunes the GLM-4.1V-Thinking-9B model [44] on the Video-Holmes dataset [23] that includes 270 manually annotated suspense short films and 1,837 questions covering seven reasoning types. Each question is accompanied by detailed answer explanations. To enhance the models reasoning capability, the team creates thinkthen-answer training samples by designing structured reasoning prompts for question-answer pairs. You are an expert video analyst. Please watch the provided video and generate coherent reasoning process based on the given correct answer and explanation. Your thought process should explain how the answer was derived, incorporating analysis of the video content. 4. Discussion and Outlook 4.1. Further Discussion The committee has collected questions and feedback from the participants and the community. Based on the above results and methods, we would like to discuss the following open problems. (1) What are the strengths and weaknesses of the submitted methods? The submitted solutions have achieved performance gains based on strong LLM base models, integrating various LLM techniques, e.g., data augmentation, prompt engineering, SFT, and RL training. However, one of the main goals of developing artificial intelligence methods is to create trustworthy, responsible, and generalizable AI. These methods would seem to still lack reliability and generalization capability. For instance, as discussed in Section 3, teams such as ActiveAlphaAgent, Location depends on guessing, and CV and RL, use RL to conduct task-oriented alignment training for LLMs, only considering IoU scores as their reward function. This reward signal is suboptimal as it only considers the task scores and likely drives the models to lose other capabilities. (2) Do LLMs already work well for multimodal reasoning? Multimodal reasoning in real-world scenarios and specialized domains is challenging task. Figure 3 and Figure 4 show randomly selected failure cases of the winning methods of Track #1 and Track #2, respectively. Two primary limitations are observed. First, in fine-grained image understanding, current multimodal large language models often confuse similar materials (e.g., mistaking plastic bags for other objects) or struggle to identify semantic targets (e.g., distinguishing bookshelf filled with books). Second, in visual question answering, models exhibit notable biases Figure 4. Failure cases of Track #2 winner on Lens. Images are shown in their original proportions. Figure 5. Failure cases of Gemini 2.5 Pro on AdsQA. in perspective understanding. These failure cases suggest that the models tend to overly rely on first-person perspective priors and misinterpret concepts such as distance, reflecting lack of physical commonsense and highlighting the difficulty in reasoning about real-world scenes. It is observed that three randomly selected failure cases. this state-of-the-art MLLM is nevertheless prone to generating suboptimal answers as well as content hallucination. The model omits some details of the advertisements and incorporates some inherent biases. For Track #3, its domain-specific challenges mainly include non-physical content reasoning, e.g., marketing logic, persuasive strategies, and audience engagement. To highlight how challenging this kind of implicit content based reasoning is, we use our AdsQA dataset to evaluate the state-of-the-art MLLM Gemini 2.5 Pro. Figure 5 presents 4.2. Outlook Developing general artificial intelligence is of great significance, and at the same time, studying LLMs tailored for specific scenarios and specialized fields is also crucial. Both generalist and specialist models are useful. Data and evaluation are crucial elements in driving LLMs and multimodal Figure 6. Samples of Track #3, demonstrating the key challenges in advertisement understanding, e.g., visual elements, persuasive strategies, core themes, emotional appeals, and target audience analysis. learning forward. Our vision is to explore more real-world scenarios and specialized domains, while also collecting more high-quality data as the testbeds for multimodal reasoning research. In particular, as shown in Figure 6, we will further explore the fine-grained specialized logic and details in advertisement videos, including but not limited to persuasive strategies, core themes, and emotional appeals. For the next challenge, we will provide more tracks and upgrade the scale of our test sets. 5. Conclusion This report has reviewed the MARS2 2025 Challenge on Multimodal Reasoning. Our goal is to bring together perspectives from multiple disciplines and to help researchers in both multimodal machine follow the state-of-the-art learning and multimodal large language models. Given the remarkable progress of LLMs and multimodal machine learning, it is time to explore more challenging and specialized tasks that require complex multimodal reasoning and slow thinking towards System 2. MARS2 2025 challenge focuses on real-world scenarios to broaden the multimodal reasoning applications of In particular, we explore two challenges, i.e., MLLMs. synergistic effects among reasoning tasks and non-stepwise complex reasoning. Thus, we released two tailored datasets Lens and AdsQA as our competition test sets that support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. Three competition tracks are provided, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). This years challenge lasted for more than two months. 76 teams from renowned academic and industrial institutions (e.g., ByteDance, NVIDIA, and Samsung) have registered and submitted 1200+ entries in total, of which 40+ submissions have been included in our final ranking lists. In addition, our committee has evaluated 40+ baselines, including generalist MLLMs and task-specific models. The sizes of the baselines and the participants models range from 3B to 72B. Thus, the scales of our datasets and results form large and comprehensive benchmark for the field of multimodal reasoning. This benchmark provides comprehensive comparison covering generalist models vs. specialist models, large models vs. small models, open-source models vs. closed-source models, etc. The submitted solutions involve open-source models and commercial LLMs, and present some shared ideas that mainly include using ensembles, data augmentation, prompt engineering, alignment training, etc. To tackle complex multimodal reasoning, most teams adopted state-ofthe-art MLLMs as their base models and then conducted multi-step alignment by using SFT and RL. The collaboration of generalist and specialist models is widely used and achieves good performance in our challenges. Our datasets (Lens and AdsQA), code sets (40+ baselines and 15+ participants methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/ mars2workshop, where our updates and announcements of upcoming events will be continuously published. Future work will continue to focus on multimodal reasoning, and we are committed to providing new application scenarios and high-quality data. We will continue to strive towards the goal of providing more comprehensive benchmark and promoting the open-source community in the field of multimodal reasoning."
        },
        {
            "title": "Acknowledgements",
            "content": "The organizing team is in part supported by the National Key Research and Development Program of China No.2022ZD0160600. Peng Xu is also supported by NSFC No.62306162. We thank the MARS2 2025 sponsors Interdisciplinary AI Research Institute of Wuhan College, and iFLYTEK. All the submissions were evaluated by an open source platform EvalAI."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 12, 14, 24, 25 [2] Remyx AI. Spaceom. Hugging Face model card, remyxai/SpaceOm, 2025. 10 [3] Remyx AI. Spacethinker-qwen2.5vl-3b. Hugging Face model card, remyxai/SpaceThinker-Qwen2.5VL-3B, 2025. 10 [4] Arjun Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo, Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas, William Freeman, et al. Metaclue: Towards comprehensive visual metaphors research. In CVPR, 2023. 27 [5] Sonnet Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet. URL https://api. semanticscholar. org/CorpusID, 273639283, 2024. [6] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In CVPR, 2022. 26 [7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 3 [8] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 6, 7, 8, 9, 10, 11, 12, 14, 24, 25 [9] Leonardo Berti, Flavio Giorgi, and Gjergji Kasneci. Emergent abilities in large language models: survey. arXiv preprint arXiv:2503.05788, 2025. 2 [10] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In AAAI, 2024. 2 [11] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second, 2025. 13 [12] Amaia Cardiel, Eloi Zablocki, Elias Ramzi, Oriane Simeoni, and Matthieu Cord. Llm-wrapper: Black-box semantic-aware adaptation of vision-language models for referring expression comprehension. In ICLR, 2025. [13] Brandon Castell. scene detection tool. readthedocs.io/, 2012. 14 Pyscenedetect: python-based https: //pyscenedetect . [14] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Spatialvlm: Endowing vision-language modFei Xia. arXiv preprint els with spatial reasoning capabilities. arXiv:2401.12168, 2024. 13 [15] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In ICML, 2024. [16] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In CVPR, 2024. 25 [17] HY Chen, Lai, Zhang, Wang, Eichner, You, Cao, Zhang, Yang, and Gan. Contrastive localized language-image pre-training. arxiv; 2025. arXiv preprint arXiv:2410.02746, 10. 26 [18] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. In CVPR, 2025. 25 [19] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 25 [20] Wei Chen, Long Chen, and Yu Wu. An efficient and effective transformer decoder-based framework for multi-task visual grounding. In ECCV, 2024. [21] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020. 26 [22] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. NeurIPS, 2024. 13 [23] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. 15 [24] Muhammad Iqbal Hasan Chowdhury, Kien Nguyen, Sridha Sridharan, and Clinton Fookes. Hierarchical relational attention for video question answering. In ICIP, 2018. 26 [25] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 11 [26] Ming Dai, Lingfeng Yang, Yihao Xu, Zhenhua Feng, and Wankou Yang. Simvg: simple framework for visual grounding with decoupled multi-modal fusion. NeurIPS, 2024. 5, 7 [27] Bruno Cesar de Oliveira Souza, Marius Aasan, Helio Pedrini, and Adin Ramirez Rivera. Selfgraphvqa: selfsupervised graph neural network for scene-based question answering. In ICCV, 2023. [28] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Transvg: End-to-end visual Zhou, and Houqiang Li. grounding with transformers. In ICCV, 2021. 5, 7 [29] FFmpeg developers. Ffmpeg. https://ffmpeg.org, 2000. 13 [30] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8 [31] Yasaman Etesam, Leon Kochiev, and Angel X. Chang. 3dvqa: Visual question answering for 3d environments. In CRV, 2022. [32] Jonathan St BT Evans. In two minds: dual-process accounts of reasoning. Trends in cognitive sciences, 2003. 2 [33] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025. 27 [34] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In CVPR, 2025. 10 [35] Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, and Ron Litman. Question aware vision transformer for multimodal reasoning. In CVPR, 2024. 26 [36] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 26 [37] Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. Agqa: benchmark for compositional spatio-temporal reasoning. In CVPR, 2021. 27 [38] Daya Guo and Zhaoyang Zeng. Multi-modal representation learning for video advertisement content structuring. In MM, 2021. 14 [39] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [40] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 8 [41] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 8, 11 [42] Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, and Stephan Gunnemann. Scene graph reasoning for visual question answering. arXiv preprint arXiv:2007.01072, 2020. 26 [43] Andreas Holzinger, Anna Saranti, Alessa Angerschmid, Bettina Finzel, Ute Schmid, and Heimo Mueller. Toward human-level concept learning: Pattern benchmarking for ai algorithms. Patterns, 2023. 26 [44] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. 7, 10, 15 [45] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. [46] Drew Hudson and Christopher Manning. Learning by abstraction: The neural state machine. NeurIPS, 2019. 26 [47] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 11, 25, 26 [48] Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and Adriana Kovashka. Automatic understanding of image and video advertisements. In CVPR, 2017. 27 [49] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2 [50] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In CVPR, 2017. 26, 27 [51] Menglin Jia, Zuxuan Wu, Austin Reiter, Claire Cardie, Serge Belongie, and Ser-Nam Lim. Intentonomy: dataset and study towards human intent understanding. In CVPR, 2021. 27 [52] Zhiwei Jia, Pradyumna Narayana, Arjun Akula, Garima Pruthi, Hao Su, Sugato Basu, and Varun Jampani. Kafa: Rethinking image ad understanding with knowledgeaugmented feature adaptation of vision-language models. arXiv preprint arXiv:2305.18373, 2023. 27 [53] Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Rex-thinker: Grounded object rearXiv preprint and Lei Zhang. ferring via chain-of-thought reasoning. arXiv:2506.04034, 2025. 9 [54] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 26 [55] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In ICCV, 2017. [56] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. 2 [57] Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, and Sumit Shekhar. Seeing the unseen: Visual metaphor captioning for videos. arXiv preprint arXiv:2406.04886, 2024. 27 [58] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. In ICCV, 2021. 26 [59] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 25 [60] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. [61] Yaman Kumar, Rajat Jha, Arunim Gupta, Milan Aggarwal, Aditya Garg, Tushar Malyan, Ayush Bhardwaj, Rajiv Ratn Shah, Balaji Krishnamurthy, and Changyou Chen. Persuasion strategies in advertisements. In AAAI, 2023. 27 [62] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SIGOPS, 2023. 13 [63] Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, and Stefan Wermter. What is right for me is not yet right for you: dataset for grounding relative directions via multi-task learning. arXiv preprint arXiv:2205.02671, 2022. 11 [64] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. Tvqa: Localized, compositional video question answering. arXiv preprint arXiv:1809.01696, 2018. 26, 27 [65] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4 [66] Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, et al. Viewspatial-bench: Evaluating multi-perspective spatial localization in visionlanguage models. arXiv preprint arXiv:2505.21500, 2025. 10 [67] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu. Llava-st: multimodal large language model for fine-grained spatialtemporal understanding. In CVPR, 2025. [68] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 25 [69] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. 27 [70] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: simple and perforarXiv preprint mant baseline for vision and language. arXiv:1908.03557, 2019. 25 [71] Ming Li, Ruiyi Zhang, Jian Chen, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Towards visual text grounding Zhou, and Tong Sun. arXiv preprint of multimodal arXiv:2504.04974, 2025. 25 large language model. [72] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pretraining for vision-language tasks. In ECCV, 2020. [73] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al. Groundinggpt: Language enhanced multi-modal grounding model. arXiv preprint arXiv:2401.06071, 2024. 25 [74] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 5 [75] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. 24, 25 [76] Jingping Liu, Ziyan Liu, Zhedong Cen, Yan Zhou, Yinan Zou, Weiyan Zhang, Haiyun Jiang, and Tong Ruan. Can multimodal large language models understand spatial relations? arXiv preprint arXiv:2505.19015, 2025. 10 [77] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 6, 7, 9 [78] Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, and Cheng-Lin Liu. Llava-c: Continual improved visual instruction tuning. arXiv preprint arXiv:2506.08666, 2025. 25 [79] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multimodal model an all-around player? In ECCV, 2024. 25 [80] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 24 [81] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. Generative multi-modal knowledge retrieval with large language models. In AAAI, 2024. [82] Xinwei Long, Jiali Zeng, Fandong Meng, Jie Zhou, and Bowen Zhou. Trust in internal or external knowledge? generative multi-modal entity linking with knowledge retriever. In ACL, 2024. 25 [83] Xinwei Long, Zhiyuan Ma, Ermo Hua, Kaiyan Zhang, Biqing Qi, and Bowen Zhou. Retrieval-augmented visual question answering via built-in autoregressive search engines. In AAAI, 2025. 27 [84] Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, and Bowen Zhou. Adsqa: Towards advertisement video understanding. In ICCV, 2025. 2, 3, 4, 5, 7 [85] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NeurIPS, 2019. 25 [86] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. 13 [87] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for In ECCV, grounding multimodal large language models. 2024. 7, 24 [88] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark. arXiv preprint arXiv:2412.07825, 2024. [89] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016. 25 [90] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, 2019. 24 [91] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey. arXiv preprint arXiv:2402.06196, 2024. 2 [92] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and Salman Khan. Videoglamm: large multimodal model for pixel-level visual grounding in videos. In CVPR, 2025. 25 [93] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. comprehensive overview of large language models. TIST, 2023. 2 [94] Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, and Bryan Plummer. Right side up? disentangling orientation understanding in mllms with fine-grained multi-axis perception tasks. arXiv preprint arXiv:2505.21649, 2025. [95] OpenAI. Introducing openai o3 and o4-mini. https:// openai.com/index/introducing-o3-and-o4mini/, 2025. Accessed 2025-08-14. 11, 12 [96] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 25 [97] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015. 25 [98] Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, and Amjad Almahairi. Jack of all tasks master of many: Designing general-purpose coarse-to-fine visionlanguage model. In CVPR, 2024. 25 [99] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2023. 13, 14 [100] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. [101] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 2022. 24 [102] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. NeurIPS, 2023. 2 [103] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. NeurIPS, 2024. 26 [104] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 6, 8, 10 [105] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 7, [106] Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi. Tag-llm: Repurposing In ICML, general-purpose llms for specialized domains. 2024. 2 [107] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 8 [108] Jiaxin Shi, Hanwang Zhang, and Juanzi Li. Explainable and explicit visual reasoning over scene graphs. In CVPR, 2019. 26 [109] Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Gholamreza Haffari, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. arXiv preprint arXiv:2411.06048, 2024. 10 [110] Roman Solovyev, Weimin Wang, and Tatiana Gabruseva. Weighted boxes fusion: Ensembling boxes from different Image and Vision Computing, object detection models. 2021. 10 [111] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Vl-bert: Pre-training of arXiv preprint Furu Wei, and Jifeng Dai. generic visual-linguistic representations. arXiv:1908.08530, 2019. [112] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. In ACL, corpus of natural language for visual reasoning. 2017. 25 [113] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019. 26 [114] Wei Tang, Yanpeng Sun, Qinying Gu, and Zechao Li. Visual position prompt for mllm based visual grounding. arXiv preprint arXiv:2503.15426, 2025. 25 [115] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Movieqa: Understanding stories through question-answering. In CVPR, 2016. 26, 27 in movies [116] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3, 24, 25 [117] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. 3 Gemma 3 technical report. [118] Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. 24 [119] Hao Wang, Pengzhen Ren, Zequn Jie, Xiao Dong, Chengjian Feng, Yinlong Qian, Lin Ma, Dongmei Jiang, Yaowei Wang, Xiangyuan Lan, et al. Ov-dino: Unified open-vocabulary detection with language-aware selective fusion. arXiv preprint arXiv:2407.07844, 2024. 10 [120] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 24 [121] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [122] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 2 [123] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 27 [124] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 3 [125] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In CVPR, 2024. 10 [126] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. 27 [127] Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, and Changsheng Xu. Clip-vg: Self-paced curriculum adapting of clip for visual grounding. TMM, 2023. 7 [128] LLM-Core-Team Xiaomi. Mimo-vl technical report. arXiv preprint arXiv:2506.03569, 2025. 8 [129] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In MM, 2017. 27 [130] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Llava-cot: Let viSong, Lichao Sun, and Li Yuan. sion language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 24 [131] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 14 [132] Peng Xu, Xiatian Zhu, and David Clifton. Multimodal learning with transformers: survey. TPAMI, 2023. 2 [133] Peng Xu, Song Bai, Bowen Zhou, David Clifton, Andrea Vedaldi, Mihaela van der Schaar, and Luc Van Gool. Guest editorial: Introduction to the special section on large-scale multimodal learning: Universality, robustness, efficiency, and beyond. TPAMI, 2025. 2 [134] Yanzhi Xu, Yueying Hua, Shichen Li, and Zhongqing for multi-modal Exploring chain-of-thought Wang. metaphor detection. In ACL, 2024. 27 [135] Dingyi Yang, Chunru Zhan, Ziheng Wang, Biao Wang, Tiezheng Ge, Bo Zheng, and Qin Jin. Synchronized video storytelling: Generating video narrations with structured storyline. arXiv preprint arXiv:2405.14040, 2024. 27 [136] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In CVPR, 2025. 24 [137] Li Yang, Yan Xu, Chunfeng Yuan, Wei Liu, Bing Li, and Improving visual grounding with visualIn CVPR, Weiming Hu. linguistic verification and iterative reasoning. 2022. 7 [138] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. 24 [139] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 24 [140] Ruilin Yao, Yi Rong, Qiangqiang Huang, and Shengwu Xiong. Ctod: Cross-attentive task-alignment for one-stage object detection. TCSVT, 2024. 5 [141] Ruilin Yao, Shengwu Xiong, Yichen Zhao, and Yi Rong. Visual grounding with multi-modal conditional adaptation. In MM, 2024. 5, [142] Ruilin Yao, Bo Zhang, Jirui Huang, Xinwei Long, Yifang Zhang, Tianyu Zou, Yufei Wu, Shichao Su, Yifan Xu, Wenxi Zeng, et al. Lens: Multi-level evaluation of multimodal reasoning with large language models. arXiv preprint arXiv:2505.15616, 2025. 2, 3, 4 [143] Keren Ye, Narges Honarvar Nazari, James Hahn, Zaeem Hussain, Mingda Zhang, and Adriana Kovashka. Interpreting the rhetoric of visual advertisements. TPAMI, 2019. 27 [144] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. NeurIPS, 2018. 26 [145] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2024. 7, 24 [146] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. 10 [147] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm arXiv preprint reinforcement learning system at scale. arXiv:2503.14476, 2025. 11 [148] Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, et al. Mme-reasoning: comprehensive benchmark for logical reasoning in mllms. arXiv preprint arXiv:2505.21327, 2025. [149] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 8, 12 [150] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019. 25 [151] Dongyu Zhang, Minghao Zhang, Heting Zhang, Liang Yang, and Hongfei Lin. Multimet: multimodal dataset for metaphor understanding. In ACL-IJCNLP, 2021. 27 [152] Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024. 2 [153] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 5, 24 [154] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Leizhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. In ECCV, 2024. 25 [155] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 24 [156] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021. [157] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation. In CVPR, 2024. 24 [158] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 4 [159] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 24 [160] Zijian Zhang, Xuecheng Wu, Danlei Huang, Siyu Yan, Chong Peng, and Xuezhi Cao. Hkd4vlm: progressive hybrid knowledge distillation framework for robust multimodal hallucination and factuality detection in vlms. arXiv preprint arXiv:2506.13038, 2025. 8, 12 [161] Zijian Zhang, Xuhui Zheng, Xuecheng Wu, Chong Peng, and Xuezhi Cao. Tokenfocus-vqa: Enhancing text-to-image alignment with position-aware focus and multi-perspective aggregations on lvlms. In CVPR, 2025. 8, [162] Kang Zhao, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wen Tao, Cong Han, Shuanglong Li, and Lin Liu. Enhancing baidu multimodal advertisement with chinese text-to-image generation via bilingual alignment and caption synthesis. In SIGIR, 2024. 27 [163] Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Dong Xu, Rui Zheng, Yinjie Zhao, Lipeng Wang, and Xibo Fan. Toward explainable 3d grounded visual question answering: new benchmark and strong baseline. TCSVT, 2023. 26 [164] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In AAAI, 2025. 13 [165] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 8 [166] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 2 [167] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [168] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 3, 7, 8, 11, 13, 24, 25 [169] Ke Zhu, Yu Wang, Jiangjiang Liu, Qunyi Xie, Shanshan Liu, and Gang Zhang. On data synthesis and postarXiv preprint training for visual abstract reasoning. arXiv:2504.01324, 2025. 24 [170] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. NeurIPS, 2024. 7 A. Related Work A.1. Multimodal Reasoning with Large VisionLanguage Models. Recent advances in LLMs have markedly improved their reasoning performance in text-only tasks. Nevertheless, owing to modality heterogeneity and the inherent complexity of cross-modal reasoning, extending these capabilities to MLLMs is still in an early exploratory phase. Current research predominantly follows three paradigms: multimodal chain-of-thought, reinforcement learning based reasoning, and native multimodal architectures. Multimodal chain-of-thought (M-CoT). Representative M-CoT approaches such as Multimodal-CoT [159], LLaVA-CoT [130], RAVEN-CoT [169], Magma-CoT [136], and MM-ReAct [139] improve performance by explicitly constructing reasoning steps within visual prompts. They achieve notable accuracy gains on datasets like ScienceQA [101] and OK-VQA [90], and show clear control over reasoning trajectories in tasks such as VG-RS and VQA-SA. However, these methods rely heavily on explicit visual cues and often falter when handling metaphors, emotional undertones, or abstract semantics common in advertising videos, where reasoning chains can become vague or ambiguous. Reinforcement learning based reasoning. The RL-based reasoning optimization paradigm, exemplified by models such as Visual-RFT [80], SRPO [118], and VL-Rethinker [120], explicitly refines the reasoning trajectories of multimodal models through reward functions grounded in visual and textual cues, group-wise relative policy optimization, and mechanisms of self-reflection and soft replay. This paradigm has demonstrated strong performance in tasks like VQA-SA, which require spatial relation understanding and multi-step logical reasoning. Nevertheless, its reliance on explicit reward signals and high-quality feedback poses challenges for establishing effective supervision in abstract reasoning scenarios such as VR-Ads of our Track #3, thus constraining its capacity to generalize. Native multimodal architectures. Native multimodal reasoning architectures enable effective fusion and reasoning across modalities through unified model designs and multimodal pretraining mechanisms. Representative closedsource models, including Gemini [116], GPT-4V [138], and Claude [5], exhibit inherent end-to-end multimodal reasoning capabilities and demonstrate robust performance across diverse task scenarios. In parallel, open-source models such as LLaVA [75] and MiniGPT-4 [167] leverage lightweight architectures to achieve broad multimodal reasoning capacity, showing competitive performance in visual grounding and spatial reasoning. Nevertheless, these architectures still encounter significant challenges in abstract conceptual reasoning tasks, such as VR-Ads. Summary. Although recent MLLMs such as GPT-4o [1], Gemini 2.5 Pro [116], InternVL3 [168], and Qwen2.5-VL [8] have demonstrated strong performance in multimodal reasoning, the Lens benchmark reveals that they continue to underperform in spatial reasoning, compositional understanding, and abstract reasoning, particularly in hierarchical reasoning tasks. This highlights the limitations of current models in fundamental visual perception and logical reasoning, as well as the shortcomings of fragmented evaluation systems that fail to provide systematic characterization of the continuum from perception to reasoning. To address this gap, the MARS2 competition introduced unified evaluation framework encompassing three complementary tracks: VG in Physical Scenes (VG-RS), VQA with Spatial Awareness (VQA-SA), and VR in Creative Advertisement Videos (VR-Ads). This initiative offers more comprehensive benchmark platform to advance the reasoning capabilities of next-generation MLLMs. A.2. Visual Grounding and Scene Perception Visual Grounding (VG) refers to the task of identifying regions of interest within an image based on textual descriptions. By analyzing image scenes, VG establishes crossmodal associations between vision and language, thereby enabling machines to achieve human-like multimodal understanding and reasoning. With the rapid development of LLMs and MLLMs, research on VG has evolved from traditional task-specific models towards perception models that leverage the inherent reasoning capacities of large models. Currently, LLM-based VG can be broadly categorized into three research paradigms: integration of external visual information, cross-modal semantic alignment, and multimodal reasoning strategies. Integration of external visual information. VG takes both text and image modalities as input, and LLMs alone are inadequate for accomplishing this task. To address this limitation, several studies have incorporated external visual encoders to process image information. Some approaches emphasize region-level visual representations, for instance, Groma [87] employs DINO [153] as the image encoder to capture key regions and applies explicit modeling with region tokens to leverage the spatial reasoning capabilities of MLLMs for localization. To support scene perception tasks at varying levels of granularity, the external visual encoders in Ferret [145] and Ferret-v2 [155] employ multi-scale visual backbone to enhance spatial semantics via multi-scale visual sampling. Furthermore, methods such as Groundhog [157] and GLaMM [100] enable mask generation as an alternative to bounding boxes for localization. Cross-modal semantic alignment. Most external visual encoder approaches rely on static perception mechanisms, which limit their ability to disentangle fine-grained semantics guided by complex textual inputs. Cross-modal semantic alignment methods instead establish unified semantic space through learnable queries, adapter modules, or contrastive learning, offering greater adaptability to linguistic variation. representative paradigm is Q-Former-based fusion, such as BLIP-2 [68] and Grounding-GPT [73]. They employ Q-Former to bridge vision and language, constructing query-guided visual representations that enhance semantic consistency across modalities and improve localization alignment capabilities. To integrate multi-scale visual information, LION [16] introduces Mixture-of-Adapters framework, using multi-granularity adapters combined with router mechanism for fusion within the LLM. As grounding requirements become more sophisticated, approaches such as Llava-grounding [154] and Grounding-GPT [73] enable mixed inputs and outputs in the form of bounding boxes, textual annotations, and masks, thereby laying the foundation for unified interfaces that support interactive multimodal dialogue. Multimodal reasoning strategies. With reasoning strategies in LLMs achieving significant breakthroughs across various tasks, multimodal reasoning strategies have also been applied to VG. Shikra [19] extends the traditional CoT framework by introducing Grounding CoT, which incorporates region referencing and localization factors to improve semantic reference resolution. To further strengthen models ability to follow linguistic instructions, multi-stage instruction tuning has been adopted in VG. Representative approaches, such as Kosmos-2 [96], employ three-stage paradigm of perceptioninstruction tuningalignment to progressively improve cross-modal semantic alignment. Building on this foundation, recent studies have further extended multi-instruction tuning strategies. VPP-LLaVA [114] constructed VPP-SFT, high-quality grounding instruction dataset comprising approximately 600k samples, and applied it for multi-instruction tuning on the LLaVA family of models, substantially improving controllability in coordinate-to-region outputs. TRIG [71] expands VG from natural images to text-rich images such as forms, documents, and receipts, proposing novel tasks along with specialized instruction dataset. In the Doc-VQA setting, it achieves localized answer alignment, thereby filling critical gap of traditional VG in document understanding. VideoGLaMM [92] focuses on video-based multiinstruction tuning, significantly enhancing spatiotemporal alignment and pixel-level localization, and extending multiinstruction paradigms to video tasks. LLaVA-ST [67] unifies temporal localization, spatial localization, and descriptive tasks under single framework for multi-task and multi-instruction training. Meanwhile, VistaLLM [98] strengthens cross-modal transfer and enhances generalization by engaging in joint grounding tasks across multiple modalities, including images, videos, and audio. LLaVA-c [78] is designed for continuously expanding multi-task instruction training scenarios, aiming to mitigate the problem of catastrophic forgetting induced by task expansion. Summary. Current VG tasks primarily depend on established benchmark datasets such as RefCOCO [18] / RefCOCO+ [18] / RefCOCOg [89], ReferItGame [59], and RefCLEF [59], while some studies have also been evaluated on open-domain benchmarks like Flickr30K Entities [97] and Visual Genome [60]. However, these datasets are insufficient to meet the evaluation requirements of MLLM reasoning. To bridge this gap, researchers have proposed more sophisticated benchmarks, including GQA [47] for scene-graph-based question answering, VCR [150] for region grounding and multi-turn QA, and NLVR2 [112] for In addition, compositional alignment of visual concepts. benchmarks such as LLaVA-Bench [75], MME [148], and MMBench [79] are designed to evaluate the zero-shot and few-shot generalization abilities of MLLMs, as well as their capacity to leverage external knowledge [81, 82]. Although existing benchmarks provide validation platforms for multimodal reasoning, most fail to evaluate spatial relationship modeling capabilities. To address this gap, the Lens Benchmark is constructed with focus on spatial reasoning, designing multi-turn dialogue scenarios in which models must not only identify targets but also perform reasoning by integrating contextual and cross-modal information. Unlike prior benchmarks that primarily relied on MS COCO or Flickr, Lens draws on real-world data with human annotations, over 80% of which were collected after September 2024, ensuring both temporal relevance and robust generalization to unseen data. Experimental results demonstrate that even advanced MLLMs such as GPT-4o [1], Gemini 2.5 Pro [116], InternVL3 [168], and Qwen2.5VL [8] continue to encounter significant challenges in spatial reasoning tasks. Consequently, Lens constitutes an important complement for assessing the depth of multimodal reasoning chains and spatial perception capabilities. A.3. Visual Question Answering with Spatial and"
        },
        {
            "title": "Commonsense Reasoning",
            "content": "Visual Question Answering (VQA) serves as cornerstone for achieving human-like multimodal reasoning, while spatial perception underpins human understanding of the physical world. Consequently, VQA research has increasingly focused on spatial perception, seeking to capture the physical structures and spatial relationships embedded in images. With the advent of visionlanguage pre-trained models such as ViLBERT [85], VisualBERT [70], and VLBERT [111], VQA has made remarkable advances. These models acquire joint visuallanguage representations via large-scale pretraining on imagetext pairs, followed by fine-tuning on tasks such as VQA, leading to substantial gains in accuracy. In this section, we review four representative paradigms: explicit spatial relationship modeling, implicit spatial-aware feature fusion, and spatial alignment and CoT reasoning enhancement. Explicit spatial relationship modeling. Explicit approaches construct structured representations of object relations based on geometric priors such as relative position, orientation, and distance, and integrate them into LLM reasoning chains to enable symbolic or structured reasoning. XNM [108], NS-VQA [144], and Neural State Machine [46] all generate objectrelation graphs from bounding box coordinates and semantic categories, thereby supporting interpretable reasoning over spatial layouts. Scene Graph Reasoning for VQA [42] further incorporates both semantic and spatial edges to address complex queries, while SelfGraphVQA [27] introduces self-supervised graph augmentation to reduce annotation costs. The GQA dataset [47], which provides annotations including scene graphs and functional programs, has become the standard benchmark for this class of methods. Implicit spatial-aware feature fusion. In contrast to explicit modeling of spatial information, implicit fusion methods leverage cross-modal attention mechanisms and multiscale feature integration, allowing models to learn spatial dependencies implicitly during training. Early cross-modal pretraining models, such as LXMERT [113], UNITER [21], and OSCAR [72], incorporate self-attention and crossattention mechanisms to align textual tokens with localized visual representations, thereby implicitly capturing spatial patterns. Subsequently, VinVL [156] enhances the visual encoder with object-centric features, while MDETR [58] adopts text-modulated detection framework to jointly model text and images in an end-to-end reasoning process. More recently, QA-ViT [35] directly injects question embeddings into the visual encoder, directing multiscale attention towards regions pertinent to the question and thereby capturing both local and global spatial relationships effectively. Spatial alignment and CoT reasoning enhancement. These methods strengthen visionlanguage spatial alignment and enable multi-step reasoning. Region-level alignment, such as CLOC [17], integrates regiontext contrastive learning into CLIP, improving the mapping between spatial descriptors (e.g., top left corner) and image regions, which is especially useful for queries with spatial prepositions. Meanwhile, CoT prompting decomposes complex spatial reasoning into interpretable steps, with resources like Visual-CoT [103] providing bounding box and reasoning annotations for supervised training. Summary. In VQA research with spatial perception capabilities, the design and selection of benchmark datasets directly affect the evaluation and comparison of model capabilities. Current mainstream benchmarks encompass both real-world and synthetic environments, as well as diverse forms across two-dimensional and three-dimensional spaces. This highlights the dual requirements of spatial reasoning tasks for dataset diversity and controllable difficulty. In two-dimensional real-world image scenarios, GQA [47] provides large-scale annotations of attributes and relations, extensively covering relative positions (e.g., is to the left of Y), making it well-suited for evaluating multi-step spatial reasoning. The spatial subset of VQA v2 [36] is more closely aligned with general VQA tasks. Although relatively limited in scale, it enables testing of spatial generalization in open-domain settings. TDIUC [55] introduces dedicated spatial-relation subtask, facilitating cross-task comparisons of model capabilities. In synthetic environments, CLEVR [54] and CLEVRHumans [43] are designed to test spatial reasoning via controllable attribute combinations and geometric relationships. CLEVR-Humans [43] further incorporates humanauthored natural language questions, increasing linguistic diversity and naturalness, and is often employed to analyze the effectiveness of specific modeling strategies. In threedimensional contexts, ScanQA [6] and 3D-VQA [31, 163] extend VQA to indoor and outdoor 3D scans and point clouds, requiring models to reason over 3D structures and object layouts. These benchmarks are highly relevant to applications in robotic navigation and AR/VR systems, while also assessing the integration of depth information and the learning of 3D representations. Compared with conventional VQA benchmarks, the Lens benchmark evaluates spatial reasoning ability from multiple dimensions by integrating tasks such as spatial relation understanding, visual reference resolution, and 3D scene comprehension. Its question design goes beyond object localization to also include spatial relation inference and multi-step reasoning, resulting in more comprehensive spatial reasoning chain. This provides more faithful assessment of models capacity to capture complex spatial semantics than traditional benchmarks that focus solely on positional relations. Moreover, in contrast to synthetic datasets, Lens is more closely aligned with real-world application scenarios such as robotic navigation, AR/VR, and cross-view retrieval, thereby mitigating the domain gap between synthetic and real data. A.4. Visual Reasoning in Creative Advertisement"
        },
        {
            "title": "Videos",
            "content": "Creative advertising video reasoning seeks to exploit multimodal information within advertisements to uncover the implicit abstract semantics underlying explicit visual elements. In contrast to short-duration factual datasets such as MSVD-QA [24] or TGIF-QA [50], and long-duration narrative datasets such as TVQA [64] and MovieQA [115], creative advertising videos are shorter in duration yet contain richer abstract cues. These videos place stronger emphasis on abstract reasoning, including emotion analysis, metaphor comprehension, and cultural symbol interpretation. Thus, the essence of the reasoning process lies not in recognizing explicit visual facts, but rather in integrating multimodal evidence to infer latent thematic meanings. Advertisement video understanding benchmarks. Early video question answering benchmarks, such as MSRVTTQA [129] and TGIF-QA [50], primarily emphasize action recognition and temporal sequencing. They are designed to evaluate fundamental perception and short-term reasoning abilities. Narrative-oriented datasets like [64] and MovieQA [115] extend the scope to multi-turn, dialoguebased reasoning requiring long-term temporal dependencies. More recent causal and compositional reasoning benchmarks, including NExT-QA [126], AGQA [37], and STAR [123], focus on assessing models ability to answer why and how questions, connecting visual evidence with underlying intentions and outcomes. While these benchmarks provide valuable references for the advancement of reasoning strategies, they are still limited in assessing abstract semantics and creative content. In the advertising domain, the understanding of advertisements has emerged as critical research direction due to the economic value of accurate content interpretation. Internet companies generate substantial revenue by automatically delivering ads to target audiences, thereby highlighting the necessity of automated advertisement understanding [52]. The earliest Pit dataset [48] formalizes this task as VQA problem [83], and is subsequently extended or adapted in later studies [57] or private datasets [135]. These studies typically focus on specific dimensions, such as persuasion strategy analysis in image advertisements [61, 143], image ad retrieval [162], intent understanding [51], and visual metaphor comprehension [4, 134, 151]. Although Pit primarily targets image advertisements, it also includes subset of video ads. Nonetheless, Pit faces challenges of limited accessibility, insufficient diversity, and constrained QA format, and to date, there remains no widely adopted comprehensive video QA benchmark for ads. Summary. To address this gap, AdsQA has recently been introduced as dedicated benchmark for creative advertisement understanding. It evaluates five key dimensions: visual concept understanding, emotion recognition, theme and message identification, persuasion strategy analysis, and target audience recognition, capturing both low-level perceptual cues and high-level abstract semantics. Unlike general benchmarks such as MVBench [69] or Video-MME [33], AdsQA explicitly frames the integration of visual, textual, and occasionally auditory modalities as means to infer underlying communicative intent, rather than merely matching explicit visual facts. Through the VR-Ads track, it facilitates the transition of multimodal reasoning techniques from explicit perception tasks to higher-level cognitive reasoning. This raises greater demands on cross-modal fusion, temporal modeling, external knowledge integration, and interpretability, thereby providing more comprehensive reflection of MLLMs capabilities in real-world multimodal reasoning scenarios. Moreover, it offers valuable implications for practical applications, including advertisement understanding, content moderation, user profiling, and recommendation systems. B. Teams and Affiliations B.1. MARS2 2025 Challenge Organizing Team Organizers: Peng Xu1 (peng xu@tsinghua.edu.cn) Shengwu Xiong2 (xiongsw@whut.edu.cn) Jiajun Zhang3 (jjzhang@nlpr.ia.ac.cn) Yaxiong Chen2 (chenyaxiong@whut.edu.cn) Steering Committee: Bowen Zhou1,4 (zhoubowen@tsinghua.edu.cn) Chen Change Loy5 (ccloy@ntu.edu.sg) David A. Clifton6 (david.clifton@eng.ox.ac.uk) Kyoung Mu Lee7 (kyoungmu@snu.ac.kr) Luc Van Gool8 (luc.vangool@insait.ai) Contributors: Ruiming He2 (361284@whut.edu.cn) Ruilin Yao2,3 (yaoruilin@whut.edu.cn) Xinwei Long1 (longxw22@mails.tsinghua.edu.cn) Jirui Huang2,3 (284387@whut.edu.cn) Kai Tian1 (tk23@mails.tsinghua.edu.cn) Sa Yang9 (yangsa2023@stu.pku.edu.cn) Yihua Shao3 (yihuajerry@gmail.com) Jin Feng10 (fengjin@kuaishou.com) Yue Zhong11 (zhongyue@cupl.edu.cn) Jiakai Zhou1 (zhoujk22@mails.tsinghua.edu.cn) Cheng Tang1 (c-tang22@mails.tsinghua.edu.cn) Tianyu Zou2 (zoutianyu@whut.edu.cn) Yifang Zhang2 (332972@whut.edu.cn) Junming Liang2 (songlier@whut.edu.cn) Guoyou Li2 (315863@whut.edu.cn) Zhaoxiang Wang2 (wangzx@whut.edu.cn) Qiang Zhou12 (amos@52cv.net) Yichen Zhao2 (zhaoyichen@whut.edu.cn) Shili Xiong2 (slxiong.illinois@gmail.com) Hyeongjin Nam7 (namhj28@gmail.com) Jaerin Lee7 (ironjr@snu.ac.kr) Jaeyoung Chung7 (robot0321@snu.ac.kr) JoonKyu Park7 (jkpark0825@snu.ac.kr) Junghun Oh7 (dh6dh@snu.ac.kr) Kanggeon Lee7 (dlrkdrjs97@naver.com) Wooseok Lee7 (adntjr4@gmail.com) Juneyoung Ro13 (juneyoung.ro@kaist.ac.kr) Turghun Osman14 (turghun@ms.xjb.ac.cn) Affiliations: 1 Tsinghua University 2 Wuhan University of Technology 3 Institute of Automation, Chinese Academy of Sciences 4 Shanghai Artificial Intelligence Laboratory 5 Nanyang Technological University 6 University of Oxford 7 Seoul National University 8 INSAIT, Sofia University St. Kliment Ohridski 9 Peking University 10 KuaiShou Inc. 11 China University of Political Science and Law 12 52CV Computer Vision Academic Community 13 Korea Advanced Institute of Science and Technology 14 Xinjiang Technical Institute of Physics & Chemistry of the Chinese Academy of Sciences B.2. MARS2 2025 Challenge Participants (alphabetically) 404 Not Found Team Members: Jingxuan Li1 (liuy33467@gmail.com) Kai Tian2 (tk23@mails.tsinghua.edu.cn) Sa Yang3 (yangsa2023@stu.pku.edu.cn) Jiaheng Ma4 (3220231772@bit.edu.cn) Yang Liu5 (liuyang2015779381@gmail.com) Affiliations: 1 Harbin Engineering University 2 Tsinghua University 3 Peking University 4 Beijing Institute of Technology 5 China Electronics Technology Group Corporation ActiveAlphaAgent Team Members: Chong Peng1 (pengchong@meituan.com) Taofeng Xue1 (xuetaofeng@meituan.com) Zijian Zhang1 (zhangzijian14@meituan.com) Jianing Wang1 (wangjianing16@meituan.com) Chengcheng Han1 (hanchengcheng02@meituan.com) Affiliations: 1 Meituan-M17 adaboost Team Members: Ziang Li1 (24241215014@stu.xidian.edu.cn) Linnan Zhao1 (lnzhao@stu.xidian.edu.cn) Xinyi You1 (24171213968@stu.xidian.edu.cn) Affiliations: 1 Xidian University CV and RL Team Members: Feng Gao1 (gaofeng24@mails.ucas.ac.cn) Zongshu Li2 (lizongshu@wair.ac.cn) Hanxiao Wu1,3 (hxwu@whut.edu.cn) Guibo Zhu1,2 (gbzhu@nlpr.ia.ac.cn) Affiliations: 1 Institute of Automation, Chinese Academy of Sciences 2 Wuhan Artificial Intelligence Research 3 Wuhan University of Technology EAIC Team Members: Nguyen Thanh Thien1 (thiennt@uit.edu.vn) Affiliations: 1 University of Information Technology, Ho Chi Minh City, Vietnam Echoch Team Members: Chenhao Qiu1 (qiuchenhao@mgtv.com) Xusheng Liu1 (liuxusheng@mgtv.com) Can Hu1 (hucan@mgtv.com) Jie Yang1 (yangjie@mgtv.com) Shien Song1 (shien@mgtv.com) Haibo Lu1 (haibo2@mgtv.com) Han Qi1 (qihan@mgtv.com) Affiliations: 1 MGTV Corporation, Changsha, China gogogo truefaler Team Members: Zhaohong Liu1 (liuzhaohong0425@cuc.edu.cn) Yiting Xi1 (17749772485@163.com) Zhenni Huang1 (hzn0806@163.com) Ziyun Xiao1 (yolandaxiao@foxmail.com) Affiliations: 1 Communication University of China GoodAI zju Team Members: Xin Chen1 (745482277@qq.com) Affiliations: 1 Zhejiang University HNU-VPAI Team Members: Zhiyu Wang1 (zhiyuwang@hnu.edu.cn) Xudong Kang1 (xudong kang@hnu.edu.cn) Shutao Li1 (shutao li@hnu.edu.cn) Affiliations: 1 Hunan University lababa Team Members: Yumei Li1 (liym0302@163.com) Cong Xu1 (xucong143336@gmail.com) Pu Luo1 (18581668812@163.com) Affiliations: 1 Xidian University Location depends on guessing Team Members: Xin Wei1 (weix11@chinatelecom.cn) Han Fang1 (fangh2@chinatelecom.cn) Xiaodong Dong1 (dongxd1@chinatelecom.cn) Hongbo Sun1 (sunhb3@chinatelecom.cn) Mengxi Jia1 (jiamx1@chinatelecom.cn) Ye Yuan1 (yuany2@chinatelecom.cn) Zhiyong Feng2 (24s136095@stu.hit.edu.cn) Tianyi Gao3 (tyigao@stu.xjtu.edu.cn) Yongkang Yu4 (yyk1249501542@gmail.com) Haobo Cheng5 (chbustc@mail.ustc.edu.cn) Muyang Yan6 (mario112112@163.com) Affiliations: 1 Institute of Artificial Intelligence, China Telecom 2 Harbin Institute of Technology 3 Xian Jiaotong University 4 Beijing University of Posts and Telecommunications 5 University of Science and Technology of China 6 East China Normal University MILVLG HDU Team Members: Zhenwei Shao1 (shaozw@hdu.edu.cn) Lihao Zheng1 (zhenglh@hdu.edu.cn) Shuai Shao1 (242050206@hdu.edu.cn) Zhou Yu1 (yuz@hdu.edu.cn) Affiliations: 1 Hangzhou Dianzi University, China mm618 Team Members: Yanan Wang1 (wangyanan@mail.dlut.edu.cn) Yicen Tian1 (yicentian@mail.dlut.edu.cn) Dailin Li1 (ldlbest@mail.dlut.edu.cn) Affiliations: 1 Dalian University of Technology NJUSTKMG Team Members: Yipeng Lin1 (lllinyp@163.com) Yang Yang1 (yyang@njust.edu.cn) Affiliations: 1 Nanjing University of Science and Technology rookiesllm Team Members: Huayong Hu1 (huhy6519@mails.jlu.edu.cn) Yuxin Qin1 (qinyx23@mails.jlu.edu.cn) Shijie Dong2 (dsj20020312@163.com) Qi Li1 (qili23@mails.jlu.edu.cn) Affiliations: 1 Jilin University 2 Beijing Institute of Technology SRCN-AIVL Team Members: Cheng Chen1 (koalala.chen@samsung.com) Ziyang Peng1 (ziyang.peng@samsung.com) Yin Tang1 (yin6913.tang@samsung.com) Jiang Yu1 (jiang0922.yu@samsung.com) Weiyang Su1 (weiyang.su@samsung.com) Affiliations: 1 Samsung Electronics (China) R&D Center Star Team Members: Yanglin Deng1 (yanglin deng@163.com) Jinglin Zhou1 (6233114044@stu.jiangnan.edu.cn) Hongyao Chen1 (6233111020@stu.jiangnan.edu.cn) Wei Zhang1 (phenixnull@gmail.com) Xujie Zhou1 (zxj165561@gmail.com) Tianyang Xu1 (tianyang.xu@jiangnan.edu.cn) Xiao-Jun Wu1 (wu xiaojun@jiangnan.edu.cn) Josef Kittler2 (j.kittler@surrey.ac.uk) Affiliations: 1 Jiangnan University 2 University of Surrey, UK SUP Team Members: Yashu Kang1,2 (kangyashu@supconit.com) Zhehao Shen1,3 (20234246028@stu.suda.edu.cn) Yuzhe Cen1,4 (yc4494@columbia.edu) Affiliations: 1 Zhejiang Supcon Information Technology 2 Zhejiang University of Technology 3 Soochow University 4 Columbia University Affiliations: 1 Xidian University Tang TUTE Team Members: Guopeng Tang1 (pastwill@163.com) Affiliations: 1 Tianjin University of Technology and Education Tele AI Team Members: Hui Li1 (lih@chinatelecom.cn) Zhaofan Zou1 (zouzhf41@chinatelecom.cn) Feiyu Wang2 (wfy 0502@163.com) Yanbin Huang3 (thinkfulcat@gmail.com) Yilin Tao4 (tyilin034725@163.com) Affiliations: 1 Institute of Artificial Intelligence, China Telecom 2 Beijing Language and Culture University 3 Huazhong University of Science and Technology 4 University of Chinese Academy of Sciences WDL Team Members: Zhenglin Du1 (zhenglin du@163.com) Yi Wen1 (18261172307@163.com) Zhengyang Li1 (li zhengyang1222@163.com) Affiliations: 1 Xidian University yqhhh Team Members: Yiqing Wang1 (24171213882@stu.xidian.edu.cn) Jing He1 (24171213874@stu.xidian.edu.cn) Affiliations: 1 Xidian University ZLC Team Members: Zihan Zhai1 (25171213969@stu.xidian.edu.cn) Tingting Li1 (25241215337@stu.xidian.edu.cn) Yuying Chen1 (25171214015@stu.xidian.edu.cn) Affiliations: 1 Xidian University zls123 Team Members: Xiaopeng Zhou1 (z2362910193@gmail.com) Chaoyang Liao1 (cylio529@gmail.com) Zhilong Song1 (908194340@qq.com)"
        }
    ],
    "affiliations": [
        "ByteDance",
        "Meituan",
        "NVIDIA",
        "Samsung"
    ]
}