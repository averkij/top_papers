{
    "paper_title": "Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation",
    "authors": [
        "Inferix Team",
        "Tianyu Feng",
        "Yizeng Han",
        "Jiahao He",
        "Yuanyu He",
        "Xi Lin",
        "Teng Liu",
        "Hanfeng Lu",
        "Jiasheng Tang",
        "Wei Wang",
        "Zhiyuan Wang",
        "Jichao Wu",
        "Mingyang Yang",
        "Yinghao Yu",
        "Zeyu Zhang",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation. Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 4 1 7 0 2 . 1 1 5 2 : r Inferix: Block-Diffusion based Next-Generation Inference Engine for World Simulation Inferix Team World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for new paradigm that moves beyond current LLM-centric vision foundation models. key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in blocks-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation. Therefore, Inferix is specifically designed as next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration. Date: November 27, 2025 Code: https://github.com/alibaba-damo-academy/Inferix"
        },
        {
            "title": "1 Introduction",
            "content": "World models are capable of generating interactive, long-form, and physically plausible video sequences. Most current video diffusion models [34] rely on the Diffusion Transformer (DiT) [28]which uses bidirectional attention without KV caching. While this enables parallelized generation and controllability, decoding is inefficient and restricted to fixed lengths. In contrast, AR-based frameworks [35] support variable-length generation and KV Cache management, but their generation quality lags behind video diffusion, and decoding is not parallelizable. Importantly, block diffusion [13, 33] interpolates between AR and diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation, as shown in Figure 1. The overall framework of Inferix is illustrated in Figure 2. The model generates clean video block from noise via iterative denoising. Crucially, the attention mechanism at each step leverages global KV Cache containing context from previously generated blocks. After new block is generated, its KV information is used to update the cache, providing context for subsequent blocks. This generate-and-cache loop facilitates efficient, arbitrary-length video generation. new paradigm inevitably brings forth new infrastructure and fundamental research, just as the LLM era gave rise to vLLM [18] & SGLang [46], the Visual Diffusion era to xDiT [5] and FastVideo [32], and the Post-training era to OpenRLHF [11] and verl [29], etc. Now, the world model era also demands its own dedicated inference engine, and Inferix is purpose-built as next-gen inference engine, empowering immersive world synthesis via optimized semi-autoregressive decoding paradigm. Figure 1 Architecture comparison. AR vs. Diffusion vs. Block Diffusion (Semi-AR). Block Diffusion combines the strengths of both AR and Diffusion, enabling arbitrary-length generation, KV caching, and high parallelizability within each block. Key features of Inferix are as follows: Next-Generation Inference Paradigm: block diffusion framework built for immersive world synthesis at scale. Efficient Long Video Generation Benchmarking: Integrated with LV-Bench, fine-grained benchmark for minute-long videos with dedicated metrics to evaluate long-range coherence. Video Streaming: Basic video streaming capabilities for generated content, with both RTMP and WebRTC supported as streaming protocols. Continuous Prompt Support: Enable dynamic narrative control with different prompts for different video segments. Advanced KV Cache Management: Intelligent memory management for persistent world simulation. Distributed World Synthesis: Support multiple parallelism for large-scale immersive environment generation. Built-in Profiling: Performance monitoring and analysis capabilities with enhanced diffusion model profiling."
        },
        {
            "title": "2 Challenges in Inference of World Simulation",
            "content": "During the inference of world simulation, the world models need to generate long-form video sequences. Moreover, for current world models and video generation models, their model size is pretty large. The large model size and long-form video sequences bring unprecedented pressure to storage and computing. For storage, the usage of KV Caches is the main bottleneck. In world simulation, the KV Caches of former blocks need to be stored as the context for the generation of current and future blocks, which is important to ease the drifting and forgetting problem [43] when generating long video sequences. However, these KV Caches will consume large amount of GPU memory. Therefore, how to make KV Cache management efficient is important for the inference of world simulation. Some advanced techniques that have been studied in LLM inference need to be brought to the inference of world simulation, such as PageAttention [18], offload [30, 19], KV Cache compression [26, 21], and so on. For computation, the large model size and extremely lone video sequences increase the amount of computation greatly. For example, it will consume about 6,800 seconds when generating 5-second video with Wan2.1 14B in single NVIDIA H20. For the inference of world simulation, the computation will be much more 2 Figure 2 Framework of Inferix. To enhance the efficiency of block diffusion models, INFERIX provides set of interconnected components: efficient parallel strategies, block-wise KV Cache management, DAX [1] quantization, real-time video streaming, and fine-grained video evaluation. heavier due to the longer context. Therefore, its significant to accelerate the computation of world simulation to make it accessible. There are several methods that can be taken to achieve this: quantization to utilize low-bit computation [45, 20], sparse attention [39, 42], decreasing the denoising steps [40, 8], leveraging the redundancy during inference [24, 44], utilizing distributed computation [6, 7], and so on."
        },
        {
            "title": "3.1 Parallelism",
            "content": "To accelerate the inference process and minimize per-GPU memory footprint, Inferix employs suite of parallelism techniques tailored for long sequence models. These include Ulysses-style sequence parallelism [16], which partitions independent attention heads across multiple GPUs to relieve memory pressure while preserving computational efficiency, and Ring Attention [25, 38], which enables scalable attention computation over long sequences by distributing attention operations in ring topology. Depending on the selected attention mechanism, ring attention can either pass queries or pass keys and values, leading to different performance profiles. Inferix selects the most suitable parallelism strategy based on model architecture, network topology, and communication overhead. This adaptive approach ensures optimal resource utilization and performance across deployment scenarios."
        },
        {
            "title": "3.2 KV Management",
            "content": "Block-Diffusion-based models leverage KV caches to accelerate the generation process. To support KV cache access from various kinds of models, Inferix provides unified KV management interface backed by block-wise KV memory management. To maintain scalability in the face of future-time models requiring both sliding-window access patterns and selective global KV context dependency, the KV management system preserves the extensibility of flexible KV fetching methods including both range-based chunked access and index-based selective fetch. Latent store used in Multi-latent Attention (MLA) [23] and offloading to main memory for GPU memory optimization are also supported to keep the KV management future-proof. 3 Table 1 Overview of the datasets used for constructing LV-Bench. Dataset Video Number Object Classes DanceTrack GOT-10k HD-VILA-100M ShareGPT4V 66 272 117 545 Humans (66, 100%) Humans (177, 65%) Animals (54, 20%) Environment (41, 15%) Humans (47, 40%) Animals (35, 30%) Environment (35, 30%) Humans (381, 70%) Animals (82, 15%) Environment (82, 15%) LV-Bench 1000 Humans (671, 67%) Animals (171, 17%) Environment (158, 16%)"
        },
        {
            "title": "3.3 Models and Pipelines",
            "content": "The Inferix framework is designed to support variety of block diffusion models, currently MAGI-1 [33], CausVid [41], and Self Forcing [13] as examples. These models differ in their foundations: CausVid and Self Forcing are built upon Wan2.1, 5-second full-attention base diffusion video model, while MAGI-1 is trained from scratch with distinct infrastructure. To efficiently accommodate this diversity, Inferix first abstracts their shared computational patterns into generalized inference pipeline. Building on this abstraction, we then design and integrate several key components, such as sophisticated KV Manager and suite of parallel strategies, to significantly boost inference performance. Users are welcome to integrate their own models with these abstractions and interfaces."
        },
        {
            "title": "3.4 System Profiling",
            "content": "Inferix provides built-in performance profiling mechanism that enables end-to-end visibility into resource utilization during inference. The profiler includes three key characteristics: Near zero overhead. The full profile only incurs minimal overhead of less than 5%, compared with no profiling. Highly customizable. In addition to GPU usage and system-wide metrics, Inferix allows users to add custom metrics during inference. Users can define custom metrics via lightweight hooks or callbacks that execute inline with inference, enabling domain-specific measurements. Easy to use. The profiler exposes both Python decorator and context manager. The Python decorator enables declarative profiling of individual functions, while the context manager supports block-level instrumentation for broader code regions with almost no code change."
        },
        {
            "title": "3.5 Video Streaming",
            "content": "When generating long videos or executing world simulation, it is important to enable dynamic narrative control with different signals for different video chunks. These signals include prompts, motions, inputs from peripherals and so on. For example, when inference with CausVid, Inferix supports generating long video whose different video chunks are controlled by different prompts specified by users. If different prompt is given when generating new video chunk, Inferix will clear the cross-attention cache to eliminate the influence brought by the former prompt."
        },
        {
            "title": "4.1 Dataset\nTo address the challenge of generating minute-long videos, we construct LV-Bench, a large-scale benchmark\ncomprising 1,000 long-form videos collected from diverse open-source sources. As summarized in Table 1, we\nselect high-resolution videos exceeding 50 seconds from DanceTrack [31], GOT-10k [12], HD-VILA-100M [37],\nand ShareGPT4V [3].",
            "content": "4 To ensure comprehensive temporal coverage and linguistic diversity, we employ GPT-4o as data engine to generate detailed captions every 23 seconds. The prompting pipeline and examples are included in Subsection 4.3. To guarantee annotation quality, we adopt rigorous human-in-the-loop validation framework across all stages: (1) data sourcing, where annotators filter out low-quality or unsuitable clips; (2) chunk segmentation, where human reviewers ensure temporal coherence and eliminate transition artifacts; and (3) caption verification, where annotators refine automatically generated descriptions for semantic accuracy and temporal alignment. Each validation stage involves at least two independent reviewers to maintain inter-rater reliability. Finally, the curated dataset is divided into an 80/20 trainevaluation split."
        },
        {
            "title": "4.2 Metrics",
            "content": "Evaluating long-form video generation requires assessing both spatial fidelity and temporal stability. Prior studies [22, 27] introduce drift penalties to quantify degradation over time, focusing on identity consistency [10] and perceptual robustness [36]. Inspired by the Mean Absolute Percentage Error (MAPE) and Weighted MAPE [17, 4], we propose unified metric, Video Drift Error (VDE), which measures relative quality changes across the temporal axis. Building upon VDE, we design five complementary metrics for long-horizon video evaluation: (1) VDE-Clarity, assessing temporal drift in image sharpness; (2) VDE-Motion, quantifying smoothness of motion dynamics; (3) VDE-Aesthetic, capturing consistency of visual appeal; (4) VDE-Background, measuring spatial stability of scene layouts; and (5) VDE-Subject, detecting identity drift in primary subjects. Lower scores in each indicate stronger temporal consistency. Following prior benchmarks [9, 2], we also integrate five complementary quality dimensions from VBench [15]: Subject Consistency , Background Consistency , Motion Smoothness , Aesthetic Quality , and Image Quality . Together, these metrics form comprehensive protocol for evaluating long video generation models."
        },
        {
            "title": "4.3 Prompts for LV-Bench’s Data Engine",
            "content": "Role. Act as professional video content analyst. Describe given video frame in English. Context. The previous frame was described as: \"{previous_description}\". Use this as context to ensure temporal coherence. Instruction. Write single, descriptive paragraph that: Identifies the main subject, their specific actions, and expressions. Describes the environment and background, including setting and lighting. Highlights the cinematic quality, such as composition, color palette, and atmosphere (e.g., tense, serene, spectacular). Constraints. Output must be one coherent paragraph, written in natural language prose, without bullet points or numbered lists. Return. The paragraph description of the current frame."
        },
        {
            "title": "5 Development Roadmap",
            "content": "Support more complex KV Management, with flexible block-sparse attention Support finetuning pretrained video generation model (Diffusion to Semi-AR) & distill models into few steps [14, 40] Support high-concurrency deployment Support more complex distributed inference Improve video streaming usage and performance Support more advanced real-time, interactive streaming capabilities"
        },
        {
            "title": "6 Conclusion",
            "content": "We develop Inferix, block-diffusion based next-generation inference engine for world simulation, which integrates some important features and new benchmark for long video generation. The inference engine takes the differences between block-diffusion generation and former generation paradigms as the starting point, which intends to make the researches in world model and long video generation more convenient. Inferix unifies the inference interfaces of different block-diffusion models and apply several efficient inference techniques, which are important to improve ease of use. The LV-Bench integrated in Inferix aims to evaluate the quality of long video generation precisely and efficiently, which is also valuable for the development of world model. For future works, more efficient inference techniques specific to block-diffusion generation will be taken into considerations, which includes sparse attention, feature cache, step distillation and so on. We hope Inferix will become useful tool for this."
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "We are joint team from Zhejiang University & Hong Kong University of Science and Technology & Alibaba DAMO Academy & Alibaba TRE. All current contributors of Inferix are listed in alphabetical order by their last names. We warmly welcome everyone to join our virtual team and together harness the collective power of community. Contributors: Tianyu Feng Yizeng Han Jiahao He Yuanyu He Xi Lin Teng Liu Hanfeng Lu Jiasheng Tang Wei Wang Zhiyuan Wang Jichao Wu Mingyang Yang Yinghao Yu Zeyu Zhang Bohan Zhuang"
        },
        {
            "title": "References",
            "content": "[1] Dax: Diffusion accelerated execution. https://github.com/RiseAI-Sys/DAX, 2025. [2] Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [4] Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, and Fabrice Rossi. Mean absolute percentage error for regression models. Neurocomputing, 192:3848, 2016. [5] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. [6] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Xibo Sun. Pipefusion: Patch-level pipeline parallelism for diffusion transformers inference. In Advances in Neural Information Processing Systems, 2025. [7] Jiarui Fang and Shangchun Zhao. Usp: unified sequence parallelism approach for long context generative ai, 2024. [8] Youping Gu, Xiaolong Li, Yuhao Hu, Minqi Chen, and Bohan Zhuang. Blade: Block-sparse attention meets step distillation for efficient video generation. arXiv preprint arXiv:2508.10774, 2025. [9] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [10] Wenkang Han, Wang Lin, Yiyun Zhou, Qi Liu, Shulei Wang, Chang Yao, and Jingyuan Chen. Show and polish: reference-guided identity preservation in face video restoration. arXiv preprint arXiv:2507.10293, 2025. [11] Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, Haotian Xu, and Yiming Liu. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework, 2025. [12] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. IEEE transactions on pattern analysis and machine intelligence, 43(5):1562 1577, 2019. [13] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [14] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion, 2025. [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [16] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. System optimizations for enabling training of extreme long sequence transformer models. In Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing, PODC 24, page 121130, New York, NY, USA, 2024. Association for Computing Machinery. [17] Sungil Kim and Heeyoung Kim. new metric of absolute percentage error for intermittent demand forecasts. International Journal of Forecasting, 32(3):669679, 2016. 7 [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [19] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. Infinigen: Efficient generative inference of large language models with dynamic kv cache management, 2024. [20] Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [21] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. [22] Zhuoling Li, Hossein Rahmani, Qiuhong Ke, and Jun Liu. Longdiff: Training-free long video generation in one go. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1778917798, 2025. [23] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [24] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model, 2025. [25] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [27] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37:131434131455, 2024. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [29] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. [30] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with single gpu, 2023. [31] Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai, Kris Kitani, and Ping Luo. Dancetrack: Multi-object tracking in uniform appearance and diverse motion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2099321002, 2022. [32] The FastVideo Team. Fastvideo: unified framework for accelerated video generation, April 2024. [33] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 8 [35] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. [36] Qi Xie, Yongjia Ma, Donglin Di, Xuehao Gao, and Xun Yang. Moca: Identity-preserving text-to-video generation via mixture of cross attention. arXiv preprint arXiv:2508.03034, 2025. [37] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50365045, 2022. [38] Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jeremy Reizenstein, Jongsoo Park, and Jianyu Huang. Context parallelism for scalable million-token inference, 2025. [39] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. In Advances in Neural Information Processing Systems, 2025. [40] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis, 2024. [41] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. 2025. [42] Jintao Zhang, Chendong Xiang, Haofeng Huang, Haocheng Xi, Jun Zhu, Jianfei Chen, et al. Spargeattention: Accurate and training-free sparse attention accelerating any model inference. In International Conference on Machine Learning, 2025. [43] Lvmin Zhang, Shengqu Cai, Muyang Li, Gordon Wetzstein, and Maneesh Agrawala. Frame context packing and drift prevention in next-frame-prediction video diffusion models, 2025. [44] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention, 2025. [45] Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation, 2024. [46] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024."
        }
    ],
    "affiliations": [
        "Alibaba DAMO Academy"
    ]
}