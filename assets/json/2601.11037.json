{
    "paper_title": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search",
    "authors": [
        "Shiyu Liu",
        "Yongjing Yin",
        "Jianhao Yan",
        "Yunbo Tang",
        "Qinggang Zhang",
        "Bei Li",
        "Xin Chen",
        "Jingang Wang",
        "Xunliang Cai",
        "Jinsong Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search."
        },
        {
            "title": "Start",
            "content": "BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search Shiyu Liu1,2,, Yongjing Yin2, Jianhao Yan4, Yunbo Tang3, Qinggang Zhang5* Bei Li2, Xin Chen2, Jingang Wang2, Xunliang Cai2, Jinsong Su1,3* 1Institute of Artificial Intelligence, Xiamen University 2Meituan Inc. 3School of Informatics, Xiamen University 4Westlake University 5The Hong Kong Polytechnic University shiyu.liu0709@foxmail.com; zqg.zhang@hotmail.com; jssu@xmu.edu.cn 6 2 0 2 6 1 ] . [ 1 7 3 0 1 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit DONT KNOW (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose BoundaryAware Policy Optimization (BAPO), novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search1."
        },
        {
            "title": "Introduction",
            "content": "Recent advances leverage reinforcement learning (RL) to optimize Large Language Models (LLMs) as autonomous agents that actively plan and execute multi-turn searches (Jin et al., 2025a; Chen et al., 2025; Song et al., 2025). While the RLbased agentic search substantially improves accuracy on complex, knowledge-intensive questions, it introduces critical reliability issue: these RLbased models almost never admit DONT KNOW *Corresponding author. Work done during internship at Meituan Inc. 1Our available code is at https://github.com/ Liushiyu-0709/BAPO-Reliable-Search. Figure 1: When the agentic search model produces wrong answers, its lengthy and complex reasoning makes it difficult for users to verify. To ensure reliability, the model should explicitly state when information is insufficient and that no answer is available. (IDK) even when evidence is insufficient or reasoning reaches its limit (Zhang et al., 2025c; Joren et al., 2025). As presented in Figure 1, when an agentic search model cannot reach concrete final answer, it should explicitly admit IDK rather than fabricating answers. This capability is especially valuable given the difficulty for users to verify the lengthy reasoning process, as honest acknowledgment of IDK helps users seek alternative information sources and prevents misinformation. Previous research attributes the ability to acknowledge uncertainty to models awareness of its internal boundary (Li et al., 2025b; Zhang et al., 2025c). Specifically, EKBM (Zheng et al., 2025) verifies information against its parametric knowledge boundary to answer factual questions accurately, while CapBound (Zhang et al., 2025c) introduces similar concept to determine whether problem is solvable within its capability boundary for mathematical reasoning. However, the reasoning boundary for an agentic search model is inherently more complex and dynamic, as such boundary is continuously shaped by the interplay between the models internal reasoning capacity for search tool interaction and the external retrieved information. compromising average accuracy. To investigate the impact of RL on an agentic search models awareness of its reasoning boundaries, we conduct preliminary experiments measuring this awareness before and after RL training. key finding is that while RL optimization improves accuracy on complex questions, it significantly degrades the models awareness of its boundary. This occurs because standard RL rewards incentivize exhaustive exploration for correctness while simultaneously discouraging any acknowledgment of uncertainty (Jin et al., 2025a; Song et al., 2025). As result, these models often lack awareness of their boundaries and fabricate plausible answers. Enhancing the boundary awareness of agentic search models is crucial but challenging. (i) It is difficult to quantitatively measure models reasoning boundary, which makes it hard to construct reliable reward signal for IDK responses. Unlike static knowledge or capability boundaries, which are tied solely to the models inherent competencies, the boundary of an agentic search model is dynamic, emergent property of the interaction between the agents planning, the quality of its retrievals, and its iterative reasoning steps. Consequently, we lack robust strategy to measure whether the agent has genuinely encountered its limit. (ii) It is challenging to integrate such signal into the RL objective without creating unintended incentives. naive reward that encourages IDK responses could be exploited as low-effort shortcut, especially on difficult problems requiring deep exploration. If the reward function prioritizes IDK over incorrect answers, the model may learn to default to IDK prematurely, ultimately harming the reasoning accuracy and exploration depth. To this end, we propose Boundary-Aware Policy Optimization (BAPO) for agentic search, designed to cultivate reliable self-awareness without compromising accuracy. Our research aims to address two fundamental research questions: (i) how to precisely model the learning signal for identifying the boundary of agentic search models, and (ii) how to integrate this signal into an RL framework to balance deep exploration and appropriate exploitation. Results across four challenging benchmarks demonstrate that BAPO consistently outperforms mainstream training-based and prompt-based methods in overall reliability. Remarkably, with only 5000 RL training samples, BAPO with Qwen2.57B-Instruct shows superior reliability compared to strong open-source agentic search models, without Our major contributions are listed as follows: We identify the key limitation of agentic search models and propose BAPO, novel RL framework that dynamically rewards IDK responses to cultivate the models selfawareness of its reasoning boundaries. BAPO introduces group-based boundaryaware reward that encourages an IDK response only when the question is out of the models boundary, and an adaptive reward modulator to prevent reward hacking. Extensive experiments on challenging benchmarks demonstrate that BAPO effectively enhances search reliability while preserving the models ability to solve complex problems."
        },
        {
            "title": "2.1 Agentic Search",
            "content": "To enhance response quality in knowledgeintensive scenarios, Retrieval-Augmented Generation (RAG) supplements LLMs parametric knowledge with external search (Lewis et al., 2020; Gao et al., 2023; Fan et al., 2024). Current promptingbased strategies often depend on predefined workflows for interleaving reasoning and search (Asai et al., 2024; Li et al., 2025c), resulting in limited effectiveness. While learning-based methods (SFT/DPO) offer an alternative (Lin et al., 2024; Wang et al., 2025; Fang et al., 2025; Li et al., 2025d), they remain constrained by complex data pipelines and suboptimal generalization. Inspired by the success of reinforcement learning on mathematics and coding (Jaech et al., 2024; DeepSeek-AI et al., 2025), RL-based agentic search systems have emerged, such as SearchR1 (Jin et al., 2025a), ReSearch (Chen et al., 2025), R1-Searcher (Song et al., 2025), and ToolStar (Dong et al., 2025). These methods employ format and outcome correctness rewards to enable autonomous query decomposition and flexible interaction with external search systems during reasoning, thereby achieving superior accuracy on complex queries. Nevertheless, current methods prioritize search accuracy while ignoring holistic reliability, which depends on both accuracy and precision. In contrast, our proposed BAPO leverages boundary-aware reward mechanisms to optimize overall reliability, thereby maximizing the practicality of agentic search models."
        },
        {
            "title": "2.2 Formalization",
            "content": "rectness of the final answer. To make it clear, we formally define Agentic Search as sequential decision-making process following the ReAct paradigm (Yao et al., 2023). Given query x, the agent aims to generate trajectory τ that concludes with final answer y. To structure this reasoning process, system prompt is included to constrain the models output format. For instance, the model is required to use tags <think>, <search>, and <answer> to indicate thoughts, search tool calls, and the predicted answers, respectively, with search results returned in <result>. The prompt template is detailed in Appendix C.1. As formalized below, the trajectory τ is sequence of interleaved reasoning steps, actions, and observations: τ = (cid:0)(rt, at, ot)T 1 t=1 , rT , y(cid:1), (1) where each rt denotes the reasoning state at step that plans subsequent actions and rT synthesizes accumulated information to conclude to the final answer y; at is the search action; and ot is the returned search results. As RL has demonstrated significant efficacy in enhancing LLM reasoning capabilities, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) has been extensively employed in training agentic search models (Jin et al., 2025a; Chen et al., 2025; Dong et al., 2025). For query x, group of trajectories {τ1, τ2, . . . , τG} is sampled from the policy, and the objective function is defined as: (θ) = xD,{τi}G i=1πθold"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (x) (2) [min (wi(θ)Ai, clip (wi(θ), 1 ϵ, 1 + ϵ) Ai)] , where wi(θ) = πθ(τix) (τix) and the KL term against reference model is omitted. The advantage Ai is computed by normalizing the rewards within the πθold group, Ai = R(τi)mean({R(τj )}G std({R(τj )}G j=1) j=1) . Rule-based rewards have demonstrated robust empirical performance and are widely adopted in current research (Song et al., 2025; Chen et al., 2025). Typical reward design focuses on correctness and consists of two parts: (i) format correctness reward that checks whether the output conforms to the prescribed structure across reasoning steps, tool calls, and answers; and (ii) an outcome correctness reward that measures the objective corRCorrect = 1(ˆy, y), correct format 1, wrong format, (3) where F1 means character-level F1 score between two strings, is the prediction answer extracted from trajectory and ˆy is the ground truth answer."
        },
        {
            "title": "3 Preliminary Study",
            "content": "In this study, we first investigate the impact of RL training with correctness-based reward on the boundary awareness of agentic search models. Subsequently, we incorporate an additional reward term designed to encourage IDK responses and evaluate its effect. Our findings reveal an inherent trade-off between maximizing accuracy and maintaining effective boundary awareness."
        },
        {
            "title": "3.1 Setup",
            "content": "Training. For training, we use high-quality RL training dataset from Dong et al. (2025), comprising 5000 multi-hop QA samples from HotpotQA and 2WikiMultihopQA. Following the coldstart-free paradigm, we directlt apply GRPO to Qwen2.5-3B-Instruct (Yang et al., 2025b), bypassing preliminary in-domain SFT. Evaluation. For evaluation, we use four multihop QA benchmarks: HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), 2WikiMultiHopQA (Ho et al., 2020), and Bamboogle (Press et al., 2023). Specifically, HotpotQA, 2WikiMultiHopQA and MuSiQue are constructed from Wikipedia or Wikidata (Vrandeˇcic and Krötzsch, 2014) using diverse multi-hop mining strategies, while Bamboogle is manually curated dataset of 2-hop challenging questions. Following the setup in Dong et al. (2025), we construct validation set of 80 multi-hop QA pairs, comprising 20 examples from the test set of each benchmark. In contrast, the results reported in Section 5 are evaluated on the complete test sets. Metrics. To comprehensively evaluate model reliability, we follow Xu et al. (2024) to use three following metrics: accuracy (acc), precision (prec) and IDK rate (ρIDK). These metrics are defined as follows: acc ="
        },
        {
            "title": "Nc\nN",
            "content": ", prec = Nc Nr , ρIDK ="
        },
        {
            "title": "Nr\nN",
            "content": ", (4) As illustrated in Figure 2, before RL training, Qwen2.5-7B-Instruct shows notable precision advantage over its accuracy (50.76 vs. 41.25), accompanied by substantial IDK rate of 18.75%, effectively filtering uncertain cases. However, after RL training (ReSearch-Qwen-7B-Instruct), while accuracy improves to 51.25, IDK rate drops sharply to 3.65%, and precision increases only marginally to 53.24. This indicates weakened boundary awareness that suppresses IDK admission."
        },
        {
            "title": "Accuracy Advancement",
            "content": "Since correctness-based rewards alone weaken the models awareness of when to acknowledge IDK, natural approach is to incorporate appropriate positive rewards for IDK responses during RL training. Following the training setting in Section 3.1, we instruct the model with reliable prompt and modify the correctness reward function by assigning an additional reward of 0.5 to IDK responses within rollout groups that lacks correct answers. This design prioritizes IDK responses over incorrect answers when correct answer is unavailable. However, as shown in Figure 3, directly incentivizing IDK responses leads to reward hacking: the model learns to maximize rewards by defaulting to IDK rather than attempting to solve challenging problems. Consequently, this modified reward impedes accuracy gains compared to the vanilla reward. This finding underscores fundamental challenge in training reliable models: balancing the models exploration for correctness rewards against its exploitation for IDK rewards."
        },
        {
            "title": "4 The Framework of BAPO",
            "content": "With observations and insights from the preliminary study, we propose Boundary-Aware Policy Optimization (BAPO), an RL algorithm built upon GRPO for training reliable agentic search models. As shown in Figure 4, BAPO uses boundaryaware reward to encourage IDK responses according to whether problem is out of models boundary. More importantly, it uses an adaptive reward modulator to balance exploration and exploitation, mitigating the reward hacking issue identified in Section 3.3."
        },
        {
            "title": "4.1 Boundary-Aware Reward",
            "content": "We regard that question exceeds the models boundary if the model fails to get any correct Figure 2: Evaluation results of accuracy, precision and IDK rate (ρIDK) of models before and after RL. The sharp drop in ρIDK coupled with the narrowing gap between accuracy and precision, indicates diminished boundary awareness after RL. Figure 3: Left: Validation accuracy under different reward settings during RL training. Right: IDK rate (ρIDK) under the modified reward during RL training. where = Nc + Nw + Nr is the total number of answers, with Nc, Nw, Nr denoting the number of correct answers, wrong answers and IDK answers, respectively. Following previous studies (Chen et al., 2025; Dong et al., 2025), we extract answers from the model output enclosed in box{}, and then use GPT-4 as judge to assess the correctness (Appendix C.2)."
        },
        {
            "title": "Boundary Awareness",
            "content": "To evaluate the impact of RL on boundary awareness, we compare Qwen2.5-7B-Instruct with its variant, ReSearch-Qwen-7B-Instruct (Chen et al., 2025), which is optimized via GRPO using correctness-based reward. To elicit explicit IDK admissions, we follow Xu et al. (2024) to append the following reliable prompt to the original one:"
        },
        {
            "title": "Reliable Prompt",
            "content": "If you cant solve this question by the reasoning process, you should output boxed{I DONT KNOW}. Figure 4: The overall framework of BAPO. Its training process begins with (a) agentic reasoning, where the model generates group of rollouts for each question by interleaving thought processes with search environment interactions. These rollouts are then passed to the (b) reward computation module, which is comprised of (b.1) correctness reward RCorrect derived from the correctness of format and outcome, (b.2) boundary-aware reward RIDK designed to incentivize IDK responses when no correct rollout exists within the group, and (b.3) adaptive reward modulator adaptively disabling RIDK based on IDK ratio at the exploration stage and diversity of rollouts at the plateau stage. answer among multiple rollouts. In this situation, we adjust the reward assignment for IDK responses. Formally, for set of rollout outputs {τi}G i=1 and their corresponding correctness rewards {RCorrect }G i=1, the boundary-aware reward is calculated as: RIDK = 0.5 I(yi = IDK), if {RCorrect(τi)}G i=1 0. (5) The final reward is the sum of RCorrect and RIDK: = RIDK + RCorrect. (6)"
        },
        {
            "title": "4.2 Adaptive Reward Modulator",
            "content": "Our preliminary analysis suggests that purely incentivizing IDK responses leads to degradation in exploration. To balance exploration and boundary awareness during RL training, we introduce an adaptive reward modulator for RIDK, which functions at two distinct levels: Stage Level. In the early stages of training, the model needs to conduct extensive exploration to learn how to solve tasks. During this exploration stage, we avoid introducing additional rewards to interfere with the models learning. RIDK is deactivated by default and only activated when the proportion of IDK responses RIDK falls below α. As training progresses, the task accuracy gradually stabilizes. At this plateau stage, we apply the reward RIDK fully to make the model start paying more attention to its boundary awareness. Some difficult queries may not be easily answered correctly or admitted as unknown under the default group size, and thus we dynamically resample groups with no correct rollouts up to times, until the model outputs IDK or provides correct answer. The transition between the exploration and plateau stages is determined by the stagnation of the validation score, e.g., when the validation score plateaus for 5 consecutive steps. Sample Level. The consistency of models generated responses can serve as proxy for its confidence. To further preserve the models exploration during the plateau stage, we adaptively modulate RIDK based on the rollout diversity of each query. Specifically, for queries exhibiting high rollout diversity, indicating that the model is actively exploring the solution space, we deactivate RIDK to prevent untimely convergence. In contrast, for samples with low rollout diversity, which signifies that the model has converged on specific output, RIDK is applied to refine its boundary awareness. Formally, sample is considered to have high rollout diversity if its corresponding rollout answers {y1...G} 2 ; otherwise, it is categorized as having low diversity."
        },
        {
            "title": "5.1 Experimental Setups",
            "content": "Baselines. We compare BAPO against (1) existing RL agentic search methods, and (2) our implementations of prompting and training-based methods. For the former, we select two representative approaches: Search-R1 (Jin et al., 2025a) and ReSearch (Chen et al., 2025). We use Qwen2.5-7BInstruct as the backbone for fair comparison. The prompt-based baselines include: (1) Naive RAG: naive retrieval-based approach that directly concatenates retrieved context with the query. (2) IRCoT (Trivedi et al., 2023): An interleaving method where retrieval and CoT mutually guide each other. (3) TIR Prompt (Dong et al., 2025): standard toolintegrated prompt, which instructs the model to use search tools during reasoning (Appendix C). (4) Reliable TIR Prompt: Building on TIR Prompt, it additionally encourages the model to acknowledge ignorance by responding with IDK (Section 3.2). For training-based baselines, we choose (1) Reliable Rejection Sampling Fine-Tuning (Reliable RFT), which generates multiple reasoning paths for each question under Reliable TIR Prompt, and incorporates both correct paths and reasonable IDK responses into the SFT dataset. Similar to BAPO, an IDK response is adopted when no correct path is available. (2) GRPO (Shao et al., 2024), which uses only the correctness reward, with other settings matching those of BAPO. Reliability Metric. We follow Xu et al. (2024) to adopt reliability metric to comprehensively balance accuracy and precision: reliability = (1 ρIDK) prec + ρIDK acc, (7) where ρIDK denotes the IDK rate. This metric functions as dynamic interpolation between precision and accuracy based on the models tendency to refuse. Specifically, when the agent rarely refuses to give an answer (low ρIDK), the metric prioritizes precision, enforcing strict correctness on generated answers. Conversely, as the agent tends to refuse more frequently (high ρIDK), the metric shifts focus toward standard accuracy . This mechanism effecively penalizes excessive lazy IDK. threshold α as 5%, and the resampling times is set to 2. The retrieval environment is based on FlashRAG (Jin et al., 2025b). We use E5base-v2 (Wang et al., 2024) as the retriever and Wikipedia data (Vrandeˇcic and Krötzsch, 2014) from December 2018 as the knowledge base. During training and evaluation, we retrieve top-5 results for each query. We train the GRPO and BAPO models with batch size of 64 for 2 epochs. The rollout size is set to 8, and the maximum tokens are set to 8192. The maximum number of tool invocations during training and inference is set to 3. Sensitivity analysis of the hyper-parameters are present in Appendix B.1."
        },
        {
            "title": "5.2 Main Results",
            "content": "The performance of BAPO and other baselines conducted on Qwen2.5-7B-Instruct is presented in Table 1. Compared to all methods from both existing agentic search and our implemented baselines, BAPO achieves significant reliability improvements across all benchmarks (+15.8 scores in average). Notably, compared to existing agentic search models such as Search-R1 (Jin et al., 2025a) and ReSearch (Chen et al., 2025), which utilize large-scale training sets of 90k and 19k samples respectively, BAPO-trained model with only 5k samples achieves competitive accuracy and substantially higher reliability. When compared to GRPO, BAPO achieves average improvements in reliability of 9.7%, with corresponding precision improvements of 11.8%, while incurring only marginal accuracy decreases of 2.2%. These results showcase BAPOs effectiveness and efficiency in training reliable agents. Regarding methods with reliable techniques, such as Reliable RFT and Reliable TIR Prompt, we find that Reliable RFT tends to be overconservative; although they achieve significant gains in precision, this come at the cost of drastic decline in accuracy, suffering 27-point accuracy drop compared to the TIR Prompt baseline. This ultimately undermines overall reliability. While Reliable TIR Prompt enhances precision without sacrificing accuracy, it inherently lacks the capacity to further improve problem-solving capability. On the contrary, BAPO achieves both boundary awareness and problem-solving capability."
        },
        {
            "title": "5.3 Generalization Across Model Scales",
            "content": "Implementation Details. Regarding BAPOspecific hyper-parameters, we set the IDK ratio To verify BAPOs generalization across different model scales, we extend our evaluation to the 3B"
        },
        {
            "title": "MusiQue",
            "content": "2Wiki."
        },
        {
            "title": "Bamboogle",
            "content": "Acc Prec Rel. Acc Prec Rel. Acc Prec Rel. Acc Prec Rel."
        },
        {
            "title": "Existing Agentic Search Methods",
            "content": "Search-R1 ReSearch 49.0 61.5 49.0 61.5 49.0 61.5 22.5 31.0 22.5 31. 22.5 31.0 39.0 54.2 39.0 54.2 39.0 54.2 52.0 54.4 52.0 54. 52.0 54."
        },
        {
            "title": "Our Implementations",
            "content": "Naive RAG IRCoT TIR Prompt Reliable TIR Prompt Reliable RFT GRPO BAPO (Ours) 49.6 52.1 51.5 52.5 24.5 60.0 58. 49.6 52.1 51.5 62.1 68.0 60.0 66.6 49.6 52.1 51.5 60.6 40.2 60.0 65. 12.7 14.2 21.5 21.0 11.0 29.5 29.5 12.7 14.2 21.5 30.0 36.6 29.5 38. 12.7 14.2 21.5 27.2 18.5 29.5 36.6 29.5 30.6 43.0 35.5 14.5 59.5 29.5 30.6 43.0 45. 48.2 59.5 57.0 64.1 29.5 30.6 43.0 43.3 23.9 59.5 63. 32.0 36.8 48.8 47.2 36.8 57.6 57.6 32.0 36.8 48.8 50.8 56.0 57.6 61. 32.0 36.8 48.8 50.5 49.4 57.6 61.2 Table 1: Performance on QA tasks with Qwen2.5-7B-Instruct as the backbones. Rel. indicates the reliability metric."
        },
        {
            "title": "Method",
            "content": "Acc Prec Rel."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Qwen2.5-3B-Instruct TIR Prompt Reliable TIR Prompt GRPO BAPO (Ours) Qwen2.5-14B-Instruct TIR Prompt Reliable TIR Prompt GRPO BAPO (Ours) 29.2 25.4 45.1 44.8 49.7 47.6 56.6 54.0 29.2 31.8 45.1 52.9 49.7 56.7 56.6 65. 29.2 30.2 45.1 51.3 49.7 55.0 56.6 63.3 Table 2: Performance of BAPO and baselines across different model scales. Rel. indicates the reliability metric."
        },
        {
            "title": "Prec",
            "content": "ρIDK"
        },
        {
            "title": "Acc",
            "content": "BAPO 44.8 52.8 16.8% w/o Boundary-Aware Reward 30.6 53.1% 62. w/o Sample Modulator 52.0 43.3 20.4% w/o Sample & Stage Modulator 37.8 35.2% 56.0 51. 44.8 50.1 49.0 Table 3: Ablation study. The metrics are averaged across four benchmarks on Qwen2.5-3B-Instruct. and 14B versions of the Qwen2.5-Instruct series. As shown in Table 2, BAPO improves the reliability of its instruct model backbones with Reliable TIR Prompt by an average of 76.1% and 27.4% on the 3B and 14B scales, respectively. Furthermore, compared to the standard GRPO baseline, BAPO achieves substantial reliability advantages of 13.9% and 11.9%. These results are consistent with our main experiments, confirming that BAPO is generalizable for building reliable agentic search. Table 3 details the ablation study on Qwen2.53B-Instruct comparing BAPO with its variants. Through the experiments results, we can draw the following observations: Fixed Reward Strategy Leads to Reward Hacking. When replacing our proposed reward mechanism with fixed positive reward of 0.5 for IDK responses, which simulates the setting used in BARREL (Yang et al., 2025a), the model exhibits extreme over-conservatism. The IDK rate spikes to 53.1%; while this boosts precision, it drastically reduces accuracy and yields the lowest overall reliability score among all variants. Adaptive Reward Modulator is Critical. Next, we ablate the two components of the Adaptive Reward Modulator to verify their effectiveness. We find that removing sample-level deactivation results in mild degradation in reliability (1.2). In contrast, removing both the stage-level and samplelevel modulators causes the model to degenerate into local optimum where it exploits IDK responses. Consequently, we observe an excessive surge in the IDK rate (16.8% 35.2%), accompanied by significant drops in accuracy (44.8 37.8) and reliability (51.3 49.0)."
        },
        {
            "title": "5.5 Reward Dynamics of Two Stage",
            "content": "In Figure 5, we visualize BAPOs training dynamics of the average correctness reward RCorrect and the average boundary-aware reward RIDK, along with the IDK ratio ρIDK across different stages on Qwen2.5-14B-Instruct. At the exploration stage (the first 60 steps), RCorrect increases from 0.3 to Figure 6: Rejection success rates calculated on Qwen2.5-Instruct series models. mitigate randomness, we adopt the results of bestof-N (with N=5) as the criterion for determining failures and refusals. Given that the GRPO-trained model acts as the upper bound of problem-solving capability under our settings, if problem triggering refusal is also unsolvable by GRPO, this suggests that the BAPOtrained models refusal reflects precise awareness of its reasoning boundaries. Figure 6 shows that the BAPO-trained models attain robust average rejection success rates of 74.7% (3B), 76.7% (7B), and 76.7% (14B). This validates BAPOs ability to enhance model self-awareness, ensuring the appropriate rejection of queries lying outside their reasoning scope. Besides that, it is worth noting that the rejection success rate does not grow with model size, primarily since the overall error rate of the GRPO baselines simultaneously decreases as the model size scales."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we reveal critical challenge that RL training with only correctness rewards undermines the models reliability in providing truthful answers to users. To this end, we propose BAPO, novel RL algorithm for training reliable agentic search models. Beyond correctness rewards, BAPO incorporates boundary-aware rewards to encourage appropriate IDK responses. To tackle the tradeoff between exploration and exploitation, we introduce an adaptive reward modulator to prevent the model from being over-encouraged to admit ignorance. Extensive experiments show that BAPO achieves superior reliability, enabling the model to maintain awareness of its reasoning boundaries without compromising its capability for deep exploration. Figure 5: Upper: Dynamics of correctness reward RCorrect and boundary-aware reward RIDK. Bottom: Dynamics of IDK ratio ρIDK during BAPO training on Qwen2.5-14B-Instruct. 0.5 at the first 60 steps, with ρIDK drops from the initial 20% to 5%, indicating that the model prioritizes acquiring problem-solving skills over boundary awareness. In the plateau stage, the growth of RCorrect slows down, whereas RIDK rises quickly and stabilizes between 0.25 and 0.3. This shift drives sustained increase in the IDK rate, eventually restoring ρIDK to over 25%. These dynamics revel that our adaptive reward mechanism effectively varies the main optimization goals across training stages, thereby preventing RCorrect and RIDK from hampering each other. It achieves superior balance between solving problems and refusing unknown questions."
        },
        {
            "title": "5.6 Awareness of When to Refuse",
            "content": "To assess the rationality of IDK responses produced by BAPO, we use rejection success rate, defined as the error rate of the GRPO-trained model on the subset of problems where the BAPO-trained model refuses to give final answer by responding with IDK: rejection success rate = SBAPO refuse SGRPO fail SBAPO refuse , (8) where SBAPO and SGRPO refuse = {x MBAPO(x) = IDK} fail = {x MGRPO(x) = ˆy}. To"
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results of BAPO in enhancing agentic search models reliability, our work has several limitations. First, our evaluation primarily focuses on knowledge-intensive tasks. Consequently, the study leaves the generalizability of our method to other types of reasoning problems to be fully explored. Furthermore, constrained by computational resources, our experiments currently scale only up to models with 14B parameters. It remains to be seen how the proposed method performs on larger-scale LLMs. Finally, due to the high costs of commercial search APIs, we only consider local RAG setup. While this ensures reproducibility, it does not fully replicate the noise, and dynamic nature of web search, which may present additional challenges for reliability."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jiuhai Chen and Jonas Mueller. 2024. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 51865200. Association for Computational Linguistics. Lida Chen, Zujie Liang, Xintao Wang, Jiaqing Liang, Yanghua Xiao, Feng Wei, Jinglei Chen, Zhenghong Hao, Bing Han, and Wei Wang. 2024. Teaching large language models to express knowledge boundary from their own signals. CoRR, abs/2406.10881. Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. Research: Learning to reason with search for llms via reinforcement learning. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Toolstar: Empowering llm-brained multi-tool reasoner via reinforcement learning. CoRR, abs/2505.16410. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on RAG meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024, pages 6491 6501. ACM. Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, and Dong Yu. 2025. Cognitive kernel-pro: framework for deep research agents and agent foundation models training. Preprint, arXiv:2508.00414. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrievalaugmented generation for large language models: survey. CoRR, abs/2312.10997. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. Preprint, arXiv:2011.01060. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, and 80 others. 2024. Openai o1 system card. CoRR, abs/2412.16720. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025a. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Jiajie Jin, Yutao Zhu, Zhicheng Dou, Guanting Dong, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, and Ji-Rong Wen. 2025b. Flashrag: modular toolkit for efficient retrieval-augmented generation research. In Companion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, pages 737 740. ACM. Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, DaCheng Juan, Ankur Taly, and Cyrus Rashtchian. 2025. Sufficient context: new lens on retrieval augmented generation systems. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Preprint, arXiv:2302.09664. Abhishek Kumar, Robert Morabito, Sanzhar Umbet, Jad Kabbara, and Ali Emami. 2024. Confidence under the hood: An investigation into the confidenceprobability alignment in large language models. Preprint, arXiv:2405.16282. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inintensive NLP tasks. formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Jiatong Li, Weida Wang, Qinggang Zhang, Junxian Li, Di Zhang, Changmeng Zheng, Shufei Zhang, Xiaoyong Wei, and Qing Li. 2025a. Mol-r1: Towards explicit long-cot reasoning in molecule discovery. arXiv preprint arXiv:2508.08401. Moxin Li, Yong Zhao, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua, and Yang Deng. 2025b. Knowledge boundary of large language models: survey. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 5131 5157. Association for Computational Linguistics. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025c. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366. Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, and Chenyan Xiong. 2025d. RAG-DDR: optimizing retrieval-augmented generation using differentiable data rewards. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 2428, 2025. OpenReview.net. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. RA-DIT: retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 90049017. Association for Computational Linguistics. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. Preprint, arXiv:2210.03350. Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, and Ningyu Zhang. 2025. Knowrl: Exploring knowledgeable reinforcement learning for factuality. arXiv preprint arXiv:2506.19807. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. CoRR, abs/2503.05592. Shuchang Tao, Liuyi Yao, Hanxing Ding, Yuexiang Xie, Qi Cao, Fei Sun, Jinyang Gao, Huawei Shen, and Bolin Ding. 2024. When to trust llms: Aligning confidence with response quality. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 59845996. Association for Computational Linguistics. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 54335442. Association for Computational Linguistics. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Preprint, arXiv:2108.00573. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. Preprint, arXiv:2212.10509. Denny Vrandeˇcic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM, 57(10):7885. Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. 2025. Chain-of-retrieval augmented generation. Preprint, arXiv:2501.14342. Qingjie Zhang, Yujia Fu, Yang Wang, Liu Yan, Tao Wei, Ke Xu, Minlie Huang, and Han Qiu. 2025c. On the self-awareness of large reasoning models capability boundaries. Preprint, arXiv:2509.24711. Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, and Kai Yu. 2025. Enhancing llm reliability via explicit knowledge boundary modeling. arXiv preprint arXiv:2503.02233. Luyao Zhuang, Shengyuan Chen, Yilin Xiao, Huachi Zhou, Yujing Zhang, Hao Chen, Qinggang Zhang, and Xiao Huang. 2025. Linearrag: Linear graph retrieval augmented generation on large-scale corpora. arXiv preprint arXiv:2510.10114. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024. Text embeddings by weakly-supervised contrastive pre-training. Preprint, arXiv:2212.03533. Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and Jinsong Su. 2025. When to use graphs in rag: comprehensive analysis for graph retrieval-augmented generation. arXiv preprint arXiv:2506.05690. Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, and Kai Yu. 2024. Rejection improves reliability: Training llms to refuse unknown questions using rl from knowledge feedback. ArXiv, abs/2403.18349. Junxiao Yang, Jinzhe Tu, Haoran Liu, Xiaoce Wang, Chujie Zheng, Zhexin Zhang, Shiyao Cui, Caishun Chen, Tiantian He, Hongning Wang, Yew-Soon Ong, and Minlie Huang. 2025a. Barrel: Boundary-aware reasoning for factual and reliable lrms. Preprint, arXiv:2505.13529. Qwen: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 23 others. 2025b. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Junnan Dong, and 1 others. 2025a. survey of graph retrieval-augmented generation for customized large language models. arXiv preprint arXiv:2501.13958. Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, and Jinsong Su. 2025b. Faithfulrag: Fact-level conflict modeling for contextarXiv faithful retrieval-augmented generation. preprint arXiv:2506.08938."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Details of GRPO We implement GRPO algorithm based on the verl2 framework. The details of training parameters are listed in Table 4. Hyper-Parameter"
        },
        {
            "title": "Value",
            "content": "Learning Rate LR Scheduler Batch Size Mini Batch Size Training Epochs Number of Rollout Rollout Temperature KL Loss Coefficient Clip Ratio (ϵ) Max Tokens 1e-6 Constent 64 16 2 8 1.0 0.001 0.1 8192 Table 4: Training hyper-parameters setting of GRPO. Consistent with previous agentic search models like Search-R1 and Tool-Star, we exclud all tool invocation results from the loss computation to avoid biasing the policy toward tool outputs. Only tokens from the text-based reasoning process and tool requests are considered during loss calculation. For fair comparison, all non-BAPO-specific parameters are set to match the standard GRPO configuration. Hyper-Parameter"
        },
        {
            "title": "Value",
            "content": "7e-6 Cosine 3 0.1 8 2 Table 5: Training hyper-parameters of Reliable RFT. prompts in the original work to ensure the reproducibility of the results. Search-R1 (Jin et al., 2025a) is reinforcement learning framework that trains model to autonomously invoke search engines during the reasoning process. Its reward is determined solely by the correctness of the final answer. The training dataset is unified dataset merging Natural Questions (NQ) and HotpotQA, amounting to approximately 90k training samples. ReSearch (Chen et al., 2025) is another framework designed to enable agentic search capabilities of LLMs. Its reward function incorporates both the prediction F1 score and format correctness. Models are trained using the MuSiQue training set, which comprises 19,938 samples. A.2 Details of Reliable RFT A.4 Dataset Details As variant of Rejection Sampling Fine-tuning (RFT), Reliable RFT aims to move beyond pure accuracy maximization and optimize the model to admit uncertainty in the fine-tuning process. We use the same training dataset as in the implementation of BAPO. For RFT data construction, we perform rollouts with 8 samples per question using sampling temperature of 1.0. For each question, if no correct answer exists across the rollouts, we adopt the models IDK response (if available); otherwise, we select the correct response(s) for training. We use LLaMA-Factory3 training framework for conducting experiments. The training hyperparameters are listed in Table 5. A.3 Details of Open-source Models For fair comparison, we directly use the released checkpoint of open-source models trained from Qwen2.5-7B-Instruct and follow the corresponding For training BAPO, we utilize the high-quality dataset introduced by (Dong et al., 2025)4. This training dataset is specifically constructed to be challenging for both Direct Reasoning and ToolIntegrated Reasoning. To target agentic search capabilities, we isolate the QA portion of this dataset. This yields 5,000 training samples derived from HotpotQA and 2WikiMultiHopQA. A.5 Benchmark Details For evaluation, we use four benchmarks of multihop QA tasks: HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), 2WikiMultiHopQA (Ho et al., 2020), and Bamboogle (Press et al., 2023). Following the setup in Dong et al. (2025), the full test set consists of 200 test samples from HotpotQA, 200 from 2WikiMultiHopQA, 200 from MuSiQue, and 125 from Bamboogle. 2https://github.com/volcengine/verl 3https://github.com/hiyouga/LLaMA-Factory 4https://huggingface.co/datasets/dongguanting/MultiTool-RL-10K Param."
        },
        {
            "title": "Prec",
            "content": "ρIDK Rel. α = 0.0 α = 0.05 α = 0.2 α = 0.3 = 0 = 1 = 2 = 3 40.7 44.8 39.2 41.4 43.4 44.5 44.8 45. 40.7 52.8 53.9 53.8 50.9 50.3 52.8 52.7 0.0% 40.7 16.8% 51.3 30.2% 49.2 25.2% 50.3 15.4% 49.0 13.2% 49.2 16.8% 51.3 16.3% 51.2 Table 6: Sensitivity analysis of BAPOs key hyperparameters: IDK ratio threshold α and resampling times k. Metrics are averaged across four benchmarks (HotpotQA, 2WikiMultihopQA, MuSiQue, Bamboogle) on Qwen2.5-3B-Instruct."
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Hyper-Parameters Sensitivity Analysis To investigate the impact of the values of BAPOs specific hyper-parameters IDK ratio α and resampling times k, we systematically evaluate models performance under different hyper-parameter settings, while keeping all other hyper-parameters fixed to the default settings used in the main experiments. All experiments are conducted on Qwen2.53B-Instruct, and the results are reported in Table 6. This hyper-parameters analysis reveals these key observations: (1) The value of α is closely tied to the models tendency to acknowledge uncertainty. Since α directly controls the strength of the reward assigned to IDK responses, when α = 0, the model is never encouraged to produce IDK responses in the early stage. As result, the model has lost the ability to generate IDK responses in exploration stage and can no longer learn this behavior in plateau stage. In contrast, larger values of α promote more IDK responses during the exploration stage, enabling the model to cultivate boundary awareness. (2) The resampling times allows us to dynamically enlarge the effective rollout size by repeatedly sampling within groups that contain no rewardable responses, thereby improving overall reliability. Increasing from 1 to 2 yields clear gains, while further increasing to 3 provides only marginal improvement, indicating that moderate resampling times is sufficient. B.2 Why We Resample Rollouts The resampling strategy is premised on the assumption that increasing the rollout size yields more Figure 7: Accuracy of the Pass@K on Qwen2.5-7BInstruct for different values of K. The accuracy improves with increasing and stabilizes after = 32. accurate estimation of the models boundary. As illustrated in Figure 7, the model shows no further improvement in pass@K accuracy as increases to 32. This indicates that failure stems from the problem lying beyond the reasoning boundary, rather than merely being an accidental failure to sample correct solution. Consequently, our dynamic resampling design adaptively expands the group size to refine this boundary estimation. The results in Table 6 corroborate our findings, demonstrating that pass@24 to pass@32 (corresponding to resampling factors = 2 and = 3 with rollout size of 8) are sufficient for accurate estimation. B.3 Exact Match Performance As complement of the main experiment results based on LLM-as-a-Judge Equal (LE), we report the performance under the Exact Match (EM) metric in Table 7. Consistent with the LLM-based findings, models trained with BAPO achieve the highest reliability across the 3B, 7B, and 14B model scales. These results further demonstrate the effectiveness and efficiency of BAPO in developing reliable agents. B.4 Case Study Table 9 and Table 10 represent the reasoning process of Qwen2.5-14B-Instruct trained with GRPO and BAPO, respectively. When the retrieved results lack relevant information, the GRPO-trained model initially identifies the absence of necessary information and attempts to search for it. However, it subsequently hallucinates that the director is Ralph Pappier, resulting in an erroneous final outcome. In contrast, BAPO enables the model to recognize its"
        },
        {
            "title": "MusiQue",
            "content": "2Wiki."
        },
        {
            "title": "Bamboogle",
            "content": "Acc Prec Rel Acc Prec Rel Acc Prec Rel Acc Prec Rel Qwen2.5-7B-Instruct Naive RAG IRCoT Search-R1 ReSearch TIR Prompt Reliable TIR Prompt Reliable RFT GRPO BAPO (Ours) Qwen2.5-3B-Instruct TIR Prompt Reliable TIR Prompt GRPO BAPO (Ours) 31.9 30.3 39.5 46.5 19.5 24.5 16.5 44.0 42.0 7.5 16.5 38.5 37.0 Qwen2.5-14B-Instruct TIR Prompt Reliable TIR Prompt GRPO BAPO(Ours) 37.0 36.0 46.0 45. 31.9 30.3 39.5 46.5 19.5 29.6 45.8 44.0 52.0 7.5 22.1 38.5 44.8 37.0 43.6 46.0 53.8 31.9 30.3 39.5 46.5 19.5 28.3 27.0 44.0 50.1 7.5 20.7 38.5 43.5 37.0 42.3 46.0 52. 6.2 6.9 13.5 15.5 4.0 7.0 5.0 14.0 14.0 2.0 3.5 11.0 11.0 9.0 12.5 14.5 16.0 6.2 6.9 13.5 15.5 4.0 9.5 16.6 14.0 20.0 2.0 5.2 11.0 15.9 9.0 16.5 14.5 22. 6.2 6.9 13.5 15.5 4.0 8.8 8.5 14.0 18.1 2.0 4.6 11.0 14.4 9.0 15.5 14.5 20.6 25.8 21.6 34.0 37.5 11.0 16.0 5.5 43.5 42.0 4.5 16.0 37.0 36.0 32.5 34.0 44.5 41. 25.8 21.6 34.0 37.5 11.0 20.1 19.6 43.5 50.2 4.5 20.2 37.0 40.0 32.5 42.5 44.5 53.2 25.8 21.6 34.0 37.5 11.0 19.3 9.4 43.5 48.6 4.5 19.3 37.0 38.9 32.5 40.8 44.5 50. 20.8 24.8 40.8 47.2 26.4 25.6 28.0 48.0 46.4 11.2 16.8 39.2 39.2 45.6 43.2 53.6 53.6 20.8 24.8 40.8 47.2 26.4 27.5 42.6 48.0 49.5 11.2 17.7 39.2 42.2 45.6 45.7 53.6 58. 20.8 24.8 40.8 47.2 26.4 27.4 37.6 48.0 49.3 11.2 17.6 39.2 42.0 45.6 45.5 53.6 57.8 Table 7: Exact Match (EM) Performance of multi-hop question answering tasks."
        },
        {
            "title": "Prec",
            "content": "ρIDK Rel. 36.7 34.1 27.5 Internal Confidence = 0.2 45.8 = 0.3 48.7 = 0.5 54.8 Expression Certainty = 0.2 40.4 40.4 = 0.3 39.6 28.6 = 0.5 43.1 26.0 Self-Reflection 17.5 - 56.2 20.0% 43.9 30.0% 44.3 50.0% 41. 0.0% 40.4 27.6% 36.5 38.2% 36.6 68.8% 29.5 Table 8: Performance of uncertainty based methods on Qwen2.5-Instruct-7B, with different values of . boundary. When no relevant information about the film Winds of the Pampas is found, the model trained with BAPO appropriately responds with DONT KNOW (IDK). B.5 Detailed Analysis of Uncertainty"
        },
        {
            "title": "Estimation Methods",
            "content": "Although uncertainty estimation methods have proven effective in standard QA or math reasoning tasks (Chen and Mueller, 2024; Kuhn et al., 2023; Zhang et al., 2025c), their effectiveness remains unexplored in agentic search scenario. In this section, we systematically evaluate three distinct uncertainty estimation methods: 1) Internal Confidence based on token probabilities, 2) Expression Certainty measuring confidence via the models verbalized expressions, and 3) Self-Reflection utilizing the models self-verification capabilities. The implementation details are as follows: Internal Confidence. Model confidence is conventionally quantified via the probability distribution of its output (Kuhn et al., 2023; Kumar et al., 2024). In our approach, we adopt the average log-probability as the proxy for internal confidence. Expression Certainty. To assess uncertainty based on models expression, we follow Zhang et al. (2025c) to calculate the proportion of confident versus uncertain expressions within the reasoning trajectory. Specifically, we utilize predefined lexicon of confident and uncertain phrases (as illustrated in Figure 8), which are manually curated and subsequently verified by an LLM. We quantify the degree of certainty as the ratio of confident expressions to uncertain expressions. Self-Reflection. Leveraging the inherent capacity of LLMs for self-reflection (Manakul et al., 2023; Madaan et al., 2023), we use prompt-based self-reflection approach to instruct the model to retrospectively audit its own reasoning trajectory to identify potential C.2 Prompt for LLM-as-a-Judge Evaluation Table 12 presents the evaluation prompt for LLMbased scoring. It guides the judge model to determine the equivalence of the predicted and labeled answers. C.3 Prompt for Self-Reflection Table 13 details the Self-Reflection Prompt, which tasks the model with identifying logical inconsistencies within its reasoning trajectory."
        },
        {
            "title": "D Detailed Related Work",
            "content": "D.1 Agentic Search. To enhance response quality in knowledgeintensive scenarios, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Gao et al., 2023; Zhang et al., 2025b) supplements LLMs parametric knowledge with external search (Fan et al., 2024; Zhang et al., 2025a; Xiang et al., 2025; Zhuang et al., 2025). Current prompting-based strategies often depend on predefined workflows for interleaving reasoning and search (Asai et al., 2024; Li et al., 2025c), resulting in limited effectiveness. While learning-based methods (SFT/DPO) offer an alternative (Lin et al., 2024; Wang et al., 2025; Fang et al., 2025; Li et al., 2025d,a), they remain constrained by complex data pipelines and suboptimal generalization. Inspired by the success of reinforcement learning on mathematics and coding (Jaech et al., 2024; DeepSeek-AI et al., 2025), RL-based agentic search systems have emerged, such as SearchR1 (Jin et al., 2025a), ReSearch (Chen et al., 2025), R1-Searcher (Song et al., 2025), and ToolStar (Dong et al., 2025). These methods employ format and outcome correctness rewards to enable autonomous query decomposition and flexible interaction with external search systems during reasoning, thereby achieving superior accuracy on complex queries. Nevertheless, current methods prioritize search accuracy while ignoring holistic reliability, which depends on both accuracy and precision. In contrast, our proposed BAPO leverages boundary-aware reward mechanisms to optimize overall reliability, thereby maximizing the practicality of agentic search models. Figure 8: Confident and uncertain expressions. logical pitfalls. The specific prompt utilized for this verification is detailed in Section C.3. For Internal Confidence and Expression Certainty, we apply percentile-based thresholding method: responses ranked within the bottom [0, 1] of confidence scores across the dataset are classified as IDK. For Self-Reflection, we rely on the models judgment, where any response flagged as uncertain by the LLM is directly labeled as IDK. As shown in Table 8, although employing these methods improves precision, it comes at the cost of substantial drop in accuracy, resulting in negligible gains or even degradation in reliability. This underscores critical challenge in agentic search scenario: the validity of reasoning is inextricably linked to the quality of retrieved content. This external dependency induces decoupling between the actual reachability of the answer and the models internal or explicit uncertainty signals, thereby limiting the effectiveness of standard estimation paradigms. It is important to note that uncertainty estimation methods do not cultivate models inherent capability to explicitly admit IDK, which often leads to significant discrepancy between the reasoning process and the final answer."
        },
        {
            "title": "C Prompt Template",
            "content": "C.1 TIR System Prompt Table 11 presents the Tool-Integrated Reasoning Prompt for formulating the reasoning process and the search tool call format. D.2 LLM Reliability. One established approach to enhancing LLM reliability involves uncertainty-based methods, which Question:What is the place of birth of the director of film Winds Of The Pampas? Answer: Italian <think> To answer this question, need to find out who the director of the film \"Winds of the Pampas\" is and then find their place of birth. will start by searching for the director of the film \"Winds of the Pampas\". </think> <search> director of film Winds Of The Pampas </search> <result> \"The Wind Journeys\" The Wind Journeys The Wind Journeys () is 2009 ColombianGerman-Argentine-Dutch drama film written and directed by Ciro Guerra... ... \"Ralph Pappier\" Ralph Pappier Ralph Pappier ( 16 January 1914 in Shanghai 29 August 1998 in Buenos Aires) was an Argentine production designer, set decorator and film director... ... \"Savage Pampas (1945 film)\" Savage Pampas (1945 film) Savage Pampas (Spanish:Pampa bárbara) is 1945 Argentine historical film directed by Lucas Demare ... ... </result> <think> From the search result, can see that the film \"Winds of the Pampas\" is not directly mentioned. However, found that Ralph Pappier is an Argentine film director who has directed several films. will check if \"Winds of the Pampas\" is one of his works. If not, will look for the director of \"Winds of the Pampas\" again. </think> <search> Winds of the Pampas director </search> <result> \"Ralph Pappier\" Ralph Pappier Ralph Pappier ( 16 January 1914 in Shanghai 29 August 1998 in Buenos Aires) was an Argentine production designer, set decorator and film director ... ... \"Pampa Ilusión\" Pampa Ilusión Pampa Ilusión (Illusion of Pampa) is Chilean telenovela produced by TVN. It was written by Víctor Carrasco, Larissa Contreras, María José Galleguillos and Alexis Moreno. Directed by Vicente Sabatini ... ... \"Law of the Pampas\" Law of the Pampas Law of the Pampas is 1939 American Western film directed by Nate Watt and written by Harrison Jacobs ... ... </result> <think> From the search result, can see that Ralph Pappier is the director of \"Winds of the Pampas\". Now, need to find his place of birth. </think> <search> Ralph Pappier place of birth </search> <result> \"Ralph Pappier\" Ralph Pappier Ralph Pappier ( 16 January 1914 in Shanghai 29 August 1998 in Buenos Aires) was an Argentine production designer, set decorator and film director... ... \"Ralph Pappier\" film \"\"Circus cavalcade\"\". The Argentine Academy of Cinematography Arts and Sciences gave him awards for Best Scenography for \"\"En el viejo Buenos Aires\"\" (1942) and \"\"Su mejor alumno\"\" (1944)... ... </result> <think> From the search result, can see that Ralph Pappier was born in Shanghai, China. </think> <answer> The final answer is boxed{Shanghai, China} </answer> Table 9: case study on Qwen2.5-14B-Instruct trained with GRPO. Text in red indicates that the model is hallucinating and providing conclusion without relevant information in retrieval results. Question:What is the place of birth of the director of film Winds Of The Pampas? Answer: Italian <think> To answer this question, need to find out the director of the film \"Winds Of The Pampas\" and then find out their place of birth. will start by searching for information on the film \"Winds Of The Pampas\" </think> <search> Winds Of The Pampas film </search> <result> \"Savage Pampas (1945 film)\" Savage Pampas (1945 film) Savage Pampas (Spanish:Pampa bárbara) is 1945 Argentine historical film directed by Lucas Demare and Hugo Fregonese and starring Francisco Petrone, Luisa Vehil and Domingo Sapelli... ... \"Law of the Pampas\" Law of the Pampas Law of the Pampas is 1939 American Western film directed by Nate Watt and written by Harrison Jacobs. The film stars William Boyd, Russell Hayden, Sidney Toler, Steffi Duna, Sidney Blackmer, Pedro de Cordoba and William Duncan... ... \"Savage Pampas (1966 film)\" Savage Pampas (1966 film) Savage Pampas is 1966 western film directed by Hugo Fregonese and starring Robert Taylor, Ron Randell and Marc Lawrence... ... </result> <think> From the search result, couldnt find any information about the film \"Winds Of The Pampas\". However, found information about other films set in the Pampas region, such as \"Savage Pampas\" (1945 and 1966) and \"Pampa Ilusión\". Since there is no information about \"Winds Of The Pampas\", will have to conclude that the film may not be well-known or may not have lot of information available online. Therefore, dont have enough information to answer the question. </think> <answer>boxed{I DON'T KNOW} </answer> Table 10: case study on Qwen2.5-14B-Instruct trained with BAPO. quantify prediction confidence through token probabilities and consistency across diverse outputs (Manakul et al., 2023; Chen and Mueller, 2024). Alternatively, calibration strategies such as prompting (Tian et al., 2023) and fine-tuning (Tao et al., 2024) are employed to align model confidence with accuracy. Recently, Zhang et al. (2025c) find that reasoning models exhibit self-awareness regarding their capability boundaries and proposed test-time monitoring strategies to terminate unproductive reasoning for unsolvable questions. Distinctly, our work focuses on activating the models intrinsic boundary awareness through RL, which is orthogonal to these uncertainty-based methods. Sharing this focus on models internal capacity to refuse queries out of its answerable boundary, Xu et al. (2024) and Chen et al. (2024) develop specialized reward models by constructing preference pairs of refusal and standard responses for subsequent RL training. KnowRL (Ren et al., 2025) incorporates fact-level verification reward into RL to improve factual accuracy, yet it remains dependent on manually curated factual databases. BARREL (Yang et al., 2025a) fine-tunes models on distilled reasoning traces that align with expected patterns, followed by RL training with static medium-level reward for uncertain responses. However, our empirical findings suggest that such static reward leads to excessively high rejection rates, thereby impairs accuracy. Unlike prior work, our approach targets the agentic search scenario, which is characterized by sophisticated synergy between reasoning and search tool interaction."
        },
        {
            "title": "E The Use of Large Language Models",
            "content": "In preparing this paper, we made limited use of Large Language Models (LLMs). Specifically, LLMs were employed for two purposes: (i) to aid in polishing the writing by improving grammar, readability, and clarity without altering the scientific content, and (ii) to assist in retrieval and discovery tasks, such as identifying and organizing related work. No LLMs were used for generating novel research ideas, designing experiments, or analyzing results. All conceptual and technical contributions presented in this paper are the sole work of the authors."
        },
        {
            "title": "TIR System Prompt",
            "content": "You are helpful assistant that can solve the given question step by step with the help of the wikipedia search tool. Given question, you need to first think about the reasoning process in the mind and then provide the answer. During thinking, you can invoke the wikipedia search tool to search for fact information about specific topics if needed. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags respectively, and the search query and result are enclosed within <search> </search> and <result> </result> tags respectively. For example, <think> This is the reasoning process. </think> <search> search query here </search> <result> search result here </result> <think> This is the reasoning process. </think> <answer> The final answer is [ boxed{answer here} ] </answer>. In the last part of the answer, the final exact answer is enclosed within boxed{}. Table 11: Tool-Integrated-Reasoning System Prompt LLM-as-a-Judge Evaluation You are an evaluation assistant. Please determine if the model output is equivalent to the labeled answer. Question: {question} Labeled Answer: {labeled answer} Model Output: {pred answer} Did the model give an answer equivalent to the labeled answer? Please respond with \"Correct\" if they are equivalent, or \"Incorrect\" if they are not equivalent. Do not include any other text. Table 12: LLM-as-a-Judge Evaluation Prompt Self-Reflection Prompt Below is the reasoning trajectory of an agentic search model. <search>...</search> indicates search queries executed via tools, <result>...</result> shows the returned search results, and <answer>...</answer> contains the models final answer. Analyze the following reasoning trajectory for logic errors or inconsistencies. Reasoning Trajectory: {reasoning trajectory} Please determine if the response exhibits any of the following issues: 1. Logical contradictions or self-contradictions 2. Obvious flaws in the reasoning process 3. Discrepancy between the answer and the reasoning process Please answer only \"Yes\" or \"No\" without explanation. exist, answer \"Yes\"; if there are no obvious issues, answer \"No\"."
        },
        {
            "title": "If any of the above problems",
            "content": "Table 13: Self-Reflection Prompt"
        }
    ],
    "affiliations": [
        "Institute of Artificial Intelligence, Xiamen University",
        "Meituan Inc.",
        "School of Informatics, Xiamen University",
        "The Hong Kong Polytechnic University",
        "Westlake University"
    ]
}