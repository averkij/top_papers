{
    "paper_title": "Attention Basin: Why Contextual Position Matters in Large Language Models",
    "authors": [
        "Zihao Yi",
        "Delong Zeng",
        "Zhenqing Ling",
        "Haohao Luo",
        "Zhe Xu",
        "Wei Liu",
        "Jian Luan",
        "Wanxia Cao",
        "Ying Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures."
        },
        {
            "title": "Start",
            "content": "Attention Basin: Why Contextual Position Matters in Large Language Models Zihao Yi1, Delong Zeng1, *, Zhenqing Ling1, *, Haohao Luo1, Zhe Xu1, Wei Liu2, Jian Luan2, Wanxia Cao2, and Ying Shen1, 1Sun Yat-sen University, Shenzhen, China 2MiLM Plus, Xiaomi Inc., Beijing, China 5 2 0 2 7 ] . [ 1 8 2 1 5 0 . 8 0 5 2 : r Abstract The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal consistent phenomenon we term the attention basin: when presented with sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), two-stage framework that (i) estimates models intrinsic positional attention preferences using small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures. Introduction Large Language Models (LLMs) have acquired vast amounts of knowledge from large-scale pretraining corpora and demonstrated exceptional capabilities in understanding and generating natural language. As result, they have achieved remarkable success across wide range of language tasks, from text summarization to multi-turn dialogue (Yi et al. 2024; Zhang, Wen, and Zhao 2025; Ling et al. 2025). With the growing ability of LLMs to process increasingly long input sequences, Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for expanding the knowledge boundaries of LLMs and improving answer accuracy. By providing relevant external documents at inference time, RAG enables models to perform competitively even in complex scenarios such as multi-hop question answering and few-shot learning (Li et al. 2024; Yi, Xu, and Shen 2025). However, behind this apparent success lies fundamental vulnerability: the performance of LLMs is highly sensi- *These authors contributed equally. Corresponding author tive to the position of information within the input context (Liu et al. 2024). This positional bias poses critical bottleneckmodels often fail to utilize long contexts effectively. Even when all necessary information is present, performance can degrade significantly if key content is placed in regions of low attention, leading to suboptimal and unpredictable outcomes (Xiao et al. 2024). This vulnerability manifests empirically as the \"lost-inthe-middle\" (LIM) phenomenon, where models show clear preference for information at the beginning or end of the context (Liu et al. 2024). Although LIM provides an insightful phenomenological analysis, it describes the effect rather than elucidating the underlying cause. Concurrently, other mitigation strategies, such as fine-tuning for better instruction following (Li et al. 2023), explore different avenues but often incur substantial computational overhead without addressing the core mechanism of the bias. To move beyond surface-level symptoms and uncover the root cause of positional bias, we turn our investigation to the very core of how LLMs process context: the attention mechanism (Vaswani et al. 2017). To this end, our empirical analysis in meticulously designed experimental sandbox across 10 mainstream LLMs reveals consistent and systematic pattern, which we term the attention basin: attention is disproportionately concentrated at the boundaries of the overall structural block of context, such as the full set of retrieved documents, creating trough in the middle where information is neglected. Guided by this discovery, we conduct theoretical investigation to establish direct link between this skewed attention allocation and the models final output probabilities, confirming that placing critical information in high-attention zones is paramount for effective context utilization. Armed with this mechanistic insight, we propose AttentionDriven Reranking (AttnRank), lightweight, training-free yet powerful two-stage method to mitigate positional bias at inference time. First, we probe the models intrinsic attention landscape using small, representative calibration set. Second, we leverage this map to re-rank the input context, strategically aligning the most critical information with the models natural high-attention regions. Extensive evaluations on multi-hop QA and few-shot learning tasks show that AttnRank consistently improves performance across 10 mainstream LLMs across varying architectures and scales, achieving significant gains without any model training or parameter modification. This work makes the following key contributions: Uncovering the Mechanism of Positional Bias: We empirically and theoretically identify the attention basin phenomenon as core mechanistic driver of positional bias: LLMs intrinsically allocate higher attention to the start and end of the overall structural block of context, aligning critical information with high-attention zones is crucial for effective context utilization. Introducing Attention-Driven Reranking Method: We propose novel, lightweight and training-free method AttnRank that first maps models inherent positional attention preferences and then reorders the input to exploit these preferences for improved performance. Validation of AttnRanks Effectiveness: Extensive experiments on multi-hop QA and few-shot learning tasks demonstrate that AttnRank consistently outperforms baseline strategies across 10 mainstream LLMs of varying architectures and scales, effectively mitigating positional bias and significantly enhancing information utilization. Related Works The sensitivity of LLMs to the positional placement of information within their context is well-documented vulnerability. This positional bias compromises their robustness, critically undermining reliability in applications like Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) and in-context learning (Brown et al. 2020). This section reviews the two primary research thrusts addressing this issue: characterizing the phenomenon and mitigating its effects. Characterizing Positional Bias. Initial research focused on characterizing the observable effects of positional bias. The most prominent finding is the \"Lost in the Middle\" (LIM) effect, where models exhibit U-shaped performance curve, recalling information from the beginning and end of context far more effectively than from the middle (Liu et al. 2024). While powerful phenomenological description, the LIM model does not elucidate the mechanistic underpinnings of this behavior. Subsequent work has sought these mechanisms. The \"attention sink\" phenomenon, for instance, reveals that models disproportionately attend to initial tokens (Xiao et al. 2024). However, this only explains the primacy effect and is limited to token-level analysis, often on semantically sparse tokens (e.g., start-of-sequence markers), failing to provide comprehensive model for positional effects across the entire input. An alternative hypothesis suggests the bias stems from lack of explicit supervision during training, which fails to teach models that all positions are equally relevant (An et al. 2024). This high-level explanation, however, lacks concrete, testable mechanism and does not offer methodology to quantify and compare the degree of bias across different models, leaving significant gap in both fundamental understanding and empirical evaluation. Mitigating Positional Bias. In response to identified biases, context reranking has emerged as primary mitigation strategy. The simplest approaches employ brittle and ad-hoc heuristics, such as manually moving important documents to the contexts edges (Liu et al. 2024). While straightforward, their effectiveness is inconsistent across different models and tasks. More sophisticated methods use an auxiliary LLM as reranker to optimize the context order before processing (Wang et al. 2025; Guo et al. 2025; Zhang et al. 2025). These LLM-based rerankers can improve performance but introduce significant computational overhead and latency, rendering them impractical for many real-time applications. More recently, training-based solutions have demonstrated the feasibility of directly reducing bias. For example, the information-intensive (IN2) training scheme by An et al. (2024) effectively alleviates positional bias. However, such methods require full fine-tuning process, which is computationally expensive and risks degrading the models general capabilities on other tasks. In summary, while prior work has successfully identified positional bias and proposed preliminary solutions, critical gap persists. Our understanding of the underlying mechanisms remains incomplete, and existing mitigation strategies force an undesirable trade-off between unreliable heuristics and computationally expensive methods. This paper bridges this gap by first conducting deeper mechanistic analysis of positional bias. Based on these insights, we introduce novel, lightweight strategy that is both principled and efficient, offering more robust and scalable solution than has been previously available. Observation and Analysis In this section, we present our key discoveries that illuminate the mechanisms behind the performance variations of Large Language Models (LLMs) due to contextual position. Our analysis unfolds in three parts: first, we identify consistent, structure-aware attention pattern we term the attention basin; second, we establish direct link between this attention pattern and model performance; and third, we pinpoint which layers are most critical in forming this positional preference. The Attention Basin Phenomenon Inputs to LLMs for tasks like RAG or few-shot learning often consist of multiple, distinct segments. We model such inputs as sequence of structural blocks = {t, d1, . . . , dk, q}, where is task-defining template, di are semantically coherent blocks of content (e.g., retrieved documents), and is the users query. While prior work has noted that attention is often high on initial tokens and immediate neighbors (Ma et al. 2024), the distribution of attention at macro level across these structural blocks has been largely overlooked. To investigate this, we designed precise experiment. For each input, we first located the token indices in the attention matrix corresponding to each document block = {d1, . . . , dk}. We then calculated the mean attention score allocated from the query tokens to each documents specific index range. As shown in Figure 1a, this quantitative analysis unearthed consistent and striking pattern across all tested LLMs: the model allocates significantly higher attention to documents located towards the beginning and the end of the input se- (a) Attention Basin Phenomenon (b) Effect of Disrupted Delimiters Figure 1: (a) The model exhibits U-shaped attention distribution, prioritizing tokens at the contexts start and end boundaries. (b) This pattern disappears following the removal of structural delimiters, indicating that the phenomenon stems from the models awareness of coherent segment boundaries. quence. We term this U-shaped distribution the attention basin phenomenon. This discovery raises critical question: Is the attention basin simple artifact of absolute position (i.e., the model just likes the start and end of any text), or is it driven by the models perception of the inputs structure? To distinguish between these possibilities, we designed disruption experiment aimed at understanding the root cause of this phenomenon. We systematically dismantled the inputs structure by removing punctuation, capitalization, and explicit delimiters like \"Document [1]\", effectively blending the distinct documents into single, unstructured block of text. As shown in Figure 1b, the attention basin effect vanished entirely after the structure was removed. This result provides profound insight: the phenomenon is not arbitrary. Instead, it reveals that the attention basin is structural-level analogue to the well-known token-level primacy and recency effects. Just as models are known to over-attend to the first and last tokens of an entire sequence, our experiment demonstrates that they apply similar heuristic at higher level of abstraction, granting special status to the structural blocks positioned at the edges of the context. The model recognizes the collection of documents as set and focuses its attention on the boundaries of that set. How Attention Influences LLMs Performance The attention basin provides mechanistic explanation for the widely observed lost-in-the-middle effect (Liu et al. 2024). If documents at the edges receive more attention, it follows that their content would more strongly influence the models output. We hypothesize that documents contribution to the final answer is directly proportional to the attention it receives. To formalize this intuition, we analyze the relationship between documents attention and the models output probability. Let αd = 1 be the cross-layer average attention weight allocated to document d, and let (y) be the generation probability of the correct answer y. Under l=1 α(l) (cid:80)L the simplifying assumption of semi-orthogonal document representations (Assumption .3), our theoretical analysis yields the following proposition, with the full proof in Appendix . Proposition 0.1 (Attention-Probability Monotonicity). For correct document and any other document dj, the partial derivatives of the correct answers probability (y) with respect to their average attention weights satisfy: (y) αd > (cid:12) (cid:12) (cid:12) (cid:12) (y) αdj (cid:12) (cid:12) (cid:12) (cid:12) 0. (1) This proposition mathematically confirms that increasing attention on the correct document provides the most effective path to improving model performance. Corollary .9 in the appendix further leverages this insight to explain the lost-in-the-middle phenomenon, demonstrating that placing crucial documents in high-attention regions (i.e., the edges) maximizes the probability of generating the correct answer. We then validated this theory empirically using the HotpotQA dataset (Yang et al. 2018). We constructed inputs with two ground-truth documents (d1, d2) and one irrelevant noise document . We tested all six permutations of these documents, measuring both the attention distribution and the final QA accuracy for each. We categorized permutations into two groups: those where the ground-truth documents received the highest cumulative attention, and those where the noise document did. The results in Figure 2 are unequivocal. Permutations where the correct documents received the most attention (blue bars) significantly outperformed those where the noise document was attended to most (red bars). Remarkably, this attention-optimized ordering not only mitigated the impact of the distractor but, in some cases, even surpassed the performance of the noise-free baseline (orange bar). This demonstrates that controlling positional attention is powerful mechanism for improving model robustness and accuracy. Figure 2: Model QA accuracy on HotpotQA across all permutations of two relevant documents and one noise document. Blue bars: permutations where relevant documents receive the highest attention. Red bars: permutations where the noise document receives the highest attention. Orange bar: noise-free baseline. Aligning relevant documents with high-attention positions consistently yields the best performance. The Critical Role of Shallow Attention Layers Prior studies (Artzy and Schwartz 2024; Van Aken et al. 2019) have shown that attention across different layers contribute unequally to its behavior. Inspired by Abnar et al. (Abnar and Zuidema 2020), who demonstrate that information becomes increasingly entangled in deeper layers, we analyze this effect by decomposing the attention score at given position and layer l. To formalize the attention basin phenomenon, our analysis is based on the assumption (detailed in Assumption .4) that expected attention can be decomposed into two parts: deterministic, position-dependent bias (p) and content-driven, position-agnostic component ϵ(l) . This is expressed as: E[α(l) ] = (p) + ϵ(l) . To quantify the balance between these two forces, we ]/V[f (p)]. define ratio ρ(l) of their variances: ρ(l) = V[ϵ(l) This ratio leads to our second proposition. Proposition 0.2 (Layer-wise Attention Regimes). There exists layer depth threshold that partitions the model into two regimes based on the attention-type variance ratio ρ(l): 1. Position-dominated regime (l < L): ρ(l) < 1, where positional bias variance exceeds content variance. 2. Content-dominated regime (l L): ρ(l) 1, where content-based attention variance becomes dominant. full derivation is in Appendix (Corollary .10). Thus, shallow-layer attention distributions more accurately reflect the models structural and positional focus on different input segments. This insight is crucial, as it suggests that the attention patterns from early layers, where the positional signal (p) is strongest, are the most reliable signal for understanding and manipulating the models positional preferences. To verify this, we designed reranking experiment. For given query, we retrieved five relevant documents. We then used the attention scores from different layers of the model Figure 3: Accuracy of reranking strategy based on attention from different Transformer layers. Reranking using shallowlayer attention consistently outperforms using deeper layers, indicating that LLMs core positional bias is established early. to determine the optimal position for the most important document. As shown in Figure 3, reranking based on attention from the shallowest layers consistently yielded the highest QA accuracy. This confirms our hypothesis: the models foundational positional bias is set early in the forward pass, making shallow-layer attention the most effective signal for understanding and ultimately mitigating this bias. Methodology In the previous section, we demonstrated that an LLMs performance is highly sensitive to the positional ordering of documents, phenomenon driven by the attention basin. This finding suggests that instead of fighting against the models intrinsic biases, we can strategically leverage them. To this end, we propose the Attention-Driven Reranker (AttnRank), lightweight yet highly effective training-free method that aligns document relevance with the models innate attention preferences. AttnRank intelligently reorders Figure 4: Overview of the AttnRank framework. Step 1: Profiling Positional AttentionWe perform one-time, low-cost analysis to capture the models intrinsic attention pattern (the attention basin) across document positions. Step 2: Attention-ariven RerankingFor any new query, we reorder the retrieved documents, mapping the most relevant document (highest similarity) to the position with the highest profiled attention score, thus aligning relevance with the models natural focus. input documents to place the most critical information in the positions where the model is naturally inclined to focus, thereby mitigating positional bias and maximizing performance. As illustrated in Figure 4, the framework operates in two key stages: Attention Distribution Extraction and Attention-based Reranking. Step 1: Attention Distribution Extraction The foundational step of AttnRank is to create stable, general-purpose \"attention profile\" for given LLM. This profile serves as map of the models positional biases. Based on our findings in Section , we use the shallowest attention layer, as it provides the purest signal of the models intrinsic positional preference. To generate this profile, we craft set of probe inputs Si = {t, d1, . . . , dk, q}, where is fixed task template, = {d1, . . . , dk} are placeholder documents, and is query. We then compute the average attention paid by the query tokens to each document position across multiple samples: = {a1, . . . , ak} (cid:88) = 1 i=1 Attention({d1, . . . , dk}i Si), (2) where is the final attention profile, aj is the average attention score for the j-th position, is the number of probe samples, and Attention() extracts the mean attention from the query to each document block. Crucially, we find that this profiling process is highly efficient. Our experiments show that stable attention profile can be established with remarkably small number of samplesoften as few as 400. For some models, the characteristic attention basin pattern emerges with just single example (see Appendix for details). This one-time, low-cost profiling step yields reusable attention map that captures the essence of the models positional bias. Step 2: Attention-based Reranking Once the models attention profile is established, it can be deployed to optimize document ordering for any subsequent task, such as Retrieval-Augmented Generation (RAG). The reranking procedure is as follows: 1. Retrieve: For given user query, use standard retriever to fetch the top-k most relevant documents, = {d1, . . . , dk}, ranked by similarity metric . 2. Rerank: Instead of feeding the documents to the LLM in their default similarity-ranked order, we reorder them according to the pre-computed attention profile A. Specifically, we map the document with the highest relevance score (d1) to the position with the highest attention score in A, the second-most relevant document (d2) to the position with the second-highest attention, and so on. 3. Generate: Concatenate the reordered documents with the query and prompt, and feed the final input into the LLM for generation. Models Random Descending Ascending LIM(Liu et al. 2024) AttnRank (Ours) InternLM3-8B-Instruct (2024) Mistral-7B-Instruct (2023) LLAMA-2-13B-hf (2023) LLAMA3-8B (2024) LLAMA3-8B-Instruct (2024) DeepSeek-R1-Distill-Llama-8B (2025) DeepSeek-LLM (2024) Qwen 2.5 1.5B (2024) Qwen 2.5 3B (2024) Qwen 2.5 7B (2024) Average Accuracy 41.41 39.47 38.37 41.36 44.77 39.45 42.14 40.62 45.77 52.32 42.57 42.92 38.65 38.20 40.76 45.04 38.59 41.20 41.23 46.42 53.31 42.63 40.91 38.90 40.74 43.19 44.81 39.81 41.51 43.56 47.45 54. 43.55 41.91 41.52 39.67 42.09 44.02 39.82 42.22 39.48 45.62 52.18 42.85 41.56 42.17 41.00 43.09 46.32 41.77 45.05 44.14 47.54 54.55 44.72 Table 1: Answer accuracy (%) on the HotpotQA dataset using different document ordering strategies. Five documents are retrieved per question, and models are evaluated on how accurately they answer the question based on the ordered input. By synchronizing the relevance hierarchy of the documents with the attention hierarchy of the model, AttnRank ensures that the most critical information is placed exactly where the model is hardwired to look. This alignment preemptively resolves the conflict between document importance and positional bias, allowing the model to focus its computational resources effectively and avoid distractions from less relevant information. As training-free and model-agnostic method, AttnRank can be universally applied to any LLM without altering its parameters. The computational overhead is negligible, consisting of single, reusable profiling step. Furthermore, because AttnRank operates as an input pre-processing stage, it is fully compatible with modern inference acceleration frameworks like Flash Attention and vLLM, allowing its benefits to be seamlessly integrated without compromising existing performance optimizations. Experiments To validate the effectiveness of AttnRank, we raise the following three key Research Questions (RQs). We conduct comprehensive experiments on various datasets using 10 mainstream LLMs across different architectures and scales to systematically address these RQs: RQ1: Does placing informative documents in highattention slots improve LLMs performance? RQ2: How does the effectiveness of AttnRank generalize across diverse range of LLM architectures and scales? RQ3: Is AttnRank robust across different tasks and datasets that rely on integrating multiple documents? Experimental setup Utilized LLMs To assess the generality of attention basin and the broad applicability of AttnRank, we conduct experiments on diverse set of models with varying structures and parameter counts, including DeepSeek series (7B, DistillLlama), Llama series (7B, 13B), MoE-based Mistral-7B, Qwen2.5 series (1.5B-7B) and InternLM3-8B, covering most of the mainstream advanced LLMs, details of the models are provided in Appendix . Baselines Four reranking baselines are applied to the retrieved document set: (1) Random: Documents shuffled randomly. (2) Similarity Descending: Documents sorted by retrieval similarity in descending order. (3) Similarity Ascending: Documents sorted by retrieval similarity in ascending order. (4) Lost-in-the-Middle (LIM) (Liu et al. 2024): Places the highest-similarity documents at the beginning and end of the input sequence. Datasets We evaluate the effectiveness of AttnRank on three widely-used benchmark datasets. For multi-hop question answering, we use HotpotQA (Yang et al. 2018) and 2WikiMultiHopQA (Ho et al. 2020) to assess complex reasoning capabilities. For multi-domain conversational tasks, we employ the MultiWOZ (Eric et al. 2020; Ye, Manotumruksa, and Yilmaz 2022) series dataset. Details for each dataset are provided in Appendix . Metrics We evaluate multi-hop QA performance using answer average accuracy as straightforward metric. For fewshot experiment, we use the Joint Goal Accuracy (JGA) (Henderson, Thomson, and Young 2014) as the evaluation metric. For each turn in dialogue, dialogue state is considered correct only if it exactly matches the ground truth. Multi-hop QA experiment Multi-hop QA serves as an ideal testbed for our hypothesis, as it forces the model to locate and integrate key facts scattered across several documents. Success is highly dependent on the models ability to process the provided context effectively. Experimental setup For each question, five candidate documents are retrieved by beam-retriever (Zhang et al. 2024), which achieves state-of-the-art retrieval performance on retrieving tasks. As shown in Appendix ), all reranked document lists are fed into unified QA model with identical prompt template. Experiment results and analyze As shown in Tables 1 and 2, AttnRank consistently outperforms baselines on HotpotQA (44.72% vs. Random 42.57%, Descending 42.63%, Ascending 43.55%, LIM 42.85%) and 2WikiMultiHopQA Models Random Descending Ascending LIM (Liu et al. 2024) AttnRank (Ours) InternLM3-8B-Instruct (2024) Mistral-7B-Instruct (2023) LLAMA-2-13B-hf (2023) LLAMA3-8B (2024) LLAMA3-8B-Instruct (2024) DeepSeek-R1-Distill-Llama-8B (2025) DeepSeek-LLM (2024) Qwen 2.5 1.5B (2024) Qwen 2.5 3B (2024) Qwen 2.5 7B (2024) Average 39.14 29.64 30.21 32.17 37.39 27.09 27.83 30.05 32.72 41.22 32.75 40.87 28.47 30.27 30.78 37.91 26.68 28.12 29.48 34.33 41.55 32.85 38.41 33.14 31.79 33.39 39.35 28.02 30.54 33.34 33.55 43.44 34. 39.94 27.74 30.18 30.35 36.59 26.78 28.16 29.35 32.18 39.71 32.10 39.56 32.54 31.79 33.97 38.14 28.24 31.31 33.97 34.08 43.55 34.72 Table 2: Answer accuracy (%) on 2WikiMultiHopQA using different document ordering strategies. Five documents are retrieved per question, and models are evaluated on answer correctness. (34.72% vs. Random 32.75%, Descending 32.85%, Ascending 34.50%, LIM 32.10%). Notably, while ascending order occasionally performs competitively, particularly on 2WikiMultiHopQA, AttnRank still consistently yields equal or superior accuracy, indicating that simple heuristics do not fully capture the models attention dynamics. These results answer RQ1 that placing the most informative documents in positions where the models attention is most focused significantly enhances its ability to reason across multiple hops. The consistent improvements across different architectures (e.g., LLAMA3, Qwen, Mistral, DeepSeek) and model sizes ranging from 1.5B to 13B further validate the generalizability of AttnRank, thereby answering RQ2. Additional experimental details, including attention distribution and case studies, are provided in Appendix . Few-shot experiment Few-shot generation task involves generating outputs based on small number of example demonstrations and user query. We design an experiment to assess the AttnRank in this setting. The IC-DST algorithm (Hu et al. 2022) retrieves small set of in-context examples to generate SQL queries, thereby extracting user intent from dialogue history and maintaining an up-to-date dialogue state. The precision of the dialogue state is strongly correlated with the quality of the code generation, for which we choose to test the effectiveness of the AttnRank based on the IC-DST algorithm. Experimental setup For each dialogue turn, the trained ICDST retriever fetch example dialogues from the few-shot context pool. The same five reranking strategies described in Section are applied to the retrieved examples. The sorted examples and the current dialogue history are then passed to the Code Llama (Roziere et al. 2023) with fixed prompt template (see Appendix ). Experiment results and analyze As shown in Table 3, AttnRank outperforms all baselines on MultiWOZ 2.1 and 2.4, improving over random by 1.58%. All structured ordering strategies (Descending, Ascending, LIM) yield gains over random, indicating that systematic example ordering"
        },
        {
            "title": "Method",
            "content": "Random Descending Ascending LIM(Liu et al. 2024) AttnRank (Ours)"
        },
        {
            "title": "MultiWOZ MultiWOZ",
            "content": "2.1 41.12 42.10 42.67 42.44 42.89 2.4 50.11 50.61 51.16 51.14 51."
        },
        {
            "title": "Average",
            "content": "45.62 46.36 46.92 46.79 47.20 Table 3: Joint Goal Accuracy (%) on MultiWOZ 2.1 and 2.4 (1% few-shot sample), under different context retrieval reranking methods. enhances context relevance for dialogue state tracking. AttnRank additional improvements over Ascending and LIM confirm that aligning high-value examples with the models attention peaks further boosts extraction accuracy, validating our hypothesis. Overall, the strong performance on both multi-hop QA and few-shot learningspanning four datasets and two distinct task typesprovides comprehensive and positive answer to RQ3. Our findings demonstrate that AttnRank is robust and effective method for mitigating negative positional effects across diverse scenarios."
        },
        {
            "title": "Conclusion",
            "content": "In this work, we identified fundamental mechanism governing positional bias in Large Language Models: the attention basin phenomenon. We demonstrated that LLMs exhibit systematic tendency to focus on the beginning and end of structured input block, predictable pattern rather than random flaw. This core insight allowed us to propose AttentionDriven Reranking (ADR), lightweight and training-free framework that transforms this bias from liability into an asset. By strategically reordering input items to align salient information with the models natural attention peaks, ADR effectively enhances knowledge utilization. Crucially, as model-agnostic and parameter-free method, it requires no architectural modifications and seamlessly integrates with existing pipelines. This makes it fully compatible with modern acceleration frameworks, offering rare combination of improved accuracy and high efficiency. We believe this principle of \"attention alignment\" opens new avenues for research, and we discuss potential limitations and future directions in Appendix to inspire further exploration. References Abnar, S.; and Zuidema, W. 2020. Quantifying Attention Flow in Transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 41904197. An, S.; Ma, Z.; Lin, Z.; Zheng, N.; Lou, J.-G.; and Chen, W. 2024. Make your llm fully utilize the context. Advances in Neural Information Processing Systems, 37: 6216062188. Artzy, A. B.; and Schwartz, R. 2024. Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers. In Belinkov, Y.; Kim, N.; Jumelet, J.; Mohebbi, H.; Mueller, A.; and Chen, H., eds., Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, 177184. Miami, Florida, US: Association for Computational Linguistics. Bi, X.; Chen, D.; Chen, G.; Chen, S.; Dai, D.; Deng, C.; Ding, H.; Dong, K.; Du, Q.; Fu, Z.; et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877 1901. Cai, Z.; Cao, M.; Chen, H.; Chen, K.; Chen, K.; Chen, X.; Chen, X.; Chen, Z.; Chen, Z.; Chu, P.; et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297. Eric, M.; Goel, R.; Paul, S.; Sethi, A.; Agarwal, S.; Gao, S.; Kumar, A.; Goyal, A.; Ku, P.; and Hakkani-Tur, D. 2020. MultiWOZ 2.1: Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines. In Proceedings of the Twelfth Language Resources and Evaluation Conference, 422428. Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Vaughan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Guo, M.; Zeng, Q.; Zhao, X.; Liu, Y.; Yu, W.; Du, M.; Chen, H.; and Cheng, W. 2025. DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router. arXiv e-prints, arXiv:2507.22050. Henderson, M.; Thomson, B.; and Young, S. 2014. Wordbased dialog state tracking with recurrent neural networks. In Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), 292299. Ho, X.; Nguyen, A.-K. D.; Sugawara, S.; and Aizawa, A. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, 66096625. Hu, Y.; Lee, C.-H.; Xie, T.; Yu, T.; Smith, N. A.; and Ostendorf, M. 2022. In-Context Learning for Few-Shot Dialogue State Tracking. In Findings of the Association for Computational Linguistics: EMNLP 2022, 26272643. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Singh Chaplot, D.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Renard Lavaud, L.; Lachaux, M.-A.; Stock, P.; Le Scao, T.; Lavril, T.; Wang, T.; Lacroix, T.; and El Sayed, W. 2023. Mistral 7B. arXiv e-prints, arXiv:2310.06825. Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, C. H.; Gonzalez, J.; Zhang, H.; and Stoica, I. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, 611626. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in neural information processing systems, 33: 94599474. Li, D.; Shao, R.; Xie, A.; Sheng, Y.; Zheng, L.; Gonzalez, J.; Stoica, I.; Ma, X.; and Zhang, H. 2023. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. Li, R.; Wang, Z.; Tran, S.; Xia, L.; and Du, X. 2024. MEQA: Benchmark for Multi-hop Event-centric Question Answering with Explanations. Advances in Neural Information Processing Systems, 37: 126835126862. Ling, Z.; Xie, Y.; Dong, C.; and Shen, Y. 2025. Enhancing Factual Consistency in Text Summarization via CounterfacIn Proceedings of the 31st International tual Debiasing. Conference on Computational Linguistics, 79127924. Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.; Petroni, F.; and Liang, P. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12: 157173. Ma, D.; Chen, L.; Zhang, S.; Miao, Y.; Zhu, S.; Chen, Z.; Xu, H.; Li, H.; Fan, S.; Pan, L.; et al. 2024. Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity. arXiv preprint arXiv:2412.02252. Roziere, B.; Gehring, J.; Gloeckle, F.; Sootla, S.; Gat, I.; Tan, X. E.; Adi, Y.; Liu, J.; Sauvestre, R.; Remez, T.; et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Van Aken, B.; Winter, B.; Löser, A.; and Gers, F. A. 2019. How does bert answer questions? layer-wise analysis of transformer representations. In Proceedings of the 28th ACM international conference on information and knowledge management, 18231832. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30. Wang, Q.; Ding, R.; Chen, Z.; Wu, W.; Wang, S.; Xie, P.; and Zhao, F. 2025. Vidorag: Visual document retrievalaugmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017. Xiao, G.; Tian, Y.; Chen, B.; Han, S.; and Lewis, M. 2024. Efficient Streaming Language Models with Attention Sinks. In The Twelfth International Conference on Learning Representations. Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 23692380. Ye, F.; Manotumruksa, J.; and Yilmaz, E. 2022. MultiWOZ 2.4: Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, 351360. Yi, Z.; Ouyang, J.; Liu, Y.; Liao, T.; Xu, Z.; and Shen, Y. 2024. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013. Yi, Z.; Xu, Z.; and Shen, Y. 2025. Intent-driven In-context Learning for Few-shot Dialogue State Tracking. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Zhang, J.; Zhang, H.; Zhang, D.; Yong, L.; and Huang, S. 2024. End-to-End Beam Retrieval for Multi-Hop Question Answering. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 17181731. Zhang, Y.; Li, M.; Long, D.; Zhang, X.; Lin, H.; Yang, B.; Xie, P.; Yang, A.; Liu, D.; Lin, J.; et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv preprint arXiv:2506.05176. Zhang, Z.; Wen, L.; and Zhao, W. 2025. Rule-KBQA: RuleGuided Reasoning for Complex Knowledge Base Question Answering with Large Language Models. In Proceedings of the 31st International Conference on Computational Linguistics, 83998417."
        },
        {
            "title": "Technical Appendix",
            "content": "The appendix is organized as follows: Sec. : Implementation of experiments. Sec. : Multi-document attention distributions. Attention distributions across models with varying numbers of input documents. Sec. : Theoretical analysis of attention-guided reranking, comprising: Sec. Optimization objective and problem statement Sec. Key assumptions Sec. Technical lemmas on hidden-state decomposition and logit formation Sec. Main proposition Sec. Corollaries on optimal document positioning and layer-wise effects Sec. Connection between our theory and AttnRank Sec. : Data volume requirements. Convergence analysis of data requirements for reliably characterizing LLM attention distribution patterns. Sec. : Supplementary experiments and case studies. Sec. : Limitation and future works. Implementation of experiments Advantages of Our Method The AttnRank framework is designed to be both effective and practical, offering several key advantages: Training-Free and Model-Agnostic: AttnRank requires no modification to the LLMs architecture or parameters. It treats the model as black box, making it universally applicable to any Transformer-based LLM. Extremely Low Overhead: The primary cost is onetime profiling step, which, as weve shown, requires minimal number of inference runs. Once the attention profile is saved, it can be reused for all future inference tasks for that model at virtually no cost. The reranking itself is simple array permutation, which is computationally negligible. Compatibility with Modern Acceleration Frameworks: Because AttnRank is an input-preprocessing step that operates before the main generation pass, it is fully compatible with popular inference acceleration libraries like Flash Attention and serving frameworks like vLLM. It does not interfere with their internal optimizations, allowing users to gain performance from our method while retaining the benefits of these high-speed tools. Experimental setup All experiments were conducted on the same hardware configuration, as detailed in TaIn the first stage of AttnRank, we use the ble 4. AutoModelForCausalLM function from the HuggingFace Transformers library to load the model, perform inference, and extract attention distributions. In the second stage, we adopt the VLLM framework (Kwon et al. 2023) to accelerate inference. Architecture x86_64 CPU GPU CUDA Toolkit Operating System Programming Language Deep Learning Framework Intel Xeon Gold 5218R @ 2.10GHz NVIDIA GeForce RTX 3090 24GB 10 11.3 Ubuntu 20.04 Python 3.9.18 PyTorch 1.13.0 Table 4: Experimental Environment Configuration Utilized LLMs DeepSeek-R1-Distill-Llama-8B (Guo et al. 2025): An 8Bparameter model distilled from DeepSeek-R1, fine-tuned via reinforcement learning to enhance reasoning capabilities. DeepSeek-LLM (Bi et al. 2024): 7B parameter models trained on 2 trillion tokens, utilizing pre-norm decoderonly Transformer architecture with grouped-query attention (GQA). LLaMA-3 Series (Grattafiori et al. 2024): Metas LLaMA 3 series includes 8B and 70B parameter models trained on 15 trillion tokens.The instruct variant is fine-tuned to enhance instruction-following capabilities. LLaMA-2-13B-hf (Touvron et al. 2023): 13B parameter decoder-only Transformer model, trained on 2 trillion tokens, featuring 4,096-token context window optimized for general-purpose language tasks. Mistral-7B-Instruct (Jiang et al. 2023): 7B parameter model fine-tuned for instruction following, employing grouped-query attention and sliding window attention mechanisms to handle long sequences. InternLM3-8B-Instruct (Cai et al. 2024): An 8B parameter model fine-tuned to follow instructions, designed to perform effectively across variety of natural language understanding tasks. Qwen 2.5 Series (Yang et al. 2024): Developed by Alibaba, the Qwen 2.5 series includes models with 1.5B, 3B, and 7B parameters, trained on extensive datasets to support multilingual capabilities and demonstrate strong performance across diverse tasks. Code LLaMA 7B (Roziere et al. 2023): codespecialized model fine-tuned from LLaMA 2, supporting code generation tasks across multiple programming languages. Utilized Datasets HotpotQA (Yang et al. 2018) is large-scale dataset designed to facilitate research in multi-hop question answering. It comprises approximately 113,000 question-answer pairs, each requiring reasoning over multiple Wikipedia articles to derive the correct answer. 2WikiMultiHopQA (Ho et al. 2020) is dataset constructed to assess the comprehensive reasoning abilities of question answering models. It contains question-answer pairs that require multi-hop reasoning over both structured data from Wikidata and unstructured text from Wikipedia. MultiWOZ (Eric et al. 2020; Ye, Manotumruksa, and Yilmaz 2022) series are multi-domain task-oriented dialogue datasets with annotated dialogue states. Each dialogue covers 1 to 5 domains, with an average of 13.7 turns per dialogue. Prompts As shown in Figure 5 and Figure 6, we present the prompt templates used in our experiments. Retrieved documents and contextual examples are inserted into Document [1] and Example [1] positions after reordering by different strategies to compare their impact on performance. More details of how LLMs distribute their attention. Figures 8, 9, and 10 show the attention distributions across different models and document counts, illustrating that the attention basin phenomenon is consistently observed. Figure 5: Prompt for multi-hop QA experiment. (a) Two documents Figure 6: Prompt for dew-shot experiment. As shown in Figure 7, we provide the prompt template used in the analysis with disrupted input structure in Section . Twenty documents are randomly shuffled and inserted, with all punctuation removed and uppercase letters converted to lowercase to destroy structural cues. Figure 7: Prompt after disrupting the structural blocks (b) Three documents Figure 8: Attention distribution with 2 and 3 documents Theoretical analysis of attention-guided reranking in long-context tasks Optimization objective Given long-context input = {t, d1, . . . , dn, q}, where is fixed prompt template, is the user query, and = {d1, . . . , dn} denotes information segments containing large numbers of tokens, such as knowledge documents in multihop QA or in-context examples in few-shot tasks. Let denote the correct document, we aim to demonstrate that strategically positioning in high-attention regions of the input sequence increases its cross-layer average attention weight αd , thereby maximizing the posterior probability (yx, D) of generating correct answer y. Formally, we (a) Four documents (a) Six documents (b) Five documents (b) Seven documents Figure 9: Attention distribution with 4 and 5 documents Figure 10: Attention distribution with 6 and 7 documents seek to prove: αd > αdj (dj = d) = (yx, D) > (ydj, x, D) (3) Key assumptions Assumption .3 (Orthogonal Semantic Representation). Let {edk } denote document-level embeddings. These satisfy pairwise orthogonality: edi, edj = 0 = (4) with edk 2 = 1 for normalization. Assumption .4 (Position-Attention Coupling). Let α(l) denote attention weight for position at layer l. There exists position-dependent bias: E[α(l) ] = (p) + ϵ(l) (5) where : R+ is U-shaped function modeling the attention basin phenomenon, and ϵ(l) represents positionp agnostic content-based attention. Assumption .5 (Compositional Weight Binding). The output projection matrix Wy decomposes as: Wy = [EdEt] Wc (6) where Ed is the document embedding matrix, Et the token embedding matrix, and Wc composition matrix satisfying WcF γ. Technical lemmas Lemma .6 (Hidden State Composition). The final hidden state hlast decomposes into document-aware components: (cid:32) (cid:88) (cid:88) (cid:33) α(l) dk v(l) dk + hnoise (7) hlast = hinit + k= l=1 = (cid:80) where α(l) pP(dk) α(l) dk tion for document dk, and v(l) dk layer l. aggregates position-wise attenrepresents its value vector at Proof. Through residual connections, each layers output accumulates document-specific contributions: h(l) = h(l1)+Attn(l)(h(l1)) = h(0)+ (cid:88) Attn(m)(h(m1)) (8) Decompose attention heads into document-level components: m=1 Attn(m) = (cid:88) k=1 α(m) dk v(m) dk + CrossDoc(m) (9) Under Assumption .3, cross-document terms CrossDoc(m) become negligible due to orthogonality, yielding the stated decomposition. Lemma .7 (Logit Formation). The logit for answer token decomposes as: logit(y) = hlast, ed (cid:124) (cid:125) (cid:123)(cid:122) document term + hlast, ey (cid:125) (cid:123)(cid:122) token term (cid:124) +by (10) where ed and ey are orthogonal components from Assumption .5. Proof. Using Assumption .5, the output projection becomes: Wyhlast = [EdEt]Wchlast = Ed(W (d) hlast) (11) Thus for target token embedded as ey = Et[i], the logit hlast)+Et(W (t) contains separate document and token alignment terms. Main proposition Proposition .8 (Attention-Probability Monotonicity). For documents and dj with average attention weights αd = 1 l=1 α(l) (cid:80)L and αdj , if αd > αdj , then: (yx, D) αdj (yx, D) αd > (12) Proof. Step 1: Document-term dominance From Lemma .6 and .7, the document alignment term dominates when contains sufficient answer evidence: (cid:88) (cid:43) (cid:42) hlast, ed = αd +O(max k= αdk ) (13) 1 l=1 v(l) , ed (cid:123)(cid:122) κ where κ > 0 due to value-key alignment in transformers. (cid:125) (cid:124) Step 2: Probability gradient analysis The output probability computes as: (yx, D) = exp(logit(y)) exp(logit(y)) Taking partial derivative with respect to αd : (cid:80) αd Similarly, αdj Step 3: Strict monotonicity = (yx, D)(1 (yx, D))κ > 0 = (1 )κj 0 for = . Given κ vd cos θvd ,ed and Assumption .3, cos θ = 1. Thus: αd > αdj = Hence complete the proof. αd > (cid:12) (cid:12) (cid:12) (cid:12) αdj (cid:12) (cid:12) (cid:12) (cid:12) (14) (15) (16) Implications for long-context tasks Corollary .9 (Optimal Document Positioning). Let Popt denote positions with maximal attention basin effect in Assumption .4. Placing at Popt maximizes αd , leading to: E[P (yx, D)]p E[P (yx, D)]p / Popt (17) Proof. From Assumption .4, positions in Popt maximize E[α(l) ]. By the monotonicity in Proposition 1, positioning at achieves: E[αd p] ="
        },
        {
            "title": "1\nL",
            "content": "l=1 (cid:88) (p) + E[ϵ(l) ] > (cid:88) (p) + E[ϵ(l) ]"
        },
        {
            "title": "1\nL",
            "content": "l=1 (18) for any / Popt. Proposition 1 then guarantees higher (yx, D). Corollary .10 (Layer-wise Attention Degradation). Let V[ϵ(l) ] V[f (p)] measure the relative strength of content-based ρ(l) = vs position-based attention at layer l. There exists layer depth threshold such that: ρ(l) < 1 < ρ(l) 1 L (position-dominated regime) (content-dominated regime) Thus, shallow layers better preserve positional bias patterns from Assumption .4, while deeper layers exhibit attenuated positional effects. Proof. From Assumption .4, decompose attention variance: V[α(l) ] = V[f (p)] (cid:124) (cid:123)(cid:122) (cid:125) positional + V[ϵ(l) ] (cid:124) (cid:123)(cid:122) (cid:125) content-based (19) Transformer architectures typically exhibit increasing ] with depth as self-attention becomes more semanp ] yields threshold where V[ϵ(l) tic. Solving V[f (p)] = V[ϵ(l) positional effects become subdominant. Remark: connecting theory to method Our theoretical analysis rigorously justifies the core principle of attention-guided document reranking: The U-shaped attention basin (Assumption .4) explains the lost-in-the-middle phenomenon through its positional expectation E[α(l) ] The attention-probability monotonicity (Proposition 1) formally establishes that boosting αd via strategic positioning directly increases answer correctness Corollary .9 provides theoretical guarantees for our methods effectiveness: reranking documents to place in Popt (typically sequence edges) maximizes its attention influence Corollary .10 reveals the layer-dependent nature of attention guidance: shallow layers position-dominated regime (ρ(l) < 1) better preserves document ordering signals critical for reranking, while deeper layers content-focused attention (ρ(l) 1) introduces positional ambiguity. This theoretically justifies why using shallower attention maps produces better reranking results. This mathematical foundation not only explains empirical observations but also guides future extensions: the composition matrix Wc in Assumption .5 suggests directions for training-based attention shaping, while the orthogonality in Assumption .3 motivates improved document encoding schemes to better satisfy theoretical prerequisites. (a) Qwen2.5-7B (a) Mistral-7B-Instruct (b) Internlm3-8b-instruct Figure 11: The relationship between long-context data volume and attention distribution in LLMs. How many documents are required? We design an experiment to determine how much data is required for attention patterns to converge. Following the setup in Section , we incrementally increase the number of samples and compare the resulting attention distributions to those from the full dataset. As shown in Figures 11 and 12, all models converge after about 400 samples. Notably, we focus on convergence at the document boundariesthe first (blue) and last (purple) documents. In DeepSeek-LLM and InternLM38B-Instruct, boundary attention converges with only 200 samples. Mistral-7B-Instruct and Qwen2.5-7B match the final pattern from the start. These results suggest that, for some models, single sample may suffice to approximate full-context attention behavior. Attention distribution experiments and case studies Following the setup in Experiment , we analyze the average attention scores assigned to ground-truth and noise documents under different reranking baselines. As shown in Table 5, AttnRank assigns the highest average attention score (b) Deepseek-llm Figure 12: The relationship between sample size and attention distribution in LLMs. to relevant documents and the lowest to noise ones, demonstrating that AttnRank effectively guides the model to focus on the most critical documents. We further conducted three case studies. Figures 1413, 1516 and 1718 present two additional case studies. AttnRank robustly maintains high attention on critical documents and low attention on noise, validating its effectiveness. In contrast, the descending strategy preserves high attention for relevant documents but also highlights noise, and the lost-in-the-middle approach reduces noise attention at the expense of under-attending to relevant documents. AttnRank consistently focuses on important content while disregarding noise. Limitation and future works The proposed AttnRank method effectively mitigates detrimental positional biases by aligning document relevance with the models intrinsic attention patterns, thereby unlocking significant performance gains. However, due to the fact that most existing closed-source models do not expose attention scores through their APIs, the effectiveness of our approach on such models cannot be verified at present. This limitation highlights the need for future work on developing re-ranking strategies compatible with closed-source LLMs. Moreover, in many applications, it is desirable for the model to attend equally to content across different positions. We therefore suggest that future research explore methods to mitigate the attention basin phenomenon, enabling more uniform attention distribution across different tokens. Table 5: Average attention scores on ground-truth and noise documents under different reranking strategies. Model Random Descending Ascending LIM Rerank Random Descending Ascending LIM AttnRank (ours) Correct Documents Wrong Documents DeepSeek-R1-Distill-Llama-8B LLAMA3-8B-Instruct LLAMA3-8B mistral-7B-Instruct deepseek-llm internlm3-8b-instruct qwen 2.5 1.5B qwen 2.5 3B qwen 2.5 7B 0.0182 0.0212 0.0238 0.0230 0.0389 0.0177 0.0118 0.0169 0.0283 0.0183 0.0212 0.0238 0.0229 0.0387 0.0178 0.0119 0.0168 0.0285 0.0194 0.0230 0.0258 0.0238 0.0401 0.0181 0.0128 0.0170 0. 0.0183 0.0214 0.0238 0.0231 0.0386 0.0177 0.0118 0.0166 0.0284 0.0193 0.0232 0.0256 0.0244 0.0404 0.0185 0.0127 0.0183 0.0304 0.0048 0.0054 0.0062 0.0059 0.0110 0.0045 0.0030 0.0043 0.0072 0.0048 0.0054 0.0062 0.0060 0.0111 0.0046 0.0031 0.0043 0.0072 0.0043 0.0045 0.0053 0.0056 0.0106 0.0044 0.0026 0.0044 0.0061 0.0048 0.0053 0.0062 0.0059 0.0112 0.0045 0.0030 0.0044 0. 0.0042 0.0043 0.0052 0.0051 0.0103 0.0041 0.0026 0.0034 0.0061 Average 0.022200 0.022211 0.023389 0. 0.023644 0.005811 0.005856 0.005311 0.005833 0. Figure 13: Case 1s input prompt and ranking outcomes under different reranking strategies. Figure 14: Average attention scores for relevant and noise documents under various reranking strategies in case 1. AttnRank attains the highest attention on relevant documents and the lowest on irrelevant documents, validating its effectiveness. Figure 15: Case 2s input prompt and ranking outcomes under different reranking strategies. Figure 16: Average attention scores for relevant and noise documents in case 2. Figure 17: Case 3s input prompt and ranking outcomes under different reranking strategies. Figure 18: Average attention scores for relevant and noise documents in case 3."
        }
    ],
    "affiliations": [
        "MiLM Plus, Xiaomi Inc., Beijing, China",
        "Sun Yat-sen University, Shenzhen, China"
    ]
}