{
    "paper_title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
    "authors": [
        "Qi Lv",
        "Weijie Kong",
        "Hao Li",
        "Jia Zeng",
        "Zherui Qiu",
        "Delin Qu",
        "Haoming Song",
        "Qizhi Chen",
        "Xiang Deng",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 1 5 9 6 0 . 9 0 5 2 : r F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions : VISION-LANGUAGE-ACTION MODEL BRIDGING UNDERSTANDING AND GENERATION TO ACTIONS Qi Lv1,2,, Weijie Kong1,, Hao Li1,2,, Jia Zeng1,, Zherui Qiu1, Delin Qu1, Haoming Song1, Qizhi Chen1, Xiang Deng2, Jiangmiao Pang1, 1Shanghai AI Laboratory 2Harbin Institute of Technology (Shenzhen) Equal Contributions, Corresponding Authors {zengjia, pangjiangmiao}@pjlab.org.cn Homepage: https://aopolin-lv.github.io/F1-VLA Github: https://github.com/InternRobotics/F1-VLA Huggingface: https://huggingface.co/InternRobotics/F1-VLA"
        },
        {
            "title": "ABSTRACT",
            "content": "Executing language-conditioned tasks in dynamic visual environments remains central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to shortsighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action (VLA) models (Kim et al., 2024; Team et al., 2025a; Black et al., 2024) aim to equip robots with the ability to execute natural language instructions in visually rich environments. By aligning language instructions with perceptual inputs and mapping them to actions, such models enable language-guide manipulation and versatile humanrobot interaction. However, reliable performance in realistic settings remains elusive: environments are inherently dynamic, i.e., objects move, contexts shift, and instructions unfold over time, so robots must ground ambiguous language, handle diverse objects, and maintain long-horizon temporal coherence as scenes evolve. These conditions expose core limitation of purely reactive state-to-action mappings: without predictive foresight about likely future states, policies become short-sighted and brittle under distribution shifts. Previous efforts on manipulation policy learning can be broadly grouped into three paradigms, as illustrated in Fig. 1. The earliest line of work employs only an action expert trained end-to-end from observations to low-level actions (Zhao et al., 2023; Chi et al., 2023), but such purely reactive mappings lack semantic grounding and generalization across tasks and embodiments (Fig. 1(a)). To overcome these limitations, subsequent approaches integrate Vision-Language-Models (VLMs) 1 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 1: The comparison of varied paradigms for manipulation policies. The earliest end-toend manipulation policies are illustrated in Fig. 1(a), such as ACT (Zhao et al., 2023) and DP (Chi et al., 2023). Policies shown in Fig. 1(b) introduce VLM, aiming to empower the action expert with the general capabilities of VLM in scene and instruction understanding, like π0 (Black et al., 2024) and gr00t-N1 (Bjorck et al., 2025). There are also approaches, as seen in Fig. 1(c), e.g., VPP (Hu et al., 2024) and Genie Envisioner (Liao et al., 2025b), that leverage video diffusion models to guide action execution through video prediction. As depicted in Fig. 1(d), we adopts an integrated architecture of understanding, generation, and execution, empowering the action execution module with capabilities in both scene and instruction comprehension as well as dynamic temporal prediction. into the policy, leveraging pretrained multimodal knowledge to enhance scene and instruction understanding (Black et al., 2024; Bjorck et al., 2025) (Fig. 1(b)). However, they lack temporal evolutionary modeling and remain reactive in nature, thus failing to cope reliably with dynamic or long-horizon manipulation tasks. More recently, visual predictionbased policies (Hu et al., 2024; Liao et al., 2025b) attempt to anticipate future observations as auxiliary signals (Fig. 1(c)), but without integrating semantic understanding from VLMs, their predictions lack semantic grounding and result in brittle control, offering limited robustness and generalization. Across these paradigms, the main limitation is their reliance on reactive state-to-action mappings, which leads to short-sighted behavior and fragility under dynamic and complex manipulation tasks. This motivates central question: What architectural and training principles are required to move beyond reactive imitation and toward robust, foresight-driven policy? Inspired by the Predictive Inverse Dynamics Models (Tian et al., 2024c; Black et al., 2023; Du et al., 2023), we introduce F1, VLA framework that integrates goal-conditioned visual foresight into the perceptionaction loop (Fig. 1d). PIDMs predict future state first, and then frame control as inferring the actions required to realize desired future observation. By adopting this principle, F1 reformulates action generation as foresight-guided inverse dynamics: actions are derived not only from the current observation, but also from an anticipated visual outcome. Specifically, F1 adopts Mixture-of-Transformer (MoT) (Liang et al., 2025) architecture with three dedicated experts for understanding, foresight generation, and action execution, thereby bridging perception, prediction, and control in unified framework. To equip the model with robust and transferable capabilities, we design progressive three-stage training recipe. From the perspective of model architecture, F1 introduces two key designs. First, progressive attention scheme regulates information exchange across experts: intra-expert attention captures rich token interactions, while inter-expert attention enforces causal flow from understanding to foresight to action, ensuring stability and interpretability. Second, the generation expert employs next-scale prediction mechanism (Tian et al., 2024b) to synthesize goal-conditioned visual foresight 2 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions efficiently, producing explicit planning targets that guide subsequent control. These designs enable F1 to explicitly couple semantic understanding, predictive foresight, and action execution within unified backbone. From the perspective of training strategy, we propose three-stage recipe that progressively aligns and integrates the experts. Stage injects foresight capability by aligning the generation expert with the understanding expert which inherits from pretrained MLLM. Stage II pretrains the entire model on large-scale public robot datasets to learn general shared visuomotor knowledge. Stage III posttrains on task-specific data to adapt the model to new embodiments and fine-grained manipulation skills. This progressive scheme not only stabilizes optimization but also endows the model with generalizable foresight, which is critical for robustness in dynamic and long-horizon tasks. Overall, we present F1, pretrained Vision-Language-Action model with 4.2B parameters. Building on predictive inverse dynamics modeling, F1 introduces visual foresight as an explicit planning signal and couples it with action generation through progressive training scheme. Extensive experiments in both real-world and simulation benchmarks demonstrate that F1 substantially improves success rates and robustness compared to reactive baselines, particularly in dynamic and long-horizon tasks. In summary, our contributions are as follows: We introduce novel paradigm for VLA models by integrating dedicated generation expert that leverages predictive inverse dynamics model to forecast visual observation, fundamentally transforming action prediction from reactive to planning-based process. To ensure robustness and generalizability, we develop three-stage training recipe. This carefully designed scheme progressively integrates the models distinct understanding, generation, and action modules. Our model F1 demonstrates superiority in both simulation and real-world tasks. We show substantial improvements over reactive baselines, particularly in challenging dynamic environments and long-horizon tasks that require robust planning and foresight."
        },
        {
            "title": "2 THE F1 FRAMEWORK: BRIDGING PERCEPTION, FORESIGHT, TO ACTIONS",
            "content": "We present F1, Vision-Language-Action (VLA) model that bridges perception, visual foresight generation, to action execution through Mixture-of-Transformer (MoT) architecture (Liang et al., 2025). By jointly modeling understanding, foresight, and control, F1 enables language-conditioned agents to plan and act robustly in complex environments. 2.1 ARCHITECTURE OVERVIEW As illustrated in Fig. 2, F1 comprises three dedicated experts: an understanding expert, generation expert, and an action expert. Given an instruction and current observation ot, the understanding expert encodes semantic and visual information to establish shared multimodal representation. This representation is then passed to the foresight generation expert, which predicts goalconditioned visual foresight ˆot+1. To capture temporal dynamics, the foresight module additionally leverages sequence of past observations {otm, . . . , ot1}, thereby grounding the prediction in both historical context and task goals. Finally, the predicted foresight image ˆot+1 is fed into the action expert, which formulates predictive inverse dynamics modeling problem, enabling the model to generate an action chunk ˆat:t+k that drives the robot toward the synthesized visual target. 2.2 UNIFIED UNDERSTANDING-GENERATION-ACTION TRANSFORMER Building on the demonstrated effectiveness of decoder-only transformers across large models (OpenAI et al., 2024; Yang et al., 2025a; Bai et al., 2025; Liu et al., 2023b), F1 instantiates all three experts with common decoder-only architectural backbone, enabling scalable autoregressive modeling while retaining expert-specific specialization. Understanding Expert. To achieve robust alignment between natural language instructions and perceptual inputs, the understanding expert is initialized from pretrained visionlanguage model trained on large-scale paired textimage data. At each timestep, the current visual observation ot is first encoded by SigLIP vision encoder (Zhai et al., 2023) to produce high-level perceptual features. 3 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 2: Overview of F1 framework. It employs the Mixture-of-Transformer (Liang et al., 2025) architecture comprising three core components: an understanding expert, generation expert, and an action expert. The understanding expert processes instructions and observations to generate foresight image. The visual foresight is then fed to action expert to predict target action via the predictive inverse dynamics modeling (Tian et al., 2024c; Du et al., 2023). These features are fused with the language prompt and processed by the decoder-only transformer, allowing the expert to capture semantic correspondences between task goals and the observed scene. This design equips the understanding expert with reliable, semantically aligned representations that serve as the foundation for subsequent foresight generation and action execution. Generation Expert. The generation expert is designed to produce foresight image ˆot+1 conditioned on the current observation ot and the language goal l, serving as an explicit intermediate target for subsequent control. In contrast to conventional reactive policies, this module anticipates plausible future visual states, thereby enabling smoother and more adaptive behaviors in dynamic environments. Efficient foresight prediction is challenging, since highfidelity visual synthesis often incurs substantial computational cost. To mitigate this issue, we employ next-scale prediction strategy that balances computational efficiency with predictive accuracy. Figure 3: Visualization of Residual VQ-VAE from 1616 to 256256 resolution. Concretely, recent observations {otm, . . . , ot} are first encoded by multi-scale residual vector quantization (VQ) encoder (Lee et al., 2022). As illustrated in Fig. 3, each frame is decomposed into 16 16 patches across spatial scales {r1, . . . , rk}, yielding discrete tokens {z0 } for each oi. To avoid prohibitively long sequences from concatenating tokens across multiple frames, temporal convolutional network aggregates motion-relevant features into compact representation. This representation is then processed by decoder-only transformer to autoregressively generate foresight tokens, which are subsequently decoded into the predicted future image ˆot+1. Through this design, the generation expert realizes an efficient mechanism for real-time foresight, supplying explicit visual targets that guide the inverse dynamics model in action execution. , . . . , zk Action Expert. The action expert is responsible for mapping the multimodal context into executable robot actions. Conditioned on the language goal l, the current observation ot, and the generated foresight image ˆot+1, it predicts short-horizon action sequence ˆat:t+k. By incorporating foresight 4 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions explicitly, the policy grounds its decisions not only in the present state but also in an anticipated visual target, which supports goal-directed and temporally consistent behavior. In practice, we employ chunked action prediction (Zhao et al., 2023) which captures motion patterns over multiple steps and flow-matching objective in continuous action space. Through this formulation, the action expert produces accurate and coherent action plans that are responsive to immediate observations while remaining consistent with long-term task objectives. Attention Mechanism. To coordinate these heterogeneous experts, we introduce hierarchical scheme termed UnderstandingGenerationAction (UGA) progressive attention. Within each expert, bidirectional intra-expert attention enables comprehensive token interactions, except in the generation expert, where foresight tokens follow causal, scale-conditioned pattern to preserve autoregressive consistency. Across experts, inter-expert attention follows causal hierarchy: the generation expert attends to the understanding expert, while the action expert attends to both, but no information flows in the reverse direction. This progressive design enforces foresight as an explicit intermediate representationpreventing information leakage from actions back into foresightthereby stabilizing training, enhancing interpretability, and ensuring that downstream control is genuinely guided by predicted visual outcomes rather than by shortcut correlations. 2.3 TRAINING RECIPE We train F1 on mixture of open-source datasets and task-specific collections, covering more than 320k trajectories across 136 tasks and 5 embodiments. Comprehensive statistics of the training corpus are provided in the Appendix A. Our training recipe follows three-stage paradigm designed to progressively build alignment, generalization, and task adaptation: (i) Pretrain Stage I., aligning the generation expert with the understanding expert, (ii) Pretrain Stage II, pretraining the full model on large-scale public robot datasets, and (iii) Post-train Stage, post-training on task-specific demonstrations for embodiment adaptation. Pretrain Stage I. The understanding expert inherits the weight from π0 (Black et al., 2024), while the generation expert is randomly initialized. We then train the generation expert to synthesize future visual tokens conditioned on historical observations and language instructions, with outputs aligned to the semantic space established by the pretrained understanding expert. This stage injects generative foresight into the model while retaining the pretrained visionlanguage alignment. Formally, given sequence of historical observations {otm, . . . , ot} and an instruction l, the generation expert predicts foresight image ˆot+1 whose VQ token representation is matched to the target. The training objective minimizes the negative log-likelihood of the ground-truth tokens: gen = E{oi,l}D Lgt (cid:88) log pθ(zj zgt 1:j1, {oi}t i=tm, l) , (1) j=1 where zgt forcing to stabilize autoregressive training at this stage. = E(ot+1)j are discrete tokens encoded by the residual VQ-VAE. We apply teacher Pretrain Stage II & Post-train Stage. After Stage aligns the generation and understanding experts, we jointly optimize all three experts under unified framework. There are two steps: Stage II pretrains the full model on large-scale public robot datasets for foundational visuomotor learning, and the post-train stage finetunes on task-specific demonstrations for embodiment adaptation. There are two objectives driving this process: (1) Autoregressive Next-Scale Prediction. In contrast to the teacher-forcing regime in Stage I, we adopt an autoregressive formulation where each foresight token is generated conditioned on previously predicted tokens. Given historical observations {otm, . . . , ot} and language instruction l, the model autoregressively generates future VQ tokens ˆz1:N , where each prediction step conditions on previously generated tokens: gen = E{oi,l}D Lpred (cid:88) log pθ(zj ˆz1:j1, {oi}t i=tm, l) , (2) This autoregressive formulation enforces distributional consistency between training and inference, thereby enhancing generation stability and long-horizon coherence. j=1 5 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions (2) Flow Matching-based Action Prediction. To establish principled connection between foresight and motor control, we adopt flow matching (Lipman et al., 2023) to model the continuous transformation from Gaussian noise to expert actions. Given interpolated actions aτ = (1 τ )ϵ + τ at with τ U(0, 1) and ϵ (0, I), the policy πθ learns vector fields that guide the transformation toward target actions: Laction = E{at,oi,qt,l}D (cid:104)(cid:13) (cid:13)πθ(l, {oi}t i=tm, qt, aτ ) (at ϵ)(cid:13) (cid:13) 2(cid:105) , (3) where qt denotes the proprioception information at time t. The overall training objective is defined as weighted sum of the two components: Ltotal = Lpred gen + λ Laction, (4) with λ balancing the objectives. This joint optimization enforces representational consistency across experts, integrates foresight with control, and facilitates generalization over tasks and embodiments. 2. IMPLEMENTATION DETAILS Model Architecture. F1 adopts Mixture-of-Transformer architecture comprising an understanding expert, generation expert, and an action expert. The architecture of understanding expert is implemented the same as PaliGemma (Beyer et al., 2024), while the generation and action experts follow the same Gemma backbone (Team et al., 2025b). The backbone integrates Swish activations (Ramachandran et al., 2017), RMSNorm normalization (Zhang & Sennrich, 2019), and Rotary Position Embeddings (Su et al., 2023). For initialization, the understanding and action experts are inherited from π0 (Black et al., 2024), whereas the generation expert is randomly initialized and equipped with pretrained residual VQ-VAE from VAR for image quantization (Tian et al., 2024a). Dataset. F1 is trained on large-scale corpus of robotic manipulation trajectories comprising approximately 330k episodes across 136 tasks. The corpus integrates widely used public benchmarks, including LIBERO (Liu et al., 2023a), Open-X-Embodiment (Collaboration et al., 2025), and AgiBotWorld (AgiBot-World-Contributors et al., 2025) (see Appendix for details). It spans broad spectrum of skills, from elementary pick-and-place to complex behaviors such as grasping, handover, and pushing, and covers diverse temporal scales with episode lengths ranging from 10 seconds to over 2 minutes. This diversity in task complexity and temporal horizon provides rich supervision for developing robust visuomotor policies. Training and Inference. Training proceeds in three sequential stages. In Pretrain Stage I, the understanding expert is frozen while the generation expert is trained with VAR to predict future images across 10 resolutions. In Stage II and the post-training stage, all experts are optimized jointly for end-to-end visuomotor learning. Further hyperparameter details, such as batch size and training steps, are provided in the Appendix B. For efficient real-time control during inference, foresight prediction is restricted to 4 scales."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "To thoroughly evaluate our proposed F1, we conduct extensive experiments across both simulated benchmarks and real-world tasks. Our evaluation not only validates the models core performance but also investigates its robustness and generalization in various challenging scenarios. We first present quantitative comparison of F1 with existing mainstream Vision-Language-Action models (Black et al., 2024; Kim et al., 2024; Bjorck et al., 2025) to demonstrate its superior performance. We then perform series of comprehensive ablation studies to justify the necessity and contribution of each key component. Furthermore, to provide deeper insights into the models robustness, generalization, and rapid adaptation, we conduct several additional experiments. Specifically, we set up kitchen environment with conveyor belt to evaluate its performance on challenging dynamic manipulation task involving moving target. We also conduct experiments on various robot embodiments to evaluate the models ability to rapidly adapt to new platforms. Finally, we evaluate its long-term robustness in complex, sequential scenarios through long-horizon task. 6 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 4: Overview of real-world robot experiments. We conduct 12 real-world robotic experiments on three different platforms: Genie-1, Franka, and ARX LIFT II. The experiments on Genie-1 are designed to evaluate the models ability to handle task diversity. The Franka experiments assess the models quick adaptation capabilities, while the ARX LIFT II tasks are used to benchmark its performance on challenging long-horizon manipulation problems. 3.1 COMPARISON ON THE REAL-WORLD TASKS To further validate the performance of our proposed F1, we conduct experiments on 9 real-world tasks using the Genie robot, dual-arm manipulation platform as illustrated in Fig. 4. Details of the hardware platform and task setup are provided in Appendix C. For each task, we collect demonstrations and fine-tune F1 and baseline VLA models, including π0 (Black et al., 2024), gr00t-N1 (Bjorck et al., 2025), and gr00t-N1.5 (Bjorck et al., 2025), for comparison. Each model is evaluated 15 times per task, and we report the average grasp and task success rates. As shown in Tab. 1, F1 demonstrates superior performance across all tasks, achieving an average grasp rate of 92.6% and an average success rate of 82.2%. In contrast, the best-performing baseline, π0, only achieves 78.5% grasp and 65.2% success rates, while gr00t-N1 and gr00t-N1.5 perform even worse. These results clearly highlight the effectiveness of our foresight-guided design and modular reasoning capabilities. In terms of individual tasks, F1 achieves perfect grasp rate (100%) on some tasks with correspondingly high success rates, e.g. Pen, Flower, Chip, and Tea (Table). Notably, on more challenging tasks, i.e., Handover (R2H), which require precise coordination and dynamic adjustment, F1 consistently outperforms baselines by large margins, achieving up to 93.3% success rate compared to π0 (40%) and gr00t-N1 (13.3%). The superior performance of F1 can be attributed to its ability to predict plausible future visual states via next-scale prediction and leverage them as explicit planning targets. By transforming action generation into foresight-guided inverse dynamics problem, F1 effectively bridges understanding, generation, and action, leading to improved robustness and generalization across diverse real-world tasks. Pen Flower Chip Tea (Table) Tea (Shelf) Bread Handover π0 gr00t-N1 gr00t-N1. 5 F1 (Ours) 66.7% 46.7% 73.3% 93.3% 66.7% 33.3% 40.0% 80.0% 86.7% 33.3% 46.6% 100.0% 86.7% 40.0% 73.5% 93.3% 73.3% 13.3% 26.6% 86.7% 66.7% 33.3% 53.3% 66.7% 33.3% 26.7% 60.0% 80.0% Handover (R2H) 40.0% 13.3% 40.0% 73.3% Mixture Average 66.7% 33.3% 66.7% 66.7% 65.2% 30.4% 53.3% 82.2% Table 1: Results on Real-world Tasks. We test each model 15 times per task and report its average success ratio. The best results are denoted in bold. The experimental results show the superior performance of our F1 comparing other VLA models, especially in the Handover (R2H) task which requires the dynamic environment understanding capabilities. 3.2 COMPARISON ON THE SIMULATION BENCHMARK We evaluate F1 against various VLA models on two simulation benchmarks: LIBERO (Liu et al., 2023a) and SimplerEnv Bridge (Li et al., 2024b). This comparison specifically aims to demonstrate how F1s integrated understanding-generation-action framework, leveraging explicit visual foresight as targets, leads to superior and more robust performance compared to prevailing methods. 7 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions LIBERO Benchmark. It evaluates robotic manipulation skills, focusing on spatial and objectcentric reasoning, and long-horizon planning. We compare F1 with Diffusion Policy (Chi et al., 2023), OpenVLA (Kim et al., 2024), SpatialVLA (Qu et al., 2025b), π0 (Black et al., 2024), π0Fast (Pertsch et al., 2025), gr00t-N1 (Bjorck et al., 2025), and CoT-VLA (Zhao et al., 2025), using Success Rate (SR) and Ranking (Rank) as primary metrics. As shown in Tab. 2, our proposed model F1 consistently achieves superior performance across all suites. This benchmark includes tasks demanding precise spatial understanding, intricate object interaction, goal-oriented execution, and particularly challenging long-horizon sequences. The significant improvement underscores the efficacy of F1s unique foresight-guided planning approach, which allows it to transform action generation into more informed inverse dynamics problem by predicting plausible future visual states. Notably, on the LIBERO-Long tasks, our model exhibits pronounced advantage over reactive methods. It highlights F1s enhanced capability for long-term planning and execution, direct benefit of its explicit visual foresight. LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average Pretrained SR () Rank () SR () Rank () SR () Rank () SR () Rank () SR () Rank () Diffusion Policy OpenVLA SpatialVLA π0 π0-Fast gr00t-N1 CoT-VLA F1 (Ours) F1 (Ours) 78.5% 84.7% 88.2% 98.0% 96.4% 94.4% 87.5% 97.4% 98.2% 11 8 6 2 3 5 7 3 1 87.5% 88.4% 89.9% 96.8% 96.8% 97.6% 91.6% 97.6% 97.8% 10 9 8 4 4 1 7 2 1 73.5% 79.2% 78.6% 94.4% 88.6% 93.0% 87.6% 94.2% 95.4% 11 9 10 2 5 4 6 3 1 64.8% 53.7% 55.5% 88.4% 60.2% 90.6% 69.0% 88.0% 91.3% 6 10 9 3 8 2 4 4 1 76.1% 76.5% 78.1% 94.4% 85.5% 93.9% 83.9% 94.3% 95.7% 9 8 7 2 5 4 6 3 1 Table 2: Results on LIBERO Benchmark. We report Success Rate (SR, higher is better) and Ranking (Rank, lower is better) across four suites: LIBERO-Spatial, LIBERO-Object, LIBEROGoal, and LIBERO-Long, as well as the averaged results. The best results are displayed in bold. SimplerEnv Bridge Benchmark. It focuses on complex, multi-step manipulation tasks that often require fine-grained control and precise interaction. We compare F1 against RT-1-X (Brohan et al., 2023), RoboVLM (Li et al., 2024a), SpatialVLA, π0, and π0-Fast, measuring Grasp Success and Overall Success for each task. Tab. 3 shows the performance of our model and other methods. Overall, F1 substantially outperforms other models, demonstrating marked improvement in average success rate compared to the next best baselines. This superior performance is largely attributed to F1s enhanced generalization capabilities for precise pick and place positions, which are critical for success in these intricate manipulation scenarios. We induce that unlike reactive policies that might struggle with different source object configurations or target locations, F1s foresight mechanism enables it to adapt more effectively. Carrot on Plate Eggplant in Basket Spoon on Towel Stack Block Overall Pretrained Grasp Success Grasp Success Grasp Success Grasp Success Average RT-1-X RoboVLM SpatialVLA π0 π0-Fast F1 (Ours) F1 (Ours) 4.2% 20.8% 25.0% 25.0% 25.0% 29.2% 0.0% 25.0% 21.9% 58.5% 70.8% 33.3% 87.5% 70.8% 16.7% 0.0% 0.0% 45.8% 58.3% 12.5% 100.0% 100.0% 20.8% 91.6% 16.6% 50.0% 83.3% 10.8% 54.0% 70.8% 75.0% 87.5% 100.0% 70.8% 66.7% 0.0% 58.3% 16.7% 62.5% 66.6% 45.8% 50.0% 8.3% 54.2% 62.5% 45.8% 62.5% 83.3% 87.5% 0.0% 29.2% 29.2% 29.1% 29.1% 62.5% 50.0% 6.3% 38.5% 47.9% 40.1% 48.3% 66.1% 72.9% Table 3: Results on SimplerEnv Bridge Benchmark. For each task, we report both Grasp Success and Overall Task Success, capturing fine-grained control and precise placement capabilities. The last column shows the overall average score, obtained by averaging the success rates of grasping and full task completion across all tasks. The best results are displayed in bold. F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions LIBERO SimplerEnv Bridge Spatial Object Goal Long Carrot Eggplant Spoon Block Avg Comparison of F1 and its variant which freezes the generation expert during Stage II and III training F1 (a) Frozen-Gen 98.2% 97.8% 95.4% 91.3% 70.8% 95.6% 97.0% 93.6% 87.4% 41.7% 50.0% 50.0% 77.5% 45.8% 75.0% 73.8% 66.7% 54.2% Comparison of F1 and its variant which is only trained by Stage and Stage III F1 66.7% 75.0% (b) Cotrain-Scratch 98.2% 97.8% 95.4% 91.3% 70.8% 97.4% 97.6% 94.2% 88.0% 33.3% 50.0% 50.0% 77.5% 45.8% 62.5% 74.2% Comparison of F1 and its variant which does not have generation expert F1 (c) No-Gen 98.2% 97.8% 95.4% 91.3% 70.8% 98.0% 96.8% 94.4% 88.4% 0.0 % 66.7% 16.6% Comparison of F1 and its variant with varying numbers of planning scales F1 (4-Scales) (d) 2-Scales (e) 6-Scales 98.2% 97.8% 95.4% 91.3% 70.8% 98.0% 97.6% 94.6% 88.6% 29.2% 97.0% 97.2% 94.2% 89.2% 50.0% 66.7% 83.3% 87.5% 50.0% 50.0% 77.5% 62.5% 29.1% 60.3% 50.0% 50.0% 77.5% 41.7% 54.2% 73.4% 37.5% 54.2% 75.8% Table 4: Ablation Studies on the Simulation Benchmark. We design five model variants to verify the effects of different components of F1: (a) Frozen-Gen, (b) Cotrain-Scratch, (c) No-Gen, (d) 2Scales, and (e) 6-Scales. 3.3 ABLATION STUDIES 3.3.1 RESULTS ON THE SIMULATION BENCHMARK To better understand the individual contributions of foresight prediction and pretraining in F1, we conduct systematic ablations on the LIBERO and SimplerEnv-Bridge benchmarks. We design five model variants to isolate the effects of visual foresight and pretraining under different configurations: (a) Frozen-Gen: The generation expert is pretrained in Stage and then frozen during Pretrain Stage II and Post-train Stage III. Its predicted foresight tokens are used as planning cues to guide action prediction, but not used as training targets. (b) Cotrain-Scratch: The model is trained by Pretrain Stage and Post-train Stage III, without Pretrain Stage II, i.e., pretraining on large-scale robotic datasets. (c) No-Gen: The generation expert is entirely removed, resulting in purely VLA model without any foresight prediction or pretraining. (d) 2-Scales: The fully pretrained model predicts foresight tokens for only two future steps at inference time. (e) 6-Scales: The fully pretrained model predicts foresight tokens for six future steps at inference, evaluating the effect of different scales of intermediate visual representation. Training vs. Freezing the Generation Expert. To understand the benefits of joint optimization, we compared our model with the (a) Frozen-Gen variant. In this variant, the generation expert was pretrained in Stage but then kept frozen, preventing it from adapting during subsequent stages. This comparison revealed moderate performance drop from 77.5% to 73.8% for the Frozen-Gen variant. Moreover, it also confirms that while fixed, pretrained generation expert can still provide useful planning cues, end-to-end adaptation is crucial for achieving better task alignment. The performance gap indicates that allowing the generation expert to be fine-tuned during later stages enables it to better capture task-specific dynamics. Ultimately, this highlights the significant benefit of jointly optimizing the foresight prediction with the downstream control policy. Effectiveness of Pretrain. We assess the impact of pretraining by comparing our model with the (b) Cotrain-Scratch variant, which was trained without the large-scale robotics dataset pretraining (Stage II). The removal of this stage results in an overall performance drop of approximately 3.3%. This finding suggests that pretraining on robotics data acts as crucial inductive prior. It effectively stabilizes the entire optimization process and allows the model to inherit foundational manipulation skills. As result, our model achieves higher success rate on downstream tasks by building upon the robust capabilities acquired during the pretraining stage. 9 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Effectiveness of Generation Expert. To evaluate the contribution of the generation expert, we conduct key ablation study by comparing our full model with the (c) No-Gen variant, where the entire visual foresight branch was removed. As the results in Table 1 show, this operation leads to significant performance drop from 77.5% to 60.3%. This marked degradation highlights the critical role of the generation expert in providing explicit visual foresight for effective task planning. The absence of foresight signals severely impairs the models ability to achieve generalized goal alignment, reducing it to more reactive policy. These findings conclusively demonstrate that the generation expert provides essential high-level guidance, enabling the policy to move beyond simple reactive behaviors and make more informed, deliberate planning decisions. Impact of Planning Scales. We assess the impact of the foresight horizon length (planning scales) by comparing our model with variants that used different numbers of future steps for foresight prediction: (d) 2-Scales, (F1) 4-Scales, and (e) 6-Scales. As shown in Tab. 4, these models consistently outperform the No-Gen baseline, underscoring the effectiveness of explicit visual foresight as planning mechanism. While increasing the planning scale from 2 to 6 steps generally improved performance, the 4-Scales configuration struck the optimal balance. This suggests that planning horizon of four steps provides sufficient temporal abstraction to guide the policy without introducing unnecessary noise or computational overhead, thus yielding the most robust and effective results. This finding highlights the importance of selecting an appropriate planning scale to maximize the benefits of visual foresight. 3.3.2 ABLATION STUDIES ON THE REAL-WORLD TASKS To further verify the contribution of pretrain and generation which are two key components within F1, we conduct ablation studies on real-world tasks. Specifically, we compare the performance of our complete model F1 against two variants: 1. Cotrain-Scratch: the variant of F1 which removes the pretraining of Stage II; 2. No-Gen: the variant which removes the foresight-related module, without any pretraining. The results, as depicted in Fig. 5, provide clear insights into the role of each component. F1 consistently outperforms both the Cotrain-Scratch variant and No-Gen across all evaluated tasks. notable performance gap is observed when comparing F1 with No-Gen, underscoring the critical role of the generation expert. For complex tasks such as Handover (R2H) and Mixture, No-Gen only obtains 40.0% and 60.0% entire success rate, respectively, while our model achieves much higher success rate of 93.3% and 73.3%. This contrast highlights that the visual foresight provided by the generation expert is essential for handling complex task dynamics and achieving robust goal alignment. Furthermore, the pretraining stage also proves to be crucial component. When comparing F1 with Cotrain-Scratch variant, our model shows clear performance advantage in many tasks indicating that pretraining on large robotics dataset provides strong inductive prior. This prior enables the model to acquire foundational manipulation skills, which significantly improves generalization. Figure 5: Ablation Studies on Real-world Tasks. We compare F1 with π0 as well as variant that removes Pretrain Stage II. For each task, we conduct 15 trials to ensure statistical reliability. The results show that without Pretrain Stage II, F1 suffers substantial performance drop, even falling below π0. In contrast, incorporating Pretrain Stage II leads to marked improvements, with F1 significantly surpassing the baseline. 10 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 6: Visualization of dynamic manipulation task. kitchen environment is set up with moving belt, where the robot must grasp specified food item according to language prompt while objects continuously move along the belt. 3.4 ROBUSTNESS AND GENERALIZATION 3.4.1 DYNAMIC ENVIRONMENT To evaluate F1s robustness, we set up dynamic manipulation task. As depicted in Fig. 6, we construct kitchen environment with moving belt. The robot is required to grasp specific food item based on given prompt, while the items are on moving belt. To further test the models generalization capabilities, we adopt novel robot, ARX LIFT II, which is absent from our pretraining dataset. The number of post-train demonstrations is only 47 to explore the shared control capability obtained from pretraining stages. As shown in Figure 3, our F1 achieves remarkable 66.7% success rate for continuous dual-arm dynamic grasping. For both the Lettuce and Bread tasks, our model achieves an impressive 80.0% success rate, while π0 obtains only 53.3% and 46.7%, respectively. This performance stands in stark contrast to the π0, which achieves success rate of only 33.3%. F1s superior performance is directly attributable to its core visual foresight module, which enables it to predict the future position of the moving object and plan its actions accordingly. detailed breakdown reveals the source of F1s superior performance. It demonstrates that F1 effectively leverages its pretrained visual knowledge to generalize to novel embodiments and robustly handle dynamic, real-world challenges. Figure 7: Results of the Dynamic Manipulation Task. 3.4.2 ADAPTATION LEARNING To further validate the generalization capabilities of our model, we conduct two additional sets of experiments, i.e., sweep and sort, on Franka robotic arm. For the sweeping task, we evaluate three key performance metrics: the number of objects successfully swept, the maximum number of attempts required to complete the task (capped at five), and the number of empty sweeps. For the sorting task, we measure the success rate over three consecutive grasps. The results are shown in Tab. 5. In the sweeping task, our model achieves more efficient and reliable interactions, reflected in higher success rates, fewer attempts, and notably fewer empty sweeps. The latter are largely attributable to vertical misalignment, and their reduction indicates more precise spatial grounding during execution. In the sorting task, our model also outperforms π0, particularly in the second and third consecutive grasps, where success rates increase substantially. This suggests that F1 is better able to sustain performance across repeated interactions, highlighting enhanced robustness in sequential manipulation scenarios. 3.4.3 LONG-HORIZON TASK To further examine the planning and foresight capabilities of F1, we design long-horizon manipulation task on the ARX LIFT II platform. This task consists of ten sequential steps and requires 11 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Sweep Success Rate of Consecutive Sort # Succ. Objects () # Max Trials () # Empty Sweeps () 1st () π0 F1 4.9/8.0 7.1/8.0 4.8/5.0 3.5/5.0 2.4/5.0 0.8/5.0 100.0% 100.0% 2nd () 86.7% 100.0% 3rd () 53.3% 66.7% Table 5: Experimental Results on Franka arm. The values for the sweep task represent the average performance over multiple trials. For the sort task, we report the success rate for each of three consecutive grasping attempts. ① Pickplace Coke ② Pickplace Banana ③ Pour Box ④ Pickup Cloth ⑤ Wipe Table 93.3% 100.0% 93.3% 100.0% 0.0% 100.0% 0.0% 93.3% 0.0% 93.3% ⑥ Pickup Broom ⑦ Pickup Dustpan ⑧ Sweeping ⑨ Catch Rolling Ball ⑩ Pour Dustpan 0.0% 73.3% 0.0% 60.0% 0.0% 40.0% 0.0% 40.0% 0.0% 40.0% π0 π0 F1 Table 6: Step-wise success rates in the long-horizon task. The task involves ten sequential steps spanning approximately two minutes. Each column reports the average success rate of specific step across 15 trials. π0 struggles beyond the first 4 stages, while F1 achieves consistently high success rates in early steps and maintains non-trivial performance across later more complex stages. approximately two minutes to complete. Unlike short episodic interactions, the long-horizon setting places greater demands on temporal consistency, error recovery, and the ability to sustain goaldirected behavior across multiple stages. By evaluating under this setup, we aim to assess whether the foresight-guided mechanism in F1 can support coherent action sequences over extended durations, thereby testing its robustness in realistic, temporally extended scenarios. The results in Tab. 6 reveal stark contrast between the baseline and our model. The baseline policy π0 can complete only the simplest grasping actions, but consistently fails when confronted with more complex interactions such as pouring, wiping, or multi-object coordination. In contrast, F1 achieves near-perfect performance in the initial stages and sustains meaningful success rates throughout the later steps. Although its performance gradually decreases as the sequence progresses, this trend is expected in long-horizon scenarios due to error accumulation and the compounding difficulty of temporally extended reasoning. The ability of F1 to complete the majority of the ten steps, while maintaining robustness over two-minute execution horizon, highlights its capacity for foresightguided planning and resilience in sequential, real-world tasks."
        },
        {
            "title": "4 RELATIONSHIPS BETWEEN GENERATION QUALITY AND ACTIONS",
            "content": "In F1, the generation expert acts as visual planner, and the quality of its foresight images is crucial for ensuring the reliability of the action expert. To investigate this relationship, we perform two-step analysis. First, we assess the foresight image quality using set of predefined evaluation metrics. Second, we study the correlation between generation quality and action prediction by examining token-level accuracy. 4.1 QUANTITATIVE ANALYSIS OF GENERATION Unlike conventional image-generation works that emphasize pixel-level or distribution-level metrics, e.g, FID (Heusel et al., 2018), or PSNR, our objective is not merely to measure visual realism, but to evaluate whether generated future observations provide actionable guidance for downstream control. To this end, we design multimodal evaluation protocol based on large visionlanguage model, Qwen2.5-VL-32B-Instruct (Bai et al., 2025). The evaluator receives task instruction, four historical frames of the robot executing the task, one predicted next-step frame, and the ground-truth frame. The detailed prompt template is included in Appendix E. The evaluation targets three dimensions directly related to action feasibility: 12 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 8: Generation Quality across Training Steps. We report the evolution of generation quality along three dimensions: scene consistency (left), object consistency (middle), and task progress following (right). The x-axis denotes training steps, and different curves correspond to distinct task subsets (AgibotWorld, Bridge, Fractal, and LIBERO). Higher scores indicate better alignment between the generated future observations and the ground-truth task progression. 1. Scene Consistency: Whether the global environment remains coherent in layout, lighting, and texture, with blurry or structurally incoherent generations penalized. 2. Object Consistency: Whether manipulated objects and the robot remain consistent in identity and spatial position, penalizing missing, deformed, or hallucinated objects. 3. Task Progress Following: Whether the generated frame depicts plausible next step toward fulfilling the instruction, consistent with the ground-truth trajectory. Each aspect is scored in binary manner (0/1), and the aggregated score forms our measure of generation quality. This design explicitly ties evaluation to task relevance, enabling us to study how higher-quality generations correlate with improved action execution. Fig. 8 shows that the models visual planning capabilities develop hierarchically. Scene Consistency improves rapidly at early stages, indicating that global coherence is easier to acquire. Object Consistency, however, presents major challenge: without pretraining on large-scale object-centric datasets, the model struggles to preserve fine-grained shapes and positions, resulting in flatter curves and lower scores throughout training. Despite this weakness, Task Progress Following improves steadily and often surpasses object consistency, suggesting that the model captures high-level temporal task logic even without pixel-perfect object representation. This finding highlights the ability of generation expert to generate actionable foresight images, which is ultimately more critical for downstream control than exact visual fidelity. 4.2 QUALITATIVE ANALYSIS OF GENERATION In addition to the quantitative evaluation, we further conduct qualitative analysis to gain deeper insights into the strengths and limitations of the generation expert. Fig. 9 presents representative examples of generated foresight images across diverse tasks, such as supermarket manipulation, and clothing folding. Overall, the predicted frames capture task-level plausibility and remain aligned with the ground-truth trajectories, suggesting that the model internalizes temporal understanding of task logic rather than merely memorizing visual appearances. This ability to generate plausible next-step states highlights the role of the generation expert as visual planner. At the same time, we observe clear limitations in visual fidelity, particularly in cases involving fine-grained object details or deformable objects, e.g., grid-shape shopping cart, plastic bags, and clothing. key reason for this weakness is that our model has not been pretrained on large-scale generative datasets, making it more difficult to preserve precise object shapes and textures. Nevertheless, these imperfections rarely prevent the predictions from providing actionable guidance for downstream control, since the generated frames still convey the essential task progression. These qualitative observations complement our quantitative results: while pixel-level precision remains challenging, the generation expert consistently produces foresight images which are sufficiently informative to support action planning. F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 9: Visualization of Generated Future Images. It demonstrates the ability of F1 to generate plausible next-step frames for various manipulation tasks, including supermarket item pickup, supermarket packing, and folding shorts. Each row compares Ground Truth frame with Prediction from F1, showcasing its accurate foresight across diverse scenarios. 4.3 CORRELATION BETWEEN GENERATION AND ACTIONS To further study the connection between generation quality and action reliability, we perform controlled experiments on the LIBERO benchmark (Liu et al., 2023a). During training, our model jointly optimizes two objectives: (i) the next visual state through next-scale prediction, and (ii) the action via flow matching. To measure progress on these two objectives, we employ accuracy metrics for both the image and action modalities. For the image modality, we adopt Residual VQ-VAE representation, which formulates image prediction as token-level classification problem. Accordingly, we measure image token accuracy, defined as the classification accuracy of predicted visual tokens against the ground truth tokens; For the action modality, we compute the action token accuracy via: Accτ = 1 (cid:88) t=1 [ˆat at < τ ] , (5) where is the total number of action tokens and τ denotes the error tolerance threshold. Since training is performed at the chunk level, we evaluate accuracy under the same granularity. Fig. 10 illustrates the relationship between image token accuracy and action token accuracy across four LIBERO suites. Across all error tolerance levels (τ =0.01, 0.02, 0.05), we observe consistent positive correlation, confirming that improvements in visual foresight are closely aligned with improvements in action prediction. Notably, the absolute value of image token accuracy remains relatively low (around 4045% on average). This limitation arises from the fact that our model is not pretrained on large-scale generative datasets, which makes fine-grained token prediction particularly challenging. Nevertheless, even with imperfect image token accuracy, the generated foresight provides sufficient task-relevant cues for the action expert, leading to high action token accuracy. These results highlight two important insights: 1. Pixel-level image prediction is not strictly necessary for effective action planning: even when the average image token accuracy remains modest, the generated foresight still conveys sufficient task-level cues to support high action accuracy. 14 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions 2. The strong positive correlation indicates that improvements in image token prediction are reflected in action reliability. Thus, while pixel fidelity is not required, advancing the quality of visual token prediction remains promising way to enhance downstream action performance. Figure 10: Correlation between Image and Action Token Accuracy. It shows the relationship between image token accuracy and action token accuracy across the four LIBERO suites. Each subfigure corresponds to one suite, and action accuracy is reported at different tolerance levels (τ =0.01, 0.02, 0.05). The consistent positive correlation across all settings suggests that higher-quality visual foresight is closely aligned with improved action prediction reliability."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 VISION LANGAUGE ACTION MODEL The rapid progress of multimodal large language models (MLLMs) (Liu et al., 2023b; OpenAI et al., 2024; Yang et al., 2025a; Bai et al., 2025) has motivated the development of Vision Language Action (VLA) models. VLA models incorporate vision language model and augment it with an action prediction module (Black et al., 2024; Kim et al., 2024; Qu et al., 2025b; Song et al., 2025; Team et al., 2025a; Cheang et al., 2025; Bjorck et al., 2025; Yang et al., 2025b; Bu et al., 2025; Qu et al., 2025a). They leverages the strong perceptual and linguistic grounding of pretrained VLMs, allowing robots to interpret human instructions more flexibly than purely reactive policies. Despite this promise, current VLA models remain limited in robustness. Most formulations still predict actions reactively from the current state without reasoning about how the scene may evolve, leading to short-sighted behavior in dynamic and long-horizon tasks. Despite some studies having attempted to incorporate temporal memory (Li et al., 2025a; Shi et al., 2025) and post-train VLA with reinforcement learning methods (Zhang et al., 2025b; Lu et al., 2025), they still struggle to cope with complex scenarios. In contrast, we propose unified VLA model which integrates the visual foresight generation into the decision-making pipeline, thereby enhancing its robustness capability. 5.2 INVERSE DYNAMICS MODEL Since directly mapping the visual observation and textual instruction to action space is challenge, prior studies (Deng et al., 2025b; Hu et al., 2024; Liao et al., 2025b; Zhong et al., 2025; Cen et al., 2025; Wang et al., 2025; Zhao et al., 2025; Gao et al., 2025) explore to enhance action prediction by injecting auxiliary intermediate representations during training, e.g., grasping poses, segmentation masks, optical flow, or future images, to guide the model toward more structured outputs. However, these representations are often domain-specific and do not fully leverage the potential competence of the pretrained large language model, leaving the policies brittle when deployed beyond their training distributions. Inverse dynamics model (Du et al., 2023) can extract the underlying actions from two consecutive images, thereby decrease the difficulty of mapping from image space to action space. Recent works (Black et al., 2023; Li et al., 2025b; Zhu et al., 2025; Cen et al., 2025; Zhao et al., 2025; Wang et al., 2025; Zhang et al., 2025a) attempt to decompose the decision-making task to first generate future images or videos, and then predict actions. Nevertheless, they mostly use the future prediction objective as regularizer when training, but seldom generate the visual guidance during the inference stage. In contrast, our model first predicts the next frame and then predicts the action conditioned on the predicted visual foresight, improving the robustness and generalization. F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions"
        },
        {
            "title": "5.3 UNIFIED VISION LANGUAGE MODEL",
            "content": "Building on MLLMs, recent research explores unified models that combine visual understanding and generation within single framework. Early approaches (Lu et al., 2023; Zhou et al., 2024; Xie et al., 2024; Wang et al., 2024) employ discrete visual tokenization to enable joint modeling, but suffer from information loss and weakened semantics. While line of works (Wu et al., 2024; Pan et al., 2025; Chen et al., 2025; Lin et al., 2025) adopt modular assemblies of pretrained MLLMs and diffusion models, sacrificing true unification. More recent efforts (Deng et al., 2025a; Liao et al., 2025a) introduce Mixture-of-Transformers (MoT) architectures with separate experts for text and visual generation, but still inherit the latency of diffusion and reliance on external encoders. However, existing unified frameworks remain centered on visual understanding and generation, leaving action outside the scope of cognitive intelligence. From the perspective of embodied AI, we argue that intelligence requires not only perceiving and imagining but also interacting with the physical world. Compared to understanding or generation, action is inherently more complex and demanding. This motivates us to extend the unified paradigm toward an understanding generation action framework, enabling agents to achieve more complete form of cognitive intelligence."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "This paper has introduced F1, pretrained Vision-Language-Action (VLA) framework that integrates goal-conditioned visual foresight into the perceptionaction loop. Building on the principle of predictive inverse dynamics, F1 reformulates control as foresight-guided inverse dynamics, allowing actions to be derived not only from the current state but also from an anticipated visual outcome. Architecturally, the model adopts Mixture-of-Transformer design with three dedicated experts for understanding, foresight generation, and action execution, while next-scale prediction mechanism and progressive attention scheme regulate the flow of information across modules. To further enhance robustness and transferability, we introduced three-stage training recipe that progressively aligns, pretrains, and adapts the experts on large-scale and task-specific robot datasets. Extensive experiments across simulation benchmarks and physical platforms demonstrate that F1 consistently surpasses reactive baselines, achieving higher success rates and improved generalization in dynamic and long-horizon tasks. Beyond the reported performance gains, this work integrates predictive foresight with multimodal grounding in unified VLA framework. The modular architecture adapts large-scale visionlanguage backbones to robotic control and incorporates dedicated experts for understanding, In parallel, the progressive training scheme offers foresight generation, and action execution. systematic way to align and integrate these components, ensuring that foresight signals remain consistent with semantic grounding while supporting transfer across tasks and embodiments. This combination contributes to policies that are less dependent on purely reactive mappings and better suited to dynamic and long-horizon scenarios. More broadly, the study provides evidence that coupling foresight with multimodal grounding is viable direction for advancing robust visuomotor control. Several avenues remain for future investigation. Scaling foresight-driven policies to more diverse embodiments and task families, e.g., locomotion, dexterous manipulation, or multi-agent collaboration, would provide stronger test of generality. Another direction is to enrich the foresight generation module with structured world models or physics-informed priors, enabling more accurate long-horizon reasoning and robustness under distributional shift. Integrating reinforcement learning or online adaptation strategies with foresight-guided architectures may further allow agents to refine policies beyond imitation, supporting continual improvement in open-ended environments. Finally, exploring how human feedback or interactive correction can be incorporated into foresight-driven policies presents an opportunity to align embodied agents more closely with human intentions."
        },
        {
            "title": "REFERENCES",
            "content": "AgiBot-World-Contributors, Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, Shu Jiang, Yuxin Jiang, Cheng Jing, Hongyang Li, Jialu Li, Chiming Liu, Yi Liu, Yuxiang Lu, Jianlan Luo, Ping Luo, Yao Mu, Yuehan Niu, Yixuan Pan, Jiangmiao Pang, Yu Qiao, Guanghui Ren, Cheng Ruan, Jiaqi Shan, Yongjian Shen, Chengshi Shi, Mingkang Shi, Modi Shi, Chonghao Sima, Jianheng Song, Huijie Wang, Wenhao 16 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Wang, Dafeng Wei, Chengen Xie, Guo Xu, Junchi Yan, Cunbiao Yang, Lei Yang, Shukai Yang, Maoqing Yao, Jia Zeng, Chi Zhang, Qinglin Zhang, Bin Zhao, Chengyue Zhao, Jiaqi Zhao, and Jianchao Zhu. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, 2025. URL https://arxiv.org/abs/2503.06669. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. URL https: //arxiv.org/abs/2407.07726. Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models, 2023. URL https://arxiv.org/abs/2310.10639. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. URL https://arxiv.org/abs/2212.06817. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions, 2025. URL https://arxiv.org/abs/2505.06111. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, and Hao Chen. Worldvla: Towards autoregressive action world model, 2025. URL https://arxiv.org/abs/2506.21539. Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. Gr-3 technical report, 2025. URL https://arxiv.org/abs/2507.15493. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. URL https: //arxiv.org/abs/2505.09568. 17 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Scholkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Joao Silverio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto MartınMartın, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models, 2025. URL https://arxiv.org/abs/2310.08864. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025a. URL https://arxiv.org/abs/2505.14683. 18 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, and He Wang. Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data, 2025b. URL https://arxiv.org/abs/2505.03233. Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation, 2023. URL https://arxiv.org/abs/2302.00111. Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning as general-purpose manipulation world model, 2025. URL https://arxiv. org/abs/2412.08261. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. URL https://arxiv.org/abs/1706.08500. Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. URL https://arxiv.org/ abs/2406.09246. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization, 2022. URL https://arxiv.org/abs/2203. 01941. Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, and Jiangmiao Pang. Cronusvla: Transferring latent motion across time for multi-frame prediction in manipulation, 2025a. URL https://arxiv.org/abs/2506. 19816. Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model, 2025b. URL https://arxiv.org/abs/2503.00200. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models, 2024a. URL https://arxiv.org/abs/2412.14058. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation, 2024b. URL https://arxiv.org/abs/2405.05941. Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models, 2025. URL https://arxiv. org/abs/2411.04996. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation, 2025a. URL https://arxiv.org/abs/2505.05472. Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025b. 19 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation, 2025. URL https://arxiv.org/ abs/2506.03147. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning, 2023a. URL https://arxiv. org/abs/2306.03310. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. URL https://arxiv.org/abs/2304.08485. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning, 2025. URL https://arxiv.org/abs/2505.18719. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models, 2023. URL https://arxiv.org/abs/2304.09842. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries, 2025. URL https://arxiv.org/abs/2504. 06256. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. URL https://arxiv.org/abs/2501.09747. Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, and Dong Wang. Embodiedonevision: Interleaved vision-text-action pretraining for general robot control, 2025a. URL https://arxiv.org/abs/2508.21112. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025b. URL https://arxiv.org/abs/2501.15830. Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017. URL https://arxiv.org/abs/1710.05941. Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang. Memoryvla: Perceptual-cognitive memory in vision-languageaction models for robotic manipulation, 2025. URL https://arxiv.org/abs/2508. 19236. Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, and Xuelong Li. Hume: Introducing system-2 thinking in visual-language-action model, 2025. URL https://arxiv.org/abs/2505.21432. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025a. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gael Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andras Gyorgy, Andre Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, 21 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Poder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Leonard Hussenot. Gemma 3 technical report, 2025b. URL https://arxiv.org/abs/2503.19786. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive In A. Globerson, L. Mackey, modeling: Scalable image generation via next-scale prediction. D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 8483984865. Curran Associates, Inc., 2024a. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction, 2024b. URL https://arxiv.org/ abs/2404.02905. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation, 2024c. URL https: //arxiv.org/abs/2412.15109. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified vision-language-action model, 2025. URL https://arxiv. org/abs/2506.19850. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024. URL https://arxiv.org/abs/ 2410.13848. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2024. URL https://arxiv.org/abs/ 2408.12528. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, 22 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL https://arxiv.org/abs/2505.09388. Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Instructvla: Vision-language-action instruction tuning from underLiao, and Jiangmiao Pang. standing to manipulation, 2025b. URL https://arxiv.org/abs/2507.17520. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. URL https:// arxiv.org/abs/1910.07467. Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge, 2025a. URL https://arxiv.org/abs/2507.04447. Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Siwei Han, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao. Grape: Generalizing robot policy via preference alignment, 2025b. URL https://arxiv.org/abs/2411.19309. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models, 2025. URL https://arxiv.org/abs/2503.22020. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. Flowvla: Thinking in motion with visual chain of thought, 2025. URL https: //arxiv.org/abs/2508.18269. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model, 2024. URL https://arxiv.org/abs/2408. 11039. Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets, 2025. URL https://arxiv.org/abs/2504.02792. F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions"
        },
        {
            "title": "A DATASET DETAILS",
            "content": "Our training corpus combines large internet-scale robot datasets with curated in-house demonstrations, spanning multiple embodiments (Genie-G1, Franka, WidowX, Google Robot, ARX LIFT II), camera viewpoints (third-person and wrist/head), and frame rates (330 FPS). In total, the corpus comprises 330.9K trajectories and 73.8M frames (Tab. 7). Training proceeds in three stages. Pretrain Stages III primarily leverage the internet datasets, i.e., Agibot-World (AgiBot-World-Contributors et al., 2025), OXE-Fractal (Collaboration et al., 2025), OXE-Bridge-v2 (Collaboration et al., 2025), and LIBERO (Liu et al., 2023a), to provide broad coverage of manipulation behaviors and visual dynamics. Post-train Stage III adapts the model to specific skills using smaller but higher-quality set of in-house demonstrations collected across diverse tasks, e.g., handover, sweeping, sorting, kitchen activities, and long-horizon manipulation, with Genie-G1, Franka, and ARX LIFT II. Dataset Source Stage Embodiment Camera Views FPS # Trajs # Frames Agibot-World LIBERO OXE-Bridge-v2 OXE-Fractal Pen Flower Chip Tea (Table) Tea (Shelf) Bread Handover Handover (R2H) Sweep Sort Dynamic-Kitchen Long-horizon Total Internet Internet Internet Internet In-house In-house In-house In-house In-house In-house In-house In-house In-house In-house In-house In-house - Genie-G1 Franka + II + II + III + II + III WidowX + II Google Robot III III III III III III III III III III III III Genie-G1 Genie-G1 Genie-G1 Genie-G1 Genie-G1 Genie-G1 Genie-G1 Genie-G1 Franka Franka ARX LIFT II ARX LIFT II head + left/right wrist 3rd + wrist 3rd 3rd head + left/right wrist head + left/right wrist head + left/right wrist head + left/right wrist head + left/right wrist head + left/right wrist head + left/right wrist head + left/right wrist 3rd + wrist 3rd + wrist head + left/right wrist head + left/right wrist - - 30 20 5 3 30 30 30 30 30 30 30 30 15 15 30 30 - 187K 1.7K 53.2K 87.2K 152 199 100 197 202 214 171 210 59 99 48 129 66.4M 0.3M 1.9M 3.8M 78.1K 132.8K 54.7K 103.3K 112.0K 117.8K 124.4K 144.5K 43.8K 68.2K 57.6K 347.3K 330.9K 73.8M Table 7: Data Statistics. Overview of internet-scale and in-house datasets used across different training stages. Internet datasets (top) provide large-scale pretraining data across varied robots and viewpoints, while curated in-house datasets (bottom) offer high-quality demonstrations for finetuning. In total, the combined corpus contains 330.9K trajectories and 73.8M frames."
        },
        {
            "title": "B TRAINING DETAILS",
            "content": "Our model training is three-stage process, with specific hyperparameters detailed in Tab. 8. Stage focuses on learning general visual representations. It uses large batch size of 1280 and high learning rate of 3.0 104 over 512K training steps. In Stage II, we refine the model with larger batch size of 2880 and constant learning rate of 5.0 105 for 100K steps. This stage introduces action prediction, using loss weight of 0.1:1 to balance generative and action losses. For Stage III, the model is finetuned on specific downstream tasks. All tasks in this stage share smaller batch size of 128 and common settings for learning rate, and loss weight. However, the number of training steps (or epochs) and the Action Chunk Size are specifically adjusted for each task to account for differences in data volume and task difficulty, ensuring optimal performance across the board. REAL-WORLD TASK DETAILS We evaluate our approach across multiple robotic platforms with tasks categorized by their core manipulation requirements and complexity levels. Our task suite spans from basic pick-and-place operations to complex long-horizon sequences, testing fundamental capabilities including precision manipulation, dual-arm coordination, human-robot interaction, and dynamic adaptation. 24 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Hyperparameters Stage Stage II LIBERO Simpler Genie-1 Franka Dynamic Long-horizon Stage III Batch Size Learning Rate LR Scheduler Loss Weight (Gen:Act) Training Epochs Training Steps Und Resolution Gen Resolution # Num Predicted Scales Action Chunk Size Denoise Steps 1280 3.0 104 Cosine - - 512K 224 224 256 256 10 - - 2880 5.0 105 Constant 0.1:1 - 100K 224 224 256 256 4 30 - 128 5.0 105 Cosine 0.1:1 - 100K 224 224 256 256 4 4 10 128 5.0 105 Cosine 0.1:1 10 - 224 224 256 256 4 8 10 128 5.0 105 Cosine 0.1:1 40 - 224 224 256 256 4 50 10 128 5.0 105 Cosine 0.1:1 40 - 224 224 256 256 4 50 10 128 5.0 105 Cosine 0.1:1 40 - 224 224 256 256 4 50 10 128 5.0 105 Cosine 0.1:1 60 - 224 224 256 256 4 50 10 Table 8: Training recipe of F1. Due to significant differences in the number of demonstrations and task difficulty across downstream tasks, the settings for Stage III are not uniform. C.1 BASIC PICK-AND-PLACE MANIPULATION This category covers fundamental grasping and placement tasks involving everyday household objects. Representative instructions include: Put the pen from the table into the pen holder, Pick up bag of chips and place it into the basket, and Pick up bottle of black tea and place it into the shopping cart. The main challenges arise from the diverse physical properties of the objects, ranging from rigid items such as pens, to smooth packages like bags of chips, and deformable object such as plastic bottles, while also requiring consistent placement accuracy across target receptacles with varying constraints. Figure 11: Basic Pick-and-Place Manipulation. Examples of fundamental grasping and placement tasks across different object types and target containers. C.2 FINE-GRAINED PRECISION MANIPULATION This category evaluates the limits of robotic fine motor control through tasks demanding high precision and delicate handling. representative example, Pick up the flower and insert it into the vase, specifically assesses fine-grained grasping and placement under tightly constrained conditions. The challenge arises from the flowers thin stem, which requires precise gripping to prevent damage, combined with the vases narrow opening, which demands accurate placement. 25 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 12: Fine-Grained Precision Manipulation. The robot is instructed to pick up the flower and insert it into the vase, which requites the precise control for handling delicate objects with narrow target constraints. C.3 DUAL-ARM COORDINATION AND HUMAN-ROBOT INTERACTION Bimanual manipulation tasks evaluate coordinated control of both robotic arms, focusing on spatialtemporal synchronization and inter-arm object transfer capabilities. The primary task in this category involves the instruction Pick bag of bread with the left arm, then handover, finally put it into the basket, and Pick bottle of black tea and hand over to the person, which requires seamless coordination between arms throughout the manipulation sequence. Figure 13: Dual-Arm Coordination and Handover. Examples of bimanual coordination and human-robot interaction tasks demonstrating inter-arm object transfer and collaborative handover capabilities. C.4 DYNAMIC ENVIRONMENT ADAPTATION Tasks in this category are performed within continuously changing environments, testing real-time tracking capabilities, and adaptive control under uncertainty. The instruction Pickup the lettuce with the right hand and then the bread with the left hand exemplifies the need for sophisticated trajectory prediction and real-time interception of moving targets. Additional challenges arise when robots must respond to unexpected dynamic events during task execution, such as objects falling or environmental conditions changing mid-operation. C.5 LONG-HORIZON SEQUENTIAL MANIPULATION Extended task sequences in this category demand comprehensive multi-step planning, coordinated tool use, and sustained task coherence across complex operation chains. As illustrated in Fig. 15, the key challenges include long-horizon planning over 10-step sequences with effective memory management, sequential coordination of multiple tools, handling objects with diverse physical propertiesranging from rigid and deformable to liquid materialsand dynamic replanning when unexpected events arise during execution. 26 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 14: Dynamic Environment Adaptation. Example of real-time tracking and motion prediction capabilities in continuously changing environments. The robot successfully acquires specific food items from moving belt. Figure 15: Long-Horizon Sequential Manipulation. Example of extended multi-step task execution requiring comprehensive planning, tool usage, and task coherence maintenance. F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions"
        },
        {
            "title": "D DEPLOY PLATFORM AND LATENCY ANALYSIS",
            "content": "All experiments are conducted on workstation equipped with an Intel i9 CPU and an NVIDIA RTX 4090 GPU. All robots are connected to the host machine via wired Ethernet, thereby avoiding the additional transmission delays typically incurred in wireless communication. This setup ensures that the measured latency primarily reflects the computational overhead of the model itself. To provide detailed view of system efficiency, we report the latency of each processing stage when the model takes three synchronized camera views as input. As shown in Tab. 9, the foresight generation and action decoding modules contribute the majority of the runtime, while image preprocessing and encoding remain relatively lightweight. Overall, the total inference time is approximately 235ms, which is sufficient for real-time deployment in embodied robotic scenarios. Model Part Inference Time image process (e.g., resize) temporal downsampling image encoder foresight generation x10 action forward pass (flow) total inference 18ms 28ms 18ms 76ms 95ms 235ms Table 9: Latency of F1 on the deployment platform (Intel i9 CPU + RTX 4090 GPU). Robots are connected via wired Ethernet, so the reported numbers exclude wireless transmission delays."
        },
        {
            "title": "E PROMPT TEMPLATE",
            "content": "Fig. 16 shows the full template used for evaluating foresight image quality with Qwen2.5-VL-32BInstruct (Bai et al., 2025). The prompt explicitly specifies the input components (task instruction, four historical frames, the predicted next-step frame, and the ground-truth frame) and guides the evaluator to assign binary scores on three aspects: (i) scene consistency, (ii) object consistency, and (iii) task progress following. The model is instructed to output three numbers (0/1) together with brief explanation. 28 F1: Vision-Language-Action Model Bridging Understanding and Generation to Actions Figure 16: Prompt Template for Future Observation Quality Evaluation. We adopt MLLM as evaluator, i.e., Qwen2.5-VL-32B-Instruct, and design prompt template from three aspects to evaluate the quality of generated future observation: 1) Scene Consistency, 2) Object Consistency, and 3) Task Progress Following."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen)",
        "Shanghai AI Laboratory"
    ]
}