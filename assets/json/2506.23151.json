{
    "paper_title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation",
    "authors": [
        "Vladislav Bargatin",
        "Egor Chistov",
        "Alexander Yakovenko",
        "Dmitriy Vatolin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical flow method that identifies a favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resource-intensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at https://github.com/msu-video-group/memfof."
        },
        {
            "title": "Start",
            "content": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation Vladislav Bargatin1 Egor Chistov1 Alexander Yakovenko1,2 Dmitriy Vatolin1,2 1Lomonosov Moscow State University, Moscow, Russia 2MSU Institute for Artificial Intelligence, Moscow, Russia {vladislav.bargatin, egor.chistov, alexander.yakovenko, dmitriy}@graphics.cs.msu.ru 5 2 0 2 J 9 2 ] . [ 1 1 5 1 3 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in optical flow estimation have prioritized accuracy at the cost of growing GPU memory consumption, particularly for high-resolution (FullHD) inputs. We introduce MEMFOF, memory-efficient multi-frame optical flow method that identifies favorable trade-off between multi-frame estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely positions our method to be trained at native 1080p without the need for cropping or downsampling. We systematically revisit design choices from RAFT-like architectures, integrating reduced correlation volumes and high-resolution training protocols alongside multi-frame estimation, to achieve state-of-the-art performance across multiple benchmarks while substantially reducing memory overhead. Our method outperforms more resourceintensive alternatives in both accuracy and runtime efficiency, validating its robustness for flow estimation at high resolutions. At the time of submission, our method ranks first on the Spring benchmark with 1-pixel (1px) outlier rate of 3.289, leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the best Fl-all error on KITTI-2015 at 2.94%. The code is available at: https: //github.com/msu-video-group/memfof. 1. Introduction Optical flow (the dense per-pixel motion between frames) estimation is fundamental task in low-level vision with wide applications from video action recognition and object detection [22, 29, 38] to video restoration and synthesis [3, 11, 16, 37]. Traditional variational methods formulated flow as an optimization problem [9, 17], but often struggled with large motions and real-time performance. The deep learning era sparked leap in both accuracy and processing speed: FlowNet [5] pioneered this shift, with PWC-Net [28] Figure 1. Comparison with state-of-the-art optical flow methods. Left: Quality-memory trade-off on the Spring [18] benchmark. MEMFOF demonstrates superior memory efficiency and the lowest zero-shot error among all methods. Right: GPU memory consumption for 1080p resolution inputs. MEMFOF outperforms state-of-the-art methods such as RPKNet [20] and StreamFlow [30]. For more details please see Tab. 2. StreamFlow [30] is omitted from the left plot due to space constraints. introducing cost volume warping for efficiency. RAFT [31] later established state-of-the-art accuracy through iterative GRU-based refinement of 4D all-pairs correlation volume. However, RAFTs quadratic memory scaling with image size creates prohibitive costs for high-resolution inference (e.g., 8 GB at FullHD and 25+ GB at WQHD), forcing input downsampling that degrades motion boundary details. Recent advances address these limitations through two complementary strategies: (1) enhancing correlation efficiency and (2) exploiting multi-frame temporal cues. Memory-efficient architectures reduce correlation costs via sparse candidate matching (SCV) [14], 1D motion decomposition (Flow1D) [35], or hybrid volumes (HCV) [39]. Transformer-based methods like GMFlow [36] and FlowFormer++ [27] enable global feature matching with fewer iterations. Multi-frame approaches such as VideoFlow [26] and MemFlow [4] leverage temporal consistency to resolve 1 occlusions, while StreamFlow [30] optimizes spatiotemporal processing efficiency. Despite this progress, high-resolution benchmarks like Spring [18] remain challenge. Some methods either downsample inputs [32] or employ tiling strategies [33] to reduce memory consumption, trading it for less accuracy or longer inference. And methods operating at native resolutions tend to use large amounts of memory, prohibiting their use on consumer grade hardware, see Fig. 1 for details. In this work, we propose MEMFOF, the first multiframe optical flow method designed for memory efficiency at FullHD. MEMFOF can be trained and run on full 1080p frames without any downsampling or tiling, using only few GB of memory at inference all while achieving stateof-the-art accuracy. To achieve this, we extend SEA-RAFT, two-frame optical flow architecture to incorporate threeframe strategy. Crucially, we adjust the RAFT-style architecture to drastically cut memory usage (about 4 lower, down to just 2.09 GB) while enabling multi-frame input, allowing our model to run at 1080p on common GPUs. Which in turn allows for training at native 1080p using under 32GB of memory. For better handling of large motions found at high resolutions, we devise training regime that overcomes the mismatch between standard optical flow datasets (often limited in image size and motion magnitude) and the FullHD domain by upscaling existing datasets and training at higher resolutions, see Figure 4. An ablation study shows that this upsampling is critical to avoid underfitting on large-motion regions, leading to consistent performance gains on real high-resolution benchmarks. Notably, our method ranks first at zero-shot evaluation on the Spring benchmark, surpassing all other published work (both zero-shot and finetuned in Spring). To the best of our knowledge, we are the first to address the issues of memory consumption of multiframe methods at high-resolutions in principled manner. In summary, our key contributions are: Memory-Efficient Multi-Frame Design. We propose refined multi-frame RAFT-style architecture that processes FullHD inputs natively, reducing GPU memory needs by up to 3.9 compared to RAFT / SEA-RAFT, requiring only 2 GB of GPU memory at 1080p inference, well within the capacity of consumer-grade GPUs. High-Resolution Training Strategy. novel FullHDcentric data augmentation and multi-stage learning approach to accurately capture large motions, preventing the underfitting that commonly arises when transferring from low-resolution to high-resolution tasks. State-of-the-Art Results on Multiple Benchmarks. MEMFOF achieves top accuracy on multiple benchmarks with substantially lower memory overhead. It leads on Spring [18], KITTI-2015 [19], and Sintel [2]. 2. Related Works Optical flow estimation is fundamental problem in computer vision, with applications ranging from motion analysis to video compression. Over the years, various approaches have been proposed to address the challenges of accuracy and efficiency. In this section, we review the existing literature, categorizing it into three main areas: twoframe optical flow, multi-frame optical flow, and memoryefficient optical flow. Two-frame optical flow. Classical approaches [6, 9, 17] optimize an energy function combining similarity and smoothness terms. With the advent of deep learning, methods like FlowNet [5] revolutionized the field by leveraging convolutional neural networks to directly predict optical flow from image pairs. PWC-Net [28] then introduced pyramid, warping, and cost volume mechanism. More recently, RAFT [31] has introduced new paradigm, employing an iterative refinement process and an all-pair correlation volume. Building on RAFTs success, several variants have been proposed to improve its efficiency and accuracy. One strategy is to introduce global receptive fields via transformers or attention. GMFlow [36] treats optical flow as global feature matching problem, while FlowFormer [10] integrates transformer into the cost volume processing. Beyond transformers, GMA [13] introduces global motion attention to focus the iterative updates on important regions. On the other hand, SEARAFT [32] aims to enhance RAFT by three simple tricks: using mixture of Laplace loss, directly regressing initial flow, and pre-training on rigid-flow dataset. Unfortunately, all RAFT-like methods require substantial memory resources on high-resolution inputs. As result, they are often applied to downscaled frames or with tiling-based approach, compromising the quality of the estimated flow by losing fine details or global motion context, respectively. Multi-frame optical flow. While two-frame methods have advanced significantly, they inherently ignore the rich temporal information available in video streams. Early multiframe attempts simply extended two-frame methods with flow propagation, for example, by fusing the backward warped past flow with current flow through fusion module, as in PWC-Fusion [23], or by using warm start initialization where the previous frames flow is used to initialize the next estimation, as in RAFT. Recent research has moved beyond pairwise estimation by explicitly modeling sequences of frames. VideoFlow [26] introduces tri-frame optical flow (TROF) module to estimate forward and backward flows from center frame to its neighboring frames. Multiple TROF modules can then be connected via motion propagation module to extend to longer video sequences. Another approach, MemFlow [4], augments RAFT-like architecture with memory buffer that car2 Figure 2. Overview of our method and FullHD inference results. Left: Outline of MEMFOF: when operating on videos we cache and reuse results of the feature extraction stage and correlation volume calculation. For each new frame we extract features and run the context network on the frame triplet, which returns the initial flow estimates, context features and hidden (recurrent) state. The flows are recurrently updated for iterations and finally upsampled to get the final predictions. Right: Comparison of our method (MEMFOF) with StreamFlow [30] on FullHD images from the DAVIS dataset [21]. Our method correctly captures the tennis balls movement while requiring much less memory. ries forward motion features. StreamFlow [30] proposes streamlined pipeline that processes multiple frames in one forward pass, avoiding redundant calculations of feature maps and correlation volumes. Unfortunately, all three of these approaches do not address the inherent limitations of the cost volume frameworks large memory consumption on modern high-resolution videos. Memory-efficient optical flow. Memory efficiency has become critical concern in optical flow estimation since the introduction of RAFT. Methods like Flow1D [35] and MeFlow [34] have explored low-dimensional representations of the correlation volume. Similarly, Sparse Cost Volume (SCV) [14] restricts the correspondence search of RAFT to few top matches. On the other hand, Deep Inverse Patchmatch (DIP) [40] uses PatchMatch [1]-based approach to avoid building the all-pairs correlation volume. While these approaches achieve notable improvements in efficiency, they often sacrifice accuracy, falling short of the performance achieved by state-of-the-art methods in the RAFT family. This trade-off between memory efficiency and accuracy highlights the need for novel approaches that can bridge the gap between these competing objectives. Notably, there has been little work that effectively applies memory-efficient techniques to multi-frame optical In this work, we address this gap by flow estimation. proposing novel method that enables high-quality optical flow estimation without excessive memory demands. 3. Method Our method introduces novel approach to optical flow estimation that combines memory efficiency with multi-frame processing without sacrificing accuracy. The method consists of three key components: (1) extending SEA-RAFT to three frames, (2) resolution reduction of the correlation volume, and (3) performance optimization techniques. Below, we describe each component in detail. 3.1. Extending SEA-RAFT to three frames To leverage temporal information, we extend the two-frame SEA-RAFT architecture to three frames. Following VideoFlow [26], we predict bidirectional flows, one between the current frame and the previous frame, and another between the current frame and the next frame. This involves calculating two correlation volumes instead of one. The update block is also modified to refine both flows at the same time, enabling the network to capture long-range dependencies. Similar to SEA-RAFT, to predict the initial flow, we pass all three frames into the context network. We will now formalize our method. Approach. Given three consecutive frames It1, It, It+1, we iteratively estimate sequence of bidirectional flows 0, 1, . . . , (RHW 2, RHW 2); where indicates the number of iterative refinements; includes flow to the previous frame tt1 and flow to the next frame 3 Figure 3. Qualitative comparison of MemFlow [4], StreamFlow [30], SEA-RAFT [32], and our method on Spring benchmark [18] crops, colorbar represents endpoint error. Our approach surpasses prior methods and demonstrates that: (1) multi-frame processing enhances temporal coherence, and (2) native Full HD resolution preserves local and global motion details. Crops are sourced from official leaderboard submissions. tt+1; and are the reduced height and width of the input images. We begin by extracting the input frame feature maps Ft, Ft1, Ft+1 RHW Df . To get the initial prediction 0, the hidden state h0 RHW Dc, and the context features RHW Dc, we pass all three frames into the context network: g, h0 = ContextNetwork(It1, It, It+1), 0 = FlowHead(h0). (1) (2) rectional motion feature m: corr = CorrEncoder(ck flow = FlowEncoder(f m = MotionEncoder(F k tt1, ck tt1, corr, tt+1), tt+1), flow). (7) (8) (9) The hidden state hk is updated iteratively using the motion feature, context features g, and previous hidden state: hk+1 = Updater(F m, g, hk), (10) The dual correlation volumes Ct,t1 and Ct,t+1 are computed as: and the residual flows are decoded from the updated hidden state: Ct,t1(u, v) = Ft(u), Ft1(v), Ct,t+1(u, v) = Ft(u), Ft+1(v), (3) (4) = FlowHead(hk+1). The flow predictions are refined as: where , denotes the dot product. Iterative refinement. The correlation values ck tt1 and ck tt+1 are retrieved from the dual correlation volumes based on the current flow predictions: ck tt1 = LookUp(Ct,t1, tt+1 = LookUp(Ct,t+1, ck tt1), tt+1). (5) (6) These values are then fused and encoded into correlation and flow features, which are in turn transformed into bidi- (11) (12) (13) k+1 tt1 = k+1 tt+1 = tt1 + tt+1 + tt1, tt+1. The final flow predictions are convexly upsampled to the input resolution as in RAFT. 3.2. Resolution reduction of the correlation volume major bottleneck in modern optical flow methods, such as RAFT and SEA-RAFT, is the memory consumption of the correlation volume, which scales quadratically with the input resolution as O((HW )2). To address this, we propose 4 Table 1. Details of our training procedure. Dataset abbreviations: T: Things, S: Sintel, K: KITTI-2015, H: HD1K. Following SEA-RAFT, the dataset distribution for the TSKH stage is S(.32), T(.31), K(.12), H(.24). indicates the number of iterative refinements used in our method during training. Memory usage is stated per GPU. Stage Weights Datasets Scale Crop size Learning rate Batch size Steps Memory (GB) TartanAir Things TSKH Sintel-ft KITTI-ft Spring-ft - TartanAir Things TSKH TSKH TSKH TartanAir T+S+K+H Spring 2x 2x 2x 2x 2x 1x [480, 960] [864, 1920] [864, 1920] [872, 1920] [750, 1920] [1080, 1920] 4 4 4 8 8 8 1.4e-4 7e-5 7e-5 3e-5 3e-5 4.8e-5 64 32 32 32 32 225k 120k 225k 12.5k 2.5k 60k 12.0 17.1 17.1 22.8 19.6 28.5 reducing the resolutions of the correlation volumes and the working flow predictions to 1/16 of the input frames, compared to the standard 1/8 resolution. Our three-frame setup benefits from this reduction, decreasing the memory footprint for two correlation volumes from 10.4 GB to just 0.65 GB. While other components (e.g., feature maps and intermediate activations) also contribute to memory usage, preventing sixteen-fold reduction in overall consumption, the total memory usage remains significantly lower than that of the original two-frame SEA-RAFT (8.19 GB vs. 2.09 GB for FullHD). To account for the correlation volume size reduction, we adapt the ResNet34 [8] backbone used in SEA-RAFT. Specifically, to get 1/16 resolution features, we apply strided convolution on the original 1/8 resolution feature maps. Additionally, to account for more information being stored in each pixel, we increase the feature map dimension Df from 256 to 1024 and the update block dimension Dc from 128 to 512. This reduction in memory usage enables training our method in native FullHD, alleviating the need for cropping or downsampling of inputs. Memory consumption during different training stages can be seen in Table 1. 3.3. Performance optimization techniques To further enhance motion coherence, we reintroduce the GMA module [13]. To better adapt to different resolutions, similar to MemFlow [4], we modify the scale factor in attention from 1/ Dc to log3 (HW )/ Dc. We additionally apply three inference-time speed and memory optimizations. Firstly, similar to StreamFlow [30], we note that when optical flow needs to be predicted for video sequence, already calculated feature maps can be reused for future predictions. Secondly, following Flow1D [35], we use convex upsampling only on the last predictions. And finally, we reuse the previously computed correlation volume Ct,t+1 for overlapping frame pairs when moving to the next frame in video sequence, instead of recomputing it from scratch. 5 4. Experiments We evaluate our method on three popular optical flow benchmarks: Spring [18] (modern high-resolution sequences), Sintel [2] (synthetic scenes with complex motion) and KITTI [19] (autonomous driving). 4.1. Training Details We follow the SEA-RAFT training protocol with some adjustments. We train our method on 32 A100 GPUs with automatic mixed precision. Our main changes with respect to SEA-RAFT are skipping FlyingChairs [5] due to its two-frame limitation, 2 upsampled frames and flows on datasets other than Spring, and in turn larger crop sizes. Training details are provided in Table 1. In cases when the crop size is bigger than the frame size or is not multiple of 16, we pad the images with black pixels. Training our main model on all stages takes from 3 to 4 days. Evaluation metrics. We adopt widely used metrics from established benchmarks [7, 18, 24] in this study: endpoint error (EPE), 1-pixel outlier rate (1px), Fl-score, and WAUC error. The 1px outlier rate measures the percentage of pixels where the flow error exceeds 1 pixel. The endpoint error (EPE) is defined as the average Euclidean distance between predicted and ground truth flow vectors. The Fl-score measures the percentage of pixels where the disparity or flow exceeds 3 pixels and 5% of its true value. Finally, the WAUC metric evaluates the inlier rates for range of thresholds, from 0 to 5 px, and integrates these rates, giving higher weight to lower-threshold rates. Please refer to the supplementary for formal definition of WAUC. Mixture-of-Laplace Loss. Following SEA-RAFT [32], we use mixture-of-Laplace (MoL) loss instead of an L1 loss. The MoL loss for optical flow frame predictions with iterative refinements is defined as: = 1 (cid:88) (cid:88) t=1 k= γN kLt,k oL, (14) where Lt,k oL is the MoL loss for the t-th optical flow frame prediction after refinement iterations and γ is set to 0.85 Table 2. Benchmark comparison of optical flow methods. Results are sourced from official leaderboard of the Spring benchmark, where minus (-) indicates the method has no published results. Speed (runtime) and peak GPU memory consumption were measured on Nvidia RTX 3090 GPU (24 GB) without automatic mixed precision or memory efficient correlation volumes. Lower values are better () except for WAUC (). The best results are indicated in bold, second-best are underlined. Method configurations are taken from submissions to the Spring benchmark if present, and from submissions to the Sintel benchmark otherwise. Method #Frames Inference Cost (1080p) Spring (test) Memory, GB Runtime, ms 1px EPE Fl WAUC Flow1D [35] MeFlow [34] PWC-Net [28] FlowNet2 [12] RAFT [31] GMA [13] FlowFormer [10] RPKNet [20] VideoFlow-BOF [26] VideoFlow-MOF [26] MemFlow [4] StreamFlow [30] MEMFOF (Ours) CrocoFlow [33] SEA-RAFT (S) [32] SEA-RAFT (M) [32] MemFlow [4] StreamFlow [30] MEMFOF (Ours) T - F E - F 2 2 2 2 2 2 2 2 3 5 3 4 3 2 2 2 3 4 3 1.34 1.32 1.41 4.16 7.97 13.26 OOM 8. 17.74 OOM 8.08 18.97 2.09 2.01 8.15 8.19 8.08 18.97 2.09 405 1028 76 167 557 1185 - 295 1648 - 885 1403 472 6524 205 885 1403 472 - - 82.265 6.710 6.790 7.074 6.510 4.809 - - 5.759 5.215 3.600 4.565 3.904 3.686 4.482 4.152 3.289 - - 2.288 1.040 1.476 0.914 0.723 0. - - 0.627 0.606 0.432 0.498 0.377 0.363 0.471 0.467 0.355 - - 4.889 2.823 3.198 3.079 2.384 1.756 - - 2.114 1.856 1.353 1.508 1.389 1. 1.416 1.424 1.238 - - 45.670 90.907 90.920 90.722 91.679 92.638 - - 92.253 93.253 94.481 93.660 94.182 94.534 93.855 94.404 95.186 to add higher weights on later predictions following RAFT. Please refer to the supplementary for more details. 4.2. Results We will now state our results on established public benchmarks. Results on Spring. Our approach fine-tunes on and processes native 1080p sequences, which allows it to preserve fine motion details as shown in Figure 3. This enables stateof-the-art accuracy we outperform SEA-RAFT (M) by 10% in 1px outlier rate and 2% in EPE  (Table 2)  . Additionally, our upsampled pre-train strategy also places us first among all non-fine-tuned submissions, even outperforming the fine-tuned SEA-RAFT (M) by 2.3% on the 1px metric. Crucially, our memory efficiency allows three-frame temporal processing at native 1080p even with low memory budget, and our method is faster than other multi-frame competitors. Results on Sintel and KITTI. Due to pre-training on 2x upsampled frames, for submissions to the Sintel and KITTI benchmarks, we bilinearly upscale all input images by factor of two and bilinearly downscale all resulting flow maps by factor of two. For Sintel submissions we use 16 update iterations. Our method leads on Sintel clean split, surpassing the five-frame version of VideoFlow and outperforms SEA-RAFT (L) by 27% on the final pass  (Table 3)  . On the KITTI benchmark, we achieve state-of-the-art performance among all non-scene flow methods. Please refer to the supplementary material for visual and zero-shot comparisons with other methods. 4.3. Ablation Study The ablation study is conducted on the Spring training set (only on the forward left 4K flow), as we mainly focus on FullHD performance. If not otherwise stated, we use the same training procedure and hyperparameters as in the experiments section models after the TSKH stage but before Spring fine-tuning, and perform 8 iterative refinements. High-Resolution Training Analysis. Commonly used optical flow datasets come in relatively small resolutions, and methods trained on such data often generalize poorly to motion magnitudes seen in high-resolution inputs. This limits the practical use of optical flow methods, causing input downsampling to resolution that better matches the training stage [15]. See Fig. 4 for motion vector histogram which illustrates this discrepancy between common datasets and Table 3. Evaluation of our method on the Sintel and KITTI-15 public benchmarks. The Sintel benchmark uses EPE as its metric for both splits, while KITTI-15 uses the Fl-all outliers metric. Method PWC-Net [28] FlowNet2 [12] Flow1D [35] MeFlow [34] RAFT [31] GMA [13] SEA-RAFT (M) [32] SEA-RAFT (L) [32] FlowFormer [10] RPKNet [20] CrocoFlow [33] DDVM [25] StreamFlow [30] MemFlow [4] MemFlow-T [4] VideoFlow-BOF [26] VideoFlow-MOF [26] MEMFOF (Ours) Sintel KITTIClean Final Fl-all 3.86 4.16 2.238 2.054 1.609 1.388 1.442 1.309 1.159 1.315 1.092 1.754 1.041 1.046 1.081 1.005 0.991 0.963 5.04 5.74 3.806 3.090 2.855 2.470 2.865 2.601 2.088 2.657 2.436 2. 1.874 1.914 1.840 1.713 1.649 1.907 9.60 10.41 6.27 4.95 5.10 5.15 4.64 4.30 4.68 4.64 3.64 3.26 4.24 4.10 3.88 4.44 3.65 2.94 Spring FullHD motion range. Plese refer to the supplementary for details on histogram creation. We evaluate three strategies to bridge resolution gaps during training, see Table 4 for detailed metrics: Native Resolution: Training on original data yields the worst performance (EPE: 0.430), as low-res motion magnitudes mismatch FullHD. We additionally test predicting the flow at half the resolution (like in SEA-RAFT [32]), which helps improve EPE at large displacements (s40+) but hinders the methods ability to predicts fine motions as shown by all the other metrics. Upsampled (2) with Crops: Training on upsampled data cropped to original resolution helps improve the quality but performs worse than full-frame training, likely due to cropped context limiting very large motion learning. Upsampled (2) Full Frames: This achieves the best FullHD results (EPE: 0.341), as full-frame upsampled training optimally aligns motion distributions with highres inference. Three-Frame Flow Estimation Strategy. We compare bidirectional (current-to-previous & current-to-next) and unidirectional (previous-to-current & current-to-next) flow estimation. As shown in Table 4, bidirectional training improves EPE by 14.75% on Spring train data. We posit this stems from simplified motion boundary learning: bidi- (a) TartanAir (b) FlyingThings (c) KITTI- (d) HD1K (e) Sintel (f) Spring (g) Combined at 1 resolution (h) Combined at 2 resolution Figure 4. We analyze motion patterns in optical flow datasets using 2D histograms. Each histogram uses the same bins, covering all possible motions at FullHD resolution. Color intensity corresponds to the number of motion vectors in each bin. Borders show the maximum motion range in each dataset. (a e) Training datasets histograms at their native resolutions. (f) Motion histogram of the Spring training set, note the large motions, not covered by any of the datasets. (g h) Combined motion histograms of training datasets without and with 2 upsampling. rectional flows share consistent boundaries of the central frame, whereas unidirectional flows face distinct boundaries for each direction, which makes the task of predicting initial flow much harder for the context network. Correlation volume resolution, feature dimension, and number of frames. We carefully examine the trade-off between the resolution of the correlation volume and the number of frames. We train 2-, 3and 5frame models with either 1/16 or 1/24 correlation volume resolution. We also ablate the use of GMA module alongside the hidden state / feature dimensions. The results are reported in Table 5. We see consistent gains when increasing the feature dimension and favorable trade-off between 2and 3-frame models. We also note the performance degradation when moving from 3 to 5 frames which we attribute to the insufficient size of the context network and recursive module, but leave further analysis to future work. Inference-Time Optimizations. Due to the slidingwindow processing of video sequences and iterative nature of RAFT-derived methods we note few calculation redundancies that can be used to further improve runtime. Feature Network Reuse: When processing sequential data, parts of the computations may be reused. Notably in three frame scenario we may reuse 2 out of 3 feature network results. We implement this by caching extracted features and only running the feature network on the new frame. This optimization cannot be applied to the context network as it takes all three frames as input. Late Convex Upsampling: During training all flow predictions are upsampled for loss computation. However, during inference, we need to apply convex upsampling only on the final iteration, eliminating redundant computations. 7 Table 4. Ablation. We validate our training design choices on the Spring training set after the TSKH stage. We compare training at original scales and inference at half scale (baseline) to inference at full resolution and training on either crops of or on full upsampled images. We also study the effect of uni-/bi-directional flow prediction. Our final method is highlighted in gray . For more details see Sec. 4.3. Configuration EPE 1px Other Flow Train Scale Dir. Crop Inf. Scale BiBiBiBiUni1x 1x 2x 2x 2x 1/2 1 1 1 avg s0-10 s10-40 s40+ avg s0-10 s10-40 s40+ WAUC Fl 0.402 0.430 0.378 0.341 0.177 0.165 0.166 0. 1.047 0.857 0.811 0.818 6.843 8.858 6.960 6.592 4.300 3.232 3.195 3.061 2.491 1.755 1.815 1.739 16.877 11.628 11.231 11.156 35.401 33.903 31.933 29. 93.840 94.230 94.192 95.604 0.400 0.137 0.869 8.732 3. 1.888 11.633 31.563 95.157 1.260 0.984 0.873 0.823 0. Table 5. Ablation. Correlation volume resolution and number of frames. In all models, we set the feature dimension Df equal to 2Dc. Please refer to the supplementary material for additional metrics. Corr. scale #Frames Dc GMA 1px Mem 1/24 1/16 1/16 1/16 1/16 1/24 1/16 1/16 1/16 1/16 1/ 2 2 2 2 2 3 3 3 3 3 5 128 128 128 256 512 512 128 256 512 512 4.235 3.644 3.547 3.420 3.375 3.480 3.560 3.144 3.061 3.151 3.809 0.78 1.11 1.29 1.12 1.30 1.03 1.78 1.86 2.09 1. 1.84 Table 6. Ablation. Inference time optimizations. Method Time, ms 3 fr (1/16) 5 fr (1/24) Baseline Only last Convex upsample + Feature network reuse + Fast correlation volume + Correlation volume reuse 611 579 483 478 472 597 533 341 334 329 Fast correlation volume: The official SEA-RAFT implementation naively computes multi-scale correlation volumes between the feature map of the first image and pooled feature maps of the second image. Instead, we compute the correlation volume once and then pool it multiple times. Correlation volume reuse: Similar to reusing feature maps, correlation volumes can also be reused. By rearranging axes in Ct,t+1 and then pooling the result multiple times, we can get Ct+1,t without performing any matrix multiplications. These optimizations reduce inference time by over 22% when compared to naive implementations of two variants of our method  (Table 6)  . 5. Conclusion In this work, we introduced MEMFOF, memory-efficient multi-frame optical flow method that achieves state-ofthe-art performance while maintaining significantly reduced GPU memory footprint. By systematically revisiting RAFT-like architectures, we identified an optimal trade-off between multi-frame accuracy and memory efficiency, enabling training at native 1080p resolution without the need for cropping or downsampling. Our approach integrates reduced correlation volumes, multi-frame estimation, and high-resolution training strategies to deliver competitive accuracy across multiple benchmarks while operating with lower computational requirements. These findings position MEMFOF as practical solution for large-scale, highresolution optical flow estimation, bridging the gap between accuracy and efficiency. Future work may further explore extending our approach to even higher resolutions and realtime applications. 6. Acknowledgments This work was supported by the The Ministry of Economic Development of the Russian Federation in accordance with the subsidy agreement (agreement identifer 000000C313925P4H0002; grant No 139-15-2025-012). The research was carried out using the MSU-270 supercomputer of Lomonosov Moscow State University. We want to additionally thank Sergey Lavrushkin, Andrey Moskalenko, Ekaterina Shumitskaya and Vladislav Pyatov for proofreading and providing valuable feedback on the manuscript."
        },
        {
            "title": "References",
            "content": "[1] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan Goldman. Patchmatch: randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24, 2009. 3 [2] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for opIn ECCV, pages 611625. Springer, tical flow evaluation. 2012. 2, 5 [3] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In CVPR, pages 49474956, 2021. 1 [4] Qiaole Dong and Yanwei Fu. Memflow: Optical flow estimation and prediction with memory. In CVPR, pages 19068 19078, 2024. 1, 2, 4, 5, 6, 7 [5] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In ICCV, pages 27582766, 2015. 1, 2, 5 [6] Gunnar Farneback. Two-frame motion estimation based on polynomial expansion. In Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29 July 2, 2003 Proceedings 13, pages 363370. Springer, 2003. [7] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 5 [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 5 [9] Berthold KP Horn and Brian Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. 1, 2 [10] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang, Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer: transformer architecture for optical flow. In ECCV, pages 668685. Springer, 2022. 2, 6, 7 [11] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In ECCV, pages 624642. Springer, 2022. [12] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, pages 24622470, 2017. 6, 7 [13] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and Richard Hartley. Learning to estimate hidden motions with In ICCV, pages 97729781, global motion aggregation. 2021. 2, 5, 6, 7 [14] Shihao Jiang, Yao Lu, Hongdong Li, and Richard Hartley. Learning optical flow from few matches. In CVPR, pages 1659216600, 2021. 1, 3 [15] Wei-Sheng Lai, Yichang Shih, Lun-Cheng Chu, Xiaotong Wu, Sung-Fang Tsai, Michael Krainin, Deqing Sun, and Chia-Kai Liang. Face deblurring using dual camera fusion on mobile phones. ACM Transactions on Graphics (TOG), 41(4):116, 2022. 6 [16] Xiaozhang Liu, Hui Liu, and Yuxiu Lin. Video frame interpolation via optical flow estimation with image inpainting. International Journal of Intelligent Systems, 35(12):2087 2102, 2020. [17] Bruce Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In IJCAI81: 7th international joint conference on Artificial intelligence, pages 674679, 1981. 1, 2 [18] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. In CVPR, pages 49814991, 2023. 1, 2, 4, 5 [19] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In CVPR, pages 30613070, 2015. 2, 5 [20] Henrique Morimitsu, Xiaobin Zhu, Xiangyang Ji, and XuCheng Yin. Recurrent partial kernel network for efficient optical flow estimation. In AAAI, pages 42784286, 2024. 1, 6, 7 [21] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, pages 724732, 2016. 3 [22] AJ Piergiovanni and Michael Ryoo. Representation flow for action recognition. In CVPR, pages 99459953, 2019. 1 [23] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik Sudderth, and Jan Kautz. fusion approach for multiframe optical flow estimation. In IEEE Winter Conference on Applications of Computer Vision, pages 20772086. IEEE, 2019. 2 [24] Stephan Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, pages 22132222, 2017. 5, [25] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. NeurIPS, 36:39443 39469, 2023. 7 [26] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. In ICCV, pages 1246912480, 2023. 1, 2, 3, 6, 7 [27] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Flowformer++: Masked cost volume autoenIn CVPR, coding for pretraining optical flow estimation. pages 15991610, 2023. 1 [28] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In CVPR, pages 89348943, 2018. 1, 2, 6, 7 [29] Shuyang Sun, Zhanghui Kuang, Lu Sheng, Wanli Ouyang, and Wei Zhang. Optical flow guided feature: fast and robust motion representation for video action recognition. In CVPR, pages 13901399, 2018. 1 [30] Shangkun Sun, Jiaming Liu, Huaxia Li, Guoqing Liu, Thomas Li, and Wei Gao. Streamflow: streamlined multiframe optical flow estimation for video sequences. NeurIPS, 37:92059228, 2025. 1, 2, 3, 4, 5, 6, 7 [31] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 1, 2, 6, 7 [32] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In ECCV, pages 36 54. Springer, 2024. 2, 4, 5, 6, 7 [33] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. Croco v2: Improved cross-view completion pretraining for stereo matching and optical flow. In ICCV, pages 1796917980, 2023. 2, 6, 7 [34] Gangwei Xu, Shujun Chen, Hao Jia, Miaojie Feng, and Xin Yang. Memory-efficient optical flow via radius-distribution orthogonal cost volume. arXiv preprint arXiv:2312.03790, 2023. 3, 6, 7 [35] Haofei Xu, Jiaolong Yang, Jianfei Cai, Juyong Zhang, and Xin Tong. High-resolution optical flow from 1d attention and correlation. In ICCV, pages 1049810507, 2021. 1, 3, 5, 6, [36] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In CVPR, pages 81218130, 2022. 1, 2 [37] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and MingHsuan Yang. Quadratic video interpolation. NeurIPS, 32, 2019. 1 [38] Yuxuan Zhao, Ka Lok Man, Jeremy Smith, Kamran Siddique, and Sheng-Uei Guan. Improved two-stream model for human action recognition. EURASIP Journal on Image and Video Processing, 2020:19, 2020. 1 [39] Yang Zhao, Gangwei Xu, and Gang Wu. Hybrid cost volume for memory-efficient optical flow. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 8740 8749, 2024. 1 [40] Zihua Zheng, Ni Nie, Zhi Ling, Pengfei Xiong, Jiangyu Liu, Hao Wang, and Jiankun Li. Dip: Deep inverse patchmatch for high-resolution optical flow. In CVPR, pages 89258934, 2022. 3 MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Definitions Here we will provide more detailed definitions used in the main text. 7.1. WAUC In optical flow, weighted area under curve (WAUC), originally from VIPER [24], is formally defined as the integral 2 5 (cid:90) 0 (x) 5 5 dx, (15) where (x) is equal to the percentage of pixels where the flow error does not exceed pixels. The metric ranges from 0 at worst to 100 at best. 7.2. Mixture-of-Laplace Loss For single flow vector coordinate, the Mixture-of-Laplace (MoL) in SEA-RAFT is defined as: Table 7. Performance of our main model depending on the number of iterative refinements (N). Metrics are calculated on the Spring train dataset after the TSKH stage. Speed (runtime) was measured on an Nvidia RTX 3090 GPU (24 GB). 0 1 2 4 6 8 10 12 1px EPE WAUC Fl Speed, ms 6.170 3.752 3.300 3.133 3.081 3.061 3.050 3.045 0.893 0.397 0.350 0.339 0.340 0.341 0.342 0.342 90.898 94.731 95.322 95.565 95.603 95.604 95.601 95.598 2.625 1.212 0.979 0.863 0.835 0.823 0.820 0. 71 172 215 299 385 472 557 642 Table 8. FullHD, method configurations taken from leaderboard sumbissions. Speed (runtime) was measured on an Nvidia RTX 3090 GPU (24 GB). MixLap(µgt; α, β, µ) = log(cid:2) α 2 1 α 2eβ + eµgtµ+ µgtµ eβ (cid:3), (16) where µgt is the target flow coordinate, µ is the predicted flow coordinate, α is the predicted mixing coefficient, and β is the predicted scale parameter. For single optical flow frame prediction, the MoL loss is defined as: LM oL = 1 2HW (cid:88) (cid:88) MixLap(cid:0)µgt(u, v)d; u,v d{x,y} α(u, v), β(u, v), µ(u, v)d (cid:1). (17) 7.3. 2D Motion histogram In order to visually demonstrate the discrepancy in motion magnitudes between common training datasets and Spring, we construct 2D histograms of motion vectors. Final results can be seen in Figure 4. The histograms are constructed in the following way: (cid:88) (cid:88) (cid:88) H(u, v) = [u fn(h, w, 0) + 1] n=1 h=1 w=1 [v fn(h, w, 1) + 1], where fn RHW 2 is the nth flow field from dataset, (u, v) is the motion vector (u [H , ] and Method Standard corr. Alt. corr. GB ms RAFT VideoFlow-BOF MEMFOF 7.97 17.74 2.09 557 1648 472 GB 1.32 7.41 1.52 ms 1302 3275 1235 [W , ]) and [] is the Iverson bracket. We set = 1080, = 1920, therefore our final histograms all have the same 2160 3840 resolution, for illustration purposes, we take the logarithm of bin counts. Maximum motion boundaries are derived as twice the size of images in the dataset, since the largest motion possible is to move diagonally from one corner of an image to the other one. 8. Additional ablations In this section, we provide ablations or ablation data not included in the main text. 8.1. Number of iterative refinements We study our methods behavior depending on the number of iterative refinements. The results are provided in Table 7. For balance between speed and accuracy, we choose to perform 8 iterative refinements. 1 Table 9. Full correlation volume and number of frames ablation table. Corr. scale #Frames Dc GMA 1/24 1/16 1/16 1/16 1/16 1/24 1/16 1/16 1/16 1/ 1/24 2 2 2 2 2 3 3 3 3 3 5 128 128 128 256 512 512 128 256 512 512 1px avg s0-10 s10s40+ 4.235 3.644 3.547 3.420 3.375 3.480 3.560 3.144 3.061 3.151 2.556 2.232 2.132 2.072 2.047 1.940 2.154 1.789 1.739 1.833 15.213 12.171 12.101 11.440 11. 13.539 12.176 11.365 11.156 10.988 35.309 32.141 32.025 30.941 30.614 32.104 31.543 30.390 29.423 30.165 EPE WAUC Fl Memory, GB 0.438 0.396 0.408 0.372 0. 0.362 0.380 0.346 0.341 0.332 93.166 94.574 94.617 94.761 95.130 94.858 94.859 95.493 95.604 95.623 1.150 1.167 1.035 1.018 0.888 0.970 1.094 0.886 0.823 0.896 1. 0.78 1.11 1.29 1.12 1.30 1.03 1.78 1.86 2.09 1.82 1.84 3.809 2.164 14. 34.620 0.408 94.546 Table 10. Generalization performance of optical flow estimation on Sintel and KITTI-15 after the Things stage. By default, all methods are trained on (FlyingChairs +) FlyingThings3D, additional datasets are listed in the Extra data column. Extra data Method Sintel (train) KITTI-15 (train) Clean Final Fl-epe Fl-all PWC-Net Flow1D MeFlow RAFT SEA-RAFT (S) SEA-RAFT (M) SEA-RAFT (L) TartanAir MemFlow MemFlow-T VideoFlow-BOF VideoFlow-MOF StreamFlow MEMFOF (ours) TartanAir MEMFOF (ours) 2.55 1.98 1.49 1.43 1.27 1.21 1.19 0.93 0.85 1.03 1.18 0.87 1.10 1.20 3.93 3.27 2.75 2.71 3.74 4.04 4.11 2.08 2.06 2.19 2.56 2.11 2.70 3. 10.4 6.69 5.31 5.04 4.43 4.29 3.62 3.88 3.38 3.96 3.89 3.85 3.31 2.93 33.7 22.95 16.65 17.40 15.1 14.2 12.9 13.7 12.8 15.3 14.2 12.6 10.08 9.93 8.2. Alternative correlation implementation 9. Additional results We additionally provide memory consumption and speed measurements for RAFT, VideoFlow and our method in Tab. 8 when using alternative correlation volume implementation that trades compute time for memory efficiency. 8.3. Corr. volume resolution and number of frames We provide the full version of Table 5 with additional metrics as Table 9. 2 In this section, we provide some other results that are not included in the main text. 9.1. Additional zero-shot evaluation Following previous works, we evaluate the zero-shot performance of our method after the Things training stage on Sintel (train) and KITTI (train). The results are provided in Table 10. Our method has the best zero-shot evaluation on KITTI and outperforms SEA-RAFT (L) on Sintel when trained on the same datasets. Figure 5. Qualitative comparison of MemFlow-T, SEA-RAFT (L), and our method on the Sintel benchmark. Sourced from official leaderboard submissions. Figure 6. Qualitative comparison of MemFlow-T, SEA-RAFT (L), and our method on the KITTI-2015 benchmark. Sourced from official leaderboard submissions. 9.2. Qualitative comparison on Sintel and KITTI We provide qualitative comparisons of our method on the Sintel and KITTI public benchmarks. As Figure 5 and Figure 6 show, our method has higher motion detail and coherence than our baseline or competitor."
        }
    ],
    "affiliations": [
        "Lomonosov Moscow State University, Moscow, Russia",
        "MSU Institute for Artificial Intelligence, Moscow, Russia"
    ]
}