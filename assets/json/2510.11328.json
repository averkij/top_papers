{
    "paper_title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
    "authors": [
        "Chenxi Wang",
        "Yixuan Zhang",
        "Ruiji Yu",
        "Yufei Zheng",
        "Lang Gao",
        "Zirui Song",
        "Zixiang Xu",
        "Gus Xia",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Xiuying Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 8 2 3 1 1 . 0 1 5 2 : r Do LLMs Feel? Emotion Circuits Discovery and Control Chenxi Wang Yixuan Zhang Ruiji Yu Yufei Zheng Lang Gao Zirui Song Zixiang Xu Gus Xia Huishuai Zhang Dongyan Zhao Xiuying Chen(cid:66) Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Peking University {chenxi.wang, xiuying.chen}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "As the demand for emotional intelligence in large language models (LLMs) grows, key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct controlled dataset, SEV (ScenarioEvent with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayers causal influence on the models final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing promptingand steeringbased methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence."
        },
        {
            "title": "Introduction",
            "content": "As large language models (LLMs) demonstrate remarkable capabilities in reasoning and problemsolving, there is growing interest in developing models that also exhibit emotional intelligence. 1Open-source framework available at https://github. com/Aurora-cx/EmotionCircuits-LLM. (cid:66): Corresponding Author. Across social media platforms and online communities, users increasingly describe LLMs such as GPT-4o as sources of emotional support or companionship, attributing to them empathy and even personality (Phang et al., 2025; Dong et al., 2025; VarastehNezhad et al., 2025; Naito, 2025). These behaviors underscore both the promise and the mystery of emotional expression in LLMs, revealing that the ability to generate emotional text emerges from mechanisms that are still poorly understood. Prior studies have shown that LLMs encode emotion-related features in their activations (Li et al., 2024; Tigges et al., 2024; Lee et al., 2025), yet these findings stop short of revealing the mechanisms that give rise to emotional expression. Existing methods for emotion control, such as steering vectors and prompt engineering (Konen et al., 2024; Turner et al., 2024; Shen et al., 2025), typically manipulate the residual stream or inject explicit stylistic cues. Although effective in practice, they address only the surface of emotion control and leave the underlying mechanisms unexplained. Without mechanistic understanding, such interventions remain black-box and unreliable. This overarching question motivates our study and can be decomposed into three subproblems: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? To answer these questions, we take an interpretabilitydriven approach to uncover the internal mechanisms of emotion in LLMs. We first construct controlled dataset, SEV (ScenarioEvent with Valence), which provides shared narrative contexts designed to elicit six basic emotions (anger, sadness, happiness, fear, surprise, and disgust) (Ekman, 1992), enabling comparable internal states across emotions under matched semantic conditions. Building on this, we design framework comprising three analytical stages: (1) Emotion diFigure 1: Overview of emotion circuit modulation. Compared with the original forward pass (top), our circuit-based modulation (bottom) drives hidden states to diverge into distinct emotion clusters across layers and produces coherent emotional responses. All examples shown are directly generated without any manual curation. rection extraction, isolating context-agnostic emotion representations from controlled generations to reveal stable emotion directions that remain consistent across contexts. (2) Local component identification, locating neurons and attention heads that implement emotional computation with respect to each sublayers emotion direction through analytical decomposition and causal intervention, and validating their functional roles through ablation and enhancement experiments. (3) Global circuit integration, unifying local components across layers by quantifying each sublayers causal influence on the final emotion representation relative to reference basis, revealing coherent emotion circuits that enable controllable expression. As shown in Fig. 1, circuit-based modulation during generation drives hidden states to diverge into distinct emotion clusters across layers, ultimately yielding coherent and natural emotional expressions in output text. Notably, the induced emotions emerge spontaneously without any explicit prompting or instruction, achieving an overall expression accuracy of 99.65% on the test set and surpassing both promptingand steering-based baselines. Our contributions are threefold. (1) General framework. We propose systematic framework that integrates emotion direction extraction, local component identification, and global circuit integration, revealing stable emotion mechanisms in LLMs that generalize across different emotions and models. (2) Circuit-level control. Building on these mechanisms, we introduce circuit-based control method that reliably induces target emotions across arbitrary inputs without relying on explicit instructions. (3) Mechanistic evidence. We provide the first mechanistic evidence that emotion generation in LLMs is supported by traceable circuits, laying the foundation for interpretable and controllable emotional intelligence. Our findings demonstrate that emotions in LLMs are not mere surface reflections of training data, but emerge as structured and stable internal mechanisms. This work offers new insights into the cognitive interpretability of LLMs and establishes principled basis for the development of emotionally intelligent AI systems."
        },
        {
            "title": "2 Related Research",
            "content": "Emotion-related mechanisms in LLMs. Recent studies have revealed the presence of emotionrelated representations inside LLMs. Broekens et al. (2023) and Yongsatianchot et al. (2023) found that LLMs can partially align with psychological dimensions of emotion and appraisal theory. Li et al. (2024) demonstrated that language-derived conceptual knowledge of emotion causally supports emotional inference in LLMs, Tigges et al. (2024) showed that emotions can be captured as approximately linear directions in activation space and exhibit causal effects under intervention. Tak et al. (2025) grounded classifiers in appraisal theory to decode emotions from hidden states and performed layer-wise repair experiments, but their approach yields unstable effects across layersimplying that emotions may arise from distributed cross-layer dynamics rather than isolated modules. Similarly, Lee et al. (2025) identified emotion neurons from activation patterns, yet their masking experiments produced inconsistent or trivial changes, revealing redundancy instead of coherent causal mechanisms. Crucially, these studies probe LLMs ability to recognize emotions in text, not the internal processes that generate emotional expression. In summary, while prior work reveals emotion-related signals in LLMs, none has constructed the underlying circuit mechanisms that drive emotional expression. Methods for emotion control. Early works (Majumder et al., 2020; Goswamy et al., 2020) designed dialogue frameworks and affective generation models to enhance empathy and emotional support. Subsequent studies introduced controllable text generation techniques (Liang et al., 2024), such as style vectors that steer the residual stream (Konen et al., 2024) and latent-space manipulation with interpretable VAEs (Shi et al., 2020). More recent advances have extended emotion control to large-scale systems: Zhang et al. (2024) proposed ESCOT, framework for empathetic and supportive generation; Shen et al. (2025) introduced CoE, which integrates contextual cues for dialogue emotion recognition; Ishikawa and Yoshino (2025) examined emotion elicitation through controlled prompting; and Song et al. (2025) presented Emotion-o1, enhancing emotional understanding through long-chain reasoning. These methods demonstrate that emotion control is technically feasible, yet most remain black-box, without uncovering the internal mechanisms underlying emotion generation. However, our work reveals emotion circuits and verifies their ability to achieve stable emotion control without explicit prompting."
        },
        {
            "title": "3 Background",
            "content": "This section outlines the key components of Transformer forward computation to facilitate understanding of the experiments presented later. 3.1 Transformer Architecture We use the common pre-norm Transformer block. Let xl RT be the residual stream entering layer (sequence length , model width d). Residual stream. The residual stream serves as the medium to carry and store information across all layers, which is essential for analyzing how emotion representations propagate through the network. Each Transformer layer consists of two sublayers, multi-head self-attention (MHA) sublayer and feed-forward MLP sublayer, each followed by residual connection. The forward computation can be expressed as: xl = xl + MHA(l)(cid:16) xl+1 = xl + MLP(l)(cid:16) Norm(l) (cid:17) , 1 (xl) (cid:17) Norm(l) 2 (xl) . (1) (2) We treat the residual stream as the primary space for analyzing emotion representations, and measure its activations right after each sublayer output is added back, i.e., at xl and xl+1. Attention sublayer. The multi-head attention mechanism allows each token to attend to contextual information from previous tokens, potentially capturing emotional cues carried by other tokens. Given the normalized hidden states ul = Norm(l) 1 (xl), for each head {1, . . . , h}, the query, key, and value projections are: Qi = ulW (l,i) , Ki = ulW (l,i) , Vi = ulW (l,i) , Hi = softmax (cid:18) QiK dh (cid:19) + Vi, , (l,i) , (l,i) where (l,i) Rddh are learned projection matrices and dh = d/h is the head dimension. The head output is computed as: al = (cid:2)H1 Hh (cid:3) (l) . We perform head-level interventions on the con- (cid:3), prior to catenated head outputs (cid:2)H1 Hh the output projection (l) . MLP sublayer. The MLP sublayer consists of an up-projection, gating activation, and downprojection: MLP(l)(vl) = (cid:2)f (vlW (l) u1 ) (vlW (l) u2 )(cid:3)W (l) , 2 (xl), (l) where vl = Norm(l) u2 Rddmlp are the up-projection matrices for the gate and main branches, respectively, and () denotes the gating nonlinearity. We extract the gated activation u1 , (l) gl = (vlW (l) u1 ) (vlW (l) u2 ), which serves as the input to (l) and the target of neuron-level ablation and enhancement analysis."
        },
        {
            "title": "4 SEV Dataset",
            "content": "In this part, we introduce the construction of dataset SEV (ScenarioEvent with Valence). Dataset Construction. To analyze how emotions are represented inside LLMs, we constructed controlled dataset named SEV. Each record consists of neutral scenario and three outcome events (positive, neutral, and negative) describing different results of the same situation. This structure enables us to observe how LLMs respond to identical contexts and ensure the input text remains domainneutral and universal. We used semi-automatic generation pipeline with GPT-4o-mini, with all prompt templates provided in Appendix A. Eight everyday domains were defined, each containing 20 neutral scenarios that were expanded into three outcome variants sharing the same participants, time, and context but differing only in result valence. Emotionally explicit words (e.g., happy, sad) were explicitly prohibited to ensure that emotional variation arises from event semantics rather than lexical emotion cues. The final dataset comprises 480 event descriptions (8 domains 20 scenarios 3 outcomes), forming clean, emotion-neutral testbed for probing how LLMs internally encode and express emotions across general, everyday language contexts. Test Set. While SEV serves as the primary dataset for identifying emotion mechanisms, using it for both discovery and evaluation would create judgeplayer overlap. To ensure generalization, we constructed an independent test set of the same size as SEV, following same generation procedure. This test set serves as an out-of-domain validation, evaluating whether the discovered emotion mechanism can generalize beyond the training context and effectively induce target emotions in unseen, neutral input text. Its inclusion verifies that our identified emotion mechanisms are disentangled from dataset-specific biases and are transferable to any natural language input. Model Selection. We conduct all main analyses on LLaMA-3.2-3B-Instruct (Grattafiori et al., 2024), chosen for its transparent architecture, moderate scale, and well-documented open-source implementation. To verify robustness and generality, we additionally reproduce our framework on Qwen2.57B-Instruct (Qwen et al., 2025) (see Appendix H)."
        },
        {
            "title": "Directions",
            "content": "In this section, we describe the extraction of context-agnostic emotion directions and validate them through emotion steering in text generation. Emotion Elicitation via Prompting. We first elicit emotional expressions in LLMs using promptbased guidance (see Appendix for templates and examples). The selectable emotions include anger, sadness, happiness, fear, surprise, and disgust. Text generation uses greedy decoding to ensure reproducibility, achieving an emotion expression accuracy of 98.85% on SEV and 98.96% on the held-out test set (per-emotion details in Appendix C). We collect all last-token residual stream vectors from successful generations on SEV. For each layer, we log the activations at two insertion pointsimmediately after the outputs of the attention and MLP sublayers are added back to the residual stream, with the latter corresponding to the hidden states typically exposed by model APIs. The hidden states under different emotion elicitation are visualized via dimensionality reduction (see Fig.2, top row (ad); full layer-wise results are in Appendix E). Each point represents the hidden state of sample at the corresponding layer. For every scenario, event pair, six points are plotted, corresponding to six emotion-elicited forward passes. Initially, points with the same user message overlap because the last input token is identical across all emotion conditions. From layer 9, distinct emotion clusters begin to emerge. By layer 12, anger and disgust clusters appear close to each other, as do sadness and fear, whereas happiness and surprise remain relatively isolated. This organization aligns well with human affective intuition and remains stable across subsequent layers. The success rate of emotion elicitation was annotated by GPT-4o-mini and manually verified for consistency (see Appendix B). All annotations in this work follow the same evaluation protocol for consistency. Figure 2: The first row (ad) visualizes the last-token hidden states of prompting-based generations across layers 0, 9, 12, and 27. Initially, all samples overlap due to identical input tokens, but representations gradually diverge and form distinct emotion clusters in deeper layers. The second row (eh) shows the layer-wise evolution of pure emotion vectors, which already display slight separation at layer 0 and become increasingly clustered with depth. Context-Agnostic Emotion Vectors Extraction. To isolate emotion representations independent of semantic content, we extract residual stream activations after both the attention and MLP sublayers. For each scenarioevent group containing six emotion variants, we subtract the mean activation across emotionstreated as neutral baselineto cancel shared semantics and preserve only emotionspecific variance. This neutrality assumption is empirically supported by the success of steering experiments on the test set, where emotion directions extracted under this formulation effectively induce the intended emotions. We then compute per-emotion means across all groups to obtain layerwise emotion centroids, remove the global mean, and apply ℓ2 normalization to derive unit-norm emotion direction vectors. Two parallel sets of directions are obtained: v(l) e,attn from attention sublayers and v(l) e,mlp from MLP sublayers. We denote e,mlp as v(l) v(l) in the following analyses. The resulting vectors capture intrinsic directions of emotional variation in the models representation space. As shown in Fig. 2 (eh), distinct emotion clusters emerge as early as layer 0 (linear-probe F1 = 1.0) and remain well-separated through deeper layers. These vectors serve as foundational emotion directions for subsequent experiments. tors v(l) are injected into layers 1120 via fore ward hooks, after removing the emotion instruction from the system prompt. For each input {scenario, event} and target emotion e, we perturb the last-token hidden state as + α RMS(cid:0)h(l) h(l) h(l) , α = 8. (cid:1) v(l) The scaling by local RMS maintains activation magnitude while enforcing the target emotional direction. Generation uses greedy decoding for deterministic outputs. On the held-out test set, steering achieves 91.22% success rate, confirming that the extracted emotion vectors capture contextagnostic representations of emotional expression (per-emotion details in Appendix C)."
        },
        {
            "title": "Representation",
            "content": "Building on the extracted emotion directions, we identify which local components in each layer contribute to these layer-wise emotional representations. For MLP sublayers, we analytically decompose neuron contributions to the emotion vector v(l) . For attention sublayers, we perform layerwise causal ablations to locate emotion-sensitive heads. Together, these analyses reveal how emotional representations are locally implemented per layer. Emotion Steering Validation. We validate the extracted emotion directions by steering the models residual activations at the last token of the user input. MLP-based layerwise emotion vec6.1 Emotion Neuron Identification Each Transformer block contains two sublayers, attention and MLP, each contributing its own residual update. Here we focus on the MLP-based emotion directions v(l) influence emotion formation. to analyze how individual neurons Let v(l) Rd denote the unit-norm MLP-based emotion direction in the residual stream at layer l, g(l) Rdff the last-token gated activation, and (l) Rddff the down-projection matrix. We compute neuron-space alignment vector: = (cid:0)W (l) β(l) (cid:1)v(l) Rdff , where β(l) e,j quantifies how strongly neuron js write vector pushes the residual stream toward the target emotion direction. For each sample n, the perneuron contribution is e,n = g(l) c(l) t,n β(l) , where g(l) t,n,j reflects the activation strength of neuron j. We average c(l) e,n over all successful samples and rank neurons by mean contribution. The topk neurons per layer, which most strongly drive the emotion direction v(l) , are later used in ablation and enhancement experiments. 6.2 Attention Head Identification While neuron-level contributions can be analytically decomposed, attention heads require direct causal analysis. We identify attention heads that most strongly drive emotion expression using SEV samples successfully elicited by prompting-based generation. For each sample, we record the lasttoken residual stream after the attention output is added back, and compute its projection onto the corresponding emotion direction v(l) e,attn as the baseline score s, reflecting the strength of emotion representation in the residual stream. We then perform causal interventions by zeroing individual attention heads before the (l) projection, recomputing the residual stream, and measuring the new projection score s. Head importance is defined as = s, where larger decrease indicates stronger causal effect. Topk heads per layer that most strongly drive emotion are used in subsequent ablation and enhancement experiments. 6.3 Causal Validation via Ablation We evaluate the identified components on the heldout test set by ablating them and observing degradation in emotion representation. Setup. For each emotion e, we use the same prompting protocol as in the elicitation stage and run all samples in test set. Let s(l) denote the projection of the last-token residual stream (after the sublayer addition) onto the corresponding emotion direction at layer l; we report the total score = (cid:80) s(l). The baseline uses k=0 with no hooks. MLP neurons. In each layer, we zero out the topk ranked neurons at the last-token position of the gated activation g(l) (before the down-projection (l) ) and recompute the forward pass. We sweep {0, 2, 4, 8, 16, 32, 64, 128} and measure the change = s, where is the baseline projection score and is the score after ablation. As shown in Fig. 3(a), drops sharply at = 2 and = 4, then quickly plateaus even as increases by over 30revealing pronounced long-tail effect where only few neurons dominate emotion expression. Attention heads. For attention, we zero out the top-k heads per layer by removing their output channel slices before the (l) projection, sweeping {0, 1, 2, . . . , 8}. The resulting change in emotion score = mirrors the neuron pattern (Fig. 3(b)): steep early decline followed by saturation, indicating that only handful of attention heads play decisive roles in shaping emotion representation. 6.4 Causal Validation via Enhancement While ablation verifies that the identified components are necessary for emotion representation, it does not confirm whether they are sufficient to induce emotion expression. We therefore conduct enhancement experiments to test whether activating these components can drive emotional representations even in the absence of explicit instruction. Emotion difference vectors. To provide additive signals for enhancement, we derive per-layer emotion difference vectors via within-group contrasts. For each scenarioevent group and layer l, we take the last-token activations of the six emotions, compute their mean, and subtract it from each emotion, thereby isolating emotion-specific variation while cancelling shared scenario and event semantics. We then average these within-group differences across all groups, yielding δ(l) e,mlp Rdff from the gated MLP activations and δ(l) e,attn Rd from the attention o-projection input. These vectors serve as Figure 3: (ab) Ablation: zeroing out the identified emotion-related components sharply decreases emotion scores, while random ablation has minimal effect. (cd) Enhancement: injecting emotion difference vectors into identified components greatly increases s. All curves are plotted with 95% confidence intervals. enhancement signals aligned with emotion e. MLP neurons. We test whether stimulating emotion-relevant neurons can promote emotion expression on the held-out test set. For each layer l, we inject δ(l) e,mlp into the top-k neurons Jl (ranked by mean contribution to v(l) ) at the last-token position of the gated activation: a(l) t,Jl a(l) t,Jl + λ δ(l) e,mlp,Jl , λ = 1.0, All prompts are identical to those in the elicitation stage but with emotion instructions removed, ensuring that any observed emotion arises from internal modulation rather than prompting. The effect size is measured by the projection change = s, where and are preand post-enhancement emotion scores aggregated over layers. As shown in Fig. 3(c), rises sharply for small but quickly saturates as increasesdemonstrating that only small subset of top-ranked neurons are sufficient to evoke emotional representations. Attention heads. Similarly, we inject the perlayer emotion difference vectors δ(l) e,attn into the output channels of the top-k attention heads before the (l) projection, again modifying only the last token. The resulting = mirrors the MLP trend (Fig. 3(d)): activating just few key heads is enough to elicit strong emotion responses, while random or lower-ranked heads yield negligible effects. These findings confirm that the identified components are not only necessary but also sufficient to drive emotional expression in LLMs. 6.5 Control Experiments with Random Interventions To ensure that the observed effects are not due to intervention size or random noise, we conduct control experiments using randomly selected units. In each layer l, random set rand of size is sampled uniformly, and the same ablation or enhancement protocol is applied as in the main experiments. Results are averaged over 10 random seeds for robustness. Across both settings, random interventions have negligible impact. As shown in Fig. 3, targeted interventions are clearly separated from random ones across k, confirming that our effects arise from the identified emotion-relevant components rather than from arbitrary perturbations."
        },
        {
            "title": "Circuits",
            "content": "This section integrates the local mechanisms identified earlier into coherent, circuit-level understanding of emotion generation. We first quantify each sublayers causal influence on the models global emotional state, then assemble emotion circuits accordingly, and finally show that modulating these circuits can reliably control emotional expression. 7.1 Layerwise Importance of Emotion Generation Emotions in LLMs emerge not from isolated components but through cross-layer propagation. To identify which layers most strongly shape the final emotional state, we compute each sublayers causal contribution relative to stable reference basis. Reference basis. We track how emotion directions evolve along the residual stream by computing pairwise cosine similarities across all 56 sublayers (Attn0, MLP0, . . . , Attn27, MLP27). As shown in Appendix G, emotion subspaces become highly consistent in later layers, with cross-sublayer similarity typically above 0.90. We therefore define per-emotion reference vector v(e) ref by sign-aligning and averaging the attention and MLP directions within layers 2125, followed by ℓ2 normalization. This direction serves as stable target for quantifying how earlier sublayers steer the models final emotional representation. Sublayer importance. We measure how perturbations along each sublayers local emotion direction influence the final emotional state. For each emotion e, we inject small, scaled offset to the last-token residual output of single sublayer (L, p): h(L,p) = α σL,p v(L) e,p , where σL,p is the RMS magnitude of that sublayers residual update. After recomputing the forward pass, we record the shift of the final hidden state along the reference basis: = final hfinal, v(e) ref . The normalized influence score IL,p = α σL,p quantifies how much sublayer (L, p) drives the model toward its stable emotion direction. Perturbing one sublayer at time isolates its direct causal effect, and averaging IL,p across samples and α values yields consistent layerwise influence profile robust to hyperparameter variation (sublayer ranking remains highly consistent across α). 7.2 Global Emotion Circuit Assembly For each emotion, we assemble sparse, layerdistributed emotion circuit by combining local component scores with measured sublayer importance. The total circuit budget is set to ten times the number of sublayers and allocated globally. To balance distributed expression and deep-layer amplification, allocation follows twostage tradeoff. First, each sublayer receives minimum quota, ensuring that lower layerswhose hidden states remain visible to attention during generationcan still encode emotional cues. Second, the remaining budget is distributed proportionally to the sublayer importance IL,p, emphasizing sublayers that more strongly influence the final emotion basis, which determines the sentiment of generated text. Within each sublayer, we select the top-ranked components identified in Section 6. The resulting circuit forms compact yet expressive backbone that integrates emotion-related signals across the residual stream. Across emotions, these circuits exhibit low neuron overlap (µ = 0.0560.033) and moderate head overlap (µ = 0.4540.047), revealing dual architecture: emotion-specific local subcircuits in MLPs and shared attention pathways that propagate global emotional context. 7.3 Controlling Emotion Expression via Circuit Modulation We evaluate whether the assembled circuits can directly control emotional expression during generation. For each emotion e, we reuse the same emotion difference vectors (λe = 1.0) defined earlier, applying them to the global circuit. Generations are still under greedy decoding, using the same inputs and injection procedure as in Sec . 6.4. The proposed circuit modulation achieves an overall emotionexpression accuracy of 99.65% on the test set, outperforming both promptingbased and steering-based approaches (see D). Notably, while steering-based control yields only 67.71% success for surprise, our circuit-based modulation achieves 100%. Beyond accuracy, the generated text exhibits strikingly natural affective toneexpressions such as Whoa?! or spontaneous exclamations appear without explicit promptingsuggesting that the model is internally generating rather than externally complying with emotional cues. These results demonstrate that circuit modulation not only exposes the hidden circuitry underlying LLM emotion mechanisms but also enables finer-grained, interpretable control. Unlike steering methods that inject single global vector into hidden states, our approach directly modulates emotion-relevant neurons and attention heads, balancing layerwise emotion propagation with final emotional realization."
        },
        {
            "title": "8 Conclusion",
            "content": "We show that emotion in LLMs arises from structured hierarchy of neurons and attention heads whose coordinated activations form interpretable emotion circuits. Tracing, assembling, and modulating these circuits enables precise and natural emotional control in text generationoutperforming promptingand steeringbased baselines in both accuracy and expressiveness. These findings reveal that emotional expression in large models is not surface artifact of lexical co-occurrence but product of distributed internal computation that can be systematically analyzed and controlled. Our framework lays foundation for extending circuit-level interpretability and controllable generation to broader cognitive and stylistic domains."
        },
        {
            "title": "Limitations",
            "content": "While our study systematically uncovers and manipulates emotion circuits in LLMs, several limitations remain. First, our analyses are limited to English inputs, and it remains to be verified whether similar emotion circuits emerge under multilingual contexts. Second, the study focuses on Ekmans six basic emotions, leaving richer affective spectra for future exploration. Finally, while the extracted circuits demonstrate strong causal control within the tested models, their stability under fine-tuning or transfer learning remains to be explored."
        },
        {
            "title": "References",
            "content": "Joost Broekens, Bernhard Hilpert, Suzan Verberne, Kim Baraka, Patrick Gebhard, and Aske Plaat. 2023. Finegrained affective processing capabilities emerging from large language models. In 2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII), pages 18. Wanghao Dong, Weijun Wang, Xinheng Han, Junhao Huang, Lie Li, and Yinghui Huang. 2025. Can gpt4 provide human-level emotion support? insights from machine learning-based evaluation framework. Computers in Biology and Medicine, 196:110789. Paul Ekman. 1992. An argument for basic emotions. Cognition and Emotion, 6(3-4):169200. Tushar Goswamy, Ishika Singh, Ahsan Barkati, and Ashutosh Modi. 2020. Adapting language model for controlled affective text generation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 27872801, Barcelona, Spain (Online). International Committee on Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Kai Konen, Sophie Jentzsch, Diaoulé Diallo, Peer Schütt, Oliver Bensch, Roxanne El Baff, Dominik Opitz, and Tobias Hecking. 2024. Style vectors for steering generative large language models. In Findings of the Association for Computational Linguistics: EACL 2024, pages 782802, St. Julians, Malta. Association for Computational Linguistics. Jaewook Lee, Woojin Lee, Oh-Woog Kwon, and Harksoo Kim. 2025. Do large language models have emotion neurons? investigating the existence and role. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1561715639, Vienna, Austria. Association for Computational Linguistics. Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan Liu, and Dan Zhang. 2024. Language-specific representation of emotion-concept knowledge causally supports emotion inference. iScience, 27(12):111401. Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, and Zhiyu Li. 2024. Controllable text generation for large language models: survey. Preprint, arXiv:2408.12599. Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway Ghosal, Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020. MIME: MIMicking emotions for empathetic response generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 89688979, Online. Association for Computational Linguistics. Hiroki Naito. 2025. The gpt-4o shock emotional attachment to ai models and its impact on regulatory acceptance: cross-cultural analysis of the immediate transition from gpt-4o to gpt-5. Preprint, arXiv:2508.16624. Jason Phang, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang, Auren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W. T. Chan, Pat Pataranutaporn, and Pattie Maes. 2025. Investigating affective use and emotional well-being on chatgpt. Preprint, arXiv:2504.03888. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Shin-nosuke Ishikawa and Atsushi Yoshino. 2025. AI with emotions: Exploring emotional expressions in large language models. In Proceedings of the 5th International Conference on Natural Language Processing for Digital Humanities, pages 614627, Albuquerque, USA. Association for Computational Linguistics. Zhiyu Shen, Yunhe Pang, Yanghui Rao, and Jianxing Yu. 2025. CoE: clue of emotion framework for emotion recognition in conversations. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2354823563, Vienna, Austria. Association for Computational Linguistics. Wenxian Shi, Hao Zhou, Ning Miao, and Lei Li. 2020. Dispersed exponential family mixture VAEs for inIn Proceedings of the terpretable text generation. 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 88408851. PMLR. Changhao Song, Yazhou Zhang, Hui Gao, Kaiyun Huang, and Peng Zhang. 2025. Emotion-o1: Adaptive long reasoning for emotion understanding in llms. Preprint, arXiv:2505.22548. Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, and Jonathan Gratch. 2025. Mechanistic interpretability of emotion inference in large language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1309013120, Vienna, Austria. Association for Computational Linguistics. Curt Tigges, Oskar J. Hollinsworth, Atticus Geiger, and Neel Nanda. 2024. Language models linearly represent sentiment. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 5887, Miami, Florida, US. Association for Computational Linguistics. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Steering language Monte MacDiarmid. 2024. Preprint, models with activation engineering. arXiv:2308.10248. Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi, MohammadHossein LotfiNia, and Hamed Farbeh. 2025. Ai in mental health: Emotional and sentiment analysis of large language models responses to depression, anxiety, and stress queries. Preprint, arXiv:2508.11285. Nutchanon Yongsatianchot, Parisa Ghanad Torshizi, and Stacy Marsella. 2023. Investigating large language models perception of emotion using appraisal theory. In 2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW), pages 18. Tenggan Zhang, Xinjie Zhang, Jinming Zhao, Li Zhou, and Qin Jin. 2024. ESCoT: Towards interpretable emotional support dialogue systems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1339513412, Bangkok, Thailand. Association for Computational Linguistics. PROMPT TEMPLATE = \"\"\" Please generate 20 **first-person neutral scenarios** related to \"{theme}\". Requirements: - Each scenario must be one sentence in English. - Use first person (\"I ...\"). - Describe objective background or activity, without any emotional words or value judgments. - Length should be 1525 words. Return as numbered list. Example (theme = Work/Job): 1. attended scheduled meeting with my colleagues this morning. 2. sent the weekly report to my manager on time. 3. received the task list for this week from my supervisor. \"\"\" EVENT TEMPLATE = \"\"\" have first-person scenario: \"{scenario}\" Definitions: - The *event* must be an outcome, change, or consequence that happens **within or after** the scenario. - It must build on the scenarios context (same subject/time/- participants) and **must not** merely restate the scenario. Please write three event versions for this scenario: 1. Positive event: clear benefit, success, or smooth outcome. 2. Neutral event: plain factual outcome with no obvious gain or loss (still an outcome, not restatement). 3. Negative event: failure, loss, delay, or violation of expectation. Requirements: - Minimal change across the three versions (same subject, time, structure). - Keep length similar (within 20%). - Do NOT use explicit emotion words (e.g., \"I am happy\", \"I feel sad\"). - Each version should be one sentence in English. - OUTPUT: SINGLE JSON OBJECT ONLY, with keys \"positive\", \"neutral\", \"negative\". Example: Scenario: \"I submitted my weekly project report to my manager.\" Output: \"positive\": \"My manager immediately praised the key points and invited me to present next week.\", \"neutral\": \"My manager noted the milestones and said we would discuss details later.\", \"negative\": \"My manager pointed out the deliverable was not up to standard and asked me to redo it this week.\" \"\"\""
        },
        {
            "title": "B Annotation Prompt",
            "content": "This section shows the prompts used in the generation of the Dateset SEV. Eight everyday themes were defined: Work/Job, School/Academia, Personal Relationships, Customer Service/Shopping, Public Services/Administration, Health/Medical, Housing/Living, and Travel/Transportation. This section shows the prompt used for deciding if the generated text of LLMs matches the target emotion. SYSTEM = You are careful rater. Given target emotion and text, decide if the texts STYLE matches the target emotion among: {EMOTIONS} Focus on tone/attitude, not content valence. USER TMPL = Target emotion: {emotion} Text: {text} Decide if the texts STYLE matches the target emotion. Return compact JSON with keys exactly: { \"match\": <0 or 1>, \"reason\": <short string> }"
        },
        {
            "title": "Prompting and Steering",
            "content": "This section reports the emotion expression accuracy for each emotion under two settingspromptbased and steering-based generation. The former one is evaluated on SEV and test set, and the latter one is on test set (see Table 1)."
        },
        {
            "title": "Circuit Modulation",
            "content": "This section reports the per-emotion accuracy of circuit-modulated generation on the test set. Detailed results are summarized in Table 2. Layer-wise Visualization of Emotion"
        },
        {
            "title": "Representations",
            "content": "This section presents the layer-wise clustering visualizations of hidden states for all samples successfully guided by the prompting-based method, as well as pure emotion vectors clustering visualizations (see Fig.4 and Fig.5). The prompts for Prompting-based generation We elicit emotional expressions in LLMs through prompt-based guidance. The prompt template and representative example are as follows: System Prompt Template: Always reply in {emotion}. Keep the reply to at most two sentences. User Prompt Template: {scenario}{event} System message: Always reply in anger. Keep the reply to at most two sentences. User message: {I organized team brainstorming session to generate ideas for the upcoming product launch strategy.} {The team generated diverse set of innovative ideas that were well-received and formed the basis of successful product launch plan.}"
        },
        {
            "title": "Emotion Subspaces",
            "content": "This section presents the pairwise cosine-similarity heatmaps of emotion directions across all 56 residual positions (Attn0, MLP0, . . . , Attn27, MLP27). As shown in Fig. 6, emotion subspaces remain highly consistent in deeper layers: cross-sublayer similarity typically exceeds 0.90 and never falls below 0.85 across all six emotions. These results support our definition of the reference directions v(e) ref using the stable range {21, . . . , 25}. Experiments on Qwen2.5-7B-Instruct H.1 Emotion Expression Accuracy of Prompting and Steering Table 3 reports the emotion expression accuracy of prompt-based and steering-based methods. While prompting achieves consistently high accuracy across all emotions (84-96%), steering exhibits notable pattern: it succeeds for positive emotions (happiness and surprise: >92%) but fails for negative emotions (anger, sadness, fear, and disgust: <5%). This suggests that Qwen2.5-7B-Instruct has strong inherent resistance to negative emotion steering, likely due to safety alignments that prevent the model from expressing harmful or negative emotional content through direct representational manipulation. H.2 Layer-wise Visualization of Emotion Representations This section presents the layer-wise clustering visualizations of hidden states for all samples successfully guided by the prompting-based method on Qwen2.5-7B-Instruct, as well as pure emotion vectors clustering visualizations (see Fig. 7 and Fig. 8). H.3 Results of MLP Neurons Intervention Experiments This section presents the results of ablation & enhancement experiments conducted on Qwen2.5-7BInstruct to verify the model-independent stability of our findings. The setup strictly follows the same Figure 4: The layer-wise clustering visualizations of hidden states for all samples successfully guided by the prompting-based method. Figure 5: The layer-wise clustering visualizations of pure emotion vectors. Table 1: Emotion expression accuracy (%) of prompt-based and steering-based generation for all six emotions. Method Anger Sadness Happiness Fear Surprise Disgust Prompting (SEV) Prompting (Test Set) Steering (Test Set) 99.58 100.0 93.33 99.38 99.79 96.04 97.92 97.49 99.58 100.00 99.58 96. 97.29 97.50 67.71 98.96 99.38 93.75 Table 2: Emotion expression accuracy (%) of circuit-modulated generation across six target emotions, evaluated on the held-out test set. Valence Anger Sadness Happiness Fear Surprise Disgust Positive Neutral Negative 98.75 98.12 97.50 100.00 100.00 100.00 100.00 100.00 99. 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 Figure 6: Cosine-similarity heatmaps of emotion directions across all 56 residual positions for six emotions, showing stable high similarity (>0.90) in deeper layers (2125). Table 3: Emotion expression accuracy (%) of prompt-based and steering-based generation for all six emotions on Qwen2.5-7B-Instruct. Method Anger Sadness Happiness Fear Surprise Disgust Prompting (SEV) Prompting (Test Set) Steering (Test Set) 96.25 95.83 0. 84.38 87.29 4.90 88.54 90.83 93.30 91.46 93.54 0.00 90.42 94.17 92.60 84.79 88.33 3.70 Figure 7: The layer-wise clustering visualizations of hidden states for all samples successfully guided by the prompting-based method on Qwen2.5-7B-Instruct. Figure 8: The layer-wise clustering visualizations of pure emotion vectors on Qwen2.5-7B-Instruct. procedures described in Section 6. As shown in Tables 4 and 5, Qwen exhibits the same long-tail pattern as LLaMA: emotion scores drop/increase sharply when small number of top-ranked components are intervened, whereas intervening random components yields negligible change, confirming that our identified components really play an important role in forming emotion representations in LLMs, and only few localized units dominate emotion expression. H.4 Results of Attention Head Intervention Experiments This section reports the results of attention headlevel intervention experiments on Qwen2.5-7BInstruct, examining the causal role of attention heads in emotion expression through both enhancement and ablation approaches. Table 6 presents enhancement results that top-k attention heads are enhanced to amplify emotion intensity, while Table 7 shows masking results where top-k heads are ablated by zeroing their outputs. The positive values in enhancement and negative values in masking provide converging evidence that the identified attention heads are causally important for emotion expression. H.5 Emotion Expression Accuracy with Scale 2.0 Steering This section reports the emotion expression accuracy of circuit-based steering with scale factor of 2.0 on Qwen2.5-7B-Instruct across different valence contexts (positive, neutral, and negative). The results demonstrate how steering effectiveness varies across emotions and context valences. Detailed per-valence accuracy for all six emotions is summarized in Table 8, which surpasses the performance of the steering-based eliciting method on the test set. Table 4: Ablation results on Qwen2.5-7B-Instruct. Emotion scores (s) decrease sharply when top-ranked neurons are ablated, while random ablations show minimal effect, mirroring the long-tail pattern observed in LLaMA. Anger Sadness Happiness Fear Surprise Disgust Random 0 2 4 8 16 32 64 128 256 512 0.00 -100.30 -171.15 -296.61 -397.25 -497.72 -566.12 -657.05 -749.72 -836.63 0.00 -155.54 -221.37 -283.06 -353.57 -439.28 -519.96 -606.55 -691.59 -795. 0.00 -153.47 -260.03 -338.65 -402.78 -473.85 -552.92 -635.91 -714.90 -795.64 0.00 -71.40 -131.05 -197.56 -286.53 -386.88 -460.57 -548.90 -621.62 -740.43 0.00 -178.11 -245.09 -340.23 -408.74 -522.58 -609.85 -674.13 -767.92 -833.84 0.00 -199.91 -254.98 -332.96 -405.73 -509.94 -602.93 -668.64 -752.66 -839.28 0.00 -0.27 -0.38 -0.76 -1.30 -3.13 -5.05 -9.81 -21.15 -41.72 Table 5: Enhancement experiment results on Qwen2.5-7B-Instruct: Impact of enhancing top-k MLP neurons on emotion intensity (s) for scale factor 1.0. Positive values indicate stronger emotion enhancement. Anger Sadness Happiness Fear Surprise Disgust Random 0 2 4 8 16 32 64 128 256 0.00 27.66 49.07 79.36 109.25 155.65 194.43 230.85 248.49 269.03 0.00 47.72 74.13 95.87 133.57 169.27 203.02 225.38 246.82 271.94 0.00 56.84 120.42 120.68 145.88 176.17 198.51 224.84 234.40 255.48 0.00 28.07 43.65 57.48 88.77 103.94 138.36 162.71 193.01 226.23 0.00 63.51 93.25 135.66 183.86 229.29 271.75 302.17 320.71 343.89 0.00 81.68 103.95 122.03 152.59 201.22 232.80 262.59 284.72 304. 0.00 0.07 0.08 0.24 0.34 0.71 1.29 2.15 4.10 7.84 Table 6: Attention head enhancement experiment results on Qwen2.5-7B-Instruct: Impact of enhancing top-k attention head on emotion intensity (s) for scale factor 1.0. Positive values indicate stronger emotion enhancement through head-level intervention. 0 1 2 3 4 5 6 7 8 9 10 Anger Sadness Happiness Fear Surprise Disgust Random 0.00 42.14 88.41 141.91 165.92 183.27 191.34 200.64 211.47 215.03 212.95 0.00 27.99 55.71 77.07 92.00 96.75 104.51 109.28 111.87 109.82 115.11 0.00 34.25 41.74 66.54 105.04 115.89 122.30 133.08 134.33 125.16 135.68 0.00 21.66 40.82 52.29 59.10 76.69 97.59 98.24 96.04 100.66 105. 0.00 112.00 157.14 224.57 244.20 275.41 291.51 306.98 300.42 287.39 305.84 0.00 17.13 48.76 85.62 100.10 109.23 116.12 128.96 136.32 148.22 151.16 0.00 1.17 6.69 19.64 14.18 19.06 22.53 33.03 22.42 43.12 42.86 Table 7: Attention head ablation experiment results on Qwen2.5-7B-Instruct: Impact of masking top-k attention heads on emotion intensity (smean). Negative values indicate emotion reduction after head masking. 0 1 2 3 4 5 6 7 8 9 Anger Sadness Happiness Fear Surprise Disgust Random 0.00 -119.02 -225.48 -286.47 -325.84 -358.13 -391.35 -454.21 -544.44 -466.15 -478.86 0.00 -62.08 -170.65 -231.54 -309.07 -320.20 -362.16 -419.93 -501.72 -439.11 -459. 0.00 -78.53 -150.63 -185.87 -241.42 -260.47 -313.11 -330.56 -433.99 -358.29 -375.23 0.00 -120.64 -216.03 -268.63 -319.77 -350.51 -368.56 -434.26 -514.20 -443.03 -477.13 0.00 -153.41 -219.96 -269.73 -320.56 -349.54 -371.71 -426.61 -517.43 -451.28 -469.62 0.00 -109.34 -223.36 -286.77 -355.15 -377.35 -409.78 -474.55 -562.08 -491.12 -515.29 0.00 -128.53 -170.60 -240.39 -209.97 -195.45 -252.30 -274.80 -322.77 -348.99 -357.83 Table 8: Emotion expression accuracy (%) of circuit-based steering (scale 2.0) across different valence contexts on Qwen2.5-7B-Instruct. Valence Anger Sadness Happiness Fear Surprise Disgust Average Positive Neutral Negative Average 7.50 8.12 39.38 18.33 31.87 42.50 95.62 56.66 100.00 100.00 60. 86.67 31.87 37.50 52.50 40.62 100.00 97.50 86.88 94.79 100.00 100.00 100. 100.00 61.87 64.27 72.40 66."
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
        "Peking University"
    ]
}