{
    "paper_title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering",
    "authors": [
        "Akash Gupta",
        "Amos Storkey",
        "Mirella Lapata"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose a meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using a fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using a few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just a few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 0 9 6 0 . 6 0 5 2 : r Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering Akash Gupta School of Informatics University of Edinburgh akash.gupta@ed.ac.uk Amos Storkey School of Informatics University of Edinburgh a.storkey@ed.ac.uk Mirella Lapata School of Informatics University of Edinburgh mlap@inf.ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to perform new tasks with minimal supervision. However, ICL performance, especially in smaller LMMs, is inconsistent and does not always improve monotonically with increasing examples. We hypothesize that this occurs due to the LMM being overwhelmed by additional information present in the image embeddings, which is not required for the downstream task. To address this, we propose meta-learning approach that provides an alternative for inducing few-shot capabilities in LMMs, using fixed set of soft prompts that are distilled from task-relevant image features and can be adapted at test time using few examples. To facilitate this distillation, we introduce an attention-mapper module that can be easily integrated with the popular LLaVA v1.5 architecture and is jointly learned with soft prompts, enabling task adaptation in LMMs under low-data regimes with just few gradient steps. Evaluation on the VL-ICL Bench shows that our method consistently outperforms ICL and related prompt-tuning approaches, even under image perturbations, improving task induction and reasoning across visual question answering tasks."
        },
        {
            "title": "Introduction",
            "content": "Humans have the remarkable ability to quickly learn new tasks in multimodal environments with just few trial-and-error attempts. Extensive research in cognitive science suggests that this ability arises from learning hierarchical abstractions and maintaining shared structural priors across related tasks based on past experiences. (Griffiths et al., 2019; Finn, 2018; Kirsch and Schmidhuber, 2022). Drawing on this prior knowledge enables rapid learning in new situations and reduces the need for large amounts of task-specific demonstrations (Finn et al., 2017). Large Multimodal Models (LMMs) are able to perform multitude of tasks ranging from reasoning to fine-grained image understanding and visual question answering (Liu et al., 2024; Li et al., 2023a; Laurençon et al., 2024). They are typically built on top of base Large Language Model (LLM) by supplementing it with vision encoder and connecting module that acts as bridge for different modalities to interact. When (pre)trained at sufficient scale and finetuned on wide range of multimodal tasks (with natural language instructions), LMMs can learn new tasks by virtue of in-context learning (ICL), i.e., by being prompted with few input-output examples, without requiring any updates to the model parameters (Zhao et al., 2024; Zong et al., 2025; Coda-Forno et al., 2023). Although the training-free nature of ICL has led to its rapid adoption across tasks and domains, its underlying mechanism remains ill-understood (Hendel et al., 2023; Huang et al., 2024) and its empirical behaviour can be inconsistent. Moreover, recent work (Zong et al., 2025) demonstrates that ICL is most effective for large-scale LMMs (72B parameters), while smaller 1We release our training and evaluation code here - https://github.com/akashgupta97/MAPD Preprint. Under review. Figure 1: Failure case of LLaVA-OneVision-7B (Li et al., 2025) on an example from the Fast OpenEnded MiniImageNet classification task (Tsimpoukelli et al., 2021). When no in-context examples are provided (0-shot), the model initially generates generic description of the image. As more examples (shots) are added, it begins to learn the answer format (single word), but still fails to grasp the task, producing incorrect or irrelevant predictions. models (<7B parameters) often struggle with increasing in-context examples and their performance either plateaus or deteriorates even when extending the context length or giving detailed instructions. Zong et al. (2025) attribute this limitation to the fact that smaller models struggle with the large number of image tokens in long sequences. As result, they become confused and perform the task haphazardly or revert to default behaviors, such as drawing from their parametric knowledge, while effectively ignoring the in-context examples. Figure 1 provides an example of such failure case with LLaVA-OneVision-7B LMM (Li et al., 2025) for task taken from the Fast Open-Ended MiniImageNet dataset (Tsimpoukelli et al., 2021). The model originally outputs generic description about the image based on parametric knowledge and ultimately fails to give correct answer, despite being prompted with few examples. Building on this observation, we hypothesize that effective few-shot adaptation at test time may be compromised by the added information introduced by the image embeddings. While more precise set of embeddings would be preferable, the continuous nature of image embeddings makes it challenging to distill task-specific information from them. As an alternative, we propose to learn fixed set of new embeddings that can be easily finetuned at test time. This idea of task adaptation has gained significant traction in the literature through prompt tuning (Lester et al., 2021) which finetunes set of continuous soft prompts while keeping the underlying language model frozen; the prompts are prepended in the context at test-time, effectively steering the model to perform the desired task. We introduce an approach for learning new tasks using learnable soft prompts that receive task information from the LLM in the form of loss gradients during finetuning. These gradients update the soft prompts which when fused with the image embeddings are able to distill relevant features from them. To facilitate this fusion, we propose an attention-mapper that uses multi-head attention (Vaswani et al., 2017) architecture responsible for extracting relevant task-specific image information. We adopt LLaVA v1.5 (Liu et al., 2024) as our base LMM and substitute its naive MLP projection layer with our attention-mapper and set of learnable soft prompts. Needless to say, the approach outlined above relies on being able to adapt quickly to new tasks at test time after seeing only few examples, since designing finetuning procedures for individual tasks is impractical. Prior work (Finn et al., 2017; Ravi and Larochelle, 2017; Vinyals et al., 2016) addressed this challenge by training meta-learner that can infer an optimal learning strategy for new task after being exposed to distribution of tasks. We propose to apply this meta-learning procedure in our multimodal prompt distillation setting. Specifically, we employ the widely known MAML algorithm (Finn et al., 2017) and use its lightweight first-order approximation for training the attention-mapper and soft prompts. Our approach builds on previous work (Qin et al., 2023; Najdenkoska et al., 2023; Li et al., 2023b) highlighting the benefits of MAML for small vision-language models in tasks like fast-concept binding and classification. We focus on visual question answering (VQA; Antol et al. 2015; see example in Figure 1), general-purpose task often used to evaluate the image understanding capabilities of LMMs, and demonstrate the benefits of MAML at scale for LLaVA v1.5 (Liu et al., 2024). Our contributions can be summarized as follows: We propose an alternative to in-context learning (ICL) by meta-learning fixed set of soft prompts within LMMs through distillation. Our approach, which we call MAPD (as shorthand for Meta-Adaptive Prompt Distillation), can quickly adapt to new tasks at 2 Figure 2: Our proposed MAPD framework based on LLaVA v1.5-7B (Liu et al., 2024) for distilling image embeddings into soft prompts during instruction finetuning. The support set (X supp ) is processed initially to the obtain loss value Lsupp which is used in the innerv loop to obtain task-specific parameters {θ, }. Next, the query set (X query , query ) is used to calculate the query loss for the outer-loop meta-parameter optimization {θ, }. , query , supp , supp test time through fine-tuning on small number of examples, and exhibits consistently monotonic performance improvements as the number of shots increases. To the best of our knowledge, this is the first exploration of meta-learned prompt distillation for cross-task generalization in LMMs under low-data settings. We incorporate an attention-mapper, inspired from Najdenkoska et al. (2023), into the LLaVA-v1.5 7B architecture that is trained jointly with soft prompts and facilitates the distillation of task-specific image information. Since the quality of soft prompts heavily depends on the capabilities of the underlying LLM, we replace LLaVAs original LLM with more powerful model, namely Qwen2.5-7B-Instruct (Qwen et al., 2025). Extensive evaluation on VL-ICL Bench2 (Zong et al., 2025), diverse benchmark for image perception and mathematical reasoning, demonstrates that our approach outperforms several other prompt distillation methods, even in the presence of image perturbations."
        },
        {
            "title": "2 Problem Formulation",
            "content": "We define the few-shot VQA learning problem and discuss our visual instruction tuning pipeline which is inspired from LLaVA v1.5 (Liu et al., 2024) but employs new attention-mapper in the projection layer and Qwen2.5-7B-Instruct (Qwen et al., 2025) as the base LLM. We also introduce MAPD, novel approach that integrates first-order meta-learning into the finetuning stage of this pipeline for prompt distillation. 2.1 Few-shot Visual Question Answering Visual Question Answering (VQA; Antol et al. 2015) is key task for evaluating the ability of vision-language models to understand images by accurately responding to questions about various aspects of visual content. These questions can vary widely, ranging from descriptions of objects inside bounding boxes (Krishna et al., 2017) to solving high-school geometry problems (Gao et al., 2025), but are mostly grounded in the visual information present in the image. In VQA, we typically have dataset = {(X v, i=1 where Xv I, Xq and Xa A, and is the set of all images, the set of all questions, and the set of all answers. Our goal is to learn function fθ parametrized by θ, that maximizes the likelihood of the answer given the q, a)}D 2We only focus on single-image few-shot VQA tasks and leave the multi-image scenario for future work. 3 aX v, i=1 pθ(X image and the question, (cid:81)D q). Following the standard train-test paradigm in deep learning, we evaluate whether fθ generalizes well by dividing dataset into (Dtrain, Dtest) such that maximizing the above likelihood on Dtrain also maximizes the likelihood of answer on Dtest. common assumption is that the size of Dtrain is large enough so that function fθ does not overfit on Dtrain. In the context of few-shot VQA, we treat the in-context examples (or shots) given to an LMM during ICL as Dtrain. Since the examples in Dtrain are few (as low as 1-shot), it becomes harder to avoid overfitting while training and still perform well on Dtest. We conceptualize this problem as one of learning about an underlying task represented by Dtrain and adopt meta-learning (Finn et al., 2017) which exploits the shared structure across distribution of tasks to learn prior over model parameters, thereby enabling stable transfer to new tasks with limited data. In the following, we describe how we enforce this prior over parameters through curation of meta-tasks containing few-shots, our proposed model architecture and training procedure based on meta-learning. 2.2 Improving Task Understanding with Meta-tasks The core idea of optimization-based meta-learning is to learn good initialization of meta-parameters, which when finetuned on specific task, enables stable transfer for that task with few gradient steps (Finn et al., 2017). To promote this capability, training involves processing batches of few-shot datasets that represent an underlying task. We refer to these few-shot datasets as meta-tasks and propose to create them from our finetuning data mixture based on the original LLaVA datasets. We provide details of our specific data mixture in Appendix A.1.1. More formally, let p(D) denote our data mixture. We create meta-task by randomly sampling fixed subset of examples from dataset Di p(D) and partitioning the examples further into support and query sets = {Dsupp, Dquery}. To be consistent with the notation introduced in Section 2.1, we treat the support set as Dsupp Dtrain and the query set as Dquery Dtest. We continue this process until all samples from Di have been assigned to at least one meta-task. This meta-task construction is performed for each dataset in p(D), resulting in meta-task distribution p(T meta). We now describe our model architecture designed to process these meta-tasks. 2.3 Model Architecture Figure 2 shows our model architecture which builds on the visual instruction tuning framework of LLaVA v1.5 (Liu et al., 2024). For clarity, we omit the distinction between support and query sets in this section as both are processed in the same manner. As shown in Figure 2, the model consists of pretrained CLIP ViT-L/14 visual encoder (gψ) with an aspect ratio of 336px; for an input image Xv, the encoder gives us hidden visual features Zv which are then passed to the projection layer that consists of an attention-mapper Mθ responsible for extracting useful features from Zv. Attention Mapper We re-design the projection layer of LLaVA v1.5 to include soft prompts by introducing an attention-mapper Mθ for improved task-specific feature extraction. Specifically, we prepend Zv with set of learnable prompt tokens to obtain sequence = (P, Zv) which is then passed to the attention-mapper (see Figure 2). Both prompt tokens and weights θ are initialized with Xavier Uniform initialization (Glorot and Bengio, 2010). We define the mapper as: Hp+v = Mθ(Q, K, ) = σ(QK ) (1) θ ; , the key is = θ , where the query is = θ ; , the value is = θ C, and their corresponding matrices are {M θ }. The mapper computes the dot product of the query and key vectors which are then passed to softmax function to compute activation scores for every feature in vector . Finally, we extract the first embeddings corresponding to the learnable prompt tokens from the set Hp+v that correspond to the task-specific image embeddings Hp. These are now passed to the LLM (fϕ) as prompts for further processing. We denote the trainable parameters for the attention-mapper with θp = {θ, }. θ , Language Model The quality of the learned prompts highly depends on the underlying language model. To this end, we employ the state-of-the-art Qwen2.5-7B-Instruct LLM, which has demonstrated strong performance on complex tasks such as mathematical reasoning and coding and supports the generation of up to 8K tokens. The LLM (fϕ) receives the concatenated sequence of image and text tokens to generate the answer Xa = fϕ([Hp, Hq]). In this pipeline, only the attention mapper parameters θp are trained, making our approach parameter-efficient for cross-task generalization. The 4 number of trainable parameters is approximately 24M (see Appendix A.1.2 for hyperparameters). The training objective maximizes the likelihood function, pθp (XaXv, Xq), parametrized by θp, where Xa is the answer, Xv is the image, and Xq is the question. For clarity, we refer to this model as LLaVA-ATT-Qwen2.5 in the following sections. 2.4 Model Training We train the attention mapper parameters to learn image-conditioned soft prompts in two stages following curriculum learning procedure similar to LLaVA v1.5 (Liu et al., 2023). In the first-stage, which is aimed at feature alignment, the attention-mapper is pretrained on the LCS-558K subset of the LAION/CC/SBU dataset filtered with more balanced concept coverage (Liu et al., 2023). Further details on pretraining are mentioned in Appendix A.1.3. In the second stage, which aims to distill task-specific image features into prompts Hp, the attention-mapper parameters θp are finetuned on diverse task-specific instructions. We describe our MAML-based finetuning procedure below and also introduce alternative methods which we compare against in our experiments. 2.4.1 Learning to Distill Prompts with First-order Meta Learning Our prompt distillation procedure, MAPD, uses the model-agnostic first-order approximation of MAML (Finn et al., 2017) which aims to learn robust initialization of meta-parameters that enable efficient adaptation to new tasks with just few gradient updates. We borrow the implementation of Antoniou et al. (2019) and use their first-order version and (learnable) per-step learning rates (α) to further optimize the training process. We sample batch of meta-tasks from p(T meta) and use the support set of each task to convert θp into task specific parameters θ with few gradient steps. Equations (2) and (3) show single step of this inner loop: Lsupp θp = 1 Dsupp Dsupp (cid:88) i=1 log(pθp (X aX v, q)) (2) = θp αθp Lsupp θ θp (3) The outer loop involves optimizing the meta-parameters which in our case are the original attentionmapper parameters θp on the query set using the task-specific parameters θ p: Lquery θ = 1 Dquery Dquery (cid:88) i=1 log(pθ (X aX v, q)) (4) θp := θp β (cid:88) j=1 θ p,j Lquery θ p,j (5) Equation (5) is the first-order approximation of the meta-update in MAML (Finn et al., 2017) that treats the gradient of θ p,j w.r.t. θp for meta task as constant. This approximation avoids backpropagating through the entire computation graph of the inner loop, which involves estimating the Hessian-vector product of the query loss, thereby saving huge GPU memory and still approximating gradient in the same direction as the true MAML gradient (Weng, 2018). We provide sketch of MAPD training in Figure 2 and more detailed algorithm in Appendix A.1.4 as Algorithm 1 2.4.2 Alternative Methods for Prompt Distillation We also implement other prompt distillation methods based on our model architecture to compare their performance with MAPD on few-shot VQA tasks. We provide more formal description of these methods below, highlighting important differences from our framework. Multi-Task Prompt Distillation We define multi-task baseline where we eliminate the bi-level optimization of MAPD. Specifically, at each iteration, we sample batch of meta-tasks from p(T meta) and optimize the following loss per task Lθp = 1 (cid:88) i=1 log(pθp (X aX v, q)) (6) such that = Dsupp + Dquery. This loss is accumulated across the entire batch of meta-tasks used to update θp. We refer to this baseline as Multi-TaskPD. In-Context Prompt Distillation Previous work (Chen et al., 2022; Min et al., 2022) suggests it is possible to meta-learn task information by reducing the bi-level optimization of MAML to sequence 5 prediction problem over in-context examples with the help of pretrained LLMs. We develop method called In-ContextPD, where we concatenate the support set with each query example in meta-task, and optimize the following loss function to distill this task information from LLMs into soft prompts: Lθp = 1 Dquery Dquery (cid:88) i=1 log(pθp (X aX v, q, Dsupp)) (7) Methods without Meta-tasks To further understand the benefit of curating meta-tasks (see Section 2.2), we compare with the original finetuning procedure of LLaVA-v1.5 7B but only θp without any meta-tasks during training. We refer to this method as NoMeta-taskPD in subsequent sections. We also compare with model averaging, which is computationally efficient and has been shown to increase performance on out-of-distribution datasets (Choshen et al., 2022; Wortsman et al., 2022).We separately finetune the attention-mapper on each dataset Di p(D), and take an average of all dataset-specific parameters θi weighted by their corresponding dataset size ratios: θavg = (cid:88) i=1 wi θi (8) where wi = Di / D. We refer to this baseline as Model-AvgPD in subsequent sections. 2.5 Test-Time Adaptation After learning optimal parameters with MAPD and alternative distillation strategies, we adapt the attention-mapper to new (test) task by finetuning for gradient steps. We experiment with range of values and explain how we select the best one for test task in Appendix A.2.2. Given steps, we finetune the parameters θp on the support set Dsupp test of test task test and evaluate model performance on the query set Dquery for the same task. Alternatively, we also compare with ICL adaptation at test-time for all methods. test"
        },
        {
            "title": "3 Experimental Results",
            "content": "3.1 Evaluation Datasets For evaluation purposes, our test datasets follow the same structure as the meta-tasks introduced in Section 2.2, with support and query examples. We use the recently introduced VL-ICL benchmark (Zong et al., 2025), designed to test the ICL capabilities of LMMs on various tasks like fast concept binding, multimodal reasoning, and fine-grained perception. Meta-tasks for testing are created by randomly sampling support set from the training split of the VL-ICL datasets and test/query set from their respective testing splits3. In line with our training pipeline, which exclusively utilizes datasets containing single image per example (see Section A.1.1), we focus solely on single imageto-text scenarios, leaving multi-image for future work. We report results on four tasks from VL-ICL: Fast Open MiniImageNet (Open-MI), where the model must name new objects based on few examples; Operator Induction, where the model must solve image tasks of the type 2 ? 7 =? given training examples like 1 ? 3 = 4; CLEVR Count Induction, where the model must count objects that satisfy given attributes like \"shape: sphere\"; and TextOCR, where the model must transcribe highlighted text contained in an image. We provide more details on these tasks in Appendix A.2.1. The final model performance is calculated as the average across all meta-tasks. 3.2 Model Comparisons Our results are summarized in Table 1, which compares MAPD against alternative prompt distillation methods (see Section 2.4.2) with LLaVA-ATT-Qwen2.5 as the base LMM. Table 1 compares two types of test-time adaptation methods, namely in-context learning (ICL) and finetuning (FT) which we further distinguish based on whether they use meta-tasks during training. In this section, we report results with up to eight shots but results on more shots are mentioned in Appendix A.2.5. 3We also keep separate validation set for each VL-ICL dataset (sampled from training) to select the best adapted model which we then evaluate on the test (query) set. More details can be found in Section A.2. 6 Table 1: Comparison of different prompt distillation approaches on single-image tasks from VL-ICL Bench (Zong et al., 2025). We report accuracy for different numbers of shots (S). \"Avg\" is only calculated for 1 shot(s). FT = Finetuning, ICL = In-Context Learning, TTA= Test-Time Adaptation. We use maximum of = 30 inner-loop gradient steps for FT adaptation (test-time). More details are mentioned in Appendix A.2.2. We do not compare on 0-shot results. The model used for this evaluation is LLaVA-ATT-Qwen2.5 which is described in Section 2.3. We also provide results for higher number of shots in Appendix A.2.5 and qualitative results in Appendix A.2.3 and A.2.4. Methods Meta Task TTA = ICL +NoMeta-taskPD +Model-AvgPD +In-ContextPD +Multi-TaskPD +MAPD TTA = FT +NoMeta-taskPD +Model-AvgPD +In-ContextPD +Multi-TaskPD +MAPD Methods Meta Task TTA = ICL +NoMeta-taskPD +Model-AvgPD +In-ContextPD +Multi-TaskPD +MAPD TTA = FT +NoMeta-taskPD +Model-AvgPD +In-ContextPD +Multi-TaskPD +MAPD Open-MI (2-way) Operator Induction 0-S 1S 2S 4S 5-S Avg 0-S 1S 2S 4S 8S Avg 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 35.0 20.0 30.0 43.0 42.5 21.5 28.5 35.5 37.0 43. 47.0 22.0 56.0 50.0 53.0 67.5 53.5 54.5 73.5 78.0 48.0 30.0 55.0 51.0 57.0 89.0 83.0 79.5 93.5 94.5 45.0 34.5 63.5 50.5 60.5 94.0 87.5 88.5 94.5 95. 43.8 26.6 51.1 48.6 53.3 68.0 63.1 64.5 74.6 77.9 11.7 8.3 10.0 8.3 15.0 11.7 8.3 10.0 8.3 15.0 13.3 11.7 20.0 13.3 13.3 26.7 31.5 21.7 31.0 32. 13.3 6.7 18.5 11.7 13.3 23.3 28.0 18.3 28.3 38.3 10.0 8.3 18.0 3.3 1.7 46.7 45.0 41.7 61.0 58.3 11.7 10.0 26.0 11.7 10.0 58.3 55.5 41.7 60.0 62. 12.1 9.2 20.6 10.0 9.6 38.8 40.0 30.9 45.1 47.7 CLEVR Count Induction TextOCR 0S 1S 2S 4S 8-S Avg 0-S 1S 2S 4S 8S Avg 0.0 1.5 0.0 1.0 2.0 0.0 1.5 0.5 0.0 0.0 8.0 17.0 13.5 5.0 11. 18.5 26.5 24.5 25.0 26.5 10.5 8.5 23.0 9.0 7.0 21.5 25.0 30 25.5 27.5 23.0 4.0 28.5 16.5 15.5 26.0 29.5 34.5 31.0 31.0 30.5 1.0 31.5 19.5 15. 37.0 35.5 34.5 38.0 40.5 18.0 7.6 24.1 12.5 12.3 25.8 29.1 30.9 29.9 31.4 20.0 12.0 16.0 18.0 21.5 20.0 12.0 16.0 18.0 21.5 4.5 3.0 22.5 4.0 5. 20.5 17.5 16.0 21.0 23.5 9.5 2.5 21.0 4.5 7.0 23.0 20.0 18.0 20.5 26.5 8.5 3.0 23.5 8.5 8.0 24.0 23.0 19.5 24.5 27.0 4.5 1.0 28.0 10.5 8. 22.5 25.5 22.0 25.5 28.5 6.8 2.8 23.8 6.9 7.3 22.5 21.5 18.9 22.9 26.4 Prompt distillation improves task induction in LMMs at test-time. Our results in Table 1 show that FT adaptation with few-shots (support examples) largely outperforms ICL at test time (query examples), with an average increase of 21.2% over all datasets. These results highly support our hypothesis that distilling task-specific information from image embeddings to create targeted prompts improves the few-shot capabilities of the underlying LLM (in our case Qwen-2.5-7B-Instruct). Additionally, our results show that finetuning just the attention-mapper parameters with few gradient steps at test-time does not lead to overfitting and can promote cross-task generalization with the right set of hyperparameters (Appendix A.2.2). For one-to-one comparison, we look into In-contextPD, which performs better with FT on 3 out of 4 tasks compared to its ICL adaptation. The ICL adaptation prompts the underlying LLM with sequence of few-shot examples but still underperforms FT that instead prompts with fixed set of learned task-specific embeddings Learning using meta-tasks is beneficial for few-shot adaptation. We next compare methods that train using meta-tasks to those that do not. As can be seen in Table 1, for both test-time adaptation procedures (ICL and FT), methods which are meta-task aware are indeed superior. For ICL-based adaptation, In-ContextPD performs best, while for FT-based adaptation, our proposed approach, MAPD, achieves the best overall performance across all four datasets. This suggests that learning meta-tasks during training by creating batches with equal examples per task avoids overfitting to single task which is beneficial for cross-task generalization. Meta-learning improves few-shot learning for FT-based adaptation. Table 1 further shows that our proposed meta-learning method, MAPD, achieves the best performance when finetuned at test-time across all datasets. This suggests that first-order MAML learns the best initialization of 7 1-Shot(M) 1-shot(I) 2-Shot(M) 2-shot(I) 4-Shot(M) 4-shot(I) 5-Shot(M) 5-shot(I) NoMeta-taskPD In-contextPD Model-AvgPD Multi-TaskPD MAPD 60 55 50 45 35 30 25 20 r A M 2 4 6 8 Number of Soft Prompts or (shown in log2(P )) 89. 82.5 83.9 91.5 85.6 86.5 75. 77.8 76.9 77.8 64.4 55.5 63. 59.2 65.1 ) % ( c e 100 80 60 40 36. 44.1 39.8 37.3 27.2 20 Combined (EM) Task Induction (EM) Perception (EM) Math Reasoning (Qwen-VL Score) Figure 3: (a) Left: Performance comparison between M=MAPD+FT and I=In-ContextPD+ICL. Mean Accuracy is computed across all VL-ICL datasets. We consider different prompt token lengths = {4, 16, 64, 256} which are shown in log2(P ) scale for different shots. (b) Right: Performance of different prompt distillation methods on three Operator Induction subtasks: Task Induction, Perception, and Math Reasoning. We report mean exact-match (EM; %) for 1,2 and 8-shots as defined in VL-ICL Bench (Zong et al., 2025) except for Mathematical Reasoning, which uses mean ratings generated by Qwen-2.5-VL-32B-Instruct. More details in Appendix A.2.4 attention-mapper parameters θp. These parameters are subsequently adapted for test task with few gradient steps and few-shot examples to produce precise set of soft prompts that improves LMM predictions on that task. We see MAPD being most effective in the 2-shot case for Operator Induction, surpassing Multi-TaskPD by 10% ; and on average, it is also better than Multi-TaskPD by 3.5% on the TextOCR dataset. Finally, MAPD with FT is the only approach that exhibits strictly monotonic improvements as the number of shots increases, showing better scaling behavior. 3.3 Ablation Studies and Analysis In this section, we perform ablation studies using test-time finetuning (FT)unless otherwise specifiedas our adaptation strategy to understand MAPD in greater depth. MAPD benefits from the addition of soft prompts in contrast to In-contextPD. We compare MAPD (the best FT approach) against In-ContextPD (the best ICL approach) across all VL-ICL datasets, as the number of soft prompts increases (under different shot scenarios). Figure 3(a) shows that MAPD scales favorably with additional prompts. Furthermore, its marginal improvement per added prompt token is substantially greater when more shots are provided. In contrast, the performance of In-ContextPD generally deteriorates with more prompts, except for 1-shot. MAPD takes advantage of the additional shots that offer more consistent task information coming from gradient updates, whereas ICL struggles to jointly attend to more examples and longer prompts. MAPD is best in mathematical task induction, perception and reasoning. We focus on the Operator Induction task and try to understand the steps involved in solving this problem. Briefly, the task involves figuring out the correct mathematical operation between two numbers in the image from few-shot examples and use it to calculate the answer for test/query example (See Figure 6). To solve this induction problem, the model should be able to (a) identify the operands in the query (Perception); (b) identify the operation from few-shots (Task Induction); and (c) using the knowledge derived from (a) and (b) follow appropriate steps to calculate the answer (Mathematical Reasoning). To isolate these subtasks, we design specific questions and prompt LLaVA-ATT-Qwen2.5 to answer them instead of calculating the original result of the mathematical operation. For the first two subtasks, we simply identify the operands and the operation via exact match. For Mathematical Reasoning, we found that it was not possible to obtain sufficiently long answers, perhaps because at test time, the models adapt to single answer token, which in turn affects their answering style. To alleviate this issue, we first curated handful of few-shot examples sampled from the original dataset and modified answers with detailed reasoning steps. Finetuning on this few-shot data allowed the models to provide sufficient reasoning during test-time adaptation. To further evaluate responses for math reasoning, we utilized Qwen-2.5-VL-32B-Instruct as judge to score the model responses using 03 scale (where 3 indicates the answer is correct). We describe our prompts and scoring method with some examples 8 Original (Random) Same Attribute Same Pair Original (Random) Same Attribute Same Pair 33.2 34.2 32.9 34.1 34.6 29. 30.9 29.9 35.8 34.9 35.4 35. 31.4 25.8 40 30 20 ) % ( r A M 30 20 18 10 ) % ( r A M 28.4 26.5 30. 28.8 24.1 23.8 24.3 21.9 13.4 11.8 7.6 12.5 12.3 0 NoMetaTaskPD Model-AvgPD In-ContextPD Multi-TaskPD MAPD 0 NoMetaTaskPD Model-AvgPD In-ContextPD Multi-TaskPD MAPD Figure 4: (a) Performance comparison of different prompt distillation approaches on the CLEVR Count Induction (details in Appendix A.2.1). Few-shot examples for Same Attribute and Same Pair are selected based on their attribute-value similarity with the query (test) example. Mean Accuracy is computed for 1,2,4 and 8 shots. Left: Finetuning (FT) based Test-time Adaptation. (b) Right: In-Context Learning (ICL) based Test-time Adaptation. Table 2: Robustness of prompt distillation methods against image perturbations on the Fast OpenEnded MiniImageNet dataset (2-way classification). We report accuracy scores as defined in VL-ICL Bench (Zong et al., 2025) across 2, and 5 shots. Original Cropping Rotation Gaussian Blur Color Jitter CutMix MixUp NoMeta-taskPD Model-AvgPD In-ContextPD Multi-taskPD MAPD 2S 67. 65.0 67.0 67.5 66.5 58.5 58.0 5S 94.0 94.0 91.0 92.5 92.5 86.0 84.0 2S 53. 51.5 50.5 51.5 50.5 45.5 46.0 5S 87.5 87.5 81.5 84.5 89.0 70.5 70.5 2S 54. 51.5 50.5 49.5 49.5 49.0 48.0 5-S 88.5 83.0 83.5 78.0 81.5 75.0 75.5 2S 73. 72.0 72.5 71.5 71.5 72.0 69.0 5S 94.5 91.5 93.5 92.5 94.0 92.0 89.0 2S 78. 76.5 78.0 77.5 77.0 75.5 76.5 5S 95.5 95.0 95.5 96.0 94.0 92.5 91.0 Mean Drop in Accuracy 3. 4.0 4.3 6.9 4.8 9.1 2.1 2.4 1.2 1.4 Net Mean Drop across Shots 3.9 5.6 7. 2.3 1.3 in Appendix A.2.4. In Figure 3(b), we show the performance of prompt distillation methods with test-time finetuning on the three subtasks and overall. We only report mean scores across 1, 2, and 8 shots (Zong et al., 2025). We observe that MAPD outperforms other prompt distillation approaches across subtasks. This suggests that our approach can quickly adapt towards variety of problem types and understands different components of task efficiently. This holds promise for MAPD to be applied in complex scenarios with multiple underlying components and reasoning steps. We further note that methods trained on meta-tasks generally perform better, except In-ContextPD which struggles at task induction and reasoning indicating its overall lower performance. MAPD and other prompt distillation approaches benefit from few-shot examples that are similar to query (test) example We further assess how performance varies for different prompt distillation approaches based on the selection of few-shot examples on the CLEVR Count Induction task (details in Appendix A.2.1). We propose two selection methods based on similarity of attributes and their corresponding values for every query (test) example. If the query has attribute and value as shape: sphere, we select the few-shot examples based on - a) Same Attribute - shape, (b) Same Pair - shape: sphere and compare both of them with the original setup as proposed in the VL-ICL benchmark (Zong et al., 2025) which retrieves the few-shot examples randomly. In Figure 4(a), we first see that for finetuning-based (FT) adaptation, the performance of all the baselines increases by 4.8% for Same Attribute and 5.3% for Same Pair on an average. MAPD performs best in the Same Attribute setting (Mean Acc = 35.4%) and Multi-TaskPD performing best in the Same Pair setting (Mean Acc = 35.8%). In Figure 4(b), we see that for In-Context Learning (ICL) adaptation, the similarity-based few-shot selection methods have greater impact in performance and improve the mean accuracy of all the baselines by 7.7% for Same Attribute and 8.6% for Same Pair on an average. In-ContextPD performs the best in both Same Attribute and Same Pair settings with mean 9 accuracies of 28.8% and 30.5% respectively for ICL adaptation. We also notice that the Same Pair setup is generally the best few-shot selection method giving best performance for all the approaches. This indicates that choosing few-shot examples that are similar to query example induces better task understanding during test-time adaptation. We also see that the selection of few-shot examples shows less variance with FT adaptation compared to ICL adaptation, thereby showing higher robustness of FT adaptation. MAPD is most robust to image perturbations in fast concept binding tasks. We assess if our prompt distillation methods are robust enough to handle perturbations applied to the images in the support set as shown in Table 2. We see that our method, MAPD, is most robust even in the presence of noise in the support examples as compared to other distillation methods that suffer huge drop in performance. Advanced techniques like CutMix (Yun et al., 2019) and MixUp (Zhang et al., 2018) change the original image distribution substantially, affecting all methods to greater degree but MAPD is still close to its original performance for both 2 and 5 shots. This robustness likely stems from MAPDs meta-learned initialization, which learns the underlying task structure from meta-tasks without over-fitting to any other spurious visual patterns and this allows it to adapt quickly to newer tasks without being influenced by noisy visual artifacts in the examples."
        },
        {
            "title": "4 Limitations",
            "content": "In this work we only focus on single-image few-shot VQA tasks and relatively simple math problems. Future work could explore extensions to tasks involving multiple images and harder reasoning problems across images or within single image. MAPD is most effective with finetuning (FT) based adaptation. As result, it requires substantially more compute at test time compared to ICL which does not perform any gradient updates. We further show in Figure 23 how computation increases with the number of shots. MAPD may not be the method of choice for extremely resource-constrained environments or rapid on-device adaptation."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduced Meta-Adaptive Prompt Distillation (MAPD), novel meta-learning approach that induces few-shot capabilities in LMMs. MAPD employs fixed set of soft prompts, distilled from task-relevant image features which can be efficiently adapted at test time using only few examples. key component of our method is an attention-mapper module, which we integrate with LLaVA v1.5 and jointly learn with the soft prompts to facilitate distillation. Extensive evaluation on the VL-ICL benchmark shows that MAPD consistently outperforms traditional ICL and other prompt tuning approaches across diverse range of VQA tasks, substantially enhancing cross-task generalization even when images are noisy and exhibits strictly monotonic improvements with an increasing number of shots."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433. Antreas Antoniou, Harrison Edwards, and Amos Storkey. 2019. How to train your MAML. In International Conference on Learning Representations. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language In Proceedings of the 60th Annual Meeting of the Association for model in-context tuning. 10 Computational Linguistics (Volume 1: Long Papers), pages 719730, Dublin, Ireland. Association for Computational Linguistics. Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. 2022. Fusing finetuned models for better pretraining. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane Wang, and Eric Schulz. 2023. Meta-in-context learning in large language models. In Thirty-seventh Conference on Neural Information Processing Systems. Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 11261135. PMLR. Chelsea Finn. 2018. Learning to learn with gradients. University of California, Berkeley. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing HONG, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. 2025. G-LLaVA: Solving geometric problem with multi-modal large language model. In The Thirteenth International Conference on Learning Representations. Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249256. JMLR Workshop and Conference Proceedings. Thomas Griffiths, Frederick Callaway, Michael Chang, Erin Grant, Paul Krueger, and Falk Lieder. 2019. Doing more with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, 29:2430. Artificial Intelligence. Roee Hendel, Mor Geva, and Amir Globerson. 2023. In-context learning creates task vectors. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 93189333, Singapore. Association for Computational Linguistics. Brandon Huang, Chancharik Mitra, Leonid Karlinsky, Assaf Arbelle, Trevor Darrell, and Roei Herzig. 2024. Multimodal task vectors enable many-shot multimodal in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. 2017. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910. Louis Kirsch and Jürgen Schmidhuber. 2022. Self-referential meta learning. In First Conference on Automated Machine Learning (Late-Breaking Workshop). Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):3273. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. 2024. What matters when building vision-language models? Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: multi-modal model with in-context instruction tuning. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2025. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research. 11 Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian, Tat-Seng Chua, and Yueting Zhuang. 2023b. Gradient-regulated meta-prompt learning for generalizable vision-language models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 25512562. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 27912809, Seattle, United States. Association for Computational Linguistics. Ivona Najdenkoska, Xiantong Zhen, and Marcel Worring. 2023. Meta learning to bridge vision and language models for multimodal few-shot learning. In The Eleventh International Conference on Learning Representations. Chengwei Qin, Shafiq Joty, Qian Li, and Ruochen Zhao. 2023. Learning to initialize: Can meta learning improve cross-task generalization in prompt tuning? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11802 11832, Toronto, Canada. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Sachin Ravi and Hugo Larochelle. 2017. Optimization as model for few-shot learning. In International Conference on Learning Representations. ShareGPT. 2023. Sharegpt. https://sharegpt.com/. Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. 2021. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88028812. Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. In Advances in Neural Information Processing Systems, volume 34, pages 200212. Curran Associates, Inc. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. Matching networks for one shot learning. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS16, page 36373645, Red Hook, NY, USA. Curran Associates Inc. Lilian Weng. 2018. Meta-learning: Learning to learn fast. https://lilianweng.github.io/ posts/2018-11-30-meta-learning/. 12 Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. 2019. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 60226031. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations. Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. 2024. MMICL: Empowering vision-language model In The Twelfth International Conference on Learning with multi-modal in-context learning. Representations. Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. 2025. VL-ICL bench: The devil in the details of multimodal in-context learning. In The Thirteenth International Conference on Learning Representations."
        },
        {
            "title": "A Appendix",
            "content": "1. Section A.1: Implementation Details (a) Section A.1.1 Finetuning Data Mixture (b) Section A.1.2 Model Configurations (c) Section A.1.3 Training Details (d) Section A.1.4 Psuedo Algorithm of MAPD 2. Section A.2 Evaluation (a) Section A.2.1 Evaluation Datasets from VL-ICL Bench (b) Section A.2.2 Test-Time Adaptation Details (c) Section A.2.3 Qualitative Results (d) Section A.2.4 Details on Ablation Study for Operator Induction (e) Section A.2.5 Scaling to More Shots 3. Section A.3 Performance Comparison with other LLaVA Models 4. Section A.4 Test-time Computation Overhead 5. Section A.5 Broader Impact A.1 Implementation Details A.1.1 Finetuning Data Mixture For model finetuning, we create our multi-task data mixture using the visual instruction tuning data of LLaVA v1.5 (Liu et al., 2023) which contains mixture of 12 different datasets4 ranging from long conversations to academic multiple-choice questions. Since we are only training image-based prompts, we remove the language-only ShareGPT-40K dataset (ShareGPT, 2023). Additionally, we include 3 different math reasoning/QA datasets from the LLaVA OneVision data mixture (Li et al., 2025) which are known to improve LMM performance on difficult reasoning and logical QA tasks (Lu et al., 2024). We further get rid of the extra answer formatting instructions to test the true few-shot transfer learning ability of our approach without the need of external task induction. Table 3 shows the list of all the datasets along with their size and question types. Table 3: Finetuning Data Mixture Statistics"
        },
        {
            "title": "Dataset",
            "content": "No. of examples Question Types LLaVA-Instruct 157,712 Conversations (57,669) Detailed Image Description (23,240) Complex Reasoning (76,803) GQA OCR-VQA TextVQA Visual Genome 72,140 Visual Reasoning 80,000 21, 86,417 Image Question Answering with Reading Comprehension Image Question Answering with Reading Comprehension Image Question Answering and Bounding Box Prediction Visual Math Question Answering MAVIS-Math-Metagen 87,348 TabMWP-Cauldron 22,717 Tabular Math Reasoning RefCOCO OKVQA VQAv2 A-OKVQA Geo-170k (QA) Total 48,447 8, Image Question Answering and Bounding Box Prediction Knowledge Grounded Image Question Answering 82,783 Image Question Answering Multiple-Choice Question Answering Math Question Answering and Reasoning 66,160 67,823 802,498 A.1.2 Model Configurations Models We use the publicly available implementation of LLaVA v1.55 and first-order MAML6 to implement our baselines. Additionally, we use the pretrained model weights from Huggingface for Qwen2.5-7B-Instruct LLM7 and the CLIP ViT-L/14-336px visual encoder8. The output embedding 4We use this dataset only for academic research purposes as mentioned by the original authors and follow the Open AI Usage Policy for GPT-4 generated datasets. Additionally, we conform to the license (CC-BY-4.0) for Cauldron datasets. 5LLaVA v1.5: https://github.com/haotian-liu/LLaVA/tree/main/llava 6How to train your MAML: https://github.com/AntreasAntoniou/HowToTrainYourMAMLPytorch 7Qwen2.5-7B-Instruct: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 8CLIP ViT-L/14-336px: https://huggingface.co/openai/clip-vit-large-patch14-336 15 dimension size of CLIP is 1,024 and the input word embedding size of the Qwen LLM is 3,584. We set the training context length as 4096 for all baselines except for in-context baseline where it is 8,192 as it requires training with longer sequences. The attention-mapper is single multi-head attention block with 8 heads. The token length of the soft prompt as described in Section 2.3 for the attention mapper is set to = 256. The total number of trainable parameters for our model is approximately 24M making our approach significantly parameter-efficient for finetuning. A.1.3 Training Details Pretraining stage During the pretraining stage, we only train the attention-mapper and soft prompts for 4 epochs and use learning rate of 2e-3 with batch size of 64 per GPU. We perform trainvalidation split on the LCS-558K dataset (Liu et al., 2023) by keeping 98% of the examples for training and 2% for validation and take the checkpoint with the lowest validation loss. We use this checkpoint as our base for further task-specific finetuning. Finetuning stage For finetuning, in order to keep balanced ratio of train-validation splits across multiple datasets in Section A.1.1 used in this stage, we divide each dataset into 98% for training and 2% for validation and then combine them separately to create the final train and validation splits. We experimented among three different learning rates [1e-3, 5e-4, 2e-5]. For MAPD, we further experimented with three different inner-loop learning rates [1e-1, 5e-2, 5e-1]. Below, we mention the best learning rates along with other hyperparameters, chosen using our validation set for the different approaches proposed in Section 2.4. All approaches were finetuned for 1 epoch to ensure complete pass over the entire finetuning data mixture. 1. MAPD: We use 5 inner-loop steps and initialize the inner-loop learning rate α=1e-1. The outer-loop learning rate is set as 1e-3 with per GPU batch size of 1 meta-task with gradient accumulation of 2 steps. Each meta-task here contains 10 support and 10 query examples. Training time 10 hours. 2. Multi-TaskPD: Similar to MAPD, we use learning rate of 1e-3 with per GPU batch size of 1 meta-task with gradient accumulation of 4 steps. Each meta-task here contains 5 support and 5 query examples. Training time 4.5 hours 3. In-ContextPD: We use learning rate of 1e-3 with gradient accumulation of 4 steps and 5 meta tasks per GPU. Each meta task contains 10 support examples and 1 query example. The support examples were concatenated with the strategy that ensured all image tokens of meta-task are present in the sequence and we truncate the text tokens if the sequence exceeded the context length of 8192. Further, the few-shot question and answers were concatenated by inserting \"Question:\" and \"Answer:\" strings in between them, inspired from (Alayrac et al., 2022). Training time 4.5 hours 4. ModelAvgPD: We first finetune individual models on each dataset in the finetuning data mixture (Section A.1.1) with learning rate of 5e-4. For all the datasets, we choose per GPU batch size of 8 with gradient accumulation of 2 steps. Average time per dataset 3 hours 5. NoMeta-taskPD: Here, we finetune on the complete data mixture in one training run by sampling batches randomly and again use per GPU batch size of 8 with gradient accumulation of 2 steps. We also use learning rate of 5e-4. Training time 4 hours. Computational Requirements For the entire model training, we use 4 H200 GPUs with VRAM of 143GB per GPU. For both the stages, the hyperparameters were tuned using their corresponding validation sets and we choose the checkpoints at the end of first epoch to report our results. A.1.4 Pseudo algorithm for MAPD We highlight our full MAPD algorithm based on FoMAML in detail with inner and outer loop that is used to train the attention-mapper parameters θp in Algorithm 1. 16 Algorithm 1: Meta-Adaptive Prompt Distillation (MAPD) Input: Meta-Task distribution p(T meta), inner-loop learning rate α, meta learning rate β Output: Meta-parameters θp = {θ, } Initialize θp with Xavier Uniform Initialization; while not converged do Sample batch of meta-tasks {Tj}N foreach task Tj = {Dsupp j=1 p(T meta); } in batch do , Dquery Dsupp (cid:88) = Evaluate Lsupp θp,j 1 Dsupp Adapt parameters with gradient steps: for = 1, . . . , do i= log(pθp,j (X aX v, q)); p,j θk1 θk p,j αθk1 p,j Lsupp θk1 p,j Evaluate Lquery θK p,j 1 Dquery First-Order Meta-Update: = Dquery (cid:88) i=1 log(pθK p,j (X aX v, q)); θp θp β (cid:88) j= θK p,j Lquery θK p,j Table 4: Evaluation Dataset Statistics Dataset Task Category Train Set (Support) Test Set (Query) Size (GB) Fast Open-MiniImageNet (OPEN_MI) CLEVR Count Induction Operator Induction TextOCR Fast-Concept Binding 5,000 Fine-Grained Perception, Task Induction Perception, Task Induction Mathematical Reasoning Perception, Task Induction 800 80 800 200 200 200 0.18 0.18 0.01 0.01 A.2 Evaluation Details A.2.1 Evaluation Datasets from VL-ICL Bench The VL-ICL Bench Zong et al. (2025) includes diverse variety of tasks to test different capabilities of models like Fast-Concept binding, Mathematical Induction, and Fine-grained perception. Given the nature of our model architecture and training (Section 2), we only focus on the single-image Image-to-text (I2T) tasks. Table 4 shows the dataset statistics. We also give brief descriptions of these tasks below along with some examples for better understanding. 1. Fast Open-Ended MiniImageNet (OPEN_MI) - This is variant of the MiniImageNet few-shot object recognition task (Vinyals et al., 2016), which was repurposed for few-shot prompting (Tsimpoukelli et al., 2021). It is essentially an open-ended image classfication problem, but contains nonsense categorical names like dax or blicket making the test performance not influenced by the prior knowledge of an LMM but only dependent on the support examples. This design ensures to test the few-shot abilities of LMMs and how quickly they can learn about new concepts. For the results shown in Table 1, we use the 2-way version of this task involving classification between two nonsense categories. An example of 2-way 1-shot task is shown in Figure 5. 17 Figure 5: 2-way Fast Open-Ended MiniImageNet Figure 6: Operator Induction 2. Operator Induction - Initially proposed by (Zong et al., 2025), this dataset tests various capabilties of LMMs like Task Induction, Perception and Mathematical Reasoning. The support examples involve two operands with missing mathematical operation and an answer. When testing, the task is to identify the hidden operation from the support example and use it to calculate the result over the operands in the query. An example of 2-shot task is shown in Figure 6. 3. CLEVR Count Induction - This dataset contains images from the widely used CLEVR dataset (Johnson et al., 2017) where each image contains set of objects that have certain characteristics based on attributes like shape, size, color and material. The task is to learn to count the objects of the given attribute in the support example and transfer that knowledge to count the objects of any attribute in the query example. An example of 2-shot task is shown in Figure 7. 4. TextOCR - This dataset has been repurposed by (Zong et al., 2025) from the TextOCR dataset (Singh et al., 2021) to create task where the LMM should learn to output the text within red bounding box from the support examples. Even though this task could be solved in zero-shot setting as we see in the 0-shot case with detailed prompt, we still only focus on inducing task knowledge from the few-shot examples. An example of 2-shot task is shown in Figure 8. 18 Figure 7: CLEVR Count Induction Figure 8: TextOCR A.2.2 Test-Time Adaptation Details We choose similar test-time adaptation procedure as (Qin et al., 2023) to find the best hyperparameter settings for every prompt distillation method for fair comparison. We first sample 10% of the examples from the training split of each test task and combine them to create validation set. After meta-task creation of VL-ICL datasets (Zong et al., 2025) using the remaining training and test splits, we then performed maximum of = 30 inner-loop steps over each support set of meta-task and chose the Kth-step model that gave the lowest validation loss. We use this model to calculate the result over the query set. To further show how the performance varies at different gradient steps, we plot the average test accuracy curves for different VL-ICL datasets for MAPD for different shots in Figure 9. We see that the accuracies converge or start decreasing under 30 gradient steps which validates our adaptation procedure designed to achieve best performance. We have also provided examples of how the predictions change during test-time adaptation in Figure 10, Figure 11, Figure 12 and Figure 13 Further to ensure reproducibility, we provide our best learning rate values in Table 5 used for different methods based on the validation set after doing hyperparameter search within the range [0.1, 1.0] with batch size of 1 meta-task. 19 (a) OPEN_MI test performance (b) Operator Induction test performance (c) CLEVR test performance (d) TEXTOCR test performance Figure 9: Average test performances of MAPD with finetuning on different datasets Table 5: Learning rates for finetuning-based (FT) test-time adaptation for results shown in Table 1 Training Methods Learning Rate (LR) MAPD Multi-TaskPD In-ContextPD ModelAvgPD NoMeta-taskPD 1.0 0.8 0.8 0.6 1.0 A.2.3 Qualitative Results Figure 10: OPEN_MI predictions at test-time Figure 11: CLEVR predictions at test-time Figure 12: TEXTOCR predictions at test-time Figure 13: Operator Induction predictions at test-time A.2.4 Details on Ablation Study for Operator Induction We break down the ablation study on operator induction tasks (Section 3.3) into 3 components: 1) Task Induction, 2) Perception, and 3) Mathematical Reasoning. We test these components separately with the help of suitable prompts for our LMM to answer questions in specific formats. Figure 14 shows our prompts used for different components. Task Induction - \"What mathematical operation should be used in this example? Strictly answer in one word.\" Perception - \"What are the numbers in this example? Do not calculate the answer after applying mathematical operation. Only give the numbers shown in the example. Stricly give numbers in numeric digits and your result should be in the format > Number A: xxx Number B: xxx.\" Mathematical Reasoning - \"Think step-by-step and give proper reasoning steps first and then given your final answer. The format should be > Reasoning: xxx Answer: xxx . The Reasoning part should contain reasons to derive the answer and the Answer part should only contain the answer. Your response should strictly follow this format and not just give the answer of the mathematical operation. Its important that you give reasoning before you answer.\" Figure 14: (Operator Induction Task) Prompts to the LMM for generating answers in specific formats suited for evaluation. We list out few examples which we curate for mathematical reasoning in the Operator Induction task to enhance mathematical reasoning. Each image in the dataset contains set of 2 numbers or operands and hidden mathematical operation. The result of the correct mathematical operation is also provided for the support set examples. The task is to induce the mathematical operation used in the support set to calculate the answer of the query image containing two new operands. As finetuning on single answer token limits the token generation capacity of the LMM, we further modify the support set examples to list out detailed mathematical steps before calculating the answer. Finetuning on this reasoning data improves both the generation capacity and reasoning ability of the LMM. We further provide few examples of this hand-curated data in Figure 15. Original Answer: 10 Detailed Answer: There are two numbers, 5 and 1. Performing some mathematical operation gives the answer 4. So if we think about adding the numbers, 5 + 1 = 6, subtracting them, 5 1 = 4, multiplying them, 5 1 = 5. This implies that the hidden operation must be subtraction () and the result is 4. Original Answer: 21 Detailed Answer: There are two numbers, 3 and 7. Performing some mathematical operation gives the answer 21. So if we think about multiplying the numbers, 3 7 = 21, adding the numbers, 3 + 7 = 10, subtracting the numbers, 3 7 = 4. This implies that the hidden operation must be multiplication or and the result is 21. Figure 15: (Operator Induction Math Reasoning) Few examples of our hand-curated data with mathematical reasoning steps. We used Qwen2.5VL-32B-Instruct (Qwen et al., 2025) as judge for evaluating the Mathematical Reasoning component of the problem where LMMs responded with detailed reasoning steps before the answer. Evaluation of responses was done by prompting the judge to score response between 22 03 based on if it thinks the reasoning and answer are correct. We then calculated mean score as the percentage of total score assigned by the Qwen-2.5-VL (Judge) to the responses relative to the maximum possible score. Mean Percent Score = (cid:80)N i=1 Si 3 (9) where Si is the score assigned by Qwen2.5-VL for the ith response and is the total number of responses. We provide the prompt to the judge for this evaluation in Figure 16. Judge Prompt - \"You are given few in-context examples of mathematical induction problem. The in-context examples each have an image with two numbers and ? which is supposed to be some mathematical operation. You are given solution that gives the answer and the reasoning on how to calculate that answer using some mathematical operation applied on those two numbers in the image. The task is to induce the correct mathematical operation from the given examples, and use that operation to calculate the result of query image with different numbers. After this, you are then given reference answer written by experts and candidate response. The candidate response is in format Reasoning: xxx Answer: xxx . The reasoning part contains reasoning about how the candidate arrived at the solution, and the Answer part contains their final answer. Your task is to judge if the reasoning and the answer of the candidate response are correct or not after considering the in-context examples, query image, question, reference answer, and your own reasoning of the mathematical induction problem. The rating should be done on scale of 03, where 0 indicates when the response is ambiguous or does not follow the format, 1 is for when both the reasoning and answer are wrong, 2 is for when either only reasoning or answer is correct, 3 is for when both the reasoning and answer are correct. Be strict in your judgement and do not give higher rating unless the candidate response contains accurate reasoning and exact answer. Thorougly check each and every part of the candidate response and make sure it does not contain extra irrelevant operations or answers. If it does then give lower rating accordingly. The candidate response should follow the format and conclude with the correct answer. If it does not, that means their answer is wrong. Also give your rationale before rating. Give the final rating as > Rating: xxx\" Figure 16: (Operator Induction Math Reasoning) Prompts for the Qwen2.5VL-32B-Instruct to evaluate LMM responses on scale of 03. It is given 1 to 4 in-context examples for understanding the mathematical induction task before the LMM (candidate) response for better evaluation. We also provide few examples of LMM predictions for task induction (Figure 17) and perception (18) and mathematical reasoning (Example 1: Figure 19, 20 and Example 2: Figure 21, 22) 23 Example (Task Induction): Support shot 1 Question: What is the result of the following mathematical expression? Answer: 42 Example (Task Induction): Support shot Question: What is the result of the following mathematical expression? Answer: 0 Example (Task Induction): Query Question: What mathematical operation should be used in this example? Strictly answer in one word. LMM prediction: Multiplication Figure 17: (Operator Induction Task Induction) An example of 2-shot task induction for multiplication operation 24 Example (Perception): Support shot Question: What is the result of the following mathematical expression? Answer: 0 Example (Perception): Support shot 2 Question: What is the result of the following mathematical expression? Answer: -2 Example (Perception): Query Question: What are the numbers in this example? Do not calculate the answer after applying mathematical operation. Only give the numbers shown in the example. Stricly give numbers in numeric digits and your result should be in the format > Number A: xxx Number B: xxx. LMM prediction: Number A: 5 Number B: 4 Figure 18: (Operator Induction Perception) An example of 2-shot perception task to detect operands 25 Example 1: Support Question: What is the result of the following mathematical expression? Answer: There are two numbers, 1 and 8. Performing some mathematical operation gives the answer 8. So if we think about subtracting the numbers, 1 8 = 7, multiplying the numbers, 1 8 = 8, adding the numbers, 1 + 8 = 9. This implies that the hidden operation must be multiplication or and the result is 8. Example1: Query Question: What is the result of the following mathematical expression? Think step-by-step and give proper reasoning steps first and then given your final answer. The format should be > Reasoning: xxx Answer: xxx .The Reasoning part should contain reasons to derive the answer and the Answer part should only contain the answer. Your response should stricly follow this format and not just give the answer of the mathematical operation. Its important that you give reasoning before you answer. LMM prediction: There are two numbers, 6 and 2. We need to find the result of an operation between them. The question is asking for the result of the operation, which is 12. So the answer is Reasoning: The two numbers are 6 and 2. The operation between them is multiplication. 6 2 = 12 Answer: 12 Figure 19: (Operator Induction Math Reasoning) An example of 1-shot mathematical reasoning task with Judge Rating: 3 (shown in Figure 20) 26 Example 1: Judge Response Judge Response ### Analysis of the Problem #### In-Context Example: The in-context example provides the expression 1 ? 8 and states that the result is **8**. The solution explains that among the possible operations (subtraction, multiplication, addition), the operation that yields the result **8** is **multiplication** (1 8 = 8). This establishes the pattern that the hidden operation is **multiplication**. #### Query Image: The query image presents the expression 6 ? 2. Based on the in-context example, we infer that the hidden operation is **multiplication**. #### Candidate Response: The candidate response states: - - **Reasoning**: The two numbers are 6 and 2. The operation between them is multiplication. 6 2 = 12. **Answer**: 12 #### Reference Answer: The reference answer is also **12**. ### Evaluation of the Candidate Response 1. **Correct Identification of the Operation**: - The candidate correctly identifies the operation as **multiplication** based on the in-context example. This is consistent with the pattern established in the in-context example. 2. **Correct Application of the Operation**: - The candidate applies the multiplication operation correctly: 6 2 = 12. 3. **Final Answer**: - The candidates final answer is **12**, which matches the reference answer. 4. **Clarity of Reasoning**: - The reasoning provided is clear and logically follows the pattern established in the in-context example. The candidate explicitly states the operation and shows the calculation step. 5. **Format Compliance**: - The candidate response follows the required format: \"Reasoning: xxx Answer: xxx\". ### Final Verdict The candidates response is both logically sound and mathematically correct. Both the reasoning and the final answer are accurate, and the response adheres to the required format. > **Rating: 3** Figure 20: (Operator Induction Math Reasoning) The Judge (Qwen2.5-VL-32B) evaluates the response of the LMM in Figure 19 to provide correct rating. 27 Example 2: Support Question: What is the result of the following mathematical expression? Answer: There are two numbers, 1 and 6. Performing some mathematical operation gives the answer 6. So if we think about subtracting the numbers, 1 6 = 5, multiplying the numbers, 1 6 = 6, adding the numbers, 1 + 6 = 7. This implies that the hidden operation must be multiplication or and the result is 6 Example 2: Query Question: What is the result of the following mathematical expression? Think step-by-step and give proper reasoning steps first and then given your final answer. The format should be > Reasoning: xxx Answer: xxx .The Reasoning part should contain reasons to derive the answer and the Answer part should only contain the answer. Your response should stricly follow this format and not just give the answer of the mathematical operation. Its important that you give reasoning before you answer. LMM prediction: Reasoning: There are two numbers, 4 and 1. Performing some mathematical operation gives the answer 5. So if we think about adding the numbers, 4 + 1 = 5, subtracting the numbers, 4 1 = 3, multiplying the numbers, 4 1 = 4. This implies that the hidden operation must be addition or + and the result is 5. Answer: 5 Figure 21: (Operator Induction Math Reasoning) An example of 1-shot mathematical reasoning task with Judge Rating: 1 (shown in Figure 22) 28 Judge Response ### Analysis of the Problem #### In-Context Example: The in-context example provides the expression 1 ? 6 and the solution states that the result is **6**. The reasoning provided suggests testing different operations: Subtraction: 1 6 = 5; Multiplication: 1 6 = 6; Addition: 1 + 6 = 7 From these tests, the solution concludes that the hidden operation is **multiplication**, as 1 6 = 6. #### Query Image: The query image is 4 ? 0. We need to determine the result of this expression based on the same hidden operation identified in the in-context example. #### Reference Answer: The reference answer provided is **0**. #### Candidate Response: The candidates reasoning and answer are as follows: - **Reasoning**: The candidate incorrectly identifies the numbers as **4 and 1** instead of **4 and 0**. They test different operations: - Addition: 4 + 1 = 5; Subtraction: 4 1 = 3; Multiplication: 4 1 = 4 - Conclusion: The hidden operation is **addition**, and the result is **5**. - **Answer**: The candidate concludes that the result is **5**. ### Evaluation of the Candidate Response 1. **Correct Identification of the Hidden Operation**: - The in-context example clearly indicates that the hidden operation is **multiplication** (1 6 = 6). - However, the candidate incorrectly assumes the hidden operation is **addition** based on their misinterpretation of the numbers. 2. **Correct Interpretation of the Numbers**: - - The query image shows the numbers **4 and 0**, but the candidate mistakenly uses **4 and 1** in their reasoning. This error leads to incorrect calculations and conclusions. 3. **Correct Application of the Operation**: - Even if we overlook the misidentification of the numbers, the candidates reasoning is flawed because they do not apply the correct hidden operation (multiplication) as indicated by the in-context example. 4. 4. **Final Answer**: - The candidates final answer is **5**, which is incorrect. The correct answer, based on the reference and the hidden operation of multiplication, should be **0** (4 0 = 0). ### Rating Justification - The candidates reasoning is **incorrect** because: 1. They misidentify the numbers in the query image. 2. They incorrectly deduce the hidden operation as addition instead of multiplication. - The candidates final answer (**5**) is also **wrong**. Given these errors, the candidates response does not align with the in-context example or the reference answer. Both the reasoning and the answer are incorrect. ### Final Rating: > Rating: 1 Figure 22: (Operator Induction Math Reasoning) The Judge (Qwen2.5-VL-32B) evaluates the response of the LMM in Figure 21 to provide correct rating. 29 A.2.5 Scaling to More Shots Here, we look into the performance of different prompt distillation methods with finetuning-based test time adaptation for larger number of shots and for 3 tasks from the VL-ICL dataset - Operator Induction, CLEVR Count Induction and TextOCR. Model used for below evaluation is LLaVA-ATTQwen2.5 (described in Section 2.3). We see similar performance gains with the introduction of more shots as shown in Table 1. Both the meta-task learning methods, Multi-TaskPD and MAPD perform quite well with MAPD showing outstanding performance for Operator Induction. Table 6: Operator Induction Results. 32-S 16-S Training Methods Meta-task NoMeta-taskPD Model-AvgPD In-ContextPD Multi-TaskPD MAPD 73.3 71.7 25.0 73.3 80.0 73.3 78.3 36.5 67.7 81.0 64-S 80.0 80.5 35.0 80.0 83.3 Table 7: CLEVR Count Induction Results. Training Methods Meta-task NoMeta-taskPD Model-AvgPD In-ContextPD Multi-TaskPD MAPD 16-S 32-S 64-S 35.5 30.0 25.5 38.0 40. 30.0 34.5 34.5 41.5 40.5 36.5 37.0 32.5 38.5 41.0 Table 8: TextOCR Results. Training Methods Meta-task NoMeta-taskPD Model-AvgPD In-ContextPD Multi-TaskPD MAPD 16-S 32-S 64-S 29.0 29.0 26.5 27.0 30.5 26.5 29.5 26.0 32.5 31.5 30.5 31.5 28.5 33.5 31.5 A.3 Performance Comparison with other LLaVA Models We also perform comparison of our model (LLaVA-ATT-Qwen2.5-7B) with other LLaVA based models. From Table 9, we see that our model along with MAPD based meta-learning and finetuningbased adaptation is able to outperform LLaVA-OneVision-72B ICL performance for the Fast OpenEnded MiniImageNet (Open-MI) task and the LLaVA-OneVision-7B ICL performance on both Open-MI and Operator Induction tasks. Note that LLaVA-ATT-Qwen2.5 does not finetune the LLM in complete training and uses significantly lesser vision-language data (1.3M) and trainable parameters (24M) compared to LLaVA-OneVision that trains the complete model with much more data. This shows promising results for our prompt distillation approach MAPD, which achieves state-of-the-art performance on Open-MI with finetuning just the attention-mapper for 30 gradient steps on the few-shot examples. 30 Table 9: Comparison of different LLaVA models on single-image tasks from VL-ICL Bench. We report the \"Avg\" accuracy for different numbers of shots - {1, 2, 4, 5, 8}. FT = Finetuning, ICL = InContext Learning, TTA= Test-Time Adaptation, VL-Data=Vision-Language Data, LAQ-7B=LLaVAATT-Qwen2.5-7B, CLIP=CLIP-ViT-L/14-336px, MLP=2-layer MLP, ATT=Attention-Mapper. Bold shows best performance and Underline shows second best. Params trained Model Architecture Methods TTA Open-MI OP_IND CLEVR TextOCR VL-Data LLaVA v1.5-7B LLaVA-Next-7B LLaVA-OneVision-7B LAQ-7B + In-ContextPD LAQ-7B + In-ContextPD LAQ-7B + MAPD LAQ-7B + MAPD CLIP+MLP+Vicuna1.5-7B CLIP+MLP+Vicuna1.5-7B SigLIP+MLP+Qwen2-7B CLIP+ATT+Qwen2.5-7B CLIP+ATT+Qwen2.5-7B CLIP+ATT+Qwen2.5-7B CLIP+ATT+Qwen2.5-7B LLaVA-OneVision-72B SigLIP+MLP+Qwen2-72B 1.2M 1.3M 10.4M 1.3M 1.3M 1.3M 1.3M 10.4M 7B 7.06B 8B 24M 24M 24M 24M 73.2B ICL ICL ICL ICL FT ICL FT ICL 12.4 34.4 42.1 51.1 64.5 53.3 77.9 75.1 5.4 5.4 41.7 20.6 30.9 9.6 47. 69.1 10.9 21.1 34.9 24.1 30.9 12.3 31.4 37.2 4.4 0.4 42.3 23.8 18.9 7.3 26.4 52.2 A.4 Test-time Computational Overhead ICL FT L 1,000 800 400 200 100 50 0 0 2 6 8 Number of Shots Figure 23: Test-time computation (TFLOPs) as number of shots increase. We compare MAPD in two settings, in-context learning (ICL) and finetuning (FT) using the LLaVA-ATT-Qwen2.5 LMM described in sections 2.3 and A.1.2. We conduct finetuning for 30 gradient steps per shot, as required by MAPD (Section A.2.2). We observe that MAPD with FT requires approximately 5 times more computation compared to ICL. This is somewhat expected given the increase in gradient computations incurred by finetuning the attention-mapper parameters. While reducing MAPDs test-time computational overhead is beyond the scope of this work, we note that MAPD only requires training 24M parameters and achieves strong performance on VL-ICL datasets (Section 3.2). This suggests promising avenues for future work focused on optimizing its efficiency. A.5 Broader Impact Our work provides novel method to quickly adapt LMMs to novel tasks with few gradient steps and limited data. We believe it holds promise for building advanced AI systems since publicly available large models struggle to generalize to new information unseen during training. We provide computationally cheaper way to adapt model to new datasets with significantly lesser computation required in comparison to training the model from scratch. In terms of negative impact, our technique could be misused to finetune on harmful data for malicious purposes. We release our code with explicit guidelines and terms of use and expect others to follow these in the future."
        }
    ],
    "affiliations": [
        "School of Informatics, University of Edinburgh"
    ]
}