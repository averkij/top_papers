{
    "paper_title": "DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes",
    "authors": [
        "Jinxiu Liu",
        "Shaoheng Lin",
        "Yinxiao Li",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360{\\deg} panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose the DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce a Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via a diffusion model with fixed resolution through a seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering a training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution. Our project page is available at \\url{https://dynamic-scaler.pages.dev/}."
        },
        {
            "title": "Start",
            "content": "DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes Jinxiu Liu*,1 , Shaoheng Lin*,1 , Yinxiao Li2 , Ming-Hsuan Yang,2,3 1SCUT, 2Google DeepMind, 3UC Merced 4 2 0 2 5 1 ] . [ 1 0 0 1 1 1 . 2 1 4 2 : r Figure 1. We introduce DynamicScaler, framework for generating dynamic panoramas conditioned on both images and text, or text alone. DynamicScaler enables the creation of arbitrary rectangular panoramas as well as 360 panoramic views, offering immersive visual experiences for AR/VR applications and displays of any size. (Please refer to our project page https://dynamic-scaler.pages. dev/ for better visualization.) *Equal contribution. Corresponding authors."
        },
        {
            "title": "Abstract",
            "content": "The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360 panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose the DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via diffusion model with fixed resolution through seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution. Our project page is available at https://dynamic-scaler.pages.dev/ 1. Introduction The increasing demand for immersive AR/VR applications has heightened the need for high-quality panoramic scene synthesis, essential for industries like digital advertising, wearable displays, and related tasks, where content often requires wide or portrait formats. However, achieving scalable panoramic scene synthesis poses unique challenges. successful approach must enable spatially scalable generation while preserving motion coherence across panoramic scenes of any size, ensuring seamless and immersive experience across various scenes in panoramic view. Recent methods for image generation face two key challenges: generating high-resolution or wide aspect ratio images, and maintaining motion consistency and memory efficiency in dynamic scene generation, such as video synthesis. Extending image generation to higher resolutions or wider aspect ratios is computationally intensive, requiring significant memory and large-scale training datasets. For example, approaches like SDXL [18] support larger range of aspect ratios but still faces scalability issues, especially when it comes to ultra wide aspect ratio and higher resolution, which limits this methods in consistency, memory usage and inference speed. Other methods, such as those that stitch patches from pre-trained diffusion models, work well for generating panoramic or landscape images with repetitive patterns [2, 13], but this approach is less effective for more complex or varied scenes. Additionally, although there have been advances in spatially scalable diffusion models for static image generation, these models are often limited to square images [6, 8, 9, 30], restricting their ability to handle broader aspect ratios. In contrast, the challenge of dynamic scene generation requires not only spatial coherence across frames, but also global motion consistency, making it even more computationally demanding. Moreover, video generation models must be designed with memory efficiency in mind, as large-scale dynamic scene synthesis often strains memory, limiting real-time inference capabilities. While recent works have explored trainingfree approaches for expanding diffusion models to new domains [12, 20, 21], the scalability of these models to highresolution video generation remains largely underexplored, requiring solutions that balance motion consistency and memory consumption effectively. Generating dynamic scenes in 360 panoramic field of view (FoV) introduces unique challenges, including: (1) the broader content distribution required for equirectangular projections (ERPs) over 360 180 FoV; (2) curved motion patterns in ERPs versus straight-line motion in standard videos; and (3) continuity requirements at the left and right ERP boundaries, which represent the same meridian. 360DVD [23] addresses these challenges by fine-tuning text-conditioned video diffusion model on panoramic data in equirectangular space, but it suffers from low resolution and interpolation artifacts due to operating in the latent space, causing blurriness and divergence from the original motion space. Other methods, like 4K4DGen [17] and Vividdream [14], attempt to animate scenes in overlapping regions, but their fixed-window denoising limits motion range and cross-scene consistency. Specifically, 4K4DGen faces constraints in motion range, relies solely on image-to-video transformations, and requires optimization procedures that reduce efficiency. Our approach overcomes these limitations by introducing tuning-free denoising method that ensures spatial and temporal coherence for long-duration, loopable, and seamless panoramic video generation, achieving high-quality continuous motion with improved efficiency and visual fidelity. We propose DynamicScaler, unified, tuning-free framework for scalable panoramic dynamic scene synthesis with seamless motion. Our method ensures spatial and motion coherence throughout video generation by utilizing shifting window that distributes noise uniformly across regions, achieving spatial scalabilitywhether overlapping or notwhile maintaining consistent motion from latent noise space. In contrast to the state-of-the-art 360DVD, DynamicScaler synthesizes higher-quality images by processing data into pre-projected space before mapping it to the 2 Figure 2. Our pipeline is divided into two stages: low-resolution stage establishes coarse motion structure, 360-degree setting(the yellow block) involves Panoramic Projecting Denoise to initialize motion that fits to spherical panorama, while the regular perspective setting(the blue block) utilizes Offset Shifting with overlap for the early denoise steps, then the remaining denoise steps are completed by our Offset Shifting Denoise. The up-scaling stage(the green block) utilizes more shift windows to produce refined, high-resolution panorama with Global Motion Guidance from the low-resoltuion video. final equirectangular projection, thereby enhancing output fidelity. We introduce the Offset Shifting Denoiser (OSD), which synchronously denoises panoramic dynamic scenes using well designed shifting Window mechanism, ensuring smooth transitions and spatial coherence while preserving motion fidelity as well as seamless transitions, and can also be adapted to generate 360 degree panorama by our Panoramic Projecting technique. To handle varying resolutions and aspect ratios, we employ Global Motion Guidance (GMG) and an upsampling strategy, ensuring local detail and global motion continuity in high-resolution scene generation. Our hierarchical approach maintains overall structure while delivering fine-grained local details, achieving seamless motion and scene-level consistency. In addition to spatial dimensions, we address the often-overlooked temporal dynamics, enabling the generation of long-duration as well as loopable dynamic scenes with continuous motion. We extend our OSD technique to the temporal domain, overcoming GPU memory constraints and enabling the synthesis of longer, temporally consistent dynamic scenes. Our contributions can be summarized as follows: We propose unified framework for scalable panoramic dynamic scene synthesis, ensuring motion coherence across various resolutions, aspect ratios, and 360 FoV settings without requiring fine-tuning. We introduce the Offset Shifting Denoiser, which efficiently denoises the entire panoramic video with overall coherence, ensuring seamless boundary transitions and scene continuity across arbitrary aspect ratios, along with Global Motion Guidance, which enhances motion consistency at higher resolutions. We introduce the Panoramic Projection Denoiser to enable 360 FoV panorama generation and extend it to the temporal dimension, allowing for the generation of longer-duration or loopable dynamic videos. This method overcomes GPU memory limitations while ensuring temporal consistency across long-duration panoramic video sequences. Extensive experiments demonstrate that DynamicScaler outperforms existing methods in visual quality and motion consistency, generating continuous, loopable dynamic scenes suitable for immersive applications. 2. Related Work Spatial Scaling of Diffusion Models. Diffusion models have achieved remarkable success in generating highquality images, with recent efforts focusing on scaling to diverse resolutions and aspect ratios. Existing approaches often rely on retraining or fine-tuning large models [25, 31], incurring significant computational costs and requiring extensive datasets. Other methods generate panoramic images 3 by stitching patches from pretrained models, which work well for repetitive patterns like landscapes [2, 7, 13, 19]. Additionally, techniques such as AnyLens [20], MagicScroll [21], and AutoDiffusion [16] extend pretrained models across various domains without retraining. However, most of these methods are limited to square aspect ratios or fixed resolutions [6, 8, 9, 30] and cannot directly generate panoramic scenes. Panoramic Scene Synthesis. Recent advancements in scene-level generation focus on synthesizing large-scale 3D scenes from text, as seen in works like LucidDreamer, GALA3D, and Wonderworld [4, 28, 32]. These methods primarily generate static 3D scenes using Gaussian representations, limiting their capacity for dynamic content or flexible perspectives. Research on immersive generation has shifted toward 360 panoramas. OmniDreamer [1] employs cyclic inference for 360 image synthesis, while ImmenseGAN [5] leverages fine-tuning for better control. Diffusion-based methods, such as DiffCollage [29] and PanoDiff [22], have shown promise for static panoramas, yet fail to address dynamic video generation. While DreamScene360 [15] integrates Gaussian splatting for text-topanorama synthesis, its reliance on static priors restricts dynamic scene applications. Dynamic Scene Generation. Dynamic scene generation for panoramic videos introduces challenges in maintaining motion coherence, temporal consistency, and visual quality over extended durations. 360DVD [23] adapts video diffusion models for panoramic data but faces limitations in generalization, interpolation accuracy, and style diversity when combining multiple LoRA models. Similarly, methods like 4K4DGen [17] and Vividdream [14] use overlapping regions for scene animation but suffer from noisy artifacts, fixed-window denoising, and restricted motion ranges. Moreover, 4K4DGen depends on inefficient optimization pipelines and is limited to image-to-video transformations. In contrast, our method overcomes these limitations with shifting-based denoising framework that ensures spatial and temporal coherence. Our approach enables the generation of seamless, long-duration, and loopable panoramic videos with high-quality dynamic motion. 3. Method We present DynamicScaler, scalable and tuning-free framework for panoramic dynamic scene synthesis that achieves seamless spatial and temporal coherence in dynamic environments. To extend video diffusion from fixed resolutions to expansive panoramas, our approach combines multiple diffusion model processes conditioned on noisy latents from previous timesteps, allowing denoising to adapt across spatial dimensions by adjusting the shifting path within panoramic space, as illustrated in Figure 2. To further enhance structural awareness and ensure global coherence in large-scale motion, we introduce Global Motion Guidance, structured motion prior tailored for panoramic video generation. This guidance, integrated with Spatial Shifting Denoising, preserves both broad motion structures and intricate details, yielding cohesive and richly detailed scenes. Additionally, our framework provides methods for continuous, loopable motion, enabling seamless transitions and frame-to-frame consistency for extended, loopable video sequences. 3.1. Offset Shifting Denoising We propose dynamic scene synthesis approach integrating multiple reference diffusion processes within unified framework. Existing methods such as SyncDiffusion [13] and MultiDiffusion [2] focus on image generation with patch-based techniques limited by local overlaps. However, these models suffer from restricted influence beyond adjacent areas, leading to blurred details and increased inference time due to the high overlap requirements. The proposed DynamicScaler utilizes shifting denoising strategy that dynamically adjusts the denoising process across the entire scene rather than being confined to fixed regions to address these limitations. This enables the synthesis of dynamic scenes, including 360 panoramic spaces, where the shifting denoising path plays crucial role in montage-style and arbitrary-sized visual generation. By conditioning motion transfer on the interaction between noisy latents from different diffusion models, DynamicScaler allows for information sharing across regions, achieving seamless scene synthesis. By combining the denoising outputs from multiple models where each contributing unique spatial details, we ensure global consistency and significantly enhance visual fidelity. This approach improves the quality of dynamic scenes and scales flexibly to various scene dimensions and orientations. In each steps, denoising windows are shifted both vertically and horizontally as illustrated in Figure 3. Specifically, in vertical shifting, the windows with in the boundaries are shifted vertically by an offset every steps, and the region that is not covered by shifting windows are handled by padding windows at the top and bottom. Horizontally, windows are also shifted by offset while the whole video latent would be regarded as ring, that is, the left and right boundaries are connected so windows can cross through the boundary, which is implemented by filling the out of boundary regions by the corresponding region from the other side. This mechanism not only allows seamless transition across the whole generated video in arbitrarily aspect ratio but also enables motion and content continuity between the left and right boundaries, as shown in fig-5, producing immersive panoramic videos. The Offset Shifting Denoising process can be formulated as: Zt = Con(Φθ(t, c, S(Zt1, i, j, t)), t) (1) Figure 3. The purposed Offset Shifting Window mechanism, which involves shifting denoising windows both vertically and horizontally between denoise steps to denoise the whole panorama video latent with arbitrary aspect ratio an resolution. Generally, all denosing windows are shifted vertically and horizontally every step, with different ways to handle the windows that cross the spatial boundary. where S(X, i, j, t) indicates dividing the panorama video latent into nw nh windows with offset at time and select the one in th column, th row. Con() means merging latent from divided windows back into X. Φθ(t, c, x) represents Denoising UNet, for T2V, = {ctext}; and for I2V, = {ctext, S(cimg, i, j, t)}. 3.2. Global Motion Guidance Shifting denoising along horizontal and vertical directions enables scalable panorama generation and seamless motion. However, complex motion patterns, such as cascading waterfalls, require coordination across the entire panoramic field. To address this, we decompose the generation process into global layout and local content stages hierarchically: global layout stage captures high-level motion structures in lower resolution, while local content stage refines fine details and support higher resolution while maintain consistency. We apply causal global motion guidance to structure overall scene dynamics, followed by progressive upscaling to enhance resolution. Starting with low-resolution generation, the process iteratively scales up by factor of K, refining details through an initialize-denoise phase and subsequent upsample-diffuse-denoise loops. The upscaling process is formulated as: pθ(zS 0 T ) = pθ(z1 0z1 ) (cid:89) s=2 (cid:0)q(zs zs 0)pθ(zs 0zs )(cid:1) , (2) 0 0 = inter(zs1 where zs ), with inter() denoting an interpolation algorithm like bicubic. This iterative process compensates for interpolation artifacts and refines details, ensuring coherent, high-quality panorama generation. 3.3. 360 FoV Panorama Generation Our framework enables the generation of 360 FoV panoramas, conditioned on text or images, using sliding window denoising mechanism. This ensures that the resulting panorama covers the full 360 field of view, making it suitable for immersive applications. For rectangular panoramas, the sliding window shifts across both rows and columns to improve scene coherence. For 360 FoV panoramas in equirectangular projections (ERPs), the image is first reprojected into spherical space, where the sliding window mechanism is applied to ensure full coverage of the entire sphere. However, interpolation in equirectangular projections can introduce distortion, particularly in large polar angles. To mitigate this, we apply equirectangular interpolation during the early denoising steps to capture the broad structure of the scene, and later shift in spherical space to refine finer details and preserve motion fidelity. This twostage process ensures high-quality 360 FoV panorama generation with minimal distortion. 360 FoV Panoramic Representation. Panoramic images in equirectangular format are represented as an matrix. To minimize distortion, we use spherical representation where pixel values are defined on the unit sphere S2. In spherical coordinates, point on the unit sphere is represented by the directional vector = (x, y, z), where = sin(θ) cos(ϕ), = sin(θ) sin(ϕ), and = cos(θ), with θ being the polar angle (latitude) and ϕ being the azimuthal angle (longitude). The pixel value at direction is then mapped as follows: SI (d) = EI (cid:18) 1 π arccos(y), 1 2π (cid:19) arcsin(z) (3) where SI (d) denotes the pixel value at direction = (x, y, z). Here, the function EI maps the spherical coorFigure 4. Results showcasing the scalability of our model across various resolutions, including 360 and rectangular panorama settings as well as both text conditioned and image conditioned generation. For more results please refer to our project page. dinates to the corresponding pixel values in the equirectangular image. Offset Shifting in Panoramic Space. Most video diffusion models are trained by datasets in perspective views, 6 thus can not be directly applied to generate spherical 360 degree panoramic video, which is usually represented in equirectangular projection. may have already been denoised by previous windows, creating inconsistent noise levels. To address this, the system maintains denoising mask Md at each timestep to track denoised regions. Before processing each window, noise is rebalanced as: = (cid:40) αtZt + 1 αtϵt Zt if Md(x, y) = 1 if Md(x, y) = 0 (5) ensuring consistent noise levels across all windows before passing into diffusion model. More details can seen from the appendix. Figure 5. The purposed Panoramic Projecting Denoise, where spherical panorama videos (represented as equirectangular projections) are denoised by projecting them into perspective view windows, followed by re-projection back to the equirectangular format, which allowed denoising 360 degree panoramic video with diffusion models trained in regular (perspective) views dataset. To address this, We apply Panoramic Projecting Denoise to spherical panorama videos, as illustrated in Figure 5 The panorama is represented in equirectangular projection, then projected onto perspective windows, denoised, and reprojected back at every denoising steps. The window angles are adjusted through yaw rotations at each step to ensure spatial coherence across the entire panorama, which can be formulated as: Zt = Vc (Φθ (t, c, (Zt1, ai,j,t, )) , t) (4) where (X, a, ) projects the panorama latent in ERP format at view angle and fov into perspective views, similar to regular OSD, the view angles have an offset at each timestep t, creating perspective view windows that can be denoised using normal diffusion models Φθ. Vc projects all the view windows back into ERP, updating the whole panorama latent. Similar with regular perspective Offset Shifting Denosing, we can also shift the view windows each steps, with an offset applied in pitch angles to denoise the whole video seamlessly, allowing us to generate spherical 360 degree panoramic video by regular diffusion models without extra training. In cases that windows have overlap with each others, which is often the case in high polar angle regions, parts of the latent space corresponding to subsequent windows Figure 6. Example frames from panorama video at the 0th, 16th, 32th, 48th, 64th and 80th frames. Despite the increasing video length, the visual quality of the panorama remains consistent, demonstrating the effectiveness of our method in generating long videos. 3.4. Infinite Long Scene Video Generation major challenge in dynamic scene synthesis is generating long-duration video sequences, as current video diffusion models typically produce only short clips. To address this, we extended Offset Shifting Denoising from spatial to temporal dimension and introduced temporal shifting strategy that enables continuous motion and the generation of videos with much longer duration by regular video diffusion models that have 16 frames capacity. By temporally shifting the denoising process, we ensure smooth transitions between frames, as shown in Figure 6. Moreover, by applying similar looping mechanism in our spatially shifting denoising process to the temporal dimension, we can achieve seamless looping video generation. Specifically, the first and last frame are connected and windows are allowed to cross the start and end boundary, treating the video frames as ring, to create loopable sequence with smooth, continuous motion. For more details please refer to our appendix. 7 4. Experiments 4.1. Qualitative Results We evaluate the performance of DynamicScaler against state-of-the-art models. We detail the implementation, experimental settings, comparison methods, and user studies conducted to assess the efficacy of our approach. The proposed DynamicScaler is built on top of existing textto-video generation model VideoCrafter2 [3] and image-tovideo generation model DynamiCrafter [26]. Compared to previous methods, DynamicScaler supports wider range of settings and can generate videos of infinite length, while other methods are limited to generating only finite-length videos. Our approach generates dynamic panoramas at various resolutions, including both standard and 360 Field-of-View (FoV) panoramas, with support for both textand image-conditioned inputs, as shown in Table 1. Currently, among the available concurrent works, only 360DVD is capable of generating dynamic panoramas, so we compare our method directly with it. We assess the visual quality by randomly selecting views from 100 generated video cases using random camera positions. Key factors such as image quality, dynamic range, motion smoothness, temporal flickering, and scene richness are evaluated according to the protocols of VBench [11] and CogVideoX [27]. Furthermore, we incorporate an LLMbased visual evaluator, Q-Align [24], to score both image and video quality. As shown in Table 1, DynamicScaler outperforms 360DVD in all metrics, demonstrating superior video quality in handling complex dynamic scenes. 4.2. Quantitative Results As demonstrated in Figure 4, our approach generates panoramas of arbitrary sizes, supporting rectangular and 360 Field of View (FoV) configurations, offering unparalleled flexibility for diverse applications. As shown in Figure 7, the left and right edges of each generated frame align seamlessly, ensuring flawless integration into 360 viewan essential feature for immersive environments. This highlights the robustness of our shift denoise technique in both non-360 and 360 settings. Furthermore, our method facilitates the creation of scene-level, long duration, and loopable videos through temporal shifting, without requiring additional training or compromising visual quality. 4.3. User Studies We conduct user studies to evaluate videos based on five criteria: graphics quality, frame consistency, left-right continuity, content distribution, and motion patterns. 20 Participants are asked to select the video with the highest quality. Table 2 shows that DynamicScaler performs favorably in all criteria. We use two comparison types: same-case comparison (using the same scene for direct comparisons) and random-case comparison (using different scenes due to limited publicly available data). DynamicScaler consistently receives the highest ratings, especially for continuity, content distribution, and motion patterns, demonstrating its superior ability to generate seamless, dynamic panorama. Figure 7. This figure demonstrates the seamlessness of generated panorama videos by horizontally concatenating frames, showcasing continuity across the left and right boundaries. 4.4. Ablation Studies We conducted ablation studies to evaluate the impact of each core component in DynamicScaler under four different configurations: Direct Inference, where the video is generated directly at the target resolution without enhancement techniques, but results in an out-of-memory (OOM) error due to high resource demands; Without OSD (Offset Spatial Denoising), which excludes the offset spatial denoising mechanism that mitigates multi-scale artifacts; Without GMG (Global Motion Guidance), where global motion guidance is omitted, reducing frame-to-frame motion continuity; and the Full Method, which integrates all components. As shown in Table 3, the Full Method consistently outperforms all other configurations across all evaluation metrics, underscoring the significance of each component. Both OSD and GMG play critical roles in improving image quality, dynamic range, motion smoothness, and temporal coherence, while also minimizing temporal flickering. 5. Conclusion This paper presents DynamicScaler for scalable, coherent panoramic dynamic scene synthesis. By introducing Spatial Rotating Denoiser and Seamless Rotating Window, our approach ensures efficient denoising and consistent boundary transitions. The Global Motion Guidance mechanism maintains local detail and global motion continuity, delivering superior content quality and motion smoothness. DynamicScaler outperforms existing methods in scalability and performance, offering practical, training-free solution for creating high-quality, immersive AR/VR dynamic content across various resolutions and aspect ratios. Source Tuning-Free CVPR24 360DVD [23] Arxiv24 4K4DGen [17] Scalecrafter [10] ICLR24 VividDream [14] Arxiv24 DynamicScaler - Arbitrary Size 360 Field-of-View Text Only Condition Image Condition Video Length 16 16 16 16 Infinite Loopable CLIP-Score Image Quality Dynamic Degree Motion Smoothness Temporal Flickering Scene Q-Align(I) Q-Align(V) 360DVD [23] DynamicScaler 0.293 0. 0.436 0.583 0.412 0.783 0.917 0.963 0.964 0.982 0.417 0.499 0.485 0. 0.532 0.613 Table 1. Quantitative comparison of dynamic scene generation methods, with best results highlighted in bold. The evaluation covers key factors such as resolution scalability, video length, and loopability, using metrics on image quality, dynamic range, motion smoothness, and temporal flickering, and user-centric Q-Align scores. DynamicScaler outperforms existing methods across all these metrics. Methods Video Criteria Graphics Quality Frame Consistency Panorama Criteria Motion Pattern Scene Richness End Continuity 3.3 4.6 Same Case Comparison 360DVD [23] Ours Random Case Comparison 360DVD [23] 4K4DGen [17] Scalecrafter [10] VividDream [14] Ours 3.3 4.5 3.5 3.6 4.3 3.5 4.7 3.4 3.6 3.7 3.7 3.9 3.6 4. 3.6 4.3 1.9 3.8 4.5 3.7 4.5 3.9 3.6 4.4 3.6 4.5 3.5 4.6 3.4 4.3 3.6 4.1 4.4 Table 2. User preference study results. Same Case Comparison refers to using the same case for comparison, while Random Case Comparison refers to using different cases due to the unavailability of some methods. Ratings range from 1 (lowest) to 5 (highest). Image Quality Dynamic Degree Motion Smoothness Temporal Flickering Q-Align(V) Direct Inference OOM OOM 0.749 0.765 0.778 w/o OSD w/o GMG Full Method 0.564 0.571 0.587 OOM 0.948 0.961 0. OOM 0.905 0.946 0.985 OOM 0.595 0.598 0.616 Table 3. Performance comparison of different configurations regarding image quality, dynamic degree, motion smoothness, and temporal flickering. OOM stands for Out-of-Memory."
        },
        {
            "title": "References",
            "content": "[1] Naofumi Akimoto, Yuhi Matsuo, and Yoshimitsu Aoki. Diverse plausible 360-degree image outpainting for efficient 3dcg background creation. In CVPR, pages 1144111450, 2022. 4 [2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. In ICML, 2023. 2, 4 [3] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: 9 Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 8 [4] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. [5] Mohammad Reza Karimi Dastjerdi, Yannick Hold-Geoffroy, Jonathan Eisenmann, Siavash Khodadadeh, and JeanFrancois Lalonde. Guided co-modulated gan for 360 field of view extrapolation. In 3DV, pages 475485, 2022. 4 [6] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising highresolution image generation with no. In CVPR, pages 6159 6168, 2024. 2, 4 [7] Stanislav Frolov, Brian Moser, and Andreas Dengel. Spotdiffusion: fast approach for seamless panorama generation over time. arXiv preprint arXiv:2407.15507, 2024. 4 [8] Alexandros Graikos, Srikar Yellapragada, Minh-Quan Le, Saarthak Kapse, Prateek Prasanna, Joel Saltz, and Dimitris Samaras. Learned representation-guided diffusion models In CVPR, pages 85328542, for large-image generation. 2024. 2, 4 [9] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. arXiv preprint arXiv:2402.10491, 2024. 2, 4 [10] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherresolution visual generation with diffusion models. In ICLR, 2023. 9 [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. [12] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Trainingfree diffusion model adaptation for variable-sized text-toimage synthesis. In NeurIPS, pages 7084770860, 2023. 2 [26] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, pages 399417. Springer, 2025. 8 [27] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 8 [28] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 4 [29] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel generation of large In CVPR, pages 10188 content with diffusion models. 10198, 2023. [30] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, and Jiajun Liang. Hidiffusion: Unlocking high-resolution creativity and efficiency in low-resolution trained diffusion models. arXiv preprint arXiv:2311.17528, 2023. 2, 4 [31] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Any-sizediffusion: Toward efficient text-driven synthesis for any-size hd images. In AAAI, pages 75717578, 2024. 3 [32] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024. 4 [13] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. In NeurIPS, pages 5064850660, 2023. 2, 4 [14] Yao-Chih Lee, Yi-Ting Chen, Andrew Wang, Ting-Hsuan Liao, Brandon Feng, and Jia-Bin Huang. Vividdream: Generating 3d scene with ambient dynamics. arXiv preprint arXiv:2405.20334, 2024. 2, 4, 9 [15] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv preprint arXiv:2404.03575, 2024. 4 [16] Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In CVPR, pages 71057114, 2023. [17] Renjie Li, Panwang Pan, Bangbang Yang, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, and Zhiwen Fan. 4k4dgen: Panoramic 4d generation at 4k resolution. arXiv preprint arXiv:2406.13527, 2024. 2, 4, 9 [18] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [19] Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, and Rita Cucchiara. Merging and splitting diffusion paths arXiv preprint for semantically coherent panoramas. arXiv:2408.15660, 2024. 4 [20] Andrey Voynov, Amir Hertz, Moab Arar, Shlomi Fruchter, Anylens: generative diffuarXiv preprint and Daniel Cohen-Or. sion model with any rendering lens. arXiv:2311.17609, 2023. 2, 4 [21] Bingyuan Wang, Hengyu Meng, Zeyu Cai, Lanjiong Li, Yue Ma, Qifeng Chen, and Zeyu Wang. Magicscroll: Nontypical aspect-ratio image generation for visual storytelling via multi-layered semantic-aware denoising. arXiv preprint arXiv:2312.10899, 2023. 2, [22] Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, and Li Song. 360-degree panorama generation from few unregistered nfov images. arXiv preprint arXiv:2308.14686, 2023. 4 [23] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation In CVPR, pages with 360-degree video diffusion model. 69136923, 2024. 2, 4, 9 [24] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 8 [25] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. In CVPR, pages 42304239, 2023."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "SCUT",
        "UC Merced"
    ]
}