{
    "paper_title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
    "authors": [
        "Yuze He",
        "Wang Zhao",
        "Shaohui Liu",
        "Yubin Hu",
        "Yushi Bai",
        "Yu-Hui Wen",
        "Yong-Jin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 0 5 9 9 1 . 1 1 4 2 : r AlphaTablets: Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos Yuze He1, Wang Zhao1, Shaohui Liu2, Yubin Hu1, Yushi Bai1, Yu-Hui Wen3, Yong-Jin Liu1 1Tsinghua University 2 ETH Zurich 3 Beijing Jiaotong University"
        },
        {
            "title": "Abstract",
            "content": "We introduce AlphaTablets, novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets."
        },
        {
            "title": "Introduction",
            "content": "3D planar reconstruction from monocular videos is crucial aspect of 3D computer vision, dedicated to the precise detection and reconstruction of underlying 3D planes from consecutive 2D imagery. The reconstructed 3D planes serve as flexible representation of surfaces, facilitating various applications such as scene modeling, mixed reality and robotics. Traditional methods for 3D planar reconstruction rely heavily on explicit geometric inputs [36, 41], hand-crafted features [6, 4], strong assumptions [10, 15] and solvers [37, 42, 16] to detect and reconstruct the planes, thereby imposing limitations on scalability and robustness. Learning-based methods [27, 26, 2, 28, 48] leverage the power of data-driven training to directly segment the plane instances and regress the plane parameters from single or sparse-view images. Notably, PlanarRecon [50], pioneering monocular video-based learning method, operates within the 3D volume space to detect and track planes, and has demonstrated promising results in recovering planar structures. However, it often falls short in detecting complete planar reconstructions and struggles with generalization across diverse scenes. How to build an accurate, complete and generalizable 3d planar reconstruction system is still challenging open problem. We inspect this problem from the perspective of plane representation. Current methodologies employ various representations, such as view-based 2D masks [42, 16, 3, 9, 26, 2, 48], 3D points [37, 30], surfels [36], and voxels [50]. While 2D masks can precisely illustrate plane contours, this 2D plane representation faces inconsistencies across different views and necessitates complex matching and fusion processes to reconstruct 3D surfaces. In contrast, 3D representations directly depict 3D planar surfaces. However, they suffer from discontinuous geometry and texture due to discretized sampling, and struggle to accurately model complex plane boundaries. Corresponding Author. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). To address above limitations, we propose novel plane representation termed AlphaTablets. AlphaTablets define 3D planes as rectangles with alpha channels, providing natural delineation of irregular plane boundaries and enabling continuous solid 3D surface representation. AlphaTablets combine the best of 2D and 3D plane presentations: by defining and optimizing in 3D, AlphaTablets ensure efficiency and consistency across all views; meanwhile, by introducing alpha channels and texture maps in planes canonical coordinates, AlphaTablets can accurately model solid surfaces and complex irregular plane boundaries. Furthermore, we introduce rasterization formulations for AlphaTablets, facilitating differentiable rendering into images. Based on AlphaTablets, we present bottom-up pipeline for 3D planar reconstruction from posed monocular videos. Initially, leveraging 2D superpixels [1], we initialize the AlphaTablets using pre-trained depth and surface normal models [19, 12]. This initialization yields dense yet noisy assembly of relatively small and overlapping AlphaTablets. Next, these small planes are optimized via differentiable rendering with hybrid regularizers to adjust both geometry, texture and alpha channels. We further introduce an effective merging scheme that facilitates the fusion of neighboring tablets, thereby promoting the growth of larger, cohesive planes. Through iterative cycles of optimization and merging, our final reconstructions boast solid surfaces, clear boundaries, and interpolatable texture maps, delivering accurate 3D planar structures. Moreover, our approach enables flexible plane-based scene editing. Extensive experiments on ScanNet [11] dataset demonstrate state-of-theart performance of 3D planar reconstruction, showing the great potential of being generic 3D plane representation for subsequent applications. In summary, our contributions are threefold: We propose AlphaTablets, novel and generic 3D plane representation which features effective and flexible modeling of plane geometry, texture and boundaries. We derive the rasterization formulation for AlphaTablets, enabling differentiable rendering to images. We build an accurate and generalizable 3D planar reconstruction system from monocular videos upon AlphaTablets. The key components are effective initialization from pre-trained monocular cues, differentiable rendering based optimization, and the proposed merging mechanism for AlphaTablets. The proposed system achieves state-of-the-art performance for 3d planar reconstruction, while also enabling flexible plane-based scene editing for various applications."
        },
        {
            "title": "2 Related Works",
            "content": "Classical 3D Plane Reconstruction. Traditional 3D plane reconstruction methods typically fit planes from geometric inputs like RGB-D images [36, 41, 35, 21] and 3D point cloud [7, 43], using robust estimators such as RANSAC [14] and its variants [37]. Another line of research approaches utilize multi-view images as input. Early works [42, 16, 15, 3] detect 3D planes from reconstructed sparse 3D points and lines, and then optimize plane masks as multi-label segmentation through image-based solvers such as Markov Random Fields (MRF) and Graph Cut [23]. Argiles et al. [3] propose plane consistency check to determine plane boundaries using square cells rather than sparse point contours. Manhattan world assumption is introduced [10, 53] to better reconstruct the dominant planes. While these methods can segment 2D complex planes, the 2D mask representation of planes causes view inconsistency, visibility issues and requires additional efforts to reconstruct 3D planes. In contrast, AlphaTablets directly model and optimize planes in 3D with differentiable rendering, eliminating the inconsistency and occlusion problems. Given monocular video as input, DPPTAM [9] reconstructs both the 3D planar and non-planar structure in SLAM fashion, and adapt superpixel to group homogeneous pixels for textureless regions. Our method also uses superpixels as 2D units to initialize AlphaTablets, but provides greater flexibility in adjusting geometry, texture, and boundaries through optimization. Learning-based 3D Plane Reconstruction. Data-driven methods leverage large-scale training data to learn geometric priors, enabling the reconstruction of 3D planes from single or sparse view images. PlaneNet [27], PlaneRecover [52] and PlaneRCNN [26] create training datasets for plane detection and reconstruction, and train deep neural networks to directly segment plane instances and regress plane parameters from single RGB image. Further enhancements along this line include the use of transformer architectures [47], multi-task collaboration [40], pairwise plane relations [34], and feature clustering [54], etc. Due to the highly ill-posed nature of single-image plane reconstruction, many 2 Figure 1: Illustration of tablet properties and rendering. Normal and up vector determines the rotation of tablet in 3D space, while every tablet maintains distance ratio between the coordinates of the 3D field and 2D-pixel space. works [2, 22, 28, 48] adopt sparse view images as inputs, and explore joint plane detection, association and optimization to help the final reconstruction. However, these methods can only recover local 3D planar structures from sparse views, and it is challenging to extend them to image sequences. PlanarRecon [50] proposes 3D volume-based planar reconstruction system with plane detection and tracking modules that sequentially process video frames and update 3D planar reconstructions. While effective in producing clean and consistent 3D planes, PlanarRecon often oversimplifies planar structures, resulting in incomplete reconstructions. Furthermore, being trained on indoor datasets with gravity-aligned camera poses, PlanarRecon struggles to generalize to unseen data. On the contrary, our method optimizes AlphaTablets for each scene, thus can generalize to any video sequence. 3D Planar Surface Representation. While most of current works employ 2D plane representation for detection and reconstruction, 3D representation is more efficient and consistent by aggregating 2D image information directly in 3D. 3D point cloud [37, 38, 33, 51, 32] is one of the most widely used 3D surface representation, yet they are discrete samples of surface and cannot fully capture continuous geometry and textures. Extending points into planar primitives, surfels [36, 17] and 2D gaussians [18, 20] represent surface with 2D disks in 3D space, and demonstrate impressive results for both planar and non-planar surfaces. Unfortunately, both surfels and 2D gaussians only provide local planar structure within one unit, making it difficult to directly represent large geometric planes. In terms of continuous surface representation, mesh is the most popular representation with mature graphic pipelines. Many works [50, 42, 49, 39] convert other 3D surface representations into mesh for visualization or further optimization. While meshes using 3D triangles or rectangles can represent 3D planes with regular quadrangular shapes, they struggle with irregular complex boundaries and are difficult to build and optimize from scratch. AlphaTablets inherit the advantages of continuous geometry and canonical texture modeling from mesh, and further introduce alpha channels to handle the irregular plane boundaries, acting as \"rectangle soup with alpha channels\". With the popularity of implicit neural representation, several works [25, 8] have explored encoding 3D plane primitives with MLPs. Compared to implicit representations, AlphaTablets offer the advantages of explicit representation, such as easy editing and fast rendering without network inference."
        },
        {
            "title": "3 Method",
            "content": "Our proposed AlphaTablets is novel and generic 3D plane representation, which enables accurate and generalizable 3D planar reconstruction from monocular video inputs. In Sec.3.1, we first introduce the data format of AlphaTablets, then discuss the differentiable rasterization of AlphaTablets in Sec.3.2. Finally, in Sec.3.3, we introduce bottom-up pipeline based on AlphaTablets to conduct 3D planar reconstruction from monocular video input. 3.1 AlphaTablets: Representing 3D Planes with Semi-Transparent Rectangular Primitives As illustrated in Figure 1, our proposed AlphaTablet is shaped as rectangle with 3D geometry properties and 2D in-tablet properties. The 3D geometry includes the tablet center point p, normal vector n, up vector and right vector r. The normal, up and right vectors are orthogonal. The 2D in-tablet information contains texture map c, an alpha channel α, and pixel range (ru, rv) that indicates the 2D resolution of the texture and transparency map. The alpha channel α ensures that arbitrary shapes can be modeled by our tablet formation. Since the ratio of unit distance in 3D space and 2D in-tablet canonical space varies across different tablets, distance ratios λu, λv of two directions on the texture map is also maintained for every tablet to acquire the tablet size in 3D space. 3 3.2 Differentiable Rasterization As generic 3D planar surface representation, efficient projection from 3D to 2D images is highly demanding for AlphaTablets. Therefore, we introduce the differentiable rasterization of AlphaTablets. The data format of AlphaTablets is the 3D rectangle soup with alpha channels, allowing us to easily adapt and utilize existing mesh-based efficient differentiable rendering frameworks, such as NVDiffrast [24] to composite and render an arbitrary number, shape, and position of tablets in fully differentiable manner. Pseudo Mesh Construction. We can leverage differentiable mesh rendering frameworks like NVDiffrast [24] by converting the tablets into pseudo meshes before each rendering pass. Note that this conversion is just used for the adaption of NVDiffrast. Given single tablet ti, we can convert it to two mesh triangle faces through the following process: vi = [pi rui/λu,i rvi/λv,i, pi rui/λu,i + rvi/λv,i, pi + rui/λu,i + rvi/λv,i, pi + rui/λu,i rvi/λv,i] fi =[[0, 1, 2], [0, 2, 3]] (1) (2) Where vi represents the 3D vertex coordinates, and fi denotes the face indices, consistent with the general mesh definition. The texture and alpha maps of all tablets are tiled onto global texture map according to their respective resolutions (rui, rvi). The specific tile location serves as the texture coordinates for the four vertices of each tablet. Multi-layer Rasterization. As transparency information is used in AlphaTablets, the rasterization process in our approach requires rasterizing multiple layers through depth peeling to extract multiple closest surfaces for each pixel. Given the model-view-projection (MVP) matrix Mk of specific view k, the rasterization result of the l-th closest surface for the image pixel (i, j) can be formed as: Rl(Mk, i, j) = (ui,j, vi,j, trii,j) (3) where ui,j and vi,j are the barycentric coordinates within triangle, and trii,j is the triangle index. The color and alpha values c(i, j) and α(i, j) are then acquired from the texture map using the two coordinates and the triangle index. Anti-aliasing for AlphaTablets. Previous anti-aliasing techniques in rasterization predominantly work on non-transparent primitives without considering learnable alpha values of each face, yet alpha channel is crucial component for AlphaTablets. An intuitive approach to incorporate alpha channels would be to conduct anti-aliasing for both texture and alpha within each rasterization layer. However, this straight-forward method does not work well on tablets with alpha channels. simple counterexample is two overlapping planes with constant alpha values of 0 and 1, respectively. The rasterization results in two rasterized layers for the intersection pixel of the two planes. And the anti-aliasing would result in both layers having an alpha value below 1, causing incorrect transparency and colors at the intersected boundaries in the final alpha blending process. To address this issue, we propose an anti-aliasing method for the semi-transparent primitives. Given pixel through which the boundary lines of two planes pass, with the original colors c1 and c2, and alpha values a1 and a2, respectively, and anti-aliasing weights and 1 w, the process to obtain the anti-aliased color caa is as follows: caa = α1c1w + α2c2(1 w) α1w + α2(1 w) (4) And the alpha value is not anti-aliased. The intuitive idea is that the blending weights of anti-aliasing should not only be determined by overlapping areas, but also take alpha values into consideration. For each rasterization layer, anti-aliasing can be further expressed using the following formula: caa = AA(αc) AA(α) , αaa = α (5) Where α and are the alpha and color values before anti-aliasing, caa and αaa are the alpha and color values after anti-aliasing, and AA is the anti-aliasing function. We show an example figure 9 in appendix to demonstrate the clear improvements after the anti-alias formula correction. Alpha Composition. Once we have multiple rasterized layers, we can stack them in depth order and blend them using the alpha compositing process widely employed in volume rendering and neural Figure 2: Pipeline of our proposed 3D planar reconstruction. Given monocular video as input, we first initialize AlphaTablets using off-the-shelf superpixel, depth, and normal estimation models. The 3D AlphaTablets are then optimized through photometric guidance, followed by the merging scheme. This iterative process of optimization and merging refines the 3D AlphaTablets, resulting in accurate and complete 3D planar reconstruction. radiance fields. The final color of pixel (i, j) can be calculated as: ci,j = (cid:88) l=1 Ti,j,lαi,j,lci,j,l, Ti,j,l = l1 (cid:89) m=1 (1 αi,j,m) (6) Where ci,j,l and αi,j,l are the color and alpha values at pixel (i, j) of the l-th rasterization layer. Ti,j,l represents the accumulated transmittance of the previous 1 layers for the given pixel. 3.3 Bottom-up Planar Reconstruction Pipeline with AlphaTablets AlphaTablets provide flexible 3D plane modeling and efficient differentiable rendering. Building on this, we propose bottom-up 3D planar reconstruction pipeline for monocular video input. As illustrated in Figure 2, AlphaTablets are first initialized via off-the-shelf geometric estimations, then the texture and geometry parameters of AlphaTablets are optimized using differentiable rendering. Tablets are examined and merged towards larger and more complete 3D planes. The optimization and merging benefit from each other through iterative refinement. By formulating the plane segmentation as bottom-up clustering of 3D AlphaTablets and plane parameter estimation as rendering based optimization, our pipeline can accurately reconstruct detailed planar surfaces. Initialization. We initialize the AlphaTablets using off-the-shelf geometric prediction models, including those for depth, surface normal and superpixel. Specifically, for each keyframe of the input video, we first apply SLIC [1] method to segment images into 2D superpixels based on local color homogeneity. Next, we utilize pretrained geometric models [19, 12] to estimate depth and surface normal for each image. To obtain initial depth and surface normal values for each superpixel, we perform 2D average pooling within each superpixels region. These 2D superpixels are then backprojected into 3D space to form the initial tablet representation. Besides 3D geometry, the texture maps and alpha channels are initialized using 2D pixel colors and superpixel masks, respectively. The rectangle bounding box is determined by the minimum and maximum values on each 3D tablets two orthogonal axes: up and right vector. Optimization. Initialized from 2D view-based depth, surface normal and superpixel estimations, the initial 3D AlphaTablets may contain errors, overlaps, and inconsistency. We thus perform differentiable rendering based optimization to update the parameters of 3D AlphaTablets. Learnable Parameters. While the 3D AlphaTablets offer significant flexibility, directly optimizing them for unrestricted movement in 3D space can cause instability. To mitigate this, we constrain each tablets center to remain on its initial camera ray. In this way, we thus optimize the normal vector 5 and distance d, rather than 3D center position p, where represents the distance between the tablets center and the camera center of the view from which it was initialized. The up vectors of tablets are updated with normal to keep the rigid transformation characteristics of the tablet. Additionally, the texture and alpha channel values of each tablet are treated as learnable parameters, enabling appearance refinement to enhance the fidelity of the reconstructed planar surfaces. Loss Design. The optimization process is driven by set of carefully designed loss functions that collectively refine the tablet parameters to achieve accurate planar reconstruction. Given input monocular video, we adopt the photometric loss as the mean squared error between the rendered image of the tablets and the observed input images cgt: Lpho = cgt2 2. By minimizing the photometric loss, the 3D AlphaTablets can be optimized to better fit the input images. However, due to the ambiguity of photometric alignment and the complexity of optimization, updating AlphaTablets only with photometric loss results in fuzzy reconstructions. We thus introduce several important losses to help mitigate the ambiguity and regularize the reconstruction. Specifically, we use alpha inverse loss to prevent the emergence of semi-transparent regions after alpha composition, which is defined as Lainv = (cid:81)L l=1(1 αl). Moreover, we observed that multiple semi-transparency tablets, instead of one solid tablet, may occupy the same surface region to blend the rendering, which harms the geometric surface reconstruction. Inspired by mip-NeRF-360 [5], we utilize the distortion loss for AlphaTablets to penalize the multiple semi-transparency surfaces and promote the merging of tablets that are in close proximity. The distortion loss is defined as below: Ldist = L1 (cid:88) i=1 TiTi+1pi pi+1 (7) Where Ti is the blending weight of the i-th rasterization layer defined in Sec. 3.2, pi is the 3D intersection point of the i-th rasterization layer. To further regularize the surface geometry and smoothness, we render the tablets to get the depths dr and surface normal maps nr, and supervise them by the input monocular depth and surface normal estimations dm, nm with mean squared error: (cid:88) dr = Tlαldl, nr = (cid:88) Tlαlnl = l=1 dr l=1(1 αl) Ldepth = dm2 (cid:81)L , = l=1 nr nr2 2, Lnormal = nm2 The final objective is defined as: = w1Lpho + w2Lainv + w3Ldist + w4Ldepth + w5Lnormal where w1, w2, w3, w4, w5 are hyperparameters to balance the losses. (8) (9) (10) (11) Merging Scheme. The optimized 3D AlphaTablets are still describing local 3D planar surface, bounded by the 2D superpixels. Therefore, to represent the exact 3D planes, we need to coherently merge the individual tablets into larger tablets. We introduce hierarchical merging strategy that considers hybrid information including color, distance, and normal, to prevent the wrong merging. Specifically, we first construct KD-tree to find the tablet neighborhoods, and initialize union-find data structure for all tablets. Each union-find set dynamically maintains the average color, center, and normal of all its constituent unit tablets. For each tablet, we search the KD-tree to find the nearest tablets and evaluate whether they satisfy the following constraints for merging: (1) The angle between the normals of the two tablets should be smaller than threshold θ. (2) The angle between the average normals of the union-find sets to which the two tablets belong should be smaller than threshold θs. (3) The projected distance between the average centers of the union-find sets along their average normal should be smaller than threshold d. (4) The difference between the average colors of the union-find sets should be smaller than threshold c. If all these constraints are satisfied, the two tablets are merged into the same union-find set. We repeat this process for all tablets, continually updating the average color, center, and normal of each union-find set as merges occur. This iterative merging procedure continues until no further merges are possible, resulting in set of coherent planar surfaces represented by the final merged tablets. More details about merging are included in the appendix. 6 Figure 3: Qualitative results on ScanNet. Error maps are included. Better viewed when zoomed in. Table 1: 3D geometry reconstruction results on ScanNet. Method Comp Acc Recall Prec NeuralRecon [46] + Seq-RANSAC Atlas [31] + Seq-RANSAC ESTDepth [29] + PEAC [13] PlanarRecon [50] Metric3D [19] + Seq-RANSAC SuGaR [18] + Seq-RANSAC Ours 0.144 0.102 0.174 0.154 0.074 0.121 0.108 0.128 0.190 0.135 0.105 0.379 0.324 0. 0.296 0.316 0.289 0.355 0.426 0.385 0.481 0.306 0.348 0.335 0.398 0.161 0.296 0."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation on 3D Plane Detection and Reconstruction F-Score 0.296 0.331 0.304 0.372 0.231 0.327 0.456 Implementation Details. We use Metric3Dv2 [19] for predicting monocular depths and Omnidata [12] for surface normals. We leverage the keyframe selection method in NeuralRecon [46] to segment the scene into multiple parts. Each part undergoes separate optimization, followed by joint optimizations. The keyframe number of each part is set to 9. The separate optimization for each part is performed for 32 epochs, while the joint optimization step is executed for 9 epochs. The weights for the loss functions are set as follows: [w1, w2, w3, w4, w5] = [1.0, 1.0, 20.0, 4.0, 4.0]. We use Adam optimizer, and the learning rates for the tablets texture, alpha, normal, and distance are set to 0.01, 0.03, 1e-4, and 5e-4, respectively. After the second merge step, the learning rate for the distance is reduced to 2e-4. The normal threshold is set to 0.93. The entire reconstruction process for single scene takes around 2 hours on average when executed on single A100 GPU. Dataset and Evaluation Metrics. We use ScanNetv2 [11] dataset to evaluate the 3D plane detection and reconstruction performance of our proposed method. Following PlanarRecon [50], our method is tested on the validation set of Atlas [31] using generated 3D plane ground truth. For evaluation metrics, we follow previous works [50, 26, 54] to use Murezs 3D metrics [31] for geometry, and rand index (RI), variation of information (VOI), segmentation covering (SC) as plane segmentation metrics. To assess the segmentation performance, the reconstructed plane instances are transferred onto the ground truth planes using the nearest neighborhood approach, following common practices. Table 2: 3D plane segmentation results on ScanNet."
        },
        {
            "title": "Method",
            "content": "NeuralRecon [46] + Seq-RANSAC Atlas [31] + Seq-RANSAC ESTDepth [29] + PEAC [13] PlanarRecon [50] Metric3D [19] + Seq-RANSAC SuGaR [18] + Seq-RANSAC Ours VOI 8.087 8.485 4.470 3.622 4.648 5.558 3.468 RI 0.828 0.838 0.877 0.897 0.862 0.775 0.928 SC 0.066 0.057 0.163 0.248 0.209 0.082 0.273 TUM Dataset Replica Dataset Figure 4: Qualitative results on TUM-RGBD and Replica datasets. Baselines. We compare our method with different types of representative works. PlanarRecon [50] is the state-of-the-art method of learning-based 3D planar reconstruction from monocular video. Following it, we compare with strong baselines that first reconstruct the 3D scene and then fit 3D planes using RANSAC, including single or multi-view depth-based methods [29, 19], 3D volumebased methods [46, 31], and the recent 3D gaussian based method [18]. Quantitative Results. The evaluation results for 3D plane geometry and segmentation are presented in Table 1 and Table 2, respectively. Our proposed method achieves clear improvements compared to other state-of-the-art approaches across various evaluation metrics. 3D volume-based reconstruct-then-fit methods suffer from reconstruction errors and noise threshold-sensitive RANSAC, and exhibit relatively low performance in terms of planes geometry and 3D coherence. Depth-based methods inherently encounter 3D inconsistency, resulting in fragmented and multi-layered predictions. PlanarRecon [50], which is specially trained on the ScanNet dataset, demonstrates the capability to predict major planes with high geometric accuracy. However, its performance is hindered by the limited coverage of the predicted planes and the failure to detect many small plane instances. Approaches based on 2D Gaussian Splatting [18] tend to be heavily influenced by the poor initialization, textureless regions and motion blurs in ScanNet, resulting in degraded reconstruction performance. Compared to other methods, our approach demonstrates much improved overall performance for both geometry and segmentation. Qualitative Results. To provide qualitative assessment of our methods performance, we follow PlanarRecon [50] and present the plane reconstruction results in Figure 3, along with the recall error maps. The SuGaR+Seq-RANSAC method suffers from erroneous geometric reconstructions, and the Metric3D+Seq-RANSAC is constrained by inconsistent fuzzy points and sub-optimal plane segmentations. PlanarRecon, while capable of reconstructing large planar surfaces with high geometric accuracy, struggles to capture and reconstruct smaller plane instances, resulting in incomplete representations of the scene. Our method benefits from the bottom-up planar reconstruction scheme, and can accurately predict the 3D plane instances while excelling in detecting and reconstructing details, particularly for smaller plane instances. This capability significantly outperforms other methods in handling fine-grained planar structures. To demonstrate the generalization ability of our method, we further test it on TUM-RGBD [45] and Replica [44] datasets. Qualitative results are shown in Figure 4. Our method can faithfully reconstruct 3D planar surfaces in various scenarios. 4.2 Ablation Studies To validate the efficacy of our methods design, we conducted series of ablation experiments exploring the impact of various components, including the loss functions, merge schemes, and tablet anti-aliasing. The results are presented in Table 3. Tablet distortion loss encourages the planar surfaces to converge and merge, leading to improved performance. Furthermore, the normal loss and depth loss significantly contribute to the geometric accuracy of the reconstructed planes, particularly 8 Table 3: Ablation studies. AlphaInv denotes the alpha inverse loss."
        },
        {
            "title": "Method",
            "content": "F-score VOI RI SC Only Photometric and AlphaInv loss + Tablet Distortion loss + Normal loss + Depth loss w/o tablet anti-aliasing w/o tablet merge Full 0.240 0.271 0.425 0. 0.415 0.188 0.456 4.096 3.741 3.490 3.466 3.545 6.991 3.466 0.936 0.937 0.944 0.944 0.937 0.939 0.944 0.191 0.253 0.263 0. 0.280 0.098 0.284 Original Scene 3D Coherent Scene Editings Figure 5: 3D scene editing examples of our method. in textureless regions where photometric loss constraints are insufficient. Merging scheme is crucial for producing appropriate 3D planes. Without merging, the 3D AlphaTablets remain small plane fragments, and thus can not reconstruct 3D planes, as shown in Table 3. Moreover, tablet antialiasing contributes to smoother results, leading to enhanced overall performance. 4.3 Example Application: 3D Plane-based Scene Editing One of the significant advantages of AlphaTablets representation is its ability to perform consistent 3D scene editing by simply modifying the 2D texture maps associated with the reconstructed planes. As illustrated in Fig. 5, our method can achieve impressive results for editing 3D scenes. The accurate plane reconstruction allows for precise texture mapping, enabling the seamless application of textures, text, or other visual elements onto the planar regions within the scene. Furthermore, our method offers the flexibility to modify the color or perform style transfer on individual planes, providing powerful tool for creative scene manipulation. 4.4 Limitations and Future Work AlphaTablets effectively represent 3D planes, but it may struggle in highly non-planar regions where the planar assumption for single superpixel does not hold. Additionally, the current AlphaTablets representation does not account for view-dependent effects. As result, the optimization relies on color consistency across views, which can be compromised by non-Lambertian surfaces or changes in lighting. In the future, we aim to enhance AlphaTablets with view-dependent modeling, and explore hybrid scene representation such as AlphaTablets with Gaussians."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce AlphaTablets, novel and versatile 3D plane representation. AlphaTablets use rectangles with alpha channels to represent 3D planes, allowing for flexible and effective arbitrary 3D plane modeling. We derive differentiable rasterization process for AlphaTablets to enable efficient 3D-to-2D rendering. Building on this, we propose novel bottom-up 3D planar reconstruction pipeline from monocular video input. Leveraging the AlphaTablets representation, along with carefully designed optimization and merging schemes, our pipeline can reconstruct highly accurate and complete 3D planar surfaces in generalizable manner. Experiments on the ScanNet dataset demonstrate significant improvements over baseline methods, highlighting the potential of AlphaTablets as general representation for 3D planar surfaces."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by Beijing Science and Technology plan project (Z231100005923029), the Natural Science Foundation of China (Project Number 62332019) and Beijing Natural Science Foundation (L222008)."
        },
        {
            "title": "References",
            "content": "[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):22742282, 2012. [2] Samir Agarwala, Linyi Jin, Chris Rockwell, and David Fouhey. Planeformers: From sparse view planes to 3d reconstruction. In European Conference on Computer Vision, pages 192209. Springer, 2022. [3] Alberto Argiles, Javier Civera, and Luis Montesano. Dense multi-planar scene estimation from sparse set of images. In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 44484454. IEEE, 2011. [4] Caroline Baillard and Andrew Zisserman. Automatic reconstruction of piecewise planar models from multiple views. In Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), volume 2, pages 559565. IEEE, 1999. [5] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54705479, 2022. [6] Adrien Bartoli. random sampling strategy for piecewise planar scene segmentation. Computer Vision and Image Understanding, 105(1):4259, 2007. [7] Dorit Borrmann, Jan Elseberg, Kai Lingemann, and Andreas Nüchter. The 3d hough transform for plane detection in point clouds: review and new accumulator design. 3D Research, 2(2):113, 2011. [8] Zheng Chen, Qingan Yan, Huangying Zhan, Changjiang Cai, Xiangyu Xu, Yuzhong Huang, Weihan Wang, Ziyue Feng, Lantao Liu, and Yi Xu. Planarnerf: Online learning of planar primitives with neural radiance fields. arXiv preprint arXiv:2401.00871, 2023. [9] Alejo Concha and Javier Civera. Dpptam: Dense piecewise planar tracking and mapping from monocular sequence. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 56865693. IEEE, 2015. [10] Alejo Concha, Muhammad Wajahat Hussain, Luis Montano, and Javier Civera. Manhattan and piecewiseplanar constraints for dense monocular mapping. In Robotics: Science and systems, 2014. [11] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. [12] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1078610796, 2021. [13] Chen Feng, Yuichi Taguchi, and Vineet Kamat. Fast plane extraction in organized point clouds using agglomerative hierarchical clustering. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 62186225. IEEE, 2014. [14] Martin Fischler and Robert Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381395, 1981. [15] Yasutaka Furukawa, Brian Curless, Steven Seitz, and Richard Szeliski. Manhattan-world stereo. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 14221429. IEEE, 2009. [16] David Gallup, Jan-Michael Frahm, and Marc Pollefeys. Piecewise planar and non-planar stereo for urban scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14181425. IEEE, 2010. [17] Yiming Gao, Yan-Pei Cao, and Ying Shan. Surfelnerf: Neural surfel radiance fields for online photorealistic reconstruction of indoor scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 108118, 2023. [18] Antoine Guédon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023. [19] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. [20] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. arXiv preprint arXiv:2403.17888, 2024. [21] Jingwei Huang, Angela Dai, Leonidas Guibas, and Matthias Nießner. 3dlite: towards commodity 3d scanning for content creation. ACM Trans. Graph., 36(6):2031, 2017. [22] Linyi Jin, Shengyi Qian, Andrew Owens, and David Fouhey. Planar surface reconstruction from sparse views. In Proc. of the IEEE/CVF International Conference on Computer Vision, pages 1299113000, 2021. 10 [23] Vladimir Kolmogorov and Ramin Zabin. What energy functions can be minimized via graph cuts? IEEE transactions on pattern analysis and machine intelligence, 26(2):147159, 2004. [24] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics (ToG), 39(6):1 14, 2020. [25] Zhi-Hao Lin, Wei-Chiu Ma, Hao-Yu Hsu, Yu-Chiang Frank Wang, and Shenlong Wang. Neurmips: Neural mixture of planar experts for view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1570215712, 2022. [26] Chen Liu, Kihwan Kim, Jinwei Gu, Yasutaka Furukawa, and Jan Kautz. Planercnn: 3d plane detection and reconstruction from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44504459, 2019. [27] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Yasutaka Furukawa. Planenet: Piece-wise planar reconstruction from single rgb image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 25792588, 2018. [28] Jiachen Liu, Pan Ji, Nitin Bansal, Changjiang Cai, Qingan Yan, Xiaolei Huang, and Yi Xu. Planemvs: 3d plane reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86658675, 2022. [29] Xiaoxiao Long, Lingjie Liu, Wei Li, Christian Theobalt, and Wenping Wang. Multi-view depth estimation using epipolar spatio-temporal networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82588267, 2021. [30] Hannes Möls, Kailai Li, and Uwe Hanebeck. Highly parallelizable plane extraction for organized point clouds using spherical convex hulls. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 79207926. IEEE, 2020. [31] Zak Murez, Tarrence Van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-end 3d scene reconstruction from posed images. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16, pages 414431. Springer, 2020. [32] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [33] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 652660, 2017. [34] Yiming Qian and Yasutaka Furukawa. Learning pairwise inter-plane relations for piecewise planar reconstruction. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16, pages 330345. Springer, 2020. [35] Carolina Raposo, Miguel Lourenço, Michel Antunes, and Joao Pedro Barreto. Plane-based odometry using an rgb-d camera. In BMVC, volume 2, page 6, 2013. [36] Renato Salas-Moreno, Ben Glocken, Paul HJ Kelly, and Andrew Davison. Dense planar slam. In IEEE international symposium on mixed and augmented reality (ISMAR), pages 157164. IEEE, 2014. [37] Ruwen Schnabel, Roland Wahl, and Reinhard Klein. Efficient ransac for point-cloud shape detection. In Computer graphics forum, volume 26, pages 214226. Wiley Online Library, 2007. [38] Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. [39] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. [40] Jingjia Shi, Shuaifeng Zhi, and Kai Xu. Planerectr: Unified query learning for 3d plane recovery from single view. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 93779386, 2023. [41] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. [42] Sudipta Sinha, Drew Steedly, and Rick Szeliski. Piecewise planar stereo for image-based rendering. In 2009 International Conference on Computer Vision, pages 18811888, 2009. [43] Christiane Sommer, Yumin Sun, Leonidas Guibas, Daniel Cremers, and Tolga Birdal. From planes to corners: Multi-purpose primitive detection in unorganized 3d point clouds. IEEE Robotics and Automation Letters, 5(2):17641771, 2020. [44] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. [45] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. benchmark for the evaluation of rgb-d slam systems. In Proc. of the International Conference on Intelligent Robot Systems (IROS), Oct. 2012. [46] Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1559815607, 2021. [47] Bin Tan, Nan Xue, Song Bai, Tianfu Wu, and Gui-Song Xia. Planetr: Structure-guided transformers for 3d plane recovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11 41864195, 2021. [48] Bin Tan, Nan Xue, Tianfu Wu, and Gui-Song Xia. Nope-sac: Neural one-plane ransac for sparse-view planar 3d reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [49] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. [50] Yiming Xie, Matheus Gadelha, Fengting Yang, Xiaowei Zhou, and Huaizu Jiang. Planarrecon: Real-time 3d plane detection and reconstruction from posed monocular videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62196228, 2022. [51] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54385448, 2022. [52] Fengting Yang and Zihan Zhou. Recovering 3d planes from single image via convolutional neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 85100, 2018. [53] Shichao Yang and Sebastian Scherer. Monocular object and plane slam in structured environments. IEEE Robotics and Automation Letters, 4(4):31453152, 2019. [54] Zehao Yu, Jia Zheng, Dongze Lian, Zihan Zhou, and Shenghua Gao. Single-image piece-wise planar 3d reconstruction via associative embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10291037, 2019."
        },
        {
            "title": "A Appendix",
            "content": "A.1 More Details of AlphaTablets Optimization Update of up vector. Note that the tablets up vector should not be learned during the optimization process. By considering the tablets motion as rigid transformation, any in-plane rotation can be accounted for by optimizing the texture and alpha values. However, we need to establish an update rule for the up vector to keep the rigid transformation characteristics of the tablet. Our design is to apply the same rotation to the up vector as the one applied to the normal vector during the update. Given the normal vectors and (before and after the update), and the up vector u, we can acquire the new up vector by: θr = n, = n, (cid:32) 0 r3 r2 (cid:33) = r3 0 r2 r1 0, = cos θr + sin θr + rrT (1 cos θr), = Ru (12) (13) (14) (15) (16) Merging Scheme. As explained in Section 3.3, we first construct KD-tree to find the tablet neighborhoods, and initialize union-find data structure for all tablets. Here we actually use the unit tablets, which are defined as the projections of all initial 3D tablets to the current tablets, to build the KD-tree. In other words, we maintain the affiliations of initial and current tablets, and use the updated initial tablets to perform the merging. The reason is that using 3D center point distances among tablets with different 3D sizes is ambiguous. For example, small tablet can have smaller center point distance than non-adjacent small tablet, compared to spatially adjacent big tablet. Using the unit tablets with similar sizes, the neighboring adjacency can be easily determined by checking the center point distances. These unit tablets are used only because they have easily defined neighborhoods. Since the unit tablets of one current tablet come from the projection on this tablet, they share the same surface normal and adjacent positions. Thus, merging with unit tablets will definitely produce larger (or the same) tablets than current tablets. After each merging, all the unit tablets are updated by the projection onto the merged new tablets, and the merged new tablets are fed into the next optimization. Using SLIC superpixel on ScanNet 1296x968 resolution image results in around 10k superpixels for each keyframe, leading to large number of initial tablets. To address the issue, We conduct an initial merge process after AlphaTablets initialization. In practice, we find it is beneficial to the accuracy and convergence speed. Table 5 shows the ablations of the initial merge. Table 4: Ablation studies on initial merge. Method F-score VOI RI SC w/o in-training merge and init merge w/o in-training merge w/o init merge w/ all merge schemes 0.188 0.438 0.454 0.456 6.991 5.171 3.754 3.466 0.939 0.941 0.944 0. 0.098 0.138 0.273 0.284 Weight Check Scheme. During the optimization process, there may be cases where some tablets are nearly invisible from all viewpoints, yet they have relatively large transparency value. In such situations, these tablets should be removed. Additionally, there could be instances where certain regions of tablet are not visible from any viewpoint. In these cases, those specific regions of the tablet should be excluded. To address these scenarios, we designed weight check mechanism: We perform rasterization step at all viewpoints and extract the points where the alpha blending weight exceeds certain threshold (we choose 0.3 in our implementation). We record the tablet index corresponding to each of these points. Before the merging step, we perform the weight check by removing tablets with an excessively 13 low number of associated points. Furthermore, for each tablet, we recalculate its boundary based on the texture map coordinate ranges of all the points associated with that tablet. Tablet-camera assignment. We always maintain affiliations between the initial tablets and the current (merged) tablets (as stated in Sec A.1), and we keep track of the camera index that initially generated each initial tablet. When tablets are merged, we count the number of each camera index corresponding to all affiliated initial tablets and assign the most frequently occurring camera to the newly merged tablet. A.2 Additional Implementation Details Baselines. For 3D volume-based methods including Atlas, NeuralRecon, PlanarRecon, and Metric3D with TSDF fusion, we followed PlanarRecon to use their enhanced version of Seq-RANSAC. We refer to PlanarRecon for detailed descriptions. For point-based methods such as SuGaR, since PlanarRecons Seq-RANSAC requires 3D TSDF volume as inputs and cannot be easily adapted to points or meshes, we use the classical vanilla Seq-RANSAC, which iteratively applies RANSAC to fit planes. Here we used Open3D plane RANSAC implementation for each iteration. The hyperparameters are carefully tuned for optimal performance. For the Metric3D baseline, We used the official Metric3D v2 implementation and pre-trained weights (v2-g) running on each keyframe to get depth maps, followed by TSDF fusion to fuse into 3D volume. Finally, PlanarRecons Seq-RANSAC is applied to the 3D TSDF volume to get the planar results. We adopted the original implementation for the SuGaR baseline, during the COLMAP pre-processing, we feed ground-truth camera poses into the pipeline, which provides better initial sparse points. After optimization, SuGaR outputs the mesh model, and we uniformly sampled 100k surface points and applied vanilla Seq-RANSAC on top of sampled points to get the 3D planar results. Quantitative results for other baselines (Atlas, NeuralRecon, PEAC, PlanarRecon) were taken from PlanerRecon. Details of tablet properties. For pixel range (ru, rv), each tablets geometry is located in 3D space, while its texture is stored in 2D. The pixel range represents the resolution at which the texture is stored: it is derived directly from the range in the source image for initial tablets; for merged tablets, the pixel range is calculated as the average of all corresponding initial tablets. The distance ratios (λu, λv) establish the relationship between the 2D texture resolution and the 3D size of the tablet. For initial tablets, the distance ratio is calculated by dividing the cameras focal length by the average initial distance of the tablet. For merged tablets, the distance ratio is the average of all corresponding initial tablets ratios. The alpha channel of tablets is learnable single-channel map with the same shape as the texture map. A.3 Additional Discussions Different initialization for SuGaR baseline. We further experiment on our ablation subset to compare the COLMAP initialization with Metric3Ds dense depth-based initialization similar to our method. For Metric3D init, we use the same keyframes as our method and randomly sample total of 100,000 points as initial points. The results are shown in the table: Table 5: Ablation studies on different initialization of SuGaR. Method F-score VOI RI SC SuGaR+COLMAP Initialization SuGaR+Metric3D Initialization Ours 0.300 0.326 0. 5.759 5.670 3.466 0.797 0.789 0.944 0.090 0.102 0.284 The ScanNet dataset presents significant challenges like numerous blurry and textureless regions, which are especially problematic for Gaussian-based methods like SuGaR when reconstructing clear geometry. Also, SuGaR heavily relies on COLMAP reconstruction to initialize, but the COLMAP reconstruction on ScanNet is sometimes noisy, affecting the final performance. The Metric3D initialization method does indeed enhance the reconstruction quality of SuGaR (as shown in Fig. 6), but the overall reconstruction quality remains constrained, with noticeable jitter and challenges in accurately delineating planar regions, leading to an inferior performance to our approach. 14 Figure 6: Qualitative Comparison of Initialization Methods for SuGaR. Breakdown of Time Budget. Below is breakdown of the time budget for the optimization process of single scene: Table 6: Breakdown of the time budget of single scene. Stage Task Initialization Render Loss Calculation Training Merge texture init geometry init pseudo mesh rasterization alpha composition photometric loss depth loss normal loss distortion loss backward kd-tree,union-find set geometric calculation tablet projection weight check Time (s) 1517.38 1672.57 10.39 316.62 2.15 1.07 28.28 102.44 5. 3347.83 96.41 23.14 22.26 62.14 The merge and rendering pipeline is relatively efficient, while the initialization process (which includes converting every superpixel to an initial tablet, and texture initialization) consumes significant amount of time. This is due to the current naive demonstration implementation, where tens of thousands of Python loops are called, which can be improved to enable parallelized initialization in future work. Furthermore, the NVDiffRast renders more than ten layers to perform alpha composition every forward pass, but most of the scenes structure is single-layered, resulting in substantial backward computation burden during training. We regard this as another potential area for considerable optimization in the future work. 3D reconstruction accuracy. The difference in 3D accuracy (termed as Acc in Tab. 1 of the main paper) between our method and PlanarRecon on the ScanNet dataset can be attributed to several factors. First is the scope of reconstruction: PlanarRecon often only reconstructs large planar regions. This allows for easier localization and high accuracy in these specific areas, but it limits overall coverage and performance. Our method enables more comprehensive reconstruction, including smaller planar regions, which can impact the accuracy metrics but provide more complete representation of the scene. Another is the ground-truth coverage: It is worth noting that the 3D ground truth planes in ScanNet only partially cover the scene within the cameras view. Even after 15 Figure 7: Demonstration of Insufficient Coverage of 3D Ground-Truth Labels: The 3D ground truth labels only partially cover the range within the cameras view. Most of the red regions in the figure highlight this issue. While these uncovered areas reduce accuracy, they should not be considered negative outcome. Figure 8: Visualization of Tablet Count Evolution. excluding areas too distant to be relevant using the camera frustum, significant portions remain uncovered. PlanarRecon learns to exclude distant reconstructions during its training stage, leading to improved accuracy metrics. Our method, however, is capable of identifying planar regions for all visible areas (evident in Fig. 7 where most of the red regions highlight this phenomenon). While these uncovered areas affect the evaluation accuracy, they should not necessarily be considered negative outcome. Our method provides more complete reconstruction of the scene, including areas not represented in the ground truth data. Tablet count evolution. We demonstrate the tablet count evolution of single scene in Fig. 8. The number of tablets decreases rapidly in the early merging stages and gradually converges into several hundred. Notably, the final tablets contain large portion of small tablets representing non-planar regions, while the primary planar scene structure is adequately represented with fewer tablets. A.4 Additional Qualitative Results We provide more qualitative results in Fig. 10 and Fig. 11. A.5 Broader Impacts 3D planar reconstruction and editing have the potential to revolutionize numerous fields such as entertainment, media, accessibility, manufacturing, etc, by enhancing visualization, interaction, and 16 Naive Anti-aliasing Our Tablet Anti-aliasing Figure 9: Qualitative comparison of our tablet anti-aliasing scheme. Naive anti-aliasing will lead to wrong strip artifacts, while our anti-aliasing scheme effectively mitigates those artifacts. understanding. However, it may raise concerns about privacy and data security, necessitating robust policies and safeguards. 17 Figure 10: More qualitative results on ScanNet. Error maps are included. Better viewed when zoomed in. Figure 11: More qualitative results on TUM-RGBD and Replica datasets."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "ETH Zurich",
        "Tsinghua University"
    ]
}