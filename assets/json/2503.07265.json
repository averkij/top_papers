{
    "paper_title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation",
    "authors": [
        "Yuwei Niu",
        "Munan Ning",
        "Mengren Zheng",
        "Bin Lin",
        "Peng Jin",
        "Jiaqi Liao",
        "Kunpeng Ning",
        "Bin Zhu",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose $\\textbf{WISE}$, the first benchmark specifically designed for $\\textbf{W}$orld Knowledge-$\\textbf{I}$nformed $\\textbf{S}$emantic $\\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce $\\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 6 2 7 0 . 3 0 5 2 : r WISE: World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation Yuwei Niu1,2*, Munan Ning1, Mengren Zheng2, Bin Lin1, Peng Jin1, Jiaqi Liao, Kunpeng Ning1, Bin Zhu1,4, Li Yuan1,3 1Peking University, 2Chongqing University, 3PengCheng Laboratory, 4Rabbitpre AI niuyuwei04@gmail.com, yuanli-ece@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at PKU-YuanGroup/WISE. 1. Introduction Text-to-Image (T2I) models [53] are capable of generating high-resolution and visually appealing images. However, these models often struggle with factual accuracy, particularly when presented with prompts requiring complex semantic understanding and world knowledge. This deficiency primarily arises from their limited incorporation of world knowledge [51] the vast and diverse information, facts, and relationships that constitute real-world understanding. *Equal contribution Corresponding Author Figure 1. Comparison of previous straightforward benchmarks and our proposed WISE. (a) Previous benchmarks typically use simple prompts, such as photo of two bananas in GenEval [9], which only require shallow text-image alignment. (b) WISE, in contrast, uses prompts that demand world knowledge and reasoning, such as Einsteins favorite musical instrument, to evaluate models ability to generate images based on deeper understanding. Critically, existing evaluation benchmarks fail to adequately assess this core capability, hindering progress towards truly robust and reliable T2I systems. Most T2I benchmarks suffer from lack of semantic complexity. As shown in Figure 1, they use overly straightforward and simple prompts, failing to effectively challenge models ability to understand and generate images based on the models world knowledge. Furthermore, the most commonly used metric FID [12] primarily focuses on the realism of generated images. Although some benchmarks [11, 46, 50] utilize models like CLIP [30] to assess image-text semantic consistency scoring. However, CLIPs limitations [52] in capturing fine-grained semantic information and handling complex reasoning hinder comprehensive assessment of models performance in processing intricate semantic information. Consequently, existing evaluations fail to fully reveal the potential of models in real-world scenarios, particularly in tasks requiring world knowledge. For instance, when generating an image depicting tadpole that has undergone metamorphosis, model needs not only to comprehend the textual description (tadpole, metamorphosis) but also to invoke its internal world knowledge. 1 This includes understanding amphibian development, the specific morphological changes involved (e.g., the growth of legs, the loss of the tail, the development of lungs), and the biological processes driving this transformation. Our experiment results firstly demonstrate that existing T2I models exhibit significant limitations in integrating world knowledge, while the unified model underperforms even compared to dedicated T2I models. To address the aforementioned problems, we propose novel benchmark, WISE (World Knowledge-Informed Semantic Evaluation), designed to holistically assess models capabilities in semantic understanding and world knowledge integration through more complex T2I prompts. WISE covers three major domains: natural science, spatio-temporal reasoning, and cultural common sense, encompassing 25 sub-domains and total of 1000 evaluation prompts. To rigorously assess the alignment of generated images with world knowledge, we introduce WiScore, novel composite metric that emphasizes the accurate depiction of objects and entities within the generated image. WiScore is calculated as weighted average of three key aspects: Consistency, Realism, and Aesthetic Quality. Moreover, traditional benchmarks always focus on dedicated T2I models, overlooking the potential of unified multimodal models, which integrate powerful LLMs trained on extensive text and image-text pairs and possess demonstrably strong world knowledge. While some studies have begun to explore whether the strong understanding capabilities of these unified models can benefit image generation, they often rely on overly simplistic benchmarks, thus failing to sufficiently prove this phenomenon. We broadened the scope of our evaluation beyond traditional dedicated T2I models. We employed our novel benchmark, WISE, to evaluate total of 20 T2I models, encompassing both 10 dedicated T2I models and 10 unified multimodal models. However, experiment results demonstrate significant deficiencies in complex semantic understanding and world knowledge integration across existing T2I models. Even for unified multimodal models, their strong understanding capabilities do not fully translate into advantages in image generation, as revealed by our WISE evaluation. Despite their theoretical advantages, unified models generally underperform compared to dedicated T2I models in leveraging world knowledge for image generation. This indicates that current approaches to integrating LLMs within unified multimodal models may not yet fully unlock their potential for image generation that effectively integrates and applies world knowledge. Our main contributions are as follows: We introduce WISE, which is specified for the world knowledge representation capabilities of T2I models, with vast meticulously crafted prompts in stead of traditional simplistic prompts. We propose WiScore, novel composite metric which goes beyond mere pixel-level evaluation and shallow textimage alignment, focusing on consistency, realism, and aesthetic quality. 2. Related Work Text-to-image (T2I) generation models, which aim to generate high-quality and diverse images from text, have garnered significant attention recently. These models fall into two main categories: Dedicated T2I Models and Unified Multimodal Models. 2.1. Dedicated T2I Models Dedicated T2I models represent the mainstream approach in the T2I field and have achieved remarkable progress in recent research. Currently, these models primarily fall into two categories: autoregressive models and diffusion models. Autoregressive [3, 6, 10, 28, 37, 40] models treat image generation as sequence generation problem, similar to text generation. However, due to their computational cost and limitations in image quality, diffusion models have become the dominant paradigm. Diffusion [13, 18, 23, 32, 34] models iteratively add noise to an image and then progressively denoise it, often using pre-trained text encoders (e.g., CLIP [30]) to transform text prompts into embeddings that guide the denoising process. Key advancements include GLIDE [27], which pioneered diffusion models for T2I; Latent Diffusion Models (LDMs) [33], which improve quality and efficiency by operating in latent space; and the Stable Diffusion series [5, 29], landmark achievement built on LDMs. 2.2. Unified Multimodal Models Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4, 8, 16, 17, 21, 25, 35, 38, 39, 41, 4345, 48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and VQ-VAE [42] or Diffusion model serves as detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model. crucial question concerning unified multimodal models is whether their understanding and generation capabilities can mutually enhance each other. Some studies [41, 45] have provided evidence supporting this phenomenon. However, in contrast to the rich and comprehensive benchmarks for multimodal understanding, T2I benchmarks are often relatively 2 Figure 2. Illustrative samples of WISE from 3 core dimensions with 25 subdomains. By employing non-straightforward semantic prompts, it requires T2I models to perform logical inference grounded in world knowledge for accurate generation of target entities. simple, lacking an in-depth examination of complex semantic understanding and world knowledge reasoning, making it difficult to fully prove the phenomenon. 2.3. Text to image evaluation Despite the Frechet Inception Distance (FID) [12] being one of the most widely adopted metrics for evaluating the quality of generated images, it falls short in assessing text-image consistency, thus failing to comprehensively measure the capabilities of text-to-image models. To address this deficiency, researchers have introduced series of more sophisticated and challenging benchmarks [24, 36, 47] and evaluation metrics [11, 19, 24]. For instance, DPG-Bench [14] focuses on evaluating models ability in dense prompt following. T2ICompBench [15] provides benchmark suite for evaluating compositional generation, where prompts typically combine multiple distinct attributes. Furthermore, GenEval [9] serves as an object-centric evaluation framework specifically designed to assess compositional attributes of images, such as object co-occurrence, position, number, and color. However, the prompts used in these benchmarks are mostly straightforward, primarily examining models ability to follow compositional instructions for generation. Recently, some studies have shifted focus towards evaluating the application of specific types of knowledge in T2I models, such as physical reasoning in PhyBench [26] and broad common-sense knowledge in Commonsense-T2I [7]. Nevertheless, these emerging benchmarks are still very limited in their research scope and the quantity of evaluations. 3. Our benchmark: WISE 3.1. WISE construction Going beyond simply mapping words to pixels, we propose to evaluate world knowledge of current T2I models, which refers to the vast and diverse information, facts, and relationships that constitute our understanding of the real world. In our work, we focus on common world knowledge that 3 standing the objects cultural role in the real world. This section encompasses diverse range of topics, categorized into 10 fine-grained sub-domains, including festivals, sports, religion, crafts, architecture, animals, plants, art, celebrities, and daily life. These categories collectively cover broad spectrum of human cultural experiences and knowledge. For example, prompts may involve generating images related to traditional festival customs, characteristic national crafts, iconic landmarks, animals and plants with specific cultural significance, common knowledge from daily life, or events and objects associated with famous figures. Spatio-temporal Reasoning. The Spatio-temporal Reasoning domain in WISE is structured around two key dimensions: Temporal Reasoning and Spatial Reasoning. Temporal Reasoning is divided into Horizontal Temporal, which assesses the understanding of relative temporal relationships between events or objects (e.g., The Statue of Liberty at 10 PM Dubai time.), and Vertical Temporal, which evaluates the comprehension of absolute temporal relationships, involving specific points in time (e.g., morning, noon, evening, seasons, specific years, or centuries). Spatial Reasoning is divided into three subcategories: Different Views, which tests the understanding of different viewpoints, including top view, bottom view, cross-section, side view, mirror image, and the effects of perspective; Geographical Relationships, which assesses the understanding of spatial relationships between cities, countries, continents, and other geographical entities; and Relative Position, which focuses on the understanding of the spatial arrangement of objects relative to each other within scene. Natural Science. Finally, the WISE benchmark includes Natural Science domain, designed to assess whether models can not only comprehend domain-specific scientific knowledge but also apply this understanding to reason about complex scientific scenarios and generate accurate, scientifically consistent images. Generative models, at their core, are designed to model the real world. We aim to evaluate whether these models understanding goes beyond mere visual replication and encompasses the complex scientific knowledge underlying real-world phenomena. For example, model should not only be able to generate images of water, ice, and steam, but also understand the underlying thermodynamic principles (e.g., freezing, evaporation, condensation) that govern the transitions between these states. This deeper understanding would demonstrate that the model is not simply mimicking visual appearances but is also incorporating fundamental scientific facts, such as how temperature influences the state of water. This domain moves beyond general knowledge and delves into specialized concepts in biology, physics, and chemistry. The Natural Science domain comprehensively evaluates models understanding of biological processes and states across different life cycles (e.g., metamorphosis, seaFigure 3. Detailed composition of WISE, consisting of 3 categories and 25 subdomains. can be represented visually. As shown in Figure 2, WISE comprises 1000 prompts designed to assess T2I models understanding and application of this knowledge across three major domains: Cultural Common Sense, Spatio-temporal Reasoning, and Natural Science, which would be divided into 25 subdomains. Data Collection and Prompt Design. We collected prompts from variety of sources, including educational materials, encyclopedias, common knowledge question sets, and synthetic data generated by LLMs. These initial prompts were then refined and expanded by human annotators to ensure clarity, complexity, and unambiguous ground truth. Each prompt is accompanied by an explanation that clarifies the required world knowledge and the reasoning process needed for successful image generation. For example, prompt like The plant often gifted on Mothers Day would have an explanation like The model should generate an image of bouquet of carnations, popular flower symbolizing love and admiration, often given on Mothers Day. Cultural Common Sense. The Cultural Common Sense domain of WISE aims to evaluate models ability to understand and apply culturally specific knowledge in image generation, reflecting crucial aspect of real-world understanding. model should not only understand the visual features of an object (e.g., shape, size, color of turkey) but also align with real-world cultural knowledge, such as the cultural significance of turkey as Thanksgiving food. model lacking this deeper understanding would be an incomplete image generator, only capable of grasping the shallow alignment between nouns and visual forms, without underFigure 4. Illustration of the WISE framework, which employs four-phase verification process (Panel to IV) to systematically evaluate generated content across three core dimensions. The two representative cases, science-domain input candle in space violates oxygendependent combustion principles, while spatiotemporal-domain close-up of summer maple leaf contradicts botanical seasonal patterns, both receiving 0 in consistency (see Evaluation Metrics in Panel III), confirming the benchmarks sensitivity in world knowledge conflicts. sonal changes), time periods (e.g., diurnal and nocturnal variations), and environmental conditions (e.g., responses to stimuli, resource availability), as well as complex animal behaviors (e.g., predation, defense, migration); fundamental physical principles and phenomena including mechanics (gravity, buoyancy, pressure, tension), thermodynamics (phase transitions: evaporation, condensation, freezing, melting, sublimation), optics (refraction, reflection, magnification, dispersion), and material properties (e.g., conductivity, elasticity); and combustion, metal corrosion, solution chemistry (acid-base reactions, precipitation reactions, displacement reactions), and characteristic chemical properties (e.g., Tyndall effect, allotropy, ion-specific solution colors). 3.2. Evaluation Framework Evaluation method In recent years, Multimodal Large Language Models (MLLMs) have achieved remarkable advancements in image understanding, leveraging the world knowledge embedded within large language models to attain strong perception and comprehension capabilities. To mitigate the limitations of CLIPs inherent world knowledge constraints, we employ GPT-4o, powerful MLLM, as the judge to evaluate the performance of T2I models in this work. We adopted meticulously designed and rigorous scoring mechanism, with details illustrated in Appendix. Evaluation Metrics We propose multi-faceted evaluation protocol to rigorously assess the quality of generated images, focusing on four key aspects: Consistency, Realism, Aesthetic Quality, and composite metric, WiScore, shown in Figure 4. Consistency evaluates the accuracy and completeness with which the generated image reflects the users prompt, capturing all key elements and nuances. Realism assesses the realism of the image, considering adherence to physical laws, accurate material representation, and coherent spatial relationships, determining how closely the image resembles real photograph. Aesthetic Quality measures the overall artistic appeal and visual quality of the image, encompassing aspects such as composition, color harmony, and artistic style. WiScore, the central metric, emphasizes the accuracy of the depicted objects or entities within the generated image, directly reflecting our benchmarks focus on world knowledge utilization. It is calculated as weighted value of the other three metrics: WiScore = 0.7 Consistency + 0.2 Realism + 0.1 Aesthetic Quality (1) The weighting prioritizes Consistency, reflecting the importance of accurately representing the prompts intended objects and their relationships, while also incorporating Real5 Figure 5. WISE assess image quality based on three criteria: how accurately the image aligns with the prompt (Consistency), its level of realism (Realism), and its overall artistic appeal (Aesthetic Quality). Each metric is scored on scale from 0 (Rejected) to 2 (Exemplary), providing comprehensive assessment of the images fidelity, believability, and visual excellence. ism and Aesthetic Quality to ensure overall image quality. higher WiScore indicates superior performance in accurately depicting objects and concepts based on world knowledge. Detailed Scoring Criteria in Figure 5. To facilitate comparison, in Table 1 and Table 2, we report the average WiScore per image, normalized by dividing by 2. We provide complete results on each score in Appendix. 4. Evaluation Results 4.1. Experiment settings We evaluate 20 T2I models, including 10 dedicated text-to-image models and 10 unified multimodal models. The dedicated T2I models are: stable-diffusion-v15 [33], stable-diffusion-2-1 [33], stable-diffusion-xl-base0.9 [29], stable-diffusion-3-medium [5], stable-diffusion-3.5medium [5], stable-diffusion-3.5-large [5], playground-v2.51024px-aesthetic [20], PixArt-XL-2-1024-MS [1], FLUX.1dev [18], and FLUX.1-schnell [18]. The unified multimodal models are: Janus-Pro-7B [4], Janus-Pro-1B [4], JanusFlow-1.3B [25], Janus-1.3B [44], show-o-demo [49], show-o-demo-512 [49], Orthus-7B-base [17], Orthus-7Binstruct [17], vila-u-7b-256 [48], and Emu3 [43]. For image generation, we use the official default configurations of each model and fix the random seed to ensure reproducibility. We utilize GPT-4o-2024-05-13 as the evaluation model. All experiments are conducted on eight NVIDIA A800 GPUs. For short, we use SD to denote stable-diffusion, playground-v2.5 for playground-v2.5-1024px-aesthetic, and PixArt-Alpha for PixArt-XL-2-1024-MS. 4.2. Overall results As the results in Table 1 show, generating images that accurately and comprehensively integrate world knowledge remains substantial challenge across all models, encompassing both dedicated text-to-image models and unified multimodal models. This highlights critical area for improvement in the T2I field, suggesting significant deficiency in current approaches when it comes to complex semantic understanding and the integration of world knowledge for image generation. Furthermore, clear distinction arises between dedicated T2I models and unified multimodal models. Notably, FLUX.1-dev, leveraging CLIP [30] text encoder and T5 [31], achieves an overall WiScore of 0.50. Table 1. Normalized WiScore of different models."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 0.48 0.39 0.45 0.49 0.34 0.30 0.43 0.42 0.43 0.44 0.34 0.16 0.13 0.20 0.30 0.07 0.23 0.28 0.28 0.26 Dedicated T2I 0.62 0.50 0.48 0.55 0.32 0.35 0.47 0.48 0.52 0. 0.42 0.31 0.49 0.43 0.28 0.33 0.44 0.39 0.41 0.44 Unify MLLM 0.48 0.35 0.28 0.45 0.49 0.12 0.38 0.40 0.48 0.37 0.41 0.28 0.20 0.24 0.36 0.15 0.28 0.23 0.30 0.35 0.58 0.44 0.50 0.58 0.35 0.38 0.48 0.44 0.50 0.50 0.45 0.26 0.26 0.28 0.37 0.10 0.31 0.36 0.40 0. 0.51 0.44 0.56 0.48 0.29 0.34 0.45 0.47 0.53 0.52 0.45 0.30 0.19 0.32 0.42 0.15 0.31 0.33 0.46 0.39 0.35 0.26 0.34 0.33 0.21 0.21 0.27 0.29 0.33 0.31 0.27 0.14 0.11 0.16 0.26 0.10 0.20 0.22 0.30 0.23 0.50 0.40 0.47 0.49 0.32 0.32 0.43 0.42 0.45 0.46 0.39 0.23 0.18 0.26 0.35 0.10 0.27 0.30 0.35 0. This underscores the potential of combining CLIPs visual language alignment with T5s powerful language representation for effective knowledge grounding, yet also underscores the persistent challenges that remain, as even this architecture falls short of achieving truly high scores, highlighting the need for even more advanced approaches. Understanding-Generation gap of unified multimodal models. As results in Table 1 indicate, dedicated T2I models generally outperform unified multimodal models in terms of overall WiScore. This suggests that despite the robust text and image understanding capabilities afforded by training on large-scale image-text pairs and massive textual datasets, unified multimodal models have yet to fully leverage their inherent strengths for effective knowledge integration in the image generation process. This gap signifies that the capability to understand and reason about the world does not automatically translate into the ability to visually represent that knowledge with sufficient fidelity and accuracy in current unified multimodal models. Performance of different categories. Furthermore, while performance varied across all categories, cultural prompts generally elicited the most accurate and coherent images, suggesting relative strength in handling common knowledge and readily available cultural concepts. In contrast, prompts about natural science and space-time consistently yielded lower scores, demonstrating clear weakness in accurately representing these more complex and specialized domains. This difference in performance likely stems from combination of factors. The superior handling of cultural prompts likely reflects the prevalence of cultural knowledge and common-sense information within the training datasets used to build these models. Conversely, accurate visual representation of science and space-time concepts often necessitates more sophisticated reasoning abilities and the integration of less frequently encountered information. Additionally, the relative scarcity of high-quality, accurately labeled training data for these technical domains may further hinder the models ability to effectively translate prompts into corresponding visual representations. Effect of language model parameters. Janus-Pro-7B exhibits noticeable performance increase compared to JanusPro-1B, highlighting the role of language model scale. It underscores that larger language models, with their increased parameter count, possess greater capacity for encoding world knowledge and stronger ability to understand and utilize complex textual information for image generation. 4.3. Evaluation on WISE rewritten prompts. We conducted an additional experiment to further highlight the limitations of current T2I models in handling 7 Table 2. Normalized WiScore results on rewritten prompts. These prompts were simplified from the original WISE benchmark using GPT-4o (e.g., The plant often gifted on Mothers Day to Carnation)."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 0.75 0.63 0.66 0.78 0.59 0.63 0.68 0.76 0.73 0.78 0.70 0.40 0.39 0.60 0.75 0.19 0.55 0.61 0.64 0.54 Dedicated T2I 0.76 0.67 0.55 0.63 0.41 0.44 0.59 0.68 0.67 0. 0.69 0.58 0.58 0.69 0.47 0.50 0.61 0.59 0.68 0.64 Unify MLLM 0.60 0.49 0.38 0.59 0.70 0.20 0.48 0.55 0.68 0.49 0.59 0.54 0.57 0.66 0.71 0.24 0.46 0.54 0.63 0.57 0.70 0.58 0.64 0.72 0.50 0.61 0.71 0.65 0.69 0.69 0.62 0.48 0.43 0.59 0.66 0.23 0.47 0.56 0.62 0. 0.71 0.58 0.64 0.67 0.44 0.49 0.67 0.67 0.67 0.70 0.56 0.53 0.44 0.63 0.73 0.21 0.45 0.53 0.69 0.56 0.68 0.44 0.62 0.60 0.36 0.41 0.55 0.59 0.60 0.64 0.52 0.44 0.41 0.58 0.59 0.21 0.42 0.56 0.59 0.58 0.73 0.60 0.63 0.71 0.50 0.55 0.65 0.69 0.69 0.72 0.63 0.46 0.42 0.60 0.71 0.21 0.50 0.57 0.64 0. world knowledge. We used GPT-4o-2024-05-13 to rewrite the prompts in our WISE benchmark, transforming them from complex, knowledge-demanding prompts into direct prompts. For example, the original prompt The plant often gifted on Mothers Day was rewritten as Carnation. The specific instructions for prompt rewriting are detailed in Appendix. The results of this experiment, using the same WiScore evaluation, are presented in Table 2. Nearly all models exhibit significant performance increase when evaluated with the rewritten prompts. Notably, dedicated T2I models like FLUX.1-dev and SD-3.5-large, as well as the unified multimodal model Janus-Pro-7B, show substantial improvements in their overall WiScore. However, even with this simplification, the scores, while improved, still do not reach level that would indicate complete and satisfactory understanding of world knowledge across all categories. This demonstrates the instability and limitations of relying solely on LLMs to rewrite prompts for evaluating world knowledge in T2I models. The substantial performance variation highlights the need for future research to focus on improvements in model training methodologies, rather than relying on prompt engineering alone. 5. Conclusion To evaluate the ability of current T2I models to generate images that goes beyond simple word-pixel mapping, we introduce WISE, novel benchmark with 1000 prompts spanning diverse domains, designed to challenge T2I models ability to generate images grounded in common world knowledge. Our evaluation of 20 T2I models, including both dedicated T2I models and unified multimodal models, reveals significant shortcomings in their capacity to effectively leverage world knowledge during image generation. Even unified multimodal models, despite their strong language understanding capabilities, do not fully translate this advantage into superior image generation performance in complex, knowledge-demanding scenarios. 6. Limitations While our work introduces novel benchmark, WISE, for evaluating the world knowledge and semantic understanding capabilities of T2I models, we acknowledge several limitations. Our benchmark categorizes prompts into broad domains, but due to the interconnected nature of knowledge, some prompts may inherently span multiple categories (e.g., the impact of climate change on polar bear habitats could fall under both natural science and spatio-temporal reasoning), potentially introducing ambiguity in cross-category analysis. Furthermore, while WISE covers range of topics, it represents sample of knowledge domains and cannot encompass all aspects of world knowledge, which is also constantly evolving. Additionally, some models were not publicly available or did not provide APIs at the time of our works deadline, precluding their evaluation in this study. Acknowledgements. We are grateful to Jiaxiu Liu, Chaoran Feng, Fanqing Meng and Hongcheng Gao for helpful discussion and drawing."
        },
        {
            "title": "References",
            "content": "[1] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 6 [2] Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, et al. Next token prediction towards multimodal intelligence: comprehensive survey. arXiv preprint arXiv:2412.18619, 2024. 2 [3] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. 2020. 2 [4] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 2, 6 [5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 6 [6] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. [7] Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense? arXiv preprint arXiv:2406.07546, 2024. 3 [8] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 2 [9] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [10] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. 2 [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 1, 3 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 1, 3 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [14] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu Ella. Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. 3 [15] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for openworld compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 3 [16] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023. 2 [17] Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127, 2024. 2, 6 [18] Black Forest Labs. Flux. https://github.com/blackforest-labs/flux, 2024. 2, [19] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024. 3 [20] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 6 [21] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, et al. Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. arXiv preprint arXiv:2412.09604, 2024. 2 [22] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. 2 [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 2 [24] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024. URL https://arxiv. org/abs/2404.01291. 3 [25] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 2, 6 [26] Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. 3 [27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [28] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, Ser-Nam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. 2 [29] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, 6 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2, 6 [31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. 2 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 6 [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [35] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. 2 [36] Shang Hong Sim, Clarence Lee, Alvin Tan, and Cheston Tan. Evaluating the generation of spatial relations in text and image generative models. arXiv preprint arXiv:2411.07664, 2024. 3 [37] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [38] Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024. 2 [39] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2 [40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2 [41] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 2 [42] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [43] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2, [44] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 6 [45] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. 2 [46] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 1 [47] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. arXiv preprint arXiv:2408.14339, 2024. 3 [48] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 2, 6 [49] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single 10 transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, 6 [50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 1 [51] Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world arXiv preprint knowledge of large language models. arXiv:2306.09296, 2023. 1 [52] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 1 [53] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion models in generative ai: survey. arXiv preprint arXiv:2303.07909, 2023. 1 [54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [55] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2 11 A. WISE Category Descriptions WISE (likely an acronym for knowledge evaluation framework, not specified in the provided context) encompasses broad spectrum of knowledge categories, categorized under three main domains: Cultural Common Sense, SpatioTemporal Reasoning, and Natural Science. Each domain is further divided into specific subcategories to assess different facets of understanding. Life: This category examines the understanding of common, everyday knowledge related to daily life practices and tools. Example: common utensil used for consuming steak in Western culture. A.2. Spatio-Temporal Reasoning (300 prompts) This domain evaluates the ability to reason about spatial and temporal relationships and contexts. It is further divided into Time and Space subcategories. A.1. Cultural Common Sense (400 prompts) A.2.1. Time (167 prompts) This domain evaluates the understanding of widely shared cultural knowledge and conventions. It includes: Festival: This category assesses knowledge related to cultural celebrations, encompassing traditional foods, customs, and activities associated with specific festivals. Example: Traditional cuisine of the Mid-Autumn Festival. Sports: This category focuses on recognizing sports that are highly representative or culturally significant within particular nations or regions. Example: The most representative sport of South Africa. Religion-related: This category examines the identification of objects, symbols, or architectural structures that hold religious significance and are associated with specific faiths or religious heritage. Example: geometric symbol commonly associated with Jewish identity and heritage. Craft-related: This category pertains to the recognition of traditional crafts that are emblematic of nations artistic and technical skills. Example: craft embodying Swiss precision and artistry. Construction-related: This category tests the ability to identify iconic architectural structures that are representative landmarks of specific countries or cities. Example: sail-like structure, an architectural icon on Sydneys harbor. Animal: This category assesses knowledge of animals that are symbolic, nationally significant or possess distinctive characteristics. Example: large animal, symbolizing national pride in Thailand. Plant: This category evaluates the recognition of plants or fruits that are culturally representative, possess notable properties, or hold symbolic meaning. Example: famous flower symbolizing wealth in China. Art: This category focuses on understanding artistic styles, recognizing representative musical instruments of different cultures, or identifying instruments with specific functional attributes. Example: triangular stringed instrument often used in Russian folk music. Celebrity: This category assesses knowledge of globally recognized figures, including historical personalities, fictional characters from literature, or iconic roles from film and television. Example: The iconic hat of the protagonist of One Piece in Japan. This subsection focuses on temporal reasoning. It includes: Horizontal Time: This category assesses the understanding of temporal relationships across different entities or events occurring concurrently. Example: The Pyramids of Giza at 8 PM Tokyo time. Longitudinal Time: This category focuses on understanding temporal progression and order of events across time, including diurnal cycles, seasonal changes, and historical periods. Example: The signature instrument of the rock and roll era in the 1950s. A.2.2. Space (133 prompts) This subsection focuses on spatial reasoning. It includes: Geographical Location: This category examines the understanding of spatial relationships between geographical entities, such as cities, countries, and continents. Example: typical beverage produced in the country where Bordeaux is located. Relative Position: This category assesses the ability to understand and interpret relative spatial positioning between objects, such as proximity, vertical placement, and size comparisons. Example: bird and dog, with the smaller animal positioned on top of and the larger animal below. Different View: This category evaluates the ability to recognize and interpret objects and scenes from various perspectives, including viewpoints like top-down, bottomup, cross-sectional, side, mirrored, and occluded views. Example: view of dense forest from within the canopy, looking upwards. A.3. Natural Science This domain evaluates understanding of fundamental principles and phenomena within the natural sciences, categorized into Biology, Physics, and Chemistry. A.3.1. Biology (100 prompts) State: This subcategory assesses knowledge of the different physiological or developmental states of living organisms under varying conditions or across their life cycle stages. Example: maple tree with leaves exhibiting chlorophyll breakdown. Behavior: This subcategory evaluates the understanding of typical behaviors exhibited by organisms, including 12 Solution Chemical Reaction: This subcategory covers various types of chemical reactions in solutions, including acid-base reactions, redox reactions, and precipitation reactions. Example: T-shirt stained by sulfuric acid. Chemical Properties: This subcategory assesses knowledge of intrinsic chemical properties, including colloidal behavior, protein chemistry, and the characteristic colors of chemical species in solution. Example: clear solution of copper sulfate exhibiting its characteristic color. This structured categorization of WISE provides comprehensive framework for evaluating models knowledge across diverse domains, ranging from cultural understanding to scientific principles. actions related to survival, reproduction, and interaction with their environment. Example: morning rooster, emphasizing its characteristic behavior. A.3.2. Physics (100 prompts) Mechanics: This subcategory covers principles of mechanics, including: Gravity: Understanding effects of gravity on objects, such as vertical suspension and equilibrium. Example: balloon filled with helium in room. Buoyancy: Understanding the behavior of objects in fluids based on buoyancy principles. Pressure: Understanding the effects of pressure and its variations. Surface Tension: Understanding phenomena related to surface tension in liquids. Other Mechanical Phenomena: Including concepts like wind effects on objects. Thermodynamics: This subcategory covers principles of heat and energy transfer, including: Evaporation: Understanding the process of vaporization at boiling points. Liquefaction: Understanding the condensation of gases into liquids. Solidification: Understanding the process of freezing. Melting: Understanding the process of fusion. Sublimation: Understanding the phase transition from solid to gas. Deposition: Understanding the phase transition from gas to solid. Example: pond at minus ten degrees Celsius. Optics: This subcategory covers principles of light and vision, including: Refraction: Understanding the bending of light as it passes through different media. Magnification: Understanding how lenses magnify objects. Dispersion: Understanding the separation of light into its spectral components. Example: laser beam passing through dusty room. Physical Properties: This subcategory assesses knowledge of material properties like electrical conductivity. Example: An electrical bulb connected to battery via copper wires. A.3.3. Chemistry (100 prompts) Combustion: This subcategory covers principles of chemical reactions involving rapid oxidation, including flame characteristics and color reactions. Example: Lithium burning, highlighting its characteristic flame color. Metal Corrosion: This subcategory assesses the understanding of the electrochemical degradation of metals over time due to environmental exposure. Example: An iron block exhibiting rust due to corrosion. 13 Figure 6. We utilize GPT-4o to evaluate the performance of text-to-image models. Above is the instruction we provided to GPT-4o. 14 Figure 7. We utilize GPT-4 rewrite the prompts in our WISE benchmark, transforming them from complex, knowledge-demanding prompts into direct prompts. Above is the instruction we provided to GPT-4o. 15 Table 3. Consistency score of different models."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 298.00 246.00 289.00 296.00 245.00 199.00 311.00 267.00 278.00 291.00 190.00 115.00 89.00 119.00 176.00 49.00 121.00 172.00 156.00 186.00 Dedicated T2I 161.00 123.00 138.00 162.00 101.00 113.00 149.00 118.00 142.00 148. 119.00 89.00 81.00 81.00 103.00 28.00 97.00 109.00 107.00 107.00 155.00 129.00 108.00 127.00 82.00 85.00 117.00 119.00 134.00 146.00 48.00 36.00 60.00 70.00 37.00 46.00 69.00 50.00 51.00 65.00 Unify MLLM 107.00 100.00 80.00 127.00 127.00 34.00 103.00 104.00 118.00 104.00 51.00 49.00 28.00 33.00 56.00 28.00 47.00 32.00 36.00 69. 77.00 67.00 72.00 88.00 42.00 50.00 74.00 76.00 90.00 81.00 65.00 59.00 32.00 52.00 72.00 26.00 46.00 51.00 74.00 74.00 41.00 24.00 40.00 41.00 25.00 23.00 30.00 35.00 40.00 32.00 29.00 19.00 11.00 16.00 30.00 11.00 27.00 24.00 37.00 41.00 183.30 148.80 170.21 182.25 136.17 121.68 182.14 158.43 170.84 178.33 124.60 86.86 66.87 88.12 120.29 35.30 90.30 111.53 110.66 124. 16 Table 4. Realism scores of different models."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 585.00 459.00 495.00 601.00 321.00 335.00 378.00 506.00 517.00 484.00 446.00 159.00 136.00 233.00 371.00 74.00 282.00 282.00 372.00 232.00 Dedicated T2I 181.00 145.00 165.00 184.00 91.00 110.00 143.00 153.00 148.00 172. 179.00 130.00 141.00 160.00 103.00 120.00 141.00 152.00 169.00 149.00 Unify MLLM 165.00 72.00 65.00 102.00 137.00 19.00 78.00 103.00 149.00 79.00 151.00 78.00 71.00 88.00 112.00 29.00 70.00 74.00 117.00 68.00 269.00 204.00 224.00 256.00 154.00 159.00 198.00 221.00 229.00 215.00 215.00 79.00 99.00 115.00 169.00 35.00 103.00 132.00 188.00 103. 161.00 138.00 151.00 163.00 98.00 115.00 133.00 144.00 149.00 164.00 144.00 65.00 58.00 89.00 115.00 38.00 91.00 96.00 131.00 85.00 146.00 119.00 126.00 127.00 90.00 96.00 112.00 115.00 141.00 142.00 107.00 54.00 49.00 70.00 110.00 39.00 58.00 80.00 109.00 54.00 351.60 275.65 299.15 352.62 195.32 208.28 241.88 300.76 310.63 297.88 276.45 106.07 97.38 150.67 228.54 48.57 162.28 173.54 235.71 141. 17 Table 5. Aesthetic Quality scores of different models."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 582.00 459.00 569.00 652.00 330.00 360.00 475.00 454.00 494.00 504.00 531.00 173.00 145.00 287.00 399.00 94.00 391.00 395.00 426.00 318.00 Dedicated T2I 190.00 139.00 195.00 209.00 87.00 108.00 151.00 141.00 137.00 164. 154.00 115.00 156.00 167.00 93.00 99.00 123.00 119.00 128.00 132.00 Unify MLLM 188.00 81.00 67.00 105.00 131.00 33.00 122.00 131.00 166.00 107.00 152.00 60.00 53.00 81.00 104.00 39.00 101.00 98.00 121.00 90.00 275.00 197.00 255.00 288.00 150.00 159.00 180.00 210.00 215.00 218.00 239.00 89.00 96.00 128.00 168.00 53.00 159.00 170.00 207.00 143. 156.00 126.00 155.00 168.00 86.00 93.00 118.00 123.00 125.00 143.00 149.00 56.00 49.00 90.00 101.00 43.00 107.00 113.00 135.00 100.00 127.00 106.00 134.00 148.00 73.00 73.00 99.00 98.00 102.00 113.00 124.00 41.00 46.00 60.00 95.00 46.00 92.00 104.00 121.00 71.00 347.69 269.69 340.62 384.99 193.82 211.42 274.14 269.42 287.23 298.62 319.82 110.54 97.74 173.24 235.08 63.64 229.18 235.31 264.75 191. 18 Table 6. Consistency scores of different models on rewritten prompts. These prompts were simplified from the original WISE benchmark using GPT-4o (e.g., The plant often gifted on Mothers Day to Carnation)."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 586.00 509.00 534.00 632.00 529.00 551.00 571.00 617.00 595.00 641.00 565.00 382.00 346.00 522.00 630.00 171.00 468.00 518.00 524.00 486.00 Dedicated T2I 204.00 183.00 134.00 154.00 111.00 113.00 154.00 180.00 183.00 185. 125.00 117.00 104.00 126.00 88.00 94.00 114.00 103.00 127.00 120.00 Unify MLLM 151.00 149.00 109.00 169.00 192.00 56.00 133.00 152.00 185.00 138.00 109.00 116.00 118.00 136.00 147.00 52.00 94.00 107.00 123.00 122.00 214.00 182.00 195.00 228.00 164.00 200.00 238.00 201.00 221.00 221.00 196.00 176.00 141.00 205.00 219.00 82.00 161.00 191.00 195.00 179. 134.00 113.00 118.00 119.00 84.00 91.00 128.00 125.00 128.00 132.00 102.00 117.00 88.00 133.00 155.00 41.00 88.00 102.00 133.00 119.00 133.00 80.00 116.00 109.00 67.00 75.00 106.00 112.00 116.00 122.00 98.00 93.00 87.00 123.00 123.00 38.00 86.00 115.00 114.00 124.00 336.47 289.33 297.79 346.76 277.65 294.83 323.43 338.31 336.35 355.31 309.71 234.61 205.74 304.71 356.61 102.64 258.58 291.71 303.77 279. 19 Table 7. Realism scores of different models on rewritten prompts. These prompts were simplified from the original WISE benchmark using GPT-4o (e.g., The plant often gifted on Mothers Day to Carnation)."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 637.00 504.00 479.00 571.00 344.00 375.00 458.00 585.00 567.00 583.00 516.00 174.00 232.00 365.00 519.00 103.00 343.00 392.00 458.00 283.00 Dedicated T2I 197.00 167.00 168.00 194.00 105.00 131.00 161.00 181.00 170.00 178. 167.00 114.00 136.00 162.00 113.00 120.00 142.00 153.00 157.00 146.00 Unify MLLM 165.00 91.00 87.00 125.00 180.00 36.00 109.00 125.00 164.00 105.00 138.00 92.00 109.00 125.00 135.00 32.00 78.00 102.00 132.00 92.00 285.00 221.00 246.00 262.00 172.00 219.00 231.00 258.00 261.00 247.00 221.00 123.00 150.00 177.00 224.00 56.00 138.00 167.00 231.00 146. 160.00 124.00 148.00 164.00 102.00 120.00 154.00 154.00 150.00 163.00 129.00 79.00 94.00 111.00 132.00 41.00 90.00 114.00 149.00 94.00 148.00 110.00 139.00 141.00 88.00 108.00 123.00 133.00 134.00 147.00 109.00 76.00 72.00 102.00 108.00 46.00 68.00 94.00 119.00 93.00 376.10 295.52 297.33 344.66 210.59 238.80 285.09 345.16 337.10 343.72 302.85 126.94 156.92 225.98 306.45 67.24 198.34 232.31 283.59 179. 20 Table 8. Aesthetic Quality scores of different models on rewritten prompts. These prompts were simplified from the original WISE benchmark using GPT-4o (e.g., The plant often gifted on Mothers Day to Carnation)."
        },
        {
            "title": "Physics Chemistry Overall",
            "content": "FLUX.1-dev FLUX.1-schnell PixArt-Alpha playground-v2.5 SD-v1-5 SD-2-1 SD-XL-base-0.9 SD-3-medium SD-3.5-medium SD-3.5-large Emu3 Janus-1.3B JanusFlow-1.3B Janus-Pro-1B Janus-Pro-7B Orthus-7B-base Orthus-7B-instruct show-o-demo show-o-demo-512 vila-u-7b-256 640.00 505.00 583.00 696.00 367.00 427.00 492.00 552.00 566.00 600.00 621.00 203.00 249.00 396.00 513.00 129.00 458.00 496.00 556.00 375.00 Dedicated T2I 194.00 162.00 194.00 215.00 111.00 126.00 158.00 176.00 166.00 170. 162.00 119.00 158.00 170.00 101.00 110.00 139.00 145.00 147.00 153.00 Unify MLLM 198.00 89.00 87.00 135.00 171.00 56.00 136.00 149.00 182.00 129.00 148.00 82.00 94.00 117.00 122.00 50.00 108.00 119.00 143.00 96.00 282.00 209.00 283.00 292.00 177.00 208.00 250.00 238.00 252.00 247.00 269.00 129.00 139.00 185.00 212.00 88.00 183.00 205.00 254.00 172. 159.00 115.00 159.00 170.00 96.00 99.00 144.00 147.00 139.00 158.00 144.00 73.00 84.00 105.00 119.00 55.00 110.00 128.00 154.00 100.00 133.00 107.00 152.00 154.00 78.00 86.00 115.00 123.00 119.00 140.00 132.00 73.00 71.00 102.00 100.00 57.00 102.00 124.00 136.00 102.00 374.30 292.55 353.16 405.16 218.62 251.79 299.36 325.45 331.06 348.96 362.06 137.38 159.28 239.65 297.45 89.94 263.85 289.55 332.32 225."
        }
    ],
    "affiliations": [
        "Chongqing University",
        "Peking University",
        "PengCheng Laboratory",
        "Rabbitpre AI"
    ]
}