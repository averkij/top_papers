{
    "paper_title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning",
    "authors": [
        "Youngbin Choi",
        "Minjong Lee",
        "Saemi Moon",
        "Seunghyuk Cho",
        "Chaehyeon Chung",
        "MoonJeong Park",
        "Dongwoo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using $79.1\\%$ fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 7 7 7 0 0 . 0 1 5 2 : r IN-PLACE FEEDBACK: NEW PARADIGM FOR GUIDING LLMS IN MULTI-TURN REASONING Youngbin Choi1,*, Minjong Lee2,*, Saemi Moon2, Seunghyuk Cho1, Chaehyeon Chung2, MoonJeong Park1, Dongwoo Kim1,2, 1Graduate School of Artificial Intelligence, POSTECH 2Computer Science and Engineering, POSTECH {choi.youngbin, minjong.lee, saemi, shhj1998, chchung, mjeongp, dongwoo.kim}@postech.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) are increasingly studied in the context of multiturn reasoning, where models iteratively refine their outputs based on userprovided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, novel interaction paradigm in which users directly edit an LLMs previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoningintensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) are increasingly positioned as in multi-turn conversations, where their effectiveness is measured by how well they generate responses that align with user intentions (Lee et al., 2022; Wang et al., 2025; Nath et al., 2025; Zhou et al., 2025; Kim et al., 2025). An example from chess demonstrates that weaker yet cooperative agent can enable player to outperform an opponent who is paired with stronger but uncooperative agent (Hamade et al., 2024). Such findings highlight the growing importance of effectively incorporating user guidance in collaborative LLMs (Wu et al., 2025; Maheshwary et al., 2025). Building on this perspective, we investigate core mechanism of collaboration, feedback. In particular, we study how feedback can be used for error correction in multi-turn reasoning. Users can provide turn-level feedback through corrections, additional constraints, or supplemental information. For example, in mathematical reasoning task, user may identify an error in an LLM response and provide feedback to correct it. Unfortunately, recent studies have shown that LLMs often fail to incorporate user feedback in multi-turn interactions (Laban et al., 2025; Jiang et al., 2025; Sirdeshmukh et al., 2025). Expanding on these studies, we observe three failure modes when LLMs attempt to incorporate feedback: 1) previously correct content becomes incorrect after feedback, 2) the model disregards the feedback and repeats its earlier output, and 3) feedback is applied, but the subsequent reasoning steps introduce new errors. To address these feedback integration failures, we propose in-place feedback, novel interaction paradigm that reframes feedback not as new instruction, but as direct state repair. In this ap- *Equal contribution. Corresponding author. 1 Figure 1: Illustration of common failure cases in multi-turn refinement and in-place feedback. After in-place feedback, the LLM continues generation from the green word requires. proach, user feedback is applied as an edit to the previous output, and the task of LLMs is to continue the generation from this corrected state. This method not only preserves correct prior reasoning but also constrains the model to build upon user-validated information. Through empirical evaluation on GPQA (Rein et al., 2024; Chandak et al., 2025), MMLU-pro (Wang et al., 2024; Chandak et al., 2025), and MATH-hard (Hendrycks et al., 2021) benchmarks, we show that in-place feedback achieves substantially improved performance and greater token efficiency compared to the standard multi-turn feedback paradigm. We also conduct fine-grained analysis of the feedback paradigms on ZebraLogic (Lin et al., 2025), where reasoning errors and feedback can be extracted in rule-based manner, to identify the precise factors underlying the effectiveness of in-place feedback. We observe that in multi-turn interaction, LLMs become less effective at incorporating feedback as the number of turns increases. In contrast, in-place feedback integrates corrections more effectively than multi-turn feedback, particularly in later turns. Moreover, in-place feedback propagates improvements through later reasoning steps, surpassing multi-turn feedback in overall refinement. 2 IN-PLACE FEEDBACK"
        },
        {
            "title": "2.1 MULTI-TURN REFINEMENT WITH FEEDBACK",
            "content": "We describe how feedback from humans or automated agents is incorporated into LLMs in interactive settings, focusing on multi-turn interactions. Let be target LLM. Given problem x, the LLM produces an initial response as y0 = M(x). Based on the problem and the initial response y0, feedback is then generated to address potential reasoning errors in the initial response. Such feedback can be formalized by function F, yielding f0 = F(x, y0). In the subsequent turn, the target LLM refines its initial response using the feedback and generates the next response conditioned on the problem, the initial response, and the feedback, as y1 = M(x, y0, f0). This illustrates the refinement process, in which each response is conditioned not only on the problem but also on the preceding response and its associated feedback. the refinement extends to multi-turn setting, where the LLM iterMore generally, yt = atively produces responses and incorporates (x, y0, f0, y1, f1, . . . , yt1, ft1), where fi = F(x, yi) denotes the feedback associated with the i-th response. We refer to this process as refinement with standard multi-turn feedback, which we hereafter simply call multi-turn feedback. feedback across multiple cycles: 2 Figure 2: Representative examples of in-place feedback on toy problem. Red marks incorrect reasoning, blue indicates the user corrections with in-place feedback, and green shows the subsequent reasoning based on the corrected context. Additional examples are provided in Appendix C.1."
        },
        {
            "title": "2.2 MOTIVATION: FAILURE CASE OF MULTI-TURN REFINEMENT",
            "content": "Recent work shows that LLMs often fail to reliably integrate user feedback (Laban et al., 2025; Jiang et al., 2025). Figure 1 illustrates common failure cases of multi-turn refinement. We observe three recurring failure modes: 1) previously correct content becomes incorrect after the feedback, 2) the model ignores the feedback and repeats its previous output, and 3) the feedback is applied but causes errors in subsequent reasoning steps. We hypothesize that these failures stem from regenerating the entire response from scratch at each turn. This process may overwrite correct reasoning and weaken the alignment between the feedback and the reasoning context it is meant to correct. We provide some examples of the failure cases of multi-turn refinement in Appendix C.2. This hypothesis highlights three requirements for effective refinement. Edits from user feedback should 1) remain focused on the targeted reasoning step, 2) preserve previously correct content outside this span, and 3) guide future reasoning from the corrected state rather than an outdated one. These considerations naturally lead us to ask: Can we mitigate the above failures by letting the user directly edit the targeted span and constraining the model to continue generation from that point? 2.3 IN-PLACE FEEDBACK To address this question, we propose in-place feedback, new multi-turn interaction mode that treats feedback as state repair rather than new instruction. As illustrated in Figure 1, our method proceeds in two stages. The first, in-place edit, allows the user to directly modify the models previous response. The user then prunes the reasoning context that depends on the corrected span, while leaving the rest unchanged. In our setting, we assume the user identifies one or two mistakes in the reasoning and corrects only those parts. The second, continuation generation, regenerates only what is necessary to continue from the updated context. Together, these stages limit unintended changes and rebuild reasoning from the correction. To illustrate how this method works in practice, Figure 2 presents representative cases of in-place feedback. For example, in math problems, in-place feedback can fix simple arithmetic mistakes or adjust flawed intermediate steps. In more complex cases, it can realign an incorrect reasoning path by revising larger portions of the solution. 3 Figure 3: Comparison of in-place and multi-turn accuracies across models in MATH-hard, MMLUpro, and GPQA. Across all datasets and LLM models, our in-place feedback approach consistently outperforms the multi-turn based feedback approach. Benefits of in-place feedback. Standard multi-turn feedback appends new turns to the history, causing early mistakes to persist and propagate across later reasoning. In-place feedback instead applies edits directly to the current output. By anchoring unchanged spans and updating only the edited portion, in-place feedback prevents error propagation. It also maintains global coherence and preserves clear trace from the users edit to the subsequent reasoning of LLMs. In-place feedback also benefits from token efficiency. Standard multi-turn feedback accumulates lengthy dialogue history and leads the model to regenerate entire reasoning chains, including parts that are already correct. In contrast, in-place feedback keeps the history compact by editing only the targeted span and continues generation from corrected span, avoiding unnecessary regeneration. As result, it reduces both input and output tokens, even under repeated feedback."
        },
        {
            "title": "3 EMPIRICAL STUDY ON THE EFFECT OF IN-PLACE FEEDBACK",
            "content": "In this section, we evaluate the effectiveness of in-place feedback in multi-turn reasoning. We compare it with standard multi-turn feedback across multiple real-world datasets and LLMs."
        },
        {
            "title": "3.1 EXPERIMENTAL SETUP",
            "content": "Datasets and evaluation. We conduct experiments on MATH-hard (Hendrycks et al., 2021), MMLU-pro free-form (Wang et al., 2024; Chandak et al., 2025), and GPQA free-form (Rein et al., 2024; Chandak et al., 2025). For MATH-hard, we sample 500 level-5 problems from MATH (Hendrycks et al., 2021). For MMLU-pro and GPQA, we use the free-form subsets introduced by Chandak et al. (2025), which contain only open-ended questions. We evaluate model answers in two stages. We first attempt exact matching. If it fails, we then apply an LLM judge to identify semantically equivalent answers expressed in different forms. We use GPT-oss-20b (OpenAI Team, 2025) as the judge model. Details on datasets and judge prompts are provided in Appendix A. Feedback function and agent for in-place feedback. For experimental evaluation, it is necessary to automate the process of generating and applying feedback, which would otherwise require human intervention. Given problem, its ground-truth solution, and the reasoning process of the LLM, the feedback function identifies the earliest critical error and generates correction. The feedback function is designed to operate in both multi-turn and in-place settings. For in-place feedback, human should apply feedback directly to the previous response of the LLM. To automate this process in our experiments, we utilize an in-place feedback agent. The agent takes the feedback and the response of the LLM, identifies the sentence to be replaced, and provides its replacement. We then substitute the sentence and remove all subsequent text, since it may depend on the corrected span. We use GPT-5-mini (OpenAI, 2025) for both the feedback function and the 4 Figure 4: Number of input and generated tokens across multiple turns. In-place feedback consistently requires fewer tokens than multi-turn feedback across all datasets and LLMs. in-place feedback agent. Further details on post-processing steps and prompt templates are provided in Appendix A. LLMs and hyperparameters. We use three open-source LLMs: Gemma-3-4b-it (Gemma and Team, Llama-3.1-8B-Instruct (Kassianik et al., 2025). Each model is evaluated for 10 turns with temperature set to 0. Further experimental settings are provided in Appendix A. Qwen2.5-7B-Instruct (Qwen 2025), 2025), Team,"
        },
        {
            "title": "3.2 RESULTS",
            "content": "Task performance. Figure 3 shows how accuracy changes under multi-turn and in-place feedback as the number of turns increases. Across all datasets and models, in-place feedback consistently achieves higher accuracy and exhibits faster improvement over turns. On GPQA with Gemma, for example, in-place feedback achieves 53% accuracy, almost twice the performance of multi-turn feedback. On MMLU-pro, its accuracy at turn 5 already surpasses the final-turn performance of multi-turn feedback across all models. These results demonstrate that in-place feedback provides more effective way to integrate external corrections into the reasoning of LLMs. We provide the qualitative examples in Appendix B. Token efficiency. Beyond task performance, in-place feedback also exhibits superior efficiency in both input and generated token usage. Figure 4 presents the number of input and generated tokens across datasets. The result shows that in-place feedback requires substantially fewer tokens than multi-turn feedback. For input tokens, multi-turn feedback appends new turns to the dialogue history, causing token usage to grow linearly with the number of turns. In contrast, since in-place feedback does not accumulate the full dialogue history, the number of input tokens remains at stable level. For generated tokens, in-place feedback preserves correct reasoning and revises only the erroneous parts, whereas multi-turn feedback generates entire reasoning steps from scratch. As result, in-place feedback consistently produces shorter generations across turns. Aggregating input and output tokens, in-place feedback reduces token usage by 79.1% relative to multi-turn feedback, demonstrating substantially higher efficiency."
        },
        {
            "title": "4 FEEDBACK EFFECTIVENESS IN CONTROLLED EXPERIMENTS",
            "content": "Prior work has mainly evaluated feedback in multi-turn interactions by measuring whether the final answer improves after feedback (Jiang et al., 2025; Sirdeshmukh et al., 2025). Such task-level eval5 uation leaves open how feedback actually influences the reasoning process across turns. Without analyzing turn-level dynamics, it is unclear whether models are using feedback or simply regenerating new responses. To address this gap, we design controlled experiments with ZebraLogic (Lin et al., 2025), where feedback is generated through rule-based manner. Using this setting, we compare how multi-turn and in-place feedback incorporate corrections over successive turns and highlight where in-place feedback provides advantages."
        },
        {
            "title": "4.1 SETUP FOR CONTROLLED EXPERIMENTS",
            "content": "Task. We conduct experiments on the ZebraLogic (Lin et al., 2025), collection of 573 logic grid puzzles designed to evaluate the reasoning capability of LLMs. Each puzzle consists of houses and attributes such as Name, Drink, and Hobby, forming an grid of cells. Attributes must take distinct values under uniqueness constraints, resulting in each cell having single correct value. set of natural-language clues specifies additional logical relations, and the task is to assign values to all cells so that all constraints are satisfied. Details of the dataset are in Appendix A. Feedback functions. We construct two rule-based feedback functions that differ in the amount of corrective information they provide: Oracle and Top-k. 1) Oracle reveals every incorrectly predicted cell along with its correct value. 2) Top-k selects the cells that most strongly violate logical constraints, identified using Z3 solver (De Moura & Bjørner, 2008), and provides their correct values. For example, if the model predicts Name of house 2 = Alice while the ground truth is Eric, the feedback specifies Name of house 2 is Eric, not Alice. In-place feedback agent. The in-place feedback agent simulates human editor by directly modifying the LLMs response during evaluation. In math problems, each reasoning step depends on the previous one, so correction usually requires discarding subsequent steps. Zebra puzzles, however, follow different structure. They involve parallel reasoning, in which multiple constraints must be satisfied simultaneously. As result, later reasoning steps can remain valid even if earlier ones are incorrect. To handle this, we retain subsequent reasoning steps after in-place feedback is applied. We first segment the models response into the reasoning steps. The agent checks each step against the received feedback and edits the response when directly mispredicted attribute value is specified (e.g., changing Name of House 2 = Alice to Name of House 2 = Eric). If there are reasoning steps that depend on the mispredicted value identified by the feedback (e.g., reasoning built on Alice in House 2), those dependent steps are removed to prevent error propagation. After applying these edits or deletions, the final solution is removed, and the prompt Further reasoning: is appended to encourage continuation of the reasoning process. We employ GPT5-mini as the in-place feedback agent, following the rule prompt in Figure A8. LLMs and hyperparameters. We use three open-source LLMs, consistent with the previous experiments. We set = 2 and = 4 for Top-k feedback. All experiments are run with three seeds. Detailed experimental settings are provided in Appendix A. Metrics. We evaluate performance using two classes of metrics. To measure overall task performance, we use grid-level and cell-level accuracy. To conduct more fine-grained analysis of the multi-turn refinement process, we introduce three complementary metrics that measure correctness preservation, feedback incorporation, and reasoning-driven self-correction. Grid-level accuracy. The proportion of grid puzzles that are solved perfectly, i.e., all cells match the solution. Cell-level accuracy. The average proportion of correctly predicted cells over all puzzles. Correctness-Preserving Ratio (CPR). The proportion of cells that are correct in yt and remain correct in yt+1, relative to the total number of cells that are correct in yt. This metric evaluates whether the model can retain valid reasoning while applying updates. Feedback Acceptance Ratio (FAR). The proportion of cells flagged by feedback ft that are corrected in yt+1, relative to the total number of feedback-provided cells of yt. This metric captures the models ability to incorporate explicit corrective signals. Figure 5: Grid and cell accuracy of LLMs on the Zebralogic dataset. Across both top-2 and top-4 feedback settings, in-place feedback consistently outperforms multi-turn feedback. (a) Top-2 feedback (b) Top-4 feedback Figure 6: Correctness-Preserving Rate (CPR), Feedback Acceptance Rate (FAR), and Correction Through Reasoning Ratio (CTRR) for 10-turn conversations of LLMs on the ZebraLogic. The points with black border represent the second response of the LLMs (i.e., y1), and the subsequent responses across turns are connected by lines. Correction Through Reasoning Ratio (CTRR). The proportion of cells that are incorrect in yt but corrected in yt+1, relative to the total number of incorrect cells in yt that are not indicated by feedback ft. This metric measures the extent to which the model can generalize beyond explicit feedback and improve its reasoning autonomously."
        },
        {
            "title": "4.2 ANALYSIS OF FEEDBACK UTILIZATION",
            "content": "Task performance. Figure 5 shows grid accuracy and cell accuracy as the number of feedback turns increases. In-place feedback consistently outperforms multi-turn feedback, as observed in previous experiments. Interestingly, the gap in cell accuracy is smaller than the gap in grid accuracy. This suggests that while multi-turn feedback encounters difficulties in correcting the remaining few cells during iterative refinements, in-place feedback is more effective in addressing these corrections. To gain deeper understanding of this phenomenon, we examine the reasoning dynamics at the turnlevel, focusing on how top-k feedback is incorporated and influences the correction process. We also provide the results with the Oracle feedback in Appendix B. 7 Figure 7: Grid and cell accuracy of LLMs on the ZebraLogic dataset without accumulated history compared against the in-place feedback. Multi-turn (w/o) shows the accuracy of responses only on the previous answer and the feedback, i.e., yt+1 = M(x, yt, ft), without accumulated history. Turn-level dynamics of LLM behavior with multi-turn feedback. Before analyzing the effect of in-place feedback, we first examine how LLMs behave under standard multi-turn feedback settings. This analysis highlights the dynamics when models incorporate feedback across multiple turns. We observe two distinct phases. In the initial phase, models are generally effective at incorporating feedback. However, they also exhibit systematic tendency to modify portions of the response that are already correct, thereby compromising previously valid reasoning steps. As the conversation progresses, the models show increasing resistance to change, which reduces the effectiveness of further feedback. This behavioral shift is illustrated in Figure 6, where we analyze CPR, FAR, and CTRR alongside cell accuracy. The capacity to preserve correct answers improves over successive turns, as reflected in the rising CPR values. In contrast, the ability to incorporate feedback exhibits steady decline between turns 5 and 9, as evidenced by the downward trend in FAR, suggesting that models become less receptive to feedback and more resistant to revising earlier responses. It is worth noting that the models are unlikely to generalize beyond the explicit feedback after the first few turns, as evidenced by lower CTRR. We also observe distinct behavior between different LLMs, despite showing similar overall trends. Specifically, Llama exhibits comparatively higher CTRR, yet this comes at the cost of lower CPR and FAR relative to the other models. Advantage of in-place feedback. We identify two key advantages of in-place feedback over multi-turn feedback. First, in-place feedback sustains the ability to incorporate feedback even when the number of turns increases, yielding higher FAR values than multi-turn feedback in later turns. This result accounts for the larger improvement observed in grid accuracy. Second, in-place feedback facilitates reasoning beyond the explicitly targeted errors, thereby leading to consistently higher CTRR. We conjecture that this improvement arises since in-place feedback reduces contextual interference from prior responses, allowing the model to more directly condition on the corrected span. By discarding subsequent content and regenerating from the point of modification, the model may better propagate the corrective signal to related parts of the reasoning process, thereby facilitating improvements even in cells not explicitly mentioned by the feedback. The relatively lower CPR and FAR observed for in-place feedback during the earlier turns may reflect the effect of more intensive reasoning compared to multi-turn feedback. Moreover, due to the characteristics of the parallel reasoning problem, an in-place edited span may contain incorrect cell-related reasoning inherited from earlier turns under Top-k feedback, which can propagate errors through subsequent reasoning. In contrast, under oracle feedback, where in-place edits could be error-free, both CPR and FAR are typically higher than in multi-turn feedback, except for CPR with Qwen in the first turn (see Figure A2)."
        },
        {
            "title": "4.3 EFFECT OF DIALOGUE HISTORY",
            "content": "As the dialogue progresses with multi-turn feedback, two factors may contribute to the observed decrease in FAR: 1) the accumulation of dialogue history that the model must condition on in multi-turn approach, and 2) the reduction in the number of remaining incorrect cells as refinement progresses. We disentangle these two effects to better understand their respective contributions. Influence of accumulated history on feedback incorporation. To investigate the effect of accumulated history, we refine the response by pruning the accumulated previous history, i.e., yt+1 = M(x, yt, ft). Figure 7 presents the accuracy of the in-place feedback approach and the history-pruned variant. The results show that even when the accumulated history is removed, the feedback is still not properly incorporated in the subsequent turn. It is often suggested that users open new chat when the response does not align with their intent, yet our findings demonstrate that this approach does not offer sufficient remedy. These results demonstrate that directly editing the LLMs response constitutes more reliable and effective means of incorporating user feedback. Effect of the number of remaining incorrect cells on feedback incorporation. We examine the top-4 feedback results in Figure 8, which report FAR together with the number of incorrect cells. Analyzing feedbackresponse pairs across all turns, we observe that FAR shows no significant correlation with the number of incorrect cells. This indicates that LLMs sustain comparable level of feedback incorporation regardless of whether few or many incorrect cells remain."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Figure 8: Change of FAR with respect to the number of incorrect cells. The x-axis denotes the number of incorrect cells in the previous LLM response across the entire puzzle, and FAR is measured under the setting where feedback for up to four cells is provided. Multi-turn interaction of LLMs. Several studies aim to improve the performance of LLMs in multiturn interaction. One line of work focuses on clarifying questions, where the LLM generates follow-up questions when the users input is ambiguous (Zhang & Choi, 2025; Zamani et al., 2020; Aliannejadi et al., 2019). Zhang & Choi (2025) proposes framework that integrates clarifying questions into the response generation process. Another line of work enhances multi-turn performance through training (Zhou et al., 2024; Shani et al., 2024; Wu et al., 2025). Wu et al. (2025), for example, fine-tunes LLMs with reinforcement learning to enhance their effectiveness. Our work focuses on how LLMs can achieve more effective interaction with users without additional training, while ensuring token efficiency. Refinement of LLMs. Recent research explores self-refinement, an approach where the LLMs generate feedback on their own outputs and improve them accordingly. (Madaan et al., 2023; Dhuliawala et al., 2024; Shinn et al., 2023; Nathani et al., 2023). Welleck et al. (2023) trains separate model to produce feedback. Han et al. (2025) uses an external LLM agent to provide feedback for evaluating model performance. Zhang et al. (2025) employs user simulation model to create interaction scenarios in multi-turn, which is closely related to our work. We show turn-level dynamics of LLMs in refinement with feedback and propose an alternative interaction scheme. Analysis on multi-turn conversations. Recent studies analyze the performance of LLMs in multiturn conversations. Jiang et al. (2025) shows that LLMs fail to reliably incorporate feedback, even when it is close to the correct answer. Laban et al. (2025) find that accuracy decreases when single problem is divided into multiple parts and solved by LLMs in multi-turn manner. Sirdeshmukh et al. (2025) analyzes the performance of LLMs in multi-turn conversations across four categories, including instruction retention and self-coherence. While these studies analyze the performance of LLMs in the multi-turn setting, they primarily report high-level metrics, such as overall task accuracy, without analyzing how reasoning evolves across turns or how errors propagate."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce in-place feedback, an interaction method where users directly edit an LLMs prior response, and the model generates an output conditioned on this edited context. This approach achieves stronger refinement performance on reasoning benchmarks and is more efficient, requiring fewer input and output tokens. Through controlled experiments on ZebraLogic, we show that in-place feedback mitigates key challenges of multi-turn feedback. While our work focuses on reasoning tasks, we expect in-place feedback to be useful for wide range of applications, such as document editing and code writing. Ethical consideration. Our method allows users to edit an LLM response and then continue generating from that edit. However, such edits can be misused to bypass safety mechanisms. For example, user might insert harmful instructions or unsafe text, similar to jailbreak attacks. straightforward defense is to run user edits through safety filter before resuming the generation process. We leave the design of defenses against such attacks to future work. Reproducibility statement. We utilize four open-sourced LLMs and one closed-sourced LLM. All experimental settings and prompts are provided in Section 3.1, Section 4.1, and Appendix to ensure reproducibility. For the closed-source LLM, we use the gpt-5-mini-2025-08-07 version."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and Bruce Croft. Asking clarifying questions in open-domain information-seeking conversations. In International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2019. Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, and Jonas Geiping. Answer matching outperforms multiple choice for language model evaluation. arXiv preprint, 2025. Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient smt solver. In Tools and Algorithms for the Construction and Analysis of Systems (TACAS), 2008. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. In Findings of Annual Meeting of the Association for Computational Linguistics (ACL-Findings), 2024. Gemma Team. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503. 19786. Karim Hamade, Reid McIlroy-Young, Siddhartha Sen, Jon Kleinberg, and Ashton Anderson. Designing skill-compatible ai: Methodologies and frameworks in chess. In International Conference on Learning Representations (ICLR), 2024. Hojae Han, Seung-won Hwang, Rajhans Samdani, and Yuxiong He. Convcodeworld: Benchmarking conversational code generation in reproducible feedback environments. In International Conference on Learning Representations (ICLR), 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, and Daniel Khashabi. Feedback friction: Llms struggle to fully incorporate external feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Paul Kassianik, Baturay Saglam, Alexander Chen, Blaine Nelson, Anu Vellore, Massimo Aufiero, Fraser Burch, Dhruv Kedia, Avi Zohary, Sajana Weerawardhena, Aman Priyanshu, Adam Swanda, Amy Chang, Hyrum Anderson, Kojin Oshiba, Omar Santos, Yaron Singer, and Amin Karbasi. Llama-3.1-foundationai-securityllm-base-8b technical report, 2025. URL https: //arxiv.org/abs/2504.21039. 10 Myeongsoo Kim, Shweta Garg, Baishakhi Ray, Varun Kumar, and Anoop Deoras. Codeassistbench (cab): Dataset & benchmarking for multi-turn chat-based code assistance. arXiv preprint, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles (SOSP), 2023. Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. Llms get lost in multi-turn conversation. arXiv preprint, 2025. Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing human-ai collaborative writing dataset for exploring language model capabilities. In Computer Human Interaction (CHI), 2022. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. In International Conference on Machine Learning (ICML), 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, and Sathwik Tejaswi Madhusudhan. M2lingual: Enhancing multilingual, multi-turn instruction alignment in large language models. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2025. Abhijnan Nath, Carine Graff, and Nikhil Krishnaswamy. Lets roleplay: Examining llm alignment in collaborative dialogues. arXiv preprint, 2025. Deepak Nathani, David Wang, Liangming Pan, and William Yang Wang. Maf: Multi-aspect feedback for improving reasoning in large language models. In Empirical Methods in Natural Language Processing (EMNLP), 2023. OpenAI. Introducing GPT5, 2025. URL https://openai.com/index/ introducing-gpt-5/. OpenAI Team. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/ abs/2508.10925. Qwen Team. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412. 15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In Conference on Language Modeling (COLM), 2024. Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, et al. Multi-turn reinforcement learning with preference human feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. ReflexIn Advances in Neural Information ion: Language agents with verbal reinforcement learning. Processing Systems (NeurIPS), 2023. Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. In Findings of Annual Meeting of the Association for Computational Linguistics (ACL-Findings), 2025. Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, and Joyce Chai. Training turn-by-turn verifiers for dialogue tutoring agents: The curious case of llms as your coding tutors. In Findings of Annual Meeting of the Association for Computational Linguistics (ACL-Findings), 2025. 11 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In International Conference on Learning Representations (ICLR), 2023. Shirley Wu, Michel Galley, Baolin Peng, Hao Cheng, Gavin Li, Yao Dou, Weixin Cai, James Zou, Jure Leskovec, and Jianfeng Gao. Collabllm: From passive responders to active collaborators. In International Conference on Machine Learning (ICML), 2025. Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck. Generating clarifying questions for information retrieval. In Proceedings of The Web Conference (WWW), 2020. Michael JQ Zhang and Eunsol Choi. Clarify when necessary: Resolving ambiguity through interaction with lms. In Findings of Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-Findings), 2025. Michael JQ Zhang, Bradley Knox, and Eunsol Choi. Modeling future conversation turns to teach llms to ask clarifying questions. In International Conference on Learning Representations (ICLR), 2025. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. In International Conference on Machine Learning (ICML), 2024. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint, 2025."
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 DATASETS A.1.1 MMLU-PRO AND GPQA FREE-FORM The MMLU-Pro and GPQA-Diamond datasets are employed to evaluate the knowledge-intensive reasoning abilities of LLMs in free-form question answering. Both datasets originate from multiplechoice benchmarks but are adapted for generative evaluation via answer matching. Motivation for free-form evaluation. Multiple-choice evaluation is efficient, but it has intrinsic limitations: models can exploit statistical patterns in the answer choices without engaging in critical reasoning. As result, multiple-choice accuracy overestimates the models ability to generate correct answers. By removing the choices and requiring free-form responses, models are forced to generate an answer directly, aligning the evaluation more closely with the capabilities that matter in realworld use cases. Selection of questions. MMLU-Pro is derived from the MMLU benchmark and initially includes approximately 12,000 questions across various domains. To ensure that the questions are answerable without the provided answer choices, an automatic filtering process is employed. This process uses rubric-based grader to narrow down the dataset to about 5,500 questions. From this reduced pool, questions that are sufficiently specific and have unique correct answer are selected. After this, 493 questions are manually filtered. This dataset offers comprehensive coverage across various domains, making it an ideal resource for evaluating general knowledge and reasoning skills. The GPQA-Diamond dataset comprises 198 graduate-level science questions, designed to be challenging and test in-depth knowledge and critical reasoning. Similar to MMLU-Pro, filtering process is applied to select questions with clear and specific correct answers, resulting in final set of 126 questions. This dataset is more focused and rigorous, emphasizing high-quality scientific questions that demand deep reasoning. These preparation steps yield two complementary free-form datasets: MMLU-Pro, which provides broader coverage across domains with 493 carefully filtered and annotated items, and GPQADiamond, which offers smaller but more rigorous collection of 126 high-quality scientific questions. This ensures that free-form evaluations are conducted only on questions with clear, unambiguous solutions. A.1.2 ZEBRALOGIC The ZebraLogic dataset comprises logic grid puzzles designed to assess the reasoning capabilities of LLMs. Each puzzle is structured around grid with certain number of houses and attributes. Specifically, each puzzle involves houses and attributes, creating an grid that needs to be filled. The attributes in these puzzles are distinct for each house, where each attribute has unique values corresponding to the houses. The puzzles come with set of clues that impose logical constraints on the grid. For example, one clue might specify that The person who likes milk is Eric, while another might state, The person who drinks water is Arnold. These clues help guide the reasoning process to fill in the grid, ensuring that all constraints are satisfied. Importantly, each puzzle has unique solution, which guarantees that any feedback provided for solving the puzzle is definitive and accurate. This structure ensures that ZebraLogic provides rigorous framework for evaluating logical reasoning in models, where the set of clues provided uniquely determines each puzzles solution. Puzzle generation. Puzzles are generated by first sampling complete solution, then constructing superset of consistent clues from fixed inventory. The clue types are as follows: FOUNDAT, SAMEHOUSE, NOTAT, DIRECTLEFT/RIGHT, SIDEBYSIDE, LEFT/RIGHTOF, and ONE/TWOBETWEEN. Each clue type provides constraint by capturing specific relationship between variables. minimal subset of clues is retained through iterative pruning while preserving the uniqueness of the solution. This guarantees that puzzles are neither under-specified nor trivially over-constrained. Dataset filtering. The ZebraLogic dataset contains 1,000 puzzles spanning all combinations of N, {2, . . . , 6}. We filter the dataset by puzzle difficulty based on the search space size, which is defined as the total number of possible configurations that satisfy only the uniqueness constraints of the puzzle; for puzzle with houses and attributes, the search space size is = (N !)M . Puzzles with small search spaces (S < 103) are excluded, along with those of size 3 4 and 3 5, as well as invalid puzzles (e.g., cases where distinct categories share identical attribute values). This yields controlled, reasoning-intensive testbed for analyzing the feedback acceptance of LLMs in multi-turn settings. Input and output format. Puzzles are presented in natural language, followed by an instruction asking the model to fill the grid. The input template prompt is provided in Figure A6. The expected output is structured JSON table. This format enables automatic cell-level evaluation and the application of fine-grained feedback. We attempt up to 30 re-generations to obtain syntactically valid JSON output. If all attempts fail, we treat the instance as wrong and omit it from CPR, FAR, and CTRR calculations. For JSON-parsed prediction, we employ fuzz score from the Python rapidfuzz library to perform exact-matching evaluation. Specifically, we compute the highest similarity score among the candidate attributes, and if the score exceeds 50, we adopt the corresponding attribute as the predicted value. Feedback functions. We generate rule-based feedback using fixed template, as illustrated in Figure A7. For Llama, we additionally append reasoning guidance to the feedback, since the model frequently produces only the final JSON-formatted answer without including the corrected reasoning process. A.2 PROMPT AND POST-PROCESSING Empirical experiments. We define answer leakage as any explicit revelation of the ground-truth answer within feedback or intervention outputs. We prevent answer leakage, following Jiang et al. (2025). To prevent leakage, we use the prompts in Figure A4 and Figure A5, and apply postprocessing for each agents: (i) after generating feedback, we scan the message and mask any span that reveals the ground truth; (ii) for the in-place feedback agent, if any part of the message exposes the solution, we prune those spans before presenting the message to the model. ZebraLogic. We constrain the in-place feedback agent to avoid introducing reasoning beyond the scope of the provided feedback. The models output is segmented into discrete reasoning steps, which are then checked for consistency with the provided feedback. In cases of conflict, the corresponding step is minimally revised, following the prompt in Figure A8. Notably, the agent does not have direct access to the puzzle itself, which limits its ability to extend reasoning beyond the explicitly given feedback. A.3 HYPERPARAMETERS. We observed that Llama-3.1-8B-Instruct can be overly verbose, resulting in degradation of generation quality. To stabilize decoding, we set its repetition penalty to 1.15 for Llama. All other models use repetition penalty of 1.0. The maximum generation length is 2048 tokens for all models and experiments. We use vLLM (Kwon et al., 2023) for efficient inference."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Figure A1 presents grid and cell accuracy as function of the number of turns. In-place feedback converges substantially faster than multi-turn feedback, indicating that it enables the model to incorporate feedback more efficiently. Figure A2 illustrates LLM behavior under oracle feedback. 14 In most cases, in-place feedback achieves higher CPR and FAR compared to multi-turn feedback, without CPR of the Qwen in the first turn. Figure A1: Grid and cell accuracy of LLMs on the Zebralogic dataset. In-place feedback consistently outperforms multi-turn feedback under an oracle setting. Figure A2: Correctness-Preserving Rate (CPR), Feedback Acceptance Rate (FAR), and Comparison of Correction Through Reasoning Ratio (CTRR) for 10-turn conversations of LLMs on the ZebraLogic. The Oracle feedback function is used. The points with black border represent the second response of the LLMs (i.e., y1), and the subsequent responses across turns are connected by lines."
        },
        {
            "title": "C QUALITATIVE EXAMPLES",
            "content": "C.1 IN-PLACE FEEDBACK EXAMPLE Figure A9, Figure A10, Figure A11, and Figure A12 are the qualitative examples of in-place feedback on the three benchmarks. C.2 MULTI-TURN FEEDBACK FAILURE EXAMPLE We observe failure cases of multi-turn feedback, and present the instances in Figure A13, Figure A14, and Figure A15."
        },
        {
            "title": "D THE USE OF LARGE LANGUAGE MODELS",
            "content": "We used an LLM assistant as writing tool for grammar checking and paraphrasing. In addition, since our experiments required carefully designed prompts, we employed the assistant to refine prompts. 15 LLM-as-a-judge prompt for MATH-hard SYSTEM PROMPT You are mathematical equivalence judge. Given question, correct answer, and models prediction, determine if they are mathematically equivalent even if they have different formatting or representation. Respond with ONLY YES if they are equivalent, or NO if they are not. If the models prediction is correct, but the formatting is wrong, please respond with YES. DO NOT RESPOND WITH ANYTHING ELSE. USER PROMPT Question: {QUESTION} Correct answer: {CORRECT ANSWER} Model prediction: {PREDICTION} Are these equivalent? Answer YES or NO. (a) Prompt format used in MATH-hard. LLM-as-a-judge prompt for MMLU-pro free-form and GPQA free-form SYSTEM PROMPT You are strict and impartial judge for evaluating model predictions. Given question, correct answer, and models prediction, decide whether the prediction is equivalent to the correct answer. Respond with ONLY YES or NO. Never output anything else. Judgment Criteria 1. The prediction must fully answer the question and cannot be vague. 2. Ignore differences in formatting, punctuation, capitalization, or spacing. 3. For numeric answers: - Compute the relative error = prediction ground truth / mean(prediction, ground truth). - The prediction is correct if the relative error < 1%. - If the ground truth is single numeric value but the prediction is range, treat it as incorrect (even if the range contains the ground truth). - If the ground truth is single value but the prediction is range, output NO. 4. If the prediction is correct in substance but expressed in different format, respond YES. Final Rule: Output must be exactly YES or NO. No explanations, no extra words. USER PROMPT Question: {QUESTION} Correct answer: {CORRECT ANSWER} Model prediction: {PREDICTION} Are these equivalent? Answer YES or NO. (b) Prompt format used in MMLU-pro free-form and GPQA free-form. Figure A3: Prompt format used for LLM-as-a-judge in MATH-hard, MMLU-pro free-form, and GPQA free-form."
        },
        {
            "title": "SYSTEM PROMPT",
            "content": "You are rigorous verifier of LLM answers. Your task: Given question, the models full answer, and the correct answer, identify the first and most critical reasoning flaw that causes the error. You MUST NOT reveal or speculate about the correct final answer. - Note that the options in previous questions might have been switched in each different attempt. Your output must follow the structure below: Explain in (maximum 23) precise sentences why that step is incorrect. Be specific in your critique. Give short and specific actionable advice to fix it. Constraints: You may only give feedback on one reasoning error at time the earliest critical one. If the models answer is correct in logic but wrong in formatting, your feedback should address the formatting issue. You DO NOT leak and mention the correct final answer. DO NOT SOLVE THE PROBLEM, JUST PROVIDE FEEDBACK WITHOUT MENTIONING THE CORRECT ANSWER Your output strictly starting with Your output is wrong because and then followed by the explanation. Do not output anything else."
        },
        {
            "title": "USER PROMPT",
            "content": "Your goal is to identify the earliest and most critical logical flaw in the models reasoning DO NOT provide or hint at the correct final answer. - Note that the options in previous questions might have been switched in each different attempt. Problem {PROBLEM} Most Recent Model Answer {MODEL ANSWER} Correct Final Answer {GROUND TRUTH} Based on this, provide feedback on the single most important error in the models answer. Do not leak and mention the correct final answer and do not add any extra commentary. Figure A4: Prompt format used for feedback agent in MATH-hard, MMLU-pro free-form, and GPQA free-form. 17 Prompt format used for in-place feedback agent SYSTEM PROMPT You are helpful assistant who INTERVENES in math solution based on user feedback. Your job is to produce JSON object for single in-place replace operation: - Identify the shortest unique substring (SUS) from the original solution that must be edited to apply the feedback. - Produce the revised text for that exact span. - Do NOT change anything before the flaw, and do NOT continue solving the problem beyond where the feedback applies. - Preserve all whitespace, punctuation, LaTeX, and casing exactly as in the original solution for the target substring. - The target must be contiguous substring that occurs exactly once in the original solution. If not unique, minimally extend the span (e.g., include adjacent tokens or punctuation) until it becomes unique. - Return ONLY valid JSON with UTF-8 and proper escaping (no trailing commas, no extra commentary). Return JSON with this schema (single edit only): { } target sentence: < exact shortest unique substring copied from the original>, edit sentence: <the revised substring after applying the feedback> Constraints: - Output must be single-line or multi-line JSON object; do not include any extra text. - Do not normalize quotes/hyphens/spaces; copy exactly from the original for target sentence. - Do not introduce additional edits beyond the specified span. - Do not provide reasoning process beyond the feedback. USER PROMPT <The Start of Answer> {ANSWER} <The End of Answer> <The Start of Original Solution> {ORIGINAL SOLUTION} <The End of Original Solution> <The Start of User Feedback> {USER FEEDBACK} <The End of User Feedback> <The Start of Instructions> Write the JSON according to the following: - Apply ONLY the given feedback to the original solution. - Identify the shortest unique substring in the original that must change to satisfy the feedback; this must appear exactly once. - If the obvious sentence occurs multiple times, minimally extend the span (e.g., prepend/append one or two nearby tokens or punctuation) until uniqueness holds. - Put the original substring in target sentence (copied verbatim from the original, including whitespace/newlines). - Put the corrected version in edit sentence. - If the feedback is sentence-like, keep it within three sentences in the edited span. - STRICTLY FOLLOW: - Do not solve the problem beyond where the feedback applies. - Stop right after applying the feedback. - Return ONLY valid JSON with keys target sentence and edit sentence. Figure A5: Prompt format used for in-place feedback agent in MATH-hard, MMLU-pro free-form, and GPQA free-form."
        },
        {
            "title": "Prompt format used for ZebraLogic",
            "content": "SYSTEM PROMPT You are helpful assistant that solves zebra puzzles. Given puzzle and json template, you need to solve the puzzle and fill in the json template. You need to fill in the json template with the correct attributes. USER PROMPT # Example Puzzle There are 3 houses, numbered 1 to 3 from left to right, as seen from across the street. Each house is occupied by different person. Each house has unique attribute for each of the following characteristics: - Each person has unique name: Peter, Eric, Arnold. - Each person has unique favorite drink: tea, water, milk ## Clues for the Example Puzzle 1. Peter is in the second house. 2. Arnold is directly left of the one who only drinks water. 3. The one who only drinks water is directly left of the person who likes milk. ## Answer to the Example Puzzle {{ solution: {{ House 1: {{ Name: Arnold, Drink: tea }}, House 2: {{ Name: Peter, Drink: water }}, House 3: {{ Name: Eric, Drink: milk }} }} }} # Puzzle to Solve {PUZZLE} # Instruction Now please solve the above puzzle. Present your reasoning and solution in the following json format: {JSON TEMPLATE} Figure A6: Input template used for ZebraLogic."
        },
        {
            "title": "Feedback template for Gemma and Qwen in ZebraLogic",
            "content": "Your answer is incorrect. Please revise your solution based on the following feedback. - {CATEGORY} of the {HOUSE} is {GROUND TRUTH}, not {PREDICTION}. - (a) Feedback template for Gemma and Qwen."
        },
        {
            "title": "Feedback template for Llama in ZebraLogic",
            "content": "Please revise your step-by-step reasoning based on the following feedback, and then provide solution in the following json format. Do not just provide the final solution, but also provide the reasoning process. - {CATEGORY} of the {HOUSE} is {GROUND TRUTH}, not {PREDICTION}. - (b) Feedback template for Llama. Figure A7: Rule based feedback template in ZebraLogic. 20 Prompt format used for in-place feedback agent in ZebraLogic SYSTEM PROMPT You are an in-place patcher. Given reasoning step and feedback items, decide if the step conflicts with the feedback. If so, produce the minimally edited step that preserves all non-conflicting content and enforces the feedback exactly. LOCKED CLUE SPANS - Any substring beginning with Clue and ending at the first period is read-only. Do not alter or delete it. - If step contains locked span(s), edit only after the last locked span. - If formatted as Clue N: ... . - editable text, edit only after the first - . If editable text is deleted, remove the dangling - . REFERENCE ATTRIBUTES {CATEGORY} - Use this only as reference to detect conflicts and apply equivalence normalization. Do not generate new information beyond feedback. EDIT RULES - Feedback is always given in positive-only form, e.g., attr of the house is A. - Always overwrite the attribute with the provided value A. - Modify only attributes mentioned in the feedback. - Do not add new reasoning, deductions, constraints, or explanations. - Do not insert additional sentences beyond the minimal correction. - Do not add new information, negations, or style fixes. - Equivalence normalization: * name of house is is in House * musicgenre of house is Y, The house is House * Apply the same for other attributes. - Rewrite conflicts to the positive corrected assignment. Never use negations. CONFLICT CRITERIA statement conflicts only if it: - Mentions the same (house, attribute) with contradictory value, or - Assigns feedback value to the wrong house by equivalence. DEPENDENCY PRUNING - If sentence depends on premise that is corrected or negated by the feedback and cannot be minimally rewritten, delete that sentence only. - Do not delete the entire reasoning step. - Never alter or delete locked clue text. OUTPUT - If no conflict: conflict:no - If conflict: conflict:yes,replacement:... where replacement is the fully patched step, including unchanged locked clues. RULES - Make the smallest edits necessary. - Preserve capitalization, spacing, and order. - Never output empty replacement if locked clue exists. - Absolutely never add new reasoning, deductions, or commentary not present in the original. USER PROMPT Reasoning step {REASONING STEP} Feedback {FEEDBACK} Figure A8: Prompt format used for in-place feedback agent in ZebraLogic. 21 In-place feedback example - MATH-hard Problem Express the infinite series 3 206 + 9 21032 + 27 21033 + as terminating decimal. Model response Let = 3 206 + 21032 + 27 21033 + . We can rewrite this as = (cid:88) n=1 3n 2 103n+1 = 1 2 (cid:88) n=1 3n 103n+1 = 1 2 (cid:88) n=1 3n 103 103n = 1 2 103 (cid:19)n (cid:88) n= (cid:18) 3 103 The sum (cid:80) 1, the series converges to (cid:0) 3 103 n=1 (cid:1)n is geometric series with first term 3 103 and common ratio 103 . Since (cid:12) (cid:12) 3 103 (cid:12) (cid:12) < (cid:19)n (cid:88) n= (cid:18) 3 103 = 3 103 1 3 103 = 3 103 1033 103 = 3 103 100 103 = 3 100 3 3 3 3 20600 = 2103 3 100 = 3 100 0.01464539 1 20600 = 3 206 100 to get 0.0001464539.... We can write 20600 Now we need to express 20600 = 0.00014645 . . . We can write 206 1 Therefore, = 1 20600 as terminating decimal. We can 3 20600 = 100 . We have divide 3 by 20600: 3 100 = 0.0001464539. We can also 206 0.01464539. Then 206100 = 3 100 . We can divide 3 by 206 to get 0.01464539.... Then we multiply write by 1 20600 = 0.0001464539... The decimal representation is 0.0001464539.... We want to express this as terminating decimal. We have = 3 20600 . Dividing 3 by 20600, we get 3 20600 = 0.0001464539... Since we want terminating decimal, we need to find common denominator. We have 20600 = 23 52 103. So 2352103 . We can write 20600 = 0.0001464539... The decimal representation is 0.0001464539.... Final Answer: The final answer is 0.000146 206100 = 3 206 1 20600 = 3 3 3 3 3 1) In-place edit Let = 206 + 9 21032 + 27 21033 + . We can rewrite this as = (cid:80) n=1 3n 2103n = 2 (cid:80) n=1 (cid:1)n (cid:0) 3 103 2) Continuation generation This is geometric series with first term = 3 the series converges and its sum is given by 103 and common ratio = 3 103 . Since = 3 103 < 1, (cid:19)n (cid:88) n= (cid:18) 3 103 = 3 103 1 3 103 = 3 103 1033 103 = 3 103 100 103 = 3 100 3 100 = 3 2 3 2102 = 3 2 Therefore, = 1 2100 = 3 3 200 = 0.015 Thus, the infinite series is equal to 0. 200 To express this as terminating decimal, we divide 3 by 200: 3 200 = 1 100 = 1.5 0.01 = 0.015 Alternatively, we can directly divide 3 by 200: Figure A9: In-place feedback example for the MATH-hard dataset. 22 In-place feedback example - MATH-hard Problem Dr. Zaius invests $10, 000 in CD with an annual interest rate of 4% that compounds semi-annually (twice year). After six months, he rolls over the CD into another CD with an annual interest rate of 5% that also compounds semi-annually. After six months in the second CD, how much does Dr. Zaius have, in dollars? Model response First, we need to calculate the amount after the first six months with the first CD. The principal is 10000, the annual interest rate is 4%, and the interest compounds semi-annually. So the interest rate per period is 4%/2 = 2% = 0.02. The number of periods is 6/2 = 3. The formula for compound interest is = (1+r)n, where is the amount after periods, is the principal, and is the interest rate per period. In this case, = 10000, = 0.02, and = 3. So, = 10000(1+0.02)3 = 10000(1.02)3 = 10000(1.061208) = 10612.08. After six months, Dr. Zaius rolls over the CD into another CD with an annual interest rate of 5% that also compounds semi-annually. The principal is 10612.08. The annual interest rate is 5%, so the interest rate per period is 5%/2 = 2.5% = 0.025. The number of periods is 6/2 = 3. Using the same formula, = (1 + r)n, we have = 10612.08, = 0.025, and = 3. So, = 10612.08(1 + 0.025)3 = 10612.08(1.025)3 = 10612.08(1.076890625) 11418.53. Therefore, after six months in the second CD, Dr. Zaius has approximately 11418.53 1) In-place edit First, we need to calculate the amount after the first six months with the first CD. The principal is 10000, the annual interest rate is 4%, and the interest compounds semi-annually. So the interest rate per period is 4%/2 = 2% = 0.02. The number of periods is 1. 2) Continuation generation The amount after six months is (1 + r)n = 10000(1 + 0.02)1 = 10000(1.02) = 10200. Then, Dr. Zaius rolls over the CD into another CD with an annual interest rate of 5% that also compounds semi-annually. The principal is 10200, the annual interest rate is 5%, and the interest rate per period is 5%/2 = 2.5% = 0.025. The number of periods is 1. The amount after six months in the second CD is 10200(1 + 0.025)1 = 10200(1.025) = 10200 + 10200 0.025 = 10200 + 255 = 10455. So, after six months in the second CD, Dr. Zaius has 10455 Figure A10: In-place feedback example for the MATH-hard dataset. 23 In-place feedback example - GPQA free-form Problem Consider an isolated system of 13 identical particles each with charge 2e and negligible mass. 12 of these charges are constrained to stay at 2 from point P. The 13th charge is fixed at P. What is the minimum energy (in Joules) of this system correct to three decimals? Model response Let be the charge of each particle, and be the distance of the 12 particles from the point P. The 13th charge is at the point P. The Coulomb force between two charges q1 and q2 separated by distance is given by = kq1q2 The potential energy of the system is given by is the Coulomb constant. [omitted for brevity...] , where = 1 4πϵ0 U = 12 (cid:88) i=1 kq2 ri where ri is the distance from the i-th charge to the point P. Since all 12 charges are at distance of 2 from P, ri = 2 for all i. = 12 kq2 The total energy is = 6kq2. = 6 6(2.56641038) π(8.8541012) 15.401038 5.570e 27J π(8.8541012) 2.7831011 5.57 1027J. The minimum energy is approximately 2 = 6kq2 [omitted for brevity...] 6(1.6021019)2 (2e)2 = 64e2 4πϵ0 = 6e2 πϵ0 1 4πϵ0 1) In-place edit Let be the charge of each particle, and be the distance of the 12 particles from the point P. The 13th charge is at the point P. The Coulomb force between two charges q1 and q2 separated by distance is given by = kq1q2 The potential energy of the system is given by is the Coulomb constant. [omitted for brevity...] , where = 1 4πϵ0 r2 = (cid:88) i<j kqiqj rij 2) Continuation generation which includes the 12 centralouter terms and the 66 outerouter pairwise terms; the outerouter distances rij depend on the arrangement of the 12 charges on the sphere of radius 2, so one must evaluate this pairwise sum for the energy-minimizing configuration before computing any numeric value. [omitted for brevity...] The minimum energy is = 13 2 4π(8.8541012) 2.82 1026J. 13(1.6021019)2 kq2 = = 13e2 4πϵ0 1 4πϵ0 (2e)2 2 2 2.820e 26J Figure A11: In-place feedback example for the GPQA free-form dataset. 24 In-place feedback example - MMLU-pro free-form Problem particle of mass and velocity u1 makes head-on collision with another particle of mass 2m at rest. If the coefficient of restitution is such to make the loss of total kinetic energy maximum, what are the velocities v1 after the collision? Model response Let m1 = and v1 = u1 be the mass and initial velocity of the first particle. Let m2 = 2m and v2 = 0 be the mass and initial velocity of the second particle. Let 2 be the velocities of the first and second particles after the collision, respectively. The initial total kinetic energy is Ki = 1 2.The loss of kinetic energy is = Ki Kf = 1 1. The final total kinetic energy is Kf = 1 2 mv 2 m2v 2. [omitted for brevity...] 2 m2(0)2 = 1 2 (2m)v 1 and 2 m1u2 2 m1v 2 mu2 2 mu2 1 + 1 1 + 1 1 1 1 To maximize K, we need to minimize 1 mv 2 K = 1 1 2m 1 2 mu2 2 mu2 1 = 1 2. Since 2 2 2 = 1 1. Final Answer: The final answer is 1, we have 2 = 1 1. Then 1 = 2v 2. 1) In-place edit Let m1 = and v1 = u1 be the mass and initial velocity of the first particle. Let m2 = 2m and v2 = 0 be the mass and initial velocity of the second particle. Let 2 be the velocities of the first and second particles after the collision, respectively. The initial total kinetic energy is Ki = 1 1. The final total kinetic energy is Kf = 1 and 1 + 1 1 + 1 2 m2(0)2 = 1 2 m2v2 2 . 2 m1v 2 m1u2 2 mu2 2) Continuation generation The loss of kinetic energy is = Ki Kf = 1 K. The coefficient of restitution is = 2v 1 u1v 2 conservation of momentum: m1u1 + m2(0) = m1v 2 = u1v [omitted for brevity...] 1 2 mu2 1 1 1 1 2 m1v2 2 m2v2 2 . We want to maximize . Since its head-on collision, we can use the 1 + 2v 1 + m2v 2 mu1 = mv 2 u1 = 1 + 2mv To maximize K, we take the derivative with respect to 1 and set it to zero. However, we are given that the loss of kinetic energy is maximized. This occurs when is maximized. For maximum loss 0+3 = u1 of kinetic energy, we want to be as small as possible, i.e., = 0. If = 0, then 3 . 1 = u1(10) Then 2 = u1v 2 = u1 1 u1 2 = 3 2u1 3 2 = 3 . 1 = u1 3 Figure A12: In-place feedback example for the MMLU-pro free-form dataset. 25 Failure example of multi-turn refinement - 1) corrupting correct content Problem In convex hexagon ABCDEF , all six sides are congruent, and are right angles, and B, C, E, and are congruent. The area of the hexagonal region is 2116( 2 + 1). Find AB. Model response To solve for AB in the given convex hexagon ABCDEF , we will follow these steps: 1. Identify the properties of the hexagon: - All six sides are congruent. - and are right angles. - B, C, E, and are congruent. 2. Determine the measure of the congruent angles: - Since the sum of the interior angles of hexagon is (6 2) 180 = 720. - Let = = = = x. - We have = 90 and = 90. - Therefore, 4x + 180 = 720. - Solving for x: 4x = 540 = = 135 3. Visualize the hexagon: - Place the hexagon on coordinate plane with at (0, 0) and at (a, 0). - Since = 90 and = 90, the hexagon can be divided into rectangle and two right triangles. 4. Divide the hexagon into simpler shapes: - The hexagon can be divided into rectangle ABCD and two right triangles ABE and CDF . Since the problem asks for AB, and we have determined that AB = s, the final answer is: 23 Feedback Your output is wrong because you assumed without justification that the hexagon can be divided into rectangle and two right triangles and then treated ABCD as rectangle (even as square) with both width and height equal to s; right angles at and alone do not imply AB CD or BC AD, so ABCD need not be rectangle and the triangle legs need not equal s. Fix it by producing correct partition justified by the given angles (for example, draw and justify specific diagonals such as BF and CE), use the 135 measure of the other four angles to deduce the shapes and side relationships of the pieces, and compute the area from that valid decomposition rather than assuming rectangle/square. Model response Lets re-evaluate the problem step-by-step, ensuring we correctly decompose the hexagon based on the given angles and properties. 1. Identify the properties of the hexagon: - All six sides are congruent. - = 90 and = 90. - = = = = 135. 2. Visualize the hexagon: - Place the hexagon on coordinate plane with at (0, 0) and at (s, 0). 3. Draw and justify specific diagonals: - Draw diagonal BF and CE. These diagonals will help us decompose the hexagon into simpler shapes. 4. Analyze the resulting shapes: - Diagonal BF splits and into two angles each: ABF = BD = 45. - Similarly, diagonal CE splits and into two angles each: BCE = ECD = 45. Since the problem asks for AB, and we have determined that AB = s, the final answer is: 23 Figure A13: Failure example of multi-turn refinement on the MATH-hard free-form dataset: corrupting correct content. 26 Failure example of multi-turn refinement - 2) ignoring feedback Problem container weighs 3.22 lb force when empty. Filled with waterat 60F the mass of the container and its contents is 1.95 slugs. Find the weight of the water in the container and its volume in cubic feet. Assume density of water= 62.4 lb force/ft3. Model response Let Wc be the weight of the empty container in lb force, which is given as Wc = 3.22 lb. Let be the mass of the container and its contents in slugs, which is given as = 1.95 slugs. Let ρ be the density of water in lb force/ft3, which is given as ρ = 62.4 lb/ft3. We want to find the weight of the water in the container, Ww, and the volume of the water in the container, in cubic feet. First, we find the weight of the container and its contents due to gravity. We know that weight = mg, where is mass and is the acceleration due to gravity. We can use the given mass in slugs and the standard value of = 32.2 ft/s2. = = 1.95 slugs 32.2 ft/s2 = 62.19 lb. The weight of the container and its contents is also the sum of the weight of the empty container and the weight of the water. Wc + Ww = 62.19 lb 3.22 + Ww = 62.19 Ww = 62.19 3.22 = 58.97 lb. Now we can find the volume of the water in the container using the density formula: ρ = is the weight of the water and is the volume. = ρ = 58.97 lb 62.4 lb/ft3 0.9455 ft3. , where Therefore, the weight of the water is 58.97 lb and the volume of the water is approximately 0.9455 ft3. Weight of water = 58.97 lb, Volume = 0.9455 ft Feedback Your output is wrong because you miscomputed the total weight: you evaluated = incorrectly (you used 1.9532.2 = 62.19), which led to an incorrect water weight. Recompute = with the correct product, then subtract the empty container weight to get the water weight and divide that by the given density to obtain the volume. Model response Let Wc be the weight of the empty container in lb force, which is given as Wc = 3.22 lb. Let be the mass of the container and its contents in slugs, which is given as = 1.95 slugs. Let ρ be the density of water in lb force/ft3, which is given as ρ = 62.4 lb/ft3. We want to find the weight of the water in the container, Ww, and the volume of the water in the container, in cubic feet. First, we find the weight of the container and its contents due to gravity. We know that weight = mg, where is mass and is the acceleration due to gravity. We can use the given mass in slugs and the standard value of = 32.2 ft/s2. = = 1.95 slugs 32.2 ft/s2 = 62.19 lb. The weight of the container and its contents is 62.19 lb. The weight of the empty container is 3.22 lb. The weight of the water is the difference between the total weight and the weight of the empty container: Ww = 62.19 3.22 = 58.97 lb. Now we can find the volume of the water in the container using the density formula: ρ = is the weight of the water and is the volume. = ρ = 58.97 lb 62.4 lb/ft3 0.9455 ft3. , where Therefore, the weight of the water is 58.97 lb and the volume of the water is approximately 0.9455 ft3. Weight of water = 58.97 lb, Volume = 0.9455 ft3 Figure A14: Failure example of multi-turn refinement on the MMLU-pro free-form dataset: ignoring feedback. Failure example of multi-turn refinement - 3) introducing new error Problem 125/250-volt three-wire system has load on the positive side of 500 amp, on the negative side of 450 amp, the neutral current therefore being 50 amp. If each machine of the balancer set has an efficiency of 86 percent, calculate the current for each of these machines and for the main generator. Model response Heres step-by-step solution to the problem: 1. Calculate the total power in the positive phase: Ppos = Vpos Ipos = 125V 500A = 62500W 2. Calculate the total power in the negative phase: Pneg = Vneg Ineg = 125V 450A = 56250W 3. Calculate the total power in the neutral phase: Pneutral = Vneutral Ineutral = 125V 50A = 6250W 4. Calculate the total system power: Ptotal = Ppos + Pneg + Pneutral = 62500W + 56250W + 6250W = 125000W 5. Determine the number of balancer sets: The problem states each balancer set has an efficiency of 86%, which implies that there are two balancer sets. [omitted for brevity...] The balancer sets are designed to correct the imbalance. The generator provides 1000A. The imbalance is 500A - 450A = 50A. The balancer sets must provide 50A. Since each set has 86% efficiency, the current each set provides is = 50 Igenerator = 1000, Iset1 = 581.39, Iset2 = 581.39 0.86 581.39A. So, I1 = I2 = 581.39A. Feedback Your output is wrong because you infer there are two balancer sets from the 86% efficiency efficiency does not determine the number of machines, so splitting total power by two is an unjustified non sequitur that corrupts all subsequent calculations. To fix it, do not assume quantity: either use the actual number of machines stated in the problem or introduce variable n, first compute the required balancing current from the load imbalance (or neutral current), then divide that required output among machines and account for each machines 86% efficiency by dividing the required output current/power by 0.86 to obtain the input/current for each machine. Model response Okay, you are absolutely correct. My apologies for the flawed reasoning. Lets redo the calculation with the correct approach. 1. Calculate the total power: Ptotal = 62500W + 56250W + 6250W = 125000W 2. Calculate the imbalance: The imbalance is the difference between the positive and negative currents: 500A 450A = 50A. This is the current the balancer sets need to correct. 3. Determine the number of balancer sets: The problem doesnt explicitly state the number of balancer sets. Lets denote the number of balancer sets as n. 4. Calculate the required current per balancer set: Each balancer set needs to provide 50A. 5. Calculate the current required from each balancer set, accounting for efficiency: Current per set = (Required current) / (Efficiency) = 50A/0.86 = 581.395A (approximately) Igenerator = 1000, Iset1 = 58.14, Iset2 = 58.14 Figure A15: Failure example of multi-turn refinement on the MMLU-pro free-form dataset: introducing new error."
        }
    ],
    "affiliations": [
        "Computer Science and Engineering, POSTECH",
        "Graduate School of Artificial Intelligence, POSTECH"
    ]
}