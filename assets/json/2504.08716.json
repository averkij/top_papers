{
    "paper_title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance",
    "authors": [
        "Wissam Antoun",
        "Benoît Sagot",
        "Djamé Seddah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 1 7 8 0 . 4 0 5 2 : r Preprint. Under review. ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance Wissam Antoun, Benoˆıt Sagot & Djame Seddah Inria, Paris, France {wissam.antoun,benoit.sagot,djame.seddah}@inria.fr"
        },
        {
            "title": "Abstract",
            "content": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERTs primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models."
        },
        {
            "title": "Introduction",
            "content": "Despite the widespread adoption of decoder-only large language models (LLMs) in our post-ChatGPT era, encoder-only transformers such as BERT (Devlin et al., 2019) continue to play central role in many NLP applications. These models remain the backbone for wide range of non-generative tasks such as classification, named entity recognition (NER), and retrieval-based systems, especially in high-throughput or latency-sensitive environments. Their relatively low compute requirements and strong performance across standard benchmarks make them practical choice for large-scale deployment, including in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) pipelines or guardrails systems (Neill et al., 2024). Continuous architectural and training objective improvements have led to more performant and efficient encoder-only transformer variants, among which DeBERTaV3 (He et al., 2021a) and the recently proposed ModernBERT (Warner et al., 2024) stand out as major improvements. According to Warner et al. (2024), their model reports superior performance relative to DeBERTaV3, the previous state-of-art model, across several popular NLP benchmarks. However, interpreting these performance improvements is challenging due to the lack of details regarding their training data. Without comparisons conducted on identical datasets, it remains unclear whether the reported gains reflect genuine architectural advances or simply differences arising from the choice of training data. This uncertainty motivates our study aimed to evaluate the impact of architecture versus training data by conducting controlled comparison among ModernBERT, DeBERTaV3, and RoBERTa models. We select CamemBERTaV2 (Antoun et al., 2024), French DeBERTaV3 model trained from scratch, as our primary reference since both its intermediate checkpoints and training dataset are publicly available. Additionally, we include 1 Preprint. Under review. CamemBERTv2 (Antoun et al., 2024), RoBERTa (Liu, 2019) based model pretrained on the same dataset, to comprehensively assess how ModernBERTs architectural advancements compare not only against the latest DeBERTaV3-based models but also against more traditional BERT/RoBERTa architectures. Leveraging these resources, we pretrained French ModernBERT using the exact same dataset as CamemBERTaV2 and CamemBERTv2, thus ensuring that differences in performance directly reflect architectural variations rather than dataset composition or quality. Furthermore, we pretrained another ModernBERT variant on carefully curated, high-quality French corpus to further explore the role of dataset quality in model performance. The key takeaways from our comprehensive experiments are as follows: When controlling for dataset differences, DeBERTaV3 outperforms ModernBERT in terms of overall benchmark performance and training sample efficiency, indicating that DeBERTaV3s architecture and training objective optimization provide superior learning capabilities compared to ModernBERTs efficiency-oriented design. Nonetheless, ModernBERT presents clear practical advantage due to its significantly faster training and inference speeds, driven by an orthogonal set of optimizations. Moreover, while ModernBERT may not surpass DeBERTaV3, it offers meaningful improvements over previous transformer-based models such as BERT and RoBERTa. We also observe that training models on high-quality, filtered datasets results in faster convergence but does not substantially increase final performance metrics. This finding highlights potential limitation of current NLP benchmarks, suggesting possible saturation that prevents fine-grained discrimination between models of similar performance. Our findings show the importance of clearly separating respective effects of architectural changes and training datasets when evaluating NLP models. Our controlled comparison using the same pretraining dataset provides more accurate insights into the strengths and limitations of ModernBERT and DeBERTaV3 architectures To foster further research and reproducibility, we publicly release both of our pretrained French ModernBERT models, including the variant trained on the CamemBERTaV2 dataset and on our high-quality filtered corpus. These models, along with training scripts, intermediate checkpoints, and evaluation results, are available on HuggingFace1."
        },
        {
            "title": "2 Literature Review",
            "content": "Transformer-based language models have become the cornerstone of modern NLP, starting with BERT (Devlin et al., 2019), which introduced masked language modeling (MLM) and next sentence prediction as self-supervised pretraining tasks used to pretrain encoder-only transformer models. RoBERTa (Liu, 2019) subsequently improved upon BERT by removing the next sentence prediction objective, training on larger corpora, and applying more robust optimization techniques. Despite these advances, both BERT and RoBERTa shared fundamental architectural limitation: they used absolute positional embeddings and standard attention mechanisms that lacked efficiency and fine-grained contextual representation. To address these limitations, DeBERTa (He et al., 2021b) introduced disentangled attention mechanism, decoupling content and positional information, thereby improving the models ability to generalize across contexts. DeBERTaV3 (He et al., 2021a) further extended these innovations by incorporating Replaced Token Detection (RTD) (Clark et al., 2020) for more sample-efficient training, as well as Gradient-Disentangled Embedding Sharing (GDES) to prevent conflicting updates in shared embeddings between the generator and discriminator during training. 1https://huggingface.co/collections/almanach/moderncamembert-67f7e6d85ede5f7cfc1ce012 2 Preprint. Under review. In parallel, architectural and efficiency-driven improvements have become an active area of research. ModernBERT (Warner et al., 2024) was recently proposed to modernize the BERT architecture by incorporating suite of design choices aimed at improving inference speed, training throughput, and context window size. These include FlashAttention (Dao et al., 2022; Dao, 2024; Shah et al., 2024), alternating global and local attention layers(Team et al., 2024), sequence packing (Portes et al., 2023), and rotary positional embeddings (RoPE)(Su et al., 2021). ModernBERT also removes architectural elements such as bias terms and introduces GeGLU (Shazeer, 2020) activations, making it strong contender for production scenarios requiring high efficiency. While the authors of ModernBERT report superior benchmark performance over DeBERTaV3, the absence of transparent training data and lack of head-to-head comparisons on shared datasets introduces ambiguity. It is thus unclear whether the reported improvements are driven by architectural enhancements or the underlying training data."
        },
        {
            "title": "3 Methodology",
            "content": "We conduct controlled study focusing on the performance of ModernBERT compared to DeBERTaV3-based and RoBERTa-based models. Our goal is to identify and separate architectural improvements from data-driven performance differences, addressing ambiguities in prior studies that used undisclosed datasets. 3.1 Pre-training Datasets Our experiments involve two distinct pre-training datasets: CamemBERTaV2 Original Dataset. We first make use of the publicly available French dataset originally used by the authors of CamemBERTaV2. This dataset has 275 billion tokens, sourced from: CulturaX-FR Corpus (Nguyen et al., 2023): The French subset of multilingual corpus containing around 265 billion French tokens, constructed from multiple snapshots of OSCAR and mC4 corpora. HALvesting Corpus (Kulumba et al., 2024): Approximately 4.7 billion tokens of academic and scientific content from French theses and research papers. French Wikipedia: Roughly 0.5 billion tokens from Wikipedia, intentionally upsampled to enhance general knowledge representation. This dataset serves as reference point, allowing us to directly compare models under identical training dataset conditions. High-Quality Filtered Dataset. We also feature second significantly larger 1T tokens French dataset created by applying heuristic and semantic filters in addition to full deduplication to the French section of the RedPajamaV2 corpus (Weber et al., 2024), combined with the HALvesting corpus and French Wikipedia. Semantic filtering was done by fine-tuning BERT classifier trained on document quality dataset automatically labeled by LLama-3 70B (Grattafiori et al., 2024). 3.2 Model Training We pretrained two variants of the ModernBERT model, one on the CamemBERTaV2 dataset, which we call ModernBERT-CV2, and the other on our high-quality filtered dataset, named ModernBERT-HQ, periodically saving intermediate checkpoints.2 Both variants are base-sized models trained with Masked Language Modeling (MLM) for 1 trillion tokens, with maximum sequence length of 1024, and use the same tokenizer as 2We use the publicly available ModernBERT codebasehttps://github.com/AnswerDotAI/ModernBERT 3 Preprint. Under review. MODEL NER F1 QA F1 EM CLS ACC PAWS-X XNLI ACC ACC CamemBERTV2 CamemBERTaV2 91.990.96 93.400. 80.390.36 83.040.19 61.350.39 64.290.31 95.070.11 95.630.16 92.000.24 93.060.45 81.750.62 84.820.54 ModernBERT-CV2 92.030.14 ModernBERT-HQ 91.800. 81.340.35 81.110.26 61.470.46 62.070.44 95.180.20 95.040.09 92.790.22 92.550.54 83.280.34 83.660.67 Table 1: Downstream tasks results. Bold indicates best score overall while underline indicates best score between the ModernBERT models. ModernBERT-CV2 is the ModernBERT model trained on the same data as CamemBERTaV2 while ModernBERT-HQ is the one trained on the high-quality filtered dataset. Scores are the 5-seed average of the best performing set of hyperparamters for each model. CamemBERTaV2. We maintained the rate of dynamic token masking to 30%, while retaining all other hyperparameters consistent with those of ModernBERT-base. Training was done on 48 NVidia H100 80GB GPUs using Pytorchs FSDP full sharding with bfloat16 mixed precision to speed up training. Since our models were trained using Warmup-Stable-Decay (WSD) learning rate schedule, each intermediate checkpoint underwent additional cooldown training over an extra 50 billion tokens, during which the learning rate decayed fully to zero, ensuring fair comparisons across checkpoints. Additionally, we leveraged publicly available intermediate checkpoints from the CamemBERTaV2 and CamemBERTv2 models, allowing direct comparisons of learning trajectories and data efficiency across different architectures."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Downstream Evaluation Tasks To evaluate our models, we consider range of French downstream tasks and datasets, including: Question Answering (QA): using FQuAD 1.0 dHoffschmidt et al. (2020) Named Entity Recognition (NER): on the 2008 FTB version (Abeille et al., 2000; Candito & Crabbe, 2009) with NER annotations by Sagot et al. (Sagot et al., 2012) Text Classification capabilities assessed using the FLUE benchmark (Le et al., 2020) using the CLS amazon reviews classification task, the PAWS-X paraphrase identification task and XNLI task. We re-used the same splits from the CamemBERTaV2 authors and performed hyper-parameter tuning on all models and datasets with 5 seed variations. 4.2 Downstream Results Analysis The downstream evaluation results summarized in Table 1 show the following insights into model architectures and pretaining dataset effects: Impact. Comparing the models and CamemBERTaV2/CamemBERTv2), we Architecture trained on identical datasets (ModernBERT-CV2 that ModernBERT-CV2 consistently outperforms CamemBERTv2, thus showing ModernBERTs improvements over BERT/RoBERTa. However, it fails to surpass CamemBERTaV2 on any task, even though the latter being only trained for single epoch on the dataset compared to three epochs (1T tokens) for ModernBERT-CV2. This clearly demonstrates that while ModernBERT offers valuable throughput-driven architectural enhancements, these observe 4 Preprint. Under review. Figure 1: Downstream Performance on QA throughout the pre-training stage. wsd are the models tested before the cooldown period. improvements do not match the contextual learning capabilities provided by DeBERTaV3s disentangled attention and RTD-based pretraining objective. Data Quality Impact. Interestingly, switching to our high-quality filtered dataset (ModernBERT-HQ) only marginally improved performance on downstream tasks, despite the dataset containing three times more unique tokens than the original CamemBERTaV2 dataset. ModernBERT-HQ slightly outperformed ModernBERT-CV2 on QA (FQuad), CLS, and XNLI tasks, but improvements remained within small margins. This limited gain suggests two potential explanations: either current transformer architectures exhibit diminishing returns when exposed to additional data beyond certain threshold, or standard French benchmarks are becoming saturated and unfit to measure model quality with further improvements in model performance. The latter possibility stresses the need for more challenging and diverse benchmarks that can effectively capture the improvements brought by higher-quality data. 4.3 Pre-training Dynamics and Sample Efficiency We further explored the learning trajectories of the various models by evaluating intermediate checkpoints on QA (FQuad) and NER tasks. This analysis offers more detailed view of the training dynamics and sample efficiency: Architectural Efficiency. The training curves (shown in Figures 1 and 2) indicate that CamemBERTaV2 reaches higher performance significantly earlier in training compared to ModernBERT-CV2. The DeBERTaV3-based models faster improvement rate confirms its better sample efficiency due to optimizations like RTD and gradient-disentangled embedding sharing (GDES). Moreover, in scenarios where pre-training data is limited or scarce, its architectures might be more advantageous. Impact of Data Quality on Convergence. When comparing ModernBERT-CV2 and ModernBERT-HQ training curves, we observed that the model trained on the higher-quality dataset achieved its performance plateau faster, indicating that improved data quality enhances training efficiency and accelerates convergence. Yet, it does not substantially 5 Preprint. Under review. Figure 2: Downstream Performance on NER throughout the pre-training stage. wsd are the models tested before the cooldown period. increase the final task-specific performance scores, further confirming the hypothesis of saturation effects on standard NLP benchmarks. Task-specific Dynamics. The intermediate checkpoints downstream score shows clear difference in learning dynamics between the QA and NER tasks. While QA scores continued to improve gradually throughout training for all models, NER performance plateaued relatively early, with minimal further improvements, except for the CamemBERTaV2 NER scores which increased steadily. This difference suggests that the disentangled attention mechanism, which separately encodes content and positional embeddings, provides an advantage on token-level tasks such as NER. 4.4 Context Length Extension and Final Model Release One of ModernBERTs advantages is supporting longer context length due to its more efficient attention implementation and alternation of local and global attention layers. On the other hand, the older models had limited context length due to the high memory usage of their attention layer implementation. Hence, in order to study the effect of context length extension, we continue our ModernBERTs pretraining, as in the original models strategy, and increase its maximum input length to 8,192 tokens. This phase also includes cooldown stage, during which the learning rate is gradually reduced to zero over high-quality, long-context data. To support this phase, we curated two dataset variants: Long-Context Subset: We filtered documents longer than 2,048 tokens and retained them fully. Shorter documents were retained with 10% probability to preserve some distributional diversity. High-Quality Long-Context Subset: For this version, we upsampled high-quality, long-form sources such as French Wikipedia and academic literature, while only retaining documents rated as high quality by our semantic filter within the HQ dataset. We resumed training for both model variants, the one trained on the CamemBERTaV2 dataset and our High-Quality dataset, using their corresponding long-context subsets. 6 Preprint. Under review. MODEL NER F1 QA F1 EM CLS ACC ModernBERT-CV2 ModernBERT-HQ 92.030.14 91.800. 81.340.35 81.110.26 61.470.46 62.070.44 95.180.20 95.040.09 PAWS-X ACC 92.790.22 92.550. XNLI ACC 83.280.34 83.660.67 ModernBERT-CV2-final ModernBERT-HQ-final 92.170.48 91.330.27 81.680.46 82.190. 62.000.53 62.660.79 94.860.16 94.920.06 92.710.39 92.520.36 82.850.45 83.620.67 Table 2: Downstream tasks results after context extension and cooldown. , and indicate an increase, decrease or no change in scores after continual pretraining. Scores are , the 5-seed average of the best performing set of hyperparamters for each model. Training was done for an additional 150 billion tokens to extend context capabilities, using fixed learning rate of 3 104. This was followed by final 100 billion token cooldown phase, during which the learning rate was linearly decayed to zero. Impact on Downstream Performance. We observe in Table 2, that ModernBERT-CV2 provides modest gains in NER (+0.14 F1) and QA (+0.34 F1 / +0.53 EM), while performance slightly decreases on classification (CLS: 0.32 Acc, XNLI: 0.43 Acc), with PAWS-X remaining stable. Meanwhile, ModernBERT-HQ-final displays clear improvements in QA (+1.08 F1 / +0.59 EM) and CLS (+0.88 Acc), while maintaining stable results on PAWS-X and XNLI. Although NER performance slightly drops (0.47 F1), the overall trend indicates that high-quality long-context pretraining primarily benefits tasks requiring deeper semantic understanding or longer-range dependencies. 4.5 Downstream Training Stability During fine-tuning on downstream tasks, we observed differences in training stability between the newer and older model families. We had several cases where only ModernBERT variants failed to converge on the FQuAD question-answering task, as illustrated in Figure 3. Figure 3: Instances of divergence during QA fine-tuning. Colored lines illustrate the maximum score at given step. 7 Preprint. Under review. Furthermore, during hyperparameter tuning of the final checkpoint, we found the newer architecture to be particularly sensitive to learning rate choices. These findings suggest that, while ModernBERT offers advantages in speed and inference throughput, its fine-tuning stability may be more brittle in practice. 4.6 Training Efficiency In addition to model accuracy, training efficiency is crucial factor in practice, impacting both resource costs and environmental footprint. For pretraining time, our ModernBERT training required 1300 H100 GPU-hours to complete one trillion tokens. In contrast, CamemBERTv2 took roughly 2100 GPU-hours to train on the same dataset size while CamemBERTaV2 required around 2700 GPU-hours to complete just single epoch, despite processing one-third of the tokens (275B). This clearly demonstrates ModernBERTs efficient architecture advantages and its practical edge during training. However, it should be noted significant portion of the speedup over DeBERTaV3-based models comes from engineering optimizations such as unpadding and FlashAttention, both of which are not implemented in the DeBERTa models at the time of this study. The key takeaway from these experiments is the trade-off between ModernBERT, which offers significantly faster training and inference speeds, making it more efficient for time-sensitive applications, and DeBERTa, which delivers higher raw performance through its most effective use of training data."
        },
        {
            "title": "5 Conclusion",
            "content": "in this work to critically evaluate the claims made in the original We set out ModernBERT (Warner et al., 2024) paper by reproducing its setup under tightly controlled conditions. We isolated the authors contributions by retraining their model under the same conditions as the previous state-of-the-art models, to assess their actual impact on training dynamics, efficiency, and downstream performance. Our findings show that while ModernBERT does offer improvements in training and inference speed compared to older architectures, these do not translate into better sample efficiency or task performance under matched conditions. In fact, under careful evaluation, we found that DeBERTaV3s architecture and training objectives are more advantageous in low-data scenarios or if the goal is to get the absolute best task performance. We also observed that increasing the size and quality of pretraining data only yielded marginal gains for the newly proposed architecture, suggesting that current benchmarks may be reaching saturation, or at least they are insufficiently sensitive to capture finer improvements. During fine-tuning, we faced practical problem with sensitivity to hyperparameters, which the V2 baselines did not have. These stability concerns present challenges for reproducibility and deployment, and deserve further investigation. In summary, ModernBERT offers fast and efficient alternative for scenarios where training and inference speed are critical, but DeBERTaV3 remains the stronger choice when performance and sample efficiency are required. Our study reinforces the importance of evaluating models under shared conditions to truly understand the contributions of architecture, training data, and design choices."
        },
        {
            "title": "Ethics Statement",
            "content": "This work involves training large-scale language models using publicly available data, with special attention given to data quality, filtering, and documentation. We applied both heuristic and semantic filters to reduce harmful, biased, or low-quality content. Nonetheless, we acknowledge that pretrained models may still reflect societal biases present in the underlying data. We encourage responsible use of our models and welcome future research focused on auditing and mitigating bias and potential misuse. 8 Preprint. Under review."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partly funded by Benoˆıt Sagots and Djame Seddahs chairs at the PRAIRIE Institute, supported by the French National Research Agency (ANR) as part of the Investissements davenir program with reference ANR-19-P3IA-0001. The authors extend their gratitude to the OPAL infrastructure of Universite ˆote dAzur for providing essential resources and support. This work was also granted access to the HPC resources of IDRIS by GENCI under the allocation 2024-GC011015610. Special thanks to Nathan Godey for his assistance with training code and for engaging in productive discussions."
        },
        {
            "title": "References",
            "content": "Anne Abeille, Lionel Clement, and Alexandra Kinyon. Building treebank for French. In M. Gavrilidou, G. Carayannis, S. Markantonatou, S. Piperidis, and G. Stainhauer (eds.), Proceedings of the Second International Conference on Language Resources and Evaluation (LREC00), Athens, Greece, May 2000. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2000/pdf/230.pdf. Wissam Antoun, Francis Kulumba, Rian Touchent, Eric de la Clergerie, Benoˆıt Sagot, and Djame Seddah. Camembert 2.0: smarter french language model aged to perfection. arXiv preprint arXiv:2411.08868, 2024. Marie Candito and Benoˆıt Crabbe. Improving generative statistical parsing with semi-supervised word clustering. In Harry Bunt and Eric Villemonte de la Clergerie (eds.), Proceedings of the 11th International Conference on Parsing Technologies (IWPT09), pp. 138141, Paris, France, October 2009. Association for Computational Linguistics. URL https://aclanthology.org/W09-3821. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020. URL https://openreview.net/pdf?id=r1xMH1BtvB. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=mZn2Xyh9Ec. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology. org/N19-1423. Martin dHoffschmidt, Maxime Vidal, Wacim Belblidia, and Tom Brendle. FQuAD: French Question Answering Dataset. arXiv e-prints, art. arXiv:2002.06071, Feb 2020. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing, 2021a. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Decoding-enhanced bert with disentangled attention. Learning Representations, 2021b. URL https://openreview.net/forum?id=XPZIaotutsD. Deberta: In International Conference on Preprint. Under review. Francis Kulumba, Wissam Antoun, Guillaume Vimont, and Laurent Romary. Harvesting textual and structured data from the hal publication repository, 2024. URL https://arxiv. org/abs/2407.20595. Hang Le, Loıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabbe, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model pre-training for French. In Nicoletta Calzolari, Frederic Bechet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hel`ene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 24792490, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020. lrec-1.302. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich uttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. James Neill, Santhosh Subramanian, Eric Lin, Abishek Satish, and Vaikkunth Mugunthan. Guardformer: Guardrail instruction pretraining for efficient safeguarding. In Neurips Safe Generative AI Workshop 2024, 2024. URL https://openreview.net/forum?id=vr31i9pzQk. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023. Jacob Portes, Alexander Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: bidirectional encoder optimized for fast pretraining. Advances in Neural Information Processing Systems, 36: 31063130, 2023. Benoˆıt Sagot, Marion Richard, and Rosa Stern. Annotation referentielle du corpus arbore de Paris 7 en entites nommees (referential named entity annotation of the Paris 7 French TreeBank) [in French]. In Georges Antoniadis, Herve Blanchon, and Gilles Serasset (eds.), Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN, pp. 535542, Grenoble, France, June 2012. ATALA/AFCP. URL https://aclanthology.org/F12-2050. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002. 05202. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Benjamin Warner, Antoine Chaffin, Benjamin Clavie, Orion Weller, Oskar Hallstr om, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, et al. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663, 2024. Preprint. Under review. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Re, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024."
        }
    ],
    "affiliations": [
        "Inria, Paris, France"
    ]
}