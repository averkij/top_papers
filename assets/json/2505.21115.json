{
    "paper_title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA",
    "authors": [
        "Sergey Pletenev",
        "Maria Marina",
        "Nikolay Ivanov",
        "Daria Galimzianova",
        "Nikita Krayko",
        "Mikhail Salnikov",
        "Vasily Konovalov",
        "Alexander Panchenko",
        "Viktor Moskvoretskii"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. A key yet underexplored factor contributing to this is the temporality of questions -- whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, a lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4o retrieval behavior."
        },
        {
            "title": "Start",
            "content": "Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA Sergey Pletenev*,2,1, Maria Marina*, 2,1, Nikolay Ivanov1, Daria Galimzianova4, Nikita Krayko4, Mikhail Salnikov2,1, Vasily Konovalov2,5, Alexander Panchenko1,2, Viktor Moskvoretskii1,3 1Skoltech, 2AIRI, 3HSE University, 4MTS AI, 5MIPT {Maria.Marina, A.Panchenko, V.Moskvoretskii}@skol.tech 5 2 0 2 7 2 ] . [ 1 5 1 1 1 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) often hallucinate in question answering (QA) tasks. key yet underexplored factor contributing to this is the temporality of questions whether they are evergreen (answers remain stable over time) or mutable (answers change). In this work, we introduce EverGreenQA, the first multilingual QA dataset with evergreen labels, supporting both evaluation and training. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they encode question temporality explicitly (via verbalized judgments) or implicitly (via uncertainty signals). We also train EG-E5, lightweight multilingual classifier that achieves SoTA performance on this task. Finally, we demonstrate the practical utility of evergreen classification across three applications: improving self-knowledge estimation, filtering QA datasets, and explaining GPT-4os retrieval behavior."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) often struggle with question answering (QA) due to hallucinated answers (Huang et al., 2025). To improve trustworthiness, recent research has focused on estimating LLMs self-knowledge their ability to recognize what they do and do not know (Yin et al., 2023; Moskvoretskii et al., 2025) and on integrating up-to-date external information through RetrievalAugmented Generation (RAG) (Su et al., 2024; Jeong et al., 2024; Trivedi et al., 2023). particularly important but underexplored factor affecting question difficulty is whether question is evergreen or mutable (Wei et al., 2024a) that is, whether its correct answer remains stable over time, depicted with illustrative Figure 1. Mutable questions are especially challenging because they often require access to up-to-date information, 1* Equal contribution. Figure 1: Some questions have answers that stay the same (evergreen), like facts of nature. Others have answers that change over time and will change in the future (mutable), like global trends or statistics. which may be missing from models fixed, parametric knowledge. Despite its practical importance, evergreen-ness remains an underexplored factor in evaluating and improving LLM behavior. Most existing studies are limited to small-scale, English-only datasets and focus primarily on QA accuracy, rarely examining its broader implications (Vu et al., 2024; Cheng et al., 2024). As result, the role of question evergreen-ness in shaping LLM reliability and interpretability remains largely unexamined. To address this gap, we conduct comprehensive study of question evergreen-ness and its practical applications. We introduce EverGreenQA - the first multilingual human-curated evergreenaware QA dataset, which includes traintest split suitable for model training. Using EverGreenQA, we evaluate 12 modern LLMs to determine whether they encode temporal knowledge explicitly (through direct prompting) or implicitly (via uncertainty-based signals). Further, we develop EG-E5 lightweight SoTA classifier trained to identify evergreen questions. We demonstrate the usefulness of EG-E5 in EverGreenQA (our work) TimeQA (Chen et al., 2021) Both EG and mutable questions Train-Test split Human-Evaluated Multilinguality MuLan FreshQA (Fierro et al., 2024) (Vu et al., 2024) (Zhao et al., 2024) TAQA Overall size 4757 40k 246k 600 20k Table 1: Comparison of EverGreenQA to other time-sensitive datasets. several downstream tasks: (1) improving selfknowledge estimation, (2) curating QA datasets to support fairer evaluation, and (3) effectively explaining GPT-4os black-box retrieval behavior. Our contributions and findings are as follows: 1. We construct EverGreenQA the first multilingual dataset for question evergreen-ness classification, covering 7 languages with 4,757 examples in total. 2. We conduct the first comprehensive evaluation of question evergreen knowledge in LLMs, assessing 12 models using both explicit signals (via prompting) and implicit signals (via uncertainty estimation). for 3. We develop EG-E5 multilingual identifying evlightweight classifier ergreen questions, which serves as SoTA approach evergreen-ness question classification while remaining suitable for low-compute settings. for (1) 4. We demonstrate the utility of EG-E5 across improving selfthree applications: (2) curating QA knowledge estimation, datasets for fairer evaluation, and (3) effectively explaining GPT-4os retrieval behavior. We release the model and data for further usage."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning about time remains fundamental challenge in question answering (QA) tasks, as temporal dynamics often complicate both the interpretation of questions and the retrieval of accurate answers. Working with time in QA tasks has been improved thanks to datasets like TimeQA (Chen et al., 2021), which has 20,000 question-answer pairs that need thinking about time. While helpful, it only addressed simple reasoning. SituatedQA (Zhang and Choi, 2021) showed the importance of context 1https://huggingface.co/collections/s-nlp/ evergreen-683465909575cb89d6b904fe by situating questions in time and place. StreamingQA highlighted the need for temporal adaptation, revealing LLMs difficulty tracking changing facts (Liska et al., 2022). TemporalAlignmentQA (TAQA) (Zhao et al., 2024) further enhances possibility for temporal alignment by providing 20K time-sensitive questions and their answers for each year from 2000 to 2023. MuLan (Fierro et al., 2024) differentiated questions by change rate and fact type, respectively. Most recently, FreshQA (Vu et al., 2024) introduced benchmark focused on freshness-sensitive information, further illustrating LLMs limitations in handling temporally dynamic knowledge. These studies indicate need for specialized temporal reasoning (Fierro et al., 2024). The comparison of datasets is presented in Table 1. Retrieval-Augmented Generation (RAG), such as DRAGIN (Su et al., 2024), IRCoT (Trivedi et al., 2023) or Rowen (Ding et al., 2024) was used to solve the problem of time sensitive QA, addressed this through dynamic retrieval decisions, but showed limited results. Dynamic retrieval decisions required selfknowledge estimation. In other words, before QA systems can be trusted, they need to know what they dont know. Often, LLMs struggle to identify questions they cant answer (Yin et al., 2023), but using self-knowledge can reduce mistakes in tasks that need lot of knowledge (Wang et al., 2023; Moskvoretskii et al., 2025). While retrieval-based methods address temporal knowledge gaps externally, another direction is to update the internal knowledge of LLMs. Updating internal knowledge in LLMs is computationally expensive, as retraining or editing models often requires substantial resources and cannot be performed daily or hourly in practice. Techniques like LLM Surgery (Veldanda et al., 2024) and parameter-efficient fine-tuning (Ge et al., 2024; Pletenev et al., 2025) have attempted to make such updates more practical, but still face issues with large-scale changes or factual hallucinations. Model Russian English French German Hebrew Arabic Chinese AVG LLaMA 3.1-8B-it LLaMA 3.1-70B-it Qwen 2.5 7B-it Qwen 2.5 32B-it Qwen 2.5 72B-it Phi-3 medium 4k-it Phi-3 medium 128k-it Gemma 2-9B-it Gemma 2-27B-it Mistral 7B-it-v0.3 Mistral Small-24B-it-2501 GPT-4.1 UAR (Cheng et al., 2024) MULAN (Fierro et al., 2024) EG-E5 (our) 0.677 0.889 0.782 0.882 0.806 0.556 0.415 0.755 0.830 0.736 0.827 0.806 0.550 0.340 0.910 0.699 0.879 0.789 0.885 0.815 0.577 0.489 0.728 0.878 0.722 0.739 0.794 0.500 0.345 0.913 0.686 0.895 0.786 0.875 0.802 0.498 0.342 0.694 0.836 0.726 0.768 0.816 0.510 0.442 0. 0.677 0.87 0.794 0.883 0.805 0.473 0.335 0.723 0.827 0.729 0.789 0.813 0.600 0.379 0.910 0.667 0.874 0.692 0.862 0.781 0.499 0.385 0.740 0.838 0.670 0.847 0.803 0.670 0.322 0.904 0.659 0.829 0.711 0.862 0.758 0.498 0.304 0.711 0.831 0.666 0.834 0.811 0.710 0.220 0. 0.652 0.873 0.774 0.872 0.768 0.420 0.289 0.746 0.826 0.731 0.839 0.809 0.710 0.279 0.897 0.674 0.875 0.761 0.874 0.791 0.503 0.366 0.728 0.838 0.711 0.806 0.807 0.490 0.340 0.906 Table 2: Comparison of verbalized LLM predictions and our trained classifier on the test part of evergreen classification task. Reported scores are weighted F1. random baseline achieves 0.637. LLMs were prompted with 10-shot examples. The best scores are shown in bold."
        },
        {
            "title": "3 EverGreenQA & EG-E5",
            "content": "Dataset Collection. We construct QA dataset consisting of real user queries sourced from an AI chat assistant, each labeled as either evergreen or mutable, along with corresponding golden answers. All questions are factual in nature and were manually validated over multiple iterations of internal alpha testing to ensure diversity and reduce topic bias. The labels and golden answers were assigned by team of trained linguists, who manually wrote the answers from scratch based on retrieved information. Due to the fact that in the initial dataset most of the questions were mutable and to avoid bias in the training data, we also generated 1,449 synthetic data for the evergreen class only. This additional dataset was similarly validated by linguists. The final dataset contains 4,757 questions, with 3,487 used for training and 1,270 reserved for testing, details of dataset collection and labeling are presented in Appendix F. Dataset Translation. We perform translations from Russian to English and from English to the target languages using GPT-4.1, following prior work that demonstrated its strong performance across wide range of languages, including accurate handling of cultural nuances (Vayani et al., 2024). The full translation prompt is provided in Appendix B. Dataset Validation. To assess translation quality, we recruited human evaluators for each target language, all of whom are either native speakers or possess advanced proficiency (B2C1 level). We randomly sampled 100 questions from the test set (50 mutable, 50 evergreen) for evaluation. No errors were found in the translations for English, Hebrew, German, or Arabic, while Chinese exhibited only two minor inaccuracies. Validation assessor instruction is provided in Appendix C. EG-E5 Training. For training and testing, we used our multilingual dataset. For validation, we employed the dev and test splits from FreshQA (Vu et al., 2024), merging the fast-changing and slow-changing classes into mutable label. To align with our multilingual setting, the FreshQA data was translated into all target languages. We experimented with multilingual versions of BERT (Devlin et al., 2019), DeBERTaV3 (He et al., 2023), and E5 (Wang et al., 2024) as encoders. The best performance was achieved using the E5Large model, which we refer to as our classifier EverGreen-E5 (EG-E5). Hyperparameter details and ablation results are provided in Appendix A."
        },
        {
            "title": "4 Are LLMs Aware of Evergreenness?",
            "content": "In this section, we evaluate whether modern LLMs can reliably identify whether given question is evergreen. We test 12 LLMs spanning diverse architectures, with full details provided in Appendix A. 4.1 Verbalized Evergreen Awareness To assess whether LLMs are capable of explicitly recognizing evergreen questions, we prompt each model to provide binary Yes/No answer. We additionally include two specifically trained methods: UAR (Cheng et al., 2024): previously proposed LLaMA2-13b fine-tuned to classify evergreen questions, and MULAN (Fierro et al., 2024) classification based on mutable and evergreen samples from Wikidata. Results. Table 2 shows that our proposed classifier, EG-E5, achieves the highest performance across all languages, significantly outperforming both general-purpose and specifically trained LLMs. Among the LLMs, LLaMA 3.1 70B and Qwen 2.5 32B are the strongest, with GPT-4.1 lagging bit behind. We observe some variations in language performance, but no clear performance gap, even for nonLatin languages (e.g., Arabic, Chinese, Russian). Baseline methods UAR and MULAN perform substantially worse than both LLMs and EG-E5, likely due to their oversimplified assumptions regarding the evergreen nature of QA datasets. (cid:10) Takeaway EG-E5 outperforms few-shot LLMs and prior methods, whose weaker results stem from unrealistic assumptions in their training data. 4.2 Internal Evergreen Awareness We next assess whether LLMs implicitly encode information about question evergreen-ness through their uncertainty estimates using balanced subset of sampled 400 questions from our test set 200 labeled as evergreen and 200 as mutable. We select two widely adopted uncertainty measures that show strong performance (Vashurin et al., 2024; Moskvoretskii et al., 2025). Perplexity the inverse probability of the predicted sequence, normalized by its length. For sequence of tokens x1, . . . , xT , it is defined as: (cid:32) PPL = exp 1 T (cid:88) t=1 (cid:33) log p(xt x<t) Mean Token Entropy the average entropy of the models predicted token distribution at each position: Entropy = 1 (cid:88) (cid:88) t=1 wV pt(w) log pt(w) where pt(w) is the predicted probability of token at position t, and is the vocabulary. Results. Table 3 shows that most models exhibit only mild correlations between uncertainty and evergreen-ness, with Mistral 7B and Qwen 2.5 32B achieving the strongest signals. Model Perplexity Mean Token Entropy Gemma 2-9B-it Gemma 2-27B-it LLaMA 3.1-8B-it LLaMA 3.1-70B-it Mistral 7B-it-v0.3 Mistral Small-24B-it-2501 Phi-3 medium 4k-it Phi-3 medium 128k-it Qwen 2.5 7B-it Qwen 2.5 32B-it Qwen 2.5 72B-it 0.23 0.26 0.33 0.20 0.33 0.34 0.23 0.27 0.25 0.33 0. 0.27 0.29 0.33 0.21 0.35 0.32 0.17 0.32 0.25 0.34 0.31 Table 3: Correlation of golden EG with UC. All results are significant (p-value < 0.05). We also observe weak trend suggesting that larger models correlate more strongly with evergreen-ness, possibly indicating greater internal reliance on temporal cues. Neither perplexity nor entropy consistently outperforms the other. Overall, uncertainty signals capture some temporal information, but are noticeably weaker than explicit verbalized judgments. Additional analysis is provided in Appendix E. (cid:10) Takeaway Uncertainty metrics encode weak and inconsistent signals of evergreen-ness, with slightly stronger trends in larger models."
        },
        {
            "title": "5 Enhancing Self-Knowledge",
            "content": "In this section, we evaluate whether incorporating knowledge about question evergreen-ness improves the estimation of self-knowledge models ability to recognize the boundaries of its own knowledge and determine when it can or cannot answer given question (Moskvoretskii et al., 2025; Yin et al., 2023). This capability is considered key factor in improving the trustworthiness of LLMs. 5.1 Task Formulation We frame self-knowledge estimation as binary classification task, where the target label {0, 1} reflects whether the models answer to given input is factually correct. Each method under evaluation assigns real-valued self-knowledge score (x) to the input. 5.2 Methods We evaluate this setup using LLaMA3.1-8BInstruct with five widely adopted and highperforming uncertainty estimators, selected to Method AUROC AUPRC PRR AUROC AUPRC PRR AUROC AUPRC PRR AUROC AUPRC PRR AUROC AUPRC PRR AUROC AUPRC PRR NQ SQuAD TriviaQA 2WikiMultihopQA HotpotQA MuSiQue EigValLaplacian LexicalSimilarity MaxTokenEnt. MeanTokenEnt. SAR 0.56 0.61 0.61 0.59 0.61 EigValLaplacian+EG 0.56 LexicalSimilarity+EG 0.59 0.56 MaxTokenEnt.+EG MeanTokenEnt.+EG 0.59 0.58 SAR+EG 0.46 0.38 0.37 0.42 0.39 0.40 0.40 0.42 0.39 0. 0.56 0.59 0.60 0.57 0.59 0.57 0.59 0.59 0.59 0.57 0.48 0.64 0.58 0.56 0.67 0.49 0.65 0.68 0.70 0.70 Uncertainty Estimation 0.19 0.13 0.18 0.19 0. 0.19 0.13 0.12 0.12 0.12 0.77 0.83 0.80 0.80 0.84 0.70 0.65 0.70 0.71 0.72 0.52 0.54 0.51 0.50 0.51 0.58 0.58 0.62 0.63 0.60 0.61 0.55 0.59 0.61 0. Uncertainty Estimation + Evergreen 0.77 0.83 0.85 0.86 0.85 0.70 0.68 0.71 0.72 0.66 0.51 0.52 0.51 0.50 0.54 0.56 0.63 0.63 0.62 0.46 0.54 0.61 0.63 0.61 0. Evergreen 0.26 0.29 0.27 0.28 0.27 0.52 0.26 0.25 0.26 0.43 0.71 0.67 0.69 0.70 0.69 0.64 0.71 0.72 0.71 0.68 0.64 0.68 0.67 0.62 0. 0.65 0.68 0.67 0.63 0.70 0.22 0.21 0.21 0.23 0.21 0.21 0.21 0.21 0.22 0.21 0.75 0.77 0.75 0.73 0.78 0.75 0.76 0.75 0.75 0.78 0.57 0.58 0.64 0.63 0. 0.50 0.61 0.55 0.64 0.67 0.09 0.09 0.09 0.09 0.08 0.12 0.10 0.11 0.08 0.11 0.86 0.85 0.84 0.81 0.83 0.84 0.86 0.84 0.85 0.87 EG 0.50 0.72 0.52 0.52 0.20 0. 0.47 0.65 0.62 0.49 0.31 0. 0.51 0.28 0.68 0.50 0.10 0. Table 4: Self-knowledge identification performance. We report classification quality using AUROC and AUPRC, and calibration efficiency using PRR. EG stand for Evergreen probability. Higher values indicate better performance. The best scores for each metric are shown in bold. represent different families of uncertainty quantification methods including logit-based and consistency-based approaches: Max Token Entropy: Evaluates uncertainty by computing token-level entropies and taking the maximum value across the sequence as the final score (Fomicheva et al., 2020). Mean Token Entropy: Similar to the above, but aggregates across the sequence by averaging tokenlevel entropy values (Fomicheva et al., 2020). Lexical Similarity: Estimates uncertainty by calculating the average lexical overlap among multiple model responses, serving as proxy for output consistency (Fomicheva et al., 2020). SAR: Combines entropy with relevance weighting by amplifying the contribution of semantically important tokens, summing the adjusted entropy values over the sequence (Duan et al., 2023). EigValLaplacian: Constructs similarity graph over sampled responses and computes the sum of eigenvalues of its Laplacian matrix to quantify response diversity (Lin et al., 2023). For each method, we evaluate the effect of incorporating the predicted probability of question being evergreen, obtained from our trained evergreen classifier. To obtain the final self-knowledge classifier (x), we train standard machine learning model on the training set, using the uncertainty estimation metrics as input features. When applicable, we also include the predicted evergreen probability as an additional feature. The full training procedure is detailed in Appendix D. 5.3 Evaluation We evaluate performance using standard metrics widely adopted in recent literature on uncertainty estimation (Fadeeva et al., 2024; Vashurin et al., 2024; Vazhentsev et al., 2025). AUROC measures how well the model distinguishes between correct and incorrect answers based on the self-knowledge score (x). Higher values indicate stronger separability. AUPRC quantifies the trade-off between precision and recall across different decision thresholds. It is particularly informative when dealing with imbalanced datasets. Prediction Rejection Ratio (PRR) measures how well uncertainty scores align with answer quality. It simulates rejecting the most uncertain responses and tracks how average quality improves. Higher PRR indicates better calibration between uncertainty and actual answer correctness. We use In-Accuracy as main QA metric. 5.4 Datasets We evaluate our methods on 6 QA datasets covering both single-hop and multi-hop reasoning. The single-hop datasets include SQuAD v1.1 (Rajpurkar et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017), while the multi-hop datasets include MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020). Following Trivedi et al. (2023); Jeong et al. (2024), we use subset of 500 questions from the original splits of each dataset to ensure consistency and comparability. Dataset Dataset release Non-EG question Expected changes (routine, scheduled) NQ MuSiQue SQuAD HotpotQA MuSiQue what city is the next winter olympics in 2015 2022 Who is the mayor presiding now where Merrill Elam was born? 2020 2018 How many teams are in the Greek Super League? Yau Ma Tei North is district of city with how many citizens? According to QS World University Rankings, where does the college that Ibrahim Shihata attended rank? 2022 Occasional changes (updates over time, but less regular) 2018 Edoardo Soleri is playing on loan from which Italian football club? HotpotQA 2WikiMultihopQA 2020 Where does Karin Stoltenbergs husband work at? 2WikiMultihopQA 2020 Who is the spouse of the performer of song Les Rois Du Monde? TriviaQA NQ 2017 What is the name of the current Attorney General for England and Wales? 2015 Reference answer Answer in 2025 Beijing Lance Bottoms 18 7.2 million Milan Andre Dickens 14 7.4 million 551-600 350 A.S. Roma United Nations Joy Esther Dominick Grieve Richard Hermer Spezia He has died Emily Surde who is the current minister for environment forest and climate change in india Dr. Harsh Vardhan Bhupender Yadav SQuAD TriviaQA 2020 What is the largest economy in Africa? 2017 Who is fifth in line to the throne? Nigeria South Africa Princess Beatrice Prince Harry Less predictable changes (complex sociopolitical shifts) Table 5: Examples of non-evergreen questions from popular QA datasets, showing discrepancies between original gold answers and updated answers in 2025. Questions are categorized by the nature of the change: expected, occasional, and less predictable. 5.5 Results"
        },
        {
            "title": "6 Filtering QA with Evergreen",
            "content": "As shown in Table 4, evergreen probability is strong signal for improving self-knowledge identification. In 16 out of 18 evaluations, the best results are achieved either by the evergreen feature alone or by combining it with an uncertainty estimation method. Moreover, it is able to improve calibration (PRR), that depends on QA accuracy, making it highly valuable for real-world applications. Notably, the evergreen feature alone performs exceptionally well on AUPRC, achieving the top score on 4 datasets. This suggests that evergreenness is powerful indicator of when model possesses reliable knowledge. However, we also observe consistent pattern: evergreen scores high on AUPRC but relatively low on AUROC. This indicates that while the feature is highly effective at identifying when the model knows the answer, it is less reliable at recognizing when the model does not (weaker true negative discrimination). In other words, if question is evergreen, the model is likely to answer it correctly but if question is not evergreen, the outcome is harder to predict. (cid:10) Takeaway Evergreen probability consistently improves self-knowledge estimation and calibration, achieving top results in 16 out of 18 settings. In this section, we demonstrate that evergreen classification is valuable for filtering QA datasets, enabling fairer evaluation by excluding mutable questions. We use the same model setting as in SelfKnowledge Section 5. QA datasets should ideally consist only of evergreen questions, emphasized in SimpleQA (Wei et al., 2024b). To achieve this, SimpleQA relied on human annotators to assess evergreen-ness. In contrast, EG-E5 enables automated dataset curation, eliminating the need for manual annotation and facilitating the scalable construction of large QA corpora. 6.1 Popular QA Datasets Analysis Mutable questions pose serious challenge for fair QA evaluation: outdated gold answers can make correct responses from modern LLMs appear wrong, especially when models are evaluated at different times. Examples. Table 5 highlights such mutable examples across six datasets (Section 5.4), showing answers that, as of 2025, diverge from the original references. These include both simple and complex queries-even from recently released datasets like MuSiQue (Trivedi et al., 2022). The nature of change varies: some are predictable (e.g., Olympic host cities, population figures), some occasional (e.g., job titles or spouses), and others unexpected (e.g., monarchs, GDP rankings). Dataset 0-Shot RAG EG Mut EG Mut EG-Mut Mut RAG Gain % 0-shot % Mut % 0.399 NQ 0.661 TriviaQA 0.171 SQuAD 0.367 HotpotQA MuSiQue 0.113 2wikiMultihopQA 0.448 0.344 0.581 0.168 0.282 0.080 0.342 0.660 0.749 0.627 0.746 0.278 0.644 0.635 0.682 0.598 0.727 0.315 0.457 16 14 2 30 41 10 13 -6 14 30 -70 18 6 12 10 17 0.1 Table 6: Performance comparison between evergreen (EG) and mutable (Mut) questions under 0-shot (no context) and RAG (with context) settings. We report absolute in-accuracies, the relative gap between evergreen and mutable questions ( EGMutable) under 0-shot, and the relative RAG gain on mutable questions. higher mutable gain indicates RAG is more beneficial for time-sensitive queries. The last column shows the proportion of mutable questions in each dataset. Gray row indicates limited applicability due to extremely low mutable sample count. Statistics. Table 6 shows that mutable questions remain common, reaching 18% in NQ and averaging 10% across datasets. This challenges the widespread assumption that QA benchmarks are temporally stable, and raises concerns about evaluation fairness. To ensure reliability, mutable questions should be filtered out, or alternatively, live benchmarks like RealTimeQA (Kasai et al., 2024) should be maintained-though they are costly to sustain. Incorrect Assumptions. UAR (Cheng et al., 2024) has implicitly assumed dataset evergreenness and MULAN (Fierro et al., 2024) treat many questions as immutable, yet some relations (e.g., Wikidatas P190, sister cities) can in fact change. This mismatch may help explain the limited realworld effectiveness of such methods when faced with temporal drift. (cid:10) Takeaway QA benchmarks include mutable questions, undermining fair evaluation. Filtering for evergreen questions is essential for reliable assessment. 6.2 Filtered QA Performance Zero-Shot Performance. As shown in Table 6, model accuracy is consistently higher on evergreen questions, with relative differences reaching up to 40% on complex tasks. This aligns with expectations, as mutable questions often require up-to-date information beyond the models static knowledge. RAG Benefits. We show that models generally benefit more from RAG with gold contexts when answering mutable questions, with relative gains reaching up to 30%. However, this effect diminishes in datasets with few mutable examples. Model ChatGPT Gemma 2-9B-it Gemma 2-27B-it LLaMA 3.1-8B-it LLaMA 3.1-70B-it Mistral 7B-it-v0.3 Mistral Small-24B-it-2501 Phi-3 medium 4k-it Phi-3 medium 128k-it Qwen 2.5 7B-it Qwen 2.5 32B-it Qwen 2.5 72B-it EG-E5 EverGreen 0.26 0.30 0.29 0.25 0.34 0.33 0.20 0.29 0.28 0.36 0.35 0.66 0.77 Table 7: Correlation of ChatGPT with UC and EG. All results are significant (p-value < 0.05). EverGreen denotes ground true labels in the selected dataset part."
        },
        {
            "title": "7 Explaining GPT-4o Retrieval",
            "content": "GPT-4o autonomously decides when to invoke its retrieval system using internal, black-box criteria. We find that question evergreen-ness is the strongest predictor of this behavior, suggesting that GPT-4os use of external search is closely linked to the temporal nature of the input. We use the same subset, as in Section 4.2 and queried GPT-4o via its web interface, recording whether it triggered retrieval call. In addition to evergreen labels, we evaluated several uncertainty-based signals from Section 4.2 and EG-E5 to assess their correlation with GPT4os retrieval decisions. As shown in Table 7, evergreen-ness and EGE5 predictions are substantially stronger predictors than any uncertainty-based signal more than twice as informative. This suggests that GPT-4o may internally model question temporality or is guided by retrieval policy highly sensitive to it. Misclassification reason Example questions False Positives (non-evergreen, but classified as evergreen) Superlatives assumed to be static facts Biographical/life data on alive people treated as static What is the biggest star in the sky? Which tea is the healthiest? What is the most popular social network in the world? In which movies has Simu Liu acted? How many works has Stephen King written? False Negatives (evergreen, but classified as non-evergreen) Superlatives treated as time-sensitive or trend-based Biological and geographical facts wrongly assumed to change frequently What is the oldest currency? The rarest element in the periodic table. How long did the shortest war in history last? What is the area of Liechtenstein? Which animal has the highest blood pressure? How many species of elephants currently live on the planet? Table 8: Error Analysis of EG-E5 Classifier: breakdown of misclassification patterns. (cid:10) Takeaway Evergreen-ness is the strongest predictor of GPT-4os retrieval behavior, suggesting that retrieval is closely tied to temporality."
        },
        {
            "title": "8 Error Analysis",
            "content": "We selected test part from our EverGreenQA dataset and conducted qualitative analysis of the errors made by the EG-E5 classifier. Table 8 presents examples of false positives and false negatives, grouped by cause. Notably, the classifier shows high uncertainty with superlatives sometimes flagging them as volatile, and other times misinterpreting trend-sensitive phrases like most, biggest, or healthiest as universally fixed. Other errors include misclassifying achievements of living people as dead and incorrectly treating stable geographical or biological facts as time-sensitive. Interestingly, there are twice as many false negatives as false positives. This suggests that the classifier is more cautious when deciding whether question refers to stable fact. In some cases, external information is crucial. For example, if person is dead, all questions about them would be evergreen, but the model needs to know whether the person is still alive. Similarly, questions about recent years (e.g., 20232024) pose challenge, as the model lacks awareness of the current date. In other cases, there is room for improvement in how the model organizes and distinguishes its knowledge. For instance, learning to differentiate between truly stable physical facts (such as the area of Liechtenstein) and more variable ones (like the brightest star in the sky), or between completed historical events (e.g., the French Revolution) and ongoing developments (such as upcoming presidential elections). Additional examples are provided in Appendix G."
        },
        {
            "title": "Conclusion",
            "content": "In this study, we explored the concept of evergreenness, whether the answer change over time. We examined the ability of LLMs to detect it and demonstrated its usefulness across several applications. To support this investigation, we introduce EverGreenQA, new multilingual dataset comprising 4,757 examples across 7 languages. Using this dataset, we benchmark modern LLMs on the task of evergreen question classification and train EGE5 lightweight classifier that outperforms both LLMs and previously trained methods. We further analyze whether LLMs implicitly encode evergreen-ness through their uncertainty estimations and find that they do it little with larger model doing it more. We further enhance existing uncertainty estimators with predicted evergreen probabilities, yielding consistent improvements. We also show that our evergreen classifier helps curate high-quality QA datasets and supports more reliable and fair evaluations. Finally, we demonstrate that evergreenness is the best predictor of GPT-4os search behavior, outperforming all other tested factors."
        },
        {
            "title": "Limitations",
            "content": "While our EverGreenQA dataset is the first multilingual, human-curated benchmark for question temporality, its size remains relatively modest (3,278 examples). Nonetheless, it offers high-quality coverage across seven diverse languages and is sufficient to reveal clear trends in model behavior. Although we cover 7 languages, the dataset does not span all major language families, and performance in truly low-resource settings remains unexplored. That said, our selection includes both Latin and non-Latin scripts, enabling meaningful multilingual evaluation. Our LLM evaluation includes 14 models across wide range of scales and families, but we primarily focus on representative models from each size tier. Extending to more instruction-tuned or domain-adapted variants could further generalize the findings. For uncertainty-based analysis, we focus on five representative metrics. While these are widely used and sufficient to draw strong conclusions, incorporating more recent or taskspecific metrics may provide additional insights. Our trained evergreen classifier demonstrates strong results, but we perform only limited ablations on its architecture, training procedure, and the use of auxiliary data. Exploring more model variants or transfer learning strategies could further improve robustness. Finally, while we demonstrate several practical uses of evergreen classification, we do not explore its potential in tasks such as active learning, answer calibration, or search reranking. We leave these promising directions for future work."
        },
        {
            "title": "Ethical Considerations",
            "content": "Our work involves the construction and analysis of multilingual QA dataset, as well as the evaluation of LLM and classifier-based approaches for detecting question temporality. We made great effort to take into account following ethical considerations and discuss them to prevent misusage: All questions in the constructed dataset were sourced from anonymized real-user queries during internal alpha testing. No personally identifiable information (PII) was collected, stored, or used. All examples are factual in nature and were manually reviewed to ensure compliance with privacy and ethical standards. Dataset labels and translations were created by trained linguists and multilingual annotators. Annotators were compensated fairly according to local labor regulations. We ensured that the task complexity was reasonable and the working conditions were ethical. The lightweight classifier and dataset are intended to support research in trustworthy QA and dataset curation. We caution against deploying these tools in high-stakes applications without rigorous domain-specific validation. Although evergreen classification can help flag outdated or unstable information, it should not be viewed as substitute for fact verification or timeliness. We explicitly discourage the use of our tools for censorship or exclusion of mutable information inappropriately. We believe this work contributes to more transparent and interpretable QA systems by introducing temporality as an explicit factor, while taking steps to ensure fairness, privacy, and responsible development."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux. 2013. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108122. Wenhu Chen, Xinyi Wang, and William Yang Wang. 2021. dataset for answering time-sensitive questions. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified active retrieval for retrieval augmented generation. Preprint, arXiv:2406.12534. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Preprint, arXiv:1810.04805. Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models. CoRR, abs/2402.10612. Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang, Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2023. Shifting attention to relevance: Towards the uncertainty estimation of large language models. arXiv preprint arXiv:2307.01379. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, and 1 others. 2024. Fact-checking the output of large language models via token-level uncertainty quantification. arXiv preprint arXiv:2403.04696. Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, and Anders Søgaard. 2024. Mulan: study of fact mutability In Proceedings of the 2024 in language models. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Short Papers, NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 762771. Association for Computational Linguistics. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Xiou Ge, Ali Mousavi, Edouard Grave, Armand Joulin, Kun Qian, Benjamin Han, Mostafa Arefiyan, and Yunyao Li. 2024. Time sensitive knowledge editing through efficient finetuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 583593, Bangkok, Thailand. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. John Hancock and Taghi Khoshgoftaar. 2020. Catboost for big data: an interdisciplinary review. Journal of big data, 7(1):94. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. Preprint, arXiv:2111.09543. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 66096625. International Committee on Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 70367050. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 16011611. Association for Computational Linguistics. Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2024. Realtime qa: Whats the answer right now? Preprint, arXiv:2207.13332. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452 466. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023. Generating with confidence: Uncertainty quantification for black-box large language models. arXiv preprint arXiv:2305.19187. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539554. Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien De Masson DAutume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-Mcmahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. 2022. StreamingQA: benchmark for adaptation to new knowledge over time in question answering models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1360413622. PMLR. Viktor Moskvoretskii, Maria Lysyuk, Mikhail Salnikov, Nikolay Ivanov, Sergey Pletenev, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Irina Nikishina, and Alexander Panchenko. 2025. Adaptive retrieval without self-knowledge? bringing uncertainty back home. arXiv preprint arXiv:2501.12835. Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, and Mikhail Salnikov. 2025. How much knowledge can you pack into LoRA adapter without harming LLM? In Findings of the Association for Computational Linguistics: NAACL 2025, pages 43094322, Albuquerque, New Mexico. Association for Computational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 23832392. The Association for Computational Linguistics. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: dynamic retrieval augmented generation based on the real-time information needs of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1299113013. Association for Computational Linguistics. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, and 179 others. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Qwen Team. 2024. Qwen2.5: party of foundation models. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1001410037. Association for Computational Linguistics. Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Daniil Vasilev, Akim Tsvigun, Sergey Petrakov, Rui Xing, Abdelrahman Sadallah, Kirill Grishchenkov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, and Artem Shelmanov. 2024. Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Transactions of the Association for Computational Linguistics, 13:220248. Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, and 1 others. 2024. All languages matter: Evaluating lmms on culturally diverse 100 languages. arXiv preprint arXiv:2411.16508. Artem Vazhentsev, Lyudmila Rvanova, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2025. Token-level density-based uncertainty quantification methods for eliciting truthfulness of large language models. arXiv preprint arXiv:2502.14427. Akshaj Kumar Veldanda, Shi-Xiong Zhang, Anirban Das, Supriyo Chakraborty, Stephen Rawls, Sambit Sahu, and Milind Naphade. 2024. Llm surgery: Efficient knowledge unlearning and editing in large language models. arXiv e-prints, pages arXiv2409. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W. Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc V. Le, and Thang Luong. 2024. Freshllms: Refreshing large language models with search engine augmentation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 1116, 2024, pages 1369713720. Association for Computational Linguistics. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. Preprint, arXiv:2402.05672. Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-knowledge guided retrieval augmentation for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 10303 10315. Association for Computational Linguistics. Evergreen Verbal Instruction Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024a. Measuring short-form factuality in large language models. Preprint, arXiv:2411.04368. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024b. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 23692380. Association for Computational Linguistics. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they dont know? arXiv preprint arXiv:2305.18153. Michael J. Q. Zhang and Eunsol Choi. 2021. Situatedqa: Incorporating extra-linguistic contexts into QA. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 73717387. Association for Computational Linguistics. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah A. Smith. 2024. Set the clock: Temporal alignment of pretrained language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 15015 15040. Association for Computational Linguistics. You are helpful assistant. You help user to classify the questions based on the temporality. There are two classes: immutable and mutable. Immutable, in which the answer almost never changes. Mutable, in which the answer typically changes over the course of several years or less. Think about each question and in the end answer with Mutable or Immutable starting with Classification:"
        },
        {
            "title": "B Translation Prompt",
            "content": "Translation Validation Instruction Translate the following English text into French, German, Hebrew, Arabic and Chinese. Provide the translations as JSON object with keys French, German, Hebrew, Arabic, Chinese. We use GPT 4.1 with TEMPERATURE=0.2 \"RESPONSE_FORMAT\": tag and additional \"JSON_OBJECT\""
        },
        {
            "title": "C Validation Instructions",
            "content": "Translation Validation Instruction For each translated question, assign score according to the following criteria: 0 the translation contains errors that distort the meaning. 1 the translation contains minor errors that do not affect the overall meaning."
        },
        {
            "title": "A Evergreen Testing Details",
            "content": "D Classifier for Self-Knowledge LLM Verbal Parameters. Each example comes with 5-shot for mutable and 5-shot for immutable examples. For llama 3.1 sampling parameters are following: TEMPERATURE=0.7, TOP_P=0.9. For Qwen 2.5: TEMPERATURE=0.6, TOP_P=0.95, TOP_K=20, MIN_P=0 Our Classificator Parameters. All models were trained for 10 epochs with early-stopping and lr = 4.6e-5, bs = 16. Additional datasets were not used. We trained one model for all languages. As shown in Table 10 multilingual-e5large-instruct gives best results. We explored seven classification models using scikit-learn (Buitinck et al., 2013) and CatBoost (Hancock and Khoshgoftaar, 2020): Logistic Regression, k-Nearest Neighbors, Multilayer Perceptron, Decision Tree, Random Forest, Gradient Boosting, and CatBoost. All models were trained with standardized features using StandardScaler. Hyperparameters were optimized on validation subset of 100 examples randomly sampled from the training data, and experiments were repeated with three random seeds per dataset to ensure robustness. For final evaluation, we selected the two bestperforming models on the validation set and combined them into soft-voting ensemble using VotingClassifier. Each component model was retrained on the full training set with its tuned hyperparameters. Hyperparameters grid. Logistic Regression : liblinear], C: [0.01, 0.1, 1], solver: class_weight: [balanced, 0: 1, 1: 1, None], max_iter: [10000, 15000, 20000] [lbfgs, KNN : n_neighbors: [5, 7, 9, 11, 13, 15], metric: [euclidean, manhattan], algorithm: [auto, ball_tree, kd_tree], weights: [uniform, distance] MLP : hidden_layer_sizes: [(50,), (100,), (50, 50), (100, 50), (100, 100)], activation: [relu, tanh], solver: [0.00001, 0.0001, 0.001, 0.01], learning_rate: [constant, adaptive], early_stopping: True, max_iter: [200, 500] [adam, sgd], alpha: Decision Tree : max_depth: [3, 5, 7, 10, None], max_features: [0.2, 0.4, sqrt, log2, None], criterion: [gini, entropy], splitter: [best, random] iterations: CatBoosting: [10, 50, 100, 200], learning_rate: [0.001, 0.01, 0.05], depth: [3, 4, 5, 7, 9], bootstrap_type: [Bayesian, Bernoulli, MVS] Gradient Boosting: n_estimators: [25, 35, 50], learning_rate: [0.001, 0.01, 0.05], max_depth: [3, 4, 5, 7, 9], max_features: [0.2, 0.4, sqrt, log2, None] Random Forest: n_estimators: [25, 35, 50], max_depth: [3, 5, 7, 9, 11], max_features: [0.2, 0.4, sqrt, log2, None], bootstrap: [True, False], criterion: [gini, entropy], class_weight: [balanced, 0: 1, 1: 1, None]"
        },
        {
            "title": "Temporality",
            "content": "Table 9 reports McFaddens pseudo-R2 values from logistic regression models trained to predict evergreen-ness based on two uncertainty metrics: perplexity and mean token entropy. Across most models, the pseudo-R2 scores remain below 0.07, indicating that uncertainty alone provides limited predictive power for evergreen classification. The only notable exception is Phi-3medium (128k), which achieves the highest scores 0.137 (perplexity)suggesting that longer context training may improve temporal uncertainty encoding, however still very limited. We observe no consistent advantage of one uncertainty metric over the other. Similarly, model Model Perplexity Mean Token Entropy Gemma 2 9B Gemma 2 27B LLaMA 3.1 8B LLaMA 3.1 70B Mistral 7B Mistral 24B Phi-3-mini 4k Phi-3-mini 128k Qwen 2.5 7B Qwen 2.5 32B Qwen 2.5 72B 0.014 0.070 0.054 0.028 0.016 0.046 0.032 0.137 0.025 0.020 0.031 0.070 0.013 0.066 0.021 0.012 0.026 0.016 0.073 0.016 0.027 0.029 Table 9: McFaddens pseudo-R2 scores from logistic regression models trained to predict evergreen probability from two uncertainty metrics: perplexity and mean token entropy. size does not correlate clearly with predictive performance; smaller models sometimes match or outperform their larger counterparts. The results indicate that uncertainty metrics capture limited signals of temporality, supporting their use as complementary features rather than standalone predictors of evergreen-ness."
        },
        {
            "title": "F Dataset collection details",
            "content": "The team of trained linguists responsible for assigning the evergreen and mutable labels, as well as writing the golden answers, each hold at least bachelors degree in linguistics, ensuring strong foundation in linguistic principles and effective communication. Additionally, each stage of the labeling process was carefully validated through consultation with the team lead, who provided oversight to maintain consistency and accuracy across the dataset. Furthermore, to support diverse applications, all answers were converted into set of aliases. The procedure for this conversion is detailed in Appendix F.4. The assessors were fairly paid according to local regulations. F.1 Golden Answers Annotation Golden answers should be complete and useful for the user. Examples of good and informative answers: Question: Who is considered the founder of physics? Answer: Isaac Newton is widely regarded as the founder of physics. Comment: The question is asked in the singular form, and according to many sources, Newton is indeed considered the founder of classical physics. Based on logic, online sources, and answers from competing systems, its clear that Galileo Galilei and René Descartes also made significant contributions. However, since the question refers to single person and sources support it, Newton is the most accurate and accepted answer in this context. Question: Who was the President of Italy in the year 2000? Answer: Carlo Azeglio Ciampi was an Italian statesman, the 10th President of the Italian Republic, and former Prime Minister of Italy. Comment: quick fact-check (as should be done for all examples in the guidelines) confirms this answer is accurate and complete. Example of an incomplete or partially useful answer that is not suitable as golden answer: Question: Do spiders have teeth? Answer: Yes, spiders have teeth. Comment: fact-check in open sources reveals that this answer is not accurate enough to be considered golden answer. The correct response would be: Spiders do not have teeth, but they have chelicerae, which contain ducts from venom glands that secrete digestive enzymes. Sometimes, chelicerae are colloquially referred to as fangs or teeth, but they are not actually teeth. Therefore, the original answer should be revised to meet the standard of golden answer. Birthday-related questions: If the question is phrased like How old is Yann LeCun?, the answer should include the exact age, not just the date or year of birth. Open-ended list questions: For questions such as What are the tallest mountains?, Which astronauts are there?, or What animals live in Africa?, good answer should list at least several correct examples and include note that this is not an exhaustive list - more exist. F.2 Evergreen-ness Annotation The evergreen criterion is nuanced one. Most questions are considered evergreen because they are related to established facts or events. However, there are domains, such as astronomy, where new discoveries occur regularly. For example, the record for the largest known star has changed quite recently. The definition of this criterion depends on the domain of the question. In most cases, facts that have remained unchanged for 2030 years are treated as established. Obviously, questions like Who is the president? are not considered evergreen due to frequent changes in political leadership. During annotation, when we encountered ambiguous cases, we often relied on domain-specific common sense. For example, it is fairly obvious that most major geographical discoveries have already been made. It is highly unlikely that new largest lake or previously unknown landmass on our planet will be discovered. As for questions involving dates, events, and notable personalities, the vast majority of these are considered evergreen, it is nearly impossible to imagine scenario in which the dates of significant historical events or key facts from someones biography would change. Mutable questions: (1) What year was the last solar eclipse? (2) Which country has the longest railway? (3) What date does Ramadan begin? (4) When is the next Olympics? (5) How old is Mike Tyson? Evergreen questions: (1) Into which two states was the Roman Empire divided, and when? (2) Who is Messi? (3) Name the years of Paul von Hindenburgs leadership in Germany. (4) Name the largest lakes on our planet. (5) What is the total area of Europe? F.3 Synthetic data generation To augment our training data, we generated and manually validated 1,449 additional questionanswer pairs using GPT-4.1. Duplicate questions were filtered out, and common templates such as how old is the person were rephrased to reduce redundancy. We also followed the FreshQA style to diversify the data: the model generated both evergreen and mutable examples, with mutable questions further categorized into two subtypes. This approach enhanced the variety and coverage of our training set. Synthetic Instruction Can you generate different question-answer pair: slow-changing questions, in which the answer typically changes over the course of several years (up to 10); fast-changing question, in which the answer typically changes within year or less; never-changing, in which the answer never changes. F.4 Short-Answer Generation Prompt"
        },
        {
            "title": "H License and Infrastructure",
            "content": "All experiments were conducted using 12 NVIDIA A100 GPUs, totaling approximately 40 GPU-hours. Model usage adhered to their respective licenses: LLaMA 3.1 (Grattafiori et al., 2024) and Gemma 2 (Team et al., 2024) under custom licenses, Phi 3 (Abdin et al., 2024) and E5 under MIT, and Qwen 2.5 (Team, 2024) and Mistrals (Jiang et al., 2023) under Apache 2.0. GPT Models were accessed via API or web-interface2. We release our dataset and classifier under the MIT License. Short-Answer Generator Instruction You are **short-answer generator**. Given factual **question** and complete (possibly long) **answer**, return several *concise, semantically-equivalent* answer variants. ### RULES 1. Every variant must be factually correct and answer the question on its own. 2. Keep each variant as short as possible (15 words) while still unambiguous. 3. Include the most common spellings, abbreviations, numerals Roman-numeral forms, and the canonical full form. 4. Do **not** add information that is not explicitly in the answer. 5. Return JSON object **exactly** like: { \"answers\": [ \"Variant 1\", \"Variant 2\", . . . ] } ### EXAMPLES Question: \"Who is king of England?\" Answer: \"The King of Great Britain Carl 3 (Charles Philip Arthur George).\" [\"Carl 3\", \"King is Carl 3\", \"Carl III\", \"Charles III\", \"Charles Philip Arthur George\"] Question: \"What is the highest mountain in the world?\" Answer: \"Mount Everest is the highest mountain above sea level.\" [\"Mount Everest\", \"Everest\", \"Mt. Everest\"] Question: \"Which element has the chemical symbol O?\" Answer: \"The chemical element with symbol is oxygen.\" [\"oxygen\", \"Oxygen\", \"element is oxygen\"] Question: \"Who wrote the play Romeo and Juliet?\" Answer: \"Romeo and Juliet was written by William Shakespeare.\" [\"William Shakespeare\", \"Shakespeare\"] Question: \"What is the currency of Japan?\" Answer: \"The Japanese currency is the yen.\" [\"yen\", \"Japanese yen\", \"JPY\"] You only have to send **one message** per call. We query GPT-4o with TEMPERATURE=0.2 tag \"response_format\": and the additional \"json_object\" to create short form answers from long form. It helps to better compare performance through an LLMs. 2https://openai.com/ Model Russian English French German Hebrew Arabic Chinese AVG Validation Data (FreshQA) BERT base cased (Devlin et al., 2019) Deberta v3 base (He et al., 2023) E5 Small (Wang et al., 2024) E5 Large (Wang et al., 2024) BERT base cased (Devlin et al., 2019) Deberta v3 base (He et al., 2023) E5 Small (Wang et al., 2024) E5 Large (Wang et al., 2024) 0.822 0.860 0.800 0.832 0. 0.783 0.854 0.818 0.811 0.851 0. 0.832 0.841 0.830 0.834 0.834 0. 0.839 0.818 0.830 0.801 0.815 0. 0.815 0.824 0.872 0.835 0.871 0. 0.835 0.864 0.848 Test Data 0.893 0. 0.889 0.884 0.889 0.883 0.902 0. 0.836 0.842 0.845 0.841 0.832 0. 0.831 0.836 0.821 0.822 0.819 0. 0.804 0.807 0.817 0.815 0.910 0. 0.909 0.910 0.904 0.900 0.897 0. Table 10: Comparison of different models on training dataset. All models are multilingual variants. The best scores are shown in bold. Misclassification reason Example questions False Positives (non-evergreen, but classified as evergreen) Temporal phrasing mistaken for fixed historical facts Superlatives assumed to be static facts Biographical/life data on alive people treated as static Geographic facts seen as immutable How-to questions with time-sensitive/legal context In what year will the presidential election take place in Russia? When will the full moon be in April? What is the biggest star in the sky? Which tea is the healthiest? What is the most popular social network in the world? In which movies has Danila Kozlovsky acted? How many works has Stephen King written? What is the length of the Amazon River? Where is the largest zoo located? How can maternity capital be used for building house? How can contact Sberbank from abroad? False Negatives (evergreen, but classified as non-evergreen) Superlatives treated as time-sensitive or trend-based Biological and geographical facts wrongly assumed to change frequently Cultural or mythological constants treated as mutable Historical events treated as recent or developing stories Recent years treated as too recent to be stable What is the oldest currency? The rarest element in the periodic table. How long did the shortest war in history last? What is the area of Liechtenstein? Which animal has the highest blood pressure? How many species of elephants currently live on the planet? Where does Ded Moroz live? How old is Ded Moroz? In what year was the last eruption of Mount Vesuvius? What is the role of the French Revolution? Who was recognized as the best actor in 2024? Who is in first place on the Forbes list in 2024? What is the subsistence minimum set in Russia in 2024? What is the most popular TV series in 2023? Table 11: Error Analysis of EG-E5 Classifier: breakdown of misclassification patterns."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "MIPT",
        "MTS AI",
        "Skoltech"
    ]
}