{
    "paper_title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding",
    "authors": [
        "Keyan Chen",
        "Chenyang Liu",
        "Bowen Chen",
        "Wenyuan Li",
        "Zhengxia Zou",
        "Zhenwei Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 2 4 6 1 . 3 0 5 2 : r 1 DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding Keyan Chen1, Chenyang Liu1, Bowen Chen1, Wenyuan Li2, Zhengxia Zou1, Zhenwei Shi,1 Beihang University1 the University of Hong Kong2 AbstractThe advancement of remote sensing technology has significantly improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretation tasks. However, existing methods exhibit limited generalization capabilities across varied remote sensing applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient crosstask adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (e.g., maritime objects, artificial structures) often occupy minimal spatial proportions (1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D token sequences (100,000) poses significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, dynamic visual perception foundation model for remote sensing imagery. The framework integrates novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge learning, we introduce multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale dataset with region-level annotations. Extensive evaluations across nine downstream tasks, including scene classification, image retrieval, region classification, object detection, SAR instance segmentation, optical instance segmentation, building extraction, road segmentation, and change detection, demonstrate the models versatility. Experimental results indicate that DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing 2048 2048 pixel images with 97 ms latency (6% of ViTs) and 833 MB GPU memory consumption (3% of ViTs). Moreover, the model surpasses Transformer-based baselines across benchmarks, achieving state-of-the-art performance in tasks requiring multi-granular visual analysis. Source code is available at https://github.com/KyanChen/DynamicVis. Index TermsRemote sensing image, foundation model, dynamic network, general model."
        },
        {
            "title": "T he rapid advancement of remote sensing technology has",
            "content": "substantially enhanced the spatial and temporal resolution of satellite imagery, driving increased emphasis on high-resolution Earth observation capabilities [13]. These technological innovations deliver critical insights for diverse applications such as land use classification, urban infrastructure planning, and socioeconomic development analysis [47]. Consequently, the demand for precise and granular interpretation of remote sensing data has grown significantly across domains. Foundation models incorporating extensive remote sensing geographic knowledge could fulfill the heterogeneous requirements of downstream tasks by providing transferable prior knowledge while maintaining accuracy with minimal adaptation costs [8]. Nevertheless, current research reveals critical gap in developing foundation models capable of delivering broadly applicable interpretative knowledge across diverse remote sensing applications [9]. Existing cross-task foundation models remain scarce and predominantly process low-resolution imagery with limited image sizes, failing to exploit the rich informational potential of highresolution data and leverage the large-scene semantics [10]. These limitations induce knowledge degradation and reduced discriminative performance, particularly in tasks requiring fine-grained feature extraction (e.g., small object detection or localized detail analysis). For instance, while RSPrompter [11] achieves state-ofthe-art instance-level discrimination through prompt learning and foundation priors, its applicability remains confined to specific tasks. Grounding DINO [12] scales its architecture using 20million-image dataset but achieves only detection task. Similarly, RingMo [13] develops remote sensing-optimized backbone via self-supervised training on 2 million 448 448 pixel images. SpectralGPT [14] employs progressive masked image modeling on 96 96 and 128 128 pixel multispectral data, yet both lack resolution scalability for detailed analysis. Despite the enhanced interpretative capabilities offered by high-resolution imagery, its adoption imposes significant computational burdens during training and deployment. Most contemporary foundation models rely on Transformer architectures, where the quadratic computational complexity of self-attention mechanisms (relative to input resolution) intensifies resource constraints [15, 16]. This scalability challenge is particularly acute for researchers using consumer-grade hardware, as training or fine-tuning high-resolution ( millions of pixels) models demands prohibitive memory and processing resources. Efforts to address these challenges have primarily focused on reducing the computational complexity of attention mechanisms through sparse token representations, categorized into two approaches: 1) visual token aggregation (spatial compression via convolution or clustering) [1719], and 2) token resampling (sequence approximation via cross-attention) [20]. However, these methods face limitations, including excessive information compression, restricted parallelization, and elevated memory access costs [16, 21, 22]. further limitation arises from the predominant reliance on ViTs [15]. ViTs compress spatial data into channel dimensions through downsampling, typically mapping 16 16 image patches into channel space. While effective for natural imagery, its efficacy diminishes in remote sensing applications characterized by numerFig. 1: a): ViTs process all visual tokens uniformly. b): DynamicVis selectively extracts key tokens at each block to perform adaptive modeling. c): Memory consumption of different model architectures at varying input resolutions. d): The proposed DynamicVis demonstrates versatility in interpreting diverse temporal and spatial localization patterns. Comprehensive evaluations across nine downstream tasks, spanning region-, instance-, and pixel-level understanding, demonstrate its efficacy, generalizability, and scalability. ous small, spatially dispersed targets. The aggressive compression risks losing localized details and spatial relationships critical for small-object analysis (e.g., targets smaller than 16 16 pixels may be lost), thereby degrading performance [23, 24]. While increasing input resolution could mitigate this issue, such adjustments incur prohibitive computational costs due to quadratic growth in GPU memory consumption, creating an intractable trade-off. Furthermore, remote sensing tasks inherently demand hierarchical feature extraction: scene-level semantics for classification, target-level discriminative features for detection, and pixel-level precision for segmentation [2528]. Although ViT-based architectures excel at capturing large-scale regional patterns, their capacity to represent fine-grained, small-target attributes limits their adaptability to such nuanced requirements in remote sensing. To address these challenges, we propose DynamicVis, dynamic visual perception foundation model for remote sensing that draws inspiration from the selective attention mechanisms of biological vision systems. Unlike ViTs that process all visual tokens uniformly (Fig. 1 a)), DynamicVis employs adaptive token routing across network layers to selectively prioritize task-relevant regions (Fig. 1 b)). Diverging from conventional Mixture-ofExperts (MoE) architectures that distribute tokens across specialized subnetworks [29], our framework identifies sparse, highsignificance tokens via router-assigned importance scores. These tokens undergo incremental feature refinement and then are projected into the original full token sequence to preserve spatialsemantic integrity. This approach is particularly effective for remote sensing scenarios where critical targets occupy spatially limited regions within expansive backgrounds. To retain fine-grained details during feature extraction, the downsampling kernel size is reduced to 4 instead of 16 in ViTs. While this modification exacerbates long-sequence modeling complexity, the challenge is addressed through selective state space models (SSMs) [30, 31] that operate exclusively on the aforementioned dynamically selected tokens. By cascading multiple SSM blocks, the DynamicVis backbone achieves comprehensive scene understanding while preserving local details, enabling computationally efficient encoding of high-resolution imagery (Fig. 1 c)). meta-embedding-based multi-instance learning (MIL) framework is further introduced to facilitate cross-task geographic knowledge transfer. Trained on the million-scale fMoW dataset with weak region-level annotations [32, 33], this paradigm disentangles heterogeneous feature distributions while distilling shared semantic representations in latent space. Extensive evaluations across nine downstream remote sensing tasks demonstrate the models effectiveness, generalizability, and scalability (Fig. 1 d). The principal contributions are summarized as follows: i) We proposed DynamicVis, dynamic visual perception foundation model inspired by biological attention mechanisms that captures generalizable visual semantics in high-resolution remote sensing data while mitigating computational constraints inherent to large-scale imagery analysis. ii) We introduce dynamic regional perception architecture based on the state space modeling balancing global scene understanding with local feature extraction, enabling efficient hierarchical representation encoding. iii) We explore and propose meta-embedding MIL paradigm leveraging weak region-level annotations for large-scale pretraining. Compared to contrastive learning or MAE, this method exhibits accelerated convergence, higher information density, and enhanced scalability relative to fully supervised methods. iv) Comprehensive validation through nine downstream tasks, demonstrating state-of-the-art performance in hierarchical visual representation learning from raw remote sensing imagery with significantly reduced computational overhead."
        },
        {
            "title": "2 Related Works\n2.1 Remote Sensing Foundation Model",
            "content": "The emergence of foundation models represents transformative milestone in pursuing artificial general intelligence (AGI), particularly through their domain-specific adaptations [34]. For instance, models such as ChatGPT have redefined performance benchmarks in natural language interaction, while concurrent efforts have expanded their applicability by integrating diverse data sources and multimodal inputs that transcend conventional language-centric frameworks [35]. However, adapting foundation models to remote sensing applications presents unique challenges due to inherent domain-specific characteristics [11, 36, 37]. Remote sensing data, typically represented as multidimensional (2D/3D) image matrices, often lack well-aligned textual annotations and exhibit significant spatial redundancies. These characteristics complicate the direct application of training methodologies originally designed for general computer vision or natural language processing tasks, as conventional approaches typically presume the availability of semantically rich, temporally structured, or textually annotated data. Despite these obstacles, some progress has been achieved in adapting foundation models to remote sensing through three principal paradigms: 1) large-scale supervised or semi-supervised pretraining, 2) unsupervised representation learning using selfsupervised objectives, and 3) domain-specific supervised finetuning of pre-trained models [38]. Supervised and semi-supervised training paradigms explicitly guide parameter optimization during model training. Tong et al.[39] demonstrated notable improvements in cross-domain generalization by pre-training deep CNNs on the GID land cover dataset. However, the scarcity of accurately annotated remote sensing data has driven the development of alternative approaches that leverage coarser supervision signals. For instance, GeoKR [40] enhanced representation learning by geographically correlating remote sensing imagery with the GlobeLand30 land cover product, thereby reducing reliance on detailed annotations. Long et al.[41] further advanced the field by curating MillionAID, large-scale scene classification dataset. Their work empirically validated the effectiveness of training classical CNN architectures from scratch, underscoring the importance of data scale in remote sensing pre-training. Additionally, multi-task supervised pretraining on the SAMRS dataset has proven effective for achieving robust generalization across diverse downstream applications [42]. While annotation availability inherently limits the scalability of these paradigms, their consistent learning efficacy highlights their relevance in scenarios with constrained calculation resources. Unsupervised approaches for remote sensing are primarily categorized into contrastive learning and masked image modeling (MIM), with the latter prominently represented by Masked Autoencoders (MAE) [43]. Contrastive methods exploit distinctive characteristics of remote sensing data, including temporal variation, seasonal periodicity, and geographic displacement. Representative implementations include SeCo and CACo, which model temporal dynamics from multi-temporal sequences [44], and SkySense, which employs multi-granularity contrastive learning across different temporal and spatial resolutions [10]. MIM adaptations address domain-specific spatiotemporal and multi-scale attributes: SatMAE [45] adapts the MAE framework for multispectral and multi-temporal inputs, while Scale-MAE [46] incorporates multi-scale perception encoder. Notably, RingMo [13] demonstrates the semantic discriminability of unsupervised mask training across 2 million images, and SpectralGPT [14] achieves state-of-the-art performance on four downstream tasks through progressive MAE pre-training at 9696 and 128128 resolutions. Hyperspectral applications are advanced by HyperSIGMA [47], which employs MAE-based pre-training on the HyperGlobal450K dataset (64 64 pixels) for both high-level semantic tasks and low-level reconstruction. Despite enabling substantial data and model capacity scaling, these methods face efficiency limitations due to inherent redundancies in spatial-spectral patterns within remote sensing imagery. 3 In contrast, domain-specific supervised fine-tuning adapts general-purpose foundation models to specialized remote sensing tasks, optimizing computational resources while enhancing taskspecific accuracy. This adaptation encompasses techniques such as prompt learning, instruction fine-tuning, in-context learning, and adapter-based methods [48]. For example, BAN [49] improves change detection by integrating dual-temporal CLIP features with task-specific adapters, while RSPrompter [11] evaluates SAMbased prompt learning for cross-domain instance segmentation. Text2Seg [50] further reduces dependency on extensive annotations by synthesizing prompts from multiple visual foundation models, enabling dataset-agnostic generalization. The development of domainand task-specific foundation models remains an active research frontier, demonstrating the potential to address specialized requirements under constrained computational resources and data availability. While remote sensing foundation models have demonstrated robust generalization capabilities across diverse applications, contemporary research prioritizes isolated task optimization over developing unified framework for comprehensive interpretation. Although certain models are capable of providing shared priors across tasks, their visual encoders are typically restricted to lowresolution inputs (e.g., 64 64 or 224 224 pixels), limiting their capacity to exploit the rich informational potential of highresolution imagery and leverage large-scale scene semantics. This constraint hinders the effective extraction of small targets and finegrained spatial features, diminishing their potential for precise, detailed analysis. To overcome these limitations, the proposed DynamicVis framework employs supervised multi-instance learning trained on corpus of millions of region-level annotations. By simultaneously discovering shared representations and disentangling heterogeneous features in deep semantic space, the framework enhances multi-scale representational capacity, enabling robust adaptation to diverse remote sensing scenarios. 2.2 Model Architecture The evolution of deep learning architectures has been significantly influenced by advancements in computational capabilities. Following Rosenblatts development of the perceptron model (1958) for pattern recognition [51], continuous efforts have been made to design increasingly sophisticated architectures for hierarchical feature representation [52]. In computer vision, CNNs have dominated over the past decade due to their ability to capture and process spatial patterns through localized convolutional operations effectively [53]. Nevertheless, CNNs exhibit inherent limitations in modeling large-scale features and global structural relationships due to their restricted receptive fields. This constraint has motivated architectural innovations to transcend spatial locality. Attention mechanisms have emerged as promising solution, enabling feature extraction across extended spatial contexts. For instance, SENet [54] pioneered channel-wise feature recalibration, while CABM [55] unified spatial and channel attention for refined feature modulation. The Non-local network [56] further extended CNN capabilities with non-local operations to model long-range dependencies. paradigm shift occurred with Transformer, which employs self-attention to establish global dependencies between input elements, demonstrating superior capability in modeling contextual relationships and enabling parallelized computation. Building on this, the ViT has catalyzed breakthroughs in computer vision, achieving state-of-the-art performance in tasks such as image classification, object detection, and segmentation [57]. Despite these advances, ViTs quadratic computational complexity and memory demands relative to input token length render it less efficient than CNNs for high-resolution image analysis. This tradeoff between global context modeling and computational efficiency remains critical challenge in contemporary architecture design. Linear attention mechanisms have emerged as promising solution to computational efficiency challenges in sequence modeling. significant advancement in this domain was introduced by Albert et al.[58] through Structured State Space Sequence Models (S4), novel architecture that serves as computationally efficient alternative to traditional Transformers. Rooted in classical state-space theory, S4 strategically integrates the complementary advantages of RNNs and CNNs, achieving linear or near-linear computational complexity relative to sequence length via recursive and convolutional processing. Despite these advancements, empirical analyses have demonstrated that such models exhibit suboptimal performance when handling discrete, informationdense data types such as textual inputs. The Mamba [30] (Selective State Space Models, S6) overcomes this limitation by enabling input-dependent parameter adaptation in SSMs, thereby facilitating context-aware information filtering. The adaptive paradigm has subsequently been extended to visual processing tasks, achieving performance on par with conventional CNN and Transformer baselines. For example, VMamba and Vim employ dual and quadruple processing branches, respectively, to enable non-causal analysis of 2D visual data [59]. Furthermore, Mamba-based frameworks such as CDmamba and ChangeMamba have demonstrated efficacy in dense prediction imagery, underscoring their versatility in tasks for geospatial complex visual domains [60, 61]. Building on these developments, our work proposes dynamic vision perception method for high-resolution remote sensing data using S6 blocks, with dual objectives: enhancing long-sequence modeling efficiency while addressing the intrinsic non-causality of image data. The proposed DynamicVis distinguishes itself through two key innovations. First, computationally efficient bidirectional processing is achieved only through forward and backward pathways. Second, dynamic token selection mechanism enables theoretically arbitrary token orders, facilitating comprehensive non-causal modeling. Comparative experiments validate that this streamlined approach reduces parameter overhead while maintaining competitive performance. 2.3 Visual Compression and Token Reduction High-resolution visual features are crucial for achieving detailed task comprehension through fine-grained discriminative capabilities, particularly for precise applications such as element counting and small object localization. However, current visual foundation models predominantly employ CLIP-derived encoders constrained to low-resolution inputs (typically 224 224 or 336 336 pixels) [9]. which limits their ability to extract spatially dense features critical for precision-oriented analysis. While straightforward increasing input resolution could theoretically enhance detail capture, the associated quadratic growth in computational complexity renders naive upscaling impractical. To resolve this trade-off, sparse token representation methods have been developed via two principal strategies: token aggregation and token resampling. Token aggregation techniques employ spatial compression through strided convolutions or token grouping mechanisms [6264]. For instance, hierarchical Transformers 4 utilize patch merging to progressively reduce spatial resolution while expanding channel dimensions [17]. Parallel solutions such as C-Abstractor [65] leverage convolutional layers for localized feature modeling and downsampling. Alternative approaches like NFormer apply k-NN clustering to select semantically similar tokens [66], while LLaVA-PruMerge adopts graph-based clustering for token grouping [67]. Conversely, token resampling methods employ cross-attention mechanisms to distill information into learnable query vectors, exemplified by Q-Former [68] and Flamingos Resampler [20]. Hybrid architectures like TokenPacker [69] combine multi-resolution cues to preserve spatial details through distinct key-value pathways. Despite these advances, three critical limitations persist: 1) Over-compression during spatial reduction, particularly detrimental in remote sensing applications where sparse target distributions demand precise spatial retention; 2) Architectural inefficiencies that impede parallel processing, especially in cluster-based algorithms; and 3) Increased memory overheads from multi-stage compression. These issues collectively degrade performance through loss of local details, disrupted spatial relationships, and insufficient multi-scale feature representation. We propose dynamic visual information-aware token reduction framework to mitigate these challenges. This approach incrementally models features through the selective retention of discriminative tokens while integrating non-key tokens via parameterfree aggregation, thereby preserving information integrity while optimizing computational efficiency. Furthermore, hierarchical architecture incorporating stacked S6 modules facilitates multiscale feature extraction, ensuring adaptability to diverse interpretation tasks across varying granularity levels."
        },
        {
            "title": "3 Methodology\n3.1 Overview",
            "content": "Motivated by the selective attention mechanism in biological vision systems and the sparse spatial distribution of objects characteristic in remote sensing imagery, we propose DynamicVis, dynamic visual perception foundation model for remote sensing image interpretation. The model adheres to the standard foundation model paradigm of pre-training followed by task-specific fine-tuning, engineered to efficiently extract transferable visual semantics from high-resolution geospatial data with large sizes. The architecture employs dynamic region-aware backbone based on SSMs, enabling simultaneous capture of large-scale scene semantics while preserving sensitivity to local details through adaptive feature aggregation. For pre-training, we proposed meta-embedding-based multiinstance learning framework that scales to process millions of regional annotations, which promotes the learning of cross-task feature representations while retaining discriminative power for the heterogeneous patterns in deep semantic space. To adapt foundation model to downstream applications, modular taskspecific decoders have been developed and validated through systematic testing across diverse remote sensing data types, scales, and interpretation objectives. The unified processing workflow is formalized as: = ΦΓ decoder Φbackbone(I), (1) where denotes the input image or image pair (for change detection), Φbackbone represents the shared foundation model backbone, ΦΓ decoder indicates the parameterized decoder for task type Γ, and 5 Fig. 2: The overview of Dynamic Region-aware SSM Backbone, comprising four interconnected stages that generate hierarchical semantic feature maps at varying scales. Red boxes highlight regions of interest, while yellows denote regions exhibiting structural simplicity or repetitive patterns. encompasses the interpretation outputs (e.g., category labels, object boxes, or pixel-wise masks). 3.3 Dynamic Region-aware SSM Backbone 3.3.1 Overall Structure 3.2 Revisit of Selective SSM The state space model (SSM) derives from continuous-time systems in control theory and shares fundamental principles with CNNs and RNNs [30, 59]. It models continuous-time signal processing through state transitions, governed by the first-order differential equations: h(t) = Ah(t) + x(t), y(t) = Ch(t), The Dynamic Region-aware SSM Backbone constitutes the core component of the DynamicVis framework. This backbone integrates multi-scale region-aware SSM feature extractor with standard Feature Pyramid Network (FPN) [70] to enable efficient cross-resolution semantic representation while demonstrating the potential for cross-task interpretability (see Fig. 2). The operational process can be formally expressed as: (2) {Fi} = Φbackbone(I) = Φfpn Φms-extract(I), (6) where the output y(t) is computed from the input x(t) and the hidden state h(t) RN. The system dynamics are parameterized by the state transition matrix RNN, input projection matrix RN1, and output projection matrix R1N. To adapt Eq. 2 into discrete-time formulation compatible with deep learning frameworks, matrices and are discretized using zero-order hold (ZOH) method with time scale parameter . This results in the discretized SSM parameters (, A, B, C), where: = exp(A), = (A)1(exp(A) I) B. (3) The discrete-time state-space representation derived from Eq. 2 is then expressed as: hk = Ahk1 + xk, yk = Chk, (4) retains equivalence to C. The model supports dual where computation modes: linear recurrence for sequential processing or global convolution for parallelized training. The convolutional formulation is parameterized as: = (cid:16) B, A B, . . . , AL1 = K, (cid:17) , (5) where denotes the input sequence length and RL represents the structured convolutional kernel. During training, convolution is prioritized for its parallel computation efficiency, enabling simultaneous processing of full input sequences. For inference, the model transitions to recurrent mode to enable memory-efficient autoregressive generation, analogous to the Transformer decoder. The Mamba enhances classical SSMs by introducing an inputdependent selection mechanism. This addresses inherent limitations in discrete-time SSMs by dynamically parameterizing , B, and as functions of the input, thereby enabling context-aware modulation of information flow. where an input image RHW3 is processed sequentially through two stages: 1) hierarchical feature extraction via the multi-scale SSM feature extractor Φms-extract, and 2) multi-scale feature fusion via the FPN Φfpn. The framework produces feature maps {Fi} at five distinct resolution levels, with each output Fi 2i+1 di , {1, 2, 3, 4, 5}. 2i+1 3.3.2 Multi-scale Region-aware SSM Extractor The SSM feature extractor comprises four sequential stages that progressively generate multi-scale feature maps with globally activated semantic representations. Each stage incorporates three core modules: patch merger, sparse visual region token mixer block, and normalization layer. The modeling process of the i-th stage is formally expressed as: Fi = Φnorm Φsparse-mixer Φpatch-merger(Fi1), (7) where Fi1 and Fi denote the input and output feature maps of the i-th stage (i {1, 2, 3, 4}), with F0 derived from the input image I. The modules operate as follows: 1) The patch merger Φpatch-merger reduces spatial dimensionality through feature compression, 2) The sparse mixer Φsparse-mixer leverages SSM to model global semantic dependencies across strategically selected visual token sequences (detailed in Sec. 3.3.4), and 3) The normalization layer Φnorm applies layer normalization to stabilize training dynamics. 3.3.3 Patch Merger The patch merger in the i-th stage utilizes 2D convolutional block (Conv2D + LN Norm) with kernel of ki ki and stride of si si, i.e., mapping each local region of size ki ki into single visual token. The merging operation is formally expressed as: Fi = ΦConv2D(Fi1, ki, si), Fi = Fi + PE if = 1, (8) 6 Fig. 3: The structure of the Sparse Mixer, including three key elements: flattening operation, Ni selective token incremental modeling (STIM) units, and an un-flattening operation. 4 where ΦConv2D represents the 2D convolution. To preserve spatial relationships between visual tokens, learnable positional encoding PE 4 d1 is exclusively incorporated at the initial stage (i = 1). Notably, different from ViTs, employing abrupt 16-stride downsampling for single-scale feature extraction, our architecture preserves fine-grained details and small object features through progressive small-stride (si = 2) downsampling. This strategy enables the hierarchical construction of multi-scale feature representations while mitigating information loss associated with aggressive resolution reduction. 3.3.4 Sparse Mixer The small-stride downsampling operation poses efficiency challenges for modeling ultra-long sequences. To mitigate this, the Sparse Mixer block employs an SSM architecture to perform selective incremental modeling of salient regions/tokens, thereby addressing computational bottlenecks. Within the Sparse Mixer block at the i-th stage, the procedure proceeds through three sequential operations (see Fig. 3): 1) The feature maps Fi are flattened into 1D sequence si RLidi , where Li = HiWi. 2) The sequence si is iteratively processed by Ni Selective Token Incremental Modeling (STIM) units to achieve comprehensive global semantic integration. 3) The refined sequence is restored to its original dimensions via an unflattening operation. Mathematically, this pipeline is formalized as: si = Φflatten(Fi), si = Φk STIM(si, ri), Fi = Φun-flatten(si), (9) where Φk STIM denotes the k-th STIM unit within the i-th Sparse Mixer. Although the quantity of STIM units Ni varies across Sparse Mixers, their architecture remains consistent. The token reduction ratio ri is critical hyperparameter governing the sparsity of the selected tokens, by dynamically adapting to the feature redundancy levels present at each processing stage. This ratio is calibrated to balance computational efficiency with representational capacity, ensuring stage-specific optimization of token selection. 3.3.5 Selective Token Incremental Modeling The Selective Token Incremental Modeling (STIM) unit (Φk STIM) performs selective global modeling on visual token sequences (si). For simplicity, the superscript and subscript are omitted. The STIM unit comprises three key components: 1) key tokens selection mechanism, 2) dual-path scanning operation based on Fig. 4: The detailed architecture of the Selective Token Incremental Modeling (STIM) unit. SSMs, and 3) an incremental connection (see Fig. 4), formally expressed as: = ΦSTIM(s, r) = Φincrement-conn Φdual-scan Φtoken-select(s, r), (10) where denotes the input token sequence and represents the predefined token reduction ratio. 1) Token Selection: Two distinct token representations are constructed from the input sequence s: 1) dense global semantics xg and 2) sparse regional semantics (xr). global semantics: The xg is derived through adaptive 1D pooling applied to the original sequence s, compressing the L. These global semantic tokens are sequence length from to fully retained to provide comprehensive contextual information throughout the modeling process. regional semantics: The selected regional tokens (xr) are identified via importance scores generated by linear projection layer Φmlp. To stabilize training and avoid local minima, Gumbeldistributed noise is incorporated into the logits before softmax normalization. The top-k token selection process is formalized as: = Φmlp(s) = σ (p + ϵ) Ω = Φtop-k(w, r) xr = Ω(s) (dimensionality reduction: 1) ϵ Gumbel(0, v(1 e/emax)) (11) where RL1 corresponds to token-wise importance logits. The noise temperature is governed by = 0.1, while e/emax denotes normalized training progress, ensuring that the noise magnitude is progressively decayed during training. The softmax function σ generates normalized importance weights, and Ω represents the selection operator that retains tokens with the highest scores according to reduction ratio r. The combined xg and xr representations enable incremental modeling that preserves global contextual awareness while dynamically focusing on critical regional features. This synergistic approach achieves an optimal balance between computational efficiency and representational fidelity through context-guided selective processing. 2) Dual-path Scanning: To enhance computational efficiency in global dependency modeling, we perform dual-path SSM scanning operation (Φdual-scan) exclusively on global tokens (xg) and key regional tokens (xr) (see Fig. 5), rather than adopting the exhaustive processing of all visual tokens as in ViTs. By leveraging top-k selection mechanism to accommodate theoretically arbitrary token sequence orderings, our method, only taking two 7 Fig. 6: The overview of the meta-embedding-based multipleinstance learning to integrate contextual and semantic feature representations in the latent space. annotation comprising ni instances, each instance is characterized by bounding box bi Rni4 and semantic category ci RniC. Here, corresponds to the predefined categories. The objective is to learn region-specific visual-semantic representations through multi-instance contrastive learning, where meta-embeddings derived from categorical information guide the alignment process (see Fig. 6). The proposed Multi-Instance Learning Noise Contrastive Estimation (MIL-NCE) loss LMIL-NCE is formulated to maximize the aggregated similarity of positive pairs while suppressing negative pairs, as expressed by: = log (cid:80) (v,t)P exp (13) (cid:80) (v,t)P (cid:16) vT ,t τ exp (cid:17) (cid:16) vT ,t τ (cid:17) + (cid:80) (v,t)N exp (cid:17) (cid:16) ,t τ (14) where denotes set of positive matches formed between image region features and their corresponding categorical metaembeddings, while comprises negative pairs sampled from nonassociated categories. For every annotated region, positive pairs are constructed with their ground-truth category, and negative pairs are generated using all unrelated categories within the batch. The loss is computed in batch-wise manner to ensure efficient optimization. Regional Visual Representation: Given an image RHW3, 2i+1 multi-level features Fi 2i+1 di are extracted through the SSM backbone, where {1, 2, 3, 4, 5}. For region annotations (b), Generic RoI Extractor (GRoIE) [73] is employed to derive region-specific visual representations of uniform dimensionality. These representations are subsequently pooled into visual embeddings. The adoption of GRoIE over conventional RoI extractors is driven by the necessity to ensure taskand dataset-agnostic generality during pre-training. To this end, features from all hierarchical scales are aggregated for RoI pooling, diverging from traditional approaches that heuristically select single feature layer deemed optimal for RoI alignment. The process is formalized as: = Φpool Φg-roi (Φbackbone(I), b) , (15) where R1d represents the visual embedding for region in image I. The final pooling operation Φpool applies average pooling over spatial dimensions to reduce embeddings to 1 1 resolution. Categorical Meta-embedding: Rather than relying on direct visual region feature classification for pre-training, multi-instance contrastive learning is employed in the feature space to strengthen the models generalized representation learning. Each semantic category is modeled as learnable point in the embedding space, termed as meta-embedding. To accelerate convergence, these Fig. 5: The structure of dual-path SSM scanning. scanning paths, can achieve performance comparable to existing approaches that rely on fouror eight-directional scans [59], thereby further reducing computational complexity. Formally, the process is defined as: g, x = Φdual-scan([xg, xr]), (12) where xg and xr are concatenated along the token dimension and processed through standard Mamba-based block with dual-path processing (see [25, 71] for details). 3) Incremental Connection: To preserve information integrity during selective token modeling, sparse token features acquired through SSM scanning are treated as incremental information and systematically integrated into the original token sequence. Specifically, unselected tokens undergo parameter-free modeling processing. This process can be formalized as: (normalized importance weights) = σ(p) = Ω(s) = Ω(w) s = + (residual integration) (importance-weighted tokens) (regional token replacement) where denotes noise-free token importance probabilities. As formalized in the third equation, combines adaptively weighted original features with globally contextualized key tokens. Incremental modeling is achieved through residual skip connections, which prioritize computational resources for salient tokens while preserving the complete original information across the network. 3.4 Meta-embedding Representation Pretraining Large-scale data pre-training represents foundational component of modern vision foundation models. To enable comprehensive cross-task visual perception capabilities in remote sensing imagery, DynamicVis employs supervised pre-training on the Functional Map of the World (fMoW) dataset [33], which provides region-level annotations across millions of instances. While supervised pre-training faces inherent scalability limitations compared to self-supervised paradigms such as MIM or contrastive learning [72], it offers distinct advantages in learning efficiency. We introduce meta-embedding-based multi-instance contrastive learning framework, operating in feature space rather than relying on conventional logit-space classification (e.g., cross-entropy optimization). We hypothesize that feature-space multi-instance learning promotes more robust cross-task knowledge transfer by distilling generalized semantic patterns, rather than task-specific classification representations. This paradigm shift is designed to preserve latent structural relationships within visual data that extend beyond narrow task-specific boundaries, thereby improving model adaptability for diverse downstream applications. Given the dataset Dtrain = {(I1, y1), . . . , (IN, yN)}, where Ii RHW3 denotes an input image and yi = {bi, ci} represents its meta-embeddings are initialized by encoding category representations using CLIPs text encoder, with prompt templates derived from OpenCLIPs zero-shot protocol1. The proposed architecture is designed to support arbitrary text encoding, thereby extending the pre-training frameworks scope beyond region-category annotations to incorporate region-text and image-text descriptors, enhancing the frameworks scalability. 3.5 Downstream Task Transferring The proposed models performance is evaluated across comprehensive suite of remote sensing visual perception tasks, organized levels: region-level (image hierarchically into three analytical classification, region classification, instanceimage retrieval), level (object detection, instance segmentation), and pixel-level (semantic segmentation, change detection). These tasks necessitate multiscale knowledge integration and heterogeneous information representation formats, including vectors, numerical values, bounding boxes, and segmentation masks. To address these diverse requirements, task-specific decoder heads are implemented while preserving standard architectural configurations from established domain practices, thereby ensuring compatibility with DynamicViss framework. Region-level Interpretation encompasses image classification, region classification, and image retrieval. For image classification, category logits are generated by applying average pooling to the highest-resolution semantic feature map, followed by linear projection layer. Region classification extends the pretraining framework by integrating classification head. Image retrieval is performed in zero-shot configuration, where high-level semantic features extracted by the backbone network undergo spatial pooling to derive compact image embedding vectors. Instance-level Interpretation employs two-stage architecture for object detection and instance segmentation, consistent with conventional methodologies. An RPN is coupled with an R-CNN head for detection [74], augmented by Mask R-CNN head for instance masking [75]. Pixel-level Interpretation targets dense prediction tasks. Semantic segmentation is implemented using UperNet [76] for hierarchical feature fusion and pixel-wise classification. For change detection, feature differences between bi-temporal inputs are computed in high-dimensional semantic space, followed by an MLPbased head to produce the final change prediction."
        },
        {
            "title": "4 Experimental Results and Analyses\n4.1 Pretraining Dataset and Settings",
            "content": "The DynamicVis foundational model backbone was pre-trained on the fMoW dataset [33] and subsequently evaluated across multiple annotated downstream tasks. This section first describes the fMoW dataset employed for multi-instance learning pre-training, with downstream task datasets detailed in subsequent experimental comparisons. The fMoW dataset [33], illustrated in Fig. 7, serves as region classification benchmark with annotation granularity intermediate between image classification and object detection. Each image includes at least one bounding box annotation; however, these annotations are loosely defined, often encompassing targets without adhering strictly to object boundaries. The dataset encompasses 62 semantic categories alongside supplementary false detection 1. https://github.com/mlfoundations/open clip 8 Fig. 7: Illustrative samples from the fMoW dataset, demonstrate diversity in geographical and temporal distributions. TABLE 1: The hyperparameter configuration of the DynamicVis backbone architecture comprises four distinct stages. Version Layers (Ni) Dimensions (di) Reduction Ratio (ri) Patch Size (ki) Stride (si) base large [2, 4, 16, 4] [2, 4, 32, 4] [96, 192, 384, 768] [128, 256, 512, 1024] [7/8, 3/4, 1/2, 0] [7/8, 3/4, 1/2, 0] [7, 3, 3, 3] [7, 3, 3, 3] [4, 2, 2, 2] [4, 2, 2, 2] class for uncategorized instances. Acquired from the DigitalGlobe satellite constellation, the data comprises paired 4-band or 8-band multispectral imagery (visible to near-infrared) and pan-sharpened RGB images. Spanning over 200 countries, the collection contains more than 1 million images enriched with metadata: UTM zone coordinates, timestamps, ground sample distance (GSD), angular measurements (off-nadir angle, target azimuth, sun azimuth, sun elevation), and image/bounding box dimensions. Two publicly available versions exist: 1) fMoW-full (3.5TB): Includes pan-sharpened RGB and 4/8-band multispectral imagery stored in TIFF format. 2) fMoW-rgb (200GB): JPEGcompressed subset retaining only RGB channels. For experimental validation, the fMoW-rgb version was employed, with the training, validation, test, and sequence sets consolidated into 1,027,691 training images and 20,000 test images (originally from the test partition). This test set served dual purposes: facilitating supervisory metrics during pre-training and validation for the region classification task. 4.2 Implementation Details 4.2.1 Architecture Details The proposed foundational model is based on Mambas selective state space architecture. To address varied application demands, two variants (base and large configurations) are introduced, with their structural specifications systematically outlined in Table 1. Positional encoding is implemented using randomly initialized learnable embedding vectors. Feature representations are derived via bidirectional Mamba scanning module that averages outputs from forward and reverse scanning directions, while maintaining the original hyper-parameter configuration of Mamba mixers. The FPN follows conventional design principles, producing five multiscale feature maps with uniform 256-channel dimensionality. The training framework incorporates dual-objective loss function that integrates MIL-NCE with standard classification cross-entropy. novel loss-free balancing strategy introduces Gumbel-distributed noise into affinity scores during training. This noise intensity is progressively annealed in later stages to ensure deterministic outcomes, thereby obviating the need for auxiliary balancing losses. Decoding heads retain established configurations from widely adopted methodologies without structural or parametric modifications. 4.2.2 Training Details The pretraining protocol was conducted using data obtained from official repositories via the AWS client. The dataset was structured into 128 tar archives following the WebDataset2 standard, enabling sequential data streaming to reduce memory overhead while maintaining efficient I/O operations. During pretraining, the framework processed exclusively RGB channels, with input images uniformly resampled to resolution of 512 512 pixels. Data augmentation strategies incorporated random horizontal flipping, random resizing, and cropping. For optimization, the AdamW optimizer was employed, initialized with learning rate of 4 104, and coupled with cosine annealing schedule to gradually reduce the learning rate. The training regimen extended over 200 epochs ( 3000 A100 GPU hours), with batch sizes set to 512 and 256 for the base and large model variants, respectively. Implementation was carried out using PyTorch on the OpenMMLab3 open-source platform. To maximize computational efficiency, automatic mixed precision (AMP) in BF16 format was consistently employed. Prior to loss computation, distributed feature aggregation was performed across nodes to improve the stability and convergence efficiency of the MIL objective. 4.3 Comparison with the State-of-the-Art Extensive comparative experiments were conducted across multiple remote sensing visual perception tasks, organized hierarchically into three granularity levels: region-level (image classification, region categorization, and image retrieval), instancelevel (object detection and instance segmentation), and pixel-level (semantic segmentation and change detection). The evaluation results demonstrated that DynamicVis exhibited robust crossgranularity knowledge integration capabilities. In comparative analyses against alternative methodological frameworks, including conventional convolutional architectures, Transformer-based models, Mamba-structured approaches, and large foundation modeldriven methods, DynamicVis consistently achieved superior performance across all evaluated granularity levels. 4.3.1 Scene Classification The primary objective of remote sensing image scene classification is to automatically categorize aerial or satellite images into predefined semantic classes (e.g., forests, deserts, and urban areas) based on their visual content. As fundamental task in geospatial analysis, this process holds significant practical importance for remote sensing applications [4]. Datasets: The evaluation framework employs two datasets with distinct spatial resolutions and categorical diversity: the UCMerced [77] and AID [78] datasets. 2. https://github.com/webdataset/webdataset 3. https://openmmlab.com/codebase TABLE 2: Comparative analysis with state-of-the-art methods across various scene classification benchmarks. 9 Method ResNet-18 ResNet-50 ResNetDeiT-T DeiT-S DeiT-B ViT-B ViT-L Swin-T Swin-S Swin-B Vim-Ti VMamba-T RSMamba-B RSMamba-L RSMamba-H DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-L DynamicVis-L DynamicVis-L Params. (M) UC Merced 11.7 25.6 44.6 5.5 21.7 85.8 87.2 305.0 27.5 48.9 86.8 7.0 30.0 6.4 16.2 33.1 36.8 36.8 36.8 91.3 91.3 91.3 87.98 91.99 92.40 86.92 88.95 89.14 91.09 91.98 90.87 91.08 91. 89.06 93.14 94.14 95.03 95.47 96.80 95.97 99.09 96.59 96.34 99.12 87.46 91.74 92.22 86.66 88.41 88.73 90.79 91.32 90.63 90.95 91.74 88.73 92.85 93.97 94.76 95.23 96.66 95.89 99.05 96.50 96.20 99. F1 87.40 91.65 92.12 86.53 88.41 88.70 90.77 91.26 90.40 90.82 91.62 88.68 92.81 93.88 94.74 95.25 96.66 95.88 99.05 96.47 96.16 99.09 88.70 89.44 91.03 85.23 85.88 87.32 89.39 90.19 86.49 87.50 89.84 87.76 91.59 92.02 92.31 92.97 94.41 94.11 96.16 94.20 94.08 96.40 AID 88.17 88.66 90. 84.52 85.19 86.07 88.65 88.86 85.66 86.80 89.01 86.98 90.94 91.53 91.75 92.51 94.22 93.82 96.00 93.96 93.82 96.29 F1 88.30 88.87 90.81 84.52 85.34 86.07 88.86 89.17 85.77 86.89 89. 87.13 91.10 91.66 91.90 92.63 94.17 93.81 96.04 93.96 93.79 96.28 1) UC-Merced [77]: This benchmark consists of 21 semantically distinct scene categories, containing 2,100 aerial images (100 samples per category) obtained from the United States Geological Survey (USGS) National Map. Each image, sized 256256 pixels, maintains uniform spatial resolution of 0.3 meters per pixel. The dataset employs standardized partition protocol, allocating 70 samples per category for training and 30 for testing. 2) AID [78]: This large-scale dataset comprises 30 scene categories with approximately 10,000 high-resolution images sourced from Google Earth. The number of samples per class varies between 220 and 420. Each image is formatted to 600 600 pixels but exhibits varying spatial resolutions ranging from 0.5 to 8 meters. balanced split of 50% training and 50% validation data was uniformly applied across all categories. Implementation Details: The pre-trained foundation model was fine-tuned by incorporating linear classifier as the classification head. The architecture employed global average pooling to compress spatial dimensions of feature maps, followed by classification via an MLP. To enhance generalization, comprehensive suite of data augmentation techniques was applied, including random flipping, resizing, and cropping. Optimization was conducted using an AdamW optimizer initialized with learning rate of 2 104, coupled with cosine annealing scheduler incorporating warm-up phase. Training spanned 500 epochs on 8 NVIDIA A100 GPUs, with global batch size of 256. Throughout finetuning, parameters in the backbones first stage remained frozen to preserve low-level feature extraction capabilities. For systematic comparison across architectural variants, experiments were executed under three configurations: 1) without key regional token selection and MIL pre-training (DynamicVis), 2) with local token selection without MIL (DynamicVis), and 3) full integration of both components (DynamicVis). Performance was rigorously evaluated using precision, recall, and F1-score metrics. Results and Analysis: systematic comparison was performed between the proposed DynamicVis and representative architectures spanning convolutional networks (ResNet [79]), transformer-based models (ViT [15], DeiT [16]), and Mambabased architectures (Vim [80], VMamba [81], RSMamba [25]). 10 Fig. 8: percentage-normalized confusion matrix of DynamicVisL on the UC-Merced test dataset, where the horizontal axis represents predictions, and the vertical axis indicates labels. Fig. 9: percentage-normalized confusion matrix of DynamicVisL on the AID test dataset, where the horizontal axis represents predictions and the vertical axis indicates annotations. Evaluation metrics were obtained from both published benchmarks and re-implementations of official codebases to ensure fair comparison. Experimental outcomes across two benchmark datasets are analyzed below: 1) UC-Merced: As shown in Table 2, DynamicVis achieves state-of-the-art performance across all evaluation metrics, with the large-scale variant (DynamicVis-L) nearing perfect classification accuracy. This performance advantage stems from its hierarchical architecture and increased network depth, which demonstrate superior effectiveness over alternative Mamba-based architectural designs. Notably, network deepening provided greater performance gains than network widening strategies in experiments. While the local token selection mechanism introduced marginal accuracy reduction, it substantially improved training efficiency and inference speed, as further quantified in our ablation studies. Additionally, integrating general knowledge through metaembedding representation learning significantly enhanced overall accuracy, confirming the efficacy of the proposed MIL pre-training strategy. The progressive performance improvements from base to large configurations underscore DynamicViss capability to scale for diverse application constraints. 2) AID: The analysis was extended to the more complex AID dataset, which presents heightened classification challenges owing to its expanded 30-class taxonomy and reduced spatial resolution. As presented in Table 2, performance trends closely mirror those observed in UC-Merced, with DynamicVis maintaining consistent advantage over baseline architectures. Interestingly, the dynamic selection mechanism exhibited less pronounced impact on classification accuracy. This discrepancy likely arises from AIDs reliance on localized regional features rather than global scene semantics for category discrimination, thereby reducing the token selection mechanisms potential interference. Visualization: The classification performance of DynamicVisL is demonstrated through confusion matrices in Fig. 8 (UCMerced) and Fig. 9 (AID). Near-perfect diagonal values in both the models high classification accuracy. For matrices reflect the UC-Merced dataset  (Fig. 8)  , minor misclassifications are Fig. 10: Training loss (left y-axis) and evaluation accuracy (right y-axis) vs. iterations for the UC-Merced dataset. concentrated in five categories: buildings, sparse residential, airplane, storage tanks, and tennis court. Notably, occasional confusion between the airplane and runway classes likely stems from their spatial co-occurrence in aerial imagery. In contrast, the AID dataset  (Fig. 9)  exhibits elevated misclassification rates for Square, Center, School, and Resort. The most pronounced confusion occurs between Resort and Park, which share overlapping visual features such as green spaces and recreational structures. Despite these isolated challenges, DynamicVisL achieves robust scene classification performance across both datasets. To evaluate the impact of model components, training dynamics were analyzed using classification loss (primary axis) and accuracy (secondary axis) on the UC-Merced dataset  (Fig. 10)  . Two key findings emerged: a) Models employing the dynamic token selection mechanism exhibited moderately slower convergence, attributable to the dual optimization of classification plane and dynamic token identification. However, final performance metrics showed no statistically significant difference compared to models without this component. b) The integration of meta-embedding representation learning substantially accelerated the convergence rate, achieving comparable performance within 50 training iterations compared to the 500 iterations required by conventional approaches. This improvement underscores the efficacy of metaembeddings in feature space optimization. TABLE 3: Performance comparison of proposed method vs. stateof-the-art on Levir-Ship dataset. 11 4.3.2 Tiny Ship Detection Ship detection in remote sensing imagery plays vital role in maritime surveillance and security enforcement. However, the extensive nature of oceanic surfaces presents significant challenges, as ship targets typically appear as small objects and are frequently obscured by environmental factors such as clouds, reefs, and waves, thereby complicating accurate detection. Dataset: The LEVIR-Ship dataset [82], comprising 3,896 optical remote sensing images acquired by the GF-1 and GF6 satellites at spatial resolution of 16 meters, was utilized to assess DynamicViss performance in small object detection. Designed to evaluate ship localization accuracy under complex background interference across expansive marine environments, the dataset features image dimensions of 512 512 pixels. It incorporates diverse maritime conditions, including variations in cloud density, terrestrial intrusion, illumination intensity, and sea surface characteristics. All annotations are provided at the shipinstance level, with target dimensions averaging 1010 pixels, and approximately 90% of instances occupying less than 2020 pixels. The dataset is partitioned into training, validation, and testing subsets containing 2,320, 788, and 788 images, respectively. Implementation Details: The FCOS [83] was employed as the baseline framework for tiny ship detection, with its original backbone and neck components substituted by pre-trained DynamicVis. All input images were resized to 1024 1024 pixels. To enhance generalization, standard data augmentation strategies, including random horizontal flipping, multi-scale resizing, and cropping, were systematically applied. The model was trained for 200 epochs using the AdamW optimizer with an initial learning rate of 2 104 and batch size of 64. Evaluation adhered to COCO evaluation metrics, with focus on average precision at an IoU threshold of 0.5 (AP50). Given the datasets characteristics, performance was further quantified using two specialized metrics: small object precision (AP50s, targeting objects below 32 32 pixels) and medium object precision (AP50m, for objects between 32 32 and 64 64 pixels). Metrics for large objects (> 64 64 pixels) were omitted due to the absence of such instances in the dataset. Results and Analysis: The proposed method was comprehensively evaluated against state-of-the-art object detection frameworks, including two-stage general detectors (e.g., Faster R-CNN [84], Sparse R-CNN [85]), single-stage methods (e.g., SSD [86], YOLO [87]), transformer-based architectures (e.g., Deformable DETR [88], ViTDet [89], DINO [90]), and specialized small object detection models (e.g., DRENet [82], IM-YOLO [91]). Experimental results, summarized in Table 3, highlight the highest-performing entries in gray. Key findings include: a) DynamicVis-L demonstrated superior performance, particularly in detecting small objects (below 32 32 pixels), attributed to its enhanced feature extraction capability for sparse small targets. b) The dynamic token selection mechanism (denoted by ) yielded significant performance gains over its counterpart without this (denoted by ). This underscores the mechanisms efficacy in prioritizing salient regions while suppressing background interference, enabling robust interpretation of sparsely distributed objects in remote sensing imagery. c) Anchor-free detectors outperformed anchor-based approaches in this task. For sparsely distributed objects, the conventional reliance on thousands of anchors can be Method AP50 AP50s AP50m Faster R-CNN SSD YOLOv3 YOLOv5s TridentNet FCOS RetinaNet YOLOX YOLOF SparseRCNN RTMDet EfficientDet CenterNet ViTDet DeformableDETR DINO CoDETR DRENet HSFNet ImYOLOv3 IM-YOLOv5s DS-YOLOv5s DSFPAPNet ORFENet HRNet ADLA DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-L DynamicVis-L DynamicVis-L 75.2 56.9 70.7 75.6 75.1 80.4 75.3 81.9 68.1 71.9 75.0 75.0 76.2 75.3 78.6 79.6 82.1 82.4 73.6 72.6 76.9 75.4 82.6 83.3 83.7 83. 78.2 80.3 82.8 80.1 81.9 84.1 74.5 55.8 71.4 - - 79.8 72.3 81.2 66.6 71.3 73.6 73.3 74.4 73.6 78.5 78.8 80.5 - - - - - - - - - 78.4 79.0 81.7 79.9 79.8 82.8 86.5 63.4 56.8 - - 82.2 75.0 89.1 85.9 73.4 89.1 90.2 80.2 90.9 75.6 91.2 95.0 - - - - - - - - - 87.7 95.3 97.3 82.9 96.0 96.8 reduced to limited set of reference points for object localization, thereby mitigating computational and optimization bottlenecks. d) Contrary to traditional assumptions, DETR-based detectors exhibited strong performance in small object detection tasks. Despite the diminutive size and sparse distribution of ship targets, the inherent global modeling capacity of transformer architectures, leveraging contextual relationships for discriminative analysis, proved advantageous in this scenario. Visualization: Fig. 11 illustrates detection performance comparisons between DynamicVis-L and baseline methods on the LEVIR-Ship test set. False positives and false negatives are demarcated by yellow and green annotations, respectively, with all results filtered using 0.3 confidence threshold. Complementary detection samples from DynamicVis-L are provided in Fig. 12. Three critical insights emerge from the visual analysis: a) The principal challenge in detecting sparse small ship targets stems from achieving effective recall of extremely small objects and those degraded by environmental interference (e.g., cloud occlusion or low illumination). This underscores the need for future research to prioritize recall enhancement strategies for such edge cases. b) DynamicVis outperforms existing methods in both localization accuracy and target recall efficiency, exhibiting fewer false negatives and spurious detections. This substantiates that selective feature discarding in dynamic token modeling retains detection fidelity while optimizing computational resource allocation. c) Despite demonstrating competitive overall performance, DynamicVis shows reduced efficacy in low-contrast scenarios, 12 Fig. 11: Detection performance comparison between DynamicVis-L and other methods on LEVIR-Ship test set, with red boxes showing true positives, yellow showing false positives, and green showing false negatives. as evidenced by residual missed detections. These limitations highlight the persistent challenges posed by heterogeneous maritime environments in large-scale remote sensing applications. Subsequent iterations could benefit from contrast-adaptive feature enhancement mechanisms. 4.3.3 Instance Segmentation Instance segmentation in remote sensing imagery entails the precise identification and delineation of individual objects or regions. This task integrates object detection and semantic segmentation methodologies, necessitating accurate pixel-level localization and discrimination of distinct instances [92]. Datasets: The performance of the proposed instance segmentation method was assessed via comparative experiments on two benchmark datasets: NWPU VHR-10 [93] and SSDD [94]. These datasets were selected for their complementary characteristics, including variations in category diversity and imaging modalities. 1) NWPU [93]: Widely adopted in remote sensing object detection, the NWPU VHR-10 dataset encompasses ten object categories: airplanes, ships, storage tanks, baseball fields, tennis courts, basketball courts, athletic fields, harbors, bridges, and TABLE 4: Comparison between the proposed methods and existing state-of-the-art approaches using the NWPU dataset. 13 Method Mask R-CNN MS R-CNN HTC SOLO v2 SCNet CondInst BoxInst Mask2Former CATNet HQ-ISNet RSPrompter-anchor RSPrompter-query DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-L DynamicVis-L DynamicVis-L APbox AP50 box AP75 box APmask AP50 mask AP75 mask 62.3 62.3 63.9 - 60.0 62.3 64.8 57.4 63.2 63.5 70.3 68. 63.7 65.0 68.5 64.4 64.9 69.1 88.3 88.6 88.9 - 87.5 87.8 89.3 75.5 89.0 89.9 93.6 90.3 88.2 89.9 90.8 89.6 89.8 93.1 75.2 73.1 75.4 - 69.1 73.3 73.0 63.7 73.8 75.0 81.0 74. 71.2 71.9 79.6 75.9 75.5 80.8 59.7 60.7 60.9 50.9 58.1 59.0 47.6 58.8 60.4 60.4 66.1 67.5 62.4 63.2 67.3 64.8 65.1 67.8 89.2 88.7 88.6 77.5 87.4 88.5 77.2 83.1 89.6 89.6 92.7 91. 87.8 88.8 91.5 90.2 90.8 91.9 65.6 67.7 64.4 54.1 62.0 62.8 51.3 63.5 65.5 64.1 70.6 74.8 68.3 67.8 73.8 68.8 69.2 75.1 TABLE 5: Comparison between the proposed methods and existing state-of-the-art approaches using the SSDD dataset. Method Mask R-CNN MS R-CNN HTC SOLO v2 SCNet CondInst BoxInst Mask2Former CATNet HQ-ISNet RSPrompter-anchor RSPrompter-query DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-L DynamicVis-L DynamicVis-L APbox AP50 box AP75 box APmask AP50 mask AP75 mask 67.7 67.8 69.3 - 66.9 68.1 62.8 62.7 67.5 66. 70.4 66.0 66.6 67.0 70.8 67.7 68.9 71.5 95.6 95.3 97.1 - 92.5 92.4 96.2 90.7 96.8 95.9 97.7 95.6 97.8 97.6 97.7 97.7 97.7 97.8 84.9 85.9 85.7 - 82.5 85.5 74.7 75.6 80.4 80. 86.2 78.7 78.0 81.3 87.1 82.0 84.2 85.2 64.3 64.9 64.1 58.5 64.9 62.5 45.2 64.4 63.9 63.4 66.8 67.3 68.3 68.3 70.8 69.6 70.1 71.0 92.6 93.3 94.4 86.2 92.6 93.4 92.3 93.0 93.7 95. 94.7 95.6 97.8 97.5 97.7 97.7 97.7 97.8 80.9 80.4 80.6 74.0 80.1 81.2 35.3 82.4 80.1 78.1 84.0 84.3 85.3 86.2 89.4 88.3 88.1 89.2 data. Additionally, the SAM-based foundation model RSPrompter [11], with its anchor-based (RSPrompter-anchor) and query-based (RSPrompter-query) variants, was also included for comparison. 1) NWPU: Experimental results on the NWPU dataset  (Table 4)  reveal the following insights, with the top three methods distinguished by darker shading: a) DynamicVis achieves competitive performance in both object detection and mask segmentation, matching SAM-based foundation models while outperforming conventional approaches. b) The dynamic token selection mechanism moderately enhances model efficacy, suggesting that prioritizing salient targets can improve instance-level interpretation. c) Compared to RSPrompter, DynamicVis exhibits superior segmentation accuracy, particularly in preserving local details and edge delineation, albeit with marginally lower detection performance. d) Scaling the network from base to large yields measurable performance gains. 2) SSDD: Experiments on the single-class SAR dataset SSDD  (Table 5)  demonstrate consistent trends. Despite inherent challenges in class discrimination within SAR data, DynamicVis surpasses RSPrompter across most metrics, regardless of metaembedded pre-training status. This underscores its adaptability to diverse remote sensing modalities. The notably high precision Fig. 12: DynamicVis-L detection examples on LEVIR-Ship test set showing ships under severe environmental interference. Left: ground-truth annotations; Right: the corresponding predictions. vehicles. The dataset contains 715 optical remote sensing images sourced from Google Earth (spatial resolution: 0.52 m) and 85 pansharpened color infrared images from the Vaihingen dataset (spatial resolution: 0.08 m). For evaluation, the data were partitioned into training and testing subsets at an 8:2 ratio. Instancelevel annotations, as specified in [95], were employed for model training and validation. 2) SSDD [94]: The SAR Ship Detection Dataset (SSDD) includes 1,160 synthetic aperture radar (SAR) images with 2,456 ship instances, exhibiting spatial resolutions between 115 m. Following an 8:2 split ratio, the dataset was randomly divided into training and testing sets. Instance masks were generated in accordance with the annotation protocol detailed in [96], ensuring pixel-level consistency for model evaluation. Implementation Details: The instance segmentation framework was constructed based on Mask R-CNN [75]. The original backbone, FPN neck, and RoI extractor were replaced with pre-trained DynamicVis. All input images were standardized to resolution of 1024 1024 pixels, and standard data augmentation strategies were applied during preprocessing. For optimization, the AdamW optimizer was utilized with an initial learning rate of 2 104, and training was conducted over 500 epochs using batch size of 64. Model performance was evaluated using the standard COCO mAP metrics, which encompass bounding box detection (APbox) and instance mask prediction (APmask), measured at multiple IoU thresholds: 0.5 (AP50) and 0.75 (AP75) for both tasks. Results and Analysis: The proposed method was evaluated against state-of-the-art instance segmentation approaches, including multi-stage frameworks (e.g., Mask R-CNN [75], Mask Scoring R-CNN [97], HTC [98]) and single-stage methods (e.g., SOLOv2 [99], Mask2Former [100]). Notably, CATNet [101] and HQ-ISNet [96] are specialized for remote sensing 14 Fig. 13: Segmentation results of DynamicVis-L on the NWPU test set. Left: ground truth annotations; Right: model predictions. in AP75 further validates its ability to generate accurate mask boundaries. Visualization: Qualitative results for DynamicVis-L are illustrated in Figs. 13 and 14, which showcase instance segmentation results on the NWPU and SSDD test datasets. Robust performance is observed across challenging scenarios, such as small-scale objects and densely clustered configurations. Notably, the model excels at capturing fine-grained local details, particularly in edge preservation, which facilitates precise object boundary delineation. These visualizations underscore DynamicVis-Ls ability to maintain structural integrity even under complex spatial arrangements, validating its effectiveness in handling diverse remote sensing contexts. 4.3.4 Semantic Segmentation Semantic segmentation constitutes pixel-level classification task that assigns categorical labels to each pixel in an image. The task requires not only robust object recognition but also precise boundary delineation, thereby presenting greater complexity than conventional image classification, which assigns singular label to the entire image. Due to its critical role in applications such as environmental monitoring, urban planning, and land-use management [102], this study evaluates the performance of semantic segmentation by focusing on two representative remote sensing tasks: building segmentation and road extraction. Datasets: The evaluation employs two single-category segmentation datasets: the Massachusetts roads dataset [103] and the Wuhan University (WHU) building dataset [104]. These datasets were selected for their contrasting morphological properties: roads Fig. 14: Segmentation results from DynamicVis-L on the SSDD test set. Left: ground truth annotations; Right: model predictions. exhibit elongated linear topologies, while buildings manifest as discrete connected regions with distinct instance-level boundaries, alongside notable differences in spatial distribution patterns. 1) Massachusetts [103]: This dataset comprises 1,171 RGB aerial images (1,500 1,500 pixels each, 2.25 km2 coverage) partitioned into 1,108 training, 14 validation, and 49 test samples. Spanning urban, suburban, and rural environments across cumulative area exceeding 2,600 km2, the test subset specifically covers 110 km2 with spatial resolution of 1 m/pixel. Ground truth annotations were generated by rasterizing OpenStreetMap road centerlines into binary masks using 7-pixel-wide unsmoothed lines, where road pixels are encoded as white and non-road pixels as black. 2) WHU [104]: The aerial photography subset, obtained from New Zealands Land Information Service, was utilized for evaluation. Original imagery at 0.075-meter resolution (450 km2 coverage) was downsampled to 0.3-meter resolution and tiled into 512 512 pixels. The final dataset contains approximately 222,000 building instances distributed across 8,189 tiles: 4,736 for training, 1,036 for validation, and 2,416 for testing. Implementation Details: The proposed semantic segmentation framework was developed based on the UperNet [76] architecture. Both the backbone and FPN neck were initialized with the pre-trained DynamicVis. Input images were standardized TABLE 6: Comparison of the proposed method with other approaches on the Massachusetts road dataset. TABLE 7: Comparison of the proposed method with other approaches on the WHU building extraction dataset. 15 Method F1 SegNet U-Net ResUNet D-LinkNet HRNetv2 DeeplabV3 DANet DeeplabV3+ Mask2Former HRNet PoolFormer PSPNet PSANet UperNet Segformer SIINet BDTNet RoadFormer GA-Net DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-B2 DynamicVis-L DynamicVis-L DynamicVis-L DynamicVis-L2 78.89 77.53 80.76 78.34 79.01 80.81 81.69 83.07 80.17 83.57 83.16 82.77 80.63 83.03 83.55 85.36 82.99 80.54 84.10 83.67 83.79 82.31 83.05 84.12 82.82 83.18 84.91 67.73 77.82 71.49 77.91 78.20 75.17 75.39 75.62 73.87 75.78 74.99 73.15 76.19 75.90 74.78 74.13 76.37 78.90 76. 73.55 71.94 76.44 76.80 73.72 73.87 76.52 76.12 72.25 77.67 75.69 78.12 78.60 77.89 78.41 79.17 76.63 79.49 78.87 77.66 78.35 79.30 78.92 79.35 79.54 79.71 80.33 78.34 78.09 79.26 79.80 78.57 78.23 79.72 80.07 IoU 57.02 63.50 61.21 64.10 64.75 63.79 64.49 65.53 62.11 65.96 65.11 63.48 64.41 65.70 65.18 65.77 66.03 66.27 67.13 64.32 63.16 65.66 66.40 64.70 64.06 66.26 67. to resolution of 512 512 pixels and subjected to comprehensive augmentation pipeline, including random resizing, cropping, flipping, and photometric distortion. For the Massachusetts road dataset, however, random resizing was omitted to preserve scale consistency, critical requirement for maintaining fixed roadwidth annotations in segmentation maps. The efficient architectural design of DynamicVis further enabled the processing of larger-scale inputs, denoted as 2 (i.e., 1024 1024 pixels), achieved by bilinear upsampling of original images. The AdamW optimizer was employed with an initial learning rate of 1 103, coupled with cosine annealing decay over 500 training epochs, utilizing batch size of 256. Model performance was quantified using precision (P), recall (R), F1-score (F1), and Intersection-over-Union (IoU), with all metrics calculated exclusively for the target class to ensure task-specific evaluation. Results and Analysis: systematic comparative analysis was conducted between the proposed DynamicVis and multiple task-specific models through numerical evaluations derived from published literature and official code re-implementations. 1) Massachusetts: The results in Table 6 reveal four principal observations: a) DynamicVis achieves superior performance relative to most existing methodologies. Quantitative evaluations using F1 and IoU metrics show that DynamicVis-L2 approaches the performance of the state-of-the-art GA-Net [105], albeit marginally lower. This discrepancy can be attributed to GANets architectural complexity, characterized by multi-level feature fusion, auxiliary supervision branches, and explicit geometric constraints. b) Token selection (denoted as ) led to minor performance degradation, primarily due to the inherent difficulty of modeling fine-grained road structures in large-area images Method F1 FCN SegNet U-Net PSPNet HRNet MA-FCN DeepLabv3 Deeplabv3+ DANet Mask2Former HRNet PoolFormer PSPNet UperNet PSANet ResUNet MAP-Net Segformer TransUNet CMTFNet RSM-SS STT DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-B2 DynamicVis-L DynamicVis-L DynamicVis-L DynamicVis-L2 92.29 93.42 94.50 93.19 91.69 94.75 95.03 94.31 95.13 92.26 94.78 95.10 94.46 95.60 94.15 94.49 93.99 94.72 94.05 90.12 95.25 - 95.59 94.91 95.43 95.82 95.36 94.45 96.39 95.87 92.84 91.71 90.88 94.21 92.85 94.92 93.12 94.53 94.12 92.22 93.64 94.24 94.38 94.21 93.96 94.71 94.82 94.42 93.07 95.21 95.12 - 94.06 93.83 95.11 95.01 94.72 94.17 94.27 95.28 92.56 92.56 92.65 93.70 92.27 94.83 94.06 94.42 94.62 92.24 94.20 94.67 94.42 94.90 94.06 94.60 94.40 94.57 93.56 92.59 95.18 94.97 94.83 94.37 95.27 95.41 95.04 94.31 95.32 95.58 IoU 86.16 86.15 86.31 88.14 85.64 90.18 88.80 89.43 89.80 85.60 89.05 89.88 89.43 90.30 88.78 89.75 89.40 89.70 87.89 86.21 90.81 90.48 90.16 89.33 90.98 91.23 90.55 89.24 91.06 91. using sparse token-based representations. c) Meta-representation learning within the token selection framework demonstrated substantial performance improvements over the baseline without token selection. This suggests that geographic prior-integrated pretraining facilitates more discriminative token activation, thereby enhancing the representational efficiency of selected tokens. d) Input resolution doubling yielded significant gains, emphasizing the critical role of high-resolution inputs in pixel-level segmentation. The increased resolution preserves finer spatial details, effectively mitigating feature loss during network downsampling. 2) WHU: As shown in Table 7, the WHU experiments exhibit trends consistent with the Massachusetts benchmark. DynamicVis achieves state-of-the-art performance on WHU, with its variants securing top-three rankings across most metrics. This performance superiority is attributed to the datasets distinct characteristics: buildings manifest as discrete, instance-level objects, where the proposed importance-aware token selection mechanism is particularly effective in modeling inter-region dependencies. In contrast, the Massachusetts datasets continuous road networks occupy extensive image regions, posing challenges for localized feature modeling. These limitations are addressable through reduced patch sizes, increased patch numbers, or resolution enhancement. Such adaptations resolve the inherent constraints of conventional selective attention mechanisms when processing elongated, interconnected structures. Visualization: Qualitative results in Fig. 15 and Fig. 16 demonstrate the efficacy of DynamicVis-L in segmenting roads 16 Fig. 15: Comparative analysis of road detection performance between the proposed method and existing approaches on the Massachusetts road dataset. True positive (TP), false negative (FN), and false positive (FP) detections are annotated with green, blue, and red markers, respectively. and buildings from the Massachusetts and WHU test datasets, respectively. Robust feature extraction capabilities are exhibited for artificial structures, with structural continuity preserved across varying geometric configurations and spatial distributions. Notably, superior robustness is observed under challenging scenarios characterized by low image clarity or significant occlusions (e.g., dense vegetation). In such cases, competing methods frequently generate spurious detections or fail to localize critical features, whereas DynamicVis-L maintains high precision. This resilience is attributed to the frameworks adaptive attention mechanisms, which dynamically prioritize discriminative regions while suppressing noise. 4.3.5 Change Detection The detection of meaningful changes in remote sensing imagery constitutes complex analytical challenge that demands advanced semantic comprehension. This process requires effective differentiation between significant spatiotemporal variations and inconsequential alterations within spatiotemporally aligned image sequences. The present study focuses on bi-temporal remote sensing image change detection, framework in which pairs of temporally separated but geographically co-registered images are processed to produce binary masks delineating regions of interest where targeted changes have occurred [106108]. Datasets: Experiments were conducted on three benchmarks: LEVIR-CD [109], WHU-CD [104], and OSCD [110], selected for their diversity in data sources, spatial scales, and resolution characteristics. These datasets serve as standard benchmarks for evaluating remote sensing foundation models. 1) LEVIR-CD [109]: This dataset contains 637 pairs of 1024 1024 Google Earth images with spatial resolution of 0.5 meters per pixel. Collected from 20 distinct geographic regions in Texas, USA, between 2002 and 2018, the corpus encompasses 31,333 annotated change instances. Following the official partitioning scheme, the data are divided into 445 training pairs, 64 validation pairs, and 128 test pairs. For computational efficiency, Fig. 16: Comparative analysis of road detection performance between the proposed method and existing approaches on the WHU building dataset. True positive (TP), false negative (FN), and false positive (FP) detections are annotated with green, blue, and red markers, respectively. all images were resized to 512 512 pixels during training and testing. 2) WHU-CD [104]: Comprising two large-scale aerial images (32507 15354 pixels) captured in 2012 and 2016 at 0.3m resolution, this dataset focuses on building changes in disaster-affected regions, with 12,796 and 16,077 building instances annotated in the respective temporal samples. To align with established evaluation protocols, the images were partitioned into non-overlapping 256 256 patches, yielding 5,947 training pairs, 744 validation pairs, and 744 test pairs. 3) OSCD [110]: This multispectral dataset consists of 24 Sentinel-2 image pairs (average dimensions 600 600 pixels) acquired between 2015 and 2018 across global regions, including Brazil, the United States, and Asia. The data exhibit heterogeneous spatial resolutions (10m, 20m, 60m) across spectral bands. Following official guidelines, 14 pairs were designated for training and 10 for validation. Images were processed into 256256 blocks with 128-pixel overlapping regions to mitigate boundary effects. Although DynamicVis supports multi-channel input, experiments were conducted using the standard three-channel (RGB) configuration for consistency. Implementation Details: The bi-temporal image pairs are processed using pre-trained DynamicVis consisting of backbone and neck. This framework generates hierarchical feature maps at multiple scales. These features are concatenated and fed into an MLP decoder to produce probabilistic change detection map. We employed composite loss function integrating crossentropy and Dice loss. Inputs denoted as 2 are upsampled to twice their original spatial dimensions. To ensure robustness against overfitting, data augmentation strategies are applied during training. The training protocol utilizes the AdamW optimizer with an initial learning rate of 5 104, combined with cosine annealing learning rate scheduler and warmup phase to stabilize convergence. The model is trained for 300 epochs with batch size of 128, leveraging distributed computing infrastructure powered by 8 NVIDIA A100 GPUs. Performance evaluation focuses on the TABLE 8: Performance comparison of various change detection methods evaluated on the LEVIR-CD test dataset. TABLE 9: Performance comparison of various change detection methods evaluated on the WHU-CD test dataset. 18 Method F1 FC-EF FC-Siam-Diff STANet SNUNet BIT GCD-DDPM ChangeStar Changen HCGMNet ChangerEx WNet C2FNet P2V-CD ChangeCLIP-ViTB BAN-BIT GFM SatLas CACo SatMAE RVSA ChangeFormer ICIFNet DMINet GASNet DDPM-CD AMTNet BiFA RSM-CD CDMamba ChangeMamba SeCo-BiT-R50 RSP-BIT-VITAEv2-S ChangeViT-T RingMo-BIT-SwinB Scale-MAE TTP MTP-IMP SkySense DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-B2 DynamicVis-L DynamicVis-L DynamicVis-L DynamicVis-L2 86.91 89.53 83.81 89.18 89.24 90.68 - - 92.96 92.97 91.16 93.69 93.32 93.68 92.83 - - - - - 92.05 - - - - 91.82 91.52 92.52 91.43 91.59 - - - 92.47 - 93.00 - - 93.22 92.89 93.65 93.79 92.73 93.00 93.52 93.97 80.17 83.31 91.00 87.17 89.37 91.24 - - 90.61 90.61 90.18 90.04 90.60 89.04 90.89 - - - - - 88.80 - - - - 89.71 89.86 89.73 90.08 88.78 - - - 91.17 - 91.70 - - 89.54 89.02 90.09 90.36 90.69 89.51 90.15 90.48 83.40 86.31 87.26 88.16 89.30 90.96 91.25 91.50 91.77 91.77 90.67 91.83 91.94 91.30 91.85 91.73 90.62 81.04 87.65 90.86 90.40 89.96 90.71 90.52 90.91 90.76 90.69 91.10 90.75 90.16 90.14 90.93 91.81 91.85 92.07 92.10 92.54 92.58 91.34 90.92 91.82 92.05 91.69 91.22 91.90 92.32 IoU 71.53 75.91 77.39 78.83 80.68 83.56 83.92 - 84.79 - 82.93 84.89 - 83.99 84.93 - - - - - 82.47 81.75 82.99 83.48 83.35 83.08 82.96 83.66 83.07 82.09 - - 84.86 - - 85.60 - - 84.06 83.32 84.75 85.19 84.66 83.86 84.81 85. change class foreground, with metrics including Precision (P), Recall (R), IoU, and F1 score (F1). Results and Analysis: comprehensive comparative evaluation was conducted to benchmark the performance of the proposed DynamicVis. The evaluation framework encompassed five methodological categories: 1) traditional convolutional architectures (e.g., STANet [109], SNUNet [111], and HCGMNet [112]); 2) Transformer-based approaches (e.g., BIT [113] and ChangeFormer [114]); 3) Mamba-based architectures (e.g., RSMCD [115], CDMamba [60], and ChangeMamba [61]); 4) diffusion probabilistic models (GCD-DDPM [116] and DDPM-CD [117]); and 5) pre-trained foundation models, which were further divided into two subgroups: those trained on natural images (ChangeCLIPViTB [6], BAN-BIT [49], and TTP [118]) and those tailored for remote sensing (SeCo-BiT-R50 [119], RingMo-BIT-SwinB [13], MTP-IMP [42], and SkySense [10]). Performance metrics were derived either from published results or through re-implementation Method F FC-EF FC-Siam-Diff STANet SNUNet CDNet BIT ChangeFormer GCD-DDPM CEECNet CGNet WNet P2V-CD DDPM-CD FresUNet ICIFNet DMINet GASNet EATDer MTCNet MSCANet RSM-CD ChangeMamba CDMamba PA-Former DARNet FMCD ChangeCLIP-ViT-B ChangeViT-T MTP-IMP DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-B2 DynamicVis-L DynamicVis-L DynamicVis-L DynamicVis-L2 78.86 84.73 79.37 85.60 91.75 86.64 90.09 92.79 95.57 94.47 92.37 95.48 - 86.55 92.98 93.84 - 91.32 75.10 91.10 93.37 94.21 95.58 94.28 91.99 96.02 96.02 - - 96.43 95.63 96.56 96.82 96.41 96.01 96.41 96.78 78.64 87.31 85.50 81.49 86.89 81.48 84.85 92.29 92.04 90.79 90.15 89.47 - 77.68 85.56 86.25 - 88.74 91.90 89.86 90.42 90.94 92.01 90.38 91.17 92.99 93.58 - - 89.83 89.23 91.48 91.86 90.04 89.64 90.83 92. 78.75 86.00 82.32 83.49 89.25 83.98 87.39 92.54 93.77 92.59 91.25 92.38 92.65 81.88 88.32 88.69 91.75 90.01 82.65 90.47 91.87 92.55 93.76 92.29 91.58 94.48 94.78 94.53 95.59 93.01 92.61 93.73 94.27 93.12 92.71 93.60 94.79 IoU 64.95 75.44 69.95 71.67 80.59 72.39 77.61 86.52 88.23 86.21 83.91 - 86.31 69.32 79.24 79.68 84.76 81.97 70.43 82.60 84.96 86.13 88.26 85.69 84.46 - 90.08 89.63 - 86.94 86.04 88.06 89.52 87.11 86.91 87.91 89.85 using official source code, with the top three methods distinguished by gray background for clarity. 1) LEVIR-CD: comparative analysis between DynamicVis and SOTA algorithms was conducted, with experimental results summarized in Table 8. Three key observations show: a) Despite employing only millions of parameters, DynamicVis achieves performance comparable to SOTA models with billions of parameters (e.g., RingMo, TTP, and MTP) across multiple evaluation metrics. This underscores the efficacy of the proposed dynamic visual perception mechanism in addressing sparse distribution priors inherent to building change detection tasks. Following extensive pre-training, the model consistently ranks among the top three performers for most metrics, matching the capabilities of foundational models. b) While the dynamic perception mechanism introduces marginal performance decline compared to dense modeling approaches, it significantly enhances both training efficiency and deployment practicality. Notably, when pre-trained the model surpasses with general remote sensing knowledge, counterparts lacking the selective modeling mechanism by substantial margin. c) Elevated input resolution further improves detection accuracy beyond most existing methods, with minimal computational overhead due to the integrated state space model and selective mechanism. TABLE 10: Performance comparison of various change detection methods evaluated on the OSCD test dataset. TABLE 11: Performance comparison on the fMoW test set. 19 Method R F1 FC-EF DTCDSCN SNUNet ChangeFormer BIT ICIFNet DMINet SwiMDiff GASNet AMTNet EATDer ChangeViT-T ChangeViT-S GASSL SeCo CACo MoCo-v2 Swin-22k ViT-22k SatMAE DINO-MC MTP-IMP SpectralGPT SpectralGPT+ MATTER GFM SkySense DynamicVis-B DynamicVis-B DynamicVis-B DynamicVis-B2 DynamicVis-L DynamicVis-L DynamicVis-L DynamicVis-L2 - - - - - - - 63.60 - - - - - - 57.71 62.87 64.49 46.88 52.09 55.18 - - 51.65 52.39 61.80 58.07 - 75.08 71.21 74.28 77.52 64.83 74.42 72.77 79. - - - - - - - 40.90 - - - - - - 49.23 44.49 30.94 59.28 52.37 50.54 - - 56.15 57.20 57.13 61.67 - 44.37 41.03 45.24 48.81 49.20 42.23 46.99 48.36 48.89 36.13 27.02 38.22 29.58 23.03 42.23 49.60 10.71 10.25 54.23 55.13 55.51 46.26 49.82 52.11 40.71 52.35 52.23 52.76 52.70 55.61 53.51 54.29 59.37 59.82 60.06 55.78 52.06 56.23 59.90 55.95 53.88 57.11 60.25 IoU - 22.05 15.62 23.62 17.36 13.02 26.76 - 5.66 5.40 36.98 38.06 38.42 - - - - - - - - - - - - - - 38.67 35.19 39.11 42.75 38.84 36.88 39.96 43.16 2) WHU-CD: As shown in Table 9, the WHU-CD dataset exhibits greater complexity than LEVIR-CD, characterized by larger sample size, higher positive pixel ratios, and superior spatial resolution. These attributes amplify challenges for dynamic visual selection mechanisms, leading to more pronounced performance gap relative to SOTA benchmarks. However, experiments reveal that DynamicViss operational efficacy can be markedly improved through two strategies: input resolution optimization and comprehensive pre-training protocols. Higher-resolution inputs, combined with robust pre-training, enhance feature discrimination in scenarios involving complex spatial-spectral variations, validating the frameworks adaptability across diverse remote sensing contexts. 3) OSCD: The OSCD dataset, distinguished by its smaller scale and lower resolution, captures macroscopic ground object changes and has been widely adopted for validating foundational remote sensing models. Table 10 highlights two critical findings from comparisons with SOTA methods: a) DynamicVis outperforms existing algorithms on data adhering to sparse distribution priors, achieving the highest composite metrics (particularly F1 and IoU scores). b) The model exhibits significantly higher precision than recall, indicating conservative yet accurate identification of changed pixels. This observation indirectly corroborates the inherent sparsity of positive samples in the dataset. Method ResNet-50 ViT-B DynamicVis-B (CE) DynamicVis-B (MIL) DynamicVis-L (CE) DynamicVis-L (MIL) R F1 95.86 96.62 96.45 97.49 97.05 97.91 95.61 96.87 96.54 97.03 96.72 97. 95.56 96.68 96.37 97.19 96.84 97.87 Visualization: The performance of DynamicVis is validated through visual analysis on the widely adopted LEVIR-CD dataset  (Fig. 17)  . The model demonstrates robust adaptability to diverse spatiotemporal conditions, including variations in illumination and seasonal transitions, while maintaining high-precision change detection. Despite inherent annotation errors in the dataset, such as omissions and misclassifications, DynamicVis achieves accurate change delineation with minimal interference from these inconsistencies. This capability is particularly pronounced in challenging scenarios involving subtle or extreme changes, which are frequently undetected in manual annotations, underscoring the models advanced perceptual robustness. Furthermore, the framework exhibits exceptional edge preservation and fine-grained segmentation accuracy, outperforming existing methods in capturing intricate structural details. 4.3.6 Region Classification Region classification represents mid-level semantic understanding task that differs fundamentally from image classification. While the latter assigns semantic labels to entire images, region classification categorizes predefined spatial segments within an image according to their semantic content. This task is further distinguished from object detection as it focuses exclusively on classifying the contents within roughly annotated object or region boxes, rather than performing box regression. The models region classification capabilities were evaluated using the fMoW dataset. Datasets: The fMoW dataset [33] provides region-level annotations, which were utilized for both pre-training and evaluation purposes in this study. Performance evaluation of region classification was conducted using the datasets test split. The training protocol and data partitioning scheme were maintained consistent with the pre-training process in this paper, whereby 20,000 images were randomly sampled to constitute the test set, with the remaining data allocated for training purposes. Implementation Details: The training protocol followed the implementation specifications outlined in the methodology section. To evaluate the influence of loss function selection on model efficacy, comparative analysis was conducted by employing Cross-Entropy (CE) and Multiple Instance Learning (MIL) loss functions. Model performance was assessed using standard evaluation metrics, including multi-class averaged Precision (P), Recall (R), and F1-score (F1). Results and Analysis: Both base and large model configurations were systematically tested using CE and MIL loss functions. The proposed DynamicVis was benchmarked against established baselines, including ResNet-50 and ViT-B, with quantitative comparisons detailed in Table 11. When trained with CE loss, DynamicVis achieved performance parity with the ViT-B architecture while surpassing the ResNet-50 baseline by significant margin. Notably, the MIL-trained variant exhibited enhanced latent space 20 Fig. 17: Detection samples from DynamicVis on the LEVIR-CD test set. The red boxes highlight the capability of DynamicVis to accurately depict changes in complex scenes, some of which have even been overlooked by human annotators. TABLE 12: Comparison results are presented as mAP at top-20 (mAP@20) for the BigEarth and ForestNet datasets. The 64-bit hash implementation utilizes the described trivial hashing, wherein embedding values are averaged for each bit position. Model Band Method BE-43 BE-19 FN-12 FN-4 Prithvi SatMAE all all Prithvi RGB ViT-B RGB DynamicVis-B RGB DynamicVis-L RGB Embedding Binary emb. 64-bit hash Embedding Binary emb. 64-bit hash Embedding Binary emb. 64-bit hash Embedding Binary emb. 64-bit hash Embedding Binary emb. 64-bit hash Embedding Binary emb. 64-bit hash 97.62 97.44 92.58 94.78 89.39 79. 92.15 91.38 82.60 89.31 88.71 79.01 94.65 94.07 88.66 94.98 94.28 89.15 97.98 97.83 93.44 95.59 90.40 80. 93.17 92.43 84.45 90.21 89.70 81.54 95.59 95.04 89.95 95.82 95.46 90.58 44.51 43.28 41.49 37.61 36.49 30. 38.65 38.11 32.58 38.92 39.19 33.60 44.15 43.94 36.50 44.86 44.71 36.98 60.76 59.85 55.93 52.94 53.04 47. 53.85 53.31 48.20 56.49 57.01 49.63 62.92 62.64 56.31 63.59 63.28 56.86 separability between semantic categories, which correlated with improved overall metrics. Empirical analysis further revealed that systematically increasing the models depth and width parameters led to measurable improvements in classification accuracy. 4.3.7 Image Retrieval The primary objective of image retrieval is to identify images that exhibit visual or semantic similarity to given input image or text-based query. In the context of remote sensing, the exponential growth of data acquisition has led to significant challenges in managing information overload. To address this, remote sensing image retrieval has emerged as critical tool for the efficient organization of large-scale data archives and the rapid extraction of relevant imagery. typical image retrieval workflow involves four sequential stages: 1) feature extraction, 2) feature representation, 3) feature matching, and 4) result ranking [120]. In this study, pre-trained foundational DynamicVis is proposed, which leverages zero-shot learning framework to autonomously derive semantic visual features from both query inputs and database entries, thereby enabling efficient image retrieval. Datasets: This study employs two publicly available multispectral datasets with multiclass annotations: BigEarthNet [121] and ForestNet [122]. 1) BigEarthNet [121]: The BigEarthNet dataset integrates Sentinel-1 and Sentinel-2 satellite imagery, with each sample spatially resolved to 120 120 pixels across 13 spectral bands. It features two distinct multilabel annotation schemes comprising 19 and 43 classes, respectively, encompassing diverse land-use categories such as mixed forests, water bodies, and urban infrastructure. In alignment with established experimental protocols from prior work, only the Sentinel-2 RGB bands are utilized in this experiment. The official validation set is adopted as the query set, while the test set functions as the retrieval database. 2) ForestNet [122]: The ForestNet dataset contains Landsat loss events, with each 8 imagery designed to monitor forest image spanning 332 332 pixels. Annotations include 12 finegrained categories hierarchically organized under 4 superclasses, distinguishing deforestation drivers such as timber plantations and small-scale agriculture. Consistent with the preprocessing applied to BigEarthNet, RGB bands are exclusively retained for analysis. The validation partition is designated as the query set, and the test partition serves as the retrieval database. Implementation Details: Semantic feature vectors are extracted from images in zero-shot manner using pre-trained foundation model and stored offline for subsequent processing. The similarity between query and database vectors is computed via Hamming distance. To optimize retrieval efficiency and minimize memory consumption, hash encoding is introduced to compress the embedding vectors further. Two primary approaches are adopted: 1) direct binarization of the embedding vectors and 2) average pooling of the embeddings to fixed length (e.g., 64 dimensions) followed by binarization. The evaluated models include the proposed DynamicVis, 21 Fig. 18: Examples from the ForestNet-12 and BigEarthNet-43 datasets: query images (left) with their corresponding labels and retrieved images (right) using DynamicVis with 64-bit hash codes. The retrieval quality is indicated by color-coded frames: green frames denote correct matches, red indicates incorrect, and orange represents partial matches (with numbers showing the count of matching labels). data, the additional infrared channel is populated by averaging the RGB values to align with Prithvi-100Ms input specifications. SatMAE, ViT-B/16 architecture, is pre-trained on ten Sentinel-2 bands at 10-meter resolution and accepts 96 96 pixel inputs. The standard ViT-B/16 model, pre-trained on ImageNet-21K, is also included for comparison. Retrieval experiments are performed using three representations: 768-dimensional embeddings, binary embeddings, and 64bit hash codes. Unless specified, inputs are restricted to RGB channels. To accommodate varying input size requirements across models, images are resampled via bilinear interpolation, potentially altering spatial resolutions relative to the original pretraining configurations. Notably, SatMAE was not trained on Landsat data, while Prithvi-100M was pre-trained exclusively on Landsat 8 and Sentinel-2 imagery from the United States. In contrast, the evaluation datasets BigEarthNet and ForestNet encompass European and Indonesian regions, respectively. Following established evaluation protocols, mAP is calculated over the top 20 retrieved images. For multilabel datasets, overlapping labels between query and retrieved images are treated as positive matches. The validation set of each dataset serves as the query set, while the test set functions as the retrieval database. Results and Analysis: The comparative performance of the proposed methodology against existing approaches is summarized Fig. 19: t-SNE visualization of the ForestNet-4 test set, with classes differentiated by color coding, comparing embeddings generated by ViT against those produced by DynamicVis. Prithvi-100M [123], SatMAE-ViT-B [45], and ViT [15]. For feature extraction, only the backbone networks of these models are utilized, with extracted features aggregated into 1D vectors through mean pooling. Prithvi-100M, ViT with 100 million parameters, is designed for 224 224 pixels. It processes six spectral bands and is pre-trained on 10-meter resolution imagery from Landsat 8 and Sentinel-2 satellites. When handling RGB-only in Table 12, which evaluates mAP at the top 20 retrieval results. Key findings from this analysis are outlined as follows: a) When restricted to RGB spectral inputs, the DynamicVis framework achieves superior performance, surpassing both the natural image pre-trained ViT and the multispectral remote sensing foundation model Prithvi. These results demonstrate the robust feature representation capabilities of DynamicVis under zero-shot evaluation scenarios. b) While the Prithvi model exhibits enhanced performance on most datasets by leveraging auxiliary spectral bands, the RGB-based DynamicVis framework remains highly competitive. Notably, on the ForestNet dataset, DynamicVis achieves parity with or exceeds Prithvis performance despite being limited to RGB inputs, underscoring its efficacy in scenarios with constrained spectral information. c) Experiments on embedding compression reveal that binary quantization of the original feature vectors incurs negligible performance degradation while significantly improving storage efficiency and computational speed. This suggests that binarization preserves the critical semantic relationships within the feature space. In contrast, 64-bit quantization introduces more pronounced performance declines, indicating its application should be contextually justified by specific operational trade-offs. d) consistent performance gap is observed between the multilabel BigEarth dataset and the single-label ForestNet benchmark. This discrepancy is attributed to the inherent flexibility of multilabel classification paradigms, which tolerate partial prediction errors, and the heightened discriminative demands of ForestNets fine-grained subclass retrieval task. The latter necessitates more precise feature extraction to distinguish semantically proximate categories. Visualization: comparative t-SNE visualization of feature embeddings and hash-encoded representations generated by ViT and DynamicVis-B architectures is presented in Fig. 19. The analysis reveals that DynamicVis-B produces more structured latent space distribution compared to ViT, conclusion consistent with the quantitative superiority demonstrated in Table 12. Notably, floating-point embeddings, binary embeddings, and 64bit trivial hashes exhibit enhanced cluster separation and intraclass compactness relative to 32-bit trivial hashes, suggesting that shorter hash lengths, while computationally efficient, compromise semantic discriminability. retrieved instances exhibit partial Further insights are provided in Fig. 18, which illustrates retrieval results using trivial hashing on DynamicVis-B generated embeddings. The model demonstrates stronger performance on the BigEarthNet-43 dataset than on ForestNet-12. For BigEarthNet43, label correspondence, whereas ForestNet-12 results include erroneous matches. These inaccuracies predominantly occur in cases where non-target images share similar spatial patterns and spectral characteristics with query samples, underscoring the inherent challenges of finegrained retrieval in remote sensing scenarios characterized by high inter-class similarity. 4.4 Ablation Study This section conducts comprehensive ablation analysis and parametric evaluation to systematically assess the efficacy of individual components within the proposed model. Each architectural module and parametric configuration is methodically isolated during experimentation to quantify their individual contributions to the models overall performance. To ensure experimental efficiency, all ablation studies and parametric optimizations were performed 22 TABLE 13: Inference efficiency across various models is compared at an input resolution of 512 512. Model ResNet18 ResNet50 ResNet101 ViT-B ViT-L DynamicVis-B DynamicVis-B DynamicVis-L DynamicVis-L Max BS 1200 642 608 268 208 186 998 132 786 Params. (M) 11.69 25.56 44.55 87.20 305. 36.76 36.77 91.27 91.29 FLOPs (G) 9.50 21.47 40.92 87.76 362.00 54.28 30.07 151.00 82.31 Throughput (Sampels/s) 1200 340 200 86 90 196 45 92 exclusively on the AID dataset, which was selected for its larger scale and relevance to remote sensing image classification tasks. This targeted strategy facilitates the efficient identification of optimal network architectures and hyperparameter settings while preserving experimental rigor and ensuring alignment with the studys research objectives. 4.4.1 Effects of Different Model Versions The model scale, defined by both depth (number of layers) and width (feature dimensions), is critical in determining efficiency and accuracy. To systematically investigate this relationship, two variants (base and large) were developed within our framework; their respective depth and width configurations are detailed in Table 1. Empirical evaluations conducted on the UC Merced and AID datasets (see Table 2) confirm that model performance improves with increasing size. Table 13 presents comprehensive efficiency analysis across various architectures by comparing maximum inference batch size, parameter count, floating-point operations (FLOPs), and inference throughput under standardized input dimensions of 512 512 pixels on an NVIDIA L20 48GB GPU. The findings reveal several key observations: a) Convolutional architectures exhibit significant speed advantages. For instance, although ResNet101 and DynamicVis-B have comparable parameter counts and computational demands, ResNet101 attains marginally higher throughput despite supporting only half the batch size of DynamicVis-B. This increased efficiency is attributed to the inherently parallelizable structure of convolutional operations and their optimized implementations in modern deep learning frameworks. b) ViTs consistently underperform across all metrics, thereby underscoring the need for architectural innovations to enhance efficiency in general-purpose artificial intelligence systems. c) DynamicVis variants equipped with dynamic token selection demonstrate 2 increase in throughput and 6 improvement in maximum batch size compared to their baseline counterparts. When combined with the accuracy improvements reported in Table 2, these results validate that DynamicVis effectively balances computational efficiency with performance accuracy, positioning it as promising foundation model for scalable vision understanding. 4.4.2 Effects of Different Input Resolutions This study systematically examines the impact of input image resolution variations on computational efficiency. Experiments were conducted with batch size of 1, with model efficiency quantified through two key metrics: inference latency (ms) and GPU memory consumption (MB). The results  (Table 14)  reveal two principal TABLE 14: Latency (ms) and GPU memory usage (MB) with varying input resolutions. The top row for each model shows processing latency, while the bottom row indicates GPU memory consumption. OOM denotes out of memory. Model Params. (M) FLOPs (G) ResNet101 44.55 40. ViT-B 87.20 87.76 DynamicVis-B 36.76 54. DynamicVis-B 36.77 30.07 128 256 1024 2048 4096 5.13 300.96 3.11 340.68 9.44 160. 14.50 155.10 5.17 309.59 3.16 349.71 9.50 202.57 14.76 160.07 5.30 328. 9.15 454.98 16.01 371.98 15.70 188.71 13.26 470.84 67.20 1050.65 307.28 3690. 109.61 1958.55 1581.18 25253.80 OOM OOM 45.76 1056.73 27.97 319.25 220.69 3793. 97.18 833.21 1247.37 14743.85 480.61 2369.06 TABLE 15: Performance with various scanning paths. Forward Reverse Shuffle Mean/Gate F1 - Mean Gate Mean Gate 92.17 94.11 93.83 94.12 94.14 91.74 93.82 93.69 93.91 93.84 91.82 93.81 93.65 93.96 93.90 data. As summarized in Table 15, multiple scanning strategies were systematically evaluated: Forward, Reverse, and Shuffle denote sequential, inverted, and random scanning paths, respectively, while Mean and Gate represent aggregation via average pooling and adaptive weighted averaging (with coefficients derived from token prediction values, as implemented in RSMamba [25]). Key experimental observations include: a) The exclusive use of the Forward path yielded suboptimal performance, underscoring the incompatibility of strictly causal SSM characteristics with noncausal image data. b) The Gate mechanism failed to produce substantial performance gains, prompting the adoption of Mean aggregation for architectural simplicity. c) The inclusion of the Shuffle path provided marginal improvements, contrasting with results reported in RSMamba. This divergence stems from our Selective Token Incremental Modeling paradigm, which inherently incorporates token shuffling by prioritizing top-k tokens based on computed importance scores rather than fixed positional order. Based on these findings and to preserve architectural efficiency, the final implementation employs only Forward and Reverse scanning paths combined with Mean aggregation for state space modeling of selectively processed tokens. 4.4.4 Effects of Various Token Reduction Ratios Selective Token Incremental Modeling forms the core mechanism of DynamicVis, where the quantity of selected tokens critically influences both computational efficiency and task performance. In the architectural design, principled approach is adopted: shallow layers processing high-resolution features are assigned elevated dropout rates, while deeper layers handling lowresolution features employ progressively reduced dropout rates. This strategy is systematically implemented across the models four distinct processing stages, as detailed in Table 16. Memory consumption analysis was conducted using 512 512 pixel input images with batch size 1, yielding three key observations: a) For image classification tasks, selective token modeling maintains comparable accuracy while significantly improving inference efficiency. b) Extreme token reduction (as demonstrated in the final row) induces measurable performance degradation that remains within acceptable limits for classification, likely due to the tasks Fig. 20: Latency (ms) of different models across various input resolutions. Fig. 21: GPU memory usage (GB) across different models with varying input resolutions. findings. a) The DynamicVis-B demonstrates superior efficiency in both latency and memory utilization compared to Transformerbased ViT architectures, particularly as input resolution increases. b) The dynamic token selection mechanism in DynamicVis-B yields significant efficiency improvements over its non-dynamic counterpart (DynamicVis-B), achieving performance parity with ResNet101. Fig. 20 illustrates the latency patterns across different models as input resolution increases, while Fig. 21 depicts the corresponding GPU memory utilization patterns. These visualizations clearly demonstrate that ViTs exhibit quadratic computational growth with increasing resolution, whereas the alternative methods maintain linear growth characteristics. Moreover, DynamicVis-B demonstrates progressively greater memory efficiency advantages over ResNet101 as input resolution increases. These findings suggest that DynamicVis-B holds considerable potential for processing ultra-high resolution imagery (approximately 104 104 pixels), making it particularly suitable for applications in largescale remote sensing image processing where efficient handling of extensive visual data is required. 4.4.3 Effects of Different Scanning Paths The sequential modeling strengths of SSMs were initially demonstrated in causal language modeling tasks. However, their application to image data faced inherent challenges due to the non-causal nature of visual information. To address these limitations, recent advancements have focused on optimizing the scanning strategies of SSMs to better accommodate non-causal data dependencies. In this work, we introduce dual-path SSM scanning block within our Selective Token Incremental Modeling framework, explicitly designed to tackle the non-causal modeling requirements of image TABLE 16: Performance and GPU memory utilization (MB) on the AID dataset for varying token reduction ratios. Reduction Ratio Usage (MB) R F1 [0, 0, 0, 0] [3/4, 1/2, 0, 0] [7/8, 3/4, 1/2, 0] [15/16, 7/8, 3/4, 1/2] [31/32, 15/16, 7/8, 3/4] 379.99 216.58 188.71 176.32 174.64 94.41 94.32 94.11 93.74 92.46 94.22 93.95 93.82 93.53 92. 94.17 94.05 93.81 93.57 92.17 TABLE 17: Performance analysis of mixer block with spatial and channel modeling. Spatial Channel Serial/Parallel R F1 - - Serial Parallel 94.11 83.31 94.52 94.22 93.82 81.69 94.25 94.01 93.81 81.91 94.29 94.04 inherent low-information discrimination requirements. However, dense prediction tasks exhibit greater sensitivity to excessive token reduction. c) Memory consumption demonstrates decreasing relationship with token reduction under batch size 1 conditions, though this trend diminishes significantly in extreme reduction scenarios where model parameter storage dominates memory allocation rather than computational data. The token reduction ratios [7/8, 3/4, 1/2, 0] were ultimately selected to maintain cross-task compatibility. Future research could explore task-specific ratio configurations to better optimize the efficiency-accuracy tradeoff for particular applications. 4.4.5 Effects of Spatial and Channel Scanning In the design of network architecture, we explore channelwise selective modeling strategy to enhance feature representation capabilities. Each feature map channel is processed as an independent token, with dimensionality reduction implemented through lightweight MLP to facilitate efficient channel-wise feature modeling. As demonstrated in Table 17, the spatial and channel configurations refer to the application of dynamic token selectivity restricted to either spatial or channel dimensions, respectively, while serial and parallel configurations describe the sequential or concurrent arrangement of spatial and channel modeling within processing block. The experimental results reveal two critical insights: a) Exclusive channel-based selective modeling significantly degrades model performance. b) Combined spatial-channel scanning yields only negligible accuracy improvements while incurring twice computational overhead. These findings motivate the design of DynamicVis, which strategically employs dynamic token modeling solely in the spatial dimension to capture global dependencies. 4.4.6 Effects of the Global Semantics During the selective modeling phase, DynamicVis enhances the models ability to focus on key foreground regions while preserving contextual awareness by concurrently conducting incremental information modeling on both selected regional semantics and global semantics. To achieve this, row-wise pooling is employed to generate condensed feature embeddings that encapsulate global image semantics, which are then integrated into the selected key regional tokens. As shown in Table 18, comparative analysis TABLE 18: Effects of the global semantics tokens. 24 Position R F1 - head head tail mid 93.11 94.11 94.03 94.13 92.75 93.82 93.69 93.85 92.85 93.81 93.76 93. TABLE 19: Performance on token loading balance. fixed indicates the use of Gumbel noise sampling at constant scale, while decay signifies gradual reduction in noise scale as training progresses. Strategy Scale F loss fixed fixed fixed fixed fixed decay - 0.4 0.2 0.1 0.05 0 0.1 92.92 92.57 93.42 93.87 93.15 92.72 94.11 92.53 92.35 92.72 93.49 92.61 92.36 93.82 92.60 92.37 92.89 93.58 92.66 92.45 93.81 evaluates the impact of global semantics inclusion and their positional insertion on model performance. Experimental results reveal two critical observations: a) The integration of global semantics significantly enhances model performance, underscoring the complementary nature of global contextual information and salient regional foreground features. b) The model demonstrates minimal sensitivity to the positional placement of global semantics. Given that key local tokens are prioritized through top-k selection, the global token is positioned at the beginning of the region token sequence in our framework, reflecting its equivalent importance to the highest-ranked local tokens. 4.4.7 Token Loading Balance The proposed token selection method bears conceptual resemblance to the Mixture-of-Experts (MoE) architecture, where effective token load balancing significantly influences model performance. Improper handling of this balance can result in model collapse. In conventional MoE frameworks, balanced loading loss is typically implemented to regulate expert activation patterns and their distribution across calculation nodes. However, DeepSeek diverges from this paradigm by employing rule-based, loss-free strategy for expert allocation. Similarly, our approach operates without explicit loss mechanisms, eliminating cross-node communication while focusing exclusively on balancing selective token activation. During training, controlled noise is introduced to the importance probabilities of tokens to equalize their load distribution likelihoods. As demonstrated in Table 19, experimental results indicate that this methodology achieves performance comparable to conventional approaches without augmenting computational complexity. Three critical insights emerge from this analysis: a) The conventional balanced loading loss exhibits limited efficacy in this context, potentially reflecting perceptual tasks inherent requirement for deterministic activation patterns rather than tolerance for activation biases observed in generative applications. b) Precise calibration of noise amplitude proves essential, as excessive perturbation induces random token selection while insufficient variation impedes effective load balancing. c) Progressive decay of noise amplitude during training yields performance improvements, suggesting distinct operational phases: early training stages benefit from uniform token selection to facilitate the discovery of critical tokens, while subsequent phases require deterministic selection to reinforce task-specific representation learning. 4.5 Discussions DynamicVis exhibits superior performance across nine key remote sensing visual interpretation tasks, demonstrating notable efficiency and operational versatility. The framework achieves performance parity with large-scale models in sparse target recognition domains, particularly small object detection and change detection, highlighting the effectiveness of its dynamic visual perception mechanism for conventional remote sensing applications. By strategically integrating the linear global modeling capabilities of SSMs for input processing, this architecture presents promising solution for interpreting large-width remote sensing imagery. implementation utilizes supervised learning through meta-category representations to enable efficient information perception and extraction, though this methodology introduces constraints on data scalability. Future iterations could address this limitation by incorporating mask modeling and crossspatiotemporal contrastive learning, potentially enabling the development of large-scale unsupervised frameworks capable of integrating fundamental geospatial knowledge. The current While the system benefits from dynamic visual perception strategies, its performance in fully dense prediction tasks remains suboptimal, suggesting the necessity of reducing token reduction ratios. Although foundational models exhibit robust generalization capabilities, the challenge of balancing computational efficiency with high precision persists when addressing diverse customized remote sensing applications using unified architecture. Current methodologies still require task-specific data and architectural fine-tuning, indicating opportunities for developing dynamically adaptive networks with task-oriented parameter optimization. Ultimately, the field requires foundational models that combine three critical attributes: a) strong representational capacity for multi-source, multi-modal understanding, b) broad generalizability across heterogeneous tasks, and c) low-resource inference requirements. Such advancements are essential to address the growing challenges posed by massive-scale remote sensing datasets and their complex analytical demands."
        },
        {
            "title": "5 Conclusion\nThis paper presents DynamicVis, a dynamic visual perception\nfoundation model for high-resolution remote sensing imagery.\nDrawing inspiration from the human visual system’s selective at-\ntention mechanism, the proposed framework is designed to capture\ngeneral visual-semantic representations by adaptively focusing on\nsalient regions. At its core, the architecture integrates a dynamic\nregion-aware backbone based on SSMs, which strikes an optimal\nbalance between local detail extraction and global contextual mod-\neling, enabling computationally efficient and scalable encoding\nof high-resolution geospatial data. The model is trained via a\nmeta-embedding MIL paradigm on a large-scale dataset compris-\ning millions of regional-level annotations. Extensive evaluations\nacross nine key remote sensing tasks demonstrate DynamicVis’s\nsuperior generalization capabilities. Notably, the model achieves\nperformance parity with ViT-based large models in sparse target\ninterpretation tasks such as small object detection and change\nanalysis, while exhibiting significantly reduced computational",
            "content": "25 demands. When processing 20482048 pixel images, DynamicVis requires only 97 ms latency (6% of ViTs) and 833 MB memory (3% of ViTs). Rigorous benchmarking confirms that the proposed framework consistently outperforms transformer-based methods across diverse granularity requirements, establishing new stateof-the-art results in tasks necessitating hierarchical visual cue integration. References [1] J. Li, D. Hong, L. Gao, J. Yao, K. Zheng, B. Zhang, and J. Chanussot, Deep learning in multimodal remote sensing data fusion: comprehensive review, International Journal of Applied Earth Observation and Geoinformation, vol. 112, p. 102926, 2022. L. Liu, Z. Zou, and Z. Shi, Hyperspectral remote sensing image synthesis based on implicit neural spectral mixing models, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 114, 2022. Z. Yu, C. Liu, L. Liu, Z. Shi, and Z. Zou, Metaearth: generative foundation model for global-scale remote sensing image generation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. K. Chen, W. Li, J. Chen, Z. Zou, and Z. Shi, Resolution-agnostic remote sensing scene classification with implicit neural representations, IEEE Geoscience and Remote Sensing Letters, 2022. B. Chen, K. Chen, M. Yang, Z. Zou, and Z. Shi, Heterogeneous mixture of experts for remote sensing image super-resolution, arXiv preprint arXiv:2502.09654, 2025. S. Dong, L. Wang, B. Du, and X. Meng, Changeclip: Remote sensing change detection with multimodal vision-language representation learning, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 208, pp. 5369, 2024. B. Chen, K. Chen, L. Liu, Z. Shi, and Z. Zou, Leveraging language-aligned visual knowledge for remote sensing image spectral super-resolution, in 2024 14th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS). R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., On the opportunities and risks of foundation models, arXiv preprint arXiv:2108.07258, 2021. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. IEEE, 2024, pp. 15. [2] [3] [4] [5] [6] [7] [8] [9] [10] X. Guo, J. Lao, B. Dang, Y. Zhang, L. Yu, L. Ru, L. Zhong, Z. Huang, K. Wu, D. Hu et al., Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 27 67227 683. [11] K. Chen, C. Liu, H. Chen, H. Zhang, W. Li, Z. Zou, and Z. Shi, Rsprompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 117, 2024. S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su et al., Grounding dino: Marrying dino with grounded pre-training for open-set object detection, in European Conference on Computer Vision. Springer, 2024, pp. 3855. [12] [13] X. Sun, P. Wang, W. Lu, Z. Zhu, X. Lu, Q. He, J. Li, X. Rong, Z. Yang, H. Chang et al., Ringmo: remote sensing foundation model with masked image modeling, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 122, 2022. [14] D. Hong, B. Zhang, X. Li, Y. Li, C. Li, J. Yao, N. Yokoya, H. Li, P. Ghamisi, X. Jia et al., Spectralgpt: Spectral remote sensing foundation model, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [16] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou, Training data-efficient image transformers & distillation through attention, in International conference on machine learning. PMLR, 2021, pp. 10 34710 357. [17] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 012 10 022. [18] Z. Feng and S. Zhang, Efficient vision transformer via token merger, IEEE Transactions on Image Processing, vol. 32, pp. 41564169, 2023. [20] [19] M. S. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova, Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021. J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, Advances in Neural Information Processing Systems, vol. 35, pp. 23 71623 736, 2022. [21] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [22] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh, Dynamicvit: Efficient vision transformers with dynamic token sparsification, Advances in neural information processing systems, vol. 34, pp. 13 93713 949, 2021. 26 [23] L. Zhang and L. Zhang, Artificial intelligence for remote sensing data analysis: review of challenges and opportunities, IEEE Geoscience and Remote Sensing Magazine, vol. 10, no. 2, pp. 270294, 2022. [24] A. A. Aleissaee, A. Kumar, R. M. Anwer, S. Khan, H. Cholakkal, G.-S. Xia, and F. S. Khan, Transformers in remote sensing: survey, Remote Sensing, vol. 15, no. 7, p. 1860, 2023. [25] K. Chen, B. Chen, C. Liu, W. Li, Z. Zou, and Z. Shi, Rsmamba: Remote sensing image classification with state space model, IEEE Geoscience and Remote Sensing Letters, 2024. [26] X. Fan, Z. Hu, Y. Zhao, J. Chen, T. Wei, and Z. Huang, small ship object detection method for satellite remote sensing data, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. [27] Q. Zhu, Y. Cai, Y. Fang, Y. Yang, C. Chen, L. Fan, and A. Nguyen, Samba: Semantic segmentation of remotely sensed images with state space model, Heliyon, vol. 10, no. 19, 2024. [28] Z. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye, Object detection in 20 years: survey, Proceedings of the IEEE, 2023. [29] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, Journal of Machine Learning Research, vol. 23, no. 120, pp. 139, 2022. [30] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state [51] 2023, arXiv preprint arXiv:2304.10597. F. Rosenblatt, The perceptron: probabilistic model for information storage and organization in the brain. Psychological review, vol. 65, no. 6, p. 386, 1958. [52] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, Advances in neural information processing systems, vol. 25, 2012. [53] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale [54] [55] image recognition, arXiv preprint arXiv:1409.1556, 2014. J. Hu, L. Shen, and G. Sun, Squeeze-and-excitation networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 71327141. S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, Cbam: Convolutional block attention module, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 319. [56] X. Wang, R. Girshick, A. Gupta, and K. He, Non-local neural networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 77947803. [57] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., survey on vision transformer, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87110, 2022. [58] A. Gu, K. Goel, and C. Re, Efficiently modeling long sequences with structured spaces, arXiv preprint arXiv:2312.00752, 2023. state spaces, arXiv preprint arXiv:2111.00396, 2021. [31] C. Liu, K. Chen, B. Chen, H. Zhang, Z. Zou, and Z. Shi, Rscama: Remote sensing image change captioning with state space model, IEEE Geoscience and Remote Sensing Letters, 2024. [32] W. Chen, Y. Liu, W. Wang, E. M. Bakker, T. Georgiou, P. Fieguth, L. Liu, and M. S. Lew, Deep learning for instance retrieval: survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 6, pp. 72707292, 2022. [34] [33] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, Functional map of the world, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 61726180. S. McLean, G. J. Read, J. Thompson, C. Baber, N. A. Stanton, and P. M. Salmon, The risks associated with artificial general intelligence: systematic review, Journal of Experimental & Theoretical Artificial Intelligence, vol. 35, no. 5, pp. 649663, 2023. S. K. Singh, S. Kumar, and P. S. Mehra, Chat gpt & google bard ai: review, in 2023 International Conference on IoT, Communication and Automation Technology (ICICAT). IEEE, 2023, pp. 16. [35] [36] K. Chen, W. Li, S. Lei, J. Chen, X. Jiang, Z. Zou, and Z. Shi, Continuous remote sensing image super-resolution based on context interaction in implicit function space, IEEE Transactions on Geoscience and Remote Sensing, 2023. [37] C. Liu, K. Chen, H. Zhang, Z. Qi, Z. Zou, and Z. Shi, Change-agent: Towards interactive comprehensive remote sensing change interpretation and analysis, IEEE Transactions on Geoscience and Remote Sensing, 2024. S. Lu, J. Guo, J. R. Zimmer-Dauphinee, J. M. Nieusma, X. Wang, S. A. Wernke, Y. Huo et al., Vision foundation models in remote sensing: survey, IEEE Geoscience and Remote Sensing Magazine, 2025. [38] [39] X.-Y. Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang, Land-cover classification with high-resolution remote sensing images using transferable deep models, Remote Sensing of Environment, vol. 237, p. 111322, 2020. [40] W. Li, K. Chen, H. Chen, and Z. Shi, Geographical knowledge-driven representation learning for remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 116, 2021. [41] Y. Long, G.-S. Xia, S. Li, W. Yang, M. Y. Yang, X. X. Zhu, L. Zhang, and D. Li, On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid, IEEE Journal of selected topics in applied earth observations and remote sensing, vol. 14, pp. 42054230, 2021. [42] D. Wang, J. Zhang, M. Xu, L. Liu, D. Wang, E. Gao, C. Han, H. Guo, B. Du, D. Tao et al., Mtp: Advancing remote sensing foundation model via multi-task pretraining, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. [43] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 00016 009. [44] U. Mall, B. Hariharan, and K. Bala, Change-aware sampling and contrastive learning for satellite images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 52615270. [45] Y. Cong, S. Khanna, C. Meng, P. Liu, E. Rozi, Y. He, M. Burke, D. Lobell, and S. Ermon, Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery, Advances in Neural Information Processing Systems, vol. 35, pp. 197211, 2022. [46] C. J. Reed, R. Gupta, S. Li, S. Brockman, C. Funk, B. Clipp, K. Keutzer, S. Candido, M. Uyttendaele, and T. Darrell, Scale-mae: scale-aware masked autoencoder for multiscale geospatial representation learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 4088 4099. [47] D. Wang, M. Hu, Y. Jin, Y. Miao, J. Yang, Y. Xu, X. Qin, J. Ma, L. Sun, C. Li et al., Hypersigma: Hyperspectral intelligence comprehension foundation model, arXiv preprint arXiv:2406.11519, 2024. [48] C. Liu, K. Chen, R. Zhao, Z. Zou, and Z. Shi, Text2earth: Unlocking text-driven remote sensing image generation with global-scale dataset and foundation model, arXiv preprint arXiv:2501.00895, 2025. [49] K. Li, X. Cao, and D. Meng, new learning paradigm for foundation modelbased remote-sensing change detection, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 112, 2024. J. Zhang, Z. Zhou, G. Mai, L. Mu, M. Hu, and S. Li, Text2seg: Remote sensing image semantic segmentation via text-guided visual foundation models. arxiv [50] [59] H. Zhang, Y. Zhu, D. Wang, L. Zhang, T. Chen, Z. Wang, and Z. Ye, survey on visual mamba, Applied Sciences, vol. 14, no. 13, p. 5683, 2024. [60] H. Zhang, K. Chen, C. Liu, H. Chen, Z. Zou, and Z. Shi, Cdmamba: Incorporating local clues into mamba for remote sensing image binary change detection, IEEE Transactions on Geoscience and Remote Sensing, 2025. [61] H. Chen, J. Song, C. Han, J. Xia, and N. Yokoya, Changemamba: Remote sensing change detection with spatio-temporal state space model, IEEE Transactions on Geoscience and Remote Sensing, 2024. S. Ren, S. Chen, S. Li, X. Sun, and L. Hou, Testa: Temporal-spatial token aggregation for long-form video-language understanding, arXiv preprint arXiv:2310.19060, 2023. [62] [63] G. Yun, J. Yoo, K. Kim, J. Lee, and D. H. Kim, Spanet: Frequency-balancing token mixer using spectral pooling aggregation modulation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 61136124. [64] W. Zeng, S. Jin, W. Liu, C. Qian, P. Luo, W. Ouyang, and X. Wang, Not all tokens are equal: Human-centric visual analysis via token clustering transformer, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 10111 111. J. Cha, W. Kang, J. Mun, and B. Roh, Honeybee: Locality-enhanced projector for multimodal llm, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 81713 827. [65] [66] H. Wang, J. Shen, Y. Liu, Y. Gao, and E. Gavves, Nformer: Robust person re-identification with neighbor transformer, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 72977307. [67] Y. Shang, M. Cai, B. Xu, Y. J. Lee, and Y. Yan, Llava-prumerge: Adaptive token reduction for efficient large multimodal models, arXiv preprint arXiv:2403.15388, 2024. [68] Q. Zhang, J. Zhang, Y. Xu, and D. Tao, Vision transformer with quadrangle attention, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 5, pp. 36083624, 2024. [69] W. Li, Y. Yuan, J. Liu, D. Tang, S. Wang, J. Qin, J. Zhu, and L. Zhang, llm, arXiv preprint Tokenpacker: Efficient visual projector for multimodal arXiv:2407.02392, 2024. [70] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, Feature pyramid networks for object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 21172125. P. Akiva, M. Purri, and M. Leotta, Self-supervised material and texture representation learning for remote sensing tasks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 82038215. [71] [72] A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, survey on contrastive self-supervised learning, Technologies, vol. 9, no. 1, p. 2, 2020. [73] L. Rossi, A. Karimi, and A. Prati, novel region of interest extraction layer for instance segmentation, in 2020 25th international conference on pattern recognition (ICPR). S. Ren, K. He, R. Girshick, and J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 6, pp. 11371149, 2016. IEEE, 2021, pp. 22032209. [74] [75] K. He, G. Gkioxari, P. Dollar, and R. Girshick, Mask r-cnn, in Proceedings of the IEEE international conference on computer vision, 2017, pp. 29612969. [76] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, Unified perceptual parsing for scene understanding, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 418434. [77] Y. Yang and S. Newsam, Bag-of-visual-words and spatial extensions for land-use classification, in Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, 2010, pp. 270279. [78] G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, and X. Lu, Aid: benchmark data set for performance evaluation of aerial scene classification, IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 7, pp. 39653981, 2017. [79] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770778. [80] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, Vision mamba: Efficient visual representation learning with bidirectional state space model, arXiv preprint arXiv:2401.09417, 2024. 27 guidance, IEEE Transactions on Geoscience and Remote Sensing, 2024. [109] H. Chen and Z. Shi, spatial-temporal attention-based method and new dataset for remote sensing image change detection, Remote Sensing, vol. 12, no. 10, p. 1662, 2020. [110] R. C. Daudt, B. Le Saux, A. Boulch, and Y. Gousseau, Urban change detection for multispectral earth observation using convolutional neural networks, in IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium. Ieee, 2018, pp. 21152118. [111] S. Fang, K. Li, J. Shao, and Z. Li, Snunet-cd: densely connected siamese network for change detection of vhr images, IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 15, 2021. [112] C. Han, C. Wu, and B. Du, Hcgmnet: hierarchical change guiding map network for change detection, in IGARSS 2023-2023 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2023, pp. 55115514. [113] H. Chen, Z. Qi, and Z. Shi, Remote sensing image change detection with transformers, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 114, 2021. [114] W. G. C. Bandara and V. M. Patel, transformer-based siamese network for change detection, in IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2022, pp. 207210. [115] S. Zhao, H. Chen, X. Zhang, P. Xiao, L. Bai, and W. Ouyang, Rs-mamba for large remote sensing image dense prediction, arXiv preprint arXiv:2404.02668, 2024. [116] Y. Wen, J. Sui, X. Ma, W. Liang, X. Zhang, and M.-O. Pun, Change diffusion: Change detection map generation based on difference-feature guided ddpm, arXiv preprint arXiv:2306.03424, 2023. [117] W. Gedara Chaminda Bandara, N. Gopalakrishnan Nair, and V. M. Patel, Ddpmcd: Denoising diffusion probabilistic models as feature extractors for change detection, arXiv e-prints, pp. arXiv2206, 2022. [118] K. Chen, C. Liu, W. Li, Z. Liu, H. Chen, H. Zhang, Z. Zou, and Z. Shi, Time travelling pixels: Bitemporal features integration with foundation model for remote sensing image change detection, arXiv preprint arXiv:2312.16202, 2023. [119] O. Manas, A. Lacoste, X. Giro-i Nieto, D. Vazquez, and P. Rodriguez, Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 94149423. [120] S. R. Dubey, decade survey of content based image retrieval using deep learning, IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 5, pp. 26872704, 2021. [121] G. Sumbul, M. Charfuelan, B. Demir, and V. Markl, Bigearthnet: large-scale benchmark archive for remote sensing image understanding, in IGARSS 20192019 IEEE international geoscience and remote sensing symposium. IEEE, 2019, pp. 59015904. [122] J. Irvin, H. Sheng, N. Ramachandran, S. Johnson-Yu, S. Zhou, K. Story, R. Rustowicz, C. Elsworth, K. Austin, and A. Y. Ng, Forestnet: Classifying drivers of deforestation in indonesia using deep learning on satellite imagery, arXiv preprint arXiv:2011.05479, 2020. [123] J. Jakubik, S. Roy, C. Phillips, P. Fraccaro, D. Godwin, B. Zadrozny, D. Szwarcman, C. Gomes, G. Nyirjesy, B. Edwards et al., Foundation models for generalist geospatial artificial intelligence, arXiv preprint arXiv:2310.18660, 2023. [81] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu, Vmamba: [82] Visual state space model, arXiv preprint arXiv:2401.10166, 2024. J. Chen, K. Chen, H. Chen, Z. Zou, and Z. Shi, degraded reconstruction enhancement-based method for tiny ship detection in remote sensing images with new large-scale dataset, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 114, 2022. [84] [83] Z. Tian, C. Shen, H. Chen, and T. He, Fcos: Fully convolutional one-stage object detection, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 96279636. S. Ren, K. He, R. Girshick, and J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in neural information processing systems, vol. 28, 2015. P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li, Z. Yuan, C. Wang et al., Sparse r-cnn: End-to-end object detection with learnable proposals, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 14 45414 463. [85] [86] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, Ssd: Single shot multibox detector, in Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14. Springer, 2016, pp. 2137. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779788. [87] [88] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable detr: Deformable transformers for end-to-end object detection, arXiv preprint arXiv:2010.04159, 2020. [89] Y. Li, H. Mao, R. Girshick, and K. He, Exploring plain vision transformer backbones for object detection, in European conference on computer vision. Springer, 2022, pp. 280296. [90] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum, Dino: Detr with improved denoising anchor boxes for end-to-end object detection, arXiv preprint arXiv:2203.03605, 2022. [91] K. Chen, S. Guo, H. Li, P. Wu, and N. Zeng, Improved yolo-v3 model with enhanced feature learning for remote sensing image analysis, in 2022 6th Asian Conference on Artificial Intelligence Technology (ACAIT). IEEE, 2022, pp. 19. [92] K. Chen, J. Zhang, C. Liu, Z. Zou, and Z. Shi, Rsrefseg: Referring remote sensing image segmentation with foundation models, arXiv preprint arXiv:2501.06809, 2025. [93] G. Cheng, J. Han, P. Zhou, and L. Guo, Multi-class geospatial object detection and geographic image classification based on collection of part detectors, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 98, pp. 119132, 2014. [94] T. Zhang, X. Zhang, J. Li, X. Xu, B. Wang, X. Zhan, Y. Xu, X. Ke, T. Zeng, H. Su et al., Sar ship detection dataset (ssdd): Official release and comprehensive data analysis, Remote Sensing, vol. 13, no. 18, p. 3690, 2021. [95] H. Su, S. Wei, M. Yan, C. Wang, J. Shi, and X. Zhang, Object detection and instance segmentation in remote sensing imagery based on precise mask rcnn, in IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2019, pp. 14541457. [96] H. Su, S. Wei, S. Liu, J. Liang, C. Wang, J. Shi, and X. Zhang, Hq-isnet: High-quality instance segmentation for remote sensing imagery, Remote Sensing, vol. 12, no. 6, p. 989, 2020. [97] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, Mask scoring r-cnn, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 64096418. [98] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang et al., Hybrid task cascade for instance segmentation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 49744983. [99] X. Wang, R. Zhang, T. Kong, L. Li, and C. Shen, Solov2: Dynamic and fast instance segmentation, Advances in Neural information processing systems, vol. 33, pp. 17 72117 732, 2020. [100] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Maskedattention mask transformer for universal image segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12901299. [101] Y. Liu, H. Li, C. Hu, S. Luo, H. Shen, and C. W. Chen, Catnet: context aggregation network for instance segmentation in remote sensing images, arXiv preprint arXiv:2111.11057, 2021. [102] K. Chen, Z. Zou, and Z. Shi, Building extraction from remote sensing images with sparse token transformers, Remote Sensing, vol. 13, no. 21, p. 4441, 2021. [103] V. Mnih, Machine learning for aerial image labeling. University of Toronto (Canada), 2013. [104] S. Ji, S. Wei, and M. Lu, Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set, IEEE Transactions on geoscience and remote sensing, vol. 57, no. 1, pp. 574586, 2018. [105] X. Chen, Q. Sun, W. Guo, C. Qiu, and A. Yu, Ga-net: geometry prior assisted neural network for road extraction, International Journal of Applied Earth Observation and Geoinformation, vol. 114, p. 103004, 2022. [106] H. Zhang, H. Chen, C. Zhou, K. Chen, C. Liu, Z. Zou, and Z. Shi, Bifa: Remote sensing image change detection with bitemporal feature alignment, IEEE Transactions on Geoscience and Remote Sensing, 2024. [107] K. Chen, C. Liu, W. Li, Z. Liu, H. Chen, H. Zhang, Z. Zou, and Z. Shi, Time travelling pixels: Bitemporal features integration with foundation model for remote sensing image change detection, in IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium. IEEE, 2024, pp. 85818584. [108] Y. Zhu, L. Li, K. Chen, C. Liu, F. Zhou, and Z. Shi, Semantic-cc: Boosting remote sensing image change captioning via foundational knowledge and semantic"
        }
    ],
    "affiliations": [
        "Beihang University",
        "the University of Hong Kong"
    ]
}