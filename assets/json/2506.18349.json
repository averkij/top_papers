{
    "paper_title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation",
    "authors": [
        "Zichong Li",
        "Chen Liang",
        "Zixuan Zhang",
        "Ilgee Hong",
        "Young Jin Kim",
        "Weizhu Chen",
        "Tuo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 4 3 8 1 . 6 0 5 2 : r SlimMoE: Structured Compression of Large MoE Models via"
        },
        {
            "title": "Expert Slimming and Distillation",
            "content": "Zichong Li1, Chen Liang2, Zixuan Zhang1, Ilgee Hong1, Young Jin Kim2, Weizhu Chen2, Tuo Zhao1 1Georgia Tech 2Microsoft Abstract The Mixture of Experts (MoE) architecture has emerged as powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their substantial memory requirements make them prohibitively expensive to fine-tune or deploy in resourceconstrained environments. To address this challenge, we propose SlimMoE, multi-stage compression framework that transforms large MoE models into significantly smaller and more efficient variants without the cost of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation typical of one-shot pruning. Using SlimMoE, we compress Phi-3.5-MoE (41.9B total / 6.6B activated parameters) into two smaller models: Phi-mini-MoE (7.6B total / 2.4B activated) and Phi-tiny-MoE (3.8B total / 1.1B activated), using only 400B tokens less than 10% of the original training data. These models can be fine-tuned on single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them well suited for academic and resource-limited settings. Our experiments show that the compressed models outperform others of similar size and remain competitive with larger models. For example, Phi-mini-MoE matches or exceeds the performance of Phi-3-mini while using only two-thirds of the activated parameters and achieves comparable MMLU scores to LLaMA 3.1 8B with significantly lower latency. These results highlight that structured pruning combined with multi-stage distillation is an effective strategy for building high-quality, compact MoE models, enabling broader adoption of MoE architectures across diverse computational environments. We release our models at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains (Dubey et al., 2024; DeepSeek-AI et al., 2024b). The Mixture of Experts (MoE) architecture Work is done during internship at Microsoft. Correspondence to zli911@gatech.edu and chenliang1@microsoft.com. 1 has emerged as particularly effective approach in open-source LLMs (DeepSeek-AI et al., 2024b; Abdin et al., 2024; Jiang et al., 2024; Team, 2024), offering superior performance compared to dense models while maintaining greater inference efficiency through sparse parameter activation. Despite these advantages, state-of-the-art MoE models developed by industry leaders typically contain an enormous number of parameters and rely on abundant computational resources. Examples include Phi-3.5-MoE (Abdin et al., 2024) and DeepSeek-V3 (DeepSeek-AI et al., 2024b), which are prohibitively large and extremely costly to train and deploy. These high costs limit their practical use in resource-constrained environments such as academic research, making it difficult for researchers to fine-tune such models and highlighting the need for smaller, more efficient MoE alternatives. (a) (b) (c) Figure 1: (a) Comparison of MMLU scores with recent open-sourced MoE and dense LLMs. (b) Comparison of MMLU scores between one-shot approach and SlimMoE multi-stage approach on Phi-tiny-MoE. One-shot approach leads to large performance degradation at the initial pruning step, limiting effectiveness of subsequent distillation. (c) Illustration of model size change throughout the compression process. Shaded area indicate additional computation overhead of approaches over one-shot approach. Training smaller MoE models from scratch is prohibitively expensive in terms of time, data, and computational resources (Team, 2024; Muennighoff et al., 2024). Given the availability of well-trained large MoE models, this work aims to obtain smaller, more efficient MoE models by compressing larger ones while preserving performance. This approach leverages existing models without incurring full-scale training costs. Specifically, we use the pre-trained Phi-3.5-MoE model (Abdin et al., 2024) as our starting point and scale it down to two target sizes: approximately 7.6B total parameters (2.4B active) and 3.8B total parameters (1.1B active). These sizes are strategically chosen to enable fine-tuning on widely available hardware, e.g., the 7.6B model can be fine-tuned on single A100 80GB GPU using memory-efficient optimizers such as 8-bit Adam (Dettmers et al., 2021) or Muon (Jordan et al., 2024), while the 3.8B model can be fine-tuned 2 on an A6000 48GB GPU. Among various compression methods, we focus on structured pruning (Ma et al., 2023; Xia et al., 2024; Muralidharan et al., 2024; Xia et al., 2022; Liang et al., 2023), which reduces parameter counts by systematically removing blocks of weights. However, compressing MoE models at high pruning ratios while preserving performance remains significant challenge. common strategy is to apply one-shot pruning to shrink the model to the target size in single step, followed by knowledge distillation to recover performance (Muralidharan et al., 2024). Yet, pruning large number of parameters at once can result in substantial performance degradation (Figure 1(b)), especially when entire architectural components, such as experts, are removed (Muzio et al., 2024; Chen et al., 2022; Chowdhury et al., 2024). Such severe degradation may hinder the effectiveness of distillation in recovering the original performance (Cho and Hariharan, 2019; Mirzadeh et al., 2020). An existing approach to address this challenge is iterative pruning (Liang et al., 2023), which gradually removes parameters during knowledge distillation to avoid large performance drop at once and facilitate progressive recovery over time. However, iterative pruning is expensive, as it uses masks to simulate pruning and requires loading the full model throughout the process (Figure 1 (c)), making it impractical for compressing LLMs. To address these challenges, we propose SlimMoE, multi-stage expert slimming and distillation framework that mitigates the performance degradation of one-shot pruning with only modest additional computation. SlimMoE compresses MoE models at high ratios while preserving performance, using less than 10% of the original training data. It introduces two key design components: (1) Instead of pruning entire experts, SlimMoE retains all experts and prunes only redundant neurons within each one, preserving expert-specific knowledge critical to model performance (Section 4.5). (2) Rather than pruning directly to the target size, SlimMoE first applies one-shot pruning to an intermediate size to avoid drastic performance degradation, followed by knowledge distillation to restore accuracy (Figure 1(b)). This prune-and-distill process is repeated until the target size is reached, with extended distillation applied at the final stage. As intermediate models retain sufficient capacity for knowledge transfer and recover quickly, this multi-stage approach ensures smoother and more stable compression trajectory. Compared to iterative pruning, SlimMoE avoids pruning masks and full-model loading, resulting in significantly lower computational overhead (see Figure 1(c) for illustration and Section 4.2 for details). Using our proposed method, we introduce Phi-mini-MoE (7.6B total / 2.4B active parameters) and Phi-tiny-MoE (3.8B total / 1.1B active parameters), which are compressed from Phi-3.5-MoE (41.9B total/6.6B activated parameters) 400B tokens of continual pre-training1. As shown in Figure 1(a), both models substantially outperform dense and MoE baselines with similar active parameter counts and achieve competitive performance compared to models with higher inference costs. For example, Phi-mini-MoE reaches similar MMLU scores to Phi-3-mini and LLaMA 3.1 8B, 1Phi-mini-MoE-instruct and Phi-tiny-MoE-instruct are further post-trained with supervised fine-tuning (SFT) and direct preference optimization (DPO) for instruction following and preference alignment. 3 while Phi-tiny-MoE outperforms LLaMA 3.2 3B and matches the performance of Qwen 1.5 MoE (See Figure 4 for comparison on inference costs). In summary, our contributions are as follows: We propose SlimMoE, multi-stage expert slimming and distillation framework that enables high-ratio compression of MoE models (18% and 9%) while preserving competitive performance. We release two compact MoE models, Phi-mini-MoE and Phi-tiny-MoE, which achieve competitive performance among models of comparable size. We conduct extensive ablation studies to validate the effectiveness of SlimMoE and show that MoE architectures can be more robust to pruning than their dense counterparts."
        },
        {
            "title": "2.1 Mixture of Experts Models",
            "content": "The Mixture of Experts (MoE) architecture enhances the standard Transformer by replacing its feed-forward network (FFN) layers with MoE layers, where each input token activates only subset of experts. This sparse activation enables the model to scale in capacity without proportional increase in computational cost (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al., 2017; Jiang et al., 2024; Abdin et al., 2024; Liu et al., 2024b). An MoE layer consists of nexpert expert networks and router. Given an input Rdmodel, each expert {1, . . . , nexpert } typically adopts Gated Linear Unit (GLU) structure (Shazeer, 2020): Experte(x) = (Act(xW 1e) xW 2e)W 3e, Rdmodel where denotes element-wise multiplication. dexpert, 3e 1e, 2e Rdexpert dmodel, Act() is an activation function (e.g., GELU), and The router is parameterized by Rdmodel nexpert and produces nexpert-dimentional scores: G(x) = Gating(TopK(xW G)), where TopK(xW G)i = (xW G)i if is among the top-k highest routing logits, and otherwise, and Gating() is gating function (e.g., softmax). The output of the MoE layer is then: MoE(x) = nexpert(cid:88) e=1 G(x)e Experte(x). In this paper, we focus on Phi-3.5-MoE (Abdin et al., 2024), which sets nexpert = 16, selects the top-2 experts, and uses the SparseMixer-v2 routing mechanism (Liu et al., 2023c,b, 2024b). SparseMixer-v2 enables gradient flow through routing by replacing hard TopK with stochastic expert selection and applying third-order approximation to the routing gradient, improving training stability and scalability. The self-attention mechanism operates on an input sequence RN dmodel, where is the sequence length (Vaswani et al., 2017). Multi-head self-attention is computed as: MHA(X) = Concat(head1, . . . , headH )W O, headh = Attn(XW , XW , XW ), where is the number of attention heads; , projection matrices for head h; and RHdhead after concatenating all heads. Rdmodel dhead are the query, key, and value , dmodel is the output projection matrix applied Further, grouped-query attention (GQA, Ainslie et al. (2023)) offers an efficient alternative to full multi-head attention. It partitions the query heads into multiple groups and lets every head in group share the same key and value projection. This trims the number of KV projections that must be stored and computed."
        },
        {
            "title": "2.2 Weight Pruning",
            "content": "Weight pruning is an effective compression technique that removes parameters from model based on predefined importance criteria (Han et al., 2015b,a; Louizos et al., 2017). Commonly used metrics are sensitivity and its variants (Molchanov et al., 2016, 2019; Sanh et al., 2020; Zhang et al., 2022), which estimate the impact of removing individual parameters. Let denote the model weights and L(X; ) the loss function. The sensitivity score for weight Wi,j is computed as: si,j = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) Wi,j Wi,j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) , which approximates the change in loss magnitude when setting Wi,j to zero, using first-order Taylor expansion of around . Weights with low sensitivity are considered redundant. Pruning methods are typically categorized into unstructured and structured approaches. Unstructured pruning removes individual weights, resulting in sparse weight matrices. While this often causes minimal performance degradation, it does not yield practical speedup without specialized hardware support and tensor structure (Fang et al., 2024). Structured pruning, on the other hand, removes entire architectural components (e.g., neurons, attention heads, or experts), which enables real efficiency gains on standard hardware but may cause greater disruption to model performance and has been widely adopted for compressing LLMs in recent years (Ma et al., 2023; Xia et al., 2024; Muralidharan et al., 2024; Xia et al., 2022; Liang et al., 2023)."
        },
        {
            "title": "2.3 Knowledge Distillation",
            "content": "Knowledge Distillation (KD) is widely used model compression technique that trains smaller student model to mimic the behavior of larger teacher model by minimizing the discrepancy between their output distributions (Romero et al., 2014; Hinton et al., 2015; Muralidharan et al., 5 2024). The objective is typically formulated as the KullbackLeibler (KL) divergence: LKD(X; ) = 1 N(cid:88) n=1 KL (cid:16) teacher(X) pn pn (cid:17) (X) , (1) where denotes the students parameters, pn next-token distributions for the n-th token in input X. teacher(X) and pn (X) are the teacher and student To improve the efficiency and robustness of knowledge distillation, Peng et al. (2024) proposed using only the top-k logits from the teacher instead of the full vocabulary distribution. The method minimizes the discrepancy over the tokens with the highest predicted probabilities, not only reduces computational requirements but also mitigates noise from low-probability tokens."
        },
        {
            "title": "3 Method",
            "content": "SlimMoE employs multi-stage framework that progressively reduces model size by alternating expert and attention pruning with knowledge distillation. We begin by describing the target MoE architecture, followed by our pruning and distillation strategies."
        },
        {
            "title": "3.1 Target Architecture",
            "content": "Expert Slimming. Since MoE layers account for over 95% of the models parameters, we focus on pruning these layers. Specifically, we reduce the expert dimension dexpert by pruning redundant neurons within each expert network, i.e., {W in Eq. 2.1, while keeping the number of experts fixed. As experts often serve specialized roles for subsets of tokens, this approach better 2e, 3e }nexpert e=1 1e, preserves expert functionality and results in smaller accuracy degradation than pruning entire experts (Section 4.5). We apply uniform slimming across all experts to maintain equal expert size for architectural consistency and deployment efficiency. Attention Pruning. We also prune attention layers as they begin to dominate the parameter count and inference time at smaller scales. Since Phi-3.5-MoE adopts GQA that ties four query heads to shared key/value projection, we prune at the granularity of entire GQA groups, eliminating both the shared KV pair and its four associated queries. Target Architecture. We reduce the expert dimension of Phi-3.5-MoE (41.9B total / 6.6B activated parameters) to 15% of its original size to yield Phi-mini-MoE (7.6B total / 2.4B activated parameters). To obtain Phi-tiny-MoE (3.8B total / 1.1B activated parameters), we further reduce the expert dimension to 7% and prune 50% of the GQA groups. Table 1 summarizes the detailed architecture configurations."
        },
        {
            "title": "3.2 Multi-stage Pruning and Distillation",
            "content": "We prune and distill the model over stages, with each stage producing smaller model than the previous one. At each stage, we apply one-shot pruning to reach an intermediate size, followed by 6 Table 1: Model configuration details and parameter counts. Model dmodel nhead (q/kv) dexpert nlayer nexpert top-k # Total # Act. Param Param Phi-3.5-MoE Phi-mini-MoE Phi-tiny-MoE 4096 4096 4096 32/8 32/8 16/ 6400 960 448 32 32 16 16 16 2 2 41.9B 7.6B 3.8B 6.6B 2.4B 1.1B distillation from the original full model. One-shot Pruning. We use the top-8 logits distillation loss to compute the sensitivity score for each parameter Wi,j: si,j = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) LMoE KD (X; ) Wi,j Wi,j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) , where the loss is defined as: LMoE KD (X; ) ="
        },
        {
            "title": "1\nN",
            "content": "N(cid:88) n=1 KL (cid:16) teacher, top-8(X) pn pn (cid:17) (X) + Aux(W ). (2) (3) Here, pn token with all but the top-8 probabilities masked to 0, and pn defined in Eq. 1. Aux() is the auxiliary load-balancing loss (Liu et al., 2024b). teacher, top-8(X) is the teachers next-token prediction probability distribution for the n-th (x) is the student distribution as"
        },
        {
            "title": "We compute the sensitivity score using the knowledge distillation loss as it captures the",
            "content": "discrepancy between the student and teacher. This allows us to identify redundant neurons that have negligible impact on the gap, yielding better pruning performance than alternative loss metrics (Section 4.5). For the e-th expert network, we first compute the sensitivity score for each parameter in the 3e of the GLU. We then aggregate these into neuron-level score by down-projection matrix taking the ℓ2-norm across each row: (cid:115)(cid:88) s2 i,j. si = (4) Neurons with the lowest scores are considered least important. We prune these by removing the 2e, and corresponding rows and columns in 3e. For attention pruning, we apply Eq. 4 to the output projection matrix (Eq. 2.1) and average 1e, the scores of all neurons within heads in GQA group to obtain the score for each group. We uniformly sample 16K training examples as calibration data to compute sensitivity scores. Distillation. After pruning, we distill the model using the original full model as the teacher to recover performance. The student model is optimized using gradient-based method (Loshchilov 7 and Hutter, 2017): η LMoE KD (x; ), (5) where η is the learning rate. To reduce computational overhead, we apply early stopping once performance improvements begin to plateau, rather than training each intermediate model to full convergence. In practice, distillation at intermediate stages consumes only 30-35% of the total training steps (Figure 2). Multi-stage Schedule. We use two stages (T =2) for Phi-mini-MoE, and for the more aggressively compressed Phi-tiny-MoE, we use three stages (T =3). To ensure balanced pruning across stages, we follow geometric compression schedule. Given target overall compression ratio α, we reduce the model size at each stage by factor of approximately α1/T . The full architectural specifications for each intermediate model are provided in Appendix A.3. This progressive strategy allows each intermediate model to retain sufficient capacity for effective knowledge transfer, resulting in smoother and more stable transition to the final compact model."
        },
        {
            "title": "4 Experiments",
            "content": "We compress the pre-trained Phi-3.5-MoE model (Abdin et al., 2024; Liu et al., 2024b) to obtain Phi-mini-MoE and Phi-tiny-MoE base models through continual pre-training. To better evaluate on downstream tasks, we further post-train the two base models by supervised fine-tuning (SFT) to enhance the models instruction-following capabilities, followed by Direct Preference Optimization (DPO, Rafailov et al. (2023)) to steer the model away from unwanted behavior. Data and Training. For continual pre-training, we perform multi-stage distillation using 400Btoken subset of the Phi-3.5-MoE (Abdin et al., 2024; Liu et al., 2024b) pre-training corpus, leveraging the top-8 logits predicted by the Phi-3.5-MoE teacher. For post-training, we apply SFT and DPO using the GRIN-MoE (Liu et al., 2024b) post-training corpus2. During SFT, we adopt top-8 logits distillation objective using the post-trained GRIN-MoE as the teacher. The training hyperparameter configurations are provided in the Appendix A.5. Evaluation. We evaluate our models against other similarly-sized models, including both MoE and dense architectures. For MoE models, we include Qwen 1.5 MoE (Team, 2024), DeepSeek V2 Lite (DeepSeek-AI et al., 2024a), OL-MoE (Muennighoff et al., 2024), and Granite 3.0 (Granite Team, 2024). For dense models, we include the Phi-3 series (Abdin et al., 2024), Llama 3 series (Dubey et al., 2024), Qwen 2.5 series (Yang et al., 2024a), and Gemma 3 series (Team et al., 2025). We evaluate these models across diverse set of downstream tasks. For commonsense and knowledge assessment, we employ MMLU (Hendrycks et al., 2021), MMLU-pro (Wang et al., 2024), 2We follow GRIN-MoEs post-training setup for its simplicity, as Phi-3.5-MoE employs more complexity to build up long-context and multilingual capabilities. 8 Bigbench-Hard (Suzgun et al., 2023), Arc-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), OpenbookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), BoolQ (Clark et al., 2019), and Winograde (Sakaguchi et al., 2020). To evaluate coding capabilities, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), while for reasoning and mathematical proficiency, we utilize GSM8K (Cobbe et al., 2021) and GPQA (Rein et al., 2023). Additionally, we report MT-bench (Zheng et al., 2023) scores to assess general instruction-following abilities. We provide the detailed settings for evaluations in Appendix A.6."
        },
        {
            "title": "4.1 Performance of Compressed Models",
            "content": "Table 2 shows the evaluation results comparing the post-trained Phi-mini-MoE and Phi-tiny-MoE against other MoE and dense models of comparable sizes. Additional benchmark results are shown in Table 5, and the performance of the compressed base models is reported in Table 6. Table 2: Comparison of Phi-mini-MoE and Phi-tiny-MoE against other MoE and dense models. Model # Total # Act. param param MMLU MMLU pro BBH Arc-C Human- (chat) eval GSM8K MTbench Mixture-of-Experts (MoE) Models Phi-3.5-MoE Qwen 1.5 MoE DeepSeek V2 Lite OL-MoE Granite 3.0 MoE Dense Models LLaMA 3.1 8B Qwen 2.5 7B Phi-3-small Gemma 3 4B Phi-3-mini LLaMA 3.2 3B Qwen 2.5 3B Gemma 3 1B LLaMA 3.2 1B Our Models Phi-mini-MoE Phi-tiny-MoE 42B 14B 16B 6.9B 3.4B 8B 7.6B 7.4B 4.3B 3.8B 3.2B 3.1B 1B 1.2B 7.6B 3.8B 6.6B 2.7B 2.4B 1.3B 0.8B 8B 7.6B 7.4B 4.3B 3.8B 3.2B 3.1B 1B 1.2B 78.36 60.73 56. 54.27 50.06 68.71 73.47 75.35 59. 69.94 61.73 65.06 40.80 46.30 59. 26.49 17.89 20.87 63.93 91.38 42. 67.24 36.30 61.09 38.00 55.63 4. 39.65 56.06 45.28 56.24 52.06 40. 45.65 36.70 41.00 14.70 18.67 50. 82.42 53.74 88.82 62.07 84.30 49. 75.85 54.94 85.58 45.46 75.77 46. 80.20 34.80 37.46 35.18 49.91 81. 46.30 54.40 37.80 51.80 69.50 81. 70.10 67.10 72.60 52.40 73.80 41. 35.40 87.87 53.07 63.23 71.49 60. 84.84 84.84 84.84 78.92 84.61 77. 76.57 41.77 44.96 2.4B 1.1B 70. 60.83 49.68 36.34 55.27 84.91 45. 76.37 73.80 58.50 84.89 78.47 8. 6.55 6.82 6.60 6.91 8.03 8. 8.03 8.28 7.46 7.46 7.60 6. 5.23 7.59 7.05 Our compressed models demonstrate strong performance while maintaining parameter efficiency. Phi-mini-MoE matches or outperforms Phi-3-mini across tasks with only two-thirds of 9 its activated parameters, and achieves performance on par with Phi-3-small on ARC-Challenge, HumanEval, and GSM8K using just one-third of the activation size. Compared to other public dense models with similar total parameter counts, Phi-mini-MoE outperforms LLaMA 3.1 8B on most benchmarks and surpasses Qwen 2.5 7B on BBH, HellaSwag, and OpenBookQA, while using only one-third of their activated parameters. Among public MoE models with similar activation sizes, Phi-mini-MoE outperforms Qwen 1.5 MoE and DeepSeek V2 Lite MoE, while having less total parameters. Phi-tiny-MoE also shows strong results, outperforming OL-MoE and Granite 3.0 MoE with similar activation parameter counts and achieving performance comparable to LLaMA 3.2 3B at similar total parameter counts. We note that the Phi-mini-MoE and Phi-tiny-MoE base models prior to post-training maintain similar relative performance compared to their respective baselines  (Table 6)  , confirming that SlimMoE effectively preserves the models capabilities."
        },
        {
            "title": "4.2 Comparing Multi-stage with One-stage and Iterative Pruning",
            "content": "We compare SlimMoE with the conventional one-stage baseline (one-shot pruning followed by distillation Muralidharan et al. (2024)) and iterative pruning (Liang et al. (2023) without the layerwise distillation objective) to demonstrate the effectiveness of our multi-stage design. To ensure fair comparison, we control the total number of training tokens throughout the process to match between baselines. For iterative pruning, we set the pruning schedule to reach the target size at the same number of tokens as SlimMoEs initialization stages (stages before the final stage). As shown in Table 3, our multi-stage approach consistently outperforms the onestage method across all evaluated benchmarks. On Phi-tiny-MoE, our approach achieves similar performance to iterative pruning despite the latters much higher computational costs, as iterative pruning requires loading the full-size model at the pruning stage, resulting in approximately 2.4 computation time during initialization phase. Table 3: Performance comparison of multi-stage, iterative and one-stage pruning approaches. Models evaluated here are pretrained version."
        },
        {
            "title": "Model Type",
            "content": "Approach MMLU Arc-C WinoGrande HellaSwag GSM8K Phi-mini-MoE One-Stage Multi-Stage One-Stage Phi-tiny-MoE"
        },
        {
            "title": "Iterative",
            "content": "Multi-Stage 68.58 69.87 58.40 60.05 60. 60.84 62.29 56.99 56.71 57.68 70. 75.85 70.80 72.20 71.90 75.93 76. 66.24 67.52 67.33 74.52 77.48 62. 69.60 69.90 Figure 2 (a) illustrates how the MMLU scores evolve throughout the compression process for 10 Phi-mini-MoE, while the corresponding plot for Phi-tiny-MoE is shown in Figure 1 (b). We can observe that the one-stage approach causes model collapse at the initial pruning step, whereas SlimMoE yields smaller drop. The gradual transition through intermediate model sizes enables the knowledge distillation process to be more effective at each stage, resulting in superior overall performance in the final compressed model. While the multi-stage approach incurs additional computation during the initialization phases, it reaches the final convergence performance of the one-stage baseline earlier, resulting in reduced computation time to achieve the same performance: 0.74 for Phi-mini-MoE and 0.91 for Phi-tiny-MoE. (a) (b) (c) Figure 2: MMLU performance analysis across compression stages. (a) Comparison between SlimMoE multi-stage and one-stage approaches for Phi-mini-MoE. (b) Impact of different initialization token counts on Phi-mini-MoE performance. (c) Impact of different initialization token counts on Phi-tiny-MoE."
        },
        {
            "title": "4.3 When to Stop Previous Stages",
            "content": "A critical question for the proposed SlimMoE framework is determining the timing to stop current stage and proceed to the next. To address this question, we experiment with different token allocations for the initialization phases (where initialization refers to all training conducted before the final stage). As illustrated in Figure 2 (b) and (c), we compare both shorter and longer initialization approaches. For Phi-mini-MoE, we evaluate initializations of 25B and 135B tokens, while for Phi-tiny-MoE, we test 50B and 130B tokens. We observe that longer initialization consistently leads to better final performance on both model sizes. This indicates that extended earlier stages facilitate more effective knowledge transfer to subsequent stages. In practice, we recommend to initiate the next stage when the performance improvement in the current stage becomes minimal."
        },
        {
            "title": "4.4 Are MoE Models Easier to Prune than Dense Models?",
            "content": "In this subsection, we investigate whether MoE architecture is more robust to pruning compared to dense model by conducting one-stage prune-and-distill on both MoE and dense architectures. 11 For fair comparison, we choose Phi-3-medium as the dense counterpart because it achieves performance comparable to Phi-3.5-MoE before compression, and both models adopt the same pre-training data sources. We then compress it based on the compression ratio of the activated parameters of our models: 35% for Phi-mini-MoE and 16% for Phi-tiny-MoE. For the Phi-3-medium model, this translates to: (1) Pruning the intermediate dimension in the FFN layer to 18% ( 5B parameters) (2) Pruning half of the attention heads and reducing the FFN layer to 6% ( 2.3B parameters), respectively. Detailed architecture configurations can be found in Appendix A.3. Figure 3 presents the MMLU scores for both model types across different numbers of training tokens. The results show consistent performance advantage for pruned MoE models over their dense counterparts. The improvements become even more pronounced at the more aggressive 16% compression level. These findings suggest that MoE architectures may indeed be more amenable to pruning, potentially due to their inherent sparse activation patterns and distributed knowledge representation across experts. Figure 3: Performance of pruned Phi-3.5-MoE versus pruned Phi-3-medium. Left: pruned ratio 35%; Right: pruned ratio 16%."
        },
        {
            "title": "4.5 Pruning Architecture and Criterion",
            "content": "SlimMoE employs top-8 logits distillation loss for computing sensitivity scores to slim all experts. To justify this design choice, we conduct comparative experiments with various pruning approaches for MoE layers. We evaluate five methods: (1) Expert slimming with knowledge distillation loss (KL) (2) Expert slimming with causal language modeling loss (CLM) (3) Expert pruning based on sensitivity scores computed from KL loss (4) Expert pruning based on activation frequency (Muzio et al., 2024) (5) M-SMoE: Merging experts based on routing logits (Li et al., 2024). As shown in Table 4, using KL loss consistently outperforms CLM loss across different pruning ratios. This improvement likely stems from the teacher models ability to mitigate noise in the training data. Our results also show that expert slimming outperforms expert pruning, suggesting that all experts contain specialized knowledge, and slimming them uniformly better preserves this knowledge compared to removing entire experts. We include studies on expert diversity in 12 Prune Expert Expert Prune Expert Prune Expert (freq) M-SMoE Ratio Slimming (KL) Slimming (CLM) 50% 25% 63.76 43. 59.30 37.52 (KL) 53.41 30.38 (Muzio et al., 2024) (Li et al., 2024) 48.59 31.87 38.54 25.50 Table 4: Ablation studies on pruning architecture and criterion. Appendix A.8. Merging experts similarly proves ineffective, potentially due to the increased size and heterogeneous weight distributions within each expert."
        },
        {
            "title": "4.6 Inference cost",
            "content": "We conduct inference speed profiling for our compressed models and compare them with models of similar performance or activated parameter counts in Figure 4. We collect the inference metrics using the inference benchmarking scripts implemented by DeepSpeed3. More details of the profiling can be found in Appendix A.7. The results demonstrate that our compressed models achieve lower latency and comparable or higher throughput across different client loads. Notably, Phi-mini-MoE and Phi-tiny-MoE maintain these computational efficiency advantages while yielding performance comparable to or better than the baseline models. (a) Phi-mini-MoE (b) Phi-tiny-MoE Figure 4: Latency versus throughput comparison across models under varying client loads."
        },
        {
            "title": "5 Discussion",
            "content": "MoE Compression. Previous MoE compression techniques can be broadly categorized into three approaches: expert-level pruning, expert merging, and unstructured expert slimming. Expertlevel pruning methods (Muzio et al., 2024; Lu et al., 2024) identify and remove entire redundant experts. Muzio et al. (2024) uses router logits and activation count to identify redundant experts, straightforward approach that however leads to large performance drop with high compression 3https://github.com/deepspeedai/DeepSpeedExamples/tree/master/benchmarks/inference/mii 13 ratio (Section 4.5). Lu et al. (2024) enumerates all possible expert combinations for each layer to find the one with lowest reconstruction loss, which can becomes computationally prohibitive for large models with many experts. Expert merging techniques (Li et al., 2024; Liu et al., 2024a) attempt to preserve knowledge by combining experts rather than removing them. Li et al. (2024) aligns neurons across experts and then merges them based on router information, but does not work well in our settings. Liu et al. (2024a) focuses on task-specific settings and employs evolutionary search to identify merging weights that maximize performance on single target task. This approach requires hundreds of iterations to converge, making it substantially more expensive than our pruning method. Additionally, determining an appropriate metric for task-agnostic setting is nontrivial. He et al. (2025) and Xie et al. (2024) study unstructured expert slimming, which cannot leads to efficiency without specified hardware. Some other works consider techniques besides pruning for compressing MoE, operating on different dimensions and are complementary to SlimMoE. Yang et al. (2024b) first employ Layer-wise expert-level non-uniform pruning, then apply Singular Value Decomposition to further compress the remaining experts. Kim et al. (2023) utilize quantization to MoE models, reducing memory through lower numerical precision rather than architectural changes. Dense Model Compression. Dense model compression has been extensively studied in recent literature (Xia et al., 2024; Muralidharan et al., 2024; Men et al., 2024). For instance, ShortGPT (Men et al., 2024) and LaCo (Yang et al., 2024c) propose to remove entire layers. Other works (Xia et al., 2024; Ashkboos et al., 2024; Ma et al., 2023) introduce various strategies for compressing width dimensions. Minitron (Muralidharan et al., 2024) represents notable work that shares similarities with our approach. They focus on dense models and employ one-shot pruning and distillation method on Nemotron 4 15B (Parmar et al., 2024) to produce Minitron 8B, followed by further pruning to 4B, which is two-stage compression pipeline. However, their comparison with singlestage approaches does not account for the tokens used in the initial stage. In contrast, SlimMoE introduces principled multi-stage approach for MoE models. We systematically studies the multi-stage approach by experimenting with different stage lengths and numbers, demonstrating that multi-stage outperforms one-stage approaches under the same token budget. Computational Cost for the Multi-stage Approach. While our experiments on Phi-3.5-MoE requires 400B tokens for performance recovery, the computational overhead remains manageable due to our strategic token allocation. Since most tokens are trained in the final stage when the model is heavily pruned, the overall computational cost is substantially reduced compared to training target-size models from scratch, which would require the full 4T tokens. This compressionbased approach thus represents significant efficiency gain for achieving competitive performance. Additionally, we can pre-compute and cache teacher model logits as one-time effort for distillation, significantly reducing memory requirements along the process. SlimMoE also demonstrates robustness to varying token budgets, enabling users to adjust computational resources based on their constraints. To illustrate this flexibility, we provide additional results under reduced 14 training budget of 90B tokens in Appendix A.4."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we presented SlimMoE, multi-stage framework for compressing large MoE models by high ratios. Using SlimMoE, we compressed Phi 3.5-MoE to create Phi-mini-MoE and Phi-tinyMoE using only 10% of the original pretraining data. These models significantly outperform both MoE and dense models with similar activated parameter counts. Our experiments demonstrate the effectiveness of the multi-stage approach and highlight the importance of preserving knowledge across experts through expert slimming. Notably, while our evaluation focuses on Phi-series models, SlimMoE is architecturally agnostic, making it broadly applicable to other MoE architectures. To the best of our knowledge, this is the first work to prune large MoE models at such high ratios (<20% of original parameters) while achieving state-of-the-art performance."
        },
        {
            "title": "7 Acknowledgment",
            "content": "We would like to thank Shuohang Wang for the helpful discussion on distillation."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H. S., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., Perez-Becker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y. and Zhou, X. (2024). Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebr on, F. and Sanghai, S. (2023). GQA: 15 training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 (H. Bouamor, J. Pino and K. Bali, eds.). Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.emnlp-main.298 Ashkboos, S., Croci, M. L., Nascimento, M. G. D., Hoefler, T. and Hensman, J. (2024). Slicegpt: Compress large language models by deleting rows and columns. In ICLR. OpenReview.net. Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V. and Sutton, C. (2021). Program synthesis with large language models. CoRR, abs/2108.07732. Bisk, Y., Zellers, R., Bras, R. L., Gao, J. and Choi, Y. (2020). PIQA: reasoning about physical commonsense in natural language. In AAAI. AAAI Press. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I. and Zaremba, W. (2021). Evaluating large language models trained on code. CoRR, abs/2107.03374. Chen, T., Huang, S., Xie, Y., Jiao, B., Jiang, D., Zhou, H., Li, J. and Wei, F. (2022). Task-specific expert pruning for sparse mixture-of-experts. CoRR, abs/2206.00277. Cho, J. H. and Hariharan, B. (2019). On the efficacy of knowledge distillation. In ICCV. IEEE. Chowdhury, M. N. R., Wang, M., Maghraoui, K. E., Wang, N., Chen, P.-Y. and Carothers, C. (2024). provably effective method for pruning experts in fine-tuned sparse mixture-of-experts. arXiv preprint arXiv:2405.16646. Clark, C., Lee, K., Chang, M., Kwiatkowski, T., Collins, M. and Toutanova, K. (2019). Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL-HLT (1). Association for Computational Linguistics. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C. and Tafjord, O. (2018). Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457. 16 Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. and Schulman, J. (2021). Training verifiers to solve math word problems. CoRR, abs/2110.14168. DeepSeek-AI, Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Deng, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Yang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Chen, J., Yuan, J., Qiu, J., Song, J., Dong, K., Gao, K., Guan, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Pan, R., Xu, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Zheng, S., Wang, T., Pei, T., Yuan, T., Sun, T., Xiao, W. L., Zeng, W., An, W., Liu, W., Liang, W., Gao, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Chen, X., Nie, X. and Sun, X. (2024a). Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L. and Zeng, W. (2024b). Deepseek-v3 technical report. CoRR, abs/2412.19437. Dettmers, T., Lewis, M., Shleifer, S. and Zettlemoyer, L. (2021). 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., 17 Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K. and et al. (2024). The llama 3 herd of models. CoRR, abs/2407.21783. Fang, G., Yin, H., Muralidharan, S., Heinrich, G., Pool, J., Kautz, J., Molchanov, P. and Wang, X. (2024). Maskllm: Learnable semi-structured sparsity for large language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024 (A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak and C. Zhang, eds.). http://papers.nips.cc/paper_files/paper/2024/hash/0e9a05f5ce62284c91e4a33498899124-Abstract-Conference. html Fedus, W., Zoph, B. and Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23 139. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K. and Zou, A. (2024). The language model evaluation harness. https://zenodo.org/records/12608602 Granite Team, I. (2024). Granite 3.0 language models. Han, S., Mao, H. and Dally, W. J. (2015a). Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149. Han, S., Pool, J., Tran, J. and Dally, W. J. (2015b). Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626. He, S., Dong, D., Ding, L. and Li, A. (2025). Towards efficient mixture of experts: holistic study of compression techniques. https://arxiv.org/abs/2406.02500 Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. and Steinhardt, J. (2021). Measuring massive multitask language understanding. In ICLR. OpenReview.net. Hinton, G., Vinyals, O. and Dean, J. (2015). Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de Las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., 18 Saulnier, L., Lachaux, M., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T. and Sayed, W. E. (2024). Mixtral of experts. CoRR, abs/2401.04088. Jordan, K., Jin, Y., Boza, V., You, J., Cesista, F., Newhouse, L. and Bernstein, J. (2024). Muon: An optimizer for hidden layers in neural networks. https://kellerjordan.github.io/posts/muon/ Kim, Y. J., Fahim, R. and Awadalla, H. H. (2023). Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness. arXiv preprint arXiv:2310.02410. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N. and Chen, Z. (2020). Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Li, P., Zhang, Z., Yadav, P., Sung, Y., Cheng, Y., Bansal, M. and Chen, T. (2024). Merge, then compress: Demystify efficient smoe with hints from its routing policy. In ICLR. OpenReview.net. Liang, C., Jiang, H., Li, Z., Tang, X., Yin, B. and Zhao, T. (2023). Homodistil: Homotopic task-agnostic distillation of pre-trained transformers. In ICLR. OpenReview.net. Liu, E., Zhu, J., Lin, Z., Ning, X., Blaschko, M. B., Yan, S., Dai, G., Yang, H. and Wang, Y. (2024a). Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs. CoRR, abs/2407.00945. Liu, J., Xia, C. S., Wang, Y. and Zhang, L. (2023a). Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=1qvx610Cu Liu, L., Dong, C., Liu, X., Yu, B. and Gao, J. (2023b). Bridging discrete and backpropagation: Straight-through and beyond. Advances in Neural Information Processing Systems, 36 1229112311. Liu, L., Gao, J. and Chen, W. (2023c). Sparse backpropagation for moe training. arXiv preprint arXiv:2310.00811. Liu, L., Kim, Y. J., Wang, S., Liang, C., Shen, Y., Cheng, H., Liu, X., Tanaka, M., Wu, X., Hu, W., Chaudhary, V., Lin, Z., Zhang, C., Xue, J., Awadalla, H., Gao, J. and Chen, W. (2024b). GRIN: gradient-informed moe. CoRR, abs/2409.12136. Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Louizos, C., Welling, M. and Kingma, D. P. (2017). Learning sparse neural networks through 0 regularization. arXiv preprint arXiv:1712.01312. 19 Lu, X., Liu, Q., Xu, Y., Zhou, A., Huang, S., Zhang, B., Yan, J. and Li, H. (2024). Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. In ACL (1). Association for Computational Linguistics. Ma, X., Fang, G. and Wang, X. (2023). Llm-pruner: On the structural pruning of large language models. In NeurIPS. Men, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han, X. and Chen, W. (2024). Shortgpt: Layers in large language models are more redundant than you expect. CoRR, abs/2403.03853. Mihaylov, T., Clark, P., Khot, T. and Sabharwal, A. (2018). Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP. Association for Computational Linguistics. Mirzadeh, S., Farajtabar, M., Li, A., Levine, N., Matsukawa, A. and Ghasemzadeh, H. (2020). Improved knowledge distillation via teacher assistant. In AAAI. AAAI Press. Molchanov, P., Mallya, A., Tyree, S., Frosio, I. and Kautz, J. (2019). Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Molchanov, P., Tyree, S., Karras, T., Aila, T. and Kautz, J. (2016). Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440. Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, P., Tafjord, O., Lambert, N. et al. (2024). Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060. Muralidharan, S., Sreenivas, S. T., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J. and Molchanov, P. (2024). Compact language models via pruning and knowledge distillation. In NeurIPS. Muzio, A., Sun, A. and He, C. (2024). Seer-moe: Sparse expert efficiency through regularization for mixture-of-experts. CoRR, abs/2404.05089. Parmar, J., Prabhumoye, S., Jennings, J., Patwary, M., Subramanian, S., Su, D., Zhu, C., Narayanan, D., Jhunjhunwala, A., Dattagupta, A., Jawa, V., Liu, J., Mahabaleshwarkar, A., Nitski, O., Brundyn, A., Maki, J., Martinez, M., You, J., Kamalu, J., LeGresley, P., Fridman, D., Casper, J., Aithal, A., Kuchaiev, O., Shoeybi, M., Cohen, J. M. and Catanzaro, B. (2024). Nemotron-4 15b technical report. CoRR, abs/2402.16819. Peng, H., Lv, X., Bai, Y., Yao, Z., Zhang, J., Hou, L. and Li, J. (2024). Pre-training distillation for large language models: design space exploration. CoRR, abs/2410.16215. https://doi.org/10.48550/arXiv.2410.16215 20 Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S. and Finn, C. (2023). Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36 5372853741. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J. and Bowman, S. R. (2023). GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022. Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C. and Bengio, Y. (2014). Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550. Sakaguchi, K., Bras, R. L., Bhagavatula, C. and Choi, Y. (2020). Winogrande: An adversarial winograd schema challenge at scale. In AAAI. AAAI Press. Sanh, V., Wolf, T. and Rush, A. (2020). Movement pruning: Adaptive sparsity by fine-tuning. Advances in neural information processing systems, 33 2037820389. Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D. and Wei, J. (2023). Challenging big-bench tasks and whether chain-of-thought can solve them. In ACL (Findings). Association for Computational Linguistics. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M. et al. (2025). Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Team, Q. (2024). Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters. https://qwenlm.github.io/blog/qwen-moe/ Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X. and Chen, W. (2024). Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In NeurIPS. Xia, M., Gao, T., Zeng, Z. and Chen, D. (2024). Sheared llama: Accelerating language model pre-training via structured pruning. In ICLR. OpenReview.net. Xia, M., Zhong, Z. and Chen, D. (2022). Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408. 21 Xie, Y., Zhang, Z., Zhou, D., Xie, C., Song, Z., Liu, X., Wang, Y., Lin, X. and Xu, A. (2024). Moepruner: Pruning mixture-of-experts large language model using the hints from its router. CoRR, abs/2410.12013. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z. and Qiu, Z. (2024a). Qwen2.5 technical report. CoRR, abs/2412.15115. Yang, C., Sui, Y., Xiao, J., Huang, L., Gong, Y., Duan, Y., Jia, W., Yin, M., Cheng, Y. and Yuan, B. (2024b). Moe-i2: Compressing mixture of experts models through inter-expert pruning and intra-expert low-rank decomposition. CoRR, abs/2411.01016. Yang, Y., Cao, Z. and Zhao, H. (2024c). Laco: Large language model pruning via layer collapse. In EMNLP (Findings). Association for Computational Linguistics. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y. (2019). Hellaswag: Can machine really finish your sentence? In ACL (1). Association for Computational Linguistics. Zhang, Q., Zuo, S., Liang, C., Bukharin, A., He, P., Chen, W. and Zhao, T. (2022). Platon: Pruning large transformer models with upper confidence bound of weight importance. In International conference on machine learning. PMLR. Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E. and Stoica, I. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Results on extended tasks. Table 5: Extended evaluation on commonsense reasoning, QA, and code generation tasks. Model # Total # Act. param param Mixture-of-Experts (MoE) Models Phi 3.5-MoE Qwen 1.5 MoE DeepSeek V2 Lite OL-MoE Granite 3.0 MoE Dense Models LLaMA 3.1 8B Qwen 2.5 7B Phi 3 small Gemma 3 4B Phi 3 mini LLaMA 3.2 3B Qwen 2.5 3B Gemma 3 1B LLaMA 3.2 1B Our Models Phi-mini-MoE Phi-tiny-MoE 42B 14B 16B 6.9B 3.4B 8B 7.6B 7.4B 4.3B 3.8B 3.2B 3.1B 1B 1.2B 7.6B 3.8B 6.6B 2.7B 2.4B 1.3B 0.8B 8B 7.6B 7.4B 4.3B 3.8B 3.2B 3.1B 1B 1.2B 2.4B 1.1B Winograde Hellaswag GPQA PIQA OpenbookQA BoolQ MBPP 78. 70.40 70.40 66.54 67.17 75.61 75. 73.38 65.67 76.95 67.72 71.10 55. 62.04 75.45 70.09 82.31 79.40 74. 75.44 70.64 78.27 72.25 81.97 55. 76.48 70.40 70.37 47.66 61.0 74. 67.44 39.90 27.27 25.76 24.75 27. 28.79 34.34 30.81 27.27 26.77 22. 23.74 22.73 21.21 81.72 80.25 79. 80.79 78.45 81.28 72.20 81.72 73. 81.63 78.45 74.54 71.98 75.35 27. 29.29 81.77 79.16 63.20 49.40 46. 48.00 44.40 52.40 50.00 57.20 49. 53.80 43.20 46.60 40.20 36.0 51. 48.00 88.93 81.77 83.30 78.78 78. 85.20 86.85 87.13 80.03 85.87 77. 83.59 70.86 57.37 73.30 42.90 59. 36.50 52.90 67.70 79.40 72.80 78. 72.50 63.00 72.50 57.70 33.10 85. 81.07 69.60 53.70 A.2 Results of pretrained models Results of pretrained models are presented in Table 6. A.3 Architecture of intermediate sizes and dense model Architecture configurations of intermediate sizes for Phi-mini-MoE and Phi-tiny-MoE are shown in Table 7. Architecture configurations of pruned Phi 3 medium are shown in Table 8. A.4 Results with lower training budgets. We evaluated SlimMoEs effectiveness under reduced training budgets. We conducted experiments on Phi-mini-MoEs size using only 90B total tokens, with 25B tokens allocated to the first stage. Table 9 presents the performance comparison between our multi-stage approach and the one-stage Table 6: Performance comparison of pretrained models on various benchmark tasks. * We report the performance of the instruct model for Phi 3.5-MoE. Model (pretrained) # Total # Act. param param Mixture-of-Experts (MoE) Models MMLU Winograde Arc-C Hellaswag BoolQ Phi 3.5-MoE* Qwen 1.5 MoE DeepSeek V2 Lite OL-MoE Granite 3.0 MoE"
        },
        {
            "title": "Dense Models",
            "content": "LLaMA 3.1 8B Qwen 2.5 7B Gemma 3 4B LLaMA 3.2 3B Qwen 2.5 3B Gemma 3 1B LLaMA 3.2 1B"
        },
        {
            "title": "Our Models",
            "content": "Phi-mini-MoE Phi-tiny-MoE 42B 14B 16B 6.9B 3.4B 8B 7.6B 4.3B 3.2B 3.1B 1B 1.2B 7.6B 3.8B 6.6B 2.7B 2.4B 1.3B 0.8B 8B 7.6B 4.3B 3.2B 3.1B 1B 1.2B 78.36 61. 58.09 54.96 48.44 65.22 74.20 59. 56.07 65.55 26.26 31.00 2.4B 1.1B 69.87 60.08 78.61 71.82 75.69 72. 67.8 77.03 75.90 72.53 72.14 71. 61.01 62.03 75.85 71.90 65.52 56. 56.31 58.53 49.57 57.51 63.31 58. 48.38 57.41 39.68 38.30 82.31 79. 79.79 79.12 73.15 80.89 79.46 77. 75.46 74.49 62.38 65.70 62.29 57. 76.03 67.33 88.93 80.79 79.34 77. 75.38 82.69 87.77 80.03 73.30 83. 64.77 65.50 85.14 80.97 baseline. The results demonstrate that even with 4.4 reduction in training tokens, our multistage framework consistently outperforms the one-stage approach, confirming that SlimMoEs advantages persist even under resource constraints. A.5 Training Details Training Hyperparameters. We use different hyperparameter settings during the compression process and subsequent SFT distillation. During compression, we use the AdamW optimizer with learning rate of 1e-4 and weight decay of 0.01. We apply cosine learning rate decay with 100 warmup steps. For SlimMoE, we apply these warmup steps at the beginning of each stage. We use batch size of 4096 and maximum sequence length of 4096. For SFT distillation, we maintain the same optimizer but reduce the learning rate to 2e-6 and weight decay to 1e-4. We use batch size of 1536. All training was conducted on 64 A100 GPUs. 24 Table 7: Model Configuration Details with Parameters"
        },
        {
            "title": "Model",
            "content": "dmodel nhead dexpert nlayer nexpert top-k Phi 3.5-MoE / GRIN-MoE Phi-mini-MoE-stage1 Phi-mini-MoE (stage2) Phi-tiny-MoE-stage1 Phi-tiny-MoE-stage2 Phi-tiny-MoE (stage3) 4096 4096 4096 4096 4096 32/8 32/8 32/8 24/6 20/5 16/ 6400 2240 960 2624 1024 32 32 32 32 32 16 16 16 16 16 2 2 2 2 2 # Total # Act."
        },
        {
            "title": "Param",
            "content": "41.9B 15.7B 7.6B 17.8B 7.6B 3.8B 6.6B 3.4B 2.4B 3.3B 1.9B 1.1B Table 8: Model Configuration Details for Phi 3 Dense Models"
        },
        {
            "title": "Model",
            "content": "dmodel nhead dffn nlayer Phi 3 Dense 35% Phi 3 Dense 16% Phi 3 Dense 5120 5120 5120 40/10 40/10 20/5 3297 1098 40 40 # Total"
        },
        {
            "title": "Param",
            "content": "14.0B 5.0B 1.9B A.6 Evaluation Details We use lm-evaluation-harness (Gao et al., 2024) for evaluations on all tasks except MT-bench, Humaneval and MBPP, where we follows original implementation of MT-bench and use evalplus base mode (Liu et al., 2023a) for coding tasks. We use different evaluation settings for pretrained and instruct model. For pretrained models, we use 5 shots for all tasks. For instruct models, we use 5 shots for MMLU, MMLU pro, Winograde, Hellaswag and PIQA, 0 shot for Arc-challenge with chat prompt, 0 shot cot for GPQA, 2 shots for BoolQ, 10 shots for openbookQA, 3 shots for BBH and 8 shots for GSM8K. We apply chat template when evaluating instruct models. A.7 Computational cost For inference speed profiling, we use vLLM as the serving backend for all evaluated models. The number of concurrent clients is set as default (1, 2, 4, 6, 8, 12, 16, 20, 24, 28, 32). The prompt of request has mean length of 2048 tokens and the generation length is 256 tokens. For each client load, we record both throughput total latency. We also conduct memory profiling during finetuning Phi-mini-MoE and Phi-tiny-MoE in Table 10. 25 Table 9: Performance comparison under reduced training budget (90B tokens total) Method MMLU Arc-C HellaSwag One-stage Multi-stage 64.11 65.59 59. 62.54 73.23 73.38 Table 10: Memory Profiling Results Model Optimizer Peak Allocated Memory (MB) Peak Reserved (MB) Phi-mini-MoE Phi-tiny-MoE 8-bit Adam Muon 8-bit Adam Muon 59204.31 59759.51 40573.62 40556.37 68756. 61690.00 47046.00 42862.00 A.8 Expert similarity We notice that in some existing works (Lu et al., 2024; Liu et al., 2024a), the performance drop observed with expert pruning is smaller than what we have observed on Phi-3.5-MoE. We attribute this discrepancy to the differences between model families. Previous studies mainly used Mixtral as their target model, where experts exhibit high similarity, possibly because of tailored initialization. This similarity could reduces the performance impact of expert pruning. In contrast, Phi-MoE-3.5 appears to distribute knowledge more heterogeneously across experts. We present our detailed study below: We calculated the expert similarity in Mixtral 78B and Phi-3.5-MoE. Figure 5 presents the distribution of maximum cosine similarities between neurons across different experts within each model. We randomly sample pair of experts from each model. For each neuron in the first expert, we identified the neuron in the second expert with the highest cosine similarity, then plotted the distribution of these maximum similarity values across all neurons. The results show that Mixtral exhibits substantially higher inter-expert similarity. We also observe that in Mixtral, the neurons with highest cosine similarity tend to be positioned at corresponding indices across experts. In contrast, Phi 3.5-MoE shows lower similarity and lacks this positional correspondence pattern. (a) Mixtral 7x8B (b) Phi 3.5-MoE Figure 5: Cosine similarity distribution of neurons in randomly sampled two experts"
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "Microsoft"
    ]
}