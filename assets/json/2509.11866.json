{
    "paper_title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding",
    "authors": [
        "Meng Luo",
        "Shengqiong Wu",
        "Liqiang Jing",
        "Tianjie Ju",
        "Li Zheng",
        "Jinxiang Lai",
        "Tianlong Wu",
        "Xinya Du",
        "Jian Li",
        "Siyuan Yan",
        "Jiebo Luo",
        "William Yang Wang",
        "Hao Fei",
        "Mong-Li Lee",
        "Wynne Hsu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/Eurekaleo/Dr.V."
        },
        {
            "title": "Start",
            "content": "Dr.V : Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding Meng Luo1 Tianlong Wu1 Xinya Du2 Shengqiong Wu1 Liqiang Jing2 Tianjie Ju1 Li Zheng Jinxiang Lai4 Jiebo Luo7 William Yang Wang8 Jian Li5 Siyuan Yan6 Hao Fei1,(cid:66) Mong-Li Lee1 Wynne Hsu1 5 2 0 2 5 1 ] . [ 1 6 6 8 1 1 . 9 0 5 2 : r 1NUS 2UTD 3WHU 4HKUST 5NJU 6Monash 7UR 8UCSB Figure 1. (a) Hierarchical taxonomy of video hallucinations in LVMs, from Level-1 (perceptive) to Level-3 (cognitive), with increasing reasoning complexity. (b) Representative examples of hallucination types: static attribute, dynamic relation, and context-based explanation."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: benchmark dataset Dr.V-Bench and satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.VAgent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective (cid:66)Corresponding Author. in diagnosing hallucination while enhancing interpretability and reliability, offering practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at https://github.com/ Eurekaleo/Dr.V . 1. Introduction Video understanding has long been crucial research area in artificial intelligence, focusing on the interpretation of dynamic content in videos, which is inherently more complex than static images. This complexity arises from the need to understand not only spatial visual semantics but also temporal sequences [2, 3, 27, 68]. Built upon the foundation of large language models [1, 5, 21, 51], large video models (LVMs) [6, 9, 35, 49] have significantly propelled progress in comprehending video content, such as video question answering [15, 34, 65], video captioning [11, 16, 60]. Despite these advancements, the generative nature of LVMs inevitably introduces notable risk of hallucinations generating content that is inconsistent with the video content, misaligned with user intent, or factually incorrect [7, 36, 63]. Such hallucinations undermine the reliability and trustworthiness of LVMs, posing critical challenges to their deployment in real-world scenarios. To combat the hallucination issues in LVMs, it is essential to develop comprehensive and accurate evaluation protocols. The research community has responded to this need by proposing several video hallucination benchmarks. Initial efforts, such as the pioneering VideoHallucer [55], established foundational framework by categorizing hallucinations into intrinsic and extrinsic types. Building on this, subsequent works have expanded the scope, often focusing on more specific facets of hallucination. For instance, some benchmarks target fine-grained temporal reasoning, including action sequences and scene transitions [28, 63], while others concentrate on motion-related fallacies [26] or action-scene inconsistencies [4]. Other novel approaches have explored using synthetic videos to test for violations of common sense and physical laws [33], or have structured their evaluation along multiple dimensions like hallucination causes and question formats [19]. However, despite these valuable contributions, we argue that the current landscape of video hallucination evaluation suffers from two critical limitations, which prevent truly holistic assessment: Fragmented and Incomplete Taxonomies. Existing benchmarks, while strong in their respective niches, tend to address isolated aspects of hallucination. This specialization has led to fragmented understanding of the problem, with no single benchmark offering sufficiently comprehensive and unified taxonomy. Many common yet complex hallucination types, such as those involving dynamic attributes or nuanced context-based reasoning, remain underexplored. The coarse-grained or overly specialized categories used to date fail to capture the multi-dimensional nature of video hallucinations. Lack of Granularity in Annotation and Analysis. The majority of current benchmarks are limited to instance-level labels, which can identify that hallucination occurred, but not why or where. They often lack fine-grained annotations, such as the precise spatialtemporal grounding of the error within the video. This methodological gap makes it challenging to conduct root-cause analysis and develop targeted mitigation strategies, as the models failure points remain unclear. To address the limitations of existing datasets, we introduce Dr.V-Bench, novel benchmark designed to comprehensively evaluate video hallucinations. First, we conduct an in-depth analysis of all possible types of video hallucinations and propose hierarchical taxonomy based on the complexity of reasoning and levels of abstraction. As illustrated in Figure 1, the taxonomy comprises three levels. Level 1: Perceptive Hallucination, which includes errors in basic perceptive understanding such as object recognition, numerical estimation, color identification, object localization, static spatial relations, and optical character recognition (OCR). Level 2: Temporal Hallucination, which involves misinterpretations of temporal dynamics, including action recognition, dynamic attribute recognition (e.g., speed, motion direction), dynamic relational inference, and event sequence understanding. Level 3: Cognitive Hallucination, which encompasses higher-level reasoning failures such as factual prediction, counterfactual prediction, context-based explanation, and knowledgebased explanation. Second, we curate large-scale collection of videos from diverse sources, emphasizing content that is complex, multi-faceted, and rich in temporal and semantic variation. Third, we carefully design three evaluation tasks, i.e., Yes/No Question Answering (QA), MultipleChoice QA, and Video Captioning, each tailored to probe the 14 identified hallucination types across different reasoning levels. Moreover, we provide fine-grained annotations for each video instance, including detailed spatialtemporal grounding of objects relevant to the associated queries. Finally, through rigorous multi-stage human verification process, we construct high-quality dataset containing 10,000 annotated instances paired with 4,974 videos. Each instance is enriched with spatial-temporal grounding information, enabling comprehensive evaluation of hallucination phenomena in LVMs. Building upon the proposed benchmark, we further aim to develop model capable of diagnosing and mitigating video hallucinations in LVMs. Drawing inspiration from the mechanisms underlying human video comprehension, we revisit the root causes of video hallucination and present two key insights: First, humans are capable of finely perceiving and localizing relevant content in video, effortlessly identifying the target objects and the specific spatialtemporal segments corresponding to given query. In contrast, LVMs often suffer from hallucinations due to their limited fine-grained spatial-temporal grounding capabilities. That is, their inability to accurately interpret pixel-level spatial details and maintain coherent temporal understanding over long video sequences. Second, human reasoning typically unfolds in structured, step-by-step manner: beginning with low-level perception, progressing through temporal inference, and culminating in high-level cognitive reasoning. However, existing LVMs often lack this staged reasoning process, leading to conflated or misaligned representations across different levels of abstraction. Motivated by aforementioned observations, we propose novel diagnostic model, Dr.V-Agent, which emulates this hierarchical reasoning paradigm to identify and analyze hallucinations in LVMs. As shown in Figure 2, and inspired by the proposed hierarchical hallucination taxonomy, the core of Dr.V-Agent lies in hierarchi2 Figure 2. The scenario consists target LVM and our Dr.V-Agent hallucination diagnosis system. The overall workflow contains four steps: 1 The target LVM generates an answer for given video and QA input. 2 Dr.V-Agent diagnoses hallucinations by analyzing the VQA pair and LVMs response using chained-hierarchical reasoner. 3 The reasoner invokes expert tools to extract relevant video information for verification. 4 Dr.V-Agent generates feedback based on the analysis, prompting the target LVM to refine its response. cal chained reasoning mechanism, From-Perception-toTemporal-to-Cognition, which systematically diagnoses hallucination in progressive, interpretable manner. Given an input question, video, and generated answers by an LVM, Dr.V-Agent initiates progressive diagnostic process. At each stage of the reasoning chain, the agent selectively invokes advanced external tools to perform fine-grained spatial-temporal grounding directly on the raw video content, localizing relevant visual entities, tracking their temporal evolution, and verifying factual or contextual consistency with the question. Throughout this reasoning cascade, Dr.V-Agent continuously refines its understanding and, ultimately, produces detailed diagnostic report that highlights the specific sources and types of hallucination present in the original LVM response. Based on this analysis, the LVM is guided to revise its answer, yielding hallucination-free response. We conduct extensive evaluations of Dr.V-Agent on the proposed benchmark, demonstrating its effectiveness in significantly reducing hallucination errors across wide range of state-of-the-art (SOTA) LVMs. In addition, the interpretability and modular design of the agent provide transparent insights into model behavior and failure modes. In summary, this paper makes the following key contributions: We first introduce the hierarchical hallucination taxonomy of video hallucinations that across three distinct reasoning levels: perception, temporal, and cognition. This taxonomy provides principled framework for analyzing hallucination phenomena in LVMs. We construct Dr.V-Bench, the most comprehensive and large-scale benchmark, which comprises 10,000 instances spanning 14 hallucination types across wide range of complex video scenarios, and is enriched with fine-grained spatial-temporal grounding annotations to support detailed diagnostic analysis. We propose Dr.V-Agent, novel diagnostic framework for video hallucination detection and mitigation. The agent performs chained hierarchical reasoning progression and leverages advanced external tools for evidence verification, producing structured diagnostic feedback to guide LVMs toward generating more faithful and interpretable outputs. We conduct extensive experiments to demonstrate the efficacy of the Dr.V-Bench, and effectiveness of the proposed Dr.V-Agent across multiple LVMs. 2. Related Work 2.1. LVMs and Hallucination In recent years, the emergence of LLMs [17, 30, 39] has demonstrated unprecedented intelligence across various AI tasks. Building on top of LLMs, visual/video LLMs [25, 34, 56, 57] (aka LVMs(cid:66)) have also garnered significant research attention, leading to evident performance improvements in vision and video tasks. However, due to their generative nature, all these models inevitably produce outputs that deviate from the given input or reality, known as hallucination problems. To combat hallucinations in LVMs, certain methods have been proposed. For example, Vista-LLaMA [40] optimizes the consistent distance between visual and language tokens. Sun et al. [48] propose optimizing video keyframe retrieval to address incorrect references in videos. Zhang et al. [63] introduce Temporal Contrastive Decoding strategy to reduce eventrelated hallucinations in existing LVMs. Wang et al. [55] attempt to prompt LVMs with self-reflection to mitigate hallucinations. Another approach, MASH-VLM [4], specifically targets action-scene hallucination by disentangling spatial and temporal representations within the model, preventing the model from incorrectly inferring actions based on scene context, or vice versa. More recently, methods (cid:66)V can refer to either Visual or Video; in this work, we focus on video LVMs. 3 Figure 3. Data construction and annotation pipeline of our Dr.V-Bench dataset. based on Direct Preference Optimization (DPO) [43] have become prominent research direction for mitigating hallucinations. These methods align model outputs with human or AI-provided preferences. For instance, some work leverages rewards from powerful language models to guide the DPO process, using detailed video captions as proxy for video content to assess the factuality of generated responses [64]. Building on this, PaMi-VDPO [14] introduces an online preference learning framework that generates negative samples through video augmentation, guided by prompt awareness to avoid false rejections and better enforce videoresponse alignment. Further pushing the boundaries of finegrained alignment, VistaDPO [23] proposes hierarchical DPO framework that optimizes text-video preferences at three distinct levels: instance, temporal, and perceptive. Similarly, the work on HAVEN [19] employs combination of supervised reasoning fine-tuning and DPO to enhance the models reasoning process and reduce hallucinations. viewpoint extensively verified in relevant studies [7, 46] emphasizes that the root cause of visual hallucination lies in the lack of faithfulness to the given input video or facts, which can be attributed to the absence of fine-grained spatial-temporal-aware grounding. Only when models can accurately capture spatial-temporal details can they correctly perceive the existence, attributes, and relationships of objects in videos, understand the inherent dynamic temporal information, and further perform accurate semantic reasoning. Therefore, this work incorporates spatial-temporal grounding as core component in the design of our video hallucination diagnosis system. 2.2. Video Hallucination Benchmarks On the other hand, efforts should be directed towards the construction of video hallucination benchmarks, which are also key to addressing hallucination issues. Several related benchmarks have been proposed in recent years. Wang et al. [55] introduce the VideoHallucer, where video hallucinations are categorized into intrinsic and extrinsic types, resulting in dataset of 1.8k Video QA instances. VidHalluc [28] provides dataset with 5k videos and defines three categories of hallucinations: action, temporal sequence, and scene transition. EventHallusion [63] further focuses on mitigating video hallucinations by evaluating the event/action understanding of models. More recent benchmarks have expanded the scope and complexity. For example, Li et al. [33] introduce VideoHallu, benchmark focused on synthetic videos designed to test models ability to detect violations of common sense and physical laws. Gao et al. [19] present HAVEN, comprehensive benchmark with 6k questions built upon three dimensions: hallucination causes, hallucination aspects, and question formats. The MASHVLM work is accompanied by the UNSCENE benchmark, which is specifically curated to evaluate action-scene hallucination by using videos with unusual contexts or sceneonly content [4]. Furthermore, to support preference-based learning methods, VistaDPO provides 7.2k question-answer pairs annotated with both chosen and rejected responses [23]. We observe that none of these existing video hallucination benchmarks provide comprehensive taxonomy definition for video hallucinations, which should be critical aspect. Through an extensive survey, we have identified that all sources of video hallucination can be categorized into three major dimensions: Perception, Temporal, and Cognition, each of which can be further subdivided into more specific subcategories. Moreover, we include high-quality spatial-temporal grounding annotations as an essential part of our benchmark, resulting in the largest-scale video hallucination dataset to date, with 10k instances, the most comprehensive category definitions, and the most diverse video scenes in the community. 3. Preliminaries Video hallucination refers to phenomenon in which an LVM may produce hallucinated output that conflicts with factual content or even invent entirely new objects, scenes, or actions to the input video. Existing LVMs typically exFigure 4. Main statistics of our Dr.V-Bench dataset. The left bar chart presents the number of instances, videos, and dataset sources for different task types, and the right pie chart illustrates the proportion of each hallucination type. hibit hallucinations across various video-related tasks. Hallucination-diagnosis System. Given target LVM , capable of video comprehension, that takes as input video and textual prompt corresponding to task query and outputs an answer after reasoning. Our goal is to develop hallucination-diagnosis system that is independent of the target LVM to diagnose potential hallucinations by analyzing the answer from the target LVM If contains based on the raw input video and . hallucination, our system will generate rationale pinpointing the specifics of the hallucination, which serves as feedback to the target to refine its response. Taxonomy of Video Hallucination. Video hallucinations differ significantly from those in static images due to the rich spatial semantics and complex temporal dynamics inherent in videos. These additional layers of complexity mean that video hallucinations can manifest across spectrum of levels, from low-level perceptive and temporal aspects to high-order cognitive functions. As such, we propose new taxonomy of video hallucination. Inspired by human intuition, we categorize hallucinations hierarchically into 3 levels, covering total of 14 fine-grained types. 4. Dr.V-Bench: Comprehensive Benchmark for Video Hallucination We introduce Dr.V-Bench, novel and comprehensive benchmark meticulously designed to evaluate and diagnose hallucinations in LVMs. This section details its construction pipeline, quality control protocols, statistical properties, and advancements over existing benchmarks. Figure 3 provides high-level overview of the entire data construction and annotation process. 4.1. Data Collection and Curation The foundation of Dr.V-Bench is built upon diverse collection of 15 well-established, public video datasets. These source datasets provide vast pool of existing QA pairs, predominantly in multiple-choice format, as shown in Table 5. This allows us to ground our work in questions that are already validated by the community. We dont create QA from scratch; instead, we meticulously curate and transform existing data for the specific purpose of hallucination analysis. Before this transformation, we perform rigorous manual curation process on the original data. We review the original QA pairs to filter out any that are ambiguous, factually incorrect, or unrelated to the video content. We also assess video quality, excluding videos that are of unreasonable length or lack sufficient information for definitive answer. This ensures that the data foundation for our benchmark is clean and reliable. 4.2. Hallucination-centric QA Generation Following curation, we restructure the validated QA pairs to facilitate the diagnosis of hallucinations through three key steps: taxonomic classification, reconstruction of answer options, and format diversification. First, we meticulously map each curated QA pair to our proposed taxonomy of hallucinations. Each question is analyzed and categorized into one of the fine-grained hallucination types under the three levels. This step imposes systematic, hierarchical structure on the previously uncategorized data, enabling targeted evaluation of specific hallucination vulnerabilities. Second, we reconstruct the answer options to create challenging, hallucination-centric test cases. The original incorrect options in the source datasets are often trivial or not designed to mimic plausible model failures. To address this, we design specific prompt templates (see Appendix C) tailored to each hallucination type. These templates guide GPT4o [24] to generate new incorrect answers that are deliberately misleading and act as convincing hallucinated foils to the ground-truth answer. This reconstruction transforms standard QA task into rigorous test of models ability to resist specific types of hallucinations. Finally, based 5 Figure 5. Our Dr.V-Agent framework diagnoses and locates hallucinations through chained perception-temporal-cognition reasoning process. Different hallucinations will select different reasoning paths. on the restructured multiple-choice QA, we diversify the task formats to create multi-faceted evaluation suite. We design three distinct QA formatsyes/no, multiple-choice, and caption generationto assess both discriminative and generative model capabilities. 4.3. Fine-grained Spatial-Temporal Grounding To enable fine-grained analysis, we manually annotate the videos with precise spatial-temporal information. The annotation process consists of four steps: extraction of target objects, annotation of start and end frames, annotation of key frames, and annotation of bounding boxes. Detailed annotation procedures are provided in Appendix C.4. 4.4. Dataset Statistics Dr.V-Bench is large-scale benchmark that sets new standard in the field. It contains 10k instances distributed across 4.9k unique videos. The dataset covers 50 diverse scenarios/domains, such as daily scenes, life recordings, artistic performances, and sports competitions. The tasks are broken down into 3k yes/no QA, 6k multiple-choice QA, and 1k caption generation QA instances, as illustrated in Appendix C. Notably, around 25% of the videos feature significant complexity (e.g., multiple scene transitions, concurrent overlapped events, and questions requiring higherlevel reasoning), ensuring rigorous test of an LVMs capabilities. This comprehensive design ensures sufficient generalizability for LVMs to adapt to tasks and contents requiring domain-specific reasoning. An overview of the dataset statistics is provided in Figure 4. 5. Dr.V-Agent for Mitigating Hallucination Dr.V-Agent is an agentic system that leverages LLMs to dynamically integrate various SOTA external tools. It employs chained-hierarchical perception-temporal-cognition reasoning process to diagnose and reason about hallucinations in fine-grained manner. The framework dynamically selects reasoning path based on the potential hallucinations complexity, as illustrated in Figure 5. For instance, perceptive level hallucinations are diagnosed via an abbreviated path (Steps 1, 2, 5, and 6), and temporal level issues require the inclusion of temporal checking (Steps 1, 2, 3, 5, and 6), while only the cognitive level hallucinations necessitate the complete six-step reasoning chain. This hierarchical approach not only mirrors the logical dependencies from perception to cognition but also ensures an efficient and targeted analysis. Step 1: Hallucination Type Classification. In this step, we use GPT-4o to analyze the QA pair and the target LVMs response to extract the set of objects denoted as = {o1, . . . , on}, events = {e1, . . . , em}, and causal claims = {c1, . . . , ck} where denotes cause-and-effect relationships stated in the QA. Step 2: Perceptive Level Checking. Perceptive level understanding requires accurate object identification and relative spatial positioning. To ensure high robustness and mitigate the risk of errors from any single tool, we employ dual-tool approach for cross-validation. Specifically, we use Grounded SAM 2 [45] andYOLO-World [10] for open-set object detection and grounding. For each object O, we enhance detection precision by taking the intersection of the bounding boxes predicted by both tools, and its timestamp is determined by averaging the timestamps they report. Step 3: Temporal Level Checking. This step is activated only for temporal and cognitive level inquiries. Temporal understanding requires event detection and verifying temporal order. Following the same principle of robust, cross-validated analysis, we utilize CG-STVG [20] and Grounded-VideoLLM [52] for temporal video grounding. With the events identified and cross-verified by these models, we then verify whether the events occur and whether their temporal order aligns with the expected sequence based on the ground truth."
        },
        {
            "title": "Avg",
            "content": "Obj. Col. Num. Loc. SRel. OCR Act. Atr. DRel. Seq. Fct. CnFct. Cxt. Knk. Open-source LVMs 41.88 38.15 30.14 64.22 36.20 30.50 34.06 28.61 37.77 28.52 36.68 VideoChat2 (7B) [31] 39.05 35.15 36.30 60.62 33.00 41.68 35.09 34.94 38.19 30.78 33.61 Video-ChatGPT (7B) [41] 53.57 50.58 48.98 80.93 56.65 42.86 38.38 39.93 45.09 45.02 48.49 Video-LLaVA (7B) [34] LLaMA-VID (7B) [32] 52.42 44.64 46.52 78.72 55.94 51.40 39.08 40.84 43.05 37.99 45.99 LLaVA-NeXT-Video-DPO (7B) [65] 54.76 58.32 58.02 83.65 63.39 49.25 48.19 44.79 49.47 39.00 45.76 68.00 65.80 73.03 83.56 76.65 57.54 43.60 49.00 55.93 44.50 59.71 PLLaVA (7B) [60] 73.88 72.95 72.82 90.93 76.14 64.20 60.66 50.96 64.84 52.83 63.47 InternVL2 (8B) [9] 76.57 76.66 72.84 90.33 79.31 73.55 65.72 55.81 70.55 64.02 70.01 Qwen2-VL (7B) [53] 33.96 36.36 39.31 51.66 60.77 65.66 61.22 68.83 36.35 30.85 36.28 38.28 39.05 38.01 61.16 50.03 50.07 61.03 53.86 50.22 70.64 57.46 56.80 71.53 61.73 62.58 74.55 64.38 67.42 81.21 71.91 72.67 Closed-source LVMs GPT-4o [24] Gemini-1.5-Pro [49] 81.25 81.16 78.03 93.02 82.81 78.51 72.58 60.83 75.20 68.76 74.33 83.29 83.55 80.90 94.04 84.97 80.34 75.76 63.51 77.76 70.54 77.20 73.74 76.46 86.03 75.85 77.29 88.24 79.01 79."
        },
        {
            "title": "Human",
            "content": "98.54 99.63 95.80 99.50 99.00 98.05 90.12 89.22 96.50 89.59 96.00 90.40 99.50 91.62 95.25 Table 1. Performance of LVMs on Dr.V-Bench. Columns represent the hallucination types: Object (Obj.), Color (Col.), Number (Num.), Location (Loc.), Static Relation (SRel.), OCR; Action (Act.), Dynamic Attribute (Atr.), Dynamic relation (DRel.), Sequence (Seq.); Factual Prediction (Fct.), Counterfactual Prediction (CnFct.), Context-based Explanation (Cxt.), Knowledge-based Explanation (Knk.). Step 4: Cognitive Level Checking. Cognitive level understanding requires integrating spatial-temporal cues with commonsense or contextual knowledge to interpret causal relationships. For each causal claim in C, we use InternVL2 [9] and Qwen2-VL [6] to generate dense descriptive captions that explicitly depict cause-and-effect relationships. The captioning process focuses on the corresponding event duration detected in Step 3. Step 5: Reasoning. We diagnose hallucinations in the target LVMs response by utilizing DeepSeek R1 [21] to identify inconsistencies between the spatial, temporal, and cognitive information obtained in Steps 24 and the responses generated by the LVM. Step 6: Feedback Generation. This final step consolidates the detected hallucinations into structured format, providing clear feedback for the LVM to improve its understanding capability. For each detected hallucination, we generate the feedback = (A, R) where is the extracted spatialtemporal-causal information, including objects with their bounding boxes, events with their timestamps, and causal claims C, as well as supporting captions, and consists of suggestions for LVM to refine its response. Our design of Dr.V-Agent offers three key advantages over foundational video models. First, Dr.V-Agent has the flexibility to utilize SOTA tools, thereby overcoming bottlenecks in the target LVM that may cause hallucinations. For example, GPT-4o serves as the core LLM for planning and reasoning, while models such as SAM2 are used for precise spatial-temporal grounding in videos. Second, our solution operates under training-free paradigm, eliminating the need for retraining or fine-tuning on additional datasets. Third, its adaptive, hierarchical reasoning process ensures efficiency by tailoring the computational path to the complexity of the hallucination, thus avoiding the overhead associated with fixed, monolithic pipeline. 6. Performance and Discussions Settings. To comprehensively evaluate the capabilities of LVMs on our VideoQA tasks, we select ten representative models: eight open-source LVMs (VideoChat2 [31], VideoChatGPT [41], Video-LLaVA [34], LLaMA-VID [32], LLaVA-NeXT-Video-DPO [65], PLLaVA [60], Qwen2VL, and InternVL2) and two advanced closed-source models (GPT-4o and Gemini-1.5-Pro [49]). For fair comparison, all models are evaluated using their default hyperparameters, including max new tokens, do sample, temperature, and num of frames. For closed-source models, we set the frame sampling rate to 1 and limit the maximum number of frames to 128 to ensure evaluation efficiency while maintaining sufficient temporal coverage. 6.1. Performance of LVMs on Dr.V-Bench We evaluate range of open-source and closed-source LVMs on Dr.V-Bench. The comprehensive results are presented in Table 1. Overall, Dr.V-Bench proves to be highly challenging benchmark, with all tested models exhibiting significant hallucinations. clear performance hierarchy emerges from the results. Models generally perform best on perceptive tasks, with accuracy declining substantially for temporal and cognitive tasks. This indicates that while current LVMs have developed reasonable capacity for static scene understanding, they lack the more rigorous spatial-temporal understanding and advanced reasoning required for complex video analysis. For instance, even the top-performing open-source model, Qwen2-VL, sees its accuracy drop from 78.75% on perceptive tasks to 65.61% on temporal tasks in the multiple-choice QA setting. We also"
        },
        {
            "title": "Model",
            "content": "Obj. Col. Num. Loc. SRel. OCR Act. Atr. DRel. Seq. Fct. CnFct. Cxt. Knk. Avg. VideoChat2 + Self-PEP + Dr.V-Agent LLaVA-NeXT + Self-PEP + Dr.V-Agent Qwen2-VL + Self-PEP + Dr.V-Agent GPT-4o + Self-PEP + Dr.V-Agent 41.88 49.52 65.40 54.76 49.24 76.76 76.57 80.37 89.19 81.25 87.63 95.23 38.15 43.60 54. 58.32 54.39 74.01 76.66 79.37 85.66 81.16 85.70 91.13 30.14 35.17 45.62 58.02 54.40 72.49 72.84 75.34 81. 78.03 82.22 87.23 64.22 66.81 72.18 83.65 81.79 91.09 90.33 91.61 94.60 93.02 95.18 97.75 36.20 45.12 63. 63.39 56.97 89.04 79.31 83.73 94.03 82.81 90.23 98.12 30.50 39.10 56.95 49.25 43.04 74.01 73.55 77.83 87. 78.51 85.69 94.26 34.06 37.19 43.69 48.19 45.93 57.20 65.72 67.27 70.89 72.58 75.19 78.31 28.61 31.04 36. 44.79 43.04 51.79 55.81 57.02 59.83 60.83 62.86 66.28 37.77 45.52 61.55 49.47 43.90 71.72 70.55 74.39 83. 75.20 81.65 89.36 28.52 32.70 41.38 39.00 35.99 51.02 64.02 66.10 70.92 68.76 72.24 76.40 36.68 43.65 58. 45.76 40.74 65.82 70.01 73.47 81.53 74.33 80.14 87.30 33.96 37.22 43.99 60.77 58.42 70.15 68.83 70.45 74. 73.74 76.46 79.71 36.35 43.91 59.61 70.64 65.20 92.39 81.21 84.96 93.69 86.03 92.33 99.06 30.85 34.68 42. 57.46 54.70 68.48 71.91 73.81 78.23 75.85 79.04 82.90 38.01 44.82 (+6.81) 53.43 (+15.42) 56.80 52.45 (-4.35) 74.21 (+17.41) 72.67 75.67 (+3.00) 82.64 (+9.97) 77.29 82.33 (+5.04) 88.36 (+11.07) Table 2. Representative results comparing Self-PEP vs. Dr.V-Agent on four indicative LVMs. ping LVMs with Dr.V-Agent consistently and substantially outperforms using the Self-PEP strategy across representative models and hallucination types. While SelfPEP yields moderate improvements in some cases, its effectiveness is inconsistent and can even lead to performance degradation compared to the vanilla model (e.g., -4.35% on LLaVA-NeXT; -5.30% on PLLaVA). In contrast, Dr.V-Agent achieves significant and robust gains across all tested models, with particularly pronounced improvement for lower-performing LVMs like VideoChat2 (+18.60%). Complete results for all models and hallucination types are provided in Appendix Table 10. Analysis of Performance and Efficiency. The superiority of Dr.V-Agent stems from two key advantages: its performance architecture and its efficiency. On Performance, unlike Self-PEPs reliance on internal knowledge which can be flawed, Dr.V-Agents success comes from its ability to leverage external, specialized tools for verification. By integrating fine-grained and reliable video information through these tools, it overcomes the limitations of self-correction and allows the base LVM to make more informed decisions. As illustrated in Figure 6, this leads to substantial improvements across specific hallucination types like Obj., SRel., OCR, and DRel., confirming that grounding the models reasoning in precise, externallyverified spatial-temporal information is pivotal for mitigating hallucinations. On Efficiency, Dr.V-Agent introduces paradigm shift. Its most critical advantage is being training-free. Unlike monolithic LVMs that require costly pre-training and fine-tuning, our agent operates by intelligently composing existing expert tools. This eliminates prohibitive computational overhead and enhances accessibility. Furthermore, its modular design makes it inherently flexible and future-proof. As better tools for object detection or temporal grounding emerge, they can be seamlessly integrated into framework without retraining the entire system, ensuring sustained high performance with minimal effort. Figure 6. VideoChat2 when equipped with Dr.V-Agent."
        },
        {
            "title": "Performance gains across hallucination types for",
            "content": "observe considerable performance gap between model families. Closed-source models like Gemini-1.5-Pro and GPT-4o consistently outperform open-source models across all categories, particularly in temporal and cognitive reasoning. While recent open-source models like Qwen2-VL and InternVL2 have narrowed this gap, significant disparity remains, especially in challenging temporal tasks like Sequence understanding and cognitive tasks like Counterfactual Prediction, as shown in Table 1. Nonetheless, no model yet approaches human-level proficiency, highlighting that video hallucination is severe and unsolved problem. 6.2. Evaluating the Effectiveness of Dr.V-Agent of the our validate proposed effectiveness To Dr.V-Agent, we conduct comparative analysis against strong and relevant baseline. We chose Self-PEP [55], representative and recent self-correction strategy, as it shares our goal of mitigating hallucinations in plug-and-play manner. Quantitative Comparison. As shown in Table 2, equip8 7. Conclusion We propose Dr.V, hierarchical framework to address hallucinations in LVMs. The Dr.V-Bench benchmark establishes three-tiered evaluation protocolperceptive, temporal, and cognitive levelssupported by 10k instances with spatial-temporal annotations across diverse video scenarios. Our Dr.V-Agent framework systematically mitigates hallucinations through structured pipeline: it first verifies fine-grained spatial-temporal grounding, then performs cognitive-level reasoning to align outputs to video content. Experiments demonstrate that this approach significantly reduces hallucination rates while enhancing interpretability, offering practical and robust solution for reliable video understanding in real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. 1 [3] Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. 1 [4] Kyungho Bae, Jinhyung Kim, Sihaeng Lee, Soonyoung Lee, Gunhee Lee, and Jinwoo Choi. Mash-vlm: Mitigating action-scene hallucination in video-llms through disentangled spatial-temporal representations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1374413753, 2025. 2, 3, 4 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, [7] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. 2, 4 [8] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190200, 2011. 12 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 7 [10] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openvocabulary object detection. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2024. 6, 18 [11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1 [12] Wey Yeh Choong, Yangyang Guo, and Mohan Kankanhalli. Vidhal: Benchmarking temporal hallucinations in vision llms. arXiv preprint arXiv:2411.16771, 2024. 13 [13] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, and Kui Ren. Sora detector: unified hallucination detection for large text-to-video models. arXiv preprint arXiv:2405.04180, 2024. 13 [14] Xinpeng Ding, Kui Zhang, Jianhua Han, Lanqing Hong, Hang Xu, and Xiaomeng Li. Pami-vdpo: Mitigating video hallucinations by prompt-aware multi-instance video preference learning. arXiv preprint arXiv:2504.05810, 2025. 4 [15] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Forty-first International Conference on Machine Learning. [16] Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, TatSeng Chua, and Shuicheng Yan. Enhancing video-language representations with structural spatio-temporal alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [17] Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, et al. On path to multimodal generalist: General-level and general-bench. In Proceedings of the International Conference on Machine Learning, 2025. 3 [18] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 12 [19] Hongcheng Gao, Jiashu Qu, Jingyi Tang, Baolong Bi, Yue Liu, Hongyu Chen, Li Liang, Li Su, and Qingming Huang. Exploring hallucination of large multimodal models in video understanding: Benchmark, analysis and mitigation. arXiv preprint arXiv:2503.19622, 2025. 2, 4 [20] Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, and Libo Zhang. In ProContext-guided spatio-temporal video grounding. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1833018339, 2024. 6, 18, 19 [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning 9 capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, [22] Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, et al. Mmworld: Towards multi-discipline arXiv multi-faceted world model evaluation in videos. preprint arXiv:2406.08407, 2024. 12 [23] Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, and Hao Fei. Vistadpo: Video hierarchical spatial-temporal direct preference optimization for large video models. arXiv preprint arXiv:2504.13122, 2025. 4 [24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 5, 7 [25] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pretraining with decoupled visual-motional tokenization. In International Conference on Machine Learning, pages 22185 22209, 2024. 3 [26] Ming Kong, Xianzhou Zeng, Luyuan Chen, Yadong Li, Bo Yan, and Qiang Zhu. Mhbench: Demystifying motion hallucination in videollms. In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 4401 4409, 2025. 2 [27] Gal Lavee, Ehud Rivlin, and Michael Rudzsky. Understanding video events: survey of methods for automatic interpretation of semantic occurrences in video. IEEE Transactions on Systems, Man, and Cybernetics, Part (Applications and Reviews), 2009. [28] Chaoyu Li, Eun Woo Im, and Pooyan Fazli. Vidhalluc: Evaluating temporal hallucinations in multimodal large lanarXiv preprint guage models for video understanding. arXiv:2412.03735, 2024. 2, 4, 13 [29] Jiangtong Li, Li Niu, and Liqing Zhang. From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2127321282, 2022. 12 [30] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, et al. survey on benchmarks of multimodal large language models. arXiv preprint arXiv:2408.08632, 2024. 3 [31] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 7 [32] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340, 2025. 7 [33] Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multi-modal hallucinations on synthetic video understanding. arXiv preprint arXiv:2505.01481, 2025. 2, 4 [34] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 3, 7 [35] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos, 2025. 1 [36] Hui Liu and Xiaojun Wan. Models see hallucinations: EvalarXiv preprint uating the factuality in video captioning. arXiv:2303.02961, 2023. 2 [37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 18 [38] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [39] Yulin Luo, Ruichuan An, Bocheng Zou, Yiming Tang, Jiaming Liu, and Shanghang Zhang. Llm as dataset analyst: Subpopulation structure discovery with large language model. In European Conference on Computer Vision, pages 235252. Springer, 2024. 3 [40] Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing hallucination in video language models via equal distance to visual tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1315113160, 2024. 3 [41] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 7 [42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 12 [43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 4 [44] Vipula Rawte, Sarthak Jain, Aarush Sinha, Garv Kaushik, Aman Bansal, Prathiksha Rumale Vishwanath, Samyak Rajesh Jain, Aishwarya Naresh Reganti, Vinija Jain, Aman Chadha, et al. Vibe: text-to-video benchmark for evaluating hallucination in large multimodal models. arXiv preprint arXiv:2411.10867, 2024. 13 [45] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: 10 Assembling open-world models for diverse visual tasks, 2024. 6, 18 ceedings of the International Conference on Machine Learning, pages 5336653397, 2024. 3 [58] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 12 [59] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 12 [60] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 1, [61] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. 12 [62] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2019. 12 [63] Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang. Eventhallusion: Diagnosing event hallucinations in video llms. arXiv preprint arXiv:2409.16597, 2024. 2, 3, 4, 13 [64] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 4 [65] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 1, 7 [66] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing Zhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu, Guangze Li, et al. Towards video text visual question answering: Benchmark and baseline. Advances in Neural Information Processing Systems, 35:3554935562, 2022. [67] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 12 [68] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. 1 [46] Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, and Aman Chadha. comprehensive survey of hallucination in large language, image, video and audio foundation models. Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1170911724, 2024. 4 [47] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. arXiv preprint arXiv:2410.23266, 2024. 12 [48] Yiwei Sun, Zhihang Liu, Chuanbin Liu, Bowei Pu, Zhihan Zhang, and Hongtao Xie. Hallucination mitigation arXiv preprint prompts long-term video understanding. arXiv:2406.11333, 2024. 3 [49] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, [50] George Tom, Minesh Mathew, Sergi Garcia-Bordils, Dimosthenis Karatzas, and CV Jawahar. Reading between the In International Conferlanes: Text videoqa on the road. ence on Document Analysis and Recognition, pages 137 154. Springer, 2023. 12 [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [52] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temarXiv poral grounding in video large language models. preprint arXiv:2410.03290, 2024. 6, 18 [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 [54] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. 12 [55] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. 2, 3, 4, 8, 13 [56] Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards semantic equivalence of tokenization in multimodal llm. arXiv preprint arXiv:2406.05127, 2024. [57] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExT-GPT: Any-to-any multimodal LLM. In Pro-"
        },
        {
            "title": "Appendix Overview",
            "content": "The appendix presents more details and additional results not included in the main paper due to page limitations. The list of items included is: Limitation in Section A. Ethic Statement results in Section B. Extended Dataset Details in Section C. Extended Details of the Dr.V-Agent in Section D. Extended Experiment Results and Analyses in Section E. A. Limitation While our design of Dr.V-Agent and Dr.V-Bench provides fine-grained and interpretable framework for video hallucination diagnosis, we acknowledge several limitations in the current work. Reliance on External Tool Performance. The effectiveness of Dr.V-Agent heavily relies on the performance of the external tools it invokes for tasks like spatial and temporal grounding. Although we employ SOTA tools, their inherent limitations can impact the accuracy of spatialtemporal annotations. Consequently, this can affect the precision of the final diagnosis and mitigation of hallucinations. System Complexity and Computational Overhead. To achieve fine-grained reasoning, our multi-agent approach invokes up to eight external models, which introduces significant system complexity and computational overhead. Compared to end-to-end models, this multi-step, sequential process can lead to higher latency. Annotation Cost and Scalability of the Benchmark. The construction of Dr.V-Bench depends on fine-grained manual spatial-temporal annotations. While this level of detail is critical for accurate hallucination diagnosis, the associated high annotation cost and time investment are key factors limiting the benchmarks scalability. This makes it challenging to rapidly expand the dataset to larger scale or to more diverse video domains. Indirect Assessment of Generative Capabilities. While Dr.V-Bench provides robust benchmark for diagnosing hallucination, its evaluation of generative capabilities is adapted into QA-based framework, specifically through caption generation QA. This design simplifies the targeted assessment of hallucinations but sacrifices direct and comprehensive evaluation of models free-form generative abilities, such as fluency, coherence, and creativity. B. Ethic Statement The source videos of the Dr.V-Bench rely exclusively on publicly available video datasets. No extra private or sensitive content is used, ensuring compliance with ethical guidelines and respect for data privacy. The development of the spatial-temporal information is made possible through the dedicated efforts of group of human annotators. All annotators are PhD students with specialized training in spatial-temporal annotation to ensure accuracy and reliability. In alignment with our goal of fostering an inclusive and collaborative research community, all data and resources of Dr.V-Bench will be openly available. This ensures equitable access for researchers and practitioners from diverse backgrounds, enabling broader contributions to advancements in video hallucination diagnosis and mitigation within LVMs. C. Extended Dataset Details The foundation of Dr.V-Bench is built upon diverse collection of 15 well-established, public video datasets. These include ActivityNet-QA [62], CausalVidQA [29], NExT-QA [58], YouCook2 [67], CLEVRER [61], ViteVQA [66], RoadTextVQA [50], TempCompass [38], Tomato [47], MMWorld [22], Video-MME [18], VCGBench-Diverse [42], MSVD [8], MSR-VTT [59], and VATEX [54]. As summarized in Table 4., these sources cover wide array of video types and tasks, providing robust basis for evaluation. C.1. Taxonomy of Video Hallucination Perceptive Hallucination. This level of hallucination involves the models failure to correctly recognize basic static content in the video, including object recognition, identification of static attributes such as number, color, and position, understanding static spatial relations between objects, and extracting textual information through processes like OCR. This is the lowest level of hallucination and is the stage at which many LVMs commonly exhibit errors. Temporal Hallucination. Temporal hallucinations involve deficiencies in models capability to process and interpret dynamic temporal information within videos. LVMs typically struggle with recognizing actions and events, identifying dynamic attributes such as the direction of movement, speed, and sequences of actions, understanding the dynamic relations between events, and recognizing the action sequence of the video. Cognitive Hallucination. Cognitive hallucinations encompass failures in the semantic understanding and cognitive processing of video content. LVMs exhibiting cognitive hallucination often fail in several key aspects. First, they may struggle with factual prediction, which requires predicting future actions beyond the content of the current video clips. Second, they may exhibit errors in counterfactual prediction, which"
        },
        {
            "title": "Level",
            "content": "#H-Types #Samples #Videos Dur.(s) #Domains"
        },
        {
            "title": "Task",
            "content": "S-T Interp. SoraDetector [13] VidHalluc [28] VidHal [12] Eventhallusion [63] VideoHallucer [55] ViBe [44] Dr.V-Bench(Ours) P, T P, T, P, P, T, 3 3 5 2 5 5 14 50 9,295 1,000 711 1,800 3,782 10,000 50 5,002 1,000 400 948 3,782 4, [8, 20] [0, 220] [0, 85] [0, 30] [7, 187] [1, 2] [0, 600] 6 3 9 7 7 5 18 Open-Ended MCQ, Binary QA, Open-ended MCQ Binary QA, Open-Ended Binary QA Open-Ended MCQ, Binary QA, Open-Ended Table 3. Comparison between our Dr.V-Bench and existing video hallucination benchmarks. We use the following abbreviations for conciseness: Level (P: Perceptive, T: Temporal, C: Cognitive), # H-Types (Number of Hallucination Types), S-T (Spatial-Temporal Grounding), and Interp. (Interpretability)."
        },
        {
            "title": "Question Type Hallucination Type",
            "content": "# Samples # Videos"
        },
        {
            "title": "MCQ",
            "content": "Yes/No"
        },
        {
            "title": "Caption",
            "content": "Object Number Color Location Static Relation OCR Action Dynamic Attribute Dynamic Relation Sequence Factual Prediction Counterfactual Prediction Context-based Explanation Knowledge-based Explanation Object Number Color Location Static Relation OCR Action Dynamic Attribute Dynamic Relation Sequence Factual Prediction Counterfactual Prediction Context-based Explanation Knowledge-based Explanation Object Number Color Location Static Relation OCR Action Dynamic Attribute Dynamic Relation Sequence Factual Prediction Counterfactual Prediction Context-based Explanation Knowledge-based Explanation 720 300 300 300 480 300 720 180 900 120 420 300 600 360 360 150 150 150 240 150 360 90 450 60 210 150 300 180 120 50 50 50 80 50 120 30 150 20 70 50 100 561 296 295 287 480 180 492 163 634 117 257 300 382 342 261 150 123 150 240 138 274 90 397 59 155 150 255 166 120 50 50 50 70 49 120 16 150 20 70 50 100 60 STAR ActivityNet-QA, NExT-QA ActivityNet-QA NExT-QA ActivityNet-QA ViteVQA, RoadTextVQA STAR TempCompass NExT-QA, STAR YouCook2 STAR Causal-VidQA NExT-QA Causal-VidQA, MMWorld, VCGBench-Diverse STAR ActivityNet-QA, NExT-QA ActivityNet-QA, CLEVRER NExT-QA ActivityNet-QA ViteVQA STAR TempCompass, Tomato NExT-QA, STAR, VCGBench-Diverse Video-MME, YouCook2 STAR Causal-VidQA NExT-QA Causal-VidQA, MMWorld, VCGBench-Diverse MSR-VTT, VATEX MSR-VTT, VATEX MSR-VTT, VATEX MSR-VTT, VATEX MSR-VTT, VATEX ViteVQA MSVD, MSR-VTT, VATEX TempCompass MSR-VTT, VATEX YouCook2 MSR-VTT MSR-VTT MSR-VTT MSR-VTT Table 4. Statistics of hallucination types across three categories of question-answering tasks, with total of 10,000 samples and 4,974 videos in Dr.V-Bench. 13 Level Category Source Example Object STAR Which object was taken by the person? (A) The clothes. (B) The shoe. (C) The broom. (D) The towel. Perception Number Color Location Activitynet-qa NExT-QA How many ducks in the video? (A) one (B) three (C) two (D) five Activitynet-qa What color is the car driving in the video? (A) blue (B) red (C) yellow (D) green NExT-QA Where is this video taken? (A) market (B) forest (C) beach (D) cafe Static Relation Activitynet-qa What is on the left of the river? (A) bench (B) tree (C) house (D) fountain OCR Action ViteVQA RoadTextVQA What is the price of the item? (A) 124.99 (B) 134.99 (C) 149.99 (D) 139. STAR What did the person do with the bag? (A) Opened (B) Took (C) Put down (D) Threw Dynamic Attribute TempCompass In which direction are the women athletes running? (A) from right to left (B) in circles (C) from left to right Temporal Dynamic Relation STAR, NExT-QA What did the lady with long hair do before she slid off? (A) go back up (B) tie her hair (C) jump around (D) dance Cognition Sequence YouCook According to this video, in which order are the following items used for magic? (a) Cards. (b) stick. (c) ring. (d) Paper money. (A) a, d, b, (B) b, a, d, (C) b, c, d, (D) c, b, a, Factual Prediction STAR What will the person do next? (A) Throw the box. (B) Take the bag. (C) Lie on the bed. (D) Sit on the floor. Counterfactual Prediction Context-based Explanation Knowledge-based Explanation Causal-VidQA NExT-QA Causal-VidQA MMWorld VCGBenchDiverse What will happen if the chair breaks suddenly? (A) Maybe the wind will blow through the window. (B) The person will fall onto the ground. (C) The person cant eat two donuts; they can only eat one. (D) Students will be confused. Why did all the babies look at the blonde boy when he walked towards the man? (A) babies are bored (B) man call him (C) curious (D) blonde boy is dancing How does the left team win point in the video? (A) They make free throw. (B) They score layup. (C) They get an offensive rebound and put-back. (D) They make basketball 3-point shot. Table 5. Multiple-choice QA examples of Dr.V-Benchs data sources. Original QA Generated QA Multiple-Choice QA Hallucination Type: Static Relation Hallucination Question: Where is the lamp positioned in the scene? Choices: A) Next to the couch. B) Behind the television. C) Above the sofa. D) The man is holding the lamp. Answer: C) Above the sofa. Hallucination Type: Object Hallucination Question: What object did the person put down? Choices: A) Hat. B) Skateboard. C) Sandwich. D) Pillow. Correct Answer: D) Pillow. Hallucination Type: Static Relation Hallucination Question: Where is the lamp positioned in the scene? Choices: A) On the table. B) Under the sofa. C) Above the sofa. D) Adjacent to the window. Answer: C) Above the sofa. Yes/No QA Hallucination Type: Object Hallucination Question: Did the person put down the pillow? Answer: Yes. Caption Generation QA Hallucination Type: Action Hallucination Original Caption: The video shows person showing red cars performance on mountain road. Hallucination Type: Action Hallucination Choices: A: { objects: [person, car], relationship: drive, attributes: {car: red} } B: { objects: [person, car], relationship: show, attributes: {car: red} } C: { objects: [person, car], relationship: clean, attributes: {car: red} } D: { objects: [person, car], relationship: close, attributes: {car: red} } Table 6. Examples of original and our generated QA. 14 Figure 7. Illustration of our designed prompts to generate multiple-choice QA. 15 Figure 8. Illustration of our designed prompts to generate yes/no QA. Figure 9. Illustration of the three different types of QA tasks. involves imagining what would happen under hypothetical or different conditions. Third, they often face challenges with context-based explanation, which requires understanding the video context to explain the intentions behind actions and the processes that lead to specific outcomes. Finally, they may struggle with knowledge-based explanation, which demands commonsense knowledge or domain-specific expertise to interpret and answer questions. C.2. Comparison with Existing Benchmarks Our Dr.V-Bench represents significant advancements over existing video hallucination benchmarks. Taxonomy depth. Dr.V-Bench has the most detailed video hallucination taxonomy to date, organizing hallucinations into three hierarchical levels with 14 distinct types. Task diversity. Dr.V-Bench incorporates diverse task formats, i.e., yes/no QA, multiple-choice QA, and captioning task that covers hallucinations across all possible video 16 tasks. Spatial-temporal grounding. Our benchmark is the only one to provide fine-grained spatial-temporal annotations with bbox coordinates in all related frames, from start to end timestamps, that capture crucial moments of object interaction or movement, for understanding context and diagnosing hallucinations accurately."
        },
        {
            "title": "Coverage",
            "content": "Temporal Diversity. and Dr.V-Bench offers unparalleled diversity in video duration, with range spanning from short clips to long videos of up to 600 seconds. This vast range is critical for robust model evaluation and stands in stark contrast to other benchmarks that are often limited to much shorter videos. By including both short and extremely long videos, our benchmark rigorously tests models ability to handle varying temporal scales, from capturing fleeting actions to understanding long-range narrative dependencies. Domain and Scenario Richness. Dr.V-Bench provides the most extensive domain coverage to date, encompassing 18 distinct domains. This number significantly surpasses that of all other existing benchmarks, which typically cover fewer than 10 domains. This richness ensures that models are evaluated across wide array of real-world contexts and scenarios, preventing overfitting to specific visual styles or topics and providing true test of their generalization capabilities. Dataset scale. The scale of Dr.V-Bench significantly surpasses that of other datasets. Its extensive coverage in terms of instances, unique videos, and diverse domains, combined with high proportion of complex scenarios, provides more challenging and comprehensive evaluation platform. This ensures that models are tested on their ability to generalize and perform higher-level reasoning, pushing the boundaries beyond existing benchmarks. C.3. Hallucination-centric QA Evaluation The yes/no and multiple-choice tasks are considered discriminative hallucination evaluation, focusing on whether model can correctly identify the ground truth among plausible but incorrect options. In contrast, the caption generation QA introduces generative hallucination evaluation. For this task, we provide the model with structured information derived from the question and instruct it to select the correct details to generate an appropriate video caption. This constrained format facilitates more focused evaluation of captioning capabilities and simplifies automated assessment. Concrete examples of these transformations are shown in Table 6. The evaluation protocol is then tailored to each task type: For Yes/No and Multiple-Choice QA, we first check if the models response explicitly includes candi17 date option (e.g., Yes, No, A, B, C, or D) that matches the ground truth. If no direct match is found, we then utilize GPT-4os language understanding capabilities to determine if the response semantically aligns with the correct answer. For Caption Generation QA, rule-based evaluation is ineffective due to the free-form nature of captions. Therefore, we rely exclusively on GPT-4o to evaluate the accuracy of the generated captions, ensuring the output faithfully reflects the structured information provided and the videos content. C.4. Fine-grained Spatial-Temporal Grounding 1. Extraction of Target Objects. In this step, the key target objects required for the QA are identified. These objects are crucial for the subsequent annotation process. The selection of target objects is based on their relevance to the question and their presence in the video content. 2. Annotation of Start and End Frames. Each target object is annotated with its start and end frames. The start frame is defined as the frame where at least 30% of the objects contours first appear, and the end frame is defined as the frame where at least 30% of the objects contours disappear. This ensures accurate capture of the objects temporal boundaries. 3. Annotation of Key Frames. Within the interval defined by the start and end frames, key frames for each target object are annotated. Key frames are those that effectively aid in the recognition and answering of the QA. These frames typically depict significant changes in the objects state, position, or interaction with other elements in the scene. 4. Annotation of Bounding Boxes. After annotating the start, end, and key frames, bounding boxes (bbox) are annotated for each frame. This involves marking the spatial location of each target object in every frame. The bounding box (bbox) coordinates are recorded to provide precise spatial information for the target objects. C.5. Quality Control To ensure the highest quality and consistency, we implement strict quality control protocol. All annotators are PhD students from computer science fields who receive standardized training and use unified annotation platform. Each video is independently annotated by two annotators. We measure inter-annotator agreement (IAA) quantitatively, achieving high score of 0.85. This is calculated using the average Intersection over Union (IoU) for bounding boxes and temporal overlap for keyframes. Any samples falling below this agreement threshold or showing significant discrepancies are either resolved by senior annotator or discarded from the final dataset. D. Extended Details of the Dr.V-Agent D.1. Open-Vocabulary Video Object Tracking Grounded SAM 2 [45] builds on the robust segmentation and tracking capabilities of SAM 2 by integrating it with Grounding DINO [37]. This combination enables openset object segmentation and tracking within video content. By processing input prompts, the model can precisely track specified objects and provide their normalized location coordinates. This functionality allows our system to achieve accurate and flexible video object tracking across diverse scenarios (see Figure 10 for results). YOLO-World [10] is next-generation YOLO detector designed for open-vocabulary object detection and grounding. Leveraging pre-training on large-scale datasets, it supports user-defined vocabulary prompts to detect and ground objects efficiently. This capability ensures robust object detection and localization, making it vital tool for video object tracking in Dr.V-Agent (see Figure 11 for results). D.2. Video Temporal Grounding Context-Guided Spatial-Temporal Video Grounding (CG-STVG) [20] is designed for precise video temporal grounding through two-stage architecture: multimodal encoder for feature extraction and context-guided decoder for refined grounding. The model has achieved great performance on many challenging benchmarks, making it highly reliable choice for temporal grounding tasks in our system (see Figure 12 for results). Grounded-VideoLLM [52] is specialized VLM optimized for fine-grained temporal grounding, demonstrating strong performance in temporal sentence grounding and grounded VideoQA tasks. The model incorporates temporal stream that encodes inter-frame relationships using discrete temporal tokens enriched with time-specific knowledge, making it versatile tool for general video understanding (see Figure 13 for results). E. Extended Experiment Results and Analyses E.1. In-depth Analysis of Hallucination Causes The performance data from Table 7 allows us to diagnose the likely reasons for model failures at each level of understanding. We identify consistent patterns of failure that hold true across nearly all evaluated LVMs. Perceptive Level: Capable in General, but Lacking Fine-Grained Fidelity. Although this is the highestscoring category, clear pattern emerges: models excel at coarse-grained tasks but falter on fine-grained details. This trend is universal across all tested models; for instance, performance on Location (e.g., Qwen2VL: 91.00%) and Object (e.g., InternVL2: 76.67%) consistently surpasses that on detail-oriented tasks like Number (Qwen2-VL: 75.51%) and Color (InternVL2: 73.58%) in the Multiple-Choice QA task. This suggests that while core visual feature extraction is robust enough for general scene understanding, it lacks the high fidelity required for precise counting or distinguishing subtle attributes. Furthermore, OCR remains distinct challenge, with average scores often lagging behind other perception tasks, indicating persistent difficulties in processing embedded text within visual scenes. Temporal Level: Fundamental Weakness in Modeling Dynamics. This level marks significant performance drop for all models, pointing to fundamental weakness in modeling temporal dynamics. Across the board, the most challenging sub-tasks are consistently Dynamic Attribute and Sequence. As stark example from Table 1, the top-performing Gemini-1.5-Pro sees its accuracy drop to just 61.13% on Dynamic Attribute (Atr.) in the Multiple-Choice QAone of its lowest scores across all categories. This highlights fundamental inability to track object state changes over time. Similarly, the universally low scores on Sequence reveal that current architectures, often adapted from static image models, are insufficient for capturing the complex, long-range dependencies and ordering of events inherent in video. Cognitive Level: Failure in Abstract and Causal Reasoning. This level represents the most significant challenge, consistently yielding the lowest average scores. Such failures are twofold. First, they are often cascade effect from temporal errors, as seen when Video-ChatGPTs accuracy plummets from respectable 67.00% on the perceptive task Location to mere 28.00% on Counterfactual Prediction (CnFct.). Second, and more critically, models fail at abstract reasoning itself. deeper look inside the cognitive tasks reveals crucial distinction: models perform significantly worse on tasks requiring true reasoningCounterfactual Prediction and Knowledge-based Explanationthan on those that can be partially solved with contextual cues (Contextbased Explanation). For example, even Gemini-1.5Pro scores near-perfect 93.53% on Context-based Explanation but drops to 81.04% on Counterfactual Prediction and 79.42% on Knowledge-based Explanation. This demonstrates that the primary bottleneck lies not only in understanding the videos context, but also in performing robust causal reasoning and integrating external world knowledge. Figure 10. Video object tracking results using Grounded SAM2. Figure 11. Video object tracking results using YOLO-World. E.2. Evaluation of Dr.V-Agents Spatial-Temporal"
        },
        {
            "title": "Grounding",
            "content": "We randomly sample 2,000 instances from Dr.V-Bench to evaluate the spatial and temporal grounding abilities of our chosen models. Following [20], we adopt tIoU, vIoU, and vIoU@R as our standard evaluation metrics. Specifically, tIoU measures temporal localization accu19 racy, computed as the mean temporal Intersection over Union (tIoU) across all test sequences, where tIoU = Pi Pu , with Pi and Pu denoting the temporal intersection and union of the predicted and ground-truth temporal segments. Similarly, vIoU evaluates spatial localization by averaging the visual IoU (vIoU) over all test videos, where vIoU = 1 and bt representPu ing the ground-truth and predicted bounding boxes at frame , bt), with IoU(b tPi (cid:80) Figure 12. Video temporal grounding results using CG-STVG. Figure 13. Video temporal grounding results using Grounded-VideoLLM. t. Lastly, vIoU@R represents the proportion of test samples where vIoU exceeds predefined threshold R. As shown in Table 8, Grounded SAM 2 and YOLO-World excel in visual segmentation and object tracking, confirming their strong perceptive-level precision, while CG-STVG and Grounded-VideoLLM demonstrate robust temporal localization, which validates their ability to accurately detect and align events over time. These results highlight the effectiveness of the perceptive and temporal-level tools integrated into Dr.V-Agent, providing solid foundation for mitigating video hallucinations in LVMs. E.3. Impact of Question Format on Task Difficulty Our motivation for designing three distinct question formats (Yes/No, Multiple-Choice, Caption Generation) is to create more comprehensive evaluation suite that assesses models capabilities from multiple angles. Different formats inherently pose different challenges, testing both discriminative understanding and generative abilities. While the main benchmark uses different questions for each format (as shown in Figure 14), here we conduct controlled experiment to specifically isolate the impact of the question format itself. For this analysis, we construct small, dedicated subset of 1,000 instances where the same underlying question about video is presented in all three formats. This allows for direct comparison of model performance on the same semantic task, with only the response format varied. As shown in Table 9, the results reveal clear and consistent difficulty gradient across all tested models. Yes/No questions consistently yield the highest accuracy, followed by multiple-choice QA, with caption generation proving to be the most challenging format. This hierarchy is intuitive: binary choice is the simplest form of discrimination, selecting from four options introduces more complex distractorbased reasoning, and generating caption requires not only correct understanding but also precise compositional language skills. This finding validates our multi-format approach, confirming that it provides progressively challeng-"
        },
        {
            "title": "Model",
            "content": "Obj. Num. Col. Loc. SRel. OCR Act. Atr. DRel. Seq. Fct. CnFct. Cxt. Knw. Avg Multiple-Choice QA 50.00 32.89 39.13 72.67 29.89 29.00 26.03 25.00 27.84 25.50 32.27 VideoChat2 35.25 30.87 28.09 67.00 25.37 37.13 26.17 28.89 25.67 26.23 25.43 Video-ChatGPT 50.97 40.60 48.49 81.33 57.86 37.67 30.42 34.44 32.33 45.00 49.76 Video-LLaVA LLaMA-VID 49.72 42.95 40.80 82.00 56.81 50.67 32.78 33.89 31.22 42.72 43.57 LLaVA-NeXT-Video-DPO 49.58 68.46 53.51 85.00 62.47 44.33 40.28 38.89 40.56 29.17 45.24 67.92 72.82 67.56 85.50 77.36 54.67 36.94 46.67 50.78 40.00 65.24 PLLaVA 76.67 71.48 73.58 92.33 79.04 64.67 59.86 45.56 67.11 55.83 69.52 InternVL2 79.00 75.51 72.58 91.00 79.08 75.33 66.03 52.22 73.67 70.50 70.86 Qwen2-VL 83.64 80.53 78.42 93.50 83.25 80.32 74.42 58.12 78.64 75.53 75.32 GPT-4o 85.31 83.42 81.32 94.09 85.56 82.52 77.63 61.13 81.52 77.13 78.14 Gemini-1.5-Pro Yes/No QA VideoChat2 27.22 22.67 36.24 50.67 49.07 31.33 46.94 35.56 60.22 33.33 44.29 Video-ChatGPT 48.33 49.33 48.32 52.67 49.58 54.67 51.67 48.88 65.78 40.00 49.52 Video-LLaVA 58.33 65.33 53.02 81.33 56.36 51.33 50.28 51.11 69.78 46.67 47.14 59.44 54.00 50.34 74.67 57.63 55.33 48.89 52.22 67.56 28.57 50.95 LLaMA-VID LLaVA-NeXT-Video-DPO 61.67 75.33 64.43 81.33 63.56 56.67 60.00 53.33 65.11 55.00 42.38 71.11 78.00 64.43 80.67 79.66 62.67 52.50 52.22 67.33 53.33 49.52 PLLaVA 69.72 78.00 72.48 89.33 71.61 64.00 61.67 60.00 62.22 46.67 51.90 InternVL2 72.78 80.67 73.83 90.67 81.36 72.00 65.28 62.22 66.67 53.33 69.52 Qwen2-VL 77.51 84.61 78.08 93.05 84.06 76.80 70.20 66.08 71.30 58.06 74.06 GPT-4o 80.35 86.18 81.31 94.60 86.08 78.08 73.50 68.27 73.25 60.20 77.29 Gemini-1.5-Pro"
        },
        {
            "title": "Caption Generation QA",
            "content": "37.14 36.00 38.04 54.14 35.40 37.00 43.56 29.41 29.96 32.20 40.30 VideoChat2 34.00 29.81 38.01 46.17 29.00 30.00 38.88 29.41 30.51 30.45 35.00 Video-ChatGPT 54.90 50.21 55.80 77.34 50.26 48.54 50.41 39.34 47.56 40.15 44.90 Video-LLaVA LLaMA-VID 47.51 45.54 50.54 71.21 45.61 43.96 47.42 48.43 40.52 37.88 45.65 LLaVA-NeXT-Video-DPO 65.15 63.93 65.82 82.50 68.35 56.53 60.21 54.54 56.00 50.00 59.05 59.14 59.35 59.34 79.56 63.31 59.35 56.90 53.33 52.63 45.05 57.12 PLLaVA 69.61 66.64 69.30 87.30 72.34 62.00 62.45 56.24 59.10 53.32 61.88 InternVL2 73.35 71.50 71.44 85.32 74.53 67.54 65.21 58.12 63.44 57.25 66.34 Qwen2-VL 78.08 74.60 75.57 90.09 76.38 72.77 68.70 61.36 66.28 60.20 69.17 GPT-4o 80.02 76.48 77.19 92.02 78.08 74.05 71.30 63.50 68.70 62.04 71.30 Gemini-1.5-Pro 28.36 28.00 37.00 55.33 61.67 68.67 64.67 73.33 78.05 81.04 46.67 54.00 42.67 45.33 60.67 64.00 54.67 61.33 67.19 69.71 29.41 33.56 43.04 48.60 55.62 52.53 60.16 64.32 67.55 69.25 33.67 30.00 34.45 32.00 34.64 32.20 63.67 48.89 47.03 63.17 52.66 48.45 73.00 55.00 53.37 74.33 61.94 62.17 80.00 64.80 68.94 85.83 71.74 74.05 91.43 76.12 79.09 93.53 79.42 81.55 40.33 30.00 39.61 51.67 48.33 50.91 57.00 51.11 55.82 55.67 54.80 53.96 67.33 60.00 61.92 67.67 61.67 64.63 65.33 62.78 65.03 74.33 72.78 71.20 77.90 76.08 75.36 80.20 79.18 77. 40.45 38.54 37.25 35.75 37.64 34.16 58.56 53.64 51.05 64.26 58.25 49.67 66.45 64.56 62.05 66.34 60.64 58.90 69.47 66.64 65.46 74.15 70.32 68.77 77.97 73.50 72.30 80.60 76.04 74.33 Table 7. Accuracy of LVMs across different hallucination types in three tasks. Columns represent the hallucination types: Object (Obj.), Color (Col.), Number (Num.), Location (Loc.), Static Relation (SRel.), OCR; Action (Act.), Dynamic Attribute (Atr.), Dynamic relation (DRel.), Sequence (Seq.); Factual Prediction (Fct.), Counterfactual Prediction (CnFct.), Context-based Explanation (Cxt.), Knowledgebased Explanation (Knk.). The highest score is in bold and the lowest is underlined."
        },
        {
            "title": "Model",
            "content": "m tIoU vIoU vIoU@0.3 vIoU@0.5 Model Yes/No Acc. Multiple-Choice Acc. Caption Generation Acc."
        },
        {
            "title": "50.06\nGrounded SAM 2\n–\nYOLO-World\nCG-STVG\n56.11\nGrounded-VideoLLM 53.90",
            "content": "42.27 43.83 38.75 62.60 61.42 59.05 39.25 37.69 36.28 Table 8. Spatial-temporal grounding performance of each model used in Dr.V-Agent. InterVL2 Qwen2-VL GPT-4o Gemini-1.5-Pro 79.45 83.08 86.50 88. 69.43 72.93 77.24 79.60 62.51 65.88 72.07 75.68 Table 9. Performance comparison across different question formats on subset. ing testbed for LVMs. E.4. Validation of the Hallucination Type Classifier The entire adaptive reasoning pipeline of Dr.V-Agent hinges on the accuracy of its initial classification step (Step 21 Figure 14. Accuracy of LVMs across different hallucination levels in three tasks. 1), where GPT-4o analyzes QA pair to determine the required level of analysis (Perceptive, Temporal, or Cognitive). An error in this foundational step would lead to the selection of an incorrect reasoning path, invalidating the subsequent diagnosis. Therefore, it is crucial to verify the reliability of this classifier. To do so, we conduct manual verification study. We randomly sample 1,000 instances from our Dr.V-Bench and have human experts manually label each instance with the correct hallucination level according to our taxonomy. We then compare these ground-truth labels against the classifications made by GPT-4o using the prompt designed for Step 1. The evaluation reveals that GPT-4o achieves an accuracy of 99.6% (996 out of 1000 correct classifications). The few misclassifications occur in ambiguous cases at the boundary between temporal and cognitive reasoning. This near-perfect accuracy confirms that our framework can reliably and automatically direct each query to the appropriate, specialized reasoning path, ensuring the validity and efficiency of the Dr.V-Agent system. E.5. Verification of GPTs Evaluation Reliability To verify the reliability of GPT-4os evaluation in Caption Generation QA, we randomly sample 200 instances from the corresponding dataset and compare its judgments with human annotations. Each instance is independently reviewed by two human annotators, with third annotator resolving conflicts when necessary. We measure agreement using accuracy and Cohens Kappa, which accounts for chance agreement. GPT-4o achieves near-perfect agreement of 98.5%, with Cohens Kappa of 0.96, confirming its reliability as an evaluator for Caption Generation QA. E.6. Evaluating the Effectiveness of Dr.V-Agent The Self-PEP framework relies on an internal two-step process: (1) self-improvement, where the model generates description to use as auxiliary context, and (2) selfexplanation, where it uses an explain-then-predict approach to verify its own reasoning. First, the model generates video description using the prompt: Describe the video: Next, the model answers the question based on both the video and the generated description: Description: {description} Please provide clear response to the question below by watching the video. If necessary, you can also use the accompanying Description to help refine your answer. Your response should be simple yes or no. Question: {question} Answer the question using yes or no: Finally, the model explains its reasoning and verifies the accuracy of its response before finalizing the answer: Description: {description} Please offer detailed explanation for your answer to the following question. After explaining, verify the accuracy of the information youve used in your explanation. Once youve confirmed the facts, please respond to the question with simple yes or no. Question: {question} Answer: {predict} Answer the question using yes or no: E.7. Qualitative Analysis and Case Studies To comprehensively evaluate Dr.V-Agents reasoning robustness, we conduct qualitative analyses across three levels: perceptive, temporal, and cognitive. Our framework 22 Model Obj. Col. Num. Loc. SRel. OCR Act. Atr. DRel. Seq. Fct. CnFct. Cxt. Knk. Avg. VideoChat2 + Self-PEP + Dr.V-Agent 41.88 49.52 65. Video-ChatGPT 39.05 47.68 58.55 + Self-PEP + Dr.V-Agent Video-LLaVA + Self-PEP + Dr.V-Agent LLaMA-VID + Self-PEP + Dr.V-Agent LLaVA-NeXT + Self-PEP + Dr.V-Agent PLLaVA + Self-PEP + Dr.V-Agent InternVL2 + Self-PEP + Dr.V-Agent Qwen2-VL + Self-PEP + Dr.V-Agent GPT-4o + Self-PEP + Dr.V-Agent Gemini-1.5-Pro + Self-PEP + Dr.V-Agent 53.57 63.99 73.68 52.42 62.10 72.22 54.76 49.24 76.76 68.00 61.30 81. 73.88 81.52 89.15 76.57 80.37 89.19 81.25 87.63 95.23 83.29 89.67 97.77 38.15 43.60 54.91 35.15 41.30 49. 50.58 58.00 64.91 44.64 51.54 58.77 58.32 54.39 74.01 65.80 61.02 75.63 72.95 78.40 83.85 76.66 79.37 85. 81.16 85.70 91.13 83.55 88.09 93.87 30.14 35.17 45.62 36.30 41.98 49.12 48.98 55.83 62.19 46.52 52.90 59. 58.02 54.40 72.49 73.03 68.62 82.09 72.82 77.85 82.88 72.84 75.34 81.14 78.03 82.22 87.23 80.90 85.09 90. 64.22 66.81 72.18 60.62 63.54 67.22 80.93 84.45 87.73 78.72 82.00 85.42 83.65 81.79 91.09 83.56 81.29 88. 90.93 93.52 96.10 90.33 91.61 94.60 93.02 95.18 97.75 94.04 96.02 98.94 36.20 45.12 63.61 33.00 43.05 55. 56.65 68.79 80.07 55.94 67.23 79.03 63.39 56.97 89.04 76.65 68.84 92.71 76.14 85.06 93.95 79.31 83.73 94. 82.81 90.23 98.12 84.97 92.39 99.04 30.50 39.10 56.95 41.68 51.39 63.62 42.86 54.60 65.49 51.40 62.31 73. 49.25 43.04 74.01 57.54 49.99 73.07 64.20 72.80 81.40 73.55 77.83 87.76 78.51 85.69 94.26 80.34 87.52 96. 34.06 37.19 43.69 35.09 38.63 43.07 38.38 42.65 46.61 39.08 43.05 47.19 48.19 45.93 57.20 43.60 40.86 49. 60.66 63.79 66.91 65.72 67.27 70.89 72.58 75.19 78.31 75.76 78.37 81.69 28.61 31.04 36.09 34.94 37.68 41. 39.93 43.24 46.32 40.84 43.92 47.14 44.79 43.04 51.79 49.00 46.87 53.38 50.96 53.39 55.82 55.81 57.02 59. 60.83 62.86 66.28 63.51 65.54 68.12 37.77 45.52 61.55 38.19 46.93 57.90 45.09 55.63 65.44 43.05 52.85 63. 49.47 43.90 71.72 55.93 49.15 69.87 64.84 72.59 80.31 70.55 74.39 83.33 75.20 81.65 89.36 77.76 84.21 92. 28.52 32.70 41.38 30.78 35.49 41.43 45.02 50.71 56.00 37.99 43.28 48.81 39.00 35.99 51.02 44.50 40.84 52. 52.83 57.01 61.17 64.02 66.10 70.92 68.76 72.24 76.40 70.54 74.02 78.45 36.68 43.65 58.13 33.61 41.48 51. 48.49 58.00 66.83 45.99 54.82 64.07 45.76 40.74 65.82 59.71 53.60 72.28 63.47 70.44 77.40 70.01 73.47 81. 74.33 80.14 87.30 77.20 83.01 90.41 33.96 37.22 43.99 36.36 40.04 44.68 39.31 43.75 47.88 51.66 55.79 60. 60.77 58.42 70.15 65.66 62.80 71.54 61.22 64.48 67.75 68.83 70.45 74.21 73.74 76.46 79.71 76.46 79.18 82. 36.35 43.91 59.61 38.28 46.81 57.53 61.16 71.46 81.02 61.03 70.62 80.63 70.64 65.20 92.39 71.53 64.91 85. 74.55 82.11 89.67 81.21 84.96 93.69 86.03 92.33 99.06 88.24 94.34 98.87 30.85 34.68 42.62 39.05 43.37 48. 50.03 55.24 60.09 53.86 58.71 63.78 57.46 54.70 68.48 61.73 58.38 68.63 64.38 68.21 72.02 71.91 73.81 78. 75.85 79.04 82.90 79.01 82.20 86.26 36.28 42.33 (+6.05) 54.88 (+18.60) 38.01 44.82 (+6.81) 53.43 (+15.42) 50.07 58.31 (+8.24) 65.96 (+15.89) 50.22 57.88 (+7.66) 65.89 (+15.67) 56.80 52.45 (-4.35) 74.21 (+17.41) 62.58 57.28 (-5.30) 73.47 (+10.89) 67.42 73.47 (+6.05) 79.49 (+12.07) 72.67 75.67 (+3.00) 82.64 (+9.97) 77.29 82.33 (+5.04) 88.36 (+11.07) 79.68 84.72 (+4.95) 91.12 (+10.84) Table 10. Performance of LVMs across various hallucination types when equipped with Self-PEP or Dr.V-Agent. effectively mitigates hallucinations in LVMs by implementing structured evidence-based verification. As illustrated in Figure 15, our approach ensures multi-faceted and robust validation of the LVMs responses. At the perceptive level, we analyze queries that require precise visual grounding of objects and their attributes. While standard LVMs frequently hallucinate the existence of target objects, Dr.V-Agent systematically cross-validates object presence and properties across multiple frames using spatial-temporal representations. At the temporal level, we analyze event understanding and the tracking of dynamic relationships over time. Due to the limitations of common LVMs, these models often misorder events. In contrast, Dr.V-Agents temporal grounding capability reconstructs event chronology through timestamp-aware state transitions, ensuring coherent event ordering. At the cognitive level, we evaluate the integration of contextual and commonsense knowledge. While conventional models often generate plausible but ungrounded hypotheses, Dr.VAgent explicitly verifies that cognitive-level conclusions remain consistent with both observed visual evidence and external knowledge bases. To further demonstrate the usefulness of this framework, we present challenging case study that showcases how Dr.V-Agents feedback refines the response of LVMs. As illustrated in Figure 16, the task requires the system to locate and track target objects spatiotemporal changes across multiple scenes and then leverage contextual knowledge to accurately infer the correct cause-and-effect relationship. Unlike LVMs that risk hallucinating non-existent objects or motives, Dr.V-Agent confirms each claim against the actual video. It produces constructive, evidence-based feedback enabling the LVM to correct its initial response, thereby alleviating hallucination. This process is direct application of the perceptive, temporal, and cognitive verification layers. Figure 15. Qualitative examples of Dr.V-Agents Perceptive, Temporal, and Cognitive reasoning. 24 Figure 16. Visualization of real example showcasing how Dr.V-Agent achieves successful video hallucination alleviation. The agents feedback loop corrects the LVMs initial incorrect assessment."
        }
    ],
    "affiliations": [
        "HKUST",
        "Monash",
        "NJU",
        "NUS",
        "UCSB",
        "UR",
        "UTD",
        "WHU"
    ]
}