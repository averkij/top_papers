{
    "paper_title": "OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models",
    "authors": [
        "Dvir Samuel",
        "Matan Levy",
        "Nir Darshan",
        "Gal Chechik",
        "Rami Ben-Ari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Omnimatte aims to decompose a given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper, we present OmnimatteZero, a training-free approach that leverages off-the-shelf pre-trained video diffusion models for omnimatte. It can remove objects from videos, extract individual object layers along with their effects, and composite those objects onto new videos. We accomplish this by adapting zero-shot image inpainting techniques for video object removal, a task they fail to handle effectively out-of-the-box. We then show that self-attention maps capture information about the object and its footprints and use them to inpaint the object's effects, leaving a clean background. Additionally, through simple latent arithmetic, object layers can be isolated and recombined seamlessly with new video layers to produce new videos. Evaluations show that OmnimatteZero not only achieves superior performance in terms of background reconstruction but also sets a new record for the fastest Omnimatte approach, achieving real-time performance with minimal frame runtime."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 3 0 8 1 . 3 0 5 2 : r OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models Dvir Samuel1,3, Matan Levy2, Nir Darshan1, Gal Chechik3,4, Rami Ben-Ari1 1OriginAI, Tel-Aviv, Israel 2The Hebrew University of Jerusalem, Jerusalem, Israel 3Bar-Ilan University, Ramat-Gan, Israel 4NVIDIA Research, Tel-Aviv, Israel Figure 1. OmnimatteZero is the first training-free generative approach for Omnimatte, leveraging pre-trained video diffusion models to achieve object removal, extraction, and seamless layer compositions in just 0.04 sec/frame (on an A100 GPU). Unlike prior methods, OmnimatteZero does not require any model training or optimization while significantly improving efficiency."
        },
        {
            "title": "Abstract",
            "content": "Omnimatte aims to decompose given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper, we present OmnimatteZero, training-free approach that leverages off-the-shelf pre-trained video diffusion models for omnimatte. It can remove objects from videos, extract individual object layers along with their effects, and composite those objects onto new videos. We accomplish this by adapting zero-shot image inpainting techniques for video object removal, task they fail to handle effectively out-of-the-box. We then show that self-attention maps capture information about the object and its footprints and use them to inpaint the objects effects, leaving *Correspondence to: Dvir Samuel <dvirsamuel@gmail.com>. clean background. Additionally, through simple latent arithmetic, object layers can be isolated and recombined seamlessly with new video layers to produce new videos. Evaluations show that OmnimatteZero not only achieves superior performance in terms of background reconstruction but also sets new record for the fastest Omnimatte approach, achieving real-time performance with minimal frame runtime. 1. Introduction Image diffusion models have achieved remarkable success in image generation [1, 3, 24, 26, 27]. Beyond generation, these models have proven highly effective for various image editing tasks, including image inpainting [6, 21], object removal [4, 32], and image matting [11] with some methods achieving these edits without additional training [6, 21]."
        },
        {
            "title": "With the recent introduction of video diffusion models",
            "content": "1 [9, 14, 29] video editing tasks have become increasingly relevant. One particularly challenging variant of video editing is the video matting task, commonly referred to as Omnimatte [19, 31]. Omnimatte aims to decompose an input video into background layer and multiple foreground layers, where each foreground layer contains an object of interest along with its associated effects, such as shadows and reflections. These extracted layers facilitate various video editing applications, including object removal, background replacement, retiming [18], and object duplication [15]. Various video inpainting and Omnimatte methods have been proposed, but all rely on model training or optimization. Recent video inpainting approaches [16, 36] are trained on annotated datasets for object removal but fail to account for their associated effects on the scene. Similarly, Omnimatte methods require expensive self-supervised optimization for each video [15, 17, 19, 28], with some demanding large-scale dataset training [15, 32] or 3D scene modeling using meshes or NeRF [17, 28]. These methods typically involve hours of training per video and prolonged inference times for rendering each frame. Given the success of fast, training-free approaches in image editing [6, 21], natural question arises: can these methods be effectively adapted to video editing? In this paper, we present new approach called OmnimatteZero, the first real-time training-free generative method to Omnimatte. OmnimatteZero (named for its zero training and optimization requirements) leverages off-theshelf pre-trained video diffusion models to remove all objects from video, extract individual object layers along with their associated effects, and finally composite them onto new backgrounds. This entire process is performed efficiently during diffusion inference, making it significantly faster than current methods. See Figure 1 for illustration. Our method begins by adapting training-free image inpainting approaches [21, 22] for object inpainting using video diffusion models. We demonstrate that directly applying these methods to videos does not work and propose simple yet effective solution for object removal in videos. Next, we show that self-attention maps capture information about both the object and its traces in the scene, such as shadows and reflections. We use these maps to inpaint the objects effects, resulting in clean background. For foreground layer extraction, we find that the object layer, along with its footprints, can be simply extracted by linearly subtracting the latent of the video (with the object) from the clean background latent (without the object). This object layer can then be recomposed to new background video, by adding the latent encodings, enabling fast and easy layer insertion and composition. To validate our approach, we apply it to two video diffusion models, LTXVideo [9] and Wan2.1 [29], and evaluate it on standard Omnimatte benchmarks both quantitatively and qualitatively. Our method outperforms all supervised and self-supervised existing inpainting and Omnimatte techniques across all benchmarks in terms of background reconstruction and object layer extraction. Additionally, since LTXVideo generates videos in real-time, we set new record for the fastest Omnimatte approach, achieving real-time performance with minimal runtime per frame. 2. Related Work 2.1. Omnimatte The first Omnimatte approaches [12, 19] use motion cues to decompose video into RGBA matte layers, assuming static background with planar homographies. Optimization isolates dynamic foreground layers, taking 3 to 8.5 hours per video and 2.5 seconds per frame to render. Later methods enhanced Omnimatte with deep image priors [8, 20] and 3D representations [17, 28, 33], but still rely on strict motion assumptions and require extensive training (6 hours) with rendering times of 2.5-3.5 seconds per frame. recent generative approach [15] uses video diffusion priors to remove objects and effects, with test-time optimization for frame reconstruction, taking 9 seconds per frame. In this paper, we propose novel, training and optimization-free method using off-the-shelf video diffusion models, achieving real-time performance in 0.04 seconds per frame. 2.2. Video Inpainting Earlier video inpainting methods [5, 10, 30] used 3D CNNs for spatiotemporal modeling but struggled with long-range propagation due to limited receptive fields. Pixel-flow approaches [7, 34, 35] improved texture and detail restoration by utilizing adjacent frame information. Recently, Zhou et al. [36] introduced ProPainter, which enhances reconstruction accuracy and efficiency by leveraging optical flowbased propagation and attention maps. Li et al. [16] proposed DiffuEraser, which combines pre-trained text-toimage diffusion model with BrushNet, feature extraction module, to generate temporally consistent videos. They use two-stage process where spatial inpainting is extended to In this pathe spatiotemporal domain in separate step. per, we propose single-step object inpainting approach, directly applied during video diffusion model inference, implicitly leveraging spatial and temporal information from adjacent frames for inpainting. 2.3. Diffusion Transformers for Video Generation Recent advancements in diffusion models have significantly improved text-to-video generation. These models, trained on large datasets of video-text pairs, can generate visually compelling videos from text descriptions. Recent work in2 troduced LTX-Video [9], real-time transformer-based latent diffusion model capable of generating videos at over 24 frames per second. It builds on 3D Video-VAE with spatiotemporal latent space, concept also used in Wan2.1 [29] and the larger HunyuanVideo [14]. Unlike traditional methods that treat the video VAE and denoising transformer separately, LTX-Video integrates them within highly compressed latent space (1:192 compression ratio), optimizing their interaction. This spatiotemporal downscaling results in 32 32 8 pixels per token while maintaining highquality video generation. Similarly, HunyuanVideo employs large-scale video generation framework, integrating an optimized VAE and diffusion-based transformer to achieve excellent visual quality and motion consistency. These models demonstrate the trend of combining highcompression VAEs with transformer-based diffusion models for efficient and scalable video generation. 3. Motivation: The failure of image inpainting approaches for videos We begin by exploring training-free approaches to image diffusion inpainting and extend it directly to video diffusion models. Our starting point is RePaint [21], widely used and straightforward inpainting method. RePaint iteratively reconstructs masked regions by adding Gaussian noise to the background, aligning it with the denoising process. denoising step then generates missing content while conditioning on known areas. This cycle repeats, refining the output by reapplying noise and rerunning the process. When directly applying RePaint to videos, our goal is to use it for inpainting both spatially and temporally (across the entire latent space, rather than frame by frame), as frame-by-frame inpainting would lead to temporal inconsistencies between frames. However, we observe significant limitations with this approach. While the background remains well-aligned with the input video, the inpainted regions often degrade into incoherent blobs, lacking both structural fidelity and temporal consistency across frames (see Figure 2 Visualization (a), red dashed blob). We hypothesize that this failure stems from the fundamental differences between image and video inpainting. In image inpainting, the model only needs to reconstruct missing regions using spatial information to ensure coherence within the image. In contrast, video inpainting requires consistency both spatially and temporally, maintaining smooth transitions across consecutive frames. While in-frame (single image) inpainting strongly relies on hallucination, in video, missing information can be inferred from different spatio-temporal regions. These additional constraints make video inpainting much more challenging, as the model must not only generate plausible spatial content but also ensure accuracy and maintain temporal stability both within and outside the masked region, an aspect that RePaint fails to achieve. To validate our hypothesis, we explore another widely used zero-shot inpainting approach known as Img2Img or SDEdit [22]. This method adds noise to the image latent, applying higher noise to the masked region and lower noise to the background to control changes. The model then iteratively denoises the noisy latent, aiming to modify the masked region. This method does not guarantee perfect background preservation and often introduces unintended artifacts into the background. When extending this approach to video diffusion models and applying spatio-temporal denoising, the model effectively removes objects and plausibly inpaints the masked regions. However, this process further generates noise and artifacts into the background (see Figure 2, Visualization (b)), causing the degradation of the outcome. This observation confirms that video diffusion models can remove objects from video content, but only when the requirement for high fidelity background preservation is relaxed. However, this comes at the cost of introducing artifacts and noise in the resulting video. 4. Method: OmnimatteZero We start with the process of object removal and inpainting. Building on our previous observations, we strive to develop method that ensures the background remains consistent with the original video while also accurately inpainting the missing object. Furthermore, we seek to eliminate the effects the object has on the scene (e.g. shadows, reflections), challenge where existing inpainting methods fall short. Our model takes as input video along with an object mask for each frame, following the common Omnimatte setup [19]. Alternatively, single-frame mask can be used and propagated across all frames using general-purpose video segmentation model, such as SAM2 [25]. We denote this mask as obj {0, 1}F H , with W, denoting the frame size and the number of frames, and designates pixel space. To address the challenges faced by current zero-shot inpainting method, we propose simple yet effective approach. The method is illustrated in Figure 2. We start with an input video with RGB frames, RF H3, The video is then passed through the VAE yielding compressed latent feature map Rf whc, with w, h, denoting the latent space dimensions and represents the number of temporally compressed latent frames. Applying mask directly in latent space is challenging due to the unclear mapping from pixel space to the latent space. We start by taking the object masks (per-frame) and computing binary video, where the frames are binary images, and pass this video as an RGB input (with duplicated channels) through the VAE encoder as well. This process produces latent representation where high values indicate the masked object, enabling its spatio-temporal footprint 3 Figure 2. Overview of OmnimatteZero-From left to right: The video and its corresponding object-masked version are encoded into the latent space. The spatio-temporal footprint of the target object is then identified and extracted from this encoding. Self-attention maps are leveraged to recognize effects associated with the object (such as cats reflection), which are incorporated into both the object mask and the latent encoding. Using this mask, we apply two imperfect video inpainting methods: (a) Background-Preserving Inpainting, which retains background details but may leave behind traces of the object, and (b) Object-Removing Inpainting, which eliminates the object but might introduce background distortions. We refine the results through Attention-based Latent Blending, selectively combining the strengths of each inpainting method. Finally, few denoising steps of the video diffusion model enhances the blended output, producing high-quality reconstruction with the object removed and the background well-preserved, as indicated by high PSNR values. identification via simple thresholding. We denote this mask as Mobj {0, 1}f wh (see Figure 2 at left). Next, we apply two inpainting processes, simultaneously: (1) Background-Preserving video inpainting, which attempt to remove the object while maintaining the background (inspired by [21]), BP : BP , and (2) Object-Removal inpainting, which successfully removes the object but fails to preserve the background (inspired by [22]), OR : OR. We apply both inpaintings for denoising steps. At step k, we blend them as: Zk = Mobj BP + (1 Mobj) OR (1) where denotes element-wise multiplication. Next, we perform an additional Tsteps denoising steps with the video diffusion model, completing the remaining diffusion steps, to blend the two latent representations. This ensures smooth transition between the inpainted object region and the surrounding background and results in video where the object is removed while the background closely matches the original input (Figure 2 Visualization (c)). Up to this point, we have demonstrated how objects can be removed from videos. However, this process does not eliminate their associated effects. In the following chapters, we show how to identify and remove both the object and its effects, extract object layers, and composite them with new layers to generate new videos. 4.1. Removing associated object effects We are interested in masking the object with its associated effects. Recently, [15] showed that pretrained video diffusion models can associate objects with their effects by analyzing self-attention maps between query and key tokens related to the object of interest. They leveraged this insight to train diffusion models for object removal along with its associated footprint. In contrast to [15], we propose to directly derive the masks from the attention maps, allowing training-free object removal approach. Specifically, we apply single noising and denoising step on the video latent and compute the attention weights between query and key tokens associated with the object. More precisely, for each latent frame at diffusion layer l, we calculate the attention map as follows: (cid:18) Ql i(Mobj d i)T (cid:19) Rwh (2) Al = softmax 4 Figure 3. Self-attention maps from (a) LTX Video diffusion model and (b) Image based Stable Diffusion. The spatio-temporal video latent attends to object associated effects (e.g., shadow, reflection) where, image models struggles to capture these associations. i, Rwhc are the queries and keys respecwhere Ql tively at layer and frame i, and is the number of channels of the latent. We then compute soft-mask per-frame, Mi [0, 1]whc, which is aggregated across all diffusion layers: Mi ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) l="
        },
        {
            "title": "Al\ni",
            "content": "(3) where is the number of layers in the video diffusion model. Then we extend Mi to the whole video by concatenating all frame masks channel-wise, deriving soft-mask latent code Mobj = concat{M1, ...Mf } Rwhf . To obtain binary mask we perform Otsu thresholding [23] for each latent frame, Ë†Mobj = I(Mobj). This new mask replaces the mask Mobj provided as input for the two inpainting methods and the soft mask Mobj is used for the latent blending in Equation (1) process we now refer to as attention-based latent blending. We found that soft blending the two latents leads to improved results, as it allows the diffusion model to leverage spatial and temporal information from both latents more effectively during the final denoising stage. Fig. 3(a) visualizes these self-attention maps (obtained from LTX-Video [9]) to two videos, demonstrating the success of the attention map, in capturing the object and its associated traces, e.g. the cat and its reflection and the bicycle rider with its shadow. Interestingly, to the best of our knowledge, this approach has not been explored for masking object effects (e.g. shadows) in images. Unlike video diffusion models, image models do not capture object effects from still images [32]. Figure 3(b) illustrates this by showing self-attention maps extracted using StableDiffusion [26], text-to-image diffusion model, which demonstrates that the object does not attend to its associated effects. We believe that this aligns with the principle of common fate in Gestalt psychology [13], 5 Figure 4. (a) Foreground Extraction: The target object is extracted by latent code arithmetic, subtracting the background video encoding from the object+background latent (Latent Diff). This initially results in distortions, which are later corrected by replacing pixel values using the original video and user-provided mask (b) Layer Composition: The ex- (Latent Diff + Refinement). tracted object layer is added to new background latent (Latent Addition). To improve blending, few steps of SDEdit are applied, yielding more natural integration of the object into the new scene (Latent Addition + SDEdit). which suggests that elements moving together are perceived as part of the same object. Video diffusion models seem to implicitly leverage this principle by grouping objects with their effects through motion. 4.2. Foreground extraction We can now use our object removal approach to extract object layers along with their associated effects. To isolate specific object, we apply our method twice: first, removing all objects from the video, leaving only the background, denoted as Vbg RF H3 with its corresponding latent Zbg; second, removing all objects except the target object, resulting in video of the object with the background, denoted as Vobj+bg RF H3 with its corresponding latent Zobj+bg. We can now derive the video of the object isolated from the background by simply subtracting the two latents Zobj = Zobj+bg Zbg. Applying thresholding on Zobj results in latent that is decoded to video Vobj containing only the object and its associated effects (see Figure 4a Latent Diff). While the extracted effects appear convincing, the object itself often suffers from distortions due to latent values falling outside the diffusion models learned distribution. To address this issue, we make use of the information available in the pixel-space. We refine the objects appearance by replacing its values in the pixel-space with those from the original video, based on the user provided mask. Specifically, Vobj = obj Vobj+bg + (cid:16) 1 obj (cid:17) Vobj. (4) This correction preserves the objects fidelity while maintaining its associated effects, resulting in high-quality object layers (see Figure 4a Latent Diff + Pixel Injection). Scene Metric Movie Kubric Average PSNR LPIPS PSNR LPIPS PSNR LPIPS Ttrain (hours) Trun (s/frame) ObjectDrop [32] Video RePaint [LTXVideo]* [21] Video RePaint [Wan2.1]* [21] Lumiere inpainting [2] Propainter [36] DiffuEraser [36] Ominmatte [19] D2NeRF [33] LNA [12] OmnimatteRF [17] Generative Omnimatte [15] OmnimatteZero [LTXVideo] (Ours) OmnimatteZero [Wan2.1] (Ours) 28.05 20.13 21.44 26.62 27.44 29.51 21.76 - 23.10 33.86 32.69 34.11 33.89 0.124 0.252 0.244 0.148 0.114 0. 0.239 - 0.129 0.017 0.030 0.015 0.019 34.22 21.15 24.16 31.46 34.67 35.19 26.81 34.99 - 40.91 44.07 44.07 44. 0.083 0.289 0.261 0.157 0.056 0.048 0.207 0.113 - 0.028 0.010 0.010 0.008 31.14 20.64 22.8 29.04 31.06 32. 24.29 - - 37.38 38.38 39.09 39.02 0.104 0.271 0.253 0.153 0.085 0.077 0.223 - - 0.023 0.020 0.012 0. - 0 0 - - - 3 3 8.5 6 - 0 0 - 0.04 3.2 9 0.083 0. 2.5 2.2 0.4 3.5 9 0.04 3.2 Table 1. Quantitative comparison: Background Reconstruction. OmnimatteZero outperforms all omnimatte and video inpainting methods, achieving the best PSNR and LPIPS without training or per-video optimization. It also runs significantly faster, with OmnimatteZero [LTXVideo] at 0.04s per frame. - denotes missing values due to unreported data or unavailable public code. 4.3. Layer composition With the object layers extracted, we can now seamlessly compose them onto new videos by adding the object layer latent to new background latent bg. Specifically,"
        },
        {
            "title": "Z N",
            "content": "obj+bg = bg + Zobj. (5) Figure 4b (Latent Addition) illustrates the initial result with some residual inconsistencies, which we finally fix, by applying 3 noising-denoising steps. This process helps smooth transitions between the video background and foreground layers, resulting in more natural and cohesive video (Figure 4b Latent Addition + Refinment). 5. Results Following [15], we evaluate OmnimatteZero on three applications: (1) Background Reconstruction, where the foreground is removed to recover clean background; (2) Foreground Extraction, where objects are extracted, together with their associated effects (e.g. shadows and reflections); and (3) Layer Composition, where extracted elements are reinserted into new backgrounds while preserving visual consistency. was set to 0 (i.e. no prompt), and all result videos were generated using the same random seed. Quantitative and qualitative results of baselines were obtained from the respective authors. However, since only OmnimatteRFs code is publicly available, we compare against all methods when possible but limit comparisons to OmnimatteRF for new videos. All qualitative results in this paper are based on LTXVideo, as it shows minimal visual differences from Wan2.1. Additionally, due to space constraints, each figure displays single frame per video for qualitative results. Full videos are available in the supplementary material. 5.1. Background layer reconstruction Compared methods: We conducted comparative evaluation of our approach with several SoTA methods. These methods fall into three categories: (A) Omnimatte methods that are trained to decompose given video into layers: Omnimatte [19], D2NeRF [33], LNA [12], OmnimatteRF [17] and Generative Omnimatte [15]. (B) Video inpainting methods that are trained to remove objects from video: RePaint [21] adapted for video, Lumiere inpainting [2], Propainter [36] and DiffuEraser [16]. Finally, (C) An image inpainting method that is applied for each video frame independently: ObjectDrop [32]. Implementation Details: We apply OmnimatteZero using two video diffusion models: LTXVideo [9] v0.9.1 and Wan2.1 [29], both running with Tsteps = 50 denoising steps. For all experiments, we set = 45, as we found that just five steps are sufficient for blending. Input videos were resized to (768, 512) before processing. The guidance scale Datasets and Metrics: We evaluate our method on two standard Omnimatte datasets: Movies [17] and Kubric [33]. These datasets provide object masks for each frame and ground truth background layers for evaluation. All methods are assessed using PSNR and LPIPS metrics to measure the 6 Figure 5. Qualitative Results: Object removal and background reconstruction. The first row shows input video frames with object masks, while the second row presents the reconstructed backgrounds. Our approach effectively removes objects while preserving fine details, reflections, and textures, demonstrating robustness across diverse scenes. Notice the removal of the cats reflection in the mirror and water, the shadow of the dog and bicycle (with the rider), and the bending of the trampoline when removing the jumpers. Figure 6. Qualitative Comparison: Object removal and background reconstruction. Our approach achieves cleaner background reconstructions with fewer artifacts while Generative Omnimatte [15] leaves some residuals, DiffuEraser [16] and ProPainter [36] struggle with noticeable traces (highlighted in red). The last two rows show that OmnimatteZero successfully removes effects where others fail. accuracy of background reconstruction, along with comparisons of training and runtime efficiency on single A100 GPU. Quantitative results: Table 1 presents comparison of OmnimatteZero with the SoTA Omnimatte and inpainting methods on the Movies and Kubric benchmarks. It shows that OmnimatteZero outperforms existing supervised and self-supervised methods designed for object inpainting or omnimatte. It achieves the highest PSNR and lowest LPIPS on both the Movie and Kubric datasets, demonstrating superior background reconstruction. Specifically, OmnimatteZero [LTXVideo] achieves PSNR of 34.11 (Movie) and 44.07 (Kubric), surpassing Generative Omnimatte [15], all while requiring no training or per-video optimization. Notably, this improvement is not due to stronger video generation model, as both OmnimatteZero and Video RePaint [21] use the same generator, yet Video RePaint records the lowest PSNR and highest LPIPS across all benchmarks. Our method is also significantly faster. OmnimatteZero [LTXVideo] runs at just 0.04s per frame (or 24 frames per 7 Figure 7. Qualitative Comparison: Foreground Extraction. Foreground extraction comparison between OmnimatteZero and OmnimatteRF [17]. Our method accurately captures both the object and its associated effects, such as shadows and reflections, in contrast to OmnimatteRF, often missing or distorting shadows (row 2) and reflections (row 3). second). These results establish OmnimatteZero as the first training-free, real-time video matting method, offering both superior performance and practical efficiency. Qualitative results: Figure 5 presents qualitative results of our object removal method across various scenes. The first row shows input video frames with masked objects, while the second row displays the reconstructed backgrounds. The videos include (1) cat running toward mirror, (2) static cat looking left and right, (3) dog running toward man, (4) cyclist passing, and (5) multiple people jumping on trampoline. Our method effectively removes objects like people, animals, and bicycles while preserving fine details and textures. Notably, in the first two columns, OmnimatteZero eliminates the cat without leaving mirror or water reflections. The last column further demonstrates its ability to remove multiple moving objects while maintaining scene integrity, even correcting the trampolines bending effect after removing the jumper. Figure 6 presents qualitative comparison of OmnimatteZero with SoTA video matting and inpainting methods. Our approach consistently produces cleaner background reconstructions with fewer artifacts. The videos in the figure include (1) penguins walking on the shore, (2) car passing on wet road, (3) girl playing with Lego bricks, and (4) people jumping on trampoline. In the first two rows, both OmnimatteZero and Generative Omnimatte [15] successfully remove the penguins and car, naturally filling the background without noticeable artifacts. In contrast, DiffuEraser [16] and ProPainter [36] struggle, leaving visible artifacts (highlighted in red). The third and fourth rows emphasize OmnimatteZeros advantage, effec8 Figure 8. Qualitative Comparison: Layer Composition. The extracted foreground objects, along with their shadows and reflections, are seamlessly integrated into diverse backgrounds, demonstrating the effectiveness of our approach in preserving visual coherence and realism across different scenes. tively removing effects like shadows, while other methods fail, leaving shadows or generating blur artifacts. 5.2. Foreground extraction We aim to extract the foreground object along with its associated effects, such as shadows and reflections. Figure 7 compares OmnimatteZero for foreground extraction with OmnimatteRF [17]. Our training-free method accurately isolates objects while preserving fine details of their interactions with the scene, such as reflections and shadows. In contrast, OmnimatteRF struggles to fully capture the associated effects. Notably, in the second and third rows, our method correctly extracts both the shadow of the cyclist and the reflection of the cat, while OmnimatteRF either distorts or omits these elements. These results demonstrate that OmnimatteZero provides superior, training-free foreground extraction, effectively handling complex visual effects where existing methods fall short. 5.3. Layer composition The objective here is to take the extracted foreground layers and seamlessly composite them onto new background layers. Figure 8 presents the results of OmnimatteZero, highlighting its ability to maintain object integrity, shadows, and reflections while enabling smooth re-composition across different scenes. Notably, it demonstrates how the model adaptively integrates the inserted object into the environment, such as enhancing the cats reflection in the clear water stain or adjusting the mans appearance to match the night lighting. 6. Summary We present the first real-time, training-free generative approach for layered video decomposition. Our method extracts clean background layers and foreground objects while preserving effects like shadows and reflections, enabling seamless re-composition without dataset-specific optimization. Evaluations demonstrate that OmnimatteZero outperforms all training based methods as well as optimization based methods in terms of background reconstruction and running time. However, some challenges persist, as the output can be sensitive to the quality of VAE video encodings, potentially leading to slight differences from the original video (see running dog example in supp). With advancements in video diffusion models, we anticipate that these issues will be addressed in the near future."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 1 [2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. 6 [3] Black-Forest. Flux: Diffusion models for layered image generation. https://github.com/black-forestlabs/flux, 2024. 1 [4] Alper Canberk, Maksym Bondarenko, Ege Ozguroglu, Ruoshi Liu, and Carl Vondrick. Erasedraw: Learning to draw step-by-step via erasing objects from images. 2024. 1 [5] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston H. Hsu. Free-Form Video Inpainting With 3D Gated In ICCV, pages Convolution and Temporal PatchGAN. 90659074, 2019. 2 [6] Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In WACV, 2024. 1, 2 [7] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf. Flow-edge Guided Video Completion. In ECCV, pages 713 729, 2020. [8] Zeqi Gu, Wenqi Xian, Noah Snavely, and Abe Davis. Factormatte: Redefining video matting for re-composition tasks. ACM Transactions on Graphics (TOG), 2023. 2 [9] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, David-Pur Moshe, Eitan Richardson, E. I. Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. LTX-Video: Realtime Video Latent Diffusion. ArXiv, 2024. 2, 3, 5, 6 [10] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grauman, and Alexander G. Schwing. Proposal-Based Video Completion. In ECCV, pages 3854, 2020. 2 [11] Yihan Hu, Yiheng Lin, Wei Wang, Yao Zhao, Yunchao Wei, and Humphrey Shi. Diffusion for natural image matting, 2024. 1 [12] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):112, 2021. 2, [13] Wolfgang Kohler. Gestalt psychology: The definitive statement of the Gestalt theory. H. Liveright, 1992. 5 [14] Weijie Kong. Hunyuanvideo: systematic framework for large video generative models, 2024. 2, 3 [15] Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, JiaBin Huang, Tali Dekel, and Forrester Cole. Generative omnimatte: Learning to decompose video into layers. CVPR, 2024. 2, 4, 6, 7, 8 [16] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. DiffuEraser: Diffusion Model for Video Inpainting, 2025. 2, 6, 7, [17] Geng Lin, Chen Gao, Jia-Bin Huang, Changil Kim, Yipeng Wang, Matthias Zwicker, and Ayush Saraf. OmnimatteRF: Robust Omnimatte with 3D Background Modeling. In ICCV, 2023. 2, 6, 8 [18] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. arXiv:2009.07833, 2020. 2 [19] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William Freeman, and Michael Rubinstein. Omnimatte: In CVPR, Associating objects and their effects in video. 2021. 2, 3, 6 [20] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, William Freeman, and Michael Rubinstein. Associating objects and their effects in video through coordination games. In NeurIPS, 2022. 2 [21] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting In CVPR, using denoising diffusion probabilistic models. 2022. 1, 2, 3, 4, 6, 7 [22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2, 3, [23] Nobuyuki Otsu. threshold selection method from graylevel histograms. IEEE Trans. Syst. Man Cybern., 1979. 5 [24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [25] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 3 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 5 [27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 1 [28] Mohammed Suhail, Erika Lu, Zhengqi Li, Noah Snavely, Leonid Sigal, and Forrester Cole. Omnimatte3d: Associating objects and their effects in unconstrained monocular video. In CVPR, 2023. 2 [29] Wan video. Wan: Open and advanced large-scale video generative models. https://github.com/Wan-Video/ Wan2.1, 2025. 2, 3, 6 [30] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang. Video Inpainting by Jointly Learning Temporal Structure and Spatial Details. In AAAI, pages 52325239, 2019. 2 [31] J.Y.A. Wang and E.H. Adelson. Representing moving images with layers. IEEE Transactions on Image Processing, 1994. 2 [32] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion, 2024. 1, 2, 5, [33] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D2nerf: Self-supervised decoupling of dynamic and static objects from monocular video, 2024. 2, 6 [34] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-Guided Flow Completion and Style Fusion for Video Inpainting. In CVPR, pages 59725981, 2022. 2 [35] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-Guided Transformer for Video Inpainting. In ECCV, pages 7490, 2022. 2 [36] Shangchen Zhou, Chongyi Li, Kelvin C.K Chan, and Chen Change Loy. ProPainter: Improving propagation and transformer for video inpainting. In ICCV, 2023. 2, 6, 7,"
        }
    ],
    "affiliations": [
        "Bar-Ilan University, Ramat-Gan, Israel",
        "NVIDIA Research, Tel-Aviv, Israel",
        "OriginAI, Tel-Aviv, Israel",
        "The Hebrew University of Jerusalem, Jerusalem, Israel"
    ]
}