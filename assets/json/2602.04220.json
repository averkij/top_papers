{
    "paper_title": "Adaptive 1D Video Diffusion Autoencoder",
    "authors": [
        "Yao Teng",
        "Minxuan Lin",
        "Xian Liu",
        "Shuai Wang",
        "Xiao Yang",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process."
        },
        {
            "title": "Start",
            "content": "Adaptive 1D Video Diffusion Autoencoder Yao Teng1 Minxuan Lin2 Xian Liu3 1The University of Hong Kong 2ByteDance Inc. Shuai Wang4 Xiao Yang2 Xihui Liu1* 3CUHK 4Nanjing University 6 2 0 2 ] . [ 1 0 2 2 4 0 . 2 0 6 2 : r Figure 1. One-Dimensional Diffusion Video Autoencoder (One-DVA). This model supports variational-length encoding, where increasing the latent length allows for the capture of richer details. Furthermore, the diffusion-based text-to-video generation can be performed on its latent space."
        },
        {
            "title": "Abstract",
            "content": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while variable-length dropout mechanism dynamically adjusts the latent length. The decoder is pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process. 1. Introduction In the field of visual generation, generative models typically rely on pre-trained video autoencoder to facilitate the generation process. This autoencoder compresses video representations from pixel space into latents (or tokens), enabling the generative model to generate the latents with relatively small sizes rather than handling the vast pixel data directly. The autoencoder consists of an encoder and decoder, and they are trained jointly. The encoder uses neural network to compress the input videos into latent, and the decoder reconstructs the video from this latent. Existing video autoencoders [1, 42, 43, 90] face several critical limitations, and we propose targeted solutions to address these challenges: (1) Fixed Compression Rate: Not all videos require the same token count. For instance, 24 fps, 5-second, 1080p video typically demands around 200,000 tokens with 16 spatial and 4 temporal compression. However, simple videos can be represented with far fewer tokens than the videos with complicated textures and motions. To optimize token efficiency, we propose to adopt dynamic variable-length compression, allowing adaptive latent sizes tailored to the video contents. Currently, several works [81, 91] transform the video inputs into variablelength 1D discrete token sequences for adaptive encoding and verify this idea on class-to-video generation. (2) Inflexible CNN Architecture. Convolutional neural networks (CNNs) rely heavily on human-designed priors, and their fixed-size kernels struggle to process variable-shaped inputs, limiting their ability to decode variable-length latents. In contrast, transformer architectures offer superior flexibility, processing inputs and outputs of any shape via attention mechanisms. Aligning with The Bitter Lesson [78], transformers leverage large-scale data and computation to achieve greater representation capacity with minimal human priors. (3) Lossy Compression: Current compression methods, whether manually determined (e.g., 16 spatial and 4 temporal) or dynamically estimated, aim to balance reconstruction quality and token count but struggle to achieve lossless results. When the token counts are too low, compression becomes overly lossy, requiring the decoder to infer missing details. Therefore, we deem the reconstruction as subtask of generation and propose to use generative decoding paradigm that allows the decoder to learn the dataset distribution and compensate omitted details through generation, minimizing reconstruction errors at the distribution level. In summary, our research aims to design transformer-based autoencoder with generative decoder and variable-length compression, and successfully train this framework to match advanced autoencoders in reconstruction quality while supporting downstream generation. In this paper, we introduce One-Dimensional Diffusion Video Autoencoder (One-DVA), transformer-based framework [89] that achieves adaptive video compression and generative reconstruction within unified design. The encoder leverages Vision Transformer (ViT) [25] that produces structural latents from spatiotemporal embeddings, while set of 1D queries interacts with the features in transformer blocks to extract 1D latents. variable-length dropout mechanism is applied to the 1D latent sequence, dynamically adjusting its length to match video complexity. The decoder is implemented as pixel-space Diffusion Transformer (DiT) [65, 97]. It treats the latents as conditional inputs and performs the diffusion process in pixel space to reconstruct the videos. To ensure One-DVA achieves high reconstruction performance across varying compression levels, we employ twostage training strategy: the first stage prioritizes encoder optimization, while the second stage integrates variable-length compression and diffusion-based decoding. With the standard compression ratio, One-DVA achieves reconstruction performance comparable to 3D-CNN VAEs. This highfidelity reconstruction ensures that the latent space faithfully preserves the information necessary for downstream latent diffusion models (LDM). To further tailor the latent space for the LDM, we project the 1D latents into the space of structural latents via an alignment loss as the regularizer, facilitating joint modeling within single LDM architecture while preserving the reconstruction performance of the autoencoder. To ensure the visual quality of generated videos, we fine-tune the One-DVA decoder using latents generated by the LDM. These latents serve as noisy inputs that help the decoder adapt to the potential artifacts produced by the process of latent generation. 2. Background and Related Work Image Autoencoders In this paper, we classify image (1) Laautoencoders based on the following attributes: tent Representation Type: continuous or discrete; (2) Latent Shape: 2D or 1D; (3) Architecture: CNN or transformer [25, 89]; (4) Decoder Paradigm: deterministic or generative. In the following paragraphs, we will discuss existing autoencoders categorized by these attributes. Continuous 2D CNN Autoencoder: The most classic image autoencoder is the continuous 2D CNN autoencoder [16, 18, 22]. The encoder accepts an input image and outputs low-dimensional 2D latent map through CNN. This latent map has reduced height and width but slightly larger channel dimension compared to the input image. The CNN decoder then takes the 2D latent map as input and reconstructs it into an image. Latent diffusion models [6, 28, 29, 38, 59, 65, 66, 70, 85, 96, 98] perform the diffusion process in the latent space. Discrete 2D CNN Autoencoder: The discrete 2D CNN autoencoder (visual tokenizer) [27, 56, 99, 115, 116, 121] is characterized by employing quantization strategies (such as VQ [27], RQ [45], FSQ [61], or BSQ [128]) to convert continuous latents into discrete tokens. Generative models, such as autoregressive models [3, 21, 24, 51, 69, 77, 86, 93, 102, 114], then learn to generate these discrete tokens to represent an image. 2D Transformer Autoencoder: Having discussed 2D CNN autoencoders, we now turn to new group of 2D autoencoders that are mainly built on transformer blocks. These autoencoders typically contain ViT [25] in their encoder. The transformer architecture enables the use of pretrained foundational models (such as CLIP [68, 87, 120] or DINO [63, 73]) as the main component of the encoder [12, 47, 57, 72, 80, 129, 130]. Additionally, the scalability of the transformer-based models in other domains prompts the exploration of their potential in image reconstruction tasks [36, 108]. 1D Autoencoder: Images can also be represented by 1D 1D autoenlatents or tokens, in addition to 2D ones. 2 coders [8, 13, 14, 26, 40, 54, 62, 67, 107, 108, 111, 117, 119] typically adopt query-based transformer architectures [11, 31, 32, 46, 76, 83, 84, 95, 131] due to their flexibility. In the encoder, the 1D learnable queries extract features from the input images by the attention mechanism [89], producing 1D continuous latents (or discrete tokens). In the decoder, another learnable vector is used to reconstruct the input image. This vector is repeated to match the shape of the input image and is then fed into another transformer to retrieve image information from the latent features, thereby fulfilling the reconstruction task. Since the latent shape can be arbitrarily determined in this autoencoder with 1D being the simplest, we can modify the compression ratio by changing the quantity of the 1D learnable queries. Specifically, during training, dynamic compression can be achieved through variable query counts using the tail dropout, resembling matryoshka learning [44]. The dropout length is randomly sampled from the uniform distribution [62] or estimated by learnable scorers [81, 110]. Autoencoder with Diffusion Decoder: Some approaches replace the deterministic decoder with diffusion-based decoder [4, 19, 30, 64, 71, 104]. For 1D autoencoders, the most intuitive way is directly substituting the learnable vector of the decoder with random Gaussian noise, enabling conditional pixel-space diffusion generation, where latents or tokens serve as the conditions injected by attention [4, 30, 64, 71, 104]. For 2D CNN autoencoders, Gaussian noise branch is added, with condition injection via ControlNet [122] or channel concatenation [126]. Video Autoencoders Similar to image autoencoders, video autoencoders also adopt 3D CNN-based continuous [1, 20, 35, 43, 49, 90, 101, 113] and discrete [1, 79, 92, 116] frameworks, which support diffusion-based [10, 23, 34, 43, 50, 58, 60, 82, 90, 125] and autoregressivebased [42, 100] video generation, respectively. In addition, 1D autoencoders [81, 91] and 3D transformer-based autoencoders [52, 55, 82] have been employed. Diffusion-based video autoencoder decoders have emerged [52, 112, 124]. Although the overall framework designs of video and image autoencoders are similar, their latent representations differ in structure. The advanced video autoencoders [1, 43, 90] typically employ first-frame plus temporal compression strategy, which transforms an input video with shape of into compressed latent with shape of (cid:16) . This design achieves Ps Ps compression in the spatial dimensions, Pt compression in the temporal dimension, and an additional pure spatial compression for the first frame. 1 + 1 Pt Ps (cid:17) Discussion Compared to prior works, this paper proposes to integrate the following three key features into one video autoencoder, and we train this model to achieve reconstruction performance comparable to existing autoencoders: (1) 3 1D variable-length encoding that enables dynamic compression ratios. (2) query-based transformer architecture that allows for flexible video information extraction, forming the foundation for the variable-length encoding. (3) diffusion decoder that improves reconstruction quality. 3. Method This paper proposes the One-Dimensional Diffusion Video Autoencoder (One-DVA), transformer-based framework that supports variable-length 1D encoding and pixel-space diffusion decoding. As illustrated in Fig. 2, the autoencoder consists of an encoder, latent-dropout module, and diffusion-based decoder. The encoder compresses video into two complementary representations: structural latent obtained from the ViT backbone, and 1D latent sequence extracted via query mechanism. The decoder reconstructs the original frames through conditional pixel-space video diffusion. The condition is formed by the structural latent and the 1D latents, and the 1D latents can be truncated via dropout to achieve variable length. 3.1. Query-based Vision Transformer Encoder The transformer architecture is particularly well-suited for encoding videos into variable-length latents, as it processes all inputs as token sequences and applies self-attention in unified manner, accommodating arbitrary shapes with ease. As illustrated in Fig. 2, the input video frames are first processed by linear patchifier, which projects the RGB pixels into high-dimensional spatiotemporal embeddings. These embeddings are then flattened into sequence and concatenated with learnable 1D queries before being fed into stack of transformer blocks. For these sequential queries, we apply learnable positional encodings on them, following [117]. For the spatiotemporal embeddings, absolute positional encodings are used. To facilitate multi-resolution training, we further concatenate special tokens representing the height, width, temporal length, spatial size, and spatial aspect ratio of the inputs to the sequence, in line with [15]. Through the self-attention mechanisms in the transformer blocks, the spatiotemporal embeddings are processed into features and the queries selectively extract the essential spatiotemporal visual content required for reconstruction. Subsequent to the transformer blocks, both the processed spatiotemporal features and the 1D query features are passed through channel compression layer to reduce their channel dimensions. Since the total number of query tokens and spatiotemporal feature vectors exceeds the latent size in standard video encoding (detailed in Sec. 2), we select subset of them to form the final latent. Specifically, given video input of shape W, we obtain the (cid:17) 1D latents by selecting the first queries. Subsequently, we derive structural latent of size Ps Ps 1 Pt (cid:16) the decoder to possess generative capabilities. Specifically, we treat the decoding process as conditional generation task, using variable-length token sequences as conditions within diffusion-based generative framework. As illustrated in Fig. 2, the decoder takes two inputs: condition (i.e., variable-length token sequences processed by the sampler) and noisy input (either random noise or the encoder input perturbed by random noise). During diffusion training, the noisy input is obtained by perturbing the groundtruth video with noise: xt = (1 t) x0 + x1, x1 (0, I), (1) where x0 represents the ground-truth video clip, is sampled timestep ranging from 0 to 1 (timestep sampling details in Sec. 3.4), and x1 is random Gaussian noise. During inference, diffusion sampling progressively refines random noise input into clean video: xt = xs + Dθ(xs, s, z) (t s), x1 (0, I), (2) where Dθ denotes the decoder output, i.e., the velocity prediction, represents the latents (more details of are in Sec. 3.4), and and are consecutive timesteps with < and starting from 1. Decoder Architecture Our decoder is pixel diffusion transformer [97]. The latent input and noisy input are first transformed into high-dimensional features via linear layers, then concatenated and fed into the transformer blocks. For the output of the transformer, the features corresponding to the noisy input positions are separated from the output sequence. Inspired by [6, 82, 97], our unpatchifier consists of long skip connection, linear projection, pixelshuffle operation, and final convolutional layer. Figure 2. Overview: our One-DVA consists of an encoder, diffusion decoder and latent dropout module. The encoder utilizes vision transformer with 1D queries to extract input video features and outputs low-dimensional latents. The latent dropout module dynamically adjusts the length of 1D latents during training. The diffusion decoder is diffusion transformer generating videos in pixel space with the latents as the input condition. Ps (1 ) by sampling the channel-compressed Ps spatiotemporal features from the ViT and performing spatial downsampling. In total, the input video is thus represented (cid:16) , by hybrid latent of shape Ps consistent with advanced video autoencoders [1, 43, 90]. 1 + 1 Pt Ps (cid:17) 3.2. Variable-length Encoding 3.4. Autoencoder Training As illustrated in Fig. 2, the variable-length dropout module dynamically adjusts the 1D latent length. This is achieved via matryoshka training strategy [44]. During training, the module applies random dropout to the 1D latents starting from the tail toward the head to vary their length, with the dropout ratio sampled from distribution governed by motion score computed from pixel differences (see the appendix for details). In the decoder, the dropped tokens are replaced with padding tokens. Furthermore, we configure 10% of conditions to use full latents and another 10% to employ only structural latents. Through this variable-length dropout mechanism, the generative models are able to learn to generate latents of different sizes. 3.3. Diffusion Decoding While compression ratios can be estimated or manually tuned, lossy compression inherently risks the reconstruction error. To enhance reconstruction quality, we consider Loss Functions As the decoder employs diffusionbased paradigm, we use diffusion loss (implemented as flow-matching loss) to train our autoencoder rather than the commonly used reconstruction loss. Specifically, the diffusion loss is as follows: Ldiff = Et,x1,x0 (cid:104) Dθ (xt, t, z) (x1 x0)2 (cid:105) . (3) The training optimizes composite loss function: = λ1Ldiff + λ2Lperceptual + λ3Lkl + λ4Lrepa, (4) where Ldiff is the diffusion loss defined in Eq. (3), Lperceptual is the perceptual loss [39] between VGG features of real and reconstructed frames, Lkl is the KL loss [41] that regularizes the latents to satisfy the standard Gaussian distribution, Lrepa is the REPA loss [118] performed on the features of noisy inputs in the decoder [30, 108], and λ1, λ2, λ3, λ4 are weighting coefficients. 4 Training Recipe We empirically observe that multistage training procedure is more effective in training our autoencoder well than the end-to-end training: Stage 1: Deterministic Pretraining. This stage focuses on training the encoder to extract features critical for reconstruction. To avoid information leakage that would simplify the reconstruction task, we input pure random noise (i.e., 1) into the decoder. This forces the encoder to capture all essential information required for reconstruction. Additionally, we disable variable-length dropout to establish the upper bound of the reconstruction ability of One-DVA. Thus, the latent inputs for the decoder are set as = Eϕ(x0) where Eϕ denotes the encoder. In this configuration, our autoencoder behaves more like an end-to-end model than diffusion model, as it does not support the multi-step denoising in principle. Stage 2: Stochastic Post-Training. We unleash the diffusion timestep sampling and variable-length dropout in this stage. Following [71], we adopt thick-tailed logit-normal sampling for diffusion timesteps and thus we sample noise level as full noise at 10% of the time. Then, we introduce variable-length compression by applying dropout to the latents: = Dropout (Eϕ(x0), l), where is the dropout ratio defined in Sec. 3.2. With this two-stage training strategy, we can train an autoencoder with high reconstruction fidelity. However, it does not inherently guarantee latent space or latent-topixel decoder optimized for downstream diffusion-based video generation. In the following section, we describe another post-training stage to adapt the autoencoder for the generation tasks. 3.5. Adapting Autoencoder for Video Generation We train latent diffusion models (LDM) on the latent space of One-DVA. Formally, the latent space exhibits clear separation between the structural latents and 1D latents, derived from the spatiotemporal patches and learnable queries with dropout, respectively. While diffusion modeling on structural latents from ViT has been validated [82], that of the variational 1D latents remains under-explored. To achieve high-quality video synthesis on such latent space, we propose latent space alignment for joint modeling and fine-tune the decoder using LDM-sampled latents to suppress generation artifacts. Latent Space Alignment The spatiotemporal patches in ViT naturally exhibit locality and spatial structural priors [63, 73], where adjacent latent vectors are identically distributed and show high local similarity. These attributes are essential for efficient diffusion learning [53, 74]. In contrast, learnable queries lack predefined positional information. While highly flexible, they offer no inherent structural guaranties. To address this issue, we inject structural priors into the 1D latents through self-alignment mechanism. Specifically, for each video, we align each 1D latent vector with its best-matching counterpart in the structural latent vector by minimizing their top-1 cosine distance. Additionally, we enforce internal continuity by maximizing the self-similarity between each 1D latent vector and its nearest neighbor. Integrating this regularization, we further fine-tune One-DVA for additional iterations. Empirically, this regularization maintains reconstruction fidelity without degradation, given proper loss weight. Decoder Fine-tuning The sampling process of generative models inevitably introduces prediction errors [97, 125]. In our framework, this manifests as distributional drift [7] between encoded and predicted latents, leading to noticeable patch-like artifacts in the pixel space. This drift could lack closed form. To bridge this training-inference gap, we directly fine-tune the decoder using predicted latents, following the intuition of [67]. Specifically, we optimize the decoder to reconstruct the original ground-truth videos by taking the latents sampled from our LDM as input, rather than the encoded ones. We freeze the encoder during this process to keep the latent space stationary. Empirically, this strategy effectively eliminates generation artifacts within few thousand iterations. 4. Experiments 4.1. Implementation Details Our autoencoder is trained to reconstruct videos of three typical resolutions: 17 456 256, 17 256 456, and 17 256 256, with fps set as 24. The FPS is set as 24 for every video sample. Since 1D latents lack spatial structure, which hinders the patchifying in latent diffusion models, we absorb the commonly-used 2 2 patchifying directly into the compression of our autoencoder, yielding spatiotemporal compression rate of 4 16 16. Then, we set the channel dimension of the latents as 64. Our training is conducted on large-scale internal data. We first trained the model for 415K iterations with batch size of 48, which took approximately 7 days on 48 80G GPUs. We then continued the training with variational 1D latent length and the diffusion scheduler for approximately 800K iterations. The model size of our autoencoder is 1.0B and we use FSDP [127] for training. For text-to-video generation, each DiT has 1.3B parameters with the condition injection using cross-attention like [90]. Evaluation We evaluate the autoencoders on random set with 1000 video clips from the dataset proposed in [5] (spatiotemporal resolution 17 256 256). We use the metrics and evaluation setup identical to Open-sora Plan [50]. The metrics include PSNR, the reconstruction FVD [88], SSIM [103], and LPIPS [123]. To quantitatively evaluate 5 Compr. Ratio (Pt Ps Ps)"
        },
        {
            "title": "Channel\nDim",
            "content": "Auxiliary Compr. Ratio rFVD () PSNR () SSIM () LPIPS ()"
        },
        {
            "title": "Autoencoders",
            "content": "CogVideoX [113] HunyuanVideo [43] Wanx2.1 [90] Wanx2.2 [90] Magi1 [82] 4 8 8 4 8 8 4 8 8 4 16 16 4 8 8 Ours Ours (Avg 55.8% 1D) Ours (Con 55.8% 1D) Ours (0% 1D) (4 16 16) ( 41616 55.8% ) ( 41616 55.8% ) / 16 16 16 48 16 64 64 64 8 8 8 8 8 8 16 16 / 16 16 16 16 16 16 16 16 68.17 51.47 62.25 60.18 70.07 56.96 70.28 72.42 149.97 34.97 35.54 34.95 35.23 36.25 36.48 35.42 35.40 32. 0.94 0.94 0.94 0.94 0.95 0.95 0.94 0.94 0.91 0.033 0.023 0.024 0.023 0.035 0.025 0.029 0.029 0.057 Table 1. Comparison of video reconstruction quality across different autoencoders. Compr. denotes Compression. Bold values indicate the best performance, while bold-underlined values represent the second best. / denotes cases where the item is not applicable. Con X% 1D means using the first X% of tokens per video. Avg X% 1D means using global average of X% tokens, with per-video selection based on the score defined in Sec. 3.2. By default, 100% 1D latents are used."
        },
        {
            "title": "Method",
            "content": "Iters rFVD () PSNR () 4.2. Comparison to State-of-the-art Methods 415K 67.56 Pretrained + Further Training + 85K 67.36 + Diffusion Post-training + 85K 65.19 36.02 35.85 36.26 Table 2. Study on the post-training with diffusion scheduler."
        },
        {
            "title": "Iters",
            "content": "rFVD () PSNR () Stage-1 End-to-end 217K 217K 115.64 230.06 34.20 31. Table 3. Study on the effectiveness of stage-wise training. (a) rFVD (b) PSNR Figure 3. Reconstruction quality across different diffusion sampling steps (1, 4, 8, and 25) and varying 1D latent lengths. the visual quality of the generated videos in the class-tovideo task, we use the same evaluation code as in [60]. 6 In this section, we compare our One-DVA to the advanced autoencoders. Note that our autoencoder is trained on multiresolution videos, where the total number of queries is set according to the maximal resolution to ensure compatibility. During inference on 17 256 256 videos, we truncate the number of queries to achieve standard compression ratio, resulting in latent length of (4 + 1) 16 16 with channel dimension as 64. In Tab. 1, we compare our autoencoder with recent state-of-the-art video autoencoders on the task of video reconstruction. Firstly, our method with standard compression ratio achieves the best overall performance in terms of PSNR and SSIM. It also attains the second-lowest rFVD score. In the subsequent row of this table, we utilize the scoring mechanism detailed in Sec. 3.2 to determine the 1D latent length for each video reconstruction. In comparison, we also evaluate baseline that applies constant latent length across all videos with the identical usage of tokens. The results demonstrate that the reconstruction using the estimation outperforms the fixed-length approach, proving the effectiveness of our scoring strategy. In the bottom row, in the structural-only setting, where videos are reconstructed using solely structural latents without 1D latents, reconstruction remains feasible, albeit at the cost of reduced visual quality. 4.3. Analysis on Reconstruction Reconstruction with Variable-length 1D Latents In addition to the results in Tab. 1, which demonstrate that our autoencoder can reconstruct videos at different compression ratios, we conduct detailed case analysis of the impact of 1D latent length on reconstruction quality. As illustrated in Fig. 5, videos containing larger motions exhibit steeper decline in PSNR as the 1D latent length decreases. For example, the chart shows that achieving 90% PSNR requires Figure 4. Reconstructed videos with various 1D latent lengths. The first row shows the ground-truth (GT) videos, while the subsequent rows depict reconstructions with 1D latent lengths of 0, 200, 600, and 1000, respectively. The red dashed boxes highlight regions where reconstruction quality varies noticeably across different 1D latent lengths. We sample frames at 5-frame interval. longer 1D latent length for videos with more motion. Moreover, we present qualitative results in Fig. 4. We observe that longer 1D latents enable more accurate reconstruction of fine details, such as scene text, whereas the video regions that contain motions appear blurry when they are reconstructed without 1D latents. Study on Training Strategy As shown in Tab. 3, with sufficient training duration, our two-stage training pipeline outperforms the end-to-end approach in reconstruction. These results show the advantages of the pretraining-thenpost-training paradigm for our autoencoder: in end-to-end training, the information of input videos is leaked to the decoder, simplifying the reconstruction task and hindering In conthe encoder from learning effective information. trast, our deterministic pretraining first compels the encoder to learn to capture features essential to reconstruction, and then the standard diffusion training is performed. Effectiveness of Diffusion Scheduling We verify that diffusion-based training and sampling yield improved reconstruction quality. As shown in Tab. 2, training without stochastic timesteps (resulting in one-step sampling directly from noise to video) produces slight changes in reconstruction performance. In contrast, by employing stochastic timesteps and multi-step diffusion sampling with the same number of iterations, we observe performance boost within sufficient number of training iterations. Furthermore, as shown in Fig. 3, we conduct ablation studies on the number of diffusion sampling steps. We observe that increasing the number of steps yields great benefits to rFVD with insufficient condition (short 1D latents). When the condition is strong (i.e., using full 1D latents), the number of sampling steps has less impact. Moreover, we observe Figure 5. Quantitative reconstruction metrics using variablelength 1D latents. Videos with greater motion exhibit steeper PSNR decline as the 1D latent length decreases. 7 Figure 6. Text-to-video results of our latent diffusion model trained on the latent space of our autoencoder. that rFVD improvements occur at the expense of PSNR. We hypothesize that the diffusion process prioritizes capturing the dataset distribution over per-sample reconstruction fidelity, thereby influencing the PSNR. 4.4. Analysis on Generation To assess the generative capabilities of the latents in our autoencoder, we train latent diffusion models for video generation and evaluate them on two tasks: class-conditional generation and text-to-video generation. For qualitative results, we present the results of textto-video generation as well as the corresponding prompts in Fig. 6. For quantitative evaluation, we conduct classconditional video generation at 17 256 256 spatiotemporal resolution following the benchmark in [52]. As reported in Tab. 4, our full framework utilizing the One-DVA latent space achieves gFVD of 210.9, which matches the performance of methods such as Hi-VAE [52]. The OneDVA decoder fine-tuning process specifically contributes to this result. Notably, the decoder fine-tuned on the text-tovideo dataset using predicted latents from the corresponding diffusion model remains effective when applied to the class-to-video task. This suggests that prediction errors are similar across different generative tasks, allowing the errorIn contrast, omitcorrection capability to be transferred. ting this fine-tuning step leads to noticeable degradation in gFVD. Also, the class-to-video LDM with structural latents alone (i.e., 0% 1D latents) yields reasonable generation results, as these latents encode the low-frequency components of the videos. However, due to the lack of sufficient high-frequency information, these latents impose an upper"
        },
        {
            "title": "Methods",
            "content": "gFVD () VideoGPT [109] StyleGAN-V [75] LVDM [37] Latte [60] iVideoGPT [106] Hi-VAE + DiT [52] Ours (0% 1D) Ours (w/o dec ft) Ours 2880.6 1431.0 372.0 478.0 254.8 210.9 325.8 274.2 210.9 Table 4. The quantitative results for class-to-video generation. bound on generation quality, consistent with the limited reconstruction performance reported in Tab. 1. 5. Conclusion In this work, we introduce One-Dimensional Diffusion Video Autoencoder (One-DVA), transformer-based framework that unifies adaptive 1D video tokenization and diffusion-based generative decoding. By combining querybased encoding with variable-length dropout, One-DVA supports dynamic video compression. The pixel-space diffusion decoder further enhances reconstruction with the latents as conditions. Extensive experiments validate that One-DVA is comparable to advanced 3D CNN VAEs in reconstruction. Moreover, One-DVA supports downstream latent diffusion models for video generation."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 1, 3, 4 [2] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 1 [3] Emu3 Team BAAI. Emu3: Next-token prediction is all you need, 2024. 2 [4] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin ElNouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. In Forty-second International Conference on Machine Learning, 2025. 3 [5] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728 1738, 2021. [6] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. 2, 4 [7] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. 5 [8] Lao Beyer, Tianhong Li, Xinlei Chen, Sertac Karaman, and Kaiming He. Highly compressed tokenizer can generate without training. arXiv preprint arXiv:2506.08257, 2025. 3 [9] Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating senIn Proceedings of the tences from continuous space. 20th SIGNLL conference on computational natural language learning, pages 1021, 2016. 3 [10] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 3 [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the European conference on computer vision, pages 213229, 2020. [12] Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning visual foundation encoders to tokenizers for diffusion models, 2025. 2 [13] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In Forty-second International Conference on Machine Learning, 2025. 3 [14] Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2835828370, 2025. 3 [15] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 3 [16] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 2 [17] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [18] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space. arXiv preprint arXiv:2508.00413, 2025. 2 [19] Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Diffusion autoenarXiv preprint Rambhatla, coders are scalable image tokenizers. arXiv:2501.18593, 2025. 3 and Ishan Misra. [20] Yu Cheng and Fajie Yuan. Leanvae: An ultra-efficient reconstruction vae for video diffusion models. arXiv preprint arXiv:2503.14325, 2025. 3 [21] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [22] Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789, 2019. 2 [23] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. 3 [24] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 2 [25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2 [26] Shivam Duggal, Phillip Isola, Antonio Torralba, and William Freeman. Adaptive length image tokenization via recurrent allocation. In First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models, 2024. 3 [27] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [28] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2, 1 [29] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. 2 [30] Ziteng Gao and Mike Zheng Shou. D-ar: Diffusion via autoregressive models. arXiv preprint arXiv:2505.23660, 2025. 3, 4 [31] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo. Adamixer: fast-converging query-based object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53645373, 2022. 3 [32] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. [33] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 3 [34] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2Vadapter: general image-to-video adapter for diffusion In ACM SIGGRAPH 2024 Conference Papers, models. pages 112, 2024. 3 [35] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 3 [36] Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv preprint arXiv:2501.09755, 2025. 2 [37] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Latent video diffusion models for arXiv preprint and Qifeng Chen. high-fidelity long video generation. arXiv:2211.13221, 2022. 8 [38] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802, 2024. [39] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694711. Springer, 2016. 4 [40] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with comarXiv preprint pact text-aware one-dimensional tokens. arXiv:2501.07730, 2025. 3 [41] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [42] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 1, 3 [43] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 3, 4, 6 [44] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. 3, 4 [45] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532, 2022. [46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [47] Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, and Zhifeng Chen. Manzano: simple and scalable unified multimodal model with hybrid vision tokenizer, 2025. 2 [48] Yan Li, Changyao Tian, Renqiu Xia, Ning Liao, Weiwei Guo, Junchi Yan, Hongsheng Li, Jifeng Dai, Hao Li, and Xue Yang. Learning adaptive and temporally causal video tokenization in 1d latent space. arXiv preprint arXiv:2505.17011, 2025. 5 [49] Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video In Proceedings of the Computer Vision diffusion model. and Pattern Recognition Conference, pages 1777817788, 2025. 3 10 [50] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 3, [51] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 2 [52] Huaize Liu, Wenzhang Sun, Qiyuan Zhang, Donglin Di, Biao Gong, Hao Li, Chen Wei, and Changqing Zou. Hivae: Efficient video autoencoding with global and detailed motion. arXiv preprint arXiv:2506.07136, 2025. 3, 8 [53] Shizhan Liu, Xinran Deng, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Jie Tang. Delving into latent spectral biasing of video vaes for superior diffusability. arXiv preprint arXiv:2512.05394, 2025. 5 [54] Yiheng Liu, Liao Qu, Huichao Zhang, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Xian Li, Shuai Wang, Daniel Du, et al. Detailflow: 1d coarse-to-fine autoregressive image generation via next-detail prediction. arXiv preprint arXiv:2505.21473, 2025. 3 [55] Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, and Yinfei arXiv Yang. Atoken: unified tokenizer for vision. preprint arXiv:2509.14476, 2025. 3 [56] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 2 [57] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [58] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 3 [59] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative arXiv models with scalable interpolant preprint arXiv:2401.08740, 2024. 2 transformers. [60] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3, 6, 8 [61] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. 2 [62] Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, and Yu Yamaguchi. One-d-piece: Image tokenizer arXiv preprint meets quality-controllable compression. arXiv:2501.10064, 2025. [63] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 5 [64] Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, and Hanwang Zhang. Generative multimodal pretraining with discrete diffusion timestep tokens. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26136 26146, 2025. 3 [65] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2 [66] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [67] Kai Qiu, Xiang Li, Hao Chen, Jason Kuen, Xiaohao Xu, Jiuxiang Gu, Yinyi Luo, Bhiksha Raj, Zhe Lin, and Marios Savvides. Image tokenizer needs post-training. arXiv preprint arXiv:2509.12474, 2025. 3, [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 2, 5 [69] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In CVPR, pages synthesis with latent diffusion models. 1067410685. IEEE, 2022. 2 [71] Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, and Jiajun Wu. Flow to the mode: Mode-seeking diffusion autoencoders for state-of-the-art image tokenization. arXiv preprint arXiv:2503.11056, 2025. 3, 5 [72] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. 2 [73] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 2, 5 [74] Jaskirat Singh, Xingjian Leng, Zongze Wu, Liang Zheng, Richard Zhang, Eli Shechtman, and Saining Xie. What matters for representation alignment: Global information or spatial structure? arXiv preprint arXiv:2512.10794, 2025. [75] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the In Proceedprice, image quality and perks of stylegan2. 11 ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36263636, 2022. 8 [76] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1445414463, 2021. 3 [77] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 [78] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. 2 [79] Anni Tang, Tianyu He, Junliang Guo, Xinle Cheng, Li Song, and Jiang Bian. Vidtok: versatile and open-source video tokenizer. arXiv preprint arXiv:2412.13061, 2024. 3 [80] Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang. Unilip: Adapting clip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025. 2 [81] Chenxin Tao, Xizhou Zhu, Shiqian Su, Lewei Lu, Changyao Tian, Xuan Luo, Gao Huang, Hongsheng Li, Yu Qiao, Jie Zhou, et al. Learning 1d causal visual representation with de-focus attention networks. Advances in Neural Information Processing Systems, 37:2591325937, 2024. 2, 3 [82] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 3, 4, 5, 6, 1 [83] Yao Teng and Limin Wang. Structured sparse R-CNN for In CVPR, pages 19415 direct scene graph generation. 19424. IEEE, 2022. [84] Yao Teng, Haisong Liu, Sheng Guo, and Limin Wang. Stageinteractor: Query-based object detector with crossstage interaction. CoRR, abs/2304.04978, 2023. 3 [85] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. 2 [86] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. 2 [87] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 2, 1 [88] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5 [89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, [90] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 3, 4, 5, 6 [91] Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with arXiv preprint learned autoregressive generative prior. arXiv:2410.21264, 2024. 2, 3 [92] Junke Wang, Yi Jiang, Zehuan Yuan, Bingyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. Advances in Neural Information Processing Systems, 37:2828128295, 2024. 3 [93] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. 2 [94] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [95] Shuai Wang, Yao Teng, and Limin Wang. Deep equilibrium object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 62966306, 2023. [96] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcnlike architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024. 2 [97] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 2, 4, 5 [98] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 2 [99] Wenxuan Wang, Fan Zhang, Yufeng Cui, Haiwen Diao, Zhuoyan Luo, Huchuan Lu, Jing Liu, and Xinlong Wang. End-to-end vision tokenizer tuning. arXiv preprint arXiv:2505.10562, 2025. 2 [100] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos arXiv preprint with autoregressive language models. arXiv:2410.02757, 2024. 3 [101] Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, and Jiang Bian. Vidtwin: Video vae with decoupled structure and dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2292222932, 2025. 12 [102] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1295512965, 2025. 2 [103] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE transactions on image proto structural similarity. cessing, 13(4):600612, 2004. 5 [104] Xin Wen, Bingchen Zhao, Ismail Elezi, Jiankang Deng, and Xiaojuan Qi. principal components enable new language of images. arXiv preprint arXiv:2503.08685, 2025. 3 [105] Bing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, Jianbing Wu, Jiangfeng Xiong, Jie Jiang, et al. Hunyuanvideo 1.5 technical report. arXiv preprint arXiv:2511.18870, 2025. 5 [106] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, ivideogpt: Interactive Jianye Hao, and Mingsheng Long. videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:6808268119, 2024. 8 [107] Pingyu Wu, Kai Zhu, Yu Liu, Longxiang Tang, Jian Yang, Yansong Peng, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Alitok: Towards sequence modeling alignment between tokenizer and autoregressive model. arXiv preprint arXiv:2506.05289, 2025. [108] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint arXiv:2504.08736, 2025. 2, 3, 4 [109] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 8 [110] Wilson Yan, Volodymyr Mnih, Aleksandra Faust, Matei Zaharia, Pieter Abbeel, and Hao Liu. Elastictok: AdaparXiv preprint tive tokenization for image and video. arXiv:2410.08368, 2024. 3 [111] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and Yue Wang. Latent denoising makes good visual tokenizers. arXiv preprint arXiv:2507.15856, 2025. 3 [112] Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, ChenWei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, et al. Rethinking video tokenization: conditioned diffusion-based approach. arXiv preprint arXiv:2503.03708, 2025. 3 [113] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, [114] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2 [115] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 2 [116] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 2, 3 [117] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940 128966, 2024. 3 [118] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 4, [119] Kaiwen Zha, Lijun Yu, Alireza Fathi, David Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Languageguided image tokenization for generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1571315722, 2025. 3 [120] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2 [121] Borui Zhang, Qihang Rao, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Quantize-then-rectify: Efficient vq-vae training. arXiv preprint arXiv:2507.10547, 2025. 2 [122] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 3 [123] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [124] Yitian Zhang, Long Mai, Aniruddha Mahapatra, David Bourgin, Yicong Hong, Jonah Casebeer, Feng Liu, and Yun Fu. Regen: Learning compact video embedding with (re- ) generative decoder. arXiv preprint arXiv:2503.08665, 2025. [125] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, and Bingyue Peng. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. 3, 5 [126] Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, and Ting Liu. epsilon-vae: Denoising as visual decoding. arXiv preprint arXiv:2410.04081, 2024. 3 [127] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, ChienChin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Pytorch fsdp: experiMyle Ott, Sam Shleifer, et al. ences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 5 13 [128] Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. 2 [129] Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizers for autoregressive image generation. arXiv preprint arXiv:2507.08441, 2025. [130] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 2 [131] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 3 14 Adaptive 1D Video Diffusion Autoencoder"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Autoencoder Details Architecture Details Both the encoder and decoder utilize transformer architecture with hidden dimension of 1152, 24 blocks, and 16 attention heads. Following [82], the spatial patch size is set to 8. Following [2], the temporal patch size is set to 2 for the decoder, while we set temporal patch size as 4 to the encoder for better efficiency. For input video sizes not divisible by the patch sizes, we apply zero padding along the spatial axes and replicate padding along the temporal axis. Since our autoencoder is trained to reconstruct videos of three typical resolutions (17 456 256, 17 256 456, and 17 256 256), we set the maximum number of queries to 1938, corresponding to compression ratio of 4 16 16. Heuristic Motion-aware Token Length Estimation To train our autoencoder to handle variable-length 1D latents, we employ heuristic motion estimator to compute motion score for each video clip, which directly determines the length of the 1D latents. We compute the raw motion score as follows: First, video frames are converted to grayscale. We then calculate absolute pixel differences between consecutive frames. Finally, the pixel differences are averaged over all spatiotemporal dimensions to obtain the non-negative scalar score. During training, exponential moving averages of the mean µ and standard deviation σ of this value are maintained online, and the raw score is normalized simply as ˆs = µ+3σ to obtain motion score in [0, 1]. The normalized motion score ˆs determines the expected fraction of the maximum 1D latent length. To introduce stochasticity while preserving the central tendency, we sample multiplicative factor, similar to the logit-normal sampling in [28]: η = 2 sigmoid(z), (0, 1) but the center value is 1. Thus, the final number of temporal tokens is computed as round(cid:0)ˆs Nmax η(cid:1), where Nmax is the predefined maximum token count. Training Details We use AdamW with (β1 = 0.9, β2 = 0.999) for optimization, and the weight decay is set to 104. Stage 1: The loss weights for autoencoder training are set as λ1 = 10, λ2 = 0.1, λ3 = 1 104, and λ4 = 0.1, where λ1 is large because we observe that the ℓ2-norm causes small loss value, and we increase the loss weight for balance. We set λ3 = 1 104 because the larger weight for KL loss causes the overall loss spike until the training is stable. For REPA loss [118], we use an image foundational model, SigLIP [87], for providing supervision, because this model shows reconstruction ability proven in [17]. As there is temporal patchifying in our model, we interpolate the features across the temporal dimension for the supervision of REPA. The learning rate for our first stage training is set to 5 105 and for the second stage is 1 105. B. Generative Model Details Architecture Details For text-to-video generation, we employ Qwen2.5-VL [94] as the text encoder, with text conditions injected via cross-attention. Each DiT has 1.3B parameters with hidden dimension of 1536, 20 blocks, and 16 attention heads. For class-to-video generation, the DiT architecture consists of 1024-dimensional hidden state, 24 blocks, and 16 attention heads. Training Details For text-to-video generation, we employ Qwen2.5-VL [94] as the text encoder, integrating text features via cross-attention. To ensure training efficiency and effectiveness, we adopt two-stage strategy. In the first stage, as the size of the structural latent is much smaller than 1D latents, we train DiT exclusively on structural latents using 48 80G GPUs (per-GPU batch size of 32) for 300K iterations, taking approximately 19 days. As demonstrated in Fig. 7, the training of this stage leads to coherent synthesized videos across diverse scenes since the structural latents alone successfully capture sufficient low-frequency semantic information and spatial constraints. In the second stage, the DiT is further trained on both structural and 1D latents with per-GPU batch size of 8 for 350K iterations, and the corresponding results are shown in Fig. 6. For classto-video generation, DiT is directly trained on the full latent space with global batch size of 24 16 for 800K training iterations. C. Autoencoder Adaptation Discrepancy between Structural and 1D Latents. As discussed in Sec. B, we initially train the video diffusion model exclusively on structural latents to efficiently establish pretrained video generation model, subsequently incorporating 1D latents. We observe that although both latent types originate from the same Transformer blocks, they exhibit representation discrepancy. Unlike structural latents, which are derived directly from ViT outputs and possess inherent spatial priors, 1D latents emerge from learnable queries that lack such locality constraints. This discrepancy manifests as various issues, e.g., different statistics, unbalanced loss scales, and distinct visual artifacts. We conduct the following experiments for analysis: 1 Figure 7. Text-to-video results of our latent diffusion model trained on the structural latents of our autoencoder. (a) Pure 1D Latents (b) Hybrid Latents (First-frame Structural) (a) Before alignment (c) 3D Structural Latents Figure 8. Three continuous frames generated across different latent spaces. (a) Results in pure 1D latent space, exhibiting distorted spatial layouts. (b) Results in hybrid latent space where only the first frame is structural. The subsequent frames show abrupt transitions and temporal discontinuity (marked by red dashed box). (c) Results in the original 3D structural latent space, maintaining spatiotemporal consistency. First, we fine-tune variant of One-DVA utilizing only 1D latents (excluding the latent alignment loss). When transferring diffusion model pre-trained on structural latents to this pure 1D latent space, the model fails to capture coherent spatial structures. Despite an extensive training phase covering over 13 million samples (105K iterations 128 global batch size), the generated character remains structurally distorted, as shown in Fig. 8a. This indicates that 1D latents encode information in manner different from their structural counterparts. Notably, this failure occurs despite the autoencoder achieving PSNR of 33.61, which is sufficient for high-quality reconstruction. Furthermore, we evaluate hybrid configuration where structural latents encode only the first frame, while 1D latents encode (b) After alignment Figure 9. Effect of latent alignment on training process. (a) Without alignment, the loss curves of different latents exhibit divergence. (b) The proposed alignment mechanism leads to more consistent loss curves. the remainder. We fine-tune the structural-latent-based diffusion model on this configuration using 28 million samples (290K iterations 96 global batch size). As shown in Fig. 8b, while solid structural foundation is established, subsequent frames suffer from abrupt transitions and temporal discontinuity. This suggests that although the structural latent provides coarse layout for the entire video clip, it is still insufficient to enforce consistency with the generated 1D latent. Moreover, standard 3D ViT-based autoencoder (utilizing 3D structural latents with 41616 compression ratio) produces stable results with preserved spatial integrity. This confirms that the structural latent framework is inherently robust, and the observed limitations are specifically tied to the unique behavior of the 1D latents. Therefore, it is imperative to align the 1D latents with the 2 Method One-DVA + Self-Align Loss Weight Iters rFVD () PSNR () / 797K 0.1 + 317K 0.01 + 135K 56.96 72.66 59.16 36.48 35.83 36.55 Table 5. Reconstruction with self-alignment regularization. (a) Decoded frames without decoder finetuning (b) Decoded frames with decoder finetuning Figure 10. Visual impact of decoder fine-tuning. (a) Without the finetuning, prediction errors manifest as prominent patch-like artifacts and blocky irregularities on surfaces such as human faces. (b) By post-training the One-DVA decoder on predicted latents, these artifacts are successfully eliminated, significantly enhancing visual smoothness. structural latents. By directly injecting the structural priors into the 1D latent space, we can ensure consistent spatial correspondence, thereby guaranteeing that each learnable query encodes meaningful and structural information. Latent Space Alignment As detailed in Sec. 3.5, we regularize the 1D latents via self-alignment loss to enforce the structural prior. This is achieved by aligning the latents with their best-matching structural counterparts which naturally exhibit smooth and low-frequency characteristics. During this phase, we also increase the KL loss weight for lower latent variance [9]. As shown in Tab. 5, regularization weight of 0.01 maintains reconstruction fidelity without compromise. Such distributional alignment facilitates the learning process for the DiT with channel latent norm, as reflected in the more consistent loss curve shown in Fig. 9. Furthermore, we plot the statistics of the latents in Fig. 11a and Fig. 11b. Our analysis reveals that the self-alignment mechanism leads to more consistent statistics across the latent space. For example, the indices of channels exhibiting high variance become nearly identical, indicating improved distributional alignment between the structural and 1D latents. propose post-training the pixel-space decoder with the welltrained latent diffusion model (LDM) to eliminate these artifacts, an approach supported by the following theoretical intuition. Using the predicted velocity vψ(zt, t, c) for simplicity, the estimated clean latent ˆz0 can be derived as: ˆz0 = zt vψ(zt, t, c) = (1 t) z0 + ϵ vψ(zt, t, c) = z0 + t(cid:2)(ϵ z0) vψ(zt, t, c)(cid:3), where U(0, 1), ϵ (0, I). (5) The term t(cid:2)(ϵ z0) vψ(zt, t, c)(cid:3) can be viewed as disturbance to the ground-truth latent z0, reflecting the gap between the true and predicted velocities. By fine-tuning the decoder using the predicted ˆz0 as conditionshifting the mapping from Dθ(xs, s, z0) to Dθ(xs, s, ˆz0), the model learns to adapt to the training error of the LDM. As illustrated in Fig. 10b, we fine-tune the decoder with batch size of 8 for 40K iterations. These patch-like artifacts are successfully eliminated, leading to enhanced smoothness. Quantitative results in Tab. 4 further confirm that this adaptation yields benefits for generation quality. D. Further Analysis on Autoencoder Scaling the Autoencoder As our autoencoder adopts transformer-based architecture, we investigate the effect of model scaling. We train variants with 1B and 3B parameters under identical settings and observe that the loss curves are nearly overlapping throughout training, as shown in Fig. 12. This phenomenon is in line with the finding in [108] where scaling autoencoders beyond 1B parameters brings little improvement in reconstruction. We attribute this phenomenon to the relative simplicity of the reconstruction objective, which appears insufficiently challenging to fully leverage the additional capacity of larger models. Accordingly, we select the 1B-parameter autoencoder as our final model, as larger variant shows similar reconstruction loss at significantly higher computational cost. Post-training with GAN Loss We further explore whether introducing GAN loss [33] after the first pretraining stage can improve perceptual quality. As shown in Tab. 6, even additional 5K iterations of GAN training significantly degrade both rFVD (67.56 75.48) and PSNR (36.02 35.67). Consequently, we avoid the GAN-based post-training in our framework. Decoder Fine-tuning We observe that training diffusion model directly on combination of structural and 1D latents often results in prominent patch-like artifacts, as shown in Fig. 10a. Even when the optimization of the loss curve appears aligned, 1D latents introduce more obvious artifacts compared to structural latents. To mitigate this, we E. Limitation and Future Work Although One-DVA achieves adaptive compression and high-fidelity reconstruction, several directions remain for further exploration. Currently, while our architecture has the potential to be compatible with streaming generation 3 (a) The statistics without the latent alignment process (b) The statistics after the latent alignment process Figure 11. The statistics of the latents provided by One-DVA."
        },
        {
            "title": "Method",
            "content": "Iters rFVD () PSNR () Pretrained 415K 67.56 + GAN Post-training + 5K 75.48 36.02 35.67 Table 6. Study on GAN-based post-training. Applying adversarial training after the pretraining stage harms both rFVD and PSNR. (e.g., utilizing overlapping spatial-temporal windows, similar to Magi-1 [82], to enable long video modeling), this feature has yet to be fully realized in the experiments. Furthermore, while we employ random sampling to determine token counts during training, identifying the theoretically optimal token length for varying video complexities remains an open question, necessitating move beyond purely emFigure 12. Loss curves for autoencoders with 1B and 3B parameters. The two curves remain extremely close for the entire training process. pirical estimations [48]. It is also worth noting that the decoder in One-DVA serves not merely to provide supervision to the encoder, but as critical pixel-space diffusion refiner during inference [105] which can be integrated with features like super-resolution. Moreover, we envision incorporating pre-trained foundation models (e.g., CLIP [68]) to develop more semantically grounded foundation autoencoder with variational or multi-scale encoding capabilities. We also aim to explore an all-in-one pixel-space diffusion decoder that integrates reconstruction and conditional generative tasks (e.g., text/image-to-video) within single framework. Such an architecture would eliminate the need for separate latent diffusion model, paving the way toward truly end-to-end, efficient, and semantically aligned video foundation model."
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "CUHK",
        "Nanjing University",
        "The University of Hong Kong"
    ]
}