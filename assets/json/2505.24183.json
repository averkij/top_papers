{
    "paper_title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "authors": [
        "Yaoyu Zhu",
        "Di Huang",
        "Hanqi Lyu",
        "Xiaoyun Zhang",
        "Chongxiao Li",
        "Wenxuan Shi",
        "Yutong Wu",
        "Jianan Mu",
        "Jinghua Wang",
        "Yang Zhao",
        "Pengwei Jin",
        "Shuyao Cheng",
        "Shengwen Liang",
        "Xishan Zhang",
        "Rui Zhang",
        "Zidong Du",
        "Qi Guo",
        "Xing Hu",
        "Yunji Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 8 1 4 2 . 5 0 5 2 : r CodeV-R1: Reasoning-Enhanced Verilog Generation Yaoyu Zhu1, Di Huang1, Hanqi Lyu1,2, Xiaoyun Zhang1,3, Chongxiao Li1,3, Wenxuan Shi1,3, Yutong Wu1,3, Jianan Mu1, Jinghua Wang3, Yang Zhao1,3, Pengwei Jin1,3, Shuyao Cheng1, Shengwen Liang1, Xishan Zhang1,4, Rui Zhang1, Zidong Du1, Qi Guo1, Xing Hu1(cid:12), Yunji Chen1,3(cid:12) 1 SKL of Processors, Institute of Computing Technology, CAS 2 University of Science and Technology of China 3 University of Chinese Academy of Sciences 4 Cambricon Technologies https://iprc-dip.github.io/CodeV-R"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NLcode pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies codeNLcode consistency via the generated testbench, and filters out inequivalent examples to yield high-quality dataset. Third, we employ two-stage distill-thenRL training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6 % and 72.9 % pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 1220 %, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have recently demonstrated remarkable progress on reasoning tasks when trained via reinforcement learning with verifiable reward (RLVR). Notable examples include OpenAI-o1 [25] and DeepSeek-R1 [3], which exhibit emergent reasoning capabilities on problems endowed with explicit verification proceduressuch as software programming and mathematical problem solving. This success suggests promising opportunity to apply RLVR within electronic design automation (EDA), specifically to the automatic generation of hardware description languages (HDLs) like Verilog from natural-language (NL) specifications [36]. However, the three foundational components required for effective RLVR (i) reliable verification environment, (ii) high-quality NL-code data, and (iii) an efficient training algorithm each present significant challenges in training reasoning LLMs for Verilog generation: Preprint. Under review. (1) Automated verification of hardware designs remains difficult. RLVR requires verification environment capable of providing accurate rewards. However, even in the data-rich software coding domain, such environments are rare. For example, most problems in the programming-contest dataset APPS [6] have only one or two sets of unit tests, and they exhibit false-positive rate of up to 60% when evaluated with an average of 20 unit tests [12]. Consequently, the software community has adopted the practice of using LLMs to generate additional unit tests in order to improve verification quality [11, 39]. Nevertheless, this approach is both costly and of limited effectiveness for hardware designs, because LLMs lack the hardware-specific knowledge needed to handle the complex state spaces and corner cases of sequential circuits. For example, if the reset and clock signals are not correctly configured, the intended functionality cannot be properly verified. (2) High-quality NLcode pairs for hardware designs are scarce. The proprietary nature of hardware designs severely limits the availability of annotated Verilog examples. Although several LLM-based methods have been proposed to synthesize NLcode pairs [5, 16, 43, 46], the resulting datasets often suffer from low-quality data (see Appendix for examples), rendering them inadequate for RLVRs stringent requirements. (3) The computational cost of RLVR is prohibitive. Training 32B LLM on 1K data for 5 epochs using 16 NVIDIA H100 GPUs with supervised fine-tuning (SFT) takes only 0.5 hours [23]. In contrast, training 14B LLM on 24K verifiable coding problems with reinforcement learning can take over 2.5 weeks on 32 NVIDIA H100 GPUs [20], making it prohibitively expensive to train Verilog reasoning LLM using RLVR. To overcome these challenges, we introduce CodeV-R1, comprehensive RLVR framework for Verilog generation. Our contributions are threefold: (1) Automated testbench generation. We develop rule-based testbench generation framework to verify the equivalence between given Verilog implementation and its golden reference as accurately as possible. For each golden reference, the framework first performs circuit-structure analysis to extract information such as input/output (I/O) ports and reset/clock signals. It then enumerates all reset and clock-synchronization scenarios to improve verification accuracy. Experiments demonstrate that our testbench achieves 35.5 % fewer false negatives than the LLM-generated counterpart and detects 62.5 % more injected errors in fuzzing tests for sequential circuits. Detailed experimental results are presented in Section 3.3.4. (2) Round-trip data synthesis for high-quality NLcode pairs. Leveraging our testbench generation framework, we propose the round-trip data synthesis approach that can automatically synthesize high-quality NLcode pairs from code snippets. Specifically, candidate code snippets are first paired with LLM-generated NL descriptions, and then verified by regenerating the code from the NL and comparing against the original for equivalence with our testbench. Only code that passes the testbench is retained and combined with the NL to form high-quality data. We theoretically prove that, given strong LLMs and an ideal verification environment, this procedure yields NLcode pairs of sufficiently high quality for RLVR with high probability. (3) Two-stage training with adaptive DAPO for cost-effective RLVR. We adopt two-stage distill-then-RL training pipeline to cold start LLMs reasoning ability through SFT and reduce the overall training cost. Specifically, we use DeepSeek-R1 as the NL-to-code LLM in our round-trip data synthesis to produce (NL, Thought, Code) triplets without equivalence checking, based on which we perform SFT on our base LLM to obtain distilled LLM with basic reasoning ability. Then, we apply RLVR on the distilled LLM using the equivalence-checked high-quality data to further enhance its Verilog generation capability. Additionally, recognizing that RLVRs bottleneck lies in sampling and verification [20], we extend dynamic sampling policy optimization (DAPO) [42] with an adaptive mechanism that dynamically adjusts the number of samples per training step based on past sample discard rates. This approach significantly reduces unnecessary sampling and verification overhead, thereby achieving 1.25x acceleration. Based on these techniques, we develop CodeV-R1-7B, the first specialized reasoning LLM for Verilog generation with only around 2,656 A100-GPU-hours. On the VerilogEval v2 [27] and RTLLM v1.1 / v2 [19] benchmarks, CodeV-R1-7B achieves 68.8% pass@1 and 72.9% / 68.0% pass@1, respectively. Remarkably, it surpasses the 671B DeepSeek-R1 by 8.1% on RTLLM v1.1 and 3.3% on RTLLM v2, demonstrating its strong RTL generation capabilities. We will release the model, training pipeline, and dataset to foster further research in this domain. Figure 1: The overview of CodeV-R1. The core components of our framework include an automated testbench (Section 2.1), supervised fine-tuning process (Section 2.2), and reinforcement learning process (Section 2.3)."
        },
        {
            "title": "2 Methods",
            "content": "i, i, i) as well as an Verilog code snippet (denoted Our framework comprises one automated testbench generation framework and 5 stages (Figure 1). Stages 1 3 constitute the distillation phase, and stages 4 and 5 comprise the reinforcement learning phase: 1 Code-to-NL. Following prior work [45, 46], we collect Verilog code snippets from GitHub (denoted y) and use an LLM (DeepSeek-V3 [4]) to produce corresponding naturallanguage summaries (denoted x), creating an NLcode corpus {(xi, )} with approximately 150K data samples. 2 NL-to-Code. Using DeepSeek-R1, we take each NL description xi from stage 1 and generate the thought (denoted i), producing NLthoughtcode triples {(xi, i)}. 3 Difficulty Filtering and Supervised Fine-Tuning. We first filter the {(xi, i)} dataset by removing any examples for which base LLMs (e.g., Qwen2.5Coder-7B-Instruct / Qwen2.5-Coder-32B-Instruct [9]) can generate correct code in any of 5 attempts (correctness is verified using our automatically generated testbench). We then perform SFT on the base LLM to bootstrap their reasoning ability, yielding the distilled model, CodeV-R1-7B-Distill. This stage uses approximately 87K examples. 4 Equivalence Checking. We use our automated testbench to verify equivalence between the original snippets and the newly generated snippets y. Any non-equivalent pairs {(xi, )} are discarded, while equivalent pairs are retained as high-quality data for subsequent RL training. After this filtering, approximately 3.1K examples remain. 5 Difficulty Filtering and Reinforcement Learning. We again filter the retained {(xi, )} set by removing any examples where the distilled model CodeV-R1-7B-Distill generates correct code in any of 5 attempts (as checked by the testbench). We then apply our adaptive DAPO algorithm, novel RLVR algorithm, to further improve Verilog-generation performance, resulting in the final model, CodeV-R1-7B. Next, we will describe in detail the automated testbench generation framework as well as the two training phases, distillation and RL. 2.1 Automated Testbench Generation Framework for Verilog Code To facilitate the rule-based reward mechanism for the RL process, we have developed specialized framework. This framework verifies the functionality of the generated Verilog code by conducting edge-triggered simulation, comparing it against the reference code. The verification framework unfolds in three consecutive phases: Phase 1: Circuit-Structure Analysis. Before performing functional verification, we extract the input/output (I/O) ports along with their respective bit-widths from the reference golden code using Yosys [37]. For sequential circuits, we identify clock signals, noting their edge polarity (rising or falling), and characterize reset signals through control flow analysis. Reset signals are categorized based on synchrony (synchronous if they depend on the clock) and polarity (active-high or active-low). Phase 2: Simulation. We simulate by providing random inputs to both the generated and the reference codes, and evaluating the equivalence of outputs. For combinational circuits, we employ = 100 independent simulation sequences for equivalence evaluation, each comprising = 1000 inputs. Regarding sequential circuits, we adopt dual-stage validation approach when dealing with circuits that have either one reset signal or no reset signal at all: Firstly, we execute simulations using = 100 sequences, each with = 1000 clock toggles (500 cycles) with randomized inputs. In this stage, deterministic reset signalsderived from golden reset behavior extracted via Yosys and 3 representing expected, consistent reset logicare applied at the start of each sequence, primarily aimed at testing the circuits core functionality. Secondly, we conduct simulations with an identical number of sequences and clock cycles with random reset signals, which validates the consistency of the reset signal operation. For circuit designs featuring multiple reset signals, we exhaustively test every non-conflicting combination, all maintaining the aforementioned 2 cycle count. Phase 3: Verification. After each clock toggle, we assess the equivalence of the outputs between the generated Verilog code and the reference implementation. This process results in total of 2M assessments for typical sequential circuits and assessments for combinational circuits. The verification outcome is quantified by an error rate metric ϵ = Error Number 2M 100%. value of ϵ = 0% indicates that the generated code functions correctly within our testbench environment. Through 32-way parallelization, the simulation achieves throughput of 15 instances per second. 2.2 CodeV-R1-7B-Distill: Supervised Distillation for Verilog Data and Verilog code snippet Our pipeline for distillation begins with set of Verilog code (denoted as ) collected from GitHub. We use DeepSeek-V3 to summarize these code snippets, producing instructions xi corresponding to (stage 1 ). Then, to produce the corpus for distillation, we ask DeepSeek-R1 to generate responses containing \"thought\" (stage 2 ). These two stages yield approximately 150K NL-thought-code triples (xi, Next, we curate challenging subset through two filters: (1) retaining only instructions where baseline models (Qwen2.5-Coder-7B-Instruct / Qwen2.5-Coder-32B-Instruct) fail to generate the code passing the functional verification (Section 2.1) to , and (2) ensuring synthesizability of with Yosys [37]. In addition, to prevent benchmark contamination, we remove samples where the generated code exhibits Rouge-L similarity > 0.5 [13] to VerilogEval v1 [14] / v2 [27] or RTLLM v1.1 [19] / v2 [17], yielding 87K high-quality samples (stage 3 ). i, i). Finally, we initialize CodeV-R1-7B-Distill from Qwen2.5-Coder-7B-Instruct and fine-tune it to generate complete responses (c i. Following DeepSeek-R1s methodology [3], we maximize the likelihood of the generated responses using our prompt template (see Appendix), with implementation specifics detailed in Section 3.1 (stage 3 ). i) given i, 2.3 CodeV-R1-7B: Reinforcement Learning on the Distilled Model To improve the models reasoning ability further, we perform reinforcement learning fine-tuning based on CodeV-R1-7B-Distill with carefully selected high-quality Verilog data (stage 4 and stage 5 ). Below we will introduce our data curation method (Section 2.3.1), RL training algorithm, and reward design for RL (Section 2.3.2). 2.3.1 High-quality Data Curation Experiences from prior research suggest that conducting RL training on problems the model can solve but requires reasoning to address can more effectively enhance the models RL capabilities [32]. Furthermore, given potential inconsistencies between the golden code {y } in the original dataset collected from GitHub and the instructions {x i} generated by DeepSeek-V3, we prioritize ensuring the validity of selected problems. To summarize, our RL (question, answer) pairs must meet three key criteria: being solvable, challenging, and error-free. To implement this framework, we identify problems where DeepSeek-R1 successfully generates code matching the golden one in the original dataset, while both Qwen2.5-Coder-7B-Instruct and Qwen2.5Coder-32B-Instruct fail to produce equivalent solutions. Specifically, we conduct equivalence checking between the {y } in the original dataset, retaining only validated {(x i} code generated by DeepSeek-R1 in the 87K dataset and {y )} pairs for RL training. i, For difficulty enhancement, we employ CodeV-R1-7B-Distill to generate five code variants per question, excluding cases where all generated codes match the golden one, as these reflect patterns already mastered during supervised fine-tuning (stage 4 ). Through this rigorous selection process, we curate final dataset of 3.1K high-quality examples for reinforcement learning. Additionally, we formalize the equivalence between code and natural language, and theoretically prove the effectiveness of our data curation. Intuitively, the process of converting between Code-to-NL 4 and NL-to-Code using LLMs inevitably leads to some information loss. Therefore, if the two codes remain equivalent after back-and-forth conversion, the probability of error during the conversion process is minimal. Detailed definition and proof is shown below. Definition 2.1 (NL-Code Deterministic Equivalence (NLCDE)). Let denote the space of all code snippets, the space of natural-language (NL) descriptions, and semantic/functional equivalence relation where (f, l) iff code fully implements NL (or precisely describes ). Consider two probabilistic models, M1 : (code-to-NL) and M2 : (NL-to-code), the NLCDE states: For all F, L: 1. If M1 generates with Pr(l ) = 1, then (f, l) (deterministic NL summaries are semantically equivalent to input code). 2. If M2 generates with Pr(f l) = 1, then (f, l) (deterministic code outputs are functionally equivalent to input NL). Theorem 2.1 (Semantic Equivalence in Round-Trip Transformations). Consider the probabilistic models M1 : (code-to-NL) and M2 : (NL-to-code) from the NL-Code Deterministic Equivalence (NLCDE) definition (Definition 2.1). Let be random code snippet drawn from = M2(X) F. For some distribution, and define the transformed objects: = M1(Y ) L, any pair of objects A, B, let EAB denote the event and are semantically equivalent. If the round-trip transformation preserves equivalence with certainty under NLCDE, i.e., Pr(cid:2)EY,Y (cid:3) = 1, then both forward and backward transformations are individually equivalent with certainty: Pr(cid:2)EY,X EX,Y (cid:3) = 1. Proof Sketch. This theorem can be proved by the Data Processing Inequality. Please refer to the Appendix for the detailed proof. 2.3.2 Adaptive DAPO Algorithm We enhance the DAPO algorithm [42] with two efficiency improvements for RL fine-tuning on the distilled model (stage 5 ). The core DAPO loss operates on groups of responses per prompt: LDAP O(θ) =E (x,y)D,{yi}G i=1πθold (x) 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) (cid:16) min i=1 t=1 ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t (cid:17) , (1) s.t. 0 < {yiis_equivalent(yi, y)} < G, πθold (yi,tx,yi,<t) , ˆAi,t = Rimean({Ri}G where ri,t(θ) = πθ(yi,tx,yi,<t) , ri,t is the importance sampling ratio unstd({Ri}G der the new policy πθ compared to the old policy πθold, ˆAi,t is the group-relative advantage, yi is the length of response to calculate token-level loss, and ϵlow < ϵhigh are asymmetric clipping thresholds introduced in DAPO to encourage exploration. The constraint 0 < {yiis_equivalent(yi, y)} < ensures each training batch contains both correct and incorrect responses. Note that we do not include the overlong filtering proposed by DAPO here. i=1) i=1) key feature of DAPO is the dynamic sampling mechanism. It notably improves the training result. However, the standard DAPO sampling strategy presents inefficiencies during sample generation: DAPOs fixed generation batch size (denoted as bgen) is suboptimal. If too few partially correct samples are generated for the RL train batch size (denoted as btrain), costly re-sampling occurs; if too many are generated, excess samples are wasted. This problem intensifies as training progresses and model accuracy reduces the yield of partially correct examples. We address this with an adaptive batch size mechanism utilizing dynamically estimated sampling effective ratio, rvalid. Initially, bgen is set to btrain. After successfully accumulating full training batch (btrain), we calculate the batch effective ratio ( number of valid samples ). The value of rvalid is then updated to the minimum of itself and the batch effective ratio. For the subsequent sampling phase, the generation batch size is adaptively set to bgen = btrain . The detailed process is given in Appendix. Note this acceleration rvalid does not involve offline updates or alter the composition of the RL training batch, which preserves DAPOs accuracy while accelerating training. bgen 5 We implement rule-based reward function that evaluates both structural correctness and semantic equivalence. response yi receives reward of 1 if it satisfies two conditions: (1) Proper formatting as <think>reasoning</think><answer>solution</answer> (2) Semantic equivalence with the golden code judged by the equivalence checker introduced in Section 2.1. The reward function R(y, y) is 1 if has correct format and (y, y) are functional equivalent, and 0 otherwise."
        },
        {
            "title": "3 Experiments",
            "content": "This section details the implementation of our method and presents comprehensive experimental results. We systematically evaluate our model through multiple dimensions: comparisons with prior state-of-the-art approaches, test-time scaling analysis across varying response length constraints, ablation studies analyzing the impact of golden code correctness and problem complexity, acceleration effects of the adaptive DAPO mechanism, and testbench performance evaluation. These analyses collectively demonstrate the effectiveness and efficiency of our proposed approach. 3.1 Implementation details We obtain our final model by first distilling DeepSeek-R1 and then applying RL on our curated 3.1K dataset. During distillation, we employ LLaMAFactory [47] to apply supervised fine-tuning (SFT) to Qwen2.5-Coder-7B-Instruct using the 87K dataset filtered for distillation. We train the model for 6 epochs with learning rate of 1 105 and batch size of 64. The total context length is set to 16384 during distillation. During RL, we use the verl [31] framework to further train the distilled model with our adaptive DAPO. We use batch size of 128, learning rate of 1 106, and train for 300 steps. The rollout temperature is set to 1.0. During this stage, the max length is set to 2048 for instruction and 16384 for response. The SFT stage is executed on 8 A100-80G GPUs, taking approximately 78 hours, while the RL stage runs on 16 A100-80G GPUs, requiring around 127 hours of computation. The whole parameter setting is provided in the Appendix. We test our distillation and RL model on various Verilog benchmarks, including VerilogEval v1 [14] / v2 [27] and RTLLM v1.1 [19] / v2 [17]. For VerilogEval v2, we examine zero-shot scenarios in both specification-to-RTL translation and code completion tasks. The maximum context length is configured to 16384 tokens during the evaluation phase for all benchmarks. The temperature during generation is 0.6 for the distillation model and 1.0 for the RL model, and 20 responses are generated per query to estimate the pass@k score for both VerilogEval and RTLLM. 3.2 Main Results Table 1: Comparison of CodeV-R1-7B against baselines on VerilogEval v1 and RTLLM v1.1. Type Model Open source VerilogEval-Machine (%) pass@5 pass@ pass@1 VerilogEval-Human (%) pass@5 pass@10 pass@1 Foundation models GPT-4o* DeepSeek-R1-671B* DeepSeek-V3-671B* QWQ-32B* DeepSeek-R1-Distill-Qwen-32B* DeepSeek-R1-Distill-Qwen-7B* Qwen2.5-Coder-32B-Instruct* Qwen2.5-Coder-7B-Instruct* RTLCoder-Mistral-7B RTLCoder-DS-6.7B BetterV-CL-7B BetterV-DS-6.7B BetterV-CQ-7B CodeV-CL-7B CodeV-DS-6.7B CodeV-CQ-7B CraftRTL-CL-7B CraftRTL-DS-6.7B CraftRTL-SC2-15B CodeV-R1-7B-Distill CodeV-R1-7B IT baselines Ours 67.7 81.0 80.8 71.1 64.7 5.3 66.6 60.2 62.5 61.2 64.2 67.8 68.1 78.1 77.9 77.6 78.1 77.8 81.9 76.2 76.5 75.5 87.4 87.5 84.0 80.5 16.9 76.6 77.8 72.2 76.5 75.4 79.1 79.4 86.0 88.6 88.2 85.5 85.5 86. 85.6 84.1 77.2 89.5 88.8 87.0 83.6 24.9 79.7 82.4 76.6 81.8 79.1 84.0 84.5 88.5 90.7 90.7 87.8 88.1 88.1 87.0 85.7 60.1 81.5 68.7 63.8 51.3 1.6 47.6 31.9 36.7 41.6 40.9 45.9 46.1 45.2 52.7 53.2 63.1 65.4 68. 65.7 69.9 71.4 87.6 79.7 78.0 68.1 6.3 58.1 46.3 45.5 50.1 50.0 53.3 53.7 59.5 62.5 65.1 67.8 70.0 72.4 76.8 79.3 74.5 88.5 82.1 81.3 72.2 10.1 61.8 50.2 49.2 53.4 53.3 57.6 58.2 63.8 67.3 68.5 69.7 72.1 74. 79.7 81.7 We evaluate the models with *, while other results are sourced from their papers. RTLLM v1.1 (%) pass@5 pass@1 41.7 64.8 60.9 50.9 42.1 0.0 47.9 32.2 - - - - - 39.4 42.4 36.6 42.6 53.1 49.0 57.4 72. 65.9 82.9 74.2 70.6 64.3 0.0 67.7 48.2 48.3 48.3 - - - 62.1 55.2 55.2 52.9 58.8 65.8 75.8 86.1 Our main experimental results are shown in Table 1 and Table 2. We evaluate DeepSeek-R1 [3], DeepSeek-V3 [4], QWQ-32B [34], DeepSeek-R1-Distill-Qwen-32B [3], DeepSeek-R1-Distill-Qwen7B [3], Qwen2.5-Coder-32B-Instruct [40], Qwen2.5-Coder-7B-Instruct [40], and GPT-4o [24] on 6 Table 2: Comparison of CodeV-R1-7B on RTLLM v2 and VerilogEval v2. VerilogEval2-SR (%) pass@5 pass@10 pass@1 VerilogEval2-CC (%) pass@5 pass@10 pass@ RTLLM v2 (%) pass@5 pass@10 pass@1 Type Model Foundation models GPT-4o DeepSeek-R1-671B DeepSeek-V3-671B QWQ-32B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-7B Qwen2.5-Coder-32B-Instruct Qwen2.5-Coder-7B-Instruct IT Baselines RTLCoder-DS-6.7B Open source 64.1 77.5 62.4 64.2 43.9 0.6 47.5 31. 31.1 73.7 84.7 71.7 77.3 63.3 2.2 60.7 49.3 47.8 76.2 87.4 75.0 80.1 69.2 3.5 64.7 54.6 52.3 57.6 79.1 68.7 64.0 53.8 2.0 46.6 30. 33.7 66.1 85.1 76.3 77.8 69.8 7.0 59.0 46.8 45.9 69.0 87.1 78.2 80.9 73.8 11.3 62.8 52.0 49.8 56.5 64.7 59.1 52.9 42.4 0.0 47.8 36. 33.6 Ours CodeV-R1-7B-Distill CodeV-R1-7B 57.2 68.0 We evaluate all models in this table. SR: Specification-to-RTL; CC: Code Completion. 65.5 69.9 75.6 78. 78.2 80.9 65.2 68.8 77.5 81.1 75.2 78.2 70.3 75.8 71.5 68.0 62.1 0.0 63.9 52. 45.3 71.9 78.2 75.2 79.7 73.3 71.2 67.0 0.0 67.8 57.6 49.2 77.1 81.7 VerilogEval and RTLLM. Meanwhile, we adopt results reported by RTLCoder [16], BetterV [43], CodeV [46], CraftRTL [15] from their papers. The results demonstrate that: Our model achieves state-of-the-art (SOTA) performance among Verilog-domain models on most benchmarks. Our model has significant advantage over previous Verilog-domain models on RTLLM v1.1, outperforming the previous SOTA model, CraftRTL-DS-6.7B, by 18.8% on the pass@1 metric. On VerilogEval v1-Human, although the performance improvement compared to the previous SOTA model, CraftRTL-SC2-15B, is not substantial, our model has smaller size (7B) compared to theirs (15B). Among 7B models, we outperform the previous best model (CraftRTL-DS-6.7B) by 4.5% on pass@1. Although our model does not perform well on VerilogEval-Machine, this benchmark is relatively easy, and even DeepSeek-R1 does not have significant advantage on it. Our model outperforms most foundation models. On VerilogEval v1 / v2 and RTLLM v1.1 / v2, our model outperforms most foundation models. We evaluate foundation models such as DeepSeekR1, DeepSeek-V3, GPT-4o, and QWQ-32 B. Although our model underperforms DeepSeek-R1 on most benchmarks (from which we distill knowledge), it performs better than other models on most benchmarks. Notably, our model outperforms DeepSeek-R1 on RTLLM-v1.1 and RTLLM-v2 after applying reinforcement learning (RL), which demonstrates the great effectiveness of the RL process. Other foundation models, including Qwen2.5-Coder-Instruct and DeepSeek-R1-DistillQwen, do not perform well on Verilog benchmarks, highlighting the low-resource nature of Verilog during pretraining and instruction tuning. Notably, distilling math and software code knowledge from large models does not enhance the Verilog capabilities of small models. Reinforcement learning significantly improves model performance. Compared with CodeV-R17B-Distill, our RL model CodeV-R1-7B shows noticeable improvement on almost all benchmarks. Especially on the RTLLM benchmark, the reinforcement learning process results in an improvement of over 10 % in the pass@1 score. This indicates RLs great potential in the context of Verilog and showcases the robustness of our testbench in providing reliable functional correctness rewards. 3.3 Additional Experiments 3.3.1 Test-Time Scaling Test-time scaling is an important ability of reasoning LLMs [23]. To verify the test-time scaling ability of our CodeV-R1-7B, we take the RTLLM v1.1 dataset as an example and evaluate the accuracy of our model and DeepSeek-R1 under varying response length budgets. Formally, we force the response length of both models to be smaller than certain thresholds (4096, 8192 and 16384 tokens), and plot the corresponding results in Figure 2a. To ensure fair comparison, we also normalized FLOPs consumption at each response length, as shown in Figure 2b. Both models accuracy improves considerably as the response length budget increases from 4096 to 16384. CodeV-R1-7Bs accuracy rises from 7.1% to 72.9%, outperforming DeepSeek-R1 (29.0% 64.1%). When evaluated in terms of FLOPs efficiency, CodeV-R1-7B demonstrated superior computational economy, delivering higher accuracy per unit of computation compared to DeepSeekR1. These results underscore CodeV-R1-7Bs exceptional test-time scaling efficiency, showcasing its ability to leverage longer contexts more effectively than DeepSeek-R1 while consuming fewer computational resources on the RTLLM v1.1 benchmark. 7 (a) (b) Figure 2: Test-time scaling on RTLLM v1.1. Figure (a) shows response length against accuracy, while Figure (b) shows FLOPs against accuracy. FLOPs are estimated according to model architecture. 3.3.2 Equivalence Checking and Difficulty Filtering Improves RL training To explore whether equivalence checking and difficulty filtering improve RL dataset quality, we conduct an ablation study by constructing two additional datasets. Our original RL dataset contains 3.1K problems where DeepSeek-R1 responses pass the equivalence checking, while both Qwen2.5-Coder-7B-Instruct and Qwen2.5-Coder-32B-Instruct fail across five sampling attempts. To conduct difficulty ablation, we introduce dataset without difficulty filtering containing 16K problems, where we additionally include samples where Qwen2.5 models succeed in some attempts under our testbench. To conduct reference code correctness ablation, we introduce dataset without round-trip equivalence checking containing 14K samples, where we treat DeepSeek-R1 outputs as pseudo-golden code. We select cases where Qwen2.5 models fail to match this pseudo-golden code in five attempts to control difficulty. To avoid time waste, we filter the problems where CodeV-R1-7B-Distill has 100% pass rate under our testbench in five attempts. (a) (b) Figure 3: Train-time scale up on some key metrics. Figure (a) tracks response length, whereas Figure (b) presents the corresponding trend for reward. We perform reinforcement learning using CodeV-R1-7B-Distill on the three aforementioned datasets, employing identical training parameters. Key metrics observed during these training processes are presented in Figure 3. Inspection of Figure 3a reveals distinct trends in response length during training. Utilizing the original RL dataset leads to noticeable subsequent increase in response length, whereas the training dataset without difficulty filtering leads to segment of response decrease. This suggests that even when initial responses are relatively long, incorporating more challenging samples during reinforcement learning facilitates further steady growth in response length. Figure 3b illustrates that the pseudo-golden dataset consistently exhibits notably lower reward throughout the training process compared to our original RL dataset. This underscores the critical role of golden code accuracy during reinforcement learning. 3.3.3 Acceleration via Adaptive DAPO To quantitatively demonstrate the acceleration achieved by our adaptive DAPO algorithm, we provide comparison of time usage in Figure 4a. The plots reveal notable increase in the time per RL step in baseline DAPO training around step 150. This performance degradation in the baseline is attributed to its fixed generation batch size, which becomes insufficient to yield enough samples for 8 complete training batch as training progresses. In contrast, our adaptive DAPO effectively mitigates this issue. It dynamically adjusts and increases the generation batch size across steps. In addition, when generation attempt does not produce sufficient valid samples for training batch, the algorithm recalculates the required remaining batch size. In Figure 4b, we provide the average speedup of adaptive DAPO, along with breakdown of performance before and after step 150. Notably, the time reduction after step 150 is significantly more pronouncedthe speedup factor reaches 1.44 after step 150, compared to 1.04 before step 150. This disparity highlights the critical benefit of eliminating sampling more than once. After applying adaptive DAPO, the final speedup factor reaches 1.25. (a) (b) Figure 4: Time comparison between adaptive DAPO and baseline DAPO. (a): Comparison of RL training time per step. (b): Acceleration ratio between adaptive DAPO and baseline DAPO, breakdown by step (whether before 150). 3.3.4 Testbench Performance Evaluation We evaluate our auto-testbench generation framework against DeepSeek-V3-generated testbench, both taking the RTL code from GitHub as the golden reference. We conduct two key tests: Correctness classification test. First, we assess whether the testbenches might misclassify correct code as incorrect. To do this, we use both testbenches to validate \"golden vs. golden\" inputs (i.e., comparing the golden code against itself). The expected outcome is 100 % correct classification. The results show that our equivalence checker incorrectly flags 2.7% of golden code as wrong, while the LLM(DeepSeek-V3)-generated testbench misclassifies 7.6% of cases. Our approachs error rate is approximately 35.5% of DeepSeek-V3 rate, demonstrating significantly fewer false negatives. Fuzzing test for sequential circuits. Second, we perform fuzzing test on sequential circuits by instructing DeepSeek-V3 to inject subtle errors into the golden code. The goal is to measure how effectively each testbench detects these mistakes. Our testbench identifies 65 % of injected errors, whereas the DeepSeek-V3-generated testbench only detects 40 %. This highlights 62.5 % relative improvement in error detection for our approach compared to DeepSeek-V3. This indicates that our method also has fewer false positives."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Large Language Models for Reasoning The OpenAI-o1 [25] series is the first closed-source model to be trained with large-scale reinforcement learning to perform reasoning through CoT. Inspired by its powerful and effective reinforcement learning training paradigm, QwQ [34], DeepSeek-R1 [3], and Kimi k1.5 [32] have all adopted and improved upon its approach, achieving promising results. Limited by computational resources, open-source communities have actively explored low-cost approaches to replicate o1-like reasoning models. Some efforts have focused on distilling the powerful closed-source reasoning models [8, 22, 23, 33, 41]. while some open-source efforts have also explored training reasoning models using reinforcement learning [7, 10, 18, 20, 21, 26, 38, 44]. The main difference between CodeV-R1-7B and the aforementioned reasoning models lies in its focus on hardware description language code generation, which poses unique challenges due to verification 9 difficulty and limited data quality. In contrast, prior works primarily specialised in domains such as mathematics, which benefit from easily verifiable numerical outputs and rich open-source datasets. 4.2 Large Language Models for Verilog Code Generation With the development of large language models for code generation, specialised code generation models for hardware description languages also receive widespread attention. Many prior works [2, 15, 16, 43, 46] focus on Verilog instruction-tuning data creation without strict correctness evaluation. Most works have syntax check in constructing instruction-response pairs: RTLCoder [16] and CodeV [46] add syntax checks when constructing supervised fine-tuning (SFT) datasets with closed-source LLMs. BetterV [43] maps code across languages using Verilog syntax constraints, while OriGen [2] leverages compiler feedback to eliminate syntax errors. For functional correctness, to date only CraftRTLs correct-by-construction approach [15] ensures functional correspondence between instruction and response through formal verification. However, its applicability remains restricted to Karnaugh maps and finite-state machines, narrow subset of Verilog design challenges. Reinforcement learning with rule-based rewards needs reliable testbench to evaluate the functional correctness of Verilog code needed when calculating rewards. Current testbench generation paradigms suffer from two systemic flaws: (1) Unverified validation frameworks: For example, VeriPrefer [35] optimizes testbench coverage, but its testbenches themselves may be flawed, sometimes failing to pass the reference code they were designed to verify. ReasoningV [28] co-generates code and testbenches via DeepSeek-R1, inheriting the models hallucination risks. (2) Cost-prohibitive iteration: AutoBench [29] and CorrectBench [30] employ multi-stage LLM workflows, where each self-correction cycle incurs escalating computational costs and latency, directly conflicting with RLs demand for rapid, low-cost reward feedback. Unlike prior work, we apply Verilog functional verification with auto-generated equivalence checking (see Section 2.1), providing robust foundation for both data curation and reinforcement learning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose CodeV-R1, unified RLVR framework designed for training RTL generation LLMs. This framework first distills data with reasoning patterns and then applies reinforcement learning on high-quality data curated by an automated testbench generation framework. The model trained via this framework, CodeV-R1-7B, achieves outstanding performance on RTL generation benchmarks like VerilogEval v2 and RTLLM v2, matching or even surpassing DeepSeek-R1, which demonstrates the effectiveness of the automated testbench generation and the two-stage training paradigm. series of analytical experiments further highlight the powerful impact of CodeV-R1 in enhancing data quality and further unlocking the RTL code generation capabilities of LLMs through reasoning."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "The limitations of this work mainly lie in two aspects, which are also the future direction of our work: (1) The automated testbench generation framework can only improve the semantic consistency between code and NL in the probabilistic sense. The synthetic dataset generated by our method both for SFT and RL may still contain small amount of low-quality data, which could potentially impact the models performance. (2) Collecting data with reasoning processes for SFT requires general reasoning model (e.g., DeepSeek-R1), which inherently depends on the teacher models reasoning capabilities. This dependency poses greater challenges in specialized domains where the teacher models performance is suboptimal, as its limitations in such contexts may directly impact the quality of the collected data. Besides, this process might be financially costly. Additionally, from an application perspective, it is promising to focus on exploring the potential of reasoning LLMs to tackle more complex hardware development tasks beyond RTL code generation in the future, such as PPA performance optimization and analog circuit synthesis."
        },
        {
            "title": "References",
            "content": "[1] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [2] Fan Cui, Chenyang Yin, Kexing Zhou, Youwei Xiao, Guangyu Sun, Qiang Xu, Qipeng Guo, Yun Liang, Xingcheng Zhang, Demin Song, et al. Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, pages 19, 2024. [3] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [4] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412. 19437. [5] Mingzhe Gao, Jieru Zhao, Zhe Lin, Wenchao Ding, Xiaofeng Hou, Yu Feng, Chao Li, and Minyi Guo. Autovcoder: systematic framework for automated verilog code generation using llms. In 2024 IEEE 42nd International Conference on Computer Design (ICCD), pages 162169. IEEE, 2024. [6] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [7] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290. [8] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024. URL https: //arxiv.org/abs/2411.16489. [9] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https: //arxiv.org/abs/2409.12186. [10] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [11] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. [12] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. doi: 10.1126/science. abq1158. URL https://www.science.org/doi/abs/10.1126/science.abq1158. [13] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Annual Meeting of the Association for Computational Linguistics, 2004. URL https://api.semanticscholar. org/CorpusID:964287. [14] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages 18. IEEE, 2023. 11 [15] Mingjie Liu, Yun-Da Tsai, Wenfei Zhou, and Haoxing Ren. Craftrtl: High-quality synthetic data generation for verilog code models with correct-by-construction non-textual representations and targeted code repair. arXiv preprint arXiv:2409.12993, 2024. [16] Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. Rtlcoder: Fully open-source and efficient llm-assisted rtl code generation technique. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [17] Shang Liu, Yao Lu, Wenji Fang, Mengming Li, and Zhiyao Xie. Openllm-rtl: Open dataset and benchmark for llm-aided design rtl generation. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, pages 19, 2024. [18] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, and Liwen Zhang. Fin-r1: large language model for financial reasoning through reinforcement learning, 2025. URL https://arxiv.org/abs/2503.16252. [19] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722727. IEEE, 2024. [20] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75. notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level, 2025. Notion Blog. [21] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/ 2025. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL, Notion Blog. [22] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. URL https://arxiv.org/abs/2412.09413. [23] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [24] OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [25] OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [26] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [27] Nathaniel Pinckney, Christopher Batten, Mingjie Liu, Haoxing Ren, and Brucek Khailany. Revisiting verilogeval: Newer llms, in-context learning, and specification-to-rtl tasks, 2024. URL https://arxiv.org/abs/2408.11053. [28] Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, and Wang Kang. Reasoningv: Efficient verilog code generation with adaptive hybrid reasoning model, 2025. URL https://arxiv.org/abs/2504.14560. [29] Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, and Bing Li. Autobench: Automatic testbench generation and evaluation using llms for hdl design. In Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD, pages 110, 2024. [30] Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, and Bing Li. Correctbench: Automatic testbench generation with functional self-correction using llms for hdl design, 2024. URL https://arxiv.org/abs/2411.08510. 12 [31] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [32] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [33] NovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novaskyai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. [34] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [35] Ning Wang, Bingkun Yao, Jie Zhou, Yuchen Hu, Xi Wang, Nan Guan, and Zhe Jiang. Insights from verification: Training verilog generation llm with reinforcement learning with testbench feedback, 2025. URL https://arxiv.org/abs/2504.15804. [36] Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, and Nan Guan. Large language model for verilog generation with code-structure-guided reinforcement learning, 2025. URL https://arxiv.org/abs/2407.18271. [37] Clifford Wolf, Johann Glaser, and Johannes Kepler. Yosys-a free verilog synthesis suite. In clifford.fm, 2013. URL https://api.semanticscholar.org/CorpusID:202611483. [38] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768. [39] Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. [40] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [41] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [42] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [43] PEI Zehua, Huiling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. Betterv: Controlled verilog generation with discriminative guidance. In Forty-first International Conference on Machine Learning, 2024. [44] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. [45] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan (Celine) Lin. MG-Verilog: multi-grained dataset towards enhanced llm-assisted verilog generation. In The First IEEE International Workshop on LLM-Aided Design (LAD24), 2024. [46] Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Xing Hu, and Yunji Chen. Codev: Empowering llms for verilog generation through multi-level summarization, 2024. URL https://arxiv.org/abs/2407.10424. 13 [47] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400410, 2024."
        },
        {
            "title": "A Method Details",
            "content": "A.1 Proof of Theorem 2.1 Proof. Observe that the sequence forms Markov chain. By the Data Processing Inequality (DPI), I(Y ; ) I(Y ; X). Under the assumption that EY,Y holds almost surely, we have H(Y ) = 0, and thus I(Y ; ) = H(Y ) H(Y ) = H(Y )."
        },
        {
            "title": "It follows that",
            "content": "H(Y ) = I(Y ; ) I(Y ; X) H(Y ) = I(Y ; X) = H(Y ) = H(Y X) = 0, meaning is determined by almost surely and hence EY,X holds. Next, since H(Y X) = 0 implies H(X) = H(Y ) and I(X; ) H(X), failure of EX,Y under the NLCDE assumption would force I(X; ) < H(X) = H(Y ), contradicting I(Y ; ) = H(Y ). Therefore, EX,Y must also hold almost surely. Combining these two results gives EY,X EX,Y holds almost surely. The need for NLCDE in Theorem 2.1 arises because EX,Y H(Y X) = 0, but H(Y X) = 0 EX,Y . counterexample is when and are incorrectly matched with probability one. To be more specific (though not fully rigorous, just to aid understanding), if the NL-to-code model wrongly transforms (e.g., design multiplier) in the NL domain to (e.g., design an adder) in the code domain, and transforms in the NL domain to in the code domain, while the code-to-NL model maps in the code domain to in the NL domain and in the code domain to in the NL domain, then H(Y ) = 0 can hold without EY,Y . Thus, the NLCDE assumption is necessary to resolve this. A.2 Algorithm Description of Adaptive DAPO In this section, we provide the algorithm description of adaptive DAPO in Algorithm 1. In this algorithm, one epoch means going through the whole training dataset, while one step is to collect enough samples and update the model parameters like standard DAPO [42]. Note that we achieve the dynamic batch size by two granularities: First, we use step-level ratio rvalid to control the generation batch size bgen. Second, if one generation does not provide enough samples for training, we use another inner-step-level ratio rstep to control the generation batch size for the remaining samples. 15 Algorithm 1 Adaptive DAPO Require: Training batch size btrain, dataset Ensure: Updated rvalid and filtered problem pool Initialize rvalid 1 for epoch = 1, 2, . . . do Shuffle Ntotal D, Nconsumed 0 while Nconsumed < Ntotal do Σbgen 0, nvalid 0, rstep rvalid while nvalid < btrain do bremain btrain nvalid bge bremain/rstep D[Nconsumed : min(Nconsumed + bge, Ntotal)] Generate bge samples from Update counters: nvalid nvalid + vnew, Σbgen Σbgen + bge rstep min (cid:17) (cid:16) rstep, nvalid Σbgen end while Update ratio: rvalid min Train DAPO with btrain valid samples rvalid, nvalid Σbgen (cid:16) (cid:17) (Epoch reset) (Process epoch) (Dynamic batch) (RL step) end while end for"
        },
        {
            "title": "B Parameter Setting",
            "content": "The full parameter setting during the SFT (distillation) stage is shown in Table 3, while the full parameter setting during the RL stage is shown in Table 4 (note we do not include adaptive DAPO [42], the algorithm we use, in the table). During testing, we use max context length of 16384 and temperature of 1.0. We set top_p to 1.0 for VerilogEval and 0.95 for RTLLM. For RL, the generation batch size in Table 4 corresponds to train_batch_size in verl [31], and the training batch size corresponds to ppo_mini_batch_size in verl. generation batch size of 128 and training batch size of 64 (with rollout number of 16) means first generating 128 16 samples for 128 problems and updating two times, each with 64 16 samples, during one RL step. Meanwhile, the clip ratio(high), clip ratio(low), overlong penalty factor, and overlong response length in Table 4 are introduced by DAPO. Here, the max train response length in Table 4 corresponds to Lmax in DAPO, and the overlong response length corresponds to Lcache. The overlong penalty in DAPO Plength(y) (where is response length) is defined as: Plength(y) = 0, y(LmaxLcache) Lcache 1, Lmax Lcache , Lmax Lcache < Lmax (2) Lmax < y, which is added to the {0, 1} reward. Table 3: SFT Parameter Setting. Parameter Category Training Mode Optimization & Scheduling Context & Data Handling Parameter Name Finetuning Type Epochs LR Scheduler Optimizer Max Context Length 16 Parameter Name Value Full Parameter Deepspeed 6 Cosine Decay AdamW 16384 Learning Rate (LR) LR Warmup Ratio Numerical Precision Packing Value Zero3 1 105 0.03 BF16 True Table 4: RL Parameter Setting. Parameter Category Batch Size Related Rollout Configuration Optimization & Regularization Clipping & Penalty Length Control Computation & Memory Optimization Parameter Name Generation Batch Size Dynamic Batch Size Rollout Number Rollout Engine Learning Rate KL Coefficient Clip Ratio (High) Overlong Penalty Factor Max Train Response Length (Full) Max Generate Response Length Gradient Clip Use Liger Kernel Tensor Parallel Size Distributed Training Configuration Number of Nodes Remove Padding Data Processing FSDP Optimizer Offload FSDP Related"
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Detailed Benchmark Statistics Rollout Temperature Rollout GPU Memory Utilization Parameter Name Training Batch Size KL Loss Coefficient Clip Ratio (Low) Value 128 True 16 VLLM 1 106 Weight Decay 0.0 0.28 1.0 16384 32768 0.5 True 4 2 True False Gradient Checkpointing VLLM Enforce Eager GPUs per Node Token Level Loss FSDP Parameter Offload Overlong Response Length Value 64 1.0 0.8 0.0 0.0 0.2 1024 True False 8 True False In this section, we take close look at the mistake type on VerilogEval v2 and the pass@k metrics of different task types on RTLLM v2. Table 5: Comparison of Error Types for VerilogEval v2. Model Compiler Errors Runtime Errors CodeV-R1-7B-Distill CodeV-R1-7B DeepSeek-R1-671B 69 63 37 107 38 59 108 46 47 27 22 0 20 3 0 1 0 1 0 1 0 0 0 0 Total 332 173 144 1699 1610 1096 107 114 110 19 1 5 Total 1825 1725 1211 Error type explanation: General Compiler Error; Syntax Error; Reg Declared as Wire; Module Missing; Unable to Bind Wire/Reg; Explicit Cast Required; Sensitivity Problem; Unable to Bind Wire/Reg clk; General Runtime Error; Timeout. Reset Issue; As shown in Table 5, our RL training notably reduces error rates, particularly for compiler errors. CodeV-R1-7B achieves 48% reduction in total compiler errors compared to CodeV-R1-7B-Distill (from 332 to 173), with the most pronounced improvements in syntax errors (S, reduced by 65% from 107 to 38) and wire declaration issues (w, down 57% from 108 to 46). Notably, our CodeV-R1-7B has remarkably fewer syntax error (38) compared to DeepSeek-R1 (59) and fewer reset issues (r) (1 vs 5). Even so, our CodeV-R1-7B still has limitations. For instance, the number of general runtime errors (R) is still notably higher than DeepSeek-R1. This might stem from the RL training data not being suitable for VerilogEval (unlike the great improvement on RTLLM). Table 6: Performance Across Different Module Categories on RTLLM v2. Model Arithmetic (%) Control (%) Memory (%) Miscellaneous (%) pass@ pass@5 pass@1 pass@5 pass@1 pass@5 pass@ pass@5 CodeV-R1-7B-Distill CodeV-R1-7B DeepSeek-R1-671B 69.47 83.68 76.58 89.19 91.66 90.65 74.17 80.00 83.33 83.06 83.33 83. 43.57 51.43 57.14 56.69 63.30 60.71 46.94 57.78 52.50 59.72 72.40 67.72 Table 6 demonstrates the comparative performance across module categories, where CodeV-R1-7B shows consistent improvements over CodeV-R1-7B-Distill while maintaining competitive results against the larger DeepSeek-R1. Notably, CodeV-R1-7B achieves superior pass@1 rates in all categories over CodeV-R1-7B-Distill, with particularly strong gains in arithmetic modules (83.68% vs 69.47%) and miscellaneous modules (57.78% vs 46.94%). It also surpasses DeepSeek-R1 in these two categories. Compared with the training dataset classification provided in Figure 5, these two categories occupy larger portion (arithmetic and others). This observation suggests that augmenting the training set with high-quality RL data for currently underperforming categories (particularly Memory and Control modules) could be productive direction for future model improvement. C.2 Dataset Statistics (a) (b) Figure 5: Problem category distribution. Left: SFT dataset; Right: RL dataset. Figure 5 presents the category distribution of our 87K SFT and 3.1K RL training datasets (categorized using both questions and answers). While both datasets show comparable distributions, the RL dataset has fewer unclassified problems. Figure 6: Prompt length distribution. Left: SFT dataset; Right: RL dataset. Figure 6 illustrates the prompt length distribution (in tokens) for our 87K SFT and 3.1K RL training datasets, both clipped to maximum prompt length of 1500 tokens. The figure reveals sharper distribution for the RL data, indicating shorter and lower-variance prompt lengths compared to the SFT data. To quantify this observation, we calculated the following statistics: The average length of SFT data is 377.81 with standard deviation of 161.30, while the average length of RL data is 336.67 with standard deviation of 153.88. These statistics align with the visual trends in the figure. Figure 7 depicts the response length distributions (in tokens) for CodeV-R1-7B-Distill and CodeV-R17B. Note that the maximum context lengththe sum of prompt length and response lengthis capped at 16384 tokens. Consequently, when responses are truncated, their recorded length is 16384 tokens minus the original response length, resulting in somewhat scattered distribution (manifested as the two rightmost bars, instead of one, become longer in the distribution plot). The response length for CodeV-R1-7B exhibits an evident right shift, indicating longer responses after reinforcement learning. Additionally, CodeV-R1-7Bs response distribution is more symmetric compared to the left-skewed distribution of CodeV-R1-7B-Distill. The underlying cause of this discrepancy warrants further investigation. We observe that incorrect samples are significantly longer, with substantial proportion exceeding the length threshold. Even excluding these overlong samples, incorrect responses remain longer, characterized by higher peak value (CodeV-R1-7B) or slower post-peak decline (CodeVR1-7B-Distill). An intriguing phenomenon is that CodeV-R1-7B has lower overlong ratio on the RL dataset but higher ratio on the SFT dataset. This may arise from overfitting the overlong penalty 18 Figure 7: Response length distribution. Left: SFT dataset; Middle: Correct samples in RL dataset; Right: Incorrect samples in RL dataset. during RL, while CodeV-R1-7Bs tendency to generate longer responses increases overlong instances on the SFT dataset."
        },
        {
            "title": "D Case Study",
            "content": "Comparison Between CodeV-R1-7B and DeepSeek-R1: In this section, we first present case study on specific Verilog problem in VerilogEval-v2 to illustrate the advantage of CodeV-R1-7B over DeepSeek-R1 in this problem. The problem, reasoning chains, and results are shown in the blocks below. Both Deepseek-R1 and our model comprehended the problem and engaged in reasoning and reflection, but only our model accurately implemented the logical function defined by the Karnaugh map, while R1s answer executed an incorrect logical function. The key distinction lies in our models use of the declaration input [4:1] x, which directly corresponds to the notation of the Karnaugh map in the problem description. In contrast, R1 employed input [3:0] x, leading to confusion in subsequent reasoning. Although our model was initially challenged by this unconventional declaration, it ultimately chose the correct declaration through reflection, avoiding potential index mapping ambiguities. This resulted in significantly clearer code that is less prone to errors. Problem Description Question: Question: would like you to implement module named TopModule with the following interface. All input and output ports are one bit unless otherwise specified. - input (4 bits) - output The module should implement the function shown in the Karnaugh map below. is dont-care, which means you may choose to output whatever value is convenient. x[1]x[2] x[3]x[4] 00 01 11 10 0 0 1 0 1 1 1 1 0 00 01 11 10 Answer: 19 Reasoning Chain (DeepSeek-R1) Okay, so need to write Verilog module called TopModule that takes 4-bit input and outputs single bit based on the given Karnaugh map. Let me start by understanding the problem step by step. ... Wait, the K-maps rows are x3x4, columns x1x2. Lets see. For example, the first row is x3x4 = 00. So the row is determined by the bits x3 and x4. Similarly, columns are determined by x1 and x2. So each cell in the K-map is determined by the combination of x3, x4, x1, x2. So the four input bits are x1, x2, x3, x4. But how are these mapped to the input in Verilog? The input is 4-bit vector. Lets assume that is declared as [3:0] x, where x[3] is the first bit (MSB) and x[0] is the last. So perhaps the bits are arranged as x[3] = x1, x[2] = x2, x[1] = x3, x[0] = x4. But this is guess. Alternatively, the problem might have typo, and the rows are x[2]x[3], and columns x[0]x[1]. But given the problem statement, need to proceed with the given labels. ... Now, the goal is to cover all the 1s and 0s with the largest possible groups, using dont-cares as either 1 or 0 to maximize grouping. First, lets identify the 1s and 0s. Looking for 1s: In row 01 (x3=0, x2=1), column 11 (x1=1, x0=1) 1. In row 11 (x3=1, x2=1), columns 00 and 01 1s. In row 10 (x3=1, x2=0), columns 00, 01 1s. In row 10, column 11 0. Wait, no: row 10, column 11 (x1=1, x0=1) is 0. So thats 0. The 0s are: Row 00, column 01 0. Row 01, columns 00, 10 0. Row 10, column 11 0. The dont-cares can be used to simplify the expression. So for f, we can choose to treat the dont-cares as 1 or 0 to form the largest possible groups. ... Therefore, the Verilog code for the module would be: module TopModule ( input [3:0] x, output ); i = ( [ 3 ] & [ 1 ] ) ( [ 3 ] & [ 1 ] & [ 0 ] ) ; endmodule But let me double-check this. Another way to check is to test some values. Test case 1: x3=1, x2=1, x1=0, x0=0 should be 1. In the K-map: row x3x2=11 (row 11), column x1x0=00 1. Our expression: (1 & 0) (0 & 0 & 0) 1 0 = 1. Correct. Test case 2: x3=1, x2=0, x1=0, x0=1 row 10, column 01 1. Expression: (1 & 1) (0 & ... ) 1. Correct. ... Test case 6: x3=0, x2=0, x1=1, x0=1 row 00, column 11 (dont-care). Our expression sets this to 1 via x3 x1 x0. So f=1 here. Which is acceptable as dont-care. Thus, the Verilog code should correctly implement the K-map. Results (DeepSeek-R1) ... **Solution** ```verilog module TopModule ( [ 3 : 0 ] , p u t ) ; i = ( [ 3 ] & [ 1 ] ) ( [ 3 ] & [ 1 ] & [ 0 ] ) ; endmodule ``` ... 20 Reasoning Chain (CodeV-R1-7B) <think>Okay, lets try to tackle this Verilog problem. The goal is to create module called TopModule with 4-bit input and single-bit output f. The function is determined by the Karnaugh map provided. ... Let me try to find the logic. Looking for the minterms where f=1, considering dont cares as 1s. Wait the f=1 in the K-map. Looking across the K-map, the entries that have 1 are: In x3x4=11 row: Columns x1x2 when: x1x2=00 (f=1) and x1x2=01 (f=1). In x3x4=10 row: x1x2=00 and 01 (so x3x4=10, x1x2=00 and 01 which gives x1=0, x2=0 and x1=0, x2=1. So x3x4 can be 10 and x1x2=00 and 01. Also: Row x3x4=01 and x1x2=11: f=1. Row x3x4=01s column x1x2=11 (third column) gives 1. ... The variables are x[1], x[2], x[3], x[4]. Wait the input is as four bits. Lets suppose the four bits are x[3:0], but the code might have x[3], x[2], x[1], x[0], but the naming here may vary. Wait the K-map labels the as x[1] to x[4]. So in Verilog, if input is declared as [4:1], which is standard way. But in Verilog, arrays are usually declared with [width-1 : 0], but the actual variable name is [high:low]. For example, input [4:1] would have indices x[4], x[3], x[2], x[1], x[0]. But in the Karnaugh map, the rows are x3 and x4, columns are x1 and x2. But perhaps in the Verilog code, the variables are named x[1],x[2],x[3],x[4]. Lets assume the input is four-bit vector named x, with each bit labeled x1, x2, x3, x4, such that: input [4:1] x; // x[1], x[2], x[3], x[4] But in Verilog, you can declare it as: input [1:4] x; // but the syntax is different. Alternatively, input [4:1] x; where x[1] is the first bit, etc. But given the problem statements variables as x[1] to x[4], the code can be written as: module TopModule (input [4:1] x, output f); Then, the variables are: x[1], x[2], x[3], x[4]. ... So the final code would look like: module TopModule (input [4:1] x, output f); i = ( [ 3 ] & [ 1 ] ) ( [ 3 ] & [ 4 ] & [ 1 ] & [ 2 ] ) ; endmodule Wait wait, the variables in the second term are x3=0 (because x[3], x4=1 (x[4]), x1=1 (x[1]), x2=1 (x[2]). Yes. So thats correct. ... </think> Results (CodeV-R1-7B) <answer> ```verilog module TopModule ( u [ 4 : 1 ] , p ) ; i = ( [ 3 ] & [ 1 ] ) ( [ 3 ] & [ 4 ] & [ 1 ] & [ 2 ] ) ; endmodule ```</answer> Low-quality Data Example: Below is an example of low-quality data in our dataset. The original code is just an empty module with some comments, while the summarized problem describes module with an unsigned 32-bit divider. The problem and the code are inconsistent in this case."
        },
        {
            "title": "Original Code from GitHub",
            "content": "module _ i d ( r , c , denom , numer , t t , a ) ; u p n i t p u t [ 3 1 : 0 ] [ 3 1 : 0 ] [ 3 1 : 0 ] [ 3 1 : 0 ] r ; c ; denom ; numer ; t t ; a ; endmodule / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a / / r a lpm lpm . _ p n . l f : PRIVATE : new_diagram STRING \" 1 \" o : LIBRARY : o : CONSTANT : LPM_DREPRESENTATION STRING \"UNSIGNED\" o : CONSTANT : LPM_HINT STRING \"MAXIMIZE_SPEED=6 ,LPM_REMAINDERPOSITIVE=TRUE\" o : CONSTANT : LPM_NREPRESENTATION STRING \"UNSIGNED\" o : CONSTANT : LPM_PIPELINE NUMERIC \" 1 1 \" o : CONSTANT : LPM_TYPE STRING \" LPM_DIVIDE \" o : CONSTANT : LPM_WIDTHD NUMERIC \" 3 2 \" o : CONSTANT : LPM_WIDTHN NUMERIC \" 3 2 \" o : USED_PORT : r 0 0 0 0 INPUT NODEFVAL \" r \" o : USED_PORT : c 0 0 0 0 INPUT NODEFVAL \" c \" o : USED_PORT : denom 0 0 32 0 INPUT NODEFVAL \" denom [ 3 1 . . 0 ] \" o : USED_PORT : numer 0 0 32 0 INPUT NODEFVAL \" numer [ 3 1 . . 0 ] \" o : USED_PORT : t t 0 0 32 0 OUTPUT NODEFVAL \" t t [ 3 1 . . 0 ] \" o : USED_PORT : o : CONNECT: @aclr 0 0 0 0 r 0 0 0 0 o : CONNECT: @clock 0 0 0 0 c 0 0 0 0 o : CONNECT: @denom 0 0 32 0 denom 0 0 32 0 o : CONNECT: @numer 0 0 32 0 numer 0 0 32 0 o : CONNECT: t t 0 0 32 0 @ t t 0 0 32 0 o : CONNECT: a 0 0 32 0 @remain 0 0 32 0 o : GEN_FILE : TYPE_NORMAL _ i d . TRUE o : GEN_FILE : TYPE_NORMAL _ i d . TRUE o : GEN_FILE : TYPE_NORMAL _ i d . cmp TRUE o : GEN_FILE : TYPE_NORMAL _ i d . TRUE o : GEN_FILE : TYPE_NORMAL _ i d _ t . TRUE o : GEN_FILE : TYPE_NORMAL _ i d _ . TRUE o : GEN_FILE : TYPE_NORMAL _ i d _ . TRUE o : LIB_FILE : a 0 0 32 0 OUTPUT NODEFVAL \" a [ 3 1 . . 0 ] \" lpm Summarized Problem for The Code You need to design Verilog module that performs unsigned division on two 32-bit numbers. The module should have an asynchronous clear signal (aclr) to reset the division operation, and clock signal (clock) to synchronize the process. The module should take two 32-bit inputs: the divisor (denom) and the dividend (numer). It should produce two 32-bit outputs: the quotient (quotient) and the remainder (remain). The module should be optimized for speed with an 11-stage pipeline and should ensure that the remainder is always positive. This Verilog module, named div_unsigned, has the interface designed as follows: Signal Name Direction Width Description aclr Input 1 Asynchronous clear signal to reset the module clock Input 1 Clock signal to synchronize the division operation denom Input 32 32-bit unsigned divisor input numer Input 32 32-bit unsigned dividend input quotient Output 32 32-bit unsigned quotient output remain Output 32 32-bit unsigned remainder output, always positive"
        },
        {
            "title": "E Prompts",
            "content": "Below shows the prompt for generating instructions given the GitHub codes by DeepSeek-V3. 22 Prompt for Generating Questions Given Codes ### demonstration1 Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog module _ u ( g , p n v a _ e , p u t o n , ) ; endmodule i i r = g & r _ e ; i o = g & r _ e ; ``` [Description] This Verilog module is top-level module that controls the ringer and motor of device. It has four ports: two inputs (ring and vibrate_mode) and two outputs (ringer and motor). The ring input is signal that is used to indicate when the device should be ringing or vibrating. The vibrate_mode input is signal that indicates whether the device should be ringing or vibrating. The ringer output is signal that is used to control the ringer of the device. It is set to 1 when the device should be ringing and 0 when it should not be ringing. The motor output is signal that is used to control the motor of the device. It is set to 1 when the device should be vibrating and 0 when it should not be vibrating. The module uses two assign statements to control the ringer and motor outputs. The first assign statement sets the ringer output to the logical AND of the ring and the logical NOT of the vibrate_mode inputs. This means that the ringer output will be set to 1 when the ring input is 1 and the vibrate_mode input is 0, and it will be set to 0 otherwise. The second assign statement sets the motor output to the logical AND of the ring and the vibrate_mode inputs. This means that the motor output will be set to 1 when both the ring input and the vibrate_mode input are 1, and it will be set to 0 otherwise. This module is used to control the ringer and motor of device based on the ring and vibrate_mode inputs. When the ring input is 1 and the vibrate_mode input is 0, the ringer output will be set to 1 and the motor output will be set to 0. When the ring input is 1 and the vibrate_mode input is 1, the ringer output will be set to 0 and the motor output will be set to 1. [Problem] Suppose you are designing circuit to control cellphones ringer and vibration motor. Whenever the phone needs to ring from an incoming call (input ring), your circuit must either turn on the ringer (output ringer = 1) or the motor (output motor = 1), but not both. If the phone is in vibrate mode (input vibrate_mode = 1), turn on the motor. Otherwise, turn on the ringer. This Verilog module, named top_module, has the interface designed as follows: Signal Name Direction Width Description -- ring Input 1 Indicates when the device should be ringing or vibrating vibrate_mode Input 1 Selects vibrate mode; when high, the device vibrates instead of ringing ringer Output 1 Controls the devices ringer; high to ring, low otherwise motor Output 1 Controls the devices motor; high to vibrate, low otherwise ### demonstration2 Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog module _ u ( u k , u s , u [ 1 : 0 ] e , u [ 1 5 : 0 ] , p e [ 1 5 : 0 ] ) ; a @( e c ) i ( ! e ) <= 0 ; s g f end end endmodule ( e [ 0 ] ) ( e [ 1 ] ) [ 7 : 0 ] <= [ 7 : 0 ] ; [ 1 5 : 8 ] <= [ 1 5 : 8 ] ; ``` [Description] This Verilog module is top-level module which is used to write data to 16-bit register. It has five inputs and one output. The inputs are clock signal (clk), reset signal (resetn), 2-bit byte enable signal (byteena), and 16-bit data signal (d). The output is 16-bit register (q). The module is triggered by rising edge of the clock signal. When the reset signal is low, the register is reset to 0. Otherwise, the data signal is written to the register based on the byte enable signal. If the first bit of the byte enable signal is high, the lower 8 bits of the data signal are written to the lower 8 bits of the register. If the second bit of the byte enable signal is high, the upper 8 bits of the data signal are written to the upper 8 bits of the register. [Problem] Create 16 flip-flops. Its sometimes useful to only modify parts of group of flip-flops. The byte-enable inputs control whether each byte of the 16 registers should be written to on that cycle. byteena[1] controls the upper byte d[15:8], while byteena[0] controls the lower byte d[7:0]. resetn is synchronous, active-low reset. All DFFs should be triggered by the positive edge of clk. This Verilog module, named top_module, has the interface designed as follows: Signal Name Direction Width Description -- clk Input 1 Clock signal resetn Input 1 Active low reset signal byteena Input 2 Byte enable signal Input 16 Data input signal Output 16 16-bit register output 23 (continued) ### demonstration3 Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog module _ u ( u k , u t r [ 3 : 0 ] ) ; e , a @( e c ) ( e == 1 0 ) s endmodule <= 1 ; <= + 1 ; ``` [Description] This top Verilog module is simple counter that increments its output by one every clock cycle. It has 3 inputs, clock (clk), reset signal (reset), and an output register (q). The output register is 4-bit register, meaning it can store values from 0 to 15. The module is triggered on the rising edge of the clock signal. When the reset signal is active, the output register is set to 1. If the reset signal is not active, the output register is incremented by one. When the output register reaches 10 (1010 in binary), it is reset to 1. This process is repeated every clock cycle. [Problem] Make decade counter that counts 1 through 10, inclusive. The reset input is active high synchronous, and should reset the counter to 1. This Verilog module, named top_module, has the interface designed as follows: Signal Name Direction Width Description -- clk Input 1 Clock signal that triggers the counter on its rising edge reset Input 1 Active-high synchronous reset signal to initialize the counter Output 4 4-bit register output representing the current count (1-10) ### demonstration4 Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog module _ u ( u k , u t r [ 4 : 0 ] ) ; e , i w @( ) i [ 4 : 0 ] _ t ; _ t = [ 4 : 1 ] ; _ t [ 4 ] = [ 0 ] ; _ t [ 2 ] ^= [ 0 ] ; end w @( e c ) i ( e ) e <= 5 h1 ; <= _ t ; end endmodule ``` [Description] The top module has 3 inputs and 1 output, where the inputs are clk, reset, and output is q. The module has 2 always blocks to define the state transition of and the logic description. The state transitions are defined in the first always block, which is triggered when changes. In the first always block, q_next is assigned with q[4:1], which is the value of except the LSB bit. Then, q_next[4] is assigned with q[0], which is the LSB bit. Lastly, q_next[2] is xored with q[0]. The second always block is triggered at positive edge of clk. If reset is active, is assigned with 5h1, which is the reset value. If reset is inactive, is assigned with q_next, which is the state transition. The port connections of instantiated modules are shown above. The module takes clk, reset, and as input. is 5 bit output, which is assigned with 5h1 at reset and q_next at positive edge of clk. [Problem] linear feedback shift register is shift register usually with few XOR gates to produce the next state of the shift register. Galois LFSR is one particular arrangement where bit positions with \"tap\" are XORed with the output bit to produce its next value, while bit positions without tap shift. If the taps positions are carefully chosen, the LFSR can be made to be \"maximum-length\". maximum-length LFSR of bits cycles through 2**n-1 states before repeating (the allzero state is never reached). Build 5-bit maximal-length Galois LFSR with taps at bit positions 5 and 3. The active-high synchronous reset should reset the LFSR output to 1. This Verilog module, named top_module, has the interface designed as follows: Signal Name Direction Width Description -- clk Input 1 Clock signal that triggers state transitions on rising edges. reset Input 1 Active-high synchronous reset signal to initialize the LFSR. Output 5 Current state of the LFSR, representing 5-bit value (00001 to 11111). (continued) ### demonstration5 Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog module _ u ( u t o u t [ 9 9 : 0 ] , [ 9 8 : 0 ] _ h , [ 9 9 : 1 ] _ , [ 9 9 : 0 ] _ f n ) ; s o _ h = & [ 9 9 : 1 ] ; i u _ = s n _ f n = ^{ [ 0 ] , [ 9 9 : 1 ] ; endmodule [ 9 9 : 1 ] } ; ``` [Description] This Verilog module is used to compare two input signals and generate three output signals. The first input signal is 100bit wide vector, and the second input signal is the same vector shifted by one bit. The module has three output signals, out_both, out_any, and out_different. The out_both signal is generated by performing bit-wise AND operation between the two input signals. This will result in 99-bit wide vector, where each bit is 1 only if both the corresponding bits of the two input signals are 1. The out_any signal is generated by performing bit-wise OR operation between the two input signals. This will result in 100-bit wide vector, where each bit is 1 if either of the corresponding bits of the two input signals is 1. The out_different signal is generated by performing bit-wise XOR operation between the two input signals. This will result in 100-bit wide vector, where each bit is 1 only if the corresponding bits of the two input signals are different. The first bit of the out_different signal is generated by performing bit-wise XOR operation between the first bit of the first input signal and the last bit of the second input signal. [Problem] You are given 100-bit input vector in[99:0]. We want to know some relationships between each bit and its neighbour: // (1) out_both: Each bit of this output vector should indicate whether both the corresponding input bit and its neighbour to the left are 1. For example, out_both[98] should indicate if in[98] and in[99] are both 1. Since in[99] has no neighbour to the left, the answer is obvious so we dont need to know out_both[99]. // (2) out_any: Each bit of this output vector should indicate whether any of the corresponding input bit and its neighbour to the right are 1. For example, out_any[2] should indicate if either in[2] or in[1] are 1. Since in[0] has no neighbour to the right, the answer is obvious so we dont need to know out_any[0]. // (3) out_different: Each bit of this output vector should indicate whether the corresponding input bit is different from its neighbour to the left. For example, out_different[98] should indicate if in[98] is different from in[99]. For this part, treat the vector as wrapping around, so in[99]s neighbour to the left is in[0]. This Verilog module, named top_module, has the interface designed as follows: Signal Name Direction Width Description - in Input 100 100-bit input vector for analyzing bit relationships out_both Output 99 Each bit indicates if both the corresponding input bit and its left neighbor are 1 out_any Output 99 Each bit indicates if either the corresponding input bit or its right neighbor is 1 out_different Output 100 Each bit indicates if the corresponding input bit is different from its left neighbor, circularly ### Instruction Please write description about the code snippet and problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] ```verilog {The Given Code Snippet} ``` ### Response Our prompt begins by presenting five distinct demonstrations. Each demonstration first provides description of code snippet, followed by the generation of corresponding problem. We then prompt the model (DeepSeek-V3) to generate problem similarly based on the given code snippet colored in red. This process mirrors the multi-level summarization mechanism in CodeV [46]. We also show the system prompt we use during training (both SFT and RL) and testing (on benchmarks) as below. System Prompt for Training and Testing You are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to write verilog code. After thinking, when you finally reach conclusion, enclose the final verilog code in ```verilog ```within <answer> </answer> tags. i.e., <answer> ```verilogn module top_module(in, out, ...); ... ```</answer>."
        },
        {
            "title": "F Broader Impacts",
            "content": "Through distillation from DeepSeek-R1 and reinforcement learning, CodeV-R1-7B even outperforms DeepSeek-R1-671B on RTLLM v1.1 and RTLLM v2, while outperforming previous Verilog-domain state-of-the-art models (typically 7 15B) by 1221 % on RTLLM v1.1 and v2. Through these results, our work demonstrates the promising potential of reinforcement learning for improving circuit design. However, analogous to other code generation models, CodeV-R1-7B may produce code that misaligns with user intentions or even be misused for unintended purposes. As comprehensively analyzed in broader impact studies [1], such risks include but are not limited to: 1. Functional misalignment: Generated code might superficially satisfy requirements but fail to execute as intended, particularly in safety-critical circuit designs. 2. Security vulnerabilities: The model could inadvertently generate insecure code (e.g., flawed logic or backdoors), which poses risks in hardware deployment. 3. Misuse in malicious contexts: Lower barriers to code generation may facilitate the creation of obfuscated or harmful designs, especially as model capabilities scale. Given the potentially severe consequences of such issues in hardware systems, we strongly recommend that users: 1. Conduct rigorous functional verification and security audits for all generated code. 2. Implement access controls and usage monitoring to mitigate abuse risks. 3. Adopt principle of \"human-in-the-loop\" oversight, particularly for high-stakes applications."
        }
    ],
    "affiliations": [
        "Cambricon Technologies",
        "SKL of Processors, Institute of Computing Technology, CAS",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}