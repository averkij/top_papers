{
    "paper_title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
    "authors": [
        "Zhengyao Lv",
        "Tianlin Pan",
        "Chenyang Si",
        "Zhaoxi Chen",
        "Wangmeng Zuo",
        "Ziwei Liu",
        "Kwan-Yee K. Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention (TACA)}, a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at \\href{https://github.com/Vchitect/TACA}"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 8 9 7 0 . 6 0 5 2 : r TACA: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers Zhengyao Lv1 Tianlin Pan2,3 Chenyang Si2 Zhaoxi Chen4 Wangmeng Zuo5 Ziwei Liu4 Kwan-Yee K. Wong1 1The University of Hong Kong 2Nanjing University 3University of Chinese Academy of Sciences 4Nanyang Technological University 5Harbin Institute of Technology cszy98@gmail.com pantianlin23@mails.ucas.ac.cn chenyang.si@nju.edu.cn zhaoxi001@ntu.edu.sg cswmzuo@gmail.com ziwei.liu@ntu.edu.sg kykwong@cs.hku.hk Figure 1. We propose TACA, parameter-efficient method that dynamically rebalances cross-modal attention in multimodal diffusion transformers to improve text-image alignment."
        },
        {
            "title": "Abstract",
            "content": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MMDiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA. 1. Introduction *Equal Contribution. Project Leader. Corresponding Author. Diffusion models [13, 39], driven by iterative denoising processes, have emerged as powerful paradigm in generative modeling. The field has witnessed significant architectural evolution, beginning with U-Net-based designs [37] that dominated early diffusion models [9, 27, 32, 36]. Recent advances introduced transformer-based architectures through Diffusion Transformers (DiT) [5, 29], demonstrating superior scalability and training stability. This progression culminated in Multimodal Diffusion Transformers (MM-DiT) [10], which unify text and visual tokens through concatenated self-attention mechanism, resulting in stateof-the-art text-to-image/video models like Stable Diffusion 3/3.5 [10, 40], FLUX [20], CogVideo [14, 48], and HunyuanVideo [19]. namic remains unaddressed in existing approaches, leading to suboptimal modality balancing."
        },
        {
            "title": "Based on the",
            "content": "above observations, we propose Temperature-Adjusted Cross-modal Attention (TACA), straightforward yet effective enhancement to the MM-DiT attention mechanism. Our approach introduces two key innovations, namely (1) modality-specific temperature scaling to mitigate cross-attention suppression, and (2) intertimestep-dependent adjustments actions. TACA only requires temperature coefficient γ(t) that adapts to the denoising timesteps, allowing for easy implementation with minimal code modifications. To mitigate potential artifacts introduced by amplified cross-attention, we complement TACA with Low-Rank Adaptation (LoRA) [15] fine-tuning for distributional alignment, helping the model generate images that better match real-world distributions. to cross-modal Figure 2. Object missing in text-to-image models. Even in SOTA models like FLUX.1 Dev, we can still observe cases with missing objects. Prompts: The square painting was next to the round mirror, blue bench and green car. Although the MM-DiT architecture has undergone significant advancements, state-of-the-art text-to-image models like FLUX still exhibit critical limitations, particularly in generating images with precise semantic alignment (see Fig. 2). Analysis of the sampling process reveals that early denoising steps require strong text-visual interaction to create proper semantic layout, while later steps focus on refining the details. Semantic discrepancies between the text and synthesized images often stem from flawed initial layouts (see Fig. 3). In typical U-Net/DiT-based text-to-image diffusion models, the cross-attention block enables modal interaction between textual and visual tokens to synthesize text-aligned images. Our analysis of the attention maps of MM-DiT layers suggests that semantic discrepancies may arise from the suppression of cross-modal attention, specifically due to the numerical asymmetry between the number of visual and text tokens. The overwhelming number of visual tokens can dilute the textual guidance in the unified softmax function of the MM-DiT architecture, resulting in the visual tokens paying significantly less attention to the textual tokens compared to the typical cross-attention paradigm (see Fig. 4). Furthermore, we noticed that current MM-DiT architectures employ static attention mechanisms with the same weighting for all timesteps, which is ill-suited to the time-varying demands of semantic composition and detail synthesis during the denoising process (see Fig. 3). This temporal dyExperiments on T2I-CompBench [16] validate the effectiveness of our method across various model architectures. For FLUX.1-Dev, incorporating TACA results in substantial improvements, yielding relative gains of 16.4% in spatial relationship understanding and 5.9% in shape accuracy. Similarly, when applied to SD3.5-Medium, TACA boosts spatial relationship accuracy by 28.3% and shape accuracy by 2.9%. These benchmark results, combined with the visual improvements shown in Fig. 1, highlight significant enhancement in text-image alignment achieved by our approach. In summary, our principal contributions are: We systematically analyze MM-DiTs unified attention mechanism, and reveal cross-attention suppression and timestep insensitivity being two key factors limiting textimage alignment in text-to-image generation. We propose TACA, the first approach to dynamically balance multimodal interactions through temperature scaling and temporal adaptation in diffusion transformers. Extensive benchmark results demonstrate that TACA can effectively improve semantic alignment with minimal computational overhead. 2. Related Work 2.1. Diffusion Transformers central challenge in developing transformer-based textto-image/video (T2I, T2V) diffusion models lies in the effective integration of multimodal data, primarily text and visual information. Several approaches, including Diffusion Transformers (DiT [29]), CrossDiT (PixArt-α [5]), and MM-DiT (Stable Diffusion 3 [10]), tackle this challenge with distinct methods for cross-modal interaction and textimage alignment. The original DiT [29] introduced transformers [1, 42] as replacements for U-Net backbones [37] in diffusion modFigure 3. The denoising process. This figure shows the predicted x0 in each step of the denoising process for the prompt The black chair is on the right of the wooden table with FLUX.1 Dev. This observation leads to our hypothesis that visual-text cross-attention plays more significant role than visual self-attention specifically during these initial steps where the images overall composition is determined. Additionally, as the temperature scaling factor γ increases in the cross-modal section of MM-DiTs unified softmax function, the initial image composition progressively aligns more closely with the corresponding text. Figure 4. Relative magnitude of visual-text attention between the typical cross attention and MM-DiT full attention (averaged over 50 samples). The numerical asymmetry between the number of visual and text tokens suppresses the magnitude of cross attention, leading to weak alignment between the generated image and the given text prompt. We can amplify the cross-attention by boosting the coefficient γ, thereby strengthening the alignment between the image and text. els [13, 39]. While not inherently multimodal, DiT established critical conditioning mechanisms via adaptive layer normalization (adaLN) [30]. This technique modulates transformer activations using timestep embeddings and class labels, enabling controlled generation based on singlemodality inputs. While effective for class-conditional generation, DiT lacks explicit mechanisms for text-image alignment, limiting its applicability in multimodal tasks. CrossDiT (PixArt-α) [5] introduced cross-modal fusion by integrating text-guided cross-attention into the DiT backbone. In this framework, cross-attention replaces adaLN for text conditioning, which enables dynamic per-token modulation based on linguistic semantics. However, CrossDiT uses unidirectional update approach that prevents the image from influencing the textual representation. This hinders its ability to model feedback loops and nuanced interdependencies between the text and generated image. MM-DiT (Stable Diffusion 3) [10] represents paradigm shift by introducing bidirectional cross-modal attention and unified token space for text and visual modalities. By concatenating text and image tokens into single sequence and employing decomposed attention matrix, MM-DiT enables full self-attention across modalities, capturing complex inter-modal relationships. Besides, the integration of multiple text encoders (e.g., CLIP [33] and T5 [34]) further improves the models ability to understand and generate text with greater accuracy. 2.2. Text-to-Image Alignment Prior research has explored generating images from text prompts using pre-trained models without requiring further training, employing techniques such as CLIP-guided optimization [12, 25] to align images with text by optimizing CLIP scores within the models latent space, and crossattention-based approaches [7] to enhance spatial layout and details in generated images, thereby improving adherence to the textual descriptions structure. Additionally, more recent research has explored augmenting guidance-based models to enhance semantic control, primarily through layout planning modules [6, 8, 18, 22, 31, 45, 46] and feedback-driven optimization [3, 11, 41]. Another direction involves attention-based methods [2, 4, 21, 26, 35, 44] that modify or constrain the attention maps within U-Net models to improve textual alignment. However, these attention-based techniques generally do not readily translate to contemporary DiT-based architectures. 3. Methodology 3.1. Preliminaries Diffusion-based generative models operate through forward diffusion process and reverse denoising process [13]. The forward process systematically degrades data samples through gradual noise injection, while the reverse process learns to recover the original data structure through iterative refinement. The diffusion mechanism progressively corrupts training samples x0 q(x0) over discrete timesteps according to predetermined variance schedule {βt}T t=1. This corruption follows Markov chain where each transition adds isotropic Gaussian noise: q(xtxt1) = (cid:16) (cid:17) xt; (cid:112)1 βtxt1, βtI . (1) (2) The denoising phase constitutes learned reversal of this progressive corruption. This reverse process estimates the ancestral distribution q(xt1xt) by learning: I(cid:1) , pθ(xt1xt) = (cid:0)xt1; µθ(xt, t), σ2 is typically fixed as βt or βt = 1 αt1 where σ2 βt with 1 αt αt = (cid:81)t s=1(1 βs). The mean µθ is derived through noise prediction network ϵθ. This network, conventionally implemented as time-conditional U-Net [37] or vision transformers [5, 10, 29] in more recent works, is optimized to predict the noise component presents in xt, enabling precise incremental denoising. Multimodal Diffusion Transformer (MM-DiT) [10] is novel approach to adopt transformers as the noise prediction network in diffusion models. The MM-DiT architecture concatenates text and visual tokens into single input sequence after projecting both modalities to shared dimensional space. The concatenated sequence undergoes multi-head self-attention where every token attends to all others, regardless of modality. Mathematically, if we use to denote the number of attention heads, Nx and Nc to denote the sequence length of visual and text tokens respectively, and to denote the dimension of the token embeddings, then for visual tokens RHNxD and text tokens RHNcD, we have: (cid:18)W (cid:19) K (cid:18)W W (cid:18)W c , = , = , (3) = (cid:19) (cid:19) and Attention(Q, K, ) = softmax (cid:33) (cid:32) QKT , (4) where the QKT term can be expanded to c(W c)T c)T x(W (cid:19) x)T x)T (cid:19) (5) QKT = c(W x(W (cid:18)W (cid:18)QtxtKT QvisKT = txt QtxtKT vis txt QvisKT vis As we can see in Eq 6, this MM-DiT formulation allows four interaction types: text-text, text-visual, visual-text, and visual-visual attentions, all within single operation. (6) . 3.2. Suppression of Cross-Attention and TimestepInsensitive Weighting in MM-DiT While the unified attention mechanism of MM-DiT provides computational efficiency through joint modality processing, it introduces inherent issues when balancing different modalities. Suppression of Cross-Attention in Unified Softmax. This issue stems from the numerical asymmetry between the number of visual and text tokens (Nx Nc), which creates systematic bias in attention weight distribution. Consider the attention computation for visual tokens in Eq. 4. Each visual tokens attention weights over text tokens (QvisKT txt) must compete against visual-visual interactions (QvisKT vis) in the softmax denominator. Formally, the probability of the i-th visual token attending to the j-th text token guidance becomes: (i, j) vistxt = esvt ij /τ ik/τ + (cid:80)Nvis k=1 esvv ik /τ (cid:80)Ntxt k=1 esvt , (7) ik = Q(i) ik = Q(i) visKT (k) txt / visKT (k) vis /"
        },
        {
            "title": "D and svv",
            "content": "where svt D. When Nvis Ntxt, the exponential sum over visualvisual interactions dominates the denominator, even if individual svv ik values are modest. For example, when using FLUX.1 Dev [20] to generate 1024 1024 image, we have Nvis/Ntxt = 4096/512 = 8. In this case, the visualtext cross-attention probabilities would be much lower than in the typical paradigm: (i, j) vistxt ik /τ esvt ij /τ (cid:80)Nvis k=1 esvv esvt ij /τ (cid:80)Ntxt k=1 esvt ik/τ (Full Attention) (8) (Typical Cross-Attention) (9) This suppression of Pvistxt, which can be observed in Fig. 4, weakens the alignment between visual and textual features. The model struggles to effectively leverage textual guidance to refine visual representations because the influence of the text tokens is diluted by the overwhelming presence of visual tokens. Crucial semantic relationships encoded in the text may be overlooked, leading to visual representation that is less informed by the corresponding textual description, like the bad cases shown in Fig. 2. Timestep-Insensitive QK Weighting. MM-DiTs current architecture employs timestep-agnostic projection of latent states into query and key vectors. This approach fails to account for the evolving influence of textual guidance throughout the denoising process. As illustrated in Fig. 3, the initial denoising steps are crucial for establishing the images global layout, heavily influenced by the text prompt. Consequently, the cross-attention mechanism, responsible for integrating textual information, should be weighted more heavily than visual self-attention during these early stages. MM-DiTs static weighting strategy, therefore, limits its ability in optimally leveraging textual guidance and adapting to the changing demands of the denoising process. Formally, when is large (i.e., early in the denoising process) and cross-modal guidance should dominate, svt ik values fail to receive proportionally larger magnitudes compared to ik . Since and are optimized for global perforsvv mance across all timesteps, they cannot focus on amplifying text-visual interactions in the early stages. This potentially leads to the overall layout of the generated image not aligning with the text prompt. Figure 5. Temperature scaling helps visual-text alignment. From this figure, we can see that as the temperature scaling factor γ increases, the characteristics of brown backpack, glass mirror and black stomach become more obvious. 3.3. Temperature-Adjusted Cross-modal Attention To address the two issues mentioned in Section 3.2, we propose Temperature-Adjusted Cross-modal Attention (TACA), simple yet effective modification to the attention mechanism of MM-DiT. Our approach introduces two key innovations, namely modality-specific temperature scaling and timestep-dependent adjustment of cross-modal interactions. Modality-Specific Temperature Scaling. To mitigate the suppression of cross-attention caused by the dominance of visual tokens (Nvis Ntxt), we amplify the logits of visual-text interactions through temperature coefficient γ > 1. The modified attention probability for visual-text interaction becomes: (i, j) vistxt = eγsvt ij /τ ik/τ + (cid:80)Nvis k=1 esvv ik /τ (cid:80)Ntxt k=1 eγsvt , (10) Figure 6. Attention map differences. We conducted visualization of the alterations in the visual-text attention map during the initial stages of the denoising process, as influenced by our proposed method. In contrast to the baseline, our approach substantially amplifies the attention directed toward the text in the early steps. This scaling effectively rebalances the competition in softmax by increasing the relative weight of cross-modal interactions. The γ coefficient acts as signal booster for text-guided attention. As shown in Fig. 5, the generated image and text prompt become more consistent as γ increases. Besides, we can also observe in Fig. ?? that the CLIP similarity [33] between the main object black chair in the prompt and the predicted fully denoised x0 at each step improves significantly as γ grows. Timestep-Dependent Adjustment. To compensate for the insensitivity of QK weights with respect to the timestep, we make γ timestep-dependent to account for the varying importance of cross-attention during denoising based on the observations in Fig. 3. Specifically, we employ piecewise function: γ(t) = (cid:40) γ0 tthresh < tthresh (11) where tthresh is threshold for the timestep that separates the layout formation and detail refinement phases. This design aligns with the denoising dynamics where early steps (i.e., large t) require strong text guidance to establish image composition and later steps (i.e., small t) focus on visual details when self-attention dominates. Notably, TACA introduces no new learnable parameters, with the temperature scaling implemented via simple element-wise operation during attention computation. The γ0 and tthresh parameters can be tuned through minimal ablation studies, making our approach both efficient and practical for deployment in existing MM-DiT architectures. LoRA Training for Artifact Suppression. While temperature scaling in TACA significantly improves visualimage alignment, the amplified cross-modal attention logits can alter the output distribution of the denoising process, occasionally introducing artifacts such as distorted object boundaries or inconsistent textures. To mitigate this, we employ Low-Rank Adaptation (LoRA) [15] to fine-tune the model, encouraging it to recover the real image distribution while preserving the benefits of temperature scaling. We apply LoRA to the attention layers of MM-DiT, where the temperature scaling exerts the most direct influence. For weight matrix Rdk, LoRA adaptation is formulated as = + α BA, Rdr, Rrk (12) where min(d, k) is the rank of the adaptation, and α scales the low-rank update. Only and are trainable during fine-tuning, keeping the original frozen. 4. Experiments 4.1. Experiment Settings Evaluation Metrics and Datasets. We evaluate our method on the T2I-CompBench benchmark [16], comprehensive evaluation suite for text-to-image alignment. All experiments use the LAION dataset [38] with captions refined by the LLaVA model [24] to enhance semantic precision. We randomly sampled 10K image-text pairs as the training dataset for our LoRA model. To ensure reproducibility throughout all evaluation phases, the random seed is fixed to 42, while all other parameters remain at their default values as provided by the Diffusers library [43]. Implementation Details. We conduct experiments on single NVIDIA A100 80GB GPU using the ai-toolkit codebase [28], with LoRA adapters implemented for FLUX.1 Dev [20] and SD3.5 Medium [40] models. We adopt the AdamW optimizer with learning rate of 1 104 and batch size of 4 for training. We evaluate two LoRA configurations: (r, α) = (16, 16) and (64, 64). To emphasize semantic alignment, we sample timesteps tthresh = 970 within the range (0, 1000). In the Diffusers flowmatching scheduler, 30-step denoising process allocates the first three steps to (970, 1000] (i.e., the initial 10% of the diffusion process), while the remaining 27 steps cover [0, 970). Setting tthresh = 970 focuses training on these early steps where semantic information is most prominent. Under the flow-matching paradigm [23], the model predicts velocity instead of noise ϵ. We fine-tune it with the following velocity prediction loss: = Ex0, ttthresh (cid:2)v(xt, t) vθ(xt, t, Ptxt, γ(t))2 2 (cid:3) , (13) where Ptxt represents text prompts and γ(t) induces the modified temperature coefficient. For benchmark results, we set the base temperature scaling factor as γ0 = 1.2, which is selected in Section 4.3. This formulation ensures the model learns the correct velocity field while adapting to temperature-scaled attention. 4.2. Main Results Quantitative Comparison. To quantitatively evaluate the effectiveness of our proposed TACA, we conduct comprehensive comparison against baseline models. Table 1 presents the alignment performance of FLUX.1-Dev and SD3.5-Medium models, respectively, with and without the integration of TACA. For FLUX.1-Dev, the incorporation of TACA, particularly with rank = 64, consistently improves performance across all Attribute Binding metrics and Spatial Relationship. Similarly, for SD3.5-Medium, TACA with = 64 yields significant gains in Attribute Binding and Spatial Relationship, and TACA with = 16 achieves the best performance on Non-Spatial Relationship and Complex prompt evaluation. These results demonstrate that TACA effectively enhances the alignment capabilities of different MM-DiT models across various dimensions of text-to-image generation quality. Image Quality Evaluation. We use widely adopted image quality assessment models MUSIQ [17] and MANIQA [47] to evaluate visual quality. As shown in the Table 2, TACA improves text-image alignment without sacrificing image quality on both SD3.5 and FLUX. User Study. We invited 50 participants for our user study. From the T2I-Compbench [16] dataset, we sampled 25 prompts and generated images using FLUX.1 Dev model with and without our TACA method. These images, along with their corresponding text prompts, were presented to the participants. Participants were asked to indicate their preferred image based on three criteria, namely overall visual appeal, attribute (color/shape/texture) quality, and text-image alignment. The results, as summarized in Table 3, demonstrate that majority of participants favored the images generated by the model incorporating the TACA method. This suggests that our method yields improvements in text alignment and does not ruin image quality. 4.3. Ablation Study Effect of Temperature Scaling Factor (γ0). The temperature scaling factor γ0 plays crucial role in modulating the influence of textual guidance during the denoising process. We explore the impact of different γ0 values on compositional generation performance. Table 4 presents the results. The results in Table 4 demonstrate that our proposed method consistently outperforms both the baseline FLUX.1-Dev and the LoRA-only approach across all attributes for reasonable values of γ0. We observe improvements in Color, Shape, Texture, and Spatial compositional accuracy. Notably, γ0 = 1.2 yields the best overall balance, achieving the highest scores in Color and Texture, and the second-best in Shape and Spatial. Increasing γ0 further to 1.3 leads to slight improvements in Shape and Spatial, but decline in Color and Texture. This suggests that moderate increase in textual influence is beneficial, but excessive Figure 7. Comparison of samples generated by FLUX.1 Dev and Stable Diffusion 3.5 Medium with and without TACA. Our method enhances the semantic representation of the primary object within the text prompt, improves spatial relationships, and maintains image quality without degradation. Table 1. Comparison of alignment evaluation on T2I-CompBench [16] for FLUX.1-Dev-based and SD3.5-Medium-based models. The best results for each model group are highlighted in bold. Model FLUX.1-Dev FLUX.1-Dev + TACA (r = 64) FLUX.1-Dev + TACA (r = 16) SD3.5-Medium SD3.5-Medium + TACA (r = 64) SD3.5-Medium + TACA (r = 16) Attribute Binding Color 0.7678 0.7843 0.7842 0.7890 0.8074 0. Shape Texture 0.5064 0.5362 0.5347 0.5770 0.5938 0.5834 0.6756 0.6872 0.6814 0.7328 0.7522 0.7467 Object Relationship Spatial Non-Spatial 0.2066 0.2405 0.2321 0.2087 0.2678 0.2374 0.3035 0.3041 0.3046 0.3104 0.3106 0.3111 Complex 0.4359 0.4494 0.4479 0.4441 0.4470 0. (a) (b) Figure 8. The effect of LoRA training. (a) Qualitative results comparing training-free image generation to generation with LoRA. Training free examples show artifacts such as floating bowl and distorted structures, which are significantly reduced by LoRA training. (b) Quantitative evaluation of CLIP Scores for training-free and LoRA-trained models across the denoising steps, showing that LoRA maintains the strong visual-text alignment and does not detract from the semantic benefits of our approach. Table 2. Results of image quality assessment. Metric MUSIQ MANIQA SD3.5 0.7182 0. +TACA FLUX +TACA 0.7212 0.7186 0.7210 0.5292 0.5149 0.4921 Table 3. Results of user study. Evaluation Criteria Overall Attribute Quality Text-Image Alignment FLUX FLUX + TACA 23.58% 29.25% 17.75% 76.42% 70.75% 82.25% Table 4. Ablation study on the effect of the temperature scaling factor (γ0). We randomly sampled 100 prompts for each attribute from the T2I-CompBench dataset to conduct the evaluation. Here Lora Only refers to training LoRA model solely on the identical dataset using the same hyperparameters without our proposed method. Bold indicates the best score and underline indicates the second best score for each attribute. Model FLUX.1-Dev LoRA Only Ours (γ0 = 1.1) Ours (γ0 = 1.2) Ours (γ0 = 1.3) Color 0.798 0.805 0.803 0.839 0.787 Shape 0.591 0.592 0.603 0.634 0.650 Texture 0.755 0.759 0.780 0.790 0.766 Spatial 0.193 0.187 0.199 0.207 0.225 amplification can negatively impact certain aspects of compositional generation. To understand the mechanism behind this improvement, we analyze the CLIP similarity between the predicted intermediate latent representations and the text prompt at each denoising step for varying γ0. As shown in Figure 8 (b), increasing γ0 leads to higher CLIP similarity, particularly in the initial denoising steps. This indicates that TACA effectively enhances the visual-text alignment early in the generation process, guiding the model towards generating images that are more consistent with the textual description. Sensitivity of γ0 and tthresh. We further investigate the sensitivity of our method to the choice of γ0 and the threshold timestep tthresh beyond which TACA is applied. Table 5 presents the average Attribute (Color, Shape, Texture) and Spatial scores for different values of γ0 and tthresh on both FLUX and SD3.5 models. As shown in Table 5, the performance exhibits minimal variation across reasonable range of both γ0 (e.g., 1.15 to 1.25) and tthresh (e.g., 930 to 970). This indicates that our method is not overly sensitive to the precise selection of these parameters, suggesting practical robustness. Furthermore, this robustness is observed across different base models=, highlighting the general applicability of the tested parameter ranges. The Effect of LoRA Training. In the original TACA method, the introduction of factor γ(t) induces shift in the output distribution of each attention layer. These modified outputs are subsequently processed by the feed-forward networks within the transformer blocks. Consequently, the overall output distribution of the diffusion transformer deviates from the distribution inherent in real images, which manifests as visual artifacts like unsupported floating bowls and distorted bridge connections in Fig 8 (a). To address this issue, we hypothesize that training LoRA [15] module can effectively mitigate these artifacts. The rationale is that by fine-tuning the attention layer weights with limited number of training samples, the LoRA module enables the modified model to readjust its output distribution to better align with the real image distribution. Table 5. Sensitivity analysis of γ0 and tthresh. We evaluate the average Attribute (mean score of Color, Texture and Shape) and Spatial scores for different values of γ0 and tthresh on FLUX.1-Dev and SD3.5-Medium models. The baseline corresponds to the respective models without TACA. γ0 Baseline 1.15 1.20 1."
        },
        {
            "title": "FLUX",
            "content": "SD3."
        },
        {
            "title": "Spatial",
            "content": "0.715 0.740 0.754 0.737 0.193 0.197 0.207 0.216 0.797 0.810 0.815 0.816 0.159 0.181 0.183 0.176 tthresh Baseline 970"
        },
        {
            "title": "FLUX",
            "content": "SD3."
        },
        {
            "title": "Spatial",
            "content": "0.715 0.754 0.757 0.763 0.193 0.207 0.218 0.208 0.797 0.815 0.811 0.811 0.159 0.183 0.172 0.171 Empirical findings from our experiments demonstrate that the incorporation of LoRA significantly enhances image quality and effectively mitigates these unrealistic artifacts, as evidenced in Fig. 8 (a). Concurrently, we evaluated whether the introduction of LoRA compromises the semantic enhancement facilitated by the temperature coefficient γ(t). The comparative analysis of CLIP Scores for trainingfree and LoRA configurations for 50 samples, presented in Fig. 8 (b), reveals that LoRA exerts negligible impact on visual-text alignment. 5. Conclusion and Discussion text-image alignment In this paper, we addressed two issues in MM-DiTs that in text-to-image generation: limit suppressed cross-attention due to token imbalance and timestep-insensitive attention weighting. We introduced Temperature-Adjusted Cross-modal Attention (TACA), simple modification that dynamically balances multimodal interactions using temperature scaling and timestepdependent adjustment. Combined with LoRA fine-tuning to reduce artifacts, TACA significantly improved text-image alignment on the T2I-CompBench benchmark. Our work demonstrates that strategically reweighting cross-modal interactions leads to more semantically accurate and visually coherent image generation, offering promising approach for diffusion model research and applications. Our work has mainly two limitations: 1) While improvements in text alignment were observed in training-free text-to-video experiments, we encountered dilution effect when training LoRA, wherein gains from increasing the temperature factor were diminished. 2) Our method lacks the ability to adaptively select an appropriate scaling factor based on the actual degree of text alignment."
        },
        {
            "title": "References",
            "content": "[1] An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2 [2] Aishwarya Agarwal, Srikrishna Karanam, K. J. Joseph, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. A-star: Test-time attention segregation and retention for text-to-image synthesis. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 22832293, 2023. 3 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3 [4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42:1 10, 2023. 3 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. The Twelfth International Conference on Learning Representations, 2023. 2, 3, 4 [6] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training2024 free layout control with cross-attention guidance. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 53315341, 2023. [7] Minghao Chen et al. Training-free layout control with crossattention guidance. arXiv preprint arXiv:2304.03373, 2023. 3 [8] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation. arXiv preprint arXiv:2403.16990, 2024. 3 [9] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint arXiv:2105.05233, 2021. 2 [10] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2, 3, 4 [11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, P. Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. 3 [12] Federico A. Galatolo, Mario G.C.A. Cimino, and Gigliola Vaglini. Generating images from caption and vice versa via clip-guided generative latent space search. arXiv preprint arXiv:2102.01645, 2021. 3 and editing with text-guided diffusion models. tional Conference on Machine Learning, 2021. 2 In Interna- [13] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020. 1, 3 [14] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [15] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: arXiv Low-rank adaptation of large language models. preprint arXiv:2106.09685, 2021. 2, 6, [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv:2307.06350, 2023. 2, 6, 7 [17] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. arXiv preprint arXiv:2108.05997, 2021. 6 [18] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 76677677, 2023. 3 [19] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [20] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 4, 6 [21] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. Divide & bind your attention for improved generative semantic nursing. arXiv preprint arXiv:2307.10864, 2023. 3 [22] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2251122521, 2023. 3 [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 6 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485. [25] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: Training-free text-to-image generation with improved clip+gan space optimization. arXiv preprint arXiv:2112.01573, 2021. 3 [26] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Conform: Contrast is all you need for high2024 IEEE/CVF fidelity text-to-image diffusion models. Conference on Computer Vision and Pattern Recognition (CVPR), pages 90059014, 2023. 3 [27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation [28] Ostris. Ai toolkit, 2025. 6 [29] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2022. 2, 4 [30] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning In AAAI Conference on with general conditioning layer. Artificial Intelligence, 2017. 3 [31] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded 2024 text-to-image synthesis with attention refocusing. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 79327942, 2023. 3 [32] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 3, [34] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2019. 3 [35] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute corresponarXiv preprint dence through attention map alignment. arXiv:2306.08877, 2023. 3 [36] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, 2021. 2 [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. arXiv preprint arXiv:1505.04597, 2015. 2, 4 [38] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 6 [39] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1, 3 and Stefano Ermon. arXiv preprint [40] Stability-AI. Stable diffusion 3.5, 2024. 2, 6 [41] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, and Cyrus Rashtchian. Dreamsync: Aligning text-to-image generation with image understanding feedback. arXiv preprint arXiv:2311.17946, 2023. 3 [42] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 2 [43] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. 6 [44] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diffusion with 2024 IEEE/CVF Conference on token-level supervision. Computer Vision and Pattern Recognition (CVPR), pages 85538564, 2023. [45] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained 2023 IEEE/CVF International Conference on diffusion. Computer Vision (ICCV), pages 74187427, 2023. 3 [46] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708, 2024. 3 [47] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. arXiv preprint arXiv:2204.08958, 2022. 6 [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 TACA: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers"
        },
        {
            "title": "Supplementary Material",
            "content": "Overview. In the supplementary material, we provide further details to support our work. Section elaborates on the implementation of TACA, including code snippets and speed comparison of different approaches. Section presents additional ablation studies focusing on text alignment, examining the effect of CFG guidance scale and the content/length of prompts. Section explains why we choose LoRA rather than full-parameter finetune. Finally, Section showcases more qualitative results with visual comparisons on both short and long prompts using FLUX.1 Dev and SD3.5 Medium. A. Code Implementation Details Given that TACA necessitates modifications to the attention mechanism, and that the functions for computing attention are typically encapsulated within pre-compiled C/C++ binary libraries, directly reimplementing these attention computation functions using PyTorch would result in significant performance degradation. To minimize the performance impact of modifying the attention mechanism while retaining the convenience of PyTorch, the following two implementation approaches for TACA can be adopted: Flex Attention 1 from torch.nn.attention.flex_attention import flex_attention 2 gamma = 1.2 3 encoder_size = 512 # T5 encoder seq_len for FLUX 4 5 def score_mod(score, batch, head, token_q, token_kv): 6 7 9 condition = (token_q >= encoder_size) & ( token_kv < encoder_size) score = torch.where(condition, score * gamma, score) return score 10 11 hidden_states = flex_attention(query, key, value, score_mod=score_mod) Listing 1. PyTorch Flex Attention Selective Attention Recomposition 1 gamma = 1.2 2 encoder_size = 512 # T5 encoder seq_len for FLUX 3 key_scaled = key.clone() 4 5 # Shape of Q, K, (B, H, N, D) 6 key_scaled[:, :, :encoder_size, :] *= gamma 7 8 # You can also change this into flash attention 9 hidden_states = F.scaled_dot_product_attention( query, key_scaled, value, attn_mask= attention_mask, dropout_p=0.0, is_causal= False 11 ) 12 13 hidden_states_orig = F. scaled_dot_product_attention( query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False 14 15 ) 16 17 hidden_states[:, :, :encoder_size, :] = hidden_states_orig[:, :, :encoder_size, :] Listing 2. Selective Attention Recomposition We conducted empirical evaluations of the computational speed of both proposed methods, comparing them against PyTorchs native scaled dot-product attention implementation. All experiments employ 30-step denoising process to generate 1024 1024 images via FLUX.1 Dev on single Nvidia A100 80G GPU. We recorded the performance differential for both single denoising step and for the complete 30-step denoising process (assuming temperature factor γ modification applied only to the initial 10% of steps). The results of this speed evaluation are presented in Table 6. Method Baseline Flex Selective Single Step All 30 Steps 0.47 sec 2.13 sec 0.95 sec 14 sec 19 sec 16 sec Speedup 1.0x 0.74x 0.88x Table 6. Speed Comparison of Different Approaches B. Further Ablation Study on Text Alignment B.1. The scale of CFG guidance To investigate the text alignment improvements offered by our TACA method in comparison to increasing the CFG guidance scale (commonly employed in text-to-image models to enhance alignment, often at the cost of image quality), we conducted series of ablation studies. These experiments aimed to determine whether TACA maintains its efficacy across varying CFG guidance scales and across different models. The results, presented in Table 7, reveal the effects of different CFG scales and the impact of TACA on both FLUX.1 Dev and SD3.5-Medium. For FLUX.1 Dev, the default guidance scale of 3.5 appears to be sweet spot: further increases in CFG intensity beyond this point yield minimal gains in text alignment,"
        },
        {
            "title": "Model",
            "content": "FLUX.1 Dev SD3.5-Medium Settings CFG = 3.5 (Default) CFG = 3.5 + TACA CFG = 5 CFG = 5 + TACA CFG = 10 CFG = 10 + TACA CFG = 7 (Default) CFG = 7 + TACA CFG = 10 CFG = 10 + TACA Color 0.798 0.839 0.787 0.835 0.667 0.751 0.812 0.843 0.804 0. Shape Texture 0.591 0.634 0.553 0.635 0.571 0.633 0.755 0.790 0.756 0.757 0.740 0.699 0.730 0.737 0.727 0.765 0.850 0.864 0.853 0.863 Spatial 0.193 0.207 0.175 0.224 0.137 0.191 0.159 0.183 0.191 0. Table 7. Ablation study on the effect of CFG scale with and without TACA (with γ0 = 1.2) on FLUX.1 Dev and SD3.5-Medium. We randomly sampled 100 prompts for each attribute from the T2I-CompBench dataset to conduct the evaluation. For both models, bold indicates the best score and underline indicates the second-best score for each attribute. and, notably, performance across several metrics degrades significantly. Concurrently, our TACA method demonstrated effectiveness across diverse guidance scales, suggesting its general applicability. For SD3.5-Medium, increasing the CFG scale also enhances text-image alignment but tends to degrade visual fidelity, resulting in reduced metrics (e.g., Color score drops at CFG=10 compared to CFG=7). Our TACA method, however, directly reinforces the dependence of image tokens on textual tokens, improving alignment without such adverse effects. TACA consistently improves results across different CFG scales on SD3.5-Medium, showing both generalization and complementarity. Overall, the combined results across both models indicate that while increasing CFG can improve alignment to some extent, it often comes at the cost of overall performance. TACA, on the other hand, offers more targeted and effective approach to enhancing text-image alignment, proving beneficial and complementary across different CFG scales and diffusion models. B.2. The content of the prompt We have identified several prevalent issues regarding text alignment in state-of-the-art text-to-image models: Difficulty in handling unrealistic scenarios, such as blue sun and yellow sea. Difficulty in handling spatial relationships, such as with the prompt squirrel to the left of the man. Models frequently interpret the left side of the image as the left side specified in the text, rather than the left side relative to the mans frame of reference within the image. Difficulty in handling specific numerical quantities. For instance, when prompted for four vases, the model may generate images containing five or three vases. B.3. The length of the prompt We also observe that models are more prone to omitting details from longer prompts, particularly when the prompts token count exceeds the maximum token limit supported by the CLIP text encoder. Our proposed TACA method demonstrates comparatively more widespread effectiveness for mitigating the attribute missing issues often found in longer prompt, rather than the shorter ones. Currently, mature benchmark for evaluating the visual-text alignment capabilities of text-toimage models with long prompts is lacking, despite the practical prevalence of longer prompts in real-world applications. Therefore, we have manually curated set of authentic, long prompts from the internet to assess our methods performance, and the corresponding results are presented in the following pages. C. Full parameter fine-tuning vs. LoRA In addition to LoRA training, we also experimented with full parameter fine-tuning as an alternative approach. However, we found that this method required significantly more computational resources and storage, especially for large models like FLUX.1 Dev. Moreover, our experiments revealed that full parameter fine-tuning is highly sensitive to learning rate settings. If the learning rate is set too high, the generated images tend to appear blurry or overly stylized, resembling oil paintings. On the other hand, if the learning rate is too low, the model struggles to learn the original data distribution effectively. These challenges, combined with the lack of superior artifact reduction compared to LoRA, led us to conclude that LoRA training is more robust, efficient, and practical solution. D. More Qualitative Results Figure 9. Visual comparisons on visual-text alignment (FLUX.1 Dev, short prompts) Figure 10. Visual comparisons on visual-text alignment (FLUX.1 Dev, short prompts) Figure 11. Visual comparisons on visual-text alignment (FLUX.1 Dev, short prompts) Figure 12. Visual comparisons on visual-text alignment (FLUX.1 Dev, short prompts) Figure 13. Visual comparisons on visual-text alignment (SD3.5 Medium, short prompts) Figure 14. Visual comparisons on visual-text alignment (SD3.5 Medium, short prompts) Figure 15. Visual comparisons on visual-text alignment (FLUX.1 Dev, long prompts) Figure 16. Visual comparisons on visual-text alignment (FLUX.1 Dev, long prompts) Figure 17. Visual comparisons on visual-text alignment (FLUX.1 Dev, long prompts)"
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Nanjing University",
        "Nanyang Technological University",
        "The University of Hong Kong",
        "University of Chinese Academy of Sciences"
    ]
}