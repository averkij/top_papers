{
    "paper_title": "On Non-interactive Evaluation of Animal Communication Translators",
    "authors": [
        "Orr Paradise",
        "David F. Gruber",
        "Adam Tauman Kalai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "If you had an AI Whale-to-English translator, how could you validate whether or not it is working? Does one need to interact with the animals or rely on grounded observations such as temperature? We provide theoretical and proof-of-concept experimental evidence suggesting that interaction and even observations may not be necessary for sufficiently complex languages. One may be able to evaluate translators solely by their English outputs, offering potential advantages in terms of safety, ethics, and cost. This is an instance of machine translation quality evaluation (MTQE) without any reference translations available. A key challenge is identifying ``hallucinations,'' false translations which may appear fluent and plausible. We propose using segment-by-segment translation together with the classic NLP shuffle test to evaluate translators. The idea is to translate animal communication, turn by turn, and evaluate how often the resulting translations make more sense in order than permuted. Proof-of-concept experiments on data-scarce human languages and constructed languages demonstrate the potential utility of this evaluation methodology. These human-language experiments serve solely to validate our reference-free metric under data scarcity. It is found to correlate highly with a standard evaluation based on reference translations, which are available in our experiments. We also perform a theoretical analysis suggesting that interaction may not be necessary nor efficient in the early stages of learning to translate."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 8 6 7 5 1 . 0 1 5 2 : r ON NON-INTERACTIVE EVALUATION OF ANIMAL COMMUNICATION TRANSLATORS Orr Paradise EPFL, Project CETI orrp@projectceti.org David F. Gruber Project CETI david@projectceti.org Adam Tauman Kalai OpenAI, Project CETI adam@kal.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "If you had an AI Whale-to-English translator, how could you validate whether or not it is working? Does one need to interact with the animals or rely on grounded observations such as temperature? We provide theoretical and proof-of-concept experimental evidence suggesting that interaction and even observations may not be necessary for sufficiently complex languages. One may be able to evaluate translators solely by their English outputs, offering potential advantages in terms of safety, ethics, and cost. This is an instance of machine translation quality evaluation (MTQE) without any reference translations available. key challenge is identifying hallucinations, false translations which may appear fluent and plausible. We propose using segment-by-segment translation together with the classic NLP shuffle test to evaluate translators. The idea is to translate animal communication, turn by turn, and evaluate how often the resulting translations make more sense in order than permuted. Proof-of-concept experiments on data-scarce human languages and constructed languages demonstrate the potential utility of this evaluation methodology. These human-language experiments serve solely to validate our reference-free metric under data scarcity. It is found to correlate highly with standard evaluation based on reference translations, which are available in our experiments. We also perform theoretical analysis suggesting that interaction may not be necessary nor efficient in the early stages of learning to translate."
        },
        {
            "title": "INTRODUCTION",
            "content": "Due to advances in language models (LMs), recent interest has surged in translating non-human animal communication, despite the complete lack of parallel training data (Goldwasser et al., 2023; Rodrıguez-Garavito et al., 2025; Sharma et al., 2024b; Paradise et al., 2025). What type of data and interaction are required to validate translators? Evaluation is crucial component of developing and validating such translators. An evaluation for reference-free translators may also prove useful in training translators, as we analyze. The experiments and analysis in this paper suggest that for complex communication systems, we may be able to evaluate translators from their English outputs without any training data or grounded observations, such as temperature. At first, this seems impossible: candidate English translation may be total hallucination, appearing perfectly fluent. And indeed, LMs sometimes hallucinate entire translations, especially in low-resource languages where there is little training data. Figure 2 illustrates the difficulty of detecting hallucinated translation without reference translation. translator takes as input communication in some format, e.g., raw audio or transcription (Sharma et al., 2024a; Hagiwara et al., 2024), and outputs text in human language.1 Our proposal is to: Segment each source communication by turn, i.e., which animal is vocalizing. Run the translator turn-by-turn. Judge how often the resulting translations make more sense in order than permuted. We refer to this evaluation approach as ShufflEval. It can be viewed as an adaptation of of the classic shuffle test (Barzilay & Lapata, 2008; Laban et al., 2021; Iyyer et al., 2015; Taware et al., 2022), an 1Throughout this work outputs will be in English, though any human language could be used. 1 established approach in NLP, to the problem of Machine Translation Quality Evaluation (MTQE), specifically to Reference Free Quality Evaluation (RFQE) where there are no reference translations for comparison. ShufflEval yields score between 0 and 1, with score above 0.5 (random guessing) being some degree of validation. ShufflEval is conservative in the sense that it may yield score of 0.5 to perfect translator of simple source communication system where different segments have no semantic interdependencies. However, ShufflEval score of greater than 0.5 means that shuffling translated segments decreases coherence, which would not be achieved by pure hallucinated translations. Figure 1 illustrates hypothetical translations of animal communication where shuffle test may succeed or fail, even for perfect translation. Other possible failure modes are discussed in Section 4. Of course, ShufflEval can be applied to any translator, including ones that translate human languages paragraph-by-paragraph. Until recently, it would have been difficult to run ShufflEval without human judges, because older LMs struggled to distinguish the original order of translated paragraphs from permutation (Laban et al., 2021). This is no longer the case with modern reasoning models. We thus validate ShufflEval methodology on languages for which we do have reference translations, including low-resource languages and constructed languages (conlangs). We recognize that real human languages, regardless of data availability, are rich, complex, and valuable (Cooper et al., 2024). Our use here is strictly methodological to test ShufflEval in settings where references exist. We do not draw any conceptual similarity between these languages and animal communication. However, experiments on human language fail to capture the domain gap between animal communication and human communication. To mimic this enormous domain gap, we also perform experiments on ten artificial conlangs, generated using GPT-5 to be quite different from human languages. ShufflEval addresses weakness in prior RFQE techniques which is problematic especially when the source is poorly understoodnamely that hallucinations may score as high as faithful translations (see Figure 2 for an example). Prior RFQE methods often evaluate the coherence and other qualities of the translation, e.g., by simply prompting an LM to score the quality of the English translation alone. Relatedly, there is growing work on hallucination detection for MT using LMs (Benkirane et al., 2024). These hallucinations can be coherent and score high on RFQE because they do not face the faithfulness/coherence tradeoff inherent in translation. ShufflEval complements these other RFQEs in that it is more resilient to hallucinations, but it is not replacement. For example, it may be insensitive to AlTeRnAtInG cAsE, which impedes readability without significantly altering meaning. Thus, ShufflEval can be combined with other RFQE metrics. When ShufflEval is inadequate (which may be especially likely for rudimentary animal signals) one may incorporate observational grounding, such as animal locations, temperature, feeding patterns, and even video. In Section 2.1, we model this with loss ℓ(T, Z) [0, 1] that measures likelihood of translation with respect to grounding Z. Learning may be cast as minimizing the expected loss E[ℓ(f (S), Z)] over translators in some family F. ShufflEval is special case of an observation-free loss (i.e., = ), and when is restricted to segment-based translators. Ethical considerations. Validation in animal studies often relies on interactive methods such as playback experiments (see e.g. Dabelsteen & McGregor 1996), in which sounds are played back to animals. These may cause welfare and ecological harms. This work suggests that playback may be avoided by enabling non-interactive evaluation (see Ethics Statement for discussion). Theoretical analysis. We also present theoretical analysis of the value of interaction in training family of parametrized translators, leveraging the connection between evaluation and training. Theorem 2 argues that noninteractive evaluations use significantly fewer resources than interactive evaluations at the price of slightly worse translations. Quality is measured by general bounded loss function for translations and compares its loss after training using interaction versus from observations alone (without interaction), based on certain quantities. The loss can depend on animal-specific observations, or in the case of the shuffle-eval it uses only temporal grounding and segment-level translators. This analysis goes beyond prior work in the literature (Goldwasser et al., 2023) which analyzes conditions under which unsupervised translation should be possible but does not provide any evaluation methodology. Limitations of this analysis are discussed in Section 4. 2 Whale A: Mother, lets go for dive soon. Whale B: Little follower-of-my-echo, we just filled our bellies. We only surfaced ten breaths ago. Whale A: It was fifty breaths. Whale B: Ten more breaths, and we will. Cow A: Im here. Where are you? Cow B: Im here. Where are you? Cow C: Im hungry. Cow A: Im here. Where are you? Cow B: Im here. Where are you? Figure 1: Hypothetical translations which the shuffle test should succeed (left, message permutations are less plausible than the original order) and fail (right). Source (in Santali): Translation A: The Sarna-Liturgical Kanthi (rosary) of the Sarna faith, the independent religious system of the indigenous people, and the sacred text of Sarna-Liturgical came to light in the light of detailed research, conversation, and meticulous efforts. According to the 2011 census, Sarna is believer of 46,73,848 people in the country, and he is recognized as a. . . Translation B: PatnaDigha Ghat line is railway line between Patna Junction and Digha Ghat railway stations in the city of Patna, Bihar. It was built by the British in 1862, and later trains were operated on it. In 196263 and again in 2004 the Railway Ministry, and Lalu Prasad Yadav, took steps regarding this line. However, regular passenger train services. . . Translation C: The Wild Animals and Their Association By the tender age of four or five, when our bodies are neither weak nor strong, the adivasi children probably learned to live with bravery and dignity without even feeling it. Indeed, we can suspect they gain strength from the innocent aspects of wild animals and perhaps have. . . Figure 2: Which translation is not hallucination? Without reference translation, one cannot distinguish faithful translations from hallucinations. See Section E.1 for the answer."
        },
        {
            "title": "2 THEORETICAL ANALYSIS",
            "content": "We now present an analysis suggesting that in the high-error regime (i.e., for translators with relatively low accuracy), observations may be more cost effective than interactive experiments. It is instructive to consider interaction in supervised learning and how it relates to unsupervised machine translation. We defer this comparison the the appendix, Section C. 2.1 TRANSLATOR AND LOSS DEFINITIONS We use log2 (resp. ln) to denote logarithms in base 2 (resp. e). Let be family of translators : from sources to targets . This paper is motivated by the setting that consists of animal communication and is English translations, but our theory holds generally. For simplicity, we assume is finite; then, informally, the number of essential parameters is proportional to log2 F, i.e., the number of bits required to represent translator.2 There is an arbitrary joint distribution (S, Z) over sources and observations Z, e.g., geoposition or water temperature measurements. Performance is evaluated by bounded loss ℓ(T, Z) [0, 1] measuring how unlikely translation is with respect to observation Z. The optimal loss for translator family is opt := min ℓD(f ) where ℓD(f ) := (S,Z)D (cid:2)ℓ(f (S), Z)(cid:3). (1) We assume that the per-sample loss ℓ(, ) can be efficiently evaluated, and therefore gives an actionable metric for measuring translation quality. 2In practice, many classes are over-parametrized meaning that smaller models (e.g., sparsified models or families of smaller size) can achieve similar performance. For simplicity of analysis, we assume that model family is already compressed. 3 Our analysis proceeds in two parts. In Section 2.2 we show how standard learning-theoretical scaling law can be applied to translation from observations. Section 2.3 compares this analysis to whalebreak scaling law for learning with interaction, revealing that in the low-accuracy regime, observations may suffice for learningand at cheaper monetary and ethical cost."
        },
        {
            "title": "2.2 LEARNING TO TRANSLATE FROM OBSERVATIONS",
            "content": "Our notion of an accurate translator is based on low average loss ℓ(T, Z) [0, 1] which can be evaluated efficiently on any translation and whatever observations are available, if any. We consider the translator learning algorithm which selects the translator ˆf (i.e., chooses its parameters) so as to minimize the average empirical loss on training data Si, Zim i=1, i.e., ˆf := argmin ˆℓ(f ), where ˆℓ(f ) := 1 m (cid:88) i=1 ℓ(f (Si), Zi). (2) In the case of multiple translators with equal average empirical loss, we assume some arbitrary tiebreaking procedure. Of course this algorithm is computationally infeasible, but this approach (often called Empirical Risk Minimization (Kearns & Vazirani, 1994)) is common in statistical learning theory. We now observe that ℓD( ˆf ) converges at rate of O(cid:0)(cid:113) 1 compressed model bit-size, proportional to the number of essential parameters. Theorem 1 (Observational scaling law). Let be finite set of translators : , be an arbitrary distribution over Z, δ (0, 1), and ℓ : [0, 1] be bounded loss. With probability 1δ over i.i.d. samples (Si, Zi) i.i.d. D, the translator ˆf which minimizes training loss satisfies, log F(cid:1) where again log2 is the ℓD( ˆf ) min ℓD(f ) + 2 ln(F/δ) , (3) (cid:114) where ˆf is defined in Eq. (2) and ℓD is defined in Eq. (1). Setting δ = 0.01, this means that with probability 99% minimizing training loss yields ˆf with true (population) risk ℓD( ˆf ) minf ℓD(f ) + . All proofs are deferred to Section F. (cid:113) 10+2 ln 2.3 COMPARING OBSERVATIONAL AND INTERACTIVE LEARNING To compare observational learning to interactive learning, we consider simple optimistic model of interactive experiments. Suppose that interactive experiments are used to find the best classifier Fn in terms of minimal expected loss optn := minf Fn ℓD(f ). With more experiments n, one may be able to learn larger families Fn, hence the translator family grows with n. We call this the whalebreak model, alluding to jailbreaks which remove restrictions on language models, sometimes revealing internal details word-for-word such as system messages (Zhang et al., 2023). Why is this an optimistic model? Let = 1 log2 Fn. Since log2 Fn is the number of bits required to encode translator, can be interpreted as the average number of parameter bits learned per experiment. In the extreme case of binary search, where each experiment rules out half of the remaining hypotheses, = 1. In noisy or less structured interactive settings, one expects 1. Thus the whalebreak model effectively grants interactive experiments maximal information efficiency, as though each query reveals the next bit of the optimal translator. This framing is deliberate: as we show next, observational data can in fact be competitive with interaction even under this generous assumptionand at fraction of the cost. Hence, the case for observational data is only stronger in more realistic scenarios. Setup: Experiment costs and budget reduction. The above stylized model enables simple comparison to observational learning. The key assumption we need is that observations are less expensive in terms of the relevant cost, which we treat as an abstract quantity. Cost can capture monetary expense, but also the time and effort of human participants, or even the ethical burden of interfering with wildlife (as is sometimes the case in interactive playback experiments). Formally, 4 we assume that observing fresh iid sample (S, Z) costs at most an ε-fraction of the price of conducting single interactive experiment, for some cost ratio ε (0, 1). The goal is reduced total budget: we allow the observational learner to spend only 1/c-fraction of the interactive cost, for some budget reduction factor > 1. Corollary 2. For any cost ratio ε (0, 1) and budget reduction factor > 1, suppose that interactive experiments identify translator of minimal loss optn := minf Fn ℓD(f ). Then, at only 1/c-fraction of the interactive cost, the translator learned from observations ˆfn := argminf Fn ˆℓ(f ) will, with probability at least 0.99, satisfy ℓD( ˆfn) ℓD(f n) + where := 1 log Fn. (cid:113) εc (cid:0) 3b 2 + 10 (cid:1), See Section for the proof. If, as in the previous section, we assume log2 Fn n, then setting = 4 shows that at just one quarter of the interactive budget, the observational model achieves loss within (cid:112)ε (6 + 40/n) of the interactive model (with 99% likelihood). For example, when ε = 1/2400, this gap is at most 6ε 0.05. In the current state of animal communication research, even coarse-grained understanding remains elusive, so the relevant regime is one of relatively large losses. In such regime, the bound indicates that observational data can be nearly as effective as interactive experiments, while incurring only fraction of the cost. If, at some future stage, accuracies above (say) 0.9 become achievable, then the tradeoff may shift and interactive experiments could become valuable. But at present, when even rough translation is beyond our reach, the analysis supports the view that observational approaches are both cost-effective and scientifically sufficient. 2.4 IMPLICATIONS FOR SHUFFLEVAL Theorem 2 implies that, in theory, ShufflEval can be used to non-trivially evaluate translator quality (e.g. as compared to the supervised or even interactive settings). Before we validate this theory in Section 3, it may be enlightening to elaborate this implication in more formal detail. The starting point of ShufflEval is segment-by-segment translator. To capture this formally, we assume that each source communication partitioned into 2 segments = s1s2 sk. As explained in the introduction, these segments may correspond to different turns in dialogue, or even simply to temporal partition of an audio recording. ShufflEval evaluates segment-by-segment translators. Therefore, we assume that all translators satisfy (s1 sk) = φ(s1) φ(sk) for some segment-translator φ : (cid:55) t. To avoid cumbersome notation, we will refer only to the translator from here onwards. Furthermore, we assume without loss of generality that any target communication is segmented into = t1 tk (e.g. by inserting special symbol between each segment in the translation (S)). The final component of ShufflEval is plausibility (pairwise-preference) model ρ : {0, 1}; here ρ(T, ) = 1 indicates that is more plausible than , and ρ(T, ) indicates is more plausible than . (In Section 3 we show how such ρ can be obtained from an LLM.) The ShufflEval loss of translation = t1 tk is defined to be ℓShufflEval(T ) := 1 k! 1 (cid:88) ρ (cid:0)t1 tk, tπ(1) tπ(k) (cid:1) , πΠk{Id} where Πk denotes the set of all permutations π : [k] [k], and Id is the identity function. Put simply, ShufflEval measures whether permuting the outputs of the translator affect plausibility; intuitively, assuming that order matters in the source, then an accurate translator should output translations whose internal order matters as well, i.e., ρ(t1 tk, tπ(1) tπ(k)) = 0 for most π. We note that computing ShufflEval naively requires exponentially (in k) many invocations of the plausibility model ρ(, ). In practice, for large the loss can be approximated by sampling random permutations π Πk and reporting the empirical mean (or using importance sampling). ShufflEval is special case of Theorem 2, by taking ℓ(T, Z) = ℓShufflEval(T ). At first glance, it may seem that ShufflEval does not depend on any grounding (observations Z). However, there is an implicit common ground: time. The sequential representation which imposes an order on segments 5 ShufflEval prompt We have (possibly poor) English translation of source_description, broken into segments. To make matters worse, we are not certain what order the segments should be in. Below are two orderings of the segments. Decide which ordering reads more natural and coherent. Reply with '1' or '2' only. <ORDERING1> text1 </ORDERING1> <ORDERING2> text2 </ORDERING2> Figure 3: Our template for ShufflEval. Either text1 or text2 is permuted and the other is in the original order. Each of ten permutations is run twice, swapping order to account for order bias. implicitly assumes temporal order to the communication, so that it can be partitioned in way that can be translated segment-by-segment. This would not hold if there was no common linear order between the source and the translation (e.g. images). Put another way, it is not clear how to apply ShufflEval to image captioning, but it could be applied to describing videos. Augmenting ShufflEval to combine both temporal consistency and observations is deferred to future work."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We validated the shuffle test as viable RFQE signal, evaluating fifteen LMs from the OpenAI API as translators of conlangs and low-resource Wikipedias. The target language for all these experiments is English. We use LMs for several purposes (increasingly common practices in MT Bavaresco et al. 2025) including: Translators. We used fifteen LMs accessed via the OpenAI API as translators. The prompt for translating source is shown in Figure 8. The LMs are numbered and listed in Figure 5. ShufflEval judge. GPT-5 (LM 15) was used to judge which of two orderings is more plausible, with the prompt of Figure 3, for reasons discussed below. Baseline: reference-based MTQE judge. GPT-4 (LM 4) has been repeatedly found to be consistent with human ratings of translation quality, with and without reference translations (e.g., Kocmi & Federmann, 2023; Liu et al., 2023; Jiang et al., 2024). We use GPT-4 with the prompt of Figure 4 to align with these prior studies. Conlang creation. GPT-5 is used to generate conlangs, as described in Section 3.3. The LMs used are associated with numbers 1-15 based on their performance on the ground-truth shuffle test of Figure 5, which is not based on translation. All LMs were used with their default OpenAI API parameters. Code and data will be made available upon publication. 3.1 SHUFFLEVAL For translation, we first independently translate the source segment-by-segment (paragraphs for Wikipedia articles, sentences for conlangs, and animal speaker turns might be used). We then perform pairwise comparisons using the prompt of Figure 3. To account for order bias, each comparison is performed twice with the two options swapped, and the results averaged. We average over ten random permutations, thus using twenty evaluations per (segment-level) translation. 6 Reference-based baseline prompt Score the following translation on continuous scale 0 to 100 where score of zero means \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\". <HUMAN_REFERENCE> target </HUMAN_REFERENCE> <MACHINE_TRANSLATION> translation </MACHINE_TRANSLATION> Just output an integer score between 0 and 100, inclusive, and nothing else. Figure 4: Prompt for judging translation quality with respect to reference, closely following GEMBA-DA ref from Kocmi & Federmann (2023) except without the source text. As discussed, prior work has validated the shuffle test over sentences as measure of coherence, finding that models specifically trained for the shuffle test we highly accurate at the sentence level, but struggled at longer segments such as paragraphs (Laban et al., 2021). Choosing judge LM. To compare the model performance on ShufflEval, we performed an experiment involving no translation. We took the paragraphs of English text and tested the models ability to distinguish the original order from permutations. The text comprised the first six paragraphs of 100 English Wikipedia articles (the same 100 English Wikipedia reference articles used in our low-resource experiments, described below). LMs were evaluated on accuracy at identifying the correct order the paragraph versus ten random alternatives. As can be seen in Figure 5, newer models have better accuracy, with the earliest model, GPT-3.5 having only 61% accuracy, while model the latest model, GPT-5, achieved 96% accuracy. Random guessing would yield 50% accuracy. Order bias was present in several models, with models 2, 3, and 4 exhibiting preference for the first answer 60-70% of the time. The gpt-4.1-nano -2025-04-14 model exhibited 99.9% order bias and was thus excluded from experiments. 3.2 LOW-RESOURCE HUMAN LANGUAGES (PROXY EXPERIMENTS) We create dataset from Wikipedia of 200 articles in data-scarce languages and their English versions, comprising 10 articles from each of 10 source languages and their English Wikipedia versions. Sources. Ten low-resource languages were selected by sorting the Wikipedias by how many entries they have, and from each taking ten articles that met the following criteria: (a) the article had at least 3,000 characters, (b) it was created after cutoff date of June 1, 2024, and (c) it had an English version in Wikipedia. Languages which did not have 10 articles of this form were excluded. The English version was considered the reference-translation, which is limitation discussed in Section 4. The ten languages can be seen in Figure 6 (right). Articles were split into segments that were single paragraphs using GPT-4o. We took the first six paragraphs from each article. Limiting the number of paragraphs increased the alignment of the source and English versions of the articles, since the articles are not direct translations of each other. Translations were performed one segment at time, and the ShufflEval and baseline referencebased MQTE were both run as described above. We computed the correlation (Pearson correlation coefficient, in [1, 1]) between the ShufflEval and the reference based score. Across the full 15 100 = 1500 pairs of articles and translators, the correlation was 0.54 (0.04 for 95% bootstrapped confidence interval) which is significantly positive. However, Figure 6 shows that when aggregated across models (left) and languages (right), ShufflEval is much more highly correlated. That is, we compute per-language and per-LM scores by averaging the two scores over those lan7 Figure 5: Without translation, accuracy at distinguishing the original paragraph order from permutation. The fifteen LMs evaluated ranged from GPT-3.5-turbo (LM #1) to GPT-5 (LM #15). Figure 6: Comparing the reference-free ShufflEval (x-axis) to the reference-based baseline translation scores, averaged across LM (left) and language (right). On the left, LM numbers are taken from Figure 5 with 1 being GPT-3.5-turbo and 15 being GPT-5. Figure 7: For conlangs, the reference-based baseline translation scores versus the reference-free ShufflEval, aggregated by LM (left) and language (right). LMs 1 and 4 were excluded due to inadequate context length. 8 guages and LMs. One possible explanation for why the aggregate correlations are so much higher than per-article correlations is that the aggregate measure washes out multiple types of noise in the experiment. First, English versions of articles are sometimes not actual translations of the source document. Second, the baseline measure may be noisy and ShufflEval is run only with 10 permutations which introduces certain amount of noise. Hallucination. Translating an entire article, not in segments, runs the risk of hallucination. The common approach of direct RFQE without the shuffle test (e.g., Kocmi & Federmann, 2023) may evaluate model that hallucinates fluently higher than model that is better translating (Zhang et al., 2024). Section demonstrates this phenomenon in Wikipedia data."
        },
        {
            "title": "3.3 CONSTRUCTED LANGUAGES",
            "content": "To stress-test ShufflEval beyond human languages, we ventured into constructed languages (conlangs) which permit the design of languages quite different from human languages. For example, the conlang Kelen, spoken by the fictitious Keleni on the planet Terjemar, has no verbs (Sotomayor, 1998). We used GPT-5 to construct ten diverse conlangs. These artificial conlangs were specifically synthesized to be quite different from human text, leveraging GPT-5s extensive knowledge of language and conlangs. Interestingly, in recent independent work, Alper et al. (2025) also constructed conlangs using LMs. Each conlang includes conculture, detailed conlang definition, and ten sources alongside reference English translations. The creation pipeline is given in Section D.1, and summary of the ten conlangs is in Table 1. While admittedly unrealistic, these conlangs do serve as tests where the domain gap between these conlangs and English is large, like animal communication. The same experimental design was applied to both the Wikipedia and conlang settings, with two differences. First, the conlang segments were sentences rather than paragraphs, which offers test of the ShufflEval at different segmentation granularity. Second, in order to translate the conlang text, the conlang and conculture definitions were provided in the LM prompt, as seen in Figure 8 (bottom). Figure 7 shows the results comparing the baseline and ShufflEval on artificial conlangs. Discussion. Across both Wikipedia articles in low-resource languages and artificial conlangs, there was significant correlation between reference-based evaluations and ShufflEvals ranking of translators and languages. Of these comparisons, the translator scores (Wikipedia correlation 0.96, conlang correlation 0.78) reflects ShufflEvals potential to rank translators, which is relevant if one were to use such an metric during training. The language scores (Wikipedia correlation 0.86, conlang correlation 0.94) are more relevant to validation, where one may wish to compare the translations of an unknown animal communication system to other (possibly known) languages."
        },
        {
            "title": "4 LIMITATIONS AND CONCLUSIONS",
            "content": "ShufflEval is far from perfect. First, it requires segmentation and translation of individual segments to be possible. Second, one can conceive of imperfect translators to which ShufflEval gives perfect score, e.g., if segments started with increasing numbers, then translator which only translates these numbers may be scored perfectly. Fortunately, such flaw may readily be discovered by examining the target translations. Wikipedia articles in different languages are not direct translations of one another. However, nontranslation references may be expected to decrease the correlation between ShufflEval scores and those determined using references. Thus, the strong correlations still serve as validation despite this issue. The theoretical analysis also has several caveats. Exceptions to Theorem 2, where interaction may be particularly beneficial, include computational complexity, fine-grained delineations, outof-distribution content, and counterfactuals. Despite these limitations, this work presents new approach to validate animal translation that benefits from less interaction, and opens the door to future improved tests. 9 Ethics statement. Better methods for understanding animal communication may have significant impact for science and conservation efforts. ShufflEval offers safety, ethics, and efficiency advantages over interactive evaluation; in the context of animal communication, interaction may come in the form of playback experiment (see, e.g. Dabelsteen & McGregor 1996), in which sounds are played back to animals. Playback experiments raise ethical concerns because artificial signals can aggravate animals, trigger defensive or aggressive responses, and distort natural behavior. Across taxa, predator or conspecific-sound playbacks have disrupted vital activitiese.g., adult male sperm whales aborted foraging/resting dives and clustered socially in response to killer-whale calls (Cure et al., 2013), toadfish suppressed calling and showed elevated cortisol after dolphin-sound exposure (Remage-Healey et al., 2006), and chronic predator-noise exposure reduced song sparrow reproductive output by 40% via fear-mediated effects (Zanette et al., 2011). Playbacks can also create behaviors rather than merely reveal them, as daily affiliative call playbacks in marmosets transiently imposed high-affiliation cultural style on groups (Watson et al., 2014), and the impacts of playback can persist for years or even lifetime (Onate-Casado et al., 2021). Given these welfare risks and ecological costs, we view the main ethical contribution of our work as demonstrating path toward evaluating animal communication translators without playback. We also recognize that many low-resource human languages are associated with marginalized communities. Our use of these languages is strictly methodological: they provide controlled setting where ground-truth translations exist, allowing us to compare our reference-free metric against established evaluation methods. Low-resource languages were used in this paper because they are (by definition) data-scarceand often underrepresented in the large language models used in our experimentsyet still come with parallel corpora that enables comparison to the gold-standard evaluation method (which uses references). Importantly, our use does not imply any similarity between real human languages and animal communication. We make this clear in the paper both explicitly (by stating their proxy role) and implicitly (by leading with constructed-language evaluations). Reproducibility statement. All code and prompts needed to reproduce our results will be made available upon publication. The reproducibility relies on the availability of the API, however similar experiments may be run using our code on other reasoning LMs. In addition to the code, the experimental setup is described in Section 3 and in the Appendix. Finally, the formal specifications and necessary assumptions for the theoretical analysis are detailed in Section 2. Acknowledgements. This study was funded by Project CETI via grants from Dalio Philanthropies and Ocean X; Sea Grape Foundation; Virgin Unite and Rosamund Zander/Hansjorg Wyss through The Audacious Project: collaborative funding initiative housed at TED."
        },
        {
            "title": "REFERENCES",
            "content": "Martın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: system for In Proceedings of the 12th USENIX Symposium on Operating large-scale machine learning. Systems Design and Implementation (OSDI 16), pp. 265283, Savannah, GA, USA, November 2016. USENIX Association. URL https://www.usenix.org/conference/osdi16/ technical-sessions/presentation/abadi. Morris Alper, Moran Yanuka, Raja Giryes, and Gaˇsper Beguˇs. Conlangcrafter: Constructing languages with multi-hop llm pipeline, 2025. URL https://arxiv.org/abs/2508. 06094. Regina Barzilay and Mirella Lapata. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):134, 2008. doi: 10.1162/coli.2008.34.1.1. URL https: //aclanthology.org/J08-1001/. Regina Barzilay and Lillian Lee. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Julia Hirschberg, Susan T. Dumais, Daniel Marcu, and Salim Roukos (eds.), Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, HLT-NAACL 2004, Boston, Massachusetts, USA, May 10 2-7, 2004, pp. 113120. The Association for Computational Linguistics, 2004. URL https: //aclanthology.org/N04-1015/. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernandez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andre F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya Surikuchi, Ece Takmaz, and Alberto Testoni. LLMs instead of In Proceedhuman judges? large scale empirical study across 20 NLP evaluation tasks. ings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 238255, Vienna, Austria, July 2025. Association for Computational Linguistics. doi: 10.18653/v1/2025.acl-short.20. URL https://aclanthology.org/2025. acl-short.20/. Kenza Benkirane, Laura Gongas, Shahar Pelles, Naomi Fuchs, Joshua Darmon, Pontus Stenetorp, David Ifeoluwa Adelani, and Eduardo Sanchez. Machine translation hallucination detection for low and high resource languages using large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 96479665, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.564. URL https://aclanthology.org/2024.findings-emnlp.564. Ned Cooper, Courtney Heldreth, and Ben Hutchinson. Its how you do things that matters: Attending to process to better serve indigenous communities with language technologies. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 204211, St. Julians, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. eacl-short.19. URL https://aclanthology.org/2024.eacl-short.19/. Charlotte Cure, Ricardo Antunes, Ana Catarina Alves, Fleur Visser, Petter H. Kvadsheim, and Patrick J. O. Miller. Responses of male sperm whales (Physeter macrocephalus) to killer whale sounds: implications for anti-predator strategies. Scientific Reports, 3(1579), 2013. doi: 10.1038/srep01579. Torben Dabelsteen and Peter K. McGregor. Dynamic acoustic communication and interactive playback. In Donald E. Kroodsma and Edward H. Miller (eds.), Ecology and Evolution of Acoustic Communication in Birds, pp. 398408. Cornell University Press, Ithaca, NY, 1996. ISBN 9781501736957. doi: 10.7591/9781501736957-031. URL https://doi.org/10.7591/ 9781501736957-031. Shafi Goldwasser, David F. Gruber, Adam Tauman Kalai, and Orr Paradise. theory of In Advances unsupervised translation motivated by understanding animal communication. in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 7571c9d44179c7988178593c5b62a9b6-Abstract-Conference.html. Masato Hagiwara, Marius Miron, and Jen-Yu Liu. ISPA: inter-species phonetic alphabet for tranIn IEEE International Conference on Acoustics, Speech, and Signal scribing animal sounds. Processing, ICASSP 2024 - Workshops, Seoul, Republic of Korea, April 14-19, 2024, pp. 828 832. IEEE, 2024. doi: 10.1109/ICASSPW62465.2024.10669911. URL https://doi.org/ 10.1109/ICASSPW62465.2024.10669911. Steve Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Carnegie Mellon University, 2009. Technical Report, Carnegie Mellon University. Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in Machine Learning, 7(23):131309, 2014. doi: 10.1561/2200000037. Steve Hanneke and Liu Yang. Minimax analysis of active learning. J. Mach. Learn. Res., 16: 34873602, 2015. doi: 10.5555/2789272.2912111. URL https://dl.acm.org/doi/10. 5555/2789272.2912111. 11 Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155, Jan 2025. doi: 10.1145/3703155. URL https://doi. org/10.1145/3703155. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daume III. Deep unordered comIn Proceedings of the 53rd Annual position rivals syntactic methods for text classification. Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 16811691, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1162. URL https://aclanthology.org/P15-1162. Lili Jiang, Yunxiao Jiang, and Lili Han. The potential of chatgpt in translation evaluation: case study of the chinese-portuguese machine translation. Cadernos de Traduc ao, 44(1):e98613, 2024. Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA, USA, 1994. ISBN 9780262111935. Yunsu Kim, Miguel Graca, and Hermann Ney. When and why is unsupervised neural machine translation useless? In Mikel L. Forcada, Andre Martins, Helena Moniz, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof Arenas, Mary Nurminen, Lena Marg, Sara Fumega, Bruno Martins, Fernando Batista, Luısa Coheur, Carla Parra Escartın, and Isabel Trancoso (eds.), Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, EAMT 2020, Lisboa, Portugal, November 3-5, 2020, pp. 3544. European Association for Machine Translation, 2020. URL https://aclanthology.org/2020.eamt-1.5/. Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pp. 193203, Tampere, Finland, June 2023. European Association for Machine Translation. URL https://aclanthology.org/2023.eamt-1.19/. Philippe Laban, Luke Dai, Lucas Bandarkar, and Marti A. Hearst. Can transformer models meaIn Proceedings of the 59th Annual Meetsure coherence in text: Re-thinking the shuffle test. ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 10581064, Online, aug 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.134. URL https://aclanthology.org/2021.acl-short.134. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato. Unsupervised machine translation using monolingual corpora only. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= rkYTTf-AZ. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL https://aclanthology.org/2023. emnlp-main.153/. Javier Onate-Casado, Michal Porteˇs, Vaclav Beran, Adam Petrusek, and Tereza Petruskova. An experience to remember: lifelong effects of playback-based trapping on behaviour of migratory passerine bird. Animal Behaviour, 182:1929, 2021. doi: 10.1016/j.anbehav.2021.09.010. O. Paradise, L. Chen, P. Muralikrishnan, H. Flores, B. Pardo, R. Diamant, D. F. Gruber, S. Gero, and S. Goldwasser. Towards translative model of Sperm Whale vocalization. In Advances in Neural Information Processing Systems (NeurIPS), NeurIPS, 2025. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, 12 and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Core Team. R: Language and Environment for Statistical Computing. Foundation for Statistical Computing, Vienna, Austria, 2024. URL https://www.R-project.org/. Sujith Ravi and Kevin Knight. Deciphering foreign language. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pp. 1221. The Association for Computer Linguistics, 2011. URL https://aclanthology.org/P11-1002/. Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. COMET: neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 26852702. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.213. URL https://doi.org/10.18653/v1/2020. emnlp-main.213. Ricardo Rei, Marcos V. Treviso, Nuno Miguel Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, Jose G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, Luısa Coheur, Alon Lavie, and Andre F. T. Martins. Cometkiwi: Ist-unbabel 2022 submission for the quality esIn Philipp Koehn, Loıc Barrault, Ondrej Bojar, Fethi Bougares, Ratimation shared task. jen Chatterjee, Marta R. Costa-juss`a, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Tom Kocmi, Andre F. T. Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aurelie Neveol, Mariana Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.), Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, pp. 634645. Association for Computational Linguistics, 2022. URL https: //aclanthology.org/2022.wmt-1.60. Luke Remage-Healey, Douglas P. Nowacek, and Andrew H. Bass. Dolphin foraging sounds suppress calling and elevate stress hormone levels in prey species, the gulf toadfish. Journal of Experimental Biology, 209(22):44444451, 2006. doi: 10.1242/jeb.02525. Cesar Rodrıguez-Garavito, David F. Gruber, Ashley Otilia Nemeth, and Gaˇsper Beguˇs. What if we understood what animals are saying? the legal impact of AI-assisted studies of animal communication. Ecology Law Quarterly, 52(1), 2025. doi: 10.15779/Z383X83N5Q. URL https://doi.org/10.15779/Z383X83N5Q. Forthcoming, Fall 2025. Gozde Gul ahin, Yova Kementchedjhieva, Phillip Rust, and Iryna Gurevych. PuzzLing Machines: Challenge on Learning From Small Data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 12411254, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.115. URL https://aclanthology.org/2020. acl-main.115/. Pratyusha Sharma, Shane Gero, Roger Payne, David Gruber, Daniela Rus, Antonio Torralba, and Jacob Andreas. Contextual and combinatorial structure in sperm whale vocalisations. Nature Communications, 15(1):3617, 2024a. Pratyusha Sharma, Shane Gero, Daniela Rus, Antonio Torralba, and Jacob Andreas. Whalelm: Finding structure and information in sperm whale vocalizations and behavior with machine learning. bioRxiv, 2024b. doi: 10.1101/2024.10.31.621071. URL https://www.biorxiv.org/ content/early/2024/11/11/2024.10.31.621071. Sylvia Sotomayor. An introduction to kelen, 1998. URL https://www.terjemar.net/ kelen.php. Official site of the Kelen language, accessed 2025-09-23. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Trans. Mach. Learn. Res., 2023, 2023. URL https://openreview.net/forum?id=uyTL5Bvosj. 13 Rutuja Taware, Shraddha Varat, Gaurav Salunke, Chaitanya Gawande, Geetanjali Kale, Rahul Khengare, and Raviraj Joshi. Shuftext: simple black box approach to evaluate the fragility of In Machine Learning, Optimization, and Data Science 7th Intertext classification models. national Conference, LOD 2021, Revised Selected Papers, Part I, volume 13163 of Lecture Notes in Computer Science, pp. 235249. Springer, 2022. doi: 10.1007/978-3-030-95467-3 18. URL https://arxiv.org/abs/2102.00238. Claire F. I. Watson, Hannah M. Buchanan-Smith, and Christine A. Caldwell. Call playback artificially generates temporary cultural style of high affiliation in marmosets. Animal Behaviour, 93:163171, 2014. doi: 10.1016/j.anbehav.2014.04.027. Liana Y. Zanette, Aija F. White, Marek C. Allen, and Michael Clinchy. Perceived predation risk reduces the number of offspring songbirds produce per year. Science, 334(6061):13981401, 2011. doi: 10.1126/science.1210908. Chen Zhang, Xiao Liu, Jiuheng Lin, and Yansong Feng. Teaching large language models an unIn Findings of the Association for Computational Linguistics: ACL seen language on the fly. 2024, pp. 87838800, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.519. URL https://aclanthology.org/2024. findings-acl.519/. Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. Effective prompt extraction from language models. arXiv preprint arXiv:2307.06865, 2023. URL https://arxiv.org/abs/2307. 06865."
        },
        {
            "title": "A USE OF LMS IN RESEARCH AND WRITING",
            "content": "Large language models were used extensively but carefully to aid in coding and writing the paper. LM suggestions were carefully reviewed, and detected errors were corrected. The LM outputs in the experiments, however, were analyzed numerically and reviewed mostly by spot checking. The authors examined conlang sources and translations and found that the Talhi conlang had source texts which included the translations within them, which were removed programmatically."
        },
        {
            "title": "B RELATED WORK",
            "content": "The evaluation of translation quality without reference translations touches on several distinct research areas. While the literature is vast, we focus here on the most directly relevant connections to ShufflEval and its analysis. Local coherence in NLP. Works as early as that of Barzilay & Lee (2004) used random permutations of text as method for empirically evaluating content (rather than translation) models. ShufflEval can be viewed as an adaptation of local coherence approaches in NLP, specifically the Shuffle Test introduced by Barzilay & Lapata (2008). Laban et al. (2021) demonstrated that supervised finetuning of transformer can lead to near-perfect accuracy on the task, but performance drops significantly as the size of shuffled blocks increases. Reference-Free Machine Translation Quality Evaluation Reference-free evaluation has become increasingly critical for scenarios where obtaining references is impossible or expensive. Particularly notable is the reference-variant of COMET (Rei et al., 2022; 2020). Active Learning Theory Active learning theory reveals when interaction may not be necessary for learning. As surveyed by Hanneke (2014), active learning can achieve exponential improvements over passive learning under favorable conditionsspecifically when complexity measures like the star number are finite (Hanneke & Yang, 2015). However, in high-noise regimes or for classes with infinite complexity, active learning may provide no advantage over passive learning. This theoretical insight suggests that for animal communication translation problems, passive evaluation through coherence assessment might suffice without requiring interactive feedback, particularly in early learning stages where noise levels are high. Our theory in Section 2 corroborates this intuition. Unsupervised Translation and Reference-Free Evaluation Unsupervised machine translation (Ravi & Knight, 2011) learns translators using only monolingual corpora from each language. While neural approaches have shown promise (Lample et al., 2018), empirical evaluations reveal that UMT is outperformed by supervised methods even with orders of magnitude more data (Kim et al., 2020). The key barriers identified include domain and data gaps. Recent theoretical work (Goldwasser et al., 2023) analyzes conditions under which UMT is possible without parallel data or shared domains, showing that translation feasibility depends on language complexity and common ground suggesting animal communication translation may be possible if the system is sufficiently complex. Crucially, such setting inherently requires reference-free evaluation, as parallel data is unavailable by definition. Our work complements this theoretical foundation by addressing the evaluation challenge: while Goldwasser et al. (2023) establishes when translation is possible, we provide method to assess translation quality without referencesa necessity for both unsupervised MT development and scenarios like hypothetical whale-to-English translation where references cannot exist. Constructed language for controlled experiments. As mentioned, in independent recent work, Alper et al. (2025) constructed conlangs using large language models. They also use pipeline to construct languages, though their pipeline is designed to create more linguistically plausible languages involving consonants and vowels, phonology, morphology, and grammar rules. As result, one might expect our conlangs to be less human-like which serves the purpose of stress-testing ShuffleEval beyond human languages. The widely popular BIG-bench (Srivastava et al., 2023) uses constructed language translation as one of its benchmarks, with similar puzzles appearing beforehand in NLP literature (S ahin et al., 2020)."
        },
        {
            "title": "C INTERACTION IN SUPERVISED LEARNING",
            "content": "Interaction in machine learning has been extensively studied in supervised learning, particularly binary classification. It provides helpful prelude to our analysis of interaction in translation. In standard binary classification, the goal is to learn binary classifier : {, +} using labeled samples (xi, yi = c(xi)) for iid xi D. Active learning adds interaction by allowing the learner to arbitrarily choose specific examples {xj}j and see their labels c(xj)importantly, here the xj need not be distributed iid from D.3 In theory, active learning can provide exponential advantages for learning restricted families of binary classifiers in noise-free settings, though these benefits typically do not extend to higher-dimensional or noisy problems (Hanneke, 2009). Binary search example. The classic example is learning binary threshold in one dimension, e.g., what counts as warm versus cold from examples of temperatures labeled based on whether the temperature > is greater than some threshold v. Active learning enables one to select examples using binary search. For example, if 30 is labeled as + and 10 is labeled as , one queries 20, and if it is, say +, one then queries 25, and so forth. Binary search rapidly achieves exponential precision in the threshold v. On the other hand, given random labeled temperature samples (xi, yi) where yi = sgn(xi v), it is not difficult to see that one can predict on future temperatures with error rate 1/m (the probability that new example is closer to the threshold than any of the training examples). Excitement from this exponential statistical savings was tempered by the realization that the savings does not generalize even to two-dimensional linear thresholds (Hanneke, 2009). Moreover, active learning has not proven popular in practice, e.g., common machine learning and statistical frameworks such as scikit-learn (Pedregosa et al., 2011), (R Core Team, 2024), and TensorFlow (Abadi et al., 2016) do not have active learning modules. One might hope to apply these active learning insights to translation by reducing it to binary classification: classify examples = (S, z) as positive when grounding data is consistent with source and negative otherwise. However, while source-grounding pairs from translation training data provide natural positive examples, its unclear how to construct meaningful negative exampleswhat would constitute grounding data thats inconsistent with source? This difficulty suggests that translation requires its own theoretical framework, which we develop in Section 2."
        },
        {
            "title": "D FURTHER EXPERIMENTAL DETAILS",
            "content": "D.1 CREATING ARTIFICIAL CONLANGS Ten artificial conlangs were generated through series of prompts to GPT-5 (LM 15). Our conlangs are summarized in Table 1. We now describe the generation process. An initial attempt to generate conlang in single LM call failed in two ways: (1) the languages were too similar and lacked diversity, and (2) the descriptions were very short and incomplete. Therefore, we adopted multistage generation pipeline. In the first step, we elicit names for the conlangs, the species that speak them, the planets on which they live, as well as an unexpected property of the species and their communication. See Figure 9 (top). This encourages diverse set of languages that differ significantly from human language. Ideating ten ideas in single prompt led to more diversity than making ten different calls to the GPT-5. For example, when prompted separately, most planet names began with the letter X. Second, like many human-authored conlangs, the artificial conlangs have an associated conculture that describes the fictitious speakers. We generate these vivid detailed descriptions, using the template of Figure 9 (bottom), based on the properties defined in the first step. Third, we generate the conlang description itself, based on the conlang prompt of Figure 10. 3An alternative formulation is to give the learner access to an arbitrarily long sequence of samples (xj)j from which they can choose which xjs to label. This equivalent formulation is less helpful as warm-up. 16 Translate low-resource language prompt Translate the following text from source_language to English: <SOURCE_LANGUAGE> text </SOURCE_LANGUAGE> Your output should be in the following format: <ENGLISH_TRANSLATION> ... </ENGLISH_TRANSLATION> Translate conlang prompt Translate the following text from \"language\" to English. language is constructed language spoken by species on the planet planet. <TEXT_TO_TRANSLATE> source </TEXT_TO_TRANSLATE> To help in the translation, here is detailed information about the culture and language language. conculture conlang # Instructions **Recall that the only text you are translating is the following, based on the above description of language:** <TEXT_TO_TRANSLATE> source </TEXT_TO_TRANSLATE> Just output your translation (no commentary) in the following format: <TRANSLATION> (english translation) </TRANSLATION> Figure 8: Our templates for translating low-resource languages, used for Wikipedia (top) and conlangs (bottom). 17 Language Unique properties Hranyu"
        },
        {
            "title": "Vekhar",
            "content": "Talhi Quol Serren Avaru Hushuun Odrial Tuliq Adults can invert their subjective time flow for brief intervals, and their speech pairs forwardand backward-time syllables so recipients can pre-hear replies; grammar marks retrocausative commitments. Each adult splits into up to 64 mobile lithic shards that remain single mind with zero latency; they converse via phase-locked microseisms where sentences are spatial chords rather than linear sequences. Their photonic-crystal skin sustains coherent light-solitons that are detachable thoughts traded between bodies; syntax is encoded in soliton phase, polarization, and delay. They can switch the molecular chirality of their entire biochemistry on command; messages are broadcast as traversing chirality fields that pass through matter and support duplex conversations without crosstalk. They maintain swarms of external organelles (exoviscera) orbiting the body; utterances are geometric rearrangements of these organelles into transient constellations that convey grammar and intent. They periodically flatten into ultrathin superconductive films and speak by imprinting non-decaying topological current knots; writing is stored as persistent braided currents in crystal veins. They grow quantized gravitational vacuoles that emit controlled microgravity chirps; discourse uses curvature signatures and allows temporary mind-merging by braiding vacuoles into shared wells. They host two interleaved biochemistries oscillating 2 milliseconds out of phase; communication exploits interference between phases, enabling self-addressed messages from immediate future selves and lossless communal recall. Individuals are federations of interchangeable limbs with transferable ownership; pronouns index limb provenance, and clauses can change author mid-utterance as limbs vote and reassign. They sense and emit neutrino flavor oscillations via specialized flavor-glands, conversing through hundreds of kilometers of rock; grammar rides controlled baseline shifts, enabling absolute timestamping and time-locked messages. Species / Planet Kelith / Ulithra Conculture + Conlang 80,483 chars Shard Choir / Qasdur 67,464 chars Rhyzont / Luminis-F 72,316 chars Naruun / Athiri-4 78,579 chars Cirriven / Xyr-Polis 87,926 chars Aelikat / Pethra 78,369 chars Quens / Otholome 78,659 chars Merethi / Nidon-Beta 76,890 chars Ythre / Serqa 72,894 chars Sookh / Helikon-7 74,382 chars Table 1: Summary of the ten conlangs generated using GPT-5. Each conlang has 10 sources with reference translations as well. 18 Conlang ideation prompt We are creating diverse set of conlangs about an alien species. First, we are choosing the: * Name of the planet * Name of the species * Name of the language * The script, which should not be very common like the Latin script. It can be something rarer like the Telugu script. * Unexpected and unique property of the species that is not known in any Earth lifeform Output JSON list of number_of_conlangs such objects, each with the following keys: Output JSON object with the following keys: { } \"planet\": \"Name of the planet\", \"species\": \"Name of the species\", \"language\": \"Name of the language\", \"script\": \"Name of the script\", \"property\": \"Unexpected and unique properties of the species and communication that is not known in any Earth lifeform (including humans)\" Conculture generation prompt Create vivid, detailed, and imaginative \"conculture\" (constructed culture) for the species alien species who inhabit the planet planet. They are unique in that the following sense: property. The conculture should describe the planet and species in enough detail to write novel about one of the aliens. The conculture should include detailed descriptions (e.g., at least 800 words each) of five practices (e.g., games, rituals, social norms, etc.). Their language, language, is written in the script script, but do not detail that here. That will be defined later. Figure 9: Our templates for ideating 10 different conlangs together, along with an elaborate conculture for each. 19 Conlang language-definition prompt Create \"conlang\" (constructed language) called language for the species aliens described below. It will be written in the script script but the language itself will not resemble any human language. <CONCULTURE> conculture </CONCULTURE> The language conlang should be unique in at least one unexpected way that differs from any known existing language. As background, describe the fascinating communication patterns of the species in detail. Their communication must be entirely different from Earth species---so much so that naive translation into English would be not be comprehensible without this background. The description should be long and detailed, especially the grammar and lexicon. The structure of conversations, meetings, and common topics should be detailed. If there are multiple dialects, just define and describe one. Figure 10: Our template for creating conlang based on detailed description of alien species and their conculture. Conlang parallel-text generation prompt Create 10 texts in the alien conlang language spoken by species, described below. The texts should be of varying lenths, with the shortest one being 6 sentences and the longest one being 20 sentences. Each text should have an English translation. At least 5 of the texts should rely on detailed descriptions of the species and practices/peculiarities in the <CONCULTURE> section below. It is fine if the texts use vocabulary not defined in the conlang below, just add it to the additional_vocabulary section. <CONCULTURE> conculture </CONCULTURE> <CONLANG> conlang </CONLANG> Your output should be in JSON with the following structure: { } \"texts\": [ { } \"language\": [list of strings for sentences], \"English\": [list of strings for sentence translations] ... # 9 more texts ] \"additional_vocabulary\": # long string with describing the additional vocabulary needed to understand the texts, if not present in the conlang above Figure 11: Template for creating ten texts in given conlang, based on conculture, along with reference English translations. 20 Figure 12: Judging the judge for the ten artificial conlangs. The LM is tested for its accuracy at determining the original order of the reference sentences. Without translation, accuracy at distinguishing the original paragraph order from permutation on the English references. The fifteen LMs evaluated ranged from GPT-3.5-turbo (1) to GPT-5 (15). Finally, we create ten parallel texts based on the prompt in Figure 11. These were divided into sentence segments, unlike the Wikipedia experiment which is based on paragraphs. Full specifications of all conlangs along with sample translations will be made available upon publication. D.2 FURTHER DETAILS FOR CONLANG EXPERIMENTS As in the low-resource Wikipedia language experiment, we also tested the raw shuffle test without translation, by running the test on the sentences of the English reference translations. As seen in Figure 12, accuracies were significantly lower than in the case of Wikipedia articles. This may be because of the unfamiliar, alien nature of the languages. Nonetheless, GPT-5 achieved remarkable 94% accuracy. Also note that the swap test has been found to be more challenging on longer-length segments, like paragraphs. HALLUCINATIONS IN WHOLE-ARTICLE TRANSLATION Note that transformer-based LMs are well-known to hallucinate (Huang et al., 2025) in translation and other contexts. To illustrate this risk in our data, we took the Wikipedia articles (which as mentioned had been truncated to six paragraphs) and translated them using the same prompt of Figure 8. We then used an RFQE prompt similar to the standard one in Figure 4 to evaluate it on its own merits. We find that one LM may score higher than another in the standard RFQE due to hallucinations. One case is GPT-4o-mini, which scored higher than GPT-5 in terms of this RFQE on whole-document translations. More than 94% of the translations scored 90 or higher out of 100 for both models. Among these, however, Figure 13 shows that many GPT-4o-mini generations score very low when scored using our baseline reference-based MTQE of Figure 4. E.1 DETAILED WIKIPEDIA HALLUCINATION EXAMPLE We now expand on the example from Figure 2. We provide: (a) three document-level translations of the article (truncated to the first six paragraphs), (b) the first six paragraphs of the English version of the Wikipedia article on the PatnaDigha Ghat line.4, (c) three paragraph-by-paragraph translations. The source in Santali: 4https://en.wikipedia.org/w/index.php?title=PatnaDigha_Ghat_line&oldid= 1220622116 21 Figure 13: Baseline reference-based scores of GPT-5 and GPT-4o-mini translations with reference, among plausible translations as judged by having RFQE score of 90. Many GPT-4o-mini translations appear to be likely hallucinations, scoring 0 out of 100 when compared to the reference. The translation of the first six paragraphs by GPT-3.5-turbo (LM 1):"
        },
        {
            "title": "The Wild Animals and Their Association",
            "content": "By the tender age of four or five, when our bodies are neither weak nor strong, the adivasi children probably learned to live with bravery and dignity without even feeling it. Indeed, we can suspect they gain strength from the innocent aspects of wild animals and perhaps have inherited the characteristics of those animals. They perhaps believe that the length of time they spend on the ground, without sophistication, is as strong as the tenderness of wild boar which can readily destroy the obstacles, when rarely needed. Such forest dweller will not tolerate wrong even in his dreams; this shows that great men have sprung up among them who are like the great animals in the forest involving efforts and with endurance. They can attain freedom from fear, either easily or with strain, as easily as these wild animals do. According to the year 1900 or 1901, the folks had gained immense courage and strength over time. However, they are living life of hardship, with poverty. At this time, these adivasi people realize the wonders of the wild forest and learn to respect and understand the life of the wild animals which keeps their households well. Healthy Morning and Evening with Domestic and Wild Animals The adivasi race is able to lead an active life by spending half their day in the hilly terrains and the other half with the domesticated animals. Due to the strength and resistance built by spending time in the forest and the hills, the adivasi people only begin to feel tired at around 7 or 8 p.m., and the domestic and wild animals keep everything harmonious with the bond they share. For fear of extinction of themselves and their animals, the Santhals had gradually preserved and carefully tried to protect both forms of life. They did so by dividing their time, half with wild animals and the rest with the other domestic animals, obviously with responsible heart. As result, their names were eternally recorded in the history, culture, and the love they had for their domesticated animals through mutual bond and careful nurturing. The midnight of the year 1908 (1912 AD). However, when they listened carefully and found they had saved and protected those animals in time, they probably felt sense of happiness on the day when they were able to prepare and consequently preserve the well-being of human and animal alike. With Peace and Harmony, they try to By-Life with Animals The translation of the first six paragraphs by GPT-4 (LM 4): The Sarna-Liturgical Kanthi (rosary) of the Sarna faith, the independent religious system of the indigenous people, and the sacred text of Sarna-Liturgical came to light in the light of detailed research, conversation, and meticulous efforts. According to the 2011 census, Sarna is believer of 46,73,848 people in the country, and he is recognized as primalmul (adivasi) by the constitution of the country. In the context, 14,73,931-48,64,238 out of them and 40.14% of the total population of the state are Sarna. The historic Sarna-Liturgical tradition is flourishing in most of the local languages (like Kurukh, Kudukh, Nagpuri, Bengali etc.), the independent and immeasurable religion of the Sarna people, being found in different Socio-cultural and linguistic background of various tribes in the state it is practicing. Since 1921, the unique religious traditions of the Adivasi-Sarna and the Sarna-Dharam traditions have been covered by karma and festivals and their complexities have not been exposed, thus creating confusion in the minds of outsiders and even among many of our own. Therefore, since 1941, Sarnas distinctive religious rituals, festivals, rituals, rituals, etc., are largely unwritten, conventional, and have long been conveyed by mere faith and practice. It is late but it is time to write it down. Threats Sarna-Faithful Kanthi or Astronomical Kanthi and its traditions have been under threat since the 1891-1950 period has brought about decolonization, Christianity and rapid change in the state of economic, social and cultural life. These threats have grown significantly in the post-globalization period of 1991-2020 and have taken many forms and serious turns. The important resources of Sarnaism like Gotul system, the indigenous education system, the wealth and purity of nature, the centuries-old forests have been destroyed or are on the verge of being destroyed. In the 2011 census, the number of Sarna believers has reduced from 46,73,848 to 46.74 percent and from social point of view, the decisive population is 25,17,958, so the Sarna scriptures have been taken to raise serious questions and challenging situations. Protection The Sarna-Liturgical Kanthi has been strictly adhered to and is written to retain its traditional and independent form, which is the main reason why rituals have been performed and emphasized for centuries. Even into the twenty-first century, where the developed world is moving towards the smart world and digital civilization, the Sarna religion of the original inhabitants is holding lot of attention. The term Non-Scripture applies to Sarna ethics and moral education (such as making man wise, useful to society, aware of right-wrong / wellness, etc.), all ingrained in the minds of the community by dozens of stories, various legends etc. The Sarna religious Kanthi has been adopted and propagated for the preservation of the invaluable and diverse elements of Sarna Economy, the knowledge and practice of mans physical, psychological, and social health. The unique religious rituals of the Sarna-Liturgical Kanthi have to be strictly adhered to and ensured to retain its traditional and independent form, which has been the basis of its original form since the 9th century (ie AD 850) till date. The total faithful population of Jharkhand, which is 9.9 percent (47.1 million), is total believer. Necessity The human race, which is 46,73,848 souls in the world, has spent centuries in the background of indigenous Sarna-Dharam Kanthi. The parrot does not have an independent, universal, common, and scriptural religious volume. The humiliation of Parrot before the scholars and intellectuals is definite conclusion. The period from 1991 to 1993, considering the Sarna religious Kanthi from the point of view of the educated society, the absence of an independent, universal, common and scriptural religious volume, is necessity. Therefore, post-education, self-development, globalization management and community development of the educated community, requires the Sarna religious Kanthi to be scripted as an independent, universal, common, and scriptural religious volume. Necessities The basic needs, the need for universal, common, and scriptural volume of the Sarna religious Kanthi, considering the view of educated society, it is indeed necessary. Social awareness of the Sarna Religious Kanthi, population of 14,73,931-48,64,238, the total population of the country (14,37,723), is discussed in proportion to the need for an independent, universal, and scriptural religions volume for the Saunas religious Kanthi. The restoration of the Sarna religious tradition became necessary. Procedures The Sarna-Liturgical Kanthi, the household ritual of Sarna religious liturgical, has been proposed in several religious rituals (household worship, weekly, monthly, annual festivals, death anniversary etc), with the people in the heart of Sarna-Dharma folk practices in their heart of hearts. Paragraph-by-paragraph translation by GPT-5 (LM 15): PatnaDigha Ghat line is railway line between Patna Junction and Digha Ghat railway stations in the city of Patna, Bihar. It was built by the British in 1862, and later trains were operated on it. In 196263 and again in 2004 the Railway Ministry, and Lalu Prasad Yadav, took steps regarding this line. However, regular passenger train services could not be run on this route; instead, it was used to carry sand and stone. Because of encroachments, it was difficult to run trains on the line. In August 2018, the railway line and its associated structures were handed over to the Bihar government to help reduce urban traffic congestion; consequently, the rails along the entire line were completely removed. The project was slated to be fully completed in 2021. In addition, for building the BlockDigha Road, the Bihar government took up work on Hardinge Road as 7-kilometre corridor, and 1.9 hectares (4.8 acres) of land from the PatnaDigha Ghat lines route was also taken. History Under British rule, in 1862 the Patna JunctionDigha Ghat rail track was built. Because Digha had 24 factories and for the movement of freight and goods, the railway line was constructed. In 196263 the Railway Minister took up renewal of this line, and passenger train service started in 1974. In 1990 goods trains also began operating on the line. However, from July 2004 the track was scarcely used, although Railway Minister Lalu Prasad Yadav had restarted slow passenger train service on it. Route The PatnaDigha Ghat railway line was 6.2 kilometres (3.9 miles) long. It ran between Patna Junction railway station and Digha Ghat railway station, with several intermediate halts (Rajiv Nagar, Shivpuri, Punaichak, Bailey Road and Secretariat), and there were many at-grade road crossings along the route. The line ran almost entirely alongside Bailey Road, and because of this and the numerous crossings, running trains on the line was difficult. Notes In earlier years, the railway line was used for transporting products and raw materials to and from the Bata factory at Digha, which made movement of goods easier. Furthermore, the track was used by the municipal corporation to carry garbage to the dumping ground at Digha. The first six paragraphs of PatnaDigha Ghat line in English Wikipedia: PatnaDigha Ghat line was railway line connecting Patna Junction and Digha Ghat railway station in the Indian city of Patna. It was constructed by the British in 1862, and functioned intermittently. The railway line was revived in 196263 and 2004 by the then Indian Railway Ministers, Jagjivan Ram and Lalu Prasad Yadav, respectively. Passenger train operations on the route were economically unviable due to low ridership. However, trains continued to run on the railway line in order to prevent encroachment. In August 2018, the railway line along with the surrounding land was transferred to the Bihar state government, which plans to convert it into fourto six-lane roadway to ease traffic congestion in the city. The project is scheduled to be completed by mid-2021. Indian Railways got around of land on Hardinge Road from Bihar government in lieu of the 7 km long PatnaDigha Ghat line land it offered to the latter for the construction of R-Block-Digha Road. History The Patna JunctionDigha Ghat rail track was laid in 1862 during the British Raj. The railway line was constructed to ease the transportation of leather items and footwear products to and from the Bata factory situated at Digha. The railway line was revived in 196263 by Jagjivan Ram, the then railway minister of India, citing the historical importance of the line. The passenger train service continued until 1974, with goods trains running on the railway line until the early 1990s. Subsequently, the track remained nonfunctional until July 2004 when the Minister of Railways, Lalu Prasad Yadav, restarted the shuttle-train service along the line. Route E.2 SEGMENT-LEVEL TRANSLATIONS We now present the segment-level translations. The translation of the first six paragraphs by GPT-3.5-turbo (LM 1): 1. The consequences of forest fire. Now days, we are facing new challenges due to climate change. Forest fire is increasing day by day, so we are losing many herbal plants and trees. If this continues, we will face another disaster that our future generation will not forgive us. We need to work together to prevent forest fire and protect our environment, so that we can leave beautiful earth for our future generation. 2. In the year 1894, the British government started to rule over our ancestors, therefore, the Great Santal Hul broke out as means of revolutionary movement against the British. As result, the Santals lost many lives and properties, and even now are suffering from that loss. Since the Great Santal Hul, the Santals have been continuing their struggle for rights and identity in various ways today, aiming at safeguarding their own existence and culture. The 25 Santals are hereditarily operational people, but to incorporate with modern civilization, they have been embracing modern education. There are evidences that they are able to achieve good results through this education. Their interest in education is growing day by day. 3. The world is characterized by inequality, injustice, and exploitation. Many people in our society are still struggling for their rights and justice. We need to work together to create world where everyone can live with dignity and respect. 4. In the year 1952-53, our ancestors originated the concept of governance through traditional customs and council of elders. They also introduced system of selecting their leaders based on merit and integrity. In the year 1974, the movement for the recognition and rights of the indigenous people came into being. They struggled for autonomy and identity, and ultimately achieved their objectives. 5. The crow and the water pot. Once upon time, there was crow. It was very thirsty. It was searching for water. At last, it found water in pot. But the water in the pot was very low. So, the crow thought of plan. It picked up stones and put them into the pot and the water level went up. Then the crow drank water and flew away. 6. tiger and man Once man, while going through the forest, saw tiger cub. Then the tiger cub grew up and became ferocious tiger in the jungle. One day, the man went to cut wood on the mountain. As soon as the tiger saw the man, it ran to eat him. But the man thought of something and lied to the tiger. The man said, My elder brother is coming here to cut wood. Thinking of the mans words, the tiger thought that it would not eat the man. Paragraph-by-paragraph translation by GPT-4 (LM 4): 1. Serma-Ravana was rich man from the city of Lanka, and Ravana was the king of huge state who lived comfortably. Suddenly, severe famine broke out in Lanka, and food and water became extremely scarce. In 1892, about two million people died of starvation, and the dead bodies were rotting all around. In the streets, people were dying of hunger, and the smell of the carcasses was unbearable. Between 1896 and 1906, approximately 20 million people died from starvation and the situation became so critical that the British government had to declare state of emergency. Many people left their homes in search of food and migrated to other places. To tackle the famine, the British Government introduced several relief programs like opening free kitchens, granting loans and resources for food. Despite the measures taken to counter the famine, the death toll continued to rise, and people lost their trust in the government. This led to the publication of report about the mismanagement done by the government in handling the famine situation, which deeply affected the people of Lanka. 2. On August 30, the whole world celebrates International Day of the Disappeared. This day is dedicated to the issue of missing people, especially in situations of violence or armed conflict. It is day that raises awareness about the fate of individuals who have suddenly disappeared and the deep suffering that their families and friends face. The International Day of the Disappeared was first recognized globally in 2011 and since then, it has been an annual event that commemorates and brings attention to the horrific act of enforced disappearance. Through this day, the spotlight is put on the victims who have disappeared in different parts of the world and the importance of addressing this issue. 3. Dreams Since the beginning they were with us. Around 2690 BC people even started analyzingneeding them in reality. The Chinese also considered dreams to be way to communicate with the dead, as way to get divine information. Egyptians also considered dreams to be answers to future riddles. This means we can learn from our dreams. 4. In 1939-40, Sir John Maffeys proposal for constitutional reforms in India was accepted by the British Government. Under this, in the state of Bihar, the Santali Pargana district was merged into the Manbvum district and made part of Bihar. But in 1947, the majority of the people of Bihar rejected the trial of the people, which meant that they wanted to live as separate district, not part of Bihar. In 1950, the Santal Pargana district was separated from Bihar and made part of West Bengal. 26 5. February Sarna-Religious flag raising festival of Sarna believers. The tradition of raising the jari (56 kg) sarna-sacrificial flag (56 in number) at the rate of one each on all the four sides of the sarna-prayer ground was started by Sadhu Ramchandra Murmu of Dumka, which is still being followed. Some rituals performed during this flag raising festival involve animal sacrifices (e.g., goats, sheep, pigeons, quails, and turtles) and sarna-worshippers believe that on this day, it is forbidden to touch anyone or anything when the sarna prayer ground is cleansed or when the Sarna-prayer grounds flag is being replaced. Those who disobey this rule will be turned into turtles. 6. He jumped Thinking about the end of the month, the poor guy is on his heels all day, and after hard day, he thinks about the next day before he goes to bed. The thought of the next days income and expenses made him scream and the peace of his heart is disturbed. sigh, the poor fellow has to think about the beginning of the month day and night, then if his income is lower than his expenditure, he has to think about borrowing, Paragraph-by-paragraph translation by GPT-5 (LM 15): 1. The PatnaDigha Ghat line is metre-gauge railway line between Patna Junction and Digha Ghat railway station, located in the city of Patna. It was built in 1862 by the British and ran for long time. In 196263 and again in 2004, Railway Ministers Jagjivan Ram In later years, and Lalu Prasad Yadav, respectively, took decisions regarding this line. converting it to broad gauge made running passenger trains impossible, so services were stopped. At present, due to encroachments, trains cannot run on the line. 2. In August 2018, the Bihar state government announced that the land of the railway line and its ancillary structures would be used to convert the railway alignment into road, creating new traffic corridor to reduce the frequent congestion seen in the city. This project will be completed in 2021 according to the estimated cost. To build the BlockDigha Road on the railway alignment, the Bihar government acquired, along Harding Road, 1.9 hectares (4.8 acres) of land from the 7-kilometre-long PatnaDigha Ghat railway line. 3. Name Under the British Raj, in October 1862, construction of the Patna JunctionDigha Ghat rail track began. At Digha, to transport factory and market goods more easily, new railway line was built. 4. In 196263, Railway Minister Jagjivan Ram, in view of the Naga movement along the border line, had two new roads laid. Passenger train service began in 1974, and in 1990 goods trains started operating on the railway line. Later, in July 2005, work on new track began. When Railway Minister Lalu Prasad Yadav was in office, slow/local train service was introduced on this line. 5. Overview The PatnaDigha Ghat railway line was 6.2 kilometres (3.9 miles) long. It ran between Patna Junction railway station and Digha Ghat railway station, with several small halts on the routeRajiv Nagar, Shivpuri, Punaichak, Bailey Road and Secretariat. Later the tracks were removed and the right-of-way was converted into road as the Digha Corridor. The work of dismantling the railway track along this line has been completed, and the railway line here has been closed. 6. Problems In our locality, the railway line has not been in use for long time; railway materials have been piled up near roadside workshop, and because of this the public/passers-by face hardship, with things scattered everywhere and the place looking very untidy. Furthermore, the railway track has had no maintenance for long time, and it is now being used by the Corporation as warehouse for storage. As one can see, not only do the GPT-4 and GPT-3.5 paragraph-by-paragraph translations fail the shuffle test, but they change topic repeatedly. However, staying on topic is not sufficient validation for translation. Thus, while one may incorporate penalty for changing topic, merely producing coherent text that stays on topic may not be considered validation. PROOFS FOR SECTION 2 (cid:113) ln(F /δ) Proof of Theorem 1. The proof is simple application of Hoeffdings inequality, following the proof of the standard Occam bound of binary classification (Kearns & Vazirani, 1994). Let 2m . Note that the term on the RHS of Eq. (3) is 2η. Let be any minimizer of η := ℓD(f ), so ℓD(f ) = opt := minf ℓD(f ). Let ˆℓ denote the empirical loss (Eq. (2)). We claim that Eq. (3) holds as long as, ˆℓ(f ) ℓD(f ) + η, and ˆℓ(f ) ℓD(f ) η for all = because in this case all bad with ℓD(f ) opt + 2η have ˆℓ(f ) > opt + η, and is non-bad option for ˆf . By Hoeffdings inequality, each of these inequalities fails for given (either = or = ) with probability δ/F. Thus the probability that any bad is in ˆF is at most δ (union bound). Proof of Theorem 2. Take := n/εc training observations. By definition of ε, this costs at most 1/c-fraction of interactive experiments. Invoking Theorem 1 with δ = 0.01, then with probability at least 0.99, ℓD( ˆfn) optn + (cid:114) 2εc ln(100 Fn) . The proof is completed using the fact that ln(100) < 5 and 2 ln Fn 3 log2 Fn."
        }
    ],
    "affiliations": [
        "EPFL, Project CETI",
        "OpenAI, Project CETI",
        "Project CETI"
    ]
}