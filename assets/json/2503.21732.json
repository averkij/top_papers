{
    "paper_title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling",
    "authors": [
        "Xianglong He",
        "Zi-Xin Zou",
        "Chia-Hao Chen",
        "Yuan-Chen Guo",
        "Ding Liang",
        "Chun Yuan",
        "Wanli Ouyang",
        "Yan-Pei Cao",
        "Yangguang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to $1024^3$ directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 3 7 1 2 . 3 0 5 2 : r SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling Xianglong He1,2 Zi-Xin Zou2 Chia-Hao Chen1,2 Yuan-Chen Guo2 Ding Liang Chun Yuan1 Wanli Ouyang3 Yan-Pei Cao2 Yangguang Li2 1Tsinghua University 2VAST 3The Chinese University of Hong Kong Figure 1. SparseFlex VAE achieves high-fidelity reconstruction and generalization from point clouds. Benefiting from sparsestructured differentiable isosurface surface representation and an efficient frustum-aware sectional voxel training strategy, our SparseFlex VAE demonstrates the state-of-the-art performance on complex geometries (left), open surfaces (top right), and even interior structures (bottom right), facilitating the high-quality image-to-3D generation with arbitrary topology."
        },
        {
            "title": "Abstract",
            "content": "Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 10243 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate complete shape modeling pipeline by training variational autoencoder (VAE) and rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with 82% reduction in Chamfer Distance and 88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling. Please see our project page at https://xianglonghe.github.io/TripoSF. Equal contribution. Corresponding authors. 1 1. Introduction 3D Generative AI is rapidly advancing, with applications spanning entertainment, design, and robotics. However, generating 3D content is fundamentally more challenging than 2D image or text generation due to the inherent complexity of representing and manipulating 3D geometry. Achieving high fidelity and supporting arbitrary topologyincluding open surfaces and complex interiorspresents particularly significant hurdles. Recent progress in 3D generative models has explored various representations, including point clouds [45, 46, 52, 68], meshes [51, 63], 3DGS [18, 32, 78, 88], and implicit fields [9, 10, 34, 35, 59, 77, 78, 87, 91, 93, 95]. Implicit field representations, such as Signed Distance Functions (SDFs) and occupancy fields, have shown favorable results [31, 77, 78, 85, 87, 95]. However, creating training data for these methods typically involves two-step process that limits their effectiveness. First, raw 3D mesh data must be converted into watertight representations [79, 91] to calculate SDF or occupancy valuesa computationally expensive preprocessing step that often degrades fine details. Second, isosurfacing techniques (e.g., Marching Cubes [43], Dual Contouring [26]) are used to extract meshes from the learned continuous field, which can introduce further inaccuracies and artifacts. While Unsigned Distance Fields (UDFs) offer potential way to model open surfaces [6, 11, 16, 38, 42, 84], they often suffer from instability and struggle to capture geometric fine details. Rendering-based supervision offers powerful, differentiable alternative for training 3D representations and 3D generative models [60, 61, 73, 74, 78, 80]. By directly comparing rendered images of generated mesh to ground-truth data, rendering losses avoid the need for the initial watertight preprocessing step and better preserve fine details. However, critical bottleneck arises: when used with dense implicit fields, rendering supervision requires extremely high memory consumption at high resolutions, severely limiting the achievable fidelity. This paper introduces SparseFlex, new sparsestructured isosurface representation that addresses these limitations and unlocks high-resolution, differentiable mesh reconstruction and generation using rendering supervisions. SparseFlex is built upon Flexicubes [61], providing accurate and differentiable isosurface extraction. The key design is the use of sparse voxel structure instead of conventional dense grid. This sparsity is crucial for two primary reasons: (1) it dramatically reduces memory consumption, enabling high-resolution modeling, and (2) it allows for the effective pruning of voxels near open boundaries, enabling the representation of open surfaces. To fully leverage the capabilities of SparseFlex, we propose frustum-aware sectional voxel training. Inspired by techniques used in real-time rendering [1], this approach activates only the subset of SparseFlex voxels that reside within the cameras viewing frustum during each training iteration. We also introduce an adaptive strategy to control the frustums parameters, further optimizing memory usage. This not only substantially reduces computational and memory overhead but also enables, for the first time, the reconstruction of mesh interiors using only rendering supervision by appropriately positioning the camera. Fig. 3 illustrates our SparseFlex representation and the efficient sectional voxel training strategy. Building on SparseFlex and our frustum-aware training, we present complete 3D shape modeling pipeline. We employ variational autoencoder (VAE) architecture, drawing inspiration from TRELLIS [78] but with key modifications. Firstly, because our focus is on high-fidelity geometry, we use point clouds as input to the VAE, providing direct and detailed representation of the shapes surface. Furthermore, we introduce self-pruning upsampling module within the decoder to further refine the sparse voxel structure, which is particularly beneficial for representing open surfaces. rectified flow transformer is then trained on the learned latent space for high-quality, image-conditioned 3D shape generation. Through extensive experiments on Toy4k [64], ABO [12], GSO [15], Meta [48], Objaverse [14], and Deepfashion3D [19], our method demonstrates state-of-the-art shape reconstruction accuracy with minimal detail degradation, and high-quality single-image 3D shape generation. Our main contributions are: We propose SparseFlex, new sparse-structured isosurface representation enabling efficient, high-resolution, and differentiable 3D shape modeling, with natural handling of open surfaces. We introduce novel sectional voxel training strategy with adaptive view frustum control, dramatically reducing memory consumption and enabling high-resolution mesh reconstruction and generation, including interiors, using rendering losses. We demonstrate state-of-the-art reconstruction accuracy and the generation of high-resolution, detailed 3D shapes with arbitrary topology, representing significant advance in the field. 2. Related Work 2.1. 3D Shape Representations for Generation Point Cloud. Point clouds are flexible 3D representation and can be easily acquired using depth sensors and LiDAR. Research has focused on point cloud processing [57, 58, 92] and generation [45, 46, 52, 68], with recent approaches treating point clouds as distribution, directly sampling point clouds from noise via generative models [4, 89]. However, due to their limitation in representing solid surfaces, an additional surface reconstruction step [22, 27, 56] is required. 2 Figure 2. Overview of the SparseFlex VAE pipeline. SparseFlex VAE takes point clouds sampled from mesh as input, voxelizes them, and aggregates their features into each voxel. sparse transformer encoder-decoder compresses the structured feature into more compact latent space, followed by self-pruning upsampling for higher resolution. Finally, the structured features are decoded to SparseFlex through linear layer. Using the frustum-aware section voxel training strategy, we can train the entire pipeline more efficiently by rendering loss. Triangle Mesh. Triangle meshes are the primary representation for 3D assets in modern industrial pipelines. Recent works employ auto-regressive models to generate triangle faces sequentially [51, 63, 66], improving resemblance to artist-created meshes. While effective for low-poly meshes, these methods struggle to produce high-quality meshes with high face count. Implicit Field. Implicit fields (SDF or occupancy) are widely used in geometry learning, particularly for 3D reconstruction [16, 22, 55, 61, 72, 96] and generation [9, 10, 34, 35, 59, 77, 78, 87, 91, 93, 95], producing highquality meshes. Enhancements with triplane [77], vector set [85, 87, 93] sparse voxels [78] and sparse hierarchical voxels [59] allow neural networks to decode the field more effectively. These methods typically use the isosurface techniques [26, 43, 53] for surface extraction but struggle with open-surface shapes like clothing and flowers. Additionally, many require time-consuming watertight conversion that degrades details, while rendering supervised approaches [78] suffer from high memory consumption when training at high resolutions. Open Surfaces. Meshes with open surfaces are common but remain challenging to process. The Unsigned Distance Field (UDF) is widely used for open-surface modeling from point clouds [8, 11, 70, 83] or multiview images [38, 42, 47]. Surf-D [84] introduces UDF-based diffusion model for generating meshes with arbitrary topology. However, UDFbased shape modeling is more difficult than SDF, and its surface extraction [17] is prone to instability due to inaccuracy in neural network gradient estimations. As result, achieving high-quality open-surface meshes remains challenging. 3PSDF [6] introduces three-pole sign to distinguish surfaceadjacent regions but relies on binary occupancy grids for surface extraction, leading to discontinuities and artifacts on the surface. Although G-Shell [40] proposes new 3D representation for extracting non-watertight meshes from watertight triangular meshes and trains diffusion model based on this representation, its dense grid structure limits it to high resolution when handling complex shapes. In this paper, we incorporate sparse structure into Flexicubes [61] to efficient high-quality isosurface representation for open faces. 2.2. 3D Generative Models and VAE Existing 3D generation studies are primarily classified into two categories: large 3D reconstruction model following 2D multiview diffusion models and native 3D generation models. The first category uses multiview diffusion models to generate multiview images from text or an image [23, 24, 37, 41, 62, 71], followed by large 3D reconstruction model [21, 33, 65, 67, 80, 81, 90, 97] is employed to reconstruct the 3D representation from these images in seconds. However, inconsistencies between generated multiview images significantly often degrade the result quality. The second category focuses on native 3D generation model that directly generate 3D models through generative models, including GAN [3, 7, 76, 94], auto-regressive [50, 86], diffusion [25, 68, 87, 91] and rectified flow [35, 78]. Due to the diverse and non-compact nature of 3D representation, many approaches use Variational Auto-Encoder (VAE) [29] or Vector Quantized VAE (VQ-VAE) [69] to encode 3D shapes into latent spaces for the generative models. Geometry-focused methods often input point clouds uniformly sampled from mesh surfaces into the VAE [5, 87]. When aim at decoding both geometry and texture, some methods [31, 32, 78] encode multiview image features into latent spaces. This two-stage process makes the VAEs reconstruction quality crucial for subsequent generation performance. Some works [5, 85] have improved generation quality by enhancing the VAEs shape encoding-decoding 3 Figure 3. Frustum-aware sectional voxel training. The previous mesh-based rendering training strategy (left) requires activating the entire dense grid to extract the mesh surface, even though only few voxels are necessary during rendering. In contrast, our approach (right) adaptively activates the relevant voxels and enables the reconstruction of mesh interiors only using rendering supervision. capabilities. In this paper, we aim to develop foundational VAE that encodes 3D shapes into latent spaces and reconstructs them with arbitrary topology while preserving the raw 3D shapes details. 3. Method We present method for high-resolution 3D shape modeling based on novel sparse-structured isosurface representation, SparseFlex. Our approach leverages differentiable rendering for training, enabling accurate reconstruction of complex geometries, including open surfaces and interiors (see Fig. 1. Fig. 2 illustrates our variational autoencoder (VAE) based shape modeling pipeline, which utilizes the SparseFlex representation to learn compact latent space of 3D shapes. The VAE decoder outputs the parameters of SparseFlex instance, facilitating high-resolution mesh reconstruction. key component of our approach is frustum-aware sectional voxel training, which significantly reduces memory consumption, allowing for training at resolutions up to 10243. Details of the SparseFlex representation are provided in Sec. 3.1, followed by description of the VAE architecture in Sec. 3.2 and the training procedure in Sec. 3.3. Sec. 3.4 describes image-conditioned 3D shape generation using rectified flow transformer on the learned latent space. 3.1. SparseFlex Representation Preliminary. To achieve differentiable mesh extraction while preserving sharp features, we build upon Flexicubes [61], method based on Dual Marching Cubes [53] (DMC). DMC places vertices at the center of voxels rather than edges/corners, leading to better feature preservation. Flexicubes constructs dense voxel with resolution of 3 , where an SDF scalar grid RN 3 is assigned to the voxel corner points, where Ng = Nr + 1. For each voxel, it inco- , β RN 3 porates interpolation weights α RN 3 to >0 each voxel cell, and deformation vectors δ RN 3 to each SDF grid. The underlying surface mesh can be effectively optimized via differentiable rendering [30]. 12 >0 Sparse Structured Flexicubes. The core design of SparseFlex is the introduction of sparse voxel structure, enabling high-resolution shape representation while drastically reducing memory consumption. Instead of dense grid, SparseFlex represents shape using significantly smaller set of voxels, V, concentrated near the surface. This sparsity is crucial for two reasons: (1) it allows us to achieve much higher resolutions than would be possible with dense grid, and (2) it enables the natural representation of open surfaces by simply omitting voxels in empty regions. Specifically, the SparseFlex is defined by set of Nv voxels, = {vi = (xi, yi, zi)}, where vi represents the 3D coordinates of the center of the i-th voxel. Let Nc be the number of corner grids associated with these voxels, where Nv = V. Each voxel is associated with interpolation weights {αi R8 >00 < Nv}. Each corner grid is associated with an SDF value {sj0 < Nc} and deformation vectors {δj0 < Nc}. Due to the sparsity, Nv 3 , representing significant reduction in memory usage compared to the dense Flexicubes representation. We only apply Dual Marching Cubes on these sparse voxels to extract the underlying surface. Formally, the SparseFlex representation, S, is defined as: and Nc 3 >0, βi R12 = (V, Fc, Fv), Fc = {sj, δj}, Fv = {αi, βi}, (1) where represents the voxel centers, Fc contains the SDF values and deformations at the corner grids, and Fv contains the interpolation weights for each voxel. SparseFlex inherits the differentiability of Flexicubes, allowing for end-to-end optimization using rendering losses. This eliminates the need for watertight mesh pre-processing, preserving fine details. Furthermore, the sparse structure, combined with the continuous and deformable nature of the SDF, allows for accurate and efficient representation of highquality open-surface meshes. The sparsity also paves the way for our efficient frustum-aware training strategy, described in Sec. 3.3. 3.2. SparseFlex VAE for Shape Modeling To learn compact and disentangled latent space of 3D shapes, we employ variational autoencoder (VAE) [29] that utilizes the SparseFlex representation. VAE learns probabilistic mapping between an input space (in our case, 3D shapes represented as point clouds) and lower-dimensional latent space, enabling both reconstruction and generation of shapes. Fig. 2 also illustrates our VAE architecture. Our architecture draws inspiration from TRELLIS [78], but with key modifications to leverage the strengths of SparseFlex. Encoder. The input to our encoder is point cloud = {pi R3}Np i=1, uniformly sampled from the surface of 3D mesh, along with corresponding normals = {ni R3}Np i=1. We first voxelize the point cloud to obtain the sparse structure of the SparseFlex representation S. We then 4 employ shallow PointNet [57] to aggregate local geometric features within each voxel. Specifically, for each voxel vi V, we apply local max-pooling operation [55] to the points contained within that voxel, producing feature vector fi. These voxel features = {fi}, along with the sparse structure V, are then fed into sparse transformer backbone. This backbone utilizes shifted window attention [39, 82], similar to TRELLIS [78], but is adapted to operate directly on the sparse voxel features and structure V. The transformer outputs latent code Rdz , which represents the encoded 3D shape. Decoder. The decoder takes the latent code as input and predicts the parameters of SparseFlex instance, = (V, Fc, Fv). We use series of transformer layers, culminating in final linear layer, to predict the SDF values (sj) and deformations (δj) for each corner grid, as well as the interpolation weights (αi, βi) for each voxel. Upsampling Modules. To achieve high-resolution reconstructions, we incorporate two convolutional, self-pruning upsampling modules within the decoder, following the transformer. These modules progressively increase the resolution of the SparseFlex representation. Each upsampling module subdivides existing voxels into smaller voxels (increasing the resolution by factor of 4). Crucially, each module also prunes redundant voxels based on predicted occupancy value. voxel is considered occupied if, after the subdivision, it contains any points from the input point cloud P. This pruning process, inspired by [59], is essential for maintaining the sparsity of the SparseFlex representation and is particularly beneficial for accurately representing open surfaces, as it removes unnecessary voxels in empty regions. 3.3. Training SparseFlex VAE We train our SparseFlex VAE end-to-end using rendering losses, leveraging the differentiability of the SparseFlex representation and new training strategy called frustum-aware sectional voxel training. This strategy dramatically reduces memory consumption during training, enabling us to achieve high resolutions (up to 10243) that would be infeasible with traditional approaches. Frustum-aware Sectional Voxel Training. Even with the sparse structure of SparseFlex, directly rendering the entire representation at high resolutions can be computationally expensive. Furthermore, standard rendering supervision typically focuses only on the visible surface, neglecting the interior of the shape. Besides, recent methods [36, 73, 75, 78, 80] relying on rendering supervision from mesh typically require extracting the entire mesh because dense representation doesnt trivially allow for partial extraction. In contrast, our sparse representation naturally enables partial extraction. To address these issues, we introduce frustum-aware sectional voxel training. Inspired by techniques used in real-time rendering for efficient visibility culling [1], this approach activates only the voxels within the cameras viewing frustum during each training iteration. Activating voxel means including it in the isosurface extraction and rendering process. This sectional approach means we only process portion of the 3D space at time, markedly reducing memory usage. As illustrated in Fig. 3, given the cameras extrinsic π, intrinsics K, and the near (n) and far (f ) clipping planes of the viewing frustum, we compute the Model-View-Projection (MVP) matrix MVP. We then use boolean operator to check whether the center of each voxel vi lies within the viewing frustum defined by the MVP matrix. We use I(vi Frustum(MVP)) to represent this check, where I() is an indicator function. The set of active voxels, Vactive, is then defined as: Vactive = {viI(vi Frustum(MVP)) = 1, vi V}. (2) Adaptive Frustum and Interior Reconstruction. We introduce visibility ratio α (0 < α 1) controlling the proportion of active voxels in SparseFlex. We adaptively adjust the near and far clipping planes to ensure that approximately αNv voxels are within the frustum. This is achieved through an iterative process: we initially set the near and far planes and iteratively adjust them based on the number of active voxels until the desired proportion is reached. This adaptive frustum also enables novel capability: reconstructing mesh interiors using only rendering supervision. By positioning virtual camera inside the object or adjusting the near clipping plane to intersect the mesh, we can render and supervise the internal structure (see Fig. 4). Moreover, the zoom-in camera viewpoint can render the mesh surface with greater details, making it better for higher-resolution training. This is significant advantage over methods that rely on watertight representations, which cannot capture interior details. Loss Function. We train our VAE in an end-to-end manner, with an objective function comprising four components: = λ1Lrender + λ2Lprune + λ3LKL + λ4Lflex, (3) where λ1, λ2, λ3, and λ4 are weighting coefficients that balance the different loss terms. Lrender is the rendering supervision loss. We employ combination of losses commonly used in differentiable rendering [28, 49]: Lrender = λdLd + λnLn + λmLm + λssLss + λlpLlp, (4) where Ld, Ln, and Lm denote the L1 loss for depth maps, normal maps, and mask maps, respectively. Lss and Llp denote SSIM loss and LPIPS loss, and are only applied on normal maps. Please refer to the supplementary material for detailed definition of these losses and their weighting coefficients. 5 Lprune is the structure loss, formulated as binary crossentropy (BCE) loss, supervising the construction of sparse voxels: Lprune = BCE , (5) (cid:16) V, ˆV (cid:17) where represents the ground-truth occupancy of voxels derived from the input point cloud and ˆV represents the predicted occupancy by the upsampling modules. LKL is the KL divergence between the learned latent distribution and standard normal prior, regularizing the latent space. Lflex is the regularization term from Flexicubes [61] to encourage smooth SDF values. 3.4. Image-to-3D Generation with Rectified Flow Building upon the trained SparseFlex VAE, we develop pipeline for high-quality, image-conditioned 3D shape generation, following similar approach to TRELLIS [61]. Our approach consists of two main components: Structure Flow Model and Structured Latent Flow Model. Structure Flow Model. First, separate, simple fully 3D convolutional structure VAE is employed to compress dense voxels representing 3D shapes into low-resolution (1/4 scale) space. Subsequently, the image condition features are extracted using DINOv2 [54] and injected into the transformer model via cross-attention, after which rectified flow model is trained within this low-resolution space. During inference, given an input image, the trained structure flow model generates the corresponding low-resolution 3D space, which is then decoded by the structure VAE to produce the sparse structure of the generated 3D shape. Structured Latent Flow Model. Based on the proposed SparseFlex VAE, the point cloud and the corresponding voxelized sparse structure of 3D shape are encoded into structured latent space. Subsequently, the image condition feature obtained via DINOv2 is injected into the sparse transformer model through cross-attention, followed by training rectified-flow model within this structured latent space. During inference, given an input image, the corresponding sparse structure is first generated using the structure flow model and its structure VAE. Then, both the sparse structure and the input image are provided to the structured latent flow model to generate the corresponding structured latent representation. Finally, the SparseFlex VAE decodes this latent representation to produce the final 3D shape. 4. Experiments 4.1. Implementation Details We develop the implementation of SparseFlex based on the official code1 provided by FlexiCubes [61]. We train SparseFlex VAE and structured latent flow model using approximately 400K high-quality 3D meshes filtered from largescale datasets, Objaverse (-XL) [13, 14]. Since incorrect normals in raw data can significantly degrade the performance Method Toys4k Dora Benchmark CD F1(0.001) F1(0.01) CD F1(0.001) F1(0.01) Craftsman [34] 13.08/4.63 11.15/2.13 Dora [5] 12.90/11.89 Trellis [78] 4.35/3.14 XCube [59] 3PSDF [6] 4.51/3.69 Ours256 Ours512 Ours 2.56/1.25 1.67/0.84 1.33/0.60 10.13/15.15 17.29/26.55 4.05/4.93 1.61/13.49 11.33/14.10 18.31/27.23 23.74/34.10 25.95/35.69 56.51/85.02 13.54/2.06 81.54/93.84 16.61/1.08 59.65/64.05 17.42/9.83 74.65/79.62 4.74/2.37 81.70/86.13 7.45/1.68 85.35/92.01 1.93/0.53 90.39/95.60 1.36/0.23 92.30/96.22 0.86/0.12 6.30/11.14 13.65/25.78 3.81/6.20 1.31/0.84 7.52/12. 16.24/28.37 21.85/36.03 25.71/39.50 73.71/91.95 78.73/96.40 62.70/71.95 75.64/86.50 79.43/91.17 88.76/97.31 91.55/98.51 94.71/99.14 Table 1. Quantitative comparison for VAE reconstruction quality on the Toys4K dataset (left) and Dora benchmark (right). The / symbol separates the results computed over the entire dataset from those obtained exclusively on the watertight subset. Method CD F1(0.001) F1(0.01) Surf-D [84] 3PSDF [6] Ours 256 Ours256 Ours 512 Ours512 Ours 1024 Ours1024 63.79 0.26 0.55 0.08 0.18 0.05 0.05 0.04 0.80 8.14 6.35 18.60 11.31 31.60 24.80 37.22 23.17 99. 94.88 99.99 99.93 100.00 100.00 100.00 Table 2. Reconstruction results on open-surface dataset Deepfashion3D. indicates the absence of the self-pruning upsampling module. of both VAE reconstruction and image-to-3D generation, we apply mesh preprocessing step to correct these flipped normals, ensuring that all normals are consistently oriented outwards. Please refer to the supplementary for more details. Building on the success of progressive training in recent works [91], we train SparseFlex VAE progressively, increasing final resolution from low to high (256, 512, and 1024). For the structure VAE and structure flow model, we adopt the model from Trellis [78] and finetune their pre-trained weights to our task. We train SparseFlex VAE on 64 A100 GPUs with batch size of 64 and train structured latent flow models with batch size of 256. We use the AdamW [44] optimizer with an initial learning rate of 1e 4 and the weight decay as 0.01. At inference, we generate the results with 3.5 CFG and 50 sampling steps. 4.2. Dataset, Baselines, and Metrics Dataset. We evaluate the reconstruction quality of VAE across different methods on diverse set of datasets, including 1) universal datasets ABO [12], GSO [15], Meta [48], Objaverse [14], Toys4k [64] and 2) open-surface dataset Deepfashion3D [19]). The test list of ABO, GSO, Meta, and Objaverse is derived from Dora benchmark [5] after excluding our training data which includes about 2.7k assets. For Toys4k, we use the full set following Trellis [78]. For the image-to-3D generation, we evaluate the methods on 200 random assets from Toys4k [64] and some images in the wild, showcasing the superior potential on generation tasks of SparseFlex VAE. Baselines. We compare our VAE with previous state-ofthe-art methods, including Craftsman [34], Trellis [78], 1https://github.com/nv-tlabs/FlexiCubes 6 Figure 4. Qualitative comparison of VAE reconstruction between ours and other state-of-the-art baselines. Our approach demonstrate superior performance in reconstructing complex shapes, open surfaces, and even interior structures. Dora [5], XCubes [59], Surf-D [84] and 3PSDF [6], with Surf-D and 3PSDF specially designed for open surfaces. We directly use the available pre-trained weights provided by these baselines, except for 3PSDF, which we re-implement and train on our dataset. We only compare Surf-D on the Deepfashion3D dataset due to the lack of available pretrained weights trained on large-scale datasets. For the generation results, we compare our method with InstantMesh [80], Direct3D [77], and TRELLIS [78]. Metrics. We evaluate the reconstruction performance of VAE by using the commonly used metrics, including Chamfer Distance (CD) and F-score with thresholds of 0.01 and 0.001. The metrics are multiplied by 104 and 102, respectively. For generation results, four orthogonal views of normal maps for each shape are rendered for quantitative comparisons. We report the Frechet Inception Distance (FID) [20] and Kernel Inception Distance (KID) [2]. 4.3. VAE Reconstruction Evaluation We conduct extensive experiments to evaluate the quantitative results of VAE reconstruction from different methods in Table Tab. 1. Since Dora [5] and Craftsman [34] are trained on watertight data, we separate the results exclusively on the watertight subset from the results computed over the entire dataset for the clear demonstration. These methods often perform poorly on non-watertight meshes, especially open surfaces like flowers. Ours256 already outperforms other baselines in terms of CD and F-score. As the resolution increases, our method achieves even better, ultimately achieving 82% reduction on CD and 88% increase in F-score. Fig. 4 demonstrate the superiority of our method 7 Figure 5. Qualitative comparison of VAE reconstruction quality between our method with different resolution and TRELLIS. Feed-Forward Time (ms) GPU Memory Cost (MB) Resolution 2563 5123 Ours (α = 0.1) Ours (α = 0.3) w/o FSV w/o FSV & Sp. 620 333 697 357 390 958 418 OOM 10243 1151 1475 OOM OOM 2563 5123 40183 35515 45675 37403 40703 62029 45505 OOM 10243 55441 69991 OOM OOM Table 3. Feed-Forward time and GPU memory cost comparisons. α stands for the visibility ratio of voxels. OOM means Out Of Memory and FSV means frustum-aware sectional voxel training strategy. Sp means SparseFlex. Method InstantMesh [80] Direct3D [77] TRELLIS [78] Ours FID KID (103) 68.74 9.68 50.84 2.04 47.66 1. 44.95 1.05 Table 4. Quantitative generation results on Toys4k. for complex shapes, open surfaces, and interior structures. Tab. 2 compares our method with those designed for open surfaces on the Deepfashion3D [19] dataset, and our method still achieves the best performance among them. 4.4. Image to 3D Genration We also validate the effectiveness of our SparseFlex VAE as foundation model for generation. Tab. 4 validates the effectiveness of our generation. Visualizations are also demonstrated in Fig. 6, which includes image-to-3D results using wild images. The generated shapes, which preserves sharp edges and fine details, highly match the given images and showcase the generalization of our method. 4.5. Ablation Studies Self-Pruning Upsampling. Tab. 2 demonstrates that the self-pruning upsampling module plays an important role in the reconstruction quality of open-surface shapes, as it allows for effective pruning of voxels near open boundaries, enabling the reconstruction boundary to better align with input. Supplementary shows the visual effects of this module in detail. Figure 6. Single image-to-3D generations with in-the-wild images.1 The geometry of generated assets accurately preserves sharp edges and fine details. aware sectional voxel training strategy, we evaluate the runtime of the feed-forward and GPU memory consumption with different settings, as shown in Tab. 3. It demonstrates that SparseFlex effectively reduces the required GPU memory and runtime during network feed-forward. However, it is not efficient to scale to higher resolution. The frustum-aware sectional voxel training strategy eliminates the reliance on the entire surface extraction during rendering, significantly reducing the memory requirements during training. Sparse Voxel Resolutions. Higher resolution leads to better VAE reconstruction quality as shown in Tab. 1. Fig. 5 illustrates the qualitative comparison of SparseFlex VAE with different resolutions, along with TRELLIS with resolution of 256. Thanks to geometry encoding, our approach achieves better geometry reconstruction with the same resolution as TRELLIS. Benefiting from efficient training, more details of complex structures are revealed, such as the tank track in the first row, as the resolution increases. SparseFlex and Frustum-aware Sectional Voxel Training. To demonstrate the effectiveness of SparseFlex and frustum1The original images in Fig. 6 are sourced from various 3D generation platforms, benchmarks (such as Tripo3D, Rodin, Meshy, Trellis, etc.). 8 5. Conclusion In this paper, we present SparseFlex, new sparse-structured isosurface representation for differentiable mesh reconstruction with high resolution using rendering supervision, enabling the reconstruction of open surfaces. Based on SparseFlex, we propose novel frustum-aware sectional voxel training strategy with adaptive frustum control to efficiently train SparseFlex VAE with high-resolution, dramatically reducing memory consumption. This strategy also allows our method to reconstruction the interiors only using rendering loss. Finally, we develop the image-to-3D generation pipeline following TELLIS [78]. Experiments demonstrate state-of-the-art reconstruction accuracy and high-quality generation with open surfaces. Limitations: Despite the strong performance of SparseFlex VAE in both reconstruction and image-to-3D generation, some limitations remain. 1) Open surface boundaries, while handled effectively by voxel pruning, may exhibit minor artifacts at lower resolutions. 2) High-resolution generation remains computationally demanding. 3) Enhanced control over the generation of interior structures is an area for future work."
        },
        {
            "title": "References",
            "content": "[1] Tomas Akenine-Moller, Eric Haines, and Naty Hoffman. Realtime rendering. AK Peters/crc Press, 2019. 2, 5 [2] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 7 [3] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, pages 1612316133, 2022. 3 [4] Jen-Hao Rick Chang, Yuyang Wang, Miguel Angel Bautista Martin, Jiatao Gu, Josh Susskind, and Oncel Tuzel. 3d shape tokenization. arXiv preprint arXiv:2412.15618, 2024. 2 [5] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024. 3, 6, 7 [6] Weikai Chen, Cheng Lin, Weiyang Li, and Bo Yang. 3psdf: Three-pole signed distance function for learning surfaces with arbitrary topologies. In CVPR, pages 1852218531, 2022. 2, 3, 6, [7] Zhiqin Chen and Hao Zhang. Learning implicit fields for In CVPR, pages 59395948, generative shape modeling. 2019. 3 [8] Zhiqin Chen, Andrea Tagliasacchi, Thomas Funkhouser, and Hao Zhang. Neural dual contouring. ACM TOG, 41(4):113, 2022. 3 [9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling high-quality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. 2, 3 [10] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In CVPR, pages 44564465, 2023. 2, [11] Julian Chibane, Gerard Pons-Moll, et al. Neural unsigned distance fields for implicit function learning. NeurIPS, 33: 2163821652, 2020. 2, 3 [12] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In CVPR, pages 2112621136, 2022. 2, 6 [13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. NeurIPS, 36:3579935813, 2023. 6 [14] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, pages 1314213153, 2023. 2, 6 [15] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In ICRA, pages 25532560, 2022. 2, 6 [16] Benoˆıt Guillard, Federico Stella, and Pascal Fua. Meshudf: Fast and differentiable meshing of unsigned distance field networks. In ECCV, pages 576592, 2022. 2, [17] Benoit Guillard, Federico Stella, and Pascal Fua. Meshudf: Fast and differentiable meshing of unsigned distance field networks. In ECCV, pages 576592, 2022. 3 [18] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In ECCV, pages 463479. Springer, 2024. 2 [19] Zhu Heming, Cao Yu, Jin Hang, Chen Weikai, Du Dong, Wang Zhangye, Cui Shuguang, and Han Xiaoguang. Deep fashion3d: dataset and benchmark for 3d garment reconstruction from single images. In ECCV, pages 512530, 2020. 2, 6, 8 [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [21] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: large reconstruction model for single image to 3d. In ICLR, 2024. 3 [22] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In CVPR, pages 43694379, 2023. 2, 9 [23] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. 3 [24] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via In CVPR, pages localized epipolar-constrained diffusion. 97849794, 2024. 3 [25] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia Conference, pages 19, 2022. 3 [26] Tao Ju, Frank Losasso, Scott Schaefer, and Joe Warren. Dual contouring of hermite data. In Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques, pages 339346, 2002. 2, 3 [27] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (ToG), 32(3):113, 2013. [28] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4):1391, 2023. 5 [29] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 3, 4 [30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM TOG, 39(6), 2020. 4 [31] Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In ECCV, pages 112130, 2024. 2, 3 [32] Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. Gaussiananything: Interactive point cloud latent diffusion for 3d generation. arXiv preprint arXiv:2411.08033, 2024. 2, 3 [33] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In ICLR, 2024. [34] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2, 3, 6, 7 [35] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025. 2, 3 [36] Minghua Liu, Chong Zeng, Xinyue Wei, Ruoxi Shi, Linghao Chen, Chao Xu, Mengqi Zhang, Zhaoning Wang, Xiaoshuai Zhang, Isabella Liu, et al. Meshformer: High-quality mesh generation with 3d-guided reconstruction model. NeurIPS, 37:5931459341, 2025. 5 [37] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In ICLR, 2024. 3 [38] Yu-Tao Liu, Li Wang, Jie Yang, Weikai Chen, Xiaoxu Meng, Bo Yang, and Lin Gao. Neudf: Leaning neural unsigned distance fields with volume rendering. In CVPR, pages 237 247, 2023. 2, [39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In CVPR, pages 1001210022, 2021. 5 [40] Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, and Bernhard Scholkopf. Ghost on the shell: An expressive representation of general 3d shapes. In ICLR, 2024. 3 [41] Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion priors for relightable text-to-3d generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. 3 [42] Xiaoxiao Long, Cheng Lin, Lingjie Liu, Yuan Liu, Peng Wang, Christian Theobalt, Taku Komura, and Wenping Wang. Neuraludf: Learning unsigned distance fields for multi-view reconstruction of surfaces with arbitrary topologies. In CVPR, pages 2083420843, 2023. 2, 3 [43] William E. Lorensen and Harvey E. Cline. Marching cubes: high resolution 3d surface construction algorithm. In SIGGRAPH, pages 163169, 1987. 2, 3 [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [45] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, pages 28372845, 2021. 2 [46] Luke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. Pc2: Projection-conditioned point cloud diffusion for single-image 3d reconstruction. In CVPR, pages 12923 12932, 2023. 2 [47] Xiaoxu Meng, Weikai Chen, and Bo Yang. Neat: Learning neural implicit surfaces with arbitrary topologies from multiview images. In CVPR, pages 248258, 2023. 3 [48] META. Digital twin catalog, 2024. 2, 6 [49] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 5 [50] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d completion, reconstruction and generation. In CVPR, pages 306315, 2022. [51] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In ICML, pages 72207229, 2020. 2, 3 [52] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2 [53] Gregory Nielson. Dual marching cubes. In IEEE visualization, pages 489496, 2004. 3, 4 10 [54] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [55] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In ECCV, pages 523540, 2020. 3, 5 [56] Songyou Peng, Chiyu Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape as points: differentiable poisson solver. NeurIPS, 34:1303213044, 2021. 2 [57] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, pages 652660, 2017. 2, 5 [58] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. NeurIPS, 30, 2017. 2 [59] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In CVPR, pages 42094219, 2024. 2, 3, 5, 6, 7 [60] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. NeurIPS, 34: 60876101, 2021. [61] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM TOG, 42(4): 37:137:16, 2023. 2, 3, 4, 6 [62] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2024. 3 [63] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In CVPR, pages 1961519625, 2024. 2, 3 [64] Stefan Stojanov, Anh Thai, and James Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In CVPR, pages 17981808, 2021. 2, 6 [65] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian In ECCV, model for high-resolution 3d content creation. pages 118. Springer, 2024. 3 [66] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Autoregressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. [67] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 3 models for 3d shape generation. NeurIPS, 35:1002110039, 2022. 2, 3 [69] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017. 3 [70] Li Wang, Weikai Chen, Xiaoxu Meng, Bo Yang, Jintao Li, Lin Gao, et al. Hsdf: Hybrid sign and distance field for modeling surfaces with arbitrary topologies. NeurIPS, 35:3217232185, 2022. 3 [71] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 3 Imagedream: Image-prompt arXiv preprint [72] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 3 [73] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In ECCV, pages 5774, 2024. 2, 5 [74] Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, and Hao Su. Neumanifold: Neural watertight manifold reconstruction with efficient and highquality rendering support. arXiv preprint arXiv:2305.17134, 2023. 2 [75] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for highquality meshes. arXiv preprint arXiv:2404.12385, 2024. [76] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29, 2016. 3 [77] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 2, 3, 7, 8 [78] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 3, 4, 5, 6, 7, 8, 9 [79] Hongyi Xu and Jernej Barbiˇc. Signed distance fields for polygon soup meshes. In Graphics Interface, pages 3541. 2014. 2 [80] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2, 3, 5, 7, 8 [81] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In ECCV, pages 120. Springer, 2024. 3 [68] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion [82] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d: 11 meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In CVPR, pages 10324 10335, 2024. 3 pretrained transformer backbone for 3d indoor scene understanding. arXiv preprint arXiv:2304.06906, 2023. 5 [83] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function for general shape representation. In CVPR, pages 1282912839, 2022. 3 [84] Zhengming Yu, Zhiyang Dou, Xiaoxiao Long, Cheng Lin, Zekun Li, Yuan Liu, Norman Muller, Taku Komura, Marc Habermann, Christian Theobalt, Xin Li, and Wenping Wang. Surf-d: Generating high-quality surfaces of arbitrary topologies using diffusion models. In ECCV, pages 419438, 2024. 2, 3, 6, [85] Biao Zhang and Peter Wonka. Lagem: large geometry model for 3d representation learning and diffusion, 2024. 2, 3 [86] Biao Zhang, Matthias Nießner, and Peter Wonka. 3dilg: Irregular latent grids for 3d generative modeling. NeurIPS, 35: 2187121885, 2022. 3 [87] Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM TOG, 42(4): 92:192:16, 2023. 2, 3 [88] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance reprearXiv preprint sentation for 3d generative modeling. arXiv:2403.19655, 2024. 2 [89] Biao Zhang, Jing Ren, and Peter Wonka. Geometry distributions. arXiv preprint arXiv:2411.16076, 2024. 2 [90] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In ECCV, pages 119. Springer, 2024. [91] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating highquality 3d assets. ACM TOG, 43(4):120, 2024. 2, 3, 6 [92] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In CVPR, pages 16259 16268, 2021. 2 [93] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. NeurIPS, 36: 7396973982, 2023. 2, 3 [94] Xinyang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. Sdf-stylegan: implicit sdf-based stylegan for 3d shape generation. In Computer Graphics Forum, pages 5263, 2022. 3 [95] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. ACM TOG, 42 (4):113, 2023. 2, 3 [96] Zi-Xin Zou, Shi-Sheng Huang, Yan-Pei Cao, Tai-Jiang Mu, Ying Shan, Hongbo Fu, and Song-Hai Zhang. Gp-recon: Online monocular neural 3d reconstruction with geometric prior. IEEE TVCG, 2024. 3 [97] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane"
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "VAST"
    ]
}