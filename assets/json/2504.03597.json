{
    "paper_title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation",
    "authors": [
        "Jad Abou-Chakra",
        "Lingfeng Sun",
        "Krishan Rana",
        "Brandon May",
        "Karl Schmeckpeper",
        "Maria Vittoria Minniti",
        "Laura Herlant"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com."
        },
        {
            "title": "Start",
            "content": "Real-is-Sim: Bridging the Sim-to-Real Gap with Dynamic Digital Twin for Real-World Robot Policy Evaluation Jad Abou-Chakra1 Lingfeng Sun1 Krishan Rana2 Brandon May1 Karl Schmeckpeper1 Maria Vittoria Minniti1 Laura Herlant1 5 2 0 2 4 ] . [ 1 7 9 5 3 0 . 4 0 5 2 : r Abstract Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, novel behavior cloning framework that incorporates dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robots joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com. I. INTRODUCTION Simulation environments offer highly favorable setting for developing and evaluating robotic manipulation policies. They support continuous monitoring of task performance during training, enable large-scale parallel rollouts, and allow for precise control over resets and environment variation. Crucially, simulation also affords flexible and often privileged access to state information, such as exact object poses or custom visual observations from arbitrary viewpoints, that can simplify policy learning. These advantages make behavior cloning particularly effective in simulation. However, transferring these benefits to the real world remains challenging. Real-world policy evaluation is slow, laborintensive, and limited in scale, often relying on training loss as proxy for success despite its poor correlation with actual task performance [1], [2], [3]. This gap between simulation and real-world applicability motivates the core question driving our work: How can we retain the strengths of simulation for behavior cloning while operating in the real world? 1Robotics and AI Institute 2Queensland University of Technology Fig. 1. The real-is-sim framework, illustrating the information flow between its components. policy trained in correctable physics simulator (leveraging Embodied Gaussians) controls simulated robot. Real-world observations continuously update the simulator, maintaining its state close to ground truth. The real robot then mirrors the simulated robots joint positions. This approach shifts the sim-to-real gap challenge from the policy to the adaptable physics simulator. Despite the clear advantages of simulation, integrating these benefits into real-world policy development remains difficult. Broadly, two paradigms exist. The first, real-only training, relies exclusively on physical hardware [4]. While this offers immediate applicability, since policies are trained and deployed in the same environment, it comes with significant limitations: (i) it prevents offline evaluation, which is essential for diagnosing failure modes and tracking performance, and (ii) it restricts large-scale parallelization, which is useful for exploration, hyperparameter tuning, and proactive planning. The second paradigm, sim-to-real, aims to overcome these constraints by training in simulation and transferring policies to the real world. This allows for extensive offline evaluation and experimentation at scale [5]. However, it introduces the well-known sim-to-real gap: discrepancies in observations and dynamics between simulation and reality that can degrade policy performance. Bridging this gap often requires extensive, task-specific tuning of either the simulator or the policy, efforts that are difficult to generalize or scale. Current approaches treat simulation and reality as separate domains, creating mismatch between the observational inputs learned manipulation policy receives during training and deployment. This discrepancy also necessitates ensuring that the simulated robot behaves identically to its realworld counterpart, making policy transfer from simulation to reality challenging. We propose real-is-sim, paradigm shift where correctable simulator remains always-in-the-loop, even during real-world deployment. correctable simulator synchronizes with the physical world using sensor data (in our case, RGB images), creating live, dynamic digital twin of the robots environment. The first simulator to achieve this generally and in real-time is Embodied Gaussians [6]. Embodied Gaussians combines the use of Gaussian splatting and particle-based physics system to represent both the visual and physical aspects of the world. It allows, through its differentiable rendering process, the correction of the simulator state from observations received from cameras. In the real-is-sim framework, the always-in-the-loop simulator acts as mediator between the physical world and the policy. Policies are trained exclusively on representations derived from the simulators state, effectively transforming real-world interactions into simulated states. This approach collapses the traditional divide between training and deployment domains. Real-is-sim collects synchronized simulation states during real-world interactions, which serve as demonstration trajectories for policy training. Policies are trained on arbitrary representations extracted from these simulated states (which may include object poses, robot configuration, and/or rendered images from fixed or robot-mounted viewpoints). The policies output desired joint-space actions that control the simulated robot. Critically, this process is identical in both online and offline operational modes. In offline mode, the simulator operates without real-world coupling, executing the policy in purely virtual environment. In online mode, the real robot acts as follower and tracks the simulated robots joint positions, while real-world sensor data (RGB cameras) continuously correct the simulators state. This design (illustrated in Figure 1) ensures the policy remains fully decoupled from the physical hardware it interacts solely with the simulated robot and environment in all modes. As such it allows the policies to always operate on indistribution simulator states, even when deployed in the real world. By maintaining persistent, real-time digital twin of the physical environment, real-is-sim unifies simulation and reality into single, coherent interface for policy development. This operational symmetry across both offline and online modes unlocks several key benefits: consistent policy behavior between simulation and deployment, scalable and safe offline evaluation, and flexible, perception-agnostic representations for policy learning. In this work, we show how real-is-sim fundamentally rethinks the role of simulation, not just as training tool, but as medium for end-toend policy evaluation and deployment. We validate our framework on the PushT manipulation task, demonstrating that offline simulation evaluations closely predict real-world performance. Real-is-sim offers scalable path forward for efficient training, reliable assessment, and robust deployment of robotic policies in the real world. II. RELATED WORK A. Real World Policy Evaluation Evaluating robotic policies directly in the real world remains the most reliable method for assessing performance in practical tasks [4]; however, it is often labor-intensive, timeconsuming, and requires substantial human oversight [1]. In many experimental pipelines, researchers must manually reset the environment, initiate policy execution, and supervise task completion. For example, in the benchmark introduced by Cheng et al. [7], the authors report manually aligning T-shaped object to match one of 20 predefined start poses for each policy rollout. This reset process involved visual overlays to ensure pose accuracy and was repeated across all baselines. This process is not parallelizable and poses scalability bottleneck when evaluating across multiple tasks, policies, and environment variations. To mitigate these challenges, growing body of work has turned to evaluating real-world policies in simulation environments, even when those policies are trained on real data [8], [9], [10], [11]. This strategy offers safety, repeatability, and especially scalability, enabling parallelized policy assessment across diverse conditions. Prior works have spent large amounts of effort trying to improve the correlation between simulated and real world performance by improving visual appearance [8], [11], refining object models [8], [10], modeling control latency [9], and tracking cross-corelation between object classes [11]."
        },
        {
            "title": "In contrast",
            "content": "to these evaluation-centric approaches, our framework incorporates the simulator as an integral component throughout the entire policy development pipeline: training, evaluation, and deployment. Rather than constructing simulation for post hoc policy assessment, we instead continuously synchronize the simulation with real-world sensor data during execution. This enables policies trained entirely in simulation to be deployed directly on real robots without additional fine-tuning or adaptation. By maintaining real-time alignment between the simulated and physical environments, our method ensures that the policy operates within unified and consistent observation and control space. This paradigm shift facilitates seamless transfer between simFig. 2. Comparison of three paradigms: Real-is-Sim, Real-Only, and Sim-to-Real, highlighting their online (real-world interaction) and offline (simulationbased) capabilities. Real-is-Sim offers unified framework where the offline mode is simplified version of the online, lacking only real-time correction. This ensures seamless transferability. Real-Only is confined to real-world execution. Sim-to-Real struggles with distribution mismatch and dynamics discrepancies, making successful transfer uncertain. In the Real-is-Sim paradigm, successful transfer between online and offline modes is mostly dependent on whether environment synchronization was successful during the demo collection phase. ulation and reality, enabling scalable evaluation and robust deployment under single, coherent framework. B. Bridging the Sim-to-Real Gap for Robotic Manipulation As discussed above, apart from direct real world evaluation [4], simulation of the corresponding real world environment offers promising alternative. major challenge lies in the discrepancy between the simulated states and reality particularly. To bridge this gap, numerous strategies have been proposed. Domain randomization injects variability into the simulation (e.g., textures, lighting, physics) to force robustness. Early work by Tobin et al. [12] demonstrated zeroshot sim-to-real transfer using randomized visuals. While powerful, this approach often requires excessive variability and long training times. Domain adaptation methods attempt to align visual or latent representations between sim and real. RCAN [13] learns translation from randomized sim images to canonical domain, achieving real-world success without real data. GAN-based methods have also shown promising results in translating observations across domains. Sim adaptation techniques aim to align simulation dynamics with reality. AdaptSim [14] meta-learns task-specific simulator parameter adjustments based on real-world performance. Similarly, Scalable Real2Sim [15] uses robotic interaction to infer object parameters and auto-generate simulator assets that can then be used for policy learning. More recent frameworks adopt real-to-sim-to-real strategy. Torne et al. [16] create real-world digital twins, use inverse policy distillation to initialize sim policies, then fine-tune in simulation for robustness. TRANSIC [17] introduces human-in-the-loop correction to adapt sim policies post-deployment, improving failure recovery. DeepMinds robot soccer framework [18] combines high-fidelity neural rendering with domain randomization for successful sim-to-real transfer using egocentric vision. Despite significant progress, most existing methods either aim to learn policies that are robust to sim-to-real discrepancies or rely on fine-tuning in the real world after training in simulation. However, these approaches still suffer from domain shift, misaligned observations, and the need for complex, task-specific simulation modeling. In contrast, our approach seeks to eliminate the sim-to-real gap at its source by keeping the simulator continuously aligned with the real world and treating it as the execution environment throughout data collection, training and deployment. This design decouples the policy from sim-real mismatches and shifts the challenge of domain alignment to the simulator itselfwhich we update in real time using sensor feedback. C. World Models for Robotics Learned world models allow robots to simulate future outcomes for planning and control, and can act as natural simulation environment for robotics if trained on specific real-world environments. Traditional models range from 2D black-box predictors to structured simulators with physical priors. UniSim [19] trains general-purpose, actionconditioned video generator that learns from diverse realworld data, simulating rich observations for both highand low-level tasks. This approach, however is not well grounded in the 3D physical world and tends to hallucinate beyond few predicted frames. More recently, 3D physically grounded world models combine particle-based simulation with visual rendering via Gaussian Splatting, enabling real-time tracking and simulated interaction of physical scenes [20], [6] without violating physical and structural constraints. In this work, we explore how they can be utilized within the behavior cloning framework to bridge the advantages of simulation to the real world. Fig. 3. The two modes of the Real-is-Sim framework: online and offline. In online mode, the real robot operates alongside the correctable simulator (Embodied Gaussians). Synchronization is maintained through fictitious visual forces derived from the rendering loss between observed and real images, as well as by the real robot mimicking the simulated robots state. In offline mode, parallel environments execute the learned policy and evaluate its success rate. The simulator can run up to 7x faster than real time with policy sampling in the loop. III. PRELIMINARIES A. Behaviour Cloning In behavior cloning (BC), the goal is to learn policy πθ parameterized by θ that maps an observation ot at time step to sequence of future actions At = [at, at+1, . . . , at+H1] over horizon H. Given dataset of expert demonstrations = {(oi, Ai)}N } is sequence of observations and Ai = {ai } is the corresponding sequence of actions, the policy is trained ot). This is to model the conditional distribution p(At typically done by minimizing the behavior cloning loss: i=1, where oi = {oi 1, . . . , ai 1, . . . , oi LBC(πθ) = E(o,A)D (cid:104) πθ(o) A2(cid:105) , (1) where the policys predicted action sequence is penalized based on its deviation from the expert-provided action sequence. Modeling temporally extended action sequences rather than single-step actions enables smoother and more consistent control [21], [22]. Our approach adopts Conditional Flow Matching (CFM) [23], generative modeling method that learns continuous trajectory distribution over actions. CFM models velocity field vθ(Aτ ot) conditioned on ot, predicting intermediate velocities along path Aτ between zero trajectory and the ground-truth At. The training objective minimizes the conditional flow-matching loss: ot)(cid:13) 2 , (cid:13) (2) where τ U(0, 1) is uniformly sampled interpolation timestep, q(Aτ At) defines reference trajectory (e.g., Gaussian perturbation around linear interpolation), and u() denotes the corresponding ground-truth velocity. LCFM = Eτ, p(Atot), q(Aτ At) vθ(Aτ (cid:13) (cid:13)u(Aτ At) B. Problem Formulation well-known challenge in behavior cloning is that low training loss LBC does not necessarily correlate with high task performance (success rate, reward etc.) during deployment [1], [2], [3]. This disconnect makes regular policy evaluation crucial component of the training process, as it helps identify optimal checkpoints and avoid both underfitting and overfitting. In simulation environments, such evaluations are feasible due to the ability to parallelize rollouts and perform controlled resets, allowing for frequent and scalable assessment of policy performance. Conversely, real-world evaluations are constrained by practical limitations: Manual Resets: Each evaluation requires manual resetting of the environment, which is labor-intensive and prone to inconsistencies. Sequential Execution: Evaluations must be conducted sequentially, as parallel execution is often impractical, leading to prolonged training cycles. Resource Intensiveness: Continuous human supervision is required to ensure safety and correctness during each trial, further escalating the time and effort involved. These challenges underscore the need for methodologies that can bridge the gap between the efficiency of simulationbased evaluations and the fidelity of real-world performance, ensuring that policies trained in simulation translate effectively to real-world applications. IV. METHOD Our method, real-is-sim, enables seamless integration of parallelisable simulation-based evaluation for real-world robot policies by maintaining continuously corrected digital twin of the environment. Figure 2(a) summarizes our framework. The policy operates exclusively in the simulator, while the simulator is updated in real time using observations from the real world. This section outlines the full pipeline of real-is-sim, including: (1) our real-time correctable physics simulator built on Embodied Gaussians, (2) the procedure for scene setup and demonstration collection, (3) training policies using Conditional Flow Matching (CFM), and (4) evaluating policies in both offline and online modes. A. Real-Time Correctable Physics Simulator The cornerstone of real-is-sim is its real-time correctable physics simulator, which maintains synchronization with the real world. Unlike traditional sim-to-real approaches Fig. 4. Comparison of different representations extracted from the Real-to-Sim framework, contrasted with real image baseline. (a) purely state-based representation, utilizing the objects position and orientation (in quaternion form). (b) virtual moving camera positioned on the robots end effector. (c) virtual camera placed at the same location as the original camera that observed the scene. (d) One of the real camera images. All representations are augmented with the robots state. In the Real-to-Sim framework, the joint states correspond to those of the simulated robot, while in the real image representation, the joint states reflect those of the real robot. (Figure 2(c)), where the simulator is used only for training and discarded during deployment, our framework keeps the simulator active at all timesduring demonstration collection, policy execution, and offline rollouts. Our implementation leverages the Embodied Gaussian simulator. Embodied Gaussians [6] is real-time simulator that dynamically aligns with real-world observations through continuous RGB-based correction. It represents the world using dual-representation which is composed of (1) set of oriented particles parameterized by their positions, rotations, masses, and radii and (2) set of 3D Gaussians parameterized by their positions, rotations, scales, color and opacity. The particles are acted upon by particle-based simulator [24] and represent the physical aspect of the world. In contrast, Gaussians represent the visual aspect of the world because they can be rendered using differentiable rendering procedure known as Gaussian splatting [25]. The Gaussians are connected to the particles with rigid links. One particle can have multiple Gaussians attached to it. As particle is moved by the physics system, it simultaneously moves its connected Gaussians as well. The main innovation in Embodied Gaussians is the use of fictitious virtual forces derived from the photometric loss between images received from posed cameras and renders of the scene from the same viewpoints. The displacement of the Gaussians required to minimize the photometric loss is interpreted as scaled fictitious force that acts on the connected particles. The virtual visual forces are, thus, corrective agents that constantly act on the simulated bodies to align them with what is being observed. The correction loop occurs at 30Hz and allows physics simulator to constantly synchronize with the real world while always being in physically feasible state. Our adaptation (built using [26]) differs from the original framework in two key aspects. First, we forgo shapematching constraints for deformable objects, instead modeling all entities as rigid bodies composed of spherical collision primitives. Thus, our simulator is rigid body simulator rather than particle-based simulator. This simplification improves performance from 30Hz to 60Hz in online mode. Second, we treat the robot as dynamic entity, simulating mass and inertia for its links and joints. This contrasts with the original kinematic formulation, where robot motion was strictly kinematic. This makes the simulation more stable and it allows the simulated robot to react to the environment. B. Scene Setup We follow the same procedure outlined in Embodied Gaussians to create the geometries of the objects being manipulated and learn their visual appearance. The result is that each body is represented by pose qb (R3 S3) implemented as position vector and quaternion, as well as set of spherical shapes with fixed radius (empirically tuned) that form the collision geometry of the object. Lastly, the Gaussians forming the visual aspect of the object are learnt with the Gaussian splatting optimization procedure. With the environment constructed and visual appearance learned, we proceed to collect demonstrations using this simulation. C. Demonstration Collection Demonstrations are collected with Embodied Gaussians in the loop. As the user is collecting demonstrations, the simulator is constantly correcting itself from the observations it gets from the cameras. For every demonstration of task, we record the full history of simulation states that occurred. Our physics loop runs at 60Hz and we record 60 states every second. The underlying physical model is rich, but the only properties that change over time are the poses of the bodies and the joint configuration of the robot qr R7 (for our 7 degree of freedom robot). Since we choose not to allow the visual states to change after they are initialized (for simplicity and to ease the data collection footprint), the state of the Gaussians are only stored at the initial time step. All other physical parameters pertaining to the simulator are also stored once at the initial timestep. full list of these are documented in [26]. The recorded synchronized simulation trajectories form the dataset used for policy learning. The success rate of an imitation learning policy as function Fig. 5. of training steps. The plot compares policy evaluation in the offline mode of real-is-sim with the online mode of real-is-sim. The high correlation between the two curves demonstrates that the (significantly faster) offline evaluation mode is reliable proxy for real-world testing, offering practical alternative for policy assessment. Error bars are 95% confidence intervals. D. Policy Training Our policy outputs two components: qd R7, the desired joint configuration of the robot, and scalar progress value [0, 1], which indicates the relative completion (progress) of the task. Progress is heuristic with ground-truth label that is calculated as the relative time elapsed in the demonstration. It assumes that every demonstration ends at the target position. This was shown to work well as heuristic in [27]. We use it in our work to evaluate the trained policy without needing to engineer reward. Our policy is conditioned on the current joint configuration of the simulated robot. In the experiment section, state-based policies are also conditioned on an objects pose encoded as position and quaternion (a quaternion is used for simplicity because the pose is stored natively in that form and because its double cover property did not prove to be problem for our task). For image-based policies we use ResNet to encode an image into 64 dimensional vector which we condition on. For image-based policies, we found that random crops were crucial for successful learning. Embodied Gaussians allows us to render images from any viewpoint and create virtual cameras in the setup. The representations used to condition the policyincluding object pose and rendered imagesare explored further in Section V-D. Our policies output 32 actions which are meant to be executed at 30Hz. During evaluation, we execute all 32 actions before resampling. E. Offline Policy Evaluation During offline policy evaluation, we create 20 parallel environments as shown in Figure 3(b) and run the policies Fig. 6. Success rates of different demonstration collection strategies. dataset consisting of 30 real demonstrations achieves success rate of approximately 58%. Since Real-to-Sim can also operate offline, the demonstrations can be augmented with additional simulated data. combined dataset of 60 demonstrations (30 real and 30 simulated) achieves an 80% success rate. In comparison, dataset of 60 real demonstrations alone achieves slightly lower success rate of 72%. Error bars are 95% confidence intervals. for predetermined time (90 seconds). We regard any policy that has not reached progress of above 0.9 as failure. For state-based policies with 20 environments, we can run the simulation with policy evaluation at 8x realtime on an NVIDIA 6000Ada. With one virtual camera rendering the scene in every environment, the speed is 5x realtime. V. EXPERIMENTS A. Experimental Setup We evaluate our framework on the PushT task: long horizon task where robot uses pusher to position T-shaped object into predefined central location. The TBlocks geometry (represented as set of spherical shapes attached to single body frame) is automatically generated using the method from Embodied Gaussians [6]. We use three RealSense D455 cameras running at 90 fps with resolution of 420 270. The robot mesh is known, and the ground plane is estimated from the camera-generated pointclouds. Gaussians attached to the robot, table, and TBlock are learned once using the Gaussian Splatting procedure outline in [6] and reused across experiments. For state-based policies, we use the pose of the object encoded as translation vector R3 and unit quaternion SO(3), where = (x, y, z) and = (qx, qy, qz, qw). For image-based policies, we use 64 dimensional feature vector output by Resnet [22]. The images input to the Resnet are rendered from the simulation using Gaussian Splatting from virtual cameras. These cameras can be placed anywhere. In (Figure 5). We then compare two evaluation methodologies: (i) offline rollouts using real-is-sim and (ii) real-world online evaluations. Our offline approach initializes environments with identical test cases to online evaluations. The offline evaluation calculates success rates through the policyreported progress. While we use this metric to maintain generality, practitioners could implement more sophisticated reward functions for their specific tasks. We selected four representative checkpoints (10k, 15k, 25k, 50k) for online evaluation (each requires around 1 hour to perform) while evaluating more comprehensively through offline mode. Figure 5 reveals strong correlation between evaluation curves, demonstrating that faster offline evaluations (minutes vs. hours) effectively approximate realworld results. Notably, later checkpoints do not consistently outperform earlier ones, suggesting practitioners could use offline evaluation for either early stopping or optimal checkpoint selection. While this experiment uses identical initial states for both evaluation modes to establish correlation, real applications could benefit from randomized initial states during training-phase evaluations. C. Offline Data Collection and Augmentation In this experiment, we demonstrate how real-is-sims offline mode can be used to to collect additional demonstrations to increase task performance (Figure 6). We train statebased policy using 30 real-world demonstrations collected in online mode - which achieves success rate of around 57%. We also augment the real-world demonstrations using the simulator only (offline mode). To collect the additional demonstrations, we deploy this policy in offline mode across multiple parallel environments and identify failure states where the policy falls into local minima. These failure states are then presented to the user to provide an additional 30 demonstrations specifically solving the task under these challenging configurations. By combining the original 30 real-world demonstrations with the 30 new simulated demonstrations, we retrain the policy and observe significant improvement to 80% success rate. Lastly as baseline, we also collect 30 demonstrations in simulation only. The resulting sim-only policy attains similar success rate during deployment. The success rate of the offline only policy is comparable to the policy trained on real demonstrations. This is largely owing to the kinematic nature of the PushT task which means that successfully policy rollouts are mostly dependent on poses and not dynamic effects. This is precisely what Embodied Gaussians can easily correct upon deployment. This experiment highlights straightforward yet powerful application of real-is-sims offline mode. The cost of augmenting the data used for policy training using real-issims simulator is much lower than doing so in the real world. As comparison, we also train policy using 60 real-world demonstrations collected without augmentation. Interestingly, this approach yields similar performance improvement as the augmented dataset, underscoring the Fig. 7. Success rates of policies using different representations to condition the flow matching policy. The baseline image-based policy (not using realis-sim) uses static camera and has success rate of 63%. The state-based policy, which uses the simulators object pose achieves higher success rate of 76%. Lastly, policies trained on the rendered virtual cameras (one statically mounted and another mounted on the gripper) achieve 73% and 82% respectively. This shows that gripper-mounted camera is likely better representation for the PushT task. Error bars are 95% confidence intervals. our experiments, we use static camera placed in the same location as the real camera and virtual gripper camera which we mount to the simulated robots end-effector (these are shown in Figure 7). To measure policy success on the PushT task, we define 20 in-distribution starting poses on the table that were not part of the demonstration set. In real-world testing, success is determined by whether the policy drives the T-Block to the target location from these poses. Each pose is tested three times, resulting in 60 measurements per success rate evaluation. Error bars in figures represent 95% confidence intervals. B. Offline Evaluation In this experiment, we demonstrate how real-is-sims offline mode facilitates policy training and evaluation. key challenge in imitation learning lies in selecting optimal checkpoints for deployment. This difficulty arises because the loss used in behavior cloning does not reliably indicate policy performance, and real-world robot evaluations require substantial time and resources. Consequently, researchers often default to using the final training checkpoints despite it being suboptimal. We demonstrate that real-is-sims offline mode enables efficient checkpoint selection through rapid evaluation. For state-based policy trained on 30 demonstrations, we implement flow-matching policy trained for 50,000 steps effectiveness of real-is-sims offline mode in efficiently enhancing policy training. D. Representation Flexibility Another major benefit of real-is-sims offline mode is its flexibility in extracting diverse representations from the it enables Embodied Gaussians simulation. For instance, access to privileged informationsuch as precise object posesthat would be difficult to obtain in the real world. Additionally, virtual cameras can be freely positioned within the system, facilitating visuomotor policy learning from multiple perspectives. In our experiments, we demonstrate that policies leveraging different representationsincluding state-based object poses, gripper-mounted virtual camera, and static virtual cameraall successfully solve the task (Figure 7, 4). Notably, these approaches achieve performance comparable to, and in some cases surpassing, the baseline policy that relies solely on real-world images and robot actions. Interestingly, the policy using the gripper-mounted virtual camera achieves the highest performance (82%) among all representations. This policy also exhibits behaviors such as as actively searching for the T-block when it leaves the cameras field of view (see videos on our website). These findings align with prior work highlighting the advantages of wristmounted cameras over static ones in certain scenarios [28]. Within the real-to-sim framework, we can systematically evaluate different virtual camera configurations [29] and their impact on policy learningall using the same set of demonstrations. This flexibility allows us to identify the optimal representation for given manipulation task in offline mode and seamlessly deploy it in online execution. VI. LIMITATIONS While our framework demonstrates promising results, its applicability is inherently constrained by the capabilities of the underlying physical simulator. Scenarios that deviate significantly from the simulators modeling assumptions pose challenges to both model accuracy and corrective mechanisms. To address these constraints, future improvements could focus on enhancing simulator fidelity through adaptive parameter estimation (e.g., real-time optimization of physical properties like friction or mass, as explored in works such as [20] and [30]). While this work focused on the PushT task to validate core contributions, the methodology is extensible to broader manipulation domains. Enabling applications such as multi-object rearrangement, cloth manipulation, or non-prehensile pushing will require tailoring the simulators dynamics model and corrective policies to handle domainspecific interactions. VII. FUTURE WORK While the always-in-the-loop simulator already corrects itself using observations, its accuracy could be further enhanced by backpropagating into physics parameters or learning residual physics network, reducing reliance on explicit corrections by making the simulators predictions more physically faithful. more accurate simulator would also enable reinforcement-learning-based fine-tuning of imitationlearned policies. Additionally, this work focused on offline policy evaluation, but future extensions could incorporate online planning during deployment for real-time adaptability. Finally, Embodied Gaussians could be extended to encode higher-level features such as DINO embeddings [31] or other semantic descriptors. This would enrich policy training and further make use of the simulators role as mediator between the real world and the policy. VIII. CONCLUSIONS We introduced real-is-sim, unified framework for policy training, evaluation, and deployment that maintains continuously corrected simulation throughout the entire pipeline. By treating simulation as persistent, real-time interface, enabled by Embodied Gaussians, we eliminate the disconnect between training and deployment domains. This reframes policy evaluation as simulation-native process, even in real-world settings. Our results on the PushT task show strong alignment between offline and real-world performance, demonstrating that real-is-sim enables scalable, efficient evaluation without sacrificing fidelity. We hope this framework serves as foundation for future real-world learning algorithms that leverage this tight coupling between simulation and reality."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Kress-Gazit, K. Hashimoto, N. Kuppuswamy, P. Shah, P. Horgan, G. Richardson, S. Feng, and B. Burchfiel, Robot learning as an empirical science: Best practices for policy evaluation, arXiv preprint arXiv:2409.09491, 2024. [2] J. A. Vincent, H. Nishimura, M. Itkina, P. Shah, M. Schwager, and T. Kollar, How generalizable is my behavior cloning policy? statistical approach to trustworthy performance evaluation, IEEE Robotics and Automation Letters, 2024. [3] D. Snyder, A. J. Hancock, A. Badithela, E. Dixon, P. Miller, R. A. Ambrus, A. Majumdar, M. Itkina, and H. Nishimura, Is your imitation learning policy better than mine? policy comparison with near-optimal stopping, arXiv preprint arXiv:2503.10966, 2025. [4] Z. Zhou, P. Atreya, Y. L. Tan, K. Pertsch, and S. Levine, Autoeval: Autonomous evaluation of generalist robot manipulation policies in the real world, arXiv preprint arXiv:2503.24278, 2025. [5] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, et al., Evaluating real-world robot manipulation policies in simulation, arXiv preprint arXiv:2405.05941, 2024. [6] J. Abou-Chakra, K. Rana, F. Dayoub, and N. Suenderhauf, Physically embodied gaussian splatting: visually learnt and physically grounded 3d representation for robotics, in 8th Annual Conference on Robot Learning, 2024. [Online]. Available: https: //openreview.net/forum?id=AEq0onGrN [7] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, 2024. [8] A. Kadian, J. Truong, A. Gokaslan, A. Clegg, E. Wijmans, S. Lee, M. Savva, S. Chernova, and D. Batra, Sim2real predictivity: Does real-world performance? IEEE evaluation in simulation predict Robotics and Automation Letters, vol. 5, no. 4, pp. 66706677, 2020. [9] X. Li, K. Hsu, J. Gu, O. Mees, K. Pertsch, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. Vuong, and T. Xiao, Evaluating real-world robot manipulation policies in simulation, in Proceedings of The 8th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, P. Agrawal, O. Kroemer, and W. Burgard, Eds., vol. 270. PMLR, 0609 Nov 2025, pp. 37053728. [10] S. Silwal, K. Yadav, T. Wu, J. Vakil, A. Majumdar, S. Arnaud, C. Chen, V.-P. Berges, D. Batra, A. Rajeswaran, et al., What do we learn from large-scale study of pre-trained visual representations in sim and real environments? in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 17 51517 521. [11] M. Khanna, Y. Mao, H. Jiang, S. Haresh, B. Shacklett, D. Batra, A. Clegg, E. Undersander, A. X. Chang, and M. Savva, Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 16 38416 393. [12] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, Domain randomization for transferring deep neural networks from simulation to the real world, in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017. [13] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, and K. Bousmalis, Sim-to-real via simto-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [14] A. Z. Ren, H. Dai, B. Burchfiel, and A. Majumdar, Adaptsim: Taskdriven simulation adaptation for sim-to-real transfer, in Proceedings of The 7th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, J. Tan, M. Toussaint, and K. Darvish, Eds., vol. 229. PMLR, 0609 Nov 2023, pp. 34343452. [15] N. Pfaff, E. Fu, J. Binagia, P. Isola, and R. Tedrake, Scalable real2sim: Physics-aware asset generation via robotic pick-and-place setups, arXiv preprint arXiv:2503.00370, 2025. [16] M. T. Villasevil, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal, Reconciling reality through simulation: real-to-sim-toreal approach for robust manipulation, in Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. [17] Y. Jiang, C. Wang, R. Zhang, J. Wu, and L. Fei-Fei, Transic: Sim-toreal policy transfer by learning from online correction, in Proceedings of The 8th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, P. Agrawal, O. Kroemer, and W. Burgard, Eds., vol. 270. PMLR, 0609 Nov 2025, pp. 16911729. [18] D. Tirumala, M. Wulfmeier, B. Moran, S. Huang, J. Humplik, G. Lever, T. Haarnoja, L. Hasenclever, A. Byravan, N. Batchelor, N. Sreendra, K. Patel, M. Gwira, F. Nori, M. Riedmiller, and N. Heess, Learning robot soccer from egocentric vision with deep reinforcement learning, arXiv preprint arXiv:2405.02425, 2024. [19] S. S. Yang, Y. Du, K. Ghasemipour, J. Tompson, L. Kaelbling, D. Schuurmans, and P. Abbeel, Learning interactive real-world simulators, arXiv preprint arXiv:2310.06114, 2024. [20] H. Jiang, H.-Y. Hsu, K. Zhang, H.-N. Yu, S. Wang, and Y. Li, Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos, arXiv preprint arXiv:2503.17973, 2025. [21] T. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained bimanual manipulation with low-cost hardware, Robotics: Science and Systems XIX, 2023. [22] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, p. 02783649241273668, 2023. [23] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, 2023. [Online]. Available: https://arxiv.org/abs/2210.02747 [24] M. Macklin, M. Muller, and N. Chentanez, Xpbd: Position-based simulation of compliant constrained dynamics, in Proceedings of the 9th International Conference on Motion in Games, ser. MIG 16. New York, NY, USA: Association for Computing Machinery, 2016, p. 4954. [Online]. Available: https://doi.org/10.1145/2994258.2994272 [25] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering, ACM Transactions on Graphics, vol. 42, no. 4, [Online]. Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ July 2023. [26] M. Macklin, Warp: high-performance python framework for gpu simulation and graphics, in NVIDIA GPU Technology Conference (GTC), 2022. [27] K. Rana, J. Abou-Chakra, S. Garg, R. Lee, I. Reid, and N. Suenderhauf, Affordance-centric policy learning: Sample efficient and generalisable robot policy learning using affordance-centric task frames, arXiv preprint arXiv:2410.12124, 2024. [28] K. Hsu, M. J. Kim, R. Rafailov, J. Wu, and C. Finn, Vision-based manipulators need to also see from their hands. in ICLR, 2022. [29] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, Rvt: Robotic view transformer for 3d object manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 694710. [30] A. Longhini, M. Busching, B. P. Duisterhof, J. Lundell, J. Ichnowski, M. Bjorkman, and D. Kragic, Cloth-splatting: 3d cloth state estimation from RGB supervision, in 8th Annual Conference on Robot Learning, 2024. [Online]. Available: https://openreview.net/ forum?id=WmWbswjTsi [31] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023."
        }
    ],
    "affiliations": [
        "Queensland University of Technology",
        "Robotics and AI Institute"
    ]
}