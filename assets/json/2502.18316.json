{
    "paper_title": "WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging",
    "authors": [
        "Ahmed Elhady",
        "Eneko Agirre",
        "Mikel Artetxe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with \"None of the above\", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 6 1 3 8 1 . 2 0 5 2 : r WiCkeD: Simple Method to Make Multiple Choice Benchmarks More Challenging Ahmed Elhady1 Eneko Agirre1 Mikel Artetxe1, 1HiTZ Center, University of the Basque Country (UPV/EHU) 2Reka AI {ahmed.salemmohamed,e.agirre,mikel.artetxe}@ehu.eus"
        },
        {
            "title": "Abstract",
            "content": "We introduce WiCkeD, simple method to increase the complexity of existing multiplechoice benchmarks by randomly replacing choice with \"None of the above\", method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chainof-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ ahmedselhady/wicked-benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Multiple choice question (MCQ) benchmarks are widely used to evaluate Large Language Models (LLMs). This format consists of question and limited set of options, which include correct (or best) answer and several distractors that are either incorrect or less appropriate (see Figure 1). There are various MCQ datasets that focus on different capabilities, including factual knowledge and reasoning as in MMLU (Hendrycks et al., 2021) and Arcchallenge (Clark et al., 2018), common sense as in Commonsense-QA (Talmor et al., 2019), truthfulness as in TruthfulQA (Lin et al., 2022), and domain-specific knowledge (Alonso et al., 2024; Hosseini et al., 2024). Unfortunately, most of these benchmarks got quickly saturated in the recent era dominated by LLMs, motivating harder datasets to 1 better gauge the abilities of newer models. However, developing benchmarks is laborious and expensive process. Motivated by this, several recent works have explored strategies to make existing benchmarks harder, which can serve as an alternative to creating new benchmarks from scratch. For example, Gema et al. (2024) identified erroneous questions in the MMLU benchmark, and re-annotated 3k questions to be harder and more robust. Similarly, Wang et al. (2024) presented MMLU-Pro, harder version of the MMLU benchmark that replaces noisy questions with harder ones and expands the number of distractors to include more plausible yet incorrect ones. While increasing the number of distractors reduces the probability of correct guesses by chance, creating plausible and coherent distractors is challenging and often requires manual verification (McIntosh et al., 2024). In this work, we propose simple yet effective method to make existing benchmarks more challenging without the need to add distractors. Namely, we present the Wild-Card Distractor (WiCkeD) which creates variant of any existing MCQ benchmark by keeping the question unchanged, and randomly replacing one of the choices with wild-card distractor, None of the above (see Figure 1). We create WiCkeD variants of 6 popular benchmarks, and use them to evaluate 18 open-weight LLMs varying in size, model family, and training recipe. The WiCkeD datasets suffer performance drop of 7.2-19.7 points with respect to the original datasets, depending on the model being evaluated. Using chain-of-thought does not prevent the drop (1.4-14.6), showing that WiCkeD can be used to assess reasoning capabilities. The large variance across models shows that WiCkeD is not only challenging, but it also uncovers differences in model capabilities that are not captured by the original benchmarks. Figure 1: Two samples from MMLU-Pro (left) and its WiCkeD variant (right), where Hydrogen and Centrifugal were removed. Correct answers in bold. Llama-3.1 8B correctly answers both original questions but fails on the WiCkeD variant for the second question. The probability distribution of the model for each answer is also shown."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Challenges in LLMs MCQ Benchmarks Several works raised concerns about the effectiveness of MCQ benchmarks in LLM assessment. For example, Balepur et al. (2024) showed that some LLMs can answer MCQs using only the answer choices, without seeing the questions, and perform well-above baselines. Furthermore, more works suggested that LLMs are biased towards certain answer keys (A/B/C/D) due to unbalanced prior probabilities rather than actual knowledge (Myrzakhan et al., 2024; Clark et al., 2018). Another line of research attributes LLMs hallucinations to being unable to identify when they lack sufficient knowledge about the subject matter (Li et al., 2024; Ji et al., 2022). Nonetheless, current evaluation benchmarks do not assess this capability effectively. We view our work as an addition towards efficient evaluation of LLMs to avoid spurious correlations and account for knowledge and reasoning gaps."
        },
        {
            "title": "2.2 None of the Above in Educational Tests",
            "content": "Multiple-choice questions (MCQs) are effective assessments when they include plausible distractors, as they encourage deeper processing to think not only about why given choice is correct, but also why other choices are wrong and improve knowledge recall (Little et al., 2019; Little and Bjork, 2015). The use of None of the above as distractor in MCQs is an area of research and debate. It can provide unique insight into the understanding of the examinees and potentially differentiate their abilities (David DiBattista and Fortuna, 2014; Dochy et al., 2001). However, None of the above can affect the confidence of the examinee, leading them to avoid selecting None of the above as the correct answer, even when it is true (Little, 2023; Odegard and Koen, 2007). Nevertheless, incorporating None of the above into practice tests can enhance the learning process by encouraging deeper engagement with the material (David DiBattista and Fortuna, 2014; Pezeshkpour and Hruschka, 2024; Zheng et al., 2024)."
        },
        {
            "title": "3 Methodology",
            "content": "We propose method to automatically create more challenging version of any existing MCQ benchmark without requiring any manual annotation. The difficulty of MCQ has been linked to the reasoning necessary to discriminate between competing options (McIntosh et al., 2024; Wang et al., 2024). We hypothesize that detecting the absence of the correct answer within the provided options is more challenging than selecting the correct one. To that end, we propose to add wild-card choice None of the above. Note that adding None of the above as an additional option would not make sense, as the correct answer is always the correct option, we thus propose to replace one of the options instead."
        },
        {
            "title": "3.1 The WiCkeD Algorithm",
            "content": "Given benchmark that consists of examples where each has choices (one correct answer and 1 distractors), we uniformly sample one option to be omitted, and append the wildcard option None of the above. When the correct option is replaced, the new correct option is None of the above. When distractor option is replaced, the correct option continues to be correct. Figure 1 shows the result of applying WiCkeD to two examples. The goal is to produce variant for each benchmark that contains the same number of examples. 2 Model Size IT Original WiCkeD DS-R1-Llama DS-R1-Qwen 8B 7B - - 8B - 8B - 70B 70B 7B - 7B 7B - 7B 14B - 14B 72B - 72B - 9B 9B 27B - 27B Llama-3.1 Mistral Qwen-2.5 Gemma-2 Average 56.6 60.8 61.4 66.0 76.8 77.1 59.8 59.0 74.7 73.5 78.9 78.9 84.6 82.6 67.3 73.3 68.0 74.8 70.78 48.6 53. 52.2 55.0 67.0 64.5 46.5 47.2 54.9 59.0 66.3 66.6 72.6 69.3 -7.9 1.1% -7.3 1.6% -9.2 1.7% -11.0 0.9% -9.8 2.1% -12.6 1.3% -13.2 1.2% -11.8 1.1% -19.7 1.5% -14.5 1.3% -12.6 2.1% -12.3 1.8% -12.0 0.9% -13.3 1.0% 56.3 57.6 54.6 61.9 58.52 -10.9 1.2% -15.7 1.2% -13.4 2.0% -12.9 2.3% -12.2 26.3% Table 1: Average performance on original and WiCkeD variants of the six benchmarks. IT: instruction-tuned. : degradation from original performance Appendix for more details about the training and evaluation procedure."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "We apply WiCkeD to six popular MCQ benchmarks that assess the knowledge, language comprehension, reasoning, and truthfulness of LLMs: MMLU, MMLU-Pro, MMLU-Redux, CommonsenseQA, Truthful-QA, and Arc-challenge. To ensure reproducibility, we use Eval-Harness (Gao et al., 2024). Given that the selection of the option to be replaced is random, we generate five WiCkeD variants for each benchmark, and report mean and standard deviation. Regarding the amount of SBA examples, MMLU, MMLU-Redux and MMLU-pro have the largest amount ( 20%), with the rest of the benchmarks having less than 5% (see Appendix A). SBA examples are copied verbatim to the WiCkeD variants, but the fact that at least 80% of the examples are effectively altered makes the WiCkeD variants significantly more challenging, as we will see. Other benchmarks have less than 5% SBAs; we also leave them unchanged."
        },
        {
            "title": "4.2 Models",
            "content": "We evaluate WiCkeD on 18 open-weight models covering different families and sizes. Namely, we evaluate the base and instruction-tuned models of Qwen2.5 7B, 14B and 72B (Qwen et al., Figure 2: Applying WiCkeD on single best answer (SBA) example (best answer D, second best answer A) would lead to an incoherent WiCkeD variant (incorrectly having None of the above as the gold correct answer instead of A). We thus copy SBA examples verbatim, see 3.2 for details. 3.2 Coherence of WiCkeD Examples The above algorithm does not always produce coherent examples. In some cases, there are more than one correct candidate, but only one of them is the most appropriate (see Figure 2, where is the best answer and is the second best answer). With the above procedure, when the replaced option is the correct one (e.g. option in the figure), the WiCkeD variant would add None of the above and take this option as the correct one. However, this would be incoherent, because having removed D, becomes the next best option. We call these examples Single Best Answer (SBA) as opposed to Single Correct Answer (SCA, where the distractors are all incorrect). As we want to keep the same number of examples we avoid adding None of the above to SBA examples and copy them unchanged to the WiCkeD variant of the benchmark. In order to train an example classifier to detect SBA examples, we selected four representative benchmarks (MMLU, MMLU-Pro, TruthfulQA and Commonsense-QA), sampled 4000 examples, and split them into evaluation (25%) and train (75%). We used GPT-4o-mini to automatically label the examples as SBA or SCA, and further annotated the evaluation split manually. Given the cost and slow speed of GPT-4o-mini, we used the synthetic labels to train classifier based on BERT1 (Devlin et al., 2019). The recall on SBA examples for the classifier is over 98.9%, showing that we are able to detect nearly all SBA examples, and would thus have 1.1% noisy WiCkeD examples (that is, examples in the benchmark that have None of the above as the correct option even if correct option exists). See 1https://huggingface.co/ahmedselhady/ bert-base-uncased-sba-clf 3 Model Size IT WiCkeD Direct DS-R1-Llama DS-R1-Qwen Llama-3.1 Mistral Qwen-2.5 GemmaAverage 8B 7B - - 8B - 8B 7B - 7B 7B - 7B 14B - 14B 9B - 9B 27B - 27B 30.3 30.6 39.7 43.6 35.9 33.5 45.5 47.1 55.6 56.7 36.1 44.1 36.1 51.3 41. -4.1 -4.3 -3.2 -2.7 -3.4 -5.7 -6.9 -5.3 -3.6 -3.4 -12.2 -9.3 -10.8 -3.8 -5. CoT WiCkeD 80.1 74.9 53.9 57.2 36.3 43.8 43.0 55.4 61.5 64. 41.2 56.3 59.2 60.3 56.26 -2.0 -2.5 -5.8 -3.4 -11.62 -4.97 -14.62 -1.73 -3.97 -1. -8.93 -4.36 -4.07 -3.77 -5.24 Table 2: Performance on WiCkeD variants for MMLU, MMLU-pro, and MMLU-Redux without and with CoT. IT: instruction-tuned. : degradation from the original benchmark. 2025), Llama3.1 8B and 70B (Grattafiori et al., 2024), Gemma2 9B and 27B (Riviere et al., 2024), and Mistral-7B (Jiang et al., 2023). We also selected two DeepSeek-R1 models for their improved reasoning capabilities: distill-Lllama3.1-8B and distill-Qwen7 (DeepSeek-AI et al., 2025). The LLM models are evaluated on the benchmarks following the standard multiple-choice prompting procedure (Robinson et al., 2023), see Appendix C. We set the number of few-shot examples to five, in order to ensure that in most cases there is at least one example where None of the above is the correct option. In addition, we also evaluate the LLM models using zero-shot chain-of-thoughts prompting (CoT) on the three benchmarks commonly used to assess the reasoning capabilities of LLMs: MMLU, MMLU-Pro, and MMLU-Redux. We set the maximum generation length to 4096, unless limited by the model itself."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 1 shows the mean accuracy of the models on the original and WiCkeD benchmarks, with significant drop in performance. Qwen2.5-7B suffers the largest degradation (19.73%), while its DeepSeekR1 distilled version (DeepSeek-R1-Qwen7B) suffers the least (7.35%). This suggests that models with better reasoning capabilities, like R1, are better equipped to deal with the added complexity. Prominently, the WiCkeD variants shuffle the ranking of models. For example, the Qwen2.5-7B and Qwen2.5-7B-IT models originally performed close to the Llama-3.1-70B model. However, on the WiCkeD variants, they lag behind it by 13% and 8%, respectively. Similar patterns can be seen in Gemma-2-9B-IT and Gemma-2-27B-IT, which lag behind Llama-3.1-70B by 9.5% and 5.3%, respectively. Qwen2.5-72B and Llama-3.1-70B are the models that perform best in WiCkeD. There is no clear advantage from instruction-tuning, as results vary depending on the model family. 5.2 Chain of Thought Results Table 2 shows the performance of the models2 on the MMLU, MMLU-pro, and MMLU-Redux WiCkeD benchmarks. The drop for these three benchmarks without CoT (direct columns in the table) is lower than the other three benchmarks, but applying CoT does not reduce the drop in WiCkeD variants, which stays above 5%. This is remarkable given that CoT is very effective at improving results on MMLU and related benchmarks. Instructiontuned models experience significantly less degradation than their base models, especially when using CoT (see Appendix for additional details). Notably, the DeepSpeed-R1 distilled models, Qwen7B and Llama3.1-8B, suffer 2% each. Similarly, Instruction tuned Qwen2.5 7B and 14B suffer less than 2%. We hypothesize this is due to their enhanced reasoning capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced simple automatic method to create more challenging variants from existing MCQ benchmark. The large drop in the results shows that WiCkeD challenges the knowledge and reasoning of LLMs, as they need to identify the absence of the correct answer, even when using CoT. We showed that models with better reasoning capabilities suffer less in WiCkeD, such as the original Qwen7B and its distilled version of DS-R1. We see WiCkeD as an addition towards efficient evaluation of LLMs to avoid spurious correlations and challenge reasoning and knowledge gaps. deeper look into why some models are more sensitive to WiCkeD than others can provide significant insights about uncovered limitations. We release all the code and data under open licenses. 2Due to computing constraints we could not run CoT for the 70B models"
        },
        {
            "title": "Limitations",
            "content": "We manually confirmed the applicability of WiCkeD on some popular multiple-choice benchmarks whose questions can be categorised into SBAs and SCAs. However, for other benchmarks, WiCkeD might need further verification. Furthermore, we focus the evaluation of WiCkeD on openweight LLMs only, the performance of closed models, such as GPT-4 and Claude, has not been explored yet."
        },
        {
            "title": "References",
            "content": "Iñigo Alonso, Maite Oronoz, and Rodrigo Agerri. 2024. Medexpqa: Multilingual benchmarking of large language models for medical question answering. Artificial Intelligence in Medicine, 155:102938. Nishant Balepur, Abhilasha Ravichander, and Rachel Rudinger. 2024. Artifacts or abduction: How do llms answer multiple-choice questions without the question? Preprint, arXiv:2402.12483. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Jo-Anne Sinnige-Egger David DiBattista and Glenda Fortuna. 2014. The none of the above option in multiple-choice testing: An experimental study. The Journal of Experimental Education, 82(2):168183. 5 others. 2024. framework for few-shot language model evaluation. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. 2024. Are we done with mmlu? Preprint, arXiv:2406.04127. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. Pedram Hosseini, Jessica M. Sin, Bing Ren, Bryceton G. Thomas, Elnaz Nouri, Ali Farahanchi, and Saeed Hassanpour. 2024. benchmark for long-form medical question answering. Preprint, arXiv:2411.09834. Yunjie Ji, Liangyu Chen, Chenxiao Dou, Baochang Ma, and Xiangang Li. 2022. To answer or not to answer? improving machine reading comprehension model with span-based contrastive learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 12921300, Seattle, United States. Association for Computational Linguistics. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. Preprint, arXiv:1810.04805. Filip Dochy, George Moerkerke, Erik De Corte, and Mien Segers. 2001. The assessment of quantitative problem-solving skills with none of the aboveitems (nota items). European Journal of Psychology of Education, 16:163177. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. 2024. Think twice before trusting: Self-detection for large language models through comprehensive answer reflection. Preprint, arXiv:2403.09972. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. Preprint, arXiv:2109.07958. Jeri Little. 2023. Does using none-of-the-above (nota) hurt students confidence? Journal of Intelligence, 11(8):157. Jeri Little and Elizabeth Ligon Bjork. 2015. Optimizing multiple-choice tests as tools for learning. Memory & cognition, 43:1426. 5 and Wenhu Chen. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models are not robust multiple choice selectors. Preprint, arXiv:2309.03882. Jeri Little, Elise Frickey, and Alexandra Fung. 2019. The role of retrieval in answering multiplechoice questions. Journal of Experimental Psychology: Learning, Memory, and Cognition, 45(8):1473. Timothy R. McIntosh, Teo Susnjak, Nalin Arachchilage, Tong Liu, Paul Watters, and Malka N. Halgamuge. 2024. Inadequacies of large language model benchmarks in the era of generative artificial intelligence. Preprint, arXiv:2402.09880. Aidar Myrzakhan, Sondos Mahmoud Bsharat, and Zhiqiang Shen. 2024. Open-llm-leaderboard: From multi-choice to open-style questions for llms evaluation, benchmark, and arena. Preprint, arXiv:2406.07545. Timothy Odegard and Joshua Koen. 2007. none of the above as correct and incorrect alternative on multiple-choice test: Implications for the testing effect. Memory, 15(8):873885. Pouya Pezeshkpour and Estevam Hruschka. 2024. Large language models sensitivity to the order of options in multiple-choice questions. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 20062017, Mexico City, Mexico. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, and 178 others. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Joshua Robinson, Christopher Michael Rytting, and David Wingate. 2023. Leveraging large language models for multiple choice question answering. Preprint, arXiv:2210.12353. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue,"
        },
        {
            "title": "B Instruct vs Base Models on Chain of",
            "content": "To ensure the reliability of the automatic identification of single-best-answer (SBA) questions, we uniformly sample 4K questions from the MMLU, MMLU-Pro, Commonsense-QA, and Truthful-QA benchmarks, which we divide into 1K and 3K splits. We then manually annotate the 1K samples and optimize GPT-4o-mini prompt on them for best recall. Table 4 shows the prompt template for GPT-4omini, which we used to annotate the 4K questions. The 3K split was then used to train our Bert-based SBA classifier on them. The classifier was trained for 2 epochs, using learning rate of 1e-04. The model was frozen, except for the last layer and the classification head. Table 3 shows the percentages of SBA questions on the 1K split as determined by our manual annotations, GPT-4o-mini, and the SBA classifier. The classifier is the preferred one, as it is the most conservative, that is, it detects the most SBA examples, which would be copied verbatim to the WiCkeD variant of the benchmark. The evaluation figures in the table confirm this choice, as the classifier has higher recall. The small drop in precision is harmless, as it means that we will not add None of the above option to those examples, and will be copied verbatim. In other words, we can estimate that WiCkeD contains 1% of incoherent examples (where there is valid option even if None of the above is recorded as the correct option), and 5% of examples which do not have None of the above option even if we could have added it if the classifier had 100% precision. These figures confirm the high quality of the WiCkeD variants. Table 5 shows the final SBA percentages for each benchmark as determined by the classifier."
        },
        {
            "title": "Thought",
            "content": "Results of CoT suggest the instruct models experience less degradation than their base models. To better understand why this happens, we analyze their answers. Figure 3 shows the change in answers from the original to the WiCkeD variants. Instruction-tuned models are less prone to reverse correct answers and can correct original mistakes in WiCkeD. This suggests that WiCkeD is useful for better gauging the reasoning capabilities of the models."
        },
        {
            "title": "C Multiple Choice Prompting",
            "content": "the model is In multiple choice prompting, prompted with few-shot demonstrations and question and the set of choices = {A, B, C, D}. It generates probability of the answer label aϵA conditioned on the prefix prompt given by: P(ac, q) = t=1 p(atc, < ) (1) The models answer is set to: argmax aϵA (P (ac, q)) (2) Figures 4, 5, 6, and 7 show example prompts for the MMLU college computer science, Arc Challenge, Common-sense QA, and MMLU-Redux benchmarks, respectively. Figure 3: The changes in models answers of the original benchmarks and the WiCkeD variant using chain-ofthoughts. 7 MMLU MMLU-Pro TQA CSQA Recall Precision Manual GPT-4o-mini SBA Classifier 17.3 18.2 19.6 12.3 13 14.2 3.3 4.2 4. 3.8 3.9 4 98.5 98.9 97.4 95.1 Table 3: The Percentage of Single Best Answer (SBA) questions in 1K questions sampled uniformly from MMLU, MMLU-Pro, TruthfulQA (TQA), and CommonsenseQA (CSQA) as determined by our manual Annotations, GPT4o-mini, and our trained SBA classifier. Recall and precision are computed with respect to the manual annotation. \"A single correct answer question is question that can have exactly one correct answer from given set of choices. single best answer question can have most appropriate answer (for example, if this answer is omitted, another answer will be correct). Classify the following questions into SBA and non-SBA questions. Assign label of 1 if the question is SBA question and label of 0 otherwise. Question: {question} Class:\" Table 4: SBA Annotation Prompt Template MMLU MMLU-Pro MMLU-Redux TruthfulQA Commonsense QA Arc Challenge 20.3% 16.8% 14.7% 3.2% 3.7% 5.2% Table 5: The Percentage of Single Best Answer (SBA) questions in the benchmarks as determined by our SBA classifier. We do not apply WiCkeD to SBA questions as it can break their coherence. Figure 4: Examples from the MMLU computer science task using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results. Figure 5: Examples from the AllenAi Arc challenge using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results. The first few-shot example does not include None of the above option because it was classified as SBA question. 8 Figure 6: Examples from the Common-sense QA using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results. The first few-shot example does not include None of the above option because it was classified as SBA question. Figure 7: Examples from the MMLU-Redux using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results."
        }
    ],
    "affiliations": [
        "HiTZ Center, University of the Basque Country (UPV/EHU)",
        "Reka AI"
    ]
}