{
    "paper_title": "Steering Rectified Flow Models in the Vector Field for Controlled Image Generation",
    "authors": [
        "Maitreya Patel",
        "Song Wen",
        "Dimitris N. Metaxas",
        "Yezhou Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifier-free guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop a theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in a deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is a unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-of-the-art results. Project Page: \\url{https://flowchef.github.io}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 0 0 1 0 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Steering Rectified Flow Models in the Vector Field\nfor Controlled Image Generation",
            "content": "Maitreya Patel, Song Wen, Dimitris N. Metaxas, Yezhou Yang Arizona State University {maitreya.patel, yz.yang}@asu.edu Rutgers University {song.wen, dnm}@rutgers.edu Figure 1. FlowChef steers the trajectory of Rectified Flow Models during inference to tackle linear inverse problems, image editing, and classifier guidance. We extend FlowChef to SOTA models like Flux and InstaFlow, enabling gradientand inversion-free control for efficient, controlled image generation. Abstract Diffusion models (DMs) excel in photorealism, image editing, and solving inverse problems, aided by classifierfree guidance and image inversion techniques. However, rectified flow models (RFMs) remain underexplored for these tasks. Existing DM-based methods often require additional training, lack generalization to pretrained latent models, underperform, and demand significant computational resources due to extensive backpropagation through ODE solvers and inversion processes. In this work, we first develop theoretical and empirical understanding of the vector field dynamics of RFMs in efficiently guiding the denoising trajectory. Our findings reveal that we can navigate the vector field in deterministic and gradient-free manner. Utilizing this property, we propose FlowChef, which leverages the vector field to steer the denoising trajectory for controlled image generation tasks, facilitated by gradient skipping. FlowChef is unified framework for controlled image generation that, for the first time, simultaneously addresses classifier guidance, linear inverse problems, and image editing without the need for extra training, inversion, or intensive backpropagation. Finally, we perform extensive evaluations and show that FlowChef significantly outperforms baselines in terms of performance, memory, and time requirements, achieving new state-ofthe-art results. Project Page: https://flowchef. github.io. 1. Introduction Recent advances in diffusion models have led to rapid progress in AI generated content (AIGC), particularly in text-to-image (T2I) and text-to-video (T2V) models across various domains such as entertainment, arts, and design [12, 32, 36, 40, 44, 45]. These developments have resulted in remarkable performance in image editing, solving in1 Figure 2. Motivation behind FlowChef based on rectified flow models trajectory space. Let p1 (0, I) and p0 be distributions, with x1 p1 as initial noise, xref as the target sample, ˆx0 as the denoised sample from x1, and xref as the specific noise leading to xref . (a) Stochasticity and nonlinear trajectories with crossovers can complicate gradient estimation at each denoising step t. (b) D-Flow 0 (c) Our method FlowChef enables (baseline) inference-time trajectory requires the backpropagation through entire denoising steps. efficient trajectory steering to guide xt along the trajectory towards xref 1 0 . 0 verse problems, and personalization. This progress could be attributed to key advances like latent diffusion models (LDMs) [40] and classifier-free guidance (CFG) [16], among other essential components. Despite their applicability to various downstream tasks, these models demand increasing computational resources. For instance, CFG requires additional unconditional training of the model, while traditional classifier guidance necessitates training noiseaware classifiers [11]. Similarly, existing approaches for solving inverse problems often require minutes of computation and additional memory overhead [1, 7, 43, 46, 47]. Moreover, image editing methods typically involve either inversion or explicit training [3, 5, 19]. These limitations can be attributed to the inherent stochasticity of diffusion models, often requiring higher number of function evaluations (NFEs). However, the recent introduction of flow-based methods [23], especially rectified flow models (RFMs) [22, 24], addresses these limitations to some extent by requiring fewer NFEs, depending on the model considered. Recent works have attempted to solve inverse problems by leveraging this property, focusing mainly on pixel models [1, 28]. While these approaches have improved computational time requirements, they are still not sufficiently efficient, as they require inversion and incur significant memory overhead. As result, they cannot be extended to large state-of-the-art models like Flux or SD3 [12]. In this paper, we introduce FlowChef, novel method that significantly enhances controlled image generation by leveraging the unique characteristics of rectified flow models. We first standardize the objective of controlled synthesis, unifying various downstream tasks within single framework. By revisiting the ordinary differential equations (ODEs) that govern these models, we analyze their error dynamics both theoretically and empirically. We discover that in nonlinear ODEs with stochasticity or trajectory crossovers, error terms emerge that hinder convergence due to inaccuracies in estimating denoised samples or improper gradient approximations (see Figure 2(a)). Contrary to diffusion models, rectified flow models exhibit straight trajectories and avoid significant trajectory crossovers due to their linear interpolation between noise and data distributions (see Figure 2(b-c)). We theoretically demonstrate and empirically validate that RFMs can achieve higher convergence rates without additional computational overhead by capitalizing on this key property. Building on this understanding, we present FlowChef, that proposes to steer the trajectories towards the target in the vector field by gradient skipping (see Figure 2(c)). This allows us to navigate the vector field in deterministic manner, akin to north star guiding sailors across dark ocean. We conduct extensive evaluations of FlowChef across tasks such as pixel-level classifier guidance, image editing, and classifier-guided style transfer. Our results demonstrate that FlowChef not only surpasses baseline methods but does so with greater computational efficiency and without the need for inversion. As illustrated in Figure 1, FlowChef efficiently addresses variety of tasks. For perspective, FlowChef handles the linear inverse problems within 18 seconds on the latent-space model, while SOTA takes 1-3 minutes per image. Furthermore, we explore its practical applicability to large-scale models (i.e., Flux) to tackle both linear inverse problems and image editing together without inversion and within 30 NFEs at billions of parameter scales. Our key contributions can be summarized as follows: We develop unified perspective to study rectified flow models theoretically and empirically for guided, controlled generation. We introduce FlowChef, the most efficient method Figure 3. Illustration of impact of guided control step on Flux.1[Dev] with mean squared error as cost function (L = ˆx0 xref 2 2). This shows that FlowChef could guide the rectified flow models on the fly without requiring either the gradients through the Flux model or inversion. Importantly, the convergence speed is slowed down for illustration purposes. 0 to date for guided, controlled generation using RFMs, achieving state-of-the-art performance without requiring inversion or gradient backpropagation through the ODESolver. We demonstrate FlowChefs superior performance across multiple tasks, including linear inverse problems in both pixel and latent spaces, image editing evaluated on the PIE benchmark [19], classifier guidance, and through large-scale human preference studies. 2. Related Works We provide detailed related works, specially diffusionbased methods and conditional sampling, in the Appendix. Inverse Problems. This task addresses training-free approaches for solving inverse problems such as in-painting, super resolution, Gaussian de-blurring etc [10]. Since Dhariwal et. al. demonstrated that guiding models with classifiers improves image generation quality [11], much of the current literature focuses on diffusion models, particularly pixel-space models [7, 10, 47, 52]. However, these models face challenges when scaled to latent-space models, as they are incompatible with off-the-shelf pretrained models and require backpropagation through ODESolvers, which can take at least three minutes per image for satisfactory results [10, 41, 43, 46]. Methods such as MPGD [14] attempt to mitigate these issues via manifold correction, but limitations persist, especially with large-scale models. Recent work has extended these approaches to ODEs (e.g., OTODE) and flow models [35]. D-Flow [1], for instance, optimizes initial noise by differentiating through the full trajectory chain; however, this comes with significant resource demands and is not adaptable to state-of-the-art (SOTA) models like Flux or SD3 [12]. In this work, we propose FlowChef, which addresses linear inverse problems in gradientand inversion-free manner. Image Editing. Diffusion-based approaches dominate image editing [17], but they rely heavily on accurate inversion [3, 18, 19, 30]. Although inversion-free diffusion methods are faster, they often lack in edit quality [9, 29, 51, 53]. 3 Despite RFMs being SOTA in text-to-image (T2I) generation, they still lack robust editing capabilities. iRDS [54] presents an inversion strategy for RFMs, especially InstaFlow [25], but it lacks quality and control. Similarly, RectifID [48] offers an optimization-based approach to modify the whole trajectory for personalized T2I generation but performs poorly with InstaFlow like straight models. To the best of our knowledge, we present the first comprehensive solution that enhances RFMs for image editing and extends beyond it that too without significant computational or time overhead. Concurrent Works. We note two concurrent works: RFInversion [42] and PnP-Flow [28]. RF-Inversion offers an optimal control-based approach for image inversion and editing, whereas our method generalizes across all controlled generation tasks. We demonstrate that inversion is unnecessary, even for RF-Inversion, making RF-Inversion special case of FlowChef, where starting noise originates from an inverted target image rather than random noise, as in FlowChef. PnP-Flow is an inversionand gradient-free method for inverse problems, but it leads to over-smoothed results and lacks extensibility to image editing. 3. Preliminaries Classifier guidance, inversion problems, and image editing involve guiding model toward specific target sample or distribution in both pixel and latent spaces. However, these tasks are often treated separately in literature. Here, we present unified problem formulation to encompass these downstream tasks, with focus on rectified flow models. 3.1. Problem Formulation Let uθ : Rd [0, ] Rd represent pretrained flow model estimating the drift = x1 x0 from xt. The denoised sample ˆx0 is obtained by integrating the drift uθ over time from = to = 0, starting from xT p1. With target sample xref , we define cost function : Rd Rd R+ that quantifies the cost of aligning ˆx0 with xref 0 , yielding the optimization problem: 0 Method NFEs CG Scale FID () VRAM () Time () DDIM MPGD MPGD Oursw/ skip grad RFPP (2-flow) RFPP (2-flow) Oursw/ backpropagation Oursw/ skip grad 50 50 50 50 2 15 15 15 - 1 10 1 - - 5 50 5.39 4.24 5.46 19. 4.56 4.29 2.77 3.13 3.67 6.56 6.56 6.56 3.29 3.36 17.98 6.64 14.22 25.01 25.01 24.95 0.28 2.75 12.79 5.85 Table 1. Performance of Various guided sampling methods on ImageNet64x64 with 32 batch size inference on A6000 GPU. min {ˆxt}T t=0 L(ˆxt, xref 0 ), (1) where {ˆxt}T t=0 represents the model-generated trajectory from xT to x0. The objective is to find the trajectory that minimizes L, effectively steering the generated sample toward the target. This can be adapted for the denoising stage with either noise-aware cost function at each timestep or by estimating x0 to refine the trajectory as needed. The gradient update is given by: xt xt xtL(ˆx0, xref 0 ), (2) where is guidance scale. This process requires estimating ˆx0, backpropagating gradients through ODESolver (uθ) to adjust xt, and iteratively refining xtt. Additional details on the baseline algorithm is in the Appendix. As it can be observed, this approach depends on accurate ˆx0 estimation and substantial computation to ensure that the trajectory remains on the data manifold. 3.2. Cost Functions Notably, explicit xref is unnecessary and can be approximated with appropriate cost functions depending on the downstream tasks. Assuming initial Gaussian noise xT leads to ˆx0, the cost function can be defined as: 0 L(ˆx0, xref 0 ) = ˆx0 xref In inverse problems, let : Rd Rn represent degradation operation (e.g., downsampling for superresolution). We then define: 0 2 2. (3) L(ˆx0, xref 0 ) = F(ˆx0) xref 0 2 2. (4) Here, xref is degraded sample, and we guide the model to generate ˆx0 such that its degraded version matches xref . For classifier guidance, the cost function can be based on the negative log-likelihood (NLL). Specifically, given classifier pϕ(cˆx0), the cost function is: 0 Remark 1. Although presented in pixel space, this formulation extends to latent space by introducing Variational Autoencoder (VAE) encoder (E) and decoder (D). 4. Proposed Method In this section, we introduce our method, FlowChef, which enables free-form control for rectified flow models by presenting an efficient gradient approximation during guided sampling. We begin by analyzing the error dynamics of general ordinary differential equations (ODEs) and then explain how the inherent properties of rectified flow models mitigate existing approximation issues. Building on these insights, we derive FlowChef, an intuitive yet theoretically grounded approach for free-form controlled image generation applicable to various downstream tasks, including those involving pretrained latent models. 4.1. Error Dynamics of the ODEs Understanding why existing methods often fail and require computationally intensive strategies is crucial. In ODEbased generative models, guiding the sampling process toward desired target typically involves computing the gradient of loss function with respect to the models parameters or state variables. As noted in Eq. (2), even though the denoised output can be estimated using ˆx0 Sample(xt, uθ(xt, t)), backpropagation through the ODE solver is still necessary to obtain xtL. This raises the question: Why is backpropagation through the ODE solver necessary? Approximating gradient computations is common approach to reduce computational overhead [14, 47]. However, in models governed by nonlinear ODEs, unregulated gradient approximations can introduce significant errors into the system dynamics. This issue is formalized in the following proposition: Proposition 4.1. Let p1 (0, I) be the noise distribution and p0 be the data distribution. Let xt denote an intermediate sample obtained from predefined forward function as xt = q(x0, x1, t), where x0 p0 and x1 p1. Define an ODE sampling process dx(t) = (xt, t)dt and quadratic = ˆx0 xref 2, where : Rd [0, ] Rd 2 is an ODESolver. Then, the error dynamics of ODEs for controlled image generation is governed by: 0 dE(t) dt = 4sE(t) + 2e(t)T ϵ(t), where e(t) = ˆx0xref , E(t) = e(t)T e(t) is the squared error magnitude, > 0 is the guidance strength, and ϵ(t) represents the accumulated errors due to non-linearity and trajectory crossovers. 0 L(ˆx0, c) = log pϕ(cˆx0). (5) The proof of Proposition 4.1 is provided in the Appendix. The term 4sE(t) denotes the exponential decay 4 of error due to guidance, while 2e(t)ϵ(t) captures the impact of non-linearity and trajectory crossovers. In diffusion models, curved sampling trajectories lead to larger ϵ(t), hindering convergence. In contrast, rectified flow models exhibit straight trajectories with minimal crossovers, causing ϵ(t) to approach zero and allowing error to decrease exponentially. To validate our findings, we conduct toy study comparing classifier guidance on two ODE sampling methods using pretrained IDDPM and Rectified Flow++ (RF++) models on the ImageNet 64x64. As reported in Table 1, skipping the gradient in DDIM-based sampling increases the FID score, indicating significant ϵ(t). Conversely, RF++ converges well and improves the FID score. These empirical evidences further bolster our hypothesis that Rectified Flow models observe smooth vector field with the help of Proposition 4.1. Although backpropagating through the ODESolver further improves performance, it incurs higher computational costs as highlighted. 4.2. FlowChef: Steering Within the Vector Field Rectified flow models inherently allow error dynamics to converge even with gradient approximations due to their straight-line trajectories and smooth vector fields, as discussed previously. Hence, vector field uθ(xt, t) is trained to be smooth, and this smoothness implies that uθ changes gradually w.r.t. xt. We formalize our approach with the following assumptions about the Jacobian of the vector field: Assumption 1 (Local Linearity): Within the small neighborhoods around any point xt along the sampling trajectory, the vector field uθ(xt, t) behaves approximately linearly with respect to xt. Doing Taylor series expansion for small perturbations δ, we get: uθ(xt + δ, t) uθ(xt, t) + Juθ (xt, t)δ, (6) where Juθ (xt, t) = duθ(xt,t) dxt uθ with respect to xt. is the Jacobian matrix of Assumption 2 (Constancy of the Jacobian): The Jacobian Juθ (xt, t) varies slowly with respect to xt within these small neighborhoods. Therefore, for small δ, it can be approximated as constant: Juθ (xt + δ, t) Juθ (xt, t). (7) Under these assumptions, we derive the following gradient relationship between xtL and ˆx0L: : Rd Lemma 4.2 (Gradient Relationship). Let uθ [0, ] Rd be the velocity function with the parameter θ. Then the gradient of the cost function (xtL) at any timestep can be approximated as: xtL = (I + Juθ )T ˆx0L. (8) 5 Algorithm 1: Proposed FlowChef (generalized). 1 Input: Pretrained Rectified-flow model uθ, input noise sample xT (0, I), target data sample xref 0 , and cost function. 2 for {T...0} do uθ(xt, t) 3 dt 1/T xt xt.require grad (T rue) for steps do 5 4 7 8 9 ˆx0 xt + loss L(ˆx0, xref xt Optimize(xt, loss) ) 0 xt1 xt + dt // Lemma 4.2 // Theorem 4. 11 RETURN x0 Therefore, we get ϵ(t) = Juθ (xt, t)T ˆx0L. Importantly, when either 0 or Juθ varies slowly (Assumption 2), the matrices + Juθ (xt, t) are close to the identity matrix. We further provide empirical evidence about this on pretrained rectified flow models by analyzing the gradients and convergence w.r.t. denoising steps in the Appendix. Where we observe that gradient direction improves linearly and quickly converges to xref as 0. Under this ap0 proximation, the difference between the two error dynamics becomes negligible. Since the ϵ(t) introduces only small correction, it leads to the convergence in error dynamics as 0. Combining the results of Preposition 4.1, Assumption 1 and 2, and Lemma 4.2, we obtain the following theorem with straightforward proof that facilitates the controlled generation for rectified flow models in the most computationally efficient way: Theorem 4.3. (Informal) Given the above assumption and notations, the update rule for the vector field driven by uθ for the free-form controlled generation is: xtt = xt + uθ(xt, t) sˆx0L, (9) where is the guidance scale. The formal statement and proof are provided in the Appendix. This theorem forms the core of FlowChef, enabling controlled generation efficiently. Algorithm Overview. Algorithm 1 provides generalized overview of FlowChef. key feature of FlowChef is that it starts from any random noise xT (0, I) and still converges to the desired distribution or sample without inversion. At each timestep t, we first estimate the ˆx0. Then we calculate the loss L(ˆx0, xref 0 ). At last, we directly Figure 4. Qualitative results on linear inverse problems. All baselines are implemented on stable diffusion v1.5, except FlowChef Flux variant. Results are reported for VRAM and time on an A100 GPU at 512 512 resolution, with Flux experiments at 1024 1024. Best viewed when zoomed in. optimize xt using the gradient ˆx0 L, as per Lemma 4.2. Thats all we need! We may repeat this optimization times per denoising step to stabilize gradients and improve convergence, though we found = 1 sufficient in most cases. Important hyperparameters include the learning rate and total number of function evaluations (NFEs) . Selecting optimal values for and the learning rate is crucial to maintain gradients within suitable range, uphold Jacobian constancy (Assumption 2), and avoid adversarial effects. To illustrate this, we analyze the effects of total FlowChef guidance steps on the Flux model (see Figure 3). Detailed study on this is in Appendix. 5. Experiments We evaluate FlowChef across multiple tasks: (1) Linear inversion problems on pixeland latent-space models, (2) Image editing, and (3) Classifier-guided style transfer. Overall, FlowChef demonstrates superior performance across all tasks, significantly reducing compute and time costs compared to baselines. Notably, FlowChef extends seamlessly to image editing tasks without inversion or additional memory overhead, allowing it to operate on recent SOTA T2I models, such as Flux, without encountering outof-memory (OOM) errors. 5.1. Linear Inversion Problems extend both FlowChef and the baselines to latent-space models to simulate real-world applications, reporting results on PSNR, SSIM [50], and LPIPS [58] across 200 images from CelebA [26] and AFHQ-Cat [6]. Memory requirements and computation time are also analyzed. 5.1.1. Pixel-space models As FlowChef requires straightness and no crossovers, we select the Rectified-Flow++ pretrained models [22]. We compare FlowChef with recent flow-based methods OT-ODE [35], D-Flow [1], and PnP-Flow (concurrent work) [28], implementing the former two baselines manually due to lack of open-source access and tuning them for optimal performance. Additionally, we extend two diffusion-based baselines, DPS [7] and FreeDoM [56], for the RFMs. For comparisons, we use the Rectified-Flow++ models that are pretrained on FFHQ (for CelebA) and AFHQ-Cat datasets. Experiments are conducted for 64x64 image resolutions. Hyper-parameters for each method are reported in the Appendix. Our selected tasks include: (1) Box inpainting with 20x20 and 30x30 centered masks, (2) Super-resolution with 2x and 4x scaling factors, and (3) Gaussian deblurring with an 11x11 kernel at intensities of 1.0 and 10.0, with added Gaussian noise at σ = 0.05 for robustness. We evaluate FlowChef against several baselines on three common linear tasks: box inpainting, super-resolution, and Gaussian deblurring, under varying difficulty levels. We Results. We present the quantitative and qualitative evaluation results in Table 2 and Appendix, respectively. It can be observed that FlowChef significantly improves the perMethod Easy Scenarios Degraded OT-ODE PnP-Flow D-FLow FreeDoM DPS FlowChef (ours) Hard Scenarios Degraded OT-ODE PnP-Flow D-FLow FreeDoM DPS FlowChef (ours) BoxInpaint Deblurring Super Resolution PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () 21.79 19.11 22.12 20.37 20.87 23.61 26.32 18.75 16.37 20.44 18.34 18.88 20.68 21.45 74.76 77.86 68.02 70.06 74.79 74.79 87. 65.12 67.35 61.96 62.62 65.07 65.06 78.75 10.92 13.49 14.70 13.67 13.92 9.35 3.36 22.54 19.22 17.53 19.94 16.83 13.06 7.73 20.17 21.86 22.00 20.22 20.21 22.49 27.69 16.83 17.89 19.50 16.93 16.50 17.58 20.31 54.03 62.51 65.79 61.99 69.73 69.73 86. 30.02 34.02 50.54 34.13 34.88 34.89 52.73 22.20 15.14 15.95 14.51 13.22 10.23 2.66 54.04 29.68 22.00 25.31 18.91 15.86 10.64 24.68 21.64 22.42 21.60 21.15 23.94 26.00 20.77 18.19 21.35 20.01 19.58 21.52 21.62 77.57 62.23 68.06 69.89 77.54 77.54 80. 55.85 39.43 61.78 56.46 55.84 55.90 60.33 11.67 26.64 14.91 12.29 12.12 8.46 4.43 38.16 36.84 17.78 17.64 14.12 10.31 10.18 Table 2. Pixel-space model-based evaluations for tackling the linear inverse problems. SSIM & LPIPS results are multiplied by 100. Method Diffusion based methods Resample PSLD (500 NFEs) PSLD (100 NFEs) Flow based methods D-Flow RectifID FlowChef (InstaFlow) FlowChef (Flux) BoxInpaint Super Resolution Deblurring PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () 20.12 28.30 26.90 19.68 23.81 22.94 25.74 79.94 93.81 93.13 65.01 75.13 73.55 82.99 19.36 4.49 5. 27.79 10.50 9.94 9.40 26.91 25.79 21.95 20.23 10.36 25.83 20.25 70.91 65.15 54.67 60.55 31.55 64.73 64.34 30.75 33.27 46. 50.30 67.08 31.38 41.88 25.27 26.64 21.25 22.42 10.40 22.50 18.98 62.97 65.44 51.62 64.43 31.16 47.42 64.37 41.94 43.10 51. 53.04 66.60 42.54 53.43 Table 3. Latent-space model based evaluations for tackling the linear inverse problems. SSIM & LPIPS results are multiplied by 100. Metric OT-ODE PnP-Flow D-Flow FlowChef VRAM (GB) Time (sec) 0.70 10.39 0.40 5.23 6.44 80.42 0.43 4. Table 4. Compute requirement comparisons on A6000 GPU. formance on both easy and hard settings across the tasks and all metrics consistently. Notably from Table 4, we find that the FlowChef is also the fastest and most memory efficient. Surprisingly, diffusion-based extended baseline (DPS) significantly outperforms even recent baselines. However, DPS requires backpropagation through billions of parameters of ODESolver. While the concurrent gradientfree work, PnP-Flow, outperforms many other baselines, FlowChef leads the benchmark. 5.1.2. Latent-space models. Flow-based baselines are not extended to the latent space models as either they are already very computationally heavy or require extra Jacobian calculations to support the non-linearity introduced by the VAE models. We adapt DFlow [1] and RectifID [48] as flow-based baselines, adding diffusion-based baselines PSLD-LDM [43] and Resample [46] for comparison. We use InstaFlow [25] (Stable Diffusion v1.5 variant) and Flux models as baseline for flow-based approaches and utilize the original Stable Diffusion v1.5 checkpoint for the diffusion-based baselines. We perform all tasks in 512 512 resolution, increasing to 1024 1024 for Flux experiments. Our task settings are: (1) Box inpainting with 128x128 mask, (2) Super-resolution at 4x scaling, and (3) Gaussian deblurring with 50x50 kernel at intensity 5.0, all without extra Gaussian noise. For consistency, settings are doubled for Flux to 256x256 mask, 8x super-resolution scaling, and 10.0 deblurring intensity. As VAE encoders add extra unwanted nonlinearity, pixel-level cost functions alone may not be optimal. Hence, we calculate the loss in the latent space only for the box inpainting task (as the degradation function is known with σ = 0), allowing us to extend to image editing later. For superresolution and deblurring, we stick with the pixel-level cost functions. We further detail the task-specific settings and hyperparameters in the Appendix. Results. Quantitative and qualitative results in Figure 4 and Table 3 show that FlowChef achieves SOTA perfor7 Figure 5. Qualitative results on image editing. As illustrated, our method attains the SOTA performance on comparison inversion-free methods. While FlowChef (Flux) variant achieves better quality and edits. to the observed nonlinearity in the trajectory of Flux (detailed discussion in Appendix). 5.2. Image Editing We extend FlowChef for image editing on Flux and InstaFlow models, with Algorithm 2 detailing the implementation. This extension reduces FlowChefs sensitivity to hyper-parameters. Currently, the approach requires userprovided mask for controlled editing but can be expanded to attention-based techniques. Therefore, we select the baselines that also accept the user-provided mask for holistic comparisons. Due to their optimization constraints, existing baselines for classifier guidance cannot be applied to image editing. For comparison, we use diffusion-based SOTA methods Ledits++ [3] (which requires the inversion), DiffEdit [9] and InfEdit [53], alongside RF-Inversion [42] (the only concurrent flow-based editing framework). We perform large-scale evaluations on PIE-Bench [19]. For fair comparisons, we use PIE-Bench-provided ground truth masks for controlling all editing methods. Additionally, we provide preliminary comparisons with RF-Inversion for wearing glasses on randomly selected SFHQ faces [2]. Figure 6. Human preference analysis for image editing. mance for flow-based methods. However, huge gap still remains w.r.t. the diffusion-based methods like Resample and PSLD. Notably, these baselines take about 5 minutes and 3 minutes, respectively, per image (see Figure 4), while FlowChef only takes only 18 seconds and less memory (only 14GB). None of the existing flow-based methods can be extended to Flux due to memory constraints. But FlowChef can seamlessly be applied, which further improves the performance. We find that FlowChef (Flux) reduces the artifacts in the images completely but observes the slight degradation in color dynamics. We attribute this 8 Figure 8. Extending FlowChef to 3D multiview synthesis. while adhering to the prompt. pretrained CLIP model was used for evaluation, and we report both CLIP-T and CLIP-S scores [38]. For baseline comparisons, we included diffusion-based methods FreeDoM and MPGD and flowbased methods D-Flow and RectifID, which were extended for this task. The backbone was fixed to Stable Diffusion v1.5 (SDv1.5), with FlowChef evaluated in its InstaFlow variant to ensure consistent comparison. Both quantitative and qualitative results are presented in Table 5, demonstrating the effectiveness of FlowChef in this setup. 5.4. Extended Applications To highlight the versatility and effectiveness of FlowChef, we extended our method to tackle multi-object image editing and 3D multiview generation. Figure 7 demonstrates FlowChef (Flux) performing complex multi-object edits, such as simultaneously modifying two pots and hats. Notably, this capability relies on the base models ability to understand textual instructions effectively. FlowChef leverages this strength of Flux, achieving edits without requiring inversion, significant advantage over traditional In Figure 8, we explore FlowChefs multimethods. view synthesis capability, inspired by Score Distillation Sampling (SDS) [37]. By incorporating the core idea of FlowChef for model steering into recent work on RFDS [54], we evaluate its effectiveness for 3D view generation. While FlowChef does not improve inference efficiency or reduce cost compared to RFDS-Rev [54], it demonstrates competitive performance in generating highquality multiview outputs. These results underline the adaptability of FlowChef, showcasing its potential for advanced generative tasks such as multi-object editing and 3D synthesis, while maintaining the state-of-the-art quality expected from RFMs. Figure 7. FlowChef (Flux) multi object editing examples. Method CLIP-I () CLIP-T () VRAM Time FreeDoM MPGD RetifID D-Flow FlowChef(10 NFEs) FlowChef(30 NFEs) FlowChef(30 NFEs 2) 0.5343 0.5285 0.4583 0.4851 0.5044 0.5301 0.5531 0.2541 0.2616 0.1702 0.2591 0.2655 0.2600 0.2478 17GB 16GB 18GB 23GB 14GB 80 sec 20 sec 30 sec 5 sec 2 sec 7 sec 12 sec Table 5. Comparison of Various Classifier Guided Style Transfer. Results. human preference evaluation on randomly selected 100 PIE-Bench edits (see Figure 6) shows FlowChef (InstaFlow) outperforming DiffEdit and competing with InfEdit. Although Ledits++ scored highest, it requires inversion, resulting in higher VRAM and time requirements. Importantly, FlowChef on Flux achieves performance comparable to Ledits++ without inversion. Comparisons with RF-Inversion show that FlowChef reduces time by almost 50% without needing inversion and achieves competitive performance, with additional detailed quantitative and qualitative results in the Appendix. 5.3. Classifier Guidance: Style Transfer We conducted classifier-guided style transfer experiments using 100 randomly selected style reference images paired with 100 random prompts. The objective was to generate stylistic images that align visually with the reference style 9 6. Conclusion In this work, we introduced FlowChef, versatile flowbased approach that unifies key tasks in controlled image generation, including linear inverse problems, image editing, and classifier-guided style transfer. Extensive experiments show that FlowChef outperforms baselines across all tasks, achieving state-of-the-art performance with reduced computational cost and memory usage. Notably, FlowChef enables inversion-free editing and scales to SOTA T2I models like Flux without memory issues. Our results demonstrate FlowChefs adaptability and efficiency, offering unified solution for both pixel and latent spaces across diverse architectures and practical constraints."
        },
        {
            "title": "Acknowledgments",
            "content": "MP and YY are supported by NSF RI grants #1750082 and #2132724. We thank the Research Computing (RC) at Arizona State University (ASU) and cr8dl.ai for their generous support in providing computing resources. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers."
        },
        {
            "title": "References",
            "content": "[1] Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through flows for controlled generation. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 6, 7, 4 [2] David Beniaguev. Synthetic faces high quality (sfhq) dataset, 2022. 8 [3] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 2, 3, 8, 4 [4] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 3 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [6] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 6 [7] Hyungjin Chung, Jeongsol Kim, Michael Mccann, Marc Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. 2, 3, 6, 4 [8] Hyungjin Chung, Jeongsol Kim, Sehui Kim, and Jong Chul Ye. Parallel diffusion models of operator and image for blind In Proceedings of the IEEE/CVF Coninverse problems. ference on Computer Vision and Pattern Recognition, pages 60596069, 2023. 4 [9] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2023. 3, 8, 4 [10] Giannis Daras, Hyungjin Chung, Chieh-Hsin Lai, Yuki Mitsufuji, Jong Chul Ye, Peyman Milanfar, Alexandros Dimakis, and Mauricio Delbracio. survey on diffusion models for inverse problems. arXiv preprint arXiv:2410.00083, 2024. 3, 4 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2, 3, 4 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, [13] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua M. Susskind, and Navdeep Jaitly. Matryoshka diffusion modIn The Twelfth International Conference on Learning els. Representations, 2024. 4 [14] Yutong He, Naoki Murata, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, Wei-Hsiang Liao, Yuki Mitsufuji, Zico Kolter, Ruslan Salakhutdinov, et al. ManIn The Twelfth Internaifold preserving guided diffusion. tional Conference on Learning Representations, 2024. 3, 4 [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. 4 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4 [17] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. 3 [18] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. 3, 4 [19] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. 2, 3, 8, [20] Changhoon Kim, Kyle Min, Maitreya Patel, Sheng Cheng, and Yezhou Yang. Wouaf: Weight modulation for user attribution and fingerprinting in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89748983, 2024. 9 [21] Changhoon Kim, Kyle Min, and Yezhou Yang. Race: Robust adversarial concept erasure for secure text-to-image diffusion model. arXiv preprint arXiv:2405.16341, 2024. 9 10 [22] Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. arXiv preprint arXiv:2405.20320, 2024. 2, 6 [35] Ashwini Pokle, Matthew J. Muckley, Ricky T. Q. Chen, and Brian Karrer. Training-free linear image inversion via flows, 2024. 3, 6, [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 2, 4 [24] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 2, 4 [25] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 3, 7, 4 [26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. 6 [27] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 4 [28] Segol`ene Martin, Anne Gagneux, Paul Hagemann, and Gabriele Steidl. Pnp-flow: Plug-and-play image restoration with flow matching. arXiv preprint arXiv:2410.02423, 2024. 2, 3, 6, 4 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3, 4 [31] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [32] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. λ-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. ArXiv, abs/2402.05195, 2024. 1, 4 [33] Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: resource-efficient text-toIn Proceedings of the image prior for image generations. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90699078, 2024. 4 [34] Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni Zou, and Hongkai Xiong. Improving diffusion models for inverse problems using optimal posterior covariance. In Forty-first International Conference on Machine Learning, 2024. 4 [36] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [37] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations, 2023. 9 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 9 [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3 [41] Litu Rout, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Beyond first-order tweedie: Solving inverse problems using latent In Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 9472 9481, 2024. 3 [42] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 3, [43] Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai. Solving linear inverse problems provably via posterior sampling with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 7, 4 [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [45] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023. 1 [46] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with 11 In The latent diffusion models via hard data consistency. Twelfth International Conference on Learning Representations, 2024. 2, 3, 7, [47] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023. 2, 3, 4 [48] Zhicheng Sun, Zhenhao Yang, Yang Jin, Haozhe Chi, Kun Xu, Liwei Chen, Hao Jiang, Yang Song, Kun Gai, and Yadong Mu. Rectifid: Personalizing rectified arXiv preprint flow with anchored classifier guidance. arXiv:2405.14677, 2024. 3, 7 [49] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 4 [50] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 6 [51] Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, and Eli Shechtman. Turboedit: Instant text-based image editing. arXiv preprint arXiv:2408.08332, 2024. 3, 4 [52] Zihui Wu, Yu Sun, Yifan Chen, Bingliang Zhang, Yisong Yue, and Katherine Bouman. Principled probabilistic imaging using diffusion models as plug-and-play priors. arXiv preprint arXiv:2405.18782, 2024. 3 [53] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Inversion-free image editing with natural language. Chai. arXiv preprint arXiv:2312.04965, 2023. 3, 8, 4 [54] Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, and Guosheng Lin. Text-to-image rectified flow as plug-and-play priors. arXiv preprint arXiv:2406.03293, 2024. 3, 9 [55] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 4 [56] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23174 23184, 2023. 6, 4 [57] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 5907 5915, 2017. 3 [58] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018."
        },
        {
            "title": "Supplementary Material",
            "content": "7. Supplementary Overview This supplementary material contains proofs, detailed results, discussion, and qualitative results: Section 8: Proposition 4.1 proof. Section 9: Theorem 4.3 proof. Section 10: Numerical accuracy analysis. Section 11: Extended related works. Section 12: Empirical study of pixel and latent models. Section 13: Detailed algorithms. Section 14: Experimental setup details. Section 15: RF-Inversion vs. FlowChef. Section 16: Hyperparameter study. Section 17: Qualitative Results. Section 18: Limitations & Future Work 8. Proof of the Proposition Proposition 4.1. Let p1 (0, I) be the noise distribution and p0 be the data distribution. Let xt denote an intermediate sample obtained from predefined forward function as xt = q(x0, x1, t), where x0 p0 and x1 p1. Define ODE sampling process dx(t) = (xt, t)dt and quadratic = ˆx0 xref 2, where : Rd [0, ] Rd 2 is nonlinear function parameterized by θ. Then, under Assumption 1, the error dynamics of ODEs for controlled image generation are governed by: 0 dE(t) dt = 4sE(t) + 2e(t)T ϵ(t), where e(t) is ˆx0 xref , E(t) = e(t)T e(t) is the squared error magnitude, > 0 is the guidance strength, and ϵ(t) represents the accumulated errors due to non-linearity and trajectory crossovers. 0 Proof. Consider the sampling process described by the ODE: dx(t) dt = (x(t), t), (10) where (x(t), t) is nonlinear function often parameterized via neural network θ. To guide the sampling process toward minimizing loss function L(ˆx0, xref ), we can adjust the dynamics by adding the gradient xt to the vector field (see Eq. 2) as: 0 dx(t) dt = (x(t), t) xtL(ˆx0, xref 0 ), (11) where is the guidance strength. Let e(t) = ˆx0xref be the error between the estimated and target samples. Since ˆx0(t) = x(t) + (cid:82) 0 (x(τ ), τ )dτ , differentiating e(t) with respect to yields: 0 de(t) dt = = dˆx0(t) dt dx(t) dt (x(t), t) = xtL(ˆx0, xref 0 ). (12) (13) (14) However, this requires the compute-intensive backpropagation through ODESolver. Therefore, it is important to find an approximation of xt. And the most convenient approximation is: xt ˆx0. However, this derivation assumes that the integral (cid:82) 0 (x(τ ), τ )dτ is well-behaved and that ˆx0(t) depends smoothly on x(t). In the presence of nonlinearity and trajectory crossovers, small changes in x(t) can lead to disproportionately large changes in ˆx0(t), due to the sensitivity of the integral to the path taken. Moreover, potential crossovers in the trajectory mean that the mapping from x(t) to ˆx0(t) is not injective; different trajectories x(t) may lead to the same ˆx0(t) or vice versa. This non-unique mapping complicates the error dynamics because ˆx0 may not provide consistent or effective direction for updating x(t). Including the effects of nonlinearity and trajectory crossovers, the error dynamics become: de(t) dt = ˆx0 L(ˆx0, xref 0 ) + ϵ(t), (15) where ϵ(t) represents the errors introduced by the nonlinearity in (x(t), t) and the sensitivity of ˆx0 to x(t) due to trajectory crossovers. In other words, the approximation error ϵ(t) can be represented as: (cid:16) ϵ(t) = xtL(ˆx0, xref ) ˆxtL(ˆx0, xref 0 (cid:17) ) . (16) Assuming quadratic loss function = ˆx0 xref 2 2, 0 we have ˆx0L = 2e(t), leading to: de(t) dt = 2se(t) + ϵ(t). (17) To understand the convergence of the error, we analyze the evolution of the error magnitude E(t) = e(t)T e(t). Differentiating E(t) with respect to time t, we get: 1 dE(t) dt dt (cid:0)e(t)e(t)(cid:1) = = 2e(t) de(t) dt = 2e(t) (2se(t) + ϵ(t)) = 4se(t)e(t) + 2e(t)ϵ(t) = 4sE(t) + 2e(t)ϵ(t). (18) (19) (20) (21) (22) This completes the proof. Notably, we derive this behavior of the ODE processes under the assumption that the error rate cannot be calculated accurately. This can either come from the incorrect estimation of ˆx0 or the nonlinearity of ODESolver itself. In the next section, we further concretize this with respect to the RFMs. 9. Proof for Theorem Lemma 4.2 (Gradient Relationship). Let uθ : Rd [0, ] Rd be the velocity function with the parameter θ. Then the gradient of the cost function at any timestep can be approximated as: xtL = (I + Juθ )T ˆx0L. (23) Proof. Leveraging the straight-line trajectories characteristic of rectified flow models, the data sample at = 0 can be estimated directly from an intermediate state xt: ˆx0 = xt + uθ(xt, t). (24) By differentiating the ˆx0 with respect to xt, we get: dˆx0 dxt = + duθ(xt, t) dxt = + Juθ (xt, t). Using the chain rule for gradients: xtL = (cid:19)T (cid:18) dˆx0 dxt ˆx0L. (25) (26) (27) Substituting the expression for dˆx0 dxt , we obtain: xtL = (I + Juθ (xt, t))T ˆx0L. According to Assumption 3, due to the constancy of Jacobian, Juθ , for rectified flow models, we can treat it as constant for any time t. Hence, we get our desired approximation: (28) This completes the proof. Hence, as either 0 or Juθ (Assumption 2), both gradients become approximately similar and ϵ(t) 0. This guarantees the convergence of the error dynamics as time passes. We further show this behavior of RFMs empirically in Section 12 and show that this remains true for even largescale latent models. Theorem 4.3 (Update Rule for Steering the RFMs). Let uθ : Rd [0, ] Rd be velocity field with constant Jacobian Juθ . Define the estimated initial state ˆx0 from an intermediate state xt by ˆx0 = xt + uθ(xt, t). Consider the quadratic loss function = ˆx0 xref where xref controlled generation is given by 0 2, 0 is reference sample. Then, the update rule for xtt = xt + tuθ(xt, t) sˆx0 L, where: ˆx0 = 2(ˆx0 xref (I + Juθ ) (I + Juθ ), is the identity matrix. 0 ), Proof. By lemma 4.2 and Assumption 2, we can further approximate the Eq. 23: xtL = (I + Juθ )T ˆx0 ˆx0L, (30) where is the constant matrics as 0 and 0. Under this formulation, we can perform controlled image generation in three steps: Step 1: Step 2: Step 3: ˆx0 = xt + uθ(xt, t) ˆxt = xt ˆx0L xtt = ˆxt + uθ(ˆxt, t). (31) However, this will require additional forward passes. But according to Assumption 2 if is sufficiently small, then by Taylor series approximation, we get: xtt = xt ˆx0 + uθ (cid:0)xt ˆx0L, t(cid:1) = xt ˆx0 + (cid:2)uθ(xt, t) Juθ ˆx0 L(cid:3) (32) (33) xtL = (I + Juθ (xt, t))T ˆx0L. (29) Now, as Juθ is constant w.r.t. t. Hence, we get: xtt = xt (I + Juθ )K ˆx0 + uθ(xt, t) (34) = xt + uθ(xt, t) sˆx0L, (35) where = (I + Juθ )K is constant and it can predetermined. Hence, this concludes the proof that for appropriate guidance scale s, we can perform the controlled generation as derived above. 10. Numerical Accuracy for Model Steering In our controlled generation framework, we aim to steer the generation process towards reference sample xref by solving the modified ODE: 0 dx(t) dt = (x(t), t) = uθ(x(t), t) sˆx0L. (36) The accuracy of this numerical integration is crucial, as errors can accumulate over time, leading to deviations from the desired trajectory. The smoothness of the modified velocity field (x(t), t) significantly impacts this accuracy. Specifically, smaller magnitude of (cid:12) dt (x(t), t)(cid:12) (cid:12) (cid:12) reduces local truncation errors. The following Proposition formalizes this relationship, stating that the numerical accuracy improves as (cid:12) dt (x(t), t)(cid:12) (cid:12) (cid:12) decreases. Proposition 10.1. (Informal). Given the prior notations, Assumptions, and Theorem, for any p-th order numerical method solving Eq. (36), the accuracy of the numerical solution increases as the magnitude of (cid:12) dt (x(t), t)(cid:12) (cid:12) (cid:12) decreases. Proof. To analyze the local truncation error, consider the Taylor series expansion of the exact solution around time when integrating backward in time from to t: x(t t) =x(t) (x(t), t) + (t)2 2 dt (x(t), t) (t)3 6 d2 dt2 (x(t), t) + (cid:0)(t)4(cid:1) . τ = x(t t) xtt (cid:20) = x(t) (x(t), t) + (t)2 2 dt (x(t), t) (t)3 6 d2 dt2 (x(t), t) + (cid:0)(t)4(cid:1) (cid:21) [xt + ϕ(xt, t)] . The first p-order terms cancel out, and we have: τ (cid:13) (cid:13) (cid:13) (cid:13) (t)p+1 (p + 2)! (cid:13) dp+1 (cid:13) dtp+1 (x(t), t) (cid:13) (cid:13) According to the Mean Value Theorem, we have τ C(t)p+1 max t[tn,tn+1] (cid:13) (cid:13) (cid:13) (cid:13) dt (x(t), t) (cid:13) (cid:13) (cid:13) (cid:13) (38) (39) where is constant depending on the method. The global error e(t) = x(t) xt accumulates these local errors over the integration interval. Under standard assumptions (e.g., Lipschitz continuity of ), the global error is bounded by: e(t) K(t)p (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (40) where is constant depending on the Lipschitz coneL(T t) 1 max t[0,T ] (x(t), t) dt (cid:13) (cid:13) (cid:13) (cid:13) (cid:17) , stant of and the total integration time . dt (xt, t)(cid:13) As the magnitude of (cid:13) (cid:13) (cid:13) decreases, both the local truncation error and the global error decrease, enhancIn the coning the accuracy of the numerical solution. text of controlled generation, ensuring that (xt, t) changes smoothly over time leads to more accurate integration and better alignment with the reference point xref 0 . This insight and prior assumptions require that the guidance scale and δt be sufficiently smaller, where higher NFEs lead to the lower t. Hence, we increase the NFEs significantly to stabilize the steering (see Section 16). By carefully selecting s, we ensure that the additional term ˆx0 does not introduce excessive variability into (x(t), t), maintaining the smoothness necessary for accurate numerical integration. The numerical method updates the solution using: 11. Extended Related Works xtt = xt + ϕ(xt, t), (37) where ϕ(xt, t) is the increment function. The local truncation error τ is the difference between the exact solution and the numerical approximation: Generative Models. Recent advances in generative models, especially diffusion models like Latent Diffusion Model (LDM) [40], GLIDE [31], and DALL-E2 [39], have significantly improved photorealism compared to GAN-based methods such as StackGAN [57] and BigGAN [4]. Pretrained diffusion models have been successfully applied to 3 diverse tasks, including image editing [15], personalization [32], and style transfer [49], but their inference flexibility remains limited, and they demand substantial resources [13, 33]. Distillation-based strategies like Latent Consistency Models [27] and Distribution Matching Distillation [55] address some limitations but lack control and broader applicability. Rectified Flow Models (RFMs) [23, 24], exemplified by Flux1, SD3 [12], and InstaFlow [25], show promise but face challenges in downstream tasks due to inversion inaccuracies and other limitations. This work addresses these gaps, extending RFMs to downstream tasks in training-, gradient-, and inversion-free manner. Conditional Sampling. Song et al. introduced noiseaware classifiers for controlling sampling in diffusion models [11], but these require task-specific training. Classifierfree guidance (CFG) [16] avoids this but necessitates an additional pretraining stage. FreeDoM [56] and MPGD [14] improve sampling control but remain computationally intensive. Initial extensions of conditional sampling to flow models face similar challenges, such as compute-heavy gradient backpropagation and limited applicability to latent space models. Our method, FlowChef, eliminates these issues, seamlessly enabling gradientand inversionfree conditional sampling in latent-space models. Inverse Problems. Inverse problems, dominated by diffusion-based methods [10], include pixel-space solutions such as DPS [7], Π-GDM [34], and BlindDPS [8]. PSLD [43] extends support to latent-space models, while manifold-based methods [14, 46] further enhance performance. Flow-based approaches like OT-ODE [35] and DFlow [1] improve speed and quality but remain resourceintensive. Recent advancements like PnP-Flow [28] achieve trainingand gradient-free solutions for pixel-space models but face issues like smoothness artifacts. Existing solutions are resource-intensive and unsuitable for large-scale latent models. FlowChef leverages vector field properties of RFMs to enhance performance, generalization, and scalability for state-of-the-art models like Flux. Image Editing. Image editing typically involves guiding model to combine reference image with an editing instruction, often through inversion [15, 18, 19, 30]. Inversion-free methods like DiffEdit [9], InfEdit [53], and TurboEdit [51] are rare, and none apply to flow models. Most state-of-the-art methods rely on cross-attention mechanisms [3, 30], which we do not prioritize. Our approach, FlowChef, introduces the first inversion-free image editing method for RFMs, achieving competitive results with 1https://huggingface.co/black-forest-labs/FLUX. 1-dev state-of-the-art methods. 12. Empirical Findings In Section 4, we provided theoretical insights into FlowChef along with an intuitive algorithm. To complement the theory, we conducted an empirical analysis on large-scale RFMs to validate the Assumptions, Propositions, Lemmas, and Theorems presented. The results are summarized in Figure 9. In Figure 9a, we compare the gradient cosine similarity with and without backpropagation through the ODESolver for InstaFlow and Stable Diffusion v1.5. For all denoising steps, the gradients of SDv1.5 behave nearly randomly, indicating that the stochasticity of the base model significantly impacts gradients, even when using the ODE sampling process during inference. In contrast, for InstaFlow, as denoising progresses (t 0), gradient alignment improves, supporting our derivation in Lemma 4.2, which states that as 0, we achieve xt ˆx0 . Further analysis was performed on the Rectified Flow++ model, which is designed for straight trajectories with zero crossovers. As shown in Figure 9b, well-trained models exhibit high gradient similarity even at the initial stages of denoising. However, as illustrated in Figure 9c, during active steering, the gradient direction initially diverges before improving. This behavior is also reflected in the convergence plot in Figure 9d. We hypothesize that this phenomenon arises due to the proximity to the Gaussian noise space (p1 (0, I)), where model steering is more error-prone since minor adjustments can disproportionately affect future trajectories. As denoising progresses and the distribution moves further from the noise (p1), these errors diminish, and convergence is achieved. These observations align well with our theoretical predictions, further reinforcing the validity of FlowChef. 13. Algorithms This section provides an overview of the algorithms underpinning FlowChef for image editing and its comparison to baseline methods for comprehensive understanding. in Section described Image Editing. As 4.2, FlowChef can be easily extended to image editing. Revisiting the core concept, FlowChef modifies random Image editing trajectories to align with target sample. involves balancing similarity with the target sample while introducing deviations to achieve desired edits. Figure 3 and Section 16 illustrate how FlowChef progressively transfers characteristics from high-level structure to finer details like color composition. However, editing requirements vary by task. For example, adding an object 4 (a) Gradient Similarity in InstaFlow vs. Stable Diffusion v1.5. (b) Gradient Similarity in Rectified Flow ++ model. (c) Gradient Similarity in Rectified Flow ++ during model steering. (d) Convergence in Rectified Flow ++ during model steering. Figure 9. Empirical analysis of gradient similarity (a, b, and c) and convergence rate. (a) and (b) analyzes the gradients without model steering. (c) contains the gradient similarity during the active model steering. And (d) shows the trajectory similarity at each timestep w.r.t. the inversion based trajectory. Algorithm 2: FlowChef vs. Baseline FreeDoM. 1 Input: Pretrained Rectified-flow model uθ, input noise sample xT (0, I), target data sample xref 0 , and cost function. 2 for {T...0} do 4 5 6 7 8 10 11 12 uθ(xt, t) dt 1/T xt xt.require grad (T rue) for steps do uθ(xt, t) ˆx0 xt + loss L(ˆx0, xref xt grad(loss, xt) xt Optimize(xt, loss) ) 0 uθ(xt, t) xt1 xt + dt // Compute heavy BP // Lemma 4.2 // Theorem 4.3 13 RETURN benefits from trajectory adjustments earlier in the denoising process, while color changes require gradual learning at later stages. We can optimize parameters for diverse tasks using the generalized FlowChef, as detailed in Algorithm 1. To simplify the process, we extend FlowChef to support off-the-shelf editing tasks, such as those in the PIEBenchmark, as detailed in Algorithm 2. Assume non-edit region mask, Medit, derived from cross-attention or human annotation. To steer the trajectory towards the desired edits, we modify the velocity (v) using classifier-free guidance strategy: = vedit + mask (vedit vbase) s, (41) Algorithm 3: : FlowChef optimized for wide range of image editing tasks. 1 Input: Pretrained Rectified-flow model uθ, input noise sample xT (0, I), target data sample xref , cedit is edit prompt, cbase is base prompt, 0 is user-provided input mask, and cost function. 4 5 2 for {T...0} do dt 1/T 3 [cedit, cbase] uθ(xt, t, c) vedit, vbase = v.chunk(2) = vedit + mask (vedit vbase) Medit xt xt.require grad (true) if < minT then for steps do 9 8 10 11 12 13 14 16 17 ˆx0 xt + if < max ull stepsT then Medit loss L(ˆx0, xref xt Optimize(xt, loss) 0 ) Medit // Lemma 4.2 xt1 xt + dt // Theorem 4.3 18 RETURN x0 where vedit corresponds to the edit prompt and vbase to the base (negative) prompt. This adjustment ensures the trajectory reflects the desired edits. To maintain alignment of non-edited regions with the target sample, we modify the cost function as follows: L(ˆx0, xref 0 ) = (ˆx0 xref 0 ) Medit2 2. (42) Preserving the original image structure is crucial for edits such as color or material changes. To achieve this, we 5 Hyperparameter OT-ODE D-Flow PnP-Flow FlowChef Iterations / NFEs Optimization per iteration Optimization per denoising Avg. sampling steps Guidance scale Cost function initial time (1 means noise) blending strength inversion learning rate 200 1 - - 1 L1 0.8 - 1 20 - 50 - 1 L1**2 - 0.05 1 50 - - 5 1 L1 - - 200 1 - - 500 MSE - - 1 Table 6. Hyperparameters for solving inverse problems using pixel-space models. Hyperparameter D-Flow RectifID FlowChef Iterations / NFEs Optimization per iteration Optimization per denoising Blending strength Guidance scale Cost function Learning rate Optimizer loss multiplier (latent/pixel) inversion 10 - 20 0.1 0.5 MSE 0.5 Adam 0.000001 4 - 400 - 0.5 MSE 1 SGD 0.0001 / 100000 100 1 - - 0.5 MSE 0.02 Adam 0.001/1000 Table 7. Hyperparameters for solving inverse problems using latent-space models (InstaFlow). introduce the parameter max ull steps , which determines the number of steps that apply full FlowChef guidance with an identity mask. This ensures structural preservation while facilitating edits. Section 16 details comprehensive reference for hyperparameters. FlowChef vs. Baseline FreeDoM. Algorithm 2 compares FlowChef to the baseline FreeDoM, diffusion model method that modifies the score function using classifier guidance-like approach. FreeDoM requires estimating velocity and calculating gradients (xt) through backpropagation via the ODESolver uθ, as marked in red. In contrast, as highlighted in green, FlowChef eliminates the need for backpropagation while still achieving convergence. This simplification makes FlowChef more efficient and practical solution without sacrificing performance. 14. Experimental Setup section outlines This the hyperparameters used for FlowChef and baseline methods in solving inverse problems. Pixel-Space Models. All evaluations were conducted using the Rectified Flow++ checkpoint. Since public implementations of OT-ODE and D-Flow are unavailable, we implemented these methods manually based on the provided pseudocode and performed hyperparameter tuning to ensure optimal performance. Notably, DPS and FreeDoM hyperparameters are the same as the FlowChef. Table 6 provides detailed overview of the hyperparameters used for each baseline. Latent-Space Models. For latent-space models, we extended D-Flow to the InstaFlow pretrained model, repurposed RectifID for inverse problems, and fine-tuned the hyperparameters for optimal results. The best-performing hyperparameters for each baseline are listed in Table 7. We utilized their baseline implementations for diffusion modelbased approaches such as Resample and PSLD-LDM, modifying only the number of inference steps. Specifically, we used 100 NFEs for Resample and 100/500 NFEs for PSLD. 15. RF-Inversion vs. FlowChef In this section, we briefly compare FlowChef with the concurrent work, RF-Inversion, which introduces an inversion strategy for rectified flow models using linear quadratic regulator perspective from optimal transport, particularly for image editing tasks. RF-Inversion relies on image inversion, significantly increasing compute timenearly doubling it compared to FlowChef. To evaluate, we conducted wearing glasses editing task using 256 randomly selected SFHQ face images on the Flux.1[dev] model. As shown in Table 9, FlowChef achieves competitive performance in half the time. At high level, RF-Inversion can be viewed as special case of FlowChef, where the starting point is an inverted image rather than random noise. We applied similar editing strategy to both methods for fair comparison, as outlined in Algorithm 1, using learning rate of 0.07, 20 optimization steps, and 30 total inference steps. On an A100 GPU, this configuration required approximately 15 seconds per inference. This comparison highlights the efficiency and versatility of FlowChef in handling image editing tasks. 16. Hyper-parameter Study Figures 10, 11, and 12 present an analysis of the impact of various hyperparameters on steering the InstaFlow model using FlowChef. Figure 10 demonstrates that lower learning rate combined with single optimization step is insufficient to effectively steer the model. Optimal performance is achieved with learning rate of 0.1. Additionally, Figure 11 shows that lower learning rates necessitate more optimization steps to achieve convergence. Finally, Figure 12 illustrates how the denoising trajectory can be controlled by adjusting the learning rate and optimization steps, enabling recovery of the target sample with the desired accuracy. This control is particularly critical for image editing tasks, where striking the right balance between preserving the reference sample and applying the editing prompt is essential. Table 8 further highlights optimal hyperparameter Model Hyperparameters Chage Object Add Object Remove Object Change Attrbiute Chage Pose Change Color Change Material Change Background Change Style FlowChef (InstaFlow) FlowChef (Flux) Learning rate Max setps Optimization steps Inference steps Full source steps Edit guidance scale Learning rate Optimization steps Inference steps Full source steps Edit guidance scale 0.5 50 1 50 30 2.0 0.4 1 30 5 4.5 0.5 50 1 50 30 2. 0.5 1 30 5 4.5 0.5 50 3 50 0 2.0 0.5 1 30 0 4.5 0.5 50 2 50 10 4.5 0.5 1 30 2 4.5 0.5 20 2 50 10 8. 0.5 1 30 5 7.5 0.5 30 2 50 20 8.0 0.4 1 30 3 10.0 0.5 50 2 50 20 4.0 0.5 1 30 5 4.5 1.0 50 4 50 0 3. 0.5 1 30 0 0.0 0.5 30 1 50 30 6.0 0.4 1 30 5 10.0 Table 8. Hyperparameter examples for which various editing tasks can be performed (following Algorithm 2). Notably, the FlowChef (Flux) variant can be further optimized for task-specific settings that will follow Algorithm 1 with careful selection of hyperparameters. Figure 10. Effect of FlowChef learning rate with fixed 20 max steps and one optimization step on InstaFlow. Methods CLIP-I () CLIP-T () Time () RF-Inversion FlowChef (ours) 0.8573 0.8269 0.2790 0. 31 sec 15 sec Table 9. Comparison of FlowChef with concurrent work RFInversion on top of Flux for editing task wearing glasses. settings for image editing tasks, providing valuable guidance for achieving high-quality edits. This study underscores the flexibility of FlowChef in adapting to diverse use cases by tuning these parameters effectively. 17. Qualitative Results Figure 14 showcases additional qualitative examples of image editing tasks. For tasks such as changing materials or removing objects, FlowChef outperforms the baselines significantly. However, some limitations are noted: while FlowChef (InstaFlow) struggles to replace cat with tiger, InfEdit handles this task effectively, and Ledits++ exhibits difficulties. On the other hand, FlowChef (Flux) achieves superior results, though it replaces dog with tiger instead of lion in one instance. In the final example, both Ledits++ and FlowChef successfully edit long hair into short hair. Importantly, the results in Figure 14 are presented without cherry-picking, using consistent hyperparameters for both baselines and FlowChef. Variability in outcomes may still arise due to random seeds and fine-tuned hyperparameter selection. Figures 15, 16, 17, 18, 19, and 20 provide pixel-level qualitative results for various inverse problems, spanning inpainting, deblurring, and super-resolution tasks under both easy and hard scenarios. Readers are encouraged to zoom in to inspect these comparisons more closely. For each task, we randomly selected 10 CelebA examples and evaluated various baselines. Across all difficulty levels, FreeDoM, DPS, and PnPFlow demonstrate better performance than D-Flow and OT-ODE. However, FlowChef consistently outperforms all baselines, producing sharp and visually appealing results where other 7 Figure 11. Effect of FlowChef optimization steps with fixed 20 max steps and 0.02 learning rate on InstaFlow. Figure 12. Effect of various FlowChefs steering parameters with increasing maximum optimization steps on InstaFlow. work is needed to address potential adversarial effects and further enhance robustness. 18. Limitations & Future Work Limitations. While FlowChef represents significant leap in steering RFMs for controlled generation, it shares some limitations with its baseline counterparts. Hyperparameter tuning remains challenge, particularly due to differences in trajectory behavior. For instance, while InstaFlow trajectories are relatively linear, Flux.1[Dev] trajectories exhibit non-linearity, necessitating careful tuning. As shown in Figure 7, FlowChef (Flux) faces difficulties in deblurring and super-resolution tasks, which we attribute to the pixel-space loss and non-linear behavior of the VAE model. Importantly, these limitations occur in less than 10% of cases and can often be resolved by simply adjusting the random seed. Furthermore, due to Fluxs lack of true classifier-free guidance (CFG), Algorithm 3 occasionally fails to perfectly execute color changes, someFigure 13. FlowChef (Flux) model failures on inverse problems and image editing. methods either fail outright or introduce excessive smoothness. Hard scenarios pose challenges for all methods, but FlowChef notably improves performance even under these conditions. While FlowChef shows promise, future 8 times producing the unaltered target image without reflecting the edit (see Figure 7). Despite these minor limitations, FlowChef still delivers state-of-the-art performance, making these challenges opportunities for further refinement rather than fundamental drawbacks. Future Work. FlowChef opens promising avenue for steering RFMs effortlessly with guaranteed convergence for controlled image generation. While this work extensively evaluates FlowChef on image generative models, future research should focus on expanding its capabilities to video and 3D generative models, areas that remain largely unexplored. Additionally, the current implementation assumes the availability of human-annotated masks for image editing. Automating this step with advanced attention mechanisms could make FlowChef fully automated image editing framework. We encourage the research community to build upon this foundation to enhance its accessibility and functionality. Ethical Concerns. As with all generative models, ethical concerns such as safety, misuse, and copyright issues apply to FlowChef [20, 21]. By enabling controlled generation with state-of-the-art RFMs, FlowChef can be leveraged for beneficial and harmful purposes. To mitigate these risks, future efforts should focus on solutions such as image watermarking, content moderation, and unlearning harmful behaviors. While these issues are not unique to FlowChef, addressing them will be key to ensuring its responsible use. 9 Figure 14. Qualitative results on image editing. Additional qualitative comparisons of FlowChef with the baselines. 10 Figure 15. Qualitative examples of various methods for easy box inpainting task on RF++. Figure 16. Qualitative examples of various methods for hard box inpainting task on RF++. 12 Figure 17. Qualitative examples of various methods for an easy deblurring task on RF++. 13 Figure 18. Qualitative examples of various methods for the hard deblurring task on RF++. Figure 19. Qualitative examples of various methods for an easy super-resolution task on RF++. 15 Figure 20. Qualitative examples of various methods for the hard super-resolution task on RF++."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Rutgers University"
    ]
}