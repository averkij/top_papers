{
    "paper_title": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection",
    "authors": [
        "Qingyu Fan",
        "Yinghao Cai",
        "Chao Li",
        "Wenzhe He",
        "Xudong Zheng",
        "Tao Lu",
        "Bin Liang",
        "Shuo Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/."
        },
        {
            "title": "Start",
            "content": "NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection Qingyu Fan1,2,3, Yinghao Cai1,2, Chao Li3, Wenzhe He3, Xudong Zheng3, Tao Lu1, Bin Liang3, Shuo Wang1,2 5 2 0 2 5 ] . [ 1 1 1 5 3 0 . 3 0 5 2 : r Abstract Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/. I. INTRODUCTION Vision based 6-DoF grasping in robotics remains significant challenge due to the wide diversity of object shapes, textures, and material properties in real-world environments [1], [2], [3], [4], [5], [6]. Accurate reconstruction of 3D scene geometry is essential for determining feasible grasps. However, this task is often compromised by limitations of depth sensors which produce unreliable depth measurements when confronted with transparent and specular objects. Such limitations result in degraded geometric representations of the scene and, consequently, unsuccessful grasps. Recent research has explored the use of neural radiance field (NeRF) [7] to address the challenge of grasping transparent and specular objects due to its ability to effectively capture complex light interactions on non-Lambertian surfaces. DexNeRF [8] pioneers this approach but it suffers from the need for dense input images and extensive per-grasp training, making it impractical for real-world applications. GraspNeRF [9] uses generalizable NeRF that requires only sparse views and eliminates the need for per-scene optimization. However, it still relies on comprehensive 360-degree image capture and ground-truth Truncated Signed Distance Function (TSDF) supervision, which may not always be 1State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences. 2School of Artificial Intelligence, University of Chinese Academy of Sciences. 3Qiyuan Lab. *This work was supported in part by the National Natural Science Foundation of China under Grants 62273342, 62473366 and U23B2038, and in part by the Qiyuan Lab Innovation Fund Project (2022-JCJQ-LA001-024). Corresponding to yinghao.cai@ia.ac.cn. Fig. 1. Overview of NeuGrasp. We introduce generalizable method that utilizes background priors within neural implicit surface framework to achieve real-time scene reconstruction and material-agnostic grasping from observations within narrow field of view. feasible in real-world scenarios. RGBGrasp [10] further improves upon this by integrating an all-in-one NeRF extension from [11] with pre-trained models for depth estimation and grasp detection, achieving robust performance with RGBonly inputs. However, it still relies on dense views and lacks generalizability, requiring time-consuming retraining for each grasp, which limits its use for real-time applications. In this paper, we introduce NeuGrasp for the 6-DoF robotic grasping tasks  (Fig. 1)  . NeuGrasp is generalizable method that efficiently utilizes background priors to reconstruct scene geometry and generate diverse grasp candidates from observations within narrow field of view, all in real time and without the need for depth-related supervision. By employing Transformer architecture within neural implicit reconstruction framework, NeuGrasp combines multi-view features enriched with spatial encoding to infer scene geometry, thereby facilitating surface reconstruction in novel grasping scenarios. Additionally, the integration of highlevel global prior volume allows NeuGrasp to perform effectively even under narrow fields of view and sparse viewing conditions. key aspect of our approach is the use of background priors for surface reconstruction with transparent and specular objects. We propose residual feature enhancement module which enhances the models attention on foreground objects by contrasting features from scene and background images. Additionally, we introduce an occupancy-prior volume that utilizes global implicit occupancy information from residual feature maps to enhance spatial perception, especially for transparent and specular objects. Experimental results show that NeuGrasp significantly outperforms baselines especially in scenarios that involve only transparent and specular objects. The performance of surface reconstruction of NeuGrasp is on par with the methods that use explicit geometry supervision. With finetuning on small real-world dataset, NeuGrasp-RA (Reality Augmentation) further improves the performance of robotic grasping which demonstrates the potential of our approach to be applied in real applications. In summary, our contributions are: We integrate view transformer, ray transformer, and global prior volume to hierarchically aggregate multiview features and spatial priors, providing an effective solution for generalizable implicit surface reconstruction under narrow and sparse view conditions. Both surface reconstruction and grasp detection are trained in an end-to-end manner. We propose to leverage scene background priors for surface reconstruction in scenes with transparent and specular objects. Through residual feature enhancement, these objects can be clearly distinguished from the background, facilitating more accurate grasp detection. We demonstrate the superiority of our method in both simulated and real-world experiments for the tasks of reconstruction and grasp detection. NeuGrasp significantly outperforms baselines in grasping particularly in scenarios with transparent and specular objects. II. RELATED WORK A. Generalizable Neural Radiance Field NeRF [7], initially proposed for novel view synthesis using volume rendering and implicit neural representations, has since been extended to scene reconstruction and augmented reality. Some existing works [11], [12], [13], [14], [15] rely on per-scene optimization, which limits the generalization to unseen scenes. IBRNet [16] introduces generalization by blending nearby views using MLPs and ray transformers. NeuRay [17] considers the view visibility. ContraNeRF [18] introduces contrastive learning for synthetic-to-real generalization. However, the implicit field representation of NeRF may produce inaccurate surface reconstruction for the task of grasp detection. NeuS [19] addresses this issue using the signed distance function (SDF) for more accurate multi-view reconstructions. Other methods such as SparseNeuS [20] use hierarchical feature volumes, and C2F2NeuS [21] combines multi-view stereo (MVS) with implicit reconstruction. VolRecon [22] and ReTR [23] use transformer for multi-view feature fusion, which helps refine details and improves accuracy of surface reconstruction. Furthermore, UFORecon [24] improves the robustness of neural surface with cross-view matching transformers in challenging viewing conditions. B. NeRF in Robotic Grasping Grasp pose detection using RGB-D sensors has been extensively studied for decades [1], [2], [3], [4], [5], [6]. However, grasp pose detection from point clouds continues to face challenges with transparent and specular objects due to inaccuracies in depth maps. NeRF provides solution to address this issue by modeling light propagation, as seen in DexNeRF [8]. DexNeRFs reliance on dense inputs and extensive training requirements limit its practicality in realworld applications. EvoNeRF [25] and MIRA [26] optimize NeRF for 6-DoF grasping but still require dense inputs. Residual-NeRF improves depth perception using static, opaque regions as priors but is impractical due to the limited availability of such priors in real scenarios. GraspNeRF [9] removes the need for per-scene optimization, but requires 360-degree captures and ground-truth TSDF for supervision, which limits real-world applications. Similarly, RGBGrasp [10] integrates an all-in-one NeRF extension but still requires dense inputs and retraining for each grasp. In contrast, we propose NeuGrasp, which overcomes these limitations by leveraging background priors and generalizable implicit surface reconstruction. NeuGrasp enables material-agnostic grasping in narrow field of view without the need for geometric supervision. A. Problem Description III. METHOD Given sequence of images {Ii}N i=1 captured from fixed trajectory with narrow field of view, the task is to reconstruct the scene geometry and predict collision-free 6DoF grasps. NeuGrasp takes as input the scene images {Ii}N i=1, and camera parameters {Pi}N i=1, background images {Bi}N i=1, reconstructing an SDF volume of the scene to generate 6DoF grasp predictions {gjgj = (tj, rj, qj, wj)} defined by the grasp center R3, the orientation SO (3), the quality score [0, 1] and the opening width R. B. Preliminaries The generalizable neural surface reconstruction aims to recover the scene geometry using neural rendering pipeline with multi-view inputs {Ij, Pj}N j=1, where Ij and Pj are images and camera parameters, respectively. The pipeline (cid:9)N uses an encoder to extract image features (cid:8)f img j=1. In volume rendering, points are sampled along ray from the camera center to the pixel. At each point pi, the geometry network Fgeo aggregates multi-view features via projection and bilinear interpolation to estimate the SDF σi, while the weight network Fweight aggregates multi-view features to estimate the blending weights, which are then multiplied by colors and summed to compute the radiance ci. The predicted SDF is converted into weights along the ray using the conversion function from [19], which are then applied for volume rendering. Framework of NeuGrasp. NeuGrasp leverages background priors for neural surface reconstruction and material-agnostic grasp detection. Fig. 2. Residual Feature Enhancement module is proposed to enhance the model attention on foreground objects instead of irrelevant background information. Through feature projection, we construct an occupancy-prior volume from residual features and shape-prior volume from scene features. These volumes are then separately combined with their corresponding multi-view features using View Transformers. After fusion, Ray Transformer further refines the spatial information. The final reconstructed geometry is represented as signed distance function and converted into radiance field. Finally, the grasping module maps the reconstructed geometry to 6-DoF grasp poses, enabling end-to-end training. C. Reconstruction with Background Priors 1) Reconstruction Transformers: As in [22], we use Transformers to integrate multi-view features and predict geometry and weights for blending. Additionally, we introduce high-level shape prior volume obtained from multi-view features to provide global shape prior. High-level Shape Prior Volume. The mean and variance of multi-view features at each voxel center are calculated in the workspace volume. To handle the inconsistencies on transparent and specular surfaces, we apply 3D U-Net to infer the shape details, producing high-level prior volume that represents the global prior. View Transformer with Feature Fusion. The viewfusion transformer Tview combines multi-view features (cid:8)f img (cid:9)N j=1 with volume feature fv at position p, which is obtained through trilinear interpolation in V. Following [22], [24], learnable query token ft is introduced to obtain the unified view feature fu. These features are then combined using linear self-attention transformers [27]: (π (p)) (cid:9)N (cid:0)(cid:8)f img fu, (cid:8)f img (cid:9)N j=1 = Tview j=1, fv, ft (5) (cid:1), where img denotes the attention-weighted feature used for color blending and π () is the projection function. Ray Transformer with Geometric Awareness. Given the non-local characteristic of SDF, we embed the workspace volume coordinates using fixed positional encoding. These embeddings are then compressed with the unified view feature through an MLP. The sampling order along the ray indicates the occlusion information. Therefore, we apply ray transformer with linear self-attention to integrate spatial information and adjust attention along the ray: fgeo = Tray (MLP (fu, γcoord) , γorder) . (6) Here, γ represents positional encoding, and fgeo is the geometry feature used to decode the SDF. 2) Background Prior Based Feature Enhancement: Although the aforementioned reconstruction pipeline is effective in capturing the geometry of diffuse surfaces, accurately estimating the surfaces of transparent and specular objects remains challenging due to their complex optical properties. In many grasping applications, the background scenarios remain static during grasping. Inspired by the background subtraction method [28], we leverage this background prior to guide the attention of the reconstruction module on foreground objects rather than background information. Our approach allows the model to notice subtle differences between the transparent and specular surfaces and the background. To reduce the effect of noise to illumination variations, instead of performing direct background subtraction, we first employ lightweight Res U-Net [29] to extract features from scene images (cid:8)f scn (cid:9)N j=1 and background images (cid:8)f bg (cid:9)N j=1. Next, we propose residual feature enhancement based on subtraction attention mechanism [30] to re-weight the original features to obtain foreground-attentive features (cid:8)f attn (cid:9)N j=1 attn = ρ (cid:0)f (cid:0)η (f scn) φ (cid:0)f bg(cid:1)(cid:1)(cid:1) δ (f scn) , 4 C. 4 (7) j where η, φ, and δ are linear projections, ρ is the sigmoid normalization function, and is mapping function. It is observed from experiments that with the residual feature enhancement, the reconstruction demonstrates improved attention to foreground objects, which produces more accurate surface reconstruction results for grasping. 3) Spatial Residual Information Aggregation: The residual features, obtained by subtraction of background features from scene features, implicitly indicates the occupancy of foreground objects. Building on the aforementioned prior volume, we extend the residual features from 2D feature maps to 3D feature volumes to obtain prior volume that encapsulates global occupancy information. This occupancy prior imposes more explicit and powerful constraint on spatial perception. Consequently, we combine the foregroundattentive features and prior knowledge from occupancy feature volume with query token using another view transformer. To avoid confusion, we restate the symbols as follows. The shape-prior volume and occupancy-prior volume are represented as Vshp and Vocc, respectively. The volume features obtained through trilinear interpolation are denoted as shp . The view transformer that aggregates source information is denoted as src view, while the view transformer that aggregates residual information is denoted as res (cid:0)(cid:8)f scn view: (cid:1), (8) (π(p))(cid:9)N and occ , (cid:8)f src src (cid:9)N j=1 = src j=1, shp , src view j j view (cid:0)(cid:8)f attn (π(p))(cid:9)N (cid:9)N j=1 = res , (cid:8)f res res Subsequently, we employ an MLP to fuse the unified view features, src . Additionally, 5-layer MLP is used to fuse two types of attention-weighted features and predict the weights for blending. and res (cid:1). (9) , res , occ j=1 D. Grasp Detection with Reconstructed Geometry With the reconstructed implicit geometry, we first use volumetric queries to obtain discrete volume grid. Leveraging its regularity, we feed the TSDF voxel grid into 3D CNN to generate grasp parameters for each voxel center [3]. In our approach, the segmentation mask of foreground objects [2] or explicit collision detection [1] is not necessary. The volumetric mapping from geometry to grasp parameters integrates seamlessly with the reconstruction pipeline, enabling end-to-end, differentiable training for efficient optimization. Finally, we apply mask-out and non-maxima suppression as in [3] to ensure reliable grasp predictions. E. Implementation Details 1) Camera Viewpoint Trajectory: While dense scene observation improves geometric reconstruction and grasp prediction, it reduces flexibility in real-world scenarios. For example, GraspNeRF [9] uses six 360 views, and RGBGrasp [10] uses 90 cylindrical path with 12 dense views. We sample 4 camera views along spiral trajectory that covers one-sixth of hemisphere with spherical coordinates: (cid:1), and radius [0.4, 0.5] m, polar angle θ (cid:0) π azimuthal angle ϕ (cid:0)0, π (cid:1), aiming to optimize both performance and flexibility. 12 , π 8 2) Training Details: We use end-to-end training to jointly optimize both reconstruction and grasping tasks. The total loss is weighted sum of the following losses: Photometric Loss. We compute the mean squared error (MSE) between rendered and ground truth pixel colors over sampled rays [7]. Grasping Loss. Following [3], we supervise grasp quality, orientation, and gripper width. Supervision is applied only to successful grasps. Regularization Loss. We include an Eikonal term to enforce correct SDF properties, ensuring gradient magnitudes are close to 1 as in [19]. Fig. 3. Real-world Experiments. (a) Experimental settings. (b) Visualization of grasps, where orange represents positive labels and light blue represents negative labels. We sample 2304 rays per batch using learning rate of 1e-4 with exponential decay. All experiments are run on an NVIDIA RTX 4090D GPU. IV. EXPERIMENTS In this section, we evaluate our approach in both simulated and real-world experiments. Ablation studies are also performed to evaluate the impact of different components. A. Experiment Setup 1) Simulation Setup: We use PyBullet [31] for physicsbased grasping simulation and Blender [32] to generate photorealistic synthetic data. Grasp data is autonomously collected by randomly sampling grasp centers and orientations near object surfaces, followed by executing these attempts in the simulation. The workspace is 30 30 30 cm3 tabletop area following [3], [9]. We simulate three types of scenes: Single setting where individual objects are placed at the tabletop center, Pile setting where multiple objects are dropped from above to form cluttered pile, and Packed setting multiple objects are placed upright in random positions to form densely packed arrangement. Single: Individual objects are placed in the workspace, each in an isolated and upright position. Pile: Multiple objects are dropped from above to form cluttered pile. Packed: Multiple objects are placed upright in random positions, simulating densely packed arrangement. For baselines that require depth information, the simulation environment also considers realistic depth noise using depth sensor simulator [33]. This ensures that the simulated depths accurately reflects real-world sensor characteristics. 2) Real-world Setup: As shown in Fig. 3, we use UR5 robot arm with Robotiq 2-Finger 85 gripper for real-world grasping tasks. An Intel RealSense D435 RGB-D camera, mounted on the robots wrist, captures RGB data. The depth information is not used. The robot operates within 3030 30 cm3 workspace, inferring grasp poses in real time after capturing 4 scene images along the predefined trajectory. Backgrounds are pre-prepared and reused across all scenes. We also collect real-world dataset consisting of backgrounds, scenes, and grasp data. This real-world dataset is used to fine-tune the model, resulting in an improved version of NeuGrasp, referred to as NeuGrasp-RA. NeuGraspRA generates more accurate grasp predictions by reducing TABLE RESULTS OF SIMULATION EXPERIMENTS IN PACKED SCENE, MATERIAL: TRANSPARENT&SPECULAR (DIFFUSE) Method VGN SwinDR-VGN GraspNeRF w/ TSDF GraspNeRF w/o TSDF RGBGrasp-GT Ours (NeuGrasp) SR (%) 27.4(47.9) 40.8(66.0) 84.2(74.2) 47.6(43.3) 36.2(42.7) 86.3(88.3) Selective Random DR (%) 18.0(36.7) 35.6(55.8) 80.3(75.7) 50.2(43.9) 18.9(33.1) 81.0(81.0) FSR (%) 21.0(40.5) 43.0(57.8) 85.5(79.9) 54.0(52.8) 23.0(40.0) 86.0(86.0) L1 0.273(0.205) 0.239(0.204) 0.110(0.112) 0.894(0.897) 0.807(0.745) 0.153(0.152) SR (%) 12.7(44.5) 46.0(46.0) 76.3(81.5) 50.4(46.9) 25.5(36.3) 86.8(90.3) Top Score FSR (%) 10.0(46.2) 40.0(39.5) 80.5(93.5) 64.5(61.4) 14.5(42.0) 92.0(94.5) DR (%) 7.3(27.8) 39.1(38.1) 76.6(81.1) 44.8(43.1) 12.9(22.4) 79.1(82.9) L1 0.288(0.222) 0.240(0.234) 0.113(0.112) 0.895(0.897) 0.811(0.733) 0.155(0.154) TABLE II RESULTS OF REAL-WORLD EXPERIMENTS IN PILE AND PACKED SCENES, MATERIAL: MIXED Method GraspNeRF w/ TSDF Ours (NeuGrasp) Ours (NeuGrasp-RA) SR (%) 38.9 (28 / 72) 63.7 (72 / 113) 84.5 (120 / 142) Pile FSR (%) 40.0 (10 / 25) 68.0 (17 / 25) 92.0 (23 / 25) DR (%) 22.4 (28 / 125) 57.6 (72 / 125) 96.0 (120 / 125) SR (%) 46.7 (42 / 90) 84.1 (106 / 126) 85.8 (121 / 141) Packed FSR (%) 64.0 (16 / 25) 92.0 (23 / 25) 88.0 (22 / 25) DR (%) 33.6 (42 / 125) 84.8 (106 / 125) 96.8 (121 / 125) TABLE III ABLATION STUDIES IN PILE AND PACKED SCENES, MATERIAL: TRANSPARENT&SPECULAR (DIFFUSE) Method w/o attn w/o Vocc w/o backgrounds Ours (NeuGrasp) Pile Packed SR (%) 51.4(45.1) 61.8(65.1) 58.3(58.8) 65.2(65.3) FSR (%) 41.6(34.0) 65.0(63.0) 62.5(62.5) 67.5(64.5) DR (%) 21.6(18.9) 42.6(42.5) 44.4(41.4) 45.6(43.2) L1 0.906(0.887) 0.360(0.379) 0.403(0.414) 0.164(0.171) SR (%) 76.3(77.4) 82.2(78.5) 79.6(76.8) 86.3(88.3) FSR (%) 73.0(75.0) 83.0(67.5) 82.0(85.5) 86.0(86.0) DR (%) 69.1(71.5) 78.1(73.6) 78.5(77.4) 81.0(81.0) L1 0.887(0.899) 0.321(0.326) 0.378(0.375) 0.153(0.152) the domain gap between simulation and real-world environments. 3) Object Set: To ensure consistency with baseline methods, we use 473 hand-scaled object meshes in simulation, with 417 for training and 56 for testing [3]. During training, we randomize object textures and materials, including transparent, specular, and diffuse types. Additionally, we collect 62 diverse household objects for real-world fine-tuning and evaluation. See Fig. 3 for an overview of these objects. B. Baseline Methods All the methods are evaluated using 4 camera views along the same trajectory, unless noted otherwise. 1) RGB-D-Based Methods: VGN [3]: volumetric grasp detection network that uses TSDF volumes from multiple depth views to predict 6-DoF grasp poses. SwinDR-VGN: variant of VGN that incorporates SwinDRNet [33] for depth restoration to address depth inaccuracies with transparent and specular materials. 2) RGB-TSDF-Based Method: GraspNeRF [9]: real-time, multi-view 6-DoF grasp detection network utilizing generalizable NeRF. GraspNeRF is material-agnostic grasping method. 3) RGB-Based Method: RGBGrasp-GT: RGBGrasp [10] is reconstructiongrasp pipeline leveraging Nerfacto [11] with pre-trained monocular depth estimator [34] and grasp detector [6]. In RGBGrasp-GT, we use ground truth depth maps from simulation instead of depth prediction from [34] to provide supervision and replace [6] with VGN for grasp detection for fair comparison. C. Evaluation Metrics Success Rate (SR): The ratio of successful grasps to the total number of grasp attempts. Declutter Rate (DR): The ratio of objects successfully removed from the scene. First Success Rate (FSR): The ratio of rounds with successful first grasp to the total number of rounds. Mean Absolute Error on TSDF (L1): The average absolute difference between the reconstructed surface and the ground truth. D. Simulated Grasping Experiments In each scene, we conduct multiple decluttering experiments with: 1) transparent and specular materials, and 2) diffuse materials. We perform 200 rounds for the pile and packed scenarios, and 56 rounds for single-object scenarios, to demonstrate the material-agnostic grasping capability of our method. Each pile and packed scene contains 5 objects. We use two grasp execution strategies: 1) Top Score, which executes the highest-scored grasp, and 2) Selective Random, which executes random grasp with score above 0.9, which considers more diverse grasp options. The robot continues executing grasps until the workspace is cleared, the prediction fails, or two consecutive grasp failures occur. To ensure fair comparison, we re-train GraspNeRF on our dataset using the viewpoint trajectory and parameter settings in [9]. We train two GraspNeRF models: one with TSDF loss (GraspNeRF w/ TSDF) and one without (GraspNeRF w/o TSDF). Since VGN was trained with extensive data augmentation and on significantly larger dataset, we directly use the best pre-trained VGN model [3]. The results for the packed scene are summarized in Tab. I. More results are provided in the supplementary materials. It is observed in Tab. that VGN struggles to generate accurate grasp predictions due to overly noisy and missing data Comparison of View Features between GraspNeRF and Fig. 4. NeuGrasp. It shows that with the residual feature enhancement, NeuGrasp improves focus on the foreground objects. The transparent and specular objects are clearly distinguished from the background. from the sensor. While SwinDR-VGN uses depth restoration, it tends to over-smooth object surfaces. RGB-D based methods still have difficulty in dealing with diffuse materials under limited view conditions, which highlights the challenge of grasping from narrow field of view. GraspNeRF w/ TSDF performs better compared with GraspNeRF w/o TSDF. The direct TSDF supervision helps the reconstruction to align closely with the ground truth. NeuGrasp outperforms all methods in most grasp metrics with no explicit geometry supervision. This is due to NeuGrasps ability to uncover geometric structures by effectively integrating scene and background information using only RGB data. Moreover, the residual feature enhancement in NeuGrasp not only facilitates the reconstruction process by better attending to foreground objects, resulting in more accurate surface reconstruction for grasping, but also sharpens the focus on target grasp region, leading to more precise and reliable grasp predictions. To illustrate, t-SNE [35] is used to visualize scene and foreground-attentive features. The features of GraspNeRF are also shown in Fig. 4. The results show that with the residual feature enhancement, object contours can be clearly distinguished from the background, which validates the models improved focus on target objects. Due to narrow and sparse viewpoints, RGBGrasp-GT fails to predict accurate depth, even with ground truth depth supervision. Similar to classical NeRF methods, RGBGraspGT relies heavily on multiple views. The sparse views result in large depth variances, which results in inaccurate TSDF reconstruction results and unreliable grasp predictions. Moreover, RGBGrasp-GT requires retraining for each grasp, with an average prediction time of 56.7 seconds. In contrast, our method achieves an average inference time of 0.274 seconds, which demonstrates its advantages in real applications. E. Real-world Grasping Experiments To evaluate the performance of real-world grasping, we collect 62 daily objects and conduct 25 rounds of experiments in both pile and packed scenes. Using the same experimental setting as in simulations, we apply only the Selective Random strategy here. Due to the absence of ground truth depth maps, we compare our method with GraspNeRF w/ TSDF. Additionally, we fine-tune our model on small real-world dataset collected from subset of objects, where test objects are not included. Tab. II shows NeuGrasp-RA achieves the best performance. Fine-tuning on small real-world dataset allows Fig. 5. Visualization Results. NeuGrasp achieves cleaner reconstruction and greater grasp diversity compared to GraspNeRF. NeuGrasp-RA, finetuned with small-scale real-world data, further improves the performance, particularly in grasp diversity. NeuGrasp to better adapt to real-world conditions, effectively reducing the domain gap. Although GraspNeRF uses geometry ground truth and domain randomization, NeuGrasp and NeuGrasp-RA significantly outperform GraspNeRF w/ TSDF. It can be observed from Fig. 5 that our method produces cleaner scene reconstruction and more accurate grasp candidates. F. Ablation Studies To evaluate the impact of each component in NeuGrasp, we conduct ablation studies in simulated pile and packed scenes (Tab. III) using the Selective Random strategy. Residual Feature Enhancement. It can be observed in Tab. III that replacing attn with scn leads to significant drop in grasp performance and large L1 error. Without this module, the model fails to leverage background information, resulting in collapse in reconstruction. Occupancy-Prior Volume. Removing this volume slightly reduces grasp performance in pile scenes but causes significant drop in packed scenes and reconstruction quality overall. The occupancy-prior volume proves crucial for handling occluded packed scenes without explicit geometric supervision. Background Priors. When all modules related to background priors are removed, the model underperforms compared to both NeuGrasp w/o Vocc and the full NeuGrasp, both of which leverage background priors. The reconstruction performance also drops significantly, highlighting the crucial role of background priors in effective object grasping in cluttered scenes. V. CONCLUSIONS In this paper, we introduced NeuGrasp, neural surface reconstruction method for robotic grasping in cluttered scenes with transparent and specular objects. By leveraging background priors and multi-view feature aggregation, NeuGrasp enables robust, material-agnostic grasp detection without depth-related supervision. Experimental results show its superior performance over state-of-the-art methods in both simulated and real-world scenarios. reconstruction, Advances in Neural Information Processing Systems, vol. 34, pp. 27 17127 183, 2021. [20] X. Long, C. Lin, P. Wang, T. Komura, and W. Wang, Sparseneus: Fast generalizable neural surface reconstruction from sparse views, in European Conference on Computer Vision. Springer, 2022, pp. 210227. [21] L. Xu, T. Guan, Y. Wang, W. Liu, Z. Zeng, J. Wang, and W. Yang, C2f2neus: Cascade cost frustum fusion for high fidelity and generalizable neural surface reconstruction, in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 18 24518 255. [22] Y. Ren, F. Wang, T. Zhang, M. Pollefeys, and S. Susstrunk, Volrecon: Volume rendering of signed ray distance functions for generalizable multi-view reconstruction, in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 16 685 16 695. [23] Y. Liang, H. He, and Y. Chen, Retr: Modeling rendering via transformer for generalizable neural surface reconstruction, Advances in Neural Information Processing Systems, vol. 36, 2024. [24] Y. Na, W. J. Kim, K. B. Han, S. Ha, and S.-E. Yoon, Uforecon: Generalizable sparse-view surface reconstruction from arbitrary and unfavorable sets, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 50945104. [25] J. Kerr, L. Fu, H. Huang, Y. Avigal, M. Tancik, J. Ichnowski, A. Kanazawa, and K. Goldberg, Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects, in Conference on Robot Learning. PMLR, 2023, pp. 353367. [26] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia, and P. Isola, Mira: Mental imagery for robotic affordances, in Conference on Robot Learning. PMLR, 2023, pp. 19161927. [27] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, Loftr: Detectorfree local feature matching with transformers, in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 89188927. [28] M. Piccardi, Background subtraction techniques: review, in 2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583), vol. 4, 2004, pp. 30993104 vol.4. [29] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional image networks for biomedical computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 2015, pp. 234241. image segmentation, in Medical [30] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun, Point transformer, in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 16 23916 248. [31] E. Coumans and Y. Bai, Pybullet, python module for physics simulation for games, robotics and machine learning, 20162021, [Online]. Available: http://pybullet.org. [32] Blender, [Online]. Available: https://www.blender.org/. [33] Q. Dai, J. Zhang, Q. Li, T. Wu, H. Dong, Z. Liu, P. Tan, and H. Wang, Domain randomization-enhanced depth simulation and restoration for perceiving and grasping specular and transparent objects, in European Conference on Computer Vision. Springer, 2022, pp. 374391. [34] K. Zhou, L. Hong, C. Chen, H. Xu, C. Ye, Q. Hu, and Z. Li, Devnet: Self-supervised monocular depth learning via density volume construction, in European Conference on Computer Vision. Springer, 2022, pp. 125142. [35] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. Journal of machine learning research, vol. 9, no. 11, 2008."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H.-S. Fang, C. Wang, M. Gou, and C. Lu, Graspnet-1billion: largescale benchmark for general object grasping, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 44411 453. [2] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, Contactgraspnet: Efficient 6-dof grasp generation in cluttered scenes, in 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021, pp. 13 43813 444. [3] M. Breyer, J. J. Chung, L. Ott, R. Siegwart, and J. Nieto, Volumetric grasping network: Real-time 6 dof grasp detection in clutter, in Conference on Robot Learning. PMLR, 2021, pp. 16021611. [4] L. Zheng, Y. Cai, T. Lu, and S. Wang, Vgpn: 6-dof grasp pose detection network based on hough voting, in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 74607467. [5] L. Zheng, W. Ma, Y. Cai, T. Lu, and S. Wang, Gpdan: Grasp pose domain adaptation network for sim-to-real 6-dof object grasping, IEEE Robotics and Automation Letters, vol. 8, no. 8, pp. 45854592, 2023. [6] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE Transactions on Robotics, vol. 39, no. 5, pp. 39293945, 2023. [7] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, in European Conference on Computer Vision. Springer, 2020, pp. 405421. [8] J. Ichnowski, Y. Avigal, J. Kerr, and K. Goldberg, Dex-nerf: Using neural radiance field to grasp transparent objects, in Conference on Robot Learning. PMLR, 2022, pp. 526536. [9] Q. Dai, Y. Zhu, Y. Geng, C. Ruan, J. Zhang, and H. Wang, Graspnerf: Multiview-based 6-dof grasp detection for transparent and specular objects using generalizable nerf, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 17571763. [10] C. Liu, K. Shi, K. Zhou, H. Wang, J. Zhang, and H. Dong, Rgbgrasp: Image-based object grasping by capturing multiple views during robot arm movement with neural radiance fields, IEEE Robotics and Automation Letters, vol. 9, no. 6, pp. 60126019, 2024. [11] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja et al., Nerfstudio: modular framework for neural radiance field development, in ACM SIGGRAPH 2023 Conference Proceedings, 2023, pp. 112. [12] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan, Mip-nerf: multiscale representation for antialiasing neural radiance fields, in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 58355844. [13] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-nerf 360: Unbounded anti-aliased neural radiance fields, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 54605469. [14] T. Muller, A. Evans, C. Schied, and A. Keller, Instant neural graphics primitives with multiresolution hash encoding, ACM transactions on graphics (TOG), vol. 41, no. 4, pp. 115, 2022. [15] G. Wang, Z. Chen, C. C. Loy, and Z. Liu, Sparsenerf: Distilling depth ranking for few-shot novel view synthesis, in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 9031 9042. [16] Q. Wang, Z. Wang, K. Genova, P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser, Ibrnet: Learning multi-view image-based rendering, in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 4688 4697. [17] Y. Liu, S. Peng, L. Liu, Q. Wang, P. Wang, C. Theobalt, X. Zhou, and W. Wang, Neural rays for occlusion-aware image-based rendering, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 78147823. [18] H. Yang, L. Hong, A. Li, T. Hu, Z. Li, G. H. Lee, and L. Wang, Contranerf: Generalizable neural radiance fields for synthetic-to-real novel view synthesis via contrastive learning, in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 16 50816 517. [19] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, Neus: Learning neural implicit surfaces by volume rendering for multi-view"
        }
    ],
    "affiliations": [
        "Qiyuan Lab",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences"
    ]
}