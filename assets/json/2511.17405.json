{
    "paper_title": "Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT",
    "authors": [
        "Yesheng Liu",
        "Hao Li",
        "Haiyu Xu",
        "Baoqi Pei",
        "Jiahao Wang",
        "Mingxuan Zhao",
        "Jingshu Zheng",
        "Zheqi He",
        "JG Yao",
        "Bowen Qin",
        "Xi Yang",
        "Jiajun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 2 5 0 4 7 1 . 1 1 5 2 : r Beyond Multiple Choice: Veriﬁable OpenQA for Robust Vision-Language RFT Yesheng Liu1,2,3, Hao Li3,4, Haiyu Xu3,5, Baoqi Pei6, Jiahao Wang1,2,3, Mingxuan Zhao3,5, Jing-Shu Zheng3 , Zheqi He3, JG Yao3, Bowen Qin3, Xi Yang3, Jiajun Zhang1,2 1Institute of Automation, CAS, 2School of Artiﬁcial Intelligence, UCAS, 3BAAI FlagEval Team, 4BUAA, 5PKU, 6ZJU Project Page: https://flageval-baai.github.io/ReVeL/"
        },
        {
            "title": "Abstract",
            "content": "Multiple-choice question answering (MCQA) has been popular format for evaluating and Its constrained reinforcement ﬁne-tuning (RFT) of modern multimodal language models. output format allows for simpliﬁed, deterministic automatic veriﬁcation. However, we ﬁnd that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), framework that rewrites multiple-choice questions into open-form questions while keeping answers veriﬁable whenever possible. The framework categorizes questions according to diﬀerent answer types, apply diﬀerent rewriting and veriﬁcation schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to ﬁnetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data eﬃciency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inﬂation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly."
        },
        {
            "title": "1 Introduction",
            "content": "As large language and multimodal models (Anthropic, 2025; OpenAI, 2025; Bai et al., 2025; OpenAI, 2023; Google, 2025; Chen et al., 2025; Pei et al., 2025) increasingly tackle diverse real-world tasks, the demand for reliable and scalable evaluation has grown signiﬁcantly. MCQA is convenient because restricting outputs simpliﬁes scoring (Moore et al., 2023) across language (Hendrycks et al., 2020; Wang et al., 2024a) and vision language benchmarks (Yue et al., 2023; 2024; Liu et al., 2023; Kembhavi et al., 2016; Zhang et al., 2024; Hao et al., 2025). However, MCQA departs from real-world usage where answers are usually open-ended (Lyu et al., 2024), while the predeﬁned options encourage selection heuristics (Zheng et al., 2023; Balepur et al., 2024) rather than genuine understanding. To quantify the unreliability of MCQA for evaluation and veriﬁcation, we conduct multiple experiments: (1) When options are added to the questions in an open-form benchmark, the accuracy metrics can be greatly boosted; (2) In MCQA benchmarks, When the ground-truth option is perturbed, or replaced with None of the above, model behavior degrades. These patterns indicate that the MCQA metrics are heavily dependent on the option set, rather than solely on the knowledge and skills required in the question stem. This fragility matters because many visual reasoning datasets used for outcome-based RFT have included large proportions of MCQA data. We ﬁnd that training on MCQA increases multiple-choice accuracy metrics but hurts open-form generalization, widening the gap between the two evaluation settings. In other words, this reward encourages shortcuts tied to options rather than transferable knowledge or reasoning (See Figure 1). Therefore, we present ReVeL (Rewrite and Verify by LLM), uniﬁed framework that rewrites MCQA into open-ended QA (OpenQA) and preserves veriﬁability whenever possible. ReVeL categorizes the original multiple-choice questions into numeric, keyword, per-option veriﬁcation, and genuinely generative cases. The ﬁrst three types can be accurately graded by deterministic rules, and only the last type may need an LLM Judge for grading. This hybrid design reduces cost and variance from trivial solution that entirely uses an LLM judge for all problems, while maintaining reliability during evaluation. Across four benchmarks, 7096% of items become rule-veriﬁable, reaching higher judging accuracy numbers than entirely using strong LLM judge (GPT 4.1 mini). 1 Based on ReVeL, we also rewrite 20k MCQA examples into OpenQA and perform GRPO-based RFT on Qwen2.5-VL-3B/7B. Models trained with ReVeL-OpenQA match MCQA accuracy on choice benchmarks while improving OpenQA accuracy by about six percentage points, demonstrating higher data eﬃciency and stronger robustness than MCQA-based training. With modest data, OpenQA-trained 7B models also exceed the counterparts trained on open-source data recipes such as VL-Rethinker-7B (Wang et al., 2025a), R1-OneVision-7B (Yang et al., 2025), and Mixed-R1-7B (Xu et al., 2025a) on open-ended evaluation. In summary, our contributions are threefold: Figure 1: Illustration of MCQA fragility. The example (left) shows an unfaithful reasoning chain that eliminates distractors incorrectly yet provide correct ﬁnal answer, yielding positive reward signal that, when used in reinforcement learning, further ampliﬁes shortcut behavior (top right). This shortcut behavior leads to widening gap between MCQA and OpenQA. The diagram motivate us to propose ReVeL, which aligns evaluation and training with reliable OpenQA. Quantifying the non-robustness of MCQA: We ﬁnd that evaluation via MCQA not only makes benchmark scores overestimating true capabilities, but also lacks robustness to trivial modiﬁcations of the options. Furthermore, RFT on MCQA improves multiple-choice accuracy at the cost of harming openended generalization. The ReVeL framework: We propose scalable framework to rewrite MCQA into OpenQA, using accurate rule-based judging whenever possible, with much less cost and variance than entirely shifting to an LLM judge. Demonstration of impact on training and evaluation: Performing RFT on 20K rewritten samples (Qwen2.5-VL-3B/7B) maintains MCQA accuracy while improving OpenQA accuracy by 6 percentage points. Rewriting four benchmarks also reveals up to 20 percentage points of score inﬂation when shifting from MCQA to OpenQA."
        },
        {
            "title": "2 Fragility of MCQA",
            "content": "Our work is directly motivated by series of experiments that quantitatively expose the weaknesses of the MCQA format. We describe our methodology and results here. 2.1 Adding options to open-ended benchmarks Setup. We start from two recent benchmarks that expect free-form answers from an LLM or VLM: SimpleQA (Wei et al., 2024) and VisualSimpleQA (Wang et al., 2025b). We convert each question into an MCQA variant (SimpleQA-Choice / VisualSimpleQA-Choice) by retaining the ground-truth answer and adding ﬁve plausible distractors via human-in-the-loop procedure with GPT-4.1. This conversion preserves the original 2 Figure 2: Performance comparison on original open-ended datasets (SimpleQA, Visual SimpleQA) and their multiple-choice versions (*-Choice, with 6 options). The Random Guess score is theoretical upper bound that combines the models actual open-ended accuracy with the probability of correctly guessing on the rest of the questions from six options. semantics, but the metrics may be aﬀected by random guessing. Therefore, besides accuracy, we also report random-guessing upper bound: AccU = AccOpen + (1 AccOpen) 1 , = 6 i.e., the model answers correctly on items it can already solve in open-ended form and guesses uniformly on the rest. Findings. Across both open-weight (e.g., Qwen2.5-72B, Llama-3.3-70B) and proprietary models (e.g., GPT-4.1, Gemini 2.5 Pro), converting to MCQA yields consistently large gains relative to the open-ended baseline and the random-guessing upper bound (Figure 2). This pattern holds for both text-only (SimpleQA) and multimodal (VisualSimpleQA) settings, indicating that when model correctly answers multiple-choice question, it is often utilizing the information embedded in the option set even when it does not actually have the required knowledge or reasoning skills. Implication. The presence of options supplies huge extra signal that can be exploited independent of task competence, directly leading to overestimation of model capabilities from MCQA accuracy. 2.2 Replacing GT with None-of-the-Above Another way to test the target knowledge or reasoning skill is to replace the ground-truth option with an option to abstain: None of the above (is correct) (NOTA), after shifting the remaining false options frontwards. We conduct such an experiment on MMLU-Pro (Wang et al., 2024a) and MMMU (Yue et al., 2023), the most popular MCQA benchmarks for LLM and VLM evaluation. When the correct option is replaced by NOTA, models frequently display logical inconsistency: the chain-ofthought reasoning process sometimes correctly eliminates the incorrect options yet still selects one of them as the ﬁnal answer. As shown in Figure 3, such contradictions occur even when the model explicitly reasons towards the correct concept (forest in that example) but ﬁnalizes with an inconsistent choice (C. home). Quantitatively, mismatch rates rise from 18% in standard MCQA to 50% under NOTA, listed in Appendix Also, we notice that models often reuse the original correct letter position even after the content was modiﬁed (after shifting) henceforth incorrect ( listed in Appendix implying potential test set contamination or shallow recall of positional cues. 3 Figure 3: Reasoning and answer can mismatch after replacing the ground-truth option with NOTA. Together, these eﬀects expose how fragile MCQA could be, motivating the shift to option-free OpenQA evaluation. 2.3 Omitting the options from an MCQ To examine the genuine reasoning ability without the aid of options, we can also remove the options for some multiple-choice questions, treating them as open-form questions. Note that after removing the options, some questions are still valid, but some would become ill-posed.1 Based on an LLM-assisted analysis (prompt attached in appendix), we ﬁnd that only about half of the questions in widely used MCQA benchmarks remain suitable using open-form evaluation: 48.9% for MMLU-Pro and 44.1% for MMMU, shown in Table 1. Table 1: Proportion of open-ended questions after ﬁltering. Dataset Total Open Ratio (%) MMLU-Pro (sampled) MMMU (validation) 1000 900 48.9 44.1 On the same questions that are still valid without options, models achieve consistently lower accuracy than the original MCQA format, as shown in Figure 4. 2.4 RFT on MCQA hurts open-ended QA Finally, we study training eﬀects by utilizing reinforcement ﬁne-tuning on MCQA data and evaluating on both MCQA and their open counterparts described in Section 2.3. We use the popular GRPO algorithm (Shao et al., 2024) in this work for RFT experiments. RFT on MCQA improves MCQA scores but degrades open-ended performance, thereby widening the MCQAOpenQA gap. For example, on MMMU, the gap grows for both 3B and 7B models; similar trends hold on EMMA (see Table 2). This indicates that the veriﬁable reward under MCQA may overﬁt to option-speciﬁc heuristics rather than transferable reasoning. Across settings, MCQA enables option exploitation that inﬂates accuracy, ampliﬁes shortcuts tied to options during training. These ﬁndings motivate our Rewrite-and-Verify approach in Section 3, which mitigate these shortcuts for both evaluation and training. 1For instance, How many apples are in the basket? is still valid question without any options, but Which of the following statements are true? is not. We illustrate four primary categories of questions that cannot apply option removal in the supplementary appendix. 4 Figure 4: On the impact of options on multiple-choice benchmarks: when options are removed, accuracy is uniformly lower, especially on VQA benchmarks like MMMU. Table 2: Impact of RFT on ViRL MCQA data. MCQ = multiple-choice benchmark score; Open = Open-ended benchmark score. denotes the inﬂation gap (MCQOpen). RFT on ViRL (5K MCQA samples) improves MCQ scores but enlarges , indicating reinforced shortcut behavior. Model MCQA OpenQA (Acc Drop) MMMU Qwen2.5-VL-3B + MCQA (ViRL) Qwen2.5-VL-7B + MCQA (ViRL) 46.6 50.9 51.6 56.4 11.8 11. 21.4 17.1 MMLU-Pro Qwen2.5-VL-3B + MCQA (ViRL) Qwen2.5-VL-7B + MCQA (ViRL) 39.5 47.4 53.4 53. 21.1 20.4 27.6 27.0 34.8 39.3 ( +4.5) 30.2 39.3 ( +9.1) 18.4 27.0 ( +8.6) 25.8 26.6 ( +0.8)"
        },
        {
            "title": "3 ReVeL: The Rewrite-and-Verify framework",
            "content": "We have shown that MCQA suﬀers from several shortcomings both in evaluation and in providing reliable training signals. Transforming MCQA to open-ended QA (OpenQA) has the potential to address these issues. In this work, we introduce ReVeL (Rewrite-and-Verify by LLMs), framework that rewrites MCQA into open ended yet veriﬁable formats while ensuring semantic ﬁdelity and minimizing information loss. 3.1 Pipeline overview As summarized in Figure 5, ReVeL operates in three phases: (1) Triage and Classiﬁcation, (2) Prompt-based Rewriting, and (3) Hybrid Evaluation and Veriﬁcation. The core principle is to maximize deterministic, rulebased evaluation for questions with unambiguous answers, while reserving LLM-based judging only for cases that genuinely require semantic understanding. During Triage, questions are ﬁrst passed through rule-based ﬁlter to leave out those expecting numeric answers, mostly quantities or ratios such as 50kg or 9.8 1023m/s2. These will be processed via pattern 5 Figure 5: Illustration of the rewrite-and-verify framework matching. Remaining non-numeric questions are routed to lightweight LLM-assisted classiﬁer that assigns each question to one of three answer veriﬁcation categories: Keywords matching: single or short tokens that have limited variations (e.g., names, dates). Open answers: short, factual or descriptive sentences that are unambiguous for typical human or LLM grader. Per-option veriﬁcation: questions heavily depend on the option set, such as Which of the following statements describes the process of .... Each category is paired with tailored rewriting prompt with the goal to preserve semantics while enabling deterministic veriﬁcation. Examples of all four categories and their rewritten counterparts are shown in Figure 5 Numeric. ReVeL reformulates them into explicit quantitative prompts by incorporating measurement units and specifying answer format (e.g., comma separated or valueunit pairs). Keywords. The rewriting step enumerates acceptable synonyms or lexical variants to permit ﬂexible but rule consistent matching. Open answers. These are rephrased into concise free form queries that solicit factual, non subjective responses without relying on the original options. Per-option veriﬁcation. Each option is converted into declarative statement, and models output comma separated list of True/False judgments, enabling structured veriﬁcation and preserving the discriminative intent of MCQA. 3.2 Benchmarks and rewriting coverage We evaluate ReVeL on four major multimodal benchmarks, including EMMA, MMMU, MME-RealWorld and MMLU-Pro. EMMA (Hao et al., 2025) targets multimodal reasoning in STEM, emphasizing visualtextual integration; we focus on the physics and chemistry subsets for domain-speciﬁc evaluation. MMMU (Yue et al., 2023) assesses college-level, multi-discipline reasoning across six domains with diverse image types; we use its 900-question validation set. MME-RealWorld (Zhang et al., 2024) oﬀers large-scale, high-quality, real-world tasks with greater diﬃculty; we adopt its Lite subset of 1,700 questions. MMLU-Pro (Wang et al., 2024a) is more challenging variant of MMLU, incorporating reasoning-oriented questions, ten-choice answers, and cleaner data. We sample 1,000 questions for evaluation. 3.3 Judge accuracy and eﬃciency To enhance evaluation consistency and eﬃciency, ReVeL reclassiﬁes the majority of tasks into deterministically veriﬁable categories: numeric, keyword, and per-option veriﬁcation. This design substantially reduces both computational cost and subjective variance by eliminating unnecessary LLM judgment on straightforward veriﬁable cases. 6 Table 3: Performance comparison of hybrid pipeline versus entirely using an LLM judge Dataset Judger EMMA MME-RW MMLU-Pro MMMU Overall LLM ReVeL LLM ReVeL LLM ReVeL LLM ReVeL LLM ReVeL Recall 100 100 PPV 100 100 FPR Acc. 100 0.0 100 0.0 93.5 95. 95.1 100 100 93.2 96.4 96.8 98.6 100 97.5 100 95.0 98. 97.2 99.6 1.4 0.0 3.2 0.0 5.4 1.3 2.0 0.3 95.9 98. 95.8 100 97.3 96.0 97.3 98.5 Table 4: Evaluation format distribution after rewriting. Num, Text, and Opt denote rule-based deterministic categories, while Open requires LLM judging. The large fraction of rule-based items demonstrates the eﬃciency of our hybrid evaluation design comparing to pure LLM-judge. Dataset LLM Rule-based Open(%) Num(%) Text(%) Opt(%) EMMA MMMU MME-RW MMLU-Pro 4.1 17.0 28.4 20.8 39.0 31.3 3.3 39. 6.6 33.5 55.7 19.6 50.3 18.2 12.6 19.9 To validate robustness, we compare ReVeLs hybrid evaluation against pure LLM-judge baseline across 600 randomly sampled responses from GPT-4.1-mini, Qwen2.5-VL-7B, and Qwen2.5-VL-72B on four benchmarks. As shown in Table 3, ReVeL achieves an overall accuracy of 98.5%, exceeding the LLM judges 97.3%, while simultaneously reducing false positive rate from 2.0% to 0.3%. These trends indicate that integrating rule-based veriﬁcation improves evaluative stability by enforcing stricter decision boundaries and conﬁrms the robustness of the hybrid veriﬁcation design. ReVeLs rewriting not only improves accuracy but also yields substantial eﬃciency gains. By turning many open-ended questions into structured formats, most items can now be graded automatically with simple rules. This reduces the need for costly and sometimes inconsistent LLM-based judging. As reported in Table 4, between 70% and 96% of questions across datasets can be evaluated through deterministic rules. For example, 95.9% of EMMA items become fully rule-checkable after rewriting, and even in MME-RealWorlds complex visual tasks, 71% are deterministically veriﬁable."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we apply our ReVeL framework to rewrite existing visual reasoning datasets for reinforcement learning. Firstly, we ﬁnd that training with our new data improves both accuracy in MCQA and open-end QA format. Then we use our data for evaluation and observe that there is large performance gap between MCQA and OpenQA across existing MLLMs. 4.1 Expertmental settings As discussed in Section 2.4, training with MCQA tends to reinforce option-exploiting behaviors and amplify format shortcuts, which can degrade model performance. Thus, we employ ReVeL to convert MCQA datasets into OpenQA form for training. 7 Table 5: Examples of our ReVeL Pipeline applied to diﬀerent question types. Each quadrant displays an original multiple-choice question and its OpenQA counterpart. Original: An ideal vapor-compression refrigeration cycle that uses refrigerant-134a as its working ﬂuid maintains condenser at 800 kPa and the evaporator at 212C. Determine this systems COP and the amount of power required to service 150 kW cooling load. Options: A. 4.07, 31.8 kW, B. 4.97, 33.8 kW, C. 4.87, 30.8 kW Rewritten Question: An ideal vapor-compression refrigeration cycle that uses refrigerant-134a as its working ﬂuid maintains condenser at 800 kPa and the evaporator at 212C. Determine this systems coeﬃcient of performance (COP) and the amount of power required to service 150 kW cooling load, in kilowatts. Provide your answer as two numbers separated by comma: COP, power (kW). Rewritten Answer: 4.87, 30.8 Original: Goya created this work while Options: A. in political exile in England B. serving as soldier on the front lines against France C. working as the court painter to the king of Spain D. studying Classical antiquity in Rome. Rewritten Question: Goya created this work while holding what professional position? Rewritten Answer: Working as the court painter to the king of Spain Original: This image shows the front view of the ego car. Predict the behavior of the ego vehicle. Options: (A) The ego vehicle is steering to the right. The ego vehicle is driving fast. (B)... (C)... (D)... (E)..."
        },
        {
            "title": "Open answer",
            "content": "Per-option veriﬁcation Rewritten Question: This image shows the front view of the ego car. Predict the behavior of the ego vehicle. Now, evaluate each of the following statements about the ego vehicles behavior. (A)... Provide your answer as single, comma separated list of True or False values corresponding to statements through E. Rewritten Answer: True, False, False, False, False Original: What is the manufacturer of the vehicle in the picture? Options: (A) Mercedes Benz (B) FORD (C) BMW (D) HYUNDAI (E) This image doesnt feature the content. Rewritten Question: What is the manufacturer of the vehicle in the picture? Rewritten Answer: BMW OR Bayerische Motoren Werke OR BMW AG"
        },
        {
            "title": "Keywords",
            "content": "8 Table 6: Performance Comparison of MCQA vs. OpenQA Training on In-Domain and Out-of-domain Benchmarks Model / Train In-domain Out-of-domain Overall Scores EMMA MMMU MME-RW MMLU-Pro MCQ Open Total MCQ Open MCQ Open MCQ Open MCQ Open R1-Onevision-7B Mixed-R1-7B VL-Rethinker-7B Qwen2.5-VL-3B + MCQA (ViRL) + OpenQA (ViRL) + OpenQA (ReVeL) + OpenQA (ViRL) Qwen2.5-VL-7B + MCQA (ViRL) + OpenQA (ViRL) + OpenQA (ReVeL) + OpenQA (ViRL) 28.9 29.8 30.6 27.4 28.2 31.0 29.8 31.4 28.9 30.2 31.7 29.2 29.8 4.7 13.2 14. 5.7 3.1 4.4 18.6 17.3 10.2 9.1 10.4 17.1 16.9 42.2 56.3 53.9 44.3 50.2 50.2 49.4 49.4 51.9 58.3 58.2 56.4 54.3 23.9 30.6 33. 23.3 22.0 23.8 27.4 26.5 31.9 25.3 33.4 37.0 36.8 44.6 45.6 44.3 35.9 39.7 42.1 41.2 41.4 44.8 50.1 47.6 50.6 50.3 31.6 32.8 32. 26.6 25.6 28.6 31.9 31.7 32.8 32.0 36.3 38.8 38.4 42.5 51.4 52.4 38.7 44.0 43.9 42.2 41.3 49.1 52.8 53.7 51.1 51.5 32.3 37.7 37. 29.6 28.0 30.3 34.1 33.4 39.0 32.4 37.7 43.0 39.9 39.5 45.8 45.3 36.6 40.5 41.8 40.7 40.9 43.7 47.8 47.8 46.8 46.5 23.1 28.6 29. 21.3 19.7 21.8 28.0 27.2 28.5 24.7 29.5 34.0 33.0 31.3 37.2 37.5 28.9 30.1 31.8 34.3 34.1 36.1 36.3 38.6 40.4 39.8 We train Qwen2.5-VL-3B and Qwen2.5-VL-7B with GRPO. To conduct controlled comparison of the impact of diﬀerent training data, we designed 4 training conﬁgurations based on the ViRL dataset, as shown in Table 1. Original MCQA Only (+MCQA (ViRL)): The baseline model is trained exclusively on the original ViRL MCQA data. Rewards are derived from rule-based exact match. 2. Original MCQA & Original OpenQA (+OpenQA (ViRL)): This conﬁguration auguments (1) by further adding the original OpenQA questions from the ViRL dataset. 3. Rewritten OpenQA Only (+OpenQA (ReVeL)): The baseline model is trained exclusively on the OpenQA data rewritten by our ReVeL pipeline. 4. Rewritten OpenQA & Original OpenQA (+OpenQA (ViRL)): This conﬁguration augments (3) by further adding the original OpenQA questions from the ViRL dataset. This setup enables controlled comparison between reinforcement driven by MCQA versus OpenQA by our ReVeL. Our evaluation is based on the four benchmarks mentioned above. 4.2 Training details We implement all experiments on the VeRL framework with near on-policy RL setup and train for up to 10 epochs. We do not use KL regularization. For ViRL-Open/MCQA-5K, we use training batch size of 256, PPO mini-batch size 128, and rollout size 8. For Mixed-R1-Open/MCQA-15K, we use training batch size of 512, PPO mini-batch size 256, and rollout size 8. Inference and serving for all models are done with vLLM. These settings are ﬁxed across regimes to isolate the eﬀect of the reward design. 4.3 Performance on rewritten training data As shown in Table 6, training on OpenQA consistently produces hight overall accuracy than MCQA across both model sizes: Qwen2.5-VL-3B achieves 34.3 overall with OpenQA vs 30.1 with MCQA (+4.2), and Qwen2.5-VL-7B achieves 40.4 vs 36.3 (+4.1). Importantly, open ended accuracy improves on every benchmarks while MCQA scores remain competitive. Models trained with ReVeL data achieves 40.4 overall score, compared to 31.3 for R1-OneVision-7B, 37.2 for Mixed-R1-7B, 37.5 for VL-Rethinker-7B. These results indicate that veriﬁable OpenQA align better with transferable reasoning and real-world usage, improving both open-ended performance and the combined overall metric. 9 4.4 Performance gap in MCQA and OpenQA To further quantify the discrepancy in model capabilities between MCQA and OpenQA, we conduct comparative analysis of model performance in MCQA and OpenQA setting with two rewritten datasets (ViRL and Mixed-R1). The comprehensive results of this evaluation are presented in Table 7. The result reveal consistent and substantial performance degradation across all evaluated models when transitioning from the MCQA to the OpenQA format, even strong MLLMs such as GPT-5 and Gemini-2.5 ﬂash are not immune to this eﬀect. For instance, GPT-5s accuracy on the MMMU benchmark drops by 19.8 points (from 79.2% to 59.5%), and Gemini-2.5 ﬂashs accuracy on EMMA decreases by 15.7 points. This indicates that the challenge of OpenQA is fundamental problem that aﬀects even the most advanced models. And we observe that the performance gap is often more pronounced for open-weight models. For example, R1-OneVision-7B exhibits staggering 24.2-point drop on EMMA, while InternVL3-8Bs performance on MMMU plummets by 27.9 points. This suggests that many open-weight MLLMs may particularly overﬁt the MCQA format, which is prevalent in many VQA datasets. Table 7: Overall accuracy (%). Accuracy drop between MCQA and OpenQA is marked after . Bold numbers indicate the smallest drop across open-sourced models EMMA MMMU MME-RealWorld MMLU-Pro MCQA OpenQA MCQA OpenQA MCQA OpenQA MCQA OpenQA Proprietary Models 36.0 (6.0) 35.0 (7.8) 27.3 (9.1) 22.3 (17.9) 33.6 (15.7) 79.2 75.2 71.7 65.3 69.6 59.5 (19.8) 55.5 (19.7) 56.1 (15.5) 51.6 (13.7) 57.7 (11.9) Open-Source Models 20.8 (13.8) 14.5 (17.6) 23.0 (19.1) 4.7 (24.2) 13.2 (16.7) 14.9 (15.8) 20.6 (15.3) 5.7 (21.7) 18.6 (11.3) 17.2 (14.1) 10.2 (18.7) 17.1 (12.1) 15.1 (14.4) 67.7 60.0 68.5 42.2 56.3 53.9 68.2 44.3 49.4 46.3 51.9 56.4 56.1 51.5 (16.2) 32.1 (27.9) 46.5 (22.0) 23.9 (18.3) 30.6 (25.8) 33.4 (20.5) 47.9 (20.3) 23.3 (21.0) 27.4 (22.0) 29.8 (16.5) 31.9 (20.0) 37.0 (19.5) 34.1 (22.0) 42.0 42.8 36.4 40.2 49.2 34.6 32.2 42.1 28.9 29.8 30.6 35.9 27.4 29.8 31.4 28.9 29.2 29. 57.8 58.3 52.7 54.8 57.3 48.9 49.6 51.7 44.6 45.6 44.3 48.4 35.9 41.2 38.0 44.8 50.6 51.9 42.4 (15.4) 43.7 (14.6) 39.6 (13.1) 44.0 (10.9) 46.5 (10.8) 31.4 (17.5) 33.2 (16.4) 41.5 (10.2) 31.6 (13.0) 32.8 (12.8) 32.7 (11.6) 37.4 (11.0) 26.6 (9.2) 31.9 (9.3) 36.3 (1.7) 32.8 (12.0) 38.8 (11.7) 39.6 (12.3) 84.6 78.7 81.2 75.4 78. 70.9 55.3 74.6 42.5 51.4 52.4 70.8 38.7 42.2 43.3 49.1 51.1 53.8 67.6 (17.0) 63.8 (14.9) 67.1 (14.1) 64.4 (11.0) 63.8 (14.5) 57.0 (13.9) 39.0(16.3) 60.7 (13.9) 32.3 (10.2) 37.7 (13.7) 37.6 (14.8) 57.6 (13.2) 29.6 (9.1) 34.1 (8.1) 32.8 (10.5) 39.0 (10.1) 43.0 (8.1) 40.9 (12.9) Model GPT-5 GPT-5 mini GPT-4.1 GPT-4.1 mini Gemini-2.5 ﬂash InternVL3-78B InternVL3-8B Qwen3-VL-8B-Instruct R1-OneVision-7B Mixed-R1-7B VL-Rethinker-7B Qwen2.5-VL-72B Qwen2.5-VL-3B +OpenQA(ViRL) +OpenQA(Mixed-R1) Qwen2.5-VL-7B +OpenQA(ViRL) +OpenQA(Mixed-R1)"
        },
        {
            "title": "5 Related work",
            "content": "Multiple-choice question answering (MCQA) has been popularly used assessment tool for ages due to simpliﬁed grading (Simkin and Kuechler, 2005; Dufresne et al., 2002; Paxton, 2000; Balepur et al., 2025; Alzahrani et al., 2024; Pezeshkpour and Hruschka, 2023). This convenience led to its wide adoption for evaluation of large language models (Hendrycks et al., 2020; Wang et al., 2024a), and in particular vision-language models (Yue et al., 2023; Clark et al., 2018; Yue et al., 2024; Liu et al., 2024) because of more diverse wording choices in describing many visual concepts or scenes. However, MCQA has many shortcuts. Performance can drop dramatically simply from changing an options placement(Zheng et al., 2023; Molfese et al., 2025). While mitigation strategiessuch as better distractors, more options, randomized order, or select all that apply formats (Zhang et al., 2025; Yu et al., 2024; Zheng et al., 2023; Zhou et al., 2024; Xu et al., 2025b) reduced some biases. And models typically cannot reject all options when the correct answer is absent (Goral et al., 2024; Tam et al., 2025). Some recent work has shown that reasoning models are good at exploiting the information 10 in the options, implying the performance may be inﬂated (Balepur et al., 2024; Raman et al., 2025). Recognizing these issues, the communitys shift to open-ended evaluation faces its own challenges. Rule-based, short-answer benchmarks (xAI, 2024; Wang et al., 2024b) are limited in scope, while general open-ended formats rely on an LLM-as-a-judge. Furthermore, simply remove options must discard signiﬁcant portion of unsuitable items and still depend on an LLM-Judge for evaluation (Myrzakhan et al., 2024). These works analyse the ﬂaws of MCQA but do not try to propose method to mitigate these shortcuts. These analyses focus on identifying the ﬂaws of MCQA rather than proposing systematic mitigation strategies. Multimodal reinforcement learning: Many visual reasoning datasets are predominantly designed in an MCQA format. For instance, earlier datasets such as ScienceQA (Lu et al., 2022), AI2D (Kembhavi et al., 2016), Geometry3K (Lu et al., 2021), and GeoQA-Plus (Chen et al., 2021) are entirely formed by multiple-choice questions. This trend continues in recent MLLMs designed for general-purpose reasoning, such as Mixed-R1 (Xu et al., 2025a), R1-OneVision (Yang et al., 2025), and VL-Rethinker(Wang et al., 2025a), which all employ considerable proportion of choice-based items, accounting for 43%, 80%, and 45% of their data, respectively. Our work is built on these visual reasoning datasets and explores open-form rewriting from those MCQA samples."
        },
        {
            "title": "6 Limitations",
            "content": "We acknowledge several limitations in our proposed pipeline. First, the rewriting and classiﬁcation phases, while highly accurate, are not perfect and may occasionally introduce errors. Hopefully such errors could diminish when the LLM components are getting stronger and stronger in the future. Second, our work focuses on converting the format of evaluation to be more robust and eﬃcient, without addressing the inherent fallibility of the LLM-judge itself. Issues such as positional bias, verbosity bias, or factual inaccuracies within the LLM-judge (Chen et al., 2024) are orthogonal to our contribution. We deliberately sidestep some of these known issues; for instance, questions in the EMMA dataset requiring the validation of SMILES chemical structures were intentionally converted to Per-Option Veriﬁcation format. This leverages rule-based checking and avoids relying on an LLM-judge for domain-speciﬁc task, thereby mitigating potential failure point of LLM-based evaluation. There are several directions for future research. One key avenue is to extend our framework beyond QA to other NLP tasks, such as long-form generation, where evaluation remains major challenge. Finally, developing adaptive evaluation systems that can dynamically choose the most appropriate and cost eﬀective judging mechanism based on the questions complexity and the models response would be valuable next step."
        },
        {
            "title": "7 Conclusions",
            "content": "In this work, we systematically demonstrated the fragility of MCQA format for both evaluation and reinforcement ﬁne-tuning. We found that MCQA metrics signiﬁcantly overestimate model capabilities, and RFT on MCQA data reinforces format-speciﬁc shortcuts, harming open-ended generalization. To solve this, we propose ReVeL, framework that rewrite MCQA into veriﬁable OpenQA by categorizing questions for hybird evaluation scheme. Applying ReVeL to RFT, we found that models trained on our rewritten OpenQA data achieved approximately 6-point improvement in open-ended accuracy while maintaining performance on original MCQA benchmarks, conﬁrming its role in fostering more robust and transferable reasoning. References Norah Alzahrani, Hisham Alyahya, Yazeed Alnumay, Sultan AlRashed, Shaykhah Alsubaie, Yousef Almushayqih, Faisal Mirza, Nouf Alotaibi, Nora Al-Twairesh, Areeb Alowisheq, Saiful Bari, and Haidar Khan. When benchmarks are targets: Revealing the sensitivity of large language model leaderboards. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1378713805, Bangkok, Thailand, 2024. Association for Computational Linguistics. Anthropic. Claude 4. https://www.anthropic.com/news/claude-4, 2025. Accessed: 2025-07-11. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Nishant Balepur, Abhilasha Ravichander, and Rachel Rudinger. Artifacts or abduction: How do llms answer multiplechoice questions without the question? In Annual Meeting of the Association for Computational Linguistics, 2024. 11 Nishant Balepur, Rachel Rudinger, and Jordan L. Boyd-Graber. Which of these best describes multiple choice evaluation In Annual Meeting of the Association for Computational with llms? a) forced b) ﬂawed c) ﬁxable d) all of the above. Linguistics, 2025. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In International Conference on Machine Learning, 2024. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. ArXiv, abs/2105.14517, 2021. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. Robert Dufresne, William Leonard, and William Gerace. Making sense of students answers to multiple-choice questions. The Physics Teacher, 40:174180, 2002. Google. Gemini2.5 pro. https://deepmind.google/models/gemini/, 2025. Accessed: 2025-07-11. Gracjan Goral, Emilia Wiśnios, Piotr Sankowski, and Pawel Budzianowski. Wait, thats not an option: Llms robustness with incorrect multiple-choice options. 2024. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. ArXiv, abs/2501.05444, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. ArXiv, abs/1603.07396, 2016. Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? ArXiv, abs/2307.06281, 2023. Ziqiang Liu, Feiteng Fang, Xi Feng, Xinrun Du, Chenhao Zhang, Zekun Moore Wang, Yuelin Bai, Qixuan Zhao, Liyang Fan, Chengguang Gan, Hongquan Lin, Jiaming Li, Yuansheng Ni, Haihong Wu, Yaswanth Narsupalli, Zhigang Zheng, Chengming Li, Xiping Hu, Ruifeng Xu, Xiaojun Chen, Min Yang, Jiaheng Liu, Ruibo Liu, Wenhao Huang, Ge Zhang, and Shiwen Ni. Ii-bench: An image implication understanding benchmark for multimodal large language models. ArXiv, abs/2406.05862, 2024. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In Annual Meeting of the Association for Computational Linguistics, 2021. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv, abs/2209.09513, 2022. Chenyang Lyu, Minghao Wu, and Alham Fikri Aji. Beyond probabilities: Unveiling the misalignment in evaluating large language models. ArXiv, abs/2402.13887, 2024. Francesco Maria Molfese, Luca Moroni, Luca Gioﬀre, Alessandro Sciré, Simone Conia, and Roberto Navigli. Right answer, wrong score: Uncovering the inconsistencies of llm evaluation in multiple-choice question answering. ArXiv, abs/2503.14996, 2025. Steven Moore, Huy Anh Nguyen, Tianying Chen, and John C. Stamper. Assessing the quality of multiple-choice questions using gpt-4 and rule-based methods. In European Conference on Technology Enhanced Learning, 2023. Aidar Myrzakhan, S. Mahmoud Bsharat, and Zhiqiang Shen. Open-llm-leaderboard: From multi-choice to open-style questions for llms evaluation, benchmark, and arena. ArXiv, abs/2406.07545, 2024. 12 OpenAI. Gpt-4v(ision) system card. OpenAI Research, 2023. OpenAI. Introducing GPT-5, 2025. Moragh Paxton. linguistic perspective on multiple choice questioning. Assessment & Evaluation in Higher Education - ASSESS EVAL HIGH EDUC, 25:109119, 2000. Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Yu Qiao, and Jiangmiao Pang. Egothinker: Unveiling egocentric reasoning with spatio-temporal cot. 2025. Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. In NAACL-HLT, 2023. Narun K. Raman, Taylor Lundy, and Kevin Leyton-Brown. Reasoning models are test exploiters: Rethinking multiplechoice. ArXiv, abs/2507.15337, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. Mark G. Simkin and William L. Kuechler. Multiple-choice tests and student understanding: What is the connection? Decision Sciences Journal of Innovative Education, 3:7398, 2005. Zhi Rui Tam, Cheng-Kuang Wu, Chieh-Yen Lin, and Yun-Nung Chen. None of the above, less of the right: Parallel patterns between humans and llms on multi-choice questions answering. ArXiv, abs/2503.01550, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing selfreﬂection of vision-language models with reinforcement learning. ArXiv, abs/2504.08837, 2025a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max W.F. Ku, Kai Wang, Alex Zhuang, Rongqi \"Richard\" Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. ArXiv, abs/2406.01574, 2024a. Yanling Wang, Yihan Zhao, Xiaodong Chen, Shasha Guo, Lixin Liu, Haoyang Li, Yong Xiao, Jing Zhang, Qi Li, and Ke Xu. Visualsimpleqa: benchmark for decoupled evaluation of large vision-language models in fact-seeking question answering. ArXiv, abs/2503.06492, 2025b. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. ArXiv, abs/2406.18521, 2024b. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. ArXiv, abs/2411.04368, 2024. xAI. Introducing grok-1.5v and realworldqa benchmark, 2024. Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, and Hao Fei. Mixed-r1: Uniﬁed reward perspective for reasoning capability in multimodal large language models. ArXiv, abs/2505.24164, 2025a. Weijie Xu, Shixian Cui, Xi Fang, Chi Xue, Stephanie Eckman, and Chandan K. Reddy. Sata-bench: Select all that apply benchmark for multiple choice questions. ArXiv, abs/2506.00643, 2025b. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. ArXiv, abs/2503.10615, 2025. Han Cheng Yu, Yu An Shih, Kin Man Law, KaiYu Hsieh, Yu Chen Cheng, Hsin Chih Ho, Zih An Lin, Wen-Chuan Hsu, and Yao-Chung Fan. Enhancing distractor generation for multiple-choice questions with retrieval augmented pretraining and knowledge graph integration. In Findings of the Association for Computational Linguistics: ACL 2024, pages 11019 11029, Bangkok, Thailand, 2024. Association for Computational Linguistics. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9556 9567, 2023. 13 Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. ArXiv, abs/2409.02813, 2024. Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, Ludwig Schmidt, and Serena Yeung-Levy. Automated generation of challenging multiple-choice questions for vision language model evaluation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 2958029590, 2025. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Jun Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tien-Ping Tan. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are diﬃcult for humans? ArXiv, abs/2408.13257, 2024. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. ArXiv, abs/2309.03882, 2023. Wenjie Zhou, Qiang Wang, Mingzhou Xu, Ming Chen, and Xiangyu Duan. Revisiting the self-consistency challenges in multi-choice question formats for large language model evaluation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1410314110, Torino, Italia, 2024. ELRA and ICCL."
        },
        {
            "title": "A Details of Removing Options From MCQA",
            "content": "A.1 Filtering Pipeline for Self-Suﬃcient Questions Figure 6: Common characteristics that make multiple-choice question unsuitable for direct conversion to free-form format. To create dataset for our option-free evaluation, we developed two-stage pipeline to systematically ﬁlter existing benchmarks and retain only self-suﬃcient questions suitable for generative format. Stage 1: Question Validity Filtering. We ﬁrst exclude questions that are fundamentally unsuitable for freeform conversion. This stage combines heuristic rules (e.g., removing questions with long, paragraph-style answers likely to be subjective) with prompted LLM that identiﬁes and removes questions exhibiting option dependency, subjectivity, or underspeciﬁcation. Stage 2: Answer Uniqueness Veriﬁcation. Questions that pass the ﬁrst stage are then checked for answer uniqueness. We use an LLM to determine if question, in the absence of options, could yield multiple substantively diﬀerent but equally valid answers. Only questions with single, unambiguous correct answer are retained. The ﬁltering prompts used to guide the LLM in each stage are detailed in Appendix. We validated this pipeline by annotating random sample of its outputs. The results, detailed in Table 8, conﬁrm the high ﬁdelity of our method. The low overall False Positive (FP) rate of 2.5% and False Negative (FN) rate of 3.0% ensure the soundness of our subsequent experiments. Table 8: Statistical of the ﬁltering pipeline. The table detailsthe False Positive (FP) and False Negative (FN) rates across sampled subset (right). FP and FN rates are computed from sampled subset. Dataset FP (%) FN (%) MMLU-Pro MMMU 4.0 2. 4.0 2.0 Figure 6 shows 4 primary patterns: Option-Dependent Questions: that explicitly refers to the choices (e.g., \"Which of the following...\"). Sentence Completion Questions: that is grammatically incomplete without options (e.g., \"The letter in the diagram represents...\"). Subjective Questions: that solicits an opinion rather than an objective, factual answer. Questions with Multiple Answers: where the MCQ format artiﬁcially presents only one correct choice."
        },
        {
            "title": "B Failure modes of MCQA",
            "content": "We conduct an analysis of the underlying causes for the unreliability of MCQA, identifying critical failure modes like reasoning-choice mismatch, positional memorization and option-anchoring. 1 B.1 Analysis of Failure Modes: Reasoning-Choice Mismatch To pinpoint the cause, we focused speciﬁcally on the subset of questions that model answered correctly in the Standard MCQA setting but incorrectly in the NOTA setting. Within this set of failures, we discovered frequent reasoning-choice mismatch. As shown in Table 9, models often correctly inference the answer in their reasoning steps. However, upon ﬁnding this answer absent from the choices, they fail to select the logical NOTA option. Instead, they revert to strategies like string matching to select an incorrect distractor. The rate of this mismatch is dramatically higher in the NOTA setting compared to the standard MCQA setting, exposing critical ﬂaw in how models interact with provided options when the correct answer is missing. Model Dataset MCQA NOTA InternVL3-78B MMMU 17.6% 54.1% InternVL3-8B MMMU 18.6% 58.8% Qwen2.5-VL-72B MMMU 12.0% 46.9% Qwen2.5-VL-7B MMMU 21.9% 63.8% Table 9: Reasoning-Choice Mismatch Rates by Model and Dataset B.2 Analysis of Failure Modes: Positional Memorization Another signiﬁcant failure mode identiﬁed in the NOTA setting is positional memorization. We observed that models often select the same option letter in the NOTA task that corresponded to the correct answer in the original MCQA task, even though the content of that option is now an incorrect distractor. This behavior, quantiﬁed in Table 10 and Table 11, indicates that models develop shallow heuristic of memorizing answer positions instead of semantically evaluating the options provided. This reliance on positional cues rather than content undermines the validity of the evaluation. Table 10: Positional Memorization Statistics by Model Model MMMU InternVL3-78B InternVL3-8B Qwen2.5-VL-72B Qwen2.5-VL-7B gemma-3-27b-it gpt-4.1 gpt-4.1-mini gpt-5-mini 30.6% 36.4% 42.2% 40.0% 35.8% 42.5% 33.8% 33.9% Table 11: MMLU-Pro Positional Memorization Statistics Model MMLU-Pro Meta-Llama-3.3-70B Qwen2.5-72B Qwen2.5-7B 28.5% 36.2% 35.6% B.3 Example of Option-Anchoring Option-Anchoring describes how models reasoning process becomes tethered to the provided choices. Rather than deriving an answer objectively from the source context (e.g., an image or text), the model ﬁrst identiﬁes plausible option and then constructs post-hoc justiﬁcation for it. As illustrated in the case study 2 in Figure B.3, the models explanation is not genuine deduction but rationalization \"anchored\" to preselected choice. This behavior suggests that the model is not performing true reasoning but rather form of constrained validation. Figure 7: Illustration of the Option-Anchoring Phenomenon. Left (Standard MCQA): when \"Egg tempera\" is an available option, the AI model analyzes the paintings featuressuch as its matte ﬁnish and delicate colorsand concludes they are characteristic of egg tempera. Right (NOTA setting): the same model is presented with the same painting, but the \"Egg tempera\" option is removed and replaced with \"None of the above options are correct\". The models reasoning now shifts, describing the paintings texture and detail as characteristic of oil paint. B.4 Deep in Fragility of MCQA: Category Level Eﬀects We conduct category-level analysis of MCQA-NOTA and OpenQA on the ﬁltered items. The eﬀect is not uniform: subjects such as Optical Character Recognition (OCR), Object Localization, and Abstract Algebra consistently exhibit the largest degradations under MCQANOTA. plausible driver is the semantic sparsity of option sets in these domains. For instance, many Object Localization questions present purely numeric options (e.g., A: 3, B: 4, C: 5, D: 6) with minimal contextual content. When models internal reasoning yields an answer not present among the options (e.g., 7), there are few semantic cues to eliminate the remaining distractors; once the correct option is replaced by NOTA, the model is especially prone to confuse itself and select distractor rather than NOTA. In contrast, subjects whose options carry richer semantics (full phrases/sentences) provide more opportunities for elimination-by-meaning and show smaller NOTA-induced drops."
        },
        {
            "title": "C Badcase of HyReV",
            "content": "While our hybrid pipeline achieves low overall error rate ( 2%), closer analysis of the misjudged cases reveals an inherent challenge in rule-based evaluation: the ambiguity of symbolic representation. The primary source of errors is not the pipeline, but rather the vast, often inexhaustible, variations in how concept can be expressed. For example, an answer representing numerical range, such as \"1.30 40.45\", presents signiﬁcant challenge for any keyword-based or rule-based system. The tilde symbol ( ) can be represented in numerous ways in models free-form response, including textually (\"1.30 to 40.45\", \"between 1.30 and 40.45\"), with diﬀerent symbols (\"1.30 - 40.45\"), or in speciﬁc formats like LaTeX (1.30 40.45). It is computationally infeasible for rule-based system to enumerate every possible permutation of such representations. Our pipeline is designed to handle common cases, but these edge cases with high representational variance account for the small residual error rate. This is not ﬂaw in the pipelines logic but rather fundamental limitation of deterministic matching when faced with the creative and diverse outputs of modern language models. 3 Model Behavior after MCQ-to-TF Reformulation We analyze model behavior when multiple-choice questions (MCQs) are reformulated into true-false (TF) statements, transformation that helps mitigate option elimination in MCQA. The Emma dataset focuses on professional physics and chemistry, whereas MMMU-Pro emphasizes high-school and college-level linguistic understanding. Our analysis thus focuses on MMMU-Pro, where semantic reasoning is more central to the observed label imbalance. As shown in Table 12, models systematically over-assign true labels compared to the ground-truth annotations after reformulation. We deﬁne the over-true ratio as: (Number of answers with >1 correct option) / (Total incorrect answers) In the MCQ format, models tend to perform comparative reasoning and elimination. When reformulated as independent TF statements, this structure disappears. Without these inter-option cues, models evaluate each statement in isolation and display stronger bias toward aﬃrmative (true) judgments. Table 12: Comparision of model behavior before and after TF reformulation on MMMU-Pro dataset Model Over-True Ratio Qwen2.5-VL-72B Qwen2.5-VL-7B Gemini-2.5 ﬂash GPT-4.1 GPT-5 InternVL3-78B Meta-Llama-3.3-70B 86% 63% 84% 80% 72% 84% 79%"
        },
        {
            "title": "E More examples of option exploiting in SimpleQA",
            "content": "As discussed in Sec 2, adding options to open-ended benchmarks can signiﬁcantly inﬂate performance metrics. This occurs because the options provide extra signals that models can exploit, allowing them to answer correctly without possessing the underlying knowledge required by the question stem. Figure 8 and Figure 9 illustrates several qualitative examples from the SimpleQA-Choice dataset. These examples show cases where model provides an incorrect or \"I dont know\" answer to the original openended question but successfully selects the correct option when presented with the multiple-choice version. This highlights the \"shortcut\" behavior and answer-guessing encouraged by the MCQA format, reinforcing the ﬁndings presented in Figure 2."
        },
        {
            "title": "F Prompts Details",
            "content": "This section provides the speciﬁc prompts used within the ReVeL framework and ﬁltering pipeline in Figure2.3. Figure 10 and Figure 11 detail two-stage ﬁltering pipeline for removing invalid questions without options. Figure 13 details the prompt used for the initial classiﬁcation (Triage) step. This prompt instructs the model to analyze the original multiple-choice question (MCQ), its options, and the ground-truth answer, then categorize the question into one of the four types suitable for veriﬁable rewriting: Numeric, Keywords, Open answer, or Per-option veriﬁcation. Figures 15, 14, 16, 18, 17 present the speciﬁc rewriting prompts used for each category, respectively. These prompts guide the LLM to transform the original MCQA item into semantically equivalent, open-ended question designed for speciﬁc veriﬁcation method (e.g., pattern matching for Numeric, exact-match for Keywords, or LLM-judging for Open answer). 4 Prompt: What month, day, and year was House Resolution 7004, Addressing the Governors unacceptable actions in matters related to the appraiser certiﬁcation program, introduced against Governor Kristi Noem? ====== (Without Options: Hallucinated search and give an incorrect answer) ====== Response: To determine the speciﬁc date when House Resolution 7004, which addresses the Governors actions related to the appraiser certiﬁcation program, was introduced against Governor Kristi Noem, we need to follow these steps: 1. Identify the Resolution: The resolution in question is House Resolution 7004. 2. Context and Subject: It is directed at Governor Kristi Noem of South Dakota and concerns her actions related to the appraiser certiﬁcation program. Search Legislative Databases: Use the oﬃcial South Dakota State Legislature website or other reliable sources to ﬁnd the exact date of introduction. After searching the South Dakota State Legislatures oﬃcial records, found that House Resolution 7004 was introduced on February 18, 2021. boxed{February 18, 2021} ====== (With Options: Hallucinated search and give correct answer) ====== Options: A. September 23, 2022, B. February 28, 2022, C. February 24, 2022, D. February 25, 2022, E. February 21, 2022, F. February 24, 2021 To determine the correct date on which House Resolution 7004 was introduced, we need to look at the available information and cross-reference it with reliable sources. 1. Identify the Context: The resolution is speciﬁcally about addressing actions taken by Governor Kristi Noem regarding the appraiser certiﬁcation program in South Dakota. 2. Review the Options: - A. September 23, 2022 - B. February 28, 2022 - C. February 24, 2022 - D. February 25, 2022 - E. February 21, 2022 - F. February 24, 20213. 3. Research the Event: - quick search for news articles or oﬃcial legislative records from the South Dakota state government can provide the exact date. - According to multiple news sources and the oﬃcial South Dakota Legislature website, House Resolution 7004 was introduced on February 24, 2022. 4. Verify the Date: - Cross-referencing with the South Dakota Legislatures oﬃcial records conﬁrms that the resolution was indeed introduced on February 24, 2022. Given this information, the correct answer is: boxed{C} Figure 8: Qwen2.5-72B-Instruct answering question from SimpleQA: After giving options, the model can choose the right answer. 5 Prompt: In what year did Dr. William Schwartz discover that sulfanilamide also acts as diuretic in people with congestive heart failure? ====== (Without Options: an incorrect answer) ====== Response: Here is step-by-step reasoning to determine the year of the discovery: 1. Identify the context: The discovery in question involves the antibiotic sulfanilamide and its unexpected side eﬀect as diuretic. The key ﬁgure is Dr. William B. Schwartz. 2. The Clinical Observation: Dr. Schwartz, at the time medical resident at Beth Israel Hospital in Boston, was treating patients with bacterial infections using large doses of the newly introduced antibiotic, sulfanilamide. 3. The Key Finding: He observed that patients receiving this treatment experienced signiﬁcant increase in urine output (a diuretic eﬀect) and also developed metabolic acidosis (a change in the bodys acid-base balance). 4. Connecting the Dots: Dr. Schwartz recognized that this was not just random side eﬀect but physiological action of the drug. He correctly hypothesized that the drug was inhibiting an enzyme in the kidneys, which was later identiﬁed as carbonic anhydrase. 5. Pinpointing the Year: This pivotal clinical observation, which laid the groundwork for the development of modern diuretics (like thiazides and acetazolamide, which are chemically related to sulfonamides), was made in 1937. Based on this historical account, Dr. William Schwartz discovered the diuretic properties of sulfanilamide in 1937. boxed{1937} ====== (With Options: give correct answer) ====== Here is step-by-step reasoning to determine the correct year: ...... 3. Distinguish between diﬀerent discoveries: Its important to diﬀerentiate between the initial observation of side eﬀect and the speciﬁc application of that eﬀect as therapy. In 1937, Dr. Hamilton Southworth reported that patients being treated with sulfanilamide for infections developed metabolic acidosis. This was key observation about the drugs eﬀect on the bodys acid-base balance and kidney function, but it was not the discovery of its use as diuretic for heart failure. This makes option (1937) incorrect for the speciﬁc discovery attributed to Dr. Schwartz. 4. Focus on Dr. Schwartzs contribution: Dr. William Schwartz, then medical student, was the one who hypothesized and then demonstrated that this eﬀect on the kidneys could be harnessed to increase salt and water excretion (diuresis) in patients with ﬂuid overload from congestive heart failure. 5. Pinpoint the year of Schwartzs discovery: Dr. Schwartz conducted his research and published his seminal paper on this topic, titled The Eﬀect of Sulfanilamide on Salt and Water Excretion in Congestive Heart Failure, in the *New England Journal of Medicine*. The publication date of this landmark paper, which formally announced the discovery to the medical community, was 1949. 6. Evaluate the given options: * A. 1937: Incorrect. This year corresponds to Southworths observation of metabolic acidosis. * B. 1955: Incorrect. * C. 1929: Incorrect. * D. 1962: Incorrect. * E. 1942: Incorrect. * F. 1950: Incorrect. While follow-up work and the development of related drugs (like acetazolamide) were happening around this time, Schwartzs initial key publication was in 1949. * G. 1949: Correct. This is the year Dr. Schwartz published his ﬁndings. Therefore, the correct year is 1949. boxed{G} Figure 9: Gemini 2.5 Pro answering question from SimpleQA: After giving options, the model can choose the right answer. 6 Question Filtering Prompt You are an expert in educational assessment. Your task is to determine whether the following multiplechoice question (without options) can be answered as an open-ended question without needing to see the answer choices. Answer No if the question: - Requires subjective judgment, personal preference. (e.g., \"best,\" \"most likely\") - Is negative question (e.g., \"...is NOT...\", \"...EXCEPT...\") - Could have multiple distinct correct answers - The question is not proper question (e.g., is statement or fragment) - The question lacks suﬃcient context or speciﬁcity, so that the set of acceptable correct answers is unclear or overly broad. - Is deﬁnitional question (e.g., ___ is a:) where the question does not indicate what kind of deﬁnition is expected (e.g., biological category, ecological role, or function), resulting in multiple possible correct answers. Answer Yes in other cases. Question: {question} Options: {option} Correct Answer with option for the question about the image: answer Output if the question can be answered as an open-ended question without needing to see the answer choices: Explain your reasoning. Your response must end with: VERDICT: Yes or VERDICT: No Figure 10: This prompt is used to ﬁlter out questions that exhibit characteristics such as option dependency, subjectivity and under-speciﬁcation in stage 1 of our pipeline 7 Answer Uniqueness Veriﬁcation Prompt Evaluate the provided question to determine if its answer remains consistent when presented without multiple-choice options. If the absence of options leads to multiple substantively diﬀerent correct answers, the question is considered option-dependent. Deﬁnition: question is option-dependent if, without provided answer options, there are two or more substantively diﬀerent correct answers (answers diﬀering in meaning or content, not merely phrasing). Minor wording or synonyms that preserve the same underlying meaning should be considered the same answer. Instructions: 1. Calculation Check: Determine if the question requires calculation (e.g., math or counting). If yes, immediately conclude: the question is not option-dependent. 2. Answer Consistency Evaluation: Without considering provided answer options, list all possible correct answers that could be reasonably inferred from the image and question alone. Treat answers as distinct only if they diﬀer substantially in content or meaning. 3. Option Dependency Determination: If multiple distinct answers emerge, and the provided options are necessary to identify the intended correct answer, the question is option-dependent. If there is only one substantively unique correct answer (allowing minor phrasing variations), the question is not option-dependent. Clearly explain your reasoning. Conclude your response explicitly with one of: VERDICT: Yes (option-dependent) VERDICT: No (not option-dependent) Input: Question: {question} Options: {option} Correct Answer with option for the question about the image: {answer} Figure 11: This prompt is used to verify the answer uniqueness in stage 2 of our pipeline 8 Prompt for verifying Reasoning-answer Mismatch You are professional evaluator. Your task is to determine if the models reasoning process is consistent with the ﬁnal answer provided in the box. Follow these steps: 1. Identify the ﬁnal option (e.g., A, B, C) from the models complete response. 2. Find the full content of that option from the provided question. 3. Compare this content with the models thinking process that precedes the ﬁnal answer. 4. Conclude whether the reasoning logically supports the ﬁnal chosen answer. Question and Options: {question} Models Raw Answer: {raw_answer} Your response must end with: VERDICT: Yes or VERDICT: No Figure 12: This prompt is used to verify Reasoning-answer Mismatch 9 Triage and Classiﬁcation Prompt You are an expert classiﬁer. Analyze the provided multiple-choice question (MCQ) and select the most appropriate rewrite method from the four options below. Your decision should be primarily based on the nature and evaluability of the answer to the rewritten question. Provide the name of the chosen method and the core rationale for your choice. Rewrite Methods & Evaluation Criteria keyword_rewrite Use When: For questions with single, unambiguous keyword answer (e.g., name, date, speciﬁc term). Core Test: If you remove the options, is there still only one correct, simple answer? open_ended_rewrite When to Use: The question has clear, objective answer, but it is complex and must be expressed as full sentence, list of items, or an explanation. Core Test: Does the question have factual, non-subjective answer that is too long or structured to be simple keyword? true_false_statement When to Use: The open-ended version of the question lacks single, deﬁnitive answer; it could be subjective or have many possible correct answers (e.g., \"Which of the following is factor...?\"). This makes it necessary to evaluate each provided option individually against the questions premise. Core Test: Without the options, would the question be ambiguous or have multiple valid answers? Does the task rely on judging each given option as True or False in the context of the question? Input & Output Format Input: Original Question: {question text with options and answer} Your Output: Rationale: {rationale} Method: {method_name} Figure 13: This prompt is used to triage and classiﬁcation questions to diﬀerent rewrite types 10 Rewrite Prompt For Numeric Answer Your task is to rewrite numerical question to ensure the answer is pure number (or numbers) without any units. Core Principle: The main goal is to move the units from the answer into the question itself. By explicitly stating the required units in the new question, the answer can be simpliﬁed to raw numerical value, which is easier for automated evaluation. Rewrite Rules: Preserve the Context: Keep the original questions stem (the descriptive part) exactly as it is. Your task is only to modify the ﬁnal interrogative phrase. Specify Units in the Question: Modify the question to explicitly ask for the answer in the required unit(s) and remove the unit from the answer. For example, change \"How much did he pay?\" to \"What was the total amount he paid, in dollars?\". Handle Multiple Values: If the original question asks for multiple values, the rewritten question must ask for each value and specify its corresponding unit, keeping the original order. Strip Units from the Answer: The corresponding answer must be pure numerical value, stripped of all units, currency symbols (like $), and descriptive labels (like =). For multiple values, provide them as comma-separated string in the order requested by the new question. Specify the Answer Format: The rewritten question should tell the user the format of the answer if necessary, such as \"Provide your answer as two numbers separated by colon.\", \"What is the current time shown in the image, in hours and minutes (HH:MM)?\" Fallback to Open-Ended Rewrite: For questions that cannot be reformulated as numeric expression (e.g., an equation), the designated output is Cannot convert to numeric question. No Options Context: Assume no multiple-choice options are available. The rewritten question must be standalone and answerable without referencing any options. Output Format: You must use the following format exactly: REWRITTEN_QUESTION: The full rewritten question ANSWER: The pure numerical answer(s), without units or currency symbols Examples(omitted) ...... Provide the name of the chosen method and the core rationale for your choice. Figure 14: This prompt is used to rewrite questions with numeric answers 11 Rewrite Prompt For keywors Answer Your task is to convert multiple-choice question into direct question that has only one single, unambiguous answer, which can be expressed as speciﬁc keyword. Instructions: Preserve the Context: Keep the original questions stem (the descriptive part) exactly as it is. Your task is only to modify the ﬁnal interrogative phrase into an open-ended question. Ensure Single Answer: The new question must have only one single, unambiguous answer. Provide Answer Variants: List all possible, equally correct variations of the single answer. Separate each variant with <OR>. Output Format: REWRITTEN_QUESTION: The original context followed by the new interrogative phrase. ANSWER: All answer variants, separated by <OR>. Example: Original Question: The structural formula of the glycinium cation is shown above. Arrows indicate the pKa values for the labile protons in the molecule. Which of the following is true about the geometry of the glycinium cation? Answer: B. Both atoms and both atoms lie in the same plane. Output: REWRITTEN_QUESTION: The structural formula of the glycinium cation is shown above. Arrows indicate the pKa values for the labile protons in the molecule. What is the spatial arrangement of the two carbon atoms and the two oxygen atoms? ANSWER: same plane<OR>planar Your task is to rewrite numerical question to ensure the answer is pure number (or numbers) without any units. Figure 15: This prompt is used to rewrite questions with keyword answers Rewrite Prompt For Open Ended Answer Given multiple-choice question (MCQ), its options, and the ground truth answer, transform the MCQ into single, open-ended question with veriﬁable answer. Instructions: Target Only the Question Phrase: Preserve the original context and scenario. Modify only the ﬁnal interrogative part to make it open-ended. Your output for the rewritten question must be the original context followed by the new interrogative part. Preserve Core Knowledge: Ensure the core knowledge being tested remains exactly the same. Make it Standalone: The new question must be answerable without the original options. Require Direct Answer: The question must result in concise, and veriﬁable answer within some words or sentences. Response Format: You must use the following format exactly when you convert the question. REWRITTEN_QUESTION: whole rewritten question ANSWER: single, veriﬁable correct answer Figure 16: This prompt is used to rewrite questions with open ended answers 12 Rewrite Prompt For True/False Answer Your task is to convert multiple-choice question into new, compound question. This is done by keeping the original question stem and adding instructions to evaluate each option as True or False. Instructions Preserve the Original Stem: The rewritten question must begin with the exact, unchanged stem from the original question. Reframe the Task: After the original stem, append new instruction that reframes the task, such as \"Now, evaluate each of the following statements.\" List Options as Statements: List each of the original multiple-choice options as complete, standalone statement for evaluation. Specify the Answer Format: Add ﬁnal instruction telling the user to provide their answer as single, comma-separated list of True or False values. Output Format: REWRITTEN_QUESTION: The full text of the new question, including the preserved stem and the ﬁnal instruction on how to answer. ANSWER: sequence of True or False values corresponding to the order of the statements, separated by commas. Examples:(omitted) Figure 17: This prompt is used to rewrite questions with True/False answers Rewrite Prompt For Reﬁning Keyword Answer Your task is to evaluate if rewritten question can be reliably judged by its keywords. Analyze the provided question and keywords, then choose one of the following three actions: Improve the Keywords: Use When: The answer is speciﬁc entity, verb phrase or number, less than 3 words. Action: Add all necessary synonyms and variations, separated by <OR>. Format: IMPROVED_KEYWORDS: The complete list of keyword variants Example: IMPROVED_KEYWORDS: 3 <OR> three Reject - Unsuitable Question Use When: The possible answer of the question is too broad, such as an explanation, description, list, or complete sentence. Simple keyword matching would be unreliable for judging correctness. Action: Reject with the speciﬁc reason INSUFFICIENT_KEYWORD_EVALUATION. Format: REJECTION_REASON: INSUFFICIENT_KEYWORD_EVALUATION Reject - Imprecise Answer Use When: The possible answer of the question is too subjective or no veriﬁable answer, or the candidate key word is more than 3 words. Action: Reject with the speciﬁc reason IMPRECISE_ANSWER. Format: REJECTION_REASON: IMPRECISE_ANSWER Figure 18: This prompt is used to reﬁne questions with keyword answers"
        }
    ],
    "affiliations": [
        "BAAI FlagEval Team",
        "BUAA",
        "Institute of Automation, CAS",
        "PKU",
        "School of Artificial Intelligence, UCAS",
        "ZJU"
    ]
}