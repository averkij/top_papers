{
    "paper_title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners",
    "authors": [
        "Lu Ling",
        "Yunhao Ge",
        "Yichen Sheng",
        "Aniket Bera"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/"
        },
        {
            "title": "Start",
            "content": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners Lu Ling1*, Yunhao Ge2, Yichen Sheng2, Aniket Bera1, 1Purdue University 2NVIDIA Research I-Scene Project Page 5 2 0 D 5 1 ] . [ 1 3 8 6 3 1 . 2 1 5 2 : r Figure 1. pre-trained 3D instance model is re-programmed into scene-level spatial learner. It learns spatial priors from non-semantic scenes (randomly composed instances) in feed-forward manner, producing coherent layouts and unseen interactive 3D scenes."
        },
        {
            "title": "Abstract",
            "content": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram pre-trained 3D instance generator to act as scene-level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generators transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generators transferable scene prior provides rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with view-centric formulation of the scene space, yielding fully feed-forward, generalizable scene generator that learns spatial relations directly *Corresponding author. from the instance model. Quantitative and qualitative results show that 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. The code is accessible at the project page. 1. Introduction Generalization remains the central obstacle to interactive 3D scene generation. An effective system must produce editable, affordance-aware, and spatially coherent object arrangements, capabilities critical for virtual content creation, simulation, and embodied AI. Recent end-to-end approaches extend powerful image-to-3D instance priors [21, 23] to multiple objects and relations in single pass, yet their spatial understanding is typically learned from curated scene datasets whose limited coverage does not scale and ultimately constrains generalization to new layouts. Recent methods learn layouts directly from annotated scenes [8, 17, 20, 24]; for example, SceneGen [17] explicitly models poses on 3D-FRONT [5], and MIDI [8] 1 For instance, learns inter-object relations from those annotations. However, available interactive scene datasets are limited in scale, diversity, and spatial variety. the widely used interactive 3D scene dataset 3D-FRONT contains only 20K indoor bedroom and living-room scenes and under-represents small and supporting objects. Consequently, scene layout-supervised models often overfit to dataset-specific biases and fail to generalize to broader scene distributionsfor example, small objects placed on or behind large furniture or outdoor settings. Our key insight is that pre-trained 3D instance model implicitly encodes transferable spatial knowledgedepth, occlusion, scale, and supporteven though it outputs single-mesh geometry. We reprogram this prior as scenelevel spatial learner to provide model-centric spatial supervision and enables it generalize to unseen layouts, eliminating dependence on curated scene annotations. key obstacle to turning an instance generator into scene-level learner is that its canonical object space suppresses spatial sensitivity: different views are collapsed into the same canonical representation, destroying layout cues needed for scene reasoning. To overcome this limitation, we replace the widely used canonical space with viewcentric scene space, in which each scene is represented in view-dependent coordinate frame that preserves the layout relationship between the camera view and the scene. Reprogramming the instance model to spatial learner and training it in the view-centric space yields fully feed-forward formulation of interactive scene generator that learns spatial relations directly from instance prior. Via this formulation, training entirely on non-semantic synthetic scenes in which objects are randomly composed without meaningful relations surprisingly yields to strong spatial reasoning ability, surpassing dataset-bounded baselines and robustly generalizes to various layouts. These results indicate that non-semantic synthetic scenes are sufficient for spatial learning and suggest promising scaling path for interactive 3D scene understanding and generation. Our contributions are summarized as follows: Model-centric supervision. We reprogram pre-trained 3D instance prior to function as scene-level spatial learner, revealing its transferable spatial knowledge without relying on scene-level annotations. View-centric scene space. We replace the canonical object space with view-aligned shared scene representation that preserves geometric and relational cues, enabling fully feed-forward scene generator. Dataset-independent layout learning. We show that the reprogrammed instance priors strong spatial learning capability from non-semantic, randomly composed synthetic scenes, relaxing annotated data dependency. Strong Generalization. Despite being trained solely on random layouts, our model surpasses SOTA methods trained on 3D-FRONT and transfers robustly to diverse unseen layouts. 2. Related Work Single geometry 3D scene generation. One line of work represents entire scenes as single global structure guided by text or visual prior. For example, Wonderland [12] predicts 3D Gaussian Splatting in feed-forward manner from single image; WonderWorld [29] generates connected scenes from single view using layered Gaussian surfels. Similarly, WorldExporer [19] targets text-driven, 3D-consistent but non-disentangled These monolithic approaches achieve impresscenes. sive view consistency and speed, but lack instance accordance, limiting fine-grained editing, instance interaction, and physics reasoning. Compositional 3D interactive scene generation. Recent pipelines decompose scenes into perception and assembly: instance detection/segmentation and depth provide amodal cues, then objects are retrieved or generated, finally are assembled into scene. Gen3DSR [1] follows modular divide-and-conquer strategy; Deep Prior Assembly [30] integrates frozen priors (e.g., SAM [11], diffusion, Shap-E [10]) with layout fitting; REPARO [6] performs compositional asset generation with differentiable 3D layout alignment; CAST [27] applies component-aware analysis with SDF-based physics correction. On the textdriven side, graph/LLM planners (e.g., LayoutGPT [4], Holodeck [25], GALA3D [31]) synthesize scene graphs or relational constraints that downstream stages realize with assets and solvers. Scenethesis [15] and SceneWeaver [26] combines LLM planning, vision guidance, and physicsaware optimization with plausibility judge. These approaches provide high fidelity and open-ended semantics, but are sensitive to early perception/planning errors, potential retrieval/solver mismatch, and incur per-scene optimization, limiting throughput and scalability. Learning-based 3D interactive scene generation. Most recent approaches learn spatial layout distributions from curated 3D scene datasets [5], differing mainly in how they i) Layout-first scene synthesis. instantiate object assets. model room as an unordered set of object tokens with attributes such as category, size, pose, which are later instantiated as geometry via retrieval or generation. For instance, ATISS [18], SceneFormer [22]) DiffuScene [20], MiDiffusion [7], DeBaRA [16], and PhyScene [24] condition on room type or floor plan, enabling controllable scene completion and synthesis; ii) End-to-end multi-instance method. With the advancement of pre-trained image-to-3D object model, recent approaches jointly model multiple objects and their spatial relations, reducing cascade errors and avoiding retrieval/solver loops at test time. MIDI-3D [8] 2 and SceneGen [17] model multiple assets and their implicit inter-object relations or instance explicit poses in one feedforward pass. PartCrafter [13] extends compositional latent diffusion transformers to jointly denoise parts/objects into explicit triangle meshes. Both paradigms achieve strong control and coherence but remain constrained by the biases and limited diversity of existing scene datasets, often struggling with rare layouts or complex arrangements such as small objects supported by or occluded behind larger ones. Our method uses the same input protocol as prior learning-based approaches but differs fundamentally in design. We introduce feed-forward, view-centric approach that jointly reasons over global scene context and instance generation. This design enables (i) layout generalization beyond dataset biases, capturing richer spatial relations such as small/supporting objects; and (ii) end-to-end inference without retrieval or solver handoffs, avoiding the stage-wise errors and latency of compositional pipelines. Unlike prior methods constrained by curated datasets or optimizationheavy reasoning, our framework could learn spatial knowledge directly from non-semantic synthetic layouts, achieving dataset independence while preserving instance-level editability. 3. Method 3.1. Problem Definition Given single image Iscene RHW 3 and its instance masks {mi}N i=1, I-Scene generates set of independently manipulable 3D instances = {Ai}N i=1, coherently placed in the scene space such that the spatial layout aligns with the input image. 3.2. Overview Figure 2 presents the overview of our generalizable spatial learner(I-Scene). I-Scene reprograms an image-to-3D instance foundation model to spatial leaner. We use TRELLIS [23] backbone in the experiment. I-Scene only modifies the sparse structure transformer. Other stages are kept the same. I-Scene has two branches: spatial guidance branch and instance generation branch. The two branches share the same weight and are trained jointly. Spatial guidance branch. The spatial guidance branch takes the scene RGB image as input and predicts the scene as single geometry represented as sparse set of active voxels. Following TRELLIS, the branch outputs fscene = {(fi, pi)}L i=1, (1) frame (an anchor axis) that all instances reference. Without this global anchor, instances would be generated independently and the resulting composited scene layout would become incoherent. Instance generation branch. The instance generation branch takes an instance RGB image Iinst as input, conditioned on the spatial guidance latent zscene, and learns function to predict voxelized instance features: finst = (cid:0)Iinst, zscene (cid:1), (2) The instance generation not only learns the instance local geometry, but also its pose in the the scene. As zscene already provides the scene layout, just needs to focus on geometry generation and follow the scene layout guidance. This effectively converts the instance generator from implicitly encoding scene priors to explicitly learning spatial layout. This key formulation unlocks generalizable 3D scene generation. In detail, is achieved by scene-context attention. 3.3. Scene-Context Attention We not only need to learn , but also need to preserve the base models prior as we do not want zscene to be catastrophically forgotten. So we need to minimize the change to the base model. We present scene-context attention to achieve this goal. We transform some of the original self-attention layers to scene-context attention(SCA) layers. In the original selfattention layer, let (Qi, Ki, Vi) be queries/keys/values from instance and (Qs, Ks, Vs) be the queries/keys/values from the spatial guidance branch. Ki = [Ki; Ks], Vi = [Vi; Vs], SCA(Qi, Ki, Vi) = softmax (cid:32) (cid:33) Vi, Qi i (3) (4) Intuitively, the instance generation is not only based on its own Ki and Vi, but also conditions on Ks and Vs. SCA is natural modification to the backbone as it does not change the latent distribution, making minimal changes to the prior. An extreme example can demonstrate this property: when the Ki/Vi is the same with the scene Ks/Vs, meaning when the scene and instance input is exact the same, the SCA gives equivalent result of self-attention layers. We show the mathematical proof in the Appendix. where pi denotes the position of the i-th active voxel, fi is its corresponding local feature, and is the number of active voxels. This branch provides two key functions: (i) it produces global scene layout that guides instance-level generation; and (ii) it establishes shared scene coordinate 3.4. View-Centric(VC) Space Existing relevant works [8, 17] follow image-to-3D base model and use canonical space for scene representation. We observe using canonical space limits the learning of and Figure 2. Overview. I-Scene has two branches: (i) spatial guidance branch takes scene RGB as input and provides spatial anchor for each instance generation. (2) Instance branch takes instance RGB and scene context tokens and output the instance in view centric space. results as shown in our ablation study  (Table 3)  . 3.5. Non-Semantic Synthetic 3D Scene 3D-FRONT is domain specific dataset that only has limited indoor assets. Although with our SCA and VC space, I-Scene already achieves the SOTA generalization results  (Table 1)  , we observe the instance quality degrades, implying the model inevitably forgets the instance prior. As the spatial leaner is not directly learning the dataset layout, but learning from the given spatial guidance, we observe that whether the training scene layout has semantic meaning is not very important for I-Scene. In the other hand, training on the random scene composed of instances from the whole instance 3D dataset further improve I-Scene generalization ability. Collision-free Random Layout. We generate our synthetic scenes via sampling high-quality 3D instances from diverse 3D asset dataset(e.g., Objaverse [3]), randomly places them with collision-free mechanism to reduce severe occlusions, and enforces common spatial relation such as right, left, front, back, and on the top of. The scene layout is purely non-semantic meaningful composition of instance; only geometric and basic physical plausibility constraints are applied. By training on this dataset, I-Scene learns general spatial reasoning while remaining agnostic to category semantics. We provide our random layout generation details in appendix. 3.6. Training The training target is conditioned rectified flow methods: LCFM(θ) = Et,x0,ϵ vθ(x, t) (ϵ x0)2 2 , (5) where vθ is the sparse structure neural network, x(t) = (1 t)x0 + tϵ, and ϵ is the noise at time step t. (a) View-centric space (b) Non-Semantic scene Figure 3. (a).Canonical space scene is view invariant. View centric(VC) space is view-dependent, which encodes strict spatial relationship between the image space and scene space. (b) nonsemantic random 3D scene example. Objects have random poses and are collision-free. hurts the model generalization ability. As illustrated in Figure 3 (a), given two camera (C1 and C2) and the scene composed of blue square and red circle in the space, the camera views of the scene are different as they are in different location and orientation, but the instance in the scene stay at the same position in the canonical space. So this formulation makes the ignores the object spatial position in the view image and only focuses on the local shape of each instance in the image. When training on some indoor dataset like 3D-FRONT, it is not significantly hurting as there are limited object in the scene and each instance usually has very different shapes. But if the scene has identical objects, e.g. several identical chairs having similar poses, often put duplicated chairs at the same position. We argue that the object spatial layout in the RGB image is also important and provides strong hint on its 3D layout pose. So we propose view-centric space where the space axis is based on the camera pose. As shown in Figure 3 (a), given the same two camera setting, the spatial layout of the two objects follows the change of the camera pose coherently. When training IScene in VC space, achieves much better generalization 4 4. Experiment 4.1. Experiment Setting Baseline. We compare the representative interactive 3D scene generation methods including MIDI [8], SceneGen [17], Partcrafter [13], and Gen3DSR [1]. For all baselines, we use the same scene RGB image and instance masks as inputs, except PartCrafter, which does not support mask control; for PartCrafter we provide the scene RGB plus the target instance count. Because PartCrafter lacks texture rendering, we evaluate it only on scene-level geometry and layout quality. For visual quality comparisons, we texture MIDIs generated scenes using MV-Adapter [9]. Metric. We evaluate scenes based on geometry quality and layout accuracy. For geometry quality, we convert the generated assets to point clouds and rigidly align the prediction to the ground truth using customized robust ICP (details in appendix). After alignment, we then report Chamfer Distance (CD) and F-Score (threshold τ =0.1) at two levels: (i) scene level on the union of all points, and (ii) object level by computing the metrics per matched instance and averaging. To evaluate scene layout, we compute matched volumetric IoU between the axis-aligned bounding boxes (AABBs) of predicted and ground-truth scenes. This metric captures the overall objects size, position, and relative placement. We also report the average inference time per instance. Evaluation Dataset. We evaluate all methods on syni) Synthetic thetic data and real-world/stylized scenes. (in-domain). Following MIDI, we adopt the 3D-FRONT split and use the same test scenes, filtering out renders with fully occluded instance masks; this yields 860 test scenes. ii) Synthetic (out-of-domain). To assess generalization to novel layouts and object sets, we additionally evaluate on scenes from BlendSwap [2] and Scenethesis [15], which yield 26 test scenes including small on large relations and outdoor settings. iii) Real-world /stylized. For in-the-wild evaluation without ground truth, we test on images from DL3DV-140 [14], Gen3DSR [1], ScanNet++ [28], and stylized scenes. Then, we report qualitative comparisons. 4.2. Quantitative Results Table 1 reports quantitative comparisons on synthetic datasets, including 3D-FRONT [5], BlendSwap [2], and Scenethesis [15]. 3D-FRONT dataset only contains bedroom/living room scenarios, while Blendswap and Scenethesis contain more diverse layout. Our method, IScene, achieves the best performance among the state-ofthe-art methods across all evaluated metrics without incurring much time consumption. Geometry quality. Object-level. On 3D-FRONT test scenes (ID), our method attains the lowest object-level CD and the highest F-score compared with MIDI [8], SceneGen [17], PartCrafter [13], and Gen3DSR [1], indicating higher-fidelity per-object geometry. Crucially, on out-ofdomain (OOD) benchmarks (BlendSwap, Scenethesis), our object-level scores remain comparable to ID and still exceed all baselines by clear margin, evidencing robust generalization. Scene-level. Across both ID and OOD settings, our method also achieves lowest scene-level CD and highest F-score than the baselines, reflecting more accurate and more complete whole-scene geometry. We ati) reprogrammed pre-trained tribute these gains to: instance prior. pre-trained instance generator supplies strong single-object shape priors, which is an advantage over methods rely primarily on reconstruction from limited data such as Gen3DSR; ii) model-centric supervision with shared, view-centric scene context. Per-instance generation conditioned on shared, view-centric scene tokens keeps shapes near canonical while aligning pose and contact, reducing fused-mesh artifacts and shape bending common in scene dataset-supervised methods such as MIDI, SceneGen, PartCrafter. Unlike all baselines, which degrade substantially on OOD scenes, our objectand scene-level metrics on BlendSwap and Scenethesis remain close to our ID scene performance, underscoring strong generalization. Layout accuracy. Our method attains the highest Volumetric IoU across all baselines, indicating more accurate object placement in position, scale, and orientation and global scene geometry consistency. Gains are largest on BlendSwap/Scenethesis. Our results suggest stronger modeling of support and proximity for various layout. The metric result also confirm the advancement of I-Scene from scene and instance geometry quality. The instance semantic and spatial relation disentangled formulation improves scene-level fidelity without additional scene annotations. Efficiency. We measure end-to-end per instance inference time on single H100 GPU for all approaches. Our method completes instance in 15.51s, although slower than PartCrafer (7.2 s) while delivering higher geometry and layout quality, and faster than the other baselines in our study. For fairness, MIDI inference time includes per instance texture rendering. We attribute our latency to feed-forward design leveraging the pre-trained instance prior, which avoids retrieval or iterative layout optimization. 4.3. Qualitative results We qualitatively compare our approach with MIDI, SceneGen, PartCrafter, and Gen3DSR in two settings: (i) synthetic scenes and (ii) real-world and stylized inputs. All methods receive the same single input image. Synthetic scenes results on 3D-FRONT-style scenes and more complicated layouts from BlendSwap and Scenethesis. Instance quality. Our Figure 4 contrasts our Table 1. Comparison on evaluation datasets. CD: Chamfer Distance; F-Score threshold τ =0.1. = scene-level; = object-level; IoU-B = volumetric IoU of scene bounding boxes. Best numbers are in bold. PartCrafter is only compared on scene-level performance. Method 3D-FRONT BlendSwap & Scenethesis Runtime CD-S F-Score-S CD-O F-Score-O IoU-B CD-S F-Score-S CD-O F-Score-O IoU-B Gen3DSR 0.2587 PartCrafter 0.0586 0.1432 SceneGen 0.0175 MIDI 0.0148 Ours 42.31 81.03 54.70 90.08 93.50 0.0697 - 0.0353 0.0877 0.0207 57.22 - 77.95 70.10 84.28 0.4838 0.7626 0.5295 0.8596 0.8762 0.1429 0.0609 0.1161 0.0212 0. 45.43 66.56 49.94 83.13 94.26 0.0722 - 0.0852 0.1884 0.0503 53.45 - 65.66 50.84 72.39 0.4736 0.5819 0.4669 0.7412 0.8568 179.0 7.2 26.0 42.5 15.51 Table 2. Comparison on 3D-FRONT and BlendSwap & Scenethesis. CD: Chamfer Distance; F-Score threshold τ =0.1. = scene-level, = object-level; IoU-B = volumetric IoU of scene bounding boxes. Best numbers are bold. Training dataset 3D-FRONT BlendSwap & Scenethesis CD-S F-Score-S CD-O F-Score-O IoU-B CD-S F-Score-S CD-O F-Score-O IoU-B 0.0137 3D-FT (25K) 0.0496 Rand-15K Rand-25K 0.0406 3D-FT+Rand-15K 0.0148 93.77 79.96 81.39 93.50 0.0278 0.0932 0.0402 0.0207 81.34 55.01 74.76 84.28 0.8792 0.7729 0.7783 0.8762 0.0118 0.0081 0.0075 0. 90.79 92.67 93.60 94.26 0.0585 0.0698 0.0580 0.0503 68.87 67.36 70.18 72.39 0.8222 0.8445 0.8471 0.8568 method produces clean, well-separated instance meshes with sharp thin parts (e.g., shelf, sofa, umbrella in the figure) and minimal artifacts, aligning with the lower object-level CD and higher F-score in Sec. 4.2. Baseline methods such as MIDI and SceneGen often exhibit duplicate placements, uncleaned mesh, or over-smoothed geometries that blur instance boundaries. Spatial relations. Our scenes preserve support and proximity (e.g., small-on-large relations such as TV/ flower on tables), maintain correct depth ordering for partially behind/occluded items, and avoid floating or colliding objects, which is common in baselines. This is consistent with our higher Volumetric IoU and scene-level F-score. Robustness on various layouts. Gains are most visible on BlendSwap and Scenethesis testing scenes, which include multi-scale clutter, small-on-large configurations, and outdoor layouts that are under-represented in curated indoor datasets. Real-world and stylized scenes We further evaluate on in-the-wild images from DL3DV-140, Gen3DSR, and ScanNet++ and on stylized/cartoon inputs as shown in Figure 5. Across these distributions, baselines are less robust to strong style shifts and complex layouts, frequently showing misaligned instances, surface bleed between nearby assets, and implausible supports such as frequently collied objects. In contrast, our method maintains consistent scale and uprightness, clear separation between instances, and plausible contacts across indoor, outdoor, and stylized scenes. Comparison with baselines. Existing feed-forward or retrieval/assembly pipelines tend to either sacrifice per-object lose global coherence when generalize to fidelity or novel layouts: duplicate objects, merged/entangled assets, collied and floating placements are common failure modes. Our results exhibit both higher per-instance fidelity and more coherent spatial relations, which we attribute to i) model-centric supervision from reprogrammed pre-trained instance prior (preserving object fidelity) and ii) view-centric shared scene context that stabilizes relative pose, depth ordering, and support for each instance from shared space during generation. The Appendix includes additional synthetic/real/stylized examples, multi-view renderings for each scene, and videos for large scenes with complex layouts. 4.4. Non-semantic random scene experiment To answer the question, we conduct In this section, we want to answer the question: do non-semantic, randomly composed scenes provide meaningful spatial supervision? Setup. the experiment that train I-Scene on four datasets: i) 3D-FT: 25K 3D-FRONT scenes including bedroom/living-room ii) Rand-15K: 15K with dedicated scene annotations. non-semantic scenes by randomly composing instances. It is generated by our synthetic data generation system. iii) Rand-25k: same as Rand-15K but scaled to 25K scenes. iv) 3D-FT+Rand-15K: mixture of 3D-FT and Rand-15K. Evaluation. is treated as The 3D-FRONT test set in-distribution for model trained on 3D-FT. BlendSwap and Scenethesis contain more diverse layouts and are used as out-of-distribution (OOD) benchmarks. Result. Table 2 presents the experiment results. We observe that training solely on 3D-FRONT (3D-FT) yields Figure 4. Qualitative comparison with all baselines on synthetic scenes. We might slightly rotate the view to better illustrate the error patterns. More details can be found in Appendix. Table 3. Ablation on I-Scenes components. SCA: scene context attention, VC: view centric, NS: non-semantic scenes. Examine component 3D-FRONT BlendSwap & Scenethesis SCA VC NS CD-S F-Score-S CD-O F-Score-O IoU-S CD-S F-Score-S CD-O F-Score-O IoU-S 0.0163 0.0137 0. 93.69 93.77 93.50 0.0286 0.0278 0.0207 80.12 81.34 84.28 0.8598 0.8792 0.8762 0.0351 0.0118 0.0059 79.12 90.79 94. 0.0829 0.0585 0.0503 63.16 68.87 72.39 0.7557 0.8222 0.8568 the stronger in-distribution layout (lowest CD-S, highest IoU-B) compered to purely trained on non-semantic scenes, but generalizes poorly to BlendSwap/Scenethesis (OOD). Interestingly, purely non-semantic randomized training (Rand-15k/25k) captures transferable spatial regularities and surpasses 3D-FT on BlendSwap/Scenethesis scene-level metrics (CD-S, F-S, IoU-B), with performance improving as we scale from 15k to 25k scenes. The combined regimen (3D-FT+Rand-15k) preserves near-optimal in-domain layout while further improving object geometry (lower CD-O, higher F-O) and achieving the best OOD results across all metrics. These findings indicate that i) I-Scene is able to learn spatial knowledge from non-semantic scenes. In particular, instead of the high-level scene semantics, geometric cue alone provides strong training signal for spatial learning and reasoning. ii) Curated scene annotations remain useful for in-distribution calibration, synergizing training with non-semantic scenes to yield the best overall performance. and iii) Non-semantic spatial relations are valuable supervision source; scaling non-semantic data provides richer spatial coverage that the model can exploit. 7 Figure 5. Qualitative comparison with scenes in-the -wild. We compare the generalization ability of different methods in different style and various spatial relations. To better illustrate the error pattern, we might slightly rotate the view. 4.5. Ablation studies We evaluate the contribution of the main design choices in I-Scene on 3D-FRONT, BlendSwap, and Scenethesis. (i) Scene-context attenWe study three components: tion(SCA): shared scene-level tokens that contain spatial cues; (ii) View-centric(VC): the view-aligned space; (iii) Non-semantic meaningful scenes (NS): randomized compositions from the Objaverse instances. We start from the full model (SCA + VC + NS) and remove one component at time. As summarized in Table 3, we observe that every component matters. Removing any of SCA, VC, NS consistently degrades geometry (CD, F-score) and layout accuracy (IoU) on all evalaution datasets, confirming their complementary roles in IScene for generalize to novel layout. VC is critical for layout coherence. Without view-centric encoding, the model tends to over-fit on 3D-FTRONT dataset and has poor generalized results on Blendswap and Scenethesis. It also misaligns scene context across objects, yielding duplicated or blended instances and contact violations; this manifests as the largest IoU drop and notable decline in scene-level F-score. NS improves instance generalization. Adding NS yields substantial gains in F-score at the instance level and increment in spatial-relation (IoU), reflecting better surface completeness and fewer near-surface errors for new scenes and inter-object placement is driven chiefly by the view-centric shared scene context. We pro8 vide qualitative results for the ablation study in appendix. Overall, these results demonstrate that view-centric scene context (VC+SC) is essential for coherent layouts and clean instance boundaries, while training on non-semantic scene (NS) supplies the instance diversity needed to generalize beyond distributions. 5. Conclusion Contribution. We reprogram pre-trained 3D instance generator as scene-level spatial learner. We define view-centric scene space and distill spatial cues into shared scene tokens that condition instance denoising, yielding fully feed-forward formulation for interactive scene generation. This formulation replaces dataset-bounded layout supervision with models own spatial prior, removing reliance on curated scene annotations and enabling learning spatial relations from non-semantic meaningful scenes. Our experiments indicate non semantic meaningful synthetic data improve geometry, layout, and generalization, pointing toward foundation model for interactive 3D scene generation. Limitations and future work. Our method performs relatively poorly on tiny-resolution inputs and in heavily occluded single-view case (see Appendix). Future work: 1) improve model robustness with heavy occlusion augmentations, and to explore optional multi-view conditioning; 2) Further investigate the scaling law of non-semantic random scene to further handle challenging in-the-wild layouts. I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Overview To better illustrate our method, the supplementary material is organized into two main parts: Method and Experiments. In addition, we provide an HTML page for improved qualitative visualization and comparison. Method. Scene-context attention. We provide mathematical derivation showing that when the scene and instance inputs are identical, the proposed scene-context attention (SCA) is equivalent to standard self-attention layer. Collision-free random layout. We describe the details of our collision-free random layout generation procedure. Experiment setting. Model implementation details. Training datasets. Metrics. We provide the full specification of our customized robust ICP metric and related details. Qualitative results. Qualitative comparison with all baselines on synthetic scenes. We either (i) show interactive 3D scene comparisons on the web page, or (ii) include two representative rendered views per scene for each method. Qualitative comparison with all baselines on real-world and stylized images, following the same visualization protocol as above. Additional qualitative results for the ablation study. Failure cases. We report the failure cases when instance mask is small. The figure including the input image, the predicted scene, and the predicted low-quality instance examples. 7. Method scene-context attention. In this section, we show the mathematic proof that when the Ki/Vi is the same with the scene Ks/Vs, meaning when the scene and instance input is exact the same, the SCA gives equivalent result of selfattention layers. Let Qi Rtd be the (row-stacked) instance queries, Ki, Ks Rnd the keys, and Vi, Vs Rndv the values. We concatenate along the token dimension, Ki = [Ki; Ks] R2nd, Vi = [Vi; Vs] R2ndv , and define scene-context attention (SCA) by (cid:16) Qi i SCA(Qi, Ki, Vi) = softmax (cid:17) Vi, where softmax() is applied row-wise across the key dimension. We show that when the scene and instance inputs coincide, i.e., Ki = Ks and Vi = Vs, SCA reduces exactly to standard self-attention: SCA(Qi, Ki, Vi) = softmax (cid:16) QiK (cid:17) Vs. Proposition. If Ki = Ks and Vi = Vs, then SCA(Qi, [Ki; Ks], [Vi; Vs]) = softmax (cid:16) QiK d (cid:17) Vs. Proof. Under Ki = Ks and Vi = Vs, write := Ks and := Vs. Then Ki = [K; K] and Vi = [V ; ]. Let := QiK Rtn. By block structure, Qi i = Qi[K; K] = (cid:2) (cid:3) Rt2n. Consider any row Rn of Z. The row-wise softmax over the concatenation [z, z] R2n yields softmax([z, z]) = (cid:104) 1 2 softmax(z) 1 (cid:105) , 2 softmax(z) because for each coordinate j, with := (cid:80)n ℓ=1 ezℓ we have ezj ℓ=1 ezℓ + (cid:80)n (cid:80)n ℓ=1 ezℓ = ezj 2s = 1 2 softmax(z)j. Applying this row-wise to [Z, Z] gives softmax([Z, Z]) = (cid:2) 1 2 1 2 (cid:3), where := softmax(Z) Rtn. Therefore, softmax([Z, Z]) Vi = (cid:2) 1 = 1 = SV 2 1 2 SV + 1 2 (cid:3) [V ; ] 2 SV (6) = softmax (cid:16) QiK (cid:17) V. This is precisely the output of standard self-attention layer evaluated on (Qi, K, ). 1 Collision-free Random Layout. When we randomly create layout, we first randomly sample instances from the instance object dataset (e.g. Objaverse). Then we generate collision-free layouts in Poisson noise pattern by treating each object as 2D disc on the ground with radius ri = si ˆri, where ˆri is computed from the mesh x/z extents and si is the sampled scale. Centers are sampled with variable-radius Poisson-disk routine that places larger radius first. We define global clearance gap = s, where is the mean of ri and is randomly sampled scaling factor that controls the layout density. candidate center xi is accepted only if xi xj2 ri + rj + gap for all placed j. We accelerate checks with uniform grid of cell 2, retry up to fixed budget, and exsize (mini ri + gap)/ pand the sampling region by 10% if needed. When table is present, it is placed first and included in the same exclusion process. For the stacked object, we slice the table mesh just below its top to extract the top polygon, use its centroid as the anchor, and choose scale that fits inside based on the distance to the nearest edge, then place the stacked object on the table. 8. Experiment Model implementation. All experiments are conducted on cluster with 8 NVIDIA H100 GPUs. We train I-Scene for 130K steps using the AdamW optimizer with learning rate of 5e5 and batch size of 8 per GPU. During inference, we adopt 25 sampling steps with the classifier-free guidance set to ω = 3.0 for both the sparse structure generation and structured latents generation. Training dataset The 3D-FRONT training dataset is the same as MIDI processed 3D-FRONT dataset, where there are roughly 24K different scenes. Using the random layout generation algorithm discussed above, randomly generate 15K and 25K scenes. Each scene has minimum 2 object and maximum 12 objects. For each scene, we use Blender to render 150 views that evenly distributed from all directions looking at the scene center. Then we transform each instance into view centric space, voxelize and encode each instance geometry using TRELLIS sparse structure encoder as ground truth. Note, in some view some object is fully occluded and not visible, we discard this render view to avoid the model memorize and hallucinate invisible objects. get stuck into local minimum. To handle this challenge, we used several ways to escape local minimum. transform search. We perform yaw-sweep Initial initialization about designated up axis (a global {x, y, z}). For candidate set of yaw angles (default {0, 45, 90, 135, 180, 225, 270, 315} degrees), we rotate the source downsampled point cloud about a, and pre-score each angle using trimmed symmetric Chamfer distance with trim ratio τ =0.2 on up to 2000 sampled points per cloud. We keep the top three yaw candidates by this prescore and run short seed ICP on the downsampled clouds for each candidate using point-to-point estimator (optionally with isotropic scale if enabled). We select the initialization T0 that minimizes rmse + λ(1 fitness) with λ set to the voxel size (λ=v). Shared normalization. For numerical staCoarse-to-fine. bility and consistency across stages, we apply the same uniform normalization to source and target: let be the midpoint of the axis-aligned bounding box over both clouds and σ the maximum side length. We work in normalized coordinates = (x c)/σ for the remainder of the registration. Downsampling and normals. We voxel downsample both normalized clouds with voxel size = 0.03 (relative to the normalized extent) for the coarse stage, and estimate normals on both downsampled and full-resolution clouds when available. Estimators. The coarse stage uses point-to-point objective; if isotropic scale is enabled and supported, we estimate global uniform scale jointly with the rigid motion. The fine stage uses point-to-plane with Tukey robust loss (scale = 1.5v); if normals or robust losses are unavailable, we fall back to point-to-point. Iteration budgets and thresholds. We split the budget evenly with safeguards: Tcoarse = max(10, 0.5 ) and Tfine = max(10, Tcoarse). Distance thresholds are 2.5v (coarse) and for point-to-plane fine refinement (or 1.5v if point-to-point is used). Execution. Coarse ICP is run on the downsampled pair from T0, yielding coarse. If isotropic scale was estimated, we pre-apply coarse to the full-resolution source and run fine ICP from identity, then compose the results; otherwise, we run fine ICP on the full-resolution pair initialized with coarse. Robust ICP. Previous work MIDI [8] uses ICP for alignment before calculating the metrics. We notice this ICP is not robust and leads to many bad alignment, making the metric calculation not reliable. Instead, we propose robust ICP to make the metric results more reliable. The main reason ICP is not robust is it is very easy to Pose projection and validation. We project the 33 rotation block to the nearest element of SO(3) via SVD. If reflections are disallowed, we enforce positive determinant. We then perform sanity checks on the transform in normalized space (finite entries, reasonable determinant when rigid, translation magnitude not excessive). On detection 2 of anomalies we fall back to the coarse solution or identity, whichever is valid. Practical notes. As different methods generate in different space with various scale, this introduce an subtle bias. Even if we align the space by min/max values, the scene size potentially still affects the results. For example, rotating an extreme long scene little bit potentially has different scene scale after normalization. So we apply the robust ICP alignment in this way, say we have two space: (i). min/max normalize space: just scale the scene space by the default output range, e.g. TRELLIS outputs into [-0.5, 0.5], then we just scale the scene isotropically by 2; (ii). AABB recentered normliaze: we move the scene into zero center and then normalize AABB min/max into [-1.0, 1.0]. We apply robust ICP and obtain transf orm1 and transf orm2. We then pick the best metric results from the two transformations. 9. Qualitative results. We present the qualitative results for the following items: Figure 6 and Figure 7 present multi-view qualitative comparison between I-Scene and all baselines using realworld /stylized images as input. Figure 8 presents multi-view qualitative comparison between I-Scene and all baselines using synthetic scene images as input. The visualization of ablation study can be found in Figure 9, where adding each component (SCA, VC, and NS) in I-Scene would significantly improve the layout coherent and instance quality. We also present the failure cases in Figure 10. As shown in the figure, instance quality is bad when the input instance mask is small in the image."
        },
        {
            "title": "References",
            "content": "[1] Andreea Ardelean, Mert and Bernhard Egger. Gen3dsr: Generalizable 3d scene reconstruction via diarXiv preprint vide and conquer from single view. arXiv:2404.03421, 2024. 2, 5, 6, 7 Ozer, [2] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6290 6301, 2022. 5 [3] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 4 William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. [5] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts In Proceedings of the IEEE/CVF Internaand semantics. tional Conference on Computer Vision, pages 1093310942, 2021. 1, 2, 5 [6] Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and WanReparo: Compositional 3d assets generation hua Li. arXiv preprint with differentiable 3d layout alignment. arXiv:2405.18525, 2024. 2 [7] Siyi Hu, Diego Martin Arroyo, Stephanie Debats, Fabian Manhardt, Luca Carlone, and Federico Tombari. Mixed arXiv preprint diffusion for 3d indoor scene synthesis. arXiv:2405.21066, 2024. 2 [8] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffusion for single image to 3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2364623657, 2025. 1, 2, 3, 5, 6, 7 [9] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: In Multi-view consistent image generation made easy. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1637716387, 2025. 5 [10] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 2 Shap-e: GeneratarXiv preprint [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2 [12] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 798810, 2025. 2 [13] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compoarXiv preprint sitional arXiv:2506.05573, 2025. 3, 5, 6, 7 latent diffusion transformers. [14] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [4] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and [15] Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, 3 [28] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 5 [29] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 59165926, 2025. 2 [30] Junsheng Zhou, Yu-Shen Liu, and Zhizhong Han. Zero-shot scene reconstruction from single images with deep prior assembly. Advances in Neural Information Processing Systems, 37:3910439127, 2024. 2 [31] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024. Scenethesis: language and vision and Zhaoshuo Li. agentic framework for 3d scene generation. arXiv preprint arXiv:2505.02836, 2025. 2, 5 [16] Leopold Maillard, Nicolas Sereyjol-Garros, Tom Durand, and Maks Ovsjanikov. Debara: Denoising-based 3d room arrangement generation. Advances in Neural Information Processing Systems, 37:109202109232, 2024. 2 [17] Yanxu Meng, Haoning Wu, Ya Zhang, and Weidi Xie. Scenegen: Single-image 3d scene generation in one feedforward pass. arXiv preprint arXiv:2508.15769, 2025. 1, 3, 5, 6, 7 [18] Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:1201312026, 2021. 2 [19] Manuel-Andreas Schneider, Lukas Hollein, and Matthias Nießner. Worldexplorer: Towards generating fully navigable 3d scenes. In SIGGRAPH Asia 2025 Conference Papers, pages 111, 2025. 2 [20] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2050720518, 2024. 1, 2 [21] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. [22] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner. Sceneformer: Indoor scene generation with transformers. In 2021 International Conference on 3D Vision (3DV), pages 106115. IEEE, 2021. 2 [23] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 1, 3 [24] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1626216272, 2024. 1, 2 [25] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1622716237, 2024. 2 [26] Yandan Yang, Baoxiong Jia, Shujie Zhang, and Siyuan Huang. Sceneweaver: All-in-one 3d scene synthesis with arXiv preprint an extensible and self-reflective agent. arXiv:2509.20414, 2025. 2 [27] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an rgb image. ACM Transactions on Graphics (TOG), 44(4): 119, 2025. 2 Figure 6. We present the multi-view evaluation results using real-world /stylized image as input (example 1). The testing scenes contain various layouts including front, back, right, left, small on large, behind, and etc. Baselines includes PartCfrater [13], Gen3DSR [1], MIDI3D [8], and SceneGen [17] 5 Figure 7. We present the multi-view evaluation results using real-world /stylized image as input (example 2). The testing scenes contain various layouts including front, back, right, left, small on large, behind, and etc. Baselines includes PartCfrater [13], Gen3DSR [1], MIDI3D [8], and SceneGen [17] 6 Figure 8. We present the multi-view evaluation results using synthetic scenes as input. The testing scenes contain various layouts including front, back, right, left, small on large, behind, and etc. Baselines includes PartCfrater [13], Gen3DSR [1], MIDI-3D [8], and SceneGen [17] Figure 9. Ablation study on scene-context attention (SCA), view-centric space (VC), and non-semantic scenes (NS). With only SCA, the model often fails to maintain coherent global layout and produces frequent object collisions. Adding the VC component substantially improves the overall arrangement of objects, but instance quality remains limited, as seen in the distorted cat, cow, and the chair under the toy bear. Incorporating NS further enhances both instance fidelity and global layout coherence. 8 Figure 10. instance quality is bad when the input instance mask is small in the image."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "Purdue University"
    ]
}