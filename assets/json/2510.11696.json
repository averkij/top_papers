{
    "paper_title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
    "authors": [
        "Wei Huang",
        "Yi Ge",
        "Shuai Yang",
        "Yicheng Xiao",
        "Huizi Mao",
        "Yujun Lin",
        "Hanrong Ye",
        "Sifei Liu",
        "Ka Chun Cheung",
        "Hongxu Yin",
        "Yao Lu",
        "Xiaojuan Qi",
        "Song Han",
        "Yukang Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs."
        },
        {
            "title": "Start",
            "content": "QERL: ENHANCED REINFORCEMENT LEARNING FOR LLMS BEYOND EFFICIENCY QUANTIZATIONWei Huang1,3 Yi Ge2,4 Hanrong Ye1 Shuai Yang1 Yicheng Xiao4 Huizi Mao1 Yujun Lin1 Sifei Liu1 Ka Chun Cheung1 Hongxu Yin1 Yao Lu1 Song Han1,2 Yukang Chen1 Xiaojuan Qi 5 2 0 2 3 1 ] . [ 1 6 9 6 1 1 . 0 1 5 2 : r 1NVIDIA 2MIT 3HKU 4THU https://github.com/NVlabs/QeRL"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose QeRL, Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 speedup in the rollout phase. Moreover, this is the first framework to enable RL training of 32B LLM on single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs. Figure 1: Rollout speedup and accuracy of QeRL on Qwen2.5-7B-Instruct. QeRL achieves faster RL rollout and end-to-end training speeds (batch=8), while delivering performance superior to vanilla LoRA and QLoRA, also comparable to full-parameter RL on mathematical benchmarks."
        },
        {
            "title": "INTRODUCTION",
            "content": "The ability to perform multi-step reasoning is critical for large language models (LLMs) to handle complex tasks, from theoretical problem solving to practical decision making (Sui et al., 2025; Xu et al., 2025; Chu et al., 2025; Yang et al., 2021). Supervised fine-tuning (SFT) is common method to improve reasoning by training models to replicate explicit reasoning steps (Huang et al., 2024d; Min et al., 2024). However, this approach risks promoting imitation rather than encouraging genuine reasoning. In contrast, reinforcement learning (RL) uses verifiable reward signals to support adaptive learning, allowing models to explore diverse reasoning traces and identify more robust solutions (Lambert et al., 2024; DeepSeek-AI, 2025; Chen et al., 2025a). 1 Figure 2: The illustration of QeRL. (a) RL via LoRA: reducing trainable parameters, but does not alleviate the rollout bottleneck. (b) RL via QLoRA: NF4 quantization with LoRA, but NF4 is slower than LoRA. (c) QeRL: NVFP4 quantization with LoRA, reducing memory and enabling faster RL while matching full-parameter finetuning performance with adaptive quantization noise. AQN dynamically adjusts quantization noise with an exponential scheduler, enhancing exploration. RL is effective for LLMs reasoning but highly resource-intensive. RL requires substantial GPU memory, as multiple models, such as policy and reference models in GRPO (Shao et al., 2024), must run concurrently. The large size of reasoning-focused LLMs (DeepSeek-AI, 2025) further exacerbates memory demands. Training is also slowed by multistage processes, including rollouts, reward computation, logit evaluation, and gradient updates. Rollouts are particularly costly, involving repeated sampling and processing of long sequences for complex tasks (Yu et al., 2025). Additionally, RLs inherent sample inefficiency (Hassani et al., 2024) further increases costs. Improving RL efficiency in LLMs presents significant challenges. One approach, exemplified by Tina (Wang et al., 2025), leverages parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) (Hu et al., 2022) to reduce trainable parameters. However, similar to LoRA in SFT (Chen et al., 2024b), these methods fail to address the core issue of slow rollout speeds. Another strategy, demonstrated by FlashRL (Liu et al., 2025a), uses quantized rollout models to reduce computational costs. However, precision mismatches between the rollout model and logits model (e.g., 8-bit vs. 16-bit) require importance sampling to correct discrepancies, necessitating both 8-bit and 16-bit models to run simultaneously, which increases memory usage. To overcome these limitations, we focus on lower-bit quantization while avoiding duplicate models in memory. Additionally, using QLoRA (Dettmers et al., 2023a) in RL slows rollouts by 1.52, further reducing efficiency. This slowdown occurs because QLoRA relies on NormalFloat 4-bit (NF4) precision, which requires unpacking and mapping to floating-point values via lookup table before matrix multiplication. To address the limitations of NF4 in QLoRA, natural solution is to adopt higher-performance quantization. However, standard quantization methods introduce static and deterministic noise, which is non-beneficial to the later-stage RL training. To avoid this drawback, our analysis surprisingly reveals that quantization noise, with precise control, can benefit RL by increasing policy entropy  (Fig.3)  . This added entropy enhances exploration by introducing uncertainty, similar to the effect of parameter noise in RL (Plappert et al., 2017; Pang & Jiang, 2021), and helps models discover better strategies (Cui et al., 2025). Our experiments show that well-designed noise strategy allows quantized LLMs to exploit this effect, reducing memory overhead while gaining better reward curves. This finding contrasts with results from SFT of LLMs (Dettmers et al., 2023a; Guo et al., 2023), demonstrating that controllable quantization noise in RL enhances exploration and enables quantized frameworks to surpass 16-bit LoRA in both efficiency and performance. We propose QeRL, quantization-based RL framework designed to train LLMs on reasoning tasks. As shown in Fig.2, QeRL uses NVFP4 quantization for LLM weights and integrates Marlinbased (Frantar et al., 2024) approach in both rollout and prefilling stages. This design accelerates rollout and prefilling without sacrificing accuracy, with gradient backpropagation enabled through LoRA layers. To address static quantization noise, we introduce adaptive quantization noise (AQN), 2 Figure 3: Advancement of Quantization in RL Exploration. Quantization noise brings higher initialized entropy, which encourages exploration in RL training, accelerating the increase of reward. which injects channel-wise random noise during training and adjusts exploration noise dynamically using an exponential schedule. Additionally, we implement noise-sharing strategy that merges the noise vector into the layer normalization layer, enabling zero-parameter overhead for noise injection. Compared to vanilla LoRA, QeRL achieves faster rollout and better reward growth. For example, as shown in Fig.1, QeRL outperforms QLoRA and vanilla LoRA in rollout and prefilling speeds on the Qwen2.5-7B-Instruct model, achieving GSM8K score of 90.8surpassing both 16-bit LoRA and QLoRA while matching full fine-tuning accuracy on MATH 500. QeRL outperforms vanilla LoRA and QLoRA in both training speed and reward performance. Notably, it achieves approximately 1.8 speedup in end-to-end training, compared to QLoRA. Additionally, QeRL demonstrates the capability to train 32B model with GRPO on single H100 80GB GPU."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Model Quantization Integer quantization requires mapping float-point weights distributed within the interval [Wmin, Wmax] to an integer range of 2N , where is the target bit-width. Given tensor Rdk, this process is defined as: sw Wmax Wmin qmax = Round( ), sw = (1) where represents the quantized weight matrix, sW is the scaling factor, and qmax defines the compressed range. For integer quantization, qmax = 2N 1. In contrast, for the floating-point quantization, such as FP4 format, qmax = 6, achieved using 1-bit mantissa and 2-bit exponent (E2M1). 4-bit NormalFloat (NF4) is new data type (Dettmers et al., 2023a), designed for normally distributed weights. Recently, the latest Blackwell GPU architecture (NVIDIA, 2024) introduces hardware support for the advanced FP4 format, MXFP4 (Project, 2023) and NVFP4 (NVIDIA, 2024). MXFP4 adopts shared FP8 (E8M0) scaling factor across parameter blocks of 32 elements, while NVFP4 employs an FP8 (E4M3) scaling factor with smaller parameter blocks of 16 elements, enabling finer-grained scaling adjustments compared to MXFP4. Both formats are seamlessly integrated into NVIDIAs Hopper (NVIDIA, 2023) and Blackwell (NVIDIA, 2024) GPUs. Low-rank Adaptation LoRA (Hu et al., 2022) is motivated by the observation that weight updates in large pre-trained models often lie in low-dimensional subspace. Instead of directly fine-tuning all parameters, LoRA introduces low-rank decomposition to model these updates efficiently: + = + BA (2) where Rdr and Rrk, with the rank min(d, k). In this setup, the original weight matrix is kept frozen, and only the low-rank matrices and are optimized during training. This formulation drastically reduces the number of trainable parameters and lowers both memory and computational cost, while retaining the expressivity required for domain adaptation. Within selfattention modules, LoRA is generally applied to the attention and feed-forward projection matrices (Wq, Wk, Wv, Wo, Wgate, Wup, Wdown), as these layers are the most critical in LLMs. Other related works are discussed in Appendix D."
        },
        {
            "title": "3 METHOD",
            "content": "Our experiments reveal that quantized LLMs can significantly enhance exploration in RL. Applying parameter-efficient fine-tuning (PEFT) to quantized models not only reduces training resource consumption but also outperforms vanilla LoRA in reward growth and evaluation scores  (Fig.2)  . This 3 challenges the conventional view in SFT that quantization degrades training effectiveness(Dettmers et al., 2023a; Guo et al., 2023). Notably, we observe that quantization error functions similarly to random noise in networks (Plappert et al., 2017; Eberhard et al., 2023; Osband et al., 2016), promoting broader exploration of potential actions or tokens in RL by increasing entropy  (Fig.3)  ."
        },
        {
            "title": "3.1 TRAINING FRAMEWORK OF QERL",
            "content": "QeRL is based on the mainstream policy optimization algorithms of LLMs, such as GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025). Group Relative Policy Optimization (Shao et al., 2024) is designed based on the Generalized Advantage Estimation (GAE) (Schulman et al., 2015), eliminating the need for separately trained reward model, as required in Proximal Policy Optimization (PPO) (Engstrom et al., 2019; Schulman et al., 2017). Instead, for given input query q, multiple samples are generated, resulting in set of candidate outputs {o1, o2, ..., oG}. These candidates are evaluated using rule-based reward, and the average reward is used for updates. The optimization objective is defined as follows: (θ) = Eq,{oi}["
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) (min( t= πθ(oi,tq) πθold(oi,tq) Ai,t, clip( πθ(oi,tq) πθold(oi,tq) , 1 α, 1 + α)Ai,t) βDKL(πθπref ))] (3) where πθ and πref denote the policy model and reference model, respectively, and the clipping range (1 α, 1 + α) stabilized the gradient steps of the policy model. KL penalty is used in GRPO to avoid the unexpected large change in updating (Schulman et al., 2017). Ai,i is the antagonist of ith completion, shared across all tokens in ot, defined as: Ai = ri mean({r1, r2, ..., rG}) std({r1, r2, ..., rG}) (4) Dynamic Sampling Policy Optimization (Yu et al., 2025) suggests higher clipping upper-bond can help avoid entropy collapse. Another improvement in DAPO is to utilize the loss of token-level policy gradients. In DAPO, the KL penalty from Eq.3 is removed to eliminate the upper limit on exploration in RL, thereby encouraging more optional tokens in the rollout process. 3.2 QUANTIZATION ENCOURAGES EXPLORATION To understand how quantization enhances RL, we analyze its effect on the models sampling behavior. Our central finding is that the noise introduced by quantization serves as an implicit exploration mechanism, similar to explicit noise injection techniques in the parameter and action space (Plappert et al., 2017; Eberhard et al., 2023; Fortunato et al., 2018; Liu et al., 2025b). study 3 different quantization for2021). on GSM8K (Cobbe al., et of FP4 and NF4) Quantization Improves Sampling Entropy We mats (NVPF4, MXFP4, Our empirical study on Qwen2.5-7B-Instruct (Team, 2024) reveals an intriguing finding: when applying PEFTbased RL, models quantized to 4-bit precision consistently outperform their 16-bit counterparts. This advantage is evident across two key metrics: significantly faster reward convergence during training and higher adjusted evaluation scores. As shown in Fig.4, the reward curves of the models exhibit steeper upward trend compared to 16-bit models, with convergence patterns closely resembling those of full-parameter fine-tuning in both DAPO and GRPO. Also, NVFP4 and MXFP4 both show better reward growth than NF4. Figure 5: Comparison of RL entropy. This unexpected performance improvement prompted us to investigate the underlying mechanism. We discover that quantization inherently increases the sampling entropy, H(π(q)) = (cid:80) otV π(otq) log π(otq), where is the vocabulary) of the policy during deployment (shown in Fig.5). During the forward pass, quantized model introduces small but systematic errors, which 4 Figure 4: Training reward performance. The upper figures illustrate the training rewards under DAPO, while the lower one is GRPO. Although MXFP4 achieves higher scores in the early stages of training, NVFP4 ultimately converges to better final rewards. LoRA rank is set to 32. can be modeled as static network noise (Fan et al., 2020). This noise propagates across the network layers, perturbing the final logits before the softmax function is applied. Consequently, the output probability distribution over the vocabulary, denoted as πθ(q), becomes flatter, with less pronounced peaks. This increase in sampling entropy plays crucial role in reinforcement learning by encouraging exploration (Cheng et al., 2025; Eysenbach & Levine, 2021). It mitigates the models overconfidence in single optimal token and instead assigns more meaningful probabilities to wider range of plausible next actions  (Fig.3)  . The entropy of other model is provided in Appendix H. Quantization Noise Functionally, this effect resembles exploration in parameters (Eberhard et al., 2023; Plappert et al., 2017), which deliberately injects noise into parameters to drive exploration: (θ + θlora) (θ + θlora) = Q(θ) θ = ϵ (5) where Q(θ) denotes the de-quantized weight, and ϵ is the quantization noise. Such exploratory noise emerges naturally as computationally free byproduct of compressing model representations. This contrasts starkly with SFT, where noise is often detrimental because the objective is to faithfully imitate the true data distribution rather than to discover novel high-reward outputs. key limitation of quantization errors is their deterministic nature, which fails to align with the dynamic exploration-exploitation trade-off required in RL. Unlike stochastic noise in traditional RL (Plappert et al., 2017; Osband et al., 2016), which is randomly sampled and independently applied at different training stages, quantization noise remains static throughout the process, lacking the adaptability needed to enhance exploration at critical phases. 3.3 ADAPTIVE QUANTIZATION NOISE IN PARAMETER SPACE To transform static quantization noise into dynamic exploration mechanism, we introduce an Adaptive Quantization Noise (AQN) technique. The core idea is to introduce small set of structured modulation vectors that slightly perturb the otherwise static quantization noise. In our approach, we utilize an advanced quantization format, NVFP4. NVFP4 Quantization NVFP4 represents weights using dual-scaling mechanism: coarse, pertensor global scaling factor in FP32, SFP32, and fine-grained tensor of block-wise FP8 (E4M3) scalers, SE4M3. The dequantization of 4-bit to the high-precision ˆW follows: ˆW = Dequant( W) = SFP32 (SE4M3 W) (6) 5 where denotes block-wise scalar multiplication, broadcasting each scaler in SE4M3 to its corresponding block of 4-bit weights in W. The quantization noise of each weight matrix, ϵ = ˆWW, is the difference between this reconstructed tensor and the original full-precision tensor W. Adaptive Quantization Noise We introduce noise vector to the static quantized weight. Specifically, for each quantized linear layer, we sample stochastic noise vector, Znoisy R1d, where is the input dimension of the layer. This vector is not fixed but is resampled for each forward pass. We define it as: Znoisy = ϵ, ϵ (0, σ2I), where σ is hyperparameter in different training stage governing the noise scale, and ϵ is random vector whose elements are drawn independently from standard Gaussian distribution (Plappert et al., 2017). Then the additive noise is defined as: ϵ = Znoisy + ϵ = Znoisy + (cid:16) ˆW (cid:17) (7) where ϵ is equivalent to the dynamic noise of each weight matrix. In our setting, we freeze the main branch weight and update the low-rank matrix during RL. The and ˆW are consistent values. In the early stages, we leverage the inherent quantization noise to enhance the models exploration capabilities. As training progresses, σ gradually reduces following an exponential decay scheduler: σ(k) = σstart (cid:19) k1 K1 (cid:18) σend σstart (8) where σstart and σend represent the initial and final noise levels, is the current stage, and is the total interval, which are evenly divided in the training steps (more scheduler comparison in Sec.4.2). For instance, our experiments in GSM8K with total of around 600 training steps, noise is injected at 10 evenly spaced intervals, initialized with quantization noise, then from σstart to σend. This approach aims to balance exploration and exploitation (Fox et al., 2015). Noise Merging While introducing noise vector enables dynamic control over quantization noise, explicitly creating separate vector for each quantized layer is not feasible. First, it imposes burden on parameter efficiency, increasing memory overhead. Moreover, highprecision noise cannot be directly added to quantized weights, as this would break the compatibility of our inference kernel designed for NVFP4 BF16 operations. We propose simple solution that integrates this noise vector directly into the layer normalization parameters of LLM architectures. Figure 6: Deployment scheme of adaptive quantization noise in LLMs. Znoise is integrated in LayerNorm (e.g., RMSNorm) of each block in LLMs. (cid:16) Znoisy + ˆW (cid:17) = Znoisy + ˆW (9) By exploiting this equivalency in Eq.9, we subsume the role of Znoisy into the learnable weight parameter of the LayerNorm operation (e.g. RMSNorm (Zhang & Sennrich, 2019)) that typically follows the scaling after normalization. RMSNormnoise(x) = wnoise (cid:113) 1 (cid:80)N i=1 x2 + δ , wnoise = Znoise + (10) where represents the scaling factor of RMSNorm. In this configuration, channel-wise additive noise Znoisy transfers to row-wise multiplicative noise Znoise + of weight (proof provided in Appendix G). Multiplicative noise has been shown to be effective in RL (Pang & Jiang, 2021; Zhang et al., 2025a). Due to the higher sensitivity of RL to multiplicative noise, we initialize the noise level with σstart = 1e-2 to ensure stability. This approach extends adaptive quantization noise to the layer parameters Wq, Wk, Wv, Wgate, and Wup within each block, as these layers directly interact with normalized activations. To align with LLM architectures (Team, 2024; Grattafiori et al., 2024), Wq, Wk, and Wv share the same RMSNorm, while Wgate and Wup share another (as shown in Fig.6). 6 (a) Performance of Qwen2.5-3B-Instruct. (b) Performance of Qwen2.5-7B-Instruct. Model W# Training GSM8K Model W# Training GSM8K Qwen2.5-3B -Instruct BF16 NF4 MXFP4 NVFP4 BF16 BF16 NF4 MXFP4 NVFP - - - Full LoRA LoRA LoRA LoRA +AQN 61.2 57.53.7 59.81.4 59.41.8 84.4+23.2 76.1+14.9 76.1+14.9 73.4+12.2 83.3+22.2 83.7+22.6 Qwen2.5-7B -Instruct BF16 NF4 MXFP4 NVFP4 BF16 BF16 NF4 MXFP4 NVFP4 - - - - Full LoRA LoRA LoRA LoRA +AQN 76.3 70.55.8 71.35.0 73.42.9 91.2+14.9 88.1+11.8 85.0+8.7 86.4+10.1 88.5+12.2 90.8+13. Table 1: Qwen2.5 Performance on GSM8K. GRPO algorithm is used to train 3B and 7B models on GSM8K dataset, while Full denotes the full-parameter training and W# represents the bit-width and data format of weight. + and - are compared with original bfloat-16 (BF16) models. Figure 7: Training reward of 7/14B models. Figure 8: Ablation of AQN on 3/7B model."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENT SETTINGS RL Training We conducted training experiments using DAPO (Yu et al., 2025) and GRPO (Shao et al., 2024) on two prominent mathematical reasoning datasets: GSM8K (Cobbe et al., 2021) and BigMath (Albalak et al., 2025). GSM8K comprises 7,500 samples with generation number of 8, while BigMath includes 122,000 samples with generation number of 16. Both datasets feature problems of medium to high difficulty, spanning levels 3 to 5. For GSM8K, we trained 3B and 7B models, whereas for BigMath, we trained 7B, 14B, and 32B models. Specifically, the 7B and 14B models were trained on problems ranging from levels 3 to 5, while the 32B model was exclusively trained on the more challenging level 45 problems. Training checkpoints were evaluated between 500 and 1000 steps. To account for the sensitivity of Znoise perturbation, we set its range from 5e-2 to 5e-4 for dynamic noise estimation. In the main experiments, the LoRA rank is fixed at 32. The speedup tests are performed on single H100 GPU, while the final evaluated model is trained using 8 H100 GPUs to ensure experimental efficiency on such large-scale data. Detailed hyperparameters and deployment of QeRL are provided in Appendix and Appendix F. Backbone Models We conduct experiments on Qwen2.5 (Team, 2024) series, using basic without any mathematic data fine-tuning. For weight-only quantization, we applied AWQ (Lin et al., 2024) to MXFP4 and NVFP4 formats. The calibration dataset included 256 sequences, each 2048 tokens long, sampled from OpenThoughts-114k (Guha et al., 2025). Weight-only formats also support inference acceleration on NVIDIA-H100 GPUs with the Marlin kernel (Frantar et al., 2024). For NF4 quantization, we used the default configuration (Dettmers et al., 2023a). Evaluation Benchmarks and Metrics We focus on several widely used mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), AIME 2024/2025 (Li et al., 2024), and AMC 23 (Li et al., 2024), for evaluation. During inference, we use temperature of 0.6, completion length of 4096, and top-p sampling with = 0.95. Each data set is evaluated multiple times, and we report primarily the average accuracy of one sample (Pass@1). 7 Model W# Training MATH 500 AIME 24 AIME 25 AMC 23 Average 7B 14B 32B BF16 NVFP4 BF16 BF16 NVFP4 BF16 NVFP4 BF16 BF16 NVFP BF16 NVFP4 BF16 BF16 NVFP4 - - Full LoRA LoRA +AQN - - Full LoRA LoRA +AQN - - Full LoRA LoRA +AQN 74.8 73.71.3 77.4+2.6 77.0+2.2 76.8+2.0 77.4+2.6 78.6 76.42.2 83.2+4.6 81.0+2.4 79.4+0.8 80.2+1. 81.4 80.60.8 84.0+2.6 83.6+2.2 81.6+0.2 83.3+1.9 9.2 8.30.9 16.7+7.5 13.3+4.1 13.7+4.5 15.5+6.3 11.3 11.20.1 20.0+8.7 14.0+3.7 16.7+5.4 17.5+6.2 14.0 11.32.7 20.0+6.0 16.7+3.7 16.7+3.7 16.7+3.7 6.6 3.33.3 10.0+3.4 10.0+3.4 10.0+3.4 10.0+3.4 9.2 8.30.9 15.1+5.9 13.3+4.1 13.3+4.1 12.6+3. 10.8 10.00.8 23.3+12.5 13.3+2.5 15.0+4.2 19.2+8.4 25.0 17.57.5 45.0+20.0 42.5+17.5 47.5+22.5 42.5+17.5 45.0 40.05.0 55.0+10.0 52.5+7.5 52.5+7.5 57.5+12.5 52.5 45.07.5 57.5+5.0 55.0+2.5 52.5+0.0 63.3+10.8 28.9 25.73.2 37.3+8.4 35.7+6.8 37.0+8.1 36.4+7.5 36.0 34.02.0 43.3+7.3 40.2+4.2 40.5+4.5 42.0+6. 39.7 36.73.0 46.2+6.5 42.2+2.3 41.4+1.7 45.6+5.9 Table 2: Performance across four benchmarks. DAPO algorithm is used to train Qwen2.5-7/14/32BInstruction models on BigMath dataset, while Full denotes the full-parameter training. Figure 9: Comparison of noise schedulers. Figure 10: Ablation of LoRA rank. 4.2 EXPERIMENT RESULTS Reasoning Performance As shown in Tab.1, we report the GSM8k training results of the 3B and 7B models using GRPO. While quantized models exhibit performance degradation compared to BF16, applying PEFT with RL to the 3B model demonstrates that NVFP4 combined with AQN achieves performance of 83.7 from 59.4, surpassing the 76.1 achieved by 16-bit PEFT training and falling only 0.7 points below full-parameter training. Similarly, for the 7B model, our method outperforms 16-bit LoRA by 1.7 points. Furthermore, compared to QLoRA, our approach improves average accuracy by 7.6 and 5.8 points for the 3B and 7B models, respectively. Tab.2 presents the results on the BigMath dataset for the 7B, 14B, and 32B models trained with DAPO. Across all datasets, QeRL consistently matches or exceeds the performance of 16-bit models trained with LoRA. Notably, QeRL trains only about 1% of the parameters required for full-parameter training while using just 40%50% of the GPU memory of vanilla LoRA. For the 7B model, QeRL improves the average score from 25.7 (quantized) to 36.4, compared to 35.7 with vanilla LoRA. Similar trends are observed in the 14B and 32B models, where QeRL consistently outperforms vanilla LoRA across benchmarks, further supporting the conclusion that quantization enhances RL. Remarkably, on the AMC 23 dataset, the 14B model with QeRL achieves 57.5, exceeding 55.0 of full-parameter training. Reward Visualization In Sec.3.2, we compare the accuracy rewards of quantized LoRA, vanilla LoRA, and full-parameter training under GRPO and DAPO. Fig.7 presents the accuracy reward curves for the 7B and 14B models on the challenging BigMath dataset. Notably, QeRL achieves rapid reward increase within 200 steps, while vanilla LoRA requires over 500 steps (Appendix H) to show improvement. This finding highlights that the inherent noise introduced by quantized LLMs enhances exploration in RL, enabling faster reward growth and higher reward targets. 8 Model Method W# Model Size Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct"
        },
        {
            "title": "LoRA\nQLoRA\nQeRL",
            "content": "BF16 NF4 NVFP4 BF16 NF4 NVFP"
        },
        {
            "title": "29.6 GB\n10.2 GB\n10.6 GB",
            "content": "Training Speedup (Batch Size) 2 - 4 - 0.8 0.8 1.5 1.4 - - 0.9 0.7 1.4 1.2 8 - 0.7 1.2 - 0.7 1.2 Table 3: Memory Saving and Speedup of 7B and 14B models. We report the end-to-end speedup in the GRPO process of each training step. Each input has length of 256 tokens, and each max completion length is 2048. More results of other models are shown in Appendix J. Figure 11: Rollout throughput of 14/32B model. The setting is aligned with Tab. 7 (batch is 1). Noise Decay Schedule Fig.9 compares the performance of different noise decay functions for the 3B model: linear, exponential, cosine, and logarithmic decay. While their performance differences are negligible in the early training stages, exponential decay achieves more stable improvements later by reducing noise to lower levels. The corresponding decay curves are provided in Appendix H. Ablation of AQN Using default quantized noise throughout the training limits the exploration in RL. To address this, we propose the AQN. As shown in Fig.8, when we start with the default quantized noise and periodically inject additional noise in later stages, the reward curve grows more steadily. Notably, when the reward approaches convergence, AQN effectively expands the models exploration space, enabling further improvements in reward. Ablation of LoRA Rank Fig.10 compares the reward curves of the 3B model during QeRL with different LoRA ranks. Specifically, ranks of 16, 32, 64, and 128 exhibit similar trends and reward growth rates, with rank 16 converging slightly faster, making it more economical choice. 4.3 MEMORY SAVING AND SPEEDUP Tab.3 compares the quantized model sizes and end-to-end RL training speedup of these PEFT methods, with all experiments conducted on single NVIDIA H100-80GB GPU (NVIDIA, 2023). For 7B and 14B models, both QLoRA (NF4) and QeRL (NVFP4, supported by the Marlin kernel (Frantar et al., 2024)) significantly reduce memory usage, shrinking the model sizes to 25%30% of their 16-bit counterparts. Due to the limitations of NF4 generation speed (Egashira et al., 2024), QLoRA slows to 0.70.8 across different batch sizes. In contrast, QeRL achieves 1.21.5 training speedups over vanilla LoRA, benefiting from the generation speed of long reasoning sequences. This efficiency is particularly evident in RL, where the computational demands of long-horizon rollouts emphasize QeRLs advantage. Notably, our speedup measurements are based on the average speed during the first 30 steps, where the output token length is relatively short. In later stages of training, as the model generates longer outputs, the speed advantage of QeRL becomes even more pronounced. Its dual benefits in memory efficiency and training speed make QeRL highly effective for end-to-end RL workflows, especially in scenarios requiring extensive rollouts. Fig.11 shows rollout performance across various LoRA ranks, with QeRL achieving over 2 speedups on 14B and 32B models. More efficiency comparisons for other models and settings are in Appendix J."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper presents QeRL, an efficient training framework for RL on LLMs, which integrates NVFP4 precision quantization with LoRA fine-tuning. The framework is based on the novel observation that quantization can enhance exploration during RL, contrary to findings in SFT. Quantized LLMs not only surpass vanilla 16-bit LoRA training but also approach full-parameter fine-tuning performance. To address the static nature of quantization noise, we introduce an AQN mechanism, which dynamically adjusts noise during training to enhance RL stability. Extensive experiments show that QeRL significantly improves accuracy across models of various sizes compared to both 16-bit LoRA and QLoRA. Additionally, with NVFP4 kernel support, QeRL achieves round 1.5 speedup in end-to-end RL training while drastically reducing memory usage."
        },
        {
            "title": "REFERENCES",
            "content": "Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR, abs/2203.03131, 2022. Roberto Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, and Dan Alistarh. Quartet: Native fp4 training can be optimal for large language models. arXiv preprint arXiv:2505.14669, 2025. Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062, 2024a. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In ICLR, 2024b. Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025a. Zaiwei Chen, Siva Theja Maguluri, and Martin Zubeldia. Concentration of contractive stochastic approximation: Additive and multiplicative noise. The Annals of Applied Probability, 35(2): 12981352, 2025b. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Brian Chmiel, Maxim Fishman, Ron Banner, and Daniel Soudry. Fp4 all the way: Fully quantized training of llms. arXiv preprint arXiv:2505.19115, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. SFT memorizes, RL generalizes: comparative study of foundation model post-training. CoRR, abs/2501.17161, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. 10 DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. NeurIPS, 35:3031830332, 2022. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In NeurIPS, 2023a. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023b. Onno Eberhard, Jakob Hollenstein, Cristina Pinneri, and Georg Martius. Pink noise is all you need: Colored noise exploration in deep reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. Exploiting llm quantization. Advances in Neural Information Processing Systems, 37:4170941732, 2024. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters in deep rl: case study on ppo and trpo. In International conference on learning representations, 2019. Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257, 2021. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020. Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles In International Conference on Blundell, and Shane Legg. Noisy networks for exploration. Learning Representations, 2018. URL https://openreview.net/forum?id=rywHCPkAW. Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. Marlin: Mixed-precision auto-regressive parallel inference on large language models. arXiv preprint arXiv:2408.11743, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. Han Guo, Philip Greengard, Eric Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning. arXiv preprint arXiv:2311.12023, 2023. Hossein Hassani, Roozbeh Razavi-Far, Mehrdad Saif, and Liang Lin. Towards sample-efficiency and generalization of transfer and inverse reinforcement learning: comprehensive literature review. CoRR, abs/2411.10268, 2024. Juan Camilo Gamboa Higuera, David Meger, and Gregory Dudek. Synthesizing neural network controllers with probabilistic model-based reinforcement learning. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 25382544. IEEE, 2018. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, and Zhipeng Wang. Liger-kernel: Efficient triton kernels for LLM training. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. URL https://openreview.net/forum?id=36SjAIT42G. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li, Si Liu, and Xiaojuan Qi. Mixture compressor for mixture-of-experts llms gains more. arXiv preprint arXiv:2410.06270, 2024a. Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291, 2024b. Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Xianglong Liu, Luca Benini, Michele Magno, and Xiaojuan Qi. Slim-llm: Salience-driven mixed-precision quantization for large language models. arXiv preprint arXiv:2405.14917, 2024c. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey - part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? CoRR, abs/2411.16489, 2024d. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. Janghwan Lee, Jiwoong Park, Jinseok Kim, Yongjik Kim, Jungju Oh, Jinwook Oh, and Jungwook Choi. Amxfp4: Taming activation outliers with asymmetric microscaling floating-point for 4-bit llm inference. arXiv preprint arXiv:2411.09909, 2024. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), EMNLP, pp. 30453059, 2021. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 45824597, 2021. Baohao Liao and Christof Monz. Apiq: Finetuning of 2-bit quantized large language model. arXiv preprint arXiv:2402.05147, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6: 87100, 2024. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS, 2022. Liyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl: 8bit rollouts, full power rl, 2025a. URL https://fengyao.notion.site/flash-rl. 12 Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In ICML, 2024. Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025b. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and JiRong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. CoRR, abs/2412.09413, 2024. NVIDIA. Nvidia h100 tensor core GPU architecture overview. https://resources.nvidia.com/en-ustensor-core, 2023. NVIDIA. Nvidia blackwell architecture technical brief. https://resources.nvidia.com/ en-us-blackwell-architecture, 2024. Accessed: 2025-05-13. OpenAI. Introducing GPT-5. https://openai.com/index/introducing-gpt-5/, aug 2025. Accessed: 2025-09-21. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in neural information processing systems, 29, 2016. Bo Pang and Zhong-Ping Jiang. Robust reinforcement learning for stochastic linear quadratic control with multiplicative noise. Trends in Nonlinear and Adaptive Control: Tribute to Laurent Praly for his 65th Birthday, pp. 249277, 2021. Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017. Open Compute Project. Ocp microscaling formats (mx) specification version 1.0. https://www. Acopencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf, 2023. cessed: 2023-09-13. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially binarized large language models. arXiv preprint arXiv:2310.00034, 2023. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In EuroSys, pp. 12791297, 2025. 13 Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. Stop overthinking: survey on efficient reasoning for large language models. CoRR, abs/2503.16419, 2025. Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In NeurIPS, pp. 2419324205, 2021. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Albert Tseng, Tao Yu, and Youngsuk Park. Training llms with mxfp4. arXiv preprint arXiv:2502.20586, 2025. Shangshang Wang, Julian Asilis, Omer Faruk Akgul, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. Tina: Tiny reasoning models via lora. CoRR, abs/2504.15777, 2025. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models. CoRR, abs/2501.09686, 2025. Yi Yang, Yueting Zhuang, and Yunhe Pan. Multiple knowledge representation for big data artificial intelligence: framework, applications, and case studies. Frontiers of Information Technology & Electronic Engineering, 22(12):15511558, 2021. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, pp. 19, 2022. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. Hanfang Zhang, Bing-Chang Wang, and Ying Cao. Reinforcement learning solutions to stochasIEEE Transactions on Circuits and tic multi-agent graphical games with multiplicative noise. Systems I: Regular Papers, 2025a. Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025b. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025."
        },
        {
            "title": "A ETHICS STATEMENT",
            "content": "This work exclusively leverages publicly available open-source datasets that have been previously established and validated in academic research. No new text, video, or audio materials are generated or incorporated as part of this study. The datasets utilized are strictly intended for research purposes and are not employed for any commercial applications."
        },
        {
            "title": "B REPRODUCIBILITY STATEMENT",
            "content": "To ensure the research community can replicate our findings, this project will be released as opensource software. The methodology is described in detail in Sec.3, while Sec.4.1 and Appendix outline the complete training protocols and implementation details, including all hyperparameter settings."
        },
        {
            "title": "C USE OF LARGE LANGUAGE MODELS",
            "content": "During the preparation of this manuscript, we utilized large language modelsGPT-5 (OpenAI, 2025)exclusively to refine the language, focusing on improving grammar, flow, and tone at the sentence and paragraph levels. These tools were not employed to generate ideas, design experiments, or draw conclusions. All technical content, methodologies, and interpretations were independently written, thoroughly verified, and approved by the authors. To minimize the risk of factual inaccuracies or citation errors, every model-edited sentence underwent human review, and all references were carefully cross-checked with their primary sources. The authors accept full responsibility for ensuring the accuracy and integrity of this manuscript."
        },
        {
            "title": "D RELATED WORK",
            "content": "Reinforcement Learning for LLMs Recent efforts have focused on enhancing reasoning in LLMs using RL (Min et al., 2024; Chu et al., 2025). DeepSeekMath (Shao et al., 2024) improves mathematical reasoning by continuing pre-training on math-intensive data and introducing Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Building on this, DeepSeekR1 (DeepSeek-AI, 2025) demonstrates that RL alone can drive strong reasoning, achieving performance comparable to proprietary models with large-scale training. Complementary system-level contributions, such as DAPO (Yu et al., 2025), offer an open-source RL framework with decoupled optimization strategy, achieving competitive results through simplified training pipeline. GSPO (Zheng et al., 2025) stabilizes RL training and reduces variance through sequence-level optimization, proving effective in large-scale mixture-of-experts models. HybridFlow (Sheng et al., 2025) introduces flexible RLHF framework with hybrid control flow and 3D-HybridEngine. Together, these works demonstrate significant progress in advancing LLM reasoning with RL. Quantization for LLMs Quantization is key technique for compressing LLMs, improving efficiency by reducing parameter precision. The most common approach, Post-Training Quantization (PTQ) (Dettmers et al., 2022; Frantar et al., 2022; Xiao et al., 2023; Shao et al., 2023; Lin et al., 2024), transforms pre-trained models cost-effectively without retraining. Recent work has pushed quantization to ultra-low bit-widths while maintaining performance (Huang et al., 2024c; Dettmers et al., 2023b; Shang et al., 2023; Huang et al., 2024b; Liao & Monz, 2024; Tseng et al., 2024; Huang et al., 2024a), including advancements in Quantization Aware Training (QAT) to improve robustness (Liu et al., 2023; Chen et al., 2024a). Additionally, novel precision formats like NF4 (Dettmers et al., 2023a), FP4 (Tseng et al., 2025; Chmiel et al., 2025), and MXFP4 (Chmiel et al., 2025) enable accurate weight representation, achieving high compression with minimal or improved accuracy loss. NVFP4 (NVIDIA, 2024) is groundbreaking 4-bit floating-point format introduced with NVIDIAs Blackwell GPU architecture. This format expands on the idea of compact, low-bit micro floating-point representations, offering developers enhanced versatility by adding another flexible option for their projects (Zhang et al., 2025b; Castro et al., 2025; Lee et al., 2024). 15 Efficient Fine-tuning Efficient fine-tuning is pivotal for adapting LLMs with minimal computational cost. LoRA (Hu et al., 2022) pioneered this approach by adding low-rank adapters to frozen weight matrices. DoRA (Liu et al., 2024) improved upon this by decomposing weight updates into directional and magnitude components, addressing low-rank constraints and enhancing stability. QLoRA (Dettmers et al., 2023a) integrated LoRA with 4-bit quantization to further reduce resource usage, while LongLoRA (Chen et al., 2024b) introduced fine-tuning methods for longcontext processing. Tina (Wang et al., 2025) demonstrated that compact models could gain reasoning ability through RL with LoRA. Beyond the LoRA family (Hu et al., 2022), other efficient fine-tuning techniques include prompt tuning, prefix tuning, IA3, BitFit, Fisher-masked tuning, and input-tuning (Lester et al., 2021; Li & Liang, 2021; Liu et al., 2022; Zaken et al., 2022; Sung et al., 2021; An et al., 2022; Guo et al., 2023). These advancements underscore the importance of efficient fine-tuning for practical LLM adaptation."
        },
        {
            "title": "E EXPERIMENT HYPERPARAMETERS",
            "content": "Training Data and Reward Function We trained the Qwen2.5-3B-Instruct, Qwen2.5-7BInstruct, Qwen2.5-14B-Instruct, and Qwen2.5-32B-Instruct models, which are widely used for evaluating reasoning capabilities. Unlike other studies that rely on math-specialized models, we aim to evaluate training performance starting from general-purpose base models. Additionally, QeRL can be smoothly transferred to other model families, such as the Qwen3 series. For the GSM8K dataset, we primarily trained the Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct models using GRPO, while for the BigMath dataset, we focused on training the Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen2.5-32B-Instruct models using DAPO. Specifically, for the 7B and 14B models, we selected data with medium to high difficulty levels (grades 35), and for the 32B model, we used high-difficulty data (grades 45). For problem prompts, we append the suffix Solve the following math problem step by step. The reasoning process and direct answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>: <think> ... </think> <answer> ... </answer>. RL Training Configuration For both GRPO and DAPO, we use the hyperparameters in Tab.4, without using entropy or KL losses. For 4-bit training, the learning rate is set to 1e5. However, due to the fragile of the BF16 model with LoRA, the learning rate can not be larger than 5e6, or it will collapse in the late training stage. Hyperparameter Value Optimizer Policy learning rate Training batch size Samples per prompt Policy updates per rollout Max response length Rollout temperature Clip range ϵlow, ϵhigh Noise range Zstart, Zend AdamW-8bit 1e5 (QeRL, QLoRA) / 5e6 (LoRA) 128 8 (GSM8K) / 16 (BigMath) 4 (GSM8K, off-policy) / 1 (BigMath, on-policy) 4096 (GSM8K) / 8192 (BigMath) 1.0 0.2, 0.28 1e-2, 5eTable 4: Hyperparameters of GRPO and DAPO training"
        },
        {
            "title": "F DEPLOYMENT OF QERL",
            "content": "In Algorithm 1, we provide detailed explanation of how QeRL is deployed within the GRPO framework. During the steps in stage 0, the added noise σ is set to 0, where only quantization noise effects. At stage 1, σ is initialized to σstart, and by the final stage (K-1) σ gradually transitions to σstart. This progressive adjustment of noise ensures structured and controlled exploration process throughout the training stages, balancing stability and exploration effectively. 16 Algorithm 1 Deploy GRPO with QeRL and Adaptive Quantization Noise Input NVFP4 policy model π θ; reward function rϕ; task prompts D; hyperparameters; LoRA rank, LoRA alpha; number of stages K; σstart, σend; 1: policy model πθ π θ+θlora 2: for iteration = 1, . . . , do 3: 4: 5: 6: Divide total steps into equal stages: steps per stage = M/K Determine current stage k: = reference model πref πθ for step = 1, . . . , do step1 steps per stage 7: Set noise level σ 0 σstart if = 0 (cid:17) k1 K1 (cid:16) σend σstart otherwise (exponential decay) Sample batch Db from Update the old policy model with AQN: πθold πθ + (0, σ2) Sample outputs {oi}G Compute rewards {ri}G Compute ˆAi,t for the t-th token of oi through group relative advantage estimation. for GRPO iteration = 1, . . . , µ do i=1 πθold ( q) for each question Db i=1 for each sampled output oi by running rϕ Update the policy model πθ by maximizing the GRPO objective (Equation 3) 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Output πθ end for end for"
        },
        {
            "title": "G PROOF OF NOISE SHARING",
            "content": "In this section, we further demonstrate the effectiveness of the noise-sharing operation proposed in Eq.10, detailing the process by which additive noise is transformed into multiplicative noise. With AQN, input of each block follows: RMSNormnoise(X) = ( Znoise + I) RMSNorm(X), (11) where RMSNorm() denotes the vanilla RMSNorm operation and is the original scaling factor in RMSNorm(). The element-wise multiplication () will be auto-broadcast during computing. Then, the operation of the following linear computation is defined as: (( Znoise + I) RMSNorm(X)) ˆW = RMSNorm(X) (( Znoise + I) ˆW), (12) Thus, the additive Gaussian noise, when incorporated into the noise-sharing mechanism of LayerNorm, can be equivalently regarded as multiplicative Gaussian noise (denoted as ( Znoise + I)) and applied row-wise to the weight matrix ˆW. Since RMSNorm is only applied to the inputs of each attention block and feed-forward network (FFN) block, this mechanism ensures that the Q, K, and matrices in the attention block share the same noise, while the down and up layers in the FFN block also share single, identical noise set. This noise-injection strategy avoids disrupting the multiplication kernels of NVFP4 and BF16 in QeRL or introducing additional matrix multiplication operations. Both additive and multiplicative noise have been shown to positively contribute to exploration in RL (Plappert et al., 2017; Higuera et al., 2018; Chen et al., 2025b). However, multiplicative noise tends to be more sensitive, especially in deep networks like LLMs. To address this, we initialize the noise standard deviation (σ) to 1e-2, which is smaller than the typical 1e-1 used in traditional noise-based networks."
        },
        {
            "title": "H ADDITIONAL EXPERIMENTS OF TRAINING",
            "content": "Training Rewards of Different Model Fig.12 and Fig.13 further compare the performance of QeRL and 16-bit LoRA training on complex reasoning datasets. In Fig.12, we present the training 17 Figure 12: Training reward of 7B model. Figure 13: Training reward of 32B model. rewards of the Qwen2.5-7B-Instruct model on the BigMath dataset with difficulty levels ranging from 3 to 5, as an extension of Fig.7. Leveraging the exploration benefits of QeRL in quantized models, rapid increase in reward is observed after approximately 200 steps, whereas 16-bit LoRA requires over 500 steps to achieve similar rise. Meanwhile, as shown in Fig.13, we trained the Qwen2.5-32B-Instruct model on the highest difficulty data (levels 45). Although the difference in reward growth between QeRL and LoRA is less pronounced in the 32B model compared to the smaller 3B, 7B, and 14B models, QeRL still consistently performs better than LoRA. More Experiments of Entropy As an extension of Fig.5, Fig.14 illustrates the entropy curve of the Qwen2.514B-Instruct model at various training steps. Notably, the entropy of QeRL remains consistently higher than that of LoRA throughout the RL process, particularly during the initial steps. This observation highlights the advantage of QeRL in promoting exploration during RL, as higher entropy indicates broader search of the solution space. The increased exploratory capacity facilitated by quantization appears to enable the model to navigate complex environments more effectively, ultimately supporting improved optimization. These results further validate the role of quantization in enhancing the explorationexploitation balance in RL tasks. Figure 14: Entropy in RL steps. Noise Scheduler Fig.15 illustrates the noise scheduler employed in our experiments, showing four distinct decay strategies: linear, exponential, cosine, and logarithmic. The scheduler adjusts the noise level in 10 stages to guide the training process. The linear decay method reduces noise uniformly across stages, ensuring consistent rate of change. The exponential decay rapidly decreases the noise at the beginning and uses smaller noise scales in later stages, which we found effective for achieving stable and higher rewards in later stages of training. The cosine decay follows smooth oscillatory pattern, gradually reducing noise with cosine curve, whereas the logarithmic decay decreases noise sharply in early stages and stabilizes in later ones. Among these, we chose the exponential decay strategy due to its ability to maintain smaller noise scales during the later stages, resulting in more stable and higher reward curve. This flexibility in controlling noise levels plays critical role in balancing exploration and convergence during training. Figure 15: Noise curve of different schedulers."
        },
        {
            "title": "I ADDITIONAL ABLATION STUDY",
            "content": "Ablation of Learning Rate We examine the impact of learning rate variations on the performance of quantized models compared to 16-bit models. As illustrated in Fig.16 and Fig.17, with relatively small learning rate of 5e-6, QeRL marginally outperforms LoRA, achieving reward close to 0.95. 18 Figure 16: Ablation of learning rate in QeRL (Qwen2.5-7B-Instruct). Figure 17: Ablation of learning rate in LoRA (Qwen2.5-7B-Instruct). Method W# Model Size BS# Throughput (Tokens/s) E2E RL Speedup"
        },
        {
            "title": "Rollout Phase",
            "content": "Speedup w/o GC w/ GC"
        },
        {
            "title": "LoRA\nQeRL",
            "content": "BF16 NVFP4 BF16 NVFP"
        },
        {
            "title": "6.2 GB\n2.8 GB",
            "content": "2 2 8 8 151.2 157.0 2226.3 2271.4 - 1.0 - 1. - 1.1 - 1.1 - 1.0 - 1.1 Table 5: Memory Saving and Speedup of Qwen2.5-3B-Instruct Model. The table reports the throughput (tokens/s) for the rollout phase under two batch size settings (2 and 8). Each input has length of 256 tokens, and each max completion length is 2048. W# denotes the data format, BS# is the number of batch size, and E2E denotes the end-to-end speed of GRPO training. GC denotes gradient checkpointing. Method W# Model Size BS# Throughput (Tokens/s) E2E RL Speedup Rollout Phase Speedup w/o GC w/ GC LoRA QeRL LoRA QeRL BF16 NVFP4 BF16 NVFP4 15.2 GB 5.9 GB 15.2 GB 5.9 GB 2 2 8 8 115.4 151.6 1641.1 2091.8 - - 1.3 1.2 1.2 - - - 1.3 1.1 1.1 - Table 6: Memory Saving and Speedup of Qwen2.5-7B-Instruct Model. The table reports the throughput (tokens/s) for the rollout phase under two batch size settings (2 and 8). Each input has length of 256 tokens, and each max completion length is 2048. W# denotes the data format, BS# is the number of batch size, and E2E denotes the end-to-end speed of GRPO training. GC denotes gradient checkpointing. When the learning rate is increased to 3e-5, the larger update magnitude in the adapter results in faster reward growth and quicker model convergence. However, in 16-bit models, the excessive update magnitude leads to instability, often causing the training process to collapse. In contrast, QeRL demonstrates remarkable robustness to larger learning rates due to the presence of NVFP4 quantization noise, which helps stabilize updates. This robustness enables QeRL to maintain stable training even under high learning rates, achieving reward growth rate nearly twice as fast as the 16-bit model. These results underscore QeRLs superior adaptability and efficiency, particularly in challenging training scenarios with high learning rates."
        },
        {
            "title": "J MORE EFFICIENCY EXPERIMENTS",
            "content": "Tab.5, Tab.6, Tab.7, and Tab.8 provide additional speed benchmarks for the Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen2.5-32B-Instruct models, evaluated under batch sizes of 2 and 8. For the 3B and 7B models, we did not enable memory-efficient techniques such as gradient checkpointing (Chen et al., 2016) or Liger loss (Hsu et al., 2025) in order to maximize training speed. However, due to the substantial size of the 14B and 32B models and the computational overhead introduced by importance sampling with gradients during RL training, we 19 Method W# Model Size BS# Throughput (Tokens/s) E2E RL Speedup"
        },
        {
            "title": "Rollout Phase",
            "content": "Speedup w/o GC w/ GC"
        },
        {
            "title": "LoRA\nQeRL",
            "content": "BF16 NVFP4 BF16 NVFP"
        },
        {
            "title": "29.6 GB\n10.6 GB",
            "content": "2 2 8 8 65.4 95.3 737.2 1091.1 - - 1.3 1.4 1.4 - - 1.5 OOM OOM 1.3 - Table 7: Memory Saving and Speedup of Qwen2.5-14B-Instruct Model. The table reports the throughput (tokens/s) for the rollout phase under two batch size settings (2 and 8). Each input has length of 256 tokens, and each max completion length is 2048. W# denotes the data format, BS# is the number of batch size, and E2E denotes the end-to-end speed of GRPO training. GC denotes gradient checkpointing. Method W# Model Size BS# Throughput (Tokens/s) E2E RL Speedup Rollout Phase Speedup w/o GC w/ GC LoRA QeRL LoRA QeRL BF16 NVFP4 BF16 NVFP4 62.3 GB 20.7 GB 62.3 GB 20.7 GB 2 2 8 8 34.0 60.0 344.3 688.2 - 1. - 2.0 OOM OOM 10.6 s/step OOM OOM OOM 12.2 s/step OOM Table 8: Memory Saving and Speedup of Qwen2.5-32B-Instruct Model. The table reports the throughput (tokens/s) for the rollout phase under two batch size settings (2 and 8). Each input has length of 256 tokens, and each max completion length is 2048. W# denotes the data format, BS# is the number of batch size, and E2E denotes the end-to-end speed of GRPO training. GC denotes gradient checkpointing. Model BF16 (Tokens/s) Model NVFP4 (Tokens/s) Rank 16 Rank 32 Rank 64 Rank 16 Rank 32 Rank 3B 7B 14B 32B 151.2 115.4 65.4 34.0 148.8 113.2 63.1 33.3 138.6 108.3 61.2 31.9 3B 7B 14B 32B 157.0 151.6 95.3 58. 153.1 149.9 92.9 56.0 140.0 137.7 86.0 51.3 Table 9: Throughput under different LoRA ranks in the rollout stage. We test the tokens/s for each model in the vLLM engine, and the setting is aligned with Tab.7. We set the batch size as 1. employ gradient checkpoint to accelerate computation. For training on GPUs with smaller memory capacity, enabling gradient checkpointing is recommended to reduce memory usage, although this may come at the cost of slower overall training speed. During the rollout phase, the precision of NVFP4, optimized by the Marlin kernel (Frantar et al., 2024), demonstrates significant acceleration, achieving speeds of 1.0 to 2.0. In particular, performance gains become more pronounced as model size increases, with the 32B model achieving up to 2.0 speedup. This indicates that NVFP4s advantages are particularly impactful for large-scale models, where computational demands are higher. In end-to-end RL efficiency evaluation, we report the per-step latency of GRPO training, defined as the wall clock time to complete an optimization step including rollout generation, log-probability computation, and parameter updates. We benchmark with rollout batch sizes of 2 and 8 while fixing the maximum input length to 256 tokens and the maximum completion length to 2,048 tokens. For fairness, we match the vLLM memory budget between BF16 and NVFP4 variants by setting the same gpu memory utilization in the engine: 0.20 for Qwen2.5-3B-Instruct, 0.30 for 7B, 0.45 for 14B, and 0.40 for 32B (the latter to enable single-GPU training). Under these controlled settings, the E2E latency reductions mirror the rollout phase acceleration and become more pronounced as the model size grows, with the largest gains observed on Qwen2.5-14B-Instruct. Additionally, Tab.9 provides comparison of inference speeds between 16-bit and NVFP4 main models across various LoRA ranks. NVFP4 consistently outperforms 16-bit models in terms of speed at all adapter ranks, showcasing its ability to maintain efficiency across diverse configurations. However, as the rank increases, both NVFP4 and BF16 experience gradual decline in rollout speed within the vLLM engine, likely due to the increased computational overhead associated with higher ranks. Despite this, NVFP4 continues to demonstrate superior performance, highlighting its robustness and adaptability for both small-scale and large-scale setups. These findings underscore NVFP4s potential to optimize inference efficiency, particularly when combined with advanced kernels and varying adapter configurations."
        },
        {
            "title": "K LIMITATION ANALYSIS",
            "content": "We have demonstrated that our method, QeRL, achieves superior performance in RL training for LLMs compared to 16-bit vanilla LoRA training. Additionally, QeRL matches the accuracy of 16bit full-parameter reinforcement fine-tuning while delivering over 2 training speedup relative to both vanilla LoRA and QLoRA. However, since RL for LLMs inherently demands significantly greater computational resources than SFT, our experiments, conducted on model sizes ranging from 3B to 32B, do not yet establish whether QeRL can maintain the same level of performance for models exceeding 70B parameters, leaving that investigation for future work. Another limitation is that RL training often requires tens or even hundreds of hours, and while we have provided comprehensive evaluations on reasoning benchmarks such as GSM8K, MATH 500, AIME 24, AIME 25, and AMC 23, we did not extend our evaluations to other benchmarks or data types, such as code, or to general-purpose language tasks unrelated to reasoning. Nevertheless, our technique can be seamlessly adapted to richer and more diverse training datasets. We encourage the community to explore and apply this method to broader range of tasks in future research."
        }
    ],
    "affiliations": [
        "HKU",
        "MIT",
        "NVIDIA",
        "THU"
    ]
}