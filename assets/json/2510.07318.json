{
    "paper_title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "authors": [
        "Yunhao Fang",
        "Weihao Yu",
        "Shu Zhong",
        "Qinghao Ye",
        "Xuehan Xiong",
        "Lai Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 1 3 7 0 . 0 1 5 2 : r Artificial Hippocampus Networks for Efficient Long-Context Modeling Yunhao Fang, Weihao Yu,, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei"
        },
        {
            "title": "ByteDance Seed",
            "content": "Equation contribution, Corresponding author (a) (b) (a) Artificial Hippocampus Networks (AHNs) transform lossless memory into fixed-size compressed Figure 1 representations for efficient long-context modeling. Lossless memory (e.g., attentions KV cache) stores exact input information but grows with sequence length, leading to high cost for long sequences. In contrast, compressed memory (e.g., RNNs hidden state) maintains constant cache size and computational cost per input token, but inevitably loses details. In our framework, sliding window attention maintains exact recent context as lossless short-term memory, while AHN recurrently compresses out-of-window information into fixed-size state as compressed long-term memory. This allows the model to process long sequences efficiently, retaining both precise short-term information and compact summary of history. (b) On the long-context benchmark LV-Eval (128k sequence length), augmenting Qwen2.5-3B-Instruct with AHNs (+0.4% parameters) reduces FLOPs by 40.5% and memory cache by 74.0%, while improving average score from 4.41 to 5.88."
        },
        {
            "title": "Abstract",
            "content": "Long-sequence modeling faces fundamental trade-off between the efficiency of compressive fixedsize memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce memory framework of artificial neural networks. Our method maintains sliding window of the Transformers KV cache as lossless short-term memory, while learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. Correspondence: Weihao Yu at weihao.yu@bytedance.com Code: https://github.com/ByteDance-Seed/AHN Models: https://huggingface.co/ByteDance-Seed Work done while at ByteDance Seed."
        },
        {
            "title": "Instruction",
            "content": "Memory is the treasury and guardian of all things [15]. Inspired by the fundamental role of memory in intelligence, researchers have long sought to model this cognitive function in artificial systems. Early efforts centered on Recurrent Neural Networks (RNNs) [14, 23, 29, 33], where sequential information is encoded by continuously updated hidden states. Over time, diverse paradigms for memory representation emerged, including key-value (KV) caches in attention mechanisms [80], external memory modules in Neural Turing Machines and Memory Networks [27, 86], and external databases for retrieval-augmented models [43]. Among these, RNN-like and attention-based models have become the most widely used, each offering distinct advantages and limitations [48, 102]. RNN-like models compress all historical information into fixed-size hidden state, which can be treated as memory. At each step, they update the memory using the current input and the previous memory. This design ensures constant memory and computation per step, making them efficient for long sequences. However, compressing all information into fixed-size memory inevitably leads to information loss, especially in tasks that require precise long-range information recall [85]. To address the limitations of RNNs, attention mechanisms and the Transformer architecture are introduced [6, 54, 80]. In causal attention, the key-value cache functions as memory: for each input token, new key and value are generated and appended to the cache. Unlike RNNs, this memory is essentially lossless, as it retains all token-level information, thereby providing much higher memory capacity. The introduction of the Transformer quickly revolutionized sequence modeling, giving rise to series of powerful models [11, 20, 59, 66, 67]. Yet, the lossless nature of KV cache is double-edged sword: while it enables powerful memory retention, the memory size grows linearly with sequence length, and the total computational cost of attention updates scales quadratically. This becomes significant challenge when processing extremely long sequences. When Transformers with growing lossless memory struggle for very long sequences, it is natural to revisit the RNNs fixed-size compressed memory, which offers constant per-token update cost regardless of context length [29, 41, 96]. This contrast highlights fundamental trade-off between the efficiency of compressive memory and the fidelity of lossless memory. To address this problem, it is instructive to consider how the human brain maintains nearly constant volume through early and middle adulthood [16, 19, 25] while still supporting efficient processing of information across the human lifespan. The theory of Multi-Store Model of memory (MSM) in Cognitive Science and Neuroscience [4] suggests that although lossless short-term memory (or called working memory [5]) has limited capacity and duration [4, 56, 65], the hippocampus continually consolidates them into long-term cortical representations [3, 55, 71, 75, 78]. Inspired by MSM [4], we propose an artificial neural memory framework that converts lossless short-term memory into compressed long-term memory. Our method maintains sliding window of the Transformers KV cache as lossless short-term memory. Information that moves beyond this window is processed by learnable compression module we term the Artificial Hippocampus Network (AHN). This network recurrently compresses the out-of-window context into fixed-size state as the long-term compressed memory. AHNs can be instantiated with RNN-like architectures, and the overall framework is illustrated in Figure 1a. To evaluate the effectiveness of AHNs, we instantiate them using Mamba2 [18], DeltaNet (DN) [70, 97] and GatedDeltaNet (GDN) [98], resulting in the AHN-Mamba2, AHN-DN and AHN-GDN. Experimental results on long-context benchmarks LV-Eval [103] and InfiniteBench [105] show that AHN-augmented models consistently outperform their sliding window counterparts, and match or even surpass full attention models while significantly reducing computational and memory cache costs. For instance, as shown in Figure 1b, augmenting Qwen2.5-3B-Instruct [93] with AHNs (+0.4% parameters) reduces FLOPs by 40.5% and memory cache by 74.0%, while improving average score from 4.41 to 5.88 on LV-Eval (128k sequence length) [103]. The contributions of this paper are twofold. First, we introduce the concept of Artificial Hippocampus Networks (AHNs), which continually transform lossless memory outside the sliding window into compressed memory representation, enabling the model to leverage both memories for efficient long-context modeling. Second, to empirically validate the effectiveness of AHNs, we instantiate the concept into AHN-Mamba2, AHN-DN, and AHN-GDN, and train these instances using an efficient self-distillation scheme. Experimental 2 results demonstrate that these instances substantially enhance model efficiency on long-sequence benchmarks, while achieving competitive performance compared to the full attention model. We will release the code and models to facilitate future research on the development of more AHN variants."
        },
        {
            "title": "2.1 Memory in neural networks",
            "content": "Memory mechanisms play crucial role in enabling neural networks to process and retain information over time, which is essential for tasks that require understanding of temporal dependencies, sequential data, or context preservation. Traditional feedforward neural networks lack the capability to maintain information across time steps, which limits their effectiveness in tasks such as language modeling, sequence prediction, and reasoning. To address this limitation, Recurrent Neural Networks (RNNs) are introduced [23, 35, 36]. RNNs maintain hidden state that is updated at each time step, allowing information to persist across sequences. However, vanilla RNNs suffer from issues such as vanishing and exploding gradients, making it difficult to capture long-term dependencies [10]. To mitigate these problems, more advanced architectures like Long Short-Term Memory (LSTM) networks [33] and Gated Recurrent Unit (GRU) [14] are proposed. These models incorporate gating mechanisms that regulate the flow of information, enabling them to learn longer-term dependencies more effectively. Because these RNN-like models maintain fixed-size memory and consistent memory update cost for each input token, they are highly efficient for processing long sequences. Therefore, our AHNs are designed within the RNN paradigm to inherit this advantageous property. Beyond RNN-based architectures, memory-augmented neural networks have been developed to further enhance the memory capacity of neural models. For example, the Neural Turing Machine (NTM) [27] and the Differentiable Neural Computer (DNC) [28] introduce external memory modules that the network can read from and write to, allowing for more complex reasoning and algorithmic tasks. Over the past decade, attention mechanisms [6] have revolutionized the way neural networks handle memory. The Transformer architecture [80], which relies entirely on self-attention mechanisms, enables direct access to all previous states in sequence, providing form of memory that is both lossless and scalable. This has led to significant improvements in various domains [20, 22, 66, 67], and has spurred the emergence of new technological paradigms and innovations [30, 5961], such as In-Context Learning [11] and Chain-of-Thought (CoT) reasoning [84]. However, modeling long sequences exacerbates the quadratic computational complexity cost of attention mechanisms [13]. Our proposed AHNs address this challenge by employing an RNN-like network to compress the historical KV cache."
        },
        {
            "title": "2.2 Memory management",
            "content": "RNN-like models [9, 14, 18, 23, 29, 33, 41, 64, 76, 9698] maintain memory through fixed-size hidden state, regardless of input sequence length. Therefore, memory caching is not major concern for these architectures. In contrast, Transformers store key-value (KV) pairs for every token in the input sequence, resulting in linear growth of the KV cache with sequence length. This results in significant memory consumption and presents major challenge for processing long sequences. To mitigate this issue, various approaches have been proposed [45], including KV cache selection [1, 26, 31, 47, 52, 79, 88, 91, 107], budget allocation [12, 24, 90, 94], merging [51, 58, 81, 83], quantization [34, 49, 72, 74, 89, 99], low-rank decomposition [21, 101], external memory [62, 82], and neural architecture design [2, 38, 50, 57, 73, 77, 87, 100]. Among them, straightforward strategy is to use sliding window for attention [80], but this method discards KV pairs outside the window, thereby losing long-range context. Sparse Transformers [13] address this by retaining KV pairs at specific pattern positions to capture long-range dependencies, but still drop portions of the KV cache, potentially missing important information. Transformer-XL [17] introduces segment-level recurrence mechanism by caching the last segment of hidden states as First-In, First-Out (FIFO) memory. Compressive Transformer [68] extends this by compressing older memories into secondary FIFO memory, but it still discards memory once the slots are full. In contrast, AHNs adopt an RNN-like paradigm that continually compresses KV pairs outside the sliding window into lifelong compressed memory, rather than discarding them outright [48, 57, 69]. AHNs (like AHN-GDN [98]) can also dynamically control memory decay [18, 70, 97, 98]. Recent studies integrate RNNs and attention either in interleaved layers [18, 44, 48, 69, 98] or within single layer 3 [46, 57]. By contrast, we abstract the compression module as an AHN concept, yielding more general memory framework. We employ sliding-window attention mechanism, activating AHNs whenever token leaves the window. Additionally, we introduce simple self-distillation scheme that trains AHNs efficiently."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Most modern autoregressive large language models are built on Transformer architecture [80], which employs self-attention as the core mechanism for token mixing. Given an input sequence of tokens = (x1, x2, ..., xL) RLD, self-attention first projects the tokens into query (Q), key (K), and value (V ) matrices via learned linear transformations: = XWQ, = XWK, = XWV (1) where WQ, WK, and WV are trainable weight matrices. The attention output is then computed as weighted sum of the value vectors: Attention(Q, K, ) = softmax (2) (cid:18) QK din (cid:19) where RLL is the causal mask, defined by Mij = 1 if i, and Mij = 0 otherwise."
        },
        {
            "title": "3.2 Artificial Hippocampus Networks",
            "content": "Definition. Inspired by MSM [4] and the hippocampus [71] that consolidates lossless short-term memory into compact and long-term representations, we introduce Artificial Hippocampus Networks (AHNs) to emulate this biological function by compressing historical information into fixed-size recurrent state. An AHN operates alongside sliding attention window of size . For the token at step > , the AHN updates the compressive memory by processing the key-value (KV) pair (ktW , vtW ) that just exited the sliding window. This recurrent memory update is defined as: htW = AHN((ktW , vtW ), htW 1) (3) where htW is the updated compressed memory summarizing context up to and including position . htW can be vector or matrix. Due to the recurrent formulation of Equation 3, AHNs can be implemented with RNN-like architectures, enabling the learnable and efficient compression of long context history. Integration with lossless memory. Within the predefined sliding window, standard causal attention is applied to preserve lossless memory of recent tokens. Once the input sequence length exceeds the window size, AHNs are activated to compress the KV pair outside the window, i.e., (ktW , vtW ), into fixed-size compressed memory htW . After this compression, the original KV pair beyond the window can be safely discarded, retaining only the KV cache within the window {(ki, vi)}t i=tW +1. Finally, the current query qt accesses information from both compressed and lossless memories to produce the output: yt = (htW , {(ki, vi)}t i=tW +1, qt) (4) An illustration of the overall model mechanism with AHNs is provided in Figure 2a. Besides, the illustration of AHNs with attention sinks [91] is shown in Figure 6 in the appendix."
        },
        {
            "title": "3.3 Instantiation",
            "content": "As discussed above, AHNs can be instantiated using RNN-like architectures. In our experiments, we focus on modern linear recurrent models for their efficient parallel training. Specifically, we utilize three architectures including Mamba2 [18], DeltaNet (DN) [70, 97], and its enhanced version, GatedDeltaNet (GDN) [96], to instantiate AHNs into AHN-Mamba2, AHN-DN and AHN-GDN, respectively. Below, we present the 4 (a) (b) Figure 2 (a) Illustration of the model augmented with Artificial Hippocampus Networks (AHNs). In this example, the sliding window length is 3. When the input sequence length is less than or equal to the window length, the model operates identically to standard Transformer. For longer sequences, AHNs continually compress the token outside the window into compact memory representation. The model then utilizes both the lossless information within window, and the compressed memory to generate the next token. (b) Self-distillation training framework of AHNs based on an open-weight LLM. During training, the base LLMs weights are frozen, and only the AHNs parameters are trained. implementation of AHN-GDN as representative example, and the other two AHN instances are described in Appendix A. Specifically, AHN-GDN updates memory via the gated delta rule [70, 96, 97]: htW = AHN-GDN((ktW , vtW ), htW 1, xtW ) = α(xtW )(I β(xtW )kT tW ktW )htW 1 + β(xtW )kT tW vtW (5) Unlike GatedDeltaNet [98], which compresses all past tokens, AHN-GDN only compresses tokens outside the sliding window. For each position t, the query qt derived from xt is used to access the compressed memory htW . The output is further modulated by gate function γ(xt) and then is transformed by linear projection: yAHN,t = γ(xt)qthtW Wo (6) Different from GatedDeltaNet [96], the output of γ(xt) is scalar shared across head channels, and the output linear is grouped by heads [39, 42] with learnable weight Wo RHH (H denotes head dimension). Finally, we simply sum the outputs from AHN and the attention mechanism: yt = yAHN,t + Attention({(ki, vi)}t i=tW +1, qt) (7) Complexity analysis. Table 1 summarizes the computational and memory complexities of the attention token mixer with and without AHN-GDN, and Figure 3 compares the complexities of Qwen2.5-3B with and without AHN-GDN. As shown, integrating AHNs significantly improves efficiency over standard full attention in both memory usage and FLOPs. In particular, AHN-GDN reduces the computational complexity of attention to linear in sequence length while keeping the memory cache size constant. By contrast, vanilla full attention incurs quadratic computational cost and memory usage that grows linearly with sequence length."
        },
        {
            "title": "3.4 Training framework",
            "content": "While an AHN-augmented model can be trained from scratch, we adopt more computationally efficient approach using self-distillation [32, 104, 106]. This allows us to leverage powerful pre-trained models. Our training framework uses an open-weight LLM (e.g., Qwen [93]) as the teacher model, with its output probability 5 Table 1 Complexity of causal attention with and without AHN-GDN. Here, L: input sequence length; D: hidden dimension; Nq/Nkv: number of query/key-value heads; H: head dimension; : sliding window size. AHNs are activated only when > . FLOPs account for matrix multiplication only; softmax, normalization, and matrix element summation are omitted. Items shown in gray can be further omitted compared to the other terms."
        },
        {
            "title": "Token mixer",
            "content": "Causal attention (Full) Causal attention (Window) + AHN-GDN"
        },
        {
            "title": "Parameters",
            "content": "2DH(Nq + Nkv) Memory cache 2LHNkv O(L)"
        },
        {
            "title": "FLOPs",
            "content": "4LDH(Nq + Nkv) + 2HNqL2 O(L2) 2DH(Nq + Nkv) + 3DNq + 2Nq 2W HNkv + 2Nq O(W ) 4LDH(Nq + Nkv) + 2HNqW 2 + 2(L ) (2W HNq + 2Nq + 3DNq + 2Nq) O(W L) denoted as p. The student model is the same LLM, but we modify its attention mechanism to operate over limited receptive field of sliding window at every layer. These window attention layers are then augmented with AHNs. The students output probability is denoted as p. We train the student to mimic the teachers output distribution by minimizing the Kullback-Leibler (KL) divergence: = KL(pp). (8) To maximize efficiency, the base models weights are frozen during training, and only the AHN parameters are optimized. This framework is illustrated in Figure 2b."
        },
        {
            "title": "4.1 Setups",
            "content": "Models and datasets. We build our AHNs on top of open-weight Qwen2.5-Instruct series (3B, 7B, 14B) [93]. To demonstrate architectural flexibility, we implement the AHN module using three modern recurrent models: Mamba2 [18], DeltaNet [70, 97], and GatedDeltaNet [96]. The training data is ChatQA2 dataset [92], an open-source collection of diverse long-context tasks. We evaluate our methods across comprehensive suite of long-context benchmarks, including LongBench [7], InfiniteBench [105], and LV-Eval [103], with an additional illustrative example drawn from PG19 [68]. Baselines. We evaluate AHN-augmented models against two primary baselines: sliding window attention (SWA) with attention sinks [91] and the Compressive Transformers (CT) [68]. We implement the Compressive Transformer using max and average pooling to compress tokens outside the sliding window at 4 compression rate. To ensure fair comparison, all methods are allocated the same lossless memory budget, and the memory size of compressed tokens for CT is set to equal the memory size of the hidden state of AHNs. The performance of full attention is also reported as reference. (a) (b) (c) (d) Figure 3 Complexity analysis of the Qwen2.5-3B-Instruct and model perplexity, with and without AHNs. AHNs are only activated when the sequence length exceeds the window size (32K in this example). (a) The model with AHN enjoys linear computational complexity with respect to sequence length. (b) The model with AHN maintains consistent memory cache size. (c) Perplexity results on the first book of the PG19 test set (57K tokens). While Qwen-3B-Instruct degrades beyond its pre-trained context length, AHN-augmented models maintain consistently low perplexity. (d) Peak GPU memory under the same example. 6 Table 2 Performance and efficiency analysis on the 128k length subset of LV-Eval and InfiniteBench. The mixing/model FLOP ratio measures the relative computational cost of the token mixer or the entire model compared with the full attention baseline. For all methods except full attention, the lossless memory of attention sinks [91] and sliding window attention (SWA) is 32k tokens. Compressive Transformers (CT) [68] are implemented with attention sinks [91] and compression function of max or average pooling. Base model Token mixer Extra param ratio Mixing FLOP ratio Model FLOP ratio Memory cache ratio LV-Eval InfiniteBench cmrc -mixup loogle-SD -mixup dureader -mixup Avg. En. QA Zh. QA Avg. - 3 - 5 . 2 Q - 7 - 5 . 2 Q - 4 1 - 5 . 2 Q r I r I r I Full Attn Sinks + SWA CT-Max CT-Average 0% 0% 0% 0% 100% 100% 100% 46.6% 59.3% 25.6% 47.1% 59.7% 26.0% 47.1% 59.7% 26.0% AHN-Mamba2 AHN-DN AHN-GDN 0.4% 46.7% 59.4% 26.0% 0.4% 46.7% 59.4% 26.0% 0.4% 46.7% 59.4% 26.0% Full Attn Sinks + SWA CT-Max CT-Average 0% 0% 0% 0% 100% 100% 100% 48.0% 65.2% 25.6% 48.5% 65.6% 26.0% 48.5% 65.6% 26.0% 7.28 7.48 6.10 6.95 7.84 9.41 7.96 4.30 9.52 8.35 9.48 AHN-Mamba2 AHN-DN AHN-GDN 12.57 0.2% 48.1% 65.4% 26.0% 0.2% 48.1% 65.4% 26.0% 11.97 0.3% 48.1% 65.4% 26.0% 12.69 Full Attn Sinks + SWA CT-Max CT-Average 0% 0% 0% 0% 100% 100% 100% 49.5% 62.3% 25.6% 49.8% 62.6% 25.9% 49.8% 62.6% 25.9% 8.79 11.96 10.55 11.89 AHN-Mamba2 AHN-DN AHN-GDN 14.03 0.3% 49.7% 62.5% 25.9% 0.3% 49.7% 62.5% 25.9% 13.13 0.4% 49.7% 62.5% 25.9% 14.16 0.89 4.59 3.88 4.70 5.20 5.99 7.21 0.17 4.76 4.02 4.86 5.54 5.67 4.71 1.45 7.59 7.53 7. 7.20 9.14 8.54 13.22 11.49 11.37 11.40 12.35 11.49 12.52 12.8 14.09 12.34 13.78 14.13 16.52 15.30 13.84 12.23 12.08 12. 15.39 14.46 13.94 4.41 4.59 4.12 4.47 5.13 5.68 5.88 3.62 5.34 4.82 5.28 6.21 6.82 6.54 4.99 5.69 5.28 5. 6.43 6.50 6.51 7.28 8.63 7.40 8.30 9.29 10.61 10.61 11.23 10.66 10.56 10.63 11.36 12.86 13.37 11.23 11.62 10.58 10. 14.21 16.54 14.48 11.75 12.31 12.59 13.32 15.58 16.41 15.87 15.76 15.66 15.45 15.99 17.06 20.10 20.48 13.19 13.45 12.73 13. 16.20 18.42 18.55 9.52 10.47 10.00 10.81 12.44 13.51 13.24 13.50 13.16 13.00 13.31 14.21 16.48 16.93 12.21 12.54 11.66 11. 15.21 17.48 16.52 Implementation details. We implement all AHN instances in PyTorch [63], building on LLaMA-Factory [108] and Flash Linear Attention [95]. During training, we freeze the base LLM and train the newly initialized AHN module using self-distillation loss, as illustrated in Figure 2b. To ensure the AHN module learns generalizable compression strategy, we randomize the starting position of the AHN modules and also the sliding window size. For optimization, we use the AdamW [53] optimizer with learning rate of 1e-4, which is warmed up linearly over the first 10% of steps and then cosine decayed. All models are trained for one epoch on the ChatQA2 dataset, using global batch size of 128."
        },
        {
            "title": "4.2 An illustrative example",
            "content": "By compressing historical information beyond the sliding window into fixed-size memory, AHN-augmented models significantly reduce both computational complexity and memory footprint, as shown in Figure 3a and 3b. We demonstrate this advantage with real example on 57K token passage from the PG19, benchmark of long-form books designed to test extended context understanding. We compare the base 3B-Instruct models against their AHN-GDN counterparts. As shown in Figure 3c, the perplexity of standard Qwen models rises sharply once the 32K token context window is exceeded. In contrast, the AHN-GDN augmented model maintains consistently low perplexity. Furthermore, Figure 3d illustrates that while the base models memory usage grows linearly under FlashAttention, AHN-GDN keeps the CUDA memory usage nearly constant, highlighting its effectiveness for processing long-context sequences."
        },
        {
            "title": "4.3 Long-context benchmarks",
            "content": "We now systematically evaluate AHN-augmented models on long-context benchmarks to assess their effectiveness and efficiency. Our evaluation is structured across two settings: First, we conduct ultra-long-context evaluation on InfiniteBench [105] and LV-Eval [103] (both use 128k-length subset), comparing AHN-augmented models with full attention, sliding window attention (SWA) with attention sinks, and Compressive Transformer (CT) using average and max pooling as the compression functions. Besides, we evaluate six tasks with average sequence lengths exceeding 8k on LongBench [8]. Ultra-long-context. LV-Eval is challenging long-context benchmark, covering both single-hop QA and multi-hop QA. It introduces several design challenges, including confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric. We evaluate all methods on the 128K-context subsets across 7 Table 3 Qwen2.5-based model performance on six LongBench tasks (average sequence length > 8k). For all methods, the lossless memory of attention sinks [91] and sliding window attention (SWA) is 8192 tokens. Compressive Transformers (CT) [68] are implemented with attention sinks [91] and compression function of max or average pooling. Base model Token mixer DuReader HotpotQA MuSiQue NarrativeQA QMSum TriviaQA Avg. Qwen2.5-3BInstruct Qwen2.5-7BInstruct Qwen2.5-14BInstruct Sinks + SWA CT-Max CT-Average AHN-Mamba2 AHN-DN AHN-GDN Sinks + SWA CT-Max CT-Average AHN-Mamba2 AHN-DN AHN-GDN Sinks + SWA CT-Max CT-Average AHN-Mamba2 AHN-DN AHN-GDN 23.28 22.81 23.28 24.38 25.12 25.47 24.93 25.08 24.81 26.10 26.42 26.97 25.46 24.63 25. 26.34 26.80 26.51 43.70 40.92 44.65 42.95 42.83 42.76 51.57 50.61 51.85 53.24 54.24 54.17 55.68 54.45 56. 56.52 58.71 58.09 16.55 17.22 16.32 18.31 19.78 19.31 22.34 20.65 21.65 27.93 29.30 26.83 29.01 27.78 29. 30.32 32.92 31.40 15.35 16.58 16.36 16.70 19.11 18.95 22.29 23.17 22.66 24.86 25.08 24.00 23.21 22.16 23. 24.01 22.95 24.71 21.54 21.07 21.18 21.89 22.35 21.85 21.49 21.34 21.54 21.97 21.69 21.80 21.45 21.16 21. 22.19 22.08 22.35 85.44 85.55 85.29 85.18 86.17 84.93 88.48 88.89 88.48 89.24 89.49 89.75 89.06 88.16 89. 88.63 87.50 88.35 34.31 34.03 34.51 34.90 35.89 35.55 38.52 38.29 38.50 40.56 41.04 40.59 40.65 39.72 40. 41.34 41.83 41.90 all 11 datasets. For sliding window-based methods (SWA and AHN), we use 32768-token lossless memory, consisting of 128-token attention sinks and 32640-token sliding window during inference. To further validate this setting, we also test on InfiniteBench, benchmark tailored to evaluate language models ability to process, understand, and reason over super-long contexts. As shown in Table 2, AHN-augmented models consistently outperform SWA with attention sinks baseline across nearly all tasks. Remarkably, they also surpass the performance of full attention, demonstrating the effectiveness of the compressed memory mechanism while offering substantial computational and memory savings. We include full results in the appendix. Long-context. To evaluate our models on broader range of practical scenarios, we use LongBench, which features diverse tasks across multiple domains and languages, designed to rigorously test long-context understanding in more realistic scenarios. While many tasks on LongBench have relatively short inputs, we focus on six tasks with an average length exceeding 8192 tokens to create challenging evaluation. In this setup, we constrain all methods to fixed 8192-token lossless memory budget (128 attention sinks and an 8064-token sliding window). As reported in Table 3, AHN-augmented models again achieve consistently superior accuracy compared to both baselines. These results strongly suggest that the recurrent hidden states effectively capture and utilize historical information, leading to improved performance across diverse scenarios."
        },
        {
            "title": "4.4 Ablation study",
            "content": "Having demonstrated the effectiveness of AHN-augmented models, we now conduct an ablation study to analyze the impact of our two design choices: the training objective and the use of randomization. For these experiments, we use AHN-GDN (Qwen2.5-7B-Instruct) as the starting point. Training objectives: self-distillation vs. next-token prediction. We train AHNs using self-distillation, minimizing the KL divergence between the AHN-augmented logits and the full attention outputs. As comparison, we also apply standard next-token prediction with cross-entropy (CE) loss, which encourages AHNs to learn to compress directly from data distribution. As shown in Table 4, this replacement results in marked performance drop on LongBench. We hypothesize this is because CE provides sparse learning signals, and pushes the small AHN modules towards shortcuts in the training data. In contrast, self-distillation offers denser guidance over the teachers entire output distribution, compelling AHNs to learn more generalizable context representations. Randomization vs. fixed windows. We train AHNs with randomized sliding window sizes to encourage general compressive module that adapts to varying look-ahead contexts. By comparison, models trained with 8 Figure 4 AHN modules demonstrate strong context generalization capacity on LongBench. Table 4 Ablation of AHN training design choices. We ablate two factors: (1) the training objective, comparing self-distillation (KL loss) with next-token prediction (no full-attention teacher model, CE loss), and (2) randomized versus fixed sliding window configurations. All experiments are based on Qwen2.5-7B-Instruct with AHN-GDN. Training target Training window size LongBench (Average of 6 tasks) Self-distillation (KL loss) 1024 (fixed) Next-token prediction (CE loss)Random size Random size Self-distillation (KL loss) 38.53 39.59 40.59 fixed windows tend to overfit to the specific configuration and fail to generalize to unseen context lengths. This suggests that the design of compression priors and training schedules deserves further investigation. To evaluate context generalization, we fix attention sinks to 128 tokens and test AHN-augmented models with sliding window sizes from 896 to 8064. As shown in Figure 4, AHN-augmented models maintain strong performance across all tested configurations."
        },
        {
            "title": "4.5 Probing AHN with gradients visualization",
            "content": "Beyond benchmark performance, we seek to understand how effectively AHNs compress and exploit out-ofwindow information. We probe the backward dynamics of AHN-augmented models by visualizing gradients of the self-distillation loss, which is formally defined by: xout KL(f (xwin, xout) (xwin, hAHN)) (9) where () and () denote the teacher and student forward models, hAHN represents the compressed memory of AHN, xwin are in-window token embeddings, and xout are out-ofwindow embeddings. Out-of-window tokens with small gradient magnitudes indicate that their information has already been well captured in AHNs compressed memory. As illustrated by the math example in Figure 5, AHN tends to preserve the information of mathematical symbols and numbers while neglecting less critical ones such as pronouns and special tokens, demonstrating its effectiveness as targeted compression module."
        },
        {
            "title": "5 Conclusion and discussion",
            "content": "We introduce Artificial Hippocampus Networks (AHNs), novel class of lightweight architectural components that enhance Transformer models for efficient long-sequence processing. AHNs address the efficiency limitation of standard transformers by maintaining sliding window of KV cache as lossless memory while transforming out-of-window information into fixed-size compressed memory. This approach enables AHN-augmented models to achieve constant memory and computational complexity per token over long sequences. Experiments demonstrate that AHNs can significantly reduce both memory cache size and computation while maintaining competitive performance on long-context benchmarks. Figure 5 Green regions mark tokens with low L2 gradient magnitudes, indicating they are preferentially selected by AHN to store in the compressed memory; red denotes the opposite. Limitations and future works. While AHNs strike an effective balance between computational efficiency and memory fidelity, their fixed-size compressed memory inevitably entails some information loss and may 9 impair performance on tasks that require exact recall, as detailed in the appendix. Furthermore, since our study adopts parameter-efficient self-distillation setup, performance remains capped by the underlying base models capacity. Future work may explore stronger recall mechanisms and full-parameter training to further unlock the potential of AHNs. For application scenarios, the AHN framework opens up opportunities in long-context domains with sparse information or constrained resources, such as lifelong learning, streaming video processing, and deployment on edge devices."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank Shi Guang, Haoqi Fan, Tianle Cai, Deyao Zhu, Tenglong Ao, Ge Zhang, Wenhao Huang, and Liang Xiang for valuable discussions."
        },
        {
            "title": "References",
            "content": "[1] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [3] Pablo Alvarez and Larry Squire. Memory consolidation and the medial temporal lobe: simple network model. Proceedings of the national academy of sciences, 91(15):70417045, 1994. [4] Richard Atkinson and Richard Shiffrin. Human memory: proposed system and its control processes. In Psychology of learning and motivation, volume 2, pages 89195. Elsevier, 1968. [5] Alan D. Baddeley and Graham Hitch. Working memory. volume 8 of Psychology of Learning and Motivation, pages 4789. Academic Press, 1974. doi: https://doi.org/10.1016/S0079-7421(08)60452-1. URL https://www. sciencedirect.com/science/article/pii/S0079742108604521. [6] Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, 2015. [7] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, 2024. [8] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. [9] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xLSTM: Extended long short-term memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=ARAxPPIAhq. [10] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157166, 1994. [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [12] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. [13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. 10 [14] Kyunghyun Cho, van Merrienboer, Caglar Gulcehre, Bougares, Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 2014. [15] Marcus Tullius Cicero. De Oratore. 55 BCE. [16] Eric Courchesne, Heather Chisum, Jeanne Townsend, Angilene Cowles, James Covington, Brian Egaas, Mark Harwood, Stuart Hinds, and Gary Press. Normal brain development and aging: quantitative analysis at in vivo mr imaging in healthy volunteers. Radiology, 216(3):672682, 2000. [17] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, 2019. [18] Tri Dao and Albert Gu. Transformers are ssms: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning, pages 1004110071, 2024. [19] Anatole Dekaban and Doris Sadowsky. Changes in brain weights during the span of human life: relation of brain weights to body heights and body weights. Annals of Neurology: Official Journal of the American Neurological Association and the Child Neurology Society, 4(4):345356, 1978. [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [21] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and Beidi Chen. Get more with less: Synthesizing recurrence with kv cache compression for efficient llm inference. In International Conference on Machine Learning, pages 1143711452. PMLR, 2024. [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. [23] Jeffrey Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990. [24] Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. [25] Anthony Fotenos, AZ Snyder, LE Girton, JC Morris, and RL Buckner. Normative estimates of cross-sectional and longitudinal brain volume decline in aging and ad. Neurology, 64(6):10321039, 2005. [26] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=uNrFpDPMyo. [27] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. [28] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using neural network with dynamic external memory. Nature, 538(7626):471476, 2016. [29] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=tEYskw1VY2. [30] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [31] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, 2024. 11 [32] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [33] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. [34] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2024. [35] John Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. [36] John Hopfield. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the national academy of sciences, 81(10):30883092, 1984. [37] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=kIoBbc76Sy. [38] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. [39] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33:1283712848, 2020. [40] Gregory Kamradt. Needle in haystack - pressure testing llms, 2023. URL https://github.com/gkamradt/ LLMTest_NeedleInAHaystack. [41] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast In International conference on machine learning, pages autoregressive transformers with linear attention. 51565165. PMLR, 2020. [42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [43] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [44] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. [45] Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, and Lei Chen. survey on large language model acceleration based on kv cache management. arXiv preprint arXiv:2412.19442, 2024. [46] Yixing Li, Ruobing Xie, Zhen Yang, Xingwu Sun, Shuaipeng Li, Weidong Han, Zhanhui Kang, Yu Cheng, Chengzhong Xu, Di Wang, et al. Transmamba: Flexibly switching between transformer and mamba. arXiv preprint arXiv:2503.24067, 2025. [47] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. [48] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [49] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. 12 [50] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [51] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Reza Haffari, and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large language models. Advances in Neural Information Processing Systems, 37:139997 140031, 2024. [52] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36:5234252364, 2023. [53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [54] Minh-Thang Luong, Hieu Pham, and Christopher Manning. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 14121421, 2015. [55] James McClelland, Bruce McNaughton, and Randall OReilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychological review, 102(3):419, 1995. [56] George Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956. [57] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. [58] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. Dynamic memory compression: retrofitting llms for accelerated inference. In Proceedings of the 41st International Conference on Machine Learning, pages 3739637412, 2024. [59] OpenAI. Gpt-4 technical report. https://arxiv.org/abs/2303.08774, 2023. [60] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [61] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [62] Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023. [63] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [64] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [65] Lloyd Peterson. Short-term retention of individual items. Exp Psychol, 58:3135, 1959. [66] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. URL https://cdn.openai.com/research-covers/language-unsupervised/ language_understanding_paper.pdf. [67] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [68] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH. [69] Liliang Ren, Yang Liu, Yadong Lu, yelong shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=bIlnpVM4bc. 13 [70] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. [71] William Beecher Scoville and Brenda Milner. Loss of recent memory after bilateral hippocampal lesions. Journal of neurology, neurosurgery, and psychiatry, 20(1):11, 1957. [72] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. In ICLR, 2024. [73] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [74] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning, pages 3109431116. PMLR, 2023. [75] Larry Squire and Stuart Zola-Morgan. The medial temporal lobe memory system. Science, 253(5026): 13801386, 1991. [76] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. [77] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. Advances in Neural Information Processing Systems, 37:73397361, 2024. [78] Atsuko Takashima, Ingrid LC Nieuwenhuis, Ole Jensen, Lucia Talamini, Mark Rijpkema, and Guillén Fernández. Shift from hippocampal to neocortical centered retrieval network with consolidation. Journal of Neuroscience, 29(32):1008710093, 2009. [79] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware In International Conference on Machine Learning, pages sparsity for efficient long-context llm inference. 4790147911. PMLR, 2024. [80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [81] Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 40654078, 2024. [82] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, and Zexue He. M+: Extending memoryLLM with scalable long-term memory. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=OcqbkROe8J. [83] Zheng Wang, Boxiao Jin, Zhongzhi Yu, and Minjia Zhang. Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks. arXiv preprint arXiv:2407.08454, 2024. [84] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [85] Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. RNNs are not transformers (yet): The key bottleneck on in-context retrieval. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=h3wbI8Uk1Z. [86] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In International Conference on Learning Representations (ICLR), 2015. URL https://arxiv.org/abs/1410.3916. [87] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= TrjbxzRcnf-. [88] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. InfLLM: Training-free long-context extrapolation for LLMs with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https:// openreview.net/forum?id=bTHFrqhASY. [89] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. [90] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. [91] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. [92] Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, and Bryan Catanzaro. ChatQA 2: Bridging the gap to proprietary LLMs in long context and RAG capabilities. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=cPD2hU35x3. [93] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [94] Dongjie Yang, Xiaodong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao. Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. In Findings of the Association for Computational Linguistics ACL 2024, pages 32583270, 2024. [95] Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/fla-org/flash-linear-attention. [96] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In International Conference on Machine Learning, pages 5650156523. PMLR, 2024. [97] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=y8Rm4VNRPH. [98] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=r8H7xhYPwz. [99] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:2716827183, 2022. [100] Howard Yen. Long-context language modeling with parallel context encoding. Masters thesis, Princeton University, 2024. [101] Hao Yu, Zelan Yang, Shen Li, Yong Li, and Jianxin Wu. Effectively compress kv heads for llm. arXiv preprint arXiv:2406.07056, 2024. [102] Weihao Yu and Xinchao Wang. Mambaout: Do we really need mamba for vision?"
        },
        {
            "title": "In Proceedings of the",
            "content": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [103] Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, et al. Lv-eval: balanced long-context benchmark with 5 length levels up to 256k. arXiv preprint arXiv:2402.05136, 2024. [104] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 37133722, 2019. [105] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. ınftybench: Extending long context evaluation beyond 100k tokens. In ACL (1), 2024. 15 [106] Ying Zhang, Tao Xiang, Timothy Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43204328, 2018. [107] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. [108] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A AHN instantiation",
            "content": "This section describes how to instantiate AHNs with Mamba2 [18] and DateNet (DN) [70, 97]. For the AHN-Mamba2 instance, the compressed memory update rule is htW = AHN-Mamba2((ktW , vtW ), htW 1, xtW ) = exp((xtW )A)htW 1 + (xtW 1)kT tW vtW As for AHN-DN, the update rule can be expressed as htW = AHN-DN((ktW , vtW ), htW 1, xt) (I β(xtW )kT tW ktW )htW 1 + β(xtW )kT tW vtW (10) (11) The output rule of AHN-Mamba2 and AHN-DN are the same as AHN-GDN, as shown in Equation 6. We also provide an illustration of AHN-augmented networks with attention sinks [91], as shown in Figure 6. Figure 6 Illustration of the model augmented with Artificial Hippocampus Networks (AHNs). In this example, the number of attention sinks is 2, and the sliding window length is 3. When the input sequence length is less than or equal to the sum of attention sinks and the window length, the model operates identically to standard Transformer. For longer sequences, AHNs continually compress the token outside the window into compact memory representation. The model then utilizes the lossless information within the attention sinks and the sliding window, as well as the compressed memory to generate the next token."
        },
        {
            "title": "B Additional benchmark results",
            "content": "This section further examines the effectiveness of AHNs in long-context scenarios, presenting additional benchmark results, while also acknowledging their inherent limitations on exact-recall tasks due to the lossy nature of compressed memory. LV-Eval[103]. We present complete results on all 11 LV-Eval tasks under the 128k context setting. All models are configured with 32768 tokens of lossless memory, including 128-token attention sinks and 32640-token sliding window. RULER [37] is comprehensive benchmark that extends the standard needle-in-a-haystack (NIAH) [40] paradigm by introducing increased task difficulty and additional categories. We evaluate an AHN-augmented model (AHN-GDN) on all NIAH tasks within the RULER-128k subset, using Qwen2.5-7B-Instruct as the base 17 model. For fair comparison, both AHN-GDN and sliding window attention with attention sinks are configured with 128 attention sinks and 32640-token sliding window. As shown in Table 5, AHN-GDN performs on par with sliding window attention but markedly worse than full attention on exact-recall tasks. This reflects the inherent trade-off of lossy compression: while AHN-augmented models enable efficient long-context reasoning, they inevitably struggle on tasks that require exact-recall from the compressed memory. This limitation suggests opportunities for future research, such as memory management that preserves critical information in lossless memory while leveraging compression for efficiency. Table 5 Performance on advanced needle-in-a-haystack (NIAH) tasks performance from RULER-128k. Both sliding window approaches use 128 attention sinks with 32640 sliding window. Method single_1 single_2 single_3 multikey_1 multikey_2 multikey_3 multivalue multiquery Full Attn Sinks + SWA AHN-GDN 98.60 26.80 26. 97.20 25.40 25.20 98.40 28.00 28.20 89.20 27.80 27.40 23.60 10.60 11.40 23.20 9.00 8.60 55.40 22.95 23. 85.45 24.00 23.35 Table 6 Complete results on all 21 tasks in the 128k subset of LV-Eval. All sliding window-based methods use lossless memory of 32768 tokens, consisting of 128 attention sinks and 32640-token sliding window."
        },
        {
            "title": "Full Attn",
            "content": "Sinks + SWA CT-Max CT-Average AHN-Mamba2 AHN-DN AHN-GDN - 3 - 5 . 2 Q r I - 7 - 5 . 2 Q u n - 4 1 - 5 . 2 Q r I"
        },
        {
            "title": "Average",
            "content": "cmrc_mixup dureader_mixup factrecall_en factrecall_zh hotpotwikiqa_mixup lic_mixup loogle_CR_mixup loogle_MIR_mixup loogle_SD_mixup multifieldqa_en_mixup multifieldqa_zh_mixup"
        },
        {
            "title": "Average",
            "content": "cmrc_mixup dureader_mixup factrecall_en factrecall_zh hotpotwikiqa_mixup lic_mixup loogle_CR_mixup loogle_MIR_mixup loogle_SD_mixup multifieldqa_en_mixup multifieldqa_zh_mixup"
        },
        {
            "title": "Average",
            "content": "cmrc_mixup dureader_mixup factrecall_en factrecall_zh hotpotwikiqa_mixup lic_mixup loogle_CR_mixup loogle_MIR_mixup loogle_SD_mixup multifieldqa_en_mixup multifieldqa_zh_mixup 4.41 7.28 13.22 6.88 2.80 0.09 7.68 0.06 0.00 0.89 0.00 9.59 3.62 4.30 12.80 5.33 0.80 0.24 3.40 0.57 0.00 0.17 0.00 12.24 4. 8.79 13.84 4.31 0.22 0.00 11.96 0.3 0.94 1.45 0.00 13.10 4.59 7.48 11.49 3.34 1.28 0.30 6.86 2.24 0.64 4.59 0.33 11.91 5.34 9.52 14.09 4.65 1.29 0.69 10.19 0.50 0.71 4.76 0.47 11.90 5. 11.96 12.23 0.45 0.07 0.64 10.18 3.64 1.56 7.59 0.41 13.82 4.47 6.95 11.4 3.59 1.18 0.48 6.49 2.28 0.58 4.70 0.08 11.41 5.28 9.48 13.78 4.65 1.35 0.82 10.07 0.47 0.92 4.86 0.45 11.27 5. 11.89 12.46 0.45 0.00 0.64 10.19 3.57 1.36 7.41 0.06 14.05 4.12 6.10 11.37 3.86 1.37 0.08 6.39 1.61 0.47 3.88 0.43 9.74 4.82 8.35 12.34 4.67 1.11 0.48 8.49 0.81 1.08 4.02 0.71 10.93 5. 10.55 12.08 0.77 0.13 0.53 9.52 2.74 1.38 7.53 0.39 12.50 18 5.13 7.84 12.35 5.58 1.57 1.11 8.13 1.55 1.39 5.20 0.00 11.72 6.21 12.57 14.13 5.84 1.43 0.16 9.27 2.26 0.91 5.54 0.00 16. 6.43 14.03 15.39 1.19 0.15 0.33 11.57 3.60 1.65 7.20 0.60 14.97 5.68 9.41 11.71 9.22 4.19 0.06 7.78 1.65 1.14 5.99 0.00 11.31 6.83 11.97 16.52 5.74 2.05 0.99 8.73 2.59 3.08 5.67 0.28 17. 6.50 13.13 14.46 0.30 0.00 0.67 12.17 2.34 1.19 9.14 1.08 17.06 5.88 7.96 12.52 12.51 1.79 0.65 7.38 1.96 1.06 7.21 0.19 11.42 6.54 12.69 15.30 5.14 1.68 0.76 10.63 1.58 2.70 4.71 0.06 16. 6.51 14.16 13.94 0.15 0.00 0.49 11.13 3.64 0.65 8.54 0.94 17."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}