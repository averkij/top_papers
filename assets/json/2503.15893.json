{
    "paper_title": "UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis",
    "authors": [
        "Jiawei Wang",
        "Kai Hu",
        "Qiang Huo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are publicly available at https://github.com/microsoft/CompHRDoc."
        },
        {
            "title": "Start",
            "content": "UniHDSA: Unified Relation Prediction Approach for Hierarchical Document Structure Analysis Jiawei Wanga,b,1,, Kai Hua,b,1, Qiang Huob aDepartment of EEIS, University of Science and Technology of China, Hefei, 230026, China bMicrosoft Research Asia, Beijing, 100080, China"
        },
        {
            "title": "Abstract",
            "content": "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts unified framework that uses multiple branches or modules, each designed to address distinct task. In this work, we propose unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into unified label space. This allows single relation prediction module to handle multiple tasks simultaneously, whether at page-level or document-level structure analysis. By doing so, our approach significantly reduces the risk of cascading errors and enhances systems efficiency, scalability, and adaptability. To validate the effectiveness of UniHDSA, we develop multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on largescale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSAs configurations are publicly available at https://github.com/microsoft/CompHRDoc. Keywords: Document Layout Analysis, Relation Prediction, Unified Label Space 5 2 0 2 6 2 ] . [ 2 3 9 8 5 1 . 3 0 5 2 : r Corresponding author. Email addresses: wangjiawei@mail.ustc.edu.cn (Jiawei Wang), hk970213@mail.ustc.edu.cn (Kai Hu), qianghuo@microsoft.com (Qiang Huo) 1Work done when Jiawei Wang and Kai Hu were interns at Multimodal Interaction Group, Microsoft Research Asia, Beijing, China. The paper is the result of an open-source research starting from May 2023."
        },
        {
            "title": "Preprint submitted to Pattern Recognition",
            "content": "March 27, 2025 1. Introduction Document Structure Analysis (DSA) involves identifying the fundamental components within document, such as headings, paragraphs, lists, tables, and figures, and establishing their logical relationships and structures. This process produces structured representation of the documents physical layout that accurately reflects its logical structure, thereby improving the effectiveness and accessibility of information retrieval and processing. In todays digital age, most mainstream documents are created using hierarchicalschema authoring software like LaTeX, Microsoft Word, and HTML. As result, Hierarchical Document Structure Analysis (HDSA), which focuses on extracting and reconstructing the inherent hierarchical structures within these document layouts, has garnered significant attention. Despite its growing popularity, HDSA remains substantial challenge due to the diversity of document content and the intricate complexity of their layouts. Recently, hierarchical document structure analysis has garnered increasing attention, with notable explorations like DocParser and HRDoc. DocParser [1] is an end-to-end system that parses document renderings into hierarchical structures, including all text elements, nested figures, tables, and table cell structures. It initially employs Mask R-CNN [2] to detect all entities of documents within an image. Subsequently, it uses set of rules to predict two predefined relationshipsparent of, which represents the hierarchical relationship (e.g., between section and its subsection), and followed by, which captures the sequential reading order between document entities. Together, these relationships enable the parsing of the documents complete physical structure. However, DocParser does not consider the logical structure, such as the table of contents, and its rule-based approach limits its overall effectiveness and adaptability. Based on it, DSG [3] replaced the rule-based relation prediction module with an LSTM-based relation prediction network to make the whole system trainable from end to end. Although DSG offers greater effectiveness and adaptability compared to DocParser, it is limited to parsing single document pages and overlooks the complexity of relationships between objects spanning multiple pages. In contrast, HRDoc [4] proposed an encoder-decoderbased hierarchical document structure parsing system (DSPS) to reconstruct document hierarchies. This system uses multimodal bidirectional encoder and structure-aware GRU decoder to predict the logical roles of text-lines and their relationships (contain, equality, and connect). Although DSPS significantly outperforms DocParser and considers the logical structure, it assumes the reading order and graphical page objects (e.g., tables and figures) are providedan essential aspect of document structure analysis that should not be overlooked. Additionally, as the number of text-lines increases, the computational complexity of DSPS grows quadratically, posing significant challenges when processing longer documents. Building on HRDoc, Detect-Order-Construct (DOC) [5] established comprehensive benchmark named Comp-HRDoc, encompassing page object detection, reading order prediction, table of contents extraction, and hierarchical structure reconstruction concurrently. As shown in Fig. 1(a), Detect-Order-Construct utilizes three-stage (a) Detect-Order-Construct (b) Ours Figure 1: Comparison between previous state-of-the-art approach and our proposed method. (a) Detect-OrderConstruct employs multi-stage tree construction based approach, designing specific modules for each sub-task, which introduces cascading errors. (b) Our approach simplifies the process into two stages, unifying sub-tasks within page-level and document-level structure analysis using unified label space. Ti denotes Text-line queries, Gi denotes Graphical object queries, Li denotes Logical role queries, Bi denotes Text-Block queries. The purple grids illustrate intra-region relationships, the green grids represent inter-region relationships, and the orange grids signify logical role relationships. approach for HDSA, designing specific modules for each sub-task. Although effective on certain HDSA benchmarks, this multi-branch and multi-stage framework may introduce cascading errors when addressing HDSA sub-tasks sequentially. Such approaches also pose scalability challenges and struggle to accommodate additional tasks. In this paper, we propose UniHDSA, unified relation prediction approach for hierarchical document structure analysis. Unlike previous frameworks, UniHDSA consolidates the process into two primary stages: page-level structure analysis and document-level structure analysis, as illustrated in Fig. 1(b). To unify various HDSA sub-tasks into these two stages, we treat tasks such as text region detection and reading order prediction at the page-level, and tasks such as table of contents extraction and hierarchical list extraction at the document-level as relation prediction problems. By defining these relation labels within unified label space at both the page-level and document-level, our approach ensures that single relation prediction module can efficiently address multiple sub-tasks in either stage. This allows us to integrate wide range of sub-tasks into cohesive framework, significantly reducing the risk of cascading errors and enhancing the systems scalability and adaptability. Specifically, at the page-level, UniHDSA unifies tasks such as graphical object detection, logical role classification, text region detection, and reading order prediction, ensuring that the fundamental components of the document layout are accurately identified and organized. At the document-level, given the page-level predictions, our method encompasses table of contents extraction, hierarchical list extraction, cross-page table grouping, and cross-page paragraph grouping by identifying the relationships among these components. key advantage of this two-stage approach is its significant improvement in efficiency, particularly for long documents such as financial reports or contracts, which may consist of numerous pages. Without this hierarchical approach, directly processing document-level tasks would be extremely inefficient and computationally expensive. By addressing these complex relationships within unified framework, UniHDSA can efficiently handle documents of varying lengths and complexities. To validate the effectiveness of our proposed framework, we introduce multimodal end-to-end system based on Transformer architectures [6]. As depicted in Fig. 2, our system employs Vision Backbone for visual feature extraction, followed by Page-Level Transformer Encoder and Decoder for page-level structure analysis, and Doc-Level Transformer Encoder for document-level structure analysis. To enhance the understanding of textual content, pre-trained language model is integrated. By leveraging our proposed unified label space approach, this system enables unified page-level and document-level relation prediction, streamlining the entire process and improving overall efficiency. In addition to relationship prediction, effectively detecting graphical objects within document pages is crucial subtask of Hierarchical Document Structure Analysis. We utilize model structure similar to Deformable DETR [7] for the page-level Transformer encoder and decoder. This approach not only enables effective localization of graphical objects but also allows for the uniform input of various document elements (e.g., text-lines, tables, and formulas) as queries to the decoder. The unified representation of these elements, learned by the page-level Transformer, incorporates robust contextual information through self-attention mechanisms and attentively considers both global and local document layout information via cross-attention mechanisms, facilitating the exploration of logical relationships among different document elements. To further enhance the semantic representation of content queries in the Transformer decoder, we introduce novel type-wise queries that capture the categorical information of diverse page objects. Extensive experimental results demonstrate that our proposed end-toend system achieves state-of-the-art performance on hierarchical document structure analysis benchmark, Comp-HRDoc [5], and competitive results on large-scale document layout analysis dataset, DocLayNet [8], which confirms the effectiveness and superiority of our approach across all sub-tasks. Figure 2: Overview of our proposed multimodal end-to-end system based on UniHDSA for hierarchical document structure analysis. The main contributions of this paper are as follows: Proposed UniHDSA, unified relation prediction approach for hierarchical document structure analysis, which consolidates the process into two primary stages: page-level structure analysis and documentlevel structure analysis. This approach reduces the risk of cascading errors and enhances efficiency, scalability, and adaptability. Treated various HDSA sub-tasks as relation prediction problems, and consolidated these relation prediction labels into unified label space. This enables single relation prediction module to handle multiple tasks concurrently either at the page-level or at the document-level. Developed multimodal end-to-end system based on Transformer architectures. This system achieves state-of-the-art performance on the hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on the large-scale document layout analysis dataset, DocLayNet, demonstrating the effectiveness and superiority of our approach. In our preliminary study presented in our conference paper [9], we explored treating various page-level document layout analysis sub-tasks as relation prediction problems and consolidated these prediction labels into unified label space. However, DLAFormer, as proposed in that work, is vision-only framework that does not support document-level structure analysis. Building upon that foundation, in this paper we 5 continue to leverage the unified relation prediction approach for hierarchical document structure analysis. Furthermore, we extend the unified label space approach to concurrently handle document-level subtasks and introduce language model to develop multimodal end-to-end Transformer-based system that significantly enhances performance across various HDSA sub-tasks. 2. Related Work 2.1. Page Object Detection Page object detection (POD) [10] plays pivotal role in document layout analysis."
        },
        {
            "title": "It encompasses",
            "content": "the identification and classification of logical objects, such as tables, figures, formulas, and paragraphs, within document pages. Deep learning-based POD approaches can be broadly classified into two categories: top-down based methods, and bottom-up based methods. Top-down based methods leverage the latest top-down object detection or instance segmentation frameworks to address the page object detection problem. PubLayNet [11] directly used Faster R-CNN [12] and Mask-RCNN [2] for scientific document page object detection. Lee et al. [13] proposed trainable multiplication layers combined with U-Net [14]. With the success of Transformer based detector in the field of computer vision, Yang et al. [15] introduced Deformable DETR [7] for page object detection. SwinDocSegmenter [16] optimized Mask-DINO [17] by utilizing both high-level and low-level features of document images to initialize the query. These methods mainly focus on enhancing generic object detectors to more suitably match layout analysis tasks and overlook the textual features of documents."
        },
        {
            "title": "In recent",
            "content": "years, self-supervised models such as LayoutLM [18] and DiT [19] have demonstrated remarkable progress in document understanding tasks by aligning cross-modal features on vast document image datasets. These methods can be used to initialize the backbone of generic object detectors to effectively identify page objects. While these methods have achieved impressive benchmark performance, challenges remain, particularly in detecting small-scale text regions with high precision. Apart from self-supervised models, some works have explored the fusion strategy among multimodal features. Zhang et al. [20] proposed constructing grid representation [21] by leveraging OCR information and fusing textual and visual features at the pixel level. M2Doc [22] adopted similar strategy in ViBERTgrid [23], employing two fusion modulesearly-fusion and late-fusionthat align and integrate visual and textual features at both the pixel and block levels to enhance DINO [24]. Although M2Doc has set new benchmarks, this early fusion strategy presents two significant drawbacks. First, it is inefficient: constructing grid representation requires repeatedly copying text-line features to their corresponding positions in the grid matrix within text-line bounding boxes, process that is GPU-unfriendly and time-consuming. Second, convolution operations struggle to capture contextual relationships among textual features within the grid representation. 6 Bottom-up based methods typically represent each document page as graph, where its nodes correspond to primitive page objects (e.g., words, text-lines, connected components), and its edges denote relationships between neighboring primitive page objects. The detection of page objects is then formulated as graph labeling problem. For instance, Li et al. [25] employed image processing techniques to initially generate line regions, followed by the application of two CRF models to classify these regions into distinct types and predict whether pairs of line regions belong to the same instance, based on visual features extracted by CNNs. More recently, Wang et al. [26] framed the page object detection problem as graph segmentation and classification problem and introduced lightweight graph neural network. While these bottom-up based methods can effectively detect small-scale text regions, accurately locating the graphical page objects based on text units within graphical objects remains challenging. While achieving remarkable results on several benchmark datasets, both top-down and bottom-up based approaches exhibit inherent limitations. In light of these constraints, Zhong et al. [27] introduced novel two-branch hybrid approach that combines the strengths of both methods. In contrast to the conventional use of multi-branch or multi-stage structures for document layout analysis, our proposed UniHDSA integrates multiple sub-tasks into unified end-to-end model. This not only combines the advantages of the hybrid approach but also demonstrates robust scalability and performance. 2.2. Reading Order Prediction The objective of reading order prediction is to determine the appropriate reading sequence for documents. Generally, humans tend to read documents in left-to-right and top-to-bottom manner. However, such simplistic sorting rules may prove inadequate when applied to complex documents with tokens extracted by OCR tools. Early approaches to reading order prediction are mainly based on heuristic sorting rules [2830]. Despite their effectiveness in certain scenarios, these rule-based methods can be prone to failure when confronted with out-of-domain cases. In recent years, deep learning models have emerged for reading order prediction. Li et al. [31] proposed an end-to-end OCR text reorganizing model, using graph convolutional encoder and pointer network decoder to reorder text blocks. LayoutReader [32] introduced benchmark dataset called ReadingBank, which contains reading order, text, and layout information, and employed Transformerbased architecture on spatial-text features to predict the reading order sequence of words. However, the decoding speed of these auto-regressive decoding methods is limited when applied to rich text documents. Recently, Quiros et al. [33] followed the idea of assuming pairwise partial order at the element level from [28] and proposed two new reading-order decoding algorithms for reading order prediction on handwritten documents. Detect-Order-Construct [5] incorporated similar relation prediction method akin to the one introduced in [27] for reading order prediction, exhibiting much better performance in their benchmark datasets. In our work, we introduced novel approach employing unified label space to simultaneously 7 address diverse relation prediction tasks. 2.3. Hierarchical Document Structure Reconstruction Reconstructing documents hierarchical structure aims to recover its logical organization, conveying semantic information beyond the character strings that make up its content. Representing the structure and layout of document is critical for this task. While graph representations can encapsulate relationships between regions and their properties, they fail to capture the hierarchical nature of document. Rooted trees, however, can effectively represent document layouts and logical structures [34]. Formal grammars, especially regular and context-free grammars, are also useful for describing document structures [35]. However, they can lead to multiple interpretations of documents structure. To address this, Tateisi et al. [36] proposed stochastic grammar to estimate the most probable interpretation of document. Despite their utility, stochastic grammars may lack the flexibility to model complex patterns and diverse data. Recently, deep learning methods have been proposed for tree-based document structure reconstruction. Wang et al. [37] focused on form understanding, treating forms as tree-like hierarchy of text fragments and using an asymmetric parameter matrix to predict relationships. This approach, however, resulted in high computational complexity for documents with many text fragments. DocParser [1] presented an end-to-end system for parsing the physical structure of documents, but it did not consider logical hierarchies and relied on rulebased algorithms, limiting its effectiveness. DSG [3] replaced the rule-based module with an LSTM-based network, making the system end-to-end trainable but still limited to single-page documents. Ma et al. [4] introduced the task of hierarchical document structure reconstruction and built the HRDoc dataset. They proposed an encoder-decoder based system to predict relationships between text-lines. While effective, it assumes given reading order and has high computational costs. Detect-Order-Construct [5] established the Comp-HRDoc benchmark and proposed tree construction approach for hierarchical document structure analysis. However, its multi-branch and multi-stage framework can introduce cascading errors and scalability challenges. In recent years, several end-to-end OCR-free methods have emerged, broadly categorized into two groups. The first group, including Donut [38], Dessurt [39], and Pix2Struct [40], focuses on document information extraction and question answering, directly extracting textual and semantic information from images without relying on OCR. The second group, such as DAN [41], Nougat [42], and GOT [43], specializes in document recognition or parsing. Nougat is designed for parsing academic documents, converting images into markup languages like Markdown. Its architecture, based on Donut, uses generative Transformer decoder to output document structures, including reading order and hierarchy, trained on millions of arXiv papers. GOT, on the other hand, leverages advancements in multimodal large language models, combining vision encoder with an LLM pre-trained on OCR-related datasets to bridge the gap between visual and textual information. While promising, OCR-free methods face challenges, such as the inability to predict the spatial 8 (a) Page-Level Relationship Definition (b) Document-Level Relationship Definition Figure 3: An example of our page-level and document-level problem definition for hierarchical document structure analysis. Purple arrow: intra-region relationship; green arrow: inter-region relationship; orange arrow: logical role relationship. Best viewed in color. For visual clarity, random relationships are omitted. layout of document elements (tables, figures, etc.) and difficulty handling multi-page documents. They also require large training datasets and can suffer from high inference latency. Nevertheless, these methods hold significant potential by eliminating cascading errors typical in OCR-based pipelines and unifying document understanding tasks into single end-to-end framework. In contrast, OCR-based approaches, while potentially introducing cascading errors, offer greater flexibility in designing task-specific models. In our work, we use OCR outputs alongside image features to predict logical relationships between document elements. By treating various HDSA sub-tasks as relation prediction problems within unified label space, our approach improves scalability, efficiency, and adaptability for both page-level and document-level tasks. 3. Problem Definition"
        },
        {
            "title": "Hierarchical document structure analysis involves understanding and defining the relationships within",
            "content": "a document at both the page-level and the document-level. These relationships help in identifying and organizing various text and non-text regions, facilitating tasks such as text region detection, logical role 9 classification, reading order prediction, table of contents extraction, hierarchical list extraction, and crosspage table grouping. 3.1. Page-Level Structure Analysis document image inherently comprises diverse regions, encompassing both Text Regions and Non-Text Regions. Text Region serves as semantic unit of written content, comprising text-lines arranged in natural reading sequence and associated with logical label, such as paragraph, list/list item, title, section heading, header, footer, footnote, and caption. Non-text regions typically include graphical elements like tables, figures, and mathematical formulas. Multiple logical relationships often exist between these regions, with the most common being the reading order relationship. Consequently, we define three distinct types of relationships at the page-level: Intra-region Relationships: As illustrated in Fig. 3(a), consider each text region, composed of several text-lines arranged in natural reading sequence. For each text region, we define two types of intra-region relationships represented by purple arrows in Fig. 3(a): (1) reading order relationship, which links adjacent text-lines in the sequence (e.g., Text-line Ti Text-line Ti+1), and (2) selfreferential relationship, which applies to single-line text regions (e.g., Text-line Ti Text-line Ti). Inter-region Relationships: To delve into the logical connections among these text regions and nontext regions, we construct inter-region relationships between all pairs of regions that exhibit logical connections. For instance, as depicted in Fig. 3(a), we define two types of inter-region relationships represented by green arrows in Fig. 3(a): (1) reading order relationship between adjacent text regions (e.g., Paragraph Pi Paragraph Pi+1), and (2) semantic association relationship between text regions and graphical objects (e.g., Caption Ci Table Gj or Table Gi Footnote Fj). Logical Role Relationships: As shown in Fig. 3(a), we delineate various logical role units, including caption, section heading, paragraph, title, etc. For each text-line in text region and each graphical object (e.g., table or figure), we establish logical role relationship, represented by orange arrows in Fig. 3(a), linking the text-line or graphical object to its corresponding logical role (e.g., Text-line Ti Title, Text-line Tj Paragraph, or Graphical object Gi Table). This relationship formalizes the assignment of logical roles to both text content and graphical elements in the document. By defining these relationships, we frame various page-level HDSA sub-tasks (such as text region detection, logical role classification, and reading order prediction) as relation prediction problems and merge the labels of different relation prediction tasks into unified label space, thereby employing unified model to handle these tasks concurrently. 3.2. Document-Level Structure Analysis Document-level structure analysis is crucial component of Hierarchical Document Structure Analysis. It extends the relationships identified at the page-level to encompass the entire document, facilitating more comprehensive understanding of the documents hierarchical structure and the logical connections among its various elements. We define two types of document-level relationships as follows: Intra-region Relationships: These relationships indicate that two parts of the document, which may appear on different pages, actually belong to the same logical unit. For example, table that starts on one page and continues on the next, or paragraph split across pages, would have an intraregion relationship (e.g., Sub-table Gi Sub-table Gi+1 or Sub-paragraph Pi Sub-paragraph Pi+1). These relationships are crucial for tasks such as cross-page table grouping and ensuring the continuity of content across pages. The purple arrows in Fig. 3(b) illustrates how intra-region relationships help maintain the coherence of content across different pages. Inter-region Relationships: These relationships define the hierarchical logical connections between logical document units, such as section headings and list items. For instance, section heading may logically connect to its subsections, and list items may also exhibit hierarchical relationships similar to section headings (e.g., Section heading Si Section heading Sj or List item Li List item Lj). Inter-region relationships help in identifying the documents structural hierarchy, enabling tasks like table of contents extraction and hierarchical list extraction. As illustrated in Fig. 3(b), inter-region relationships, represented by green arrows, establish the logical hierarchy, showing how different sections, lists, and other elements are interconnected. By defining these relationships, document-level structure analysis can address several complex tasks, including table of contents extraction, hierarchical list extraction, cross-page table grouping and cross-page paragraph grouping. These document-level relationships integrate seamlessly with page-level relationships to form comprehensive framework for hierarchical document structure analysis. In summary, defining relationships at both the page-level and the document-level provides robust framework for understanding and analyzing complex document structures. Page-level relationships focus on organizing and connecting text and non-text regions within single page, while document-level relationships extend these connections across the entire document, addressing hierarchical and cross-page coherence. By treating various HDSA sub-tasks as relation prediction problems and consolidating these relation prediction labels into unified label space, UniHDSA enables single relation prediction module to handle multiple tasks concurrently, either at the page-level or the document-level, paving the way for more advanced and accurate document processing applications. 11 4. Methodology 4.1. Overview The overall system architecture for hierarchical document structure analysis is depicted in Fig. 2. Following UniHDSA, this architecture systematically addresses document structure from both page-level and document-level perspectives, facilitating comprehensive understanding of the documents layout and logical relationships. Each module within this architecture is explored in dedicated subsections. The system begins with multimodal feature extraction module that utilizes vision backbone network and language model to capture detailed visual and semantic features from document renderings, providing robust foundation for subsequent analyses, as detailed in Section 4.2. Next, the page-level structure analysis module, discussed in Section 4.3, identifies and categorizes key document components, such as tables, figures, and text blocks, while establishing logical reading sequence that reflects the documents intended structure. The document-level structure analysis module, elaborated in Section 4.4, extends the analysis to encompass the entire document, identifying both short-range and long-range contextual relationships that contribute to holistic representation of the documents hierarchical organization. The integration of these modules within our system ensures comprehensive analysis that not only dissects individual pages but also synthesizes the documents hierarchical structure, offering profound insights applicable to various applications such as information retrieval and automated document summarization. 4.2. Multimodal Feature Extraction Module In HDSA scenarios, which predominantly involve rich text documents, employing multimodal methods proves to be more appropriate and intuitive. Unlike previous multimodal document layout analysis approaches [20, 22], which fuse textual and visual features at the pixel level, our method explicitly injects semantic representations into query features by modeling text-lines as object queries and employs late fusion strategy to obtain multimodal features. To this end, we utilize vision backbone, such as ResNet- [44], to extract visual features from document renderings. Concurrently, pre-trained language model, like BERT [45] and LayoutXLM [46], is employed to derive the textual representation of each text-line extracted from the document. These visual and textual features are then fed into subsequent structure analysis module, resulting in more robust representation for various downstream tasks. Formally, consider document image RHW 3 with text-lines, where and represent the height and width of the image, respectively. We have corresponding extracted data (ti, bi) for {1, . . . , }, where ti originates from the OCR result or PDF parser result, representing the i-th text-line. The data bi = [(x1 )] indicates the coordinates of the top-left and bottom-right corners of the bounding boxes corresponding to the i-th text-line. To extract visual features, we pass the document image through ), (x2 , y1 , y2 vision backbone, obtaining multi-scale feature maps {C3, C4, C5} for the page-level Transformer encoder. 12 Figure 4: Overall architecture of our page-level structure analysis module. The different colors of the cells represent distinct types of queries. Here, Lf , Lt, and Lr denote the length of multi-scale features, the number of text lines, and the number of predefined logical role queries, respectively. represents the embedding dimension, and is the number of selected graphical object proposals. Simultaneously, for the textual features, each text-line ti extracted from the OCR or PDF parser is fed into pre-trained language model to generate sequential text embeddings Ti Rd, where represents the feature dimension of the language model. 4.3. Page-level Structure Analysis Module Following the DETR-like model architecture, our page-level structure analysis module incorporates multi-layer Transformer encoder, multi-layer Transformer decoder, unified relation prediction head, and coarse-to-fine regression head. The overall pipeline is depicted in Fig. 4. After extracting multi-scale features {C3, C4, C5} from the multimodal feature extraction module, these features are then fed into the Transformer encoder, along with corresponding positional embeddings. To enhance computational efficiency in handling multi-scale features, we integrate deformable Transformer encoder to enhance these extracted features. Following the feature enhancement in the encoder, we employ type-wise query selection strategy to obtain reference box and category label for each potential graphical object proposal. For text-lines within the given document image, we have corresponding extracted data (ti, bi) and their text embeddings Ti for {1, . . . , }. These graphical object proposals and text-lines will be input as queries into the Transformer 13 decoder. To bolster the physical meaning of these queries and facilitate adaptive feature capturing from distinct regions for various types of queries, we introduce type-wise query initialization module to initialize type-wise queries as content queries for the subsequent decoder. Following the acquisition of positional queries, content queries, and their reference boxes, we leverage deformable Transformer decoder to enhance these queries. This involves incorporating self-attention module to model interactions among these queries, while deformable cross-attention module is employed to capture both global and local layout information from multi-scale feature maps. Taking inspiration from Deformable DETR [7], we adopt coarse-to-fine regression strategy to iteratively refine the reference boxes of graphical object queries layer-by-layer. Finally, to unravel the logical connections between these queries, we introduce unified page-level relation prediction head that effectively and efficiently handles page-level relation prediction tasks concurrently. 4.3.1. Type-wise Query Selection In the latest DETR variants [7, 24, 47], each object query consists of two parts: positional query and content query, which represent the positional information and semantic information of the object respectively."
        },
        {
            "title": "The positional query and content query are initialized randomly or selected from the feature maps generated",
            "content": "by the Transformer encoder. To address potential ambiguities and confusion for the decoder arising from selected encoder features, DINO [24] proposes mixed query selection approach. This approach selectively enhances only the positional queries with the top-K selected features, while keeping the content queries learnable as before. Although the approach in DINO has brought significant improvements, the unclear physical meaning of the learnable content query remains an issue. To address this, we introduce typewise query selection strategy, which involves leveraging potential class information to initialize the content query, thereby departing from the use of static content queries. Given the substantial differences in visual features among various types of graphical objects, such as formulas, tables, and figures, initializing content queries with category information will enable these queries to adaptively capture crucial features in the decoder. Specifically, we substitute the binary classifier in the auxiliary detection head with multi-class classifier to discern the class of each selected feature. While the predicted reference boxes are still utilized for initializing the positional queries, the predicted category is directed toward the subsequent type-wise query initialization module. Within this module, type-wise learnable content query is assigned for each type of query. 4.3.2. Type-wise Query Initialization Module We introduce type-wise query initialization module to standardize the modeling of logical relationships among different queries, ensuring uniform input into the decoder. As depicted in Fig. 4, the type-wise query initialization module takes three components as input: reference boxes and categories of graphical object proposals, bounding boxes and text embeddings of extracted text-lines, and predefined logical role types. For 14 Figure 5: The page-level unified label space in UniHDSA. Ti denotes Text-line queries, Gi denotes Graphical object queries, and Li denotes Logical role queries. The purple grids illustrate intra-region relationships, the green grids represent inter-region relationships, and the orange grids signify logical role relationships. graphical object proposals from the encoder, we initiate the positional queries by applying sine positional encoding [6] to the reference boxes. Simultaneously, we define learnable features for each category and initialize content queries by selecting the corresponding features based on the category. Similarly, for textlines, we adopt comparable approach. We begin by initializing positional queries based on the bounding box. To explicitly leverage the semantic representations of these text-lines, we use the corresponding text embeddings as the initialization for the content queries. Previous approaches to logical role classification typically used static parameter classifier, treating it as straightforward multi-class classification task. Inspired by dynamic algorithms [48, 49], we reformulate logical role classification as relationship prediction problem. In this framework, we establish both positional and content queries for predefined logical roles, such as titles, section headings, captions, etc. This approach allows the logical role query to dynamically adapt its feature extraction process to the specifics of each image. Each basic unit within an image is then tasked with predicting pointer toward these dynamic logical role queries, enhancing the models adaptability and responsiveness to unique image content. Additionally, we use learnable features for each logical role type as content query initialization. For uniformity in query structure, corresponding learnable reference boxes are defined for each logical role type. During training, we introduce auxiliary supervision by utilizing the union boxes of all queries belonging to specific logical role as the reference box supervision for that role. 4.3.3. Unified Page-Level Relation Prediction Head Following the enhancement process in the decoder layers, three types of queries including text-line queries, graphical object queries, and logical role queries, are input into our unified page-level relation prediction head to uncover the logical connections among these queries. As discussed in Section 3.1, we define three distinct types of relationships at the page-level: intra-region relationships, inter-region relationships, and logical role relationships. To effectively and efficiently handle these relation prediction tasks concurrently, 15 we introduce unified label space approach, as illustrated in Fig. 5. Specifically, we define label matrix ZHW , where each element in the i-th row and j-th column can take on four possible values. Taking Fig. 5 as an example, the empty cell in the label matrix signifies no logical relationship pointing from the i-th query to the j-th query. The other three types of cells represent three predefined relationships, each with its distinct interpretation. With this unified label space, our unified page-level relation prediction head consists of two modules: relation prediction module and relation classification module. Relation Prediction Module. Taking into account the logical relationships between text-line/graphical object queries and their connections to logical role queries, we group all text-line and graphical object queries as {q1, q2, ..., qH } and logical role queries as {qH+1, qH+2, ..., qW }. We calculate the scores sij, representing the probability of logical relationship from qi to qj, as follows: fij = sij = (qi) r k(qj), H, (cid:80)H exp(fij ) j=1 exp(fij ) , (cid:80)W exp(fij ) j=H+1 exp(fij ) , < (1) (2) where each of k represents single fully-connected layer with 1,024 nodes (the superscript indicates the FC layer used in the Relation Prediction Module), serving to map qi and qj into distinct and feature spaces; denotes dot product operation. Relation Classification Module. We use multi-class classifier to determine the relation type between qi and qj by computing the probability distribution across various classes. The process unfolds as follows: pij = BiLinear(F (qi), c k(qj)), H, cij = argmax(pij) (3) (4) where both k represent single fully-connected layers with 1,024 nodes (the superscript indicates the FC layer used in the Relation Classification Module); BiLinear signifies the bilinear classifier; and and argmax identifies the index cij with the highest value in the probability distribution pij, serving as the predicted relation type. 4.4. Document-level Structure Analysis Module Hierarchical document structure analysis examines the logical relationships among document entities, not just within individual pages but also across the entire document. Due to the variable length of documents and the relative scarcity of document entities that have relationships across pages, such as section headings and list items, considering all entities at the document-level is highly inefficient. Therefore, we divide the task of HDSA into page-level structure analysis and document-level structure analysis. As presented in the 16 Figure 6: Overall architecture of our document-level structure analysis module. page-level structure analysis module, the queries for each document entity capture both local and global features within the page. Consequently, in the document-level structure analysis phase, we only employ the self-attention mechanism to consider interactions between entities across different pages. We use the queries for specific types of document entities, output from the page-level structure analysis module, as inputs for the document-level structure analysis module. This approach allows the entire model to be trained end-to-end, enabling the queries to simultaneously focus on robust features at both the page and document-levels. Specifically, our document-level structure analysis module consists of multi-layer Transformer encoder and unified relation prediction head, as depicted in Fig. 6. Here, we primarily focus on four types of document-level tasks: table of contents extraction, hierarchical list extraction, cross-page table grouping, and cross-page paragraph grouping. Consequently, only section headings, list items, tables, and the first and last paragraphs on document pages need to be processed. We first group the corresponding queries into text block queries QB for 1, ..., based on the predictions of text-line grouping and logical role classification from the page-level structure analysis module, as follows: = (cid:0)mean(qi QB 1, . . . , qi k)(cid:1) (5) where represents single fully-connected layer that maps the average features of the queries {qi k} belonging to the same text block into text block feature. Based on the table detection results at the 1, ...qi 17 Figure 7: The document-level unified label space in UniHDSA. Bi denotes Text-block queries, and Gi denotes Graphical object queries. The purple grids illustrate intra-region relationships, and the green grids represent interregion relationships. page-level, we select the corresponding table queries as graphical object queries QG for {1, ..., L}. Subsequently, we sort these queries based on the reading order sequence predicted at the page-level and input them into the document-level Transformer encoder to further enhance the representations. Following DetectOrder-Construct [5], to incorporate the relative position in the reading order sequence and accommodate longer document, we utilize an efficient and effective relative positional encoding method, RoPE [50], in the Transformer encoder. The enhanced text block queries EB and enhanced graphical object queries EG are obtained as follows: (EB 1 , ..., EB K, EG 1 , ..., EG ) = TransformerEncoder((QB 1 , ..., QB K, QG 1 , ..., QG ), RotaryP osEmb) (6) where TransformerEncoder() represents the multi-layer Transformer encoder processing the input queries QB and QG , and EB and EG denote the corresponding enhanced query representations. After the enhancement process in the encoder layers, these enhanced text block queries and graphical object queries are fed into unified relation prediction head to uncover the document-level logical connections among them. As outlined in Section 3.2, we define two distinct types of relationships at the document-level: intra-region relationships and inter-region relationships. We employ the same unified label space approach used in the page-level structure analysis module to concurrently handle document-level relation prediction tasks. Unlike the page-level label space, the document-level label space does not consider logical role relationships. Consequently, the document-level label matrix is square matrix. As illustrated in Fig. 7, 18 each element in this matrix, located at the i-th row and j-th column, can take on one of three possible values: an empty cell, or one of two types of cells representing the two predefined relationships. Due to the similarity between unified relation prediction at the page-level and the document-level, the unified documentlevel relation prediction head is identical to the unified page-level relation prediction head. Further details about this head can be found in Section 4.3.3. 4.5. Optimization 4.5.1. Loss for Graphical Page Object Detection In our page-level structure analysis module, we utilize the same detection heads as those in Deformable DETR [7], with the exception of replacing the binary classifier in the encoder with multi-class classifier. The loss function for graphical page object detection in the decoder, Ldec graphical, is identical to that used in Deformable DETR, consisting of multiple bounding box regression losses and classification losses derived from the prediction heads. The bounding box regression loss is linear combination of the L1 loss and the GIoU loss [51], while the classification loss is the focal loss [52]. In the two-stage Deformable DETR, the classification loss used in the Transformer encoder is binary classification loss to determine whether query belongs to foreground objects or the background. The total loss in the decoder, Ldec graphical, is applied to each layer of the decoder and is defined as:"
        },
        {
            "title": "Ldec",
            "content": "graphical = (cid:88) (cid:88) (cid:104) λcls Lcls(classl,i pred, classi target) + λL1 L1(bboxl,i pred, bboxi target) + λGIoU LGIoU(bboxl,i pred, bboxi (cid:105) target) , (7) where indexes the decoder layers, indexes the predicted and target pairs, and λcls, λL1, and λGIoU are weights for the classification, L1, and GIoU losses, respectively. For the encoder, the loss is applied only to the final layer. The total loss in the encoder, Lenc graphical, is defined as:"
        },
        {
            "title": "Lenc",
            "content": "graphical = (cid:88) (cid:104) λcls Lcls(classi pred, classi target) + λL1 L1(bboxi pred, bboxi target) + λGIoU LGIoU(bboxi pred, bboxi target) (cid:105) , (8) where the classification loss Lcls is adjusted for multi-class classification in our encoder. All these hyperparameters follow the settings used in Deformable DETR. 4.5.2. Loss for Unified Relation Prediction The unified page-level relation prediction head and the unified document-level relation prediction head follow the same structure. Consequently, the loss functions used for these two modules are also identical. The unified relation prediction head consists of two prediction modules: relation prediction module and 19 relation classification module. Inspired by [27], for the relation prediction module, we adopt softmax cross-entropy loss as follows: Lrel ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) (cid:88) LCE (si, ) , (9) where si = [si1, si2, . . . , siN ] is the predicted relation score vector calculated by Eqs. 1-2, and is the target label. We also adopt softmax cross-entropy loss for the relation classification module, which is defined as: Lrel cls ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) (cid:88) LCE (ci, ) , (10) where ci is the predicted relation label calculated by Eqs. 3-4, and is the corresponding ground-truth label. The presented in Eq. 9 and Eq. 10 indexes the decoder layers. In summary, the total loss for both page-level and document-level relation prediction is given by: relation = λrel Lpage Lpage relation = λrel Ldoc Ldoc rel + λrel cls Lpage rel + λrel cls Ldoc rel cls rel cls (11) (12) Here, λrel and λrel cls are weights for the relation prediction and relation classification losses, respectively, and are both set to 1.0 in this work. 4.5.3. Overall Loss All the components in our approach are jointly trained in an end-to-end manner. The overall loss is the sum of Lenc graphical, Ldec graphical, Lpage relation and Ldoc relation: Loverall = λgraphical (Lenc graphical + Ldec graphical) + λpage relation Lpage relation + λdoc relation Ldoc relation . (13) Here, λgraphical, λpage relation, and λdoc relation are all set to 1.0. 5. Experiments 5.1. Datasets and Evaluation Protocols In our conference paper [9], we conducted experiments on two benchmarks: the large-scale document layout analysis benchmark, DocLayNet [8], and the comprehensive hierarchical document structure analysis benchmark, Comp-HRDoc [5]. These experiments validated the effectiveness of our unified relation prediction approach for page-level document layout analysis tasks."
        },
        {
            "title": "It is important to highlight that the",
            "content": "DLAFormer approach proposed in our conference paper solely considered visual information for page-level structure analysis. In contrast, UniHDSA not only incorporates the semantic information of each text-line but also includes document-level structure analysis module for document-level tasks. Consequently, in this work, we conduct more extensive experiments on these two benchmarks to demonstrate the effectiveness of 20 our approach. Given that Comp-HRDoc is constructed based on HRDoc and evaluates broader range of hierarchical document structure analysis sub-tasks, we have opted not to conduct experiments on HRDoc. Additionally, due to the limited scenario and restricted definition of logical categories in PubLayNet [11], using DocLayNet for evaluating the performance of page object detection is more reasonable. 6Doc [53] is more recent page object detection dataset; however, it defines text blocks based on physical layout analysis, meaning that single paragraph may be split into multiple text blocks due to layout constraints. In contrast, our method is designed to generate semantically complete paragraph, ensuring the preservation of its full meaning rather than producing fragmented text blocks. Given this fundamental difference in problem formulation, 6Doc is not suitable benchmark for evaluating our approach. Comp-HRDoc [5] is an extensive benchmark specifically designed for comprehensive hierarchical document structure analysis.2 It encompasses range of document layout analysis tasks, including page-level tasks such as page object detection and reading order prediction, as well as document-level tasks like table of contents extraction and hierarchical structure reconstruction. Comp-HRDoc is constructed based on the HRDoc-Hard dataset [4], which includes 1,000 documents for training and 500 documents for testing. In the page object detection task, which covers sub-tasks such as graphical page object detection, text region detection, and logical role classification, the COCO-style Segmentation-based mean Average Precision (Segm. mAP) is used as the evaluation metric. Additionally, the assessment of the reading order prediction task includes Reading Edit Distance Score (REDS), which encompasses both Text Region REDS and Graphical Region REDS as proposed in [5]. For the table of contents extraction and hierarchical structure reconstruction tasks, the Semantic-TEDS [4] is employed as the evaluation metric. DocLayNet [8] stands out as challenging human-annotated dataset for page object detection recently introduced by IBM. This dataset comprises 69,375 pages for training, 6,489 for testing, and 4,999 for validation. Spanning various document categories such as Financial reports, Patents, Manuals, Laws, Tenders, and Scientific Papers, DocLayNet includes 11 pre-defined types of page objects. These objects encompass Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text (i.e., Paragraph), and Title. The evaluation metric of DocLayNet is the COCO-style box-based mean Average Precision (mAP) at multiple intersections over union (IoU) thresholds between 0.50 and 0.95 with step of 0.05. In addition to mAP, the F1 score is also crucial metric for page object detection [10], as it helps explore the best trade-off between the precision and recall of different approaches. Following the definition of mAP, the mean F1 score is calculated at multiple IoU thresholds ranging from 0.50 to 0.95, with increments of 0.05. In addition to document images, both datasets provide OCR files containing information about bounding boxes and the reading order of text-lines. Notably, Comp-HRDoc has pre-filtered text-lines within graphical 2The dataset and evaluation code for Comp-HRDoc are available at https://github.com/microsoft/CompHRDoc. 21 objects. However, OCR files of DocLayNet present challenge with considerable number of text-lines within graphical objects, creating potential long-tail problem for unified models in logical role classification and relation prediction. 5.2. Implementation Details Our approach is implemented using PyTorch v1.11, and all experiments are conducted on workstation equipped with 16 Nvidia Tesla V100 GPUs (32 GB memory). In the page-level structure analysis module, both the Transformer encoder and decoder are configured with 3 layers. Both are designed with the number of heads, the dimension of the hidden state, and the dimension of the feedforward network set as 8, 256, and 1024, respectively. In the document-level structure analysis module, the document-level Transformer encoder is also configured with 3 layers. The number of heads, the dimension of the hidden state, and the dimension of the feedforward network are set as 8, 256, 2048, respectively. For the initialization of graphical object queries, we opt for the top-50 encoder features on Comp-HRDoc and the top-100 features on DocLayNet. In training phase, the parameters of CNN backbone network are initialized with ResNet18 or ResNet-50 model [44] pre-trained on ImageNet classification task, while the parameters of the text embedding extractor are initialized with the pretrained BERTbase model. We refer to the models using ResNet-18 and ResNet-50 as the backbone as UniHDSA-R18 and UniHDSA-R50, with sizes of 150M and 162M, respectively. Due to the significant variation in document lengths, training an end-to-end model to process an entire long document as single sample is challenging. To address this, we adopt scalable approach by sampling sequences of consecutive pages from document as individual samples. Experiments demonstrate that our method, when trained with samples of 6 to 8 pages, still achieves excellent results on test sets consisting of complete documents of around 20 pages. The optimization process employs AdamW algorithm [54] with mini-batch size of 1 (i.e., one sub-document sampled from longer document), trained for 40 epochs on Comp-HRDoc and mini-batch size of 4, trained for 24 epochs on DocLayNet. For the CNN backbone network, the learning rate and weight decay are set to 4e-5 and 1e-2, respectively. For the pretrained BERT model, these values are set to 8e-5 and 1e-2, respectively. For all other parameters, the learning rate and weight decay are set to 4e-4 and 1e-2, respectively. Other hyper-parameters of AdamW, including betas and epsilon, are set as (0.9, 0.999) and 1e-8. Additionally, multi-scale training strategy is adopted, where the shorter side of each image is randomly rescaled to length chosen from [320, 416, 512, 608, 704, 800], while the longer side should not exceed 1024. In testing phase, we set the shorter side of input image as 512. As mentioned earlier, the large number of text-lines within graphical objects in DocLayNet gives rise to serious long-tail problem in classification during training. Therefore, drawing inspiration from [55], we opt for the logit-adjusted softmax cross-entropy loss when training on DocLayNet. Additionally, we conducted an in-depth analysis of the OCR files in DocLayNet and found that single text-line is often fragmented into multiple words or characters. This fragmentation poses challenges for 22 Table 1: Comparison results of different models in tasks including page object detection, reading order prediction, table of contents extraction and hierarchical document reconstruction on Comp-HRDoc (in %). The symbol indicates that the evaluation of this result relies on the provided reading order ground-truth and bounding box ground-truth for graphical objects. The results of Mask2Former, Lorenzo et al., MTD, and DSPS Encoder are extracted from [5]. R18 and R50 refer to the ResNet-18 and ResNet-50 backbones, respectively. The average performance standard deviation for UniHDSA-R18 is 0.16, while for UniHDSA-R50, it is 0.23."
        },
        {
            "title": "Methods",
            "content": "Mask2Former [56] Lorenzo et al. [33] MTD [57] DSPS Encoder [4] DOC-R18 [5] Ours (UniHDSA-R18) Ours (UniHDSA-R50)"
        },
        {
            "title": "Graphical Region",
            "content": "Micro-STEDS Macro-STEDS Micro-STEDS Macro-STEDS mAP 73.54 - - - 88.1 90.9 91."
        },
        {
            "title": "REDS",
            "content": "- 77.4 - - 93.2 96. 96.7 - 85.8 - - 86. 90.6 91.0 - - 67.6 57. 86.1 87.9 88.3 - - 71. 62.3 87.9 89.5 88.8 - - - 69.0 83.7 88.0 88.9 - - - 69.7 83.7 87.8 88. bottom-up methods in understanding the semantic content of text-lines. Therefore, we adopt heuristic method to group the separated words or characters into coherent text-line. By calculating the horizontal and vertical distances between each pair of text units, we can determine whether these distances are less than the average height of all text units in the document page. If both distances fall below this threshold, the two text units will be grouped into single text-line. Detailed configuration settings can be found at https://github.com/microsoft/CompHRDoc/tree/main/UniHDSA. 5.3. Comparisons with Prior Arts In this section, we compare UniHDSA with several state-of-the-art methods on Comp-HRDoc and DocLayNet to showcase the effectiveness of UniHDSA. Comp-HRDoc. As shown in Table 1, we provide comprehensive performance evaluation for all tasks in Comp-HRDoc, including page object detection, reading order prediction, table of contents extraction, and hierarchical document reconstruction. Previous works primarily focused on individual subtasks, with DOC [5] being the first end-to-end system for hierarchical document structure analysis. DOC consists of four modules, each tailored to specific subtask in HDSA: text region detection, graphical object detection, reading order prediction, and table of contents extraction. Despite its strong performance on Comp-HRDoc compared to earlier models, the design of separate modules can introduce cascading errors and neglect the implicit information interaction among these modules. In contrast, our unified relation prediction approach, UniHDSA, models the relationship among document components uniformly at both the page-level and document-level through the introduction of unified label space. As illustrated in Ta23 Table 2: Performance comparison on DocLayNet testing set using Average Precision (in %). Bold indicates the SOTA result, and underline indicates the second-best result. The symbol represents the results of our replication, excluding duplicate box predictions. Method Model CapFootForList- PagePagePicSection- tion note mula item footer header ture header Table Text Title mAP Human [8] - 89 91 88 94 81 71 84 86 72 83 Faster R-CNN [8] R101 70. 73.7 63.5 81.0 58.9 72.0 72. 68.4 82.2 85.4 79.9 73.4 Mask R-CNN [8] R101 71.5 71.8 63.4 80.8 59. 72.0 72.7 69.3 82.9 85.8 80. 73.5 YOLOv5 [8] v5x6 77.7 77.2 66. 83.3 61.1 70.7 77.1 74.6 86. 88.1 82.7 76.8 SwinDocSegmenter [16] Swin 83. 84.7 64.8 82.3 65.1 66.4 84. 66.5 87.4 88.2 63.3 76.9 DINO [24] R101 71.1 78.8 72.6 83.4 65. 76.6 74.1 82.5 83.4 79.4 84. 77.7 VSR [20] R101 72.6 72.1 68. 83.6 81.8 84.1 63.1 82.5 79. 84.4 73.1 78.4 DOC [5] R50 83. 69.7 63.4 88.6 90.0 76.3 81. 83.2 84.8 84.8 84.9 81.0 DLAFormer R50 89.7 63.1 81.1 86.0 88. 90.5 82.4 87.7 85.3 83.5 83. 83.8 M2Doc (DINO) [22] R50 90.0 75.5 88.5 91. 90.1 87.3 85.3 89.3 86.2 92. 83.9 87.3 Ours (UniHDSA-R50) R50 90.8 74. 86.0 89.4 96.0 89.5 82.2 86. 86.9 86.8 88.4 87.0 ble 1, UniHDSA achieves state-of-the-art results across all subtasks in Comp-HRDoc, demonstrating the effectiveness of our approach. Notably, although DOC employs two-stage approach for predicting reading order relationships, which necessitates additional parameters, UniHDSA still achieves improvements of 3.2% in text region reading order prediction and 4.2% in graphical region reading order prediction. For the table of contents extraction task, DOC introduced an effective decoding strategy that combines parent finding and sibling finding predictions. In contrast, our approach maintains scalability and uniform prediction for each document-level task without leveraging such complex strategy, yet still surpasses DOC by 1.8% and 1.6% in Micro-STEDS and Macro-STEDS of the table of contents extraction task, respectively. These results highlight the effectiveness of our unified relation prediction approach in reducing cascading errors and enhancing document analysis accuracy. DocLayNet. We benchmark our approach against other highly competitive methods on the DocLayNet testing set using the Average Precision metric, including state-of-the-art object detection methods such as DINO [24] and YOLOv5 [58], as well as multimodal approaches like DOC [5] and M2Doc [22]. As shown in Table 2, our approach surpasses all these vision-based object detection methods. M2Doc significantly improved the performance of the top-down method DINO in the page object detection task by employing two multimodal fusion methods: early fusion and late fusion. However, we observed critical flaw in these top-down methods: their unreasonable post-processing approach allows one bounding box to generate multiple prediction category results. While this approach can enhance AP performance, it negatively impacts 24 Table 3: Performance comparison on DocLayNet testing set using F1 scores (in %). The symbol represents the results of our replication. Method Model CapFootFor- ListPagePagePic- Sectiontion note mula item footer header ture header Table Text Title mean F1 score M2Doc (DINO) w/o Early Fusion R50 86.1 73.2 71.5 84.0 72. 69.2 82.9 68.5 84.4 83.4 83. 78.1 DLAFormer R50 90.1 68.8 81. 87.9 92.5 89.9 81.9 89.1 82. 87.6 81.6 84.9 M2Doc (DINO) R50 90. 83.2 88.3 90.7 90.7 88.1 84. 88.7 82.8 91.1 84.6 87.5 Ours (UniHDSA-R50) R50 91.4 80.4 85.4 91.4 97. 91.9 79.8 89.1 82.2 89.8 88. 87.9 subsequent document structure analysis tasks. For example, when restoring the markdown structure of the entire document, one paragraph cannot be assigned to two different categories simultaneously, leading to inconsistencies and errors in the documents hierarchical representation. To address this issue, we used the model weights officially provided by M2Doc and modified the post-processing code to ensure that each box is restricted to generating only one category, aligning it with our method. The final results indicate that, despite our method being based on smaller and simpler baseline model, Deformable DETR [7], it still achieves performance comparable to M2Doc. Due to the efficiency drawbacks of the early fusion method used in M2Doc, our approach leverages only the late fusion method, which is both effective and efficient. Moreover, we benchmark our approach against other methods on the DocLayNet testing set using the F1 score metric. As illustrated in Table 3, our approach achieves the best trade-off between precision and recall. To further demonstrate the effectiveness of our method, we present detailed comparison of scores for M2Doc and UniHDSA at various IoU thresholds, as depicted in Fig. 8. From the bar chart, we can observe that UniHDSA consistently performs well across all IoU thresholds, especially at the highest threshold (0.95), where M2Docs performance drops significantly. This suggests that UniHDSA, hybrid approach, is more robust in maintaining high accuracy even when stricter overlap criteria are applied. 5.4. Discussion 5.4.1. Effectiveness of Unified Modeling of Text and Graphical Elements Previously, DOC utilized two separate branches to handle Text Region Detection and Graphical Object Detection: bottom-up approach to group text-lines into text blocks, and top-down approach to directly detect the bounding boxes of graphical objects. While these two branches shared feature maps, they had no other interactions. Our method introduces unified label space approach and page-level Transformer to learn the unified representation of various document elements and their relationships. This approach not only eliminates the need for multi-stage page-level structure analysis but also leverages the interaction among text-lines and graphical objects, thereby enhancing both relationship prediction and graphical object detection. 25 Figure 8: Comparison of F1 Scores (in %) for M2Doc and UniHDSA at various IoU thresholds on DocLayNet testing set. Table 4: Ablation study of unified modeling of text and graphical elements on DocLayNet testing set (%). Method Model CapFootForList- PagePagePicSection- tion note mula item footer header ture header Table Text Title mAP UniHDSA-R50 R50 91.2 74.8 86. 89.3 96.1 89.8 82.2 UniHDSA-R50 90.4 75.5 59.6 88.2 94.7 88. 82.9 86.3 84.7 86.5 86.4 88. 87.1 84.5 85.5 83.5 83.5 with Attention Mask (-0.8) (+0.7) (-26.6) (-1.1) (-1.4) (-0.9) (+0.7) (-1.6) (-2.0) (-0.9) (-5.2) (-3.6) To demonstrate the effectiveness of the unified modeling of text and graphical elements, we conduct an experiment on DocLayNet testing set by setting an attention mask to prevent interaction between text-line queries and graphical object queries. As illustrated in Table 4, the performance of graphical objects, such as formulas and tables, drops significantly after removing the unified modeling. This indicates that text-lines within graphical objects are very helpful for top-down graphical object detection and can effectively provide more information for these text-rich graphical objects. Furthermore, we observe that the detection results of most text regions have also decreased to certain extent, further demonstrating the effectiveness of unified modeling of text and graphical elements. 5.4.2. Scalability for Long Documents Although our method is trained on sampled documents of 6-8 pages, it consistently outperforms previous approaches on the Table of Contents Extraction tasks, as illustrated in Table 1, which were trained on entire documents. As shown in Table 5, we conduct an ablation study by training our model on larger sampling window (14-16 pages) and find that the overall performance shows no significant improvement. This suggests that our method demonstrates strong robustness even when trained on smaller windows. Furthermore, in scenarios involving long documents, such as financial reports, our sampling-based approach offers notable 26 Table 5: Ablation study on training with larger sampling window. Methods UniHDSA-R18 + Larger Sampling Window Page Object Detection Reading Order Prediction Table of Contents Extraction Hierarchical Reconstruction Segmentation Text Region Graphical Region mAP 90.9 90. REDS REDS 96.4 96.4 90.6 90. Micro-STEDS Macro-STEDS Micro-STEDS Macro-STEDS 87.9 88.7 89.5 89.9 88. 88.2 87.8 88.0 Figure 9: An example illustrating the effectiveness of our method on 44-page document. The top images show the Table of Contents extracted from the original PDF. The bottom images show the Table of Contents predicted by our proposed method. efficiency advantage, particularly when GPU resources are limited. One potential reason for this success is the introduction of data diversity through sampling, which enables the model to learn more robust and generalizable features for document-level relation prediction. The sampling strategy helps the model to adapt better to varying document structures, leading to improved performance across different document lengths. Additionally, our approach is fully end-to-end, allowing the page-level module to adaptively extract robust features directly from images, which in turn support document-level tasks. This ensures that the documentlevel module can effectively interact and utilize these features to perform comprehensive document structure analysis. This two-stage process, where document-level and page-level features are processed and integrated, allows our method to efficiently and effectively handle the hierarchical structure of long documents. These observations confirm that dividing the hierarchical document structure analysis into two stages not only enhances scalability but also ensures accurate and reliable performance on tasks involving long and complex documents. 27 Table 6: Analysis of the impact of incorporating semantic features in various models for the page object detection task on Comp-HRDoc (in %). The symbol indicates the results from our replication. R18 and R50 refer to the ResNet-18 and ResNet-50 backbones, respectively. Segm. mAP 83.4 88.1 Method Title Author Mail Affil Sect Para Table Figure Cap Foot Head Footn DOC-R18 [5] DOC-R18 (w/ Bert) [5] DLAFormer-R18 Ours (UniHDSA-R18) DOC-R50 [5] DOC-R50 (w/ Bert) [5] DLAFormer-R50 Ours (UniHDSA-R50) 94.5 94.6 79. 90.7 51.6 84.5 70.9 84.9 89. 83.4 80.3 89.5 81.7 80.6 86. 86.7 89.0 89.4 95.9 95.1 84. 96.4 95.0 82.9 (+0.1) (+11.3) (+32.9) (+14.0) (-0.2) (-1.7) (+0.3) (+0.3) (+0.4) (+0.5) (-0.1) (-1.8) (+4.7) 96.9 97. 89.4 91.5 70.1 86.5 84.5 90. 91.8 86.3 79.5 83.9 92.1 97. 98.3 90.8 88.4 93.0 86.3 80. 84.4 93.7 98.8 97.4 91.3 90. (+0.1) (+2.1) (+16.4) (+6.1) (+1.2) (0.0) (+0.6) (+0.5) (+1.6) (+1.4) (-0.9) (+0.5) (+2.5) 95.3 94.1 85.3 84.3 65. 84.3 77.6 86.1 91.0 85.0 80. 86.6 91.9 96.4 95.6 88.7 86. 90.2 82.9 80.8 86.4 91.7 96. 95.6 88.7 88.5 (-1.2) (-1.0) (+19.3) (+8.5) (-0.8) (-2.1) (+0.4) (-0.2) (-0.2) (0.0) (0.0) (0.0) (+2.0) 96.8 97. 91.1 93.5 75.4 86.3 86.0 89. 92.8 87.7 79.4 85.5 92.5 97. 98.2 91.7 89.6 92.9 87.5 80. 85.5 94.5 97.8 97.4 92.2 91. (+0.2) (+2.4) (+10.9) (+3.6) (+0.1) (-0.2) (+0.7) (0.0) (+2.0) (0.0) (-0.8) (+0.5) (+1.6) To illustrate the effectiveness of our method on long documents, consider the example3 shown in Fig. 9. Although our model is trained on much shorter document samples, it shows promising results when applied to complex 44-page document. The prediction presented in Fig. 9 is nearly complete, missing only one ambiguous section heading, Abstract, and adding two in-line subsections, 3.1.1 and 4.1.2 Vision Generative Models.. Nevertheless, the hierarchy of the table of contents remains perfectly intact. This figure demonstrates the performance of our model for table of contents extraction on 44-page document, highlighting its ability to maintain high accuracy and reliability across significantly longer document. The results clearly indicate that our method is robust and scalable, capable of handling extensive documents without compromising on performance. 5.4.3. Impact of Incorporating Semantic Features UniHDSA, as relation prediction approach, explicitly models the representation of each text-line and groups these text-lines into logical text blocks by predicting their relationships. Therefore, unlike M2Doc, which implicitly fuses the semantic features into visual feature maps, UniHDSA directly fuses the semantic feature of each text-line with its corresponding visual feature, i.e., using late fusion strategy. To analyze the impact of incorporating semantic features, we conducted series of experiments using two similar approaches, DOC [5] and UniHDSA, for the page object detection task on the Comp-HRDoc dataset. The experimental results are summarized in Table 6, from which we can draw two key conclusions. 3The original document can be accessed at https://arxiv.org/pdf/2303.04226. 28 Table 7: Relation prediction performance at page-level and document-level on Comp-HRDoc (%). Intra denotes Intra-region Relationships, Inter denotes Inter-region Relationships, and Logical Role denotes Logical Role Relationships."
        },
        {
            "title": "Relationship\nCategory",
            "content": "UniHDSA-R18 UniHDSA-R50 Precision Recall F1 Score Precision Recall F1 Score Page-Level Document-Level"
        },
        {
            "title": "Intra\nInter",
            "content": "97.2 93.6 98.0 88.2 92.1 97.1 92.4 97.8 87.3 93.2 97.2 92.8 97.9 86.9 92. 97.4 93.6 98.1 88.8 93.2 97.3 93.2 98.0 88.1 93.1 97.3 93.3 98.1 87.7 93. Firstly, for models that have not been pre-trained for visual-language alignment, the proposed late fusion strategy does not universally enhance performance across all categories. For categories with welldefined semantic patterns, such as author, mail, affiliate, and caption, incorporating semantic information significantly boosts performance. However, in categories where semantic ambiguity is more common, such as header and title, this approach can be counterproductive. These categories, despite their semantic similarity, exhibit distinct visual differences, making it more effective to rely solely on visual information for better results. For instance, the classification for header and title might suffer from semantic confusion, but these two categories visual features are sufficiently distinct to allow for accurate predictions based solely on visual cues. Despite some ambiguity in the header category, our method is generally more robust than DOC in integrating multimodal features. Secondly, the impact of semantic information on enhancing performance decreases as the visual capabilities of the models improve. The results indicate that DLAFormer [9], which excels in visual representation, achieves performance comparable to DOC, even when DOC utilizes multimodal information. Notably, the performance gain from incorporating semantic information in DLAFormer is less pronounced compared to DOC. This suggests that as the visual models capability increases, the marginal benefits of incorporating semantic information diminish. Furthermore, upgrading to more powerful backbone further reduces the improvement gains, underscoring that the primary driver of performance in these scenarios is the models visual capability, with semantic features serving only as supplementary factor. Therefore, effectively coordinating multimodal information remains crucial area of research. 5.4.4. Analysis of Relation Prediction Performance Our approach defines various tasks as relation prediction problems, addressing relationships at both page-level and document-level. This enables more intuitive evaluation of the systems ability to capture diverse relationships. As shown in Table 7, we evaluated UniHDSA-R18 and UniHDSA-R50 on different Table 8: Comparison results of different backbones with UniHDSA on Comp-HRDoc (in %). Backbones Page Object Detection Reading Order Prediction Table of Contents Extraction Hierarchical Reconstruction Segmentation Text Region Graphical Region Micro-STEDS Macro-STEDS Micro-STEDS Macro-STEDS ResNet-18 (Sampling Window Size [6, 8]) ResNet-50 (Sampling Window Size [6, 8]) ResNet-101 (Sampling Window Size [6, 8]) Swin-Tiny (Sampling Window Size [6, 8]) InternImage-Tiny (Sampling Window Size = 5) InternImage-Small (Sampling Window Size = 4) mAP 90. 91.2 90.8 91.1 91.1 91.7 REDS REDS 96.4 96.7 96.5 96.6 96. 96.8 90.6 91.0 89.6 90.9 90. 90.7 87.9 88.3 87.6 87.9 85. 83.3 89.5 88.8 89.3 89.3 87. 85.7 88.0 88.9 87.8 88.2 88. 87.5 87.8 88.6 87.7 88.0 88. 87.5 types of relationships, measuring macro F1 score, precision, and recall across all documents. The results show that page-level relationships achieve over 93% in all metrics, demonstrating the effectiveness of our unified relation prediction approach. Notably, intra-region and logical role relationships exceed 97%, highlighting exceptional model performance. For document-level relationships, intra-region performance is lower, reflecting the difficulty of the cross-page paragraph grouping task in the Comp-HRDoc dataset. This task requires resolving the lack of contextual connections across pages and addressing ambiguities caused by similar content. On the other hand, for inter-region relationships at the document-level, which primarily involve hierarchical structure extraction, our method also performs strongly, with an F1 score exceeding 93%. This further validates the robustness of our approach in handling complex document structures effectively. Overall, the results affirm the strength of our unified relation prediction framework, particularly for page-level tasks, while also pointing out potential areas for enhancement in cross-page tasks. 5.4.5. Impact of Combining Different Vision Backbones with UniHDSA To investigate the impact of various vision backbones on hierarchical document structure analysis, we conducted experiments by integrating backbones of different depths and architecturesincluding the ResNet series, Swin-Transformer [59], and InternImage [60]with UniHDSA on the Comp-HRDoc dataset. Due to GPU memory constraints, we only employ smaller sampling window size for InternImage-Tiny and InternImage-Small during training, which negatively affects document-level tasks such as table of contents extraction and hierarchical reconstruction. For all other backbones, we maintain consistent sampling window size. As shown in Table 8, UniHDSA paired with ResNet-50 achieves slightly superior overall performance compared to the other configurations. This suggests that the integration of the language model enables different vision backbones with similar number of parameters to perform comparably on CompHRDoc dataset. Notably, InternImage-Small, despite using smaller sampling window size, achieves markedly better performance on the page object detection task due to its larger parameter count. This indicates that with sufficient computational resources, larger vision backbones still have the potential to drive further performance improvements. 30 Figure 10: Visualization of hierarchical document structure analysis generated by our system. The orange boxes represent meta-information such as page headers and page numbers. The blue boxes denote individual text blocks, while the green boxes highlight captions and graphical objects. The blue arrows indicate the logical reading order between text blocks, whereas the green arrows depict the association between graphical objects and their respective captions. 5.4.6. Hierarchical Document Structure Analysis Visualization To better illustrate the effectiveness of our system, we present visualization of the hierarchical document structure analysis in Figure 10. This visualization highlights the organization of document elements, including their spatial positioning, logical roles, and reading order. By accurately capturing these predefined relationships, our method effectively reconstructs the documents structure while preserving its hierarchical integrity. Moreover, by integrating this structured information, the system can generate outputs in formats such as HTML or JSON, ensuring seamless accessibility for various applications, including web-based document rendering and structured data analysis. 6. Limitations Despite achieving promising results on academic benchmarks, our work has several limitations. Firstly, there is lack of suitable datasets to fully validate the effectiveness of our approach in cross-page table grouping and hierarchical list extraction. Developing such datasets will significantly expand the application scope of our method. Secondly, our current architecture utilizes independent vision backbone and language model to extract vision and semantic features respectively. This approach does not optimally align the visual and semantic features during the pretraining stage, which may limit the overall performance of our system. Additionally, while our relationship-based approach offers several advantages, it may encounter scalability concerns when processing large volumes of text or numerous graphical regions, potentially leading to an expansive unified label space that complicates model training and inference. Lastly, we lack comprehensive benchmark and reasonable evaluation metrics for document digitization. Establishing these benchmarks will not only standardize the evaluation process but also facilitate fair and robust comparisons across different methodologies, such as specialist models and general multimodal large language models. Addressing these issues will be the focus of our future research. 7. Conclusion and Future Work In this study, we introduce unified relation prediction approach named UniHDSA for hierarchical document structure analysis. Unlike traditional multi-stage or multi-branch frameworks, UniHDSA consolidates the process into two primary stages: page-level structure analysis and document-level structure analysis. This innovative approach treats various HDSA sub-tasks as relation prediction problems and integrates the corresponding prediction labels into unified label space, enabling single module to efficiently handle multiple tasks concurrently. Moreover, we develop multimodal end-to-end system based on Transformer architectures to validate the effectiveness of UniHDSA on several benchmarks. In future work, we plan to address several limitations identified in this study. Firstly, we will focus on developing and curating suitable datasets to fully validate the effectiveness of our approach in crosspage table grouping and hierarchical list extraction. This will involve collaboration with academic and industry partners to ensure comprehensive and representative data coverage. Additionally, we aim to explore integrated multimodal architectures that better align visual and semantic features during the pretraining stage. By leveraging joint pretraining strategies and more advanced Transformer-based models, we hope to achieve more cohesive feature integration and enhance the overall performance of our system."
        },
        {
            "title": "References",
            "content": "[1] J. Rausch, O. Martinez, F. Bissig, C. Zhang, S. Feuerriegel, DocParser: Hierarchical document structure parsing from renderings, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 43284338. [2] K. He, G. Gkioxari, P. Dollar, R. Girshick, Mask R-CNN, in: Proceedings of the International Conference on Computer Vision, 2017, pp. 29612969. [3] J. Rausch, G. Rashiti, M. Gusev, C. Zhang, S. Feuerriegel, DSG: An end-to-end document structure generator, in: 2023 IEEE International Conference on Data Mining (ICDM), IEEE, 2023, pp. 518527. [4] J. Ma, J. Du, P. Hu, Z. Zhang, J. Zhang, H. Zhu, C. Liu, HRDoc: Dataset and baseline method toward hierarchical reconstruction of document structures, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2023, pp. 1870 1877. [5] J. Wang, K. Hu, Z. Zhong, L. Sun, Q. Huo, Detect-Order-Construct: tree construction based approach for hierarchical document structure analysis, Pattern Recognition 156 (2024). [6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, (cid:32)L. Kaiser, I. Polosukhin, Attention is all you need, in: Proceedings of the Advances in Neural Information Processing Systems, 2017. [7] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, J. Dai, Deformable DETR: Deformable transformers for end-to-end object detection, in: Proceedings of the International Conference on Learning Representations, 2021. [8] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, P. Staar, DocLayNet: large human-annotated dataset for documentlayout segmentation, in: Proceedings of the ACM SIGKDD conference on knowledge discovery and data mining, 2022, pp. 37433751. [9] J. Wang, K. Hu, Q. Huo, DLAFormer: An end-to-end transformer for document layout analysis, in: Proceedings of the International Conference on Document Analysis and Recognition, 2024. [10] L. Gao, X. Yi, Z. Jiang, L. Hao, Z. Tang, ICDAR2017 competition on page object detection, in: Proceedings of the International Conference on Document Analysis and Recognition, 2017, pp. 14171422. [11] X. Zhong, J. Tang, A. J. Yepes, PubLayNet: largest dataset ever for document layout analysis, in: Proceedings of the International Conference on Document Analysis and Recognition, 2019, pp. 10151022. [12] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards real-time object detection with region proposal networks, in: Proceedings of the Advances in Neural Information Processing Systems, 2015, pp. 9199. [13] J. Lee, H. Hayashi, W. Ohyama, S. Uchida, Page segmentation using convolutional neural network with trainable cooccurrence features, in: Proceedings of the International Conference on Document Analysis and Recognition, 2019, pp. 10231028. [14] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for biomedical image segmentation, in: Medical image computing and computer-assisted intervention, 2015, pp. 234241. [15] H. Yang, W. Hsu, Transformer-based approach for document layout understanding, in: Proceedings of the International Conference on Image Processing, 2022, pp. 40434047. [16] A. Banerjee, S. Biswas, J. Llados, U. Pal, SwinDocSegmenter: An end-to-end unified domain adaptive transformer for document instance segmentation, arXiv preprint arXiv:2305.04609 (2023). [17] F. Li, H. Zhang, H. Xu, S. Liu, L. Zhang, L. M. Ni, H.-Y. Shum, Mask DINO: Towards unified transformer-based framework for object detection and segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 30413050. [18] Y. Huang, T. Lv, L. Cui, Y. Lu, F. Wei, LayoutLMv3: Pre-training for document ai with unified text and image masking, in: Proceedings of the ACM International Conference on Multimedia, 2022, pp. 40834091. [19] J. Li, Y. Xu, T. Lv, L. Cui, C. Zhang, F. Wei, Dit: Self-supervised pre-training for document image transformer, in: Proceedings of the ACM International Conference on Multimedia, 2022, pp. 35303539. [20] P. Zhang, C. Li, L. Qiao, Z. Cheng, S. Pu, Y. Niu, F. Wu, VSR: unified framework for document layout analysis combining vision, semantics and relations, in: Proceedings of the International Conference on Document Analysis and Recognition, 2021, pp. 115130. [21] T. I. Denk, C. Reisswig, Bertgrid: Contextualized embedding for 2d document representation and understanding, in: Document Intelligence Workshop at NeurIPS, 2019. [22] N. Zhang, H. Cheng, J. Chen, Z. Jiang, J. Huang, Y. Xue, L. Jin, M2Doc: multi-modal fusion approach for document layout analysis, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, 2024, pp. 72337241. [23] W. Lin, Q. Gao, L. Sun, Z. Zhong, K. Hu, Q. Ren, Q. Huo, ViBERTgrid: jointly trained multi-modal 2d document representation for key information extraction from documents, in: Proceedings of the International Conference on Document Analysis and Recognition, Springer, 2021, pp. 548563. [24] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, H. Shum, DINO: DETR with improved denoising anchor boxes for end-to-end object detection, in: Proceedings of the International Conference on Learning Representations, 2023. 33 [25] X. Li, F. Yin, C. Liu, Page object detection from pdf document images by deep structured prediction and supervised clustering, in: Proceedings of the International Conference on Pattern Recognition, 2018, pp. 36273632. [26] J. Wang, M. Krumdick, B. Tong, H. Halim, M. Sokolov, V. Barda, D. Vendryes, C. Tanner, graphical approach to document layout analysis, in: Proceedings of the International Conference on Document Analysis and Recognition, 2023, pp. 5369. [27] Z. Zhong, J. Wang, H. Sun, K. Hu, E. Zhang, L. Sun, Q. Huo, hybrid approach to document layout analysis for heterogeneous document images, in: Proceedings of the International Conference on Document Analysis and Recognition, 2023, pp. 189-206. [28] T. M. Breuel, High performance document layout analysis, in: Proceedings of the Symposium on Document Image Understanding Technology, 2003, pp. 209218. [29] J. Meunier, Optimized xy-cut for determining page reading order, in: Proceedings of the International Conference on Document Analysis and Recognition, 2005, pp. 347351. [30] S. Ferilli, A. Pazienza, An abstract argumentation-based strategy for reading order detection, in: Proceedings of the AI*IA Workshop on Intelligent Techniques, Vol. 1509, 2015. [31] L. Li, F. Gao, J. Bu, Y. Wang, Z. Yu, Q. Zheng, An end-to-end OCR text re-organization sequence learning for rich-text detail image comprehension, in: Proceedings of the European Conference on Computer Vision, 2020, pp. 85100. [32] Z. Wang, Y. Xu, L. Cui, J. Shang, F. Wei, LayoutReader: Pre-training of text and layout for reading order detection, in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2021, pp. 47354744. [33] L. Quiros, E. Vidal, Reading order detection on handwritten documents, Neural Computing and Applications 34 (12) (2022) 95939611. [34] G. Nagy, S. C. Seth, Hierarchical representation of optically scanned documents (1984) 347349. [35] A. Conway, Page grammars and page parsing. syntactic approach to document layout recognition, in: Proceedings of the International Conference on Document Analysis and Recognition, 1993, pp. 761764. [36] Y. Tateisi, N. Itoh, Using stochastic syntactic analysis for extracting logical structure from document image, in: Proceedings of the IAPR International Conference on Pattern Recognition, 1994, pp. 391394. [37] Z. Wang, M. Zhan, X. Liu, D. Liang, DocStruct: multimodal method to extract hierarchy structure in document for general form understanding, in: Findings of the Association for Computational Linguistics, Vol. EMNLP 2020, 2020, pp. 898908. [38] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, S. Park, Ocr-free document understanding transformer, in: Proceedings of the European Conference on Computer Vision, Springer, 2022, pp. 498517. [39] B. Davis, B. Morse, B. Price, C. Tensmeyer, C. Wigington, V. Morariu, End-to-end document recognition and understanding with dessurt, in: Proceedings of the European Conference on Computer Vision, Springer, 2022, pp. 280296. [40] K. Lee, M. Joshi, I. R. Turc, H. Hu, F. Liu, J. M. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang, K. Toutanova, Pix2struct: Screenshot parsing as pretraining for visual language understanding, in: Proceedings of the International Conference on Machine Learning, 2023, pp. 1889318912. [41] D. Coquenet, C. Chatelain, T. Paquet, Dan: segmentation-free document attention network for handwritten document recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (7) (2023) 82278243. [42] L. Blecher, G. Cucurull, T. Scialom, R. Stojnic, Nougat: Neural optical understanding for academic documents, in: Proceedings of the International Conference on Learning Representations, 2024. [43] H. Wei, C. Liu, J. Chen, J. Wang, L. Kong, Y. Xu, Z. Ge, L. Zhao, J. Sun, Y. Peng, et al., General ocr theory: Towards ocr-2.0 via unified end-to-end model, arXiv preprint arXiv:2409.01704 (2024). [44] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2016, pp. 770778. 34 [45] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, pp. 41714186. [46] Y. Xu, T. Lv, L. Cui, G. Wang, Y. Lu, D. Florencio, C. Zhang, F. Wei, LayoutXLM: Multimodal pre-training for multilingual visually-rich document understanding, arXiv preprint arXiv:2104.08836 (2021). [47] S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu, L. Zhang, DAB-DETR: Dynamic anchor boxes are better queries for detr, in: Proceeding of the International Conference on Representation Learning, 2022. [48] X. Jia, B. De Brabandere, T. Tuytelaars, L. V. Gool, Dynamic filter networks, in: Advances in neural information processing systems, 2016, p. 29. [49] Z. Tian, C. Shen, H. Chen, Conditional convolutions for instance segmentation, in: Proceedings of the European Conference on Computer Vision, 2020, pp. 282298. [50] J. Su, Y. Lu, S. Pan, B. Wen, Y. Liu, RoFormer: Enhanced transformer with rotary position embedding, arXiv preprint arXiv:2104.09864 (2021). [51] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, S. Zagoruyko, End-to-end object detection with transformers, in: Proceedings of the European Conference on Computer Vision, Springer, 2020, pp. 213229. [52] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar, Focal loss for dense object detection, in: Proceedings of the International Conference on Computer Vision, 2017, pp. 29802988. [53] H. Cheng, P. Zhang, S. Wu, J. Zhang, Q. Zhu, Z. Xie, J. Li, K. Ding, L. Jin, M6Doc: large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1513815147. [54] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101 (2017). [55] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, S. Kumar, Long-tail learning via logit adjustment, in: Proceddings of the International Conference on Representation Learning, 2020. [56] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, R. Girdhar, Masked-attention mask transformer for universal image segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12801289. [57] P. Hu, Z. Zhang, J. Zhang, J. Du, J. Wu, Multimodal tree decoder for table of contents extraction in document images, in: Proceedings of the International Conference on Pattern Recognition, 2022, pp. 17561762. [58] G. Jocher, et al., ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 models, AWS, Supervise.ly and YouTube integrations (Apr. 2021). [59] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 1001210022. [60] W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu, T. Lu, L. Lu, H. Li, et al., Internimage: Exploring large-scale vision foundation models with deformable convolutions, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 1440814419."
        }
    ],
    "affiliations": [
        "Department of EEIS, University of Science and Technology of China, Hefei, 230026, China",
        "Microsoft Research Asia, Beijing, 100080, China"
    ]
}