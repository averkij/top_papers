{
    "paper_title": "FreSca: Unveiling the Scaling Space in Diffusion Models",
    "authors": [
        "Chao Huang",
        "Susan Liang",
        "Yunlong Tang",
        "Li Ma",
        "Yapeng Tian",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets."
        },
        {
            "title": "Start",
            "content": "FreSca: Unveiling the Scaling Space in Diffusion Models Chao Huang1, Susan Liang1, Yunlong Tang1, Li Ma2, Yapeng Tian3, Chenliang Xu1 1University of Rochester, 2Netflix Eyeline Studios, 3The University of Texas at Dallas 5 2 0 2 2 ] . [ 1 4 5 1 2 0 . 4 0 5 2 : r Figure 1. FreSca: Generalizable Plug-and-Play Enhancement for Diffusion Models. Our method seamlessly integrates to boost performance without retraining. We show applications: (a) Diffusion-based Depth Estimation (Top Half): FreSca refines depth predictions from Marigold [26], improving detail (see zoom-in) at not additional cost. (b) Diffusion-based Image Editing (Bottom Half): FreSca enables precise, prompt-aligned image edits compared to the baselines LEdits++ [2] and DDPM Inversion [23], requiring no extra training."
        },
        {
            "title": "Abstract",
            "content": "gains across multiple datasets. Our project page is here: https://wikichao.github.io/FreSca/. Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode taskspecific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines scaling space whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions (ϵ) carries key semantic information. Our core contribution stems from Fourier analysis of ϵ, revealing that its lowand high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative 1. Introduction In recent years, diffusion models [45] have emerged as cornerstone in content generation. The core mechanism involves establishing Markov chain that progressively adds noise to data, followed by learning to reverse this noise addition process to generate novel content iteratively from random noise. This versatile framework has been successfully applied across diverse domains, including image creation [19, 33, 36, 37, 39], video synthesis [4, 5, 14, 17, 27, 44, 47, 50], and audio production [12, 2022, 29, 30, 54]. Notably, despite their origins as generative models, diffusion models have been effectively adapted for tasks beyond pure generation, such as languageguided image editing and image understanding, including semantic segmentation and depth estimation. key component shared across diverse diffusion moda different location within the space. While the linear scaling property of ω offers intuitive control, the underlying temporal dynamics of ϵ throughout the editing process remains unclear. To address this, we conduct Fourier spectrum analysis, uncovering frequency-dependent dynamics. Our analysis reveals dichotomy: high-frequency components exhibit relatively stable behavior, while low-frequency parts display significant temporal fluctuations and heightened sensitivity to ω. This distinction arises from the inherent roles of frequency components: low-frequency signals govern structural layouts, whereas high frequencies encode fine-grained textures. Consequently, structural edits can lead to cascading error accumulation, where even small perturbations in early timesteps propagate into significant structural distortions through subsequent denoising steps. As result, the heightened sensitivity of low-frequency components creates an unstable equilibrium. While this sensitivity is essential for effective structural manipulation, it necessitates careful regularization to ensure stable trajectory control during the editing process. Building on these insights, we expand our investigation to understand the role and dynamic of different frequency components. By decomposing ϵ into lowand highfrequency components and treating them independently, (i) Scaling the lowwe identify two distinct regimes: frequency parts exhibits functional equivalence to amplifying classifier-free guidance ω, as both primarily govern structural transformations. (ii) High-frequency scaling introduces diversification without compromising semantic fidelity to . Modulating high-frequency components adds small directional noise to the scaling space at each denoising step, and its accumulation results in perceptually distinct yet semantically consistent outputs. Grounded in these observations, we introduce simple yet effective method, termed FreSca, which transforms the original scaling space into Frequency-aware Scaling space. This transformation enables independent modulation of different frequency components, enhancing editing quality without imposing additional fine-tuning. Specifically, for ϵ, we apply Fourier transform to decompose it into highand low-frequency components using predefined filters. We then introduce two multiplicative factors, and h, serving as scaling parameters for the lowand high-frequency components, respectively. This decomposition of the scaling space provides finer-grained control over the editing process. Empirically, we observe that setting > 1 typically refines fine details and yields more visually appealing results, while setting < 1 can influence the images semantic content. The modulated low and highfrequency parts are combined and scaled by ω. All operations are performed in the Fourier domain, maintaining close connection to the original formulation; for instance, Figure 2. Key components for image editing: (a) latent vector xT is obtained through inversion techniques; (b) the visualizations of ϵ at the first editing step; (c) the final edited output. els is their denoising process. Specifically, at each denoising step, neural network predicts the noise component, and classifier-free guidance is often employed to adjust this prediction. Two important insights arise from this standard design: (i) the predicted noise inherently contains semantic information relevant to the task (e.g., image editing, semantic segmentation, depth estimation), and (ii) applying scalar classifier-free guidance can effectively augment or suppress task-relevant information (e.g., achieving more aggressive editing results while preserving desired attributes). These properties implicitly define scaling space, which can be utilized to modulate the presence of target representations. In this paper, we investigate the fundamental mechanisms of this scaling space, using inversion-based editing methods as representative case study. As illustrated in Fig. 2(a), the editing begins by inverting real image into latent vector xT . To define an editing trajectory, target text prompt is introduced into the diffusion models prediction framework. Here, the conditional noise prediction branch, guided by c, encodes target-specific information, while the model simultaneously maintains an unconditional prediction branch based on its original training. The key insight is that the difference between these two predictions, denoted as ϵ, effectively encapsulates the semantic direction of the desired edit, as evidenced in Fig. 2(b). Furthermore, classifier-free guidance ω, is often applied to amplify the target concepts influence. Intuitively, ω ϵ constructs semantic scaling space for the target concept , where adjusting ω controls the magnitude (or speed) along the editing trajectory, and modifying ϵ shifts the trajectory to when = h, FreSca reduces to the original scaling space. tiple concurrent modifications. To emphasize, FreSca is straightforward extension of the fundamental scaling space inherent to nearly all diffusion models. Consequently, it exhibits seamless adaptability when integrated into existing diffusion-based image understanding methods, functioning as plug-and-play strategy. To illustrate this versatility, we extend FreSca to the fundamental task of diffusion-based image and video depth estimation, including state-of-the-art methods like Marigold [26] and ChronoDepth [42]. We observe an instant boost in depth prediction results across various image depth estimation datasets and also qualitatively confirm its effectiveness in the challenging video depth estimation task, highlighting the broad applicability and versatility of our approach. In all, our contributions can be summarized as follows: We revisit the foundation of the standard scaling space ω ϵ and, through Fourier analysis, identify the distinct roles of low-frequency components in driving structural evolution and high-frequency components in refining edited results. We propose FreSca, simple yet effective approach that transitions the original scaling space into the Fourier domain, enabling independent scaling of lowand highfrequency components for finer control. We conduct extensive evaluations across various editing models, showcasing FreScas plug-and-play capability to enhance the quality of edited results. We extend FreSca to diffusion-based depth estimation tasks to illustrate its versatility and broad applicability. 2. Related Works 2.1. Diffusion-based Image Editing Diffusion models have revolutionized realistic content generation, achieving state-of-the-art results in diverse areas such as image synthesis [19, 28, 33, 36, 37, 39, 48] and video production [4, 14, 17, 27, 44, 47, 50]. key driver of their success lies in the seamless integration of language understanding into the generative process. For example, GLIDE [33] pioneered text-conditional diffusion models for controlled image generation, with subsequent works like DALL-E 2 [36] and Imagen [39] further advancing this capability. In parallel, growing body of work has focused on adapting these powerful generative models for image editing. These efforts can be broadly categorized into approaches that fine-tune or control diffusion models for specific editing tasks, such as DreamBooth [38], Nulltext Inversion [32], and InstructPix2Pix [3], and trainingfree, inversion-based editing techniques [2, 9, 23, 24, 53]. Among these, DDPM Inversion [23] stands out for its effective inversion method, and LEdits++ [2] delivers highquality edits with reduced diffusion steps and enabling mul2.2. Diffusion-based Image Understanding Beyond their natural extension to image editing, welltrained diffusion models have also been increasingly leveraged for various image understanding tasks. The underlying intuition is that image diffusion models, trained on vast internet-scale image collections to generate high-quality images across wide range of domains, acquire encyclopedic representations of the visual world. Consequently, it should be possible to adapt these powerful pretrained image diffusion models to serve as effective backbones or provide strong priors for diverse downstream tasks, such as image semantic segmentation [1, 52], monocular depth estimation [7, 26], video depth estimation [18, 41], and image relighting [55], among others. Despite these exciting results and applications, they all rely on diffusion models employing the vanilla scaling space, which typically involves global scalar multiplier to regulate the intensity of noise predictions. However, several studies have shown that diffusion models can exhibit different strategies during test time. For instance, FreeU [43] investigates the effect of skip connections within the diffusion U-Net and showcases that adjusting the scale of skipconnection and backbone features can yield improved generation results. Similarly, Visual Anagrams [11] and Factorized Diffusion [11] demonstrated that noise predictions can be manipulated through various operations like filtering and flipping to create artistic effects. These works contribute to the intuition behind our investigation, suggesting that the original scaling space may not be optimal. Through our study, FreSca transitions the scaling space to the frequency domain to allow for more refined control. 3. Method In this section, we use inversion-based editing methods as representative case study to study the effect of the scaling space. We begin by reviewing the foundational concepts of diffusion models in Sec. 3.1, which provide the foundation for our analysis. Next, in Sec. 3.2, we explore the inversionbased editing paradigm and emphasize its key properties for effective editing. We further conduct Fourier analysis to uncover the dynamics of the editing process in Sec. 3.3. In Section 3.4, we introduce our novel approach, FreSca, which enables flexible and independent control over different frequency components throughout the denoising process. Finally, in Sec. 3.5, we discuss the generalization capability of our method and its natural extension to applications beyond image editing. 3.1. Preliminary Text-guided diffusion models excel at generating multimodal content by progressively denoising random noise , t) in Fig. 3(a)). The conditional noise prediction ϵθ(xt, for target prompt , and unconditional noise prediction ϵθ(xt, t).These predictions are combined via classifier-free guidance [16] ω: ˆϵt = ϵθ(xt, t) + ω (ϵθ(xt, ). , t) ϵθ(xt, t) (cid:125) (cid:123)(cid:122) Target prompt representation (cid:124) (3) Intuitively, the noise difference term encodes spatial regions relevant to the target prompt. We validate this through three editing scenarios as shown in Fig. 2: 1) Replacement editing, the target prompt describes semantically related but absent attributes; 2) Self-editing, where describes existing attributes (e.g., yellow car); and 3) Unrelated prompt, has no semantic correlation with the input. As illustrated in Fig. 2, the noise difference maps highlight promptrelevant regions for semantically related cases, while showing random activation for unrelated prompts. This reveals three key properties: 1. semantically rich inversion: The inverted latent xT preserves high-level semantics from the input, enabling meaningful association with 2. noise difference as prompt proxy: The prediction difference, denotes as ϵt = ϵθ(xt, c, t) ϵθ(xt, t), provides spatial guidance for editing, effectively disentangling the prompt-relevant representation as noise. 3. ω for controlling strength/direction: since the target prompt representation is approximately disentangled as ϵt, the sign of scalar ω indicates the editing direction (either suppressing or enhancing the target concept), while its magnitude determines the strength of the edit. . In all, these properties collectively enable precise and interpretable control over the editing process. 3.3. Understanding the Editing Dynamic While prior works [2, 9, 23, 32] have primarily focused on improving the inversion process to obtain better latent representations xT , the roles of ω and ϵt in the editing process remain less explored. In this section, we analyze how these components influence the editing dynamics. In the generation pipeline, the unconditional branch prethereby capturing dicts noise without any conditioning, In contrast, the conditional the general data distribution. branch, guided by prompt (e.g., photo of cat), learns how the data distribution depends on that prompt. The direction and magnitude of the edit are controlled by the scaled difference term ω ϵt. To further investigate its role, we analyze the editing process in the Fourier domain. Specifically, given ϵt, we apply Fourier transform and decompose it into lowand high-frequency components using cutoff threshold. Intuitively, the low-frequency component (ϵl t) captures global structural changes, such as layout and smooth color transitions, which define the images core appearance. In contrast, the high-frequency comFigure 3. Original scaling space (a) vs. Our proposed Fourier scaling space. We introduce the scaling factors l, h, and ω in the Fourier domain to decompose the control mechanisms. into data samples conditioned on text prompts c. Given real input x0, the forward diffusion process gradually adds Gaussian noise to produce xT , while denoising network ϵθ learns to reverse this process. The training objective is: ℓsimple = ϵ ϵθ(xt, c, t). (1) Inversion techniques. While primarily designed for generation, diffusion models can be reproposed for editing through inversion methods [8, 32, 51]. These techniques map real images x0 back to noisy latents xT via reverse Ordinary Differential Equations (ODEs).For example, DDIM inversion [46] operates as: xt = (cid:113) αt αt xt + (cid:16)(cid:113) 1 αt 1 (cid:113) 1 αt (cid:17) ϵθ(xt, c, t), (2) where = 1 during generation and = + 1 during inversion under noise schedule {αt}. However, the approximate nature of inversion introduces cumulative errors. Recent work addresses this through improved inversion strategies like Edited-Friendly DDPM inversion [23], ReNoise [9], and LEdits++ [2]. 3.2. How Do Diffusion Models Perform Editing? As shown in Eq. (2), the inversion process establishes semantic link between the input image and text prompt c, while the subsequent denoising process enables editing through prompt-guided deviation from the original input. Lets revisit the denoising process: at each step t, the editing process combines two noise predictions (as demonstrated Figure 4. Frequency-aware scaling effects: We set the target prompt to increasing the size of stones and apply three different scaling strategies in the frequency domain: uniform scaling (h = = 1.5), low-frequency scaling (l = 1.5, = 1), and high-frequency scaling (h = 1.5, = 1). Each approach produces distinct effects. ponent (ϵh ) encodes finer details, including edges and textures. The conventional scaling approach ω ϵt applies the same factor ω to both components, leading to synchronized change across frequencies. While standard editing methods treat both components equally, we pose an important yet unexplored question: Are the dynamics of editing lowand high-frequency components equivalent? Addressing this question could offer new insights into general editing processes, as different tasks may favor distinct frequency patterns. To systematically investigate this, we introduce two independent multiplicative factors, and h, which separately scale the lowand high-frequency components: When = h, this formulation reduces to standard synchronized scaling. By varying and independently, we can better understand the distinct roles of frequency components in the editing process. As shown in Fig. 4, applying distinct scaling factors to the lowand high-frequency components reveals significant differences. In later diffusion steps (e.g., for < 15), the relative Fourier log-amplitude patterns remain relatively stable regardless of the combination of and h. However, in earlier steps, changes in scaling factors introduce more drastic modifications. This observation aligns with the coarseto-fine nature of diffusion models: In early stages, diffusion models establish the coarse structure that defines the images overall shape and layout. And in later stages, the model refines details, such as textures and edges, while preserving the established structure. Since editing often involves modifying global structures, changes in early-stage scaling have more pronounced effect. This highlights the importance of regularization to ensure stable trajectory control during the editing process. To further illustrate the impact of frequency-aware scaling, consider the following cases in Fig. 4: Synchronous scaling: When both and increase simultaneously (e.g., from (l, h) = (1, 1) to (1.5, 1.5)), the target prompt (e.g., big stones) is strengthened uniformly. This case is equivalent to applying global factor of 1.5 in the standard scaling space. scaling: Asynchronous Adjusting only the lowfrequency component (l = 1.5, = 1) enhances the target prompt by enlarging the overall structure (e.g., making stones appear larger) while better preserving the original shape and style. Conversely, increasing only the highfrequency component (h = 1.5, = 1) does not effectively reinforce the target prompt but introduces excessive local noise, adding texture details. In summary, standard scaling space ω ϵt uniformly modifies both frequency components of ϵt, but our analysis reveals that lowand high-frequency components can exhibit significant fluctuations and are not always synchronous. This highlights the need for more flexible scaling space to achieve high-quality and faithful edits during inference. 3.4. Scaling in the Fourier Domain Building on our previous observations, the entanglement of lowand high-frequency components in original scaling space often forces compromises between structural fidelity and detail preservation. To address this, we propose FreSca, simple yet effective approach that shifts scaling control to the Fourier domain. Unlike prior works that rely Figure 5. Scaling up the high-frequency parts (h = 2.0) effectively enhances the editing fidelity.The red hat is successfully injected, and the edge of the LEGO flowers is sharpened. (a) input image, (b) results from FreSca with different being set, and (3) results from LEdits++. Figure 6. High-frequency component adjustment. By reducing the high-frequency parts (h = 0.5), the editing effect is mitigated. (a) input image, (b) results from FreSca with different being set, and (3) results from LEdits++. on single scaling factor ω, FreSca decomposes scaling into distinct lowand high-frequency components. This decoupling mitigates suboptimal compromises in editing quality caused by entangled frequency interactions, enabling precise control over structural fidelity and fine-grained details. Fourier-aware scaling. To disentangle frequency-specific adjustments, we reformulate the scaling operation in the Fourier domain: (4) ω ϵt 1(cid:0)ω (M F(ϵt))(cid:1), where and 1 denote the Fourier transform and inverse Fourier transform, respectively. The frequency mask allows selective modulation of different frequency bands, with representing the Hadamard product. This formulation introduces flexible frequency control, as the mask can be adjusted arbitrarily to emphasize or suppress specific frequency components. Empirically, we found that decomposing into separate lowand high-frequency masks provides effective control: (cid:40) Mh(r) = , Ml(r) = 1 Mh(r), (5) 1 0 if > r0 otherwise where is the frequency radius, and r0 is the cutoff threshold (set to 20 for latent spatial dimension of 128 in SDXL [34]). These masks act as high-pass and low-pass filters, respectively. After decomposition, different scaling factors and can be applied independently to control the contribution of each frequency component (as illustrated in Fig. 3(b)), enabling more nuanced and high-quality edits. As shown in Fig. 4, increasing the low-frequency components directly influences the strength of target prompt modifications, aligning with the primary function of ω. To simplify control, we set = 1 in practice and allow ω to amplify the overall strength. Interestingly, modifying high-frequency components not only affects fine details but also impacts global object arrangements. This suggests that low-frequency components define target space, ensuring that generated outputs share coherent structural foundation, while high-frequency components determine instance variations within that space, influencing how individual elements manifest. Based on this finding, we treat as the primary control factor and adjust its value dynamically. Consequently, the frequency mask is defined as: = Mh + Ml. (6) Here, is scalar that modulates the high-frequency components. Impact of frequency-aware scaling. As evidenced by Fig. 5, the standard scaling space successfully transforms Algorithm 1 Frequency-Aware Scaling Require: ϵt, h, l, ω Ensure: Scaled noise ϵ 1: FFT(ϵt) 2: Xlow Ml(X) 3: Xhigh Mh(X) 4: Xcombined Xlow + Xhigh 5: Xweighted ω Xcombined 6: ϵ IFFT(Xweighted) 7: return ϵ Image Editing Results. We add FreSca to different Table 1. editing methods and evaluate the common FID-30k and CLIP-text score metrics. Lowpass filtering Highpass filtering Edited-Friendly DDPM [23] DDPM [23] w/ FreSca LEdits++ [2] LEdits++ [2] w/ FreSca FID-30k CLIP-text (%) 255.5 253.4 255.3 255. 31.35 31.54 31.08 31.34 real flowers into LEGO structures but produces blurry edges. When setting = 0.5, reducing the high-frequency components of ϵt does not significantly affect the shape. However, setting = 2 enhances high-frequency components, improving structural sharpness and adherence to the target prompt. Similarly, for texture-focused edits, as illustrated in Fig. 6, increasing introduces shape deviations (e.g., distorted contours), while reducing diminishes the editing effect without altering the underlying structure. These observations suggest that the high-frequency components of ϵt play crucial role in achieving the desired editing outcome. One hypothesis is grounded by that lowfrequency structural components continue to fluctuate even in the later stages of the diffusion process (e.g., = 20), as shown in Fig. 7, while high-frequency patterns converge more quickly. Therefore, altering can help balance the effect of low-frequency fluctuations and stabilize the editing process. By disentangling frequency bands, FreSca provides fine-grained control over editing dynamics. 3.5. Discussion While the above analysis is conducted on diffusion-based image editing, we argue that FreSca is not exclusively limited to image editing but holds the potential to extend to broader diffusion-based applications. The key properties, as illustrated in Sec. 3.2 (namely, the semantic richness of noise predictions and the common use of classifierFigure 7. Low frequency components of ϵt changes more drastically than high frequency components. free guidance), are generalizable and therefore applicable to other tasks. We further demonstrate this generalization capability in Sec. 5, where we effortlessly boost the performance of diffusion-based depth estimation with FreSca. Remarkably, FreSca requires no additional training or fine-tuning. Extending the classifier-free guidance ω into Fourier-based scaling factors involves only few lines of code (as demonstrated in Algorithm 1), making the approach both flexible and computationally efficient. 4. Experiments on Image Editing 4.1. Experimental Settings TEdBench [25] dataset. To assess the performance of our proposed FreSca, we conduct experiments on the public image editing dataset TEdBench [25], which comprises 40 images from diverse categories paired with various editing prompts. Please refer to our supplementary materials for further details. Evaluation methods. Since our method only modifies the forward process of diffusion models, it can be seamlessly integrated into existing image editing frameworks without significantly altering their core architectures. Accordingly, we benchmark our approach against state-of-the-art, training-free methods, including LEdits++[2] and EditedFriendly DDPM Inversion[23], strictly following their prescribed settings while exclusively incorporating our Fourier scaling space. 4.2. Comparison with Edited-Friendly DDPM Inversion [23] and LEdits++ [2] Edited-Friendly DDPM Inversion was proposed to address potential error accumulation in DDIM inversion by introducing an edited-friendly noise space, which allows for perfect reconstruction and flexible transformations within the noise space. This capability supports various manipulations, such as shifting and color edits, and has made the method popular in the image editing community. Similarly, LEdits++ achieves perfect reconstruction and employs DPMSolver++ [31] to perform inversion in fewer steps, while also incorporating semantic grounding during the inference process to ensure more accurate localization of the edit region. Given the superior performance of these two Table 2. Evaluation results from the Large Vision Language Model, which assesses the success rate of the editing and gives the edited image score from 1 to 5 to evaluate the image quality. Edited-Friendly DDPM [23] DDPM [23] w/ FreSca LEdits++ [2] LEdits++ [2] w/ FreSca Success Rate (%) Quality 75.0 80.0 72.5 72.5 4.23 4.18 4.08 4. methods, it is compelling to investigate whether the original scaling space is optimal for them. We adopt the original codebases of these methods and integrate our FreSca into their inference stages. For all methods, we set ω = 15, and for our approach, we additionally set = 1.2 for Edited-Friendly DDPM Inversion and = 2 for LEdits++. The difference in parameter selection primarily stems from LEdits++s semantic grounding property. In LEdits++, the semantic mask is computed from ϵt by dynamically selecting quantile to generate the mask, which implicitly highlights the impact on the target edited region and reduces the high-frequency components, thereby necessitating larger value of h. We employ the CLIP-text score [35] to assess whether the target concept has been successfully enhanced and FID30k [15] to evaluate the overall image quality after editing. As shown in Tab. 1, starting from the same latent vector, equipping both methods with FreSca improves the quality of the final results and enhances the presence of the target prompt. These findings demonstrate that modulating the high-frequency components can positively impact the editing process, which is further validated qualitatively in Fig. 1. In addition to our quantitative metrics, we further evaluate the edited images using the large vision-language model InternVL2.5-8B [6]. This model provides binary decision (0 or 1) to indicate whether the editing was successful and assigns qualitative score on scale of 1 to 5where 1 denotes poor quality and 5 reflects excellent performance in both concept fidelity and overall image quality. As shown in Tab. 2, incorporating FreSca not only improves the overall quality of the edited outputs but also increases the editing success rate. This demonstrates the effectiveness of our approach in achieving high-quality, semantically faithful edits. Additional details regarding the prompt design for this experiment are provided in the supplementary materials. 4.3. Ablation Studies Effect of low-frequency scaling factor l. As illustrated in Fig. 4, increasing the low-frequency scaling factor has similar effect as raising the guidance scale ω, effectively amplifying the influence of the target prompt (e.g., emphasizing bigger stones). Consequently, we set = 1. Figure 8. Continuous adjustment of high-frequency components. We scale the from 0.5 to 2 to examine its impacts on the editing performance. Figure 9. Comparison between LEdits++ [2] and FreSca on the TexSlider [13] dataset. For FreSca, we fix = 1.5 and vary ω within the set [5, 10, 15], evaluating both positive (up) and negative (down) directions. The role of high-frequency scaling factors h. As demonstrated in Fig. 5 and Fig. 6, adjusting the high-frequency scaling factor produces two distinct effects: when > 1, the representation of shape, structure, and contour is enhanced, while setting < 1 introduces counter-effect that pulls the edited result closer to the original image. This creates practical trade-off between inducing more pronounced shape changes and better preserving the original structure. In the original scaling space, highand lowfrequency components are coupled, which restricts flexible control. In contrast, FreSca decouples these components, achieve varying levels of subtle control on without altering the primary editing direction. Transition with varying h. For given image, altering from values below 1 to values above 1 produces an intriguing transition. When < 1, gradually increasing (e.g., from 0.5 to 0.8) introduces fundamental structural details, as evidenced by the appearance of the riding horse person. In contrast, when > 1, further increases enhance edges, contours, and other high-frequency features. These findings indicate that spans scaling space that governs both high-frequency patterns and the underlying structural composition of the image, demonstrating that FreSca offers superior controllability compared to prior scaing space. Fixing and varying ω. To evaluate how ω controls scalFigure 10. FreSca improves qualitative depth map predictions for input RGB images (top). Red arrows indicate regions of interest, with zoomed-in views shown on the right for each image. Visual comparison between Marigold [26] (bottom) and Marigold + FreSca (middle) demonstrates that our method generates depth maps with more distinct object shapes, effectively reducing blurriness and recovering missing structures present in the original Marigold results. ing, we experiment on the TexSlider dataset [13], which consists of various texture and material images. Unlike TEdBench, TexSlider emphasizes texture editing rather than object manipulation. As shown in Fig. 9, compared to LEdits++ [2], our method (FreSca with = 1.5) enhances the texture removal capability (i.e., scales down the texture) by boosting the high-frequency components during inference. In contrast, when the objective is to enhance texture details, FreSca maintains favorable CLIP-FID trade-off, demonstrating its robustness across different editing goals. 5. Extension to Depth Estimation Diffusion-based image depth estimation. Monocular depth estimation aims to infer scene geometry from single 2D imagea task essential for applications such as autonomous driving, robotics, and augmented reality. Despite the inherent ambiguity of recovering 3D structure from 2D projections, recent deep learning advances have enabled models to learn meaningful depth cues from large-scale visual data. Building upon the success of latent diffusion models, Marigold [26] is recent method, built on the architecture of Stable Diffusion [37], and fine-tuned for monocular depth prediction. It efficiently modifies only the denoising U-Net using synthetic RGB-D data, preserving the models pretrained latent space. Notably, Marigold achieves strong zero-shot performance on real-world depth benchmarks without ever being trained on real depth maps, making it an interesting testbed for further exploration of diffusion model capabilities in image understanding. Given that Marigold [26] is built upon diffusion model, it inherently possesses mechanism to control its depth estimation predictions, which we can consider its scaling space. Specifically, its noise predictor is defined as: ϵt = ϵθ(dt, x, t), where dt represents the noisy depth map at timestep t, and is the input RGB image. Notably, unlike typical image editing diffusion models, Marigolds noise prediction ϵt for depth information utilizes fixed classifierfree guidance of 1, relying solely on the conditional noise prediction. Through our observations, we found that while Marigold generalizes well to in-the-wild images, it sometimes struggles to capture fine-grained details or accurately represent objects at distant, leading to less precise depth predictions. To mitigate this, we apply FreSca to Marigold by selectively enhancing the high-frequency components of its noise prediction ϵt while maintaining the low-frequency components, specifically by setting > 1 and = 1. As demonstrated in Tab. 3, the integration of our FreSca with Marigold consistently improves results compared to the baseline Marigold (both with and without ensemble) across three challenging zero-shot benchmarks: DIODE [49], KITTI [10], and ETH3D [40]. The variant equipped with our approach achieves the best performance in terms of AbsRel and δ1 on DIODE and KITTI, and the best AbsRel on ETH3D. The original Marigold employs an ensemble technique to reduce prediction variance, which can sometimes lead to smoothing effect, potentially losing fine details such as textures and edges during the denoising process. Equipped with our FreSca, we demonstrate that selectively adjusting the high-frequency components during prediction, while preserving the low-frequency information, leads to more deterministic and accurate prediction process. For instance, on the high-resolution DIODE dataset, we observe that modifying the high-frequency components enables the recovery of finer structural details. As visualized in Fig. 10, compared to the original Marigold, our method effectively recovers more accurate depth in regions Table 3. Quantitative comparison of Marigold [26] variants and our method applied to Marigold on zero-shot depth estimation benchmarks. All metrics are presented as percentages, with bold indicating best results and underlined indicating second best. Lower AbsRel (Absolute Relative difference) is better, while higher δ1 (percentage of pixels with relative error within threshold of 1.25) is better. Our method outperforms Marigold on both indoor and outdoor scenes at no additional cost. Marigold implementation [26] sourced from the official GitHub repository. Method DIODE [49] KITTI [10] ETH3D [40] AbsRel δ1 AbsRel δ1 AbsRel δ1 Marigold (w/o ensemble) [26] Marigold (w/ ensemble) [26] Marigold [26] w/ FreSca 31.0 30.8 30.2 77.2 77.3 77.8 10.5 9.9 9. 90.4 91.6 91.7 7.1 6.5 6.4 95.1 96.0 95.9 Figure 11. FreSca improves the depth estimation results of ChronoDepth [42] across different frames without compromising the temporal consistency. with intricate geometry and sharp edges, while maintaining the overall coherence of the depth map. Diffusion-based video depth estimation. Applying singleimage depth estimation models to video often results in flickering and temporal inconsistencies because these models typically process each frame independently, disregarding the temporal relationships between them. This lack of contextual information across frames becomes particularly problematic for applications requiring smooth and consistent depth over time. To address this, ChronoDepth [42] has been introduced as novel video depth estimator that leverages video diffusion model. It aims to provide both intraclip and inter-clip context by fine-tuning the video diffusion model and employing consistent context-aware inference strategy. This strategy utilizes previously predicted depth frames as context for subsequent frames without adding noise, ensuring smoother and more temporally consistent depth estimation across the entire video while maintaining good spatial accuracy. This makes ChronoDepth compelling approach for leveraging diffusion models in the temporal domain. To demonstrate the versatility of FreSca, we also integrated it into the ChronoDepth model, adopting similar strategy of amplifying the high-frequency components of its noise estimation while keeping the lowfrequency parts unchanged (i.e., setting > 1 and = 1). As demonstrated in Fig. 11, integrating FreSca does not compromise the temporal consistency of ChronoDepth. Instead, it improves the depth estimation results across different frames, showcasing its generalization capability to video diffusion models. 6. Conclusion This work revisits inversion-based image editing and explores its scaling space, ω ϵ. Our Fourier analysis reveals that low-frequency components control editing strength and structure, while high-frequency parts enable trajectory diversification. Based on this, we propose FreSca, simple yet effective approach that disentangles scaling into independent frequency components in the Fourier domain. FreSca enhances diffusion-based editing and shows remarkable effectiveness in image understanding tasks like depth estimation, as validated by extensive experiments. Its simplicity allows for easy integration into existing diffusion models without additional training or computational cost, offering more nuanced control for various diffusion-based applications."
        },
        {
            "title": "References",
            "content": "[1] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 3 [2] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 1, 3, 4, 7, 8, 9 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 3 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 3 [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. [6] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 8 [7] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estiIn European Conference on mation from single image. Computer Vision, pages 241258. Springer, 2024. 3 [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 4 [9] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real imarXiv preprint age inversion through iterative noising. arXiv:2403.14602, 2024. 3, 4 [10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE, 2012. 9, 10 [11] Daniel Geng, Inbum Park, and Andrew Owens. Factorized diffusion: Perceptual illusions by noise decomposition. European Conference on Computer Vision (ECCV), 2024. 3 [12] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Text-to-audio generation using arXiv and Soujanya Poria. instruction-tuned llm and latent diffusion model. preprint arXiv:2304.13731, 2023. 1 [13] Julia Guerrero-Viu, Milovs. Havs.an, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero, Diego Gutierrez, Belen Masia, and Valentin Deschaintre. Texsliders: In InternaDiffusion-based texture editing in clip space. tional Conference on Computer Graphics and Interactive Techniques, 2024. 8, [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 3 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 8 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4 [17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. 1, 3 [18] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095, 2024. 3 [19] Chao Huang, Susan Liang, Yunlong Tang, Yapeng Tian, Scaling conarXiv preprint Anurag Kumar, and Chenliang Xu. cept with text-guided diffusion models. arXiv:2410.24151, 2024. 1, 3 [20] Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. High-quality visually-guided sound separaIn Proceedings of the Asian tion from diverse categories. Conference on Computer Vision (ACCV), pages 3549, 2024. [21] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation. arXiv preprint arXiv:2305.18474, 2023. [22] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation In ICML, pages with prompt-enhanced diffusion models. 1391613932, 2023. 1 [23] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. 1, 3, 4, 7, 8 [24] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. International Conference on Learning Representations (ICLR), 2024. 3 [25] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 7 [26] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 9, [27] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 1, 3 [28] Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Language-guided joint audio-visual editing via one-shot adaptation. In Proceedings of the Asian Conference on Computer Vision, pages 10111027, 2024. 3 [29] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. In ICML, pages 2145021474, 2023. 1 [30] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023. 1 [31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. 7 [32] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3, 4 [33] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In ICML, pages 1678416804, 2022. 1, [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 8 [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1, 3 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 1, 3, 9 [38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 1, 3 [40] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highIn Proceedresolution images and multi-camera videos. ings of the IEEE conference on computer vision and pattern recognition, pages 32603269, 2017. 9, 10 [41] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. 3 [42] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors, 2024. 3, 10 [43] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In CVPR, 2024. 3 [44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. 1, 3 [45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020. 4 [47] Yunlong Tang, Gen Zhan, Li Yang, Yiting Liao, and Chenliang Xu. Cardiff: Video salient object ranking chain of thought reasoning for saliency prediction with diffusion. arXiv preprint arXiv:2408.12009, 2024. 1, 3 [48] Yunlong Tang, Junjia Guo, Pinxin Liu, Zhiyuan Wang, Hang Hua, Jia-Xing Zhong, Yunzhong Xiao, Chao Huang, Luchuan Song, Susan Liang, et al. Generative ai for celarXiv preprint arXiv:2501.06250, animation: survey. 2025. 3 [49] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 9, 10 [50] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 1, [51] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey. IEEE TPAMI, 45(3):31213138, 2022. 4 [52] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panopIn tic segmentation with text-to-image diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 3 [53] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Inversion-free image editing with natural language. Chai. arXiv preprint arXiv:2312.04965, 2023. 3 [54] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete IEEE/ACM diffusion model for text-to-sound generation. Transactions on Audio, Speech, and Language Processing, 2023. 1 [55] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lightIn ACM ing control for diffusion-based image generation. SIGGRAPH 2024 Conference Papers, pages 112, 2024."
        }
    ],
    "affiliations": [
        "Netflix Eyeline Studios",
        "The University of Texas at Dallas",
        "University of Rochester"
    ]
}