{
    "paper_title": "Tensor Logic: The Language of AI",
    "authors": [
        "Pedro Domingos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Progress in AI is hindered by the lack of a programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to a long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, a language that solves these problems by unifying neural and symbolic AI at a fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. I show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially a basis for the wider adoption of AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 6 2 2 1 . 0 1 5 2 : r Tensor Logic Tensor Logic: The Language of AI Pedro Domingos Paul G. Allen School of Computer Science & Engineering University of Washington Seattle, WA 98195-2350, USA pedrod@cs.washington.edu Abstract Progress in AI is hindered by the lack of programming language with all the requisite features. Libraries like PyTorch and TensorFlow provide automatic differentiation and efficient GPU implementation, but are additions to Python, which was never intended for AI. Their lack of support for automated reasoning and knowledge acquisition has led to long and costly series of hacky attempts to tack them on. On the other hand, AI languages like LISP an Prolog lack scalability and support for learning. This paper proposes tensor logic, language that solves these problems by unifying neural and symbolic AI at fundamental level. The sole construct in tensor logic is the tensor equation, based on the observation that logical rules and Einstein summation are essentially the same operation, and all else can be reduced to them. show how to elegantly implement key forms of neural, symbolic and statistical AI in tensor logic, including transformers, formal reasoning, kernel machines and graphical models. Most importantly, tensor logic makes new directions possible, such as sound reasoning in embedding space. This combines the scalability and learnability of neural networks with the reliability and transparency of symbolic reasoning, and is potentially basis for the wider adoption of AI. Keywords: deep learning, automated reasoning, knowledge representation, logic programming, Einstein summation, embeddings, kernel machines, probabilistic graphical models 1. Introduction Fields take off when they find their language. Physics took off when Newton invented calculus, and couldnt have done so without it. Maxwells equations would be unusable without Heavisides vector calculus notation. As mathematicians and physicists like to say, good notation is half the battle. Much of electrical engineering would be impossible without complex numbers, and digital circuits without Boolean logic. Modern chip design is made possible by harware description languages, databases by relational algebra, the Internet by the Internet Protocol, and the Web by HTML. More generally, computer science would not have gotten far without high-level programming languages. Qualitative fields also depend critically on their terminology. Even artists rely on the idioms and stylistic conventions of their genre for their work. fields language saves its practitioners time, focuses their attention, and changes how they think. It unites the field around common directions and decreases entropy. It makes key things obvious and avoids repeatedly hacking solutions from scratch. Has AI found its language? LISP, one of the first high-level programming languages, made symbolic AI possible. In the 80s Prolog also became popular. Both, however, suffered from poor scalability and lack of support for learning, and were ultimately displaced, even within AI, by general-purpose languages like Java and C++. Graphical models provide 1 Domingos lingua franca for probabilistic AI, but their applicability is limited by the cost of inference. Formalisms like Markov logic seamlessly combine symbolic and probabilistic AI, but are also hindered by the cost of inference. Python is currently the de facto language of AI, but was never designed for it, and it shows. Libraries like PyTorch and TensorFlow provide important features like automatic differentiation and GPU implementation, but are of no help for key tasks like automated reasoning and knowledge acquisition. Neurosymbolic AI seeks to ameliorate this by combining deep learning modules with symbolic AI ones, but often winds up having the shortcomings of both. In sum, AI has clearly not found its language yet. There are clear desiderata for such language. Unlike Python, it should hide everything It should facilitate that is not AI, allowing AI programmers to focus on what matters. incorporating prior knowledge into AI systems and reasoning automatically over it. It should also facilitate learning automatically, and the resulting models should be transparent and reliable. It should scale effortlessly. Symbolic AI has some of these properties and deep learning has others, but neither has all. We therefore need to merge them. Tensor logic does this by unifying their mathematical foundations. It is based on the observation that essentially all neural networks can be constructed using tensor algebra, all symbolic AI using logic programming, and the two are fundamentally equivalent, differing only in the atomic data types used. begin with brief review of logic programming and tensor algebra. The core of the paper defines tensor logic and describes its inference and learning engines. then show how to elegantly implement neural networks, symbolic AI, kernel machines and graphical models in it. show how tensor logic enables reliable and transparent reasoning in embedding space. propose two approaches to scaling it up. The paper concludes with discussion of other potential uses of tensor logic, prospects for its wide adoption, and next steps toward it. 2. Background 2.1 Logic Programming The most widely used formalism in symbolic AI is logic programming (Lloyd, 1987). The simplest logic programming language, which suffices for our purposes, is Datalog (Greco and Molinaro, 2016). Datalog program is set of rules and facts. fact is statement of the form r(o1, . . . , on), where is relation name and the os are object names. For example, Parent(Bob, Charlie) states that Bob is parent of Charlie, and Ancestor(Alice, Bob) that Alice is an ancestor of Bob. rule is statement of the form A0 A1, . . . , Am, where the arrow means if, commas denote conjunction, and each of the As has the form r(x1, . . . , xn), with being relation name and the xs being variables or object names. For example, the rule Ancestor(x, y) Parent(x, y) says that parents are ancestors, and the rule Ancestor(x, z) Ancestor(x, y), Parent(y, z) says that is zs ancestor if is ys ancestor and is zs parent. Informally, rule says that its left-hand side or head is true if there are known facts that make all the relations 2 Tensor Logic on its right-hand side or body simultaneously true. For example, the rules and facts above imply that Ancestor(Alice, Charlie) is true. In database terminology, Datalog rule is series of joins followed by projection. The (natural) join of two relations and is the set of all tuples that can be formed from tuples in and having the same values for the same arguments. When two relations have no arguments in common, their join reduces to their Cartesian product. The projection of relation onto subset of its arguments is the relation obtained by discarding from the tuples in all arguments not in G. For example, the rule Ancestor(x, z) Ancestor(x, y), Parent(y, z) joins the relations Ancestor(x, y) and Parent(y, z) on and projects the result onto {x, z}; the tuples Ancestor(Alice, Bob) and Parent(Bob, Charlie) yield the tuple Ancestor(Alice, Charlie). Two common inference algorithms in logic programming are forward and backward chaining. In forward chaining, the rules are repeatedly applied to the known facts to derive new facts until no further ones can be derived. The result is called the deductive closure or fixpoint of the program, and all questions of interest can be answered simply by examining it. For example, the answer to the query Ancestor(Alice, x) (Who is Alice an ancestor of?) given the rules and facts above is {Bob, Charlie}. Backward chaining attempts to answer question by finding facts that match it or rules that have it as their head and facts that match the body, and so on recursively. For example, the query Ancestor(Alice, Charlie) does not match any facts, but it matches the rule Ancestor(x, z) Ancestor(x, y), Parent(y, z) and this rules body matches the facts Ancestor(Alice, Bob) and Parent(Bob, Charlie), and therefore the answer is True. Forward and backward chaining in Datalog are sound inference procedures, meaning that the answers they give are guaranteed to follow logically from the rules and facts in the program. Logic programs have both declarative and procedural semantics, meaning rule can be interpreted both as statement about the world and as procedure for computing its head with the given arguments by calling the procedures in the body and combining the results. The field of inductive logic programming (ILP) is concerned with learning logic programs from data (Lavraˇc and Dˇzeroski, 1994). For example, an ILP system might induce the rules above from small database of parent and ancestor relations. Once induced, these rules can answer questions about ancestry chains of any length and involving anyone. Some ILP systems can also do predicate invention, i.e., discover relations that do not appear explicitly in the data, akin to hidden variables in neural networks. 2.2 Tensor Algebra tensor is defined by two properties: its type (real, integer, Boolean, etc.) and its shape (Rabanser et al., 2017). The shape of tensor consists of its rank (number of indices) and its size (number of elements) along each index. For example, video can be represented by an integer tensor of shape (t, x, y, c), where is the number of frames, and are frames 3 Domingos width and height in pixels, and is the number of color channels (typically 3). matrix is rank-2 tensor, vector rank-1 tensor, and scalar rank-0 tensor. tensor of rank and size ni in the ith dimension contains total of (cid:81)r i=1 ni elements. The element of tensor at position i1 along dimension 1, position id along dimension d, etc., is denoted by Ai1,...,id,...,ir . This generic element of tensor is often used to represent the tensor itself. The sum of two tensors and with the same shape is tensor such that Ci1,...,id,...,ir = Ai1,...,id,...,ir + Bi1,...,id,...,ir . The tensor product of two tensors and of rank respectively and is tensor of rank + such that Ci1,...,id,...,ir,j1,...,jd ,...,jr = Ai1,...,id,...,ir Bj1,...,jd ,...,jr . Einstein notation simplifies tensor equations by omitting all summation signs and implicitly summing over all repeated indices. For example, AijBjk represents the product of the matrices and B, summing over and resulting in matrix with indices and k: Cik = AijBjk = (cid:88) AijBjk. More generally, the Einstein sum (or einsum for short) of two tensors and with common indices β is tensor such that Cαγ = (cid:88) β AαβBβγ, where α, β and γ are sets of indices, α is the subset of As indices not appearing in B, the elements of α and β may be interspersed in any order, and similarly for and γ. Essentially all linear and multilinear operations in neural networks can be concisely expressed as einsums (Rocktaschel, 2018; Rogozhnikov, 2022). Like matrices, tensors can be decomposed into products of smaller tensors. In particular, the Tucker decomposition decomposes tensor into more compact core tensor of the same rank and factor matrices, each expanding an index of the core tensor into an index of the original one. For example, if is rank-3 tensor, in Einstein notation its Tucker decomposition is Aijk = MipM jqM krCpqr, where is the core tensor and the are the factor matrices. 3. Tensor Logic 3.1 Representation Tensor logic is based on the answers to two key questions: What is the relation between tensors and relations? And what is the relation between Datalog rules and einsums? The answer to the first question is that relation is compact representation of sparse Boolean tensor. For example, social network can be represented by the neighborhood 4 Tensor Logic matrix Mij, where and range over individuals and Mij = 1 if and are neighbors and 0 otherwise. But for large networks this is an inefficient representation, since almost all elements will be 0. The network can be more compactly represented by relation, with tuple for each pair of neighbors; pairs not in the relation are assumed to not be neighbors. More generally, sparse Boolean tensor of rank can be compactly represented by an n-ary relation with tuple for each nonzero element, and the efficiency gain will typically increase exponentially with n. The answer to the second question is that Datalog rule is an einsum over Boolean tensors, with step function applied elementwise to the result. (Specifically, the Heaviside step function, H(x) = 1 if > 0 and 0 otherwise.) For example, consider the rule Aunt(x, z) Sister(x, y), Parent(y, z). Viewing the relations Aunt(x, z), Sister(x, y) and Parent(y, z) as the Boolean matrices Axz, Sxy and Pyz, respectively, Axz = H(SxyPyz) = (cid:33) SxyPyz (cid:32) (cid:88) will be 1 iff Sxy and Pyz are both 1 for at least one y. In other words, the einsum SxyPyz implements the join of Sister(x, y) and Parent(y, z). If is zs aunt, is the sibling of who is also parent of z. The step function is necessary because in general for given (x, z) pair there may be more than one for which Sxy = Pyz = 1, leading to result greater than 1. The step function then reduces this to 1. Let and be arbitrary tensors, and α, β and γ be sets of indices. Then Tαγ = H(UαβVβγ) is Boolean tensor whose element with indices αγ is 1 when there exists some β for which UαβVβγ = 1. In other words, represents the join of the relations corresponding to and . Since there is direct correspondence between tensors and relations and between einsums and Datalog rules, there should also be tensor operations that directly correspond to database join and projection. We are thus led to define tensor projection and tensor join as follows. The projection of tensor onto subset of its indices α is πα(T ) = (cid:88) β Tαβ, where β is the set of indices not in α. (βs elements may be interspersed with αs in any order.) In other words, the projection of onto α is the sum for each value of α of all the elements of with that value of α. For example, vector may be projected onto scalar by summing all its elements, matrix onto column vector by summing each row into an element of the vector, cubic tensor onto any one of its faces and then that face onto one of its edges and then onto corner, etc. If the tensors are Boolean and the projection is followed by step function, tensor projection reduces to database projection. The join of two tensors and along common set of indices β is (U )αβγ = UαβVβγ, 5 Domingos where α is the subset of dimensions not in and similarly for γ and . (Again, α, β and γ may be interspersed in any order.) In other words, the join of two tensors on common subset of indices β has one element for each pair of elements with the same value of β, and that element is their product. If has rank r, has rank r, and β = q, has rank + q. When two tensors have no indices in common, their join reduces to their tensor product (Kronecker product for matrices). When they have all dimensions in common, it reduces to their elementwise product (Hadamard product for matrices). If the tensors are Boolean, tensor join reduces to database join. tensor logic program is set of tensor equations. The left-hand side (LHS) of tensor equation is the tensor being computed. The right-hand side (RHS) is series of tensor joins followed by tensor projection, and an optional univariate nonlinearity applied elementwise to the result. tensor is denoted by its name followed by list of indices, comma-separated and enclosed in square brackets. The join signs are left implicit, and the projection is onto the indices on the LHS. For example, single-layer perceptron is implemented by the tensor equation = step(W[i] X[i]), where joining on and projecting it out implements the dot product of and X. Tensors can also be specified by listing their elements, e.g., = [0.2, 1.9, 0.7, 3] and = [0, 1, 1, 0]. Typing Y? then causes to be evaluated. Notice that, like the einsum implementations in NumPy, PyTorch, etc., tensor equation is more general than the original Einstein notation: the summed-over indices are those that do not appear in the LHS, and thus repeated index may or may not be summed over. For example, the index in Y[i] = step(W[i] X[i]) is not summed over. The implementation of multilayer perceptron below utilizes this. Tensor elements are 0 by default, and equations with the same LHS are implicitly summed. This both preserves the correspondence with logic programming and makes tensor logic programs shorter. Tensor types may be declared or inferred. Setting tensor equal to file reads the file into the tensor. Reading text file results in Boolean matrix whose ijth element is 1 if the ith position in the text contains the jth word in the vocabulary. (The matrix is not stored in this inefficient form, of course; more on this later.) For example, if the file is the string Alice loves Bob and its read into the matrix M, the result is M[0, Alice] = M[1, loves] = M[2, Bob] = 1 and M[i, j] = 0 for all other i, j. (Notice how arbitrary constants, not just integers, can be used as indices.) Conversely, setting file equal to tensor writes the tensor to the file. This is the entire definition of tensor logic. There are no keywords, other constructs, etc. However, it is convenient to allow some syntactic sugar that, while not increasing the expressiveness of the language, makes it more convenient to write common programs. For example, we may allow: multiple terms in one equation (e.g., = step(W[i] X[i] + C)); index functions (e.g., X[i, t+1] = W[i, j] X[j, t]); normalization (e.g., Y[i] = softmax(X[i])); other tensor functions (e.g., Y[k] = concat(X[i, j])); alternate projection operators (e.g., max = or avg = instead of + =, which = defaults to); slices (e.g., X[4 : 8]); and procedural attachment (predefined or externally defined functions). Tensor logic accepts Datalog syntax; denoting tensor with parentheses instead of square brackets implies that its Boolean. In particular, Tensor Logic sparse Boolean tensor may be written more compactly as set of facts. For example, the vector = [0, 1, 1, 0] can also be written as X(1), X(2), with X(0) and X(3) being implicitly 0. Similarly, reading the string Alice loves Bob into the matrix produces the facts M(0, Alice), M(1, loves) and M(2, Bob).) As another simple example, multilayer perceptron can be implemented by the equation X[i, j] = sig(W[i, j, k] X[i1, k]), where ranges over layers and and over units, and sig() is the sigmoid function. Different layers may be of different sizes (and the corresponding weight matrices are implicitly padded with zeros to make up the full tensor). Alternatively, we may use different equation for each layer. basic recursive neural network (RNN) can be implemented by X[i, t+1] = sig(W[i, j] X[j, t] + V[i, j] U[j, t]), where is the state, is the input, and range over units, and ranges over time steps. The notation indicates that is virtual index: no memory is allocated for it, and successive values of the X[i] vector are written to the same location. Since RNNs are Turing-complete (Siegelmann and Sontag, 1995), the implementation above implies that so is tensor logic. 3.2 Inference Inference in tensor logic is carried out using tensor generalizations of forward and backward chaining. In forward chaining, tensor logic program is treated as linear code. The tensor equations are executed in turn, each one computing the tensor elements for which the necessary inputs are available; this is repeated until no new elements can be computed or stopping criterion is satisfied. In backward chaining, each tensor equation is treated as function. The query is the top-level call, and each equation calls the equations for the tensors on its RHS until all the relevant elements are available in the data or there are no equations for the subqueries. In the latter case (sub)query elements are assigned 0 by default. The choice of whether to use forward or backward chaining depends on the application. 3.3 Learning Because there is only one type of statement in tensor logicthe tensor equationautomatically differentiating tensor logic program is particularly simple. Univariate nonlinearity aside, the derivative of the LHS of tensor equation with respect to tensor on the RHS is just the product of the other tensors on the RHS. More precisely, if then Y[...] = T[...] X1[...] . . . Xn[...], Y[...] T[...] = X1[...] . . . Xn[...]. 7 Domingos Special cases of this include: if = AX, then Y/X = A; if = W[i] X[i], then Y/W[i] = X[i]; and if Y[i, j] = M[i, k] X[k, j], then Y[i, j]/M[i, k] = X[k, j]. As result, the gradient of tensor logic program is also tensor logic program, with one equation per equation and tensor on its RHS. Omitting indices for brevity, the derivative of the loss with respect to tensor is T (cid:88) = dL dY dY dU X, (cid:89) UT where are the equations whose RHSs appears in, is the equations LHS, is its nonlinearitys argument, and are the tensors in U. Learning tensor logic program requires specifying the loss function and the tensors it applies to by means of one or more tensor equations. For example, to learn an MLP by minimizing squared loss on the last layers outputs we can use the equation Loss = (Y[e] X[e, N, j])2, where ranges over training examples and over units, contains the target values, is the MLP as defined above extended with virtual index for examples, and is the number of layers. By default, all tensors that are not supplied as training data will be learned, but the user can specify if any should remain constant (e.g., hyperparameters). The optimizer itself can be encoded in tensor logic, but typically pre-supplied one will be used. While backpropagation in traditional neural networks is applied to the same architecture for all training examples, in tensor logic the structure may effectively vary from example to example, since different equations may apply to different examples, and backpropagating through the union of all possible derivations of the example would be wasteful. Fortunately, solution to this problem is already available in the form of backpropagation through structure, which for each example updates each equations parameters once for each time it appears in the examples derivation (Goller and Kuchler, 1996). Applying this to RNNs yields the special case of backpropagation through time (Werbos, 1990). Learning tensor logic program consisting of fixed set of equations is quite flexible, since an equation can represent any set of rules with the same join structure. (E.g., an MLP can represent any set of propositional rules.) Further, tensor decomposition in tensor logic is effectively generalization of predicate invention. For example, if the program to be learned is the equation A]i, j, k] = M[i, p] M[j, q] M[k, r] C[p, q, r] and is the sole data tensor, the learned M, M, and form Tucker decomposition of A; and thresholding them into Booleans turns them into invented predicates. 4. Implementing AI Paradigms The implementations below use forward chaining unless otherwise specified. 8 Tensor Logic 4.1 Neural Networks convolutional neural network is an MLP with convolutional and pooling layers (LeCun et al., 1998). convolutional layer applies filter at every location in an image, and can be implemented by tensor equation of the form Features[x, y] = relu(Filter[dx, dy, ch] Image[x+dx, y+dy, ch]), where and are pixel coordinates, dx and dy are filter coordinates, and ch is the RGB channel. pooling layer combines block of nearby filters into one, and can be implemented by Pooled[x/S, y/S] = Features[x, y], where / is integer division and is the stride. This results in the filter outputs at successive positions in each dimension being summed into one. This implements sum-pooling; maxpooling would replace = with max =, etc. convolutional and pooling layer can be combined into one with the equation Pooled[x/S, y/S] = relu(. . .). Graph neural networks (GNNs) apply deep learning to graph-structured data (e.g., social networks, molecules, metabolic networks, the Web) (Zhou, 2022). Table 1 shows the implementation of simple GNN. The networks graph structure is defined by the Neig(x, y) relation, with one fact for each adjacent (x, y) pair; or equivalently, by the Boolean tensor Neig[x, y] = 1 if and are adjacent and 0 otherwise. The main tensor is Emb[n, l, d], containing the d-dimensional embedding of each node in each layer l. Initialization sets each nodes 0th-layer embeddings to its features (externally defined or learned). The network then carries out iterations of message passing, one per layer. Each iteration starts by applying one or more perceptron layers to each node. (Table 1 shows one. To preserve permutation invariance, the weights WP do not depend on the node. Although there are no sub/superscripts in tensor logic, will use them here for brevity.) The GNN then aggregates each nodes neighbors new features by joining the tensors Neig(n, n) and Z[n, l, d]. For each node, this zeroes out the contributions of all non-neighbors; the result is the sum of the neighbors features. (Internally, this can be done efficiently by iterating over the nodes neighbors or by other methods; see Section 6.) The aggregated features may then be passed through another MLP (not shown), after which they are combined with the nodes features using weights WAgg and WSelf to produce the next layers embeddings. The most common applications of GNNs are node classification, edge prediction and graph classification. For two-class problems, each node is classified by doing the dot product of its final embedding with weight vector, and passing the result through sigmoid to yield the class probability. For multiclass problems (not shown), each nodes final embedding is dotted with weight vector for each class c, WOut[c, d], yielding vector of logits that is then passed through softmax to yield the class probabilities Y[n, c]. Edge prediction predicts whether there is an edge between each pair of nodes by dotting their embeddings and passing the result through sigmoid. Graph classification produces class prediction for the entire graph, and is identical to node classification save for the result being scalar instead of vector Y[n]. Attention, the basis of large language models, is also straightforward to implement in tensor logic (Vaswani et al., 2017). Given an embedding matrix X[p, d], where ranges over Domingos Table 1: Graph neural networks in tensor logic Component Graph structure Initialization MLP Aggregation Update Node classification Edge prediction Graph classification = sig(WOut[d] Emb[n, L, d]) Equation Neig(x, y) Emb[n, 0, d] = X[n, d] Z[n, l, d] = relu(WP[l, d, d] Emb[n, l, d]), etc. Agg[n, l, d] = Neig(n, n) Z[n, l, d] Emb[n, l+1, d] = relu(WAgg Agg[n, l, d] + WSelf Emb[n, l, d]) Y[n] = sig(WOut[d] Emb[n, L, d]) Y[n, n] = sig(Emb[n, L, d] Emb[n, L, d]) items (e.g., positions in text) and over embedding dimensions, the query, key and value matrices are obtained by multiplying by the corresponding weight matrices: Query[p, dk] = WQ[dk, d] X[p, d] Key[p, dk] = WK[dk, d] X[p, d] Val[p, dv] = WV[dv, d] X[p, d] Attention can then be computed in two steps, the first of which compares the query at each position with each key: Comp[p, p.] = softmax(Query[p, dk]) Key[p, dk] / sqrt(Dk)), where sqrt(Dk) scales the dot products by the square root of the keys dimension. The notation p. indicates that is the index to be normalized (i.e., for each p, softmax is applied to the vector indexed by p). The attention head then returns the sum of the value vectors weighted by the corresponding comparisons: Attn[p, dv] = Comp[p, p] Val[p, dv]. We can now implement an entire transformer with just dozen tensor equations  (Table 2)  . As we saw in Subsection 3.1, text can be represented by the relation X(p, t), stating that the pth position in the text contains the tth token. (Tokenization rules are easily expressed in Datalog, and are not shown.) The texts embedding EmbX[p, d] is then obtained by multiplying X(p, t) by the embedding matrix Emb[t, d]. The next equation implements positional encoding as in the original paper (Vaswani et al., 2017); other options are possible. (Incidentally, this equation also shows how conditionals and case statements can be implemented in tensor logic: by joining each expression with the corresponding condition.) The residual stream is then initialized to the sum of the texts embedding and the positional encoding. Attention is implemented as described above, with two additional indices for each tensor: for the attention block and for the attention head. The attention heads outputs are then concatenated, added to the residual stream and layer-normalized. MLP layers are implemented as before, with additional indices for block and position, and their outputs 10 Tensor Logic Table 2: Transformers in tensor logic Component Input Embedding Pos. encoding Residual stream Stream[0, p, d] = EmbX[p, d] + PosEnc[p, d] Attention Equation(s) X(p, t) EmbX[p, d] = X(p, t) Emb[t, d] PosEnc[p, d] = Even(d) sin(p/Ld/De) + Odd(d) cos(p/Ld1/De) Query[b, h, p, dk] = WQ[b, h, dk, d] Stream[b, p, d], etc. Comp[b, h, p, p.] = softmax(Query[b, h, p, dk] Key[b, h, p, dk]/sqrt(Dk)) Attn[b, h, p, dv] = Comp[b, h, p, p] Val[b, h, p, dv] Merge[b, p, dm] = concat(Attn[b, h, p, dv]) Stream[b, p, d.] = lnorm(WS[b, d, dm] Merge[b, p, dm] + Stream[b, p, d]) MLP[b, p] = relu(WP[p, d] Stream[b, p, d]), etc. Y[p, t.] = softmax(WO[t, d] Stream[B, p, d]) Merge and layer norm MLP Output are also normalized and added to the stream (not shown). Finally, the output (token probabilities) is obtained by dotting the stream with an output weight vector for each token and passing through softmax. 4.2 Symbolic AI Datalog program is valid tensor logic program. Therefore anything that can be done in Datalog can be done in tensor logic. This suffices to implement many symbolic systems, including reasoning and planning in function-free domains. Accommodating functions (as in Prolog) requires implementing unification in tensor logic (Lloyd, 1987). 4.3 Kernel Machines kernel machine can be implemented by the equation Y[Q] = f(A[i] Y[i] K[Q, i] + B), where is the query example, ranges over support vectors, and f() is the output nonlinearity (e.g., sigmoid) (Scholkopf and Smola, 2002). The kernel is then implemented by its own equation. For example, polynomial kernel is K[i, i] = (X[i, j] X[i, j])n, where and range over examples, ranges over features, and is the degree of the polynomial. Gaussian kernel is K[i, i] = exp((X[i, j] X[i, j])2 / Var). (More precisely, is the Gram matrix of the kernel with respect to the examples.) Structured prediction, where the output consists of multiple interrelated elements (Bakr et al., 2007), can be implemented by an output vector Y[Q, k] and equations stating the interactions among outputs and between outputs and inputs. 11 Domingos Table 3: Graphical models in tensor logic Component Factor Marginalization Pointwise product Join tree P(QueryEvidence) Prog(Q,E)/Prog(E) Belief propagation Sampling Implementation Tensor Projection Join Tree-like program Forward chaining Selective projection 4.4 Probabilistic Graphical Models graphical model represents the joint probability distribution of set of random variables as normalized product of factors, (X = x) = 1 (cid:89) ϕk(x{k}), (cid:81) where each factor ϕk is non-negative function of subset of the variables x{k} and = (cid:80) ϕk(x{k}) (Koller and Friedman, 2009). If each factor is the conditional probability of variable given its parents (predecessors in some partial ordering), the model is Bayesian network and = 1. Table 3 shows how the constructs and operations in discrete graphical models map directly onto those in tensor logic. factor is tensor of non-negative real values, with one index per variable and one value of the index per value of the variable. The unnormalized probability of state is the product of the element in each tensor corresponding to x. Bayesian network can thus be encoded in tensor logic using one equation per variable, stating the variables distribution in terms of its conditional probability table (CPT) and the parents distributions: PX[x] = CPTX[x, par1, ..., parn] P1[par1] . . . Pn[parn]. Inference in graphical models is the computation of marginal and conditional probabilities, and consists of combinations of two operations: marginalization and pointwise product. The marginalization of subset of the variables in factor ϕ sums them out, leaving factor over the remaining variables X: ϕ(X) = (cid:88) ϕ(X, ). Marginalization is just tensor projection. The pointwise product of two potentials over subsets of variables and combines them into single potential over , and is the join of the corresponding tensors. Every graphical model can be expressed as join tree, tree of factors where each factor is join of factors in the original model. All marginal and conditional queries can 12 Tensor Logic be answered in time linear in the size of the tree by successively marginalizing factors and pointwise-multiplying them with the parents factor. join tree is tree-like tensor logic program, i.e., one in which no tensor appears in more than one RHS. As result, lineartime inference can be carried out by backward chaining over this program. Specifically: the partition function can be computed by adding the equation = T[...] to the program, where T[...] is the LHS of the root factors equation, and querying Z; the marginal probability of evidence (E) can be computed by adding to the program as set of facts, querying Z, and dividing by the original one; and the conditional probability of query given evidence can be computed as (E) = (Q, E)/P (E). However, the join tree may be exponentially larger than the original model, necessitating approximate inference. The two most popular methods are loopy belief propagation and Monte Carlo sampling. Loopy belief propagation is forward chaining on the tensor logic program representing the model. Sampling can be implemented by backward chaining with selective projection (i.e., replacing projection by random subset of its terms). 5. Reasoning in Embedding Space The most interesting feature of tensor logic is the new models it suggests. In this section show how to perform knowledge representation and reasoning in embedding space, and point out the reliability and transparency of this approach. Consider first the case where an objects embedding is random unit vector. The embeddings can be stored in matrix Emb[x, d], where ranges over objects and over embedding dimensions. Multiplying Emb[x, d] by one-hot vector V[x] then retrieves the corresponding objects embedding. If V[x] is multi-hot vector representing set, is the superposition of the embeddings of the objects in the set. The dot product S[d] = V[x] Emb[x, d] D[A] = S[d] Emb[A, d] for some object is then approximately 1 if is in the set and approximately 0 otherwise (with standard deviation (cid:112)N/D, where is the cardinality of the set and is the embedding dimension). Thresholding this at 1 2 then tells us if is in the set with an error probability that decreases with the embedding dimension. This is similar to Bloom filter (Bloom, 1970). The same scheme can be extended to embedding relation. Consider binary relation R(x, y) for simplicity. Then EmbR[i, j] = R(x, y) Emb[x, i] Emb[y, j] is the superposition of the embeddings of the tuples in the relation, where the embedding of tuple is the tensor product of the embeddings of its arguments. This is type of tensor product representation (Smolensky, 1990). It can be computed in time linear in by iterating over the tuples adding the corresponding tensor product to the result. The equation D[A, B] = EmbR[i, j] Emb[A, i] Emb[B, j] Domingos retrieves R(A, B), i.e., D[A, B] is approximately 1 if the tuple (A, B) is in the relation and 0 otherwise, since D[A, B] = EmbR[i, j] Emb[A, i] Emb[B, j] = (R(x, y) Emb[x, i] Emb[y, j]) Emb[A, i] Emb[B, j] = R(x, y) (Emb[x, i] Emb[A, i]) (Emb[y, j] Emb[B, j]) R(A, B). The penultimate step is valid because einsums are commutative and associative. (In particular, the result does not depend on the order the tensors appear in, only on their index structure.) The last step is valid because the dot product of two random unit vectors is approximately 0. By the same reasoning, the equation D[A, y] = EmbR[i, j] Emb[A, i] Emb[y, j] returns the superposition of the embeddings of the objects that are in relation to A, and D[x, y] = EmbR[i, j] Emb[x, i] Emb[y, j] returns the entire relation R(x, y). EmbR[i, j], Emb[x, i] and Emb[y, j] form Tucker decomposition of the data tensor D[x, y], with EmbR[i, j] as the core tensor and Emb[x, i] and Emb[y, j] as the factor matrices. The relation symbols themselves may be embedded. (E.g., R, and in R(A, B) may all be embedded.) This results in rank-3 tensor. Relations of arbitrary arity can be reduced to sets of (relation, argument, value) triples. Thus an entire database can be embedded as single rank-3 tensor. The next step is to embed rules. We can embed Datalog rule by replacing its antecedents and consequents by their embeddings: if the rule is its embedding is where Cons(...) Ant1(...), . . . , Antn(...), EmbCons[...] = EmbAnt1[...] . . . EmbAntn[...], EmbAnt1[...] = Ant1(...) Emb[...] . . . Emb[...], etc. Reasoning in embedding space can now be carried out by forward or backward chaining over the embedded rules. The answer to query can be extracted by joining its tensor with its arguments embeddings, as shown above for any relation. This gives approximately the correct result because each inferred tensor can be expressed as sum of projections of joins of embedded relations, and the product Emb[x, i] Emb[x, i] for each of its arguments is approximately the identity matrix. The error probability decreases with the embedding dimension, as before. To further reduce it, we can extract, threshold and re-embed the inferred tensors at regular intervals (in the limit, after each rule application). 14 Tensor Logic The most interesting case, however, is when objects embeddings are learned. The product of the embedding matrix and its transpose, Sim[x, x] = Emb[x, d] Emb[x, d], is now the Gram matrix measuring the similarity of each pair of objects by the dot product of their embeddings. Similar objects borrow inferences from each other, with weight proportional to their similarity. This leads to powerful form of analogical reasoning that explicitly combines similarity and compositionality in deep architecture. If we apply sigmoid to each equation, σ(x, ) = 1 (1 + ex/T ) , setting its temperature parameter to 0 effectively reduces the Gram matrix to the identity matrix, making the programs reasoning purely deductive. This contrasts with LLMs, which may hallucinate even at = 0. Its also exponentially more powerful than retrievalaugmented generation (Jiang et al., 2025), since it effectively retrieves the deductive closure of the facts under the rules rather than just the facts. Increasing the temperature makes reasoning increasingly analogical, with examples that are less and less similar borrowing inferences from each other. The optimal will depend on the application, and can be different for different rules (e.g., some rules may be mathematical truths and have = 0, while others may serve to accumulate weak evidence and have high ). The inferred tensors can be extracted at any point during inference. This makes reasoning highly transparent, in contrast with LLM-based reasoning models. Its also highly reliable and immune to hallucinations at sufficiently low temperature, again in contrast with LLM-based models. At the same time, it has the generalization and analogical abilities of reasoning in embedding space. This could make it ideal for wide range of applications. 6. Scaling Up For large-scale learning and inference, equations involving dense tensors can be directly implemented on GPUs. Operations on sparse and mixed tensors can be implemented using (at least) one of two approaches. The first is separation of concerns: operations on dense (sub)tensors are implemented on GPUs, and operations on sparse (sub)tensors are implemented using database query engine, by treating (sub)tensors as relations. The full panoply of query optimization can then be applied to combining these sparse (sub)tensors. An entire dense subtensor may be treated as single tuple by the database engine, with an argument pointing to the subtensor. Dense subtensors are then joined and projected using GPUs. The second and more interesting approach is to carry out all operations on GPUs, first converting the sparse tensors into dense ones via Tucker decomposition. This is exponentially more efficient than operating directly on the sparse tensors, and as we saw in the previous section, even random decomposition will suffice. The price is that there will be small probability of error, but this can be controlled by appropriately setting the embedding dimension and denoising results by passing them through step functions. Scaling up via 15 Domingos Tucker decompositions has the significant advantage that it combines seamlessly with the learning and reasoning algorithms described in previous sections. 7. Discussion Tensor logic is likely to be useful beyond AI. Scientific computing consists essentially of translating equations into code, and with tensor logic this translation is more direct than with previous languages, often with one-to-one correspondence between symbols on paper and symbols in code. In scientific computing the relevant equations are then wrapped in logical statements that control their execution. Tensor logic makes this control structure automatically learnable by relaxing the corresponding Boolean tensors to numeric ones, and optionally thresholding the results back into logic. The same approach is applicable in principle to making any program learnable. Any new programming language faces steep climb to wide adoption. What are tensor logics chances of succeeding? AI programming is no longer niche; tensor logic can ride the AI wave to wide adoption in the same way that Java rode the Internet wave. Backward compatibility with Python is key, and tensor logic lends itself well to it: it can initially be used as more elegant implementation of einsum and extension of Python to reasoning tasks, and as it develops it can absorb more and more features of NumPy, PyTorch, etc., until it supersedes them. Above all, adoption of new languages is driven by the big pains they cure and the killer apps they support, and tensor logic very much has these: e.g., it potentially cures the hallucinations and opacity of LLMs, and is the ideal language for reasoning, mathematical and coding models. Fostering an open-source community around tensor logic will be front and center. Tensor logic lends itself to IDEs that tightly integrate coding, data wrangling, modeling and evaluation, and if it takes off vendors will compete to support it. It is also ideally suited to teaching and learning AI, and this is another vector by which it can spread. Next steps include implementing tensor logic directly in CUDA, using it in wide range of applications, developing libraries and extensions, and pursuing the new research directions it makes possible. For more information on tensor logic, visit tensor-logic.org."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was partly funded by ONR grant N00014-18-1-2826."
        },
        {
            "title": "References",
            "content": "G. Bakr, T. Hofmann, B. Scholkopf, A. Smola, B. Taskar, and S. Vishwanathan, editors. Predicting Structured Data. MIT Press, Cambridge, MA, 2007. B. Bloom. Space/time tradeoffs in hash coding with allowable errors. Comm. ACM, 13: 422426, 1970. 16 Tensor Logic C. Goller and A. Kuchler. Learning task-dependent distributed representations by backpropagation through structure. In Proc. Int. Conf. Neural Networks, pp. 347352, 1996. S. Greco and C. Molinaro. Datalog and Logic Databases. Morgan & Claypool, San Rafael, CA, 2016. P. Jiang, S. Ouyang, Y. Jiao, M. Zhong, R. Tian, and J. Han. Retrieval and structuring augmented generation with large language models. In Proc. Int. Conf. Knowl. Disc. & Data Mining, pp. 60326042, 2025. D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, Cambridge, MA, 2009. N. Lavraˇc and S. Dˇzeroski. Inductive Logic Programming: Techniques and Applications. Ellis Horwood, Chichester, UK, 1994. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86:22782324, 1998. J. W. Lloyd. Foundations of Logic Programming (2nd ed.). Springer, Berlin, Germany, 1987. S. Rabanser, O. Shchur, and S. Gunnemann. Introduction to tensor decompositions and their applications in machine learning. arXiv:1711.1078, 2017. T. Rocktaschel. Einsum is all you need Einstein summation in deep learning. https://rockt.ai/2018/04/30/einsum, 2018. A. Rogozhnikov. Einops: Clear and reliable tensor manipulations with Einstein-like notation. In Proc. Int. Conf. Learn. Repr., 2022. B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, 2002. H. Siegelmann and E. Sontag. On the computational power of neural nets. J. Comp. & Sys. Sci., 50:132150, 1995. P. Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artif. Intel., 46:159216, 1990. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. Adv. Neural Inf. Proc. Sys., 30:59986008, 2017. P. Werbos. Backpropagation through time: What it does and how to do it. Proc. IEEE, 78:15501560, 1990. Z. Liu & J. Zhou. Introduction to Graph Neural Networks. Morgan & Claypool, San Rafael, CA, 2022."
        }
    ],
    "affiliations": [
        "Paul G. Allen School of Computer Science & Engineering University of Washington Seattle, WA 98195-2350, USA"
    ]
}