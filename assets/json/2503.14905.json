{
    "paper_title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation",
    "authors": [
        "Siwei Wen",
        "Junyan Ye",
        "Peilin Feng",
        "Hengrui Kang",
        "Zichen Wen",
        "Yize Chen",
        "Jiang Wu",
        "Wenjun Wu",
        "Conghui He",
        "Weijia Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 5 0 9 4 1 . 3 0 5 2 : r Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation Siwei Wen1,3, Junyan Ye2,1, Peilin Feng1,3, Hengrui Kang4,1, Zichen Wen4,1, Yize Chen5, Jiang Wu1, Wenjun Wu3, Conghui He1, Weijia Li2,1 1Shanghai Artificial Intelligence Laboratory, 2Sun Yat-Sen University, 3Beihang University, 4Shanghai Jiao Tong University, 5The Chinese University of Hong Kong, Shenzhen"
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting new benchmark for synthetic image detection. The dataset and code will be released in: https: //github.com/opendatalab/FakeVLM . 1. Introduction As AI-generated content technologies advance, synthetic images are increasingly integrated into our daily lives [1, 11, 36]. Technologies like Diffusion [15, 36, 59] and Flux can generate highly realistic images. However, synthetic image data also poses significant risks, including potential misuse and social disruption [9, 20]. For instance, synthetic fake Equal Contribution. Corresponding author. Figure 1. FakeVLM is specialized large multimodal model designed for both DeepFake and general synthetic image detection tasks across multiple domains. news [19, 34, 42] and forged facial scams [31, 51] make it increasingly challenging to discern the truth and establish trust in information. In response to these threats, the field of synthetic data detection has garnered widespread attention in recent years. However, traditional synthetic detection methods typically focus on simple authenticity judgments [2, 32, 39, 40, 56, 60], ultimately providing only forgery probability or classification result. This approach still has significant limitations in terms of human interpretability. As result, users find it challenging to understand the reasons behind the systems decisions, affecting the decision-making processs transparency and trustworthiness. The rapid development of large multimodal models has accelerated progress in synthetic data detection [6, 16, 17, 24, 52, 61]. These models, particularly, can provide explanations for authenticity judgments in natural language, thus laying the groundwork for enhancing the interpretability of synthetic detection. For instance, Jia et al. [18] explored ChatGPTs ability to assess synthetic data, highlighting the potential of large models in this domain. LOKI [58] and Fakebench [28] further delved into the capabilities of large models in offering explanations for image detail artifacts. However, these studies primarily focus on pre-trained large models, utilizing strategies such as different prompt wordings to enhance model performance on this task rather than developing specialized multimodal model for this specific domain. Moreover, existing general large models still show significant performance gap compared to expert models or human users in detection tasks. Researchers have further explored and designed multimodal models specifically for synthetic data detection tasks [6, 17, 58]. For instance, works like DD-VQA [61] and FFAA [17] focus primarily on the performance of large models in Deepfake detection tasks, especially in the context of artifact explanation. However, their performance on more general types of synthetic images still requires further investigation. Other works, such as Fakeshield [52] and ForgeryGPT [24], effectively examine the ability of large models to localize forgery artifacts and explain manipulated synthetic data. However, artifacts in forged synthetic images often concentrate in transitional areas, exhibiting more noticeable edge artifacts. In contrast, direct synthetic images are more likely to show structural, distortion, or physical artifacts, highlighting significant differences between the two. Additionally, current large models still lag behind expert models in pure authenticity classification. For example, studies such as X2-DFD [6] and FFAA [17] attempt to integrate traditional expert-based synthetic detection methods to improve classification accuracy without fully unlocking the potential of large models in synthetic detection tasks. To address the challenges outlined above, we introduce FakeVLM, large multimodal model specifically designed for fake image detection and artifact explanation. As shown in Fig 1, FakeVLM focuses on artifacts generated from synthetic image models rather than forgery artifacts. Moreover, it is not limited to facing Deepfake tasks but extends to more general synthetic data detection. Notably, FakeVLM achieves performance comparable to current expert models based on binary classification without requiring additional classifiers or expert models. Additionally, we introduce FakeClue, dataset containing over 100,000 real and synthetic images, along with corresponding artifact cues for the synthetic images. FakeClue includes images from seven different categories (e.g., animal, human, object, scenery, satellite, document, deepfake), and leverages category-specific knowledge to annotate image artifacts in natural language using multiple LMMs. Our main contributions are as follows: We propose FakeVLM, multimodal large model designed for both general synthetic and deepfake image detection tasks. It excels at distinguishing real from fake images while also providing excellent interpretability for artifact details in synthetic images. We introduce the FakeClue dataset, which includes rich variety of image categories and fine-grained artifact annotations in natural language. Our method has been extensively evaluated on multiple datasets, achieving outstanding performance in both synthetic detection and abnormal artifact explanation tasks. 2. Related Work 2.1. Synthetic Image Detection Traditional synthetic detection tasks are typically approached as binary classification tasks [14, 18, 27, 63], treating them as either true or false, based on data-driven methods, with various detectors such as CNNs [22] and transformers [45]. Some research [5, 44, 54] has explored methods like learning anomalies in the frequency domain, reconstruction learning, and adversarial learning. The focus of traditional synthetic detection tasks has mainly been on addressing the generalization issue [32, 56], where the training and testing domains differ, in order to counter the rapidly evolving synthetic models. However, these methods only provide authenticity predictions without offering detailed explanations behind the predictions. Some works have gone beyond the fundamental true/false classification problem, focusing on interpretable synthetic detection. For example, gradient-based methods are used to visualize highlighted regions of predictions [38, 41, 43]. Alternatively, model designs like DFGNN [21] enhance interpretability by applying interpretable GNNs to deepfake detection tasks. Additionally, research has explored the detection and localization of forgeries by constructing image artifacts or modifying labels [39, 40, 60]. While these methods enhance model interpretability, using natural language to describe the identified reasons remains underexplored. 2.2. Synthetic Image Detection via LMMs Recently, the development of large multimodal models (LMMs) has been rapid. Models such as the closed-source GPT-4o [33] and open-source models like InternVL [7] and Qwen2-VL [46] have demonstrated outstanding performance across various tasks, showcasing impressive capabilities. In the domain of synthetic data detection, several works like [18], Fakebench [28], and LOKI [58] have investigated the potential of LMMs, where these large models not only deliver accurate synthetic detection judgments but also offer natural language explanations for their true/false predictions, enhancing the interpretability of synthetic detection. However, these studies mainly focus on evaluating pretrained large models rather than training expert multimodal models. Furthermore, existing general models still lag behind expert models or humans in detection tasks. An increasing number of studies have further explored the use of LMMs for synthetic detection, such as DD-VQA Figure 2. Construction pipeline of FakeClue dataset, including data collection from open source and self-synthesized datasets, pre-processing with categorization, label prompt design based on category knowledge, and multiple LMMs annotation with result aggregation. [61] and FFAA [17] have explored the performance of large models in the deepfake domain, yet the performance on more general synthetic image types remains underexplored. Fakeshield [52], SIDA [16] and ForgeryGPT [24] effectively investigate the ability of LMMs to detect and explain artifacts in manipulated synthetic data. However, tampering artifacts primarily focus on transitional artifacts, while direct image synthesis artifacts tend to involve structural distortions or other types of image warping, which differ significantly. Additionally, existing large models still lag behind expert models in simple real/false classification tasks [6, 18]. For example, studies like X2-DFD [6] and FFAA [17] have combined traditional expert synthetic detection methods with large models to improve classification accuracy. 3. Dataset 3.1. Overview We introduce FakeClue, multimodal synthetic data detection benchmark designed for both general and DeepFake detection tasks. It includes two tasks: synthetic detection and artifact explanation, where the model is required to not only determine the authenticity of an image but also explain its artifacts. FakeClue covers 7 different categories of images, with over 100k image samples. Using multi-LMM labeling strategy based on category priors, FakeClue is organized as image-caption pairs for both the image and its artifact explanation in natural language. Additionally, it focuses more on directly synthesized image artifacts rather than tampered image artifacts. The training and test sets are randomly split, with the test set containing 5,000 samples covering various image types. More detailed information about the dataset will be provided in the supplementary materials. 3.2. Construction of FakeClue Data Collection: As shown in the data collection phase of Figure 2, FakeClue has two data sources, including open synthetic datasets and our newly synthesized data for specialized types. For open synthetic datasets, we extracted approximately 80K data from GenImage [64], FF++ [37] and Chameleon [55], maintaining 1:1 ratio of fake to real data. For specialized types of data, such as remote sensing and document images, we generated the data ourselves. We collected remote sensing images from public datasets like CVUSA [49] and VIGOR [65], using GAN and Diffusionbased methods [26, 57], covering urban, suburban, and natural scenes. For document images, we adopted layoutfirst, content-rendering approach, generating synthetic images of newspapers, papers, and magazines with real-world data sourced from the M6Doc dataset [8]. Data Pre-Processing: At this stage, we first categorize the collected raw image data based on authenticity labels. Since the data from GenImage and Chameleon lack category information, we use classification model [12] to divide the data into four categories: animals, objects, people, and landscapes. The FF++ dataset corresponds to the DeepFake category, while the newly synthesized satellite and document data also have clear category divisions. The distribution and proportions of these categories are shown in Figure 2. The labels obtained during the data preprocessing stage will serve as the foundation for the subsequent Label Prompt Design. Label Prompt Design based on category knowledge: To overcome the hallucinations and limited synthetic detection capabilities of large models, we inject external knowledge to aid in artifact detection. Based on the authenticity labels obtained in the data pre-processing phase, these labels are retained as prior knowledge within the prompt. For real images, we analyze the plausibility of the image as photographic result, while for fake images, we focus on detecting artifacts throughout the image. Classification labels are used as category-specific knowledge to guide the models attention to key areas, such as the artifact detection cues for facial images, as shown in Fig. 2. Accordingly, we design 12 disDataset Field Artifact Annotator Category Number DD-VQA [61] FF-VQA [17] LOKI [58] Fakebench [28] MMTD-Set [52] DF DF Gen Gen Gen Syn Syn Syn Syn Tam Human GPT Human Human GPT FakeClue DF+Gen Syn Multi-LMMs 5k 95K 3k 6k 100k 100k Table 1. Comparison with existing synthetic detection datasets (DF: DeepFake, Gen: General, Syn: Synthesis, Tam: Tampering). tinct types of prompts to address various authenticity and category labels. In addition, for synthetic images that may have high quality and no visible artifacts (e.g., images from the Chameleon dataset), we apply special image tags to allow the model to refrain from forced artifact explanation. Multiple LMMs Annotation: To mitigate the bias or hallucination effects of single multimodal model, we adopt strategy of annotating and then aggregating results from multiple high-performance open-source large models. For given image set 𝐼𝑖 , we first use three large multimodal modelsQwen2-VL, InterVL, and Deepseekto generate candidate captions 𝐴1 for each image. Different models may focus on different aspects of image artifacts in their captions. We then define an aggregation function to combine these candidate captions into unified annotation. 𝑖 , 𝐴3 𝑖 𝑖 , 𝐴2 𝑁 𝑖=1 𝐴𝑖 = (𝐴1 𝑖 , 𝐴2 𝑖 , 𝐴3 𝑖 ). (1) During this stage, the model extracts common points from multiple model responses, filters out irrelevant observations that appear in only one model (unless they are critical, like glaring artifacts) and organizes them into hierarchical structure by categories such as texture, geometry, and lighting, before outputting in fixed format. 3.3. Comparisons with Existing Datasets Table 1 presents comparison of the synthetic detection performance of FakeClue and existing evaluated LMMs datasets. FakeClue covers broader range of domains, and unlike DDVQA, FF-VQA is not limited to DeepFake detection tasks. The corresponding synthetic images exhibit well-defined category classifications, including specialized types such as satellite images and documents. In contrast, LOKI and Fakebench are limited by the number of annotations and can only serve as evaluation sets. Compared to the recent MMTDSet dataset, FakeClue focuses more on directly synthesized image artifacts rather than tampered artifacts. 4. Method In this section, we first analyze the challenges of large multimodal models in synthetic image detection (Section 4.1). Then, we provide detailed description of the FakeVLM architecture (Section 4.2) and training strategy (Section 4.3). Figure 3. Comparison of synthetic image detection approaches on LOKI and FakeClue datasets: (1) QA with Frozen LMMs (no training), (2) Frozen backbone + linear probe (only linear layer trained), (3) Direct Real/Fake QA tuning, and (4) VQA with artifact explanations tuning. 4.1. Re-thinking LMMs Challenges in Synthetic"
        },
        {
            "title": "Image Detection",
            "content": "Recent studies have shown that directly using LMMs for synthetic image detection still faces significant challenges [18, 28, 58]. Although large models possess strong text explanation capabilities, when tasked with determining whether an image was AI-generated or identifying forged images from set, pretrained LMMs often fail to achieve satisfactory performance. This phenomenon highlights the difficulty of relying on LMMs for authenticity judgment, which is closely related to the fact that these models are not inherently designed for synthetic data detection tasks. Nevertheless, through extensive pretraining tasks, multimodal large models have developed strong visual feature extraction abilities and alignment with text. This raises the question: do the internal representations of these large models potentially encode information that can distinguish real images from synthetic ones? Inspired by Zhang et al. [62], we explored simple yet effective method on FakeClue: extracting visual features from the last layer of pre-trained LMM and training lightweight linear classifier to determine image authenticity. If the representations learned by the model indeed contain discriminative information related to authenticity, even simple classifier can perform initial detection using these features. As shown in Figure 3, the results confirm that the LMM has potential in distinguishing authenticity. However, in contrast to the conclusions of Zhang et al. [62], framing the task as \"Does the image look real/fake?\" by using fixed answers like \"Real\" or \"Fake\" not only limits the models ability to provide textual explanations but also results in suboptimal performance. This is likely due to large models challenges in aligning complex visual content with such binary answers. Building on the dataset we constructed, we found that framing the task as visual question answering, where the model is required to provide not just Real/Fake answers but also explain the image artifacts, leads to better alignment between the response text and the image content in Figure 3 (For detailed results, please refer to Appendix). This approach Figure 4. Overview of FakeVLM, our proposed framework for detecting synthetic images and explaining their artifacts. Built upon LLaVA, FakeVLM integrates multiple captioning models to assess key visual aspects. not only improves the performance of artifact explanation but also significantly enhances the overall performance of synthetic image detection. performance across diverse tasks. To further enhance its reasoning abilities on synthetic data, we perform full-parameter fine-tuning, optimizing the following objective: 4.2. Model Architecture Our approach follows the architecture of LLaVA-v1.5, as illustrated in the framework diagram (see Figure 4 in the top-right corner), which consists of three core components: i) Global Image Encoder, ii) an MLP Projector, and iii) Large Language Model (LLM). We detail each component as follows: Global Image Encoder: We employ the pretrained vision backbone of CLIP-ViT(L-14) [35] as our global image encoder. The encoder processes input images with resolution of 336336 to preserve synthetic artifact details, resulting in 576 patches per image. 𝑉 = CLIP-ViT(𝐼) ℝ𝑁𝑑𝑣 (2) denotes the number of patches (𝑃 = 14), where 𝑁 = 𝐻𝑊 𝑃 2 and 𝑑𝑣 = 1024 the feature dimension. Multi-modal Projector: two-layer MLP adaptor bridges visual and textual modalities: 𝐻 = GeLU(𝑉 𝑊1 + 𝑏1) 𝑍 = 𝐻𝑊2 + 𝑏2 (3) where 𝑊1 ℝ10244096, 𝑊2 ℝ40964096 are learnable parameters. The projected features 𝑍 ℝ𝑁4096 combine with text embeddings of the task prompt 𝑃 through concatenation. Large Language Model: We utilize Vicuna-v1.5-7B, 7B language model, as our base LLM. Vicuna-v1.5 is renowned for its strong instruction-following capabilities and robust (𝜃) = 𝑇𝑖 𝑡=1 log 𝑝𝜃 ) (𝑎𝑖,𝑡 𝑎𝑖,<𝑡, [𝑍; 𝐸(𝑃 )] (4) where 𝐸() denotes text embeddings. we update all parameters 𝜃 of the LLM during training, enabling comprehensive adaptation to synthetic data reasoning while maintaining the models original instruction-following capabilities through full-parameter optimization. By integrating these components, our pipeline leverages the strengths of multimodal models and fine-tunes LLaVA to achieve robust performance in detecting and explaining synthetic data. 4.3. Training strategy As described in the construction process of FakeClue, we leverage category knowledge and utilize multiple LMMs to annotate image artifacts in natural language. As shown on the left side of Fig. 4, we obtain high-quality artifact descriptions as target outputs for the model. Then, we use the QA pairs obtained after the multi-model annotation and summarization steps as our training data. Each data sample consists of: (1) an image 𝐼; (2) standardized prompt 𝑃 : \"Does the image look real/fake?\"; (3) the aggregated answer 𝐴 from the multi-model annotations. Our model was initialized from the original weights of the LLaVA-1.5 7B model [29], and we performed full-parameter fine-tuning using the QA pairs from our constructed dataset. The training is conducted for two epochs on eight NVIDIA"
        },
        {
            "title": "Method",
            "content": "Deepseek-VL2-small [50] Deepseek-VL2 [50] InternVL2-8B [7] InternVL2-40B [7] Qwen2-VL-7B [46] Qwen2-VL-72B [46] GPT-4o [33] FakeClue (Ours) LOKI Evaluations (ICLR 2025) Acc 0.404 0.475 0.506 0.507 0.457 0.578 0.474 F1 0.542 0.541 0.490 0.463 0.592 0.565 0.420 ROUGE_L CSS Acc 0.253 0.431 0.526 0.507 0.571 0.554 0. 0.171 0.172 0.180 0.176 0.266 0.175 0.134 0.504 0.505 0.581 0.552 0.565 0.544 0.407 F1 0.387 0.392 0.340 0.376 0.350 0.409 0.572 ROUGE_L CSS 0.391 0.388 0.472 0.473 0.384 0.432 0.354 0.164 0.169 0.179 0.184 0.182 0.173 0."
        },
        {
            "title": "FakeVLM",
            "content": "𝟎.𝟗𝟖𝟔 𝟎.𝟗𝟖𝟏 𝟎.𝟓𝟖𝟎 𝟎.𝟖𝟕𝟕 𝟎.𝟖𝟒𝟑 𝟎. 𝟎.𝟐𝟎𝟏 𝟎.𝟓𝟖𝟐 Table 2. The experimental results on the FakeClue and LOKI datasets include both Detection and Artifact Explanation performance. A100 GPUs with batch size of 32 per GPU. We employ learning rate of 2e-4, with linear learning rate warmup over the first 3% of the training steps, followed by cosine decay. By fine-tuning all parameters of the model, we enable comprehensive adaptation to the nuances of synthetic data detection and explanation while preserving its general instruction-following capabilities. 5. Experiment In this section, we introduce three additional datasets used in the experiments, alongside FakeClue, and describe our experimental setup. We then present FakeVLMs performance on general synthetic and DeepFake detection tasks, as well as its ability to explain image artifacts. Finally, we conduct ablation studies and further exploratory experiments to assess the models performance. 5.1. Other Benchmarks LOKI [58] is recently proposed benchmark for evaluating multimodal large models in general synthetic detection tasks. Beyond just distinguishing real from fake, LOKI also includes human manually annotated fine-grained image artifacts, enabling thorough exploration of the models ability to explain image artifacts. FF++ [37] is widely used benchmark dataset for facial forgery detection, containing face images and videos generated by different types of forgery techniques. The dataset includes forged data created using four common forgery methods: DeepFakes, Face2Face, FaceSwap, and NeuralTextures. We used the commonly employed C23 versions. DD-VQA [61] is new dataset for artifact explanation in the face domain, designed to leverage human common-sense perception to assess image authenticity. It includes artifacts like blurred hairlines, mismatched eyebrows, rigid pupils, and unnatural skin shadows. Built on FF++ image data, DD-VQA uses manual artifact annotations in Visual Question Answering (VQA) format, where the model answers questions about artifacts using common-sense reasoning. Method CNNSpot [48] Gram-Net [30] Fusing [20] LNP [3] UnivFD [32] AntifakePrompt [4] SIDA [16] Real Fake Overall Acc 87.8 22.8 87.7 63.1 89.4 91.3 92.9 F1 88.4 34.1 86.1 67.4 88.3 92.5 93.1 Acc 28.4 78.8 15.5 56.9 44.9 89.3 90. F1 44.2 88.1 27.2 72.5 61.2 91.2 91.0 Acc 40.6 67.4 40.4 58.2 53.9 90.6 91.8 F1 43.3 79.4 36.5 68.3 60.7 91.2 92. FakeVLM 98.2 99.1 89.7 94.6 94. 94.3 Table 3. Comparison with other detection methods on the DMimage [10] dataset, using the original weights for each method. 5.2. Experimental setup Task Settings. The LOKI dataset is used solely as the evaluation set, while training is conducted on the FakeClue dataset, with testing performed on LOKI. For the FF++ and DD-VQA datasets, we use their default training-test splits for evaluation. The evaluation metrics are divided into two tasks: detection and artifact explanation. Classification accuracy is represented by accuracy (Acc) and F1 scores, while artifact explanation accuracy is measured using CSS and ROUGE_L. When evaluating large models, since the results are based on generated text rather than binary classification scores, we follow the DD-VQA [61] task and report ACC instead of AUC. However, for evaluating expert models, we still rely on the AUC metric. Compared Baselines. For tasks that require both synthetic detection and artifact explanation, such as FakeClue, DDVQA, and LOKI, we compared various general-purpose LMMs, including advanced closed-source models like GPT4 and open-source models such as Qwen2-VL, LLaVA, InternVL2, and Deepseek-VL2. We also included the CommonDF method proposed in the DD-VQA dataset paper. For pure synthetic data detection tasks, we further compared with recent state-of-the-art expert detection methods. Figure 5. Comparative case analysis of synthetic image detection, covering animals, people, objects, documents, and remote sensing. FakeVLM outperforms GPT in precision, comprehensiveness, and relevance, indicating its superior detection and interpretation capabilities. 5.3. Universal synthetic detection Table 2 presents comparison of our method (FakeVLM) with other leading general-purpose large models. It is clear that FakeVLM outperforms on multiple metrics, both in terms of synthetic detection performance and artifact explanation capabilities. Specifically, compared to the current powerful open-source model Qwen2-VL-72B, FakeVLM achieves an average improvement of 36.1% in Acc and 41.3% in F1 on both FakeClue and LOKI. Additionally, LOKI includes an extra evaluation metric for human performance, with an Acc of 80.1, whereas FakeVLM achieves an Acc of 84.30, surpassing human performance. This is likely attributed to FakeVLMs ability to capture deep, image-level features that are imperceptible to the human eye for accurate authenticity judgment. We present the qualitative evaluation results of FakeVLM in Figure 5. It is evident that FakeVLM not only accurately identifies potential issues arising from the synthesis process, such as visual artifacts, texture distortions, and structural anomalies, but also provides detailed explanations in natural language. This sets FakeVLM apart from traditional synthetic detection methods, which typically rely on probability thresholds to determine if an image is synthetic. FakeVLM offers intuitive and easily understandable descriptions for the detection results, thereby significantly enhancing the interpretability of the synthetic detection process. Moreover, it DD-VQA (ECCV 2024) Method InternVL2-8B [7] InternVL2-40B [7] Qwen2-VL-7B [46] Qwen2-VL-72B [46] GPT-4o [33] Common-DF-T [13] Common-DF-I [13] Common-DF-TI [13] Acc 0.569 0.525 0.457 0.595 0.532 0.837 0.849 0.875 F1 0.531 0.577 0.589 0.579 0.317 0.876 0.884 0.901 FakeVLM 0.932 0.931 ROUGE_L CSS 0.511 0.545 0.565 0.566 0.427 0.143 0.222 0.266 0.205 0.130 0.577 0.588 0.609 0. - - - 0.866 Table 4. The experimental results evaluated on the DD-VQA datasets. Common-DF-T and Common-DF-I represent text or image contrastive losses, respectively, while Common-DF-TI denotes both text and image contrastive losses. helps users make confident decisions when assessing synthetic content and provides reliable grounds. We also present the generalization experiment results of FakeVLM on DMimgae [10] in Table 3, following Huang et al. [16]. The experimental results show that the performance gap between FakeVLM and other expert models is not significant, with FakeVLM even outperforming some of them. FakeVLM does not rely on additional classifiers or expert models, yet it achieves performance comparable to or even exceeding that of expert classifiers while retaining its language capability for artifact explanation. This demonstrates the potential of large models in synthetic detection. Figure 7. Performance of FakeVLM on real images. Method LLaVA + Linear Head LLaVA + Explanatory Text LOKI Evaluations (ICLR 2025) Acc 0.816 0.843 F1 0.778 0.801 ROUGE_L CSS - 0.609 - 0.582 Table 6. Ablation study comparing linear classification and explanatory text paradigms. 5.5. Ablation Study and More Exploration Impact of explanatory text. To validate the effectiveness of our explanatory VQA text paradigm, we conduct ablation experiments comparing two variants under same training settings: (1) LLaVA + Linear Head: Full parameter fine-tuning with linear classification head trained for binary prediction; (2) LLaVA + Explanatory Text: Training LLaVA to generate explanatory answers (e.g., This image is synthetic fake [. . . ]) instead of fixed labels. Both variants are trained on FakeClue and tested on LOKI, and all parameters are trainable. As shown in Table 6, the explanatory text paradigm demonstrates advantages in out-of-distribution (OOD) generalization on the LOKI benchmark. Performance on real images. In real-world applications, authentic images still make up the vast majority. This requires models to avoid forcibly identifying artifacts in genuine images and incorrectly filtering them out. Fig. 7 demonstrates FakeVLMs performance when encountering authentic images. It is evident that the model comprehensively assesses multiple layers of information within the image, such as object structure, physical lighting, color distribution, and fine textures, to make holistic judgment on the images authenticity. This capability not only enhances the models reliability in real-world applications but also ensures its robustness in distinguishing between synthetic and real images. The supplementary materials include more experiments, such as the Robustness Study on image perturbations. 6. Conclusion The rapid growth of AI-generated images has posed challenges to the authenticity of information, driving the demand for reliable and transparent detection methods. As image synthesis detection techniques have advanced alongside multimodal large language models (MLLMs), approaches have shifted from non-MLLM to MLLM-based methods. Our proposed FakeVLM is large model that integrates both synthetic image detection and artifact explanation. Through an Figure 6. Typical cases on DD-VQA dataset. Our model accurately identifies and explains the synthetic artifacts, demonstrating its effectiveness in fine-grained DeepFake detection and interpretation. Method FWA [47] Face X-ray [25] SRM [23] CDFA [53] FF++ (ICCV 2019) - AUC(%) FF-DF 92.1 97.9 97.3 99.9 FF-F2F 90.0 98.7 97.0 86. FF-FS 88.4 98.7 97.4 93.3 FF-NT 81.2 92.9 93.0 80.7 Average 87.7 95.9 95.8 90.2 FakeVLM 97.2 96. 96.8 95.0 96.3 Table 5. Experimental results of fine-tuning BLIP on DD-VQA dataset, which is built on FF++, including both deepfake detection and answer generation performance. 5.4. DeepFake detection We next evaluate the performance of FakeVLM on DeepFake detection, and Table 4 shows its results on DD-VQA. FakeVLM not only outperforms general-purpose multimodal large models but also significantly exceeds the performance of the specialized vision-language model, Common-DF. Compared to Common-DF, FakeVLM improves Acc by 5.7%, F1 by 3%, and ROUGE_L, which represents artifact explanation performance, by 9.5%. Figure 5 presents the qualitative results on DD-VQA. FakeVLM is capable of not only describing the artifacts across the entire image but also answering questions based on specific detail regions. Table 5 presents the evaluation results of FakeVLM on the DeepFake synthesis detection dataset FF++, including its performance across multiple sub-categories such as DeepFakes, Face2Face, FaceSwap, and NeuralTextures. The experimental results demonstrate that FakeVLM continues to exhibit strong performance in these tasks. Not only does it maintain leading position in overall detection, but its performance across each category is also relatively balanced, avoiding the issue of overfitting. effective training strategy, FakeVLM leverages the potential of large models for synthetic detection without relying on expert classifiers. It performs well in both synthetic detection and artifact explanation tasks, offering new insights and directions for future research in synthetic image detection."
        },
        {
            "title": "References",
            "content": "[1] Malak Abdullah, Alia Madain, and Yaser Jararweh. Chatgpt: Fundamentals, applications and social impacts. In 2022 Ninth International Conference on Social Networks Analysis, Management and Security (SNAMS), pages 18. Ieee, 2022. 1 [2] Mauro Barni, Kassem Kallas, Ehsan Nowroozi, and Benedetta Tondi. Cnn detection of gan-generated face images based on cross-band co-occurrences analysis. In 2020 IEEE international workshop on information forensics and security (WIFS), pages 16. IEEE, 2020. 1 [3] Xiuli Bi, Bo Liu, Fan Yang, Bin Xiao, Weisheng Li, Gao Huang, and Pamela Cosman. Detecting generated images by real images only. arXiv preprint arXiv:2311.00962, 2023. 6 [4] You-Ming Chang, Chen Yeh, Wei-Chen Chiu, and Ning Yu. Antifakeprompt: Prompt-tuned vision-language models are fake image detectors. arXiv preprint arXiv:2310.17419, 2023. 6 [5] Feiyi Chen, Yingying Zhang, Zhen Qin, Lunting Fan, Renhe Jiang, Yuxuan Liang, Qingsong Wen, and Shuiguang Deng. Learning multi-pattern normalities in the frequency domain for efficient time series anomaly detection. In 2024 IEEE 40th International Conference on Data Engineering (ICDE), pages 747760. IEEE, 2024. 2 [6] Yize Chen, Zhiyuan Yan, Siwei Lyu, and Baoyuan Wu. X2dfd: framework for explainable and extendable deepfake detection. arXiv preprint arXiv:2410.06126, 2024. 1, 2, 3 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2, 6, 7 [8] Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, and Lianwen Jin. M6doc: large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1513815147, 2023. 3 [9] Di Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly. As good as coin toss human detection of ai-generated images, videos, audio, and audiovisual stimuli. arXiv preprint arXiv:2403.16760, 2024. [10] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 6, 7 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 [12] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171, 2024. 3 [13] Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense?, 2024. 7 [14] Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Lips dont lie: generalisable and robust approach to face forgery detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50395049, 2021. 2 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [16] Zhenglin Huang, Jinwei Hu, Xiangtai Li, Yiwei He, Xingyu Zhao, Bei Peng, Baoyuan Wu, Xiaowei Huang, and Guangliang Cheng. Sida: Social media image deepfake detection, localization and explanation with large multimodal model. arXiv preprint arXiv:2412.04292, 2024. 1, 3, 6, [17] Zhengchao Huang, Bin Xia, Zicheng Lin, Zhun Mou, and Wenming Yang. Ffaa: Multimodal large language model based explainable open-world face forgery analysis assistant. arXiv preprint arXiv:2408.10072, 2024. 1, 2, 3, 4 [18] Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, and Siwei Lyu. Can chatgpt detect deepfakes? study of using multimodal large language models for media forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43244333, 2024. 1, 2, 3, 4 [19] Harry Jiang, Lauren Brown, Jessica Cheng, Mehtab Khan, Abhishek Gupta, Deja Workman, Alex Hanna, Johnathan Flowers, and Timnit Gebru. Ai art and its impact on artists. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, pages 363374, 2023. 1 [20] Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. Fusing global and local features for generalized ai-synthesized image detection. In 2022 IEEE International Conference on Image Processing (ICIP), pages 34653469. IEEE, 2022. 1, 6 [21] Fatima Khalid, Ali Javed, Hafsa Ilyas, Aun Irtaza, et al. Dfgnn: An interpretable and generalized graph neural network for deepfakes detection. Expert Systems with Applications, 222: 119843, 2023. 2 [22] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [23] HyunJae Lee, Hyo-Eun Kim, and Hyeonseob Nam. Srm : style-based recalibration module for convolutional neural networks, 2019. 8 [24] Jiawei Li, Fanrui Zhang, Jiaying Zhu, Esther Sun, Qiang Zhang, and Zheng-Jun Zha. Forgerygpt: Multimodal large language model for explainable image forgery detection and localization. arXiv preprint arXiv:2410.10238, 2024. 1, 2, 3 [25] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. Face x-ray for more general face forgery detection, 2020. 8 [26] Weijia Li, Jun He, Junyan Ye, Huaping Zhong, Zhimeng Zheng, Zilong Huang, Dahua Lin, and Conghui He. Crossviewdiff: cross-view diffusion model for satellite-tostreet view synthesis. arXiv preprint arXiv:2408.14765, 2024. 3 [27] Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi: Exposing ai created fake videos by detecting eye blinking. In WIFS, 2018. 2 [28] Yixuan Li, Xuelin Liu, Xiaoyang Wang, Shiqi Wang, and Weisi Lin. Fakebench: Uncover the achilles heels of fake images with large multimodal models. arXiv preprint arXiv:2404.13306, 2024. 2, [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 5 [30] Zhengzhe Liu et al. Global texture enhancement for fake face detection in the wild. In CVPR, pages 80608069, 2020. 6 [31] Mekhail Mustak, Joni Salminen, Matti Mäntymäki, Arafat Rahman, and Yogesh Dwivedi. Deepfakes: Deceptions, mitigations, and opportunities. Journal of Business Research, 154:113368, 2023. 1 [32] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2448024489, 2023. 1, 2, [33] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o, 2024. Accessed: 2024-8-18. 2, 6, 7 [34] Maria Pawelec. Deepfakes and democracy (theory): How synthetic audio-visual media for disinformation and hate speech threaten core democratic functions. Digital society, 1(2):19, 2022. 1 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [37] Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. FaceForensics++: Learning to detect manipulated facial images. In ICCV, 2019. 3, 6 [38] Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Gradcam: Visual explanations from deep networks via gradientbased localization. In Proceedings of the IEEE international conference on computer vision, pages 618626, 2017. 2 [39] Rui Shao, Tianxing Wu, and Ziwei Liu. Detecting and groundIn Proceedings of ing multi-modal media manipulation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69046913, 2023. 1, 2 [40] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. Detecting and grounding multi-modal media manipulation and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1, [41] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. 2 [42] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60486058, 2023. 1 [43] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International conference on machine learning, pages 33193328. PMLR, 2017. 2 [44] Bing Tu, Xianchang Yang, Wei He, Jun Li, and Antonio Plaza. Hyperspectral anomaly detection using reconstruction fusion of quaternion frequency domain analysis. IEEE Transactions on Neural Networks and Learning Systems, 2023. 2 [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 6, 7 [47] Peng Wang, Li Shen, Zerui Tao, Yan Sun, Guodong Zheng, and Dacheng Tao. unified analysis for finite weight averaging, 2024. [48] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86958704, 2020. 6 [49] Scott Workman, Richard Souvenir, and Nathan Jacobs. Widearea image geolocalization with aerial reference imagery. In IEEE International Conference on Computer Vision (ICCV), pages 19, 2015. Acceptance rate: 30.3%. 3 [50] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. 6 [51] Danni Xu, Shaojing Fan, and Mohan Kankanhalli. Combating misinformation in the era of generative ai models. In Proceedings of the 31st ACM International Conference on Multimedia, pages 92919298, 2023. 1 [52] Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, and Jian Zhang. Fakeshield: Explainable image forgery detection and localization via multi-modal large language models. arXiv preprint arXiv:2410.02761, 2024. 1, 2, 3, 4 [53] Beyazit Yalcinkaya, Niklas Lauffer, Marcell VazquezChanlatte, and Sanjit A. Seshia. Compositional automata embeddings for goal-conditioned reinforcement learning, 2025. [54] Akifumi Yamada, Tomohiro Shiraishi, Shuichi Nishino, Teruyuki Katsuoka, Kouichi Taji, and Ichiro Takeuchi. Time series anomaly detection in the frequency domain with statistical reliability, 2025. 2 [55] Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for ai-generated image detection. arXiv preprint arXiv:2406.19435, 2024. 3 [56] Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, and Baoyuan Wu. Deepfakebench: comprehensive benchmark of deepfake detection. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 1, 2 [57] Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Jinhua Yu, Haote Yang, and Conghui He. Skydiffusion: Street-to-satellite image synthesis with diffusion models and bev paradigm. arXiv preprint arXiv:2408.01812, 2024. 3 [58] Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. In ICLR, 2025. 2, 4, 6 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 1 [60] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 75797590, 2023. 1, [61] Yue Zhang, Ben Colman, Xiao Guo, Ali Shahriyari, and Gaurav Bharaj. Common sense reasoning for deepfake detection. In European Conference on Computer Vision, pages 399415. Springer, 2024. 1, 2, 3, 4, 6 [62] Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. Why are visually-grounded language models bad at image classification? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 4 [63] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun Xiong, and Wei Xia. Learning self-consistency for deepfake detection, 2021. 2 [64] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. Genimage: million-scale benchmark for detecting ai-generated image. NeurIPS, 36, 2024. 3 [65] Sijie Zhu, Taojiannan Yang, and Chen Chen. Vigor: Crossview image geo-localization beyond one-to-one retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36403649, 2021."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "Sun Yat-Sen University",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}