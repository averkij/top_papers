{
    "paper_title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
    "authors": [
        "Hui En Pang",
        "Shuai Liu",
        "Zhongang Cai",
        "Lei Yang",
        "Tianwei Zhang",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present \\textbf{Disco4D}, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. \\textbf{1)} Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. \\textbf{2)} It adopts diffusion models to enhance the 3D generation process, \\textit{e.g.}, modeling occluded parts not visible in the input image. \\textbf{3)} It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in \\url{https://disco-4d.github.io/}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 0 8 2 7 1 . 9 0 4 2 : r Disco4D: Disentangled 4D Human Generation and Animation from Single Image Hui En Pang1, Shuai Liu3, Zhongang Cai1,2,3, Lei Yang2,3, Tianwei Zhang1, Ziwei Liu1 1S-Lab, Nanyang Technological University 2SenseTime Research 3Shanghai AI Laboratory {huien001, tianwei.zhang, ziwei.liu}@ntu.edu.sg {caizhongang, yanglei}@sensetime.com {liushuai}@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "We present Disco4D, novel Gaussian Splatting framework for 4D human generation and animation from single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. 1) Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. 2) It adopts diffusion models to enhance the 3D generation process, e.g., modeling occluded parts not visible in the input image. 3) It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in https://disco-4d.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: Disco4D is novel Gaussian Splatting framework for 4D disentangled human generation, animation and editing from single image. The development of high-fidelity 3D digital humans is increasingly important across variety of augmented and virtual reality applications. To streamline the creation of these digital avatars from easily accessible in-the-wild images, multitude of research efforts have been made on reconstructing 3D clothed human models from single image [2, 81, 82, 33, 42, 117, 104, 103, 104, 40, 41]. These works predominantly focus on the simultaneous reconstruction of the human body and clothing. Unfortunately, integrating them into applications that require virtual try-on or avatar customization poses significant challenges. This is primarily because the models are rendered as single-layer, non-animatable meshes where distinct attributes (e.g., hair, clothing, accessories) are merged into one continuous surface, with underlying layers completely obscured and self-contact areas inseparably connected. Such limitation complicates the re-animation and dynamic customization tasks. To address these issues, we propose Disco4D, novel 4D clothed human reconstruction method that distinctly separates the human body from clothing elements. It supports human animation as the 4th dimension, which cannot be realized by prior static 3D reconstruction works [2, 81, 82, 33, 42, 117, 104, 103]. To achieve this, it employs the SMPL-X [71] parametric model to represent the human body, capitalizing on its efficacy in capturing body structure and kinematics. Conversely, clothing, along with dynamic and variable elements such as hair and accessories, is represented using Gaussian models, which are able to model the large variability in clothing. By binding Gaussians to SMPL-X model and fixing it during the training phase, Disco4D ensures the integrity of the body while focusing the learning process on the appearance aspects. To model occluded portions not visible in the input image, diffusion models are used to enhance the 3D generation process. Moreover, Disco4D includes an identity grouping mechanism for the Gaussians, which is instrumental in maintaining the separability and individuality of each clothing asset. (1) Enhanced The independent reconstruction of clothing and body offers several advantages. reconstruction fidelity. The SMPL-X body serves as stable anchor for the clothing to conform to. By isolating the focus to learn clothing gaussians, we achieve more refined geometry and intricate detailing in the clothed model. (2) Fine-grained categorization and extraction of clothing items. Disco4D is able to separate clothing Gaussians into their respective categories, which is crucial for the recovery and utilization of individual clothing assets. (3) Extensive editing capabilities. Disco4D supports different editing functions, including the removal of specific items, inpainting (altering color or material), and other modifications. Such rich editing options allow for precise adjustments to individual assets without inadvertently affecting adjacent elements. This level of control is particularly beneficial in applications requiring detailed customization, such as virtual fashion design and digital content creation. (4) Improved animation capabilities. The body Gaussians adhere to the deformations dictated by the SMPL-X model, while clothing Gaussians conform to the underlying body movements but also exhibit behaviors true to their material characteristics. The disentangled deformation allows for nuanced adjustments to clothing behavior in response to complex body movements, thereby elevating the quality of clothed human animation."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 3D Generation Table 1: Overview of 3D/4D generation methods from single image. Single-image 3D Generation. Single-image reconstruction leverages advanced methods [67, 45] to generate 3D assets in the form of 3D point clouds or NeRF [65] from one image. While earlier efforts using auto-encoders focused on synthetic objects [105, 14, 93, 21, 88, 12], newer approaches treat the task as conditional generation, employing diffusion models [35] for 3D generation from both image and text [35, 64, 91, 60, 19, 79, 74, 76]. One-2-3-45 [59] uses 2D diffusion models [60, 87] to generate multi-view images for reconstruction, while LRM [36] adopts transformer-based architecture to scale up the task on large datasets [19, 112]. Gaussian-based methods [47], particularly DreamGaussian [89] and LGM [90], offer efficient, highresolution 3D model generation from text or images. Method LGM [90] PiFU [81] DreamFusion [74] DreamGaussian [89] PiFU [81] D-IF [107] HiLo [108] ECON [104] SHERF [40] Disco4D Type General Human-centric General General Human-centric Human-centric Human-centric Human-centric Human-centric Human-centric Layered Animatable Recently, video diffusion models have attracted significant attention due to their remarkable ability to generate intricate scenes and complex dynamics with great spatio-temporal consistency [56, 7, 8, 9, 31, 116, 4]. They are employed to generate consistent multi-view images, and then reconstruct underlying 3D assets with high quality [15]. Single-image human-centric 3D Generation. Significant research efforts have been made for 3D human reconstruction, which can be classified into the following categories. (1) Explicit-shape-based methods rely on Human Mesh Recovery (HMR) using parametric models like SMPL [62] and SMPLX [71] to generate 3D body meshes [46, 51, 16, 44, 22, 49, 48, 24, 17, 80, 118, 66]. To account for 3D garments, several approaches incorporate offsets [119, 99] or templates, utilize deformable garment templates [43, 6], or employ non-parametric forms for clothed figures [27, 104, 115]. Despite their advancements, they face limitations in handling complex outfit variations and loose clothing due to inherent topological constraints. (2) Implicit-function-based methods utilize implicit representations like occupancy or distance fields for modeling clothed humans with complex geometries, such as loose garments. Techniques range from end-to-end regression of free-form implicit surfaces [2, 81, 82] to use of geometric priors [33, 42, 117, 104, 103] and implicit shape completion [104]. Notable works such as PIFu [81], ARCH(++) [33, 42], and PaMIR [117] can extract textured models from images, but struggle with depth ambiguities and texture inconsistencies. (3) NeRF-based methods incorporate model-based priors (i.e., SMPL-X) for accurate human reconstruction, with efforts like SHERF [40] and ELICIT [41] improving reconstruction coherence by addressing 2D observation incompleteness leveraging appearance priors. Most of these 3D clothed human reconstruction and animation works [2, 81, 82, 33, 42, 117, 104, 103] require training on human-specific datasets, which brings another limitation on the availability of such datasets. 3D Clothing Modeling. Reconstructing clothing from images and videos as separate layer over the human body poses significant challenges due to the diversity of clothing topologies. Previous efforts relied on either template meshes or implicit surface models, and required extensive, high-quality 3D data from simulations [5, 70, 83, 95] or tailored template meshes [13, 32, 73, 100]. New methods were developed [43, 34] for multi-clothing models and versatile template meshes, respectively, facilitating diverse clothing topology encoding. However, these techniques typically fall short in capturing the clothing texture and appearance. The reliance on predefined clothing style templates further constrains their ability to handle real-world clothing variations. Corona et al. [18] addressed these shortcomings by representing clothing layers with deep unsigned distance functions and an auto-decoder for style and cut differentiation, though this often produces overly-smooth reconstructions [18]. On the other hand, SCARF [25] and DELTA [26] significantly enhance the visual fidelity by applying NeRF to clothing layers, but require self-rotating video inputs and considerable processing times. 2.2 4D Animation 4D Animation. This task aims at capturing dynamic 3D scenes over time. Two primary approaches have emerged: modeling 4D scenes by adding time dimension or latent codes to spatial coordinates [98, 28, 57]; combining deformation fields with static 3D scenes [75, 58, 68, 69, 20, 92, 114]. Recent efforts in explicit or hybrid representations, like planar decomposition [11, 84, 85], hash representations [94], and other innovative methods [23, 1, 29], have improved reconstruction speed and quality. Gaussian Splatting, especially, stands out for balancing efficiency with quality, with dynamic 3D Gaussians [63] and 4D Gaussian Splatting [97, 110] techniques introducing timedependent deformations to enhance reconstructions. Notably, DreamGaussian4D [78] stands out by significantly reducing the optimization time while delivering high-quality 4D reconstructions. Human-centric 4D Animation. Recent works leverage Gaussian-based methods [50, 55, 61, 113, 77, 38, 54] for 4D human reconstruction, requiring extensive frame sequences (50-100 frames) and/or multiple viewpoints. Currently there has not been any work on 4D layered human generation and animation from single or few images, which will be achieved in this paper."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminary Table 2: Overview of 4D generation methods from video. Model unseen views Layered Method MonoHuman [113] 3DGS-Avatar [77] Gaussian-Avatar [38] GART [54] Disco4D 3D Gaussian Splatting employs explicit 3D Gaussian points as its primary rendering entities. 3D 2 (xµ)T Σ1(xµ), where µ and Σ are the spatial Gaussian point is defined as function G(x) = 1 mean and covariance matrix, respectively. Each Gaussian is also associated with its own rotation r, scaling s, opacity α, view-dependent color represented by spherical harmonic coefficients . SMPL-X parameterization [71] is an extension of the SMPL body model [62] with face and hand, designed to capture more accurate representation of intricate body movements. SMPL-X is defined as function (β, θ, ψ) : Rβθψ R3N , parametrized by the pose θ R3J (where denotes the number of body joints), face and hands shape β Rβ and facial expression ψ Rψ. 3 Figure 2: Overview of Disco4D. (a) 3D Generation utilizes single image to obtain disentangled body and clothing Gaussians. Body, face and hand poses are refined to be pixel-aligned. For faster initialization, clothing Gaussians and visual hull are obtained with Gaussian Reconstruction Models. These clothing Gaussians are embedded to SMPL-X mesh and adopt the local coordinate system of the triangle. Subsequently, the iterative optimization process (pruning, identity encoding and densifying) separates the body and garments. The learned identity encodings guide the densification of the clothing Gaussians. (b) 4D Animation are achieved by either direct driving of SMPL-X poses or leveraging video to learn extra clothing deformation. Given driving video, we first obtain static 3D Disentangled GS model. Body and clothing Gaussians are deformed by pose transformations. We then optimize deformation network to learn extra deformations for clothing GS at different timestamps. Various (c) 3D/4D Editing operations can be performed with our disentangled representation. 3.2 Overview Given single image, Disco4D generates animatable 3D clothed human avatars in bottom-up manner, facilitating natural separability. Our generated 3D clothed avatars, denoted as Shuman, are represented as the concatenation of Sbody and Scloth. Inspired by prior works [89, 90], capitalizes on Gaussian representations: = G(µ, r, s, α, c, e), (1) where µ, r, s, α, and denote positions, rotation, scaling, opacity, spherical harmonics coefficients and identity encoding, respectively. Different from traditional Gaussian representations, we add identity encoding to associate each Gaussian with its clothing category. Figure 2 depicts our framework. The initial step is to generate colored SMPL-X Gaussians that represent the body beneath clothing (Sec. 3.3). Then we obtain visual hull for canonicalization and refine the Gaussians predictions from Gaussian Reconstruction Models to ensure they are properly aligned with and envelop the SMPL-X mesh (Sec. 3.4). Next we iteratively optimize canonical clothing Gaussians external to the SMPL-X mesh (Sec. 3.5). Lastly, we showcase the animation and editing of generated clothed avatars (Sec. 3.6). Notably, we leverage diffusion models to refine textures during 3D generation (Sec. 3.5) and extrapolate unseen views during 4D animation (Sec. 3.6). 3.3 SMPL-X Gaussians Given an image, we first estimate coarse SMPL-X parameters with an off-the-shelf model [10], and then refine coarse predictions by fitting on 2D keypoints and clothing segmentation masks [72], obtaining pixel-aligned SMPL-X parameters (β, θ, ψ). Mesh Binding. To convert the SMPL-X [71] mesh representation (β, θ, ψ) into Gaussians Sbody for rendering, flat 3D Gaussians are bound directly on each mesh triangle face similar to SuGaR [30]. The Gaussians means µbody are explicitly computed using predefined barycentric coordinates in the corresponding triangles while the rotations of the Gaussians rbody are derived from the surface normals. Initial scaling of the Gaussians sbody is carefully selected to guarantee dense coverage of 4 the mesh surface, thereby preventing any gaps. specific scaling factor for the last axis is set to 0.1 to achieve thin flat surface throughout. In addition to structural representation, color representation beneath clothing is also consideration. This is achieved by setting the opacity value αbody to 1.0 and optimizing set of spherical harmonics cbody for each Gaussian. The skin color in visible regions are supervised, while the skin color of occluded body Gaussians are encouraged to be similar to non-occluded regions. We minimize the difference between the body colors in the occluded region and the average skin color of the visible regions. ebody is set to fixed categorical label for rendering and is not updated during training. Notably, during the optimization of clothing Gaussians Scloth, the parameters of SMPL-X Gaussians Sbody remain fixed. This preserves the integrity of the underlying body representation while allowing for flexible learning of clothing Gaussians. 3.4 Initialization of Clothing Gaussians Cloth styles are diverse, making proper initialization crucial for effective clothing modeling. In synchronization with estimating SMPL-X, we first employ the Video Diffusion Model [7] to estimate multi-view images. Subsequently, we leverage Gaussian Reconstruction Models [90] to obtain initial 3D Gaussians and their corresponding visual hull. Yet, the reconstructed 3D outputs often suffer from geometric inaccuracies, such as incorrect poses due to pose ambiguity or missing limbs. To address this, we refine the coarse visual hull to ensure it accurately aligns with and overlays the SMPL-X mesh and encapsulates good geometry for the clothed figure. With SMPL-X aligned visual hull, we derive the refined Gaussians by adopting properties from their nearest neighbors. The refined visual hull and Gaussians are then canonicalized for the optimization phase. Mesh embedding. Each 3D clothing Gaussian is embedded on one triangle of the canonical mesh. The embedding directly defines the position of the Gaussians in both the canonical and posed space. We set the mean position of the vertices as the origin of the local coordinate system. Given the vertices of triangle, we define the mean position of the vertices as the origin of the local coordinate system. We define the position of Gaussian µ = + by an offset vector from the local origin. The displacement vector in local coordinates is given by = σi + βj + γk, where σ, β, and γ are the components of the displacement vector along the (tangent), (bitangent), and (normal) respectively. Unlike SplattingAvatar [86], which uses displacement along the interpolated normal vector to position Gaussian within the triangle plane, our method allows for embedding Gaussians in the non-nearest but most suitable triangle. For instance, hair Gaussians are tagged to the faces in the head instead of the nearest face used in other reposing methods [86, 39]. When the mesh is deformed for animation, the embedding provides additional rotation (δr) and scaling (δs) upon each Gaussian. Specifically, the Gaussian adopts the rotation of the embedding triangle face while its scaling is based on the change in lengths of the embedded triangles, not just the area. The scaling adjustments for each Gaussian are dynamically computed based on individual face transformations, providing finer control over the deformation. During optimization, both the Gaussian parameters and embedding parameters are updated simultaneously. 3.5 Optimization of Separable Gaussians With the SMPL-X Gaussian and initialized clothing Gaussian, we aim to optimize canonical clothing Gaussians Scloth outside the SMPL-X mesh. This involves three steps: 1) we use Signed Distance Function (SDF) loss and pruning to discourage and remove Gaussians that reside within the body; 2) we introduce identity encoding to attach clothing label for each clothing Gaussian, by lifting multi-view 2D segmentations of the target object onto the 3D Gaussians; and 3) guided by ebody and ecloth, we selectively densify only the relevant clothing points while ignoring body points. Once the disentangled clothing is obtained, we use SDS loss to in-paint high-resolution texture from the reference image to individual clothing Gaussians, thereby enriching the details of unseen regions. SDF Loss and Pruning. In reality, the clothing is always external to the body. During refinement, we ensure that the clothing Gaussians are positioned externally to the SMPL-X mesh by applying the SDF loss and pruning strategy. Specifically, the SDF loss Lsdf penalizes any new densified Gaussians that intrude into the space of the SMPL-X mesh, ensuring that the clothing Gaussians consistently remain outside the bodys surface. Pruning is applied at fixed intervals to reinforce this separation, and systematically remove any Gaussians located within the SDF of the SMPL-X mesh. Identity encoding. To associate each Gaussian to its clothing category, we introduce Identity Encoding (e) as new parameter to each Gaussian. We adopt the clothing segmentation masks from 5 SegFormer [101] as supervision, with labels for different categories. ei for each Gaussian is learnable and compact vector of length 15, representing the remapped categories from the segmentation masks1. During training, similar to Spherical Harmonic (SH) coefficients on representing the color of each Gaussian, we render these encoded identity vectors into 2D images in differentiable manner following [111]. Given the rendered 2D features Eid, we take sof tmax(Eid) for identity classification, where is the total number of categories. We adopt standard cross-entropy loss L2d for (K+1)-category classification and an unsupervised 3D regularization loss L3d to enforce spatial consistency and proximity among the top k-nearest 3D Gaussians Identity Encodings. Consequently, the overall identity loss is Lid = L2d + L3d. Refer to Appendix 6.2 for more details. Densification of clothing Gaussians. To learn clothing more efficiently, we perform sampling for categorical Gaussians that belong to the same clothing category and embedding. We find the k-nearest Gaussian points for the resampled points and inherit their Gaussian properties (scaling, rotation, opacity, SH properties). By selectively densifying clothing Gaussians, we only add necessary Gaussians while ignoring body Gaussians. Anisotropy. To prevent overly-skinny kernels that point outward from the object surface under large deformations, we enforce the anisotropy of Gaussian kernels following [102]. During optimization, we employ Lani = 1 τ where sp is the scalings of 3D Gaussians. This loss essentially constrains that the ratio between the major and minor axis lengths does not exceed τ . min(sp) , τ pP max (cid:16) max(sp) (cid:80) (cid:17) Total loss. To inpaint occluded textures, we use the LSDS loss on the Gaussians in the canonical pose. This step follows the optimization of the front view for 500 training steps. Combined with the conventional 3D Gaussian Loss Lori on image rendering, the total loss for end-to-end training is: = Lori + Lid + Lani + Lsdf + LSDS (2) 4D Human Animation and Editing 3.6 Disco4Ds disentangled representation naturally supports animation and editing. The canonical Gaussians Sbody and Scloth enable separate deformations for clothing and body, ensuring realistic animation. Besides, individual clothing categories can be easily edited using image or text prompts. The learned clothing can be transferred to different body shapes and poses, for versatile customization. Animating Gaussians. As shown in Figure 2, Disco4D enables animation of the canonical human Gaussian via two methods. Firstly, Gaussians can be directly driven using 3D SMPL-X sequences obtained from motion database or estimated from 2D videos. Secondly, Disco4D enhances the model by learning detailed clothing dynamics from monocular videos. This disentanglement enables the focused modeling of clothing dynamics without altering the underlying human representation. To extend static 3D Gaussians into dynamic 4D Gaussians, deformation network is trained to predict changes in position, rotation, and scale of the reposed clothing Gaussians based on timestamp, as described in DreamGaussian4D [78]. Unlike [78], which learns deformations for all Gaussians, Disco4D models body Gaussians using the SMPL-X mesh, while clothing Gaussians employ posed transformations and learned deformations. The transformation is defined as = ϕ(S, t) where ϕ is the deformation network, is the spatial descriptions of the reposed 3D clothing Gaussian, is the timestamp, and is the spatial descriptions of the deformed and reposed 3D clothing Gaussians. Following [78], the deformation model is initialized to predict zero deformation at the start of training to avoid divergence between dynamic and static models. The weights and biases of the final prediction heads are initialized to zero, and skip connections are introduced to enable gradient backpropagation. To optimize the deformation field using the reference view video, we minimize the reconstruction loss LRef between the rendered image and video frame at each timestep. To propagate the motion from the reference view to the entire 3D model, we leverage Zero-1-to-3-XL [19] to predict the deformation of the unseen part to calculate LSDS. Despite per-frame predictions of image diffusion models, the fixed color and opacity of static 3D Gaussians help preserve temporal consistency. Editing Clothing Gaussians. We extract the Gaussians corresponding to the specific category and edit them. This allows fine-grained editing and ensures that other Gaussians are not affected. Instead of fine-tuning all 3D Gaussians, we freeze the properties for most of the well-trained Gaussians and only adjust small part of 3D Gaussians relevant to the target categories. For 3D object removal, we 1Categories: 0: \"Background\", 1: \"Hat\", 2: \"Hair\", 3: \"Sunglasses\", 4: \"Upper-clothes\", 5: \"Skirt\", 6: \"Pants\", 7: \"Dress\", 8: \"Belt\", 9: \"Left-shoe\", 10: \"Right-shoe\", 11: \"Face\", 12: \"Skin\", 13: \"Bag\", 14: \"Scarf\" simply delete the 3D Gaussians of the editing target. For 3D object colorization by in-painting or text guidance, we reinitialise the color and tune the color (SH) parameters of the corresponding Gaussian group, while fixing the 3D positions and other properties to preserve the learned 3D geometry."
        },
        {
            "title": "4 Experiments",
            "content": "Our detailed implementation and experiment setup can be found in Appendix 6.3. 4.1 3D Generation Table 3: CLIP-embedding loss for generated humans and segmented assets, and performance (PSNR, SSIM, LPIPS) comparisons for novel poses and views on the Synbody and CloSe datasets across DreamGaussian, LGM, SHERF, and Disco4D. Method SynBody CLIP Novel View CLIP CloSe Novel View Novel Pose All Pants Shirt Shoes PSNR SSIM LPIPS All Pants Shirt Shoes PSNR SSIM LPIPS PSNR SSIM LPIPS DreamGaussian [89] LGM [90] SHERF [40] Ours 0.751 0.807 0.766 0.851 0.715 0.724 0.649 0.784 0.710 0.747 0.636 0.753 0.749 0.760 0.714 0.801 13.118 12.884 15.189 15.691 0.883 0.876 0.852 0. 0.229 0.228 0.189 0.185 0.734 0.829 0.777 0.856 0.693 0.727 0.785 0.858 0.674 0.712 0.729 0.810 0.767 0.778 0.801 0.842 20.08 20.50 18.96 20. 0.939 0.939 0.912 0.918 0.089 0.077 0.083 0.081 - - 15.54 17.96 - - 0.844 0.851 - - 0.165 0.136 Figure 3: Qualitative comparison of Image generation across DreamGaussian, LGM, SHERF, and Disco4D. Generation and Disentanglement. Our generation and disentanglement results are presented in Figure 3 and Table 3. We assessed the disentanglement quality using the Synbody [109] and CloSe [3] datasets, rendering 30 and 110 clothed human meshes respectively from four angles and evaluating CLIP-similarity, PSNR, SSIM, and LPIPS for various poses and views within the CloSe dataset. Our method leverages on diffusion models without the need for training on human specific datasets. Therefore, we compare it with DreamGaussian [89] and LGM [90] that reconstruct 3D objects from diffusion models. Additionally, we conducted comparisons with SHERF, human-centric baseline for evaluating novel poses and views. We observe from Figure 3 that our method has higher fidelity and better geometry for body parts such as face and limbs due to the representation using SMPL-X Gaussians. We outperform DreamGaussian, LGM and SHERF on benchmarks for SynBody [109] and CloSe [3]. Editing. We can edit specific clothing appearance given an image or text prompt, repose the person and transfer person characteristics. The disentanglement allows fine-grained editing and modification of individual assets without affecting other assets, and stacking multiple edits (Figure 3). User study. We conducted user study to evaluate the generative quality of our image-to-3D Gaussians reconstruction on random in-the-wild images from SHHQ, detailed in Table 4. This study focuses on reference view consistency and overall generation quality, crucial aspects in image reconstruction tasks. We rendered 360degree rotation videos for 25 images generated by DreamGaussian, LGM, and our method. We Table 4: User study rates quality of generated 3D Gaussians from 1-5, the higher the better. DreamGaussian [89] LGM [90] Disco4D (Ours) Image Consistency Overall Quality 2.017 2.338 3.142 1.852 2.017 3.037 Metric 7 invited 43 volunteers to rate 2427 mixed samples from these methods on image consistency and overall model quality, yielding 1080 valid scores. As shown in Table 4, our method was preferred, demonstrating better alignment with the original image content and superior overall quality. 4. 4D animation Pose-Driven Animation. Our method generates canonical Gaussians that can be animated with any pose sequence. Figure 8 in the Appendix demonstrates our animation capabilities and compares them with current SOTA 2D animation methods. Using identical inputsa single frame and pose sequenceour approach more effectively preserves the body shape and fine details such as facial features and clothing. It surpasses Animate-Anyone [37] and Magic-Animate [106] in accurately modeling fine-grained body parts like hands and faces, and exhibits greater consistency compared to CHAMP [120]. Our methods disentanglement feature further allows for direct manipulation of Clothing Gaussians, as shown in Figure 6. Table 5: CLIP-embedding loss for generated humans and segmented assets, and performance (PSNR, SSIM, LPIPS) comparison on the 4D-Dress dataset across various video-to-4D methods. All Assets PSNR SSIM LPIPS DreamGaussian4D [78] MonoHuman [113] GART [54] GaussianAvatar [38] DreamGaussian4D (LGM init) DreamGaussian4D (Disco4D init) Disco4D (reposed) Disco4D (reposed) + learned deformations 0.784 0.762 0.800 0.822 0.809 0.870 0.853 0.900 0.769 0.743 0.772 0.768 0.795 0.849 0.774 0.865 20.54 20.22 18.81 20.01 19.16 21.02 23.94 25. 0.93 0.92 0.92 0.93 0.93 0.93 0.95 0.96 0.080 0.086 0.086 0.069 0.086 0.065 0.049 0.035 4D Reconstruction. For the 4D-Dress Dataset[96], we evaluated 8 sequences, assessing CLIP similarity scores against ground-truth meshes and disentangled assets, along with novel view performance (PSNR, SSIM, LPIPS) from four different viewpoints. The quantitative results of our generation approach are summarized in Table 5, where we benchmark our method against existing video-to-4D general GS approaches, such as DreamGaussian4D [78], as well as human-centric GS methods, including MonoHuman [113], GART [54], and GaussianAvatar [38]. Figure 4: Qualitative comparison of 4D generation between DreamGaussian4D, MonoHuman, GART, GaussianAvatar, and Disco4D. We outperform MonoHuman [113], GART [54], and GaussianAvatar [38]  (Table 5)  as these methods reconstruct using known video information, unable to model unseen regions. Consequently, these methods cannot accurately model back views from front-facing videos, leading to artifacts in other perspectives and canonical space (see Figure 4). In contrast, our method first performs reconstruction and subsequently incorporates details, such as clothing deformation, from the input frames, enabling consistent reconstruction even in unseen viewpoints. While DreamGaussian4D [78] is capable of modeling back-view information, the details remain coarse. Our results demonstrate that initializing with our model from the first frame (DreamGaussian4D Disco4D-init) significantly outperforms other initialization methods (DreamGaussian4DLGM init, DreamGaussian init) in both fidelity and geometry  (Table 5)  . Nevertheless, without incorporating human priors, DreamGaussian4D [78] still faces challenges, such as missing limbs and difficulty modeling fine details like facial features (see Figure 5). Figure 5: 4D reconstruction results on 4D-Dress Dataset. Reposing our canonical avatar enables us to align the body and assets accurately with the inferred postures from the source video, yielding high-quality reconstruction of faces, hands, and garments. Our reposed method surpasses DreamGaussian4D in geometry and fidelity by incorporating human priors. However, reposing alone cannot capture clothing dynamics. To address this, our disentangled approach models clothing deformations on the reposed Gaussians, guided by diffusion model. As demonstrated in Figure 5 and Table 5, this process enhances the accuracy of clothing resemblance to the ground truth. The combination of asset repositioning and learned deformations improves modeling quality, with repositioning handling pose-driven changes and learned deformations simulating dynamic asset movements as observed in the driving video. 4D Editing. For normal pipeline in character animation, editing the person in the video requires high consistency throughout all frames. For pose-driven animation methods, first frame editing and generation is required. Our method directly edits the gaussians, which is more straightforward, fine-grained and consistent. This is seen from Figure 6. Figure 6: First frame Editing and Animation. Betas Editing, Recoloring (Text/Image-guided), Composition (Removal, Swap). 9 4.3 Ablation Studies Initialization of clothing Gaussians. This process is crucial for high fidelity reconstruction. As shown in Figure 10 in the Appendix, we evaluate different strategies, including random, surface, and hull-based initialization. Hull-based initialization significantly enhances the model accuracy and realism over other methods. Initialization directly on the SMPL-X surface often leads to inaccurate geometries, particularly with complex or loose garments, creating elongated, thin Gaussians and visual artifacts. In contrast, hull-based initialization captures garment details more effectively and maintains pose consistency, closely aligning with the true geometry of the clothed body. Figure 7: Ablation of points geometry (left) and editing results (right). Points (\"All\") are visualised with Gaussian Scale of 0.1. Geometry of Clothing Gaussians. Figure 7 highlights the differences in clothing geometry between DreamGaussian [89], LGM [90] and our method. In DreamGaussian, all points are confined within the body geometry, whereas in LGM, about half of the points extend beyond the SMPL-X body. Removing internal points leaves sparse, translucent representations for clothing. This sparsity suggests reliance on internal points for visual representation, failing to accurately depict the objects geometry where appearance should primarily originate from surface points. Often, clothing Gaussian points are incorrectly positioned inside the bodys hull rather than on the surface. To better represent clothing geometry, our method positions all clothing Gaussians externally to the SMPL-X body mesh, accurately reflecting the garments actual physical characteristics (Figure 7). Clothing editing. Figure 7 shows our editing results with the prompt \"Color the top pink\". Disco4D allows for precise editing of the targeted clothing without affecting other areas."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose Disco4D, novel approach for the generation of 3D animatable clothed human Gaussians from single image, emphasizing high-fidelity detail and separation of assets. We manage to compositionally generate separate components, such as haircut, accessories, and decoupled outfits. Our core insight is the fixing of SMPL-X Gaussians, fitting segmented Gaussians over SMPL-X Gaussians, and application of diffusion models to enhance 3D reconstruction, including modeling occluded parts not visible in the input image. Its capability to separate assets offers significant advantages, including localized, fine-grained editing of individual assets and enhanced animatability. Limitations and Future Works. Despite achieving impressive results, some failure cases still exist, as shown in Figure 9 in the Appendix. Disco4D relies on robust and pixel-aligned SMPL-X estimation, which is still an unsolved problem. Disco4D occasionally fails for poor visual hull initialization. The extraction of mesh assets from clothing Gaussians using Local Density Query, as per DreamGaussian [89], currently loses fine-grained details. Enhancing the detail level of geometry derived from clothing Gaussians could bolster the utility of reconstructed assets in animation and simulation applications. Furthermore, the initialized visual hulls obtained from multi-view SMPL-X guided images are often of suboptimal quality and suffer from poor side and back views, necessitating refinement. Improving pose guidance models to achieve more accurate visual hulls could alleviate the need for extensive refinement. In addition, future works could look into modeling multi-layered clothing and reconstructing the occluded clothing. Current animation only supports small number of frames, future works could look into modelling long sequences. Disco4D has many positive applications. On the other hand, it has the potential to facilitate deepfake avatars and raise IP concerns. Regulations should be built to address these issues alongside its benefits in the entertainment industry."
        },
        {
            "title": "References",
            "content": "[1] Jad Abou-Chakra, Feras Dayoub, and Niko Sünderhauf. Particlenerf: Particle based encoding for online neural radiance fields. arXiv preprint arXiv:2211.04041, 2022. [2] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2022-June:14961505, 2022. ISSN 10636919. doi: 10.1109/CVPR52688.2022.00156. [3] Dimitrije Antic, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin, and Gerard Pons-Moll. CloSe: 3D clothing segmentation dataset and model. In International Conference on 3D Vision (3DV), March 2024. [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. [5] Hugo Bertiche, Meysam Madadi, and Sergio Escalera. Cloth3d: clothed 3d humans. In European Conference on Computer Vision, pp. 344359. Springer, 2020. [6] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multigarment net: Learning to dress 3d people from images. In IEEE International Conference on Computer Vision (ICCV). IEEE, oct 2019. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models, 2023. [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [10] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, et al. Smpler-x: Scaling up expressive human pose and shape estimation. Advances in Neural Information Processing Systems, 36, 2024. [11] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. CVPR, 2023. [12] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University Princeton University Toyota Technological Institute at Chicago, 2015. [13] Xin Chen, Anqi Pang, Yang Wei, Wang Peihao, Lan Xu, and Jingyi Yu. Tightcap: 3d human shape capture with clothing tightness field. ACM Transactions on Graphics (Presented at ACM SIGGRAPH), 2021. [14] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary space partitioning. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [15] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators, 2024. 11 [16] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee. Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from 2D Human Pose. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12352 LNCS:769787, 2020. ISSN 16113349. doi: 10.1007/ 978-3-030-58571-6_45. [17] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. In European Conference on Computer Vision (ECCV), 2020. URL https://expose.is.tue.mpg.de. [18] Enric Corona, Albert Pumarola, Guillem Alenyà, Gerard Pons-Moll, and Francesc MorenoNoguer. Smplicit: Topology-aware generative model for clothed people. In CVPR, 2021. [19] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. [20] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [21] Shivam Duggal and Deepak Pathak. Topologically-aware deformation fields for single-view 3d reconstruction. CVPR, 2022. [22] Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Kocabas, and Michael J. Black. Learning In IEEE/CVF ISBN to Regress Bodies from Images using Differentiable Semantic Rendering. International Conference on Computer Vision (ICCV), pp. 1123011239, 2021. 9781665428125. doi: 10.1109/iccv48922.2021.01106. [23] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers, SA 22. ACM, November 2022. doi: 10.1145/ 3550469.3555383. URL http://dx.doi.org/10.1145/3550469.3555383. [24] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Collaborative regression of expressive bodies using moderation. In International Conference on 3D Vision (3DV), 2021. [25] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black, and Timo Bolkart. Capturing and animation of body and clothing from monocular video. In SIGGRAPH Asia 2022 Conference Papers, SA 22, 2022. [26] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc Pollefeys, and Michael J. Black. Learning disentangled avatars with hybrid 3d representations. arXiv, 2023. [27] Valentin Gabeur, Jean-Sebastien Franco, Xavier Martin, Cordelia Schmid, and Gregory Rogez. Moulding humans: Non-parametric 3d human shape estimation from single images, 2019. [28] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In Proceedings of the IEEE International Conference on Computer Vision, 2021. [29] Shanyan Guan, Huayu Deng, Yunbo Wang, and Xiaokang Yang. Neurofluid: Fluid dynamics grounding with particle-driven neural radiance fields, 2022. [30] Antoine Guédon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. arXiv preprint arXiv:2311.12775, 2023. [31] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2023. 12 [32] Oshri Halimi, Fabian Prada, Tuur Stuyck, Donglai Xiang, Timur Bagautdinov, He Wen, Ron Kimmel, Takaaki Shiratori, Chenglei Wu, and Yaser Sheikh. Garment avatars: Realistic cloth driving using pattern registration, 2022. [33] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. ARCH++: AnimationReady Clothed Human Reconstruction Revisited. Proceedings of the IEEE International Conference on Computer Vision, pp. 1102611036, 2021. ISSN 15505499. doi: 10.1109/ ICCV48922.2021.01086. [34] Zhu Heming, Cao Yu, Jin Hang, Chen Weikai, Du Dong, Wang Zhangye, Cui Shuguang, and Han Xiaoguang. Deep fashion3d: dataset and benchmark for 3d garment reconstruction from single images. In Computer Vision ECCV 2020, pp. 512530. Springer International Publishing, 2020. ISBN 978-3-030-58452-8. [35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. [36] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [37] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. [38] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [39] Shoukang Hu and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. arXiv preprint arXiv:, 2023. [40] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from single image. arXiv preprint arXiv:2303.12791, 2023. [41] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, and Deng Cai. One-shot implicit animatable avatars with model-based priors. In IEEE Conference on Computer Vision (ICCV), 2023. [42] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. ARCH: Animatable Reconstruction of Clothed Humans. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 30903099, 2020. ISSN 10636919. doi: 10.1109/CVPR42600.2020.00316. [43] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu, and Hujun Bao. Bcnet: Learning body and cloth shape from single image. In European Conference on Computer Vision. Springer, 2020. [44] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar Fine-Tuning for 3D Human Model Fitting Towards In-the-Wild 3D Human Pose Estimation. Proceedings - 2021 International Conference on 3D Vision, 3DV 2021, pp. 4252, 2021. doi: 10.1109/3DV53792.2021. 00015. [45] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions, 2023. [46] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-End Recovery of Human Shape and Pose. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 71227131, 2018. ISBN 9781538664209. doi: 10.1109/CVPR.2018.00744. [47] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. [48] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black. PARE: Part attention regressor for 3D human body estimation. In Proc. International Conference on Computer Vision (ICCV), pp. 1112711137, October 2021. [49] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch, Lea Müller, Otmar Hilliges, and In Proc. Michael J. Black. SPEC: Seeing people in the wild with an estimated camera. International Conference on Computer Vision (ICCV), pp. 1103511045, October 2021. [50] Muhammed Kocabas, Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats, 2023. URL https://arxiv.org/abs/2311.17910. [51] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3D human pose and shape via model-fitting in the loop. Proceedings of the IEEE International Conference on Computer Vision, 2019-Octob:22522261, 2019. ISSN 15505499. doi: 10.1109/ICCV.2019.00234. [52] Georgios Kopanas, Julien Philip, Thomas Leimkühler, and George Drettakis. Point-based neural rendering with per-view optimization. Computer Graphics Forum (Proceedings of the Eurographics Symposium on Rendering), 40(4), June 2021. URL http://www-sop.inria. fr/reves/Basilic/2021/KPLD21. [53] Georgios Kopanas, Thomas Leimkühler, Gilles Rainer, Clément Jambon, and George Drettakis. Neural point catacaustics for novel-view synthesis of reflections. ACM Transactions on Graphics, 41(6):Article201, 2022. [54] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models, 2023. URL https://arxiv.org/abs/2311.16099. [55] Mengtian Li, Shengxiang Yao, Zhifeng Xie, and Keyu Chen. Gaussianbody: Clothed human reconstruction via 3d gaussian splatting, 2024. [56] Xuanyi Li, Daquan Zhou, Chenxu Zhang, Shaodong Wei, Qibin Hou, and Ming-Ming Cheng. Sora generates videos with stunning geometrical consistency. arXiv preprint arXiv: 2402.17403, 2024. [57] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [58] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [59] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928, 2023. [60] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. [61] Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, and Haoqian Wang. Animatable 3d gaussian: Fast and high-quality reconstruction of multiple human avatars. arXiv preprint arXiv:2311.16482, 2023. [62] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, October 2015. [63] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. [64] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 84468455, 2023. [65] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [66] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Accurate 3d hand pose estimation for whole-body 3d human mesh estimation. In Computer Vision and Pattern Recognition Workshop (CVPRW), 2022. [67] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts, 2022. [68] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. ICCV, 2021. [69] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), dec 2021. [70] Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. Tailornet: Predicting clothing in 3d as function of human pose, shape and garment style. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, jun 2020. [71] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. Osman, DImitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 2019-June, pp. 1096710977, 2019. ISBN 9781728132938. doi: 10.1109/CVPR.2019.01123. [72] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1097510985, 2019. [73] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael Black. Clothcap: Seamless 4d clothing capture and retargeting. ACM Transactions on Graphics, (Proc. SIGGRAPH), 36 (4), 2017. URL http://dx.doi.org/10.1145/3072959.3073711. Two first authors contributed equally. [74] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. [75] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. [76] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, HsinYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. In The Twelfth International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=0jHkUDyEO9. [77] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting. 2024. [78] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [79] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 15 [80] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. FrankMocap: Monocular 3D WholeBody Pose Estimation System via Regression and Integration. In Proceedings of the IEEE International Conference on Computer Vision, volume 2021-Octob, pp. 17491759, 2021. ISBN 9781665401913. doi: 10.1109/ICCVW54120.2021.00201. [81] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In The IEEE International Conference on Computer Vision (ICCV), October 2019. [82] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixelaligned implicit function for high-resolution 3d human digitization. In CVPR, 2020. [83] Igor Santesteban, Miguel A. Otaduy, and Dan Casas. Learning-Based Animation of Clothing for Virtual Try-On. Computer Graphics Forum (Proc. Eurographics), 2019. ISSN 1467-8659. doi: 10.1111/cgf.13643. [84] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. [85] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. [86] Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, and Zeyu Wang. SplattingAvatar: Realistic Real-Time Human Avatars with MeshEmbedded Gaussian Splatting. In Computer Vision and Pattern Recognition (CVPR), 2024. [87] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. [88] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction, 2023. [89] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [90] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. [91] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2281922829, October 2023. [92] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video. In IEEE International Conference on Computer Vision (ICCV). IEEE, 2021. [93] Alex Trevithick and Bo Yang. Grf: Learning general radiance field for 3d scene representation and rendering. In arXiv:2010.04595, 2020. [94] Haithem Turki, Jason Y. Zhang, Francesco Ferroni, and Deva Ramanan. Suds: Scalable urban dynamic scenes, 2023. [95] Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On. Computer Graphics Forum (Proc. SCA), 2020. 16 [96] Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, and Otmar Hilliges. 4d-dress: 4d dataset of real-world human clothing with semantic annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [97] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. [98] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 94219431, 2021. [99] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica Hodgins. Monoclothcap: Towards temporally coherent clothing capture from monocular rgb video. In Proceedings of International Conference on 3D Vision (3DV 20), pp. 322 332, November 2020. [100] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. Modeling clothing as separate layer for an animatable human avatar. ACM Transactions on Graphics, 40(6):115, December 2021. ISSN 1557-7368. doi: 10.1145/3478513.3480545. URL http://dx.doi.org/10.1145/3478513.3480545. [101] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. CoRR, abs/2105.15203, 2021. URL https://arxiv.org/abs/2105.15203. [102] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023. [103] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1329613306, June 2022. [104] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans Optimized via Normal integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. [105] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 492502. 2019. [106] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. 2023. [107] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, and Zhaoxin Fan. D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field. In Proceedings of the IEEE/CVF international conference on computer vision, 2023. [108] Yifan Yang, Dong Liu, Shuhai Zhang, Zeshuai Deng, Zixiong Huang, and Mingkui Tan. Hilo: Detailed and robust 3d clothed human reconstruction with high-and low-frequency information of parametric models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1067110681, 2024. [109] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, Wayne Wu, Chen Qian, Dahua Lin, Ziwei Liu, and Lei Yang. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2028220292, October 2023. 17 [110] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. [111] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. arXiv preprint arXiv:2312.00732, 2023. [112] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Mvimgnet: large-scale dataset of multi-view images. In CVPR, 2023. [113] Zhengming Yu, Wei Cheng, Xian Liu, Wayne Wu, and Kwan-Yee Lin. Monohuman: Animatable human neural field from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1694316953, 2023. [114] Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven Lovegrove. Star: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1314413152, 2021. [115] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor Lempitsky. Point-based modeling of human clothing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1471814727, October 2021. [116] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. arXiv:2311.10982, 2023. [117] Zheng Zerong, Yu Tao, Liu Yebin, and Dai Qionghai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction, 2021. [118] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [119] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. Detailed human shape estimation from single image by hierarchical mesh deformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 44914500, 2019. [120] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance, 2024."
        },
        {
            "title": "6 Appendix",
            "content": "6.1 Preliminary 3D Gaussian Splatting utilizes explicit 3D Gaussian points as the core elements for rendering. Each 3D Gaussian point is defined by the function: G(x) = 1 2 (xµ)T Σ1(xµ), where µ represents the spatial mean, and Σ denotes the covariance matrix. Additionally, each Gaussian is assigned an opacity value α and view-dependent color c, parameterized by spherical harmonic coefficients . During rendering, these 3D Gaussians are projected onto the 2D view plane via splatting technique. The 2D projection is computed using the projection matrix, while the 2D covariance matrices are approximated as: Σ = JgWgΣW , where Wg is the viewing transformation, and Jg is the Jacobian of the affine approximation for perspective projection. The final pixel color is obtained through alpha-blending of layered 2D Gaussians from front to back = (cid:80) iN Tiαici, with Ti = (cid:81)i j=1(1 αj). The opacity α is determined by multiplying γ with the contribution of the 2D covariance, derived from Σ and the pixel coordinate in image space. The covariance matrix Σ is parameterized using quaternion and 3D scaling vector to aid in optimization. SMPL-X parameterization [71] extends the original SMPL body model [62] by incorporating detailed face and hand deformations to capture more expressive human movements. SMPL-X expands SMPL joint set by including additional joints for facial features, toes and fingers, enabling more accurate representation of complex body movements. SMPL-X is defined by function (β, θ, ψ) : Rβθψ R3N , where θ R3K represents the pose (with being the number of body joints), β Rβ represents body shape, and ψ Rψ captures facial expressions. Further details can be found in [71]. 6.2 Training details of Identity Encoding loss To optimize the introduced Identity Encoding of each Gaussian, we render these encoded identity vectors into 2D images in differentiable manner following [111]. We adapt the differentiable 3D Gaussian renderer from [47], approaching the rendering process similarly to the color optimization using spherical harmonic (SH) coefficients, as described in [47]. In this method, 3D Gaussian splatting utilizes neural point-based α-rendering [52, 53], where the influence weight α is calculated in 2D for each Gaussian and pixel. Following the approach in [47], the influence of all Gaussians on pixel is computed by sorting them based on depth and blending the ordered Gaussians that overlap with that pixel: Eid = (cid:88) iN i1 (cid:89) eiαi (1 α j) j=1 (3) Here, the rendered 2D mask identity feature Eid is the sum of the Identity Encoding ei (of length 15) for each Gaussian, weighted by the Gaussians influence factor α on that pixel. The value of α is determined by evaluating 2D Gaussian with covariance Σ2D, which is scaled by learned per-point opacity αi: Σ2D = JW Σ2D3DW (4) where Σ3D is the 3D covariance matrix, Σ2D represents the splatted 2D counterpart, is the Jacobian of the affine approximation for the 3D-to-2D projection, and is the world-to-camera transformation matrix. To ensure consistency in the Identity Encoding ei during training, we apply an unsupervised 3D regularization loss. This loss encourages the Identity Encodings of the top k-nearest 3D Gaussians to remain close in feature space, promoting spatial consistency. Using the softmax function , we define the KL divergence loss with sampled points as follows: L3d = 1 (cid:88) j=1 DKL(P Q) = 1 mk (cid:88) (cid:88) j=1 i=1 (ej) log (cid:19) (cid:18) (ej) (ej) (5) 19 Here, is the sampled Identity Encoding of 3D Gaussian, and consists of the k-nearest neighbors in 3D space, represented as k. The total identity encoding loss is then defined as: 2, ..., 1, Lid = L2d + L3d (6) 6.3 Implementation details The 3D generation experiments were conducted using single 24GB RTX3090 GPU, while the 4D generation experiments utilized single 48GB RTX6000 GPU. For the 3D generation process, the SMPL-X fitting was performed with 3000 iterations in 3 minutes, followed by skin color inpainting on SMPL-X Gaussians for 100 iterations in 30 seconds. Reconstruction and disentanglement optimization required 3000 iterations, completed in 12 minutes. In video reconstruction, SMPL-X fitting aligned 14 frames in 6 minutes for in-the-wild videos. The 4D-Dress [96] experiments involved 1000 iterations for clothing deformation over 18 minutes. 6.4 Visual comparisons to 2D animation methods. Figure 8: Comparison to 2D animation methods. Compared to Magic-Animate and Animate-Anyone, we have better preservation of body shape and details. Compared to CHAMP, we have better geometry and consistency. 20 6.5 Failure cases Figure 9: Failure cases of Disco4D. (a) Poor SMPL-X estimation (b) Poor visual hull initialization (c) Misclassification of clothing categories. Disco4D relies on robust and pixel-aligned SMPL-X estimation, which is still an unsolved problem, especially for challenging poses. In Figure 9a, it is difficult to correct the pose with keypoints and segmentation mask due to depth ambiguity. Disco4D occasionally fails for poor visual hull initialization (9b), which is common for difficult poses. Lastly, poor disentanglement is common problem due to misclassification of clothing category by the segmentation model. This is seen in Figure 9c where the arms are wrongly classified under the \"top\" category. 6.6 Initialization Figure 10: Ablation of initialization. (a) Random Initialization (b) SMPL-X Initialization (c) Visual Hull Initialization."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Shanghai AI Laboratory"
    ]
}