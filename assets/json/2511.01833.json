{
    "paper_title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning",
    "authors": [
        "Ming Li",
        "Jike Zhong",
        "Shitian Zhao",
        "Haoquan Zhang",
        "Shaoheng Lin",
        "Yuxiang Lai",
        "Chen Wei",
        "Konstantinos Psounis",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 2 3 3 8 1 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TIR-BENCH: COMPREHENSIVE BENCHMARK FOR AGENTIC THINKING-WITH-IMAGES REASONING Ming Li1 Jike Zhong2 Shitian Zhao1 Haoquan Zhang1, 4 Shaoheng Lin1 Yuxiang Lai3 Chen Wei5 Konstantinos Psounis2 Kaipeng Zhang1 1Shanghai AI Laboratory 2University of Southern California 3 Emory University 4 Chinese University of Hong Kong 5 Rice University lm1640362161@gmail.com, zhangkaipeng@pjlab.org.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-with-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-with-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce TIR-Bench, comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tooluse augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present pilot study comparing direct versus agentic fine-tuning."
        },
        {
            "title": "INTRODUCTION",
            "content": "The reasoning abilities of recent multimodal large language models (MLLMs) (Hurst & OpenAI, 2024; Gemini Team, 2025) have advanced significantly, driven in large part by reasoning techniques such as chain-of-thought (CoT) (Wei et al., 2022). By decomposing reasoning of complex visual questions into series of textual steps, MLLMs are able to achieve improved performance. While promising, these techniques are confined to the textual domain, conducting their reasoning solely through language while treating the visual information as static, unalterable input (Su et al., 2025b). To effectively process visual information, thinking-with-images has been proposed (OpenAI, 2025c; Su et al., 2025b; Hu et al., 2024). This approach enables model to generate new visual information by actively manipulating input images with tools. For example, when faced with complex visual problem, OpenAIs o3 model (OpenAI, 2025c) first writes code to create an image-processing tool, then executes it to modify the image (e.g., cropping, flipping, or rotating). The transformed visual data then informs the next stage of its linguistic reasoning. To assess the agentic thinking-with-images capabilities of MLLMs, existing benchmarks (Wu & Xie, 2024; Wang et al., 2025) have largely centered on visual search and tasks requiring the analysis of high-resolution images. These evaluations primarily validate models ability to accurately localize and crop specific regions within an image for better capturing detailed information to answer corresponding questions. However, these assessments tend to focus narrowly on the visual search Equal contribution. Corresponding Author: zhangkaipeng@pjlab.org.cn"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of performance of open models (left), proprietary models (middle), and agentic tool-using models (right). SR: Symbolic Reasoning, WS: Word Search, LL-VQA: Low-Light VQA, IR: Instrument Reasoning, SD: Spot Difference, JG: Jigsaw Game, VS: Visual Search, RG: Rotation Game, Pro.: Proportion VQA. o3-TU: o3-tool-using, i.e., o3 with code interpreter. capabilities of agentic MLLMs, leaving broader spectrum of thinking-with-images abilities unevaluated. Therefore, there is an urgent need for benchmark that incorporates diverse range of tasks requiring sophisticated tool use to properly assess integrated multimodal reasoning. In this paper, we introduce TIR-Bench, comprehensive benchmark designed to evaluate the diverse thinking-with-images capabilities of agentic MLLMs. Unlike previous benchmarks focusing solely on visual search problems, TIR-Bench incorporates diverse set of 13 tasks that require wide range of tool-based interactions, such as zooming, rotating, increasing image contrast, adding auxiliary lines, and others to assess models tool integrated reasoning capabilities. The design of each task is predicated on the human intuition that solving it requires actively manipulating the visual input, rather than relying on static observation alone. For example, in TIR-Bench, math problem might require the model to draw auxiliary lines or coordinate system to find solution, while jigsaw puzzle task demands that it segment and then reassemble the image pieces. Consequently, TIRBench enables more holistic evaluation of models thinking-with-images abilities, assessing spectrum of skills not limited to visual search. Using TIR-Bench, we conduct comprehensive performance evaluation of 22 leading MLLMs across three categories: open-source models, proprietary models, and tool-using agents. The overall experimental results, illustrated in Figure 1, reveal that TIR-Bench is challenging benchmark for thinking-with-images abilities, as the best performance achieved is only 46%. Moreover, traditional non-agentic models perform poorly on TIR-Bench, with the best-performing model Gemini-2.5-pro reaching an accuracy of merely 28.9%. These findings highlight the importance of the thinkingwith-images ability for this benchmark, as models equipped with tool-use capabilities, such as o3, o4-mini, and PyVision (Zhao et al., 2025), achieve much higher performance than other models. Lastly, we assess the function-calling proficiency of various MLLMs and conduct pilot study contrasting direct supervised fine-tuning (SFT) with an agentic SFT approach on rotated image OCR task. Many recent MLLMs are equipped with function-calling capabilities. Our evaluation on rotation game task of TIR-Bench measures models proficiency in accurately executing the correct tool parameters as part of its reasoning chain. Results show that recent models like o3 perform well, whereas earlier models such as GPT-4o perform significantly worse. The pilot study on the rotated image OCR task compares two training methodologies and examines whether end-to-end SFT can achieve strong performance across different data scales for tasks involving image operations. Our findings indicate that the agentic SFT on full problem-solving trajectories with generated images is significantly more effective than direct SFT. This implies that agentic fine-tuning enables the emergence of more complex and robust problem-solving behaviors, allowing models to tackle multistep tasks that are intractable with direct fine-tuning alone."
        },
        {
            "title": "2.1 MULTIMODAL BENCHMARKS",
            "content": "As the capabilities of Multimodal Large Language Models (MLLMs) evolve rapidly, variety of benchmarks have been proposed to evaluate their performance, identify limitations, and guide future improvements (Lu et al., 2023; Zhang et al., 2024; Qiao et al., 2024; Wang et al., 2024a;b; Li et al., 2024c; Lu et al., 2021; Li et al., 2025). These benchmarks are typically either specialized for specific domains (Wang et al., 2024b; Li et al., 2024c; Lu et al., 2021; Li et al., 2025) or designed to be versatile and cover broad range of tasks (Li et al., 2024b; Liu et al., 2023; Yue et al., 2024; Wang et al., 2023). MLLMs often use CoT reasoning on these benchmarks, though solving them only requires static image information. More recently, benchmarks such as V* Bench and HR-Bench have been introduced to evaluate agentic capabilities, specifically for visual search in high-resolution images (Wu & Xie, 2024; Wang et al., 2025). While these benchmarks advance agentic evaluation by requiring models to programmatically crop high-resolution images, their focus is narrowly confined to visual search. Consequently, broader spectrum of thinking-with-images abilities, such as rotation and drawing, remains largely underexplored. 2.2 THINK WITH IMAGES In previous works, Visual Sketchpad (Hu et al., 2024), CoGCoM (Qi et al., 2025), DeepEyes (Zheng et al., 2025), Pixel Reasoner (Su et al., 2025a), OpenThinkIMG (Su et al., 2025b), Chain-ofFocus (Zhang et al., 2025a), Mini-o3 (Lai et al., 2025) and REVPT (Zhou et al., 2025) generate and executes tool calling in predefined visual-specific toolset. In the intermediate reasoning steps, processed images are reinjected to the context, resulting in multi-modal rational. However, these methods rely on fixed collection of external visual parserssuch as detection models (e.g., GroundingDINO (Liu et al., 2024b)) and segmentation models (e.g., SAM (Kirillov et al., 2023a))which constrains their generality across diverse vision tasks and introduces bottlenecks due to dependency on external models. In contrast, ViperGPT (Surıs et al., 2023), o3 (OpenAI, 2025a), o4-mini, Thyme (Zhang et al., 2025b) and PyVision (Zhao et al., 2025) adopt Python as its primitive tool. Capitalizing on the advanced coding and multimodal understanding capabilities of modern MLLMssuch as Claude-4.0 (Anthropic, 2025) and GPT-4.1 (OpenAI, 2025b), enables the MLLM to dynamically write and execute code to construct complex, task-specific tools on demand, thereby supporting more general and flexible reasoning. This aligns with the emerging paradigm of thinking with images highlighted in o3s blog (OpenAI, 2025c) as powerful cognitive capability. To assess this important ability, we propose TIR-Bench, covering diverse tasks on which multi-modal rationals are necessary."
        },
        {
            "title": "3 TIR-BENCH",
            "content": "In this section, we introduce TIR-Bench. The overview of the benchmark is shown in Figure 2. We first introduce the task design strategy in subsection 3.1. Next, we introduce the data collection process in subsection 3.2. Finally, we present the benchmark summary in subsection 3.3. 3.1 TASK DESIGN To extensively validate the models ability to think with images we design 13 tasks. The benchmarks tasks are designed to evaluate models ability to perform active, tool-based visual reasoning, moving far beyond static image analysis. This includes tasks that require programmatic analysis, such as calculating color proportions or calling external models for object segmentation. Other challenges test the models ability to overcome suboptimal conditions by programmatically enhancing lowlight images or correcting the orientation of rotated text before performing OCR. The benchmark also features complex spatial and algorithmic puzzles, requiring models to solve mazes, reassemble jigsaw pieces, or draw auxiliary lines to solve geometric problems. Finally, it assesses fine-grained perception through tasks like spotting differences between images, reading instruments via cropping and zooming, and locating anomalies in visual puzzles. In every case, the model is forced to engage in dynamic, multi-step process of visual manipulation and reasoning to arrive at the correct answer. More details about task design are introduced in Appendix B."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Benchmark Overview. TIR-Bench is composed of 13 tasks, meticulously designed to evaluate wide spectrum of thinking-with-images capabilities. 3.2 DATA COLLECTION Guidelines. As previously mentioned, current benchmarks focus almost exclusively on visual search, overlooking wide range of other tool-using abilities. To bridge this evaluation gap, we designed TIR-Bench in accordance with the following guidelines: (1) Diverse, Application-Grounded Tasks: TIR-Bench covers 13 distinct tasks across multiple domains, including spatial reasoning, visual perception, and mathematics, to mirror the complexity of real-world applications where static image analysis is insufficient. (2) Comprehensive Skill Assessment: The benchmark incorporates diverse visual contexts and requires range of programmatic skillsfrom drawing auxiliary lines in geometry to executing pixel-level analysisto foster well-rounded evaluation of models agentic capabilities. (3) Probing Model Limitations: Each task is designed to be unsolvable without multi-step, tool-based strategy. This intentional difficulty probes the limitations of current models, effectively distinguishing true thinking-with-images reasoning from simpler visual recognition. (4) Deterministic Evaluation: all tasks are designed with objectively verifiable answers, providing robust framework for deterministic and reproducible evaluations. (5) Many samples in TIR-Bench are newly annotated or generated, making it more reliable benchmark that minimizes the risk of data contamination from models pre-training corpora. Collection. We briefly introduce the data collection process of the 13 tasks here, while the detailed process can be found in Appendix B. The summary of data collection is shown in Table 1. (1). Collection of Math VQA, Symbolic Reasoning, Low-Light VQA, and Instrument Reading tasks: Data from these tasks are newly created and annotated data. We tasked two Ph.D. students with sourcing free images from the Internet and MathVista (Lu et al., 2023) or create images using"
        },
        {
            "title": "Preprint",
            "content": "Attribute Color VQA Proportion VQA Symbolic Reasoning Math Word Search Rotated OCR Image Source ColorBench RefCOCO VlmsAreBlind + Web-sourced Web-sourced Web-sourced OCRBench QA Construction Human-annotated Synthetic Human-annotated Human-annotated Synthetic Dataset-provided Samples 100 120 50 120 100 Answer Type Single-choice Single-choice Single-choice Single-choice Open-ended Open-ended Attribute Maze Low-Light Instrument Reading Spot the Difference Jigsaw Visual Search Rotation Image Source Synthetic Web-sourced Web-sourced Web-sourced Synthetic Web-sourced CVBench QA Construction Synthetic Human-annotated Human-annotated Human-annotated Synthetic Synthetic Synthetic Samples 120 50 80 100 120 75 Answer Type Single-choice Open-ended Open-ended Open-ended Open-ended Single-choice Single-choice Table 1: Overview of all tasks, grouped into reasoning-oriented (top) and perception-oriented (bottom), with their sources, QA constructions, sample sizes, and answer types. Geogebra, and then creating corresponding question-answer pairs. The annotators were explicitly instructed to design problems that, in their judgment, would necessitate tool-based image manipulation for solution, ensuring the tasks could not be solved by static observation alone. (2). Collection of Color VQA task: We tasked Ph.D. student with curating samples from the ColorBench (Liang et al., 2025), specifically selecting instances that could not be solved by static observation alone and therefore necessitate programmatic analysis to answer correctly. Finally, we select 100 problems from ColorBench (Liang et al., 2025). (3). Collection of Jigsaw, Maze, Rotation, Word Search Tasks: The data for these tasks were programmatically generated to create controlled and scalable challenges. For the Jigsaw Puzzle, we selected 120 images from the RefCOCO dataset, chosen for their prominent objects. These images were then segmented into grids ranging from 33 to 66 and shuffled to create puzzles of varying difficulty. For the Maze task, we programmatically generated 100 mazes with sizes scaling from 55 to 6262. For the Rotation Game, we utilized 75 images from CVBench (Tong et al., 2024), to which we applied random rotations. Three difficulty tiers were established based on the magnitude of the rotation angle (e.g., 5, 10, or 15 degrees). Finally, for the Word Search task, we programmatically generated 85 samples of different sizes and supplemented them with 15 complex puzzles sourced from the internet. (4). Collection of Spot the Difference: We sourced pairs of nearly identical cartoon and real-world images from the internet, with one image in each pair containing subtle alterations. Both images were then segmented into an grid of corresponding patches. Finally, two annotators reviewed the pairs to identify and label the specific patches that contained the differences. (5). Collection of Proportion VQA: For this task, we collected 120 images from the RefCOCO dataset and used the ground-truth segmentation masks to calculate the correct object proportions. The incorrect multiple-choice options were then generated by adding or subtracting eight percentage points from the true value. (6). Collection of Rotated Image OCR Task: We selected 60 images from OCRBench and applied rotation to each. The rotation probabilities were 25% for 90, 25% for 270, and 50% for 180. (7). Collection of Visual Search: Recognizing that problems in existing benchmarks like V* Bench (Wu & Xie, 2024) are often too simple for current models, we curated more challenging dataset for this task. We began by selecting 32 difficult problems from HR-Bench (Wang et al., 2025). To supplement these, we collected 88 new samples, which include 25 high-resolution art images and 63 high-resolution real-world images sourced from the internet. For each of these new images, an annotator was tasked with generating unique question-answer pair. In total, the Visual Search task comprises 120 samples, 88 of which are newly created for this benchmark. 3.3 BENCHMARK SUMMARY TIR-Bench consists of total of 1215 examples divided into 13 diverse but essential tasks. Questions in our benchmark are categorized into two types: multi-choice and free-form problems, counting 665 and 550, respectively. The average length for question text is 46.29 and the average length for"
        },
        {
            "title": "Preprint",
            "content": "answer text is 3.41. The distribution of each task is shown in Figure 7. Image and question examples for each task are shown in Appendix C."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we detail our evaluation setup and results for 22 leading MLLMs, considering both models with and without agentic tool-use capabilities. Our goal is to show that MLLMs lacking the ability to think with images perform poorly on TIR-Bench. We describe the experimental setup in subsection 4.1, report the results in subsection 4.2, present experiments on function calling in subsection 4.4, and compare agentic SFT with end-to-end SFT in subsection 4.5."
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "Model selection. We categorize the MLLMs we use into open-source, proprietary, and tool-using. For open-sourced, we evaluated 11 models across three widely used and up-to-date model families: LLaVA (Li et al., 2024a; Liu et al., 2024a), Qwen2.5-VL (Bai et al., 2025), and InternVL3 (Zhu et al., 2025), ranging from 3B to 78B. Results from these models accurately reflect open-source MLLMs performance on thinking-with-image reasoning tasks. For proprietary models, we selected 7 models across 3 model families: GPT (Hurst & OpenAI, 2024), Gemini-2.5 (Gemini Team, 2025), and Grok-4 (xAI, 2025). We test GPT series, including GPT-4.1, GPT-4o (Hurst & OpenAI, 2024), as well as o-series models (OpenAI, 2025a), including o3 and o4-mini. For these GPT models, we use Azure API for calling models w/o python interpreter or sandbox (OpenAI, 2025c). For agentic tool-using (TU) MLLMs, we evaluate three open-sourced frameworks: DeepEyes (Zheng et al., 2025), and PyVision (Zhao et al., 2025), and 2 proprietary models o4-mini-TU and o3-TU. We use the official OpenAI API and turn on code interpreter and set the container parameter as auto. Evaluation. We follow previous works (Lu et al., 2023; Li et al., 2025) to first generate answers from models and subsequently using GPT-4o to extract the final answer from the answer content. For multiple-choice and short-form answers, we compare the extracted value directly against the ground-truth to calculate accuracy; for grounding type problems such as Jigsaw Game and Spot the Difference with list type answer, we calculate the intersection over union (IoU). Implementation details. We conduct all evaluations in zero-shot manner for fair comparison and better generalization. For open models, all experiments are done on NVIDIA A100 GPUs. For proprietary models, we use the official API. More details can be found in the Appendix E. 4.2 EXPERIMENTAL RESULTS Both average and task-wise accuracies are reported in Table 2. We discuss several findings below. Result 1: TIR-Bench is challenging for all model types. The highest performance observed among all models is only 46%, result that underscores the difficulty of the TIR-Bench. This benchmark proves to be significant challenge for assessing Thinking-with-Images capabilities, even for advanced models such as o3-TU, which leverage code interpreter. Result 2: Traditional non-agentic models perform poorly on TIR-Bench. Across all tasks, non-tool-using MLLMs show poor performance: the performance of most open-source models is close to or slightly higher than the random guess performance while the top-performing MLLM, Gemini-2.5-pro, surpasses random guess results by only 15%. These results highlight that, without agentic tool-using abilities, MLLMs can not perform well on TIR-Bench. Result 3: Agentic tool-using is essential for TIR-Bench. The o3-TU model (OpenAI, 2025a) demonstrates the strongest overall performance, achieving the highest average accuracy at 46%. This represents substantial lead, outperforming the Gemini-2.5-Pro model by nearly 17% and the o3 model without code interpreter by 19%. With tool-use enabled, it achieves state-of-the-art results on the majority of the tasks, winning 10 out of the total 14 categories. The o3-TU and o4TU models demonstrate large performance improvement on tasks involving straightforward image"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Model Accuracy (%) Across Various Evaluation Tasks. SR: Symbolic Reasoning, WS: Word Search, LL-VQA: Low-Light VQA, IR: Instrument Reasoning, SD: Spot Difference, JG: Jigsaw Game, VS: Visual Search, RG: Rotation Game, Pro.: Proportion VQA. o3-TU: o3-toolusing, i.e., o3 with code interpreter. o3: o3 without code interpreter. Model All Color Pro. OCR SR Maze Math WS LL-VQA Random Guess 13.5 28.0 6.7 - 14.0 13.3 15.8 0.0 4.0 IR 8.8 SD 22.6 11.3 Llava-1.6-M-7B Llava-1.6-V-7B 11.5 13.0 Llava-1.6-34B Llava-Next-72B 11.2 17.7 Qwen2.5-VL-3B Qwen2.5-VL-7B 16.0 Qwen2.5-VL-32B 18.7 Qwen2.5-VL-72B 19.7 16.9 InternVL3-8B 19.1 InternVL3-38B InternVL3-78B 21.4 GPT-4.1 GPT-4o Gemini-2.5-Flash Gemini-2.5-Pro Grok-4 o4-mini o3 DeepEyes PyVision o4-mini-TU o3-TU 18.8 17.3 25.2 28.9 22.5 21.2 26.9 17.3 31.8 37.5 46.0 27.0 27.0 31.0 20.0 27.0 21.0 26.0 37.0 23.0 24.0 25.0 36.0 26.0 34.0 44.0 35.0 39.0 36.0 22.0 53.0 53.0 55.0 7.5 10.8 6.7 15.8 20.0 10.8 19.2 15.0 11.7 10.8 21. 7.5 22.5 20.0 21.7 53.3 17.5 34.2 6.7 26.7 21.7 31.7 3.3 0.0 1.7 3.3 31.7 48.3 25.0 33.3 0.0 3.3 3.3 11.7 10.0 30.0 25.0 6.7 8.3 8.3 41.7 63.3 53.3 53.3 Open-Source MLLMs 16.0 10.0 20.0 8.0 12.0 14.0 10.0 24.0 6.0 26.0 24.0 4.2 15.0 15.8 10.8 21.7 15.0 18.3 35.0 33.3 23.3 32.5 16.7 12.5 18.3 15.0 20.8 24.2 23.3 22.5 21.7 29.2 23.3 Proprietary MLLMs 12.0 10.0 26.0 34.0 20.0 12.0 34.0 17.5 20.0 17.5 24.2 25.8 13.3 29. 25.0 15.8 30.8 30.8 19.2 21.7 24.2 Tool-Using MLLMs 19.9 54.0 58.0 66.0 16.7 15.8 34.2 42.5 20.0 25.8 31.7 50.0 0.0 1.0 0.0 0.0 0.0 0.0 2.0 3.0 2.0 8.0 8. 4.0 0.0 10.0 12.0 2.00 5.0 4.0 1.0 10.0 55.0 64.0 14.0 8.0 16.0 10.0 12.0 22.0 14.0 32.0 22.0 28.0 28.0 24.0 26.0 42.0 42.0 22.0 30.0 28.0 16.0 32.0 44.0 42.0 6.3 7.5 16.3 11.3 13.8 11.3 15.0 12.5 8.8 13.8 16. 11.3 7.5 13.8 20.0 12.5 18.8 17.5 3.8 17.5 13.8 21.3 18.0 12.2 11.9 16.3 29.7 24.4 13.1 14.1 16.6 14.6 18.9 30.9 19.4 18.5 28.5 27.0 33.0 37.2 19.9 36.4 38.9 41.0 JG 5.8 0.0 0.0 0.0 0.0 0.0 0.0 5.4 0.0 4.5 5.1 5.8 5.1 6.2 8.0 10.4 10.0 8.0 10.8 3.9 7.6 11.8 16.4 VS RG 22.5 16.0 22.5 24.2 21.7 23.3 26.7 21.7 48.3 25.8 36.7 44.2 39.2 34.2 35.0 55.8 58.3 25.8 39.2 47.5 50.8 55.0 47.5 57.5 12.0 13.3 10.7 12.0 12.0 9.3 13.3 12.0 17.3 13.3 26. 22.7 20.0 29.3 30.7 18.7 26.7 33.3 12.0 46.7 52.0 77.3 manipulation or processing, including the rotation game, rotated image OCR, and word search. Similar phenomenon appears in PyVision, which implements thinking-with-images abilities based on GPT-4.1. PyVision brings 13% accuracy improvement compared with GPT-4.1. We also observe that, although o3-TU excels in most categories, the improvements are not uniform. For example, for complex tasks such as Jigsaw game, the performance of o3-TU is still very low. On Proportion VQA, performance surprisingly decreases from 34.2% to 31.7%. This task requires calling external segmentation models such as Segment Anything (Kirillov et al., 2023b) to obtain good rough estimate of the object segments. However, the current o3-TU model can only write executable code to manipulate images, but lacks the capability to call segmentation models specifically. 4.3 QUALITATIVE ANALYSIS We present several examples of o3-TU responses here and examples of other model responses in Appendix. F. . Since the responses from the OpenAI API contain no reasoning process, we re-ran the questions in the web-based ChatGPT interface and analyzed the responses. Figure 4 shows an case of instrument reading. The model repeatedly crops the region around the pointer, checks its position, and guesses the answer (highlighted in blue). After several cycles of cropping, observing, and reflecting, o3 finally confirms its answer as 400, which is correct. Figure 5 shows case of jigsaw puzzle involved lot of ineffective attempts. The model first divided the complete image into segments, then tried to compare and judge similarity by understanding the image as whole. In the early empirical stage, the model made nearly 55 iterations, but failed to obtain any usable partial results. After that, it began to use edge comparison, sampling the borders of image pieces and comparing them pair by pair, which also did not produce effective outcomes. The model then started to develop algorithms, applying brute-force permutations to explore possible arrangements and calculate similarity. However, the initial algorithm did not work. After modifying the sampling method, the model, through more than 36,000 attempts, achieved high similarity score that corresponded to the correct solution. This demonstrates that relying solely on the models raw visual capabilities is insufficient; only code-based perception proves to be reliable."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Comparison of function calling and writing code ability of different models. Figure 4: An example of o3 solving an case of instrument reading from TIR-Bench. 4.4 FUNCTION CALL EXPERIMENT RESULTS In this subsection, we report the results on the function calling ability of different MLLMs. Set-up. Using the Rotation Game task as case study, we experimented with two distinct functioncalling strategies: (1) providing the model with predefined rotate function, requiring it only to output the degree parameter; and (2) requiring the model to generate the full image rotation code itself. Additionally, we tested three prompt variations to assess the impact of guidance: (1) baseline prompt containing only the question; (2) prompt including hint to leverage the rotation function; and (3) more explicit prompt instructing the model to systematically test each degree from the answer choices. The specific function definitions and prompts used in this analysis are provided in Appendix D. During inference, we repeatedly call functions until the model produces final answer without requiring any function parameters. Results. We report the experimental results in Figure. 3 and report the average number of calling for each problem in Table. 3. Since Gemini-2.5-flash does not write code for all three prompts, we do not report it for writing code. We brief discuss some key findings here: (1). Clear Performance Hierarchy. o3 emerges as the top performer, achieving the highest accuracy, with o4-mini following closely. (2). Prompting Strategy is Key. The prompt strategy hinting to check each degree choice (prompt 3) works best. The performance of most models increases with prompt 3 compared to the other two prompts. This implies that models may not know how to best utilize the functions without explicit guidance. (3). Increased function call number in Newer Models. The average number of function calls for more recent advanced models (e.g., o3) is much higher than for previous models (e.g., GPT-4o). This implies that recent models are better trained for iterative function calling."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: The o3 models reasoning process for jigsaw puzzle task in TIR-Bench involved large number of ineffective attempts. (a) Loss (b) Accuracy Figure 6: Comparison of change of loss and accuracy between direct SFT and tool-use SFT. 4.5 FINE-TUNING COMPARISON EXPERIMENT RESULTS In this subsection, we report the experimental results on the different fine-tuning strategies. Setup. We use the rotated image OCR task as case study to evaluate data scaling performance. We created training sets of varying sizes: 1k, 5k, 10k, and 15k samplesby randomly selecting and rotating images from the OCRDataset (Minh, 2024). We then compared two distinct training strategies: (1). Direct SFT: standard supervised fine-tuning approach where the model is trained to map the rotated image directly to the ground-truth text. (2). Tool-Use SFT: An agentic approach where the model first learns to output the correct rotation degree. The restored image is then concatenated with the original context, and the model is subsequently trained to read the text from this corrected visual input. We use Qwen-2.5-VL-7B and fully fine-tune all parameters with 5 epochs. Results. We report accuracy on the Rotated OCR task in Figure 6b and the loss curves on 15k samples in Figure 6a. Overall, Tool-use SFT significantly outperforms Direct SFT. For Tool-use SFT, performance scales positively with data size, whereas Direct SFT shows no such trend. This suggests that simply scaling data for Direct SFT is ineffective on tasks requiring image-based reasoning. We also observe that Tool-use SFTs loss decreases much faster despite starting from higher initial value. This is likely because Qwen-2.5-VL was not pretrained with function-calling, leading to higher initial loss. Furthermore, since Qwen-2.5-VL was trained mostly on correctly oriented OCR data, fine-tuning it directly on rotated data may cause forgetting. In contrast, restoring image orientation before text output avoids this issue, which may explain the faster loss reduction."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose TIR-Bench, comprehensive benchmark designed to evaluate the thinkingwith-images ability of agentic MLLMs. TIR-Bench is composed of 13 meticulously collected tasks that assess wide range of tool-assisted reasoning skills. By assessing diverse MLLMs on TIRBench, including both standard models and those augmented with tooluse capabilities, we find that TIR-Bench is challenging benchmark for all models that necessitates thinking-with-images capabilities for successful completion. Lastly, we conduct pilot study comparing direct and agentic finetuning for image-operation tasks and evaluating the function-calling abilities of various MLLMs."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/ claude-4. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint, (arXiv:2507.06261), 2025. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. In NeurIPS, 2024. Aaron Hurst and OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. doi: 10.48550/arXiv.2410.21276. URL https://arxiv.org/abs/2410.21276. for omni large multimodal model accepting text, audio, image, and video inputs. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023a. URL https://arxiv.org/abs/2304.02643. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023b. URL https://arxiv.org/abs/2304.02643. Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024b. Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, and Jing Ma. Mmcode: Evaluating multimodal code large language models with visually rich programming problems. arXiv preprint arXiv:2404.09486, 2024c. Ming Li, Jike Zhong, Tianle Chen, Yuxiang Lai, and Konstantinos Psounis. Eee-bench: comprehensive multimodal electrical and electronics engineering benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1333713349, 2025. Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou. Colorbench: Can vlms see and understand the colorful world? comprehensive benchmark for color perception, reasoning, and robustness. arXiv preprint arXiv:2504.10514, 2025."
        },
        {
            "title": "Preprint",
            "content": "Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024a. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Minh. Combined ocr dataset. Hugging Face Datasets, 2024. URL https://huggingface. co/datasets/ducto489/ocr_datasets. OpenAI. Openai o3: reasoning model trained to think before responding. Blog post / system card, April 2025a. Introducing OpenAI o3 and o4-mini. OpenAI. Introducing gpt-4.1 in the api, 2025b. URL https://openai.com/index/ gpt-4-1/. OpenAI. Thinking with images, 2025c. URL https://openai.com/index/ thinking-with-images/. Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, and Jie Tang. Cogcom: visual language model with chain-of-manipulations reasoning. In ICLR, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning, 2025a. URL https://arxiv.org/abs/2505.15966. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025b. Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In ICCV, 2023. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024a. URL https://arxiv.org/ abs/2402.14804. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025."
        },
        {
            "title": "Preprint",
            "content": "Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635, 2023. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. xAI. Grok 4: Native tool use, real-time search integration, and frontier reasoning. xAI blog post / documentation, July 2025. Includes the Grok 4 Heavy variant, 256 000 token context window, and performance improvements via reinforcement learning and expanded training data. Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, and Chen Wei. Play to generalize: Learning to reason through game play. arXiv preprint arXiv:2506.08011, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing Li. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl, 2025a. URL https://arxiv.org/abs/2505.15436. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu arXiv preprint Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv:2508.11630, 2025b. Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling. arXiv preprint arXiv:2507.07998, 2025. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning, 2025. URL https://arxiv.org/abs/2505.14362. Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, arXiv preprint Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv:2509.01656, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Task Distribution of TIR-Bench. THE USE OF LARGE LANGUAGE MODELS (LLMS) In this paper, we only used LLMs to correct the grammar and spelling errors in the writing. All the results are produced by authors."
        },
        {
            "title": "B DETAILED TASK DESIGN AND COLLECTION",
            "content": "B.1 TASK DESIGN To extensively validate the models ability to think with images we design 13 tasks. The overview of the benchmark is shown in Figure 2. We describe these 13 tasks below: Color VQA task: This task assesses the models ability to answer questions related to an images color composition. Answering these questions requires the model to programmatically process the image to obtain visual informationfor instance, by writing and executing code to calculate the proportion of specific color. Referring Object Proportion VQA: This task assesses the models agentic capabilities, requiring it to call powerful external segmentation model to obtain an objects mask and then programmatically calculate its proportion relative to the entire image. Rotated Image OCR: This task evaluates models ability to execute multi-step visual reasoning process. The model must first identify that an image containing text is incorrectly oriented, then use tool to rotate the image to its correct position, and finally perform optical character recognition (OCR) to accurately read the content. Symbolic Reasoning: This task assesses the models ability to apply abstract, rule-based logic to visual information. For instance, when asked to count the edges of complex polygon, the model cannot simply guess; it must systematically identify and enumerate each distinct edge, process that may require internal algorithms for vertex or line detection to arrive at the correct count. Maze: This task assesses the models ability for advanced spatial planning and algorithmic execution. The model must analyze the visual structure of the maze, devise solution using image processing tools (such as morphological operations), and apply pathfinding algorithm to solve"
        },
        {
            "title": "Preprint",
            "content": "it. Finally, it must draw the solution path onto the image, demonstrating its ability to translate an abstract plan into concrete visual action. Math Problems: This task assesses the models ability to solve geometric problems by programmatically augmenting the visual input. To find solution, the model must use tools to draw auxiliary constructs, such as adding lines to diagram or imposing coordinate system to define and calculate properties like relative lengths. Word Search Puzzle: This task evaluates the models ability to perform fine-grained visual discrimination and anomaly detection. The image presents field of numerous, nearly identical characters, with only few subtle differences. Standard OCR is designed to fail in this scenario, forcing the model to devise programmatic solution. To succeed, the model must write and execute code to perform more fundamental analysis, such as pixel-level comparison or targeted visual search, to locate and identify the characters that deviate from the pattern. Low-Light Image VQA: This task evaluates the models ability to overcome suboptimal visual conditions. Given low-light image where details are obscured, the model must first recognize the issue and then programmatically enhance the image, for instance, by writing and executing code to increase its contrast or brightness, before it can accurately answer questions about the content. Instrument Reading: This task evaluates the models ability to perform sequential, toolassisted analysis. To succeed, the model must first locate the key areas for reading the instrument, then programmatically crop and enlarge these specific regions for clarity, and finally, accurately read the value from the enhanced view. Spot the Difference: This task evaluates the models ability to perform precise, programmatic visual comparison. To identify the differences between two nearly identical images, the model must execute tool-based strategy, such as programmatic image subtraction, to highlight differing regions at pixel level. After locating the discrepancies, the model must then isolate and identify the specific image patches that contain these alterations. Jigsaw Puzzle: This task assesses the models ability to perform complex spatial reasoning through an iterative, tool-based approach. The model is required to programmatically segment an image into pieces and then repeatedly attempt to reassemble them. This process involves continuous loop of action and self-correction, where the model must evaluate each configuration to determine if the image has been successfully restored, continuing the cycle until the solution is achieved. Visual Search: This task evaluates the models ability to locate specific targets within complex or high-resolution image through deep, multi-turn reasoning. To succeed, the model must engage in an iterative search process, strategically using tools to repeatedly zoom in on and analyze different regions until the object or information is found. Rotation Game: This task assesses the models ability to perform iterative orientation correction (Xie et al., 2025). To restore rotated image to its upright position, the model must programmatically test various rotation angles. After each transformation, it must visually evaluate the result to determine if the orientation is correct, engaging in trial-and-error loop of tool-based action and visual judgment until the problem is solved."
        },
        {
            "title": "C DETAILED TASK EXAMPLE",
            "content": "C.1 COLOR Figure 8: Additional example of Color task. C.2 LOW-LIGHT Figure 9: Additional example of Low-Light task."
        },
        {
            "title": "Preprint",
            "content": "C."
        },
        {
            "title": "INSTRUMENT READING",
            "content": "Figure 10: Additional example of Instrument Reading task. C.4 JIGSAW Figure 11: Additional example of Jigsaw task."
        },
        {
            "title": "Preprint",
            "content": "C.5 MATH C.6 MAZE Figure 12: Additional example of Math task. Figure 13: Additional example of Maze task."
        },
        {
            "title": "Preprint",
            "content": "C.7 ROTATED OCR Figure 14: Additional example of Rotated OCR task. C.8 PROPORTION Figure 15: Additional example of Proportion task."
        },
        {
            "title": "Preprint",
            "content": "C.9 ROTATION Figure 16: Additional example of Rotation task. C.10 SPOT THE DIFFERENCE Figure 17: Additional example of Spot the Difference task."
        },
        {
            "title": "Preprint",
            "content": "C.11 SYMBOLIC REASONING Figure 18: Additional example of Symbolic Reasoning task. C.12 VISUAL SEARCH Figure 19: Additional example of Visual Search task."
        },
        {
            "title": "Preprint",
            "content": "C.13 WORD SEARCH Figure 20: Additional example of Word Search task."
        },
        {
            "title": "D DETAILED PROMPTS",
            "content": "D.1 WRITE CODE prompt being is the prompt for the beginning turn and prompt return is the prompt for returning processed images to models. Prompt strategy 1: prompt 1 = user prompt prompt 2 = \"The returning status and the processed image or text (if any) of the code is attached.\" Prompt strategy 2: prompt being = user prompt + nPlease consider to write code to process the image. prompt return = The returning status and the processed image or text (if any) of the code is attached, you can continue to write code to process the original image for better understanding or proceed to answer the question. For prompt 3: prompt 1 = user prompt + \"nPlease try to write code to rotate the original image with the rotation degree in the options to verify the correctness. You can try as many as you can.\" prompt 2 = \"The returning status and the processed image or text (if any) of the code is attached, you can continue to write code to rotate the original image with degree of other difference options for better understanding or proceed to answer the question if you have meet the correct option.\" D.2 FUNCTION CALLING For prompt strategy 1:"
        },
        {
            "title": "Preprint",
            "content": "prompt 1 = user prompt prompt 2 = \"The rotated image is attached.\" For prompt strategy 2: prompt 1 = user prompt + nPlease consider to call the rotation function. prompt 2 = The rotated image is attached, you can continue to call the function to rotate the original image with difference degree for better understanding or proceed to answer the question. For prompt strategy 3: prompt 1 = user prompt + \"nPlease try to call the rotation function with the rotation degree in the options to verify the correctness. You can try as many as you can.\" prompt 2 = \"The rotated image is attached, you can continue to call the function to rotate the original image with degree of other difference options for better understanding or proceed to answer the question if you have meet the correct option.\" Table 3: Average number of function calling across different prompt strategies and calling stratefies. Model Function Write Code Prompt 1 Prompt 2 Prompt Prompt 1 Prompt 2 Prompt 3 GPT-4.1 GPT-4o Gemini-2.5-flash Gemini-2.5-pro o4-mini o3 0.00 0.00 0.64 0.75 0.07 0.33 1.15 1.12 1.05 0.87 1.52 1. 4.68 4.29 1.73 1.09 2.87 3.59 1.32 2.15 - 0.25 3.43 6.05 1.68 1.87 - 0.28 3.68 5.77 3.95 4.07 - 0.21 2.68 5."
        },
        {
            "title": "E IMPLEMENTATION DETAILS",
            "content": "We conduct all evaluations in zero-shot manner for fair comparison and better generalization. For open models, all experiments are done on NVIDIA A100 GPUs. For proprietary models, we use the official API. We set do sample = False, temperature = 0, max new tokens = 2048 for all open models."
        },
        {
            "title": "F EXAMPLES OF MODEL RESPONSES",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "Question: Measure the distances of AC and CB according to the scale. Driving from point to point and then from point to point B, with speed of 60 km/h, how many hours are needed? Answer with single number, such as 1, 2, 3. Ground Truth: 4. Response from o3-TU: 4 Response from o3: 3. Response from o4-mini-TU: 3 Response from o4-mini: 2 Response from Gemini-2.5-Pro: By measuring the distances using the provided scale, we find that the line segment CB is equal in length to the scale bar representing 80 km, so Distance CB = 80 km. The segment AC is shorter than the full 80 km scale; it appears to be three-quarters of the scale, giving Distance AC = 0.75 80 = 60 km, or equivalently, 1.5 times the 040 km portion of the scale, which also gives 60 km. Therefore, the total distance is Total Distance = Distance AC + Distance CB = 60 + 80 = 140 km. Assuming driving speed of 60 km/h, the time needed is Time = Total Distance 60 2.33 hours, which rounded to the nearest whole number gives 2 hours. Response from Gemini-2.5-flash: To solve this problem, we first determine the map scale: the scale bar shows that 223 pixels correspond to 80 km, so 1 pixel 0.3587 km. The pixel coordinates are A(112, 102), C(352, 417), and B(686, 417). Using the distance formula, the pixel length of AC is (cid:112)(352 112)2 + (417 102)2 396.01 pixels, and CB is (cid:112)(686 352)2 + (417 417)2 = 334 pixels. Converting to kilometers, AC = 396.01 80 223 119.82 km. Thus, the total distance is 142.06 + 119.82 = 261.88 km. With driving speed of 60 km/h, the travel time is 261.88 60 4.36 hours, which rounds to 4 hours. Therefore, the final answer is 4 . 223 142.06 km and CB = 334 80 = 140 Speed"
        },
        {
            "title": "Preprint",
            "content": "If the length of AB is 1, what is the length of BC? Provide your answer as an Question: integer. Ground Truth: 3. Response from o3-TU: 3 Response from o3: 2. Response from o4-mini-TU: From the picture the segment BC is three times as long as AB. Hence if AB = 1 then BC = 3. Response from o4-mini: 8 Response from Gemini-2.5-Pro: Based on the visual representation of the line segments, the length of segment BC is twice the length of segment AB. Given that AB = 1, we calculate: BC = 2 AB = 2 1 = 2. The final answer is 2 Response from Gemini-2.5-flash: We are given that the length of segment AB is 1. From the visual representation, it is clear that AB BC. Therefore, if AB = 1, then BC = 1. The final answer is Figure 21: Examples of model responses."
        },
        {
            "title": "Preprint",
            "content": "Question: How many times does the number 6 appear in this image? Answer with an integer, such as 1,2,3. Ground Truth: 4. Response from o3-TU: 4 Response from o3: 10 Response from o4-mini-TU: 4 Response from o4-mini: 2 Response from Gemini-2.5-Pro: 8 Response from Gemini-2.5-flash:Lets carefully examine the image to count the occurrences of the number 6. Scanning the original image row by row: 1. Row 1: All 5s. 2. Row 2: All 5s. 3. Row 3: All 5s. 4. Row 4: All 5s. 5. Row 5: All 5s. 6. Row 6: 5555**6**5555555555 (Found 1 6) 7. Row 7: All 5s. 8. Row 8: All 5s. 9. Row 9: All 5s. 10. Row 10: All 5s. 11. Row 11: 55555**6**555555555 (Found 1 6) 12. Row 12: All 5s. 13. Row 13: All 5s. 14. Row 14: All 5s. 15. Row 15: All 5s. 16. Row 16: 5**6**5555555555555 (Found 1 6) 17. Row 17: All 5s. 18. Row 18: All 5s. 19. Row 19: All 5s. 20. Row 20: All 5s. By carefully checking all rows, we find three instances of the number 6. The provided crops also highlight these same three instances. The total number of times the number 6 appears in the image is 3. Figure 22: Examples of model responses."
        },
        {
            "title": "Preprint",
            "content": "Question: In the figure, in which row and column does the number 9 appear? The final answer should output two number list. The first number represents the row count from top to bottom, and the second number represents the column count from left to right. For example, [1, 2] means the first row and the second column. Ground Truth: [54, 50]. Response from o3-TU: [54, 50] Response from o3: [27, 80] Response from o4-mini-TU: The odd-one-out 9 falls in the 54th row from the top and the 50th column from the left. So the answer is [54, 50] Response from o4-mini: The grid only contains the digit 4 in every position there is no 9 anywhere. Hence no row/column coordinate can be given for 9. Response from Gemini-2.5-Pro: 1. **Analyze the image:** The image is large grid composed almost entirely of the number 4. The task is to find the single occurrence of the number 9. The hint images, particularly the last one, help to zoom in on the relevant area. 2. **Locate the 9:** By examining the last hint image, we can clearly see the number 9 embedded among the 4s. 3. **Determine the row:** We need to find the row number of the 9 in the original, full image. We count the rows from the top down, starting with row 1. * Counting the rows one by one, we find that the line containing the 9 is the 34th row from the top. 4. **Determine the column:** Now, we need to find the column number of the 9 in that 34th row. We count the numbers from left to right, starting with column 1. * In the 34th row, lets count the position of the 9: 4 4 4 4 4 4 4 4 4 4 4 4 4 4 9 ... * Counting from the left, we can see there are 14 4s before the 9. Therefore, the 9 is in the 15th position. 5. **Final Answer:** The number 9 is in the 34th row and the 15th column. The output should be list of these two numbers. [34, 15] Response from Gemini-2.5-flash:The number 9 appears in the figure. To find its location, we need to count the rows from top to bottom and the columns from left to right. 1. **Count the total number of rows:** By carefully examining the original image, we can count that there are 50 rows of numbers. 2. **Locate the row of the number 9:** Observing the last provided hint image, which clearly shows the number 9, we can see that it is located in the 8th row from the bottom of that specific crop. If we apply this to the full 50-row image, the row containing the 9 is 50 - 8 + 1 = 43rd row from the top. (Alternatively, if we count from the bottom, the 9 is in the 8th row from the bottom. So, 50 - 7 = 43rd row from the top). 3. **Locate the column of the number 9:** In the 43rd row, lets count the characters from left to right. The sequence for the 43rd row is: 4444444444444444444444444444444444444944444444444 Counting from the left, the number 9 is the 38th character in this row. Therefore, the number 9 appears in the 43rd row and the 38th column. The final answer is [43, 38] 26 Figure 23: Examples of model responses."
        },
        {
            "title": "Preprint",
            "content": "Figure 24: Response from o3, case proportion estimation from TIR-Bench. Figure 25: Response from o3, case word searching from TIR-Bench."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "Emory University",
        "Rice University",
        "Shanghai AI Laboratory",
        "University of Southern California"
    ]
}