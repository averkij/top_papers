{
    "paper_title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
    "authors": [
        "Shaolei Zhang",
        "Qingkai Fang",
        "Zhe Yang",
        "Yang Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 9 8 3 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "LLAVA-MINI: EFFICIENT IMAGE AND VIDEO LARGE MULTIMODAL MODELS WITH ONE VISION TOKEN Shaolei Zhang1,3, Qingkai Fang1,3, Zhe Yang1,3, Yang Feng1,2,3 1Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS) 2Key Laboratory of AI Safety, Chinese Academy of Sciences 3University of Chinese Academy of Sciences, Beijing, China zhangshaolei20z@ict.ac.cn, fengyang@ict.ac.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large multimodal models (LMMs), such as GPT-4o (OpenAI, 2024), equip large language models (LLMs) (OpenAI, 2022; 2023) with the ability to understand visual information, exhibiting common trend toward low-latency responses to enable real-time multimodal interactions. Recently, the most widely adopted LMMs (Liu et al., 2023b; 2024a; Zhu et al., 2024), exemplified by the LLaVA series (Liu et al., 2023b), involves embedding image patches into vision tokens through vision encoder (Radford et al., 2021) and incorporating them into the LLMs context to facilitate visual information comprehension, leading to strong performance in image and video understanding. However, the substantial computational costs of LMMs present ongoing challenges. Unlike LLMs (Touvron et al., 2023a;b; Dubey et al., 2024), which only process textual inputs, LMMs must incorporate large number of additional vision tokens into the LLMs context to represent visual information (Liu et al., 2023b), significantly increasing computational complexity. For instance, in the widely used vision encoder CLIP ViT-L/336px, single image is encoded into 24 24 = 576 vision tokens (Radford et al., 2021), where integrating such large number of vision tokens into Corresponding author: Yang Feng. 1Code: https://github.com/ictnlp/LLaVA-Mini; Model: https://huggingface.co/ ICTNLP/llava-mini-llama-3.1-8b"
        },
        {
            "title": "Preprint",
            "content": "the context of parameter-heavy LLM results in significant computational overhead and higher inference latency. This issue becomes even more pronounced in high-resolution image modeling (which requires more vision tokens per image) (Liu et al., 2024b) or video processing (which involves processing more images) (Maaz et al., 2024; Lin et al., 2023a). Therefore, developing efficient LLMs is essential for achieving GPT-4o-like low-latency multimodal interactions. The computational demands of LMMs are primarily driven by model scale and the number of tokens in the input context. Existing approaches to improving LMM efficiency typically focus on model downsizing (Chu et al., 2023; 2024; Yuan et al., 2024a; Zhou et al., 2024a) or quantization techniques (Yuan et al., 2024b), but often overlook another critical avenue: reducing the number of vision tokens to shorten the input context. Some token reduction methods rely on predefined rules to reduce the number of tokens output by the vision encoder (Bolya et al., 2023; Shang et al., 2024; Li et al., 2024e; Ye et al., 2024c; Hu et al., 2024), which leads to the loss of visual information and inevitably results in performance degradation (Wang et al., 2024; Fan et al., 2024). In this paper, we aim to develop efficient LMMs by minimizing the number of vision tokens while maintaining comparable performance. To this end, we begin by exploring foundational question: How does the LMM (particularly the LLaVA architecture) understand vision tokens? Through layer-wise analysis (refer to Sec.3), we observe that the importance of vision tokens changes across different layers of LLM. In the early layers, vision tokens play crucial role, receiving considerable attention from the following text tokens (e.g., user input instructions and responses). However, as the layers deepen, the attention devoted to vision tokens decreases sharply, with most attention shifting towards the input instructions. Notably, even when we entirely remove vision tokens in some later layers, LMM keeps certain visual understanding capabilities. This finding suggests that vision tokens are more critical in early layers, where text tokens fuse visual information from vision tokens. Figure 1: LLaVA-Mini achieves comparable performance to LLaVA-v1.5 using only 1 vision token instead of 576, yielding efficient computation, lower latency, and reduced VRAM usage. Based on this finding, if the fusion process can be shifted from the early layers of LLM to perform before LLM, we can significantly reduce the number of vision tokens fed into the LLM without sacrificing performance. Along with this idea, we propose LLaVA-Mini, an efficient and high-quality LMM with minimal vision tokens. LLaVA-Mini introduces modality pre-fusion module before LLM to fuse visual information into the instruction text in advance, and employs compression module to highly compress the vision tokens before inputting them into LLM, thereby enhancing efficiency while preserving high-quality visual understanding. Under extreme settings, LLaVA-Mini requires only one vision token per image fed into LLM backbone, offering significant advantages in inference time and memory consumption for high-resolution image and long video processing. Experiments across wide range of 11 image-based and 7 video-based understanding benchmarks show that LLaVA-Mini achieves performance comparable to LLaVA-v1.5 (Liu et al., 2023b) while using only 1 vision token instead of 576 (compression rate of 0.17%). With minimal vision tokens, LLaVA-Mini offers substantial benefits in terms of computational efficiency (77% FLOPs reduction) and lowering GPU memory usage (360 MB 0.6 MB per image), as shown in Figure 1. As result, LLaVA-Mini decreases inference latency of image understanding from 100 ms to 40 ms and also enables the processing of long videos exceeding 10,000 frames (over 3 hours) on an NVIDIA RTX 3090 with 24GB of memory, paving the way for low-latency multimodal interactions."
        },
        {
            "title": "2 RELATED WORK",
            "content": "As Large multimodal models (LMMs) are increasingly deployed in real-time applications (OpenAI, 2024), enhancing their efficiency has become critical concern. Recent efforts focus on either"
        },
        {
            "title": "Preprint",
            "content": "reducing the model size or the number of tokens that fed into LMM. To reduce LMMs model size, previous methods directly replace the LLM backbone with smaller one (Chu et al., 2023; 2024; Yuan et al., 2024a; Zhou et al., 2024a), while directly reducing the parameter scale can impact the LLM backbones capabilities, resulting in performance declines in visual tasks (Shang et al., 2024). Another efficiency determinant for LMMs is the context length provided to the LLM backbone, including vision and text tokens. In practice, the number of vision tokens can be substantial, particularly when processing high-resolution images and videos. For image-based LMMs, token merging (Bolya et al., 2023), PruMerge (Shang et al., 2024), and TokenPacker (Li et al., 2024e) aggregate vision tokens based on similarity. Qwen-VL (Bai et al., 2023) and MQT-LLaVA (Hu et al., 2024) utilize Q-former (Li et al., 2023a) to compress vision tokens into fixed length. However, directly reducing vision tokens inevitably results in the loss of visual information (Fan et al., 2024). For video-based LMMs, Video-ChatGPT (Maaz et al., 2024), VideoChat (Li et al., 2024c), VideoLLaVA (Lin et al., 2023a), and Video-LLaMA (Zhang et al., 2023), select fixed number of frames from videos of varying lengths. MovieChat (Song et al., 2024a) applies memory techniques to condense videos into fixed-length representation. Such frame selection or merging methods may lose some key frames or misunderstand the temporal information of the video (Zhou et al., 2024b). Previous methods have primarily focused on token reduction on the vision encoder. LLaVA-Mini takes this step further by exploring how vision tokens and text tokens interact within the LLM backbone, and accordingly introduces modality pre-fusion module, enabling an extreme compression of vision tokens (1 vision token fed into LLM) while achieving comparable performance."
        },
        {
            "title": "3 HOW DOES LLAVA UNDERSTAND VISION TOKENS?",
            "content": "To compress visual tokens while preserving visual understanding, we sought to figure out how LMMs understand visual tokens. Given the complexity of this issue, our preliminary analysis concentrated on the LLaVA architecture (Liu et al., 2023b), focusing on the role of visual tokens (particularly their quantity) in LMMs from an attention-based perspective (Xiao et al., 2024)."
        },
        {
            "title": "3.1 LLAVA ARCHITECTURE",
            "content": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023b) is an advanced multimodal architecture that integrates vision and language processing capabilities. Building upon vision Transformers (ViT) (Dosovitskiy et al., 2021) for visual inputs and LLMs for text, LLaVA can generate language response Xa based on the given language instruction Xq and visual inputs Xv. Typically, pre-trained CLIP ViT-L/14 (Radford et al., 2021) and projection layer are employed to encode the visual inputs Xv into vision tokens (i.e., continuous representations) Hv. Then, vision tokens Hv and language instructions embedding Hq are fed into an LLM, such as Vicuna (Chiang et al., 2023) or Mistral, to generate the response Xa. In practice, vision tokens are often inserted into the middle of the language instruction, so the inputs of LLM can be formally represented as: 1, , lv where lv and lq denote the lengths of the vision tokens and language instruction, respectively. For 1, , instance, in LLaVA-v1.5, the system prompts are positioned before the image (i.e., k), while the user inputs follow the image (i.e., k+1, , lq ) (Liu et al., 2023b). 1, , (cid:68) k, , (1) (cid:69) , k+1, , lq"
        },
        {
            "title": "3.2 PRELIMINARY ANALYSES",
            "content": "We begin by analyzing the significance of visual tokens in LMMs to guide the strategies for compressing vision tokens. Specifically, we evaluate the importance of visual tokens at each layer of LMMs from an attention-based perspective. Our analysis encompasses several LMMs, including LLaVA-v1.5-Vicuna-7B, LLaVA-v1.5-Vicuna-13B, LLaVA-v1.6-Mistral-7B, and LLaVA-NeXTVicuna-7B (Liu et al., 2023b; 2024b), to identify common characteristics across models of varying sizes and training datasets. Appendix gives the formal expression of the preliminary analyses. Vision Tokens are More Important in Early Layers To find out which layers in LMM the vision tokens play more important role, we measure the attention weights assigned to different token"
        },
        {
            "title": "Preprint",
            "content": "(a) LLaVA-v1.5-Vicuna-7B (b) LLaVA-v1.5-Vicuna-13B (c) LLaVA-v1.6-Mistral-7B (d) LLaVA-NeXT-Vicuna-7B Figure 2: Layer-wise variation of attention weights assigned to different types of tokens (including instruction, vision, and response) in LMMs. AB means the attention weights from to B. (a) LLaVA-v1.5-Vicuna-7B (b) LLaVA-v1.5-Vicuna-13B (c) LLaVA-v1.6-Mistral-7B (d) LLaVA-NeXT-Vicuna-7B Figure 3: Attention entropy assigned to different types of tokens across different layers in LMMs. types (including instruction, vision, and response) at each layer. As shown in Figure 2, the attention assigned to vision tokens varies significantly across layers. Visual tokens receive more attention in the earlier layers, but this attention sharply decreases in the deeper layers, with over 80% of the attention being directed towards instruction tokens. This change in attention suggests that vision tokens play central role in the early layers, with the instruction tokens seeking relevant visual information from vision tokens through attention mechanisms. In the later layers, the model relies more on instructions that have already fused the visual data to generate responses. Most Vision Tokens are Focused in Early Layers To further assess the importance of individual visual tokens, we calculate the entropy of the attention distribution at each layer. As shown in Figure 3, we find that the entropy of attention toward visual tokens is much higher in the earlier layers, indicating that most visual tokens are evenly attended to in the early layers. Figure 4: Attention visualization at different layers in LLaVA-v1.5 (color bar: logarithmic scale). To intuitively illustrate the layer-wise variations in the importance of visual tokens, Figure 4 visualizes the attention distribution across each layer of LLaVA-v1.5. Almost all visual tokens receive broader attention in the early layers, while only some visual tokens are focused in the later layers. These observations suggest that all visual tokens are crucial in the early layers, and reducing their quantity inevitably results in loss of visual information. This explains why previous methods of direct token reduction will compromise visual understanding capabilities (Shang et al., 2024; Ye et al., 2024c; Hu et al., 2024). To further substantiate our finding that visual tokens are particularly critical in the early layers, we evaluated the visual understanding ability of LMMs when visual tokens were dropped at different layers. Specifically, we measured the performance of LLaVA-v1.5 on the GQA (Hudson & Manning, 2019) and MMBench (Liu et al., 2024c), with visual tokens being dropped at layers 1-4, 5-8, ... , 29-32, respectively. As shown in Figure 5, removing visual tokens in the early layers leads to complete loss of visual understanding ability, while removing tokens in the higher layers has minimal effect, with the model retaining much of its original performance. In conclusion, our analyses and ablation study reveal that vision tokens play crucial role in the early layers of LLaVA, where text tokens fuse visual information from the vision tokens at this stage. This insight can inform our strategies for compressing vision tokens. Figure 5: Performance of LLaVA-v1.5 when removing all vision tokens in various layers of LMM."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Architecture of LLaVA-Mini. Left: LLaVA-Mini represents each image with one vision token. Right: Detailed view of the proposed query-based compression and modality pre-fusion."
        },
        {
            "title": "4 LLAVA-MINI",
            "content": "We introduce LLaVA-Mini, an efficient large multimodal model with minimal vision tokens. Like previous work, LLaVA-Mini uses vision encoder to encode an image into several vision tokens. To enhance the efficiency, LLaVA-Mini significantly reduces the number of vision tokens fed into LLM backbone through compression module. To retain visual information during compression, based on previous findings that vision tokens play crucial role in the early layers for fusing visual information, LLaVA-Mini introduces modality pre-fusion module before the LLM backbone to fuse the visual information into the text tokens. The details of LLaVA-Mini are as follows."
        },
        {
            "title": "4.1 ARCHITECTURE",
            "content": "The architecture of LLaVA-Mini is illustrated in Figure 6. For the visual inputs Xv, pre-trained CLIP vision encoder (Radford et al., 2021) is employed to extract visual features from each image. These features are then mapped into the word embedding space via projection layer, producing vision tokens Hv RN 2dh, where 2 is the number of vision tokens and dh is the LLMs embedding dimension. For the language instruction Xq, LLMs embedding layer is used to generate text token representations Hq Rlqdh , where lq is the number of text tokens. Vision Token Compression To enhance the efficiency of LMMs, LLaVA-Mini reduces the number of vision tokens fed into the LLM backbone by utilizing query-based compression module. To learn compression of the vision tokens, LLaVA-Mini introduces learnable compression queries Qv. These queries interact with all vision tokens Hv through cross-attention (Li et al., 2023a), selectively extracting the important visual information to produce compressed vision tokens ˆHv RC2dh. To preserve the spatial information in the image during compression, we introduce 2D sinusoidal positional encoding E() (He et al., 2021) on the learnable queries and original vision tokens. Formally, the compression can be expressed as: ˆHv = Hv, where = Softmax (cid:16) (Qv + E(Qv)) (Hv + E(Hv))(cid:17) , (2) where RC2N 2 is the similarity and ˆHv are compressed vision tokens. Modality Pre-fusion The compression of vision tokens inevitably results in some loss of visual information. To retain as much visual information during compression as possible, LLaVA-Mini introduces modality pre-fusion before the LLM backbone, enabling text tokens to fuse relevant visual information from all vision tokens in advance. Based on our previous observations, where this fusion stage occurs implicitly within the early layers of the LLM, the modality pre-fusion module () consists of Nf usion Transformer blocks (Vaswani et al., 2017), where each Transformer block shares the same structure and hyperparameters with LLM backbone. Vision tokens Hv and text"
        },
        {
            "title": "Preprint",
            "content": "tokens Hq are concatenated and fed into the pre-fusion module, and the outputs corresponding to the text tokens are then extracted as fusion tokens, expressed as: ˆHq = (Concat (Hv, Hq)) [lq : ] , (3) where ˆHq Rlqdh are fusion tokens of text representations with related visual information. Finally, the compressed vision tokens ˆHv and fusion tokens ˆHq of text representations with related visual information (C 2 + lq tokens in total) are fed to LLM together to generate the response."
        },
        {
            "title": "4.2 HIGH-RESOLUTION IMAGE AND VIDEO MODELING",
            "content": "LLaVA-Mini uses minimal vision tokens to represent visual information, making it possible to handle high-resolution images and videos much more efficiently. High-Resolution Image The resolution of LMM is typically determined by the vision encoder, such as CLIPs ViT-L, which encodes at resolution of 336*336 pixels. To perceive images at higher resolution, we divide each image into four sub-images by splitting it horizontally and vertically into two parts (Liu et al., 2024b). Each of these sub-images is processed by the vision encoder and projection individually, yielding 2 4 vision tokens with high resolution of 672*672 pixels. The proposed compression module is then employed to reduce these 2 4 vision tokens into 2 compressed vision tokens ˆHv. The modality pre-fusion module takes the 4 sub-images (N 2 4 vision tokens), the original image (N 2 vision tokens), and the language instruction (lq text tokens) as inputs, and then generates lq fusion tokens ˆHq with richer global and local visual information. Finally, the number of tokens input to the LLM is 2 + lq. Note that when handling high-resolution images, is set slightly higher than in standard-resolution settings to preserve more details. Video When handling videos, LMMs often extract multiple frames from the video (Li et al., 2023b), which incurs significant computational costs. For instance, in the case of LLaVA-v1.5, extracting frames at rate of 1 frame per second (fps) from an 8-second video results in 5768 = 4608 vision tokens, leading to substantial VRAM usage. LLaVA-Mini can represent each image with minimal vision tokens, providing significant advantage in processing long videos. For video consisting of frames, LLaVA-Mini processes each frame individually, generating 2 vision tokens and lq fusion tokens per frame. 2 vision tokens from each of frames are sequentially concatenated to yield total of 2 vision tokens, i.e., ˆHv. Then, lq fusion tokens corresponding to frames are aggregated through pooling operation to generate the videos fusion tokens ˆHq. As result, the number of tokens fed to the LLM is reduced from 2 + lq to 2 + lq for video of frames."
        },
        {
            "title": "4.3 TRAINING",
            "content": "LLaVA-Mini follows the same training process as LLaVA, consisting of two stages. Stage 1: Vision-Language Pretraining In this stage, compression and modality pre-fusion modules are not yet applied (i.e., the 2 vision tokens remain unchanged). LLaVA-Mini learns to align vision and language representations using visual caption data. The training focuses solely on the projection module while the vision encoder and LLM remain frozen (Liu et al., 2023b). Stage 2: Instruction Tuning In this stage, LLaVA-Mini is trained to perform various visual tasks based on minimal vision tokens, using instruction data. Compression and modality pre-fusion are introduced to LLaVA-Mini, and all modules except the frozen vision encoder (i.e., the projection, compression, modality pre-fusion, and LLM backbone) are trained in an end-to-end manner."
        },
        {
            "title": "5.1 EXPERIMENTAL SETTING",
            "content": "Benchmarks We evaluate LLaVA-Mini on image and video understanding tasks. Experiments are conducted on 11 image benchmarks and 7 video benchmarks. Refer to Appendix for details. Baselines LLaVA-Mini is an image/video LMM, so we compare it with several advanced imagebased and video-based LMMs. Detailed description of baselines refer to Appendix D."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance on 11 image-based benchmarks. Res. is resolution and #Vision Tokens is the number of vision tokens fed to LLM backbone. * indicates that involving extra training data. Methods LLM Res. #Vision Tokens VQAv2 GQA VisWiz SciQA VQAT POPE MME MMB SEED LLaVAW MMVet Avg. (%) BLIP-2 InstructBLIP IDEFICS-9B IDEFICS-80B Qwen-VL Qwen-VL-Chat SPHINX SPHINX-2k mPLUG-Owl2 Video-LLaVA LLaVA-v1.5 MQT-LLaVA MQT-LLaVA MQT-LLaVA PruMerge PruMerge++ LLaMA-VID VoCo-LLaMA TokenPacker Vicuna-13B Vicuna-7B LLaMA-7B LLaMA-65B Qwen-7B Qwen-7B LLaMA-13B LLaMA-13B LLaMA-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B Vicuna-7B LLaVA-Mini Vicuna-7B compare to LLaVA-v1.5 LLaVA-Mini-HD Vicuna-7B LLaVA-Mini* (Image & Video) compare to LLaVA-v1.5 LLaMA-3.18B-Instruct 224 224 224 224 448 448 224 762 448 224 336 336 336 336 336 336 336 336 336 336 672 336 32 32 64 64 256 256 289 2890 1024 256 2 36 256 32 144 2 1 36 65.0 50.9 60.0 78.8 78.2 78.1 80.7 79.4 74.7 78.5 41.0 49.2 38.4 45.2 59.3 57.5 62.6 63.1 56.1 60.3 62.0 19.6 34.5 35.5 36.0 35.2 38.9 39.9 44.9 54.5 48.1 50.0 61.0 60.5 67.1 68.2 69.3 70.6 68.7 66.4 66.8 42.5 50.1 25.9 30.9 63.8 61.5 51.6 61.2 54.3 51.8 58. LMMs with fewer vision tokens 56.0 57.1 49.0 65.0 66.8 67.6 68.5 68.3 68.8 65.4 50.8 58.8 61.6 55.5 57.0 59.6 48.5 51.0 53.1 50.2 61.0 73.7 76.8 72.0 76.8 72.3 75.0 Ours 1 0.17% 64 11.1% 1 77.6 -0.9 78.9 +0.4 79.0 60.9 -1.1 61.8 -0.2 61. 56.2 +6.1 58.5 +8.5 70.4 +3.6 69.7 +2.9 57.0 -1.3 59.1 +0.9 57.4 83.1 58. 85.3 80.7 87.2 - 84.4 85.9 74.5 81.9 84.4 76.3 84.0 83.1 81.4 86.2 84.4 -1.5 85.3 -0.6 85.3 1293.8 1487.5 1476.1 1470.6 1450.2 - 1510.7 1144.0 1416.3 1434.5 1350.3 1462.4 1323.3 1466.0 -44.7 1476.8 -33.9 36.0 48.2 54.5 38.2 60.6 66.9 65.9 64.5 60.9 64.3 54.4 63.4 64.3 60.9 64.9 58.8 62.8 65.6 +1.3 67.5 +3.2 46.4 53.4 56.3 58.2 56.2 57.9 57.8 - 58.6 53.7 58.5 -0.1 60.2 +1.6 38.1 60.9 73.5 76.9 - 73.1 63.4 41.7 59.6 64.6 68.9 +5.5 69.3 +5.9 22.4 26.2 36.0 40.2 36.2 32.0 30.5 19.5 27.8 29.8 29. 36.6 +6.1 33.9 +3.4 - - 56.0 59.0 - - 56.3 57.9 +1.6 58.6 +2.4 1522.7 71. 63.0 70.2 37.2 60.7 Table 2: Performance on video-based open-ended generative benchmarks. We report accuracy (%) for question-answer, and scores (1-5, higher is better) for question-answer and generative performance. Results marked with bold and underlined indicate the best and second best, respectively. Methods #Frames #Vision Tokens per Frame Video-based Question-Answer Video-based Generative Performance MSVD-QA MSRVTT-QA ActivityNet-QA Score Acc. Score Score Acc. Acc. Correctness Detail Contextual Temporal Consistency Avg. LLaMA Adapter VideoChat Video-LLaMA Video-ChatGPT BT-Adapter MovieChat LLaMA-VID Video-LLaVA LLaVA-Mini 5 16 16 100 100 2048 1fps 8 1fps 256 32 64 3.6 2.6 32 2 256 1 54.9 56.3 51.6 64.9 67.5 75.2 69.7 70.7 70. 3.1 2.8 2.5 3.3 3.7 3.8 3.7 3.9 4.0 43.8 45.0 29.6 49.3 57.0 52.7 57.7 59.2 59.5 2.7 2.5 1.8 2.8 3.2 2.6 3.2 3.5 3. 34.2 26.5 12.4 35.2 45.7 45.7 47.4 45.3 53.5 2.7 2.2 1.1 2.7 3.2 3.4 3.3 3.3 3.5 2.03 2.23 1.96 2.40 2.68 2.76 2.96 2.87 2. 2.32 2.50 2.18 2.52 2.69 2.93 3.00 2.94 2.99 2.30 2.53 2.16 2.62 3.27 3.01 3.53 3.44 3.61 1.98 1.94 1.82 1.98 2.34 2.24 2.46 2.45 2. 2.15 2.24 1.79 2.37 2.46 2.42 2.51 2.51 2.67 2.19 2.30 1.99 2.37 2.69 2.65 2.88 2.84 2.94 Configuration For fair comparison, LLaVA-Mini employs the same configurations as LLaVAv1.5 (Liu et al., 2023b), using the CLIP ViT-L/336px (Radford et al., 2021) as the vision encoder and Vicuna-v1.5-7B (Chiang et al., 2023) as the LLM backbone. The compressed hyperparameter is set to 1, meaning vision tokens are compressed to one token. The number of modality prefusion layers Nf usion is set to 4. LLaVA-Mini uses the same training data as LLaVA-v1.5 (Liu et al., 2023b), using 558K caption data for pretraining and 665K instruction data for instruction tuning. The high-resolution version with 672*672 pixels (refer to Sec.4.2) is denoted as LLaVAMini-HD. To capture more visual details, the compressed hyperparameter of LLaVA-Mini-HD is set to 8, i.e., compressing to 64 vision tokens. For video processing, LLaVA-Mini extracts 1 frame per second (1 fps) from the video and sets = 1 to represent each frame with one vision token. To further explore the potential of LLaVA-Mini, we introduce variant that uses the CLIP ViTL/336px (Radford et al., 2021) as vision encoder and the advanced LLaMA-3.1-8B-Instruct (Dubey et al., 2024) as LLM backbone. During instruction tuning, we combine 665K image instruction data from LLaVA (Liu et al., 2023b), 100K video instruction data from Video-ChatGPT (Maaz et al., 2024), and part of open-source data (Li et al., 2024a), resulting in 3 million training samples. LLaVA-Mini is trained using 8 NVIDIA A800 GPUs. Training details are provided in Appendix B."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Image-based Evaluation We compare LLaVA-Mini with LLaVA-v1.5 across 11 benchmarks to thoroughly assess the performance of LLaVA-Mini with minimal vision tokens. The results are"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance on MVBench (accuracy). Detailed scores are reported in Appendix H. Methods Action Object Position Scene Count Attribute Pose Character Cognition Avg. mPLUG-Owl Video-ChatGPT Video-LLaMA VideoChat LLaMA-VID Video-LLaVA LLaVA-Mini 28.4 32.1 34.4 38.0 43.4 48.0 52.1 33.0 40.7 42.2 41.2 36.7 46.5 43.2 25.0 21.5 22.5 26.3 39.8 27.8 31. 29.0 31.0 43.0 48.5 22.0 84.5 85.5 29.3 28.0 28.3 27.8 36.5 35.5 37.5 42.0 44.0 39.0 44.3 37.3 45.8 44. 24.0 29.0 32.5 26.5 37.5 34.0 29.5 31.0 33.0 40.0 41.0 34.0 42.5 52.0 25.3 30.3 29.3 27.7 60.5 34.2 35. 29.7 32.7 34.1 35.5 41.4 43.1 44.5 Table 4: Results on MLVU (accuracy) of long video understanding. Evaluation includes Topic Reasoning (TR), Anomaly Recognition (AR), Needle QA (NQA), Ego Reasoning (ER), Plot QA (PQA), Action Order (AO), and Action Count (AC). Methods #Frames Avg. Video Duration (minute) Max Video Duration (minute) Video-ChatGPT MovieChat Movie-LLM TimeChat LLaMA-VID MA-LMM LLaVA-Mini 100 2048 1fps 96 1fps 1000 1fps Holistic Single Detail Multi Detail TR AR NQA ER PQA AO AC 7 20 26.9 29.5 30.0 23.1 50.8 51.9 10 24.0 25.0 29.0 27.0 34.5 35.5 14 139 40.3 24.2 29.6 24.5 30.1 43.1 10 20 42.0 24.7 24.7 28.4 32.7 38.9 8 29.9 25.8 24.1 25.8 32.5 35.8 16 137 25.1 28.6 20.5 24.7 23.9 25.1 13 130 31.1 22.8 24.8 32.0 27.8 24.3 76. 50.0 44.5 37.5 49.0 24.3 18. Avg. 11 143 31.3 25.8 26.1 30.9 33.2 36.4 42.8 5:"
        },
        {
            "title": "Results",
            "content": "on Table EgoSchema (accuracy), long-form video benchmark ( 3 minutes) for first-person view temporal reasoning. Methods Random mPLUG-Owl InternVideo Video-ChatGPT VideoChat TimeChat LLaMA-VID Video-LLaVA LLaVA-Mini #Frames EgoSchema - 16 16 100 16 96 1fps 8 1fps 20 31.1 32.1 36.2 42.2 33.0 38.5 38.4 51. reported in Table 1, where LLaVA-Mini achieves performance comparable to LLaVA-v1.5 while using only 1 vision token instead of 576. Previous efficient LMMs with fewer vision tokens often merged similar tokens directly after the vision encoder (Shang et al., 2024; Ye et al., 2024c), resulting in significant loss of visual information and negatively impacting visual understanding of LMMs. For instance, LLaMA-VID, VoCo-LLaVA, and MQT-LLaVA, which reduce vision tokens to 1-2 tokens, lead to 5% performance drop on average. In contrast, LLaVA-Mini employs modality pre-fusion to integrate visual information into text tokens before compressing the vision tokens, achieving performance comparable to LLaVA-v1.5 at token compression rate of 0.17%. Furthermore, LLaVA-Mini-HD shows an average performance improvement of 2.4% over LLaVA-v1.5 due to high-resolution image modeling. Note that LLaVA-Mini-HD has computational load of 8.13 TFLOPs, which remains lower than LLaVA-v1.5s 8.55 TFLOPs. More efficiency analyses refer to Sec.5.3. Overall, LLaVA-Mini preserves strong visual understanding capabilities while compressing vision tokens, enhancing the usability of efficient LMMs in visual scenarios. Video-based Evaluation We compare LLaVA-Mini with advanced video LMMs on 5 widely used video-based benchmarks. The results are reported in Table 2 and 3, where LLaVA-Mini demonstrates superior overall performance. Video LMMs such as VideoChat (Li et al., 2024c), VideoLLaVA (Lin et al., 2023a), and Video-LLaMA (Maaz et al., 2024) use much more vision tokens to represent each frame, and thereby can extract only 8-16 frames from video due to the limited context length of LLMs, which may result in the loss of visual information in some frames. In contrast, LLaVA-Mini uses one vision token to represent each image and accordingly can extract frames from the video at rate of 1 frame per second, thus performing better on video understanding. Extrapolation to Long Videos Furthermore, we compare LLaVA-Mini with advanced long-video LMMs (can process video over 100 frames) on long-form video benchmarks, MLVU (Zhou et al., 2024b) and EgoSchema (Mangalam et al., 2023). Note that LLaVA-Mini is trained only on VideoChatGPT instruction data and has not been exposed to any long video data, so its performance on long videos is entirely derived from the length extrapolation capabilities of its framework (Press et al., 2022). As shown in Table 4 and 5, LLaVA-Mini exhibits significant advantages in long video understanding. By representing each frame as one token, LLaVA-Mini facilitates straightforward extension to longer videos during inference. In particular, LLaVA-Mini is only trained on videos shorter than 1 minute (< 60 frames), and performs well on MLVUs long-form video, which encompasses videos over 2 hours (> 7200 frames) during inference. Overall, with one vision token per frame, LLaVA-Mini demonstrates high-quality video understanding in more efficient manner."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: FLOPs and latency of LLaVA-Mini. Figure 8: FLOPs and latency of LLaVA-Mini-HD. Figure 9: VRAM usage (3hour video) of LLaVA-Mini. Table 6: Performance of LLaVA-Mini with different numbers of modality pre-fusion layers Nf usion. Table 7: Performance of LLaVA-Mini with various vision tokens. Methods LLaVA-v1.5 LLaVA-Mini (w/o pre-fusion) LLaVA-Mini (w/ pre-fusion) Pre-fusion #Layers #Vision Tokens FLOPs (T) Performance VQAv2 GQA MMB Methods Res. #Vision Tokens Performance VQAv2 GQA MMB - 0 0 0 0 1 2 3 4 576 1 16 64 144 1 1 1 1 8. 0.96 1.16 1.79 2.85 1.21 1.46 1.81 1.96 78.5 72.4 74.1 75.3 76.9 74.8 76.0 76.9 77.6 62. 54.2 55.4 56.7 58.9 55.5 57.6 59.1 60.9 64.3 57.7 59.2 62.1 64.9 60.4 63.1 64.9 65.6 LLaVA-v1. LLaVA-Mini 336 336 336 336 336 672 672 672 672 576 1 4 16 16 64 144 576 78.5 77.6 77.7 78.1 78.5 78.5 78.9 79.3 80.0 62.0 60.9 60.9 61.3 61. 61.5 61.8 62.3 62.9 64.3 65.6 66.7 66.6 67.5 67.4 67.5 67.9 68."
        },
        {
            "title": "5.3 EFFICIENCY",
            "content": "With the performance comparable to LLaVA-v1.5, we further explore the computational efficiency offered by LLaVA-Mini. Figures 7, 8, 9 illustrate the advantages of LLaVA-Mini in terms of computational load, inference latency, and memory usage, where FLOPs are calculated by calflops (Ye, 2023), and latency is tested on the A100 without any engineering acceleration techniques. FLOPs and Inference Latency As shown in Figure 7, LLaVA-Mini significantly reduces computational load by 77% compared to LLaVA-v1.5, achieving speedup of 2.9 times. LLaVA-Mini achieves response latency lower than 40 ms, which is crucial for developing low-latency real-time LMMs. As shown in Figure 8, when modeling at high resolutions, the efficiency advantages of LLaVA-Mini are even more pronounced, yielding 82% FLOPs reduction and 3.76 times speedup. Memory Usage Memory consumption poses another challenge for LMMs, particularly in video processing. Figure 9 demonstrates the memory requirements of LMMs when processing videos of varying lengths. In previous methods, each image requires approximately 200-358 MB memory (Liu et al., 2023b; Lin et al., 2023a), limiting them to handle only about 100 frames on 40GB GPU. In contrast, LLaVA-Mini with one vision token requires just 0.6 MB per image, enabling it to theoretically support video processing of over 10,000 frames on RTX 3090 with 24 GB of memory."
        },
        {
            "title": "6.1 SUPERIORITY OF MODALITY PRE-FUSION",
            "content": "The proposed modality pre-fusion is central to LLaVA-Mini, as it integrates visual information into text tokens in advance, facilitating extreme compression of vision tokens. To investigate the effects of modality pre-fusion, we conduct an ablation study in Table 6. Without pre-fusion, token compression results in performance drop of around 5%, even with 144 vision tokens retained, the performance of LMMs falls short of LLaVA-v1.5. This also explains why previous token merging methods often exhibit poor performance (Ye et al., 2024c) or can only achieve compression rate of over 40% (Shang et al., 2024). Notably, under the same FLOPs, increasing the number of pre-fusion layers yields greater benefits than increasing the number of compression vision tokens. This sup-"
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Case of image understanding. Figure 11: Case of video understanding. ports our preliminary analysis, which indicated that vision tokens exhibit varying importance across different layers and vision tokens are more critical in early layers. Investing more computational overhead in earlier stages where vision tokens are more important results in better performance."
        },
        {
            "title": "6.2 EFFECT OF COMPRESSION",
            "content": "Table 8: Effect of query-based compression. LLaVA-Mini employs query-based compression to achieve high compression ratio for vision tokens. We compare the performance of query-based compression with direct average pooling in Table 8. Query-based compression can adaptively capture important features in the image while requiring only minimal additional computational cost, demonstrating significant advantage. Appendix gives visualization of the compression process and more detailed analysis. Average Pooling Query-based Average Pooling Query-based #Vision Tokens 1.96T +2.42G 2.01T +2.44G"
        },
        {
            "title": "Compression",
            "content": "76.1 77.6 76.9 77.7 59.8 60.9 60.3 60."
        },
        {
            "title": "FLOPs",
            "content": "Performance VQAv2 GQA MMB 64.0 65.6 65.1 66.7"
        },
        {
            "title": "6.3 PERFORMANCE WITH VARIOUS VISION TOKENS",
            "content": "LLaVA-Mini uses 1 vision token for standard images and 64 for high-resolution images. We explore the potential of LLaVA-Mini when further increasing the number of vision tokens (larger C) in Table 7. The results indicate that as the number of vision tokens increases, LLaVA-Mini continues to improve in performance. In particular, LLaVA-Mini outperforms LLaVA-v1.5 when both using 576 vision tokens, demonstrating its effectiveness when computational resources are plentiful."
        },
        {
            "title": "6.4 CASE STUDY",
            "content": "Figures 10 and 11 present examples of LLaVA-Mini in image and video understanding tasks (refer to Appendix for more cases). Despite using only one vision token, LLaVA-Mini performs effectively in capturing visual details, such as accurately identifying price information (OCR) in website screenshots. For video understanding, Video-LLaVA extracts 8 frames per video, regardless of video duration (Lin et al., 2023a). Training on only 8 frames (sometimes missing key frames) can cause hallucinations (Khattak et al., 2024), encouraging LMM to forge information beyond the extracted frames. For instance, given celebration scene, Video-LLaVA mistakenly imagines group of men playing soccer on field before the celebration. This fixed-length frame extraction is forced compromise due to the large number of vision tokens required per image while LLMs context length is limited. In contrast, LLaVA-Mini, utilizing just one vision token per frame, can process videos at 1 fps, resulting in more robust video understanding. Overall, LLaVA-Mini ensures strong visual understanding while enhancing efficiency, making it practical solution for multimodal interaction."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. LLaVA-Mini excels in image and video understanding while exhibiting superiority in computational efficiency, inference latency, and memory usage, facilitating the real-time multimodal interaction with efficient LMMs."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster, 2023. URL https://arxiv.org/abs/ 2210.09461. Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 190200, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https: //aclanthology.org/P11-1020. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur"
        },
        {
            "title": "Preprint",
            "content": "C elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Tim-"
        },
        {
            "title": "Preprint",
            "content": "othy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Mousi: Poly-visual-expert vision-language models, 2024. URL https: //arxiv.org/abs/2401.17221. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv.org/abs/ 2306.13394. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern understanding. Recognition (CVPR), 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners, 2021. URL https://arxiv.org/abs/2111. 06377. Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang. Matryoshka query transformer for large vision-language models, 2024. URL https://arxiv. org/abs/2405.19315. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Jameel Hassan, Muzammal Naseer, Federico Tombari, Fahad Shahbaz Khan, and Salman Khan. How good is my video lmm? complex video reasoning and robustness evaluation suite for video-lmms, 2024. URL https: //arxiv.org/abs/2405.03690. Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, et al. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https://huggingface. co/blog/idefics. Accessed, pp. 0918, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024a. URL https: //arxiv.org/abs/2408.03326. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1329913308, June 2024b."
        },
        {
            "title": "Preprint",
            "content": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1973019742. PMLR, 2329 Jul 2023a. URL https://proceedings.mlr.press/v202/li23q.html. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024c. URL https://arxiv. org/abs/2305.06355. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2219522206, June 2024d. Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm, 2024e. URL https://arxiv. org/abs/2407.02392. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023b. URL https://arxiv.org/abs/2311.17043. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023c. URL https://openreview.net/forum?id= xozJw0kZXF. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection, 2023a. URL https://arxiv. org/abs/2311.10122. Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023b. URL https://arxiv.org/abs/2311.07575. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3489234916. Curran Associates, Inc., 2023a. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3489234916. Curran Associates, Inc., 2023b. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, June 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas Li, and Ge Li. One for all: Video conversation is feasible without video instruction tuning. arXiv preprint arXiv:2309.15785, 2023c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024c. URL https://arxiv.org/abs/2307.06281."
        },
        {
            "title": "Preprint",
            "content": "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Learn to explain: Multimodal reaOyvind Tafjord, Peter Clark, and Ashwin Kalyan. soning via thought chains for science question answering. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 25072521. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models, 2024. URL https: //arxiv.org/abs/2306.05424. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnosIn Thirty-seventh Confertic benchmark for very long-form video language understanding. ence on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=JVlWseddak. OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt. OpenAI. Gpt-4 technical report, 2023. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, URL https://proceedings.mlr.press/v139/radford21a. 1824 Jul 2021. html. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. ArXiv, abs/2312.02051, 2023. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models, 2024. URL https://arxiv.org/abs/ 2403.15388. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 18221 18232, June 2024a. Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing long video understanding with ai-generated movies, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a."
        },
        {
            "title": "Preprint",
            "content": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. An-Lan Wang, Bin Shan, Wei Shi, Kun-Yu Lin, Xiang Fei, Guozhi Tang, Lei Liao, Jingqun Tang, Can Huang, and Wei-Shi Zheng. Pargo: Bridging vision-language with partial and global views, 2024. URL https://arxiv.org/abs/2408.12928. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning, 2022. URL https://arxiv.org/abs/2212.03191. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. URL https://arxiv.org/abs/2309.17453. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl: Modularization empowers large language models with multimodality, 2024a. URL https://arxiv.org/abs/2304.14178. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1304013051, June 2024b. Xiaoju Ye. calflops: flops and params calculate tool for neural networks in pytorch framework, 2023. URL https://github.com/MrYxJ/calculate-flops.pytorch. Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. Voco-llama: Towards vision compression with large language models, 2024c. URL https://arxiv.org/ abs/2406.12275. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. URL https://arxiv.org/abs/2308.02490. Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, and Lichao Sun. TinyGPT-v: EfIn 2nd Workshop on Adficient multimodal large language model via small backbones. vancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ICML 2024), 2024a. URL https://openreview.net/forum?id= lvmjTZQhRk."
        },
        {
            "title": "Preprint",
            "content": "Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. Llm inference unveiled: Survey and roofline model insights, 2024b. URL https://arxiv.org/ abs/2402.16363. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023. URL https://arxiv.org/abs/2306.02858. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zeroinit attention, 2024. URL https://arxiv.org/abs/2303.16199. Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models, 2024a. URL https://arxiv.org/ abs/2402.14289. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024b. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing In The Twelfth Internavision-language understanding with advanced large language models. tional Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=1tZbq88f27."
        },
        {
            "title": "A DETAILED SETTING OF PRELIMINARY ANALYSES",
            "content": "In Sec.3.2, we analyze the importance of visual tokens in LMMs from an attention-based perspective to inform strategies for compressing vision tokens. Here, we give detailed introduction of the experimental setup for the attention analysis. We focus on the LLaVA series architecture, where the input tokens to the LLM are composed of instruction tokens, vision tokens, and response tokens, as shown in Eq.(1). We compute the average attention received by each type of token to reveal how the importance of different token categories changes across layers. Calculation of Attention Weights Formally, we denote the attention of the ith token hi to the jth token hj as aij, where aij is the average attention across all attention heads. All tokens fed to the LLM are divided into instruction tokens, vision tokens, and response tokens according to inputs type, denoted as sets Tinstruction, Tvision, and Tresponse respectively. Finally, denoted the target and source token types as tgt type, src type {instruction, vision, response}, the average attention weights from tgt type type tokens to src type type tokens in our analyses are calculated as: Attn(tgt type src type) = (cid:80) where 1(cid:80) hj Tsrc type aij >0 = (cid:40) (cid:80) hiTtgt type 1(cid:80) hiTtgt type if (cid:80) otherwise 1 0 (cid:80) hj Tsrc type aij , hj Tsrc type aij >0 hj Tsrc type aij > 0 (4) (5) (cid:80) hiTtgt type hj Tsrc type Specifically, (cid:80) aij calculates the sum of attention weights from all tgt type type tokens to all src type type tokens, (cid:80) aij >0 counts the number of tgt type type tokens, thus Attn(tgt type src type) represents the average attention weight from tgt type type tokens to src type type tokens. Attn(tgt type src type) is consistent with the legend in Figure 2. hiTtgt type hj Tsrc type 1(cid:80) Calculation of Attention Entropy The calculation of attention entropy is similar to that of attention weights, with the key difference being the addition of normalization step. When computing the entropy of specific type of token (e.g., vision tokens), the sum of attention weights for this token type may not equal 1. Thus, we perform normalization on the attention of these tokens (e.g., vision tokens) to ensure the definition of entropy is satisfied. In practice, for LLaVA-v1.5 (pad) (Liu et al., 2023b) and LLaVA-NeXT (anyres) (Liu et al., 2024b), which may involve different resolution vision inputs, we use their original settings. In our analysis, we do not further distinguish between different types of vision tokens (e.g., global or local), but treat them collectively as vision tokens."
        },
        {
            "title": "B TRAINING DETAILS",
            "content": "Implementation Details The compression method of LLaVA-Mini can be easily plugged into existing multi-modal pipelines, as it only requires the addition of two extra modules (the compression module and the modality pre-fusion module) before the LLM, while the other components (such as the vision encoder, the LLM, and the training loss) remain unchanged. The pre-fusion module applies the same decoder-only architecture as the LLM, including both the structure and hyperparameters. The motivation behind this setting is to ensure flexible adaptation to existing LLM frameworks and other acceleration techniques. Training The overall training process follows two-stage paradigm similar to LLaVA, consisting of vision-language pretraining followed by instruction tuning. Table 9 reports the two-stage training details of LLaVA-Mini."
        },
        {
            "title": "C BENCHMARKS",
            "content": "We conduct comprehensive evaluation of LLaVA-Mini, including both image and video understanding benchmarks."
        },
        {
            "title": "Hyperparameters",
            "content": "Table 9: Training details of LLaVA-Mini. Stage1 Vision-Language Pretraining Stage2 Instruction Turning Vision Encoder Projection Large Language Model Compression Modality Pre-fusion"
        },
        {
            "title": "Batch Size\nLearning Rate\nMM Learning Rate\nSchedule\nWarmup Ratio\nOptimizer\nEpoch",
            "content": "Frozen Trainable Frozen N/A N/A 256 - 1e-3 1 Cosine decay 0.03 AdamW"
        },
        {
            "title": "Frozen\nTrainable\nTrainable\nTrainable\nTrainable",
            "content": "256 1e-4 1e-5 2 C.1 IMAGE-BASED BENCHMARKS Following the LLaVA framework (Liu et al., 2023b), we conduct experiments on 11 widely adopted benchmarks, including VQA-v2 (VQAv2) (Goyal et al., 2017), GQA (Hudson & Manning, 2019), VisWiz (Gurari et al., 2018), ScienceQA-IMG (SciQA) (Lu et al., 2022), TextVQA (VQAT) (Singh et al., 2019), POPE (Li et al., 2023c), MME (Fu et al., 2024), MMBench (MMB) (Liu et al., 2024c), SEED-Bench (SEED) (Li et al., 2024b), LLaVA-Bench-in-the-Wild (LLaVAW) (Liu et al., 2023a), and MM-Vet (Yu et al., 2023), which cover diverse range of visual tasks. The evaluation pipelines for all benchmarks are consistent with those used in LLaVA. C.2 VIDEO-BASED BENCHMARKS Video-based Generative Performance Benchmark For video-based evaluation, we conduct experiments on video open-ended question-answering benchmarks, including MSVD-QA (Chen & Dolan, 2011), MSRVTT-QA (Xu et al., 2016), and ActivityNet-QA (Caba Heilbron et al., 2015). Furthermore, we use the video-based generative performance benchmark (Maaz et al., 2024) to assess the performance of LLaVA-Mini across five dimensions: correctness, detail orientation, contextual understanding, temporal understanding, and consistency. The evaluation pipelines for both the open-ended question-answering and the generative performance benchmarks adhere to VideoChatGPT (Maaz et al., 2024), employing the GPT model (gpt-3.5-turbo version) to evaluate the accuracy of responses (True or False) and to assign score ranging from 1 to 5 for response, where higher scores indicate superior performance. MVBench (Li et al., 2024d) MVBench is comprehensive benchmark for multimodal video understanding that encompasses 20 challenging tasks. The evaluation aspects of MVBench include Action (such as Action Sequence, Action Prediction, Action Antonym, Fine-grained Action, and Unexpected Action), Object (Object Existence, Object Interaction, Object Shuffle), Position (Moving Direction, Action Localization), Scene (Scene Transition), Count (Action Count, Moving Count), Attribute (Moving Attribute, State Change), Pose (Fine-grained Pose), Character (Character Order), and Cognition (Egocentric Navigation, Episodic Reasoning, Counterfactual Inference). The evaluation of MVBench employs multiple-choice format, using accuracy as the metric. MLVU (Zhou et al., 2024b) MLVU is comprehensive benchmark for multi-task long video understanding. The evaluation aspects of MLVU include Topic Reasoning (TR), Anomaly Recognition (AR), Needle QA (NQA), Ego Reasoning (ER), Plot QA (PQA), Action Order (AO), and Action Count (AC). The evaluation of MLVU also employs multiple-choice format, using accuracy as the metric. EgoSchema (Mangalam et al., 2023) EgoSchema is long-form video question-answering dataset, which serves as benchmark for assessing the long video understanding capabilities of first-person videos. The evaluation of EgoSchema also employs multiple-choice format, using accuracy as the metric."
        },
        {
            "title": "D INTRODUCTION TO BASELINES",
            "content": "LLaVA-Mini is an image and video LMM, so we compare it with several advanced image-based and video-based LMMs. D.1 IMAGE-BASED LMMS We compare LLaVA-Mini with LLaVA-v1.5 (Liu et al., 2023b) and other advanced LMMs of similar data and model scales, including BLIP-2 (Li et al., 2023a), InstructBLIP (Liu et al., 2024a), IDEFICS (Laurencon et al., 2023), Qwen-VL (Bai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), SPHINX (Lin et al., 2023b), mPLUG-Owl2 (Ye et al., 2024b). LMMs with Fewer Vision Tokens Additionally, we assess LLaVA-Mini against various efficient LMMs that utilize fewer vision tokens, showing advantages in compression rate and performance. Most of these models share the same architecture and training data as LLaVA, primarily focusing on the merging of vision tokens in the vision encoder. These efficient LMMs are introduced as follows. MQT-LLaVA (Hu et al., 2024) introduces flexible query transformer that allows encoding an image into variable number of visual tokens (up to predefined maximum) to adapt to different tasks and computational resources. PruMerge (Shang et al., 2024) reduces visual tokens in LMMs by identifying and merging important tokens based on the attention sparsity in vision encoder. PruMerge has variant, named PruMerge++, which enhances the original PruMerge method by evenly adding more vision tokens (about 144 vision tokens) to further improve performance. LLaMA-VID (Li et al., 2023b) LLaMA-VID compresses the instruction and image into one token respectively, with total of two tokens representing each image, thus facilitating the understanding of longer videos. VoCo-LLaMA (Ye et al., 2024c) compresses all vision tokens using language models, significantly improving computational efficiency. TokenPacker (Li et al., 2024e) is visual projector that efficiently reduces visual tokens by 80% using coarse-to-fine approach. Previous methods have often focused on reducing the number of vision tokens output by the vision encoder. LLaVA-Mini takes this step further by shifting attention to how vision tokens and text tokens interact within the LLM backbone. Based on this insight, we propose modality pre-fusion, which enables better performance even under the extreme compression of reducing vision tokens to just one token. D.2 VIDEO-BASED LMMS LLaVA-Mini can also perform high-quality video understanding, so we compare LLaVA-Mini with the current advanced video LMMs, including LLaMA-Adaptor (Zhang et al., 2024), InternVideo (Wang et al., 2022), VideoChat (Li et al., 2024c), Video-LLaMA (Zhang et al., 2023), mPLUG-Owl (Ye et al., 2024a), Video-ChatGPT (Maaz et al., 2024), BT-Adapor (Liu et al., 2023c), LLaMA-VID (Li et al., 2023b), and Video-LLaVA (Lin et al., 2023a). We also compare LLaVA-Mini with several video LMMs specifically designed for long videos, including MovieChat (Song et al., 2024a), Movie-LLM (Song et al., 2024b), TimeChat (Ren et al., 2023), MA-LMM (He et al., 2024). Note that among these video LMMs, LLaVA-Mini and VideoLLaVA can complete image and video understanding with unified model."
        },
        {
            "title": "E EXTENDED EXPERIMENTAL RESULTS",
            "content": "E.1 EFFECT OF COMPRESSION MODULE To verify the effectiveness of the compression module, we compared the compression module in LLaVA-Mini with previous advanced token merging methods. To ensure fair comparison of"
        },
        {
            "title": "Preprint",
            "content": "Table 10: Comparison of LLaVA-Mini with previous token merging methods."
        },
        {
            "title": "Methods",
            "content": "#Vision Tokens Performance VQAv2 GQA MMB MQT-LLaVA MQT-LLaVA MQT-LLaVA PruMerge PruMerge++ LLaVA-Mini LLaVA-Mini LLaVA-Mini LLaVA-Mini 2 36 256 32 144 1 16 64 61.0 73.7 76.8 72.0 76.8 72.4 74.1 75.3 76.9 50.8 58.8 61.6 - - 54.2 55.4 56.7 58.9 54.4 63.4 64.3 60.9 64.9 57.7 59.2 62.1 64. token compression performance, we remove the modality pre-fusion module from LLaVA-Mini for the comparison with SOTA token merging methods, including PruMerge (Shang et al., 2024), PruMerge++ (Shang et al., 2024), and MQT-LLaVA (Hu et al., 2024). Specifically, PruMerge applies the widely-used token merge (ToMe) technique (Bolya et al., 2023) on ViT, PruMerge++ improves upon PruMerge by uniformly sampling additional vision tokens, and MQT-LLaVA employs Matryoshka representation learning to compress vision tokens. As shown in Table 10, LLaVA-Minis compression module outperforms PruMerge, PruMerge++, and MQT-LLaVA at the same compression rate, showing the advantages of query-based compression. E.2 EFFECT OF MODALITY PRE-FUSION Table 11: Performance of LLaVA-Mini when using only pre-fusion module without compression."
        },
        {
            "title": "Methods",
            "content": "LLaVA-v1.5 LLaVA-Mini (w/o compression) #Vision Tokens 576 576 Performance VQAv2 GQA MMB 78.5 80.0 62.0 62.9 64.3 66. To validate the effect of the pre-fusion module, we remove the compression module and retained only the modality pre-fusion module, thereby comparing with LLaVA-v1.5 while both using 576 vision tokens. As shown in Table, when using only the pre-fusion module without compression, LLaVA-Mini achieves superior performance compared to LLaVA-v1.5 with both using 576 vision tokens, demonstrating the effectiveness of the pre-fusion module. E.3 WHY PREFORMING COMPRESSION AND PRE-FUSION OUTSIDE LLM BACKBONE? LLaVA-Mini performs compression and modality pre-fusion before the LLM backbone. The motivation for conducting these processes outside the LLM backbone, rather than conducting at the Lth layer within the LLM, stems from two key considerations: Vision representations after the Lth layers contain contextual information, which hinders the compression module: After the vision tokens are fed into the LLM, the early layers cause the visual representations to carry contextual information. Applying query-based compression on top of these representations makes it difficult for the compression module to distinguish between different vision tokens. The inter-layer operations within the LLM may not be compatible with existing acceleration frameworks: One of the main motivations for placing the compression and pre-fusion modules outside the LLM backbone in LLaVA-Mini is to keep the LLM backbone unchanged. This design allows for compatibility with nearly all existing LLM acceleration technologies and frameworks, further enhancing efficiency."
        },
        {
            "title": "Preprint",
            "content": "Table 12: Comparison of performing compression and pre-fusion outside or within LLM backbone."
        },
        {
            "title": "Methods",
            "content": "LLaVA-Mini LLaVA-Mini (perform compression and pre-fusion within LLM) #Vision Tokens FLOPs (T) 1 1.96 1.84 Performance VQAv2 GQA MMB 77.6 60.9 65. 76.3 60.1 64.5 We also conduct comparison between LLaVA-Mini and LLaVA-Mini (compression and pre-fusion within LLM) in Table 12. The results demonstrate that the configuration of LLaVA-Mini is more advantageous. We will incorporate this result and the architectural motivation into the manuscript as per your recommendation. E.4 EFFICIENCY ACROSS VARIOUS HARDWARE Table 13: Inference latency (millisecond) of LLaVA-Mini on various hardware platforms."
        },
        {
            "title": "Methods",
            "content": "#Vision Tokens RTX 3090 (24G) A100 (40G) A800 (80G) LLaVA-v1.5 LLaVA-Mini 576 1 4 16 64 198. 64.52 65.52 68.97 80.10 113.04 38.64 38.84 39.28 46.23 87.43 27.43 27.71 28.92 34.65 The efficiency improvements brought by LLaVA-Mini stem from reduced computational load (FLOPs), which is consistent across different hardware platforms. To demonstrate the scalability of model efficiency across different hardware platforms, we compute the inference latency of LLaVA-Mini on three hardware platforms: RTX 3090, A100, and A800. As shown in Table 13, the efficiency improvements brought by LLaVA-Mini are scalable across these hardware platforms. E.5 COMPUTATIONAL OVERHEAD OF EACH COMPONENT Table 14: Computational overhead (FLOPs) of each component in LLaVA-Mini."
        },
        {
            "title": "Methods",
            "content": "Res. LLaVA-v1.5 LLaVA-Mini LLaVA-v1.5 LLaVA-Mini 336 336"
        },
        {
            "title": "Vision Encoder",
            "content": "FLOPs (T) Projection Compression 0.349 0.349 1.745 1.745 0.024 0.024 0.121 0.121 - 0. - 0.009 Pre-fusion"
        },
        {
            "title": "LLM Total",
            "content": "- 0.125 - 1.183 8.177 1.460 38.623 4.131 8.55 1.96 40.49 7. LLaVA-Mini significantly reduces the computational load of LMMs by decreasing the number of vision tokens. To further study the proportion of computational load contributed by each component in LLaVA-Mini, we compute the FLOPs of each module, as shown in Table 14. The proposed compression module and pre-fusion module incur minimal computational cost, while the computation required by the LLM backbone is significantly reduced."
        },
        {
            "title": "F VISUALIZATION OF COMPRESSION",
            "content": "LLaVA-Mini introduces query-based compression to adaptively compress vision tokens while preserving essential information. The learnable queries in compression module interact with all vision tokens through cross-attention to capture key visual information. To verify the effectiveness of the proposed compression, Figure 12 visualizes the cross-attention during the compression process. Across various image types and styles (e.g., photographs, text, screenshots, and cartoons),"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Visualization of the cross-attention in the compression module introduced in LLaVAMini. The left side is the original image, and the right side is the cross-attention distribution heat map, where brighter areas are more heavily weighted during compression. The example images are all from the LLaVA-Bench-in-the-Wild benchmark. LLaVA-Minis compression exhibits strong interpretability, effectively extracting key visual information from images. In cases where critical information is concentrated (such as (b), (d), (h), (i) in Figure 12), LLaVA-Mini focuses on these key locations. Conversely, in cases where the main object is unclear. (such as (f), (j), (i), (m) in Figure 12), LLaVA-Mini exhibits more dispersed attention pattern during the compression process, thereby preserving broader range of visual information. In particular, for complex image like Figure 12(k), which contain multiple sub-figures with logical relationships, the proposed compression module adaptively pay attention to the VGA-shaped charger, the product name on the charger packaging, and the charging port of the charger, demonstrating the superiority of the proposed compression. Overall, compared to compression based on average pooling, query-based compression allows LLaVA-Mini to adaptively assign greater weight to key information, effectively retaining important visual details after compression."
        },
        {
            "title": "G MORE CASES",
            "content": "Image Understanding Figure 13 illustrates an example of LLaVA-Minis capabilities in more complex image reasoning. The image in Figure 13 incorporates features such as metaphor and counterfactual reasoning, requiring LMMs to accurately interpret the visual information and reason about the humorous aspects based on the entities present in the scene. The results demonstrate that only LLaVA-Mini and GPT-4o successfully capture the phrases the musicians are performing on giant violin and The violin, being large and buoyant object, is placed on the water, while both Video-LLaVA and LLaVA-v1.5 fail to understand this image. In terms of the perception of"
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Example demonstrating LLaVA-Minis image understanding capability on more complex image reasoning. Output marked in red indicates factual errors. entities in the picture, both Video-LLaVA and LLaVA-v1.5 exhibit hallucinations in their descriptions. Specifically, Video-LLaVA erroneously interprets the image as The image is panel of four pictures and the man is playing guitar, while LLaVA-v1.5 fails to recognize the presence of the violin entirely. Figure 14 illustrates an example of LLaVA-Minis capabilities in more challenging OCR task. The text in the image is presented in an unusual cursive handwriting style, which can significantly hinder the recognition quality of LMMs. For this challenging OCR case, both LLaVA-Mini and GPT-"
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Example demonstrating LLaVA-Minis image understanding capability on more challenging OCR task. Output marked in red indicates recognition errors. Figure 15: Example demonstrating LLaVA-Minis video understanding capability on first-person view video. Output marked in red indicates factual errors."
        },
        {
            "title": "Preprint",
            "content": "4o accurately identify the text in the image, particularly with LLaVA-Mini using only one vision token. In contrast, Video-LLaVA and LLaVA-v1.5 incorrectly recognize Duke and wedding, and erroneously add an exclamation mark ! at the end. Overall, LLaVA-Mini demonstrates superior performance in perceiving and reasoning about visual information. Video Understanding Figure 15 illustrates an example of LLaVA-Minis capabilities in processing longer first-person video. The results show that LLaVA-Mini exhibits more comprehensive and detailed understanding of the video, effectively capturing entities in the room, such as the yoga mat. In contrast, Video-LLaVA mistakenly imagines he takes shower due to its limitation of extracting only 8 frames from the video. Video-ChatGPT provides much shorter responses, lacking some detailed information. Overall, LLaVA-Mini exhibits superior understanding of the video."
        },
        {
            "title": "H DETAILED RESULTS ON MVBENCH",
            "content": "Table 15 reports the detailed results on each subset of MVBench, corresponding to Table 3. Table 15: Detailed results on 20 subsets of MVBench."
        },
        {
            "title": "Temporal",
            "content": "mPLUGOwl VideoChatGPT VideoLLaMA VideoChat LLaMA-"
        },
        {
            "title": "VID",
            "content": "41.4 63.5 42.0 26.5 43.0 42.0 39.0 34.5 36.5 44.0 35.5 22.0 44.5 28. 19.0 55.6 37.5 34.0 84.5 40.5 56.5 VideoLLaVA LLaVAMini 43.1 44.5 50.0 49.0 42.0 54.5 52.5 46.5 40.5 27.0 28.5 84.5 44.5 26. 53.0 38.5 34.0 42.5 32.5 38.0 32.0 44.5 44.5 44.5 76.0 37.0 58. 50.0 50.0 29.5 31.0 32.5 85.5 35.0 40.0 48.0 41.0 29. 52.0 31.0 38.0 36."
        },
        {
            "title": "Position",
            "content": "Action Sequence Action Prediction Action Antonym Fine-grained Action Unexpected Action Object Existence Object Interaction Object Shuffle Moving Direction Action Localization"
        },
        {
            "title": "Scene",
            "content": "Scene Transition"
        },
        {
            "title": "Count",
            "content": "Action Count Moving Count"
        },
        {
            "title": "Attribute",
            "content": "Moving Attribute"
        },
        {
            "title": "Pose",
            "content": "State Change Fine-grained Pose"
        },
        {
            "title": "Character",
            "content": "Character Order"
        },
        {
            "title": "Cognition",
            "content": "Egocentric Navigation Episodic Reasoning Counterfactual Inference 29.7 22.0 28.0 34.0 29.0 29.0 40.5 27.0 31.5 27.0 23.0 29. 31.5 27.0 40.0 44.0 24.0 31.0 26.0 20.5 29.5 32. 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 39. 48.5 29.0 33.0 29.5 26.0 35.5 34.1 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38. 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40. 30.0 21.0 37.0 35.5 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48. 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36."
        }
    ],
    "affiliations": [
        "Key Laboratory of AI Safety, Chinese Academy of Sciences",
        "Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}