{
    "paper_title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
    "authors": [
        "Xuanhua He",
        "Tianyu Yang",
        "Ke Cao",
        "Ruiqi Wu",
        "Cheng Meng",
        "Yong Zhang",
        "Zhuoliang Kang",
        "Xiaoming Wei",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 1 6 0 2 . 2 1 5 2 : r Active Intelligence in Video Avatars via Closed-loop World Modeling Xuanhua He1, Yong Zhang2, Tianyu Yang2, Ke Cao3 Ruiqi Wu2 Cheng Meng2 Zhuoliang Kang2 Xiaoming Wei2 Qifeng Chen1, 1The Hong Kong University of Science and Technology 2Meituan 3University of Science and Technology of China https://xuanhuahe.github.io/ORCA/"
        },
        {
            "title": "Abstract",
            "content": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agencythey cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), task and benchmark for evaluating goaldirected planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior. 1. Introduction The generation and control of video avatars [13, 14, 26] represents frontier in computer vision, aiming to create virtual agents that exhibit human-like intelligence in their Work done during an internship at Meituan. Corresponding authors. Project leader. actions. Recent methods enable the generation of fullbody [6, 9] avatars that maintain high fidelity to reference identity, conditioned on driving signals such as text, speech [21], or predefined pose sequences [18]. For long video generation, these models typically operate in chunklevel autoregressive manner, where each new video segment is generated conditioned on the final frames of the previous one [13, 14, 28]. Despite this progress in identity preservation and alignment with driving signals, the avatars generated by current methods still lack genuine agency; they can execute predefined actions or follow simple commands, but cannot autonomously pursue long-term goals through multi-step planning and adaptive interaction with their environment. This gap from passive animation to active, goaloriented behavior limits the application of video avatars in broader and more dynamic scenarios, such as virtual human livestreaming or autonomous product hosting. This limitation raises question: How can we transition from passive animation to active, goal-oriented intelligence? To achieve this, an avatar must: (1) maintain understanding of task progress from incomplete visual observations (the agent only sees generated clips), (2) predict how actions affect future states, and (3) plan coherent action sequences toward long-term goals. This decision-making ability requires an internal representation that synthesizes observation history to estimate the true world state. In cognitive science and control theory, such representations are formalized as Internal World Models (IWMs) [4], which enable agents to: (i) estimate states from observations and predict future states, and (ii) simulate action outcomes for deliberate planning [24]. Formally, the setting of decision-making under partial observability with goaldirected planning is characterized as Partially Observable Markov Decision Process (POMDP) [20]. Recent work in multi-turn VLM agents [24] and biological intelligence [4] confirms that explicit world modeling is essential for robust long-horizon planning. For video avatars, IWMs are particularly critical as the agent must continuously track what 1 (a) Passive-condition-driven methods. Figure 1. Comparison of video avatar generation approaches. (a) Speech-driven and pose-driven methods produce passive motions with limited semantic understanding. In contrast, (b) our Online Reasoning and Cognitive Architecture (ORCA) enables complex, multi-step task execution through OTAR (Observe-Think-Act-Reflect) closed-loop reasoning. (b) Online reasoning and cognitive architecture. has been accomplished and predict how each generated clip advances toward the goal, and the state of the environment which cant be directly seen from video. However, realizing IWMs for generative video avatars presents fundamentally different challenges than in robotics or embodied AI, where agents interact with deterministic physical environments. We identify two core challenges: State Estimation and Tracking under Generative Uncertainty. Traditional IWMs assume fixed world model where repeated actions yield consistent outcomes. In contrast, video generation is inherently stochastic, the same action specification can produce diverse visual outcomes due to the probabilistic nature of I2V models. This creates challenge in maintaining internal state. Without verifying generated outcomes against intended states, the agents internal state may lead to errors in long-horizon planning. Unlike physical robots that can rely on sensors, video avatars must infer state solely from their own generated clips under partial observability. This necessitates closed-loop mechanism that continuously grounds states through outcome verification. Planning in Open-domain Action Space. Unlike robotics with bounded action spaces (joint angles), video avatar actions are semantic and open-domain without predefined primitives. Simple action textual descriptions alone (pick up the red cup) leave visual details unspecified, causing diverse, often incorrect generations. This demands hierarchical planning, not only decide the next action, but also translating this action into detailed, model-specific control signal to precise control of avatars. To systematically evaluate active intelligence in video avatars, we propose L-IVA (Long-horizon Interactive Visual Avatar) task and benchmark for autonomous goal completion through multi-step environmental interactions. Unlike video generation benchmark evaluating single-clip aesthetics, L-IVA tests goal-directed planning in stochastic generative environments. The task is formulated as chunklevel autoregressive generation: given an initial scene and high-level goal (e.g., host product demo), agents generate sequential video clips depicting coherent task completion via meaningful object interactions. To succeed on L-IVA, we propose ORCA (Online Reasoning and Cognitive Architecture), which embodies IWM capabilities in this specific domain through two designs, grounded in established theories from control theory: To address Challenge 1s uncertainty and correctly update the internal state, OCRA works in an Observe-Think-Act-Reflect (OTAR) loop. The agent observe clips and history state to estimate current state and update the internal belief, think to plan sub-goals, and reflect to verify outcomes and trigger corrections. This prevents error accumulation in stochastic generation. Second, to address challenge 2s action specification problem, we implement two specialized VLM modules inspired by dual-process theory [5]. System 2 works in Observe-Think-Reflect stage, it evaluates progress, plans sub-goals, and predict next state, addressing open-domain planning through reasoning. System 1 operates during Act stage, it translates abstract sub-goals into detailed action captions which are specially designed for specific I2V model for execution. Together, these innovations enable autonomous behavior where System 2 maintains strategic coherence over long horizons and System 1 ensures execution precision. The OTAR cycle provides robustness in probabilistic generative environments. As illustrated in Figure 1, our method enables multi-step complex task generation, in contrast to passive animation methods such as speech-driven or pose-driven approaches. Our contributions can be summarized as follows: 2 We introduce L-IVA benchmark for evaluating video long-horizon task avatars capability for autonomous, completion in interactive scenarios. We propose ORCA, the first framework enabling active (1) intelligence in generative video avatars through: closed-loop OTAR cycle for robust state tracking and execution in stochastic generative worlds, and (2) dualsystem hierarchical architecture for open-domain planning with precise action grounding. Through extensive experiments on L-IVA, we demonstrate that ORCA outperforms open-loop and nonreflective baselines in task success rate and behavioral coherence, validating the effectiveness of our IWM-inspired design for video avatar intelligence. 2. Related Work 2.1. Video Avatar Model The creation of controllable video avatars has fallen into two categories: generating motion [13, 14, 1719] for given human image and maintaining consistent identity [8, 11, 25, 32] given facial image. Audio-driven methods translate speech into motion, evolving from two-stage pipelines using 3D meshes to end-to-end models [13, 28] Identity-consistent text to that directly synthesize video. video generation preserves appearance from reference images using textual prompts [8, 22, 32]. Recent works like InterActHuman [26] merge these streams, driving consistent identities with audio and text. However, these approaches frame generation as reactive signal processingmotion as audio response, identity as feature fusionlacking goaloriented planning. We introduce cognitive reasoning that plans actions from long-term goals before synthesis, enabling purposeful behavior beyond reactive generation. 2.2. Agent for Video Generation Recent agent-based frameworks tackle complex video creation through task decomposition. Multi-agent systems like DreamFactory [27], StoryAgent [10], and Mora [30] employ specialized agents for story design, storyboard generation, and synthesis to ensure narrative coherence across scenes. VideoGen-of-Thought [31] decomposes sentences into multi-shot storylines. second class introduces iterative refinement: VISTA [15] and GENMAC [12] employ generation-critique cycles to enhance prompts for improving clip quality. Despite these advances, prior agents focus on perfecting single clips through feedback correcting deviations from initial prompts. In contrast, our framework maintains goal-directed behavior across long horizons, where each clip is step in an evolving interaction rather than an endpoint to refine. 2.3. World Models for Agentic Planning Internal world models enable agents to maintain an internal belief while estimating states from observations and predict action outcomes to make decisions under partial observability [4]. In embodied AI, world models support planning through learned forward dynamics [1]. Recent work extends this concept to VLM agents for game playing [2, 7, 24] and navigation through RL training. However, these methods assume low-variance environments where repeated actions yield consistent outcomes, which differ from our work in generative environment. 3. Method In this section, we first formally define our proposed L-IVA task. We then present the ORCA architecture. 3.1. L-IVA Task as POMDP We formulate new task, which we term Long-horizon, Interactive Visual Avatar (L-IVA). Unlike traditional avatar animation tasks driven by predefined signals, L-IVA requires an agent to autonomously achieve high-level intentions through sequential interactions with generative environment, treating an Image-to-Video (I2V) model as its world simulator. In this task, the agent cannot obtain the true world state directly from generated video frames as the frame only contains partial information, it must infer state from observation based on its knowledge. Moreover, identical actions yield diverse outcomes due to the stochastic I2V model. This combination of hidden states and generative stochasticity necessitates the POMDP framework [20]. Formally, L-IVA is defined by the tuple (S, A, , R, Ω, O): The true state of the world, st S, is latent, unobservable representation encompassing the complete properties of the avatar and its environment. The agent interacts with this world through an open-ended action space composed of natural language captions. Each action at is highlevel command. The agent perceives the world through observations ot Ω sequences of video frames generated by the I2V model. The observation function O(otst) provides only partial view of the true state st. The true state transition (st+1st, at) is implicit and hidden. What is observable is the stochastic observation defined by the pre-trained I2V model: ot+1 Gθ(ot, at). For given high-level intention I, the reward is sparse and terminal, yielding value of 1 only if the final state of the trajectory satisfies the intention. Since true states st are hidden, the agent cannot condition its policy directly on states. Instead, it maintains an internal belief state ˆst, state with history information and updated via observation. The agent must execute policy π(atˆst), conditioned on the belief state that maximizes the probability of task success. 3 Figure 2. Overview of the ORCA framework. ORCA operates through closed-loop OTAR cycle: Observe updates internal world state from generated clips, Think (System 2) decomposes tasks and predict next state, Act (System 1) translates subgoals into action captions for I2V generation, and Reflect verifies completion to accept/reject outcomes. This hierarchical dual-system architecture enables robust long-horizon task execution through continuous state tracking and adaptive replanning. 3.2. ORCA: IWM-inspired Architecture 3.2.1. Overview To succeed in L-IVA, we propose ORCA (Online Reasoning and Cognitive Architecture) realizes IWM capabilities through two innovations addressing the unique challenges of generative environments, as shown in Figure 2. ORCA operates in an Observe-Think-Act-Reflect loop where predicted states are continuously verified against actual outcomes, triggering re-generation when mismatched. This closed-loop design prevents belief corruption caused by the randomness of the I2V model, where the model generated different results given the same caption. Open-domain actions require both high-level strategic planning and lowlevel execution grounding, which operate at different levels of abstraction. ORCA adopts dual-system architecture to address this: System 2 performs reasoning, while System 1 translates these plans into precise, I2V-compatible captions. This separation ensures both strategic coherence and execution fidelity. Both designs leverage pre-trained VLMs through structured prompting, requiring no task-specific training. We detail each component below. 3.2.2. Hierarchical Dual-System Architecture Effectively controlling generative avatar requires balancing two demands at different levels of abstraction: highlevel strategic reasoning for open-domain actions, and lowlevel, precise prompt to constrain the stochastic I2V model. monolithic system struggles to manage this, since reliable generative control is often model-specific. To address this, ORCA adopts Hierarchical Dual-System Architecture inspired by dual-process theory [5]. This design decouples planning from execution: System 2 performs reasoning, while System 1 translates these abstract commands into detailed, I2V-model specific captions. System 2 for strategic reasoning System 2 maintains the IWMs high-level belief state ˆst and performs strategic reasoning. In the Think stage: gt, gˆs = πSys2(ˆst, I) (1) where ˆst is current belief state, containing the environment information, history ht and task checklist C, and gt, gˆs represent the textual command and the predicted next state respectively. System 2s strategic focus enables it to leverage pretrained VLMs broad world knowledge for compositional reasoning over open-domain scenarios without being constrained by the specific formatting requirements of generative models. It operates in multiple OTAR stages: analyzing observations for state estimation and update, selecting subgoals and predicting outcomes, and verifying results. System 1 for action grounding Operating in the Act stage, System 1 serves for grounding System 2s abstract plan into concrete control signal. Precise avatar control is highly sensitive to the prompt and different I2V models require different prompt format; therefore, System 1s core task is to translate the multi-modal intention (gt, gˆs) into detailed action caption at tailored for the specific I2V model Gθ. This process leverages extensive prompt engineering to ensure high-fidelity translation. The grounding policy is defined as: at = πSys1(gt, gˆs, ot, ˆst) (2) 4 where at is the final, executable caption. Further details on our prompt engineering strategies are provided in the Appendix section 10. 3.2.3. Closed-loop OTAR Cycle Open-loop plans fail easily in the L-IVA task, as even minor execution errors accumulate. conventional ObserveThink-Act cycle is also insufficient as results from I2V model may significantly differ from the agents intention. failed generation can produce erroneous states that are difficult integratto recover from in subsequent steps, ing these outcomes would corrupt the agents internal belief. ORCA addresses this through an Observe-ThinkAct-Reflect (OTAR) cycle, where the Reflect stage continuously verifies outcomes against predictions before belief updates. This closed-loop design prevents belief corruption from stochastic generation. Algorithm 1 presents the complete procedure, which we detail stage-by-stage below. Initialization. Given intention and initial scene o0, System 2 initializes the belief state ˆs0 = (sscene, Cplan, h). sscene represents interactive objects and properties in o0. Cplan decomposes into plan of sub-goals defined upon these objects. is the empty interaction history. Observe At turn t, System 2 updates the belief state from the latest clip: ˆst = fobserve(ot, ˆst1) (3) producing structured observations of scene changes, updated object states, and completed sub-goals, maintaining the agents understanding of task progress. Thinking System 2 plans the next action based on current belief ˆst, intention I, and current observation ot: gt, gˆs = fthink(ˆst, I, ot) (4) where gt is the command toward finishing the next goal and gˆs is detailed structural description of the predicted outcome state. Action System 1 translates the abstract sub-goal gt into detailed action caption at tailored to the I2V model, then generates: vt+1 Gθ(ot, at) (5) where detailed text conditioning at ensures generation fidelity. Reflect System 2 verifies whether the generated outcome matches the predict states: δt, analysis = freflect(ot+1, gt, gˆs) (6) producing δt {accept, reject}. ot+1 is the sampled frames from vt+1. If accepted, the agent proceeds to turn + 1 to update beliefs. If rejected, System 2 analyzes the failure and either retries with revised action anew = Algorithm 1 ORCA: Closed-loop OTAR Execution Require: Initial observation o0, Intention I, I2V model Gθ, Max retries Nretry Ensure: Video sequence = [v1, ..., vT ] 1: ˆs0 Initialize(o0, I) {Scene analysis + plan decomposition} 2: 0, [] 3: while ˆst.Cremaining = do // Observe Stage 4: ˆst fobserve(ot, ˆst1) 5: // Think Stage 6: gt, gˆs fthink(ˆst, I, ot) // Act Stage at πSys1(gt, gˆs, ot, ˆst) vt+1 Gθ(ot, at) ot+1 SampleFrames(vt+1) // Reflect Stage retry count 0 repeat 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: δt, analysis freflect(ot+1, gt, gˆs) if δt = reject AND retry count < Nretry then at frevise(at, ot+1, analysis) vt+1 Gθ(ot, at) ot+1 SampleFrames(vt+1) retry count retry count + end if until δt = accept OR retry count Nretry if δt = accept then V.append(vt+1), + 1 Re-plan from current state {Adaptive recovery} else 25: 26: 27: 28: end while 29: return end if frevise(at, ot+1, analysis) (up to Nretry attempts) or adaptively re-plans for the next iteration. This prevents belief corruption from failed generations.The cycle continues until ˆst.Cremaining = , returning video sequence = [v1, ..., vT ]. By continuously verifying predictions and triggering corrections, ORCA maintains accurate beliefs despite generative uncertainty, enabling robust long-horizon task completion. 4. L-IVA Benchmark 4.1. Overview While Section 3.1 formally defined the task of achieving active agency as POMDP, empirically evaluating this capability requires new benchmark. Existing frameworks are inadequate, video generation benchmarks evaluate perclip aesthetics rather than long-horizon, goal-directed task completion. We introduce the L-IVA Benchmark comprising 100 tasks across 5 real-world scenario categories. Notably, each category includes 5 two-person collaborative tasks. Each task requires 3-8 interaction steps with more than three objects, testing autonomous planning, environmental reasoning, and precise control. Tasks use fixedviewpoint, single-room settings to avoid spatial inconsistencies in current I2V models. Figure 3 summarizes task statistics; Appendix provides detailed specifications in section 8. 4.2. Dataset Construction Our benchmark scenes consist of AI-generated and real images. Each task is annotated with: (1) an object inventory (names, positions, initial states); (2) high-level natural language intention; and (3) reference action sequence as ground-truth solution. The evaluation focuses on goal achievement over rigid imitation, measuring successful task completion and accepting alternative valid action orderings, not exact trajectory matching. 4.3. Evaluation Protocol and Metrics We evaluate agents on L-IVA using VLM-based judgments and human annotations. Our primary metric is the Task Success Rate (TSR), measuring sub-goal progress. TSR is: TSR = 1 (cid:88) i=1 ki Mi (7) The final score is weighted by the ratio of completed subgoals (ki) to total sub-goals (Mi), where sub-goal completion is manually verified by human annotators. To evaluate overall quality, we employ Best-Worst Scaling (BWS) [16] to derive robust human preference rankings. Additionally, we introduce two diagnostic metrics, which are detailed in Section 9: (1) human-rated Physical Plausibility Score to assess object permanence and spatial consistency; and (2) VLM-based Action Fidelity Score to quantify the semantic alignment between commands and video clips (at, vt). Together, this hybrid human-VLM evaluation framework provides comprehensive assessment of goal-oriented success and execution reliability. 5. Experiments 5.1. Benchmark and Dataset Since L-IVA represents novel task without direct prior work, we evaluate three representative paradigms adapted to our setting: (1) Open-Loop Planner plans the complete action sequence upfront given the initial scene and intention, then executes without feedback; (2) Reactive Agent [29] operates in turn-by-turn observe-act loop without maintaining belief state and reflection; (3) VAGEN-style CoT [24] Figure 3. L-IVA Benchmark Overview. Top: Statistical analysis showing (left) balanced scene distribution across 5 categories, (center) data source composition with 92 synthetic and 8 real images, and (right) task complexity distribution averaging 5.0 sub-goals per task. Bottom: Representative scenes from our benchmark including Garden, Kitchen, and livestream scenarios, demonstrating diverse real-world settings requiring multi-step object interactions. adapts world model-based reasoning with state estimation and transition prediction, but assumes deterministic environments where action outcomes are predictable. We evaluate all methods on L-IVA across 5 scenarios (Kitchen, Livestream, Workshop, Garden, Office) using multiple metrics: Task Success Rate (TSR) measuring goal completion, Physical Plausibility Score (PPS) and Action Fidelity Score (AFS) assessing execution quality, video generation quality (Aesthetics and Subject consistency), and Human Preference via BWS. Implementation details and evaluation protocols are provided in Appendix section 9. 5.2. Implement Details ORCA is training-free and leverages pre-trained models, including Gemini-2.5-Flash as the vision-language model for both System 1 and System 2, and Wanx2.2 [23] with distilled LoRA as the I2V generation model [3]. All baseline methods share the same VLM and generation settings to ensure fair comparison. We carefully design prompts tailored to each methods reasoning structure, with detailed prompts provided in the Appendix section 10 . 5.3. Comparison with Planning methods 5.3.1. Quantitative Comparison Table 1 presents comprehensive evaluation across 5 scenarios and multiple dimensions. Results reveal important trade-offs between task completion, execution quality, and video coherence. Figure 4. Qualitative Comparison on Transfer Plant Task. We compare four methods on long-horizon video generation. Top: Ground truth subgoals for reference. Red boxes indicate execution failures or error accumulation. Open-Loop planner cannot detect execution errors. Reactive agent lacks world state knowledge, leading to repetitive actions. VAGENs I2V errors corrupt the final state without reflection. ORCA (Ours) successfully completes all subgoals with consistent execution quality. Table 1. Main Results on L-IVA Benchmark. All metrics are evaluated per scenario. (a) Task completion metrics. (b) Video quality and human preference. Best in bold. (a) Task Success Rate (%) and Execution Quality Task Success Rate (%) Physical Plausibility (1-5) Action Fidelity (0-1) Method Kit. Live. Work. Gard. Off. Avg Kit. Live. Work. Gard. Off. Avg Kit. Live. Work. Gard. Off. Avg Reactive 56.7 58.7 Open-Loop 72.3 72.3 70.8 63.6 VAGEN 45.8 72.7 61.1 55.0 46.2 60.0 38.1 50.9 47.9 62.3 50.4 61. 3.47 3.05 3.57 3.50 3.56 3.59 3.13 3.27 3.67 3.08 2.92 2.54 2.80 3.11 2.60 3.17 2.73 3.22 0.53 0.56 0.57 0.72 0.64 0.63 0.41 0.53 0. 0.61 0.64 0.58 0.63 0.55 0.65 0.62 0.68 0.62 ORCA 73.8 58.4 80.4 81. 61.0 71.0 3.53 3.68 3.93 3.77 3.67 3.72 0.64 0. 0.54 0.63 0.70 0.64 (b) Video Generation Quality and Human Preference Aesthetics Subject Consistency BWS (%) Method Kit. Live. Work. Gard. Off. Avg Kit. Live. Work. Gard. Off. Avg Kit. Live. Work. Gard. Off. Avg 0.61 0.53 Reactive Open-Loop 0.59 0.51 0.60 0.52 VAGEN 0.61 0.58 0.59 0.62 0.60 0. 0.52 0.59 0.50 0.56 0.50 0.57 0.92 0.93 0.91 0.91 0.91 0.93 0.91 0.89 0.92 0.91 0.88 0.91 0.93 0.92 0.91 0.90 0.92 0.92 0.00 -20.6 -45.0 8.80 -13.3 3.30 13.3 -10.0 5. 15.4 -40.0 -18.0 -23.1 -13.3 -7.52 -23.1 -6.70 -4.12 ORCA 0.63 0.53 0.62 0.60 0.51 0. 0.92 0.94 0.94 0.91 0.92 0.93 6.70 5. 40.0 30.8 60.0 28.7 Kit.: Kitchen, Live.: Livestream, Work.: Workshop, Gard.: Garden, Off.: Office. PPS and AFS measure execution quality. Evaluation on Task Success Rate and Execution Quality Table 1 presents comprehensive evaluation across 5 scenarios. ORCA achieves the highest average Task Success Rate (71.0%) and Physical Plausibility (3.72), validating the effectiveness of our closed-loop architecture. However, nuanced analysis reveals an important trade-off. OpenLoop Planner remains highly competitive in scenarios with lower state dependency, such as Livestream and Kitchen, where it performs on par with or even surpasses ORCA. This phenomenon has an explanation that Open-Loop Planner schedules the complete action sequence upfront and executes all steps regardless of intermediate outcomes. In loose, low-dependency tasks, this strategy ensures the agent attempts all sub-goals within the fixed step budget, often achieving nominal completion. In contrast, ORCAs rigorous Reflect mechanism invests computational steps in error correction; while this ensures high fidelity, it risks exhausting the step budget on retries in simpler tasks. However, the advantage of ORCA becomes decisive in complex, highdependency environments like Garden and Workshop. In the Garden scenario, Open-Loop planner fails significantly because execution errors in early steps go undetected, rendering subsequent actions meaningless. Regarding execution quality, ORCA achieves the highest Physical Plausibility Score, significantly outperforming the Reactive Agent. The low score of the Reactive agent baseline stems from its lack of world model; without maintaining structured belief state, it suffers from severe object permanence issues. Evaluation on Video Quality and Human Preference Table 1 (b) exposes critical flaw in Open-Loop Planning, despite competitive TSR, it suffers from the lowest Subject Consistency. This validates our hypothesis that without outcome verification, visual artifacts accumulate, degrading avatar identity across long horizons. In contrast, ORCA achieves the highest Subject Consistency by actively filtering low-quality generations in the Reflect stage. Crucially, human evaluation via Best-Worst Scaling ranks ORCA significantly higher than all baselines, with Reactive agent and Open-Loop planner receiving negative scores. The results demonstrate that active intelligence requires more than just per-clip image quality. While Reactive agent maintains high aesthetics, its inability to execute coherent tasks leads to the lowest human preference. Open-Loop achieves task completion but sacrifices visual consistency. ORCA delivers the most robust performance, combining superior consistency with high task success, thereby validating the necessity of closed-loop world modeling. 5.3.2. Qualitative Comparison Figure 4 compares our method with baselines on the multistep Transfer Plant task, which requires coordinating four sequential subgoals while maintaining world state consistency. Open-Loop Planner generates all I2V captions for the entire task sequence in one shot and directly passes them to the video generation model for execution, without any intermediate validation or reflection mechanisms. As shown in Figure 4 (second row), while the initial steps appear plausible, inevitable execution deviations in the generated videos (e.g., incorrect seedling removal in Step 2) go undetected and accumulate through subsequent steps. By Step 4, the generated video depicts actions on completely misaligned object. Reactive agent enables closed-loop correction but lacks world state modeling. Row 3 shows the method repeatedly performing the same action (adding soil) without recognizing subgoal completion, resulting in physically implausible repetitive behaviors. VAGEN combines planning with closed-loop execution but suffers from I2V model hallucinations. Without reflection mechanisms, these errors can corrupt the final state(row 4, red boxes). ORCA successfully executes most subgoals through the OTAR cycle. The reflect phase detects errors early. This prevents all three failure modes: undetected errors, repetitive actions, and hallucination corruption. The results demonstrate that dual-system reasoning with reflection is essential for longhorizon video generation. 5.4. Ablation Study Table 2. Ablation on ORCA components. Note: To ensure fair comparison within ablation variants, we re-evaluated the Workshop scene. Minor variations in absolute scores compared to Table 1 are due to the stochastic nature of human evaluation. Variant TSR Cons. BWS ORCA (Full) w/o System 1 w/o Reflect w/o Belief State 0.77 0.74 0.72 0.67 0.94 0.93 0.92 0.93 26.7% -6.72% -20.0% 0.00% Our ablation study systematically validates ORCAs design principles by answering three core questions established in Section 1. Each question corresponds to key challenge in realizing IWMs for stochastic generative environments. We report metric on workshop scene. Is explicit world modeling necessary for active intelligence? Removing belief state tracking (w/o Belief State) causes the severe TSR degradation. Without maintaining ˆst containing scene state, the agent cannot track completed sub-goals or reason about action dependencies, leading to repetitive or out-of-order actions. Is closed-loop verification necessary under generative stochasticity? Removing reflection (w/o Reflect) degrades Subject Consistency and BWS. Without outcome verification, incorrect generations corrupt subsequent steps. Is hierarchical action specification necessary for opendomain control? Removing System 1s detailed grounding (w/o System 1) reduces TSR and BWS. Direct use of 8 System 2s abstract commands yields imprecise I2V generation. This confirms that separating strategic reasoning from execution grounding is critical for reliable control. 6. Conclusion This paper introduces paradigm shift from passive animation to active intelligence in video avatars. We presented LIVA, the first benchmark for autonomous goal completion in interactive scenarios, and ORCA, framework grounded in Internal World Model theory. ORCA achieves robust long-horizon behavior through closed-loop OTAR cycle for state tracking under generative uncertainty and dualsystem architecture for hierarchical planning and execution. Experimental results validate that our reflection mechanism prevents belief corruption and enables coherent multi-step task completion."
        },
        {
            "title": "References",
            "content": "[1] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 3 [2] Shiqi Chen, Tongyao Zhu, Zian Wang, Jinghan Zhang, Kangrui Wang, Siyang Gao, Teng Xiao, Yee Whye Teh, Junxian He, and Manling Li. Internalizing world models via self-play finetuning for agentic rl. arXiv preprint arXiv:2510.15047, 2025. 3 [3] LightX2V Contributors. Lightx2v: Light video generation inference framework. https://github.com/ ModelTC/lightx2v, 2025. 6 [4] Ilka Diester, Marlene Bartos, Joschka Bodecker, Adam Kortylewski, Christian Leibold, Johannes Letzkus, Matthew Nour, Monika Schonauer, Andrew Straw, Abhinav Valada, et al. Internal world models in humans, animals, and ai. Neuron, 112(14):22652268, 2024. 1, 3 [5] Jonathan St BT Evans and Keith Stanovich. Dual-process theories of higher cognition: Advancing the debate. Perspectives on psychological science, 8(3):223241, 2013. 2, 4 [6] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. ArXiv, abs/2506.18866, 2025. 1 [7] Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang, Xinqi Huang, Xinyu Fang, Lu Sun, Gautham Kishore, Bo Ai, et al. Do vision-language models have internal world models? towards an atomic evaluation. arXiv preprint arXiv:2506.21876, 2025. [8] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 3 [9] Liucheng Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. ArXiv, abs/2502.06145, 2025. 1 [10] Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. arXiv preprint arXiv:2411.04925, 2024. 3 [11] Jiehui Huang, Xiao Dong, Wenhui Song, Zheng Chong, Zhenchao Tang, Jun Zhou, Yuhao Cheng, Long Chen, Hanhui Li, Yiqiang Yan, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. arXiv preprint arXiv:2404.16771, 2024. 3 [12] Kaiyi Huang, Yukun Huang, Xuefei Ning, Zinan Lin, Yu Wang, and Xihui Liu. Genmac: compositional text-to-video generation with multi-agent collaboration. arXiv preprint arXiv:2412.04440, 2024. [13] Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Instilling an active Omnihuman-1.5: mind in avatars via cognitive simulation. arXiv preprint arXiv:2508.19209, 2025. 1, 3 [14] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang, Yuan Zhang, and Jingtuo Liu. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1384713858, 2025. 1, 3 [15] Do Xuan Long, Xingchen Wan, Hootan Nakhost, Chen-Yu Lee, Tomas Pfister, and Sercan Arık. Vista: testtime self-improving video generation agent. arXiv preprint arXiv:2510.15831, 2025. 3 [16] Jordan Louviere, Terry Flynn, and Anthony Alfred John Marley. Best-worst scaling: Theory, methods and applications. Cambridge University Press, 2015. 6 [17] Junxian Ma, Shiwen Wang, Jian Yang, Junyi Hu, Jian Liang, Guosheng Lin, Kai Li, Yu Meng, et al. Sayanything: Audiodriven lip synchronization with conditional video diffusion. arXiv preprint arXiv:2502.11515, 2025. 3 [18] Yue Ma, Yin-Yin He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. ArXiv, abs/2304.01186, 2023. [19] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 3 [20] Matthijs TJ Spaan. Partially observable markov decision proIn Reinforcement learning: State-of-the-art, pages cesses. 387414. Springer, 2012. 1, 3 [21] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive - generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, 2024. 1 [22] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2109621106, 2024. 3 [23] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 6 [24] Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. arXiv preprint arXiv:2510.16907, 2025. 1, 3, 6 [25] Runqi Wang, Yang Chen, Sijie Xu, Tianyao He, Wei Zhu, Dejia Song, Nemo Chen, Xu Tang, and Yao Hu. Dynamicface: High-quality and consistent face swapping for image and video using composable 3d facial priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1343813447, 2025. 3 [26] Zhenzhi Wang, Jiaqi Yang, Jianwen Jiang, Chao Liang, GaoInjie Lin, Zerong Zheng, Ceyuan Yang, and Dahua Lin. teracthuman: Multi-concept human animation with layoutaligned audio conditions. arXiv preprint arXiv:2506.09984, 2025. 1, 3 [27] Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with multi-agent framework. arXiv preprint arXiv:2408.11788, 2024. 3 [28] Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, and Xiaoming Wei. Infinitetalk: Audio-driven video generation for sparse-frame video dubbing. ArXiv, abs/2508.14033, 2025. 1, 3 [29] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. [30] Zhengqing Yuan, Yixin Liu, Yihan Cao, Weixiang Sun, Haolong Jia, Ruoxi Chen, Zhaoxu Li, Bin Lin, Li Yuan, Lifang He, et al. Mora: Enabling generalist video generation via multi-agent framework. arXiv preprint arXiv:2403.13248, 2024. 3 [31] Mingzhe Zheng, Yongqi Xu, Haojian Huang, Xuran Ma, Yexin Liu, Wenjie Shu, Yatian Pang, Feilong Tang, Qifeng Chen, Harry Yang, et al. Videogen-of-thought: collaborative framework for multi-shot video generation. arXiv eprints, pages arXiv2412, 2024. 3 [32] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, Towards univerarXiv preprint identity-preserving video synthesis. and Chongxuan Li. sal arXiv:2503.14151, 2025. 3 Concat-id: 10 Active Intelligence in Video Avatars via Closed-loop World Modeling"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Supplementary Material 9. Evaluation Protocols This supplementary material provides comprehensive details to support the main paper. It is organized as follows: Section details the construction of the L-IVA benchmark; Section elaborates on the evaluation protocols and metrics; Section presents additional qualitative visualizations comparing different methods; Section lists the specific prompts utilized in the ORCA framework; Section discuss the failure case and limitations. 8. Benchmark Construction Details To ensure diversity in visual complexity and environmental dynamics, we construct the L-IVA benchmark through hybrid pipeline combining real-world photography and AIsynthesized imagery. Real-world Data Curation. We source high-quality realworld images from Pexels to serve as initial observations. The selection process is strictly guided by scene affordance: we filter for images containing distinct, interactive objects capable of supporting multi-step physical manipulations necessary to achieve high-level goals. For each selected scene, we manually define high-level intention (I). To generate ground-truth annotations efficiently, we leverage Gemini-2.5-Pro. Given the image and the intention, the model generates structured set of metadata, including sequential subgoals, detailed object descriptions, and reference action prompts. Each data sample is stored as pair consisting of the initial image and corresponding YAML file containing these hierarchical annotations (see Figure 5(c)). Synthetic Data Generation. For synthetic scenarios, we utilize Nanobanana to create controlled environments with specific object configurations. Unlike the real-world pipeline, we adopt goal-first approach: we first design high-level intention and ensure that all requisite object interactions are logically solvable within single scene. Based on these requirements, we craft detailed text prompts to generate the initial scene image. This design-thengenerate strategy ensures precise alignment between the visual assets (objects in the scene) and the task requirements, guaranteeing that the generated environments inherently support the intended interaction sequence. To systematically assess the capabilities of active video avatars, we employ hybrid evaluation strategy combining VLM-based automated metrics and human judgment. All automated evaluations utilize Gemini-2.5-Flash due to its strong multimodal reasoning and long-context video understanding capabilities. We use user study to measure the physical score and TSR, as current VLMs struggle with subtle physical inconsistencies and long-horizon causal reasoning. The webpage of the user study is shown in Figure 9. Our user study involves 8 human evaluators. We constructed 130 comparative evaluation sets sampled across different scenarios. Since each set includes videos from 4 methods (ORCA + 3 baselines), this results in total of 520 evaluated videos, ensuring statistical reliability. The evaluation focuses on three key dimensions: Task Success Rate (TSR). TSR measures the agents ability to complete the high-level intention through multistep subgoals. Unlike single-clip generation, our task requires causal completion of sequence of actions. Human evaluators are presented with the high-level intention I, the ground-truth subgoal list {g1, ..., gN }, and the generated video sequence . For each sample, evaluators count the number of successfully completed subgoals based on visual evidence. The final TSR score is calculated as the ratio of completed subgoals to the total number of subgoals: TSR = 1 (cid:88) k=1 (k) completed (k) total (8) where (k) ecuted in the k-th test case. completed is the number of subgoals successfully exAction Fidelity Score (AFS). AFS measures the semantic alignment between the planned action command at and the executed video clip vt. As detailed in Table 3, this is binary classification task (0/1). The VLM verifies if the Core Action, Key Objects, and Directional Consistency in the video match the text caption. It is designed to be tolerant of minor visual artifacts but strict on semantic errors (e.g., performing cut instead of peel). Physical Plausibility Score (PPS). This metric evaluates the physical consistency of the generated world, specifically targeting I2V-specific hallucinations. Human evaluators score each video sequence on 1-5 Likert scale: 1 Figure 5. Overview of the L-IVA Benchmark Construction Pipeline. (a) Our data curation process employs hybrid strategy: Pipeline sources real-world images from Pexels, filtered by scene affordance and annotated via Gemini-2.5-Pro. Pipeline utilizes goal-first design for synthetic data, where scenes are generated by Nanobanana to strictly align with intended interactions. (b) representative scene image (e.g., Checking beehives) from the benchmark. (c) The corresponding structured metadata (YAML), including object inventory, high-level intention, subgoals, and reference prompts. 5 (Perfect): Perfect physical interaction with realistic 1 (Fail): Complete breakdown; failure to adhere to basic gravity, collision, and contact points. physical laws. 4 (Good): Physics generally correct; minor artifacts do not impede comprehension. 3 (Fair): Noticeable floating or clipping, but the action logic remains coherent. 2 (Poor): Severe physical violations (e.g., object teleportation, interpenetration). Human Preference Ranking Protocol. To evaluate holistic performance, we employ Best-Worst Scaling (BWS), which is shown to be more statistically robust than simple ranking. For each query, annotators are presented with the initial scene, the intention, and anonymized videos 2 (e.g., an object flickering out of existence for just 2 frames) fall between sampled frames. Consequently, the VLM may generate False Positive judgment, accepting flawed video. Lack of 3D Spatial Awareness: Current VLMs operate in 2D pixel space and often struggle with depth perception. As illustrated in Figure 6(b), the VLM may misinterpret background object as being within the avatars reach, leading to geometrically impossible instructions (e.g., pick up the distant cup) that the I2V model cannot execute realistically. I2V-Centric Failures: Control and Consistency. Even when ORCA generates perfect instructions, the generative backbone (I2V model) acts as physical execution bottleneck: Instruction Following: In handling complex, finethe I2V model often exhibits grained manipulations, strong prior biases and fails to adhere to the prompt. Although ORCAs Reflect module correctly rejects these failures and triggers retries (up to Nretry times), the I2V model may persistently fail to generate the correct physics, leading to an eventual task termination. Object Permanence and Hallucination: Generative models inherently struggle with long-term object permanence. As shown in Figure 6(d), sudden object disappearance or the spontaneous hallucination of new objects can occur. While ORCAs state tracking attempts to catch these errors, severe hallucinations can sometimes corrupt the agents belief state if they are too subtle for the VLM to detect immediately. Conclusion. These limitations highlight that ORCA is framework for active intelligence, currently operating on imperfect substrates. We posit that as VLM spatial reasoning and I2V controllability improve, ORCAs performance will scale accordingly without architectural changes. generated by the comparing methods side-by-side. Annotators are instructed to select the Best and Worst models based on hierarchical criterion: first prioritizing Task Completion (whether the intention was fulfilled), followed by Logical Coherence (smoothness of the action plan), and finally Visual Aesthetics. The BWS score for each method is computed as: BWS Score = #Best #Worst #Total Cases 100% (9) This score ranges from -100% to +100%, where positive score indicates that the method was chosen as Best more often than Worst. 10. Prompt Details for ORCA In this section, we provide the full system prompts used in the ORCA framework. These prompts are designed to be model-agnostic but are optimized for Gemini-2.5-Flash. Figure 7(a): The Initialization Prompt used by System 2 to parse the initial scene and decompose the high-level intention into structured plan. Figure 7(b): The Observation Prompt for updating the belief state based on generated video clips. Figure 8(a): The Thinking Prompt (System 2) for strategic reasoning, discrepancy detection, and next-step planning. Figure 8(b): The Action Grounding Prompt (System 1) for translating abstract plans into high-fidelity, I2Vcompatible captions. Figure 8(c): The Reflection Prompt for verifying action execution and triggering error correction. 11. Discussion on Failure Cases and Limitations While ORCA demonstrates superior performance in longhorizon task execution compared to open-loop baselines, its capabilities are inevitably bounded by the underlying foundation models (VLM and I2V). We categorize the observed failure cases into two dimensions: Perceptual Bottlenecks (VLM-side) and Generative Constraints (I2V-side). It is important to note that these failures stem primarily from the intrinsic limitations of current pre-trained models rather than the algorithmic design of the ORCA framework. VLM-Centric Failures: Perception and Depth Ambiguity. The reliability of ORCAs Reflect and Observe modules depends on the VLMs visual grounding ability. We observe two specific issues: Temporal Information Loss due to Sampling: To manage context length, ORCA feeds sampled frames (e.g., 5 frames) to the VLM. This discrete sampling can cause Temporal Aliasing, where critical but fleeting glitches 3 Figure 6. Qualitative Analysis of Failure Cases attributed to Foundation Model Limitations. (a) Temporal Information Loss: Due to discrete frame sampling, the VLM misses the teleportation artifact where the fertilizer bag instantaneously appears in the hand (red arrow), falsely accepting it as valid pickup. (b) Lack of 3D Spatial Awareness: The VLM misinterprets the depth of the scene, instructing the avatar to fetch watering can that is actually in the distant background, resulting in an unnatural reaching motion. (c) Weak Instruction Following: For fine-grained tasks like light the alcohol lamp, the I2V model consistently fails to execute the interaction despite ORCA triggering multiple retries (separated by dashed lines). (d) Object Disappearance: clear example of generative instability where key object (the water bottle) vanishes mid-clip (red box) despite no interaction occurring. Table 3. Full Prompt for Action Fidelity Score (AFS) Evaluation Prompt Content # ROLE: Action Fidelity Evaluator You are an expert evaluator assessing alignment between video clip and its action caption. ## EVALUATION CRITERIA ALIGNED (af = 1) if: 1. 2. 3. Core Action Match: Object Correctness: Spatial/Directional Consistency: Primary verb (e.g., \"pick up\") is clearly visible. Key objects match the caption. Motion direction and location match roughly. NOT ALIGNED (af = 0) if: 1. 2. 3. 4. Wrong Action: Wrong Object: Missing Action: Impossible Outcome: Action shown is fundamentally different (e.g., \"cut\" vs \"stir\"). Object manipulated is incorrect. Described action is absent. Visual contradicts captions physics. ## TOLERANCE GUIDELINES - ACCEPT: Visual quality issues (blur), Incomplete visibility (partially off-screen), Semantic equivalence (\"bowl\" vs \"container\"), Timing flexibility. - REJECT: Contradictory actions, Wrong action type, Missing critical steps. ## OUTPUT FORMAT Provide reasoning in <thinking>, then output JSON: { } \"af\": \"reason\": 0 or 1, \"Brief explanation...\" Figure 7. Prompt for Initialization module and observer 5 Figure 8. Prompt for Thinking, System-1 and Reflector 6 Figure 9. The user interface for human evaluation. For each test case, annotators are presented with high-level goal (Intention) and generated videos from four anonymized methods (Video and Video are shown here; and are omitted for brevity). Evaluators are asked to assess each video based on two criteria: (1) Physical Score (1-5 Likert scale) regarding simulation stability, and (2) Subgoals 7 Checklist to verify task completion. Finally, they select the best and worst videos based on overall quality."
        }
    ],
    "affiliations": [
        "Meituan",
        "The Hong Kong University of Science and Technology",
        "University of Science and Technology of China"
    ]
}