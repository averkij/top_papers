{
    "paper_title": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding",
    "authors": [
        "Tao Zhang",
        "Xiangtai Li",
        "Zilong Huang",
        "Yanwei Li",
        "Weixian Lei",
        "Xueqing Deng",
        "Shihao Chen",
        "Shunping Ji",
        "Jiashi Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA."
        },
        {
            "title": "Start",
            "content": "Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding Tao Zhang1,2 Xiangtai Li1 Zilong Huang1 Yanwei Li1 Weixian Lei Xueqing Deng1 Shihao Chen2 1 Bytedance Seed Shunping Ji2 2 WHU Jiashi Feng1 5 2 0 A 4 1 ] . [ 1 5 6 4 0 1 . 4 0 5 2 : r Project Page: https://zhang-tao-whu.github.io/project/pixelsail Figure 1. Comparison of current MLLMs for pixel-wise understanding with our method. (a) and (b). Current MLLMs for pixel-wise understanding feature highly complex system architectures, including an LLM, CLIP-like vision backbone, an object token extraction model, segmentation vision backbone, and SAM-like decoder. (c). Our method employs only single transformer."
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design learnable upsampling module to refine visual token features. Secondly, we propose novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce vision expert distillation strategy to efficiently enhance the single transformers finegrained feature extraction capability. In addition, we have collected comprehensive pixel understanding benchmark (PerBench), using manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with much simpler pipeline. Code and model will be released at https://github.com/magicresearch/Sa2VA. 1. Introduction Multi-modal Large Language Models (MLLMs) have garnered significant research efforts, driven by advancements of Large Language Models (LLMs) [22, 56, 65]. While most studies focus on open-ended visual question answering tasks, there is growing interest [51, 80] in fine-grained, pixel-level understanding. This enables broader applications, such as facilitating precise region-level editing and generation and achieving precise understanding of designated mask regions. Recent pixel-wise MLLMs [27, 51, 54, 63, 72, 80, 81] mainly adopt visual and language fusion frameworks, following design patterns [17, 42, 68] established before the For example, LAVIT [68] adopts encoderLLM era. fusion approach, injecting language embedding (generated by BERT [13]) into vision transformers. With the advent of LLMs [22, 65, 66], recent works [27, 54, 72, 80] integrate state-of-the-art segmentation models [26, 33, 53], for pixellevel understanding, by either appending them to LLM outputs or embedding LLM within segmentation pipelines. While effective, the overall architectures are complex, requiring specialized components such as vision-language fusion modules and additional decoders. Moreover, their final performance often heavily depends on either MLLMs or the segmentation models, which may lead to suboptimal results due to limitations within individual submodules. In this work, we explore novel, simple yet effective pixel-wise MLLM design, drawing inspiration from recent advancements in SAIL architecture, which is also called Encoder-free MLLMs. These methods drop the extra vision encoder and jointly co-train vision and language tokens on large scale datasets, with simpler design. Moreover, they show competitive performance on image-level VQA tasks, compared with LLaVA. Motivated by this success, we extend the framework to pixel-level understanding tasks, aiming to reduce the complexity of existing approaches. To the best of our knowledge, this is the first study to explore the simplest architecture for pixel-wise MLLM tasks, including referring segmentation and visual prompt understanding. We first directly extend SAIL architecture by adding segmentation token and visual prompt tokens to generate segmentation masks and output region caption, following previous works [27, 51, 74]. However, this leads to inferior results on both segmentation and visual prompt understanding. Several reasons are: (1), The misalignments on high resolution features since there are no segmentation decoders since SAIL directly reshape the vision tokens into features. (2), Previous works directly adopt mask pooling on high level visual tokens where SAIL baseline only maps RGB inputs with one projection layer, where most tokens are low level features. (3), The mask quality is low since no segmentation experts are involved. To solve these problems, we present three simple technical improvements, which lead to our Pixel-SAIL framework. First, we design simple learnable up-sampling module to refine the low resolution visual tokens in high resolution features. Our goal is to keep the design as simple as possible, where only one transposed 2D convolution is involved. Then, for visual prompt understanding, we design novel visual prompt injection method, where we map the visual prompts into special text tokens without introducing extra visual prompt encoder in the middle stage of SAIL. Next, we propose to distill the previous segmentation experts into SAIL to improve mask quality. All the improvements are plug-in-play, and we verify the effectiveness on various SAIL architectures, including SOLO [8] and EVEv2 [16]. Then, to further indicate the effectiveness of our PixelSAIL and facilitate the development of pixel-LLM community, we further design new challenging benchmark, PerBench. Compared with previous pixel-wise MLLM benchmarks, we have three innovative and challenging features. First, we include detailed object caption where most existing benchmarks only contain short captions without fine-gained contents. Secondly, we re-evaluate visualprompt understanding as multi-choice VQA tasks following MME [20] and MMBench [43] to achieve more accurate region caption evaluation. Thirdly, we introduce task by segmenting objects jointly referenced by visual prompts and text. Our benchmark reveals the limitation of current stateof-the-art pixel-wise MLLM on fine-grained understanding and mixed referring tasks. Pixel-SAIL is jointly co-trained with mixed data engine on referring segmentation datasets, VQA datasets, and visual prompt datasets. Experimental results show that our method can achieve better results on five pixel-wise benchmarks. In particular, on RefCOCOg and RefCOCO+ datasets, our method with 3B size can outperform previous pixel MLLMs, including GLaMM (7B) and OMG-LLaVA (7B), by 1.5-3.0%, with simpler pipeline. On our PerBench, our method achieves 24.2 METEOR, 74% accuracy, 33.4 cIoU and 42.2 overall score, surpassing the SOTA MLLMs GLaMM (7B) and Sa2VA (4B) with overall scores of 26.9 and 3.2, respectively. 2. Related Work Large Vision Language Models. Staring from CLIP [50] and ALIGN [24], modern vision language models have adopted contrastive learning on large-scale image-text datasets for learning vision-text aligned representations. The trained models are also proven to work well on openvocabulary perception, such as segmentation [45, 71, 78, 79] and detection [21, 58, 61, 75]. The following works [31, 32, 64, 76] share the same network design, exploring modified loss functions and targeting data quality and filtering. Then, with the rise of large language models [5, 22, 56, 65], recent works [1, 10, 11, 40, 55, 77] mainly focus on multimodal large language models for open-ended settings, such as visual question answering or OCR benchmarks. On representative work, LLaVA [40], uses the CLIP to encode images into visual tokens and sends the visual tokens to LLMs. After that, the following works [1, 30, 41] improve designs with scaled high quality datasets, images, and videos cotraining. Meanwhile, several recent works [8, 14, 16, 46] also explore the visual encoder-free designs, which jointly learn the image and text representation in single transformer architecture. For example, SOLO [8] collects mixed language and vision datasets and trains one transformer for VQA tasks, while EVE [14] designs CLIP supervision to enhance visual token learning. Our work follows the visual encoder-free design, and we go step further by exploring pixel-grounded understanding tasks, including ground2 ing tasks and visual prompt understanding. To our knowledge, we are the first to apply encoder-free architecture for pixel-grounded understanding tasks. Referring Expression Segmentation. This task outputs specific masks driven by text description. Earlier works [19, 23, 36, 39, 67] explore various fusion architecture and modules to enhance text and vision feature alignments. Equipped with LLMs, several recent advanced works [27, 48, 49, 51, 63, 72, 73, 80, 82] propose more complex referring tasks, including reasoning referring or joint mask and caption generation. In particular, LISA [27] involves complex expression while GLaMM [51] annotates new dataset and proposes region-level caption and segmentation tasks. However, all these works contain complex designs: extra vision encoders, segmentation encoders, mask decoders, and prompt encoders. Our method, Pixel-SAIL, only has one transformer to jointly learn the joint visual and language feature. With proposed data engine and improved methods, Pixel-SAIL achieves good results with much simpler architecture. Visual Prompt Understanding. Understanding visual prompts plays an important role when building interaction between VLMs and human. Recent works [4, 38, 47, 51, 74] build new visual prompt datasets for region caption generation and prompt-aware VQA tasks. ViP-LLaVA [4] overlays the visual prompts directly onto the image canvas and fine-tunes the LLaVA on specific visual prompt dataset, while Osprey [74] explores pixel-wise mask regions into language instructions. Our method can also be extended into visual prompt understanding with our proposed prompt token injection design. 3. Method 3.1. Encoder Free MLLM and Plain Baseline Recently, several encoder-free MLLMs [8, 15, 16, 46] achieve comparable performance with those extra vision encoders. These models jointly learn vision and text features in single transformer, with much simpler architecture. In particular, SOLO uses simple project layer to map the image into visual tokens and then combines language tokens as the inputs of the transformer. However, no works have explored such new architecture for fine-grained vision language tasks (region caption, referring masks). Plain Baseline. To fill this gap, we first construct plain single transformer baseline, motivated by the previous ViTbased MLLMs [27, 72]. We start it with pre-trained encoder-free MLLM. For segmentation tasks, we modify previous mask generation methods into the single transformer. First, we reshape the hidden states of the last transformer layer of vision tokens RN into image features C. represents the number of vision tokens, denotes the channel size, and indicate 3 W the height and width of the image, stands for the downsampling stride. Then, the image features are then crossmultiplied with the hidden states of the predicted segmentation token RKC to generate the segmentation masks RK . signifies the number of predicted segmentation tokens, following previous works [27, 51]. For visual prompt understanding, we employ pooling-based method [74] to derive object representations RM from image patch embeddings C. These object embeddings are fed into the single transformer to represent the corresponding objects. represents the number of visual prompts, and denotes the patch size. For segmentation tasks, we adopt extra mask loss. Otherwise, we adopt the same text loss for VQA tasks and visual prompt understanding tasks. Limitation. The plain baseline demonstrates certain level of pixel-text alignment capability since both segmentation token and visual prompt token are jointly learned with vision and language tokens. However, the plain baseline exhibits several significant shortcomings: 1) The segmentation mask quality is poor due to the large feature downsampling stride (16 or 32), even when using simple pixel shuffle or bilinear interpolation for up-sampling. 2) The single transformer struggles to comprehend the referential target of object representation, as the object representation is summarized from image patch embeddings with poor semantic information. 3.2. Pixel-SAIL Method Given the substantial shortcomings, the performance of plain baseline in fine-grained pixel understanding tasks falls significantly, compared to vision-expert competitors (Sec.4). To solve these challenges, we have implemented three key enhancements to the baseline architecture. First, we integrate learnable up-sampling module to fully exploit the segmentation capabilities of the single transformer architecture. Second, we develop an innovative visual prompt injection mechanism that facilitates effective interpretation of visual prompt inputs. Our method enables early-stage fusion between vision tokens and visual prompt embeddings. Finally, we introduce dense feature distillation strategy that significantly improves the models capacity for extracting fine-grained visual features. These improvements collectively address the shortcomings of the plain baseline while maintaining its architectural simplicity. Learnable Up-sampling Module. Inspired by [35], we also incorporate simple learnable up-sampling model to generate the high-resolution features Fh 4 essential for pixel-level grounding. The up-sampling module comprises multiple up-sampling blocks, each consisting of transposed 2D convolution followed by depth-wise convolution. It effectively upscales the low-resolution features Fl C, derived from resized vision tokens, 4 Figure 2. The architecture of our proposed plain baseline and Pixel-SAIL. Pixel-SAIL is as simple and elegant as the plain baseline but demonstrates significantly improved performance. The examples on the right demonstrate that Pixel-SAIL possesses the capability for general conversation and comprehensive pixel-grounded understanding. to one-quarter of the original resolution. Visual Prompt Injection. Previous works [51, 72, 74] summarize the referenced object features via pooling on vision tokens from ViT encoder. However, there are no such visual tokens for encoder-free MLLMs. Thus, the inherent semantic deficiency hinders the single transformers ability to precisely identify referenced objects based solely on feature summaries derived from patch embeddings, where most are low-level cues, such as edges. To overcome this limitation, we propose an innovative visual prompt injection mechanism. Our approach integrates multiple visual prompt special tokens {V Pii [1, ]} into the large language models vocabulary. These tokens text embeddings VP RN are used to fill mask-based visual prompts Mvp RN , thereby creating visual prompt tokens VP HW 2 C. The vision tokens HW 2 are first added with these visual prompt tokens VP before being processed by the single transformer. This enhancement enables the model to accurately identify referenced objects by leveraging the corresponding special tokens {V Pii [1, ]} within the text instructions. Dense Feature Distillation. Due to the lack of large-scale, high-quality segmentation data like SA-1B [26], the method produces poor-quality masks, particularly at object boundaries. However, directly training on large-scale segmentation datasets would be costly and damage the original instruction following capabilities. To address both, we employ pre-trained segmentation experts to distill the single transformer, ensuring optimization of object details without hurting VQA capabilities. We perform distillation by leveraging mask features generated by Mask2Formers [12] pixel decoder on the upsampled mask features Fh 4 C, and utilizing features produced by SAM2s [53] encoder 4 Figure 3. Visual examples on our PerBench. Best view it in color and zoom in. on the low-resolution features Fl C. This simple distillation strategy improves segmentation quality with only negligible increase in training time. 3.3. Benchmark and Dataset Engine Our Benchmark: PerBench. We further manually annotate benchmark named PerBench (Pixel-grounded Understanding Benchmark). PerBench aims to address three aspects lacking in existing pixel grounding benchmarks. The first aspect is detailed object caption. Previous works [6, 34] have emphasized more detailed image captions, demonstrating that comprehensive captions signifi4 cantly enhance model performance. However, current object caption datasets such as Osprey-724k [74] and evaluation benchmarks like Refcocog provide only cursory object captions. To address this limitation, we leverage SOTA models InternVL2.5-78B [11] and Qwen2.5VL-72B [2] to generate detailed object captions. These detailed object captions are then meticulously screened and refined through manual review, ultimately yielding 500 precise, nuanced object captions to serve as robust evaluation benchmark. METEOR [3] serves as the evaluation metric for the detailed object caption task. The second aspect is the assessment of visual-prompt understanding ability in multiple-choice format. Although captioning tasks can accurately reflect models visual prompt understanding ability, precise and fair evaluation is difficult. Rule-based metrics such as CIDEr [57] and METEOR [3] are affected by response length, format, and ground-truth quality, while using models as evaluators inevitably introduces model bias. Therefore, fair and quantitative visual-prompt understanding benchmark is necessary. Inspired by MMBench [43] and MME [20], we manually annotated 500 multiple-choice questions based on detailed object captions, covering the examination of models understanding of referenced objects appearance, attributes, uses, and relationships with surrounding objects. MLLMs need to perceive the attributes of referenced objects accurately and have instruction-following ability to select the appropriate choice correctly. Accuracy is selected as the evaluation metric for the visual prompt-based multiple-choice questions. The third aspect is segmenting objects jointly referenced by visual prompts and text, abbreviated as V-T RES. It aims to test the models ability to understand objects indicated by user-input visual prompts and segment associated objects according to text instructions. This task comprehensively assesses the MLLMs pixel-grounded understanding ability, requiring the model to possess precise visual prompt understanding capabilities, text reasoning abilities, and pixel grounding skills. We also manually annotate 500 V-T RES samples, which five expert annotators double-check. Similar with RefCOCO series datasets, we select cIoU and gIoU as the evaluation metric for V-T RES task. The overall score of PerBench is the average of the normalized scores (0-100) from the above three tasks. Our benchmark can be used to evaluate pixel-wise MLLMs and point out more challenging directions for detailed object understanding, joint visual prompts, and text understanding to the current community. Dataset Engine. To fully unleash the potential of the single transformer, we collect diverse pixel-grounded data, including segmentation datasets and visual-prompt understanding datasets, following previous works [16, 46]."
        },
        {
            "title": "For",
            "content": "segmentation-related data, we first use RefCOCO/+/g [25, 70] and COCO [37] semantic segmentation data used in LISA [27], the Grandf dataset (214k samples) used in GLaMM [51], and MUSE data (246k samples) used in PixelLM [54]. We also use recent Pixel2Cap [69] data (comprising 20k images) and organized it into the referring segmentation format. Finally, we further add COCO [37] panoptic segmentation data and structured it as: Question: Please segment the {class name} in instance mode. Answer: {class name}-1 [SEG], ..., {class name}-n [SEG]. For visual prompt understanding, we employ two public datasets: Osprey-724k [74] and Pixel2Cap [69]. Additionally, we reformat the COCO dataset into question-answer structure specifically designed to query object categories. To enhance the models capability for fine-grained object description, we prompt the InternVL2.5-78B [11] model to generate approximately 300k detailed object captions derived from 10k SA-1B [26] images. Lastly, to maintain the instruction following ability, we also integrate the LLaVA1.5 [40] 665k dataset into our training data. Training. We combine all the aforementioned data for cotraining. The loss function consists of the next token prediction loss Lntp, the segmentation loss Lseg, and the distillation loss Ldistill: = Lntp+Lseg +αLdistill, Lseg = λLce+βLseg, (1) where α is set to 0.5, λ to 2.0 and β to 0.5. 4. Experiment Implementation Details. We extensively evaluate our meta-architecture using two open-source encoder-free multimodal large language models: SOLO [8] and EVEv2 [16]. For SOLO, following [28], we modify the attention mechanism between vision tokens from causal attention to full attention and conduct supervised fine-tuning on the LLaVA1.5 665k dataset. For SOLO, we modify the attention mechanism between vision tokens from causal attention to full attention and replace the LLM with Qwen2.5 [66] 0.5B and 3B, respectively. For EVEv2, we retain its original architecture and weights without any modifications. We build PixelSAIL 0.5B and 3B based on our modified SOLO baseline, and 7B on EVEv2. When training Pixel-SAIL based on SOLO, we maintain the original resolution of input images. For images with long side exceeding 1024, we preserve the aspect ratio and resize the long side to 1024. When training Pixel-SAIL based on EVEv2, we resize the images to the closest to 8002 pixels to reduce training costs, which differs from the original setting of 16002. The training process is conducted on 32 A100 (80GB) GPUs using the AdamW [44] optimizer with cosine decay learning rate scheduler. We set the initial learning rate to 4e-5, the warmup ratio to 0.03, and the batch size to 256. The training duration for the 0.5B and 3B models is 12 hours and 24 hours, respectively. 5 Table 1. Performance on referring segmentation benchmarks. The evaluation metric is cIoU. ft denotes fine-tuning on the specific dataset. Method LLM Size RefCOCO+ RefCOCOg RefCOCO gRefCOCO val testA testB val(U) test(U) val testA testB val testA testB VLT [17] CRIS [59] LAVT [68] PolyFormer-L [42] ReLA [39] LISA (ft) [27] PixelLM [54] GSVA (ft) [63] GroundHog [81] GlaMM (ft) [51] SAM4MLLM [9] LaSagnA [60] OMG-LLaVA (ft) [80] F-LLM [62] Sa2VA [72] - - - - - 7B 7B 7B 7B 7B 7B 7B 7B 7B 4B Pixel-SAIL Pixel-SAIL (ft) Pixel-SAIL Pixel-SAIL (ft) 0.5B 0.5B 3B 3B Referring Segmentation Specialist Without MLLM 56.3 62.3 62.1 69.3 66. 65.1 66.3 64.5 70.5 72.6 73.5 66.4 69.1 65.8 74.3 70.8 73.0 75.7 76.2 61.0 68.1 68.4 74.6 71.0 50.1 53.7 55.1 61.9 57.7 55.0 59.9 61.2 69.2 65.0 57.7 60.4 62.1 70.2 66. MLLMs With Vision Expert 70.8 71.7 67.7 75.0 78.7 77.8 70.6 73.1 75.2 - 58.1 58.3 58.6 64.9 64.6 65.8 60.1 63.0 58.5 - 67.9 69.3 71.1 74.1 74.2 74.5 70.6 72.9 70.1 76.7 70.6 70.5 72.0 74.6 74.9 75.6 71.9 72.9 71.7 - MLLMs Without Vision Expert 75.8 77.0 79.7 79.7 65.4 68.0 72.0 71.2 75.4 75.6 78.7 78.5 76.7 76.1 80.4 79.4 67.5 70.5 72.7 76.0 73.8 74.9 73.0 76.4 78.5 79.5 79.6 76.8 78.0 75.8 80. 77.9 79.1 80.8 81.8 70.5 73.2 75.8 78.3 76.5 79.1 76.5 77.4 79.9 83.2 82.8 78.7 80.3 79.5 - 80.5 81.7 82.6 83.4 65.2 66.1 68.8 73.3 70.2 72.3 68.2 72.8 75.7 76.9 76.1 73.8 74.1 72.4 - 75.9 77.0 79.0 78.8 52.5 55.3 57.6 - 56.4 - - 61.7 66.7 - 66.3 38.1 - - - 63.9 68.0 67.7 72.1 62.2 63.8 65.3 - 59.0 - - 69.2 - - 70.1 50.4 - - - 71.5 74.0 74.6 77.1 50.5 51.0 55.0 - 58.4 - - 60.3 - - 63.2 42.1 - - - 63.6 66.8 67.1 70.4 Table 2. Region caption performance on RefCOCOg dataset. Method Size METEOR Pixel-SAIL Pixel-SAIL Sa2VA OMG-LLaVA Osprey GLaMM 0.5B 16.0 3B 17. 4B 17.3 7B 15.3 7B 16. 7B 16.2 Table 3. The performance on our PerBench. Due to the lack of visual prompt understanding capability, LISA scores 0 on all tasks. Model Size Detailed Caption MCQ Acc METEOR V-T RES cIoU gIoU Overall Score LISA [27] Osprey [74] GLaMM [51] Sa2VA [72] 7B 7B 7B 4B Pixel-SAIL Pixel-SAIL 0.5B 3B 0 13.4 12.6 19.2 21.4 24.2 0 0.12 0.14 0.71 0.69 0.74 0 0 24.3 31. 29.7 33.4 0 0 14.6 21.9 19.8 23.5 0 8.5 15.3 39.0 38.4 42.2 Table 4. Performance on the VQA benchmarks. refers to the use of an 8002 resolution, which differs from the 16002 resolution in the pre-trained model. Model LLM Size MME MMBench SEED MMStar SOLO SOLO EVEv2 Pxiel-SAIL Pixel-SAIL Pixel-SAIL 0.5B 3B 7B 0.5B 3B 7B 523.2/222.5 1155.7/257/5 1128.0/240.7 564.1/150.7 1187.3/242.9 1081.0/260.4 13.8 53.4 60. 31.8 56.3 58.9 45.5 65.4 54.2 52.2 66.1 64.7 26.2 40.3 44.9 26.3 40.1 44.3 Evaluation Setup. For visual prompt understanding and general image QA tasks, we adhere to the same setting as the base MLLM. In the case of segmentation-related tasks, if the model fails to predict [SEG] token, we compel it to produce [SEG] token to ensure the generation of the segmentation result. 4.1. Main Results Results on Referring Segmentation Benchmarks. We compare Pixel-SAIL with other pixel-grounded MLLMs and segmentation specialists on the RefCOCO+ [70], RefCOCOg [70], RefCOCO [25], and gRefCOCO [39] datasets. The comparison results are shown in Tab. 1. PixelSAIL 0.5B achieved 70.8, 75.4, and 77.9 cIoU on the validation splits of RefCOCO+, RefCOCOg, and RefCOCO, outperforming all segmentation specialists with comparable model sizes while also maintaining image conversation capabilities. Compared to the classical SAM-based MLLM competitor LISA-7B [27], Pixel-SAIL 0.5B surpassed it by 4.2, 7.9, and 7.8 cIoU on RefCOCO, RefCOCO+, and RefCOCOg respectively, despite having much smaller model size (0.5B vs. 7B). On the more complex gRefCOCO dataset that includes multi-object segmentation, Pixel-SAIL 0.5B outperformed the carefully designed GSVA-7B [63] by 6.3, 4.8, and 6.5 cIoU on validation, testA, and testB splits respectively. When scaling the model to 3B, Pixel-SAIL achieved 75.7, 78.7, 80.8, and 67.7 cIoU on RefCOCO+, RefCOCOg, RefCOCO, and gRefCOCO datasets respectively, surpassing all larger-sized (7B) MLLMs assisted with vision experts. Pixel-SAIL-3B even outperformed the SOTA Sa2VA-4B [72] (which uses the powerful InternVL26 Table 5. Ablation study on the components of Pixel-SAIL. RC denotes region caption on RefCOCOg dataset. Model RefCOCO/+/g RC Plain Baseline + Upsampling + Training Data + VP Injection + Distillation 64.5/57.3/60.1 69.7/62.5/65.3 76.2/69.6/73.8 77.4/70.4/75.2 77.9/70.8/75.4 1.0 0.9 1.4 16.1 16.0 Table 6. Ablation study on Base MLLM. The training data only includes LLaVA-665k and RefCOCO/+/g. Table 7. Ablation on the training data. RC denotes region caption on RefCOCOg dataset. MLLM Size RefCOCO/+/g Data RefCOCO/+/g RC SOLO 0.5B 69.7/62.5/65.3 73.2/66.4/69.1 SOLO 74.9/68.7/71.3 EVEv 3B 7B Basic Data + Seg Data + VP Data 69.7/62.5/65.3 76.2/69.6/73.8 77.4/70.4/75.2 - - 16.1 Table 8. Ablation study on the distillation strategy."
        },
        {
            "title": "Data",
            "content": "RefCOCO/+/g w/o Distill M2F SAM2 Both 77.5/70.5/75.5 77.7/71.0/75.8 77.8/70.9/75.9 78.1/70.8/76.1 4B [10] and SAM2-L [53]), achieving performance advantages of 1.4 and 2.0 cIoU on the more challenging RefCOCO+ and RefCOCOg datasets respectively. Results on Visual Prompt Understanding Benchmarks. We evaluate the region caption performance on the RefCOCOg dataset, with results shown in Tab. 2. The training dataset of Pixel-SAIL does not include the RefCOCOg region caption dataset, so we directly evaluate its zero-shot performance. Pixel-SAIL-0.5B achieves METEOR score of 16.0, surpassing OMG-LLaVA 7B by 0.7 points. When scaling the model to 3B, Pixel-SAIL achieves METEOR score of 17.6, outperforming carefully designed larger models such as Osprey 7B and GLaMM 7B by 1.0 and 1.4 points respectively. Results on PerBench. We have benchmarked several popular pixel-grounded MLLMs on our proposed PerBench, with results shown in Tab. 3. LISA [27] scores 0 points across all tasks due to its inability to understand visual prompt inputs. Osprey [74] demonstrates strong object caption capabilities; however, it achieved only 13.4 METEOR in detailed caption tasks and 12.0% accuracy in MCQ tasks due to limitations from short object caption lengths in its training data and impaired instruction-following ability. GLaMM [51] and Sa2VA [72] both exhibit comprehensive prompt understanding and segmentation capabilities, though GLaMMs weaker instruction-following ability resulted in only 14.0% accuracy in MCQ tasks. PixelSAIL-0.5B achieves an overall score of 38.4, comparable to Sa2VA-4B despite Pixel-SAIL having more powerful base MLLM and segmentation expert. Notably, Pixel-SAIL-3B achieves an overall score of 42.2, outperforming Sa2VA-4B across all three tasks. Results on VQA Benchmarks. We compare the visual question answering performance of Pixel-SAIL with the corresponding base MLLMs on the MME [20], MMBench [43], SEED [29], and MMStar [7] benchmarks, and the results are presented in Tab. 4. When the model size is 0.5B, Pixel-SAIL demonstrates performance improvements over the base MLLM across all four benchmarks, particularly on MMBench, where the score increased from 13.8 to 31.8. However, when the model size is 3B and 7B, PixelSAILs performance is on par with that of the base MLLMs, which may be constrained by the current quantity (less than 2M) and quality of visual prompts and segmentation data. Figure 4. Visualization results of Pixel-SAIL on diversity tasks. Best view it in color and zoom in. From top to bottom are visual prompt-based object caption, single/multi-object referring segmentation, vision-text referring segmentation, image caption and QA, and visual-prompt based conversation. Visual prompts in the form of points and boxes are converted into mask prompts using SAM [26]. For more visualization results and comparisons with other MLLMs, please refer to the appendix. Figure 5. Image feature visualization results. From left to right are the image feature of the base MLLM, the image feature of Pixel-SAIL, and the mask feature of Pixel-SAIL. 4.2. Ablation Studies Effectiveness of Each Component. We conduct comprehensive ablation studies on the proposed components, with results presented in Tab. 5. Our plain baseline, trained with LLaVA-665k and RefCOCO/+/g data, achieves only 64.5, 57.3, and 60.1 cIoU on the RefCOCO, RefCOCO+, and RefCOCOg datasets, respectively. Moreover, this baseline completely fails on the visual prompt understanding task, attaining merely 1.0 METEOR on the region caption task. Upon incorporating the learnable upsampling mod7 ule, segmentation quality improves dramatically, with the model reaching 76.2, 69.6, and 73.8 cIoU on RefCOCO, RefCOCO+, and RefCOCOg. However, the model still cannot effectively interpret user-input visual prompts due to insufficient semantic information in the object representation. When we scale up the training data by introducing substantial amounts of segmentation data and visualprompt understanding data, the models segmentation capabilities are further enhanced. Despite scaling the training data, the model continues to struggle with visual prompt inputs because of the limited semantic information in the object representation. After implementing our proposed visual prompt injection mechanism, the model demonstrates significant improvements in visual prompt understanding, achieving 16.1 METEOR on the region caption task. Interestingly, we observe that enhanced visual prompt understanding capabilities positively influence referring segmentation performance. Finally, incorporating the distillation strategy further refines the models detailed segmentation quality. Ablation on Various MLLMs. To demonstrate the effectiveness of Pixel-SAIL, we validate across different architectures and sizes, with results shown in Tab. 6. To reduce training costs, we use only LLaVA-665k and RefCOCO/+/g data for training and evaluate on the referring segmentation task. When using our modified 0.5B SOLO as the base MLLM, Pixel-SAIL achieves cIoU scores of 69.7, 62.5, and 65.3 on RefCOCO/+/g. When scaling the model size to 3B, Pixel-SAILs performance improves by 3.5, 3.9, and 3.8 cIoU on RefCOCO/+/g. When using EVEv2-7B as the base MLLM, despite the attention between vision tokens changing from full attention to causal attention and the architecture transitioning to an MOE architecture, PixelSAIL achieves cIoU scores of 77.4, 70.4, and 75.2 on RefCOCO/+/g, demonstrating that performance consistently increases with model scaling. Ablation on Data Scaling. Data plays crucial role in the performance of Pixel-SAIL. As showin in Tab. 7, we conduct comprehensive ablation studies on the training data to evaluate its impact. When trained solely with basic data (including LLaVA-665k and RefCOCO/+/g datasets), PixelSAIL achieves 69.7, 62.5, and 65.3 cIoU on RefCOCO, RefCOCO+, and RefCOCOg, respectively. Upon scaling the segmentation-related data, Pixel-SAIL demonstrates significant performance improvements of 6.5, 7.1, and 8.5 cIoU on these datasets. Furthermore, incorporating visual prompt data for mixed training not only enhances the models visual prompt understanding capabilities but also yields additional performance gains of 1.2, 0.8, and 1.4 cIoU on RefCOCO, RefCOCO+, and RefCOCOg, respectively. Ablation on Distillation Strategy. Distillation is highly effective method for infusing knowledge into Pixel-SAIL. We conduct ablation studies on the distillation strategy, and the results are presented in Tab. 8. We use the average cIoU across all splits as the evaluation metric. When only Mask2Former [12] is employed to distill high-resolution mask features, Pixel-SAIL achieves performance gains of 0.2, 0.5, and 0.3 on RefCOCO/+/g. When SAM2 [53] is used to distill low-resolution image features, Pixel-SAIL obtains performance improvements of 0.3, 0.4, and 0.4 on RefCOCO/+/g. When both teacher models are utilized collaboratively, performance gains of 0.6, 0.3, and 0.5 are achieved. Additionally, the extra computational cost introduced by the distillation strategy is minimal, increasing the training time by only about 5% for Pixel-SAIL-0.5B. 4.3. Visualization Analysis Visual Comparison. In Fig. 4, we showcase Pixel-SAILs visualization results on diverse tasks. Pixel-SAIL flexibly interprets both visual prompts and text instruction inputs, responding with text and segmentation masks. Visual Affinity Map Analysis. We use PCA dimensionality reduction algorithm to visualize vision features, with results shown in Fig. 5. Our Pixel-SAILs image features (3rd column) are denser and more diverse compared to the base MLLMs image features (2nd column). Pixel-SAILs mask features, after the upsampling module, are denser and have better segmentation edges. Interestingly, Pixel-SAILs image features (more focused on understanding, combining factors such as categories, colors, positions, etc.) exhibit different characteristics from mask features (more focused on perception, categories, and instances). As seen in the second rows third and fourth columns, the cars on the left and right have relatively distant feature representations in the image features, while they are very close in the mask features. 5. Conclusion We explore the simplest architecture for pixel-grounded unIn particular, we present Pixel-SAIL, derstanding tasks. which extends current SAIL-like MLLM for fine-grained understanding with three technical improvements (learnable upsampling module, new visual prompt encoding , and segmentor feature distillation). For the first time, our work proves that even without extra visual experts (visual encoder, segmentation models), one single transformer can still achieve stronger performance on four public referring segmentation benchmarks. We further introduce more challenging benchmark, Perbench, to promote the development of pixel-MLLM community. Limitation and Future Work. Our work provides the simplest solution for pixel-grounded tasks. However, one limitation is that we only adopt 1.7M data for co-training. We will further explore Pixel-SAIL on more data (for example, billion-level masks along with visual prompts [26]) for cotraining."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. ACL, 2005. [4] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In CVPR, 2024. [5] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. Internlm2 technical report. [6] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [8] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. single transformer for scalable vision-language modeling. TMLR, 2024. [9] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. Sam4mllm: Enhance multimodal large language model for referring expression segmentation. ECCV, 2024. [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. [12] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask In CVPR, transformer for universal image segmentation. 2022. [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019. [14] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. [15] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. NeurIPS, 2025. [16] Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free visionlanguage models. arXiv preprint arXiv:2502.06788, 2025. [17] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In ICCV, 2021. [18] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACMMM, 2024. [19] Guang Feng, Zhiwei Hu, Lihe Zhang, and Huchuan Lu. Encoder fusion network with co-attention embedding for referring image segmentation. In CVPR, 2021. [20] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [21] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In ICLR, 2022. [22] Louis Martin Hugo Touvron, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. [23] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In ECCV, 2020. [24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [25] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. ICCV, 2023. [27] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. [28] Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang. The scalability of simplicity: Empirical analysis of vision-language learning with single transformer. arXiv, 2025. [29] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024. 9 [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. [33] Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen, and Chen Change Loy. Omg-seg: Is one model good enough for all segmentation? In CVPR, 2024. [34] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024. [35] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. ECCV, 2022. [36] Chen Liang, Wenguan Wang, Tianfei Zhou, Jiaxu Miao, Yawei Luo, and Yi Yang. Local-global context aware transformer for language-guided video segmentation. arXiv preprint arXiv:2203.09773, 2022. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [38] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. [39] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized referring expression segmentation. In CVPR, 2023. [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [41] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [42] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and Manmatha. Polyformer: Referring image segmentation as sequential polygon generation. CVPR, 2023. [43] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint, 2017. [45] Timo Luddecke and Alexander Ecker. Image segmentation using text and image prompts. In CVPR, 2022. [46] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. CVPR, 2025. [47] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. In ECCV, 2024. [48] Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, and Salman Khan. Videoglamm: large multimodal model for pixel-level visual grounding in videos. arXiv preprint arXiv:2411.04923, 2024. [49] Lu Qi, Yi-Wen Chen, Lehan Yang, Tiancheng Shen, Xiangtai Li, Weidong Guo, Yu Xu, and Ming-Hsuan Yang. Generalizable entity grounding via assistance of large language model. arXiv preprint arXiv:2402.02555, 2024. [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [51] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. [52] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, 2020. [53] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. [54] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. [55] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. [56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. [57] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. CVPR, 2015. [58] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In ICCV, 2023. [59] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In CVPR, 2022. 10 Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [73] Haobo Yuan, Tao Zhang, Xiangtai Li, Lu Qi, Zilong Huang, Shilin Xu, Jiashi Feng, and Ming-Hsuan Yang. 4th pvuw mevis 3rd place report: Sa2va. arXiv preprint arXiv:2504.00476, 2025. [74] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In CVPR, 2024. [75] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions. In CVPR, 2021. [76] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [77] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [78] Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, and Pengfei Wan. DVIS: Decoupled video instance segmentation framework. In ICCV, 2023. [79] Tao Zhang, Xingye Tian, Yikang Zhou, Shunping Ji, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, and Yu Wu. Dvis++: Improved decoupled framearXiv preprint work for universal video segmentation. arXiv:2312.13305, 2023. [80] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Change Loy Chen, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. [81] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation. In CVPR, 2024. [82] Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, and Lu Qi. Are they the same? exploring visual correspondence shortcomings of multimodal llms. arXiv preprint arXiv:2501.04670, 2025. [60] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. LaSagnA: Language-based segmentation assistant for complex queries. arXiv preprint arXiv:2404.08506, 2024. [61] Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem, and Dacheng Tao. Towards open vocabulary learning: survey. arXiv pre-print, 2023. [62] Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, and Chen Change Loy. F-lmm: Grounding frozen large multimodal models. CVPR, 2025. [63] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized segmentation via multimodal large language models. In CVPR, 2024. [64] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. [65] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [66] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [67] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In CVPR, 2022. [68] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In CVPR, 2022. [69] Zuyao You, Junke Wang, Lingyu Kong, Bo He, and Zuxuan Wu. Pix2cap-coco: Advancing visual comprehension via pixel-level captioning. arXiv preprint arXiv:2501.13893, 2025. [70] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. [71] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, and Chen Change Loy. Open-vocabulary sam: Segment and recognize twenty-thousand classes interactively. arXiv preprint, 2024. [72] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and 11 Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "We first present more details on training and testing of our Pixel-SAIL in Sec. 6. Then, we present the detailed benchmark building process, in Sec. 7 and more challenging examples in PerBench in Sec. 8. Next, we present more comparison with current state-of-the-art pixelgrounded MLLMs, in Sec. 9. 6. More Detailed Training and Testing Training. We will present more details about the training, including dataset sampling specifications and distillation methodology. For the RefCOCO series [25, 70] datasets, we randomly sample 5 referring expressions per image and organize them into multi-round dialogue format as single training data point, processing all images for four epochs. For COCO [37] data, we sample 5 categories per image and randomly select either instance mode or semantic mode to structure the responses. In instance mode, objects are arranged by their center points from left to right. We process the COCO dataset for one epoch. For Pixel2Cap [69], our generated detailed object caption data, and Osprey [74] object description data, we randomly sample 1-5 visual prompts per image and randomly incorporate questions about non-existent visual prompts, with responses indicating that these visual prompts do not exist. These object caption datasets are processed for five epochs. For other segmentation-related or visual prompt-related data, we conduct one epoch. For LLaVA-665k, we randomly sample at 1:1 ratio alongside other data for joint training to ensure that the base MLLMs instruction-following capability remains intact. When the length of input tokens (including the length of vision tokens) exceeds 8192, we truncate the excess portion. For the 0.5B model, we use DeepSpeed Zero-1 [52] for training, and for the 3B and 7B models, we use DeepSpeed Zero-2 [52] for training."
        },
        {
            "title": "We distill",
            "content": "the mask features generated by the Mask2Former [12] pixel decoder and the lowest resolution features generated by the SAM2 [53] image encoder onto the upsampled mask features from Pixel-SAIL and the image features directly reshaped from vision tokens, respectively. We use bilinear interpolation to align spatial dimensions and implement learnable linear layer to align the channel size. The distillation process employs MSE loss with weight of 0.5. Testing. We have elaborated on the testing details of Pixel-SAIL on pixel-grounded benchmarks in the main text. For general image question answering benchmarks, we follow the prompt settings of the base MLLMs and use VLMEvalKit [18] for evaluation, without using additional LLM assistance to identify answers. 7. More Detailed Process on Benchmarking"
        },
        {
            "title": "Building",
            "content": "The construction of PerBench combines an automated model-generated pipeline with manual screening, correction, and annotation. The process is divided into three stages. The first stage involves annotating detailed object captions. We crop objects and draw visual prompts on the original images to prompt InternVL-2.5 78B [10] and Qwen-VL 2.5 72B [2] to generate detailed captions for the objects. These captions are then cross-validated using Qwen2.5 72B [66]. If all captions are consistent, they are integrated using an LLM; otherwise, the data are discarded. After the model automatically generates the detailed object captions, we manually select and correct 500 of them to form the final 500 detailed object caption data points in the benchmark. The second stage focuses on annotating visual-prompt question-answering data in an MCQ (Multiple Choice Question) format. In this phase, we manually generate multiple-choice question for each object caption obtained from the first stage. After completing the annotations, two quality control specialists perform cross-verification to identify and rectify any potential errors. The final stage contains the annotation of visual-text referring segmentation data. At this stage, we manually select and annotate object segmentation masks, referring visual prompts, and text from SAM images. During the annotation process, we consider various factors such as positional relationships, event relationships, appearance, size, and more, including cases with both single and multiple visual prompts. Once the annotation is complete, two individuals review it, and correct the errors. 8. More Challenging Cases in PerBench We present more detailed object caption samples from our PerBench in Fig. 6. The objects are derived from diverse senses and categories, encompassing humans, man-made objects, and natural landscapes. The object captions include basic categories, attributes, purposes, and relationships with surrounding objects. This high-quality benchmark will effectively advance the development of the visual prompt understanding community. More referring segmentation samples are illustrated in Fig. 7. Our manually annotated samples cover variety of scenes, such as indoor and outdoor settings, and include 1 Figure 6. More visualization examples of detailed object captions from our PerBench. objects of multiple granularities. The referring text encompasses positional relationships, event relationships, and more. This new task is more challenging than current pure text referring segmentation tasks. 2 Figure 7. More visualization examples of vision-text referring segmentation from our PerBench. tasks. Additionally, in the right example, Sa2VA demonstrates significantly weaker attention to non-core areas of the image, such as edges, compared to Pixel-SAIL, leading to frequent failures in segmenting objects near image boundaries. 9. More Comparison With SOTA Pixel-"
        },
        {
            "title": "Grounded MLLM",
            "content": "We conduct qualitative comparative analysis with the SOTA pixel-grounded MLLM, Sa2VA [72], and present the visualization results in Fig. 8. We observe that both PixelSAIL and Sa2VA achieve excellent results in most cases. However, Sa2VA performs significantly weaker than PixelSAIL in certain scenarios, despite utilizing the much more powerful InternVL2.5 [10] compared to our base encoderfree MLLM [8]. In the left examples, Sa2VA performs notably worse than Pixel-SAIL in multi-object segmentation 3 Figure 8. Visualization Comparison of Sa2Va and Pixel-SAIL."
        }
    ],
    "affiliations": [
        "Bytedance Seed",
        "WHU"
    ]
}