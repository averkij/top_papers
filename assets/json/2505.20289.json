{
    "paper_title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection",
    "authors": [
        "Zeyi Huang",
        "Yuyang Ji",
        "Anirudh Sundara Rajan",
        "Zefan Cai",
        "Wen Xiao",
        "Junjie Hu",
        "Yong Jae Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 8 2 0 2 . 5 0 5 2 : r VisualToolAgent (VisTA): Reinforcement Learning Framework for Visual Tool Selection Zeyi Huang1, Yuyang Ji, Anirudh Sundara Rajan1, Zefan Cai1, Wen Xiao2, Junjie Hu1, Yong Jae Lee1 1University of Wisconsin-Madison 2Microsoft Abstract We introduce VisTA, new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO) [1], our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTAs ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems. Project website: https://oodbag.github.io/vista web/."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) [2, 3, 4] and Vision Language Models (VLMs) [5, 6, 7] have unlocked impressive capabilities across tasks such as mathematical problem solving, code generation, and visual question-answering. However, these models are still inherently limited by the static nature of their architectures and the fixed information stored in their weights. To overcome these constraints, recent work explores augmenting LLMs and VLMs with external tools [8, 9, 10, 11, 12, 13, 14], dramatically expanding their functionality. Tool augmentation enables access to expert knowledge sources and dynamic computation, such as invoking Python interpreter for self-verification, thereby enhancing reasoning performance on complex tasks. However, the current paradigm for tool integration faces significant limitations in both LLMs and VLMs. Current approaches typically either rely on large-scale fine-tuning with human supervision to teach LLMs how to invoke tools [15, 16] or depend purely on the LLMs internal world knowledge in training-free manner [9, 17, 12]. These methods often rely on tool demonstrations [15, 16] or detailed tool descriptions to instruct LLMs on their usage [12, 17]. As result, they lack the ability to automatically explore, select, or adapt tool choices based on the specific characteristics of each query, particularly when multiple tools of the same type with varying capabilities are available which is common in real world settings. The challenge is particularly pronounced when integrating tools with unknown, partially documented capabilities or inconsistently performing capabilities, where actual performance may differ from descriptions. When retrieving tools from diverse sources, the LLMs lack comprehensive knowledge of their strengths and weaknesses. Without mechanism for experiential learning, the system cannot determine optimal tool selection or discover synergistic tool combinations that might emerge through collaborative deployment. In realistic applications, tools vary substantially in functionality and applicability across different problem domains. Within each tool category, individual implementations exhibit varying capabilities that make them differentially effective across contexts. This presents sophisticated decision-making challenge ideally suited for reinforcement learning (RL) [18]. RLs intrinsic exploration-exploitation mechanism enables agents to systematically assess and adaptively identify the most effective tools based 1 Figure 1: Overview of VisTA. (Left) Our method trains an agent to autonomously discover effective combinations of visual tools without human supervision. (Right) By decoupling the agent from the reasoner, the learned policy can be seamlessly integrated with wide range of reasoning models. on empirical performance rather than pre-specified rules. Through iterative interactions with its environment, an RL agent can learn adaptive strategies that dynamically adjust tool combination based on specific queries, or even potentially discover non-obvious tool utility patterns that may not be apparent from tool descriptions alone. Therefore, we introduce new RL framework, VisualToolAgent (VisTA), that trains autonomous agents to intelligently select optimal tools from multiple available options. Unlike training-free [9, 17, 12] and fine-tuning approaches [15, 16], our RL-based method inherently supports exploration-exploitation mechanisms, allowing agents to systematically experiment with various tool combinations through iterative interactions. In this work, we focus on the visual reasoning task. Our framework consists of an autonomous agent that learns through end-to-end RL training to dynamically select optimal tools for guiding fixed VLM in solving complex visual reasoning problems. As bonus, our framework allows the VLM itself to remain frozen during RL training, which means that the agents learned selection strategies can be transferred to different reasoning models without retraining, critical advantage for deployment flexibility. Our framework employs the Group Relative Policy Optimization (GRPO) [1] algorithm to enable our agent to autonomously discover effective tool-selection pathways entirely from scratch, without explicit reasoning examples. For detailed look at how the agent performs inference and selects tools in practice, see the examples in Fig 6, 7. We evaluate our method on ChartQA [19], challenging benchmark for visual reasoning that requires models to interpret numerical data, textual labels, and complex visual structures, such as precisely estimating bar heights in charts. We also test on Geometry3K [20], which assesses fine-grained diagram understanding and logical reasoning. This task requires models to accurately parse visual elements (e.g., figures, labels) and align them with textual question conditions to perform math-based reasoning. Our experimental results demonstrate that our RL-based approach significantly outperforms training-free methods. The performance gap widens further when testing on Out-of-Distribution (OoD) variants, robust version that introduces perturbations to the standard dataset. This demonstrates our methods superior ability to generalize to novel scenarios and maintain performance under challenging visual conditions."
        },
        {
            "title": "2 Related Work",
            "content": "Tool-Augmented Reasoning. LLMs have shown significantly improved reasoning capabilities when augmented with external tools such as search engines [21], calculators [22], and Python interpreters [8, 10]. Programming-based approaches [8, 10], for example, integrate Python interpreters to simplify intermediate steps and validate final outputs, enhancing accuracy on mathematical tasks. Similar strategies 2 have been adopted in the visual domain. Recent VLM methods [13, 14, 12] generate Python code to invoke specialized vision modules, decomposing complex visual tasks into simpler sub-tasks, each addressable by dedicated vision tools. However, existing approaches often rely on human demonstrations or annotations [15, 16], or operate in training-free manner using only the models internal knowledge [17, 12]. These methods typically offer limited tool diversity and depend heavily on explicit tool descriptions [15, 12, 16, 17], lacking the capacity to autonomously explore or adapt tool use to specific queries. In contrast, our proposed VisTA framework enables VLMs to autonomously explore and select tools based on empirical performance, without human-designed priors. By training an agent with RL, VisTA discovers context-dependent tool selection policies that adaptively tailor tool usage to the nuanced requirements of each visual reasoning task. Reinforcement Learning for Enhanced Reasoning. RL has shown strong potential in enhancing complex reasoning abilities and enabling inference scaling. Models like OpenAIs o1 [23] and DeepSeekR1 [24] have achieved notable success in tasks such as mathematical problem solving by leveraging long chain-of-thought (CoT) [25] reasoning. These models excel at strategies like mistake correction, step decomposition, and iterative refinement, resulting in more structured and extended reasoning. Recently, several studies [26, 27] have adapted the DeepSeek-R1 framework to visual reasoning, training models to generate CoT-based outputs directly from visual inputs. While these approaches focus on finetuning the reasoning model itself to perform end-to-end visual inference, our work takes an orthogonal perspective. Instead of modifying or retraining the reasoning model, we propose to train an autonomous agent that reasons about which tools to select to best assist frozen reasoning model in solving given query. By learning to select supportive tools based on each query, our agent enhances performance without altering the models internal parameters (thereby preserving generalization to other tasks). This design also ensures broad compatibility across different visual reasoning models and offers flexible, modular strategy for improving multimodal reasoning systems. ReTool [28] is concurrent work that uses RL to teach LLMs to invoke code execution tools for text-based reasoning. In contrast, our work tackles the challenging setting of visual reasoning with diverse tool choices, requiring agents to adaptively select the most effective tools. Visual Reasoning Tasks. Some visual reasoning tasks, such as depth estimation and spatial reasoning [29], can be effectively solved using straightforward, specialized tools like depth estimators or object detectors. In contrast, we target more complex and cognitively demanding tasks, where optimal tool selection is query-dependent and may not be obvious. Chart understanding [19, 30] is challenging task and strong indicator of visual reasoning capabilities. It requires models to process numerical data, textual labels, and complex visual structures, not only identifying visual elements but also performing precise interpretation and measurement, such as accurately estimating bar heights in bar charts. Geometry questions [20, 30, 31] pose similarly demanding challenge, requiring fine-grained diagram understanding followed by text-based reasoning grounded in visual details. It evaluates the multi-modal logical reasoning abilities of VLMs by asking them to decode visual elements in diagrams, such as characters and geometric figures, and align them with conditions specified in textual questions for mathematical problem solving. Our method achieves strong performance on both types of benchmarks, demonstrating the effectiveness of learning to select tools adaptively for chart and geometry reasoning tasks."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present VisTA, reinforcement learning (RL) framework for tool-augmented visual reasoning. In contrast to previous methods that rely on training-free or fine-tuned strategies, VisTA empowers an agent to learn how to select tools through trial-and-error interaction, without requiring manual supervision. By harnessing the exploration-exploitation dynamics of RL, the agent adaptively chooses from wide array of tools based on performance feedback. Notably, the core reasoning model is kept fixed, allowing the learned tool-selection policy to transfer across different reasoning backbones without the need for additional training."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "Let (q, I) denote visual-language query, consisting of an image and an associated text query q, sampled from task distribution D. We consider setting with frozen visual-language model (the reasoner ) fθ and library of external tools = {T1, . . . , TM }, where is the total number of available tools. These tools include diverse specialized vision modules; e.g., in chart understanding tasks, we utilize chart-to-SVG converters, chart-to-table extractors, and chart-to-caption generators. 3 Figure 2: Policy Optimization. Given user query, the agent selects tools from pre-defined set of external tools. The tools are applied to the image, and their outputs and the query are fed to frozen reasoner model. Both the Direct Path (query+image) and the Tool-Augmented Path (query+tools+image) are evaluated to compute reward signal, which is used to update the agents tool-selection policy. We define the agents observation or state as = (q, I), encompassing both the image and its corresponding query. Our objective is to learn selection policy πϕ(t s), implemented as vision-language model (the agent), that determines which tools to deploy for given query. Formally, the selection policy maps the state to sequence of selected tools = (1), . . . , (K), where (i) and . Here, represents the number of tools selected for the specific query, which adaptively varies based on task complexity."
        },
        {
            "title": "3.2 VisTA Framework",
            "content": "Our VisTA framework implements an end-to-end pipeline to learn policy for dynamic tool selection in visual reasoning tasks (Figure 2), leveraging reinforcement learning to enable systematic exploration of tool combinations. The pipeline operates as follows: 1. The vision-language agent observes the state = (q, I) and selects sequence of tools = (1), . . . , (K) via policy πϕ(t s). 2. Each selected tool is executed on the image to produce outputs o(i) = (i)(I) for {1, . . . , K}. 3. These outputs are combined with the original inputs to form an augmented prompt. 4. The frozen reasoner fθ processes the augmented prompt to produce the final answer yimg+tools = fθ(q, I, {o(1), . . . , o(K)}). During training, we also compute baseline prediction yimg = fθ(q, I) using only the original query and image, which enables us to measure the impact of selected tools on reasoning performance, as will be detailed in our reward formulation in the following section."
        },
        {
            "title": "3.3 Policy Optimization",
            "content": "To train the agent to discover effective tool combinations through systematic exploration, we build upon the GRPO algorithm [1]. The key difference is that, to adapt it for visual tool selection, we design novel task-specific reward function. Given context, particular tool may be helpful, harmful, or neutral from the reasoners perspective. We reward the agent if the selected tool enables the reasoner to answer the question correctly. If the tool does not lead to correct answer, the reward depends on the reasoners performance without the tool-augmented input. Specifically, if the reasoner also fails without the tool, the reward is set to 0, since 4 the tool had no effect on the outcome. However, if the reasoner could have answered correctly on its own, we assign reward of -0.5, penalizing the negative influence on the final answer. More formally, at each training step, given an input state = (q, I), we sample tool selection j=1 from the current policy πϕ. For each candidate tj, we compute tool-aware reward candidates {tj}G rj by comparing the reasoners predictions with and without tools: rj = if yimg = and yimg+tools +1 0.5 if yimg = and yimg+tools if yimg = and yimg+tools 0 if yimg = and yimg+tools +1 j = = = = (tools help) (tools hurt) (no change) (tools neutral) where yimg is the prediction from the reasoner using only the question, yimg+tools is the prediction with selected tools from tj, and is the ground truth. This reward design incentivizes tool selections that lead to correct predictions when the image alone is insufficient, while penalizing those that hurt performance. We then calculate each candidates group-relative advantage as Aj = rj mean({r1,...,rG}) . This normalization contextualizes each tool selections performance relative to other candidates, enabling the agent to distinguish which tool combinations performed better for specific query regardless of absolute reward values. Finally, the policy πϕ is updated by maximizing the following: std({r1,...,rG}) JVisTA(ϕ) = EsDE {tj }G j=1πϕold (s)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 min (cid:0)rratio Aj, clip(rratio (cid:1) βDKL(πϕπref) , 1 ϵ)Aj = πϕ(tj s) where rratio πϕold (tj s) , πϕold is the previous policy, and πref is fixed reference policy. This objective balances exploration and exploitation through ratio clipping and KL regularization. The ratio term prevents large policy updates, while the KL divergence term maintains stability by keeping the policy close to reference distribution. Through this optimization, tools that consistently improve reasoning performance receive higher selection probabilities, allowing the agent to discover effective tool combinations based on empirical evidence rather than predefined rules."
        },
        {
            "title": "3.4 Tool Selection Prompting",
            "content": "We design structured prompt template to guide the agents tool selection process. For chart understanding tasks, this template presents 9 tools (indexed 0-8) organized across three functional categories: You You have access are an expert agent tasked with selecting tools to solve chart reasoning tasks. to 9 tools indexed from 0 to 8, each belonging to one of three functional types: type1, type2, and type3... Function: (E) ... helpful. Output only selected indices as comma-separated list within <answer> tags.\" This prompt structure provides only minimal tool categorization without detailed function descriptions or usage demonstrations that training-free methods typically rely on. Instead, our framework incentivizes the agent to empirically discover and adaptively select the most effective tools through reinforcement learning. This approach is particularly valuable when integrating tools with unknown, partially documented, or inconsistently performing capabilities. Given chart and query {Question}, select the index number(s) of tools that are most type1 (C), 3: type2 (D), 4: type1 (A), 1: type1 (B), 2: type 0:"
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our VisTA framework on visual reasoning benchmarks, comparing to training-free baselines, alternative RL-based methods. We also analyze VisTAs tool selection strategies and distribution, and agent behavior dynamics over training."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We conduct experiments on two main datasets: ChartQA [19] and Geometry3K [20]. Both datasets provide paired visual inputs and questions, but differ in their reasoning styles and evaluation protocols. We train our VisTA agent using the training sets of both datasets, and evaluate on their respective test sets. For ChartQA, in addition to the original test set, we construct an out-of-distribution (OoD) 5 Method Agent Model Reasoning Model ChartQA ChartQA(OoD) Geometry3K Training-Free Training-Free Training-Free RL Ours Training-Free Training-Free Training-Free Ours - QwenVL 7B GPT4o - QwenVL 7B - QwenVL 7B GPT4o QwenVL 7B QwenVL 7B QwenVL 7B QwenVL 7B QwenVL 7B QwenVL 7B GPT4o GPT4o GPT4o GPT4o 76.4 76.1 73.0 77.5 79.4 84.3 82.3 84.6 88.9 62.3 66.8 66.4 64.3 73.2 50.1 67.1 73.3 76.8 54.0 51.3 51.5 41.0 55. 50.1 48.7 49.5 52.4 Table 1: Main Results. These results highlight VisTAs ability to support complex, multi-modal reasoning where tools provide complementary visual understanding, and its flexibility and compatibility with stronger reasoning models like GPT-4o at deployment time. variant by manually removing textual labels from the charts. This setting is designed to assess the models reliance on textual cues versus its ability to reason visually. As result, we evaluate on three test sets in total: ChartQA (in-domain), ChartQA-OoD (perturbed version), and Geometry3K. ChartQA adopts an open-ended answer format, so we report relaxed accuracy (within 5% of the gold answer) as the evaluation metric, which credits semantically correct answers. In contrast, Geometry3K is multiplechoice benchmark and uses standard accuracy to measure exact match with the correct option. For both ChartQA and Geometry3K, we randomly sample 800 examples for training and evaluate on their full test sets. We train our method using 8 NVIDIA A100 GPU, with batch size of 8 and 4 answer generations per query to encourage diverse tool interactions. We use the AdamW[32] optimizer with learning rate of 5 105. The agent is trained for 100 iterations. During inference, we use only the Tool-Augmented prediction path. We curate benchmark-specific tool pools tailored to the reasoning demands of each domain. For ChartQA, we construct pool of 9 tools grouped into three functional types: chart-to-SVG, chartchart-to-table tools (T0: UniChart [33], T1: to-table, and chart-to-caption, with 3 tools per type. DePlot [34], T2: ChartMoE [35]) convert charts into structured format tables, chart-to-SVG tools (T3: OpenCV [36], T4: ChartDet [37], T5: ChartQCR [38]) extract geometry elements (bar, line, pie), and color from charts, and captioning modules (T6: ChartAssistant [39], T7: ChartVLM [40], T8: QwenVL32B [41]) summarize visual content at global level. For Geometry3K, we use smaller pool of 4 tools categorized into two types: symbolic parsers and specialist geometry solvers, with 2 tools in each category. Symbolic parsers extract entities and relations (e.g., angles, labels), while the specialist models take in the question and diagram to generate sequence of steps which to solve the problem. For the symbolic parser, we use T0: DiagramFormalizer [42] and T1: Inter-GPS [20]. For our specialist solvers, we use T2: G-LLaVA 13B [43] and T3: MultiMath [44]. Each tool within type is implemented independently, allowing the agent to learn fine-grained preferences among tools with overlapping but distinct capabilities."
        },
        {
            "title": "4.2 Main Results",
            "content": "We first evaluate our VisTA framework across three visual reasoning benchmarksChartQA, ChartQAOoD, and Geometry3K, comparing against training-free baselines, alternative RL-based approaches, and state-of-the-art VLMs [45, 46, 47, 48, 49, 7, 50, 51, 52, 53]. Table 1 summarizes the results. For tool selection, we use QwenVL-7B [41] as the agent and keep the reasoning model (QwenVL-7B or GPT4o) frozen during training. When both the agent and reasoner use QwenVL 7B, our method consistently outperforms baselines across all benchmarks. On ChartQA, VisTA achieves 79.4% accuracy, improving over the best training-free baseline (76.4%) by 3.0 points. On Geometry3K benchmark, which requires fine-grained diagram understanding and multi-step symbolic reasoning, VisTA achieves 55.6%, outperforming the best training-free baseline (54.0%). This result highlights VisTAs ability to support complex, multi-modal reasoning where tools provide complementary visual understanding. key strength of our method is its transferability. Without any retraining, the tool-selection policy learned with QwenVL 7B can be paired with GPT-4o as the reasoner. In this setting, we achieve 88.9% on ChartQA and 76.8% on ChartQA-OoD, surpassing the best training-free GPT-4o baseline by 3.5 points on both, and also achieve 52.4% on Geometry3K, outperforming the best baseline (49.5%) by 2.9 points. This demonstrates VisTAs flexibility and compatibility with stronger reasoning models at deployment time. We also compare against an RL baseline where the reasoner is directly trained using GRPO [24] to generate chain-of-thought reasoning in <think> block Method Ours Intern2VL-8B [49] Intern2VL-8B-ShortCoT [49] Geo-Intern2VL-8B [49] G-LLAVA-7B [43] Math-LLAVA-13B [54] QvQ-72B-Preview RedStar-Geo-8B [55] GPT4v [4] GPT4o-0513 [7] Gemini 1.5 Flash [45] Gemini 1.5 Pro [45] Claude-3 Haiku [46] Claude-3 Opus [46] Claude-3.5 Sonnet [46] PaliGemma-mix-3B [47] Phi3.5-Vision-4B [48] InternVL2-Llama-3-76B [49] Pixtral-12B [52] Llama-3.2V-11B-Instruct [50] Llama-3.2V-90B-Instruct [50] LLaVA-1.5-7B [56] LLaVA-1.5-13B [56] xGen-MM-interleave-4B [57] Cambrian-1-8B [51] Cambrian-1-34B [51] LLaVA OneVision-7B [58] LLaVA OneVision-72B [58] MolmoE-1B [53] Molmo-7B-O [53] Molmo-7B-D [53] Molmo-72B [53] ChartQA Geometry3K 88. 83.3 78.1 85.7 85.4 87.2 81.7 80.8 90.8 33.7 81.8 88.4 81.8 83.4 85.5 17.8 18.2 60.0 73.3 75.6 80.0 83.7 78.0 80.4 84.1 87.3 55.6 26.5 29.7 30.7 22.4 33.1 29.4 33.6 Table 2: Comparison to state-of-the-art VLMs. For our approach, the agent model is QwenVL 7B, and the reasoners are GPT4o for ChartQA, and QwenVL 7B for Geometry3K. followed by final <answer> block, without using tools. VisTA outperforms this RL-trained reasoner by 1.9 points on ChartQA and substantial 8.9 points on ChartQA-OoD, showing that tool-augmented reasoning offers greater gains than direct model optimization. The advantage of our method over the baselines is even more pronounced on ChartQA-OoD, where VisTA reaches 73.2%, 6.4-point gain over the best baseline (66.8%). This result demonstrates that by learning to select visual tools via RL, our VisTA agent framework is able to reason better visually than other baselines, which may rely more on textual cues. Finally, Table 2 shows the comparison to state-of-the-art VLMs. VisTA achieves the best performance on Geometry3K, significantly outperforming all prior methods. On ChartQA, VisTA ranks second overall, only slightly behind Claude-3.5 Sonnet (90.8 vs. 88.9), and surpasses other strong baselines such as Molmo-72B, Gemini 1.5 Pro, and InternVL2-Llama-3. This demonstrates that our approach is both highly effective on complex chart reasoning tasks and substantially more capable on geometric benchmarks."
        },
        {
            "title": "4.3 Tool Selection Analysis",
            "content": "To evaluate whether our tool selection policy effectively learns to combine and select the most appropriate tools for each query, we compare its performance against using individual tools in isolation. Specifically, we feed each tools output along with the original input to the frozen reasoner and record its accuracy on the ChartQA benchmark. We also compute pseudo-upper bound of 88.0% by treating query as correct if any single tool enables the reasoner to produce the correct answer. This serves as loose upper limit on what could be achieved with perfect single-tool selection, though it does not account for the benefits of combining multiple tools. Fig. 3 shows the performance of each tool individually (T0T8), as well as the no-tool baseline (76.4%). While certain tools, such as T2 (78.3%) and T1 (78.0%), improve upon the no-tool baseline, the large gap to the pseudo-upper bound (88.0%) suggests that no single tool consistently performs 7 Figure 3: Comparison of ChartQA accuracy across individual tools (T0T8), the no-tool baseline (No), our RL-based selection policy (Ours), and pseudo-upper bound (Upper). Figure 4: Pearson correlation between tool usage frequency and individual tool performance. best across all queries. Different tools appear to be optimal for different subsets of the data. Ideally, well-trained policy should learn to select the most effective tool(s) for each specific query, achieving performance that surpasses any static tool choice. Our method achieves 79.4% accuracy, outperforming all individual tools. This suggests that the policy learns to go beyond fixed tool usage and adapt its selection based on query-specific context. Our policy also closes some gap between the best individual tool and the pseudo-upper bound, indicating progress toward optimal tool selection without explicit supervision."
        },
        {
            "title": "4.4 Agent Behavior Evolution During RL Training",
            "content": "To analyze whether the agent is learning to prefer more effective tools, we track the correlation between tool usage frequency and individual tool performance over training. Specifically, every 10 iterations, we compute the Pearson correlation coefficient between the usage counts of each tool and their corresponding standalone accuracy (as reported in Fig. 3). Figure 4 shows the evolution of this correlation across training iterations. Despite some initial fluctuations, we observe clear upward trend, with the correlation increasing from near zero to over 0.8 as training progresses. This indicates that the agent is gradually aligning its tool selection strategy with the relative utility of each toolfavoring those that contribute more to the reasoners accuracy. These results suggest that our RL-based policy does not rely on fixed heuristics but instead learns to discriminate among tools based on their empirical contribution to task success. The emergence of this alignment over time provides further evidence that the agent is effectively adapting its behavior through reinforcement feedback."
        },
        {
            "title": "4.5 Tools Selection Distribution",
            "content": "Figure 5 illustrates the tool selection distribution on the test set for our RL-trained agent, as well as for the training-free baselines using QwenVL-7B and GPT-4o. Our method clearly shows strong preference for Tool 1 and Tool 2, both are chart-to-table tools, which are among the most effective tools based on 8 Figure 5: Tool selection frequency across our RL-trained agent, QwenVL-7B, and GPT-4o. Our method strongly favors effective tools (Tools 1 and 2) and avoids less useful ones, while QwenVL-7B shows uniform distribution and GPT-4o selects broadly without clear alignment to tool performance. individual performance (see Fig. 3). In contrast, low-performing tools such as Tool 3 (chart-to-SVG) and Tool 6 (caption module), are selected far less frequently, suggesting that the learned policy has effectively adapted to favor high-utility tools based on empirical feedback. The QwenVL-7B baseline, which operates in training-free manner without reinforcement feedback, exhibits more balanced selection pattern that resembles near-normal distribution. This indicates that it lacks strong preferences and does not consistently prioritize the most effective tools. Meanwhile, GPT-4o tends to select more tools per query, rarely opts for no tool, and distributes its selections across broader set of tools. However, this broader usage still lacks clear alignment with tool effectiveness, showing no strong correlation between selection frequency and tool performance. These differences highlight the benefit of learning tool selection strategy through RL. Unlike training-free approaches that rely solely on static prompt understanding, our agent adapts its behavior based on downstream task outcomes, leading to more effective tool use."
        },
        {
            "title": "4.6 Comparison with Random and All Selection Strategies",
            "content": "To further evaluate the effectiveness of our learned tool selection policy, we compare it with two alternative tool selection strategies: All Tools: In this baseline, all nine tool outputs are simultaneously provided to the reasoning model alongside the original image, creating comprehensive but potentially overwhelming input. Random Tools: This baseline randomly selects fixed number of tools for each query. Since our policy selects approximately 1.2 tools per query on average, we randomly sample 2 tools per query for fair comparison. Table 3 presents the comparative results across different reasoning models (QwenVL-7B and GPT4o). Our selective approach consistently outperforms both baselines, demonstrating that strategic tool selection provides substantial benefits over both comprehensive and random approaches. Reasoning Model All Tools Random Tools Ours QwenVL-7B GPT-4o 74.6 81.3 75.8 83.5 79.4 88.1 Table 3: Performance comparison of different tool selection strategies. The performance gap between our method and the All Tools baseline (4.8 and 6.8 percentage points for QwenVL-7B and GPT-4o, respectively) reveals that simply providing more tool outputs can be counterproductive. This suggests that extraneous information may confuse the reasoning model, highlighting the importance of selective augmentation. Similarly, our method outperforms the Random Tools baseline by 3.6 and 4.6 points, highlighting that effective reasoning depends on tool choice, not just quantity."
        },
        {
            "title": "4.7 Evaluation on MathVerse Geometry Benchmark",
            "content": "We evaluate our method on the MathVerse [31] benchmark, recent dataset designed to assess visual reasoning in geometry problems. Since our toolset focuses on 2D plane geometry, we report results on the corresponding subset of MathVerse in Table 4, which includes 2,550 samples out of the full 3,940. Importantly, we do not retrain the agent on MathVerse. Instead, we directly apply the agent trained on Geometry3K [20] to assess its zero-shot generalization capability to new benchmark. As shown in 9 Table 4, our method outperforms all training-free baselines, including setups where GPT-4o is directly prompted with tool outputs. Additionally, we compare our method with several recent state-of-the-art (SOTA) VLMs, including GPT-o1, Claude-3.5 Sonnet, and Gemini-2.0-Flash. Our method achieves the best performance across all configurations, including strong result of 73.1 when combined with GPT-o1 as the reasonersurpassing the best prior SOTA accuracy of 67.7. These results demonstrate the robustness and generalizability of our tool selection policy, even when applied to more structurally diverse and challenging geometry benchmarks. Method Agent Model Reasoning Model MathVerse - Training-Free Training-Free QwenVL 7B Training-Free Ours GPT4o QwenVL 7B GPT4o GPT4o GPT4o GPT4o Training-Free Training-Free Training-Free Ours - - - QwenVL 7B Claude-3.5-Sonnet Gemini-2.0-Flash GPT-o1 GPT-o1 50.2 51.4 53.6 56.1 58.2 65.6 67.7 73.1 Table 4: Results on the plane geometry subset of the MathVerse benchmark. Our method achieves the best performance, demonstrating its effectiveness on structurally diverse geometry reasoning tasks."
        },
        {
            "title": "4.8 Results on BlindTest Benchmark",
            "content": "Finally, to assess the generalizability of our framework, we evaluate on BlindTest [59], which targets low-level vision tasks requiring fine-grained spatial understanding where even state-of-the-art VLMs like GPT-4o [7] and Gemini 1.5 Pro [45] underperform. Following our previous training setup, we train QwenVL-7B agent to select from six tools, grouped into three categories, with each category responsible for detecting specific geometric element: lines, circles, or rectangles. Each category includes two tools to offer complementary perspectives. As shown in Table 5, our method achieves higher accuracy than both training-free baselines, highlighting the advantage of learned tool selection in scenarios that demand fine-grained visual reasoning. Method Agent Model Reasoning Model BlindTest Training-Free Training-Free Ours - GPT4o QwenVL 7B GPT4o GPT4o GPT4o 48.5 51.8 53.4 Table 5: Performance of different tool selection strategies on BlindTest."
        },
        {
            "title": "4.9 Comparison with Visual Sketchpad on Geometry3K",
            "content": "To further benchmark our method, we compare against Visual Sketchpad [12], recent state-of-the-art system on Geometry3K. Visual Sketchpad enables VLMs to draw with lines, boxes, marks, which better facilitates visual perception and reasoning. For fair comparison, we follow their evaluation protocol and test our method on the same curated subset of Geometry3K*, which is smaller than the official test set. As shown in Table6, our method still outperforms Visual Sketchpad. This result highlights the strength of our learned tool selection policy in driving effective visual reasoning. Method Agent Model Reasoning Model Geometry3K* Visual Sketchpad [12] Ours - QwenVL 7B GPT4o GPT4o 66.7 68.8 Table 6: Comparison with Visual Sketchpad on Geometry3K* test subset. We follow their evaluation *Indicates the smaller subset used by Visual Sketchpad. Our protocol using the same curated set. method achieves higher accuracy, demonstrating the strength of our tool selection policy. 10 Figure 6: Tool Selection for Geometry Question. Our agent selects the formal diagram parser, Inter-GPS, which accurately extracts essential details from the diagram and represents them in formal structure. Leveraging this representation, the reasoner is able to determine the correct answer."
        },
        {
            "title": "4.10 Visualizations",
            "content": "In this section, we provide qualitative examples showing how our agent selects appropriate tools according to the query. From the Geometry3k dataset, we present an example in Figure 6. Based on the context, our agent selects the Inter-GPS tool, which represents the objects in the diagram and their relationships using formal language. This structured information is then used by the Reasoner model to correctly answer the question. Figure 7 presents qualitative example of chart understanding. The question is complex, requiring reasoning about the sizes of various objects identified by specific colors. To handle this, our agent invokes both chart-to-table converter to extract numeric values (shown as an image for illustration) and an image-to-SVG converter to capture color information. Using this combined input, the reasoner is able to correctly answer the question."
        },
        {
            "title": "4.11 Constructing the ChartQA-OoD Test Set",
            "content": "Chart-based question answering poses unique challenges, requiring models to reason over numerical values, textual labels, and complex visual structures. Effective chart comprehension demands not only the identification of visual elements but also precise interpretationsuch as accurately estimating bar heights in bar charts. To better evaluate the robustness and reasoning capabilities of VLMs, we construct an out-ofdistribution (OoD) test set for ChartQA [19] with two targeted perturbations. We manually remove textual labels from geometric elements (e.g., bars and points) in each chart. This isolates the models reliance on visual features by eliminating access to direct numeric answers via OCR. To further test the models spatial reasoning robustness, we apply random geometric perturbations to the charts. With 50% chance, each chart is either horizontally or vertically stretched to twice its original width or height. These transformations maintain semantic structure but introduce visual variability. The resulting ChartQA-OoD test set allows us to probe whether models depend heavily on textual cues and how sensitive their reasoning is to mild visual distortions. This setup provides more rigorous benchmark for evaluating the generalization and visual understanding capabilities of VLMs. 11 Figure 7: Tool Selection for Chart Question. An example demonstrating case where the agent calls set of complementary tools. This question requires understanding both numeric values (extracted from the table) and color information (extracted from the SVG). The reasoner combines this information to arrive at the correct answer."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "We introduced VisTA, an RL framework enabling visual agents to autonomously select effective external tools for multimodal reasoning. Unlike prior methods, VisTA learns adaptive tool-selection strategies without explicit supervision. Our experiments showed significant accuracy gains over strong baselines, highlighting VisTAs potential for robust, flexible visual reasoning. Limitations. Our framework enables agents to learn visual tool selection through experience, but it currently does not handle cases requiring sequential composition of multiple tools. Exploring this sequential tool-composition capability represents promising direction for future research. Second, our framework currently relies on fixed, manually curated set of tools, which could be enhanced by methods for automatically discovering and integrating new tools. Developing such automated tool-selection and integration methods could significantly improve the scalability and adaptability of our approach. Broader Impact. Our framework enhances the ability of AI systems to autonomously learn when and how to utilize external tools, such as image captioners and object detectors, potentially improving generalization and robustness in multimodal environments. While our current study focuses on relatively low-stakes settings, such as chart understanding and geometry problems, deploying similar approaches in high-stakes domains like healthcare could introduce subtle yet significant errors due to incorrect tool usage. Ensuring reliability, transparency, and appropriate human oversight in these sensitive applications will be crucial."
        },
        {
            "title": "References",
            "content": "[1] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 12 [3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [5] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [6] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [7] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [8] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. [9] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. [10] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 1076410799. PMLR, 2023. [11] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544 126565, 2024. [12] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [13] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962, 2023. [14] Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1188811898, 2023. [15] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [16] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer Vision, pages 126142. Springer, 2024. [17] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An agentic framework with extensible tools for complex reasoning. arXiv preprint arXiv:2502.11271, 2025. [18] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [19] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [20] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [21] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021. [22] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [23] OpenAI. to learning-to-reason-with-llms/, 2024. Accessed: 2025-05-13."
        },
        {
            "title": "Learning",
            "content": "reason llms. with https://openai.com/index/ [24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [25] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [26] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [27] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [28] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. [29] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [31] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [33] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. [34] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022. [35] Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, and Jian Guo. Chartmoe: Mixture of expert connector for advanced chart understanding. arXiv preprint arXiv:2409.03277, 2024. [36] Gary Bradski. The opencv library. Dr. Dobbs Journal: Software Tools for the Professional Programmer, 25(11):120123, 2000. [37] Pengyu Yan, Saleem Ahmed, and David Doermann. Context-aware chart element detection. In International conference on document analysis and recognition, pages 218233. Springer, 2023. [38] Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. Chartocr: Data extraction from charts images via deep hybrid framework. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 19171925, 2021. [39] Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. arXiv preprint arXiv:2401.02384, 2024. [40] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. [41] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [42] Zeren Zhang, Jo-Ku Cheng, Jingyang Deng, Lu Tian, Jinwen Ma, Ziran Qin, Xiaokai Zhang, Na Zhu, and Tuo Leng. Diagram formalization enhanced multi-modal geometry problem solver. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [43] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [44] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [45] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [46] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. [47] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726, 2024. [48] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [49] OpenGVLab Team. Internvl2: Better than the bestexpanding performance boundaries of opensource multimodal models with the progressive scaling strategy, July 2024. [50] Meta AI. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [51] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, visioncentric exploration of multimodal LLMs. In NeurIPS, 2024. [52] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amelie Heliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [53] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [54] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [55] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. [56] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction 15 tuning. In CVPR, 2024. [57] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xGen-MM (BLIP-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. [58] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [59] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza, Taesiri, and Anh Totti Nguyen. Vision language models are blind. In ACCV, 2024."
        }
    ],
    "affiliations": [
        "Microsoft",
        "University of Wisconsin-Madison"
    ]
}