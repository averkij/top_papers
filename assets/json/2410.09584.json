{
    "paper_title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
    "authors": [
        "Guanting Dong",
        "Xiaoshuai Song",
        "Yutao Zhu",
        "Runqi Qiao",
        "Zhicheng Dou",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io."
        },
        {
            "title": "Start",
            "content": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation Guanting Dong1, Xiaoshuai Song2, Yutao Zhu1, Runqi Qiao2, Zhicheng Dou1*, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2School of Artificial Intelligence, Beijing University of Posts and Telecommunications {dongguanting,dou}@ruc.edu.cn 4 2 0 2 2 1 ] . [ 1 4 8 5 9 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instructionfollowing (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github. io. 1. Introduction The advancement of Large Language Models (LLMs) (OpenAI 2023; Yang et al. 2024) has profoundly revolutionized variety of real-world tasks expressed in natural language (Wei et al. 2022; Luo et al. 2023). However, they still suffer from hallucinations and factual inconsistencies (Bang et al. 2023), impacting the authenticity of generated answers. Retrieval-Augmented Generation (RAG) has gained recognition as promising solution, empowering LLMs to leverage reliable information from retrieved documents, thereby *Corresponding author Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: The task format of instruction-following tasks for LLMs in RAG scenarios. returning high-quality responses (Guu et al. 2020; Lewis et al. 2020). In real-world interaction scenarios, users often deviate from standard templates when posing questions, instead of imposing diverse instructions on model outputs to meet specific task requirements (Jiang et al. 2023b; Chung et al. 2024). Consequently, improving instruction-following (IF) capabilities is foundational to the effective application of LLM and RAG systems. The core goal of IF is to enable models to adapt to the diverse intents of users, which has garnered widespread attention in the LLM community. Existing efforts on instruction-following alignment primarily focus on multi-grained evaluation (Zhou et al. 2023a; Jiang et al. 2024a; Wen et al. 2024) and high-quality instruction data synthesis (Sun et al. 2024a; Zhao et al. 2024) to enhance LLMs natural instruction-following capabilities. However, in complex RAG scenarios, the diverse knowledge introduced by retrieval-augmented techniques presents significant challenges for LLMs in effectively handling complex instructions (Figure 1). As shown in Figure 2, after supervised fine-tuning on high-quality general and knowledgeintensive QA datasets, LLMs demonstrate robust performance in both IF and RAG tasks (Mistral-base vs. MistralSFT). However, these capabilities do not always generalize well to instruction-following tasks under RAG scenarios and may even conflict with the performance of other fundamenFigure 2: The performance comparison between Mistral-7B base and SFT version models on different tasks. The SFT version refers to the base model that has been fine-tuned using mixed dataset, including NQ, TQ, HQ, and ShareGPT. Details results and setups can be found in Table 1 & 2 tal abilities (Dong et al. 2024b; Zhu et al. 2024). Unfortunately, research on instruction-following in RAG systems remains limited, significantly hindering their application in real-world interactions. To tackle these challenges, our aim is to address following critical research questions: RQ1. How can we comprehensively evaluate the complex instruction-following capabilities in the RAG scenario? RQ2. How can we achieve scalable and reliable instruction-following alignment in RAG systems while preserving the its foundational abilities from conflict? In this paper, we propose VIF-RAG, the first automated, scalable, and reliable data synthesis pipeline for achieving complex instruction-following alignment in RAG scenarios. The core insight of VIF-RAG is to ensure every step of data augmentation and combination includes proper verification process. Specifically, we start by manually crafting minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for seed set. We then use supervised models for instruction rewriting. Motivated by tool execution studies (Le et al. 2022; Qiao et al. 2024b), we employ the same supervised model to generate verification code and automatically verify the quality of augmented instructions through the Python compilers outputs. Finally, we combine these high-quality instructions with RAG datasets from various domains (each containing retrieved documents per query), performing the augmentation and dual validation process to synthesize high-quality instruction-based RAG dataset, named VIF-RAG-QA (>100K samples). To further bridge the gap in automatic instructionfollowing evaluation for RAG systems, we introduce FollowRAG, the first benchmark dedicated to comprehensively assessing the complex instruction-following capabilities of RAG systems. FollowRAG aggregates constraints from realworld scenarios. It includes approximately 3K test samples, spanning 4 knowledge-intensive QA benchmarks and 22 types of constraints. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. To summarize, our contributions are as follows: To first achieve instruction-following alignment in the RAG system, we propose VIF-RAG, the first automated, scalable, and verifiable data synthetic framework. VIFRAG uniquely combines augmented rewriting with diverse validation processes to synthesize high-quality instruction-following alignment data from almost scratch (<100), scaling up to over 100K samples. We introduce FollowRAG, the first benchmark designed to comprehensively evaluate LLMs complex instructionfollowing abilities in RAG tasks. FollowRAG includes nearly 3K test samples, spanning four knowledgeintensive QA benchmarks and 22 types of constraints. Its design ensures seamless integration with various RAG benchmarks, providing strong scalability. With FollowRAG and 8 widely-used IF and 3 foundational abilities benchmarks, we demonstrate that different LLMs with VIF-RAG achieve extraordinary alignment on general instruction following in both RAG and standard scenarios while effectively preserving other foundational capabilities. Further analysis offers practical insights for optimizing IF alignment in RAG systems. 2. Related Work Instruction-Following Alignment for LLMs. Instructionfollowing ability is core capability of large language models. Existing works fall into two main categories. The first includes efforts like MMLU and MTbench (Hendrycks et al. 2021; Zheng et al. 2024a), which rigorously evaluate models adherence to general instructions. Moreover, works like IFEval and Followbench (Zhou et al. 2023a; Jiang et al. 2024a) focus on fine-grained assessment under specific constraints, using stricter criteria such as instruction difficulty, domain, and task formats (Qin et al. 2024; Xia et al. 2024; Yan, Luo, and Zhang 2024; Wen et al. 2024). The other category focuses on improving IF alignment. Manual design of instructions and responses by human annotators (Wei et al. 2021) is challenging and costly. To address this, methods are developed to synthesize diverse instructions, allowing weaker models to mimic the responses of advanced models (Dubois et al. 2024; Dong et al. 2024a; Xu et al. 2023), achieving strong-to-weak alignment (Cao et al. 2024)."
        },
        {
            "title": "Alignment",
            "content": "for Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) addresses the issue of knowledge hallucination in LLMs by retrieving relevant information, offering promising solution (Guu et al. 2020; Lewis et al. 2020). However, efficiently aligning retrieved knowledge with LLMs preferences remains challenge. Researchers have developed robust reranker-based methods (Sun et al. 2023; Qin et al. factual 2023; Ma et al. 2023b) and data filtering approaches (Wang et al. 2023) to reduce noisy information and bridge this gap. Additionally, approaches like RePLUG (Shi et al. 2023) integrate LLMs preferences into training objectives to improve alignment. Query rewriting methods (Ma et al. 2023a; Ren et al. 2023) attempt to adjust inputs based on these preferences. Furthermore, SelfRAG and MetaRAG (Asai et al. 2024; Zhou et al. 2024) use multi-round retrieval and generation to refine outputs and achieve better alignment. Despite these advancements, the diverse knowledge introduced by retrieval-augmented techniques poses significant challenges for LLMs in handling complex instructions. This highlights the need for further exploration into achieving effective instruction-following alignment in RAG systems. 3. Preliminaries Retrieval-Augmented Generation (RAG). RetrievalAugmented Generation systems usually operates under retrieve-then-read framework (Lewis et al. 2020). The external retriever is integrated to gather supporting knowledge and improve the generation process. Given query q, retriever recalls relevant documents Dq = {di}k i=1 from an external corpus comprised of documents. We employ the DPR (Karpukhin et al. 2020) to obtain hidden vectors for queries and documents. The relevance score is determined by measuring the dot-product similarity between the query and document representations, allowing the retrieval of the top-k documents Dq:: Dq = argtop-k (cid:2)Ed(di) Eq(q) = {1 . . . }(cid:3) . Then, the retrieved documents are concatenated with the (1) query into an LLM reader to generate the target text: = R(q, Dq) = log Pθ (q, Dq), (2) where Pθ is the output probability distribution. Instruction-following Alignment for RAG. Following instructions is one of the most foundational ability for LLMs in RAG systems. Given an instruction = {Ij}M j=1 with specific constraints and specific query with corresponding relevant retrieved documents Dq, The LLM πθ in the RAG system is expected to produce an accurate response πθ(y q, Dq, I) while obeying with the specified constraints. 4. VIF-RAG Framework In this section, we propose VIF-RAG, verifiable automated instruction data synthesis framework for RAG scenarios. The core design of VIF-RAG is that each step of the automated generation or combination is accompanied by an appropriate verification process. ViF-RAG framework can be broadly split into two sections: (1) the instruction synthesis stage and (2) instruction-query synthesis, scaling from almost scratch (<100) to over 100K high-quality instructionquery samples. Below, we will delve into the specifics. 4.1. Instruction Synthesis from Scratch Handwritten Seed Instructions. We initially develop minimal seed instruction set Datom seed manually, using four foundational categories of constraints: format constraints, semantic constraints, knowledge constraints, and lexical constraints, as themes for instruction writing. The following presents specific criteria regarding the 4 constraints: Format Constraints require the output to adhere to specific standards in terms of format, length, and structure. The content should be organized, clear, and meet the required format specifications. Semantic Constraints require the outputs theme, language style, personality, and sentiment to align with the given instructions. The content should be semantically consistent with expectations and adhere to the specified tone or expression. Knowledge Constraints require the output to be accurate, comprehensive, and in-depth. The content should be informative, cover all necessary information, and maintain consistency in knowledge expression. Lexical Constraints require the output to include specific keywords or phrases, ensuring precision and relevance in word choice. The content should meet the expected requirements in terms of vocabulary selection. We hire only one well-educated human annotator to manually create 15 single-atomic instructions for each type of constraint. Notably, this is the only process in our data synthesis process that includes human supervision. Instruction Composition & Verification. Real-world instructions often involve multiple constraints in one user query. To address this complexity, we design rules to automatically combine atomic instructions into diverse, complex instructions: Multiple Constraints: As illustrated in Figure 3, we randomly sample pairs of instructions from Datom seed and insert them into constraint template. By directly concatenating these instruction pairs, we create complex instructions that contain dual and triple constraints. This type of instruction requires the model to generate results that satisfy multiple constraints simultaneously. Chain Rule Constraints: We design sequential conditional constraint templates and selected atomic instructions from Datom seed to form chain constraints. Formally, the chain consists of tasks {T1, T2, ..., Tn}, requiring the models output to complete these tasks sequentially. Verification. Randomly combining these atomic instructions can easily lead to conflicts between them (e.g., dont use words containing the letter I, use words that end with -ing). These semantic conflicts can be challenging to detect using simple Natural Language Inference model. To detect potential conflicts between these instructions, we use robust supervised model that rates their consistency from 1 to 10. Samples scoring below 8 are excluded to refine our high-quality complex instruction set Dcomplex . Ultimately, we arrive at the initial seed instruction set Dseed = {Datom seed Dcomplex }. Detailed information about the prompt templates seed are listed in the Appendix. seed Instruction Rewriting & Quality Verification. To automate the scaling up of instructions, the instruction rewriting strategy is considered the most natural augmentation Figure 3: The overall framework of VIF-RAG. The top section presents the pipeline for automated IF data synthesis in RAG scenario, while the bottom section shows examples of Instruction Composition and Executor-based Verification, respectively. method, and has received significant attention in the RAG and reasoning fields (Mumuni and Mumuni 2022; Xie et al. 2020; Yuan et al. 2023a; Li et al. 2024b,a). We use supervised model1 to iteratively rewrite instructions from the Dseed set in batches of 50 for rounds, generating an augmented set Daug. Subsequently, we merge the seed and augmented samples to form the combined instruction set Dins = Dseed Daug, removing duplicates. Inspired by tool execution works (Le et al. 2022), we aim to leverage the powerful coding abilities of LLMs to assist in verifying the quality of auto-generated instructions. As shown in Figure 3, for each instruction Dins, we use the supervision model to generate verification function codes and corresponding test cases {f uncI j=1 Dverify, and assess the instructions quality by analyzing the output of the executor E. For any function and test case {f uncI } Dverify, its execution output is: , cI , cI }K E(f uncI , cI ) = (cid:26)1 0 If output is True If output is False or Error (3) Therefore, we can calculate the accuracy Accfunc of each verification function based on test samples, as well as the accuracy Acccase of each case evaluated using verification functions. These can be formulated as: Accfunc = 1 Acccase = 1 (cid:80)K j=1 E(f uncI , cI )K j= (cid:80)K j=1 E(f uncI , cI )K j=1 (4) Based on the above cross metrics, we require that at least one Accfunc and Acccase of the each instruction must exceed 0.5, Ultimately, we obtain the auto-verified instruction set as"
        },
        {
            "title": "Dverify",
            "content": "ins = {d Dins Accfunc(d) > 0.5 & Acccase(d) > 0.5} (5) The samples that do not meet the cross metrics are discarded. 4.2. Scalable Instruction-Query Synthesis Random Instruction-Query Combination. In real-world interactions with RAG systems, achieving IF alignment depends on effectively integrating the synthesized instructions with the queries used by the RAG system. To meet this goal, as depicted in Figure 3, we first extract high-quality queries from two different data sources. 1) RAG Domain: Building effective RAG system need to prepare sufficient amounts of QA-format data with relevant knowledge to enhance its knowledge-based interaction capabilities. Consequently, we randomly select query set from mixed QA data sources, including open-domain multi-hop and knowledge base QA scenarios 2. Following the retrieve-then-read paradigm (Lewis et al. 2020), We employ the dense retriever to fetch the top-K relevant documents Di for each query from an external knowledge base, resulting in the dataset DRAG = {qi, Di}K i=1. Furthermore, we randomly select queries along with their corresponding retrieved documents from DRAG for each instruction and combine them to create the RAG query set with IF constraints DIF-RAG = {Ij, qj, Dj}K j=1. 2) General Domain: In addition to incorporating RAGspecific abilities, the RAG system has to possess basic human-aligned abilities to meet users daily interaction needs. Therefore, ShareGPT (Chiang et al. 2023), which provides authentic multi-turn human dialogue data, is our natural choice. Similar to how we handle the RAG domain, for each instruction Dins, we randomly select queries from the ShareGPT to combine with the instruction and construct the general dataset DIF-General for each instruction. Ultimately, we merge the instruction-constrained query 1For the supervised model, we use GPT-4-turbo-2024-04-09. 2We use the training sets from Natural Questions, TriviaQA, We conduct the ablation for supervision model in Table 7. HotpotQA, and WebQuestionsSP as mixed QA sources. Figure 4: The construction pipeline, diagram and statistics of FollowRAG. sets from these two domains into the final query set of VIFRAG-QA, formulated as Dq VIF-RAG. Instruction-Query Rejection Sampling. It is worth noting that under diverse instruction-following constraints, the original grounding truth answers for queries in both the RAG and general datasets become unreliable. To address this issue and improve synthetic data diversity, we adopt rejection sampling strategy (Yuan et al. 2023b). Specifically, we use the supervision model to generate responses i=1 for each instruction-query pair Dq yx = {yi}K VIF-RAG, resulting in {x, yx} DVIF-RAG. Dual Stage Verification. To further ensure comprehensive quality control of the synthetic dataset, we employ dual stage verification process for the instruction-query data: Executor-based Verification: To automatically verify whether model-generated responses comply with the constraints of the instruction-query samples, we leverage pre-existing verification functions to evaluate adherence in the augmented outputs. As in the Instruction Rewriting & Quality Verification section, at least one response in DVIF-RAG must achieve an accuracy rate Acccase above 0.5 across all verification functions; otherwise, the sample is discarded. Consistency Verification: We have noticed that combined instructions and queries often conflict. simple example is when the query Please write brief biography of Barack Obama. does not meet the instruction Strictly limit your answer to less than 10 tokens. Building on previous consistency verification of instructions, we employ supervision model to evaluate the alignment between queries and instructions on scale of 1 to 10, discarding samples that receive score below 8. After dual stage verification, we have automatically obtained large-scale, high-quality VIF-RAG-QA dataset. 5. FollowRAG Benchmark To bridge the gap in automatic instruction-following evaluation for RAG systems, we introduce FollowRAG in this section. We provide detailed introduction from two aspects: Data Construction and Evaluation and Statistics. 5.1. Dataset Construction Instruction Collection & Extraction. FollowRAG aims to assess the models ability to follow user instructions in complex multi-document contexts. Drawing from general IF datasets like IFEval (Zhou et al. 2023b) and FollowBench (Jiang et al. 2024b), we collect and verify definitions and examples of atomic instructions using rules (e.g., code), excluding those irrelevant to RAG scenarios. Ultimately, we identify 22 types of instruction constraints, encompassing language, length, structure, and keywords. Instruction Reforming. We use widely-used questionanswering (QA) datasets, such as Natural Questions (Kwiatkowski et al. 2019), as the foundation for constructing FollowRAG samples. For query sampled from the QA datasets, we need to generate complex instruction containing atomic instruction constraints (with ranging from 1 to 4). To enhance the diversity of atomic instruction representations, we employ GPT-4o as the instruction generator. Specifically, given query, we first sample instructions from the atomic instruction set and perform conflict detection. Subsequently, with examples as demonstrations, we prompt the LLM to generate new varied instruction text for each type of atomic instruction, along with parameters for instruction-following evaluation. Combination. Finally, we integrate the retrieved passages, query and atomic instructions to construct the sample input for FollowRAG. To avoid mechanically concatenating the query and instructions in template-based manner, we prompt supervised model to naturally blend the multiple atomic instructions and the query into coherent instructionquery paragraph. We then add the top-K document passages retrieved based on the query to the instruction-query paragraph, forming the complete sample input. 5.2. Evaluation and Statistics After obtaining the models output, we evaluate it from two perspectives: instruction following and question answering (QA) under the RAG paradigm: Instruction Following: Utilizing the verifiable nature of our atomic instructions and following the IFEval approach, we automate the verification of the models adherence to each instruction through code validation. We then calculate the average pass rate for each atomic instruction across all samples to determine the instructionfollowing score in FollowRAG. RAG: Under new instruction constraints, the models target output differs from the gold answers in the original QA dataset, rendering traditional metrics like ExactMatch ineffective. To address this, we use the original gold answers as reference and utilize GPT-4o to evaluate whether the models outputs correctly address the questions. The scoring criteria are as follows: Completely correct (1 point), Partially correct (0.5 points), Completely incorrect (0 points). The average score of all samples is taken as the RAG score for FollowRAG. For detailed statisticsin in Figure 4, FollowRAG is the first instruction-following evaluation dataset under RAG scenario comprising 2.8K samples, covering 22 fine-grained atomic instructions across 6 categories. The queries in FollowRAG are sourced from 4 QA datasets across 3 types: 1) Open-Domain QA: Natural Questions (NQ) (Kwiatkowski et al. 2019) and TriviaQA (TQA) (Joshi et al. 2017); 2) Multi-Hop QA: HotpotQA (HQA) (Yang et al. 2018); and 3) Knowledge Base QA: WebQuestionsSP (WebQSP) (Yih et al. 2016). To further construct varying levels of instruction-following difficulty, FollowRAG includes 0.9K samples of single and dual atomic instructions, as well as 0.5K complex multi-instruction samples containing 3 and 4 atomic instructions, respectively. 6. Experiment 6.1. Experimental Setup Datasets. In this section, we evaluate over 10+ benchmarks to comprehensively evaluate the VIF-RAG. For the instruction-following tasks in RAG scenarios, we use the FollowRAG benchmark as mentioned in Section 5, which covering 4 question-answering (QA) datasets. For general instruction-following evaluation, we selected two commonly used complex instruction-following datasets, IFEval (Zhou et al. 2023a) and FollowBench (Jiang et al. instruction dataset MT2024a), along with the natural Bench (Zheng et al. 2024a) and the challenging ChatBot instruction-following bench, Arena-Hard (Li et al. 2024c). Additionally, to measure that the foundational abilities of LLMs, we further evaluate two widely used LLMs general abilties evaluation sets, C-Eval (Huang et al. 2023) and MMLU (Hendrycks et al. 2021), as well as the mathematical reasoning dataset GSM8K (Cobbe et al. 2021) and the code evaluation bench HumanEval (Chen et al. 2021). For baselines, we select Mistral-7B (Jiang et al. 2023a), Llama3-8B (Meta 2024), Qwen1.5-7B, and Qwen1.514B (Yang et al. 2024) as our backbone models, fine-tuning ShareGPT and four QA training sets as SFT version. Besides, we introduce several strong IF baselines, including Conifer (Sun et al. 2024a), Evol-Instruct (Xu et al. 2023), and Deita (Liu et al. 2024). To ensure fairness, we add an equal-sized RAG training set to the original synthetic data used for these models. More details on the baselines and implementation can be found in the appendix. 6.2. Main Result Our primary findings are presented in Table 1. Overall, VIFRAG consistently surpasses all baselines in FollowRAG across multiple configurations, highlighting the clear advantages of our method. Additionally, we have discovered several key insights: 1) Existing IF baselines struggle in complex RAG scenarios. Comparisons between different base models and SFT versions in Tables 1 & 2 show that while SFT general data like ShareGPT improves performance on IFEval, it actually shows performance decline in the instructionfollowing aspect of FollowRAG (e.g., NQ-IF: 25.721.0 in Mistral). Moreover, several strong IF baselines, such as Conifer (Sun et al. 2024b), also perform poorly in FollowRAGs IF aspect (HQ-IF: 26.926.45). This corroborates the issue highlighted in the introduction: traditional synthetic data may improve LLMs vanilla instructionfollowing ability but often fails to generalize in RAG scenarios, sometimes even leading to decreased performance. 2) VIF-RAG shows exceptional IF alignment capability across various datasets, models, and parameter sizes. It consistently outperforms all baselines by over 10% on average accuracy, including 44% improvement over Llama3-base, showcasing the significant performance advantage of our method. On four detailed QA benchmarks, VIF-RAG achieves the best results across all tested backbones. Moreover, whether using Qwen1.5-7B or Qwen1.514B, our method maintains stable and significant performance increase of over 10%. These results highlight that VIF-RAG is not only plug-and-play but also exhibits strong generalization capabilities. 3) The RAG capability is effectively preserved. Protecting RAG capability is core focus of RAG systems. Compared to various SFT version baselines, our VIF-RAG significantly enhances IF capability while maintaining more stable RAG performance. This allows us to be optimistic about its potential in real-world RAG system applications. 6.3. Cross-Domain Validation To explore the transferability of VIF-RAG, we conduct cross-domain validation on four natural instructionfollowing datasets and four foundational abilities benchmarks for LLMs in Tabel 2. Our findings are as follows: 1) Consistent IF alignment in both standard and RAG scenarios. Table 1 shows that VIF-RAG achieves remark-"
        },
        {
            "title": "Model",
            "content": "NQ TQ HQ"
        },
        {
            "title": "ALL",
            "content": "IF"
        },
        {
            "title": "RAG AVG",
            "content": "IF"
        },
        {
            "title": "RAG AVG",
            "content": "IF"
        },
        {
            "title": "RAG AVG",
            "content": "IF"
        },
        {
            "title": "RAG AVG",
            "content": "IF"
        },
        {
            "title": "RAG AVG",
            "content": "Llama3-8B-base Llama3-8B-SFT Llama3-8B-SFT-VIF-RAG Mistral-7B-base Mistral-7B-SFT Mistral-7B-SFT Conifer Mistral-7B-SFT Evol-Instruct Mistral-7B-SFT-VIF-RAG Deita-7B-V1.0-SFT Qwen1.5-7B-base Qwen1.5-7B-SFT Qwen1.5-7B-SFT-VIF-RAG 3.2 15.7 43.9 25.7 21.0 29.9 41.7 51.2 31.4 27.7 16.1 38."
        },
        {
            "title": "33.7\nQwen1.5-14B-base\nQwen1.5-14B-SFT\n22.0\nQwen1.5-14B-SFT-VIF-RAG 42.1",
            "content": "5.7 59.5 65.0 31.1 48.5 49.5 41.5 56.5 31.5 34.4 50.5 41.5 38.1 54.5 53.0 4.4 37.6 54.5 28.4 34.7 39.7 41.6 53.8 31. 31.0 33.3 40.2 35.9 38.3 47.6 4.1 15.0 42.7 25.9 17.2 30.5 37.0 45.9 29.0 27.7 14.3 35.8 32.5 18.7 40. 15.9 76.5 78.0 44.4 71.5 67.0 63.5 70.5 42.5 45.9 70.0 78.0 54.7 66.0 71.0 10.0 45.7 60.4 35.2 44.3 48.7 50.4 58.2 35. 36.8 42.2 56.9 43.6 42.3 55.5 3.6 15.0 39.6 26.9 17.6 26.5 35.4 44.9 26.5 27.5 14.8 38.1 32.4 18.8 38. 7.3 52.5 46.0 19.9 46.5 40.0 35.0 43.0 30.5 19.8 40.0 45.0 26.5 41.0 39.5 5.5 33.8 42.8 23.4 32.1 33.2 35.2 44.0 28. 23.6 27.4 41.6 29.5 29.9 39.2 10.0 14.4 42.5 24.7 21.7 31.1 39.4 47.8 26.3 29.9 13.7 31.9 33.0 19.9 35. 23.1 70.0 70.5 20.4 66.5 63.0 54.0 58.0 40.0 45.8 59.0 60.0 48.3 63.0 69.0 16.5 42.2 56.5 22.6 44.1 47.1 46.7 52.9 33. 37.9 36.3 45.9 40.7 41.4 52.3 5.2 15.0 42.2 25.8 19.3 29.5 38.4 47.4 28.3 28.2 14.7 36.2 32.0 19.8 39. 13.0 64.6 64.9 29.0 58.3 54.9 48.5 57.0 36.1 36.5 54.9 56.1 41.9 56.1 58.1 9.1 39.8 53.5 27.4 38.8 42.2 43.5 52.2 32. 32.3 34.8 46.2 36.9 38.0 48.6 Table 1: The main results on FollowRAG.AVG represents the weighted average of the corresponding IF and RAG scores. The top two results in each column are highlighted in bold and underlined."
        },
        {
            "title": "IFEval",
            "content": "Pr (S) Pr. (L) Ins. (S) Ins. (L) FollowBench (SSR Avg.) MT-Bench Arena-Hard C-Eval MMLU GSM8k HumanEval (Pass@1) Llama3-8B-base Llama3-8B-SFT Llama3-8B-SFT-VIF-RAG Mistral-7B-base Mistral-7B-SFT Mistral-7B-SFT-VIF-RAG Qwen1.5-7B-base Qwen1.5-7B-SFT Qwen1.5-7B-SFT-VIF-RAG Qwen1.5-14B-base Qwen1.5-14B-SFT Qwen1.5-14B-SFT-VIF-RAG 24.6 32.5 37. 14.6 23.3 34.6 25.1 36.4 42.3 35.5 38.4 46.3 26.1 34.3 42.7 15.3 24.6 41.0 27.9 39.3 46. 39.0 41.7 49.9 38.1 43.3 48.8 25.8 38.4 46.3 37.8 46.4 53.5 46.7 49.4 60.0 39.7 45.4 54. 27.0 45.7 52.0 40.6 49.4 57.1 50.2 52.6 62.2 11.6 33.6 49.2 38.0 42.9 53.4 38.7 46.3 51. 45.5 49.8 56.3 4.0 5.6 6.2 3.5 6.2 6.5 5.4 5.7 6.1 5.8 6.0 7.3 0.5 2.2 3. 0.6 3.1 3.6 3.2 2.1 3.9 6.4 6.5 7.0 24.2 35.6 39.6 31.8 26.2 33.0 72.8 69.1 75. 77.8 76.2 79.5 38.8 45.2 49.6 44.5 32.1 49.6 58.3 55.5 61.2 64.7 62.0 66.5 0.5 12.6 22. 16.0 7.3 16.0 50.6 48.6 61.4 71.8 71.5 73.8 0.6 3.6 8.0 25.6 13.9 32.9 36.0 39.0 44. 59.1 58.5 59.1 Table 2: The cross-domain validation on 4 general instruction-following (Left 4) and 4 foundational abilities (Right 4) benchmarks. Pr. and Ins. refer to the prompt level and instruction level metric, respectively. or denote the strict or loose metrics used in IFEval."
        },
        {
            "title": "Model",
            "content": "Mistral-7B-SFT-VIF-RAG FollowRAG (NQ)"
        },
        {
            "title": "IFEval",
            "content": "IF 51."
        },
        {
            "title": "RAG",
            "content": "Ins(L) Prompt(L) 56.5 41.0 52.0 w/o Multiple Constraints 46.5 (-5.1) 52.3 (-4.2) 37.9 (-3.1) 48.6 (-3.4) w/o Chain rule Constraints 49.2 (-2.4) 53.3 (-3.2) 39.2 (-1.8) 49.9 (-2.1) w/o Executor based Verification 43.5 (-8.1) 56.1 (-0.4) 33.2 (-7.8) 47.6 (-4.4) w/o Consistency Verification 47.6 (-4.0) 46.2 (-10.3) 38.4 (-2.6) 46.5 (-5.5) Table 3: Ablation study on different designs of VIF-RAG. able IF alignment in RAG scenarios. In Table 2, comparing Llama3-8B SFT version, VIF-RAG demonstrates strong gains on two widely-used IF benchmarks, IFEval and FollowBench, with improvements of 8.8% (Ins.L) and 15.5% respectively. It also maintains stable improvement across different parameter sizes (7B & 14B). These results confirm that VIF-RAG consistently enhances IF alignment in both RAG and standard scenarios. 2) Robust General IF Transferability. To assess general instruction-following alignment, we test VIF-RAG on challenging benchmarks Arena-Hard and MT-Bench. The results demonstrate that VIF-RAG maintains consistent alignment across various backbones, with notable 1.3% improvement on MT-Bench for the 14B model. This reveals significant potential for larger models in achieving better natural instruction alignment. 3) Great Preservation of foundational Abilities. Previous research highlights that enhancing specific capabilities often compromises others (Dong et al. 2024b; Hui et al. 2024). As indicated in Table 2, VIF-RAG effectively preserves general capabilities (MMLU, C-Eval), math reasoning (GSM8K), and coding skills (HumanEval) across different configurations, with some slight performance improvements. This preservation is largely attributed to the integration of ShareGPT data in the synthesis process, demonstrating VIF-RAGs ability to balance diverse capabilities while maintaining broad applicability. 6.4. Quantitative Analysis Ablation Study. To examine the effects of various components in VIF-RAG, we conduct an ablation study in Table Figure 6: The analysis of instruction counts on FollowRAG (IF) performance. 7. Conclusion In this paper, we propose VIF-RAG, the first automated, scalable, and verifiable data synthesis pipeline for aligning complex instruction-following in RAG scenarios. VIF-RAG integrates verification process at each step of data augmentation and combination. We begin by manually creating minimal set of atomic instructions (<100) and then apply steps including instruction composition, quality verification, instruction-query combination, and dual-stage verification to generate large-scale, high-quality VIF-RAG-QA dataset (>100K). To address gaps in instruction-following evaluation for RAG systems, we present FollowRAG Bench, featuring around 3K samples with 22 types of complex instruction constraints. Using FollowRAG and 8 widely-used IF and foundational abilities benchmarks, we show that VIFRAG significantly enhances alignment on general instruction constraints and effectively demonstrates the core abilities of LLMs. Further analysis offers insights for optimizing instruction-following alignment in RAG systems. Figure 5: The scaling analysis of retrieved document count and FollowRAG (IF) performance. 3. The term w/o indicates versions where specific components are removed. Our key observations are: Removing any component from VIF-RAG results in decreased performance, indicating that all components, such as the complex instruction composition strategy and quality verification design, are crucial to its effectiveness. The largest performance decline in FollowRAG is observed when executor verification is removed. This underscores the critical role of automated instructionresponse validation in improving synthetic data quality and confirms the advantage of using LLMs to oversee instruction-following abilities through other core skills like coding. Surprisingly, the consistency verification proves beneficial in preserving RAG capabilities. It effectively filters out samples with high-level semantic conflicts between instructions and queries, reducing noise in IF tasks and maintaining RAG performance integrity. Scaling Analysis. To explore the impact of retrieved document quantity on instruction-following performance in RAG scenarios, we refer to Table 5. For the baseline models (SFT versions), instruction-following capability declines as the number of passages increases. Specifically, performance drops sharply by over 6% when the document quantity in FollowRAG increases from 0 to 1. Further increasing the number to 10 leads to significant performance decline, with Qwen-14B-SFT experiencing drop of over 10%. This indicates that integrating knowledge through retrievalaugmented techniques challenges the instruction-following abilities of existing models. In contrast, VIF-RAG shows minor performance drop (<3%) when encountering the first document. As the number of documents increases to 10, VIF-RAGs performance remains relatively stable, demonstrating its robustness. Instruction Difficulty Analysis. To explore the effect of different instruction quantities (i.e., instruction-following difficulty) on model performance in RAG scenarios, we evaluate VIF-RAG and various baseline models on the FollowRAG benchmark, using test sets with 1, 2, and 3 instructions. As shown in Figure 6, as the number of instructions increases, all models generally show decline in instructionfollowing capability, but VIF-RAG consistently outperforms the rest. Notably, even with 3 instructions present simultaneously, VIF-RAG still demonstrates over 5% IF prompt (strict acc.), further validating its superior capability in handling complex instruction-following tasks in RAG scenarios. References Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan, Y.; Ge, W.; Han, Y.; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin, J.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men, R.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.; Wang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang, H.; Yang, J.; Yang, S.; Yao, Y.; Yu, B.; Yuan, H.; Yuan, Z.; Zhang, J.; Zhang, X.; Zhang, Y.; Zhang, Z.; Zhou, C.; Zhou, J.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report. arXiv preprint arXiv:2309.16609. Bang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie, B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023. Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: collaboratively created graph database In Proceedings of the for structuring human knowledge. 2008 ACM SIGMOD international conference on Management of data, 12471250. Cao, B.; Lu, K.; Lu, X.; Chen, J.; Ren, M.; Xiang, H.; Liu, P.; Lu, Y.; He, B.; Han, X.; et al. 2024. Towards Scalable Automated Alignment of LLMs: Survey. arXiv preprint arXiv:2406.01252. Chen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017. ReadarXiv ing wikipedia to answer open-domain questions. preprint arXiv:1704.00051. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. D. O.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y.; Fedus, W.; Li, Y.; Wang, X.; Dehghani, M.; Brahma, S.; et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70): 153. Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; Hesse, C.; and Schulman, J. 2021. Training Verifiers to Solve Math Word Problems. CoRR, abs/2110.14168. Dao, T.; Fu, D. Y.; Ermon, S.; Rudra, A.; and Re, C. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135. Dong, G.; Li, R.; Wang, S.; Zhang, Y.; Xian, Y.; and Xu, W. 2023. Bridging the kb-text gap: Leveraging structured knowledge-aware pre-training for kbqa. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 38543859. Dong, G.; Lu, K.; Li, C.; Xia, T.; Yu, B.; Zhou, C.; and Zhou, J. 2024a. Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models. CoRR, abs/2406.13542. Dong, G.; Yuan, H.; Lu, K.; Li, C.; Xue, M.; Liu, D.; Wang, W.; Yuan, Z.; Zhou, C.; and Zhou, J. 2024b. How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. In Ku, L.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 177198. Association for Computational Linguistics. Dong, G.; Zhu, Y.; Zhang, C.; Wang, Z.; Dou, Z.; and Wen, J. 2024c. Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation. CoRR, abs/2406.18676. Dubois, Y.; Li, C. X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P. S.; and Hashimoto, T. B. 2024. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36. Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020. REALM: Retrieval-Augmented Language Model PreTraining. CoRR, abs/2002.08909. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. arXiv:2009.03300. Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He, J. 2023. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. arXiv:2305.08322. Hui, T.; Zhang, Z.; Wang, S.; Xu, W.; Sun, Y.; and Wu, H. 2024. HFT: Half Fine-Tuning for Large Language Models. CoRR, abs/2404.18466. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de Las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023a. Mistral 7B. CoRR, abs/2310.06825. Jiang, Y.; Wang, Y.; Zeng, X.; Zhong, W.; Li, L.; Mi, F.; Shang, L.; Jiang, X.; Liu, Q.; and Wang, W. 2023b. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410. Jiang, Y.; Wang, Y.; Zeng, X.; Zhong, W.; Li, L.; Mi, F.; Shang, L.; Jiang, X.; Liu, Q.; and Wang, W. 2024a. FollowBench: Multi-level Fine-grained Constraints Following Benchmark for Large Language Models. arXiv:2310.20410. Jiang, Y.; Wang, Y.; Zeng, X.; Zhong, W.; Li, L.; Mi, F.; Shang, L.; Jiang, X.; Liu, Q.; and Wang, W. 2024b. FollowBench: Multi-level Fine-grained Constraints Following Benchmark for Large Language Models. In Ku, L.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 46674688. Association for Computational Linguistics. Joshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017. TriviaQA: Large Scale Distantly Supervised Challenge In Barzilay, R.; and Dataset for Reading Comprehension. Kan, M., eds., Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, 16011611. Association for Computational Linguistics. Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov, S.; Chen, D.; and tau Yih, W. 2020. Dense Passage Retrieval for Open-Domain Question Answering. arXiv:2004.04906. Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A. P.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019. Natural Questions: Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics, 7: 452466. Le, H.; Wang, Y.; Gotmare, A. D.; Savarese, S.; and Hoi, S. C. H. 2022. CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. arXiv:2207.01780. Lewis, P. S. H.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Kuttler, H.; Lewis, M.; Yih, W.; Rocktaschel, T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Li, C.; Dong, G.; Xue, M.; Peng, R.; Wang, X.; and Liu, D. 2024a. DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning. CoRR, abs/2407.04078. Li, C.; Yuan, Z.; Yuan, H.; Dong, G.; Lu, K.; Wu, J.; Tan, C.; Wang, X.; and Zhou, C. 2024b. MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning. In Ku, L.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 1023010258. Association for Computational Linguistics. Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu, B.; Gonzalez, J. E.; and Stoica, I. 2024c. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv preprint arXiv:2406.11939. Liu, W.; Zeng, W.; He, K.; Jiang, Y.; and He, J. 2023. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685. Liu, W.; Zeng, W.; He, K.; Jiang, Y.; and He, J. 2024. What Makes Good Data for Alignment? Comprehensive Study of Automatic Data Selection in Instruction Tuning. In The Twelfth International Conference on Learning Representations. Luo, H.; E, H.; Tang, Z.; Peng, S.; Guo, Y.; Zhang, W.; Ma, C.; Dong, G.; Song, M.; Lin, W.; Zhu, Y.; and Luu, A. T. 2024. ChatKBQA: Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. In Ku, L.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, 20392056. Association for Computational Linguistics. Luo, H.; Sun, Q.; Xu, C.; Zhao, P.; Lou, J.; Tao, C.; Geng, X.; Lin, Q.; Chen, S.; and Zhang, D. 2023. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. CoRR, abs/2308.09583. Ma, X.; Gong, Y.; He, P.; Zhao, H.; and Duan, N. 2023a. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283. Ma, X.; Zhang, X.; Pradeep, R.; and Lin, J. 2023b. ZeroShot Listwise Document Reranking with Large Language Model. CoRR, abs/2305.02156. Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. Mumuni, A.; and Mumuni, F. 2022. Data augmentation: comprehensive survey of modern approaches. Array, 16: 100258. Oguz, B.; Chen, X.; Karpukhin, V.; Peshterliev, S.; Okhonko, D.; Schlichtkrull, M.; Gupta, S.; Mehdad, Y.; and Yih, S. 2022. UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain QuesIn Findings of the Association for Comtion Answering. putational Linguistics: NAACL 2022, 15351546. Seattle, United States: Association for Computational Linguistics. OpenAI. 2023. abs/2303.08774. Qiao, R.; Tan, Q.; Dong, G.; Wu, M.; Sun, C.; Song, X.; GongQue, Z.; Lei, S.; Wei, Z.; Zhang, M.; et al. 2024a. We-Math: Does Your Large Multimodal Model Achieve arXiv preprint Human-like Mathematical Reasoning? arXiv:2407.01284. Qiao, S.; Gui, H.; Lv, C.; Jia, Q.; Chen, H.; and Zhang, N. 2024b. Making Language Models Better Tool Learners with Execution Feedback. arXiv:2305.13068. Qin, Y.; Song, K.; Hu, Y.; Yao, W.; Cho, S.; Wang, X.; Wu, X.; Liu, F.; Liu, P.; and Yu, D. 2024. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601. Qin, Z.; Jagerman, R.; Hui, K.; Zhuang, H.; Wu, J.; Shen, J.; Liu, T.; Liu, J.; Metzler, D.; Wang, X.; and Bendersky, M. 2023. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. CoRR, abs/2306.17563. Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020. DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. KDD 20. Ren, R.; Wang, Y.; Qu, Y.; Zhao, W. X.; Liu, J.; Tian, H.; Wu, H.; Wen, J.; and Wang, H. 2023. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. CoRR, abs/2307.11019. GPT-4 Technical Report. CoRR, Shi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.; Lewis, M.; Zettlemoyer, L.; and Yih, W. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. CoRR, abs/2301.12652. Sun, H.; Liu, L.; Li, J.; Wang, F.; Dong, B.; Lin, R.; and Huang, R. 2024a. Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models. arXiv preprint arXiv:2404.02823. Sun, H.; Liu, L.; Li, J.; Wang, F.; Dong, B.; Lin, R.; and Huang, R. 2024b. Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models. CoRR, abs/2404.02823. Sun, W.; Yan, L.; Ma, X.; Wang, S.; Ren, P.; Chen, Z.; Yin, D.; and Ren, Z. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, 1491814937. Association for Computational Linguistics. Vrandeˇcic, D.; and Krotzsch, M. 2014. Wikidata: Free Collaborative Knowledgebase. Commun. ACM, 57(10): 7885. Wang, Z.; Araki, J.; Jiang, Z.; Parvez, M. R.; and Neubig, G. 2023. Learning to Filter Context for Retrieval-Augmented Generation. CoRR, abs/2311.08377. Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned arXiv preprint language models are zero-shot learners. arXiv:2109.01652. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022. Chainof-Thought Prompting Elicits Reasoning in Large Language In Koyejo, S.; Mohamed, S.; Agarwal, A.; BelModels. grave, D.; Cho, K.; and Oh, A., eds., Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Wen, B.; Ke, P.; Gu, X.; Wu, L.; Huang, H.; Zhou, J.; Li, W.; Hu, B.; Gao, W.; Xu, J.; et al. 2024. Benchmarking Complex Instruction-Following with Multiple Constraints Composition. arXiv preprint arXiv:2407.03978. Xia, C.; Xing, C.; Du, J.; Yang, X.; Feng, Y.; Xu, R.; Yin, W.; and Xiong, C. 2024. FOFO: Benchmark to Evaluate LLMs Format-Following Capability. arXiv:2402.18667. Xie, Q.; Dai, Z.; Hovy, E.; Luong, T.; and Le, Q. 2020. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33: 6256 6268. Xu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao, C.; and Jiang, D. 2023. WizardLM: Empowering Large Language Models to Follow Complex Instructions. arXiv:2304.12244. Yan, J.; Luo, Y.; and Zhang, Y. 2024. RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models. arXiv:2402.13463. Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li, C.; Li, C.; Liu, D.; Huang, F.; et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Yang, S.; Chiang, W.-L.; Zheng, L.; Gonzalez, J. E.; and Stoica, I. 2023. Rethinking Benchmark and Contamination for Language Models with Rephrased Samples. arXiv:2311.04850. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J., eds., Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, 23692380. Association for Computational Linguistics. Yih, W.; Richardson, M.; Meek, C.; Chang, M.; and Suh, J. 2016. The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers. The Association for Computer Linguistics. Yuan, Z.; Yuan, H.; Li, C.; Dong, G.; Tan, C.; and Zhou, C. 2023a. Scaling Relationship on Learning MathematCoRR, ical Reasoning with Large Language Models. abs/2308.01825. Yuan, Z.; Yuan, H.; Li, C.; Dong, G.; Tan, C.; and Zhou, C. 2023b. Scaling Relationship on Learning MathematCoRR, ical Reasoning with Large Language Models. abs/2308.01825. Zhao, C.; Jia, X.; Viswanathan, V.; Wu, T.; and Neubig, G. 2024. SELF-GUIDE: Better Task-Specific Instruction arXiv preprint Following via Self-Synthetic Finetuning. arXiv:2407.12874. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2024a. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36. Zheng, Y.; Zhang, R.; Zhang, J.; Ye, Y.; Luo, Z.; and Ma, Y. 2024b. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. arXiv preprint arXiv:2403.13372. Zhou, J.; Lu, T.; Mishra, S.; Brahma, S.; Basu, S.; Luan, Y.; Zhou, D.; and Hou, L. 2023a. Instruction-Following Evaluation for Large Language Models. arXiv:2311.07911. Zhou, J.; Lu, T.; Mishra, S.; Brahma, S.; Basu, S.; Luan, Y.; Zhou, D.; and Hou, L. 2023b. Instruction-Following Evaluation for Large Language Models. CoRR, abs/2311.07911. Zhou, Y.; Liu, Z.; Jin, J.; Nie, J.-Y.; and Dou, Z. 2024. Metacognitive Retrieval-Augmented Large Language Models. arXiv:2402.11626. Zhu, Y.; Huang, Z.; Dou, Z.; and Wen, J.-R. 2024. One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models. arXiv preprint arXiv:2405.19670."
        },
        {
            "title": "Datasets and Baselines",
            "content": "Training Datasets In the data synthesis process of VIF-RAG, we use the following open-source datasets: Natural Questions (NQ) (Kwiatkowski et al. 2019). Natural Questions is comprehensive dataset created to train and evaluate automatic question-answering systems. It collects large number of queries from Googles website and employs humans to annotate them using relevant knowledge from Wikipedia. The dataset contains 307,372 training examples, 7,830 for development, and 7,842 reserved for testing. TriviaQA (TQA) (Joshi et al. 2017). TriviaQA is an extensive and challenging text-based QA dataset comprising over 950,000 samples from 662,000 documents sourced from Wikipedia and other web pages. It is designed to test the limits of traditional QA systems, offering scenarios where answers arent easily extracted through simple span prediction due to the lengthy and complex contexts. HotpotQA (HQA) (Yang et al. 2018). HotpotQA aims to enhance the systems ability to answer multi-hop questions and its robustness in integrating external knowledge. Unlike other QA datasets that often lack the complexity needed for training systems in reasoning and explanation, HotpotQA offers new challenge with 113,000 questionanswer pairs based on Wikipedia. WebQuestionsSP (WebQSP) (Yih et al. 2016). WebQuestionsSP is resource developed to assess the impact of using semantic parse labels in knowledge base question answering. It extends the original WebQuestions dataset by adding SPARQL queries for 4,737 questions and includes partial annotations for 1,073 questions where complete parse was either unachievable or the questions were poorly formed or needed descriptive answers. ShareGPT (Chiang et al. 2023). ShareGPT is collection of around 90,000 conversations gathered via the ShareGPT API before it was discontinued. The dataset includes user prompts and responses from OpenAIs ChatGPT, providing valuable insights into human-AI interactions. It primarily features messages in English and other Western languages, showcasing the linguistic diversity of its users. Evaluation Benchmarks For cross-domain verification in this paper, we use the following public datasets for evaluation. IFEval (Zhou et al. 2023a). IFEval is the most commonly used comprehensive instruction-following evaluation set for LLMs. The dataset includes over 500 prompts aimed at testing how effectively LLMs perform specific, verifiable tasks. It covers 25 types of atomic instructions, each verifiable using simple, interpretable, and deterministic programs to determine if the responses adhere to the instructions. FollowBench (Jiang et al. 2023b). Followbench evaluates models ability to follow complex instructions by categorizing the instruction-following assessment into five different categories. It employs multi-level mechanism to precisely enforce these constraints by evaluating the associated difficulty levels. MT-Bench (Zheng et al. 2024a). MT-Bench aims at evaluating multitask learning models, particularly in multiturn dialogue and instruction-following tasks. It includes 80 high-quality multi-turn dialogue questions across eight common use cases: writing, role-playing, information extraction, reasoning, mathematics, coding, STEM knowledge, and humanities/social sciences. MT-Bench emphasizes challenging questions to effectively distinguish the capabilities of different models. Arena-Hard (Li et al. 2024c). Arena-Hard is dataset used to assess the robustness of dialogue systems by testing their performance on challenging and diverse scenarios. It includes 500 carefully selected user queries that reflect complex real-world conversations, including language variations, spelling errors, and grammatical mistakes. C-Eval (Huang et al. 2023). C-Eval, as comprehensive evaluation of general capabilities for Chinese large models, categorizes 13,948 test samples into 52 sub-domains and encompasses difficulty system across four dimensions.. There is also subset, C-Eval Hard, focusing on problems that demand advanced reasoning skills. MMLU (Hendrycks et al. 2021). MMLU, as the most widely used general capability evaluation set, covers assessments across 57 domains, including science and math, with difficulty level spanning multiple tiers. It serves as key tool for assessing language model performance across varied tasks. GSM8K (Cobbe et al. 2021). GSM8K is classic mathematical reasoning evaluation dataset that primarily focuses on math problem-solving at the grade school level. It requires models to arrive at answers by outlining their reasoning paths. The dataset contains over 8,000 samples, with the training set comprising 7,473 samples and the test set containing 1,319 samples. HumanEval (Chen et al. 2021). HumanEval as benchmark for evaluating code generation models, featuring 164 unique programming problems, each with about 9.6 test cases. It assesses the functional accuracy of generated code through these diverse test cases. HumanEval+ extends this by increasing the average number of test cases to 774.8 per problem, providing more rigorous evaluation of code generation models. Baselines We compare the VIF-RAG framework with several strong baselines for the instruction-following task as follows: Evol-Instruct (Xu et al. 2023). Evol-Instruct is the publicly available WizardLM-Evol-Instruct dataset, which includes 143k samples consisting of blend of Alpaca and ShareGPT evolved data. In accordance with the approach outlined in the original paper. To ensure fair comparison, we combined their dataset with the same amount of RAG data (NQ, TQ, HQ, WebQ) as used in VIF-RAG. Conifer (Sun et al. 2024a). Conifer is an advanced language model designed to excel at following complex, constraint-based instructions. It stands out for its progressive learning approach, where tasks start simple and increase in complexity, allowing the model to handle intricate instructions effectively. The models dataset was meticulously crafted using GPT-4 to ensure diverse and challenging set of instructions. This makes Conifer particularly strong in real-world applications that require precise instruction adherence, setting it apart from other models in its ability to manage complex tasks. To ensure fair comparison, we combined their dataset with the same amount of RAG data (NQ, TQ, HQ, WebQ) as used in VIF-RAG. Deita-7B-V1.0-SFT (Liu et al. 2023). Deita-7B is data selection method that focuses on high-quality data selection and instruction fine-tuning. It utilizes the DEITA method, which combines complexity, quality, and diversity filtering to optimize model training. Despite using smaller dataset, Deita-7B excels in various natural language processing tasks by effectively leveraging highquality data. SFT Version. To build strong baseline model and fair comparison with VIF-RAG, we use the same amount of ShareGPT and RAG data (NQ, TQ, HQ, WebQ) as in VIF-RAGs data synthesis process, mixing them together to fine-tune (SFT) different baseline models. This resulted in the strong baseline models labeled as Backbone-SFT in the main experiments. Additionally, We list all the backbone models mentioned in the articles here. Llama3-8B (Meta 2024). Llama3-8B is part of the opensource Llama3 series, developed by MetaAI, is the latest and most advanced model in the Llama series. It offers notable improvements over Llama 2, Especially supporting longer input windows. These upgrades enhance performance in contextual understanding and language generation, making Llama 3 standout in the series. Qwen1.5-7B & 14B (Bai et al. 2023). Qwen1.5 is significant release in Alibaba Clouds Qwen series of large language models, featuring models with 7B and 14B parameters. It excels in multilingual tasks and supports longcontext processing up to 32K tokens, making it ideal for applications like chatbots and language understanding. These enhancements made Qwen 1.5 powerful tool for diverse AI applications. Mistral-7B (Jiang et al. 2023a). Mistral 7B, released in September 2023 by Mistral AI, is an efficient language model utilizing advanced techniques. Despite having only 7 billion parameters, it outperforms many LLMs with same param size, such as reasoning, mathematics, and code generation. The model is open-source, making it widely accessible and customizable for different applications."
        },
        {
            "title": "Implementation Details",
            "content": "Details about Instruction-Query Synthesis For the data synthesis part of RAG in section 4.2 Scalable Instruction-Query Synthesis, following several RAG works (Oguz et al. 2022; Dong et al. 2023; Luo et al. 2024), we use DPR (Karpukhin et al. 2020) as retriever for encoding knowledges. We use it to retrieve the top-K (k=3) relevant documents from the Wikipedia (Vrandeˇcic and Krotzsch 2014) retrieval corpus based on similarity.. In this paper, we randomly samples 60K ShareGPT samples and 40K RAG samples (10K each from NQ, TQ, HQ, and WebQ), then concatenate them with our high-quality synthetic instructions. Finally, we refer to data templates from previous RAG studies (Wang et al. 2023; Ren et al. 2023; Dong et al. 2024c; Qiao et al. 2024a) and directly concatenate the instruction with the RAG dataset. For the ShareGPT data, we directly concatenate the instruction with the query without using any specific templates. For consistency checks across multiple instructions in section Instruction Composition & Verification, The prompt are listed here: Prompt Template for Multi-Instructions Verification You are an expert proficient in determining whether multiple instructions are suitable to be implemented as simultaneous constraints. [Instructions]{instruction} The text contains two or more instructions. Based on the semantic coherence and logical connection, assess whether these instructions are suitable to be implemented as simultaneous constraints. Please first conduct thorough analysis and then assign score ranging from 0 to 10 on the last line. score of 0 indicates that the instructions are highly inappropriate to coexist, while score of 10 signifies that the instructions are very suitable to serve as concurrent constraints. Please ensure that only score is provided in the format Score: score without any additional content on the last line. Our VIF-RAGs prompt templates, instruction data format, verfication code, test cases and more can be found in the supplementary materials. Details about Supervised Fine-tuning For all LLM fine-tuning, we use global batch size of 128, an input window of 4096, and learning rate of 7e-6 with 2% warm-up. Each set of experiments involves finetuning for 3 epochs. Our training framework is DeepSpeed Zero3 (Rasley et al. 2020). To reduce memory usage, we also employed the Flash Attention (Dao et al. 2022) strategy during training and utilized BF16 for mixed precision testing. Our experiments are performed on NVIDIA A800 GPUs. Specifically, Qwen1.5-7B, Mistral-7B, and LLaMA3-8B are trained on 8 A800 GPUs. We use the Llama Factory framework (Zheng et al. 2024b) (version 0.6.3) for training and employed greedy decoding to test the HumanEval dataset, with the metric being Pass@1. We use five sets of random seeds to conduct the same series of experiments. Knowledge Bases for RAG For the NQ, TQ, and HQ datasets, We used Wikipedia as the retrieval knowledge base. We follow the DPR approach by first applying the pre-processing code from DrQA (Chen et al. 2017) to extract clean text, removing tables, infoboxes. Each article is then divided into 100-word text blocks, resulting in total of 21,015,324 passages. Each passage is prefixed with the article title and an [SEP] token. For WebQSP, we utilize Freebase (Bollacker et al. 2008) as the knowledge base, following the methods described in unikQA and SKP. Freebase, which includes over 125 million tuples, more than 4,000 types, and over 7,000 properties, is used for knowledge retrieval. To handle the challenge of indexing billions of relations, we implement twostep retrieval process. DPR retrieves relations from this reduced set. The retrieved relations, typically short sentences, are combined into passages of no more than 100 tokens and provided to the FiD reader as text paragraphs."
        },
        {
            "title": "Details of FollowRAG",
            "content": "Atomic instructions in FollowRAG We present the 22 types of atomic instructions included in FollowRAG in Table 4. Judging Prompt for RAG Scores in FollowRAG Under multiple instruction constraints, the models target output differs from the gold answers in the original QA dataset, rendering previous evaluation metrics like exact match ineffective. To address this issue, we use the original gold answers as the reference and employ GPT-4o to assess whether the models output correctly answers the questions. The prompt used to instruct GPT-4o to evaluate the responses is as follows:"
        },
        {
            "title": "Judging Prompt for RAG Scores",
            "content": "Please act as an impartial judge and perform the task: Given [Question], you need to evaluate whether the [Response] correctly answers or hits the correct answer, and output your judgment after [Judge]. will provide correct answer [Reference] as reference. Scoring criteria: - If the [Response] is completely correct and aligns with the correct answer, it scores 1 point; - If the [Response] partially answers correctly, it scores 0.5 point; - If the [response] is completely incorrect compared to the [Reference], it scores 0 point. Note: - Your only evaluation criterion is whether the [Response] correctly answered the answer, regardless of the format, language, case, length, etc., of the [Response]. Besides, providing more information than the [Reference] in the [Response] cannot be reason for point deduction. - Use the [Reference] as the correct answer reference rather than your own knowledge. - The rating reply must strictly follow the for- [judge score]nReason: mat [judge reason], and do not output any other content. For example: Rating: [0]nReason: [Response and Reference are completely unrelated.]. Ensure that judge score and judge reason are enclosed in []. Rating: below: [Question] {question} [Reference] {answer gold} [Response] {response} [Judge] Considering that evaluating the RAG scores for all samples in FollowRAG requires substantial number of GPT-4o calls, we randomly sampled 100 entries from NQ, TQ, HQ, and WebQ for scoring and calculating the RAG scores. Consistency with Human Evaluation To evaluate the effectiveness of GPT-4 scoring in assessing LLM responses, we conducted consistency experiment between GPT-4 prediction scores and human scores. For fillin-the-blank and open-ended questions, we randomly sampled 30 instances each from the base model, the SFT version model, and the VIF-RAG model test cases, totaling 90 instances, and had human annotator score these predictions. In Table 5, we report the consistency between the average human scores and GPT-4 scores, measured by Pearson correlation. The strong alignment between human and GPT-4 scores validates the effectiveness of GPT-4 scoring. More Experiments for VIF-RAG Details of Main Results Since FollowRAG adopts the code-based instruction following verification method following IFEval, its instruction following metrics can correspond to the two levels in IFEval as well: Instruction: The proportion of followed atomic instructions to the total number of atomic instructions in the entire dataset."
        },
        {
            "title": "End with\nPostscript\nFirst Word",
            "content": "Include specific keywords in the response. Exclude specific keywords in the response. Frequency constraint for including specific keywords in the response. Constraint on the number of words. Constraint on the number of sentences. Constraint on the number of paragraphs. Wrapped the response in JSON format. Response wrapped in double quotes. No commas allowed. Restrict output language. Repeat the question before answering. Include specific title. Constrain the number of sections. The answer must highlight at least {N} parts. Constrain the number of bullet points. Constrain the number of placeholders. Response must be in all capital letters. Response must be in all lowercase letters. Constrain the number of capitalized words Response must end with specific content. Use special markings at the end of the Response, such as P.S. Constrain the starting word of paragraph n. Table 4: Names and explanations of the 22 types of atomic instructions included in FollowRAG."
        },
        {
            "title": "Model",
            "content": "Qwen1.5-14Bbase Qwen1.5-14BSFT Qwen1.5-14BSFT-VIF-RAG"
        },
        {
            "title": "IFEval",
            "content": "IF"
        },
        {
            "title": "RAG",
            "content": "Ins(L) Prompt(L)"
        },
        {
            "title": "Consistency",
            "content": "0.9639 0.9598 0.9619 0.9626 Table 5: The Pearson correlation coefficient between GPT4o scoring and human scoring for the RAG score in FollowRAG. Qwen1.5-7B-base 28.2 36.5 27.9 40.6 Supervision Model: GPT-4 Qwen1.5-7B-SFT-VIF-RAG 36.2 56.1 46.0 57.1 Supervision Model: Qwen2-72B Qwen1.5-7B-SFT-VIF-RAG 39.0 55.1 44.0 54."
        },
        {
            "title": "Setup",
            "content": "Bench. Train Test Rephrase Percentage N-gram Supervision Model: Llama3-70B FollowRAG 10K 2.8K 11 ShareGPT+ RAG"
        },
        {
            "title": "IFEval",
            "content": "10K"
        },
        {
            "title": "Followbench",
            "content": "10K 820 FollowRAG 10K 2.8K VIF-RAG-QA"
        },
        {
            "title": "IFEval",
            "content": "10K"
        },
        {
            "title": "Followbench",
            "content": "10K 820 2 1 3 0 0.4% 0.05% 0.01% 0.1% 0.01% 0.01% 5.3% 4.9% 2.7% 3.1% 4.3% 2.6% Table 6: Contamination analysis on VIF-RAG data. Train & Test denotes the size of corresponding set. Rephr. represents samples similar to the test sample Prompt: The proportion of samples where all atomic instructions are followed to the total number of samples in the entire dataset. In addition, Strict and Loose indicate whether the response will be processed before scoring, such as removing common font modifiers, introductory phrases like Sure, here it is:, and closing phrases like Hope it helps. We adopt the Loose Instruction score in main text and present Qwen1.5-7B-SFT-VIF-RAG 40.6 52. 41.0 52.3 Table 7: Ablation study on supervision models from GPT-4 with Qwen2-72B and Llama3-70B. all the different instruction follwoing scores in Table 8. Data Contamination Analysis. We evaluate the contamination of VIF-RAG-QA on FollowRAG, IFEval and FollowBench. Our detailed analysis is conducted separately from two aspects: rule-based detection and model-based detection. For rule-based detection, we report contamination findings detected by traditional n-gram contamination algorithms. As shown in Table 6, both contamination rates are lower than those of the ShareGPT+RAG dataset we used. For model-based detection, we employ LLM contamination detectors from LM-Sys (Yang et al. 2023), which utilize advanced chatbots to identify potentially rephrased contaminated test samples. Compared to ShareGPT+RAG dataset, Conifer shows relatively lower percentage of similar samples, which indicates an absence of data contamination. This Model Llama3-8B-base Llama3-8B-SFT Llama3-8B-SFT-VIF-RAG Mistral-7B-base Deita-7B-V1.0-SFT Mistral-7B-SFT Conifer Mistral-7B-SFT Evol-Instruct Mistral-7B-SFT Mistral-7B-SFT-VIF-RAG Qwen1.5-7B-base Qwen1.5-7B-SFT Qwen1.5-7B-SFT-VIF-RAG Qwen1.5-14B-base Qwen1.5-14B-SFT Qwen1.5-14B-SFT-VIF-RAG NQ TQ HQ WebQ ALL Pr. (S) Pr. (L) Ins. (S) Ins. (L) Pr. (S) Pr. (L) Ins. (S) Ins. (L) Pr. (S) Pr. (L) Ins. (S) Ins. (L) Pr. (S) Pr. (L) Ins. (S) Ins. (L) Pr. (S) Pr. (L) Ins. (S) Ins. (L) 1.6 6.1 22.3 13.0 17.1 11.4 12.7 6.7 19.4 13.7 6.1 20.3 17.6 9.6 23.6 1.6 6.1 23. 13.9 18.7 14.1 21.0 8.9 31.3 13.9 6.1 20.9 18.3 9.7 24.9 3.1 15.7 43.1 24.5 29.3 25.0 30.7 15.9 37.6 26.9 16.0 37. 32.7 21.7 40.7 3.2 15.7 43.9 25.7 31.4 29.9 41.7 21.0 51.2 27.7 16.1 38.9 33.7 22.0 42.1 1.3 5.3 20. 15.4 13.7 10.3 12.3 5.7 17.0 13.6 5.4 18.7 17.7 7.4 22.3 1.3 5.3 20.1 16.3 14.4 12.3 17.0 6.6 25.6 14.0 5.4 18. 18.1 7.4 22.7 4.00 15.0 41.7 24.9 27.0 26.6 28.0 14.9 34.4 26.6 14.3 34.5 31.8 18.3 38.5 4.1 15.0 42. 25.9 29.0 30.5 37.0 17.2 45.9 27.7 14.3 35.8 32.5 18.7 40.1 1.6 5.1 18.6 15.1 15.3 8.4 11.0 5.3 17.0 13.6 4.9 19. 15.7 7.4 22.9 1.6 5.3 19.3 15.6 16.3 10.3 15.1 6.3 23.7 14.4 4.9 20.0 16.4 7.6 23.6 3.6 15.0 38. 25.9 24.8 22.5 27.7 15.7 33.9 26.6 14.8 36.5 31.6 18.7 37.3 3.6 15.0 39.6 26.9 26.5 26.5 35.4 17.6 44.9 27.5 14.8 38. 32.4 18.8 38.8 6.3 6.1 19.0 13.1 16.6 13.0 14.1 7.1 20.6 16.6 6.1 15.1 17.1 9.0 17.9 6.6 6.1 19. 13.4 16.9 15.4 18.9 8.1 27.9 17.4 6.1 15.9 17.6 9.0 18.4 9.7 14.4 41.6 24.3 25.1 27.4 31.8 18.2 37.9 28.3 13.6 30. 31.7 19.5 34.8 10.0 14.4 42.5 24.7 26.3 31.1 39.4 21.7 47.8 30.0 13.7 31.9 33.0 19.5 35.7 2.7 5.7 20. 14.2 15.7 10.8 12.5 6.2 18.5 14.4 5.6 18.3 17.0 8.3 21.7 2.8 5.7 20.4 14.8 16.6 13.1 18.0 7.5 27.1 14.9 5.6 18. 17.6 8.4 22.4 5.1 15.0 41.2 24.9 26.6 25.4 29.5 16.2 35.9 27.1 14.7 34.7 32.0 19.6 37.8 5.2 15.0 42. 25.8 28.3 29.5 38.4 19.3 47.4 28.2 14.7 36.2 32.9 19.8 39.2 Table 8: Detailed scores of instruction following under different metrics for FollowRAG. Pr. and Ins. represent Prompt and Instruction levels, while and denote Strict and Loose. multi-instruction problem in RAG scenarios continues to be promising direction with major implications for complex RAG interactions. More Complex Instruction Types: As the first benchmark for RAG scenarios, FollowRAG provides comprehensive evaluation of existing models accuracy in these scenarios. However, the variety of instructions in the real world is vast, and it is impossible to cover all types in one work. Further evaluating and improving the handling of complex instructions that are difficult to validate (e.g., interaction styles, domain-specific knowledge in RAG) will be an important focus for our future research. We believe that future work on instruction-following alignment will offer greater promise for the practical applications of RAG systems. allows us to confidently assert that there is no contamination between the self-generated training samples and the test sets. Ablation for Supervision Model. Table 3 presents the results of replacing the supervision model from GPT-4 with Qwen2-72B and Llama3-70B. We observe that in the VIF-RAG framework, the stronger supervision model (GPT-4) demonstrates more effective strongto-weak distillation alignment. However, Qwen2-72B and Llama3-70B also maintain solid performance, with accuracy consistency in IFEval loose prompts exceeding 50%. This highlights the flexibility and robustness of our VIFRAG framework, which can adapt well to different supervision models. Case Presentation and Analysis Our VIF-RAG synthetic instruction data, code, test cases, and more can be found in the supplementary materials. The Case Study of VIF-RAG To gain deeper understanding of how VIF-RAG achieves instruction-following alignment in RAG scenarios, we conducted case study and manual analysis, as shown in the figure 7, 8 and 9. Since there is no truly gold response after following the instructions, we can only use the original gold response from the RAG dataset as reference. Challenges and Future Work In this paper, we first explore instruction-following alignment in RAG scenarios and develope high-quality RAG instruction-following data synthesis framework, VIF-RAG, along with comprehensive benchmark, FollowRAG. However, during our research, we encounter several more challenging scenarios: Increased Number of Instructions: As shown in Figure 1, our experiments revealed that existing models can handle up to 4 instructions in RAG scenarios. Even in such cases, VIF-RAG still manages to correctly answer several questions, while other baseline models lose accuracy entirely. Therefore, the challenge of increasing the number of instructions remains significant. Effectively addressing the Figure 7: case study (1) of VIF-RAG and different models in the followrag benchmark, with the backbone Mistral-7B. Figure 8: case study (2) of VIF-RAG and different models in the followrag benchmark, with the backbone Mistral-7B. Figure 9: case study (3) of VIF-RAG and different models in the followrag benchmark, with the backbone Mistral-7B."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "School of Artificial Intelligence, Beijing University of Posts and Telecommunications"
    ]
}