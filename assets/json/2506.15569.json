{
    "paper_title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification",
    "authors": [
        "Chengye Wang",
        "Yifei Shen",
        "Zexi Kuang",
        "Arman Cohan",
        "Yilun Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks."
        },
        {
            "title": "Start",
            "content": "SCIVER: Evaluating Foundation Models for Multimodal Scientific Claim Verification"
        },
        {
            "title": "Yale NLP Lab",
            "content": "5 2 0 2 8 1 ] . [ 1 9 6 5 5 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce SCIVER, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within multimodal scientific context. SCIVER consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals substantial performance gap between these models and human experts on SCIVER. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models comprehension and reasoning in multimodal scientific literature tasks."
        },
        {
            "title": "Data\nCode",
            "content": "chengyewang/SciVer QDRhhhh/SciVer"
        },
        {
            "title": "Introduction",
            "content": "Scientific claim verification has become increasingly vital as the research community grapples with an ever-expanding body of scientific literature across diverse domains (Dasigi et al., 2021; Wadden et al., 2022; Lee et al., 2023; Asai et al., 2024). The accuracy of claim verification in scientific paper is not merely matter of cross-checking numerical consistency or validating conclusionsit necessitates holistic understanding of the papers context (e.g., textual content, charts, and tables). Despite the significance of multimodal reasoning, existing benchmarks in scientific claim verification have often treated these components in isolation. Predominantly, prior works have focused either on textual content alone (Wadden et al., 2022) or on verifying claims based on single table (Lu et al., 2023). While previous multimodal questionanswering (QA) benchmarks in scientific literature comprehension incorporate scientific charts, they still remain limited to QA tasks over single chart (Li et al., 2024d; Wang et al., 2024b; Li et al., 2024c), failing to capture the broader multimodal context of scientific literature. Consequently, the lack of comprehensive multimodal benchmark restricts the systematic evaluation of foundation models ability to reason across the diverse and interconnected modalities in scientific literature. In this work, we introduce SCIVER, comprehensive and high-quality benchmark for evaluating multimodal SCIentific claim VERification. SCIVER consists of 3,000 expert-annotated examples over 1,113 scientific papers spanning diverse domains within computer science. To ensure our benchmark reflects real-world scenarios in scientific literature comprehension, we design four finegrained tasks (as illustrated in Figure 1): direct reasoning, parallel reasoning, sequential reasoning, and analytical reasoning. Each task targets common reasoning type in multimodal scientific claim verification. Moreover, each example includes expert-annotated supporting evidence, facilitating fine-grained performance evaluation. We conduct an extensive evaluation on SCIVER, covering 21 frontier open-source and proprietary multimodal foundation models. Our experimental results reveal that while state-of-the-art models achieve human-comparable performance on simpler tasks (e.g., direct reasoning), they continue to struggle with more complex challenges. For instance, GPT-4.1, achieves an accuracy of 70.8% on analytical reasoning, falling significantly short of human expert performance (i.e., 90.0%). This demonstrates the challenging nature of SCIVER. Furthermore, our analysis of retrieval-augmented generation (RAG) and human-conducted error analyses provide insights for future advancement. Figure 1: An illustration of the four subsets in the SCIVER benchmark. Our benchmark is designed to evaluate document-grounded scientific claim verification in multimodal setting. To effectively perform this task, models must go through the full context of scientific paperincluding text, charts, and tablesto locate the appropriate supporting evidence before verifying claim. The complete data examples are provided in Appendix C. We summarize our contributions as follows: We introduce new claim verification benchmark to challenge foundation models across diverse reasoning scenarios in multimodal scientific literature comprehension. Each example undergoes expert annotation and strict quality control to ensure benchmark reliability and high standards. We conduct an extensive evaluation that encompasses 21 open-source and proprietary foundation models, comprehensively assessing their capabilities and limitations in our task. We provide an in-depth analysis of Chain-ofThought reasoning, RAG settings, and model reasoning errors, offering valuable insights for future advancements and targeted improvements."
        },
        {
            "title": "2.1 Claim Verification",
            "content": "2021; Wadden et al., 2022; Rangapur et al., 2024). The second is context-grounded claim verification, where claims are verified based solely on given context, without relying on external retrieval (Chen et al., 2020; Kamoi et al., 2023; Lu et al., 2023; Glockner et al., 2024; Zhao et al., 2024). This work focuses on the latter setting, as it removes variability introduced by retriever performance and enables more controlled evaluation of foundation models ability to verify claims within multimodal scientific context. As shown in Table 1, existing multimodal claim verification benchmarks primarily use either single table (Chen et al., 2020; Gupta et al., 2020; Lu et al., 2023) or single chart (Akhtar et al., 2024) as input context. In real-world scenarios, however, verifying claims in scientific literature requires reasoning across multiple modalities, including textual descriptions, tables, and figures."
        },
        {
            "title": "2.2 Scientific Literature Comprehension",
            "content": "Claim verification is well-established research area that can be categorized into two main settings. The first is the open-domain setting, where an external retriever is used to fetch relevant information from large corpus to verify claims (Vlachos and Riedel, 2014; Thorne et al., 2018; Aly et al., With the rapid expansion of research publications, evaluating and applying foundation models for scientific literature comprehension has become increasingly important (Asai et al., 2024; Skarlinski et al., 2024; Li et al., 2024b). Existing benchmarks primarily focus on QA tasks, assessing models on Dataset Input Context Data Construction # Task / Subsets Rationale Annotation? Scientific Literature Comprehension Single NLP paper QASPER (Dasigi et al., 2021) Single AI/ML paper (text-only) QASA (Lee et al., 2023) Multiple figures or charts from STEM papers MMSci (Li et al., 2024d) Single chart from arXiv papers ArXivQA (Li et al., 2024c) CharXiv (Wang et al., 2024b) Single Chart from arXiv papers SCIFACT (Wadden et al., 2020) Multiple STEM paper abstracts Expert annotation Expert annotation GPT-4o generation GPT-4V generation Expert annotation Expert annotation Claim Verification over Multimodal Context INFOTABS (Gupta et al., 2020) Single wikipedia table TABFACT (Chen et al., 2020) Single wikipedia table ChartCheck (Akhtar et al., 2024) Single wikipedia chart SCITAB (Lu et al., 2023) Single scientific table from NLP&ML paper Crowdsourcing Crowdsourcing Crowdsourcing Expert+InstructGPT SCIVER (ours) Multiple tables, charts, paragraphs from CS papers Expert annotation 4 3 2 2 2 2 4 Evidence Evidence Rationale Evidence Table 1: Comparison of SCIVER with existing claim verification and scientific literature comprehension benchmarks. their ability to extract or infer information from scientific papers (Dasigi et al., 2021; Lee et al., 2023). While recent efforts have extended QA tasks to incorporate tabular and visual information (Li et al., 2024c; Wang et al., 2024b; Li et al., 2024d), they remain constrained by their single-modality focus, neglecting the rich multimodal context inherent in scientific papers. Claim verification, on the other hand, demands more comprehensive understanding of scientific literature, as claims are often supported by combination of textual descriptions, tables, and charts. Additionally, each example in SCIVER includes detailed supporting evidence, facilitating fine-grained evaluation."
        },
        {
            "title": "3 SCIVER Benchmark",
            "content": "SCIVER is comprehensive evaluation framework designed to assess the ability of foundation models to verify scientific claims within multimodal context. Figure 2 provides an overview of the SCIVER construction pipeline. In the following subsections, we detail the benchmark design, data construction process, and quality validation methodology."
        },
        {
            "title": "3.1 Benchmark Design",
            "content": "We first present the task formulation and the four specialized subsets of our dataset that we designed to evaluate different aspects of model performance. Task Formulation. We formally define the task of SCIVER within the context of foundation model as follows: Given scientific claim and multimodal contexts {P, I, } collected from scientific paperwhere denotes textual paragraphs, denotes multiple charts, and denotes multiple tablesthe model is is tasked with determining the entailment label ℓ = {entailed, refuted}: ℓ = arg max ℓL FM(ℓ c, P, I, ) (1) It challenges foundation models to perform complex reasoning by integrating and interpreting textual, tabular, and visual data to verify scientific claims. Since scientific tables often have intricate structures that are difficult to represent in textual format, we follow recent work in multimodal table understanding (Zheng et al., 2024; Deng et al., 2024) by using table screenshots as inputs. Subset Design. SCIVER includes the following four distinct subsets, each designed to evaluate specific reasoning type commonly required for scientific claim verification over multimodal context: (1) Direct Reasoning, which evaluates models ability to extract and interpret single piece of information to verify scientific claim. (2) Parallel Reasoning, which evaluates models ability to simultaneously process and integrate information from multiple distinct sources. (3) Sequential Reasoning, which evaluates models ability to perform step-by-step inference chains across different modalities. Models are required to establish logical connections between multiple pieces of evidence, where each steps conclusion becomes premise for subsequent reasoning steps. (4) Analytical Reasoning, which evaluates models ability to verify claims that require both sophisticated domain knowledge and complex reasoning beyond direct data extraction. Models must not Figure 2: An overview of the SCIVER benchmark construction pipeline. only interpret the provided data but also apply relevant scientific principles and methodological understanding to arrive at valid conclusions. Appendix presents detailed examples of each subset. These subsets enable fine-grained evaluation across different reasoning paradigms commonly encountered in scientific literature comprehension."
        },
        {
            "title": "3.2 Preliminary Setup",
            "content": "We next discuss the preliminary setup for data construction, including the process of scientific paper collection and expert annotator recruitment. Expert Annotator Recruitment and Training. Existing claim verification datasets primarily rely on crowdsourced data curation (as shown in Table 1). However, our preliminary study suggests that crowd-sourced annotators often lack the necessary domain expertise for our task. To mitigate this, we recruit 18 CS graduate students with relevant subject-specific knowledge, requiring each to have at least two peer-reviewed publications in their assigned subfields. Detailed annotator biographies are provided in Table 5 in Appendix. To further enhance annotation quality and consistency, all selected experts undergo mandatory two-hour individual training session with one of the authors, ensuring that they are familiar with the annotation guidelines and protocol. fewer than two tables or two charts."
        },
        {
            "title": "3.3 Claim Annotation",
            "content": "Given paper relevant to their research field, the annotators follow these steps for claim annotation: Multimodal Scientific Context Preparation. Scientific papers are often lengthy, exceeding the maximum context length of certain foundation models. Including the full text may overwhelm these models and hinder their ability to integrate information effectively across modalities. To address this, annotators refine the paper context by removing textual sections that are not essential to understanding the core research problem, such as related work, acknowledgments, references, and appendix sections. Entailed Claim Annotation. To reduce bias stemming from the positioning of evidence, the annotation interface randomly selects three charts or tables from the curated context, along with their surrounding textual paragraphs. Annotators are then tasked with writing an entailed claim that aligns with the pre-given reasoning types (i.e., subset). They are required to ensure that verifying the claim requires referencing at least one of the three sampled multimodal elements. Subsequently, annotators identify all relevant supporting evidence, which is later reviewed by second annotator. Scientific Paper Collection. SCIVER focuses on arXiv papers published between September 1, 2024, and November 15, 2024, covering eight key areas of computer science. To ensure high-quality content, we prioritize papers that include comments indicating acceptance by peer-reviewed venue. For each paper, we extract its multimodal contextincluding textual content, tables, and chartsfrom the HTML versions available on the arXiv platform. We filter out papers that contain Refuted Claim Annotation. Following established practices in the field (Wadden et al., 2022; Chen et al., 2020; Lu et al., 2023), and given the difficulty of directly obtaining refuted claims, we instead generate them by perturbing original entailed claims through semi-automated annotation process. Specifically, to curate refuted claims, annotators modify the initially annotated entailed claim by introducing factual errors that contradict the supporting evidence. Property (avg.)"
        },
        {
            "title": "Test",
            "content": "Adopted Chain-of-Thought Prompt"
        },
        {
            "title": "Multimodal Scientific Context",
            "content": "# Words in text paragraphs # Tables Table caption length # Charts Chart caption length 583.6 0.55 14.2 0.94 39."
        },
        {
            "title": "Claim Verification",
            "content": "Claim length # Entailed # Refuted Supporting Evidence Scientific papers Total examples 30.5 505 495 2.63 327 1, 567.4 0.54 13.7 0.95 40.2 33.9 995 1,005 2.62 786 2,000 Table 2: Data statistics of SCIVER."
        },
        {
            "title": "3.4 Supporting Evidence Annotation",
            "content": "After completing the claim annotation, second annotator, who is also an expert in the relevant research field, is tasked with annotating the supporting evidence. The annotators are required to carefully review the claim and identifying all relevant paragraphs, tables, and charts that serve as supporting evidence. To ensure consistency and accuracy, we compare the supporting evidence and entailment label annotated in this step with those from the initial claim annotation. If discrepancies arise between the two annotations, third expert annotator is introduced to adjudicate the differences. Our process achieves an inter-annotator agreement of 94.0% for entailment label annotation, demonstrating strong reliability in our annotation."
        },
        {
            "title": "3.5 Data Validation",
            "content": "Each annotated example undergoes comprehensive validation process conducted by different expert annotator within the same research field. The validation focuses on the following five aspects: (1) The claim must be grammatically correct, wellstructured, and free of spelling or typographical errors. (2) The claim must align with the annotation requirements of its corresponding subset and should not be verifiable using textual context alone. (3) The claim must be meaningfully situated within the paper context and hold practical significance for scientific literature comprehension. (4) The annotated supporting evidence must be directly relevant to the claim and comprehensive enough to support claim verification without requiring additional, unannotated context. If an example fails to meet any of these crite- {Paper Context (textual paragraphs, tables, charts)} You are given multimodal scientific context that includes textual paragraphs, tables, and charts. Your task is to determine whether the given claim is Entailed or Refuted. Be skeptical and cautious: if there is any inconsistency, missing evidence, or ambiguity, consider the claim incorrect. Claim to verify: {Claim} Start by explaining your reasoning process clearly, focusing on identifying potential contradictions, lack of support, or misleading interpretations. Think step by step before answering. Figure 3: The Chain-of-Thought prompt used. ria, validators are responsible for making necessary revisions. In practice, 232 initially annotated examples required revisions before being finalized."
        },
        {
            "title": "3.6 Data Statistics and Analysis",
            "content": "Table 2 presents the data statistics of SCIVER. It is randomly divided into the validation and test sets. The validation set contains 1,000 examples and is intended for model development and validation. The test set comprises the remaining 2,000 examples and is designed for standard evaluation. To approximate human-expert-level performance on SCIVER, we randomly sampled 10 claims from each subset, totaling 40 claims. Two expert annotators independently evaluated these claims, providing the natural language explanation and final entailment label for each claim. They achieve an average accuracy of 93.8%  (Table 3)  ."
        },
        {
            "title": "4 Experiment",
            "content": "This section first outlines the experiment setup, and then discusses our experiment results and analysis."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "We use accuracy as the primary metric to evaluate model performance on SCIVER. Following recent benchmark studies (Yue et al., 2024, 2025), we adopt rule-based methods to derive the final entailment label from the model response, which is then compared to the ground-truth label. We evaluate broad range of frontier foundation models that support multiple images and text as input. Specifically, we evaluate 11 series of open-source models, including InternVL-2, 2.5,"
        },
        {
            "title": "Test Set",
            "content": "Avg. Validation Avg. Test Human Expert Random Guess o4-mini Gemini-2.5-Flash GPT-4o Gemini-2.0-Flash GPT-4.1 GPT-4o-mini"
        },
        {
            "title": "Baseline Settings",
            "content": "100.0 50.0 95.0 50."
        },
        {
            "title": "Proprietary Models",
            "content": "2025-04 2025-05 2024-11 2025-02 2025-04 2024-07 85.0 79.8 77.0 78.0 77.6 71.4 80.6 76.0 71.2 72.2 73.2 67.6 Open-source Models Mistral-Small-3.1-24B 2025-03 2025-01 Qwen2.5-VL-72B 2025-04 InternVL3-38B 2024-11 Qwen2-VL-72B 2024-11 InternVL2.5-38B 2024-09 Pixtral-12b 2025-04 InternVL3-8B 2025-01 Qwen2.5-VL-7B 2024-11 InternVL2.5-8B 2024-06 InternVL2-8B 2024-11 Qwen2-VL-7B 2024-09 Llama-3.2-11B-Vision 2025-03 Phi-4-Multimodal 2024-09 LLaVA-OneVision 2024-08 Phi-3.5-Vision 74.8 70.8 65.8 70.4 65.0 60.8 64.2 55.8 53.8 54.0 52.6 53.6 50.8 49.8 46. 66.0 69.2 64.6 61.0 55.8 54.6 54.6 57.4 56.4 52.6 54.0 50.6 50.8 48.2 52.0 90.0 50.0 77.6 73.2 73.6 69.4 71.2 61.4 68.6 68.2 65.2 63.0 62.4 63.4 56.0 57.0 53.2 50.2 52.0 51.8 51.2 49.6 48.0 90.0 50.0 67.6 71.4 73.8 73.4 70.8 62. 75.6 69.2 70.4 67.2 66.8 65.2 63.0 60.2 58.2 54.6 52.0 53.2 51.0 53.6 49.2 50.0 79.6 76.0 72.3 73.0 74.3 63.8 73.6 70.2 70.6 65.9 63.8 61.1 58.8 53.5 55.5 52.9 52.8 48.9 52.1 51.0 51.5 93.8 50.0 77.7 75.1 73.9 73.3 73.2 65. 71.3 69.4 66.5 65.4 62.5 61.0 59.5 57.6 55.4 52.9 52.7 52.3 51.0 50.3 48.8 Table 3: Model accuracy on SCIVER validation and test sets with CoT prompts, ranked by test set performance. and 3 (Chen et al., 2023, 2024b,a), Qwen2-VL and Qwen2.5-VL (Bai et al., 2023; Wang et al., 2024a), Pixtral (Agrawal et al., 2024), Mistral-Small3.1 (Mistral AI, 2025), LLaVA-OneVision (Li et al., 2024a), Llama-3.2-Vision (Meta, 2024), Phi-3.5Vision and Phi-4-Multimodal (Microsoft, 2024; Microsoft et al., 2025). We also evaluate five series of proprietary models, including OpenAI o4-mini (OpenAI, 2025a), GPT-4o and GPT-4.1 (OpenAI, 2024, 2025b), Gemini-2.0 and Gemini2.5 (Google, 2024, 2025). Appendix details the parameter settings and configurations of the evaluated models. For open-source models, we utilize the vLLM pipeline (Kwon et al., 2023) for model inference; while for proprietary models, we use their official API service. We evaluate the models with the Chain-ofThought prompt, which is presented in Figure 3."
        },
        {
            "title": "4.2 Main Findings",
            "content": "Table 3 presents the evaluated models performance. Our main findings are as follows: Figure 4: Comparison of model performance on the validation set, with claims requiring varying amounts of annotated supporting evidence. Each piece of evidence is defined as single table, chart, or paragraph (3.4). SCIVER presents substantial challenges for current models. While the recently released reasoning models, o4-mini and Gemini-2.5-Flash, demonstrate leading performance, other models fall short of human expert capabilities. For instance, GPT-4.1 achieves 73.2% accuracy with CoT prompting, considerably lower than the 93.8% accuracy achieved Figure 5: Illustration of two error types: Visual Element Misinterpretation (left) and Failure in Multi-step Reasoning (right). Additional error examples are provided in Appendix C. by human experts. This performance gap highlights SCIVER crucial role in advancing and assessing the capabilities of models in multimodal scientific literature comprehension. Performance of open-sourced models. Opensource models continue to lag behind their proprietary counterparts. However, models such as Mistral-Small-3.1, Qwen2.5-VL, and InternVL3 have achieved competitive performance, narrowing the gap with top proprietary models. These advancements highlight the rapid progress in opensource development. In the following subsections, we provide detailed analysis of open-source models and offer insights for future improvements. Model performance declines with increasing evidence requirements. To provide fine-grained analysis of model performance on multi-hop reasoning in SCIVER, we compare frontier models on the validation set across claims that require different numbers of annotated supporting evidence. As shown in Figure 4, model performance consistently declines as the number of ground-truth evidence pieces increases. This trend suggests that current models struggle with multi-hop reasoning and with synthesizing information across multiple multimodal contexts."
        },
        {
            "title": "4.3 Error Analysis and Case Study",
            "content": "To better understand the limitations of open-source models, we perform detailed error analysis on Qwen2.5-VL-72B. We randomly select 25 instances from each of the four subsets for evaluation. Through detailed inspection of model response, we identify five common error types: Failure to Retrieve Relevant Information (32%), where models fail to retrieve and consider all the key evidence from the provided multimodal context, leading to incomplete reasoning or incorrectly classify verifiable claims as lacking enough information. Visual element misinterpretation (21%), where models misinterpret charts or tables. Failure in multi-step reasoning (17%), where models struggle to connect intermediate reasoning steps over extracted information, leading to incorrect entailment predictions. Prompt for Evidence Filtering {Single Multimodal Element} Analyze the given context and determine whether it contains relevant information to verify the following claim: {Claim} Respond with either yes if the context contains the necessary information to verify the claim, or no if it does not."
        },
        {
            "title": "Setting",
            "content": "Original with RAG Recall@5 4o-mini Qwen2.5-VL 63.8 70. Contriever BM25 OAI Embedding Oracle 70.7 74.3 81.0 LLM Evidence Filter 64.7 0.9 65.4 1.6 67.0 3.2 73.3 9.5 67.5 3.7 71.8 1.6 72.2 2.0 72.9 2.7 75.3 5.1 74.4 4. Figure 6: The prompt for evidence filtering in 4.4. Table 4: Performance comparison of GPT-4o-mini and Qwen2.5-VL-72B under different RAG settings. Heavy reliance on text modality (12%), where models focus primarily on textual input, failing to properly integrate crucial information from tables and charts. Domain-specific misconceptions (10%), where models misapply domain terminology or rely on irrelevant memorized knowledge when verifying the given claims. Other observed errors include incorrect numerical computations and instances where models refuse to generate response. For each error type, we provide illustrative examples and corresponding error analyses in Figure 5 and Appendix C."
        },
        {
            "title": "4.4 Retrieval-Augmented Generation Analysis",
            "content": "The preceding error analysis highlights that the failure to retrieve relevant information is primary error type. This finding motivates us to explore how RAG settings can be leveraged to improve model performance on SCIVER. Experiment Setup. Implementing RAG for scientific multimodal data presents challenges, as existing open-source retrieval models do not natively support scientific tables and charts. To overcome this limitation, we construct the textual representations for tables and charts as the concatenation of their original captions and GPT-4o-generated descriptions. Each representation is indexed as separate evidence alongside the textual paragraphs extracted from the paper. We evaluate three widely used retrieval systems, i.e., BM25, Contriever (Izacard et al., 2021), and OpenAIs text-embedding-3large, to retrieve the top-5 most relevant evidence for the given claim. The retrieved evidence is then fed into the model in its original form. Additionally, we assess an alternative setting (i.e., Evidence Filtering) where the model first determines, one by one, whether each piece of evidence is relevant to the claim (prompt shown in Figure 6), and then incorporates all confirmed relevant evidence into the final input. Findings. We evaluate the GPT-4o-mini and Qwen2.5-VL-72B models on the validation set. As shown in Table 4, enhancements in information retrieval quality generally lead to improved entailment classification performance on SCIVER. Among the three retrievers tested, the OpenAI embedding model achieves the highest retrieval accuracy, which correlates with the most substantial gains in downstream LLM performance (i.e., 70.2% 75.3% for Qwen2.5-VL-72B). Additionally, applying an LLM-based evidence filter further boosts overall system performance."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduces SCIVER, comprehensive benchmark for evaluating multimodal scientific claim verification. By providing diverse set of fine-grained, expert-curated examples and reliable automated evaluation system, SCIVER advances the development of foundation models capable of accurately and robustly interpreting realworld scientific texts, tables, and figures. Our experimental results expose significant performance gaps between state-of-the-art foundation models and human experts, revealing key challenges such as reasoning limitations across textual, tabular, and visual data, as well as difficulties in retrieving and integrating relevant multimodal evidence."
        },
        {
            "title": "Acknowledgement",
            "content": "We are grateful to Google TRC program for providing computing resources and Together AI for granting LLM API credits."
        },
        {
            "title": "Limitations",
            "content": "While SCIVER presents significant advancement in multimodal scientific claim verification, there are several limitations that we acknowledge, which also point to promising directions for future research. First, SCIVER is primarily constructed from computer science papers sourced from arXiv, focusing on verifying claims within this discipline. While this allows us to control for domain expertise in our annotation process and ensures high-quality claim verification, it may limit the generalizability of SCIVER to other fields. Second, SCIVER primarily focuses on claim verification over textual paragraphs, tables, and charts, as these are the most common multimodal elements in scientific literature. However, some domains rely heavily on other modalities such as equations, figures, or experimental images, which SCIVER does not explicitly consider in its current version. Third, SCIVER relies on expert annotations with domain expertise, ensuring high-quality labels and reasoning rationales. However, this approach is labor-intensive and may not scale easily to larger datasets."
        },
        {
            "title": "References",
            "content": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, and Sophia Yang. 2024. Pixtral 12b. Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sahar Tahmasebi, Oana Cocarascu, and Elena Simperl. 2024. ChartCheck: Explainable fact-checking over real-world chart images. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1392113937, Bangkok, Thailand. Association for Computational Linguistics. Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and VERification over unstructured and structured information In Proceedings of the (FEVEROUS) shared task. Fourth Workshop on Fact Extraction and VERification (FEVER), pages 113, Dominican Republic. Association for Computational Linguistics. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen tau Yih, Pang Wei Koh, and Hannaneh Hajishirzi. 2024. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile vision-language model for understanding, localizaarXiv preprint tion, arXiv:2308.12966. text reading, and beyond. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020. Tabfact: large-scale dataset for table-based fact verification. In International Conference on Learning Representations. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. 2024a. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2023. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. dataset of information-seeking questions and answers anIn Proceedings of the chored in research papers. 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 45994610, Online. Association for Computational Linguistics. Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea. 2024. Tables as texts or images: Evaluating the table reasoning ability of LLMs and MLLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 407426, Bangkok, Thailand. Association for Computational Linguistics. Max Glockner, Ieva Staliunaite, James Thorne, Gisela Vallejo, Andreas Vlachos, and Iryna Gurevych. 2024. AmbiFC: Fact-checking ambiguous claims with evidence. Transactions of the Association for Computational Linguistics, 12:118. Google. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Google. 2025. Gemini 2.5 flash. Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020. INFOTABS: Inference on tables as semi-structured data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 23092324, Online. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and Greg Durrett. 2023. WiCE: Real-world entailment for claims in Wikipedia. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75617583, Singapore. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae Lee. 2023. QASA: Advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1903619052. PMLR. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llavaonevision: Easy visual task transfer. Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, and Arman Cohan. 2024b. M3SciQA: multi-modal multi-document scientific QA benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1541915446, Miami, Florida, USA. Association for Computational Linguistics. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024c. Multimodal ArXiv: dataset for improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, Bangkok, Thailand. Association for Computational Linguistics. Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, Linda Ruth Petzold, Stephen D. Wilson, Woosang Lim, and William Yang Wang. 2024d. Mmsci: dataset for graduate-level multi-discipline multimodal scientific understanding. Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. 2023. SCITAB: challenging benchmark for compositional reasoning and claim verification on scientific tables. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 77877813, Singapore. Association for Computational Linguistics. Meta. 2024. The llama 3 herd of models. Microsoft, :, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Arindam Mitra, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, and Xiren Zhou. 2025. Phi-4mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. Microsoft. 2024. Phi-3 technical report: highly capable language model locally on your phone. Mistral AI. 2025. Mistral-small-3.1-24bhttps://huggingface. instruct-2503. co/mistralai/Mistral-Small-3. 1-24B-Instruct-2503. cense. Apache 2.0 LiOpenAI. 2024. Hello gpt-4o. OpenAI. 2025a. Addendum to openai o3 and o4-mini system card: Openai o3 operator. OpenAI. 2025b. Introducing gpt-4.1 in the api. Aman Rangapur, Haoran Wang, Ling Jian, and Kai Shu. 2024. Fin-fact: benchmark dataset for multimodal financial fact checking and explanation generation. Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, and Andrew D. White. 2024. Language agents Yilun Zhao, Yitao Long, Tintin Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Xiangru Tang, Yiming Zhang, Chen Zhao, and Arman Cohan. 2024. FinDVer: Explainable claim verification over long and hybrid-content financial documents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1473914752, Miami, Florida, USA. Association for Computational Linguistics. Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. 2024. In Proceedings Multimodal table understanding. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 91029124, Bangkok, Thailand. Association for Computational Linguistics. achieve superhuman synthesis of scientific knowledge."
        },
        {
            "title": "James",
            "content": "Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for fact extraction In Proceedings of the 2018 and VERification. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809819, New Orleans, Louisiana. Association for Computational Linguistics. Andreas Vlachos and Sebastian Riedel. 2014. Fact checking: Task definition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 1822, Baltimore, MD, USA. Association for Computational Linguistics. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 75347550, Online. Association for Computational Linguistics. David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, and Hannaneh Hajishirzi. 2022. SciFact-open: Towards open-domain scientific In Findings of the Association claim verification. for Computational Linguistics: EMNLP 2022, pages 47194734, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. 2024b. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. CoRR, abs/2406.18521. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. 2024. Mmmu-pro: more robust multidiscipline multimodal understanding benchmark. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. 2025. MMMU-pro: more robust multi-discipline multimodal understanding benchmark."
        },
        {
            "title": "Assigned Subjects",
            "content": "# Relevant Publications Author? 2nd year PhD Computer Vision and Pattern Recognition 1 Final year PhD Computer Vision and Pattern Recognition 2 Computer Vision and Pattern Recognition Postdoc 3 Computation and Language 4 Computation and Language 5 Computation and Language 6 Robotics 3rd year PhD 7 Postdoc Robotics 8 Final year PhD Software Engineering 9 Software Engineering Postdoc 10 Machine Learning 2nd year PhD 11 Machine Learning 4th year PhD 12 Artificial Intelligence 3rd year PhD 13 Artificial Intelligence 14 Postdoc Information Retrieval 15 Master Student Information Retrieval 16 17 18 3rd year PhD Final year PhD Cryptography Cryptography Postdoc 1-5 5-10 >10 >10 1-5 1-5 5-10 >10 5-10 >10 1-5 5-10 5-10 >10 1-5 5-10 5-10 >10 Table 5: Biographies of 18 expert annotators involved in SCIVER construction (Author biographies are hidden to protect identity confidentiality."
        },
        {
            "title": "B Configurations of Evaluated Models",
            "content": "Organization Model Release Version # Inference Pipeline OpenAI Google Alibaba Mistral AI Shanghai AI Lab o4-mini GPT-4.1 GPT-4o GPT-4o-mini Gemini-2.5-Flash Gemini 2.0 Flash Qwen2.5-VL-72B Qwen2-VL-72B Qwen2.5-VL-7B Qwen2-VL-7B Mistral-Small-3.1 Pixtral-12B InternVL3-38B InternVL3-8B InternVL2.5-38B InternVL2.5-8B InternVL2-8B Proprietary Models o4-mini-2025-04-16 gpt-4.1-2025-04-14 gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 gemini-2.5-flash-preview-05-20 gemini-2.0-flash-exp 2025-04 2025-04 2024-08 2024-07 2025-05 2024-12 Open-source Multimodal Foundation Models 2025-01 2024-09 2025-01 2024-08 2025-03 2024-09 2025-04 2025-04 2024-11 2024-11 2024Qwen2.5-VL-72B-Instruct Qwen2-VL-72B-Instruct Qwen2.5-VL-7B-Instruct Qwen2-VL-7B-Instruct Mistral-Small-3.1-24B Pixtral-12B-2409 InternVL-3-38B InternVL3-8B InternVL2.5-38B InternVL2.5-8B InternVL2-8B Meta Microsoft Llama-3.2-11B-Vision 2024-09 Llama-3.2-11B-Vision-Instruct Phi-3.5-Vision Phi-4-Multimodal 2024-07 2025-03 Phi-3.5-Vision-Instruct Phi-4-Multimodal Llava Hugging Face LLaVA-OneVision-7B 2024-09 llava-onevision-qwen2-7b-ov-chat-hf API API vLLM vLLM vLLM vLLM vLLM vLLM Table 6: Details of the multimodal foundation models evaluated in our study. Models are organized by organization and aligned with performance data from the main text."
        },
        {
            "title": "C Error Analysis",
            "content": "C.1 Failure to Retrieve Relevant Information Figure 7: Illustration of Failure to Retrieve Relevant Information with the example from the Analytical Reasoning subset. C.2 Visual element misinterpretation Figure 8: Illustration of Visual element misinterpretation with the example from the Direct Reasoning subset. C.3 Heavy Reliance on Text Modality Figure 9: Illustration of Heavy Reliance on Text Modality with the example from the Analytical Reasoning subset. C.4 Domain-Specific Misconceptions Figure 10: Illustration of Domain-Specific Misconceptions with the example from the Analytical Reasoning subset. C.5 Other Observation Error Figure 11: Illustration of Other Observation Error with the example from the Parallel Reasoning subset."
        }
    ],
    "affiliations": []
}