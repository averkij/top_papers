{
    "paper_title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models",
    "authors": [
        "Yuqi Liu",
        "Liangyu Chen",
        "Jiazhen Liu",
        "Mingkang Zhu",
        "Zhisheng Zhong",
        "Bei Yu",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs) include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). SFT leverages external guidance to inject new knowledge, whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities and overall performance. However, our analysis reveals that SFT often leads to sub-optimal performance, while RLVR struggles with tasks that exceed the model's internal knowledge base. To address these limitations, we propose ViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement \\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the strengths of both SFT and RLVR within a single stage. We analyze the derivation of the SFT and RLVR objectives to establish the ViSurf objective, providing a unified perspective on these two paradigms. The core of ViSurf involves injecting ground-truth labels into the RLVR rollouts, thereby providing simultaneous external supervision and internal reinforcement. Furthermore, we introduce three novel reward control strategies to stabilize and optimize the training process. Extensive experiments across several diverse benchmarks demonstrate the effectiveness of ViSurf, outperforming both individual SFT, RLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates these findings, validating the derivation and design principles of ViSurf."
        },
        {
            "title": "Start",
            "content": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models Yuqi Liu1 Liangyu Chen3 Jiazhen Liu2 Mingkang Zhu1 Zhisheng Zhong1 Bei Yu1 Jiaya Jia2 CUHK1 HKUST2 RUC3 https://github.com/dvlab-research/ViSurf 5 2 0 2 2 1 ] . [ 1 6 0 6 0 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Typical post-training paradigms for Large Vision-andLanguage Models (LVLMs) include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). SFT leverages external guidance to inject new knowledge, whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities and overall performance. However, our analysis reveals that SFT often leads to suboptimal performance, while RLVR struggles with tasks that exceed the models internal knowledge base. To address these limitations, we propose ViSurf (Visual Supervisedand-Reinforcement Fine-Tuning), unified post-training paradigm that integrates the strengths of both SFT and RLVR within single stage. We analyze the derivation of the SFT and RLVR objectives to establish the ViSurf objective, providing unified perspective on these two paradigms. The core of ViSurf involves injecting ground-truth labels into the RLVR rollouts, thereby providing simultaneous external supervision and internal reinforcement. Furthermore, we introduce three novel reward control strategies to stabilize and optimize the training process. Extensive experiments across several diverse benchmarks demonstrate the effectiveness of ViSurf, outperforming both individual SFT, RLVR, and two-stage SFT RLVR. In-depth analysis corroborates these findings, validating the derivation and design principles of ViSurf. 1. Introduction Developing Large Vision-and-Language Models (LVLMs) that excel in diverse visual perception tasks is promising direction for visual intelligence. To this end, prior work has predominantly relied on two training paradigms: Supervised Fine-Tuning (SFT) [1, 17, 38] and Reinforcement Learning with Verifiable Rewards (RLVR) [20, 21]. Both these two paradigms have its advantages and disadvantages. SFT directly optimizes the model using expertannotated data. This approach provides explicit external Figure 1. (a) Examples of vision-ang-language tasks. (b) For tasks within LVLMs knowledge base, RLVR performs better than SFT. (c) For tasks that exceed LVLMs knowledge, SFT performs better, whereas RLVR performs worse than baseline. guidance, enabling the model to memorize the target distribution. However, it often exhibits limited generalization capabilities and can lead to catastrophic forgetting of pretrained knowledge. Recently, RLVR has garnered significant research attention. Methods such as Group Relative Policy Optimization (GRPO) [34] or Dynamic Sampling Policy Optimization (DAPO) [42] are on-policy algorithms wherein an initial policy generates rollouts that are evaluated by pre-defined reward functions, and the policy is subsequently optimized based on this internal feedback. By leveraging internal reinforcement signals, RLVR mitigates catastrophic forgetting and often achieves superior generalization. Nevertheless, its performance can degrade when tasks extend beyond the initial models knowledge base. This limitation is particularly pronounced in LVLMs compared to Large Language Models (LLMs), as the visual and textual domains exhibit significant variance in both input modalities and expected outputs. We evaluate these two paradigms across diverse set of vision-and-language tasks, with our key findings summarized in Figure 1. The results indicate that SFT is more effective for tasks that fall out1 Surf. Our contributions are summarized as follows: Based on our theoretical analysis, we propose ViSurf, unified post-training paradigm that leverages the complementary benefits of SFT and RLVR. We design three reward control strategies to stabilize the training process, the necessity of which is validated through ablation studies. Empirically, ViSurf outperforms existing methods (SFT, RLVR, and SFT RLVR). Additional analyses offer thorough understanding of its operational mechanics. 2. Related Works 2.1. Supervised Fine-tuning for LVLMs Supervised Fine-Tuning (SFT) is predominant paradigm for training Large Vision-Language Models (LVLMs). This approach involves fine-tuning pre-trained models on expertannotated data. The pioneering work in this area is LLaVA [17], which has inspired numerous subsequent models. Notable examples include the LLaVA-series [13, 18], QwenVL-series [1, 38], MGM-series [14, 36, 43], InternVL [3] and LLaMA-3.2 [27], all of which adopt this paradigm. SFT has proven particularly effective for adapting LVLMs to diverse downstream applications, such as image quality assessment [41], visual counting [5], and autonomous driving [40]. 2.2. Reinforcement Learning for LVLMs Reinforcement Learning (RL) is standard method for finetuning Large Vision-Language Models (LVLMs). Among RL algorithms, Direct Preference Optimization (DPO) [30] relies on pre-collected human preference datasets, which can be costly to produce. Similarly, Proximal Policy Optimization (PPO) [32] requires well-trained reward model to evaluate responses generated by the policy. Recently, RLVR algorithms, such as GRPO [34] and DAPO [42], have gained attention for their ability to assess model outputs against objective criteria. This approach reduces the dependency on manually annotated data and pre-trained reward models. The effectiveness of RLVR for LVLMs has been demonstrated in recent works [9, 1922] such as SegZero [20] and VisualRFT [22]. 3. ViSurf We begin by analyzing the objective functions of SFT and RLVR in Section 3.1. case study in Section 3.2 then highlights the limitations of both methods. To address these limitations, we introduce ViSurf, detailing its design and relationship to SFT and RLVR in Section 3.3. Subsequently, Section 3.4 presents three novel mechanisms for reward control during training, and Section 3.5 provides an analysis of the optimization process. Figure 2. Radar Chart: ViSurf achieves superior performance across different training paradigms. Bar Chart: SFT and two-stage SFT RLVR exhibit catastrophic forgetting. side the pre-training distribution of LVLMs, whereas RLVR yields superior performance on tasks that align with its preexisting knowledge base. We also provide case study in Section 3.2 to illustrate this phnomenon. Although sequential SFT RLVR pipeline leverages their complementary strengths, it incurs the combined computational cost of both stages and remains susceptible to catastrophic forgetting during the initial SFT phase. To address the aforementioned limitations, we propose ViSurf (Visual Supervised-and-Reinforcement FineTuning), unified, single-stage post-training paradigm designed to integrate the complementary advantages of SFT and RLVR. We begin with an analysis of the underlying objectives and gradients of SFT and RLVR. We theoretically demonstrate that their formulations share similar patterns, enabling them to be integrated into single, unified objectivethe ViSurf objective. While subtle differences exist, the gradient of ViSurf objective can be interpreted as composite of the gradients from both SFT and RLVR. Based on the theoretical analysis, we design three reward control strategies to stabilize training. Specifically, for the groundtruth labels, we i) aligning them with rollouts preference, ii) eliminating thinking reward for them, and iii) smoothing the reward for them. The implementation of ViSurf is consequently simplified to interleaving ground-truth demonstrations with on-policy rollouts in unified training phase. We conduct extensive experiments across various domains, with comparative summary presented in Figure 2. The results indicate that our method, ViSurf, achieves superior performance than SFT, RLVR and SFT RLVR. Models trained with ViSurf also exhibit reasoning abilities similar to those established in previous works [20, 21]. Furthermore, ViSurf successfully mitigates catastrophic forgetting, as evidenced by its stable performance on VQA tasks. The ablation study confirms the critical contribution of the proposed reward control mechanism. Additional in-depth analysis provides empirical validation for our theoretical analysis and offers insights into the operational principles of Vi2 3.1. Preliminary Let πθ denote large vision-and-language model (LVLM), parameterized by θ. Common post-training paradigms for optimizing πθ include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). Both SFT and RLVR utilize the same input dataset, Dinput = {(vi, ti)}N i=1, where vi is visual input, ti is textual input, and is the dataset size. Supervised Fine-Tuning (SFT) optimizes πθ against set of pre-annotated ground-truth labels, Dlabel = {yi}N i=1. The objective is to minimize the negative log-likelihood of the labels: LSFT(θ) = E(v,t)Dinput yDlabel [log πθ(y v, t)] , (1) where corresponds to (v, t). more precise notation would be (v, t, y) zip(Dinput, Dlabel). Nevertheless, we retain the current notation, (v, t) Dinput, Dlabel, for the sake of clarity and ease of comparison in the subsequent discussion. Reinforcement Learning with Verifiable Rewards (RLVR). We illustrate RLVR using the on-policy Group Relative Policy Optimization (GRPO) algorithm [34]. GRPO optimizes the policy πθ using verifiable reward function, which typically combines measures of output format and accuracy [7, 20, 21]. For given input (vi, ti) Dinput, the old policy πθold (from previous optimization step) generates group of rollouts {oj}G j=1 by sampling with different random seeds. Each rollout oj is then evaluated by reward function r(), resulting in set of rewards {r(oj)}G j=1. The advantage for each rollout is subsequently computed as follows: ˆAj = r(oj ) mean (cid:16) {r(oj )}G j= (cid:16) {r(oj )}G j=1 (cid:17) std (cid:17) , (2) The objective of RLVR is to minimize the following equation: LRLVR(θ) = (v,t)Dinput j=1πθold {oj }G 1 (cid:88) j=1 (cid:40) min πθ(oj v, t) πθold (oj v, t) ˆAj , (cid:32) clip πθ(oj v, t) πθold (oj v, t) (cid:33) (cid:41)(cid:35) , 1 ϵ, 1 + ϵ ˆAj . (3) For simplicity, both in equation and in our practical implementation, we omit the KL divergence term. 3.2. Case Study: Non-Object Scenarios We present case study on non-object referring expression segmentation. In this task, instructions comprise both correct expressions (referring to existing objects) and incorrect ones (where the referred objects are absent). For this study, 3 Figure 3. (a) Illustration on Non-Object Segmentation and VisionReasoner. (b) Performance comparison of SFT and RLVR on gRefCOCO. While RLVR achieves higher overall GIoU, it fails on non-object instructions. (c) Specifically, the RLVR model consistently outputs mask even when no relevant object is detected. we utilize the VisionReasoner model [21], which is initialized with Qwen2.5VL [1] and SAM2 [31] and has demonstrated strong performance on standard referring segmentation tasks. Figure 3(a) illustrates the non-object segmentation and the architecture of VisionReasoner. Detailed experimental settings are demonstrated Section 4.1. The gRefCOCO [16] serves as the benchmark for this scenario. We compare gIoU (average IoU across images) and N-Acc (accuracy in identifying non-existent object). Quantitative and qualitative results are presented in Figure 3(b)-(c). Our analysis reveals that SFT yields suboptimal gIoU and fails to produce coherent reasoning process, while models trained with RLVR tend to generate mask even when no relevant object is detected. The latter issue arises because the model, relying solely on self-rollouts, cannot correct itself to produce no object output. In essence, RLVR enhances overall performance through internal self-guidance, whereas SFT provides crucial external guidance when self-rollouts fail. This analysis leads to key question: how can we integrate the benefits of both training paradigms in single training stage efficiently? 3.3. Combining SFT and RLVR To harness the complementary benefits of SFT and RLVR, we propose ViSurf: unified, single-stage post-training algorithm. This section elaborates on the derivation of the ViSurf algorithm. Gradient Analysis of SFT and RLVR. The gradient of SFT can be derived from Equation (1) as: θLSFT(θ) = E(v,t)Dinput [θ log πθ(y v, t)] . (4) yDlabel The gradient of RLVR can be derived from Equation (3) using approximation πθ πθold and log-derivative trick. We also omit clip operation for simplicity: θLRLVR(θ) = (v,t)Dinput j=1πθold {oj }G 1 G (cid:88) j=1 ˆAj θ log πθ(oj v, t) (5) . θθold We observe that the gradients of the SFT and RLVR losses, θLSFT(θ) and θLRLVR(θ), share similar form. The difference between them is the guidance signal (y vs. {oj}G j=1) and coefficient (1 vs. ˆAj). Objective of ViSurf. To combine SFT and RLVR into single stage, we design an objective function that naturally yields gradient combining both θLSFT(θ) and θLRLVR(θ). Our key insight is to include the ground-truth label as high-reward sample within the RLVR framework. We construct an augmented rollout set {oj}G j=1. Then the corresponding rewards are r(y){r(oj)}G j=1. This formulation modifies the advantage calculation of rollouts in Equation (2) as follows: ˆAj = (cid:16) r(oj ) mean (cid:16) r(y) {r(oj )}G j=1 (cid:17) std {r(y) {r(oj )}G j=1} (cid:17) , and the advantage of ground-truth is calculated as: ˆAy = r(y) mean (cid:16) r(y) {r(oj )}G j=1 (cid:17) j=1} {r(y) {r(oj )}G (cid:16) std (cid:17) . (6) (7) The objective of ViSurf is to minimize the following equation: (cid:34) 1 + 1 (cid:32) (cid:88) j= (cid:40) min πθ(oj v, t) πθold (oj v, t) ˆAj , (cid:32) clip πθ(oj v, t) πθold (oj v, t) , 1 ϵ, 1 + ϵ (cid:33) (cid:41) ˆAj (8) (cid:40) + min πθ(y v, t) πθold (y v, t) ˆAy, (cid:32) clip πθ(y v, t) πθold (y v, t) , 1 ϵ, 1 + ϵ (cid:33) (cid:41)(cid:33)(cid:35) ˆAy . With the objective function demonstrated above, the pseudocode of ViSurf Optimization Step is shown in Algorithm 1. 4 Algorithm 1: ViSurf Optimization Step Input: policy model πθ; reward function r(); input data Dinput; label data Dlabel for step = 1, . . . , do j=1 πθold () for each Sample mini-batch Binput and corresponding Blabel; Update the old policy model πθold πθ; Sample outputs {oj }G (v, t) Binput; Compute rewards {r(oj )}G Compute rewards r(y) for label Blabel; Compute ˆAj and ˆAy through relative advantage estimation; Update the policy model πθ using Equation (8); j=1 for each sampled output oi; Output: πθ Gradient Analysis of ViSurf. The gradient of Equation (8) can be derived using approximation πθ πθold and log-derivative trick. We also omit clip operation for simplicity: θLViSurf (θ) = (v,t)Dinput j=1πθold yDlabel {oj }G (cid:34) 1 + 1 (cid:32) (cid:88) j=1 ˆAj θ log πθ(oj v, t) (9) + ˆAy θ log πθ(y v, t) (cid:33)(cid:35) . θθold To better illustrate the structure of the gradient, we reformulate Equation (9) as following: θLViSurf (θ) = (v,t)Dinput j=1πθold {oj }G (cid:124) (cid:34) 1 + 1 (cid:88) j=1 ˆAj θ log πθ(oj v, t) (cid:123)(cid:122) RLVR Term ˆAy θ log πθ(y v, t) (cid:35) (cid:34) 1 + 1 (cid:123)(cid:122) SFT Term θθold (cid:125) θθold (cid:125) (cid:35) . G+1 ˆAj vs. (10) Relation to SFT and RLVR. We analyze the gradients of ViSurf, SFT, and RLVR to explain their connections. The RLVR term in Equation (10) is structurally identical to the standard RLVR gradient in Equation (5), differing only in ˆAj). Similarly, the its scaling coefficient ( 1 SFT term in Equation (10) resembles the SFT gradient from Equation (4), with two key distinctions: (i) the coefficient ˆAy instead of 1, and (ii) the use of the is weighted by approximation πθ πθold . This approximation implies that the ground-truth label should align with the models internal generative preference to be effective. Crucially, Equation (10) integrates both the external guidance from SFT and the internal guidance from RLVR into unified gradient. 1 G+1 1 LViSurf (θ) = (v,t)Dinput j=1πθold yDlabel {oj }G E(v,t)Dinput yDlabel (cid:124) Figure 4. ViSurf Framework. Upper: The integration of external guidance with internal guidance oi, which is critical when self-rollouts are unsuccessful. Bottom: Three reward control strategies designed to regulate y, thereby preventing entropy collapse. 3.4. Reward Control for Ground-Truth Label The advantage ˆAy for the ground-truth label is always positive till now, as the label is correct and receives higher reward, which can lead to reward hacking. The setup is sub-optimal for two reasons: (i) the ground-truth lacks reasoning trace, and (ii) it suppresses the relative advantage ˆAj even if rollouts have already generated correct trace and answer. Furthermore, as analysis above, we must ensure the ground-truth aligns with the self-rollouts to satisfy the approximation πθ πθold. To address these issues, we propose three reward control strategies. Aligning Ground-truth Labels with Rollouts Preference. To ensure compatibility between the groundtruth data and the self-rollouts generated by πθ, we reformat the ground-truth annotations to match the models preferred output style. For instance, we adjust the whitespace in JSON-like structures from {bbox:[x1,y1,x2,y2]} to {bbox: [x1, y1, x2, y2]}(i.e., adding space after the punctuation), as these variants yield different tokenizations. This alignment minimizes the distribution shift between πθ and πθold , satisfying the underlying assumption πθ πθold. Eliminating Thinking Reward for Ground-truth Labels. Since the ground-truth labels lack annotated reasoning path, we assign reasoning format score of zero to them. This operation ensures the model learns reasoning trace directly from its self-rollouts without being biased by missing external annotations. among generated Smoothing the Reward for Ground-truth Labels. to advantage estimation, we compare Prior rollouts, the maximum reward max{{r(oj)}G j=1}, against the ground-truth reward r(y). If max{{r(oj)}G j=1} r(y), it indicates the policy model πθ has already produced high-quality output without external guidance. In this case, we set r(y) = mean{{r(oj)}G j=1}. This smoothing ensures that the advantage for the groundtruth, ˆAy, becomes zero (as per Equation (7)), effectively eliminating the external supervision signal when it is unnecessary. 3.5. Optimization Analysis During Training Building on the reward control strategy in Section 3.4, we analyze the dynamics of the terms in Equation (10) throughout training. As defined by Equations (6) and (7), the advantages ˆAj (for rollouts) and ˆAy (for the ground-truth) govern the balance between the RLVR and SFT terms. This balance is self-adaptive. When the policy fails to generate high-quality rollouts, ˆAj decreases (potentially becoming negative), while ˆAy remains high. Consequently, the SFT term dominates the policy update, providing strong external guidance from the ground-truth label. Conversely, when the policy successfully generates desirable rollouts, our reward control mechanism sets ˆAy 0, causing the optimization to be dominated entirely by the RLVR term. This automatic shifting between learning modes is core feature of the single-stage ViSurf paradigm. Upper Bound Analysis. As mentioned above, our ViSurf is particularly beneficial when old policy model πθold cannot generate correct rollouts. When the old policy model πθold already achieves desirable rollouts, the SFT Term in Equation (10) near to zero, thus the upper bound of ViSurf is the RLVR. However, when the policy model cannot generate desirable rollouts, the upper bound is better than using either SFT or RLVR alone. 4. Experiments Section 4.1 details the experimental settings. We validate ViSurf across diverse domains in Section 4.2, followed by an ablation of the reward control design in Section 4.3. Finally, Section 4.4 provides in-depth analysis of ViSurf. 4.1. Experimental Settings We verify our ViSurf on the following benchmarks across several domains. Non-Object Segmentation. The gRefCOCO [16] includes queries that do not contain corresponding objects. The evaluation metrics are gIoU and N-Acc. We use Multi5 Table 1. Comparison on different benchmarks in different domains under different training paradigms. Method Non-Object gRefCOCO val gIoU N-Acc Baseline SFT RLVR SFT RLVR ViSurf 33.4 41.6 42.8 65.0 66.6 1.8 3.3 0.0 52.1 57.1 Segmentation ReasonSeg val gIoU 56.9 63.8 66.0 57.2 66.5 test gIoU 52.1 60.3 63.2 55.2 65.0 GUI OmniACT test Anomaly RealIAD subset Medical:Skin ISIC2018 test Math MathVista test-mini Acc 60.4 55.4 65.5 64.5 65.6 ROC AUC Bbox Acc 50.1 65.5 50.0 66.9 69.3 78.8 91.7 90.3 93.6 94.7 Acc 68.2 68.3 71.2 68.5 71.6 Avg 50.2 56.2 56.1 65.4 69. objects-7K plus with 200 non-object data for training. Reasoning Segmentation. The ReasonSeg [12] includes test samples that need reasoning for correct segmentation. It has 200 validation images and 779 test images. The evaluation metric is gIoU. We use Multi-objects-7K proposed in VisionReasoner [21] for training. GUI Grounding. The OmniACT [11] is GUI grounding task for Desktop and Web. We derive 6,101 samples in training split and verify on the test split. The accuracy is calculated as whether the predict point correctly locates in the interest region. Anomaly Detection. The RealIAD [35] includes realworld, multi-view industrial anomaly. We derive 3,292 training samples and 2,736 test samples, ensuring the two sets are disjoint. We calculated the ROC AUC. Medical Image: Skin. The task one of ISIC2018 [4, 10] is lesion segmentation. It includes 2,594 training samples and 1,000 test samples. We measure the bbox acc metric, which computes the ratio of predicted bounding boxes whose IoU with the ground truth exceeds 0.5. MathVista. The MathVista-testmini [24] includes 1,000 diverse mathematical and visual tasks. We gather around 10k training data from WeMath [29], MathVision [37], Polymath [8], SceMQA [15], Geometry3K [23]. Implemention Details. We choose Qwen2.5-VL [1] and SAM2 [31] (if applicable) as the baseline model. We employ constant learning rate of 1e-6 for all methods, with batch size of 32 for SFT and 16 for RLVR and ViSurf. We employ same training steps for fair comparison. For MathVista, the reward function consists of format and accuracy rewards. For other tasks, we adopt the rewards from VisionReasoner [21], which include format accuracy, point accuracy, and bounding box accuracy rewards, etc. 4.2. Comparison of Different Training Paradigms We compare different post-training paradigms and verify the effectiveness of ViSurf in various domains. Main Results. comparative analysis of post-training paradigms across various domains is presented in Table 1. Empirical results indicate that ViSurf consistently outperforms existing post-training paradigms across various doTable 2. Comparison on VQA under different training paradigms. Method ChartQA DocVQA val Baseline SFT RLVR SFT RLVR ViSurf 83.8 80.8 86.7 85.0 87.4 94.9 89.6 95.0 92.9 95.0 mains, with an average relative gain of 38.6% over the baseline model. This advantage is particularly significant in domains where the baseline model demonstrates lower competency, such as Non-Object and Anomaly, suggesting the methods efficacy in addressing domain exceeding models knowledge base. However, in domains where the baseline model is already highly proficient, the incremental gains are relatively marginal. We also observe that SFT leads to performance degradation in OmniACT, which may be attributable to potential test data contamination during pretraining. In comparison, both RLVR and ViSurf preserve the original model performance. Furthermore, in the case of RealIAD and non-object detection in gRefCOCO, the pure RLVR approach underperforms relative to the original model. We attribute this to the fact that self-generated rollouts frequently produce incorrect answers, thereby hindering model optimization. Catastrophic Forgetting. We evaluate the performance of ChartQA [25] and DocVQA [26]. As illustrated in Table 2, VQA performance exhibits notable variation across training paradigms. Both RLVR and ViSurf different demonstrate robustness against catastrophic forgetting. In contrast, SFT and SFT RLVR suffer from performance degradation, which is attributable to catastrophic forgetting. ViSurf on Other Models. We apply ViSurf to the Qwen2VL-7B [38]. As shown in Table 3, our method consistently outperforms its counterparts across different models. Furthermore, the pure RLVR approach yields the weakest performance on both datasets and even underperforms the baseline on RealIAD, which indicates that external supervision is critical. 6 Table 3. Employ ViSurf on Qwen2VL-7B. Method Baseline SFT RLVR SFT RLVR ViSurf RealIAD subset ROC AUC ISIC2018 test Bbox Acc 60.0 56.7 57.1 67.5 76.0 51.8 94.2 90.5 94.6 95. Table 4. Ablation of Reward Control Strategy in Section 3.4. Align: Aligning ground-truth labels with rollouts; Eliminate: Eliminating thinking format reward for groud-truth labels; Smooth: Smoothing accuracy reward for ground-truth labels; -: aligning is not applicable for math. Figure 5. Entropy Analysis of RLVR, SFT RLVR and ViSurf. ViSurf exhibits an initial drop, then converges slowly. Align Eliminate Smooth gRefCOCO ReasonSeg MathVista val testmini val gIoU N-Acc gIoU 59.0 72.9 61.0 66. 40.2 74.1 45.7 57.1 63.6 58.2 62.7 66.5 Acc 67.1 66.8 71.6 4.3. Ablation of Reward Control Table 4 presents an ablation study of the reward control strategy for ground-truth labels, detailed in Section 3.4. Aligning Ground-truth Labels with Rollouts Preference. The empirical results substantiate the critical importance of this strategy, as evidenced by consistent performance decline across multiple datasets upon its ablation. This observation provides strong empirical validation for the theoretical requirement of πθ πθold presented in Equation (10). Eliminating Thinking Reward for Ground-truth Labels. The results indicate that the reasoning strategy is critical for tasks requiring complex inference, such as those in ReasonSeg and MathVista, as it encourages the model to generate reasoning process prior to delivering the final answer. Conversely, for the gRefCOCO dataset, where queries are typically limited to simple class types (e.g., human) and basic references (e.g., woman on the right), omitting the reasoning step yields superior performance. This suggests that the necessity of explicit reasoning is contingent upon the complexity of the underlying task. Smoothing the Reward for Ground-truth Labels. The performance decline observed across datasets following the ablation of reward smoothing underscores its necessity. Concurrently, the results suggest that the SFT Term in Equation (10) becomes superfluous and can be removed when the models self-rollouts already achieve higherquality solution. Figure 6. Performance on gRefCOCO in different training steps. ViSurf demonstrates greater stability as training proceeds. 4.4. In-depth Analysis To facilitate deeper understanding of ViSurf, we provide detailed analysis of its behavior and properties. Entropy Analysis During Training. Figure 5 compares the entropy of RLVR, SFT RLVR and ViSurf. higher entropy indicates greater exploratory behavior, while lower entropy signifies convergence toward certainty. We observe that ViSurf exhibits an initial entropy drop, indicating the model is fitting the external guidance. Subsequently, ViSurf converges at slower rate than others, thereby effectively avoiding entropy collapse. Training Stability. Figure 6 demonstrates that models trained with our method exhibit greater stability than those trained with pure RLVR and SFT RLVR, as the performance of others decline with longer training. This observation confirms the effectiveness of our approach, indicating that the introduced external guidance acts as constraint, which stabilizes the training process. Boundary Analysis. As shown in Table 1, the performance gain from our method is contingent on the baseline models initial performance. When the baseline performs poorly (e.g., below 50%), indicating its inadequacy for the task, our method yields substantial improvement. Conversely, when the baseline already achieves high performance (e.g., above 50%), signifying strong starting point, the upper bound of our method aligns with that of RLVR alone. This observation corroborates our theoretical analy7 Table 5. Comparison of different prompt design. ViSurf achieves satisfying results even without detailed formatting prompt. Detailed Prompt RLVR ViSurf ReasonSeg val 0.0 66.0 62.3 66.4 test 0.0 63.2 57.8 65.0 Table 6. training Comparison of training cost of different paradigms with same batch size. Time for two-stage SFT RLVR is estimated as the addition of SFT and RLVR. Method Mem / GPU (G) Time / Step (s) SFT RLVR SFT RLVR ViSurf 97.7 81.8 97.9 81.8 9.0 22.7 31.7 22. Table 7. Comparison with SoTAs. - means not available. Method gRefCOCO val ReasonSeg test val gIoU N-Acc gIoU gIoU LISA-7B GSVA-7B SAM4MLLM-7B Qwen2.5VL-7B + SAM2 SegZero-7B VisionReasoner-7B ViSurf (Qwen2.5VL-7B + SAM2) 61.6 66.5 69.0 41.6 - 41.5 72.9 54.7 62.4 63.0 3.3 - 0.0 74.1 53.6 - 46.7 56.9 62.6 66.3 66.4 48.7 - - 52.1 57.5 63.6 65.0 sis in Section 3.5. Reduce the Burden of Prompt Design. The RLVR paradigm relies heavily on carefully engineered user prompts to guide the model toward generating rollouts in specific format, often requiring explicit instructions such as output with format point 2d: [2, 3]. In contrast, ViSurf incorporates external guidance with desired format, thereby reducing the dependency on manual prompt engineering. Table 5 compares performance with and without detailed prompts (appendix B), demonstrating that our approach achieves consistent gains in both settings, whereas RLVR fails without explicit formatting instructions. Training Cost. We conducted comparative analysis of the per-step training time for different fine-tuning paradigms. Each method was implemented using wellestablished frameworks: DeepSpeed [28] and TRL [6] for SFT, and VeRL [33] for RLVR and ViSurf. The results indicate that while RLVR and ViSurf offer greater memory efficiency, they incur higher computational cost per step, attributable to the overhead of generating rollouts. 4.5. Comparison with State-of-The-Arts We compare ViSurf against state-of-the-art (SoTA) models on two visual perception tasks: gRefCOCO and ReasonSeg. We compare LISA [12], GSVA [39], SAM4MLLM [2], SegZero [20], VisionReasoner [21]. As shown in Table 7, ViSurf achieves the state-of-the-arts performance on ReasonSeg and gRefCOCO. 5. Conclusion We propose ViSurf, single-stage post-training paradigm that integrates the benefits of both SFT and RLVR, motivated by theoretical analysis of their objectives and gradients. The practical implementation of ViSurf involves interleaving ground-truth labels with model-generated rollouts, augmented by three reward control strategies to ensure training stability. Experimental results across diverse benchmarks demonstrate that ViSurf outperforms SFT, RLVR, and sequential SFT RLVR pipeline. The subsequent in-depth analysis provides further insights and corroborates the theoretical analysis. 6. Discussion and Future Work The principal insight of ViSurf is the effective combination of RLVRs internal reinforcement and the external guidance of SFT. Although the ground-truth labels in this work are limited to final answers, our ViSurf paradigm is inherently compatible to incorporate explicit reasoning traces. The flexibility also ensures compatibility with advanced techniques like knowledge distillation, where reasoning traces from larger models could be directly incorporated. We anticipate that this work will provide foundation for future research in LVLMs posttraining."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 3, 6 [2] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. Sam4mllm: Enhance multimodal large language model for referring expression segIn European Conference on Computer Vision, mentation. pages 323340. Springer, 2024. 8 [3] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 2 [4] Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma 8 detection 2018: challenge hosted by the internaarXiv preprint tional skin imaging collaboration (isic). arXiv:1902.03368, 2019. 6 [5] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. [6] Hugging Face. TRL - Transformer Reinforcement Learning. https://github.com/huggingface/trl, 2024. 8 [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [8] Himanshu Gupta, Shreyas Verma, Ujjwala Anantheswaran, Kevin Scaria, Mihir Parmar, Swaroop Mishra, and Chitta Baral. Polymath: challenging multi-modal mathematical reasoning benchmark. arXiv preprint arXiv:2410.14702, 2024. 6 [9] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2 [10] International Skin Imaging Collaboration (ISIC). Isic 2018: Skin lesion analysis towards melanoma detection. https: //challenge.isic-archive.com/data/#2018, 2018. 6 [11] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161 178. Springer, 2024. 6 [12] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 6, [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [14] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 2 [15] Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. Scemqa: scientific college entrance level multimodal question answering benchmark. arXiv preprint arXiv:2402.05138, 2024. 6 [16] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. 3, 5 [17] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. 2 [19] Jiazhen Liu, Yuchuan Deng, and Long Chen. Empowering small vlms to think with dynamic memorization and exploration. arXiv preprint arXiv:2506.23061, 2025. 2 [20] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 1, 2, 3, 8 [21] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025. 1, 2, 3, 6, 8 [22] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 2 [23] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 6 [24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6 [25] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [26] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6 [27] AI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI Blog. Retrieved December, 20:2024, 2024. 2 [28] Microsoft and DeepSpeed Team. DeepSpeed. https:// github.com/deepspeedai/DeepSpeed, 2020. 8 [29] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 6 [30] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 2 9 ment learning system at scale, 2025. URL https://arxiv. org/abs/2503.14476, 2025. 1, [43] Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, et al. Lyra: An efficient and speech-centric framework for omni-cognition. arXiv preprint arXiv:2412.09501, 2024. 2 [31] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 3, 6 [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2 [33] ByteDance Seed. verl: Volcano Engine Reinforcement Learning for LLMs. https://github.com/ volcengine/verl, 2024. 8 [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1, 2, [35] Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, Jiangning Zhang, Zhihao Gu, Shuguang Qian, Mingang Chen, and Lizhuang Ma. Real-iad: real-world multi-view dataset for benchmarking versatile industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2288322892, 2024. 6 [36] Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, and Jiaya Jia. Mgm-omni: Scaling omni llms to personalized long-horizon speech. arXiv preprint arXiv:2509.25131, 2025. 2 [37] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 6 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 6 [39] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: Generalized segmentation In Proceedings of via multimodal large language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38583869, 2024. 8 [40] Zhenhua Xu, Yan Bai, Yujia Zhang, Zhuoling Li, Fei Xia, Kwan-Yee Wong, Jianqiang Wang, and Hengshuang Zhao. Drivegpt4-v2: Harnessing large language model capabilities for enhanced closed-loop autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1726117270, 2025. [41] Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1448314494, 2025. 2 [42] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforce10 ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Explanation on pure RLVR The performance of pure RLVR is heavily influenced by randomness. In approximately one out of ten runs, the old policy model πθold can generate correct non-object outputs in the initial steps, thereby achieving competitive performance (see Table 8), yet still marginally inferior to ViSurf. Table 8. Comparison under different training paradigms. Method gRefCOCO val Baseline SFT RLVR (90% runs) RLVR (10% runs) SFT RLVR ViSurf gIoU 33.4 41.6 42.8 62.9 58.6 66.6 N-Acc 1.8 3.3 0.0 49.3 38.1 57.1 B. Illustration of Prompt Design Table 9 shows prompt with detailed format instruction, where we provide desired output format for model. In contrast, Table 10 shows prompt without detailed format instruction, where we simply write answer here between answer tags. 1 Table 9. Prompt with detailed format instruction. We provide detailed format instructions between answer tags. Prompt with Detailed Instruction Please find {Question} with bboxs and points. Compare the difference between object(s) and find the most closely matched object(s). Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. Output the bbox(es) and point(s) inside the interested object(s) in JSON format. i.e. <think> thinking process here </think> <answer>[{\"bbox_2d\": [10, 100, 200, 210], \"point_2d\" [30, 110]}, {\"bbox_2d\": [225, 296, 706, 786], \"point_2d\": [302, 410]}]</answer> Table 10. Prompt without Detailed Instruction. We do not provide detailed format instructions between answer tags. Prompt without Detailed Instruction Please find {Question} with bboxs and points. Compare the difference between object(s) and find the most closely matched object(s). Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. Output the bbox(es) and point(s) inside the interested object(s) in JSON format. i.e. <think> thinking process here </think> <answer> answer here </answer>"
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "RUC"
    ]
}