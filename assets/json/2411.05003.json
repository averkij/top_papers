{
    "paper_title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
    "authors": [
        "David Junhao Zhang",
        "Roni Paiss",
        "Shiran Zada",
        "Nikhil Karnad",
        "David E. Jacobs",
        "Yael Pritch",
        "Inbar Mosseri",
        "Mike Zheng Shou",
        "Neal Wadhwa",
        "Nataniel Ruiz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique."
        },
        {
            "title": "Start",
            "content": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning David Junhao Zhang1,2 Roni Paiss1 Yael Pritch1 Inbar Mosseri1 1Google Shiran Zada1 Mike Zheng Shou2 2National University of Singapore Nikhil Karnad1 Neal Wadhwa1 David E. Jacobs1 Nataniel Ruiz1 4 2 0 2 7 ] . [ 1 3 0 0 5 0 . 1 1 4 2 : r generative-video-camera-controls.github.io Figure 1. Given user-provided source video, using ReCapture, we are able to generate new version of the video with new customized camera trajectory. Notice that the motion of the subject and scene in the video is preserved, and the scene is observed from angles that are not present in the source video."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to userprovided videos that are not generated by video model. In this paper, we present ReCapture, method for generating new videos with novel camera trajectories from single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating noisy anchor video with new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into clean and temporally consistent reangled video using our proposed masked video fine-tuning technique. Recently, diffusion models have enabled significant advances in video generation and editing [10, 27, 31, 32, 35, 55, 66, 76, 94, 99, 103, 105, 107, 109], revolutionizing workflows in digital content creation. Camera control plays vital role in practical applications of video generation and editing, allowing for greater customization and stronger user experience. Recent efforts have introduced camera control capabilities to video diffusion models [2, 29, 38, 89], yet, in this case the videos are entirely generated by the video model from text-prompt and are neither captured in the real world, nor provided by user. Effectively generating new videos with user-specified camera motion from an existing user-provided video that contains complex scene motion is still an open and challenging problem. The task is inherently ill-posed due to the limited amount of information in the reference video: one cannot know exactly how the scene looks like from all angles if there is not 1 full knowledge of the scenes 4D content. However, this does not preclude an approximate solution that is plausible and appreciated by users. Previous studies have shown promising results by generally assuming the availability of synchronized multi-viewpoint videos [64] and constructing 4D neural representations. Later works [45, 53, 80, 87, 92] enable 4D reconstruction using single monocular video, but require accurate camera pose and depth estimation, and cannot capture content outside the original field of view. In this paper, we reformulate this problem as video-to-video translation task. Camera Dolly [82] also develops videoto-video pipeline, but requires 4D video data with different camera poses obtained via simulation, which limits it to in-domain scenes like driving or cubic objects. Given the challenge of obtaining paired videos in the wild with varying camera movements, it is hard to solve this problem with video-to-video pipeline in an end-to-end manner and we separate it into two steps instead. Our approach leverages the prior knowledge of diffusion generative models, in both image and video domains, to effectively reangle the video as if filmed from the requested camera trajectory. For the first stage of our method, we want to generate an incomplete anchor video conditioned on the user-provided camera trajectory and reference video. We initially obtain partial frame-by-frame depth estimation of the target video. We project each frame into 3D space using depth estimator to obtain sequence of point clouds. Then, we simulate the user-specified camera movement, which can include zoom, pan, and tilt, and render the point cloud sequence according to the new camera trajectory. This estimation is only partial since, as illustrated in Figure 4, these camera movements can introduce black areas outside the original video boundaries, cause some blurring due to the nature of point cloud projection and have poor temporal consistency since they are generated frame-by-frame. Another way to obtain the noisy anchor video, that uses recent advances in 3D reconstruction, is to use multiview diffusion model [24] conditioned on camera pose and individual video frames. This method also results in an anchor video that has poor temporal consistency, along with blurring, artifacts and black areas outside the scene. Using this anchor video our method is able to generate clean output with the desired camera trajectory. To achieve this we propose the novel technique of masked video finetuning. This technique consists of training context-aware spatial LoRA and temporal motion LoRA on the known pixels from the generated anchor video, as well as from additional reference frame data. Specifically, the spatial LoRA is incorporated into the spatial layers of the video diffusion model and finetuned on augmented frames extracted from the source video. This enables the model to learn the subjects appearance and the background context of the source video. The temporal LoRA enables the model to learn the scene motion with respect to the new camera trajectory, and is inserted into the temporal layers of the video diffusion model and finetuned using masked loss on the anchor video. Unknown regions are masked, which excludes them from the loss computation, enabling the model to focus on meaningful and known regions and motion while ignoring the unknown areas. During inference, equipped with both video specific spatial and temporal LoRAs, the diffusion model can automatically fill the unknown regions of the anchor video with plausible content, leveraging the video diffusion models prior and the context provided by the spatial LoRA. It also significantly improves temporal consistency and removes anchor video jittering. This results in coherent and meaningful video output, preserving the motion and layout of the original anchor video as learned through the temporal LoRA training. Finally, as refining step, we can remove the temporal LoRA and retain only the context-aware spatial LoRA to apply SDEdit [56] to the generated video, thereby further reducing blurring and improving temporal consistency. In the end, we generate video with new camera trajectories while preserving the original complex scene motion and the full content of the source video. Notably, this is accomplished without the need for paired video data. Ultimately, our method outperforms the generative approach Generative Camera Dolly [82], which requires paired videos as training data, and other 4D reconstruction methods [48, 93] on the Kubric dataset [82]. Furthermore, each component of our proposed method is validated through ablation studies on VBench [40]. 2. Related Work Video Diffusion Model. Recent video generation methods [4, 15, 16, 25, 30, 37, 41, 70, 97, 105], predominantly utilize diffusion models [34, 60, 79] due to their stateof-the-art performance and robust open-source communities. Some approaches use 3D-UNet architecture, inflating an image diffusion model with trainable temporal layers [4, 7, 10, 26, 37]. Other works explored the transformers architecure, with both discrete [43] and continous tokens [11, 27, 54]. In this paper, we use the open-source Stable Video Diffusion (SVD) [7] as our video diffusion model. Personalization of Video Diffusion Models. At this stage the problem of personalization of image generative models has been well explored in the last several years, with work on subject-driven generation [18, 22, 71, 72, 100], styledriven generation [33, 69, 78, 86], style+subject-driven generation [73, 75] and image-level personalization for inpainting [81]. The research direction of personalization of video models is more sparse, albeit with important recent work such as Dreamix [58] which proposed to finetune video models on given video, Still-Moving [14] which mitigates the need for customized video data by elevating cus2 Figure 2. ReCapture consists, at setup time, of (a) Anchor video generation (b) Masked video fine-tuning using spatial and temporal LoRAs. To generate the clean output video with the new camera trajectory we simply perform inference of the video model. tomized image models to the video domain using spatial and temporal adapters and Movie Gen [62] which proposes directly training conditioning pathways for video models. Our method targets wholly different application than this body of work, although these methods are important and related. Video Generation with Camera Control. Recent works studied adding explicit camera control to video generation models, in order to produce videos that both match the input text and align with specified camera trajectory. Most methods [2, 29, 44, 89, 98] introduce additional modules that accept camera parameters, and train in supervised manner. Other approaches, such as Viewcrafter [102] and Training-Free [38], leverage rendering priors as input to achieve camera control. Unlike these prior works, which incorporate camera control into the video generation process, our method alters the camera trajectory of user provided videos, while preserving the original content and dynamics. Novel View Synthesis of Dynamic Scene. Novel View Synthesis has been long standing task, where given few sparse view of scene the model predicts novel view from an unobserved camera position [17, 28, 36, 49, 52, 61, 63, 83, 85, 88, 95, 106]. However, extending such methods to videos is highly non-trivial since they rely on multiview training data. While obtaining different views of scene difficult, it is significantly more feasible than obtaining synchronized video pairs with varying camera trajectories. Recently, methods that generate novel view of dynamic scenes (i.e. videos) have been introduced. Initially, such methods relied on multiple synchronized input videos [3, 5, 46], which limited its practical application. With the invention of NeRF [57], newer methods built 4D volumetric representation of the scene and rendered different views using using time-evolving NeRFs [20, 47, 47, 59, 65, 96]. These methods requires the input video to include various camera views and struggles to generalize outside of the field of view present in the original video. Recent approaches support more natural monocular videos. DynIBaR [48] employs volumetric image-based rendering framework that aggregates features from nearby views from monocular video in camera-aware manner. DpDy [84] leverages an image-based diffusion model to create hybrid 4D representation, combining static and dynamic NeRF components. More recently, some approaches have employed dynamic 3D Gaussian splatting [21, 91] to reconstruct 4D scenes and achieve real-time rendering. While, generating appealing results, these methods often rely on effective multi-view cues generated by camera motion, which requires camera teleportation or quasi-static In scenes and may be lacking in monocular videos [23]. addition, they often struggle to extrapolate outside of the provided field of view. Recently, several text-to-4D and image-to-4D papers have emerged; however, their results are mostly confined to animations of individual objects or animals [1, 50, 67, 68, 77, 104, 108]. Other studies [90, 101] address more complex scenes, but they require 4D training data or sophisticated 4D reconstruction. Instead of building an explicit 4D representation, our method harnesses the motion prior of video generation models, and reformulates the task as video-to-video translation (regenerating given video from the requested camera trajectory). We show that our method performs effectively on real-world videos and generate meaningful content even when the camera moves beyond the original field of view. 3. Method Our method works in two stages, first by generating an incomplete and noisy anchor video with respect to the new 3 Figure 3. Anchor video generation using image-level multiviewdiffusion models to generate new views frame-by-frame. camera trajectory and second by regenerating this anchor video into clean and temporally consistent reangled video using our proposed masked video fine-tuning technique. In more detail, the first stage consists of image-based view synthesis, in which we independently transform each input video frame to produce noisy anchor frames with the new camera pose, along with their validity masks. These frames are typically incomplete; they have artifacts such as missing information from revealed occlusions, and have structural deformations and temporal inconsistencies such as flickering. Our overall method is agnostic to the specific technique used to generate the anchor frames in the first stage, and in this work we explore two different techniques: point-cloud sequence rendering, and multi-view per-frame image diffusion. Our full method can correct errors in the anchor video, solve temporal inconsistency and complete missing information. In the second stage we apply novel masked video finetuning strategy. We train temporal motion LoRA on the anchor video with masked loss (that masks the corrupted parts of the anchor video). This directs the model to generate video that follows the camera motion and dynamics of the anchor video while completing missing parts according to the model prior. In addition, we train context-aware spatial LoRA on augmented images generated from frames of the source video. This is done by disabling the temporal layers of the video model allowing us to train on images. This step allows the model to fix structural artifacts in the anchor video. After fine-tuning, the video diffusion model is able to regenerate the anchor video such that the missing areas are seamlessly filled with temporally and spatially consistent video content. Furthermore, it preserves the coherence, content and dynamics of the original video and eliminates artifacts from the first stage, all while adapting to the new camera motion. The final output is simply the source video with the new camera trajectory, which often includes views of the scene that are unseen in the source video. Figure 4. Anchor video generation using depth estimation to turn each frame into point cloud and then generating new views by controlling the camera pose. 3.1. Preliminaries 3D U-Net. Video diffusion models train 3D U-Net (spatial-temporal for 3D) to denoise sequence of Gaussian noise samples to generate videos, guided by text or image prompts [8, 10]. The 3D U-Net architecture comprises down-sampling, middle, and up-sampling blocks. Each block includes multiple spatial convolution layers, spatial transformers, cross-attention layers, and either temporal transformers or convolution layers. Low-Rank Adaptation (LoRA). Low-rank adaptation was introduced to fine-tune large pre-trained language models [39] and has since been applied to text-to-image and text-to-video tasks for appearance customization [14, 71]. It updates the weight matrix using low-rank factorization as: = W0 + = W0 + BA, (1) where W0 represents the original weights, and and are low-rank factors with much fewer dimensions. LoRA is computationally efficient and can be more easily regularized, which preserves more of the models prior knowledge compared to fine-tuning the entire network. 3.2. Anchor Video with New Camera Motion this stage, we are given reference video = At [I0, . . . , IN 1] RN 3HW , where represents the number of frames. The main objective is to transform these frames into new sequence, denoted as Va, based on different camera trajectory provided by the used. We refer to this video as an anchor because it anchors the final output of our method and serves as condition for the next stage. This first draft contains artifacts from out-of-scene regions and temporal inconsistencies, which will be corrected in the second stage. We present two approaches to obtain an anchor video based on new camera trajectory. The first method uses point cloud rendering technique, suitable for typical camera 4 movements such as panning, tilting, and zooming, which involve simple translations and small rotations. The second approach is designed for camera movements that involve larger rotations such as orbiting. In this approach, we utilize multiview diffusion model [24] to generate novel views of the scene. Point Cloud Sequence Rendering. We begin by lifting the pixels from the input image plane into 3D point cloud representation. For each frame of the source video Ii, {0, ..., 1}, we independently estimate its depth map Di using an off-the-shelf monocular depth estimator [6]. By combining the image with its depth map, the point cloud Pi can be initialized as: Pi = ϕ([Ii, Di], K), (2) where ϕ denotes the mapping function from RGBD to 3D point cloud in the camera coordinate system, and represents the cameras intrinsics using the convention in [19]. Next, we take as input the camera motion as predefined trajectory of extrinsic matrices {P1, ..., PN 1}, where each includes rotation matrix and translation matrix representing the cameras pose (position and orientation), which are used to rotate and translate the point cloud in the cameras coordinates. We then project the point cloud of each frame back onto the anchored camera plane using the function ψ to obtain rendered image with perspective change: Ia = ψ(Pi, K, Pi). By calculating the extrinsic matrices corresponding to the cameras movement, we can express variety of camera motions including zoom, tilt, pan, pedestal, and truck, enabling flexible camera control to yield anchor videos: Va = (cid:8)Ia 0, ..., Ia (cid:9) = {ψ(Pi, K, Pi)i {0, ..., 1}} . (3) Simultaneously with the color frames, we obtain binary mask for each frame. Valid pixels after projecting the point cloud, represented with value of 1. Regions missing due to camera movement as shown in Fig. 4, which extend beyond the original video scene, are marked as 0. We denote the corresponding sequence of binary masks as Ma RN 1HW . Multiview Image Diffusion for Each Frame. When camera trajectory involves significant rotation and viewpoint changes, point cloud rendering usually fails [102]. To address this, we employ multiview diffusion model [24]. This approach leverages the fact that multiview image datasets are generally easier to obtain compared to multiview video datasets. Specifically, as shown in Fig. 3, for each frame Ii of the source video, which represents the condition view, along with its corresponding camera parameters Pcond, the model learns to estimate the distribution of 5 the target image Ia (Ia Ii, Pcond, Pi) . (4) where Pi is the target camera parameters which are also provided as input. The multiview diffusion model employs 3D U-Net as its backbone, and for simplicity of notation, we omit the latent VAE encoder. The 3D U-Net is adapted from 2D text-to-image U-Net by inflating the 2D selfattention mechanism into 3D, allowing it to operate across both 2D spatial dimensions and multiple view images. Due to the difficulty in obtaining the camera pose with the condition frame, we follow CAT3D and use raymap with the same dimensions as the condition images, computed relative to the first images camera pose. This makes the pose representation invariant to rigid transformations. Raymaps are then concatenated channel-wise to the corresponding condition image. By applying this multiview diffusion model to each frame, we can generate the anchor video Va. However, processing each frame independently leads to significant temporal inconsistencies and artifacts  (Fig. 4)  . While CAT3D can complete unseen regions, it hallucinates these region differently at each frame. To mask out this regions, we obtain the mask Ma with the same approach as point cloud rendering, which indicate the regions missing due to added camera movement. 3.3. Masked Video Fine-tuning At this stage, our objective is to take an incomplete anchor video (with significant artifacts and inconsistencies) as input and produce clean, high-quality output. To achieve this, we introduce the technique of masked fine-tuning of video diffusion model, incorporating context-aware spatial LoRA and temporal motion LoRA. By leveraging the strong prior of the video diffusion model, we can generate high-quality video conditioned on the anchor video. Next we present our components: Temporal LoRAs with Masked Video Fine-tuning. The anchor video from the first stage may exhibit significant artifacts, such as revealed occlusions due to camera movement and temporal inconsistencies such as flickering. To address these issues, we propose masked video fine-tuning strategy using temporal motion LoRAs. LoRAs are applied to the linear layers of the temporal transformer blocks in the video diffusion model. Since LoRA operates in low-rank space and the spatial layers remain untouched, it focuses on learning fundamental motion patterns from the anchor video without over-fitting to the entire video. The strong temporal consistency prior from the video diffusion model helps minimize temporal inconsistencies. We introduce masked diffusion loss, where the invalid regions in the anchor video are excluded from the loss calculation, ensuring Figure 5. Comparisons with generative camera dolly [82] using an orbit camera trajectory. the model only learns from meaningful pixels. During inference, the video diffusion model regenerates the video and automatically fills in the invalid regions while maintaining the original motion of the anchor video. The temporal loss for diffusion training is defined as: Ltemp = Eϵ,t [Ma ϵ ϵθ(Va , t, y)] , (5) where ϵ represents noise added to the anchor video Va, Va denotes the noisy anchor video at time step t, refers to the text or image condition, and θ indicates the weights of the 3D U-Net along with the LoRA weights. Context-Aware Spatial LoRAs. Although the video diffusion model with masked fine-tuning automatically fills the invalid regions of the anchor video, the filling may not be consistent with the original context or appearance, and might appear pixelated, as shown in Fig. 8 Line 2. To address this issue, we propose enhancing the spatial attention layers of the video diffusion model by incorporating spatial LoRA, which is fine-tuned on the frames of the source video. At each training step, frame is randomly selected from the source video, and the temporal layers are bypassed. The spatial LoRA loss is defined as follows Lspatial = Eϵ,t,iU {0,...N 1} [ϵ ϵθ((Ii,t), t, y)] , (6) where Ii, denotes the noisy frame Ii of the source video at time step t. The spatial LoRA captures the original context from the source video, ensuring seamless integration of filled pixels with the original pixels. Consequently, our final diffusion loss is the sum of Ltemp and Lspatial. To ensure compatibility between the spatial and temporal LoRAs, features from the corrupted video for training temporal LoRA are also passed through the spatial LoRA, without updating its parameters. Eliminating blurriness. After finishing training both LoRAs, we can directly use the video diffusion model to generate the desired video with new camera motion while maintaining high visual quality. Finally, as post-processing stage to further eliminate blurriness, which cannot be fully addressed by masked video fine-tuning as shown in Fig. 8 Line 3, we use the video diffusion model with spatial LoRA while omitting the temporal LoRA to perform SDediting [56] on the output video. Typically, SD-editing introduces randomness that alters the appearance of the subject, but since our spatial LoRA has been fine-tuned on the source video, it preserves the original appearance while simultaneously removing the blur. Our method successfully adds dynamic camera motion to the existing videos without the need for large-scale training on 4D multi-view video data, and it generalizes to wide array of videos and camera trajectories. Next, we present our experiments, datasets, evaluations, and comparisons. 4. Experiments We conduct comprehensive qualitative and quantitative evaluation to assess the performance of our method, comparing it against prominent baselines for the task of novel view synthesis in existing videos. Our quantitative analysis in Sec. 4.1 encompasses two complementary subsets of automatic metrics: low-level statistics, including PSNR and SSIM, and high-level semantic measures, such as subject 6 Figure 6. Gallery of generated videos with novel and unseen user-provided camera trajectories using ReCapture. 7 Models Generative Camera Dolly [82] Ours Subject Consistency 83.02% 88.53% Background Consistency 80.42% 92.02% Temporal Flickering 74.64% 91.12% Motion Smoothness 82.33% 98.24% Aesthetic Dynamic Degree Quality 51.24% 38.67% 57.35% 49.03% Object Imaging Quality Class 58.62% 76.46% 64.75% 82.07% Table 1. Quantitative comparisons with Generative Camera Dolly on VBench. Figure 7. Visualization of the effectiveness of masked video fine-tuning (Stage 2) for generating spatially and temporally coherent outputs from noisy anchor videos. Method HexPlane [12] 4D-GS [93] DynIBaR [48] Vanilla SVD [8] ZeroNVS [74] Generative Camera Dolly [82] Ours PSNR (all) SSIM (all) LPIPS (all) PSNR (occ.) SSIM (occ.) 15.38 14.92 12. 13.85 15.68 20.30 0.428 0.388 0.356 0.312 0.396 0.587 0.568 0.584 0.646 0.556 0.508 0.408 14.71 14.55 12. 13.66 14.18 18.60 0.428 0.392 0.358 0.326 0.368 0.527 20.92 0.596 0. 18.92 0.541 Table 2. Comparison results on Kubric-4D. We evaluate gradual dynamic view synthesis models following [82] to use video with resolution 384 256. Our method achieves superior performance compared to other reconstruction and generative methods. consistency. Additionally, we incorporate user study for non-automatic evaluation. An ablation study in Sec. 4.3 further demonstrates the importance of each component in our In Sec. 4.2, we present qualitative results that approach. visually illustrate our methods superiority, and implementation details are provided in Sec. 4.4. 4.1. Quantitative Evaluation Low-level evaluation metrics. We utilize the Kubric-4D dataset, consisting of 3,000 scenes, including an evaluation subset of 100 scenes. These scenes, generated with the Kubric simulator, showcase complex multi-object interactions and dynamic movement patterns. Each scene includes synchronized video from 16 fixed camera viewpoints at resolution of 576 384 across 60 frames at 24 FPS. Additionally, each scene contains 7 to 22 objects of varying sizes, with roughly one-third initially positioned mid-air to introduce intricate dynamics. This arrangement leads to frequent and complex occlusions, posing substantial challenge for accurate novel view synthesis. For consistency, we adhere to the evaluation protocol outlined in [82], downsampling the video to 14 frames at resolution of 384 256. As shown in Table 2, our method outperforms existing 4D reconstruction methods, underscoring the effectiveness of framing camera control as video-to-video translation task using generative models. Notably, our approach achieves superior results even without 4D training data, surpassing Generative Camera Dolly, which depends on extensive 4D data for training. High-level semantic evaluation metrics. As noted in [51], low-level evaluation metrics often fall short in accurately capturing the true quality of video. For example, while PSNR values may be similar, the visual quality of Dolly (as shown in Fig. 5) is significantly blurrier and inferior to ours. To provide more comprehensive and fair assessment of video quality, we conduct evaluations following the VBench dataset [40]. The benchmark comprises 35 videos and evaluates seven critical dimensions of video generation, including factors like subject identity consistency, motion smoothness, and temporal flickering. These finegrained evaluation metrics employ feature extractors such as DINO [13] for assessing consistency and MUSIQ [42] for measuring image quality. This approach allows for 8 Models Anchor Video + Temporal LoRAs w/ Masks ) ++ Spatial LoRAs) +++ SD-Edit Subject Consistency 82.41% 85.24% 86.02% 88.53% Background Consistency 77.45% 90.88% 91.24% 92.02% Temporal Flickering 64.50% 89.60% 90.02% 91.12% Motion Smoothness 74.27% 97.32% 97.32% 98.24% Dynamic Degree 49.72% 49.64% 49.64% 49.03% Aesthetic Quality 34.94% 40.41% 49.18% 57.35% Object Imaging Quality Class 55.90% 79.82% 62.34% 80.02% 63.03% 80.02% 64.75% 82.07% Table 3. Ablation studies for each component of mask video diffusion finetuning: + Temporal LoRAs applies temporal LoRAs solely for masked video finetuning. ++ Spatial LoRAs introduces additional context-aware LoRAs, using both spatial and temporal LoRAs for finetuning. +++ SD-Edit involves applying SD-editing after completing training with both LoRAs for eliminating blurriness. higher-level evaluation, offering deeper insights into each models distinct strengths and areas for improvement. We supply the specific prompt associated with each video to facilitate Object Class evaluation As shown in Table 1, our method outperforms in most evaluation dimensions by large margin. This approach allows for higher-level evaluation, offering deeper insights into each models distinct strengths and areas for improvement. 4.2. Qualitative Comparisons For qualitative results, we compare our method with Generative Camera Dolly [82], as shown in Figure 4. The comparison demonstrates that our method more accurately follows the camera trajectory, generates fewer artifacts, and exhibits less blurriness than Generative Camera Dolly. Notably, even when the camera position differs significantly from the original video, our method faithfully preserves the subjects motion and appearance, whereas Dolly suffers from significant artifacts under these conditions. 4.3. Ablation Studies. We evaluate the impact of each component of our method in Table 3. As demonstrated, fine-tuning the temporal LoRA with masking significantly improves temporal consistency and visual quality. This effect is also visible in Fig. 8, where generating the video with temporal LoRA helps to fill regions not visible in the anchor video, enhancing both visual quality and temporal coherence. Additionally, incorporating the context-aware spatial LoRA further boosts visual quality and subject consistency by leveraging prior knowledge of the original video. Finally, integrating SDEdit further enhances overall quality by reducing artifacts. These results affirm the effectiveness of each component in our method. 4.4. Implementation Details. We use the the CAT3D [24] multi-view model without any further adjustments. We employ SVD [9] as our video diffusion model in all our experiments, since it is I2V model, we use Va 0 as the image prompt. For the LoRA finetuining we use rank of 16 for both spatial and temporal LoRAs. Figure 8. Detailed ablation of all components of our method. The spatial LoRA is added to the self-attention layers, while the temporal LoRA is integrated into the temporal attention layers, with the learning rate set to 5e4. The total number of fine-tuning steps is set 400 and requires 5 min on single 80GB A100 GPU. 5. Conclusion In this work we present ReCapture, our proposed method to generate new videos with novel camera trajectories from existing user-provided videos. Compared to previous work, and thanks to the strong prior of video models, ReCapture has surprisingly strong ability to generalize to vastly different videos and scenes, and in many cases faithfully preserves complex scene and subject motion, as well as the details of the scene."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hyIn Proceedings of the brid score distillation sampling. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 3 [2] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. 1, 3 [3] Ajay Bansal, Mykhaylo Vo, Yaser Sheikh, Deva Ramanan, and Srinivasa Narasimhan. 4d visualization of dynamic events from unconstrained multi-view videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53665375, 2020. 3 [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: spacetime diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 2 [5] Mostafa Bemana, Karol Myszkowski, Hans-Peter Seidel, Implicit neural view-, and Tobias Ritschel. X-fields: light-and time-image interpolation. ACM Transactions on Graphics (TOG), 39(6):115, 2020. 3 [6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [7] A. Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, and Dominik Lorenz. Stable video diffusion: Scaling latent video diffusion models to large datasets. ArXiv, abs/2311.15127, 2023. 2 [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv, 2311.15127, 2023. 4, 8 [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 9 [10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video syntheIn Proceedings of the sis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 1, 2, 4 [11] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [12] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 8 [13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 8 [14] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. arXiv preprint arXiv:2407.08674, 2024. 2, 4 [15] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2 [16] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2 [17] Ruitao Chen, Yuwei Chen, Nitesh Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for arXiv preprint high-quality text-to-3d content creation. arXiv:2303.13873, 2023. [18] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 36, 2024. 2 [19] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 5 [20] Yilun Du, Yinan Zhang, Hanjun Xiong Yu, Joshua Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view In 2021 IEEE/CVF Insynthesis and video processing. ternational Conference on Computer Vision (ICCV), pages 1430414314. IEEE Computer Society, 2021. 3 [21] Y. Duan, S. Ren, J. Luo, Y. Chen, H. Wang, L. Zheng, and Q. Dai. 4d radiance fields with multi-scale occupancy networks for dynamic scene reconstruction. In CVPR, 2024. 3 [22] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. ArXiv, abs/2208.01618, 2022. 2 [23] H. Gao, R. Li, S. Tulsiani, B. Russell, and A. Kanazawa. Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems, 35: 3376833780, 2022. 10 [24] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv, 2024. 2, 5, 9 [25] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning. arXiv, 2311.10709, 2023. 2 [26] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [27] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 1, 2 [28] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF International Conference on Computer Vision, pages 1974019750, 2023. 3 [29] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 1, [30] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for higharXiv fidelity video generation with arbitrary lengths. preprint arXiv:2211.13221, 2022. 2 [31] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for highfidelity long video generation. 2022. 1 [32] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for higharXiv fidelity video generation with arbitrary lengths. preprint arXiv:2211.13221, 2022. 1 [33] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47754785, 2024. 2 [34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 2 [35] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [36] Felix Hollein, Yan-Pei Fu, Chi-Hao Weng, Sifei Liu, Jan Kautz, Aditya Ramesh, Arash Vahdat, Jonathan Ho, and Text2room: Extracting textured 3d Matthias Nießner. arXiv preprint meshes from 2d text-to-image models. arXiv:2303.14435, 2023. 3 [37] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [38] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 1, 3 [39] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [40] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 8 [41] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion video synthesis with stable diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2268022690, 2023. 2 [42] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. 8 [43] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 2 [44] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon. Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. In arXiv, 2024. 3 [45] Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, and Feng Liu. Fast view synthesis of casual videos with soup-ofplanes. arXiv preprint arXiv:2312.02135, 2023. [46] T. Li, M. Slavcheva, M. Zollhoefer, S. Green, C. Lassner, C. Kim, T. Schmidt, S. Lovegrove, M. Goesele, and R. Newcombe. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55215531, 2022. 3 [47] Z. Li, S. Niklaus, N. Snavely, and O. Wang. Neural scene flow fields for space-time view synthesis of dynamic In Proceedings of the IEEE/CVF Conference on scenes. Computer Vision and Pattern Recognition, pages 6498 6508, 2021. 3 [48] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42734284, 2023. 2, 3, 8 [49] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: HighResolution Text-to-3D Content Creation. In CVPR, 2023. 3 11 [50] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85768588, 2024. 3 [51] Jia-Wei Liu, Yan-Pei Cao, Tianyuan Yang, Zhongcong Xu, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Hosnerf: Dynamic human-object-scene neural raIn Proceedings of the diance fields from single video. IEEE/CVF International Conference on Computer Vision, pages 1848318494, 2023. [52] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: In Proceedings of the Zero-shot one image to 3d object. IEEE/CVF International Conference on Computer Vision (ICCV), pages 92989309, 2023. 3 [53] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 2 [54] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [55] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video In Proceedings of the IEEE/CVF Conference synthesis. on Computer Vision and Pattern Recognition, pages 7038 7048, 2024. 1 [56] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2, 6 [57] B. Mildenhall, P.P. Srinivasan, M. Tancik, J.T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [58] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. [59] K. Park, U. Sinha, J.T. Barron, S. Bouaziz, D.B. Goldman, R. Martin-Brualla, and S.M. Seitz. NeRFies: Deformable neural radiance fields. In ICCV, 2021. 3 [60] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2 [61] Zigang Geng Po, Chunyu Wang, Yixuan Wei, Ze Liu, Houqiang Li, and Han Hu. Human pose as compositional tokens. arXiv preprint arXiv:2303.14435, 2023. 3 [62] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 3 [63] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In ICLR, 2022. [64] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields arXiv preprint arXiv:2011.13961, for dynamic scenes. 2020. 2 [65] A. Pumarola, E. Corona, G. Pons-Moll, and F. MorenoNoguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 3 [66] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023. 1 [67] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 3 [68] Jiawei Ren, Kevin Xie, Ashkan Mirzaei, Hanxue Liang, Xiaohui Zeng, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, et al. L4gm: Large 4d gaussian reconstruction model. arXiv preprint arXiv:2406.10324, 2024. 3 [69] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 2 [70] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [71] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 4 [72] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 2 [73] Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David Jacobs, and Shlomi Fruchter. Magic insert: Style-aware drag-and-drop. arXiv preprint arXiv:2407.02489, 2024. 2 [74] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: 12 Zero-shot 360-degree view synthesis from single real image. arXiv preprint arXiv:2310.17994, 2023. [75] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422438. Springer, 2025. 2 [76] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1 [77] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, and Yaniv Taigman. Text-to-4d dynamic scene generation. arXiv:2301.11280, 2023. 3 [78] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: text-to-image generation in any style. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 6686066889, 2023. 2 [79] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. [80] Colton Stearns, Adam W. Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In ArXiv, 2024. 2 [81] Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, et al. Realfill: Reference-driven generation for authentic image completion. ACM Transactions on Graphics (TOG), 43(4):112, 2024. 2 [82] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExarXiv treme monocular dynamic novel view synthesis. preprint arXiv:2405.14868, 2024. 2, 6, 8, 9 [83] Vikram Voleti, Girish Varma, and Venkatesh Babu. arXiv preprint Sv3d: Scalable video 3d generation. arXiv:2303.14435, 2024. 3 [84] C. Wang, P. Zhuang, A. Siarohin, J. Cao, G. Qian, H.Y. Lee, and S. Tulyakov. Diffusion priors for dynamic arXiv preprint view synthesis from monocular videos. arXiv:2401.05583, 2024. [85] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1261912629, 2023. 3 [86] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. 2 13 [87] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. 2024. 2 [88] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: HighFidelity and Diverse Text-to-3D Generation with Variational Score Distillation. In NeurIPS, 2023. 3 [89] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2023. 1, 3 [90] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024. [91] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. 4d gaussian splatting arXiv preprint Liu, Q. Tian, and W. Xinggang. for real-time dynamic scene rendering. arXiv:2310.08528, 2023. 3 [92] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2031020320, 2024. 2 [93] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. 2, 8 [94] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 1 [95] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors. 2023. [96] W. Xian, J.B. Huang, J. Kopf, and C. Kim. Space-time neural irradiance fields for free-viewpoint video. In CVPR, 2021. 3 [97] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2025. 2 [98] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 3 [99] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [100] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [101] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. arXiv preprint arXiv:2406.07472, 2024. 3 [102] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3, 5 [103] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023. 1 [104] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024. 3 [105] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. 1, 2 [106] Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, and Jingbo Shang. Scenewiz3d: Interactive 3d scene generation with scene graphs. arXiv preprint arXiv:2303.14435, 2023. [107] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customizaarXiv preprint tion of text-to-video diffusion models. arXiv:2310.08465, 2023. 1 [108] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: AnimatarXiv preprint ing one image to 4d dynamic scene. arXiv:2311.14603, 2023. 3 [109] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022."
        }
    ],
    "affiliations": [
        "Google",
        "National University of Singapore"
    ]
}