{
    "paper_title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
    "authors": [
        "Ziyin Zhang",
        "Jiahao Xu",
        "Zhiwei He",
        "Tian Liang",
        "Qiuzhi Liu",
        "Yansi Li",
        "Linfeng Song",
        "Zhengwen Liang",
        "Zhuosheng Zhang",
        "Rui Wang",
        "Zhaopeng Tu",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
        },
        {
            "title": "Start",
            "content": "DeepTheorem DEEPTHEOREM: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning Ziyin Zhang,1,2 Jiahao Xu, ,1 Zhiwei He1,2 Tian Liang1 Qiuzhi Liu1 Yansi Li1,2 Linfeng Song1 Zhengwen Liang1 Zhuosheng Zhang2 Rui Wang,2 Zhaopeng Tu,1 Haitao Mi1 Dong Yu1 5 2 0 M 9 2 ] . [ 1 4 5 7 3 2 . 5 0 5 2 : r 1Tencent 2Shanghai Jiao Tong University https://github.com/Jiahao004/DeepTheorem https://huggingface.co/datasets/Jiahao004/DeepTheorem (a) Dataset Scale (b) Performance Figure 1: (a): Our dataset surpasses others with extremely challenging theories; (b): RL-Zero training with our DeepTheorem datasets on 7B model achieves strong results. Abstract Theorem proving serves as major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorems potential to fundamentally advance automated informal theorem proving and mathematical exploration. Equal Contribution. The work was done when Ziyin Zhang, Zhiwei He and Yansi Li were interning at Tencent AI Lab. Correspondence to: Jiahao Xu <jettexu@tencent.com>, Zhaopeng Tu <zptu@tencent.com> and Rui Wang <wangrui12@sjtu.edu.cn>. 1 DeepTheorem"
        },
        {
            "title": "Introduction",
            "content": "Theorem proving is widely regarded as pinnacle challenge for evaluating advanced reasoning capabilities of both human and artificial intelligence. It requires integrating diverse cognitive facets such as abstraction, strategic inference, pattern recognition, and meticulous logical deduction. Recent advancements in deep learning, especially in large language models (LLMs), have significantly reshaped the landscape of automated theorem proving (ATP). Much prior work attempts ATP by integrating LLMs with either formal proof engines such as Lean, Coq, and Isabelle (Zheng et al., 2022; Liu et al., 2023; Tsoukalas et al., 2024) or domain-specific languages from ProofWiki (Welleck et al., 2022). However, these proof methods impose significant barrier for LLMs whose primary strength derives from the vast corpus of natural language and LaTeX-based mathematical texts used during pre-training. This inherent misalignment limits LLMs capability in theorem proving, leaving considerable gap between their potential and actual performance. In this paper, we present DeepTheorem, novel informal framework expressly designed to leverage natural language to unleash the latent mathematical reasoning ability of LLMs. Instead of relying on formal proof assistants, DeepTheorem offers scalable, intuitive, and flexible alternative, enabling LLMs to generate informal mathematical proofs aligning closely with human mathematicians heuristic-driven thinking. Central to our approach is the construction of large-scale benchmark comprising 121K IMO-level informal mathematical theorems with precise annotations such as correctness labels, difficulty levels, diverse mathematical domains, and verifiable theorem variants amenable to advanced reinforcement learning paradigms. Additionally, we introduce reinforcement learning-based training protocol (RL-Zero) to significantly enhance LLMs ability to reason mathematically beyond the constraints of supervised fine-tuning (SFT). We also propose comprehensive evaluation metrics that rigorously assess the correctness of generated proofs and the processes underlying the proofs themselves. Through extensive experiments, we show that leading LLMs still exhibit significant limitations in theorem proving. However, when trained with DeepTheorem, they achieve substantial performance improvements over models trained using existing datasets, showcasing the effectiveness of our natural-language-focused approach. Our results underscore the promise of DeepTheorem to redefine LLM-driven mathematical reasoning, offering robust platform for continued progress in automated and scalable informal theorem proving. Contributions Our key contributions are: We introduce DeepTheorem, large-scale natural-language dataset of 121K informal mathematical theorems and corresponding high-quality proofs at IMO-level difficulty. This dataset is systematically annotated for correctness, difficulty, topic diversity, and includes verifiable theorem variants suitable for advanced reinforcement learning. We propose novel RL-Zero training method explicitly designed for informal theorem proving, significantly enhancing LLMs reasoning capacity beyond traditional SFT methods. We develop comprehensive evaluation frameworks assessing both the correctness of theorem proofs (outcome evaluation) and the completeness, logical validity, and correctness of generated reasoning processes (process evaluation). Through extensive experiments, we establish the superiority of our DeepTheorem paradigm, achieving state-of-the-art performance and surpassing existing informal theorem datasets and training methods."
        },
        {
            "title": "2 Dataset",
            "content": "Overview The DeepTheorem dataset1 is novel, large-scale resource designed to advance LLMs in informal mathematical theorem reasoning. Mined from diverse web corpus, it addresses the need 1In the rest of this section, DeepTheorem refers specifically to the DeepTheorem training dataset. 2 DeepTheorem Figure 2: data sample from the DeepTheorem dataset. for challenging, decontaminated, and diverse topics to push LLMs toward frontier theorem proving. As illustrated in Figure 2, each entry in the dataset offers distinct features tailored to support diverse research objectives. Theorem-Proof Pairs: Each sample includes theorem and its corresponding proof, enabling researchers to develop models for end-to-end mathematical reasoning and automated proof generation. Clearly-Framed Problems Each theorem is presented in standardized format. The theorem itself is either correct or incorrect. If theorem is incorrect, then we make sure that it can be mathematically disproved. Correctness Labels: Each theorem is annotated with True-or-False label, supporting benchmarking of theorem classification, training models for theorem verification, and error detection in theorem formulation. Difficulty Annotations: fine-grained difficulty score (e.g., complex theorems of level 510) enables stratified analysis of model performance, curriculum learning, and adaptive training strategies. Topic Categories: Mathematical topics (e.g., number theory, geometry) are tagged, allowing targeted studies on domain-specific reasoning, cross-domain transfer learning, and topicspecific model optimization. Step-by-Step Solutions: Detailed proofs generated by o3-mini provide interpretable reasoning paths, ideal for training explainable NLP models, reasoning-augmented generation, and step-wise critique mechanisms. Comprising approximately 121K curated samples, DeepTheorem empowers large-scale exploration of LLM mathematical reasoning, curriculum learning, and cross-domain generalization. It serves as versatile resource for advancing automated theorem proving, enhancing model reasoning capability, and developing adaptive learning frameworks, positioning it as cornerstone for next-generation NLP research in mathematical theorem proving and logical reasoning. In this section, we analyze the key features of DeepTheorem as demonstrated in Figure 2, and describe its overall processing pipeline. Large scale Comprising 121K theorems, DeepTheorem provides substantial resource for robust model training and evaluation. Unlike smaller training corpus such as Lean-Workbook (Ying et al., 2024), Deepseek-Prover-v1s training corpus (Xin et al., 2024), and theorems from OpenR1-Math (Face, 2025), DeepTheorem leverages the vastness of web-sourced content to ensure comprehensive coverage of mathematical concepts and problem types. As shown in Figure 1(a), our dataset consists of approximately 121K theorems, significantly outscaling prior datasets. 3 DeepTheorem Figure 3: Difficulty density histogram comparison across widely used training dataset (i.e. LeanWorkbook, Deepseek-Prover-V1, OpenR1 Math) and testing benchmarks (i.e. miniF2F, PutnamBench, FIMO, HMMT2425). Frontier and extremely challenging theorems DeepTheorem is distinguished by its inclusion of advanced mathematical theorems, each annotated with difficulty levels to enable targeted evaluation and training across spectrum of complexities. Following the strategy from Gao et al. (2025), we utilize LLM agents for difficulty annotations, which considers factors including problem-solving time and logical complexity, ensuring consistent and reliable categorization. As shown in Figure 3, DeepTheorem emphasizes theorems at high difficulty levels (69), surpassing existing corpora in complexity and challenge, presenting significant challenges for state-of-the-art LLMs while aligning with frontier, IMO-level benchmarks such as FIMO (Liu et al., 2023). This focus enables fine-grained benchmarking of LLMs across spectrum of cognitive demands, from moderately complex to highly intricate mathematical reasoning tasks. Diverse topics We categorize all the data samples from DeepTheorem following Gao et al. (2025). As shown in Figure 4, DeepTheorem captures the breadth of informal theorem-based reasoning by covering wide range of mathematical domains, including algebra, discrete math, applied math, calculus, geometry, mathematical analysis, number theory, etc. This extensive topical diversity ensures that the dataset spans nearly the entirety of the mathematical landscape, from foundational concepts to advanced theoretical challenges. By encompassing this wide array of domains, DeepTheorem enables researchers to assess model performance on both specialized and interdisciplinary mathematical tasks, fostering the development of LLMs that can generalize effectively across the full spectrum of mathematical reasoning. Figure 4: Statistics of DeepTheorem hierarchical topics. Strict decontamination To preserve evaluation integrity, DeepTheorem employs rigorous decontamination processes to avoid overlap with widely used benchmarks. The targets of our decontamination includes general math reasoning benchmarks - MATH (Hendrycks et al., 2021b), AIME (MAA, a), AMC (MAA, b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), Omni-MATH (Gao et al., 2025), MathOdyssey (Fang et al., 2024), GAOKAO (Zhong et al., 2024), JEEBench (Arora et al., 2023), MMLU-STEM (Hendrycks et al., 2021a), CMATH (Wei et al., 2023), OlympicArena (Huang et al., 4 DeepTheorem 2024), GSM8K (Cobbe et al., 2021), GPQA (Rein et al., 2024) - and theorem proving benchmarks: miniF2F (Zheng et al., 2022), PutnamBench (Tsoukalas et al., 2024), FIMO (Liu et al., 2023), and HMMT (Harvard-MIT Mathematics Tournament, 2024, 2025). Contaminated Example Benchmark Example Let p(x) be univariate polynomial. Then p(x) is nonnegative for all if and only if p(x) can be expressed as sum of squares (SOS), i.e., p(x) = (x) for some polynomials q1(x), . . . , qk(x). i=1 q2 Let p(x1, x2, . . . , xn) be real polynomial. If p(x1, x2, . . . , xn) non-negative for all (x1, x2, . . . , xn) Rn, is then p(x1, x2, . . . , xn) can be expressed as sum of squares of polynomials if and only if belongs to the quadratic module generated by the constraints of certain semialgebraic set. Formally, there exists set of polynomials qi(x1, x2, . . . , xn) i=1 qi(x1, x2, . . . , xn)2, prosuch that vided certain conditions on and the domain hold to ensure the SOS representation. Let be positive integer. If is not prime, then 2n 1 is not prime. p(x1, x2, . . . , xn) = Relationship Identical Generalizing Let p(x) be polynomial that is nonnegative for all real x. Prove there are polythat nomials such that p(x) = f1(x), . . . , fk(x) j=1( fj(x))2. for some k, p(x) be polynomial that is Prove there are polyf1(x), . . . , fk(x) such that Let nonnegative for all real x. that nomials p(x) = for some k, j=1( fj(x))2. Show that if is positive integer and 2n 1 is prime, then is prime. Logically Converse Table 1: Examples of benchmark contamination in polynomial and number theory problems. Generalizing and logically equivalent parts are highlighted. Proofs from advanced LLMs DeepTheorem includes concise, high-quality proof solutions generated by o3-mini, tailored for supervised fine-tuning (SFT). These proofs provide compact yet complete outline of the logical steps required to prove (or disprove) each theorem, optimized for clarity and brevity. Unlike verbose or overly formal proofs, these proofs, expressed in LaTeX, align with the informal nature of LLMs, making them an effective learning signal. By incorporating these proofs, the dataset enables models to internalize structured reasoning patterns, improving their ability to generate coherent and logically sound mathematical arguments. 2.1 Constructing DeepTheorem Figure 5: The process pipeline of constructing DeepTheorem dataset. As shown in Figure 5, the construction pipeline of DeepTheorem starts by aggregating raw data from multiple sources, including MMIQC (Liu et al., 2025), WebInstruct (Yue et al., 2024), and NuminaMath-CoT (LI et al., 2024). Decontamination To ensure the integrity of DeepTheorem, we implement rigorous decontamination process to eliminate overlap between training and testing datasets. The process employs recall-and-justify pipeline to identify and remove potential duplicates, consisting of three key steps: 5 DeepTheorem 1. Embedding Generation: We use an embedding model2 (Reimers & Gurevych, 2019; Toshniwal et al., 2025) to generate sentence embeddings for all theorem statements in the training and testing datasets. 2. Similarity Recall: For each training sample, we compute its embedding cosine similarity to all test samples, and recall the top five testing samples exceeding similarity threshold3. 3. Contamination Justification: An LLM4 evaluates whether the recalled test samples are contaminated within the current training sample (Appendix A.1). This process removed approximately 199K contaminated samples, effectively identifying identical cases, generalized questions, and converse theorems, Removed contaminated examples are shown in Table 1. Approximately 2.6M samples remain for the next processing step. Quality control and proof generation We also implement rigorous quality control pipeline for generating and validating theorem statements and their proofs. The process involves four key steps: 1. Theorem Justification: An LLM verifies that the question is complete, and indeed theorem-proving question (Appendix A.2). 2. Rationale Summarization: An LLM summarizes the original question and generates formatted, concise, self-contained theorem (Appendix A.3). 3. Proof Generation: o3-mini (high effort) generates the proof solution with True-or-False conclusion about the theorem (Appendix A.4). 4. Logical Validation: The LLM performs an extra justification step to check that the theorem-proof pair is logically coherent. (Appendix A.5) This systematic approach yielded 1.08M high-quality, mathematically sound theorem-proof pairs. Difficulty and single statement annotation We annotate the difficulty levels of DeepTheorem, and remove questions with multiple statements to prove. 1. Difficulty Annotation: An LLM analyzes each theorem statement following the strategy of Gao et al. (2025), considering factors such as logical complexity, mathematical prerequisites, and proof length, to assign difficulty score on scale of 1 to 9. Only questions with difficulty score of at least 5 are retained. (Appendix A.6) 2. Single-Statement Filtering: We filter out samples that query for proving multiple statements, retaining only those with single, well-defined theorem to ensure clarity and consistency with evaluation. 3. Topic Annotation: Finally, we annotate the topic domain of the mathematical theorems with LLMs. (Appendix A.7) Finally, the difficulty and single statements filtering results in 121K challenging theorems, yielding the DeepTheorem dataset."
        },
        {
            "title": "3 Theorems Proving via RL",
            "content": "Motivation Conventionally, informal theorem-proving datasets are utilized through supervised fine-tuning (SFT), where models learn to generate proofs by imitating dataset examples. However, recent studies on RL-Zero demonstrate its superior performance over SFT by leveraging base models pretrained knowledge and exploratory capabilities (Jaech et al., 2024; DeepSeek-AI et al., 2025). This raises natural question: Can we harness the base models exploration ability for informal theorem proving? In this section, we explore the possibility of utilizing RL-Zero for informal theorem 2 paraphrase-multilingual-MiniLM-L12-v2 3 Threshold here is 0.7. 4 GPT-4o is used for annotation in this section unless otherwise specified. 6 DeepTheorem proving. The process involves three key steps: 1) data augmentation to generate contradictory theorem variants for binary rewards; 2) RL-Zero training with GRPO (Shao et al., 2024); and 3) Evaluation of the theorem-proof generation. 3.1 Theorems with Verifiable Rewards Theorems can be disproved To construct theorem with rewards for RL-Zero, we make the key observation: Insight statement need not be correct but can be also proven incorrect, enabling binary reward structure compatible with RL-Zero. This observation allows us to transform DeepTheorems theorems into true-or-false variants, facilitating RL training that incentivizes robust reasoning. To construct such training data, we use an LLM to expand the original theorems into contradictory variants that can be disproved. Specifically, we strictly limit the transformation made to the original theorem, so that the resulting variant is either entailed by or contradictory from the original theorem. For example, consider the following three variants of theorem (omitting the hypotheses for simplicity): Theorems Example Original Variant 1 Variant 2 > 1 > 0 < 1 Table 2: An example of theorem variation given an original theorem. If the original theorem can be proved, Variant #1 is also correct and can be mathematically proved in the same manner as the original one, while Variant #2 must be incorrect and can be disproved. With such logically entailing or contradictory transformations, we are able to construct variants of theorem that are guaranteed to be correct or incorrect by only accessing the theorem itself but not the proof process, which makes this transformation task much easier than annotating new math statements, and thus allowing relatively weaker LLM (e.g. Qwen2.5-72B-Instruct, Yang et al., 2024) to perform it. After this expansion phase, we further annotate the completeness of the resulting theorem pool and finally acquire training set of 242K mathematical theorems that can either be proved or disproved, each with complete proof trajectory (see Appendix A.8 for more details). 3.2 Binary Rewards Activate Theorem Proof Generation With the aforementioned theorem variants, we can now apply reinforcement learning to natural language theorem proving. Specifically, we adopt the GRPO algorithm (Shao et al., 2024). Bench Scale Variants (Avg.) Random Acc. 172 FIMO HMMT 205 Putnam 281 2.7 3.5 2.9 17.4 11.2 15. Table 3: Test data statistics. Each original theorem is manually expanded into multiple entailing or contradictory variants. Random accuracy indicates the expected score of random guessing following the outcome criteria in Figure 6. 7 DeepTheorem Proof generation with RL Inspired by the success of reasoning-specialized models such as R1 and its open-source reproductions (DeepSeek-AI et al., 2025; Hu et al., 2025), we encourage the model to enclose its reasoning process in <think> </think> tags in the system prompt to incentivize more detailed reasoning behaviours (see Appendix A.9), and then ask the model to end each proof with either boxed{proved} or boxed{disproved}. In the reward function, we extract this answer and compare it against the ground truth, giving reward of 1 if the answer matches, and 0 otherwise. We also enforce several sanity checks to prevent model collapse: if the ratio of white spaces in models solution is less than 0.05 or the average character repetition count is greater than 300, then reward of 0 is issued regardless of the answer. 3.3 Evaluation The theorem-proving questions used for evaluation are drawn from two challenging benchmarks FIMO (Liu et al., 2023) and Putnam (Tsoukalas et al., 2024) and newly constructed theoremproving subset of HMMT (Harvard-MIT Mathematics Tournament, 2024, 2025). Outcome Criteria test case in theorem testing set is passed if and only if: 1. The model explicitly produces truth value (true or false) for theorems and variants; 2. The predicted truth value for the original theorem is correct; 3. The predicted truth values for all entailing variants are the same as the original theorem; 4. The predicted truth values for all contradictory variants are the inverse of the original theorem. Figure 6: Outcome evaluation criteria given testcase with variants Outcome evaluation Evaluating the correctness of natural language (NL) proofs poses significant challenge, as it mirrors the complexity faced by humans in assessing the logical coherence and mathematical validity of informal reasoning. Unlike formal theorem-proving systems that rely on structured logic, NL proofs lack standardized format, making their evaluation inherently subjective and difficult to automate. To address this, we propose novel evaluation framework that leverages multiple entailing and contradictory variants derived from each theorem. By assessing the models ability to consistently assign correct truth values across these variants, we indirectly estimate its theorem justification ability. When the number of variants is sufficiently large, this approach provides robust proxy for evaluating the correctness of NL proof generation. Thus, we manually expand each question in the three data sources into multiple entailing or contradictory variants following the same protocol as Section 3.1, and the resulting benchmarks are shown in Table 3. When evaluating model, we ask it to either prove or disprove each theorem and corresponding variants with the criteria shown in Figure 6. Process evaluation Since theorem proving requires generating logically validated proofs for each reasoning step, we also develop process evaluation framework that evaluates the quality of proof along four dimensions: Logical Validity: Check if each step follows logically from the previous one. Flag any logical errors.; Completeness: Verify if all necessary cases and steps are included to prove the theorem; Correctness: Confirm if the final conclusion is correct; Clarity: Assess if the proof is clear, unambiguous, and well-explained. 8 DeepTheorem Model Strategy FIMO HMMT Putnam Avg. out. proc. out. proc. out. proc. out. proc. Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B SFT OpenR1-Proof 20.63 DeepTheorem 31.75 RL 34.92 OpenR1-Proof DeepTheorem 31.75 SFT OpenR1-Proof 23.81 DeepTheorem 33.33 RL OpenR1-Proof 34.92 DeepTheorem 38. SFT OpenR1-Proof 30.16 DeepTheorem 34.92 RL OpenR1-Proof 42.86 DeepTheorem 55.56 8.66 18.86 11.86 15.25 4.80 9.41 35.42 36.46 18.98 21. 22.64 27.82 10.81 16.57 8.54 15.23 16.95 23.73 5.10 10.15 55.21 52. 17.92 22.79 35.69 35.85 10.52 16.06 12.85 20.38 15.25 20.34 6.90 12. 43.75 36.46 27.96 25.43 27.60 30.04 15.90 19.32 14.33 23.39 23.73 25. 11.72 13.56 57.29 52.08 35.11 33.84 38.65 38.53 20.39 23.60 18.23 26. 15.25 22.03 8.63 15.41 48.96 41.67 32.95 33.50 31.46 32.87 19.94 25. 22.79 39.07 25.42 28.81 13.15 20.85 60.42 57.29 38.94 42.20 42.90 47. 24.96 34.04 Table 4: Outcome (out.) and Process (proc.) evaluation of models trained on OpenR1-Math-Proof and DeepTheorem. We use GPT-4o as the LLM judge and ask it to score the proof using weighted sum of the four dimensions. The complete prompt is presented in Appendix A.10."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings We train two sets of models, using supervised fine-tuning (SFT) and zero reinforcement learning (RL-Zero) respectively, both starting from Qwen2.5-Base (Yang et al., 2024). For SFT, we train the models for 3 epochs on the complete proof solutions in the dataset, using one machine for training each model. For RL-Zero, we adopt GRPO with batch size 128, group size 64, and maximum rollout length 8192. We train the models for 1000 steps, and distribute each model across two machines during training. Following the settings of Hu et al. (2025), we do not apply any KL regularization or entropy loss, as we find that KL regularization has negligible impact on model performance, while entropy loss leads to model collapse. As baseline, we select the theorem-proving subset of OpenR1-Math (Face, 2025), the highestquality existing theorem-proving dataset with complete questions and responses. We apply the same processing pipelines to it as detailed in Section 2.1, which yields 66K original theorems and 130K variants in total. We dub this processed dataset OpenR1-Math-Proof. 4.2 Main Results DeepTheorem with RL-Zero achieves the best performance The main results are presented in Table 4. DeepTheorem demonstrates superior performance over OpenR1-Math-Proof, especially for the 7B backbone and in terms of process evaluation. On the other hand, our RL-Zero training paradigm consistently outperforms SFT, validating the effectiveness of RL-Zero in pushing the models reasoning capabilities beyond the limit of SFT. DeepTheorem Model FIMO HMMT Putnam Avg. #Rank out. proc. out. proc. out. proc. out. proc. out. proc. Claude3.7-Sonnet Gemini2.5-Pro Gemini2.5-Flash GPT-4o o1-mini o1 o3-mini 34.92 57.14 30.16 34.92 60.32 66.67 80.95 30.16 Qwen2.5-Inst-7B 49.21 Qwen2.5-Inst-72B Qwen2.5-Math-Inst-7B 28.57 Qwen2.5-Math-Inst-72B 47.62 25.40 DS-Prover-v1.5-RL-7B 30.16 DS-Prover-v2-7B 6.35 R1-Distill-7B 17.46 R1-Distill-70B 17.46 QwQ-32B 41.27 Llama3.3-Inst-70B 55.56 *DeepTheorem-RL-7B Commercial Models 13.56 57.63 25.42 16.95 35.59 47.46 45.76 8.29 49.82 22.02 14.59 30.90 47.30 43.47 45.83 64.58 42.71 22.92 61.46 62.50 78. Open Source Models 10.17 13.56 3.39 11.86 11.86 5.08 0 16.95 11.86 10.17 28.81 6.83 9.78 1.61 8.61 6.32 1.71 0 13.52 10.10 4.12 20.85 33.33 48.96 25.00 47.92 34.38 40.62 4.17 30.21 25.00 36.46 57.29 26.28 54.06 28.95 30.70 55.23 61.00 77.61 21.13 37.35 18.86 36.02 13.81 21.86 4.27 14.05 15.41 27.33 39. 38.33 58.75 38.98 18.88 52.88 57.55 75.12 25.39 41.00 18.79 38.04 22.42 28.54 2.58 23.10 18.19 25.30 42.20 31.44 59.78 32.76 24.93 52.46 58.88 68.28 24.55 37.24 18.99 35.80 23.88 25.29 3.51 21.54 18.11 29.30 47.22 24.30 54.21 29.98 21.39 46.34 55.28 65.40 17.78 29.38 13.09 27.56 14.18 17.37 2.28 16.89 14.57 18.92 34. 9 2 8 12 4 3 1 13 6 16 7 14 11 18 15 17 10 5 9 3 6 10 4 2 1 12 7 17 8 16 13 18 14 15 11 5 Table 5: Evaluation comparison of our model trained with RL on DeepTheorem with SOTA LLMs for both commercial models and open source models. *: denotes our method. Inst: Instruct; DS: Deepseek. DeepTheorem achieves strong parameter efficiency We demonstrates that our DeepTheorem-RL strategy achieves strong parameter efficiency in Figure 7. It shows that compared to the Qwen2.5 series, which serves as the backbone model, training DeepTheorem on 1.5 to 7B models significantly improves the informal theorem proving boundary at parameter-performance space. Moreover, DeepTheorem parameter efficiency also surpasses SOTA commercial models such as o1 and o3-mini. SOTA performance at equal model scale In Table 5, we also provide the evaluation results of SOTA LLMs on the three benchmarks. These results suggest that theorem proving, especially our newly constructed HMMT benchmark, is still quite challenging for LLMs. On the other hand, our 7B model, trained with RL-Zero on DeepTheorem, outperforms SOTA models of much larger sizes, including those specialized in math and reasoning, demonstrating the superior quality of DeepTheorem and our innovative outcome-supervised RL training approach for theorem proving. Figure 7: Distribution of used proof techniques. 4.3 Analysis RL-Zero with DeepTheorem activate backbone model theorem proving ability We provide case study for the proofs generated by our DeepTheorem-RL trained model. 10 DeepTheorem Model-generated proof Question: Let be polynomial of degree > 1 with integer coefficients and let be any positive integer. Consider the polynomial Q(x) = P(P(. . . P(P(x)) . . .)), with pairs of parentheses. Prove or disprove that has more than integer fixed points, i.e. integers satisfying the equation Q(x) = x. Model Solution: <think> To determine whether the polynomial Q(x) = P(P(. . . P(P(x)) . . .)) with pairs of parentheses has more than integer fixed points, we need to analyze the properties of the polynomial and how the composition of affects its fixed points. First, recall that P(x) is polynomial of degree > 1 with integer coefficients. The number of integer roots of P(x) = 0 (i.e., the number of integer fixed points of P) is at most by the fundamental theorem of algebra and properties of polynomials. Lets denote the integer fixed points of by r1, r2, . . . , rm, where n. Now, consider the polynomial Q(x) = P(P(. . . P(P(x)) . . .)) with pairs of parentheses. We need to determine the number of integer solutions to the equation Q(x) = x. If is an integer fixed point of Q, then must be an integer fixed point of some composition of P. Specifically, if is fixed point of Q, there exists some such that P(m)(r) = r, where P(m) denotes the m-th iterate of P. The key observation is that the number of integer fixed points of is bounded by the number of integer preperiodic points of P. Since is polynomial of degree n, the number of integer preperiodic points of is at most for each iterate. However, we need to consider the composition which is the k-th iterate of P. The polynomial Q(x) is also of degree nk, and the number of integer roots of Q(x) = 0 is at most nk. However, we need to check if it is possible for Q(x) to have more than integer fixed points. Since each application of can introduce at most new fixed points, the total number of integer fixed points of is still bounded by the number of preperiodic points of P, which is at most for each iterate. To be more precise, lets consider the fixed points. If is fixed point of Q, then must be root of the polynomial Pk(x) x. The polynomial Pk(x) is of degree nk, and by the fundamental theorem of algebra, it can have at most nk roots. However, we are interested in integer roots, and the number of integer roots is at most the number of integer preperiodic points of P, which is at most for each iterate. Therefore, the number of integer fixed points of Q(x) is at most n, the same as the number of integer fixed points of for each iterate. Hence, Q(x) cannot have more than integer fixed points. Thus, the statement that has more than integer fixed points is false. </think> <answer> disproved </answer> This is non-cherry-picked example from our DeepTheorem-RL-7B model. We analyze this proof in the following dimensions: DeepTheorem Correctness. The model accurately identifies that integer fixed points of Q(x) satisfy Q(x) = x, and recognizes that Q(x) has degree nk, with Q(x) = 0 having at most nk roots. Its insight that the number of integer fixed points is constrained by the fixed points of P, which are at most n, effectively guides the solution. The model leverages the iterative structure of Q(x) to argue that additional compositions do not increase the number of integer fixed points beyond n, aligning with expected polynomial behavior. Clarity. The proof is well-structured, progressing logically from problem definition to conclusion. Mathematical notation, such as P(m) for the m-th iterate, is precise and standard, enhancing readability. The solution is concise, making it accessible and suitable for mathematical audience. Strengths. The models ability to distill complex problem into clear argument showcases its strength in handling polynomial compositions. Its focus on fixed points and iterative properties demonstrates solid grasp of the underlying mathematics, making it valuable tool for such proofs. In summary, the model delivers clear and correct disproof, highlighting its efficacy in tackling advanced mathematical problems with confidence and precision. Reasoning with theorem proving skills In Figure 8, we visualize the techniques used by our 7B model trained with RL on DeepTheoremdirect proof is most commonly used, followed by proof by exhaustion and construction."
        },
        {
            "title": "5 Related Work",
            "content": "Theorem proving Theorem proving is nontrivial task for natural language processing. In the context of LLMs, theorem proving is typically achieved via formal languages such as Lean, Coq, and Isabelle (Zheng et al., 2022; Liu et al., 2023; Tsoukalas et al., 2024). LLMs specialized at theorem proving have been proposed over the years, represented by GPT-f (Polu & Sutskever, 2020), DeepSeek-Prover (Xin et al., 2024; 2025; Ren et al., 2025), TheoremLlama (Wang et al., 2024), InterLM-StepProver (Wu et al., 2024), and MPS-Prover (Liang et al., 2025). Although most of these works focus on formal theorem proving, NaturalProofs (Welleck et al., 2021) and NaturalProver (Welleck et al., 2022) have emerged as some of the few works that attend to informal theorem proving. However, NaturalProofs use domain-specific language from the ProofWiki website. In comparison, DeepTheorem represents the first attempt at exploring the more commonly used LaTeX-based natural language theorem proving at scale. Figure 8: Distribution of used proof techniques. Learning to reason with RL-zero RL-Zero (DeepSeek-AI et al., 2025) is streamlined framework designed to develop reinforcement learning (RL) capabilities in large language models (LLMs) without SFT. While recent advances in LLM reasoning have been significantly influenced by RL techniques (Jaech et al., 2024; DeepSeek-AI et al., 2025; Team, 2024; xAI, 2025; Google, 2025), existing approaches predominantly focus on closed-form questions, addressing only subset of reasoning problems. In this work, we investigate the application of RL-Zero in process-oriented reasoning, specifically in informal mathematical theorem-proving tasks. To the best of our knowledge, this is the first study to apply the RL-Zero framework to informal theorem proving, marking significant advance in enabling LLMs to address more diversified reasoning tasks in mathematical and logical domains. 12 DeepTheorem"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced DeepTheorem, novel informal theorem-proving paradigm involving large-scale annotated dataset of 121K IMO-level informal mathematical theorems and corresponding high-quality natural-language proofs, alongside systematically constructed verifiable theorem variants. We further proposed novel RL-Zero method specifically tailored for informal mathematical reasoning, significantly surpassing supervised fine-tuning frameworks in theorem proving. Comprehensive evaluations involving outcome accuracy and detailed reasoning process assessment demonstrate the effectiveness of our approach, achieving state-of-the-art theorem-proving performance and significantly pushing LLM reasoning boundaries. Through these contributions, DeepTheorem provides robust foundation for future advancements in automated mathematical theorem proving, leveraging natural language flexibility to empower scalable, human-like reasoning abilities in large language models."
        },
        {
            "title": "References",
            "content": "Daman Arora, Himanshu Singh, and Mausam. Have LLMs advanced enough? challenging problem solving benchmark for large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 75277543, Singapore, December 2023. Association for Computational Linguistics. doi: 10.186 53/v1/2023.emnlp-main.468. URL https://aclanthology.org/2023.emnlp-main.468. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. CoRR, abs/2406.18321, 2024. doi: 10.48550/ARXIV.2406.18321. URL https://doi.org/10.48550/arX iv.2406.18321. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International 13 DeepTheorem Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=yaqPf0KAlN. Google. Gemini 2.0 flash thinking, 2025. URL https://cloud.google.com/vertex-ai/generati ve-ai/docs/thinking. Accessed on March 25, 2025. Harvard-MIT Mathematics Tournament. Hmmt dataset and resources. https://www.hmmt.org/, 2024, 2025. Accessed: 2025-05-12. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 38283850. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.211. URL https://doi.org/10.18653/v1/2024.acl-long.211. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/h ash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. CoRR, abs/2503.24290, 2025. doi: 10.48550/ARXIV.2503.24290. URL https: //doi.org/10.48550/arXiv.2503.24290. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent AI. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/222d2eaf24cf8259a35d6c7130d31 425-Abstract-Datasets_and_Benchmarks_Track.html. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, 14 DeepTheorem Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. URL https://doi.org/10.48550/arXiv.2412.16720. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 38433857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abbee f8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/Numi naMath-1.5](https://github.com/project-numina/aimo-progress-prize/blob/main/rep ort/numina_dataset.pdf), 2024. Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, and Dong Yu. Mpsprover: Advancing stepwise theorem proving by multi-perspective search and data curation, 2025. URL https://arxiv.org/abs/2505.10962. Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, and Qun Liu. FIMO: challenge formal dataset for automated theorem proving. CoRR, abs/2309.04295, 2023. doi: 10.48550/ARXIV.2 309.04295. URL https://doi.org/10.48550/arXiv.2309.04295. Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew C. Yao. Augmenting math word problems via iterative question composing. In Toby Walsh, Julie Shah, and Zico Kolter (eds.), AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 2460524613. AAAI Press, 2025. doi: 10.1609/AAAI.V39I23.34640. URL https://doi.org/10.1609/aaai.v39i23.34640. MAA. American invitational mathematics examination (AIME). Mathematics Competition Series, a. URL https://maa.org/math-competitions/aime. MAA. American mathematics competitions (AMC 10/12). Mathematics Competition Series, b. URL https://maa.org/math-competitions/amc. Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 39803990. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1410. URL https://doi.org/10.18653/v1/D19-1410. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=Ti6758 4b98. 15 DeepTheorem Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, and Chong Ruan. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. CoRR, abs/2504.21801, 2025. URL https://arxiv.org/abs/2504.21801. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, Nov 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=mTCbq2QssD. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. Putnambench: Evaluating neural theorem-provers on the putnam mathematical competition. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/1582eaf9e0cf349e1e5a6ee45 3100aa1-Abstract-Datasets_and_Benchmarks_Track.html. Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. Theoremllama: Transforming general-purpose llms into lean4 experts. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 1195311974. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-main.667. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. CMATH: can your language model pass chinese elementary school math test? CoRR, abs/2306.16636, 2023. doi: 10.48550/ARXIV.230 6.16636. URL https://doi.org/10.48550/arXiv.2306.16636. Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hanna Hajishirzi, Yejin Choi, and Kyunghyun Cho. Naturalproofs: Mathematical theorem proving in natural language. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/d9d4f495e875 a2e075a1a4a6e1b9770f-Abstract-round1.html. Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_f iles/paper/2022/hash/1fc548a8243ad06616eee731e0572927-Abstract-Conference.html. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2.5-stepprover: Advancing automated theorem proving via expert iteration on largescale LEAN problems. CoRR, abs/2410.15700, 2024. doi: 10.48550/ARXIV.2410.15700. URL https://doi.org/10.48550/arXiv.2410.15700. xAI. Grok: Artificial intelligence assistant, 2025. URL https://x.ai. Developed by xAI, accessed on March 25, 2025. 16 DeepTheorem Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through largescale synthetic data. CoRR, abs/2405.14333, 2024. doi: 10.48550/ARXIV.2405.14333. URL https://doi.org/10.48550/arXiv.2405.14333. Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=I4YAIwrsXa. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: large-scale lean problem set formalized from natural language math problems. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/bf236666a2cc5f3ae05d2 e08485efc4c-Abstract-Datasets_and_Benchmarks_Track.html. Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/a 4ca07aa108036f80cbb5b82285fd4b1-Abstract-Conference.html. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. minif2f: cross-system benchmark for formal olympiad-level mathematics. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/for um?id=9ZPegFuFTFv. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 22992314. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.149. URL https://doi.org/10.18653/v1/2024.findings-naacl.149. 17 DeepTheorem"
        },
        {
            "title": "A DeepTheorem Prompt Template",
            "content": "A.1 Contamination Justification Meta Prompt for Contamination Justification will now give you two theorems: the Original Theorem and the Candidate Theorem. Please help me determine if the following two theorems are the same. Original Theorem: ***theorem1*** Candidate Theorem: ***theorem2*** Disregard the names and minor changes in word order. If their theorem prompts are very similar, without considering the proving process, we consider them to be the same theorem. Note that you should not consider the solution process, only the theorem prompts. You should only respond with True or False. Do not respond with anything else. A.2 Theorem-Proving Annotation Meta Prompt for Theorem-Proving Annotation You are an expert in classifying questions based on their type and intent. Given the following discussion: # Discussion - problem: ***problem*** - solution: ***solution*** Determine whether the question is: - question-answering (QA) question seeking specific value or factual response, or - theorem-proving question requiring logical reasoning, derivation, or proof of mathematical or theoretical statement. Provide clear classification (QA or theorem-proving) and justify your decision with concise explanation. Consider the following: - QA questions typically ask for specific fact, value, or definitive answer (e.g., \"What is the capital of France?\" or \"What is the value of in 2x = 8?\"). - Theorem-proving questions typically involve logical reasoning, mathematical derivation, or proving general statement (e.g., \"Prove that the sum of two even numbers is even\" or \"Derive the Pythagorean theorem\"). Return True if the question is theorem-proving question, and False if it is QA question. A.3 Rationale Summarization Meta Prompt for Rationale Summarization You are provided with corpus of forum discussions about mathematical topics. Your task is to analyze the discussion and: 1. Identify the key mathematical concepts, ideas, or rationales driving the discussion. 2. Act as teacher to formulate theorem based on the discussion, presented as formal theorem statement. # Requirements - All mathematical equations must be formatted in LaTeX. - The theorem should be clear, formal statement (e.g., \"Let : be smooth function, ...\"). - The output must be in JSON format, with the following structure: { 18 DeepTheorem \"rationale\": \"A description of the main mathematical concepts or ideas in the discussion.\", \"theorem\": \"A formal theorem statement based on the discussion.\" } # Discussion - problem: ***problem*** - solution: ***solution*** A.4 Proof Generation Meta Prompt for Proof Generation from o3-mini (high) You are provided with corpus of forum discussions about mathematical topics. theorem statement is summarized from the discussion. Your task is to provide proof for the theorem statement based on the discussion. # Requirements - All mathematical equations must be formatted in LaTeX. - The proof should be clear, formal statement (e.g., \"To prove this theorem, we can start by ...\"). - The output must be in JSON format, with the following structure: { \"proof\": \"A proof for the theorem statement.\" } # Discussion - problem: ***problem*** - solution: ***solution*** # Theorem Rationale ***theorem*** A.5 Logical Validation 19 DeepTheorem Meta Prompt for Logical Validation of the Proof You are an expert in mathematical theorem proving and logical analysis. Given the following theorem and its proof or disproof, your task is to analyze each step of the proof or disproof to determine if it is valid, providing detailed justification for each steps correctness or identifying any errors. # Theorem ***theorem*** # Proof or Disproof ***Proof*** # Instructions 1. **Analyze Each Step**: - Verify if the step is mathematically correct, logically sound, and relevant to proving or disproving the theorem. - Check for adherence to mathematical definitions, theorems, or properties cited in the step. - Ensure the step follows from previous steps or given assumptions without logical gaps. - If the step involves disproof, confirm that it correctly demonstrates counterexample or contradiction. 2. **Overall Assessment**: - Conclude whether the entire proof or disproof is valid. - If invalid, return False and summarize the critical errors and recommend how to fix the proof/disproof. - If valid, return True and confirm that it fully addresses the theorem. A.6 Difficulty Annotation Meta Prompt for Difficulty Annotation # CONTEXT # am teacher, and have some high-level olympiad math problems. want to evaluate the difficulty of these math problems. There are some references available regarding the difficulty of the problems: <difficulty reference> ## Examples for difficulty levels For reference, here are problems from each of the difficulty levels 1-10: 1: How many integer values of satisfy < 3π? (2021 Spring AMC 10B, Problem 1) 1.5: number is called flippy if its digits alternate between two distinct digits. For example, 2020 and 37373 are flippy, but 3883 and 123123 are not. How many five-digit flippy numbers are divisible by 15? (2020 AMC 8, Problem 19) 2: fair 6-sided die is repeatedly rolled until an odd number appears. What is the probability that every even number appears at least once before the first occurrence of an odd number? (2021 Spring AMC 10B, Problem 18) 2.5: A, B, are three piles of rocks. The mean weight of the rocks in is 40 pounds, the mean weight of the rocks in is 50 pounds, the mean weight of the rocks in the combined piles and is 43 pounds, and the mean weight of the rocks in the combined piles and is 44 pounds. What is the greatest possible integer value for the mean in pounds of the rocks in the combined piles and C? (2013 AMC 12A, Problem 16) 3: Triangle ABC with AB = 50 and AC = 10 has area 120. Let be the midpoint of AB, and let be the midpoint of AC. The angle bisector of BAC intersects DE and BC at and G, respectively. What is the area of quadrilateral FDBG? (2018 AMC 10A, Problem 24) 20 DeepTheorem 3.5: Find the number of integer values of in the closed interval [500, 500] for which the equation log(kx) = 2 log(x + 2) has exactly one real solution. (2017 AIME II, Problem 7) 4: Define sequence recursively by x0 = 5 and for all nonnegative integers n. Let be the least positive integer such that xn+1 = x2 + 5xn + 4 xn + xm 4 + 1 220 . (B) [27, 80] In which of the following intervals does lie? (A) [9, 26] Problem 24 and 2019 AMC 12B, Problem 22) 4.5: Find, with proof, all positive integers for which 2n + 12n + 2011n is perfect square. (USAJMO 2011/1) 5: Find all triples (a, b, c) of real numbers such that the following system holds: (E) [729, ) (2019 AMC 10B, (D) [243, 728] (C) [81, 242] + + = a2 + b2 + c2 = 1 + 1 + 1 , 1 a2 + 1 + 1 c2 . (JBMO 2020/1) 5.5: Triangle ABC has BAC = 60, CBA 90, BC = 1, and AC AB. Let H, I, and be the orthocenter, incenter, and circumcenter of ABC, respectively. Assume that the area of pentagon BCOI is the maximum possible. What is CBA? (2011 AMC 12A, Problem 25) 6: Let ABC be an acute triangle with circumcircle ω, and let be the intersection of the altitudes of ABC. Suppose the tangent to the circumcircle of HBC at intersects ω at points and with = 3, HX = 2, and HY = 6. The area of ABC can be written in the form n, where and are positive integers, and is not divisible by the square of any prime. Find + n. (2020 AIME I, Problem 15) 6.5: Rectangles BCC1B2, CAA1C2, and ABB1 A2 are erected outside an acute triangle ABC. Suppose that BC1C + CA1 + AB1B = 180. Prove that lines B1C2, C1 A2, and A1B2 are concurrent. (USAMO 2021/1, USAJMO 2021/2) 7: We say that finite set in the plane is balanced if, for any two different points A, in S, there is point in such that AC = BC. We say that is centre-free if for any three points A, B, in S, there is no point in such that PA = PB = PC. Show that for all integers 3, there exists balanced set consisting of points. Determine all integers 3 for which there exists balanced centre-free set consisting of points. (IMO 2015/1) 7.5: Let be the set of integers. Find all functions : such that (2 (y) x) + y2 (2x (y)) = (x)2 + (y (y)) for all x, with = 0. (USAMO 2014/2) 8: For each positive integer n, the Bank of Cape Town issues coins of denomination 1 . Given finite collection of such coins (of not necessarily different denominations) with total value at most most 99 + 1 2 , prove that it is possible to split this collection into 100 or fewer groups, such that each group has total value at most 1. (IMO 2014/5) 8.5: Let be the incentre of acute triangle ABC with AB = AC. The incircle ω of ABC is tangent to sides BC, CA, and AB at D, E, and F, respectively. The line through perpendicular to EF meets ω at R. Line AR meets ω again at P. The circumcircles of triangle PCE and PBF meet again at Q. Prove that lines DI and PQ meet on the line through perpendicular to AI. (IMO 2019/6) 21 DeepTheorem 9: Let be positive integer and let be finite set of odd prime numbers. Prove that there is at most one way (up to rotation and reflection) to place the elements of around the circle such that the product of any two neighbors is of the form x2 + + for some positive integer x. (IMO 2022/3) 9.5: An anti-Pascal triangle is an equilateral triangular array of numbers such that, except for the numbers in the bottom row, each number is the absolute value of the difference of the two numbers immediately below it. For example, the following is an anti-Pascal triangle with four rows which contains every integer from 1 to 10. 4 7 2 3 6 10 9 5 8 Does there exist an anti-Pascal triangle with 2018 rows which contains every integer from 1 to 1 + 2 + 3 + + 2018? (IMO 2018/3) 10: Prove that there exists positive constant such that the following statement is true: Consider an integer > 1, and set of points in the plane such that the distance between any two different points in is at least 1. It follows that there is line ℓ separating such that the distance from any point of to ℓ is at least cn1/3. 4 and ## Some known difficulty ratings of the competitions. ### HMMT (November) Individual Round, Problem 6-8: 4 Individual Round, Problem 10: 4.5 Team Round: 4-5 Guts: 3.5-5.25 ### CEMC **Part A: 1-1.5** How many different 3-digit whole numbers can be formed using the digits 4, 7, and 9, assuming that no digit can be repeated in number? (2015 Gauss 7 Problem 10) **Part B: 1-2** Two lines with slopes 1 4 intersect at (1, 1). What is the area of the triangle formed by these two lines and the vertical line = 5? (2017 Cayley Problem 19) Part (Gauss/Pascal): 2-2.5 Suppose that 2009 in lowest terms. What is the sum of the digits of the smallest positive integer for which is multiple of 1004? (2014 Pascal Problem 25) **Part (Cayley/Fermat): 2.5-3** Wayne has 3 green buckets, 3 red buckets, 3 blue buckets, and 3 yellow buckets. He randomly distributes 4 hockey pucks among the green buckets, with each puck equally likely to be put in each bucket. Similarly, he distributes 3 pucks among the red buckets, 2 pucks among the blue buckets, and 1 puck among the yellow buckets. Once he is finished, what is the probability that green bucket contains more pucks than each of the other 11 buckets? (2018 Fermat Problem 24) ### Indonesia MO **Problem 1/5: 3.5** In drawer, there are at most 2009 balls, some of them are white, the rest are blue, which are randomly distributed. If two balls were taken at the same time, then the probability that the balls are both blue or both white is 1 2 . Determine the maximum amount of white balls in the drawer, such that the probability statement is true? , where a, b, and are positive integers with 2014 + 2019 = 22 DeepTheorem **Problem 2/6: 4.5** Find the lowest possible values from the function (x) = x2008 2x2007 + 3x2006 4x2005 + 5x2004 2006x3 + 2007x2 2008x + 2009 for any real numbers x. **Problem 3/7: 5** pair of integers (m, n) is called good if n2 + and m2 + Given 2 positive integers a, > 1 which are relatively prime, prove that there exists good pair (m, n) with and n, but and m. **Problem 4/8: 6** Given an acute triangle ABC. The incircle of triangle ABC touches BC, CA, AB respectively at D, E, F. The angle bisector of cuts DE and DF respectively at and L. Suppose AA1 is one of the altitudes of triangle ABC, and be the midpoint of BC. (a) Prove that BK and CL are perpendicular with the angle bisector of BAC. (b) Show that A1KML is cyclic quadrilateral. ### JBMO **Problem 1: 4** Find all real numbers a, b, c, such that r+1 = 1. + + + = 20, ab + ac + ad + bc + bd + cd = 150. **Problem 2: 4.5-5** Let ABCD be convex quadrilateral with DAC = BDC = 36, CBD = 18 and BAC = 72. The diagonals intersect at point P. Determine the measure of APD. **Problem 3: 5** Find all prime numbers p, q, r, such that **Problem 4: 6** 4 4 table is divided into 16 white unit square cells. Two cells are called neighbors if they share common side. move consists in choosing cell and changing the colors of neighbors from white to black or from black to white. After exactly moves all the 16 cells were black. Find all possible values of n. ### Problem 1/4: 5 There are + bowls arranged in row, numbered 1 through + b, where and are given positive integers. Initially, each of the first bowls contains an apple, and each of the last bowls contains pear. legal move consists of moving an apple from bowl to bowl + 1 and pear from bowl to bowl 1, provided that the difference is even. We permit multiple fruits in the same bowl at the same time. The goal is to end up with the first bowls each containing pear and the last bowls each containing an apple. Show that this is possible if and only if the product ab is even. **Problem 2/5: 6-6.5** Let a, b, be positive real numbers such that + + = 4 3 that abc. Prove 2(ab + bc + ca) + 4 min(a2, b2, c2) a2 + b2 + c2. **Problem 3/6: 7** Two rational numbers and are written on blackboard, where and are relatively prime positive integers. At any point, Evan may pick two of the numbers and written on the board and write either their arithmetic mean x+y 2 or their harmonic mean 2xy x+y on the board as well. Find all pairs (m, n) such that Evan can write 1 on the board in finitely many steps. ### HMMT (February) Individual Round, Problem 1-5: 5 Individual Round, Problem 6-10: 5.5-6 Team Round: 7.5 HMIC: 8 ### APMO Problem 1: 6 Problem 2: 7 Problem 3: 7 Problem 4: 7.5 Problem 5: 8.5 ### Balkan MO Problem 1: 5 Solve the equation 3x 5y = z2 in positive integers. Problem 2: 6.5 Let MN be line parallel to the side BC of triangle ABC, with on the side AB and on the side AC. The lines BN and CM meet at point P. The circumcircles of triangles BMP and CNP meet at two distinct points and Q. Prove that BAQ = CAP. Problem 3: 7.5 9 12 rectangle is partitioned into unit squares. The centers of all the unit squares, except for the four corner squares and eight squares sharing common side with one of them, are coloured red. Is it possible to label these red centres C1, C2..., C96 in such way that the following to conditions are both fulfilled (i) the distances C1C2, ...C95C96, C96C1 are all 23 DeepTheorem 13 (ii) the closed broken line C1C2...C96C1 has centre of symmetry? Problem 4: 8 equal to Denote by the set of all positive integers. Find all functions : such that (cid:19) (cid:18) 2(m) + 2 2(n) = m2 + 2n2 for all m, S. ### USAMO Problem 1/4: 6-7 Problem 2/5: 7-8 Three nonnegative real numbers r1, r2, r3 are written on blackboard. These numbers have the property that there exist integers a1, a2, a3, not all zero, satisfying a1r1 + a2r2 + a3r3 = 0. We are permitted to perform the following operation: find two numbers x, on the blackboard with y, then erase and write in its place. Prove that after finite number of such operations, we can end up with at least one 0 on the blackboard. Problem 3/6: 8-9 Prove that any monic polynomial (a polynomial with leading coefficient 1) of degree with real coefficients is the average of two monic polynomials of degree with real roots. ### USA TST Problem 1/4/7: 6.5-7 Problem 2/5/8: 7.5-8 Problem 3/6/9: 8.5-9 ### Putnam Problem A/B,1-2: 7 Find the least possible area of concave set in the 7-D plane that intersects both branches of the hyperparabola xyz = 1 and both branches of the hyperbola xwy = 1. (A set in the plane is called convex if for any two points in the line segment connecting them is contained in S.) Problem A/B,3-4: 8 Let be an matrix all of whose entries are 1 and whose rows are mutually orthogonal. Suppose has an submatrix whose entries are all 1. Show that ab n. Problem A/B,5-6: 9 For any > 0, define the set S(a) = {[an]n = 1, 2, 3, ...}. Show that there are no three positive reals a, b, such that S(a) S(b) = S(b) S(c) = S(c) S(a) = , S(a) S(b) S(c) = {1, 2, 3, ...}. ### China TST (hardest problems) Problem 1/4: 8-8.5 Given an integer m, prove that there exist odd integers a, and positive integer such that 2m = a19 + b99 + 21000. Problem 2/5: 9 Given positive integer = 1 and real numbers a1 < a2 < . . . < an, such that + + . . . + 1, prove that for any positive real number x, 1 a1 1 a2 1 an (cid:32) 1 a2 1 + + 1 a2 2 + + . . . + (cid:33)2 1 + 1 2a1(a1 1) + 2x . Problem 3/6: 9.5-10 Let > 1 be an integer and let a0, a1, . . . , an be non-negative real numbers. Define Sk = i)ai for = 0, 1, . . . , n. Prove that i=0 (k 1 n1 k=0 S2 (cid:33)2 1 n2 (cid:32) k= Sk 4 45 (Sn S0)2. ### IMO **Problem 1/4: 5.5-7** Let Γ be the circumcircle of acute triangle ABC. Points and are on segments AB and AC respectively such that AD = AE. The perpendicular bisectors of BD and CE intersect minor arcs AB and AC of Γ at points and respectively. Prove that lines DE and FG are either parallel or they are the same line. **Problem 2/5: 7-8** Let P(x) be polynomial of degree > 1 with integer coefficients, and let be positive integer. Consider the polynomial Q(x) = P(P(. . . P(P(x)) . . .)), where occurs times. Prove that there are at most integers such that Q(t) = t. **Problem 3/6: 9-10** Let ABC be an equilateral triangle. Let A1, B1, C1 be interior points of ABC such that BA1 = A1C, CB1 = B1 A, AC1 = C1B, and BA1C + CB1 + AC1B = 480 Let BC1 and CB1 meet at A2, let CA1 and AC1 meet at B2, and let AB1 and BA1 meet at C2. Prove that if triangle A1B1C1 is scalene, then the three circumcircles of triangles AA1 A2, BB1B2 and CC1C2 all pass through two common points. ### IMO Shortlist 24 DeepTheorem Problem 1-2: 5.5-7 Problem 3-4: 7-8 Problem 5+: 9-10 </difficulty reference> # OBJECTIVE # 1. Summarize the math problem in brief sentence, describing the concepts involved in the math problem. 2. Based on the source of the given problem, as well as the difficulty of the problems referenced in these materials and the solution to the current problem, please provide an overall difficulty score for the current problem. The score should be number between 1 and 10, with increments of 0.5, and should align perfectly with the materials. # STYLE # Data report. # TONE # Professional, scientific. # AUDIENCE # Students. Enable them to better understand the difficulty of the math problems. # RESPONSE: MARKDOWN REPORT # ## Summarization [Summarize the math problem in brief paragraph.] ## Difficulty [Rate the difficulty of the math problem and give the reason.] # ATTENTION # - Add \"=== report over ===\" at the end of the report. 6 . Therefore, the number that replaces the is 6. , what is the number that replaces the to make the equation true? <example math problem> [Question]: If 1 18 = 1 9 + 1 [Solution]: We simplify the left side and express it as fraction with numerator 1: 1 3 18 = 1 [Source]: 2010_Pascal </example math problem> ## Summarization The problem requires finding value that makes the equation 1 adding two fractions and determining the equivalent fraction. ## Difficulty Rating: 1 Reason: This problem is straightforward and primarily involves basic fraction addition, making it suitable for early middle school students. === report over === . This involves 18 = 1 18 = 2 18 + 1 9 + 9 + 1 18 = <example math problem> [Question]: Let be convex polygon with sides, 3. Any set of 3 diagonals of that do not intersect in the interior of the polygon determine triangulation of into 2 triangles. If is regular and there is triangulation of consisting of only isosceles triangles, find all the possible values of n. [Solution]: 25 DeepTheorem We label the vertices of as P0, P1, P2, . . . , Pn. Consider diagonal = Pa Pa+k, n/2 in the triangulation. We show that must have the form 2m for some nonnegative integer m. This diagonal partitions into two regions Q, R, and is the side of an isosceles triangle in both regions. Without loss of generality suppose the area of is less than the area of (so the center of does not lie in the interior of Q); it follows that the lengths of the edges and diagonals in are all smaller than d. Thus must the be the base of the isosceles triangle in Q, from which it follows that the isosceles triangle is PaPa+k/2 Pa+k, and so 2k. Repeating this process on the legs of isosceles triangle (PaPa+k/2, Pa+kPa+k/2), it follows that = 2m for some positive integer (if we allow degeneracy, then we can also let = 0). Now take the isosceles triangle PxPyPz, 0 < < < in the triangulation that contains the center of in its interior; if diagonal passes through the center, select either of the isosceles triangles with that diagonal as an edge. Without loss of generality, suppose PxPy = PyPz. From our previous result, it follows that there are 2a edges of on the minor arcs of PxPy, PyPz and 2b edges of on the minor arc of PzPx, for positive integers a, b. Therefore, we can write = 2 2a + 2b = 2a+1 + 2b, so must be the sum of two powers of 2. We now claim that this condition is sufficient. Suppose without loss of generality that + 1 b; then we rewrite this as = 2b(2ab+1 + 1). Lemma 1: All regular polygons with = 2k + 1 or = 4 have triangulations that meet the conditions. By induction, it follows that we can cover all the desired n. For = 3, 4, this is trivial. For > 1, we construct the diagonals of equal length P0P2k1 and P2k1+1P0. This partitions into 3 regions: an isosceles P0P2k1 P2k1+1, and two other regions. For these two regions, we can recursively construct the isosceles triangles defined above in the second paragraph. It follows that we have constructed 2(2k1 1) + (1) = 2k 1 = 2 isosceles triangles with non-intersecting diagonals, as desired. Lemma 2: If regular polygon with sides has working triangulation, then the regular polygon with 2n sides also has triangulation that meets the conditions. We construct the diagonals P0P2, P2P4, . . . P2n2P0. This partitions into isosceles triangles of the form P2kP2k+1P2k+2, as well as central regular polygon with sides. However, we know that there exists triangulation for the n-sided polygon that yields 2 isosceles triangles. Thus, we have created (n) + (n 2) = 2n 2 isosceles triangles with non-intersecting diagonals, as desired. In summary, the answer is all that can be written in the form 2a+1 + 2b, a, 0. Alternatively, this condition can be expressed as either = 2k, 2 (this is the case when + 1 = b) or is the sum of two distinct powers of 2, where 1 = 20 is considered power of 2. [Source]: USAMO 2008 </example math problem> ## Summarization The problem asks for the possible values of for regular n-sided polygon that can be completely triangulated into isosceles triangles using non-intersecting diagonals. The solution involves analyzing the properties of the diagonals forming isosceles triangles and deducing that can be expressed in terms of powers of 2. ## Difficulty Rating: 7 Reason: The problem involves understanding properties of isosceles triangles in the context of polygon triangulation and requires critical reasoning to establish relationships between the number of sides and powers of 2, making it more complex than typical undergraduate-level problems. === report over === 26 DeepTheorem <math problem> [QUESTION]: ***Question*** [SOLUTION]: ***Solution*** [SOURCE]: ***SOURCE*** </math problem> Your answer should be in JSON format for example: json { \"Rating\": YOUR RATING, \"Reason\": YOUR JUSTIFICATION, } A.7 Topic Domain Meta Prompt for Topic Domain Annotation am teacher, and have some high-level Olympiad math problems. want to categorize the domain of these math problems. # OBJECTIVE # 1. Summarize the math problem in brief sentence, describing the concepts involved in the math problem. 2. Categorize the math problem into specific mathematical domains. Please provide classification chain, for example, Applied Mathematics -> Probability -> Combinations. The following is basic classification framework in the field of mathematics. <math domains> ... </math domains> # STYLE # Data report. # TONE # Professional, scientific. # AUDIENCE # Students. Enable them to better understand the domain of the problems. # RESPONSE: MARKDOWN REPORT # ## Summarization [Summarize the math problem in brief paragraph.] ## Math domains [Categorize the math problem into specific mathematical domains, including major domains and subdomains.] # ATTENTION # - The math problem can be categorized into multiple domains, but no more than three. Separate the classification chains with semicolons(;). - Your classification MUST fall under one of the aforementioned subfields; if it really does not fit, please add \"Other\" to the corresponding branch. For example: Algebra -> Intermediate Algebra -> Other. Only the LAST NODE is allowed to be \"Other\"; the preceding nodes must strictly conform to the existing framework. 27 DeepTheorem - The math domain must conform to format of classification chain, like \"Applied Mathematics -> Probability -> Combinations\". - Add \"=== report over ===\" at the end of the report. <example math problem>. [Question] Determine the greatest real number C, such that for every positive integer 2, there exists x1, x2, ..., xn [1, 1], so that 1i<jn (xi xj) n(n1) 2 </example math problem> ## Summarization The problem seeks to find the greatest real number such that, for every integer 2, there exist real numbers x1, x2, . . . , xn [1, 1] satisfying the inequality 1i<jn(xi xj) . This involves maximizing to ensure the product of all pairwise differences among points in the interval [1, 1] is at least raised to the power of the number of such pairs, n(n1) 2 n(n1) 2 . ## Math domains Algebra -> Intermediate Algebra -> Inequalities; Discrete Mathematics -> Combinatorics === report over === <example math problem> [Question] Given integer 2. Find the minimum value of λ, satisfy that for any real numbers a1, a2, , an and b, (cid:113) λ i=1 ai + (cid:118) (cid:117) (cid:117) (cid:116)n (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i=1 ai (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:113) ai. i=1 </example math problem> ## Summarization Let 2 be an integer. The problem seeks the minimum value of λ such that for any real (cid:112)ai numbers a1, a2, . . . , an and b, the inequality λ holds. The goal is to find the smallest λ that ensures this inequality is satisfied for all possible choices of ai and b. ## Math domains Algebra -> Intermediate Algebra -> Inequalities; Calculus -> Differential Calculus -> Applications of Derivatives. === report over === (cid:112)ai + (cid:112)n i=1 ai i=1 i= <math problem> [Question] ***Question*** [Solution] ***Solution*** [Source] ***Source*** </math problem> Your answer should be in JSON format for example: \"\"\" json 28 DeepTheorem { \"Summary\": \"YOUR_SUMMARY\", \"Domains\": [domain1, ...] } \"\"\" A.8 Theorem Variant Construction Meta Prompt for Constructing Theorem Variants Im going to give you math proof question and its solution. Your task is to follow the steps below to write new question based on the given one. Here is the original question and solution: question <question> , solution <solution> Please follow these steps: 1. The original question asks to \"prove or disprove\" statement, where the statement can be \"proved\". Please write new question by negating the original statement, so that it can now be \"disproved\". For example, if the original statement is = y, you may change it to < or = y; if the statement is \"there exists xxx\", you may change it to \"there does not exist xxx\". When negating the original question, you should make minimal changes, i.e. leave as much background information unchanged as possible. 2. After changing the question, the solution should be changed accordingly. You do not have to write new solution, and the original solution can probably be reused. For example, if the original question asks to prove = and the new question asks to prove < y, you may simply add step to the original proof like \"since we proved = y, the statement < is disproved\". However, check the wording of the solution so that it tries to \"prove\" the statement at first, and then naturally transit to finding that it cannot be proved, but can be disproved instead. 3. The original solution ends with \"boxed{proved}\". Your new solution should end with \"boxed{disproved}\". Output the new question and solution in two blocks: question new question solution corresponding solution 29 DeepTheorem A.9 System Prompt for RL Training System Prompt for RL Training conversation between User and Assistant. The User gives statement, and the Assistant either proves or disproves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and the answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. If you prove the statement, answer with \"proved\". If you disprove the statement, answer with \"disproved\". You must put your answer inside <answer> </answer> tags, i.e., <answer> boxed{proved} or boxed{disproved} </answer>. And your final answer will be extracted automatically by the boxed{} tag. A.10 Process Evaluation Framework Meta Prompt for Process Evaluation You are an expert in scoring solutions for mathematical proof questions. The following question asks to prove or disprove statement, where the statement may be either true or false. The test subject is asked to end their proof with boxed{proved} if they prove the statement to be true, and boxed{disproved} if they prove the statement to be false. The question: <question> The ground truth of the statement: <answer> The test subjects solution: <solution> Your task is to evaluate the proofs quality and assign score from 0 to 1 based on four criteria: logical validity (40%), completeness (30%), correctness (20%), and clarity (10%). Instructions: 1. Analyze the proof step by step. 2. For each criterion: - Logical Validity: Check if each step follows logically from the previous one. Flag any logical errors. - Completeness: Verify if all necessary cases and steps are included to prove the theorem. - Correctness: Confirm if the final conclusion is correct. - Clarity: Assess if the proof is clear, unambiguous, and well-explained. 3. Assign sub-score (0 to 1) for each criterion and compute the total score using the weights: (0.4 validity) + (0.3 completeness) + (0.2 correctness) + (0.1 clarity). 4. Provide brief explanation (2-3 sentences) summarizing any errors or issues and justifying the score. Final output format: { \"score\": float, \"validity\": float, \"completeness\": float, \"correctness\": float, \"clarity\": float, 30 DeepTheorem \"explanation\": str } where \"score\" is the total score, and \"validity\", \"completeness\", \"correctness\", \"clarity\" are the subscores."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tencent"
    ]
}