{
    "paper_title": "Large Language Model Evaluation via Matrix Nuclear-Norm",
    "authors": [
        "Yahan Li",
        "Tingyu Xia",
        "Yi Chang",
        "Yuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 2 7 6 0 1 . 0 1 4 2 : r Large Language Model Evaluation via Matrix Nuclear-Norm Yahan Li1, Tingyu Xia1, Yi Chang1,2,3, Yuan Wu1,2 1School of Artificial Intelligence, Jilin University 2Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University 3International Center of Future Science, Jilin University yahan23@mails.jlu.edu.cn, xiaty21@mails.jlu.edu.cn, yichang@jlu.edu.cn yuanwu@jlu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their O(n3) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as metric to quantify the data compression proficiency of LLM but also provides convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the L1,2-norm to further approximate the nuclear norm, we can effectively assess the models information compression capabilities. This approach reduces the time complexity to O(n2) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is reliable, scalable, and efficient tool for assessing LLMs performance, striking balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs), such as Gemini [20], Llama [47], and GPT-4 [21], have demonstrated remarkable performance across variety of natural language processing (NLP) tasks [59]. They are not only transforming the landscape of NLP [39, 32, 40] but also bring beneficial impacts on computer vision [29, 51] and graph neural networks [57, 8], achieving stellar results on various leaderboards. Despite these advancements, assessing models ability to compress information remains crucial research challenge [13]. Compression focuses on efficiently distilling essential information from vast training datasets while discarding redundant elements, showcasing models ability to learn and recognize the underlying structure of the data [53]. LLMs are expected to perform this form of compression during their training process [59]. Specifically, in the early stages of training, after random initialization, the representations produced from the data are often chaotic. However, as training progresses, these representations become more organized, allowing the model to filter out unnecessary information. Hence, assessing an LLMs capacity for information compression is crucial for understanding its learning efficiency and representational power. Corresponding author Preprint. Under review. Current compression evaluation methods, such as Matrix Entropy introduced by Wei et al. [53], measure information compression efficiency through processing models output representations on datasets. However, the reliance of Matrix Entropy on Singular Value Decomposition (SVD) [28, 58] leads to significant computational complexity, typically O(n3), which limits its applicability in large-scale models. To tackle this challenge, we propose novel evaluation metric called Matrix Nuclear-Norm. This metric effectively measures predictive discriminability and captures output diversity, serving as an upper bound for the Frobenius norm and providing convex approximation of the matrix rank. Furthermore, we enhance the Matrix Nuclear-Norm by employing the L1,2-norm to approximate the nuclear norm, addressing stability issues during evaluation across multiple classes. This approach enables an efficient assessment of models compression capabilities and redundancy elimination abilities, streamlining the evaluation process. Notably, the Matrix Nuclear-Norm achieves computational complexity of O(n2), significant improvement over Matrix Entropys O(n3). This reduction facilitates faster evaluations, making the Matrix Nuclear-Norm practical choice for large-scale models while maintaining accuracy. To validate the effectiveness of the Matrix Nuclear-Norm, we first conducted preliminary experiments on two language models of differing sizes. The results indicated consistent decrease in Matrix Nuclear-Norm values as model size increased, signifying enhanced compression capabilities. Subsequently, we performed inference experiments on two widely used benchmark datasets, AlpacaEval [16] and Chatbot Arena [10], which cover diverse range of language generation tasks. These benchmarks facilitate comprehensive assessment of model inference performance. Our experimental findings confirm that the Matrix Nuclear-Norm accurately measures model compression capabilities and effectively ranks models based on performance, demonstrating its reliability and efficiency in practical applications. Our empirical investigations yield the following insights: 1. Proposal of the Matrix Nuclear-Norm: We present new method that leverages the nuclear norm, successfully reducing the computational complexity associated with evaluating language models from O(n3) to O(n2). This reduction minimizes dependence on SVD, making the Matrix Nuclear-Norm more efficient alternative to Matrix Entropy. 2. Extensive Experimental Validation: We validated the effectiveness of the Matrix NuclearNorm on language models of various sizes. Results indicate that this metric accurately assesses model compression capabilities, with values decreasing as model size increases, reflecting its robust evaluation capability. 3. Benchmark Testing and Ranking: We conducted inference tests on widely used benchmark datasets, AlpacaEval and Chatbot Arena, evaluating the inference performance of models across different sizes and ranking them based on the Matrix Nuclear-Norm. The results demonstrate that this metric can efficiently and accurately evaluate the inference performance of medium and small-scale models, highlighting its broad application potential in model performance assessment."
        },
        {
            "title": "2 Related Work",
            "content": "Evaluating Large Language Models: Metrics and Innovations. The evaluation of large language models (LLMs) is rapidly evolving, driven by diverse tasks, datasets, and benchmarks [50, 5, 60, 6]. Effective evaluation is crucial for enhancing model performance and reliability. Traditional metrics, such as accuracy, F1 score [38], BLEU [36], and ROUGE [31], assess model outputs against annotated labels, while perplexity and cross-entropy loss focus on input data. Recent innovations, like Matrix Entropy [53], provide means to evaluate information compression through activation matrix entropy, shedding light on internal representations. However, the computational intensity of Matrix Entropy, reliant on SVD, highlights the urgent requirement for developing more efficient evaluation methods in the context of LLMs. Scaling Laws and Performance Metrics. Recent studies have consistently shown that as models and datasets expand, performance metrics such as loss improve predictably [24, 27, 11]. This trend is observed across various domains, including NLP and computer vision [25, 1]. Consequently, both industry and academia have prioritized the development of larger models [52, 47]. Notably, scaling laws reveal power-law relationship in performance enhancement. In addition to traditional metrics 2 like loss and task accuracy [56, 14, 3], we introduce the Matrix Nuclear-Norm as novel metric for assessing information compression and redundancy reduction in LLMs. Insights from Information Theory for Large Language Models. Information theory has significantly enhanced our understanding of neural networks. The information bottleneck concept has been pivotal in elucidating supervised learning dynamics [46]. Recently, researchers have applied information-theoretic principles to self-supervised learning in vision tasks [44, 42]. Additionally, lossless compression techniques for LLMs, utilizing arithmetic coding, have been explored [7, 48, 13]. In this paper, we build on these foundations to introduce the Matrix Nuclear-Norm, an information-theoretic metric aimed at effectively evaluating the information compression and redundancy reduction capabilities of LLMs."
        },
        {
            "title": "3 Preliminaries",
            "content": "This section presents the fundamental concepts and metrics used in our study to assess model performance, specifically focusing on discriminability, diversity, and the nuclear norm. 3.1 Discriminability Measurement: F-norm Higher discriminability corresponds to lower prediction uncertainty in the response matrix A, quantified using Shannon Entropy [41]: H(A) = 1 (cid:88) (cid:88) i=1 j=1 Ai,j log(Ai,j). (1) In this equation, represents the number of samples, while denotes the number of distinct categories. Minimizing H(A) results in maximal discriminability [49, 33], achieved when each row contains only one entry equal to 1 and all other entries equal to 0, indicating complete prediction certainty. Alternatively, we can focus on maximizing the Frobenius norm AF : AF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) i=1 j= Ai,j2. (2) The Frobenius norm serves as measure of the overall magnitude of the entries in and also reflects prediction certainty. Theorem 1. Given matrix A, H(A) and AF are strictly inversely monotonic. The proof is detailed in Sect A.4 of the Appendix. Thus, minimizing H(A) is equivalent to maximizing AF . The bounds for AF are: (cid:114) AF B, (3) where the upper and lower bounds correspond to maximum and minimum discriminability, respectively. 3.2 Diversity Measurement: Matrix Rank Prediction diversity involves the number of unique categories in matrix A, denoted as Cp(A). The expected value EC represents the average number of categories: In practice, Cp(A) can be approximated by the rank of when AF is near its upper bound: EC = EAD (Cp(A)) . Cp(A) rank(A). (4) (5) 3.3 Nuclear Norm The nuclear norm is an important measure related to diversity and discriminability. Theorem 2. When AF 1, the convex envelope of rank(A) is the nuclear norm A. The theorem is proved in Fazel [17]. The nuclear norm A, defined as the sum of singular values of A, has significant implications for assessing model performance. With AF B, we have: AF AF , 1 (6) where = min(B, C). Therefore, maximizing ensures high diversity and discriminability. The upper bound of is given by: B. (7)"
        },
        {
            "title": "4 Methodology",
            "content": "4.1 Motivation This section introduces the Matrix Nuclear-Norm, novel metric designed to enhance the efficiency of model evaluation. Traditional nuclear norm calculations rely on computing all singular values, which typically involves the computationally intensive SVD. This method not only consumes significant time for large-scale data but may also fail to converge in certain cases, severely impacting practical application efficiency. Therefore, we propose the Matrix Nuclear-Norm, which utilizes the L1,2-norm to approximate the nuclear norm, effectively eliminating computational bottlenecks. This innovation significantly reduces computational demands and ensures scalability, providing robust framework for the LLM evaluation. 4.2 Matrix Nuclear-Norm Calculating the nuclear norm of matrix requires calculating its SVD, with time complexity of (cid:0)min(B2C, BC 2)(cid:1), simplifying to O(n3), where represents the maximum of dimensions and C. While this computation is manageable for smaller dimensions, it becomes increasingly time-consuming as dimensions grow. Furthermore, SVD may not converge in some cases, it is thus important to develop more efficient approximations of singular values. Since is typically sparse, with only few non-zero responses in each category, its singular values can be approximated by aggregating these non-zero responses. B, the j-th largest singular value σj can be Theorem 3. When AF approaches the upper bound approximated as: (cid:32) (cid:88) (cid:33) σj top i,j, , {1, . . . , D}. (8) The proof is provided in Sect. A.5 of the Supplementary Materials. Using this approximation, the batch nuclear norm can be efficiently calculated as: i= ˆA = (cid:88) j=1 top (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 A2 i,j, . (9) This approach indicates that the primary components of the L1,2-norm can effectively approximate the nuclear norm when AF is close to B, while other components can be considered noise. Compared to traditional SVD-based methods, such as Guo et al. [23], our Matrix Nuclear-Norm calculation offers two main advantages: (1) it reduces computational complexity from O(n3) to O(n2), and 4 (2) it relies solely on ordinary floating-point calculations, avoiding risks of non-convergence. The complete algorithm for Matrix Nuclear-Norm is detailed in Algorithm 1. Definition of Matrix Nuclear-Norm. The approach can ultimately be expressed as: Matrix Nuclear-Norm(X) = (cid:80)D i=1 (cid:16)(cid:113)(cid:80)m j=1 2 i,j (cid:17)"
        },
        {
            "title": "Linput",
            "content": "(10) Here, Linput denotes the length of the input sequence, ensuring comparability through normalization. Our observations indicate that Matrix Nuclear-Norm values increase with longer sequences; further details can be found in Section 5.3.2. Algorithm 1 Algorithm of Matrix Nuclear-Norm Require: Sentence representations (hidden states from LLM) = {Xi}m the hidden dimension of representation, and Linput is the length of the sentence. i=1, where xi Rd1, is (cid:80)m 1: µ = 1 i=1 xi 2: Xnorm = Xiµ Xiµ2,row (cid:113)(cid:80)m i=1 X2 3: L2(Xnorm) = i,j 4: ΣD = {σ1, σ2, . . . , σD} 5: Matrix Nuclear-Norm(X) = 6: return Matrix Nuclear-Norm // Calculate the mean embedding // Normalize the activation matrix // Calculate L2-norm for each column // Sort L2-norm and select top // Calculate Matrix Nuclear-Norm (cid:80)D i=1( (cid:80)m Linput j=1 2 j,i)"
        },
        {
            "title": "5 Experiments of Large Language Models",
            "content": "5.1 Implementation Details 5.1.1 Baselines Cross-Entropy Loss. For text sequence = {u1, . . . , uT }, the cross-entropy loss is defined as follows [55]: L(U ) = 1 T (cid:88) i=1 log (uiu<i; Θ) (11) This loss quantifies the models ability to predict ui based on preceding words u<i. We evaluate the performance of the Matrix Entropy metric in comparison to this baseline, utilizing the same datasets and models as referenced in [27]. Perplexity. The perplexity of language model for text sequence = {u1, . . . , uT } is defined as [35]: (cid:32) L(U ) = exp (cid:33) log (uiu<i; Θ) 1 T (cid:88) i=1 (12) Perplexity measures the models capacity to predict the next word, reflecting the average number of attempts required for correct predictions. Matrix Entropy of Dataset. Given dataset = {Si}n i=1, where Si represents the samples (sentence embeddings) within the dataset, the (normalized) matrix entropy of is defined as [53]: H(D) = (cid:80)n i=1 H(ΣSi) log . 5 (13) 5.1.2 Language Models In our experiments, we selected range of widely used transformer-based LLMs. Notably, we included Cerebras-GPT [19], pre-trained model well-suited for studying scaling laws. The selection of Cerebras-GPT is particularly advantageous due to its diverse model sizes, which span from 111 million to 13 billion parameters. This diversity allows for comprehensive analysis of pre-trained language models across varying scales. Additionally, we utilized various scaled versions of the Pythia model [3], ranging from 14 million to 12 billion parameters, to further examine performance variations as model scale changes, thus validating the effectiveness of the proposed Matrix NuclearNorm metric. We conducted Matrix Nuclear-Norm calculations and comparative analyses on inference responses from these models using two benchmark datasets: AlpacaEval and ChatBot Arena. The specific models included in our study are the DeepSeek series [22] (1.3B, 6.7B, 7B), the Llama3 series [15] (8B, 70B), the QWEN 2 series [54] (0.5B, 1.5B, 7B, 72B), and the Vicuna series [9] (7B, 13B, 33B). We also evaluated models of the same scale, specifically Gemma-7B [45] and Mistral-7B [26]. The inclusion of these diverse models enriches our research perspective and facilitates an in-depth exploration of the inference performance and scaling laws of LLMs across different parameter sizes. 5.1.3 Language Datasets Pretraining Datasets. All sizes of the Cerebras-GPT models were pretrained on the Pile dataset. Due to resource constraints, we randomly selected subset of 10,000 data samples from Wikipedia [18] and openwebtext2 [43] to observe the Matrix Nuclear-Norm, as these datasets are part of the Pile. In addition to the datasets used for model pretraining, we incorporated supplementary datasets that were not directly involved in the training process. Instruction-Tuning Datasets. To assess the effectiveness of the Matrix Nuclear-Norm, we selected the dolly-15k dataset [12], generated by human workers, as one of our instruction datasets. We specifically focused on the \"context\" portion of this dataset, which contains more informative text. This dataset will also be referenced in Section 5.3.2 for analyzing context length, prompt learning, and contextual operations. RLHF Dataset. We utilized the hh-rlhf dataset [2], which contains human preference data related to usefulness and harmlessness, as our Reinforcement Learning with Human Feedback (RLHF) dataset. Each entry includes pair of texts, one marked as \"chosen\" and the other as \"rejected.\" We input the \"chosen\" portions of the dataset into the model and subsequently calculated the performance of the Matrix Nuclear-Norm. Benchmark Datasets. To evaluate the performance of the Matrix Nuclear-Norm, we selected three benchmark datasets: OpenBookQA [34], Winogrande [37], and PIQA [4]. These benchmarks are structured in multiple-choice format. We used each question along with its correct answer as input to compute the respective evaluation metrics. Prompt Learning Datasets. We conducted prompt learning experiments using the OpenOrca dataset [30], where each question-answer (QA) pair features unique system prompt. To explore the impact of these prompts on Matrix Nuclear-Norm, we carefully selected three generally instructive prompts (see Table 13) to observe their roles in the QA pairs. Validation Datasets. Additionally, we performed inference and evaluation on two widely used datasets: AlpacaEval [16] and Chatbot Arena [10]. We conducted inference on both datasets, evaluated the responses using our proposed metrics, and ranked the models accordingly. 5.2 Matrix Nuclear-Norm Observation 5.2.1 Comparative Analysis of Computational Time To evaluate the computational efficiency of Matrix Nuclear-Norm in comparison to Matrix Entropy for LLMs, we conducted experiments across various model sizes using multiple benchmark datasets. The results, summarized in Table 1, demonstrate clear advantage of Matrix Nuclear-Norm in terms of computation time, particularly for larger models. Figure 1: CEREBRAS-GPT: Time comparison As model sizes increased, Matrix Entropys computation time rose dramatically, reaching approximately 16.3 hours for the 13B model . In contrast, Matrix Nuclear-Norm only required about 0.82 hours for the same model, representing nearly 20-fold reduction in computation time. This trend was consistent across all model sizes, with Matrix Nuclear-Norm consistently proving to be much faster (as illustrated in Figure 1). For example, the 111M model showed that Matrix Nuclear-Norm was 8.58 times quicker than Matrix Entropy. The significant efficiency gain is attributed to the lower computational complexity of Matrix NuclearNorm, O(m + log n), compared to Matrix Entropys O(n3). This makes Matrix Nuclear-Norm not only an effective but also highly efficient metric for evaluating LLMs, especially as models continue to scale in size. In summary, Matrix Nuclear-Norm achieves comparable evaluation accuracy to Matrix Entropy but with vastly superior computational efficiency, making it practical and scalable choice for assessing LLMs. Model Size Matrix Entropy Time (s) Matrix Nuclear-Norm Time (s) Ratio 111M 256M 590M 1.3B 2.7B 6.7B 13B 623.5367 1213.0604 2959.6949 6760.1893 12083.7105 38791.2035 59028.4483 72.6734 110.8692 184.7785 379.0093 732.6385 1598.4151 2984.1529 8.5800 10.9414 16.0175 17.8365 16.4934 24.2685 19.7806 Table 1: CEREBRAS-GPT: Time Comparison between Matrix Entropy and Matrix Nuclear-Norm 5.2.2 Scaling Law of Matrix Nuclear-Norm To affirm Matrix Nuclear-Norms efficacy as an evaluative metric, we evaluated Cerebras-GPT models on four datasets including dolly-15k, Wikipedia, openwebtext2, and hh-rlhf comparing Matrix Nuclear-Norm, matrix entropy, perplexity, and loss. Results, detailed in Table 11 (Appendix), demonstrate Matrix Nuclear-Norms consistent decrease with model size enlargement, signifying better data compression and information processing in larger models. This trend (see in Figure 2d) validates Matrix Nuclear-Norms utility across the evaluated datasets. Notably, anomalies at the 2.7B and 13B highlight areas needing further exploration. 7 (a) Cross-Entropy Loss (b) Perplexity (c) Matrix Entropy (d) Matrix Nuclear-Norm Figure 2: Comparison of Matrix Nuclear-Norm, matrix entropy, loss, and perplexity when model scales up. 5.2.3 Relationship of Benchmark INDICATORS Findings indicate the efficacy of the Matrix Nuclear-Norm as metric for evaluating LLM, as shown in Table 10 (Appendix), there is an overall downward trend in Matrix Nuclear-Norm values with increasing model sizes, signifying enhanced compression efficiency. However, notable anomalies at the 2.7B and 13B checkpoints suggest that these specific model sizes warrant closer examination. Despite these discrepancies, the Matrix Nuclear-Norm consistently demonstrates superior computational efficiency and accuracy compared to traditional metrics, highlighting its promising applicability for future model evaluations. 5.3 Language Investigation 5.3.1 Sentence Operation Experiments Figure 3 clearly indicates that sentence manipulations significantly influence Matrix Nuclear-Norm values, which generally decline as model size increases. This trend confirms the enhanced information compression capabilities of larger models. The ranking of Matrix Nuclear-Norm values by operation is as follows: Reverse > Shuffle & Reverse > Shuffle > Base. This indicates that disrupting sentence structure through Reverse and Shuffle & Reverse operations leads to higher Matrix Nuclear-Norm values due to increased information chaos and processing complexity. In contrast, the Shuffle operation has minimal effect on compression, while the Base condition consistently yields the lowest Matrix Nuclear-Norm values, signifying optimal information compression efficiency with unaltered sentences. Despite the overall downward trend in Matrix Nuclear-Norm values with increasing model size, the 2.7B model exhibits slightly higher values for Shuffle and Base operations compared to the 1.3B model. This anomaly suggests that the 2.7B model may retain more nuanced information when handling shuffled data or operate through more intricate mechanisms. However, this does not detract from the overarching conclusion that larger models excel at compressing information, thereby demonstrating superior processing capabilities. Figure 3: Results of sentence operation. Shuffling and reversing disrupt the text structure and diminish the informational content, leading to an increase in Matrix Nuclear-Norm. 8 5.3.2 Analysis of Length Dynamics Figure 4: The Matrix Nuclear-Norm values for contexts of varying lengths show that as text length increases, the Matrix Nuclear-Norm continues to rise and tends to converge. The analysis reveals that Matrix Nuclear-Norm values generally increase as input length rises, aligning with our expectations (see Figure 4). Longer inputs necessitate that the model manage and compress more information, which naturally leads to higher Matrix Nuclear-Norm values. Most models exhibit this trend, indicating effective handling of the increased information load. However, the gpt-2.7B and gpt-13B models display anomalies in their Matrix Nuclear-Norm values at 64 and 128 tokens, where the value at 128 tokens is lower than that at 64 tokens. This discrepancy may be attributed to these models employing different information compression mechanisms or optimization strategies tailored to specific input lengths, allowing for more effective compression at those lengths. Overall, aside from few outliers, the results largely conform to expectations, demonstrating that Matrix Nuclear-Norm values increase with input length, reflecting the greater volume and complexity of information that models must handle. To address the observed trend of rising Matrix Nuclear-Norm values with longer sentences, we incorporated normalization step in our methodology via dividing the Matrix Nuclear-Norm values by the sentence length. This adjustment helps mitigate any biases introduced by models that tend to generate longer sentences during inference. GPT MODEL SIZE LENGTH 111M 256M 590M 0.3787 0. 0.4125 64 1.3B 2.7B 6.7B 13B 0.3486 0.4053 0.3315 0.4148 128 0.5293 0.4680 0.4270 0.3835 0.4143 0. 0.4032 0.7883 0.6978 0.6251 0.5554 0. 0.4468 0.4422 1024 0.9132 0.8787 0. 0.6953 0.6351 0.5383 0.5028 Table 2: Analysis of Length Dynamics 5.3.3 Analysis of Prompt Learning The experimental results (shown in Table 3) indicate that we performed inference on different sizes of GPT models using three carefully selected prompts (shown in Table 13) and calculated the Matrix Nuclear-Norm values of their responses. As the model size increased, the Matrix Nuclear-Norm values gradually decreased, demonstrating that larger models possess greater information compression capabilities. The prompts significantly influenced Matrix Nuclear-Norm, with variations reflecting the models responses to prompt complexity. Specifically, GPT-1.3B showed notable decrease in Matrix Nuclear-Norm after the input prompts, indicating its sensitivity to them, while GPT-2.7B exhibited smaller changes. In contrast, GPT-6.7B displayed minimal variation across all prompts, 9 suggesting stable performance regardless of prompt detail. Overall, more detailed prompts resulted in larger information volumes in the models responses, leading to corresponding changes in Matrix Nuclear-Norm values. Table 3: Results of prompt learning with (Empty Prompt) and without (Prompt 1, 2, 3) the use of prompts. Incorporating prompts as prefixes before the QA pairs enhances the models ability to achieve better compression. MODELS EMPTY PROMPT PROMPT PROMPT 2 PROMPT 3 AVERAGE ADDING PROMPT TO QA PAIRS CEREBRAS-GPT-1.3B CEREBRAS-GPT-2.7B CEREBRAS-GPT-6.7B 0.150955 0.150130 0.132042 0.147577 0.151522 0. 0.140511 0.142834 0.124094 0.141358 0.151842 0. 0.14453 0.14844 0.12923 0.006425 0.001690 0.002812 6 Implementing Proposed Metrics: Evaluating and Ranking Language Models in Practice 6.1 Inference-Based Model Assessment In this section, we evaluated model inference across the AlpacaEval and Chatbot Arena benchmarks using the Matrix Nuclear-Norm metric prior to the final MLP classification head. The analysis revealed that Matrix Nuclear-Norm reliably ranks model performance, with lower values indicating enhanced information processing efficiency, particularly as model size scales up. For instance, the Llama-3 70B model demonstrated superior compression capabilities compared to its 8B counterpart, as reflected by significantly lower Matrix Nuclear-Norm values across both benchmarks (see Table 9 in the Appendix). similar trend was observed in the Vicuna family, where Matrix Nuclear-Norm values consistently decreased from 0.4623 for the 7B model to 0.3643 for the 33B model on the AlpacaEval dataset, indicating progressive improvements in information handling (see Table 4). Additionally, the DeepSeek models exhibited consistent decrease in Matrix Nuclear-Norm values as model size increased, further demonstrating the metrics validity. Overall, these results substantiate Matrix Nuclear-Norm as robust and reliable tool for evaluating and ranking LLMs, demonstrating its capacity to capture critical aspects of model performance across diverse benchmarks. Model DataSet 7B 13B 33B Model DataSet 1.3B 6.7B 7B Vicuna Alpaca Arena 0.4623 0.4824 0.4159 0. 0.3643 0.3734 DeepSeek Alpaca Arena 0.4882 0.5754 0.3472 0.4175 0.3352 0. Table 4: Matrix Nuclear-Norms in Vicuna and DeepSeek Responses 6.2 Matrix Nuclear-Norm Benchmarking: Ranking Mid-Sized Models In this experimental section, we utilized Matrix Nuclear-Norm to evaluate the responses of LLMs, focusing on 7B and 70B variants. Notably, lower Matrix Nuclear-Norm values indicate more efficient information compression, serving as robust indicator of model performance. Among the 7B models, DeepSeek-7B exhibited the most efficient information processing with the lowest average Matrix Nuclear-Norm score of 0.3855 across Alpaca and Arena datasets (see Table 4). Gemma-7B followed closely with an average score of 0.3879, whereas QWEN 2-7B demonstrated less efficient compression with an average score of 0.5870. In contrast, the 70B models showed varied performance, with Llama 2-70B achieving the best average score of 0.3974, slightly outperforming Llama 3-70B (0.4951) and QWEN models, which scored around 0.5. Interestingly, certain 7B models, like DeepSeek-7B and Gemma-7B, outperformed larger 70B models, underscoring that model efficiency is not solely determined by size. These results highlight that factors such as architecture, training methodology, and data complexity play crucial roles in information processing capabilities beyond scale. To validate the design rationale and robustness of the Matrix Nuclear-Norm, we conducted series of ablation studies. Due to space constraints, detailed results are provided in A.1 (appendix) to maintain 10 MODEL Matrix Nuclear-Norm Rank Alpaca Arena-Hard Avg Score DeepSeek-7B Gemma-7B Vicuna-7B LLaMA 2-7B QWEN 1.5-7B Mistral-7B QWEN 2-7B QWEN 1.5-72B QWEN 2-72B Llama 3-70B Llama 2-70B 0.3352 0.3759 0.4623 0.4648 0.4866 0.4980 0.5989 0.5291 0.5261 0.4935 0.3862 0.4357 0.3998 0.4824 0.5038 0.5165 0.5126 0. 0.5065 0.4689 0.4967 0.4086 0.3855 0.3879 0.4724 0.4843 0.5016 0.5053 0.5870 0.5178 0.4975 0.4951 0.3974 Table 5: Matrix Nuclear-Norm Rankings: Comparative Analysis of Model Performance brevity in the main text. These experiments included evaluations across different model families, such as Cerebras-GPT and Pythia, as well as comparisons of various data sampling strategies.The results demonstrate that the Matrix Nuclear-Norm consistently performs well across different model scales and sampling variations. This not only confirms its applicability across diverse models but also verifies its stability and reliability in handling large-scale datasets. We also provide an ablation study in the appendix, further proving the methods efficiency and accuracy in evaluating LLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "In conclusion, Matrix Nuclear-Norm stands out as promising evaluation metric for LLMs, offering significant advantages in assessing information compression and redundancy elimination. Its key strengths include remarkable computational efficiency, greatly exceeding that of existing metrics like matrix entropy, along with exceptional stability across diverse datasets. Matrix Nuclear-Norms responsiveness to model performance under varying inputs emphasizes its ability to gauge not only performance but also the intricate adaptability of models. This metric marks significant advancement in NLP, establishing clear and effective framework for future research and development in the evaluation and optimization of language models."
        },
        {
            "title": "8 Limitations",
            "content": "Although Matrix Nuclear-Norm performs well in evaluating the performance of LLMs, it still has some limitations. First, since Matrix Nuclear-Norms computation relies on the models hidden states, the evaluation results are sensitive to both the model architecture and the training process. As result, under different model designs or training settings, especially for models like GPT-1.3B and GPT-2.7B, inconsistencies in Matrix Nuclear-Norms performance may arise, limiting its applicability across wider range of models. Additionally, while Matrix Nuclear-Norm offers computational efficiency advantages over traditional methods, it may still face challenges with resource consumption when evaluating extremely large models. As model sizes continue to grow, further optimization of Matrix Nuclear-Norms computational efficiency and evaluation stability is required."
        },
        {
            "title": "9 Ethics Statement",
            "content": "Our study adheres to strict ethical guidelines by utilizing only publicly available and open-source datasets. We ensured that all datasets used, such as dolly-15k, hh-rlhf, OpenBookQA, Winogrande, PIQA, AlpacaEval, and Chatbot Arena, are free from harmful, biased, or sensitive content. Additionally, careful curation was conducted to avoid toxic, inappropriate, or ethically problematic data, thereby ensuring the integrity and safety of our research. This commitment reflects our dedication to responsible AI research and the broader implications of using such data in language model development."
        },
        {
            "title": "References",
            "content": "[1] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. Proceedings of the National Academy of Sciences, 121(27):e2311878121, 2024. [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [3] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. [5] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: survey. arXiv preprint arXiv:2006.14799, 2020. [6] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145, 2024. [7] Jun Chen, Yong Fang, Ashish Khisti, Ayfer Ozgur, Nir Shlezinger, and Chao Tian. Information compression in the ai era: Recent advances and future challenges. arXiv preprint arXiv:2406.10036, 2024. [8] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential of large language models (llms) in learning on graphs. ACM SIGKDD Explorations Newsletter, 25(2):4261, 2024. [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL: https://arxiv. org/abs/2403.04132, 2024. [11] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International conference on machine learning, pp. 40574086. PMLR, 2022. [12] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the worlds first truly open instruction-tuned llm. Company Blog of Databricks, 2023. [13] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023. [14] Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208, 2023. 12 [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [17] Maryam Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stanford University, 2002. [18] Foundation. Foundation. https://dumps.wikimedia.org, 2024. [Online; accessed 202409-27]. [19] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [20] Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [21] Josh GPT-4 Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [22] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang, 2024. URL https: //arxiv.org/abs/2401.14196. [23] Qiang Guo, Caiming Zhang, Yunfeng Zhang, and Hui Liu. An efficient svd-based method for image denoising. IEEE transactions on Circuits and Systems for Video Technology, 26(5): 868880, 2015. [24] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. [25] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [26] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [28] Sun-Yuan Kung, Si Arun, and DV Bhaskar Rao. State-space and singular-value decomposition-based approximation methods for the harmonic retrieval problem. JOSA, 73 (12):17991811, 1983. [29] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023. [30] Lian, Goodson, Pentland, et al. Openorca: An open dataset of gpt augmented flan reasoning traces, 2023. [31] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. 13 [32] Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al. Llmrec: Benchmarking large language models on recommendation task. arXiv preprint arXiv:2308.12241, 2023. [33] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael Jordan. Unsupervised domain adaptation with residual transfer networks. Advances in neural information processing systems, 29, 2016. [34] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [35] Graham Neubig. Neural machine translation and sequence-to-sequence models: tutorial. arXiv preprint arXiv:1703.01619, 2017. [36] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. [37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [38] Yutaka Sasaki. The truth of the f-measure. Teach tutor mater, 2007. [39] Lawrence Saul, Yair Weiss, and Léon Bottou. Advances in neural information processing systems 17: proceedings of the 2004 conference, volume 17. MIT Press, 2005. [40] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John Nay, Kshitij Gupta, and Aran Komatsuzaki. Arb: Advanced reasoning benchmark for large language models. arXiv preprint arXiv:2307.13692, 2023. [41] Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. [42] Oscar Skean, Jhoan Keider Hoyos Osorio, Austin Brockmeier, and Luis Gonzalo Sanchez Giraldo. Dime: Maximizing mutual information by difference of matrix-based entropies. arXiv preprint arXiv:2301.08164, 2023. [43] Skylion007. OpenWebText Corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. [Online; accessed 2024-09-27]. [44] Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan, and Yifan Zhang. Information flow in self-supervised learning. arXiv preprint arXiv:2309.17281, 2023. [45] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [46] Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [48] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, and Srinivas Shakkottai. Llmzip: Lossless text compression using large language models. arXiv preprint arXiv:2306.04050, 2023. [49] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 25172526, 2019. [50] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: comprehensive assessment of trustworthiness in gpt models. In NeurIPS, 2023. [51] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36, 2024. [52] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [53] Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Large language model evaluation via matrix entropy. arXiv preprint arXiv:2401.17139, 2024. [54] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [55] Zhilin Yang. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. [56] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [57] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori Hashimoto. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:3957, 2024. [58] Zhihua Zhang. The singular value decomposition, applications and beyond. arXiv preprint arXiv:1510.08532, 2015. [59] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [60] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Ablation Study To thoroughly validate the rationale behind our metric design, experimental framework, and the efficacy of Matrix Nuclear-Norm, we conducted series of ablation studies. A.1.1 Different Model Family In addition to evaluating Matrix Nuclear-Norm within the Cerebras-GPT model series, we extended our experiments to the Pythia model family, which spans from 14M to 12B parameters and is trained on consistent public datasets. Utilizing the same datasets as described in Section 5.2.2, we computed matrix entropy, loss values, and Matrix Nuclear-Norm for these models. The empirical results (see Figure 5c) demonstrate that the Matrix Nuclear-Norm values for the Pythia models adhere to established scaling laws. However, we excluded metrics for the 14M, 31M, and 1B models due to notable deviations from the expected range, likely stemming from the inherent instability associated with smaller parameter sizes when tackling complex tasks. This further reinforces Matrix NuclearNorm as robust metric for assessing model performance, underscoring its utility in the comparative analysis of LLMs. Moreover, we compared the computation times for Matrix Entropy and Matrix Nuclear-Norm across the Pythia models (can see in Figure 7). The results unequivocally indicate that Matrix Nuclear-Norm necessitates considerably less computation time than Matrix Entropy, underscoring its efficiency. Detailed results are summarized in Table 12. (a) Cross-Entropy Loss (b) Matrix Entropy (c) Matrix Nuclear-Norm Figure 5: Pythia Model Metrics: Matrix Nuclear-Norm, Matrix Entropy, and Loss A.1.2 Sampling Strategy In the ablation experiments, we extracted baseline subset of 10,000 entries from the extensive Wikipedia dataset using three random seeds to evaluate the robustness of the Matrix Nuclear-Norm metric. We also tested additional subsets of 15,000 and 20,000 entries due to potential entry count issues. Given the large scale of the datasets, comprehensive calculations were impractical, so we employed random sampling. The results showed that variations in random seeds and sample sizes had minimal impact on Matrix Nuclear-Norm values, with standard deviation of only 0.0004975 (see Table 6), indicating high consistency across trials. These findings confirm the Matrix Nuclear-Norm as reliable metric for large-scale datasets, effectively evaluating information compression and redundancy elimination in LLMs. Table 6: Ablation study of differnet sampling strategies on the Wikimedia[18] dataset. SAMPLING STRATEGY MODEL STANDARD DEVIATION 10000 (SEED 1) 10000 (SEED 2) 10000 (SEED 3) 15000 20000 CEREBRAS-GPT-1.3B 0. 0.5670 0.5676 0.5699 0.5693 0.0004975 A.2 Supplementary Experiment Results The following results provide additional insights into the Matrix Nuclear-Norm evaluations and comparisons across various language models: 1. Tables 9 and 8 present the Matrix Nuclear-Norm evaluation results during the inference process for Llama-3 and QWEN-2. 16 2. Figure 6 illustrates that as model size increases, the computation time for Matrix Entropy grows exponentially, while Matrix Nuclear-Norm demonstrates significant time advantage. This further emphasizes Matrix Nuclear-Norms efficiency in assessing model performance.The complete results are presented in Table 7, which includes all relevant time data for the Pythia model family. 3. Table 11 contains the complete results for the comparison of Matrix Nuclear-Norm and other metrics based on Cerebras-GPT family considered in Figure 2d. 4. Table 10 demonstrates the correlation between Matrix Nuclear-Norm and other benchmark indicators, showing consistent trend where values decrease as model size increases. This analysis examines the performance of language modeling indicators across OpenBookQA, Winogrande, and PIQA datasets. 5. Table 12 illustrates the numerical results of Figure 5c in the ablation study of Pythia family. 6. Table 13 shows the prompts used for the investigation of prompt learning. Figure 6: Pythia: Time Comparison of Matrix Entropy and Nuclear-Norm Model Size Matrix Entropy Time (s) Matrix Nuclear-Norm Time (s) Ratio 14M 31M 70M 160M 410M 1B 1.4B 2.8B 6.9B 12B 52.8669 114.0820 320.6641 631.9762 1040.9764 4650.1264 6387.0392 8127.1343 28197.8172 47273.5235 22.2652 28.1842 24.3188 41.6187 80.9814 114.0639 347.8670 343.3888 816.6332 1276.1128 2.3772 4.0477 13.1855 15.1817 12.8481 40.8387 18.3858 23.6778 34.5350 37.0485 Table 7: Pythia Model: Matrix Entropy vs. Matrix Nuclear-Norm Time Comparison Model DataSet 0.5B 1.5B 7B 72B Model Data Set 8B 70B QWEN 2 Alpaca Arena 0.6551 0. 0.6176 0.6374 0.5989 0.5751 0.5261 0.4689 Llama-3 Alpaca Arena 0.5782 0. 0.4935 0.4967 Table 8: Matrix Nuclear-Norm in QWEN 2 Responses Table 9: Matrix Nuclear-Norm in Llama3 esponses 17 BENCHMARKS INDICATORS 111M 256M 590M 1.3B 2.7B 6.7B 13B GPT MODEL SIZE OPENBOOKQA WINOGRANDE PIQA ACCURACY MATRIX ENTROPY LOSS PPL 0.118 0.3575 5.6196 148.38 MATRIX NUCLEAR-NORM 0.4447 ACCURACY MATRIX ENTROPY LOSS PPL 0.488 0.4073 4.7869 39.81 MATRIX NUCLEAR-NORM 0. ACCURACY MATRIX ENTROPY LOSS PPL 0.594 0.4168 4.8425 69.80 MATRIX NUCLEAR-NORM 0.4868 0.158 0.3416 5.3536 108.10 0.4057 0.511 0.3915 4.5854 30.25 0.4479 0.613 0.3991 4.5470 47.94 0.4327 0.158 0.3237 5.1881 83.45 0. 0.498 0.3706 4.4141 26.57 0.4440 0.627 0.3783 4.4029 37.88 0.4164 0.166 0.3140 4.9690 65.10 0.3644 0.521 0.3605 4.2513 21.87 0.4133 0.664 0.3676 4.1613 28.76 0.3826 0.206 0.2991 4.8723 50.93 0. 0.559 0.3419 4.1107 18.55 0.5232 0.701 0.3504 4.0075 23.15 0.4452 0.238 0.2848 4.7195 41.80 0.3672 0.602 0.3272 4.0109 16.53 0.4220 0.739 0.3344 3.8545 19.76 0.3675 0.286 0.2767 4.7050 40.89 0. 0.646 0.3149 4.0266 16.94 0.4964 0.766 0.3264 3.8826 19.72 0.4149 Table 10: Language modeling indicators on openbookqa, winogrande and piqa. DATASET INDICATORS 111M 256M 590M GPT MODELS SIZE 1.3B 2.7B 6.7B 13B DOLLY-15K WIKIPEDIA OPENWEBTEXT2 HH-RLHF MATRIX ENTROPY LOSS PPL 0.5976 3.6710 39.93 MATRIX NUCLEAR-NORM 0.6207 MATRIX ENTROPY LOSS PPL 0.6177 3.2900 31.38 MATRIX NUCLEAR-NORM 0. MATRIX ENTROPY LOSS PPL 0.6527 3.7509 36.79 MATRIX NUCLEAR-NORM 0.7147 MATRIX ENTROPY LOSS PPL 0.5753 3.3078 18.97 MATRIX NUCLEAR-NORM 0.6309 0.5840 3.2907 27.53 0.5565 0.6077 2.9343 22.51 0. 0.6479 3.3852 25.82 0.7066 0.5635 2.9964 14.01 0.5716 0.5582 3.0359 21.42 0.5063 0.5848 2.6854 17.89 0.6094 0.6206 3.1414 20.34 0.6823 0.5350 2.8171 11.62 0. 0.5477 2.7517 16.15 0.4553 0.5786 2.4282 13.85 0.5639 0.6142 2.8860 15.89 0.6363 0.5268 2.6431 9.73 0.4771 0.5240 2.5015 12.50 0.4639 0.5523 2.2045 11.08 0. 0.5855 2.6465 12.51 0.6017 0.4971 2.4622 8.12 0.4959 0.5064 2.2911 10.23 0.3904 0.5368 2.0216 9.19 0.4660 0.5683 2.4708 10.57 0.5133 0.4813 2.3526 7.27 0. 0.4859 2.3098 10.30 0.4859 0.5126 2.0327 9.32 0.4708 0.5463 2.4685 10.51 0.4991 0.4640 2.3323 7.19 0.4518 Table 11: Language modeling indicators on the Dolly-15k, Wikipedia, OpenWebText2, and HHRLHF datasets. A.3 Analysis of Algorithmic Complexity The primary computational expense of Matrix Nuclear-Norm arises from the calculation and sorting of the L2 norm of the matrix. By avoiding Singular Value Decomposition (SVD), we reduce the time complexity from the traditional nuclear norm of O(n3) to O(n2), giving Matrix Nuclear-Norm significant advantage in handling large-scale data. This reduction in complexity greatly enhances the algorithms practicality, especially for applications involving large matrices. When analyzing the time complexity of the newly proposed Matrix Nuclear-Norm (L2-Norm Based Approximation of Nuclear Norm) against traditional Matrix Entropy, our objective is to demonstrate that Matrix Nuclear-Norm significantly outperforms Matrix Entropy in terms of time efficiency. We will support this claim with detailed complexity analysis and experimental results. A.3.1 Time Complexity Analysis Analysis 1: Time Complexity of Matrix Entropy 18 Table 12: The language modeling indicators (where lower values indicate better performance) for the Pythia models were evaluated on the Dolly-15k, Wikipedia, OpenWebText2, and HH-RLHF datasets. PYTHIA MODELS SIZE DATASETS INDICATORS 14M 31M 70M 160M 410M 1B 1.4B 2.8B 6.9B 12B DOLLY-15K WIKIPEDIA OPENWEBTEXT2 HH-RLHF MATRIX ENTROPY LOSS 0.7732 4.4546 MATRIX NUCLEAR-NORM 0.7508 MATRIX ENTROPY LOSS 0.7938 4.1112 MATRIX NUCLEAR-NORM 0. MATRIX ENTROPY LOSS 0.8144 4.3965 MATRIX NUCLEAR-NORM 0.5041 MATRIX ENTROPY LOSS 0.7673 3.7466 MATRIX NUCLEAR-NORM 0.7353 0.7155 4.0358 0.7735 0.7442 3.6921 0. 0.7749 4.0033 0.6186 0.7114 3.4018 0.7674 0.6707 3.5990 0.6984 0.7003 3.2694 0.6996 0.7370 3.6284 0.7142 0.6607 3.1146 0. 0.6243 3.1323 0.6104 0.6580 2.8207 0.6718 0.6980 3.2031 0.7258 0.6126 2.7366 0.6182 0.5760 2.6752 0.5760 0.6039 2.4017 0. 0.6415 2.7838 0.7105 0.5552 2.4340 0.5886 0.5328 2.4843 0.4710 0.5584 2.2213 0.5591 0.5944 2.6198 0.6215 0.5054 2.3311 0. 0.5309 2.3816 0.4922 0.5587 2.1292 0.5787 0.5916 2.5228 0.6378 0.5032 2.2687 0.5141 0.5263 2.2484 0.4585 0.5553 2.0140 0. 0.5887 2.4005 0.5967 0.4977 2.1992 0.4839 0.5003 2.1368 0.4202 0.5314 1.9120 0.4850 0.5591 2.3133 0.5275 0.4699 2.1199 0. 0.4876 2.0616 0.4181 0.5140 1.8489 0.4768 0.5417 2.2502 0.5110 0.4528 2.0905 0.4481 Prompt ID Prompt Prompt 2 Table 13: The prompts selected from OpenOrca[30] dataset. You are an AI assistant. You will be given task. You must generate detailed and long answer. You are helpful assistant, who always provide explanation. Think like you are answering to five year old. Prompt Content Prompt You are an AI assistant. User will give you task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps. The computation of Matrix Entropy involves several complex steps, with the key bottleneck being Singular Value Decomposition (SVD), which is central to computing eigenvalues. The following steps primarily contribute to the time complexity: 1. Matrix Normalization: This step has time complexity of O(m n), where is the number of rows and is the number of columns. 2. Computing the Inner Product Matrix: Calculating has time complexity of O(n2 m) due to the multiplication of two matrices sized n. 3. Singular Value Decomposition (SVD): The time complexity of SVD is O(n3), which is the primary computational bottleneck, especially for large n. Therefore, the total time complexity of Matrix Entropy can be approximated as: O(m + n2 + n3) = O(n3) This complexity indicates that Matrix Entropy becomes increasingly impractical for large-scale models as grows. Analysis 2: Time Complexity of Matrix Nuclear-Norm Matrix Nuclear-Norm avoids the SVD step by approximating the nuclear norm using the L2 norm, resulting in more efficient computation. The analysis is as follows: 1. Matrix Normalization: Similar to Matrix Entropy, this step has time complexity of O(m n). 2. Calculating the L2 Norm: For each column vector, the L2 norm is computed with complexity of O(m n), where we take the square root of the sum of squares for each column vector. 3. Sorting and Extracting the Top Features: Sorting the L2 norms has complexity of O(n log n). Therefore, the overall time complexity of Matrix Nuclear-Norm is: O(m + log n) O(n2) when This indicates that Matrix Nuclear-Norm is computationally more efficient, especially as increases. A.3.2 Experimental Validation and Comparative Analysis To empirically validate the theoretical time complexities, we conducted experiments using matrices of various sizes. Figure 6 shows that as increases, Matrix Nuclear-Norm consistently outperforms Matrix Entropy in terms of runtime, confirming the theoretical advantage. 19 Discussion of Assumptions and Applicability Our complexity analysis assumes n, which holds in many real-world applications, such as evaluating square matrices in large-scale language models. However, in cases where = n, the time complexity might differ slightly. Nonetheless, Matrix Nuclear-Norm is expected to maintain its efficiency advantage due to its avoidance of the costly SVD operation. Impact of Constant Factors Although both O(n2) and O(n3) indicate asymptotic behavior, Matrix Nuclear-Norms significantly smaller constant factors make it computationally favorable even for moderately sized matrices, as evidenced in our experimental results. A.3.3 Conclusion of the Complexity Analysis Through this detailed analysis and experimental validation, we conclude the following: Matrix Entropy, with its reliance on SVD, has time complexity of O(n3), making it computationally expensive for large-scale applications. Matrix Nuclear-Norm, by using the L2 norm approximation, achieves time complexity of O(m + log n) O(n2), significantly reducing computational costs. Experimental results confirm that Matrix Nuclear-Norm offers superior time efficiency for evaluating large-scale models, particularly those with millions or billions of parameters. A.4 Proof of Theorem 1: Detailed Version Here we present the proof of Theorem 1, demonstrating the strictly inverse monotonic relationship between H(A) and AF . Given matrix RBC, where each element is non-negative and each row sums to 1, i.e., (cid:88) j= Ai,j = 1 for = 1, 2, . . . , B. We define the Shannon entropy H(A) as: H(A) = 1 (cid:88) (cid:88) i=1 j=1 Ai,j log(Ai,j), and the Frobenius norm AF as: AF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) i=1 j=1 Ai,j2. Step 1: Analysis for Fixed Row Vector For fixed row i, let aj = Ai,j such that aj 0 and (cid:80)C is given by: j=1 aj = 1. The entropy for this single row and the Frobenius norm for this row vector is: Hi = (cid:88) j=1 aj log aj, a2 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 a2 . To determine when Hi is maximized or minimized, we apply the method of Lagrange multipliers. Consider the function: L(a1, a2, . . . , aC, λ) = (cid:88) j=1 aj log aj + λ (cid:88) aj 1 . j=1 Taking partial derivatives with respect to aj and setting them to zero: aj = log aj 1 + λ = 0 = aj = eλ1. 20 Since (cid:80)C case, j=1 aj = 1, we find aj = 1 for all j. Thus, Hi is maximized when all aj are equal. In this a2 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 (cid:18) 1 (cid:19) (cid:114) 1 . = Conversely, Hi reaches its minimum when one element ak = 1 and all other aj = 0. At this point, (cid:112) a2 = 12 + 0 = 1. This demonstrates that Hi and a2 are inversely related for each row. Step 2: Inverse Monotonicity for the Entire Matrix Since H(A) is the average of all Hi values: H(A) ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 Hi, and AF is the square root of the sum of the squared norms a2 2 for each row: AF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 a2 2, the inverse relationship between Hi and a2 implies that H(A) and AF are also inversely related across the entire matrix A. Step 3: Determining the Bounds of AF To find the bounds, first note that when aj = 1 for all j, AF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) i=1 j= (cid:18) 1 (cid:19)2 (cid:114) = 1 = (cid:114) , which corresponds to the minimum discriminability. For the upper bound, when each row has only one element equal to 1, AF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 B, 1 = indicating maximum discriminability. Step 4: Practical Implications The inverse monotonic relationship between H(A) and AF is significant in machine learning and classification tasks, where maximizing AF implies achieving greater certainty in predictions. This property can be used to evaluate model performance, where higher AF corresponds to model that is more confident and discriminative in its predictions. Therefore, H(A) and AF are strictly inversely monotonic, and the proof is complete. (cid:17) (cid:16)(cid:80)B B, we want to approximate the j-th largest singular value σj as . This approximation will be derived by examining the contributions of the A.5 Proof of Theorem 3 Given that AF is near i=1 A2 top columns of A. 1. Decomposition of and Eigenvalues: By singular value decomposition (SVD), = ΣV , where Σ is diagonal matrix containing singular values σ1, σ2, . . . , σD. When looking at the product AT A, the element at position (j, j) is: i,j, (AT A)j,j = (cid:88) i=1 A2 i,j This sum represents the total squared magnitude of the entries in the j-th column of A. 21 2. Connecting to Singular Values: Since singular values represent the contribution of orthogonal projections of A, if AF is near B, the main contribution to the nuclear norm comes from columns with the largest sums of squared values (cid:80)B 3. Approximation: For matrices with well-distributed entries in each column (a common property B), the top singular values can be effectively estimated by sorting these column when AF norms. Hence, the largest singular values σj approximately correspond to the largest column norms (cid:113)(cid:80)B i=1 A2 i,j. i=1 A2 i,j. Therefore, for each {1, . . . , D}, σj top (cid:33) i,j, . (cid:32) (cid:88) i=1 This concludes the proof that the j-th largest singular value σj can be approximated by the top values of column-wise norms of A, as stated in Theorem 3."
        }
    ],
    "affiliations": [
        "International Center of Future Science, Jilin University",
        "Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University",
        "School of Artificial Intelligence, Jilin University"
    ]
}