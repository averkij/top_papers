{
    "paper_title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
    "authors": [
        "Yang Zhou",
        "Yifan Wang",
        "Jianjun Zhou",
        "Wenzheng Chang",
        "Haoyu Guo",
        "Zizun Li",
        "Kaijing Ma",
        "Xinyue Li",
        "Yating Wang",
        "Haoyi Zhu",
        "Mingyu Liu",
        "Dingning Liu",
        "Jiange Yang",
        "Zhoujie Fu",
        "Junyi Chen",
        "Chunhua Shen",
        "Jiangmiao Pang",
        "Kaipeng Zhang",
        "Tong He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world."
        },
        {
            "title": "Start",
            "content": "2025-9-16 OmniWorld: Multi-Domain and Multi-Modal Dataset for 4D World Modeling Yang Zhou1, Yifan Wang1, Jianjun Zhou1,2, Wenzheng Chang1, Haoyu Guo1, Zizun Li1, Kaijing Ma1, Xinyue Li1, Yating Wang1, Haoyi Zhu1, Mingyu Liu1,2, Dingning Liu1, Jiange Yang1, Zhoujie Fu1, Junyi Chen1, Chunhua Shen2, Jiangmiao Pang1, Kaipeng Zhang1 and Tong He1 1Shanghai AI Lab, 2ZJU The field of 4D world modelingaiming to jointly capture spatial geometry and temporal dynamicshas witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-controlled video generation. To address this gap, we introduce OmniWorld, large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as powerful resource for training and evaluation. We envision OmniWorld as catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines holistic understanding of the physical world. GitHub ƒ± Data (cid:209) Homepage 5 2 0 S 5 1 ] . [ 1 1 0 2 2 1 . 9 0 5 2 : r Figure 1. We introduce OmniWorld, large-scale, multi-domain, and multi-modal dataset. OmniWorld provides rich resource for 4D world modeling by integrating high-quality data from multiple domains and offers variety of data types, including depth maps, camera poses, text captions, optical flow and foreground masks. OmniWorld is designed to accelerate the development of more general models for modeling the real physical world. Corresponding author: Tong He, tonghe90@gmail.com 2025 Shanghai Artificial Intelligence Laboratory. All rights reserved. Dataset Scene Type Motion Resolution # Frames Depth Camera Data modality Text Optical flow Fg. masks MPI Sintel (Butler et al., 2012) FlyingThings++ (Harley et al., 2022; Mayer et al., 2016) TartanAir (Wang et al., 2020) BlendedMVS (Yao et al., 2020) HyperSim (Roberts et al., 2021) Dynamic Replica (Karaev et al., 2023) Spring (Mehl et al., 2023) EDEN (Le et al., 2021) PointOdyssey (Zheng et al., 2023) SeKai-Game (Li et al., 2025) OmniWorld-Game (Ours) Mixed Outdoor Mixed Mixed Indoor Indoor Mixed Outdoor Mixed Outdoor Mixed Dynamic Dynamic Dynamic Static Static Dynamic Dynamic Static Dynamic Dynamic Dynamic 1024 436 960 540 640 480 768 576 1024 768 1280 720 1920 1080 640 480 960 540 1920 1080 1280 720 1K 28K 1,000K 17K 77K 169K 23K 300K 216K 4,320K 18,515K Table 1. Comparisons between OmniWorld-Game and existing synthetic datasets. OmniWorld-Game surpasses existing public synthetic datasets in modal diversity and data scale. 1. Introduction The development of world models (Agarwal et al., 2025; DeepMind, 2025; Ha and Schmidhuber, 2018; Hafner et al., 2023; LeCun, 2022) has become central pursuit in visual intelligence systems, aiming to build systems that can simulate and reason about the physical world. This capability goes beyond simple static perception, demanding models that can simulate dynamic environments, predict object motion, infer causality, and generate content that adheres to physical laws. Such spatio-temporal modeling is cornerstone for effective world models, with its development critically dependent on large-scale, multi-domain, and multi-modal datasets (Chen et al., 2025; Feng et al., 2024; He et al., 2025b; Team et al., 2025a,b). Two fundamental tasks that reflect models world modeling capability have drawn widespread attention: 3D geometric foundation models (Leroy et al., 2024; Tang et al., 2024; Wang et al., 2025a,b, 2024c, 2025d; Yang et al., 2025; Zhang et al., 2024, 2025), and camera control video generation models (Bahmani et al., 2024; Bai et al., 2025; He et al., 2024; Wang et al., 2024d; YU et al., 2025; Zheng et al., 2024). The former aims to extract comprehensive 3D geometric information from 2D image inputs, while the latter focuses on generating dynamic video content that follows precise spatio-temporal instructions. Both tasks heavily rely on large-scale, high-quality datasets with rich modalities, including RGB images, depth maps, and camera poses. However, existing benchmarks and datasets for evaluating and training these models have significant limitations. In the domain of 3D geometric foundation models, existing benchmarks suffer from short sequence lengths, which constrain the evaluation of models long-term robustness. For example, Sintel (Butler et al., 2012), which is widely used dataset, consists of videos with an average length of only 50 frames. Furthermore, the limited motion amplitude and single-action types within these datasets (e.g., Bonns (Palazzolo et al., 2019) focuses on indoor human motion, Kittis (Geiger et al., 2013) focuses on outdoor street scenes) fail to comprehensively evaluate model performance in complex, dynamic environments. Similarly, in the field of camera control video generation, mainstream datasets like RealEstate10K (Zhou et al., 2018) primarily consist of static scenes with smooth camera trajectories. This lack of diverse object motion and complex camera operations results in noticeable gap between the datasets content and real-world scenarios, thereby hindering comprehensive assessment of models true capabilities. From the perspective of training data, there is critical scarcity of high-quality, multi-domain, multi-modal datasets that include rich geometric annotations. For instance, in image or video generation, while there are numerous image-text (Gadre et al., 2023; Schuhmann et al., 2022) or video-text datasets (Chen et al., 2024; Ju et al., 2024; Nan et al., 2024), they often lack critical geometric modalities such as depth maps, camera poses, and optical flow. Similarly, the demand for large-scale, diverse datasets with accurate geometric annotations is increasingly urgent for 3D Dataset Domain # Seq. FPS Resolution # Frames Data modality Depth Camera Text Opt. flow Fg. masks OmniWorld-Game Robot AgiBot (Bu et al., 2025) Robot DROID (Khazatsky et al., 2024) RH20T (Fang et al., 2024) Robot RH20T-Human (Fang et al., 2024) Human HOI4D (Liu et al., 2022) Human Epic-Kitchens (Damen et al., 2018) Human Ego-Exo4D (Grauman et al., 2024) Human Human HoloAssist (Wang et al., 2023) Human Assembly101 (Sener et al., 2022) Human EgoDex (Hoque et al., 2025) Internet CityWalk (Li et al., 2025) Simulator 96K 20K 35K 109K 73K 2K 15K 4K 1K 4K 242K 7K 24 1280720 18,515K 39,247K 30 640480 60 1280720 26,643K 53,453K 10 640360 8,875K 10 640360 891K 15 19201080 3,635K 30 1280720 30 10241024 9,190K 30 896504 13,037K 60 19201080 110,831K 30 19201080 76,631K 30 1280720 13,096K Table 2. OmniWorld structure. smiling face ( ) indicates the modality is newly (re-)annotated by us, green check () denotes ground-truth data that already exists in the original dataset, and red cross () marks missing modalities. geometric foundation models. To address these shortcomings, we introduce OmniWorld, large-scale, multi-domain, and multimodal dataset composed of self-collected high-quality OmniWorld-Game synthetic dataset and several public datasets. Its core characteristics are: 1) High-Quality 4D Data. OmniWorld-Game is massive synthetic video dataset comprising over 96K clips and more than 18M frames, with total duration of over 214 hours. It is captured from diverse game environments with 720P RGB images, dense ground truth depth maps, accurate camera poses, and annotations for text captions, optical flow and foreground masks. As shown in Table 1, the dataset significantly surpasses existing public synthetic datasets in modal diversity and scale. 2) Multi-Domain Coverage. By integrating datasets from four key domains including simulator, robot, human, and the internet, OmniWorld covers wide range of real-world and virtual scenarios, greatly enhancing data diversity. 3) Multi-Modality Annotations. OmniWorld provides rich suite of multi-modal annotations, crucial for detailed world modeling, as shown in Table 2. Based on OmniWorld-Game, we propose new benchmark for both 3D geometric foundation models and camera control video generation models. Our OmniWorld-Game benchmark provides challenging, complex scenarios and dynamics that accurately reflect models true world capabilities, revealing the limitations of current SOTAs. By fine-tuning existing SOTAs (e.g., DUSt3R (Wang et al., 2024c), CUT3R (Wang et al., 2025b), Reloc3r (Dong et al., 2024), AC3D (Bahmani et al., 2024)) with OmniWorld, we demonstrate significant performance improvements on public benchmarks. This strongly validates OmniWorld as powerful training resource for enhancing world modeling capabilities. In summary, our contributions are as follows: 1. We introduce OmniWorld, multi-domain and multi-modal dataset designed to address the lack of diversity in existing datasets. Its self-collected subset, OmniWorld-Game, surpasses current synthetic datasets in both modality diversity and data volume. 2. We establish comprehensive benchmark for 3D geometric foundation models and cameracontrolled video generation models based on OmniWorld-Game, providing unified platform for evaluation. 3. We fine-tune several SOTAs on OmniWorld and observe significant performance gains, underscoring its value as training resource. 3 Figure 2. OmniWorld acquisition and annotation pipeline. 2. OmniWorld Dataset To advance comprehensive spatio-temporal modeling of the real physical world, we curate OmniWorld, large-scale, multi-domain and multi-modal dataset that mirrors the complexity of the physical world. We design and implement detailed data acquisition and annotation pipeline to ensure high-quality multi-modal annotations, as illustrated in Figure 2. 2.1. Data Acquisition To address the scarcity of high-precision, temporally consistent, and dynamically rich data, we develop sophisticated data acquisition pipeline. Our approach is centered on novel self-collected dataset, OmniWorld-Game, which we supplement with data from three other domains: robot, human, and internet. This strategy allows us to integrate the strengths of diverse data sources to comprehensively capture real-world complexity. Simulator Domain. To acquire the high-precision and temporally consistent multimodal data that is hard to obtain in the real world, we collect OmniWorld-Game from game environments. Following prior works (Feng et al., 2024; Richter et al., 2016; Team et al., 2025a; Yang et al., 2024a), we utilize ReShade (ReShade Contributors, 2024) to access depth information during the rendering process, and simultaneously capture synchronized RGB images from the screen using OBS (Contributors, 2024). This approach offers significant advantages: 1) High-Precision Modal Data. We can precisely control the environment and acquire accurate depth data, which is often unattainable in real-world settings and is crucial for spatio-temporal modeling. 2) Rich Real-World Scene Simulation. Modern virtual environments provide highly realistic graphics and diverse simulations of real-world scenarios, encompassing complex settings from wilderness to urban areas, and from day to night. Robot Domain. We integrate public datasets from robot manipulation and human-robot interaction tasks, including AgiBot (Bu et al., 2025), DROID (Khazatsky et al., 2024), and RH20T (Fang et al., 2024). These datasets provide valuable sequences of robot-environment interactions and navigation, which are essential for tasks involving robotic manipulation and physical world understanding. Human Domain. We incorporate public datasets describing various human activities, including RH20T-Human (Fang et al., 2024), HOI4D (Liu et al., 2022), Epic-Kitchens (Damen et al., 2018), 4 Ego-Exo4D (Grauman et al., 2024), HoloAssist (Wang et al., 2023), Assembly101 (Sener et al., 2022), and EgoDex (Hoque et al., 2025). These datasets capture diverse human behaviors, ranging from daily activities to complex assembly tasks, from both egocentric and exocentric perspectives. Internet Domain. To acquire large-scale, realistic, and diverse in-the-wild scene data, we utilize the CityWalk dataset (Li et al., 2025). This dataset offers rich real-world street view videos from the internet. We specifically focus on supplementary camera pose annotation for this data, providing valuable real-world information for 3D geometry and camera pose estimation tasks. To prepare raw data for our annotation pipeline, we first perform video slicing to ensure all clips are of high quality and temporal coherence. This process has two main objectives: first, to remove frames unsuitable for geometric or motion analysis, such as those with motion blur, insufficient feature points, or excessively large dynamic areas; and second, to segment long videos into shorter, manageable clips. After this preprocessing step, the filtered, high-quality video segments are then passed to our multi-modal annotation pipeline. 2.2. Data Annotation To provide high-quality multi-modal annotation information, we design an innovative data processing pipeline. We primarily annotate the following key modalities: depth maps, camera poses, text captions, optical flow, and foreground masks (see Figure 2 for the overall pipeline). These modalities are crucial for models to achieve comprehensive spatio-temporal modeling. Here we briefly introduce the annotation method of each modality, please refer to supplementary material for more details. Depth maps. Accurate depth information is paramount for geometric modeling. To ensure the quality and consistency of depth maps, we adopt tailored approach based on the data source. For the self-collected dataset OmniWorld-Game, as mentioned in Section 2.1, we directly access depth information during the rendering process using tools like ReShade (ReShade Contributors, 2024). For public datasets AgiBot (Bu et al., 2025) and HOI4D (Liu et al., 2022), these datasets typically provide raw depth maps that are often noisy and sparse. We employ Prior Depth Anything (Wang et al., 2025e) to optimize these noisy depth maps, generating denser and more accurate depth maps. For the public stereo dataset DROID (Khazatsky et al., 2024), we leverage FoundationStereo (Wen et al., 2025) for stereo depth estimation on this dataset. Foreground masks. To provide precise, temporally consistent masks of primary subjects for tasks like subject-environment interaction and behavior analysis, we develop specialized automated pipelines. For robot domain data, we use RoboEngine (Yuan et al., 2025) to generate initial masks for keyframes, followed by temporal tracking and fusion with SAM 2 (Ravi et al., 2024). For OmniWorld-Game (e.g., player characters in third-person view), we leverage Grounding DINO (Liu et al., 2023) to detect initial bounding boxes within predefined regions of keyframes, which then serve as prompts for SAM (Kirillov et al., 2023). These generated masks can be used as dynamic foreground masks to guide camera pose estimation, as detailed in the following section. Camera poses. Accurate camera pose annotation in dynamic videos is highly challenging due to transitions, weakly textured areas, and abrupt movements that hinder traditional Structure-fromMotion methods (Li et al., 2024; Rockwell et al., 2025). Following prior work (Team et al., 2025a), we develop robust, automated, two-stage pipeline for dynamic camera pose annotation, whose principles are validated across diverse data types. The pipeline leverages the pre-computed foreground masks to focus on static background regions. 5 (a) OmniWorld Compositional Distribution (b) OmniWorld-Game Internal Composition (c) Caption Tokens Distribution Figure 3. Statistical information of OmniWorld. The left side displays compositional distribution of data from different domains within OmniWorld, while the right side presents the internal composition of OmniWorld-Game. The stages include: 1) Coarse camera pose estimation leveraging VGGT (Wang et al., 2025a) for videos without depth or DroidCalib (Hagemann et al., 2023) with depth constraints; 2) Camera pose refinement through dense point tracking (SIFT (Lowe, 2004), SuperPoint (DeTone et al., 2018) with CoTracker3 (Karaev et al., 2024)) on static regions and subsequent bundle adjustment to minimize reprojection errors, optionally enhanced by forward-backward reprojection with depth information (Chen et al., 2019). Text captions. We generate high-quality text descriptions for video sequences using semi-automated approach primarily driven by Qwen2-VL-72B-Instruct model (Wang et al., 2024a). We design specific prompting strategies tailored to different data domains. For robot and human domain data, we first annotate overall video tasks, then annotate in units of 81-frame segments. For OmniWorld-Game, we develop distinct prompts for various viewpoints (e.g., first-person, third-person), encompassing types such as short caption, player character caption, background caption, camera caption, video caption, and key tags, utilizing 81-frame segments. Optical flow. Optical flow, as dense motion vector field, is crucial for capturing pixel-level motion information in videos and serves as fundamental modality for accurate spatio-temporal modeling. We select DPFlow (Morimitsu et al., 2025) for optical flow annotation. Unlike mainstream models such as RAFT (Teed and Deng, 2020) which require downsampling inputs when processing high-resolution videos, DPFlow can directly perform predictions on the original resolution. Given that our dataset includes various resolutions, the choice of DPFlow ensures that the optical flow annotation accurately reflects subtle movements within the videos. 2.3. Data Statistics OmniWorld comprises 12 heterogeneous datasets from four domains: simulators, robots, humans, and the internet. Table 2 summarizes the key metadata for these datasets. OmniWorld collectively contains over 600 thousand video sequences and more than 300 million frames. Notably, our collection includes significant portion of high-resolution videos, with more than half of the data having resolution of 720P or higher. We meticulously annotate the data with multiple modalities, including depth, camera poses, text, optical flow, and foreground masks. 6 Figure 3a illustrates the compositional distribution of data from different domains within OmniWorld. Notably, data from the human domain constitutes the largest share, underscoring the datasets richness in reflecting real-world human activities and interactions. Figure 3b further elaborates on the internal composition of OmniWorld-Game, showcasing its high diversity across multiple dimensions. For scene type, OmniWorld-Game encompasses outdoor-urban, outdoor-natural, indoor, and mixed scenes, with outdoor-urban scenes having the highest proportion. For camera perspective, OmniWorld-Game includes both first-person and third-person-following perspectives, predominantly featuring first-person views. Regarding the historical era, OmniWorldGame covers diverse styles, including ancient, modern, and futuristic sci-fi periods. In terms of dominant object, OmniWorld-Game includes various types such as natural terrain, architecture, vehicles, and mixed elements. Most scenes incorporate multiple object types, significantly enhancing the datas challenge and complexity. These statistics collectively demonstrate that OmniWorld-Game exhibits an exceptionally diverse and challenging scene distribution. For the text modality, we provide comprehensive and detailed annotations. As shown in Figure 3c, our text captions primarily contain between 150 and 250 tokens per description. This rich annotation density significantly surpasses that of most existing video-text datasets, such as OpenVid-1M (Nan et al., 2024) and Panda-70M (Chen et al., 2024). 3. OmniWorld-Game Benchmark To comprehensively evaluate and advance world modeling, we construct OmniWorld-Game benchmark, providing comprehensive and challenging evaluation platform for two critical tasks: 3D geometric prediction and camera control video generation. 3.1. 3D Geometric Prediction Benchmark Benchmark design and motivation. Existing benchmarks for 3D geometric foundation models (GFMs) suffer from significant limitations. Specifically, many current benchmarks have the following drawbacks: First, sequence lengths are generally short, which restricts evaluating models ability in long sequences reconstruction. For instance, Sintel (Butler et al., 2012) video sequences average only 50 frames. Second, the dynamic motion in these datasets is relatively small in amplitude and uniform in type. For example, Bonn (Palazzolo et al., 2019) focuses on human dynamics in indoor scenes, NYU-v2 (Silberman et al., 2012) focuses on indoor static objects, and KITTI (Geiger et al., 2013) datasets only include outdoor street views, making it challenging to comprehensively test model performance in complex dynamic environments. To address this, OmniWorld-Game offers an advanced evaluation environment featuring extended temporal sequences (up to 16 seconds with 384 frames), rich and diverse motion, extreme scenarios with environmental diversity (e.g., mixed scene types), and high-resolution realistic data (720P). These characteristics allow for deeper and more comprehensive assessment of GFMs capabilities. Evaluated baselines and experiment details. We thoroughly assess current GFMs, including DUSt3R (Wang et al., 2024c), MASt3R (Leroy et al., 2024), MonST3R (Zhang et al., 2024), Fast3R (Yang et al., 2025), CUT3R (Wang et al., 2025b), FLARE (Zhang et al., 2025), VGGT (Wang et al., 2025a), and MoGe (Wang et al., 2024b, 2025c), within the OmniWorld-Game benchmark. These models are evaluated on two core tasks: monocular depth estimation and video depth estimation. All images are consistently resized to long side of 512 pixels while preserving aspect ratio. Quantitative analysis. Our quantitative analysis on OmniWorld-Game reveals key performance Method Mono-Depth Video-Depth scale scale scale&shift Abs Rel ùõø<1.25 Abs Rel ùõø<1.25 Abs Rel ùõø<1.25 DUSt3R (Wang et al., 2024c) MASt3R (Leroy et al., 2024) MonST3R (Zhang et al., 2024) Fast3R (Yang et al., 2025) CUT3R (Wang et al., 2025b) FLARE (Zhang et al., 2025) VGGT (Wang et al., 2025a) MoGe-1 (Wang et al., 2024b) MoGe-2 (Wang et al., 2025c) 0.742 0.485 0.670 0.755 0.624 0.664 0.531 0.459 0.401 0.460 0.560 0.493 0.404 0.518 0.475 0.554 0.586 0.589 0.709 0.482 0.669 0.741 0.690 0.757 0.440 0.447 0.579 0.505 0.384 0.479 0.453 0.625 0.379 0.217 0.272 0.464 0.429 0.511 0.194 0.560 0.724 0.648 0.531 0.603 0.527 0.755 FPS 0.96 0.79 0.95 14.99 10.75 4.24 18.75 Table 3. Monocular Depth & Video Depth Estimation on OmniWorld-Game benchmark. insights and bottlenecks. For monocular depth estimation, MoGe-2 achieves the best results, though significant room for improvement remains across models, underscoring the benchmarks challenge on single-frame geometric understanding  (Table 3)  . In the more demanding video depth estimation task, VGGT demonstrated superior performance across all metrics under both scale-only and scale-and-shift alignments, with significantly higher FPS than competitors. While MASt3R also showed competitive metrics, its low FPS due to global alignment limits its practicality  (Table 3)  . Overall, no single GFM achieves top-tier performance across all metrics, indicating that current SOTAs still face considerable challenges in handling the high-dynamic, long-sequence 3D geometric understanding and consistency problems introduced by OmniWorld-Game. Visual Results. In Figure 4, we provide visual comparison of the monocular depth prediction results from various methods on the OmniWorld-Game benchmark. As model specifically designed for monocular geometry tasks, MoGe-2 (Wang et al., 2025c) achieves superior accuracy and produces visually sharp depth maps, surpassing the performance of other multi-view methods. To show the challenges of video depth estimation on the OmniWorld-Game benchmark, we present qualitative comparison of feed-forward reconstruction methods using point cloud visualizations in Figure 5. The video-depth estimation task demands high temporal consistency. Our visualizations show that VGGT (Wang et al., 2025a) generates more coherent 3D structures than other methods in dynamic scenes. However, even VGGT shows noticeable artifacts, revealing limitations in capturing complex details. These observations indicate that the robustness of current methods needs improvement on OmniWorld-Game. Our benchmark provides clear direction for advancing the next generation of GFMs with stronger spatio-temporal consistency. 3.2. Camera Control Video Generation Benchmark Benchmark design and motivation. Existing benchmarks for camera control video generation often rely on static datasets with smooth camera trajectories (e.g., RealEstate10K (Zhou et al., 2018)), which do not reflect real-world complexity. OmniWorld-Game benchmark addresses this by providing challenging testing environment with rich dynamic content (e.g., diverse motions, complex interactions), extremely diverse scenes and environments (e.g., varied geographical, weather, lighting conditions), complex camera trajectories reflecting real patterns, and multi-modal input with diverse subjects (e.g., various perspectives, characters, vehicles). This enables rigorous evaluation of models Method Sintel Bonn KITTI NYU-v2 Abs Rel ùõø < 1.25 Abs Rel ùõø < 1.25 Abs Rel ùõø < 1.25 Abs Rel ùõø < 1.25 DUSt3R (Wang et al., 2024c) MonST3R (Zhang et al., 2024) DUSt3R* CUT3R (Wang et al., 2025b) CUT3R* 0.488 0.402 0.370 0.420 0.408 0.532 0.525 0.529 0.520 0.522 0.139 0.069 0.067 0.058 0. 0.831 0.954 0.948 0.967 0.944 0.109 0.098 0.088 0.097 0.087 0.873 0.895 0.932 0.914 0. 0.081 0.094 0.089 0.081 0.075 0.909 0.887 0.902 0.914 0.920 Table 4. Comparison of Original and Fine-tuned Models for Monocular Depth Estimation on Sintel (Butler et al., 2012), Bonn (Palazzolo et al., 2019), KITTI (Geiger et al., 2013) and NYU-v2 (Silberman et al., 2012). The notation * denotes models that have been fine-tuned on OmniWorld. Figure 4. Visual results of Monocular Depth Estimation on OmniWorld-Game benchmark. Figure 5. Qualitative comparison of multi-view 3D reconstruction on OmniWorld-Game benchmark. ability to handle complex spatio-temporal dynamics and adhere to precise control instructions. Evaluated baselines and experiment details. We benchmark mainstream SOTAs, including AC3D (Bahmani et al., 2024) (T2V), CamCtrl (He et al., 2024), MotionCtrl (Wang et al., 2024d), and CAMI2V (Zheng et al., 2024) (all I2V). These models represent different conditioned video generation models and are evaluated adhering to their default configurations. Following CAMI2V (Zheng et al., 2024), metrics include Camera Parameter Metrics (RotError, TransError, and CamMC) to quantify adherence to camera commands, and Fr√©chet Video Distance (FVD) (Unterthiner et al., 2018) to assess perceptual 9 Method TransErr RotErr CamMC FVD VideoGPT StyleGAN AC3D (T2V) (Bahmani et al., 2024) 6.2788 0. 6.6965 1745.778 1594.885 MotionCtrl (I2V) (Wang et al., 2024d) CamCtrl (I2V) (He et al., 2024) CAMI2V (I2V) (Zheng et al., 2024) 7.8633 1.2882 5.9626 1.1402 0.2022 0.5087 8.2710 1.3856 6. 694.342 615.417 837.185 745.652 637.574 742.594 Table 5. Camera Control Video Generation Evaluation on OmniWorld-Game benchmark. Figure 6. Visual results of Camera Control Video Generation models on OmniWorld-Game benchmark. In T2V setting, AC3D takes the text as condition signal. In I2V setting, MotionCtrl, CamCtrl, CAMI2V takes the image as condition signal. Condition images are the first images of each row. realism. Quantitative analysis. Our quantitative analysis on OmniWorld-Game reveals key insights and challenges. In the Text-to-Video task, AC3D showed basic camera control but high FVD, indicating the difficulty of generating high-fidelity, dynamic content with camera control and text prompts in complex scenes  (Table 5)  . For Image-to-Video models, CamCtrl achieves superior performance in both camera control accuracy and video quality. However, all evaluated SOTAs still exhibit significant room for improvement across OmniWorld-Game, especially in simultaneously ensuring video generation quality and precise camera control. This highlights ongoing challenges and future research directions. Visual results. To visually demonstrate the challenges posed by the OmniWorld-Game benchmark, 10 Method DUSt3R (Wang et al., 2024c) DUSt3R* CUT3R (Wang et al., 2025b) CUT3R* Align scale scale DUSt3R (Wang et al., 2024c) DUSt3R* scale&shift CUT3R (Wang et al., 2025b) CUT3R* scale&shift Sintel Bonn KITTI Abs Rel ùõø < 1.25 Abs Rel ùõø < 1.25 Abs Rel ùõø < 1.25 0.652 0.512 0.417 0. 0.570 0.520 0.537 0.314 0.436 0.456 0.510 0.516 0.493 0.480 0.556 0. 0.151 0.083 0.078 0.078 0.152 0.084 0.075 0.067 0.839 0.920 0.937 0. 0.835 0.914 0.944 0.964 0.143 0.135 0.123 0.107 0.135 0.136 0.111 0. 0.814 0.800 0.875 0.907 0.818 0.808 0.884 0.912 Table 6. Comparison of Original and Fine-tuned Models for Video Depth Estimation on Sintel (Butler et al., 2012), Bonn (Palazzolo et al., 2019) and KITTI (Geiger et al., 2013). The notation * denotes models that have been fine-tuned on OmniWorld. we present the qualitative results of various camera control video generation models in Figure 6. In the T2V setting, although AC3D (Bahmani et al., 2024) generates semantically coherent video content, the depicted human motion is minimal, and the model fails to accurately follow the input camera trajectory. This highlights fundamental limitation of current models in understanding and generating complex dynamic motions from abstract text instructions. In the I2V setting, while the camera trajectory of CamCtrls (He et al., 2024) generated video aligns well with the input conditions, the visual quality of moving characters is blurry, and the overall video quality is poor. Similar quality degradation issues are observed in the outputs of MotionCtrl (Wang et al., 2024d) and CAMI2V (Zheng et al., 2024). These results reveal the unique challenges of the OmniWorld-Game benchmark. 4. Model Fine-tuning and Efficacy Validation Through comprehensive experiments, we systematically validate OmniWorld as training source. We select baselines for two core tasks: 3D geometric foundation models and camera control video generation models, and fine-tuned them using OmniWorld. The experimental results clearly demonstrate that models fine-tuned with OmniWorld consistently achieve significant performance improvements over their original published versions, powerfully confirming OmniWorlds capabilities in spatio-temporal modeling. 4.1. Improving 3D Geometric Prediction with OmniWorld We select DUSt3R (Wang et al., 2024c), CUT3R (Wang et al., 2025b), and Reloc3r (Dong et al., 2024) as our primary baselines and conduct fine-tuning experiments on subsets of OmniWorld. The quantitative results confirm that models fine-tuned with OmniWorld consistently surpass their original performance across multiple critical tasks: monocular depth estimation  (Table 4)  , video depth estimation  (Table 6)  , and camera pose estimation. This outcome strongly demonstrates that OmniWorlds scale and diversity enable it to serve as valuable large-scale training source, effectively enhancing the generalization capabilities and robustness of 3D geometric foundation models. For monocular depth estimation  (Table 4)  , fine-tuned DUSt3R significantly outperformed its original baseline performance, even surpassing MonST3R, which is fine-tuned on multiple dynamic datasets (Mehl et al., 2023; Sun et al., 2020; Wang et al., 2020; Zheng et al., 2023). Similarly, CUT3R also showed improved performance after fine-tuning compared to the original baseline. 11 Method Benchmark TransErr RotErr CamMC AC3D (Bahmani et al., 2024) AC3D* AC3D (Bahmani et al., 2024) AC3D* RealEstate10K OmniWorld-Game 3.4433 2.8648 6.2788 4.1428 0.6308 0.5314 0.8867 0. 3.6615 3.0518 6.6965 4.4854 FVD VideoGPT StyleGAN 479.320 472. 409.795 416.948 1745.778 1594.885 1437.247 1249.1858 Table 7. Comparison of Original and Fine-tuned Models for Camera Control Video Generation Evaluation on RealEstate10K (Zhou et al., 2018) and OmniWorld-Game benchmark. The notation * denotes models that have been fine-tuned on OmniWorld. For video depth estimation  (Table 6)  , both DUSt3R and CUT3R exhibited enhanced performance after fine-tuning on OmniWorld, demonstrating OmniWorlds utility in improving temporal consistency. For camera pose estimation, please refer to supplementary materials. 4.2. Enhancing Camera Control Video Generation with OmniWorld Current public datasets for camera control video generation models have significant limitations. For example, most datasets like RealEstate10K (Zhou et al., 2018) primarily consist of static scenes and relatively smooth camera movements, which hinders models ability to generate dynamic video content. To address this data bottleneck and validate OmniWorlds effectiveness, we select AC3D (Bahmani et al., 2024) as our baseline and fine-tune it. Our experimental results further verify the finding from prior work (e.g., CAMERACTRL II (He et al., 2025a)), which highlight the critical importance of dynamic data for improving models camera control capabilities. The fine-tuned model is evaluated on two distinct benchmarks: random subset of 150 video samples from the RealEstate10K test set and OmniWorld-Game benchmark, which consists of 200 video samples. For fair comparison, all models are configured to output videos at uniform resolution of 720 480 with sequence length of 25 frames. As shown in Table 7, the model fine-tuned on OmniWorld significantly outperforms the original baseline model on both the RealEstate10K (Zhou et al., 2018) and OmniWorld-Game benchmarks. This outcome provides strong evidence that OmniWorld serves as an effective training resource, substantially enhancing the ability of controllable video generation models to follow precise camera control instructions in complex and dynamic scenarios. 5. Related Work 5.1. World Model Datasets The ability of models to perform world modeling is intrinsically linked to the availability of large-scale, high-quality spatio-temporal datasets. Static 3D datasets, such as ScanNet (Dai et al., 2017), NYU-v2 (Silberman et al., 2012), and MegaDepth (Li and Snavely, 2018), have advanced 3D reconstruction by providing precise geometric information. However, their static nature limits their utility for modeling motion and dynamic interactions. In video generation, large-scale video-text datasets (Bain et al., 2021; Chen et al., 2024; Ju et al., 2024; Nan et al., 2024) offer rich semantic annotations but lack geometric information (e.g., depth, camera poses, optical flow), making them unsuitable for applications requiring precise 3D 12 world modeling. To bridge this gap, researchers have created dynamic real-world datasets like KITTI (Geiger et al., 2013) and Waymo (Sun et al., 2020) for autonomous driving, and Bonn (Palazzolo et al., 2019), HOI4D (Liu et al., 2022), RH20T (Fang et al., 2024), and EPIC-Kitchens (Damen et al., 2018) for human-robot interaction. While valuable, these datasets often suffer from lack of scene diversity and noisy/sparse geometric annotations. The sim-to-real gap has been significantly reduced due to the advancement of modern rendering technology (Wang et al., 2020). Synthetic datasets have emerged as valuable alternative, providing rich and precise ground-truth annotations. Pioneers like MPI Sintel (Butler et al., 2012) are instrumental in optical flow research, but their small scale (e.g., an average sequence length of less than 50 frames) is insufficient for training large-scale foundation models. Other recent synthetic datasets, such as FlyingThings++ (Harley et al., 2022; Mayer et al., 2016), TartanAir (Wang et al., 2020), Dynamic Replica (Karaev et al., 2023) and Spring (Mehl et al., 2023), have made progress but still fall short in terms of scale, diversity, and modal richness compared to our self-collected OmniWorld-Game dataset, as shown in Table 1. The design of OmniWorld aims to systematically address these limitations. By integrating selfcollected OmniWorld-Game dataset and several public datasets from various domains, we provide highprecision geometric annotations and rich spatio-temporal dynamics, enabling more comprehensive evaluation and enhancement of world modeling. 5.2. 3D Geometric Foundation Models Recently, 3D geometric foundation models have emerged as data-driven alternative to traditional methods like Structure-from-Motion (SfM), capable of directly predicting scenes 3D structure in single feed-forward pass. Early works like DUSt3R (Wang et al., 2024c) and MonST3R (Zhang et al., 2024) operate on image pairs, requiring expensive global alignment for larger scenes. To overcome this, Fast3R (Yang et al., 2025) enables simultaneous inference on thousands of images. Other methods explore simplifying the learning task. FLARE (Zhang et al., 2025) decomposes the problem into separate pose and geometry prediction steps. CUT3R (Wang et al., 2025b) is an online model that continuously updates its state from an image stream. VGGT (Wang et al., 2025a) achieves superior performance through multi-task learning, while ùúã3 (Wang et al., 2025d) employs permutation-equivariant architecture to remove the dependency on fixed reference view. For monocular inputs, MoGe (Wang et al., 2024b, 2025c) achieves accurate monocular geometry estimation by predicting affine-invariant point maps. The performance of these methods is highly dependent on being trained on large-scale, multi-modal spatio-temporal datasets. When evaluated on the OmniWorld-Game benchmark, these methods show room for improvement, particularly when handling long sequences with highly dynamic, complex motions. By fine-tuning these models on OmniWorld, we achieve significant performance gains, powerfully demonstrating OmniWorlds value as an effective training resource for enhancing models spatio-temporal modeling capabilities. 5.3. Camera Control Video Generation Camera control video generation aims to empower users with the ability to control the camera within generated video. Most methods in this field inject camera parameters (such as extrinsics or Pl√ºcker embeddings) into pre-trained video diffusion model (Blattmann et al., 2023; Chen et al., 2023; Yang et al., 2024b) with representative works including MotionCtrl (Wang et al., 2024d), CameraCtrl (He 13 et al., 2024), CAMI2V (Zheng et al., 2024), and AC3D (Bahmani et al., 2024). Despite this progress, these methods still struggle to generate dynamic content with complex camera control. They are typically trained on datasets like RealEstate10K (Zhou et al., 2018) or DL3DV-10K (Ling et al., 2024), which consist of static scenes with smooth camera motions. This data limitation inherently restricts models ability to handle dynamic scenes (He et al., 2025a). Our experiments confirm this limitation. When evaluated on OmniWorld-Game benchmark, which features rich dynamics and complex camera movements, these methods show considerable room for improvement in both visual quality and camera control accuracy. By fine-tuning them on OmniWorld, their performance in dynamic scenes is significantly enhanced, demonstrating our datasets value for improving models spatio-temporal modeling capabilities. 6. Conclusion In this work, we introduce OmniWorld, large-scale, multi-domain, and multi-modal dataset designed to address the critical data bottleneck for world modeling. By integrating self-collected OmniWorldGame dataset and several public datasets from various domains, we create comprehensive data resource for world modeling. We demonstrate that OmniWorld-Game serves as challenging benchmark for 3D geometric foundation models and camera control video generation models, revealing the limitations of current SOTAs. Furthermore, we provide strong evidence that fine-tuning with OmniWorld significantly boosts the performance of these models, underscoring its value as powerful training resource. We believe that OmniWorld will serve as crucial data resource for the community, accelerating the development of more general and robust models for understanding and interacting with the real physical world."
        },
        {
            "title": "References",
            "content": "N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. S. Bahmani, I. Skorokhodov, G. Qian, A. Siarohin, W. Menapace, A. Tagliasacchi, D. B. Lindell, and S. Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. J. Bai, M. Xia, X. Fu, X. Wang, L. Mu, J. Cao, Z. Liu, H. Hu, X. Bai, P. Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. M. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu, T. Gebauer, B. Joffe, D. Kurz, A. Schwartz, and E. Shulman. ARKitscenes - diverse real-world dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=tjZjv_qh_CE. A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 14 Q. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, X. He, X. Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2025. D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for opIn European Conference on Computer Vision, 2012. URL https://api. tical flow evaluation. semanticscholar.org/CorpusID:4637111. H. Chen, M. Xia, Y. He, Y. Zhang, X. Cun, S. Yang, J. Xing, Y. Liu, Q. Chen, X. Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. J. Chen, H. Zhu, X. He, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, Z. Fu, J. Pang, et al. Deepverse: 4d autoregressive video generation as world model. arXiv preprint arXiv:2506.01103, 2025. T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang, and S. Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. Y. Chen, C. Schmid, and C. Sminchisescu. Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In Proceedings of the IEEE/CVF international conference on computer vision, pages 70637072, 2019. O. Contributors. Obs studio, 2024. URL https://obsproject.com/. A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie√üner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV), 2018. G. DeepMind. Genie 3. https://deepmind.google/discover/blog/ genie-3-a-new-frontier-for-world-models/, 2025. Accessed: 2025-08-27. D. DeTone, T. Malisiewicz, and A. Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 224236, 2018. S. Dong, S. Wang, S. Liu, L. Cai, Q. Fan, J. Kannala, and Y. Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. arXiv preprint arXiv:2412.08376, 2024. H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu. Rh20t: comprehensive robotic dataset for learning diverse skills in one-shot. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 653660. IEEE, 2024. R. Feng, H. Zhang, Z. Yang, J. Xiao, Z. Shu, Z. Liu, A. Zheng, Y. Huang, Y. Liu, and H. Zhang. The matrix: Infinite-horizon world generation with real-time moving control, 2024. URL https: //arxiv.org/abs/2412.03568. S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. A. Hagemann, M. Knorr, and C. Stiller. Deep geometry-aware camera self-calibration from video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 34383448, 2023. A. W. Harley, Z. Fang, and K. Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. H. He, C. Yang, S. Lin, Y. Xu, M. Wei, L. Gui, Q. Zhao, G. Wetzstein, L. Jiang, and H. Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025a. X. He, C. Peng, Z. Liu, B. Wang, Y. Zhang, Q. Cui, F. Kang, B. Jiang, M. An, Y. Ren, et al. Matrixgame 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025b. R. Hoque, P. Huang, D. J. Yoon, M. Sivapurapu, and J. Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. X. Ju, Y. Gao, Z. Zhang, Z. Yuan, X. Wang, A. Zeng, Y. Xiong, Q. Xu, and Y. Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. URL https: //arxiv.org/abs/2407.06358. N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. CVPR, 2023. N. Karaev, I. Makarov, J. Wang, N. Neverova, A. Vedaldi, and C. Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Doll√°r, and R. Girshick. Segment anything. arXiv:2304.02643, 2023. H. Le, P. Das, T. Mensink, S. Karaoglu, and T. Gevers. EDEN: Multimodal Synthetic Dataset of Enclosed garDEN Scenes. In Proceedings of the IEEE/CVF Winter Conference of Applications on Computer Vision (WACV), 2021. Y. LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. 16 V. Leroy, Y. Cabon, and J. Revaud. Grounding image matching in 3d with mast3r, 2024. Z. Li and N. Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Computer Vision and Pattern Recognition (CVPR), 2018. Z. Li, R. Tucker, F. Cole, Q. Wang, L. Jin, V. Ye, A. Kanazawa, A. Holynski, and N. Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In arxiv, 2024. Z. Li, C. Li, X. Mao, S. Lin, M. Li, S. Zhao, Z. Xu, X. Li, Y. Feng, J. Sun, Z. Li, F. Zhang, J. Ai, Z. Wang, Y. Wu, T. He, J. Pang, Y. Qiao, Y. Jia, and K. Zhang. Sekai: video dataset towards world exploration. arXiv preprint arXiv:2506.15675, 2025. L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, et al. Dl3dv-10k: In Proceedings of the IEEE/CVF large-scale scene dataset for deep learning-based 3d vision. Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. Y. Liu, Y. Liu, C. Jiang, K. Lyu, W. Wan, H. Shen, B. Liang, Z. Fu, H. Wang, and L. Yi. Hoi4d: 4d egocentric dataset for category-level human-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2101321022, June 2022. D. G. Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2):91110, 2004. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. L. Mehl, J. Schmalfuss, A. Jahedi, Y. Nalivayko, and A. Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. H. Morimitsu, X. Zhu, R. M. Cesar, X. Ji, and X.-C. Yin. Dpflow: Adaptive optical flow estimation with dual-pyramid framework. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1781017820, 2025. K. Nan, R. Xie, P. Zhou, T. Fan, Z. Yang, Z. Chen, X. Li, J. Yang, and Y. Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. E. Palazzolo, J. Behley, P. Lottes, P. Gigu√®re, and C. Stachniss. ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. arXiv, 2019. URL https: //arxiv.org/abs/1905.02082. N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. R√§dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Doll√°r, and C. Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. J. Reizenstein, R. Shapovalov, P. Henzler, L. Sbordone, P. Labatut, and D. Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 17 ReShade Contributors. ReShade, 2024. URL https://reshade.me/. S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European conference on computer vision, pages 102118. Springer, 2016. M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. Rockstar Games. Policy on Posting Copyrighted Rockstar Games Material, 2024. URL https://support.rockstargames.com/articles/7bNaeoMFTV0iUDGhStTXvz/ policy-on-posting-copyrighted-rockstar-games-material. C. Rockwell, J. Tung, T.-Y. Lin, M.-Y. Liu, D. F. Fouhey, and C.-H. Lin. Dynamic camera poses and where to find them. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1244412455, 2025. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. URL https://arxiv.org/abs/2210.08402. F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: large-scale multi-view video dataset for understanding procedural activities. CVPR, 2022. N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In European conference on computer vision, pages 746760. Springer, 2012. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 573580. IEEE, 2012. P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. Z. Tang, Y. Fan, D. Wang, H. Xu, R. Ranjan, A. Schwing, and Z. Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. A. Team, H. Zhu, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, J. Chen, C. Shen, J. Pang, and T. He. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025a. H. Team, Z. Wang, Y. Liu, J. Wu, Z. Gu, H. Wang, X. Zuo, T. Huang, W. Li, S. Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025b. Z. Teed and J. Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025a. 18 P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Q. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025b. R. Wang, S. Xu, C. Dai, J. Xiang, Y. Deng, X. Tong, and J. Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision, 2024b. URL https://arxiv.org/abs/2410.19115. R. Wang, S. Xu, Y. Dong, Y. Deng, J. Xiang, Z. Lv, G. Sun, X. Tong, and J. Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details, 2025c. URL https://arxiv.org/abs/ 2507.02546. S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024c. W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer. Tartanair: dataset to push the limits of visual slam. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. X. Wang, T. Kwon, M. Rad, B. Pan, I. Chakraborty, S. Andrist, D. Bohus, A. Feniello, B. Tekin, F. V. Frujeri, N. Joshi, and M. Pollefeys. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2027020281, October 2023. Y. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He. ùúã3: Scalable permutation-equivariant visual geometry learning, 2025d. URL https://arxiv.org/abs/2507. 13347. Z. Wang, Z. Yuan, X. Wang, Y. Li, T. Chen, M. Xia, P. Luo, and Y. Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024d. Z. Wang, S. Chen, L. Yang, J. Wang, Z. Zhang, H. Zhao, and Z. Zhao. Depth anything with any prior, 2025e. URL https://arxiv.org/abs/2505.10565. B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield. Foundationstereo: Zero-shot stereo matching. CVPR, 2025. H. Xia, Y. Fu, S. Liu, and X. Wang. Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2237822389, 2024. J. Xing, M. Xia, Y. Zhang, H. Chen, W. Yu, H. Liu, X. Wang, T.-T. Wong, and Y. Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. H. Yang, D. Huang, W. Yin, C. Shen, H. Liu, X. He, B. Lin, W. Ouyang, and T. He. Depth any video with scalable synthetic data. arXiv preprint arXiv:2410.10815, 2024a. J. Yang, A. Sax, K. J. Liang, M. Henaff, H. Tang, A. Cao, J. Chai, F. Meier, and M. Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025. 19 Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17901799, 2020. M. Ye, P. Yin, W.-C. Lee, and D.-L. Lee. Exploiting geographical influence for collaborative point-ofinterest recommendation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 325334, 2011. C. Yeshwanth, Y.-C. Liu, M. Nie√üner, and A. Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. M. YU, W. Hu, J. Xing, and Y. Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. In ICCV, 2025. C. Yuan, S. Joshi, S. Zhu, H. Su, H. Zhao, and Y. Gao. Roboengine: Plug-and-play robot data augmentation with semantic robot segmentation and background generation. arXiv preprint arXiv:2503.18738, 2025. J. Zhang, C. Herrmann, J. Hur, V. Jampani, T. Darrell, F. Cole, D. Sun, and M.-H. Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arxiv:2410.03825, 2024. L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models, 2023. S. Zhang, J. Wang, Y. Xu, N. Xue, C. Rupprecht, X. Zhou, Y. Shen, and G. Wetzstein. Flare: Feedforward geometry, appearance and camera estimation from uncalibrated sparse views, 2025. URL https://arxiv.org/abs/2502.12138. G. Zheng, T. Li, R. Jiang, Y. Lu, T. Wu, and X. Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. Y. Zheng, A. W. Harley, B. Shen, G. Wetzstein, and L. J. Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018. 20 A. Overview Section discusses more details of OmniWorld. Section and Section discuss more details of our benchmark and fine-tuning experiments. B. OmniWorld Dataset B.1. Data Statistics Figure 7. The OmniWorld-Game distribution of scene category (the primary POI locations). To quantitatively analyze the scene diversity of OmniWorld-Game, we adopt the methodology from DL3DV (Ling et al., 2024) to classify and count scenes across 16 Point-of-Interest (POI) categories (Ye et al., 2011). The statistical results are shown in Figure 7. OmniWorld-Game encompasses wide variety of scene categories, including \"Nature & Outdoors,\" \"Tourist Attractions,\" \"Parks and Recreation,\" and \"Hotels and Accommodations.\" \"Nature & Outdoors\" represents the largest share, reflecting its dominant presence in the dataset. The distribution of these scene categories aligns with their prevalence in the real world and the characteristics of the games themselves. For instance, scenes related to \"Government & Civic Services\" and \"Events & Conferences\" are typically less frequent in games, leading to their lower representation in our dataset. These statistics further validate the richness and real-world attributes of OmniWorld-Game. To provide more detailed analysis of the dominant \"Nature & Outdoors\" scenes in OmniWorldGame, we further subdivide this category into 5 second-level and 40 third-level categories. The detailed distribution is shown in Figure 8. Our statistics reveal that \"Natural Landforms & Ecosystems\" is the dominant second-level category. Within this category, scenes depicting \"Forests & Rainforests\" Figure 8. Scene Diversity within the \"Nature & Outdoors\" Category. quantitative breakdown of secondand third-level scene categories in OmniWorld-Game dataset, demonstrating the high internal diversity and distribution of natural environments. and \"Cliffs & Rock Formations\" are the most prevalent. \"Outdoor Sports & Scenic Routes\" is the second-largest category, with scenes of \"Rock-Climbing Areas\" and \"Scenic Drives & Viewpoints\" being particularly prominent. Additionally, \"Urban Outdoor Spaces & Activities\" and \"Agricultural & Rural Landscapes\" also make up small portion of the data. These detailed statistics confirm that the \"Nature & Outdoors\" scenes in OmniWorld-Game are not only abundant but also internally diverse. This rich composition provides diverse data source for world modeling in complex natural environments. B.2. Ethics Statements To ensure compliance, we strictly adhere to the terms of use for relevant game content (e.g. Rockstar Games (Rockstar Games, 2024)), including usage for non-commercial purposes only and avoiding story spoilers. We also automatically remove UI elements and text information via ReShade (ReShade Contributors, 2024) plugin and manually filter specific scenes to ensure no unauthorized content is disclosed. C. OmniWorld-Game Benchmark C.1. 3D Geometric Prediction Experiment Details. We adhere to the default configurations of each evaluated model. The entire evaluation process is conducted on single A800 GPU. For the monocular depth Estimation, we evaluate the first 200 frames of 18 test sequence from the OmniWorld-Game benchmark. Following the evaluation protocols of prior works (Wang et al., 2025b,d; Zhang et al., 2024), we focus on scale-invariant monocular depth accuracy. The primary evaluation metrics are Absolute Relative Error (Abs Rel) and threshold accuracy (ùõø < 1.25). Under this setting, the depth map of each frame is independently aligned with its corresponding ground truth. 22 Method ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot Sintel TUM-dynamics ScanNet CUT3R (Wang et al., 2025b) CUT3R* 0.210 0.178 0.071 0.055 0.627 0. 0.045 0.041 0.014 0.013 0.441 0.374 0.096 0.095 0.022 0.022 0.733 0. Table 8. Comparison of Original and Fine-tuned Models for Camera Pose Estimation on Sintel (Butler et al., 2012), TUM-dynamics (Sturm et al., 2012) and ScanNet (Dai et al., 2017). The notation * denotes models that have been fine-tuned on OmniWorld. For the video depth estimation, we select the first 100 frames of the same test sequence from the OmniWorld-Game benchmark. To ensure fair comparison across all models, we cap the input sequence length at 100 frames, as some models (e.g., FLARE (Zhang et al., 2025)) cannot handle longer sequences without errors. Similar to the mono depth estimation, we report Abs Rel and ùõø < 1.25. To more comprehensively evaluate depth consistency across video sequences, we provide results under two different alignment settings: (i) scale-only alignment (scale) and (ii) combined scale and translation alignment (scale & shift). These settings test models depth estimation capabilities under different constraints, particularly in handling motion and viewpoint changes. It is important to note that since the benchmark data is included in the training set of ùúã3 (Wang et al., 2025d), we did not evaluate it in our benchmark. C.2. More Visual Results. C.3. Camera Control Video Generation Experiment Details. AC3D (Bahmani et al., 2024) uses CogVideoX-5B (Yang et al., 2024b) as base T2V model, it generates 25 frames per inference at resolution of 480 720. CamCtrl (He et al., 2024) and MotionCtrl (Wang et al., 2024d) use Stable Video Diffusion (SVD) (Blattmann et al., 2023) as base I2V model and generate 14-frame video sequences at resolution of 320 512. CAMI2V (Zheng et al., 2024) uses DynamiCrafter (Xing et al., 2023) as base I2V model. It generates 16-frame video sequences at resolution of 320 512. For fair comparison with CamCtrl and MotionCtrl, we use the first 14 frames of its generated videos for evaluation. We use ùúã3 (Wang et al., 2025d) to get camera poses of the generated videos. All methods are evaluated on an A800 GPU. D. Model Fine-tuning D.1. Camera Pose Estimation. Following (Wang et al., 2025b,d), we report the Absolute Trajectory Error (ATE), Relative Pose Error for translation (RPE trans), and Relative Pose Error for rotation (RPE rot) on Sintel (Butler et al., 2012), TUM-dynamics (Sturm et al., 2012) and ScanNet (Dai et al., 2017). The results in Table 8 show that CUT3Rs performance notably improved after fine-tuning on OmniWorld in camera pose estimation. Following (Dong et al., 2024), we assess performance with three indicators: AUC@5/10/20, which measure the area under the pose accuracy curve. This curve is based on minimum thresholds of 5, 10, and 20 degrees for rotation and translation angular errors. Reloc3r demonstrated substantial improvements in its ability to estimate dynamic camera poses after fine-tuning on OmniWorld in relative camera pose evaluation  (Table 9)  . Method DynPose-100K OmniWorld-CityWalk AUC@5 AUC@10 AUC@20 AUC@5 AUC@10 AUC@20 Reloc3r (Dong et al., 2024) Reloc3r* 6.9 14. 15.4 25.5 27.1 37.8 33.3 42.5 49.4 58.0 63.1 70.3 Table 9. Comparison of Original and Fine-tuned Models for Relative Camera Pose Evaluation on DynPose-100K (Rockwell et al., 2025), OmniWorld-CityWalk(Li et al., 2025). The notation * denotes models that have been fine-tuned on OmniWorld. D.2. Implementation Details We conduct comprehensive fine-tuning experiments on several SOTAs to validate the efficacy of our OmniWorld as training resource. All experiments are performed on 8 NVIDIA A800 GPUs. DUSt3R (Wang et al., 2024c). For fine-tuning, we use OmniWorld-Game alongside portion of DUSt3Rs original training sets, including ARKitScenes (Baruch et al., 2021), MegaDepth (Li and Snavely, 2018), and Waymo (Sun et al., 2020). We load the pre-trained weights of DUSt3R and performed full fine-tuning. The model is fine-tuned on images with random resolutions (e.g., 288512, 384512, 336512). The training runs for 40 epochs, with each epoch consisting of 800 iterations. We use the AdamW optimizer with an initial learning rate of 2.5 105 and weight decay of 0.05. Each GPU had batch size of 7, with each batch containing two images. CUT3R (Wang et al., 2025b). We fine-tune CUT3R using OmniWorld-Game and subset of its original training data, including CO3Dv2 (Reizenstein et al., 2021), WildRGBD (Xia et al., 2024), ARKitScenes (Baruch et al., 2021), Waymo (Sun et al., 2020), and TartanAir (Wang et al., 2020). We load the pre-trained weights and follow the training strategy from CUT3Rs training stage 3. We fine-tune on higher-resolution images with varied aspect ratios, setting the maximum side to 512 pixels. The encoder is frozen, with only the decoder and heads being trained on longer sequences of 4 to 64 views. The model is fine-tuned for 2,000 iterations with total batch size of 96 and learning rate of 1.0 106, optimized by AdamW with weight decay of 0.05. Reloc3r (Dong et al., 2024). For fine-tuning Reloc3r, we utilize OmniWorld-Game, OmniWorldCityWalk, OmniWorld-HoloAssist, and OmniWorld-EpicKitchens, along with portion of its original training sets, including CO3Dv2 (Reizenstein et al., 2021), ARKitScenes (Baruch et al., 2021), Scannet++ (Yeshwanth et al., 2023), BlendedMVS (Yao et al., 2020), and MegaDepth (Li and Snavely, 2018). We load the pre-trained weights, freeze the ViT encoder, and only update the weights for the decoder and pose regression head. Fine-tuning is performed on images of random resolutions, including 288 512, 384 512, and 336 512. The model is trained for 80 epochs, with each epoch comprising 400 iterations. We use the AdamW optimizer with learning rate of 5.0 106 and weight decay of 0.05. Each GPU has batch size of 32, with each batch containing two images. AC3D (Bahmani et al., 2024). We fine-tune AC3D using OmniWorld-Game, OmniWorld-EpicKitchens, OmniWorld-HOI4D, OmniWorld-HoloAssist, OmniWorld-EgoExo4D, and OmniWorld-EgoDex, as well as the original training set, RealEstate10K (Zhou et al., 2018). We load the pre-trained weights of the AC3D ControlNet (Zhang et al., 2023), which is based on CogVideoX-5B (Yang et al., 2024b). Only the ControlNet model is fine-tuned, with other network structures frozen. The fine-tuning is performed on video clips of 49 frames with resolution of 352 640. The model is fine-tuned for 6,000 iterations with total batch size of 8 and learning rate of 5.0 105, optimized by AdamW with weight decay of 0.0001. 24 Figure 9. Qualitative comparison of DUSt3R (Wang et al., 2024c) and CUT3R (Wang et al., 2025b) on the Sintel (Butler et al., 2012) subset of the Video Depth Estimation benchmark. The notation * denotes models that have been fine-tuned on OmniWorld. After fine-tuning, both models recover finer geometric details and produce more accurate depth maps, highlighting the efficacy of OmniWorld as geometric supervision source. Figure 10. Visual comparison of AC3D (Bahmani et al., 2024) fine-tuned on OmniWorld. The visualizations show that fine-tuning with our dataset significantly improves the models ability to generate videos that more accurately follow camera trajectories and maintain higher temporal consistency for moving objects. D.3. Visual Results. Figure 9 provides qualitative comparison of DUSt3R (Wang et al., 2024c) and CUT3R (Wang et al., 2025b) on the Sintel (Butler et al., 2012) subset of the Video Depth Estimation benchmark, evaluated both before and after fine-tuning on OmniWorld. After fine-tuning, both models recover finer geometric details and generate more accurate depth maps. These results indicate that OmniWorld offers strong geometric supervision and can substantially enhance models geometric prediction capability. Figure 10 presents visual comparison of AC3D (Bahmani et al., 2024) on the OmniWorld-Game benchmark before and after fine-tuning on the OmniWorld dataset for the camera control video generation task. The visualizations clearly show that after fine-tuning, the generated videos more closely follow the desired camera trajectory and exhibit higher temporal consistency for moving objects. This demonstrates that OmniWorld can significantly enhance models ability to model dynamics."
        }
    ],
    "affiliations": [
        "Shanghai AI Lab",
        "ZJU"
    ]
}