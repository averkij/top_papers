{
    "paper_title": "FedPS: Federated data Preprocessing via aggregated Statistics",
    "authors": [
        "Xuefeng Xu",
        "Graham Cormode"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 0 7 8 0 1 . 2 0 6 2 : r FedPS: Federated data Preprocessing via aggregated Statistics Xuefeng Xu University of Warwick xuefeng.xu@warwick.ac.uk Graham Cormode University of Oxford graham.cormode@cs.ox.ac.uk February 12, 2026 Abstract Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical In practical FL systems, privacy for model performance but is largely overlooked in FL research. constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments."
        },
        {
            "title": "Introduction",
            "content": "Data preprocessing (Garcıa et al., 2016) is vital stage of the machine learning pipeline, transforming raw inputs into clean, structured, and analyzable forms. In tabular data, common preprocessing tasks include handling missing values, normalizing feature scales, and encoding categorical variables. Effective preprocessing improves model accuracy, accelerates convergence, and enhances interpretability. Yet despite its importance to model performance, preprocessing remains largely neglected in federated learning (Kairouz et al., 2021), where multiple entities collaboratively train model on decentralized data. Most federated learning research focuses on improving training algorithms (McMahan et al., 2017; Stich, 2019; Karimireddy et al., 2020; Li et al., 2020a,b), typically assuming that data has already been cleaned and transformed. This assumption hides significant practical bottleneck: without consistent and reliable preprocessing, even state-of-the-art federated learning algorithms fail to achieve their full potential. In practical federated systems, preprocessing introduces distinct challenges. Privacy constraints prohibit centralizing raw data for joint preparation, communication efficiency limits the information clients can exchange, and data heterogeneity across clients complicates the design of consistent preprocessing pipelines. We discuss several possible strategies for preprocessing in FL and highlight their limitations. Option 1: Centralized preprocessing. Many simulation-based FL studies preprocess the data centrally before partitioning it among clients. While this ensures consistent preprocessing and strong baselines, it is infeasible in real deployments. Centralized preprocessing requires collecting raw data, which directly violates FLs foundational privacy constraints. Thus, it is suitable only for simulation, not practical for FL. Option 2: No preprocessing. One may simply train on raw, unprocessed data, avoiding privacy issues but severely compromising model performance. Real-world data is often incomplete, inconsistent, or heterogeneously scaled, all of which harm convergence and generalization. As shown in Section 5, models trained without preprocessing perform substantially worse than those using properly prepared data. Option 3: Transfer preprocessing. Another approach is to reuse preprocessing parameters derived from public datasets or pretrained models (Qi and Wang, 2024). While this avoids learning from private data, its success hinges on the similarity between public and private datasets, an unrealistic assumption Figure 1: Illustration of inconsistent local scaling. Clients and hold data with different label distributions. The raw data is linearly separable (left). After each client applies local standardization, the combined data becomes no longer linearly separable (right). Figure 2: Overview of federated data preprocessing in FedPS. in heterogeneous FL environments. Transfer techniques may handle basic format alignment but fail for distribution-dependent tasks such as imputation or discretization. Option 4: Local preprocessing. Each client may preprocess its own data locally, preserving privacy but sacrificing cross-client consistency. In non-IID (independent and identically distributed) settings (Li et al., 2022), local transformations (e.g., normalization) can distort the global data distribution. Figure 1 illustrates how independent standardization can render previously well-separated classes non-separable. Our experiments (Section 5) show that inconsistent local preprocessing may degrade performance even below using raw data, underscoring the need for coordinated preprocessing. Option 5: Federated preprocessing. Federated preprocessing overcomes the limitations above by coordinating preprocessing without exposing raw data. As illustrated in Figure 2, this approach consists of five steps: 1 Compute local statistics; 2 Share and aggregate statistics; 3 Derive preprocessing parameters; 4 Broadcast parameters to clients; 5 Apply preprocessing locally. This paradigm ensures consistency, keeps raw data local, and addresses challenges posed by non-IID data distributions. We present FedPS1, unified framework and open-source library for federated data preprocessing. FedPS comprises two complementary components: (1) general workflow for federated preprocessing based on aggregated statistics, illustrated in Figure 2, and (2) comprehensive suite of preprocessing methods that instantiate this workflow. To demonstrate the frameworks flexibility and practical utility, we implement broad range of preprocessing techniques spanning scaling, encoding, transformation, discretization, and imputation. These methods leverage data-sketching techniques (Cormode and Yi, 2020) to support communication-efficient computation of complex global statistics (e.g., quantiles, frequent items). We further extend core preprocessing-related models, including k-Means (Lloyd, 1982), k-Nearest Neighbors, and Bayesian Linear Regression (Tipping, 2001), to both horizontal and vertical FL settings, making the library flexible 1https://github.com/xuefeng-xu/fedps 2 and applicable across diverse federated data preprocessing scenarios. Our main contributions are summarized as follows: We introduce FedPS, unified framework for federated preprocessing that maintains consistency across clients through summarization, aggregation, and parameter distribution (Section 3.1). We implement comprehensive preprocessing methods leveraging data-sketching techniques for communicationefficient computation of complex global statistics (Section 3.1). We analyze the sufficient statistics and communication costs required by different preprocessing operations, providing practical guidance for scalable deployment in federated settings (Section 3.2). We develop federated Bayesian linear regression for both horizontal and vertical settings, enabling sophisticated model-based preprocessing while avoiding cross-client feature interactions (Section 4). Our empirical results across various datasets indicate that federated preprocessing significantly surpasses both local preprocessing and raw data baselines, particularly in heterogeneous data contexts, thereby confirming the efficacy of federated preprocessing (Section 5). The rest of the paper is organized as follows. Section 2 reviews techniques foundational to federated preprocessing. Section 3 presents the FedPS framework. Section 4 develops federated Bayesian linear regression. Section 5 reports empirical results, followed by discussion in Section 6 and conclusions in Section 7."
        },
        {
            "title": "2.1 Data Preprocessing",
            "content": "Data preprocessing refers to the collection of techniques used to prepare raw data for downstream analysis or modeling. In tabular data, each column represents feature, and preprocessing is often applied to individual columns, although operations involving multiple columns are also possible. Common steps include feature scaling, encoding, discretization, imputation of missing values, and other transformations tailored to specific learning tasks. Mature software packages such as Scikit-learn (Pedregosa et al., 2011) provide standardized and reliable implementations of these techniques."
        },
        {
            "title": "2.2 Federated Learning",
            "content": "Federated learning is distributed paradigm where multiple clients collaboratively train model without sharing their raw data. central challenge is communication cost, since exchanging large volumes of information can be inefficient. The FedAvg algorithm (McMahan et al., 2017) mitigates this by allowing several rounds of local updates before aggregation, thereby reducing the required number of communication rounds. Another difficulty is data heterogeneity. Clients may hold data drawn from different distributions, and these differences can lead to divergent updates when local models are combined by simple averaging. variety of methods have been proposed to improve stability under heterogeneous data (Li et al., 2020a,b). Federated learning is typically divided into two settings based on data partitioning. Horizontal FL is when clients share the same feature space but hold different examples. Vertical FL is when clients share common identifier space but possess different feature spaces. Since most preprocessing methods operate on each feature separately, our main focus is on the horizontal setting, with vertical extensions introduced when required."
        },
        {
            "title": "2.3 Statistics Aggregation",
            "content": "In federated learning, each client computes local statistics or summaries and transmits them to the server, which then aggregates them into global quantities. Basic statistics such as the minimum, maximum, sum, mean, and variance are straightforward to compute in distributed manner with minimal communication. Set union can also be performed by merging local sets. More complex quantities, such as quantiles and frequent items, require specialized algorithms to achieve both accuracy and communication efficiency. 3 Quantiles. Quantiles divide the dataset into equal sized partitions. Computing them exactly requires storing the entire dataset (Munro and Paterson, 1980), which is impractical in federated settings due to the associated communication cost. Approximate quantile algorithms based on data sketches are therefore preferred. Examples include the KLL sketch (Karnin et al., 2016), which provides additive error guarantees, and the REQ sketch (Cormode et al., 2023), which provides multiplicative error guarantees. Both are designed to operate efficiently in distributed environments. Frequent Items. Identifying the most common items in dataset requires counting all occurrences, which is expensive when performed exactly. Frequent item sketches (Anderson et al., 2017) provide compact approximation. In our implementation, we use the DataSketches library (The DataSketches Authors, 2023), which offers an effective balance between accuracy and communication cost."
        },
        {
            "title": "2.4 Bayesian Linear Regression",
            "content": "Bayesian linear regression (BLR) (Tipping, 2001; Bishop, 2006) is an important tool within preprocessing, used for imputation of missing values. It formulates linear regression within probabilistic framework by placing prior distribution over the model parameters ω. common choice is an isotropic Gaussian prior with zero mean p(ω α) = (ω 0, α1I), where α denotes the prior precision. Given the data matrix and label Y, and assuming Gaussian noise precision β, the posterior distribution over ω is also Gaussian: p(ω X, Y, β) = (ω ˆω, Σ), with posterior mean and covariance given by: ˆω = βΣ1XY, Σ = αI + βXX. (1) The hyperparameters α and β follow Gamma hyperpriors p(α) = Gamma(α a1, a2), p(β) = Gamma(β b1, b2), where Gamma(α a, b) = Γ(a)1baαa1ebα. Since α and β cannot be computed in closed form, they are updated iteratively together with ˆω and Σ as described in Appendix of Tipping (2001): α = γ + 2a1 ε + 2a , β = γ + 2b1 ˆω2 2 + 2b2 , γ = (cid:88) αΛi β + αΛi , ε = Xˆω2 2. (2) Here, Λi denotes the i-th eigenvalue of XX. Equivalently, letting = USV be the singular value decomposition of X, we have Λ = S2. Using this decomposition, the inverse of Σ in Equation (1) can be computed efficiently as Σ1 = V(αI + βΛ)1V, which avoids explicitly inverting large dense matrix."
        },
        {
            "title": "3 Federated Data Preprocessing",
            "content": "In this section, we present FedPS, unified framework for federated preprocessing via aggregated statistics, describing representative methods and analyzing their communication costs."
        },
        {
            "title": "3.1 The Framework",
            "content": "Federated preprocessing follows the workflow in Figure 2: clients compute local statistics (Step 1 ), which are aggregated at the server (Step 2 ). The server derives preprocessing parameters (Step 3 ), broadcasts them to clients (Step 4 ), who apply them locally (Step 5 ). We outline representative preprocessors. Why these preprocessors? We select these methods to span broad range of statistical complexity and practical relevance in federated learning. These are core techniques implemented in standard machine learning libraries, particularly Scikit-learn, which makes them both familiar to practitioners and representative of real-world preprocessing pipelines. StandardScaler represents perhaps the most commonly used normalization technique and relies only on firstand second-order moments. KBinsDiscretizer captures discretization methods based on ranges, quantiles, and clustering, allowing us to illustrate the use of quantile sketches and federated k-Means. KNNImputer and IterativeImputer represent substantially more complex imputers that operate in both horizontal and vertical settings and rely on nontrivial federated primitives such as k-NN regression and Bayesian linear regression. Together, these methods demonstrate how FedPS supports preprocessing from simple aggregations to iterative, model-based procedures, while maintaining compatibility with widely-used preprocessing libraries. 4 Table 1: Preprocessors and associated statistics."
        },
        {
            "title": "Encoding",
            "content": "Transformation Discretization Imputation"
        },
        {
            "title": "MaxAbsScaler\nMinMaxScaler\nStandardScaler\nRobustScaler\nNormalizer",
            "content": "x/xmax (x xmin)/(xmax xmin) (x µ)/σ (x Q2)/(Q3 Q1) x/x Max Min, Max Mean, Variance Quantile Sum, Max"
        },
        {
            "title": "PowerTransformer\nQuantileTransformer\nSplineTransformer",
            "content": "Binarizer KBinsDiscretizer SimpleImputer KNNImputer IterativeImputer one-hot(y) multi-hot(y) ordinal(y) one-hot(x) ordinal(x) λ(ni) niY ni + (1 λ(ni)) nY ψ(λ, x) CDF(x), Φ1(CDF(x)) B-spline(x) Set Union Set Union Set Union Set Union, Frequent items Set Union, Frequent items Set Union, Mean, Variance Sum, Mean, Variance Quantile Min, Max, Quantile 1 if > else 0 if Tj < Tj+1 Min, Max, Quantile, Mean mean(x), median(x), freq(x) Mean, Quantile, Freq-items HFL: Min, Mean; VFL: Sum Sum mean(k-NN of x) RegressionModel(x) x2 , the sum of values = (cid:80) StandardScaler computes global mean and variance. Each client reports three quantities: the sum of squared values = (cid:80) xi, and the number of samples (Step 1 ). The server aggregates these statistics by summation to obtain (S, C, ) (Step 2 ). After aggregation, the global mean is µ = C/N and the variance is σ2 = S/N µ2 (Step 3 ). This requires one data pass and one communication round. The server then sends µ and σ to clients (Step 4 ), who apply the transformation (x µ)/σ (Step 5 ). KBinsDiscretizer partitions continuous features into discrete bins using one of three strategies: uniform, quantile-based, or clustering-based. For uniform binning, clients compute local min/max values, which are aggregated at the server to obtain global bounds. The server sends these values to clients, who derive equal-width bin edges and use them for discretization. For quantile-based binning, clients construct local quantile sketches (Step 1 ), which the server merges to form global sketch (Step 2 ). The resulting quantile-based bin edges are computed centrally (Step 3 ) and sent back to clients for discretization (Steps 4 and 5 ). For clustering-based binning, we employ federated k-Means (Appendix A.1). Clients iteratively contribute sufficient statistics of cluster assignments, while the server updates centroids by computing the global mean of points in each cluster. Final bin boundaries derived from the centroids are shared with clients. KNNImputer fills missing values using the mean of the nearest neighbors, based on distance function that accounts for missing coordinates and normalizes by the number of valid comparisons (Dixon, 1979). We implement this via federated k-Nearest Neighbors Regression (Appendix A.2). In the horizontal setting, each client computes distances between its local samples and query samples and reports its smallest distances to the server. The server aggregates these candidates to identify the global nearest neighbors and requests the corresponding feature values for imputation. In the vertical setting, distance computation is distributed across clients according to their feature subsets. Each client contributes partial distances, which the server aggregates and normalizes to determine the nearest neighbors used for imputation. IterativeImputer treats each feature with missing values as regression problem, predicting missing entries from all other features in an iterative, round-robin manner (Buck, 1960). This procedure resembles simplified form of MICE (Buuren and Groothuis-Oudshoorn, 2011). The regression model is implemented using Federated Bayesian Linear Regression (Section 4). In each iteration, clients contribute the necessary second-order statistics, such as XX in the horizontal setting or XX in the vertical setting. The server aggregates these statistics to compute global regression parameters, which are then used to update missing 5 values before proceeding to the next iteration. Other preprocessing methods, including MinMaxScaler , OrdinalEncoder , PowerTransformer , and others, follow the same principle: identify the sufficient statistics, aggregate them across clients, compute global parameters at the server, and broadcast them back. For clarity, we group methods into five categories: scaling, encoding, transformation, discretization, and imputation. Table 1 summarizes the sufficient statistics for each method, with further details in Appendix B. Data heterogeneity is common challenge in federated learning. Fortunately, the aggregated statistics we need are insensitive to distribution shifts among clients. Simple statistics such as minimum, maximum, sums, means, and variances are exact after aggregation. Quantile and frequent-item sketches maintain theoretical guarantees regardless of local distribution differences, making them well suited to heterogeneous environments."
        },
        {
            "title": "3.2 Communication Overhead Analysis",
            "content": "Communication overhead is critical element of federated learning. To assess the communication efficiency of different preprocessing methods, we analyze the statistics required by each method together with the resulting number of communication rounds and per client communication cost. Some preprocessors depend on single aggregated statistic, while others require multiple statistics or iterative protocols. The total overhead is therefore determined by the combination of required statistics and their aggregation frequency. We use the following parameters: n, m: number of samples and number of features in the training set. n: number of samples in the test set. t: number of iterations in k-Means, power transform, and iterative imputation. k: method-dependent parameter such as number of clusters in k-Means, number of neighbors in k-Nearest Neighbors, or sketch size in frequent-item sketch. d: number of distinct categories (for encoding tasks). Table 2 reorganizes preprocessing methods by the type of aggregated statistic they require, rather than by functionality. This view complements Table 1, which identifies sufficient statistics for each method. Here, our goal is to make explicit how different statistical primitives translate into communication rounds and asymptotic costs per client under horizontal and vertical data partitioning. When preprocessor relies on multiple statistics, its communication cost is the sum of the corresponding components. Table 2 summarizes the communication cost of aggregating each class of statistics. Simple statistics such as minimum, maximum, sum, mean, and variance require constant amount of communication per feature, resulting in O(m) cost per client. Normalizer needs sums or maxima per row, which gives O(n) total cost. More complex methods incur higher overhead due to sketch-based summaries, iterative procedures, or pairwise sample interactions, as exemplified by quantile estimation, clustering-based discretization, and k-nearest neighbor imputation. Overall, the table highlights how FedPS supports wide range of preprocessing methods while making their communication requirements explicit and comparable. Encoding methods based on set union depend on the number of distinct categories d, while TargetEncoder requires mean and variance per category, giving O(d) cost. Frequent-item sketches cost O(k log d) per feature. Quantile-based methods rely on KLL sketches (Karnin et al., 2016), whose communication cost follows the sketch size determined by error parameters ϵ and δ. REQ sketches (Cormode et al., 2023) can also be used when relative error guarantee is needed. For KBinsDiscretizer with k-means-based binning, the cost is O(tkm). Quantile-based binning inherits the KLL sketch cost, while uniform binning requires only O(m) communication since only min and max statistics are needed. For KNNImputer , the cost depends on the size of test set n. In the horizontal setting, each client sends O(nk) distances per feature, and once the server identifies the nearest neighbors, the relevant clients send the associated O(nk) values in the worst case. This results in cost of O(nkm) per client. In the vertical setting, each client computes partial distances from all training samples to all test samples, yielding cost of O(nn). 6 Table 2: Aggregated statistics and associated preprocessors."
        },
        {
            "title": "Associated Preprocessors",
            "content": "Partitioning Comm. Round Comm. Cost (Client) Min / Max"
        },
        {
            "title": "Sum",
            "content": "Mean Variance Quantiles Set Union MaxAbsScaler MinMaxScaler Normalizer (max norm) KBinsDiscretizer (uniform) SplineTransformer (uniform) KNNImputer Normalizer (l1 or l2 norm) PowerTransformer KNNImputer IterativeImputer IterativeImputer StandardScaler (µ = 0) SimpleImputer (mean) TargetEncoder PowerTransformer (µ = 0) KBinsDiscretizer (kmeans) KNNImputer StandardScaler (σ = 1) TargetEncoder PowerTransformer RobustScaler KBinsDiscretizer (quantile) QuantileTransformer SplineTransformer (quantile) SimpleImputer (median) LabelBinarizer MultiLabelBinarizer LabelEncoder OneHotEncoder OrdinalEncoder TargetEncoder Freq-items OneHotEncoder (ignore infreq.) OrdinalEncoder (ignore infreq.) SimpleImputer (most-frequent)"
        },
        {
            "title": "Horizontal\nHorizontal\nHorizontal\nHorizontal\nHorizontal\nHorizontal",
            "content": "Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal Horizontal 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 O(m) O(m) O(n) O(m) O(m) O(nkm) O(n) O(m) O(nn) O(tm2 min(n, m)) O(tmn(min(n, m) + t)) O(m) O(m) O(dm) O(m) O(tkm) O(nkm) O(m) O(dm) O(tm) O( 1 ϵ log2 log 1 δ m) O(d) O(dm) O(k log m) For IterativeImputer , the algorithm iterates over all features with missing values (worst case m) for rounds. In each round, one feature is treated as the target and regressed on all other features using Federated Bayesian Linear Regression. The communication cost per iteration depends on the sufficient statistics aggregated by BLR. For the horizontal setting, BLR costs O(m min(n, m)) (Theorem 4.1), and for the vertical setting, it costs O(n min(n, m) + nt) (Theorem 4.3). Since IterativeImputer performs iterations over features, the total communication cost is O(tm2 min(n, m)) for the horizontal setting and O(tmn(min(n, m) + t)) for the vertical setting. 7 Algorithm 1 Horizontal Federated BLR (Server) Y(c) 1: Input: Client holds X(c) and Y(c) 2: Initialize α and β 3: Aggregate XY = (cid:80) X(c) X(c) 4: Aggregate XX = (cid:80) 5: Compute eigenvalues Λ and eigenvectors of XX 6: repeat 7: 8: 9: Compute Σ1 = V(αI + βΛ)1V Compute ˆω = βΣ1XY Broadcast ˆω to all clients Aggregate global error ε = (cid:80) Update α and β using Equation (2) Y(c) X(c) ˆω2 2 X(c) 10: 11: 12: until Convergence or maximum number of iterations reached 13: Output: model parameter ˆω"
        },
        {
            "title": "4 Federated Bayesian Linear Regression",
            "content": "Bayesian Linear Regression (BLR) (Section 2.4) is foundational technique used within IterativeImputer to model conditional feature distributions. Unlike simpler preprocessing models such as k-Means or k-Nearest Neighbors, which rely on straightforward sufficient statistics, BLR maintains full posterior distribution over model parameters and iteratively refines hyperparameters α and β. This iterative refinement is essential for accurate imputation but introduces complexity absent in one-shot aggregation methods. In federated settings, the key challenge is computing sufficient statistics for parameter updates without exposing raw data, while accommodating the distinct computational patterns of horizontal versus vertical data partitioning. We develop federated BLR for both settings to illustrate how FedPS extends beyond simple aggregations to support sophisticated model-based preprocessing with multiple communication rounds. The horizontal variant follows standard sufficient-statistic aggregation and serves as baseline. The vertical variant introduces an algebraic reformulation that enables exact Bayesian inference without cross-client feature interactions. Both variants iteratively update model parameters until convergence."
        },
        {
            "title": "4.1 Horizontal Federated BLR",
            "content": "In the horizontal setting, samples are partitioned across clients, and each client holds all features for its local data. BLR depends on the sufficient statistics XY and XX, both of which decompose additively across clients. Each client computes its local contributions X(c) X(c), which are aggregated by summation at the server (Algorithm 1, steps 3-4). For example, XX is computed as: Y(c) and X(c) XX = (cid:104) X(1) X(2) (cid:105) . . . X(1) X(2) ... = (cid:88) X(c) X(c). (3) This procedure is equivalent to centralized BLR and avoids iterative FedAvg-style averaging (McMahan et al., 2017), since the sufficient statistics are exact after aggregation. We include this formulation for completeness and as reference point for comparison with the vertical setting. To avoid repeatedly inverting the current covariance matrix Σ in each iteration, the server performs an eigenvalue decomposition of XX once: XX = VΛV (step 5). This enables fast updates of Σ1 in each iteration (step 7). The server then computes ˆω and broadcasts it to clients (step 8-9). Clients compute their local reconstruction error contributions, which are summed at the server to obtain the global error ε (step 10), followed by updates to α and β using Equation (2). The full procedure is outlined in Algorithm 1, with blue highlighted steps indicating communication from clients to server, and orange highlighted steps indicating communication from server to clients. In 8 Algorithm 2 Vertical Federated BLR (Server) 1: Input: Client holds feature block X(c); one client holds 2: Initialize α and β 3: Receive from the client holding the target X(c)X(c) 4: Aggregate XX = (cid:80) 5: Compute eigenvalues Λ and eigenvectors of XX 6: repeat 7: 8: 9: 10: 11: 12: Compute ˇΣ1 = U(αI + βΛ)1U Broadcast β ˇΣ1Y to all clients Each client computes ˆω(c) = X(c) Aggregate ˆY = (cid:80) X(c) ˆω(c) Compute error ε = ˆY2 2 2 = (cid:80) Aggregate ˆω2 ˆω(c) Update α and β using Equation (2) 2 2 β ˇΣ1Y 13: 14: until Convergence or maximum number of iterations reached 15: Output: model parameter ˆω each iteration, clients communicate only scalar errors, making the iterative refinement phase communicationefficient. The dominant cost comes from the initial aggregation of sufficient statistics. Theorem 4.1. The communication cost per client of Horizontal Federated BLR is O(m min(n, m)). Proof. Since X(c) has size m, in step 3 clients communicate X(c) Y(c) of size O(m). In step 4, the aggregate X(c) X(c) is O(m2), but can be reduced to O(m min(n, m)) via local eigenvalue decomposition. During iterative refinement, clients communicate only scalar errors at step 10, which is negligible. Thus, the dominant cost is the initial aggregation of X(c) X(c), yielding O(m min(n, m)) per-client communication."
        },
        {
            "title": "4.2 Vertical Federated BLR",
            "content": "In the vertical setting, features are partitioned across clients while samples are aligned. Standard BLR requires XX, whose off-diagonal blocks X(j) X(k) (j = k) involve cross-client feature interactions that cannot be computed without sharing raw data. XX = X(1) X(2) ... (cid:2)X(1) X(2) . . .(cid:3) = X(1) X(2) ... X(1) X(1) X(1) X(2) X(2) X(2) . . . . . . . (4) To avoid this limitation, we reformulate the posterior mean computation using an equivalent expression based on XX instead of XX: (5) Although ˇΣ is not the posterior covariance, this reformulation yields the exact same posterior mean as the standard BLR solution. Crucially, XX decomposes additively across clients as (cid:80) , eliminating all cross-client feature products. X(c)X(c) ˆω = βX ˇΣ1Y, ˇΣ = αI + βXX. Theorem 4.2. The posterior mean ˆω computed by the standard BLR formulation in Equation (1) is identical to that computed by the reformulated expression in Equation (5). Proof. To prove the equivalence of the two formulations, we show that (αI + βXX)1X and X(αI + βXX)1 are equal. Apply the Woodbury matrix identity (Golub and Van Loan, 2013) with = αI, = X, 9 = βX. Then we have (A + )1U = (αI + βXX)1X. corollary of the Woodbury identity gives (A+U )1U = A1U (I +V A1U )1. Substituting and simplifying yields A1U (I +V A1U )1 = α1X(I+ βXα1X)1. Factoring α1 inside the inverse cancels with the leading α1, giving X(αI + βXX)1, which completes the proof. We assume single client holds the target vector, Y, which is sent once to the server (Algorithm 2, step 3). The sufficient statistic required for BLR becomes XX = (cid:80) . After aggregating this statistic (step 4), the server performs eigen-decomposition XX = UΛU (step 5) to facilitate efficient computation of ˇΣ1 = U(αI + βΛ)1U in each iteration (step 7). The updates of ˆω and ˇΣ are tailored to the vertical setting as shown in Equation (5). X(c)X(c) Since the features are partitioned across clients, each client only computes its local subset ˆω(c). At each iteration, the server broadcasts β ˇΣ1Y (step 8), and each client computes its local coefficients ˆω(c) (step 9). The server aggregates the global prediction ˆY (step 10) and computes the error ε (step 11), and aggregates the squared norm ˆω2 2 (step 12), which is required to update β. Algorithm 2 provides the full workflow. Theorem 4.3. The communication cost per client of Vertical Federated BLR is O(n min(n, m) + nt). In step 4, each client Proof. In step 3, the client holding sends it to the server, which costs O(n). communicates X(c)X(c) , which has size O(n2) but can be reduced to O(n min(n, m)) via local eigendecomposition. During the iterative refinement phase, at each iteration, clients communicate predictions in step 10 and parameter norms in step 12, each amounting to O(n) per client. Across iterations, this contributes O(nt) communication. Thus, the total per-client communication cost is dominated by the initial aggregation of X(c)X(c) plus iterative refinement, yielding O(n min(n, m) + nt)."
        },
        {
            "title": "5 Empirical Evaluation",
            "content": "We study the impact of preprocessing in federated learning environment and address some practical questions: (1) To what extent does preprocessing improve model performance? (2) How do different preprocessing choices behave under both IID and non-IID data partitions? (3) What are the actual communication costs per client of different preprocessing methods? Experiment Setup. Referring back to the possible strategies outlined in Section 1, Centralized preprocessing (Option 1) is not feasible in realistic federated learning scenarios, and transfer preprocessing (Option 3) is not suitable when client distributions differ. Therefore, we evaluate three options: no preprocessing (Option 2), local preprocessing (Option 4), and federated preprocessing (Option 5). Experiments are conducted on three public tabular datasets: Adult (Becker and Kohavi, 1996), Bank (Moro et al., 2012), and Cover (Blackard, 1998). Twenty percent of each dataset is held out for testing, and the remaining eighty percent is used for training. Table 3 in the Appendix summarizes the dataset statistics. We focus on two representative preprocessing techniques, OrdinalEncoder and StandardScaler , given their importance for tabular data where categorical features need encoding and feature magnitudes require normalization. For model training, we use FedAvg (McMahan et al., 2017) with the Adam optimizer (Kingma and Ba, 2015) to train Logistic Regression (LR) and Multi Layer Perceptron (MLP) with two hidden layers of size 128 and 64. Each experiment runs for 100 communication rounds with one local epoch per round and batch size of 32. The learning rate is tuned from {105, 104, 103, 102, 101}, and results are averaged over five runs. Code is available at https://github.com/xuefeng-xu/fl-tabular. IID Data Partitioning. In the IID setting, data is randomly shuffled and uniformly allocated across clients, so each client has distribution close to the global distribution. Figure 3 shows the test accuracy over communication rounds for the three preprocessing options, and Table 4 (in the Appendix) reports the final accuracies. Preprocessing consistently improves model performance across all datasets and models. On the Cover dataset with the MLP model, preprocessing increases accuracy by 17%. On the Adult dataset, the improvement is 5% for Logistic Regression and 12% for the MLP model. For the Bank dataset, the improvement is smaller, around 1 to 2%. As we would expect, under the IID setting, the local and federated preprocessing options behave similarly because local statistics align closely with global statistics. Non-IID Data Partitioning. In the non-IID setting, data is partitioned using label distribution skew strategy. For each client j, we sample pk,j from Dirichlet distribution with parameter α = 0.5 and Figure 3: Test accuracy comparison in the IID setting. assign pk,j fraction of examples with label to that client. This creates heterogeneous label distributions, which is common scenario in non-IID federated learning (Li et al., 2022). Results in Figure 4 and Table 4 (Appendix) show that preprocessing still provides substantial performance improvement over no preprocessing, especially on the Adult and Cover datasets. However, local preprocessing now performs appreciably worse than federated preprocessing. For instance, on the Cover dataset with the MLP model, local preprocessing yields an accuracy that is 11% lower than federated preprocessing. For Logistic Regression on the same dataset, the gap is 8%, which performs even worse than no preprocessing due to inconsistent local statistics. These results demonstrate that consistent preprocessing is essential for federated learning, particularly when client distributions differ. Communication cost. To validate the communication cost analysis in Section 3.2, we measure the actual communication cost per client for different preprocessing methods on datasets with 1000 samples per client. Table 5 (in the Appendix) reports the total communication cost in kilobytes (KB). The results align with our theoretical analysis. StandardScaler incurs minimal overhead of approximately 0.6 KB per client on the Adult dataset, while KBinsDiscretizer with quantile-based and k-means-based binning incurs substantially higher costs of around 18 KB and 61 KB, respectively, due to quantile sketch aggregation and iterative clustering. Similarly, SimpleImputer with median and most-frequent strategies costs more than the mean strategy. For Federated Bayesian Regression in the horizontal setting, the Adult dataset incurs approximately 3 KB per client, reflecting the O(m2) cost of aggregating XX, where is the number of features. For PowerTransformer , the communication cost is relatively high at around 73 KB, reflecting the need to iteratively optimize transformation parameters for each feature. Overall, these measurements confirm the communication cost estimates and demonstrate the trade-offs between preprocessing complexity and communication efficiency in federated learning. 11 Figure 4: Test accuracy comparison in the non-IID setting."
        },
        {
            "title": "6 Discussion",
            "content": "Existing works on federated preprocessing is limited, as most frameworks focus on model training. few frameworks, including FATE (Liu et al., 2021), SecretFlow (The SecretFlow Authors, 2022), and Baunsgaard et al. (Baunsgaard et al., 2021, 2022), support only small set of preprocessing techniques, mainly based on simple aggregation. For instance, MinMaxScaler computes global minimum and maximum values, while StandardScaler requires only global means and variances. Table 6 summarizes the available options. Compared to existing frameworks, FedPS supports significantly broader range of preprocessing techniques with flexible, user-configurable parameters and explicit communication overhead analysis. For example, none of the existing implementations address dimensional explosion in OneHotEncoder , which we handle by restricting category counts and filtering low-cardinality items using frequent-item sketches. FedPS provides more versatile and comprehensive solution for federated data preprocessing. Privacy is another important aspect of federated preprocessing. Prior work has explored secure preprocessing protocols, such as secure multi-party computation for the Yeo-Johnson transform (Marchand et al., 2022) and private encoding using fully homomorphic encryption (Hsu and Huang, 2022). These techniques can be incorporated into our framework. However, designing private protocols for complex statistics like quantiles requires careful consideration of trade-offs between latency, privacy, accuracy, and communication cost. Additionally, iterative preprocessing introduces practical challenges that complicate privacy protection, such as deciding whether intermediate results need to be kept private. Due to these complexities, we leave privacy-preserving preprocessing for future work."
        },
        {
            "title": "7 Conclusion",
            "content": "This work highlights the essential yet often overlooked role of data preprocessing in federated learning. We introduce FedPS, unified suite of tools that combines aggregated statistics, data sketches, and federated models. Experiments show that proper preprocessing substantially improves model accuracy, whereas inconsistent local preprocessing can reduce performance under non-IID data. By offering systematic and flexible framework for federated preprocessing, FedPS bridges the gap between data preparation and model training and contributes to the development of more robust and efficient federated systems."
        },
        {
            "title": "References",
            "content": "Anderson, D., Bevan, P., Lang, K. J., Liberty, E., Rhodes, L., and Thaler, J. (2017). high-performance algorithm for identifying frequent items in data streams. In Proceedings of the 2017 Internet Measurement Conference, IMC 2017, London, United Kingdom, November 1-3, 2017, pages 268282. ACM. Baunsgaard, S., Boehm, M., Chaudhary, A., Derakhshan, B., Geißelsoder, S., Grulich, P. M., Hildebrand, M., Innerebner, K., Markl, V., Neubauer, C., Osterburg, S., Ovcharenko, O., Redyuk, S., Rieger, T., Rezaei Mahdiraji, A., Wrede, S. B., and Zeuch, S. (2021). Exdra: Exploratory data science on federated raw data. In Proceedings of the 2021 International Conference on Management of Data, SIGMOD 21, page 24502463, New York, NY, USA. Association for Computing Machinery. Baunsgaard, S., Boehm, M., Innerebner, K., Kehayov, M., Lackner, F., Ovcharenko, O., Phani, A., Rieger, T., Weissteiner, D., and Wrede, S. B. (2022). Federated data preparation, learning, and debugging in apache systemds. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, CIKM 22, page 48134817, New York, NY, USA. Association for Computing Machinery. Becker, B. and Kohavi, R. (1996). Adult. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5XW20. Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg. Blackard, J. (1998). Covertype. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C50K5N. Buck, S. F. (1960). method of estimation of missing values in multivariate data suitable for use with an electronic computer. Journal of the Royal Statistical Society: Series (Methodological), 22(2):302306. Buuren, S. v. and Groothuis-Oudshoorn, K. (2011). MICE: Multivariate imputation by chained equations in R. Journal of Statistical Software, 45(3). Cormode, G., Karnin, Z. S., Liberty, E., Thaler, J., and Vesely, P. (2023). Relative error streaming quantiles. J. ACM, 70(5):30:130:48. Cormode, G. and Yi, K. (2020). Small summaries for big data. Cambridge University Press. de Boor, C. (1978). Practical Guide to Splines. Applied Mathematical Sciences. Springer. Dixon, J. K. (1979). Pattern recognition with partly missing data. IEEE Transactions on Systems, Man, and Cybernetics, 9(10):617621. Garcıa, S., Ramırez-Gallego, S., Luengo, J., Benıtez, J. M., and Herrera, F. (2016). Big data preprocessing: methods and prospects. Big Data Analytics, 1(1). Golub, G. H. and Van Loan, C. F. (2013). Matrix Computations - 4th Edition. Johns Hopkins University Press, Philadelphia, PA. Hsu, R. and Huang, T. (2022). Private data preprocessing for privacy-preserving federated learning. In 5th IEEE International Conference on Knowledge Innovation and Invention, ICKII 2022, Hualien, Taiwan, July 22-24, 2022, pages 173178. IEEE. 13 Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Nitin Bhagoji, A., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., DOliveira, R. G. L., Eichner, H., El Rouayheb, S., Evans, D., Gardner, J., Garrett, Z., Gascon, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Konecny, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R., Ozgur, A., Pagh, R., Qi, H., Ramage, D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tram`er, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S. (2021). Advances and open problems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210. Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S. J., Stich, S. U., and Suresh, A. T. (2020). SCAFFOLD: stochastic controlled averaging for federated learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51325143. PMLR. Karnin, Z. S., Lang, K. J., and Liberty, E. (2016). Optimal quantile approximation in streams. In IEEE 57th Annual Symposium on Foundations of Computer Science, FOCS 2016, 9-11 October 2016, Hyatt Regency, New Brunswick, New Jersey, USA, pages 7178. IEEE Computer Society. Khedr, A. M. (2008). Learning k-nearest neighbors classifier from distributed data. Comput. Informatics, 27(3):355376. Kingma, D. P. and Ba, J. (2015). Adam: method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Li, Q., Diao, Y., Chen, Q., and He, B. (2022). Federated learning on non-iid data silos: An experimental study. In 38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur, Malaysia, May 9-12, 2022, pages 965978. IEEE. Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. (2020a). Federated optimization in heterogeneous networks. In Dhillon, I. S., Papailiopoulos, D. S., and Sze, V., editors, Proceedings of the Third Conference on Machine Learning and Systems, MLSys 2020, Austin, TX, USA, March 2-4, 2020. mlsys.org. Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. (2020b). On the Convergence of FedAvg on Non-IID Data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Liu, Y., Fan, T., Chen, T., Xu, Q., and Yang, Q. (2021). Fate: an industrial grade platform for collaborative learning with data protection. J. Mach. Learn. Res., 22(1):16. Lloyd, S. P. (1982). Least squares quantization in PCM. IEEE Trans. Inf. Theory, 28(2):129136. Marchand, T., Muzellec, B., Beguier, C., Ogier du Terrail, J., and Andreux, M. (2022). SecureFedYJ: safe feature gaussianization protocol for federated learning. In Advances in Neural Information Processing Systems, volume 35, pages 3658536598. Curran Associates, Inc. McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. y. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 12731282. PMLR. Micci-Barreca, D. (2001). preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. SIGKDD Explor., 3(1):2732. Moro, S., Rita, P., and Cortez, P. (2012). Bank Marketing. UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5K306. 14 Munro, J. and Paterson, M. (1980). Selection and sorting with limited storage. Theoretical Computer Science, 12(3):315323. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., VanderPlas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12:28252830. Qi, D. and Wang, J. (2024). CleanAgent: Automating data standardization with LLM-based agents. CoRR, abs/2403.08291. Stich, S. U. (2019). Local SGD converges fast and communicates little. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. The DataSketches Authors (2023). The apache datasketches library for python. Accessed: November 7, 2025. The SecretFlow Authors (2022). Secretflow. Accessed: November 7, 2025. Tipping, M. E. (2001). Sparse bayesian learning and the relevance vector machine. J. Mach. Learn. Res., page 211244. Xu, X. and Cormode, G. (2025). Power transform revisited: Numerically stable, and federated."
        },
        {
            "title": "A Federated Machine Learning Models",
            "content": "A.1 Federated k-Means The k-Means clustering algorithm is an unsupervised method for identifying cluster centroids µ1, . . . , µk. Each iteration computes the distances between every data point and all centroids, assigns each point xi to the closest cluster Sj, and then updates each centroid as the mean of the points in that cluster: µj = (cid:80) xi/nj, where nj is the number of points in cluster Sj. xiSj In the horizontal federated setting, summarized in Algorithm 3, the server broadcasts the centroids to all clients. Each client locally assigns its data points to clusters and computes the sum and count of points in every cluster. The server aggregates these values to update the global centroids. The process repeats until convergence or until the maximum number of iterations is reached. Communication steps are color coded: blue for server receiving from clients, and orange for server sending to clients. Algorithm 3 Horizontal Federated k-Means (Server) 1: Input: Client holds {x(c) } 2: Initialize centroids {µ1, . . . , µk} 3: repeat 4: 5: Broadcast centroids {µ1, . . . , µk} to all clients Each client assigns its samples x(c) Aggregate local sums {s(c) to the closest cluster Sj } where s(c) = (cid:80) 6: 1 , s(c) s(c) 7: 8: until Convergence or reaching the maximum number of iterations 9: Output: Centroids {µ1, . . . , µk} 2 , . . . , s(c) / (cid:80) n(c) Update centroids µj = (cid:80) Sj x(c) x(c) and and counts {n(c) 1 , . . . , n(c) } A.2 Federated Nearest Neighbors Regression The k-Nearest Neighbors (k-NN) regression algorithm predicts the value of target variable by averaging the target values of the closest samples to given point x, typically measured using the Euclidean distance. Weighted averages can also be used, where weights are inversely related to distances. In the horizontal setting (Khedr, 2008), each client locally computes the smallest distances between the query point xp and its own data, then sends these distances to the server. The server merges the candidates to obtain the global top neighbors, and then requests the corresponding labels from clients. Algorithm 4 describes the full workflow. Algorithm 4 Horizontal Federated k-NN Regression (Server) 1: Input: Client holds data {x(c) 2: Broadcast xp to all clients , y(c) }; query point xp 3: Collect local top-k minimum distances {d(c) 4: Compute global top distances and identify their indices 5: Send the selected indices to the corresponding clients 1 , . . . , d(c) } from each client 6: Collect the corresponding values of y(c) and compute (weighted) mean µ 7: Output: Predicted value µ In the vertical setting, different clients hold different features of the same samples. Each client computes partial distance contribution, which the server aggregates to compute the full distance. The server then identifies the global nearest neighbors and sends their indices to the client responsible for prediction. The procedure is shown in Algorithm 5. Algorithm 5 Vertical Federated k-NN Regression (Server) } and query point x(c) 1: Input: Client holds data {x(c) 2: Each client computes partial distance between x(c) 3: Aggregate all partial distances to obtain full distances 4: Select global top distances and their indices 5: Send indices to the client that holds the target values 6: That client computes the (weighted) mean µ of the corresponding targets 7: Output: Predicted value µ and all samples ; one client holds the target {yi}"
        },
        {
            "title": "B Federated Data Preprocessors",
            "content": "Scaling adjusts data to specific range before model training. Common techniques include: MaxAbsScaler scales each feature so its maximum absolute value is one. It only requires the global maximum absolute value xmax. The scaling rule is x/xmax. MinMaxScaler maps data to target interval, typically [0, 1]. This requires the global minimum xmin and maximum xmax. The default rule is (x xmin)/(xmax xmin). RobustScaler uses quantiles, such as the lower quartile Q1 (0.25), the median Q2 (0.5), and the upper quantile Q3 (0.75), to reduce sensitivity to outliers. Clients send quantile sketches, which are aggregated to obtain global quantiles. The scaling rule is (x Q2)/(Q3 Q1). Normalizer rescales each data sample to have unit norm (l1, l2, or max norm). In horizontal federation, each client normalizes its samples independently. In vertical federation, the global norm of each sample must be computed by summing partial norms (for l1 and l2) or taking the maximum (for max norm) across clients, then dividing each feature value by the global norm, i.e., x/x. Encoding categorical values into numerical representations is crucial for machine learning models. All encoders require computing the global union of categories. LabelBinarizer , MultiLabelBinarizer , and LabelEncoder are typically used for label encoding, which usually involves single column. OneHotEncoder and OrdinalEncoder are used for feature encoding, often across multiple columns. They can optionally ignore infrequent categories or limit the number of output categories using frequent-item sketch. TargetEncoder (Micci-Barreca, 2001) assigns value to each category based on the distribution of the target . For binary label , the encoded value for category is λ(ni) niY , where ni is ni the number of samples in category i, niY is the number of samples in category with = 1, nY is the global number of positives. The shrinkage parameter λ(ni) = ni depends on the smoothing factor is the within-category variance and τ 2 is the global variance of . Thus, this = σ2 encoder requires computing global per-category means and variances of the target. /τ 2, where σ2 + (1 λ(ni)) nY m+ni Transformations apply nonlinear operations to reshape feature distributions. PowerTransformer is parametric method that aims to make data more Gaussian. The parameter λ is estimated by maximizing the log-likelihood, which requires global sums and variances of the transformed data. After applying the transformation, StandardScaler is used to obtain zero mean and unit variance. Xu and Cormode (2025) further discusses federated implementations with improved numerical stability. QuantileTransformer is non-parametric method that maps data to Uniform or Gaussian distribution. For the uniform case, the transformation outputs the empirical CDF (cumulative distribution function) value. For the Gaussian case, it applies the inverse Gaussian CDF Ψ1 to that value. Both transformations require global quantiles, computed via quantile sketch. 17 SplineTransformer constructs B-spline bases (de Boor, 1978). It follows procedure similar to KBinsDiscretizer , where knot positions are chosen uniformly using global minimum and maximum values or along the global quantiles. Discretization converts continuous variables into discrete categories. Binarizer applies fixed threshold and does not require any federated computation. Imputation addresses missing values in datasets. SimpleImputer is univariate method that replaces missing values with the feature mean, median, or the most-frequent value. Means require global sums and counts. Medians use the quantile sketch, and the most-frequent value uses the frequent-item sketch."
        },
        {
            "title": "C Additional Tables",
            "content": "Table 3: Dataset statistics. Datasets # Examples # Features # Categorical # Classes Adult Bank Cover 33K 45K 581K 14 16 54 8 9 2 2 7 Table 4: Test accuracy comparison of FedAvg using Logistic Regression (LR) and Multi-Layer Perceptron (MLP) in IID and non-IID settings with different preprocessing options. Partition Model Preprocessing Adult Bank Cover IID Non-IID LR MLP LR MLP No (Opt. 2) Local (Opt. 4) Federated (Opt. 5) No (Opt. 2) Local (Opt. 4) Federated (Opt. 5) No (Opt. 2) Local (Opt. 4) Federated (Opt. 5) No (Opt. 2) Local (Opt. 4) Federated (Opt. 5) 0.79 0.82 0.83 0.76 0.85 0.85 0.77 0.79 0.82 0.77 0.81 0.83 0.88 0.89 0.89 0.88 0.90 0.90 0.88 0.88 0.88 0.88 0.89 0.89 0.71 0.72 0.72 0.75 0.88 0. 0.69 0.65 0.71 0.74 0.78 0.88 18 Table 5: Communication cost per client of different federated preprocessors."
        },
        {
            "title": "Cover",
            "content": "OrdinalEncoder OrdinalEncoder (ignore infreq.) TargetEncoder StandardScaler KBinsDiscretizer (uniform) KBinsDiscretizer (quantile) KBinsDiscretizer (kmeans) PowerTransformer SimpleImputer (mean) SimpleImputer (median) SimpleImputer (most-frequent) Bayesian Regression (horizontal)"
        },
        {
            "title": "0 KB\n0.69 KB\n1.51 KB\n0 KB\n1.91 KB\n0.84 KB\n0 KB\n24.89 KB 16.39 KB\n1.21 KB\n0.61 KB\n0.57 KB\n1.11 KB\n0.48 KB\n0.51 KB\n18.88 KB 21.53 KB\n55.05 KB\n61.00 KB 96.40 KB 193.36 KB\n73.46 KB 75.18 KB 360.18 KB\n0.82 KB\n0.46 KB\n0.48 KB\n70.83 KB\n18.40 KB 21.02 KB\n55.88 KB\n22.29 KB 20.01 KB\n25.21 KB\n3.69 KB\n3.19 KB",
            "content": "Table 6: Support of federated data preprocessors across existing frameworks and FedPS. Preprocessor FATE SecretFlow Baunsgaard et al. FedPS MaxAbsScaler MinMaxScaler StandardScaler RobustScaler Normalizer LabelBinarizer MultiLabelBinarizer LabelEncoder OneHotEncoder OrdinalEncoder TargetEncoder PowerTransformer QuantileTransformer SplineTransformer KBinsDiscretizer SimpleImputer KNNImputer IterativeImputer Coverage 3/ 4/18 6/18 18/18 Some preprocessors are renamed for consistency."
        }
    ],
    "affiliations": [
        "University of Oxford",
        "University of Warwick"
    ]
}