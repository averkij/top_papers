{
    "paper_title": "DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding",
    "authors": [
        "Jungbin Cho",
        "Junwan Kim",
        "Jisoo Kim",
        "Minseo Kim",
        "Mingu Kang",
        "Sungeun Hong",
        "Tae-Hyun Oh",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this \"discord\" between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, a novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as a robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: https://whwjdqls.github.io/discord.github.io/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 7 2 5 9 1 . 1 1 4 2 : r DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding Jungbin Cho1* Junwan Kim1* Sungeun Hong2 Jisoo Kim1 Minseo Kim1 Mingu Kang2 Tae-Hyun Oh1,3 Youngjae Yu1,3, 1Yonsei University 2Sungkyunkwan University 3POSTECH Figure 1. While continuous methods generate smooth and natural motion, they often lack faithfulness to the conditional signal. In contrast, discrete methods achieve high faithfulness but tend to produce less natural results such as under-reconstruction and frame-wise noise. Our method, DisCoRD, combines the strengths of both approaches, effectively balancing faithfulness and naturalness of the generated motion."
        },
        {
            "title": "Abstract",
            "content": "Human motion, inherently continuous and dynamic, presents significant challenges for generative models. Despite their dominance, discrete quantization methods, such as VQ-VAEs, suffer from inherent limitations, including restricted expressiveness and frame-wise noise artifacts. Continuous approaches, while producing smoother and more natural motions, often falter due to high-dimensional complexity and limited training data. To resolve this discord between discrete and continuous representations, we introduce DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, novel method that decodes discrete motion tokens into continuous motion through rectified flow. By employing an iterative refinement process in the continuous space, DisCoRD captures fine-grained dynamics and ensures smoother and more natural motions. Compatible with any discrete-based framework, our method enhances naturalness without compromising faithfulness to the conditioning signals. Extensive evaluations demonstrate that *Equal contribution. Corresponding author. DisCoRD achieves state-of-the-art performance, with FID of 0.032 on HumanML3D and 0.169 on KIT-ML. These results solidify DisCoRD as robust solution for bridging the divide between discrete efficiency and continuous realism. Our project page is available at: Project Page. 1. Introduction Human motion generation controlled by diverse signals has become pivotal area in computer vision, driven by its vast applications in virtual reality to animation, gaming, and human-computer interaction. The ability to generate realistic human motions that are precisely aligned with input conditionssuch as textual descriptions [11, 13, 44, 55], human speech [25, 27, 58], or even music [9, 20, 40]is essential for creating immersive and interactive experiences. Two critical qualities define the success of such systems [49]: faithfulness, ensuring that the generated motion accurately reflects the conditioning signal, and naturalness, producing smooth and lifelike motions that are comfortable and convincing to human observers. deficit in faithfulness can produce motions that are misaligned or irrelevant to the inOur key insight is to leverage the strong faithfulness of discrete generation methods [13, 36] by employing score-based models [14, 24, 26, 41] to translate pretrained discrete tokens back into raw motion space, enhancing naturalness while preserving precise alignment with conditioning signals. Our approach offers two primary advantages over traditional discrete decoding methods, shown in Figure 2. First, by replacing the deterministic, feed-forward decoding of standard quantization techniques with stochastic, iterative refinement with score-based models, we can restore lost details from the quantization process [5, 64] which is not possible with deterministic decoder. This enables the reconstruction of dynamic and complex movements that were challenging for previous methods. Second, instead of directly decoding discrete tokens, we employ them exclusively as condition to steer motion generation in the raw-motion space, effectively reducing fine-grained noise and producing smoother, more natural motion outputs. While our method generates natural motions, evaluating the naturalness of reconstructed motions remains challenging as traditional metrics like MPJPE do not align well with human perception [8], and FID fails to capture subtle, framewise noise, shown in Figure 4. This limitation reduces the effectiveness of these metrics in assessing naturalness, particularly in cases of under-reconstruction and frame-wise noise. To address this, we introduce novel metric: the symmetric Jerk Percentage Error (sJPE), designed to evaluate reconstructed motion by ensuring it is free from underreconstruction while also minimizing fine-grained noise. Our experiments demonstrate the effectiveness of DisCoRD in enhancing sample-wise naturalness without suffering from under-reconstruction. Additionally, our method is adaptable to any discretebased motion generation framework, independent of conditioning signals, to improve performance. Extensive experiments across diverse taskstext-to-motion generation, co-speech gesture generation, and music-to-dance generationdemonstrate that our approach integrates seamlessly with various existing methods, achieving state-of-the-art naturalness in most cases according to both standard evaluation metrics and our proposed metric, sJPE. Overall, our contributions are as follows: 1. We introduce DisCoRD, novel method for decoding discrete tokens in continuous motion space, enhancing the naturalness of generated motions, while maintaining faithfulness. 2. We propose novel evaluation scheme, the symmetric Jerk Percentage Error, designed to evaluate both underreconstruction and frame-wise noise, which are often overlooked but critical for motion generation. 3. Our extensive experiments demonstrate that our methods achieve state-of-the-art performance on existing human motion generation scenarios. Figure 2. Concept of DisCoRD. Discrete quantization methods encode multiple motions into single quantized representation. While existing methods deterministically decode from this quantized representation, DisCoRD iteratively decodes the discrete latent in continuous space to recover the inherent continuity and dynamism of motion. To assess the gap between reconstructed and real motion, prior work primarily used FID as the metric. Here, we additionally propose symmetric Jerk Percentage Error (sJPE) to evaluate the differences in naturalness between reconstructed and real motion. tended context, while even slight unnaturalness can disrupt user immersion, creating an uncanny valley effect [32]. Since human motion is inherently continuous, human motion generation based on continuous representations is inherently well-suited for producing smooth and natural motion [33, 44, 55, 61]. However, due to the high dimensionality of continuous representation, they often encounter challenges with cross-modal mapping ambiguity [42, 56] which can result in low faithfulness. This issue becomes especially pronounced in data-constrained settings, such as motion capture datasets [30], where limited examples lead to difficulty of learning consistent mappings between signals and motions. On the other hand, discrete quantization methods [13, 36, 60] utilize motion VQ-VAEs [48] to discretize motion representation, simplifying the learning of high-dimensional data mappings by reformulating it as classification task. This discretization enables more efficient learning and can be particularly beneficial when dealing with limited data, improving faithfulness. However, motion VQ-VAEs face two main challenges: under-reconstruction, caused by representing continuous motion with finite set of tokens, and frame-wise noise, caused when decoding these discrete representations. Under-reconstruction can lead to loss of fine-grained motion details, essential for dynamic motion, and frame-wise noise can introduce unnatural artifacts, degrading the naturalness of generated motions and diminishing user immersion. In this paper, we propose DisCoRD, novel approach that combines the strengths of continuous and discrete methods to achieve natural and faithful human motion generation. 2 2. Related Work 3. Method The goal of human motion generation is to generate natural motions that adhere to control signals. Recent advancements in this field can be broadly categorized into two main approaches: motion generation from continuous representations, where models directly regress on continuous values, and motion generation from discrete representations, where motion is quantized into discrete tokens, transforming the generation task into classification problem in the discrete domain. Motion Generation from Continuous Representation. Motion generation using continuous representations can be achieved through various approaches. Early methods for signal-to-motion generation primarily focused on mapping control signals to motions within continuous representation space [2, 10, 22, 34, 43], often leveraging the Variational Autoencoder (VAE) architecture [17] or utilizing CLIP features [38]. More recently, with the success of scorebased models [14, 41], new advancements have emerged in generating motion through continuous representations [6, 21, 23, 29, 45, 46, 55, 59, 61, 62]. These approaches aim to model the inherently continuous nature of motion using continuous representation space, making them wellsuited to the task of motion generation. However, due to the data-constrained nature of the current motion generation tasks, the complexity of the continous representation space often makes it challenging to establish reliable cross-modal mappings between control signals and motions, resulting in suboptimal performance. Motion Generation from Discrete Representation. Recently, to simplify the complex mapping between control signals and motions, some methods have reformulated the generation task as discrete classification problem, achieving notable performance in motion generation [12, 13, 16, 25, 27, 35, 36, 40, 57, 58, 60]. These approaches often employ VQ-VAE [47] to create motion tokens, which are then used to generate motion sequences via token prediction models, such as autoregressive transformers. More recently, discrete diffusion models have been introduced to directly denoise these discrete tokens [7, 28]. While these methods effectively bypass complex signal-to-motion mapping challenges, their inherent characteristicssuch as information loss and discretenessfrequently result in unnatural artifacts, including under-reconstruction and frame-wise noise. Our method combines the inherent naturalness of continuous approaches with the signal faithfulness of discrete methods, achieving both natural and well-conditioned motion generation within the data-constrained motion generation task. 3 In this section, we propose DisCoRD, novel method for decoding pretrained discrete representations in the continuous domain using score based models. This allows the generated motions to: 1) capture dynamic, fast-paced motion due to iterative and stochastic nature of score-based models and 2) maintain the natural smoothness of motion by decoding in the raw motion domain, conditioned on discrete tokens. We begin by introducing score-based models, including rectified flow, then present the concept of motion tokenization. This is followed by an explanation of our conditional rectified flow model, conditioning projection module, and training details. 3.1. Preliminaries Score-based Models. Given data distribution px, scorebased models [14, 41] define forward diffusion process that progressively adds noise to the data x0 px, eventually transforming it into xT (0, I). Typically, this process can be expressed as: xt = αt x0 + σt xT , where [0, ]. (1) Here, the choice of functions αt and σt determines the trajectory that connects the data and noise. Score based model then learns the reverse diffusion process to recover the original data from noise. Recent works have introduced various parameterizations for αt and σt to enhance the reverse diffusion process. Beyond specific parametrization of αt and σt, the iterative nature of score-based models significantly strengthens their capacity to capture complex data variations, particularly excelling in generating high-dimensional data. Rectified Flow. Rectified flow [26] is generative model developed to address the transport mapping problem by employing flow matching algorithm [1, 24, 26]. Flow matching focuses on constructing transport map, denoted as : Rd Rd, that effectively maps observations from source distribution x0 π0 on Rd to observations from target distribution x1 π1 on Rd. This transport map is formalized via the ordinary differential equation (ODE): dxt = v(xt, t) dt. (2) In this context, is typically referred to as vector field, while xt represents the forward process parameterized over [0, 1]. Rectified flow specifies parametrization of Equation (1) such that the forward process follows straight path. This trajectory is expressed as xt = tx1 + (1 t)x0, with defined as (x1 x0). Since the target x1 is unknown during the generation phase, simple least squares regression problem: min (cid:90) 1 (cid:104) (x1 x0) v(xt, t)2(cid:105) dt, 0 with xt = tx1 + (1 t)x0. (3) Figure 3. An overview of DisCoRD. During the Training stage, we leverage pretrained quantizer to first obtain discrete representations (tokens) of motion. These tokens are then projected into continuous features C, which are concatenated with noisy motion Xt. This concatenated feature is used to train vector field v. During the Inference stage, we use pretrained token prediction model based on the pretrained quantizer to first generate tokens from the given control signal. These generated tokens are then projected into continuous features ˆC, concatenated with Gaussian noise X0 (0, I), and iteratively decoded through the learned vector field vθ into motion ˆX1. is used to train causal approximation of v, denoted vθ. Once the vector field vθ has been trained, samples from target distribution π1 can be generated by solving Equation (2) using an ODE solver, where the initial conditions for the ODE solver comes from source distribution π0. Due to the nearly straight trajectory that directly bridges the source and target distributions, the rectified flow model effectively enables the efficient transport of samples from readily accessible distributions to the target distribution. Motion Tokenization. The objective of generative models based on discrete quantization methods is to reformulate the regression problem into classification problem. These models typically undergo two-stage training process. In stage 1, VQ-VAE is trained to encode motion sequence = [x1, x2, . . . , xT ] where xt Rdmotion, using an encoder E, into sequence of discrete tokens = [z1, z2, . . . , zT /q]. Each token zt is retrieved from the codebook = (cid:8)zk Rdcode (cid:9)N k=1, and represents the length of the original motion sequence while is the downsample factor. Then decoder reconstructs the motions Xrecon from Z, with the network trained using reconstruction loss and commitment loss. In stage 2, the index sequence = [s1, s2, . . . , sT /q], representing the one-hot encoded codebook indices of the discrete token sequence Z, is used to train next-index prediction model conditioned on various signals. After training both stages, generating new motion sequence ˆX from given condition involves two steps: first, the stage 2 model produces sequence of predicted indices ˆS, which is then converted into discrete tokens ˆZ using the learned codebook. Finally, reconstructs the motion sequence ˆX from ˆZ, yielding the desired motion output. 3.2. DisCoRD Traditional deterministic feed-forward decoders suffer from limited expressiveness and propagate token discreteness into decoded motions, resulting in under-reconstructed and noisy outputs (see Figure 5). To address these issues, we propose decoding pretrained tokens in continuous space by replacing with an expressive score-based model. Specifically, we propose rectified flow model that synthesizes human motion from Gaussian noise, conditioned on frame-wise motion features. These features are extracted by transforming discrete tokens through conditional projection module, resulting in expressive frame-wise motion representations. The overall pipeline of DisCoRD is depicted in Figure 3. 4 Rectified Flow Decoder. We approach token decoding as conditional generation task using rectified flow. Specifically, given motion data distribution PX, we define the transport process from Gaussian noise X0 (0, I) to motion X1 PX, guided by which are features extracted from discrete tokens, formulated as: (cid:90) 1 min (cid:104) (X1 X0) v(Xt, t, C)2(cid:105) dt, (4) with Xt = tX1 + (1 t)X0. In contrast to traditional score-based models that generate images or motion [44] from sparse conditions like text, our goal is to decode discrete codebooks into raw motion using dense, frame-wise conditions. We achieve this by concatenating frame-wise features extracted from discrete tokens along the channel dimension, similar to image generation methods [15, 64]. This setup allows each motion frames to be conditioned on per-frame codebook features, enabling more expressive decoding. During the inference stage, features ˆC, extracted from tokens by pretrained token generation model, are iteratively decoded into ˆX1 by solving conditional ODE using the Euler method. Condition Projection. To enable our decoder to generate expressive motion, we extract continuous, frame-wise conditioning features from downsampled discrete tokens = [z1, . . . , zT /q]. Rather than applying naive upsampling and linear projectionwhich would lose temporal informationeach token zt is repeated times to restore the original temporal resolution. The repeated tokens are then stacked into tensor Zstacked R(T /q)(qdcode) and processed by linear layer. Subsequently, we unstack the processed tensor to yield sequence of frame-wise conditioning features, denoted as = [c1, . . . , cT ], where each frame is assigned unique set of temporal features. This method enhances the expressiveness of our frame-wise features, effectively guiding the iterative decoding process for improved motion quality. Training. In typical score-based models, training is performed over the entire data instance with fixed sequence lengths, such as 196 frames in HumanML3D [11], standard in motion generation [44, 55]. While this approach improves reconstruction quality in stage 1, our results indicate that these improvements do not translate effectively to generation quality in stage 2. To address this limitation and improve generalization to unseen motion sequences during inference, we train our rectified flow model on sliding windows of motion frames rather than max length spans. Additionally, although conventional U-Net diffusion models often incorporate attention mechanisms to enhance performance [39], we found this strategy to be suboptimal in our context, resulting in performance degradation. 4. Experiments In this section, we evaluate the effectiveness of DisCoRD in achieving motion naturalness compared to other discrete methods. We begin by assessing the naturalness of reconstructed motions to highlight the expressive capabilities of our score-based decoder. Then, we examine how this naturalness carries over to stage 2, generating natural motions while preserving faithfulness. We focus on text-to-motion generation due to its complex motions and diversity, but also evaluate our approach on other motion generation tasks, including co-speech gesture generation and music-to-dance generation, demonstrating the flexibility of our method. 4.1. Dataset and Evaluation Datasets. For the text-to-motion task, we employed HumanML3D [11] and KIT-ML [37]. HumanML3D is 3D motion dataset with language annotations, including 14,616 motion sequences paired with 44,970 text descriptions averaging 12 words. Sourced from motion capture data, motions are standardized to skeletal template, scaled to 20 FPS, and cropped to 10 seconds if longer. Each sequence has at least three descriptive texts covering diverse actions. KIT-ML is smaller dataset with 3,911 motion sequences paired with 6,278 text descriptions averaging 8 words. Motion capture data are downsampled to 12.5 FPS, with 14 descriptions per sequence. For co-speech gesture generation, we utilize the SHOW [58] dataset, while mixed version of AIST++ [20] and HumanML3D is used for music-to-dance generation. Further dataset details are provided in the Supplementary Section B. Evaluation. We evaluate DisCoRD on both motion reconstruction and motion generation separately. For motion reconstruction, the primary objective is to assess how effectively the decoder reconstructs motion from tokens. This is measured by Frechet Inception Distance (FID), which assesses motion realism by comparing the feature distributions of generated and ground truth motions, and Mean Per Joint Position Error (MPJPE), which quantifies positional accuracy. For text-to-motion generation, we follow prior works [44] and we employ several established metrics: FID; RPrecision, which measures retrieval accuracy by matching generated motions to corresponding text descriptions and reports Top-1, Top-2, and Top-3 accuracies; Multimodal Distance (MM-Dist), which calculates the average feature distance between generated motions and associated text descriptions, capturing cross-modal alignment; and Multimodality (MModality), which assesses the models capacity to produce diverse motions from single text prompt by averaging distances between multiple generations for the same prompt. For co-speech gesture generation, we employ Frechet Gesture Distance (FGD) [31], and for music-to-dance generation, following [46], we utilize Distk and Distg to assess the 5 Figure 4. sJPE and FID response to gaussian noise. Noise sJPE is very sensitive to small level frame-wise noise than FID, while Static sJPE remains small. Note that FID values are very small. quality of generated motions by comparing the distributional spread of generated and real motions. Additional specifics on these evaluation metrics are provided in the Supplementary Section B. Symmetric Jerk Percentage Error Motion evaluation at stage 1 often relies on Mean Per Joint Position Error (MPJPE) and Frechet Inception Distance (FID). However, MPJPE lacks alignment with human perceptual preferences [8], and FID, as model-level metric derived from pretrained network features, fails to capture sample-wise naturalness effectively [50]. Our findings further reveal that FID is insensitive to subtle, fine-grained noise, as shown in Figure 4an essential aspect of immersive motion quality [53]. To address these limitations, we propose an evaluation approach targeting both under-reconstructed motions and frame-wise noise by incorporating jerk. Jerk has proven effective for assessing motion quality [4, 7] and serves as key indicator of kinetic inconsistencies [3, 19], making it valuable metric for capturing nuanced deviations in motion. It can be obtained by the third derivative of position with respect to time, effectively capturing the rate of change of acceleration, allowing us to quantify subtle deviations and inconsistencies in motion. Let Jpred,t and Jtrue,t denote the predicted and ground truth jerk, respectively, at time over time points. Then the symmetric Jerk Percentage Error (sJPE), capturing the average deviation between predicted and ground truth jerk values, is defined as sJPE = 1 (cid:88)n t=1 Jpred,tJtrue,t Jtrue,t+Jpred,t . (5) Within sJPE, we identify two components: Noise sJPE and Static sJPE. Noise sJPE corresponds to instances where Jpred,t > Jtrue,t, indicating an overestimation of jerk in the predicted motion, which reflects the presence of fine-grained noise. This effect is evident in discrete-based methods, where discrete tokens introduce frame-wise noise, as shown in Figure 5. Static sJPE captures instances where Jpred,t Jtrue,t, indicating underestimation of jerk, or insufficiently dynamic predicted motion, highlighted by green boxes in Figure 5. Together, these components provide comprehensive measure Figure 5. Under-reconstruction and frame-wise noise. We visualize joint positions (top) and corresponding jerk graphs (bottom). Compared to MoMask, DisCoRD demonstrates lower noise levels (Noise sJPE, blue area) and reduced under-reconstruction (Static sJPE, red area), with under-reconstruction regions in Momask highlighted in green boxes. This results in lower sJPE for our method, indicating improved naturalness in the reconstructed motion. Methods MLD (cont.) T2M-GPT +DisCoRD(Ours) MMM +DisCoRD(Ours) MoMask +DisCoRD(Ours) T2M-GPT +DisCoRD(Ours) MoMask +DisCoRD(Ours) FID HumanML3D 0.017 0.089.001 0.031.001 0.097.001 0.020.001 0.019001 0.011000 KIT-ML 0.470.010 0.284.009 0.113.001 0.103. MPJPE sJPE 14.7 60.0 71.5 46.9 56.8 29.5 33.3 46.4 58.7 37.5 33.0 0.404 0.564 0.488 0.517 0.429 0.512 0.385 0.526 0.395 0.384 0. Table 1. Quantitative evaluation on the HumanML3D and KIT-ML test sets for reconstruction. DisCoRD serves as discrete models decoder, outperforming MLD in both FID and sJPE, demonstrating its effectiveness in decoding discrete tokens in continuous space. of prediction accuracy, capturing both overand underestimations of jerk within unified score. See Supplementary Section for additional details on sJPE and qaulitatve results. 4.2. Quantitative results Natural Motion Reconstruction. To assess the expressiveness of our decoding strategy, we replaced the decoders of state-of-the-art text-to-motion discrete methods with DisCoRD and evaluated their performance in stage 1. Shown in Table 1, while MLD benefits from continuous repre6 Datasets Methods Human ML3D KITML MDM [45] MLD [6] MotionDiffuse [61] ReMoDiffuse [62] MMM [36] T2M-GPT [60] + DisCoRD (Ours) MoMask [13] + DisCoRD (Ours) MDM [45] MLD [6] MotionDiffuse [61] ReMoDiffuse [62] MMM [36] T2M-GPT [60] + DisCoRD (Ours) MoMask [13] + DisCoRD (Ours) Top 1 - 0.481.003 0.491.001 0.510.005 0.504.003 0.491.003 0.476.008 0.521.002 0.524.003 - 0.390.008 0.417.004 0.427.014 0.404.005 0.398.007 0.382.007 0.433.007 0.434.007 Precision Top - 0.673.003 0.681.001 0.698.006 0.696.003 0.680.003 0.663.006 0.713.002 0.715.003 - 0.609.008 0.621.004 0.641.004 0.621.006 0.606.006 0.590.007 0.656.005 0.657.005 Top 3 0.611.007 0.772.002 0.782.001 0.795.004 0.794.002 0.775.002 0.760.007 0.807.002 0.809.002 0.396.004 0.734.007 0.739.004 0.765.055 0.744.005 0.729.005 0.715.004 0.781.005 0.775.004 FID MultiModal Dist MultiModality 0.544.044 0.473.013 0.630.001 0.103.004 0.080.003 0.116.004 0.095.011 0.045.002 0.032.002 0.497.021 0.404.027 1.954.062 0.155.006 0.316.019 0.718.038 0.541.038 0.204.011 0.169.010 5.566.027 3.196.010 3.113.001 2.974.016 2.998.007 3.118.011 3.121.009 2.958.008 2.938.010 9.191.022 3.204.027 2.958.005 2.814.012 2.977.019 3.076.028 3.260.028 2.779.022 2.792.015 2.799.072 2.413.079 1.553.042 1.795.043 1.164.041 1.856.011 1.831.048 1.241.040 1.288.043 1.907.214 2.192.071 0.730.013 1.239.028 1.232.026 1.887.050 1.928.059 1.131.043 1.266. Table 2. Quantitative evaluation on the HumanML3D and KIT-ML test set. indicates 95% confidence interval. +DisCoRD indicates that the baseline models decoder is replaced with DisCoRD. Bold indicates the best result, while underscore refers the second best. DisCoRD improves naturalness, as evidenced by FID scores, while preserving faithfulness, demonstrated by R-Precision and MM Distance. Methods TalkSHOW [58] + DisCoRD(Ours) ProbTalk [27] + DisCoRD(Ours) sJPE 0.284 0.077 0.406 0.349 FGD 74.88 43.58 5.21 4.83 Table 3. Quantitative results on co-speech gesture generation. DisCoRD outperforms baseline models on sJPE and FGD. Methods TM2D [9] +DisCoRD(Ours) sJPE Distk (9.780) Distg (7.662) 0.275 0.261 4.225 8.519 8.851 9.830 Table 4. Quantitative results on music-to-dance generation. DisCoRD outperforms baseline model on sJPE, Distk and Distg. sentation that enhances naturalness, discrete methods often struggle to reconstruct smooth and natural motions, resulting in increased sJPE values. Although models like MoMask achieve competitive FID, they fall short of MLD in motion naturalness, as indicated by significantly higher sJPE. Our approach yields substantial improvements in both FID and sJPE when applied to discrete models and even surpasses MLD in sJPE, demonstrating that leveraging discrete tokens to decode motion in continuous space combines the advantages of both methods effectively. Natural Motion Generation. To evaluate DisCoRDs effectiveness in decoding predicted tokens, we leveraged mod7 els trained in stage 1 to assess their performance in decoding tokens generated by pretrained token predictors. As shown in Table 2, our method outperforms baseline models, particularly in FID, achieving state-of-the-art performance and demonstrating that an expressive decoder enhances both reconstruction quality and motion generation. While R-precision scores indicate decrease in faithfulness for T2M-GPT, our results with MoMaskpaired with powerful tokenizer, RVQVAEshow that DisCoRD attains high naturalness without sacrificing faithfulness, highlighting its potential as default decoder replacement for discrete methods. Performance on Various Tasks. To validate our approach as general method for enhancing naturalness in discretebased human motion generation, we trained DisCoRD on co-speech gesture and music-driven dance generation, conducting comparative analysis against baseline models. As shown in Tables 3 and 4, our method consistently outperforms baseline models across both tasks, achieving superior performance on sJPE and standard evaluation metrics. We present additional evaluation results in Supplementary Section D. Effect of Sample Steps. By selecting the rectified flow algorithm for our score-based model, we exploit its efficient transport mechanism to achieve inference speeds comparable to baseline models. As shown in Figure 6, we evaluated the decoding times for both Momask and our model on tokens Methods MoMask + DisCoRD (Ours) Ours (Upconv) Ours (Repeat & Linear) Ours (w/ Attention) Ours (w/o WM) MPJPE Reconstruction FID 0.019.001 0.011.000 0.010.000 0.011.001 0.020.000 0.008.000 31.5 31.8 32.7 33.4 29.5 33.3 Generation FID 0.051.002 0.032.002 0.039.003 0.038.001 0.043.002 0.038.002 MM-Dist 2.957.008 2.938.010 2.943.006 2.947.008 2.983.009 2.952.009 Figure 6. Time spent decoding batch of token sequences with batch size of 32. All tests are conducted on the same NVIDIA RTX 4090Ti. Each experiment was repeated 20 times across the entire HumanML3D test set, and the average values were reported. generated by pretrained token generator. At the default setting of 16 sampling steps, our model achieves decoding speeds on par with MoMask while delivering superior sJPE, stage 1 FID, and stage 2 FID. Furthermore, by reducing the sampling steps, our method can decode tokens significantly fasterer than MoMask, maintaining comparable or enhanced FID and sJPE performance. Additionally, although it was not the primary focus of this experiment, we observed that sJPE responds more sensitively to changes in sampling steps compared to FID, further confirming that our sJPE metric effectively captures subtle variations in motion quality. Ablation Studies. To evaluate the contribution of each component to model performance, we conducted ablation studies on our models configuration. Table 5 presents the results on the HumanML3D test set, assessing the impact of each component on reconstruction and generation quality. First, we examined alternative projection mechanisms. Replacing our projection method with standard up-convolution or repeat followed by linear layer resulted in significant performance degradation, underscoring the effectiveness of our approach. Additionally, incorporating attention into the U-Net backbone and using full motion sequences instead of windowed motion segments resulted in performace degradation. This indicates that focusing on localized motion segments enhances the models generalization capability, particularly in stage 2. 4.3. Qualitative Results To validate the superiority of DisCoRD in generating both faithful and natural human motions, we conducted qualitative evaluation comparing it with discrete and continuous motion generation models. As illustrated in Figure 7, our method generates motions that align closely with textual descriptions while maintaining high degree of naturalness. Additionally, we conducted user study to validate continuous models are generally more effective at generating natural Table 5. Ablation studies. We perform ablations on the projection, attention, and motion windowing strategies during training on the HumanML3D dataset. (WM stands for Windowed Motion.) Figure 7. Qualitative comparisons on the test set of HumanML3D. Figure 8. User study results on the HumanML3D dataset. Each bar represents comparison between two models, with win rates depicted in blue and loss rates in red, evaluated based on naturalness and faithfulness. motions but often struggle to adhere strictly to conditions, whereas discrete models tend to excel in condition faithfulness but often lack naturalness of motion. In Figure 8, the results show that the discrete model, Momask, outperforms the continuous model, MDM, in faithfulness but falls short in naturalness. Our method demonstrates its effectiveness by surpassing both discrete and continuous models in terms of both naturalness and faithfulness, achieving balanced and superior performance. Additional qualitative comparisons are provided in the Supplementary Section E. 8 5. Conclusion In this paper, we presented DisCoRD, novel approach to human motion generation that effectively combines the naturalness of continuous representations with the faithfulness of discrete quantization methods. To demonstrate gains in naturalness, we also introduce symmetric Jerk Percentage Error (sJPE), specifically designed to capture subtle artifacts overlooked by traditional metrics. Extensive experiments across text-to-motion, co-speech gesture, and music-to-dance tasks demonstrate that DisCoRD consistently achieves state-ofthe-art performance, providing versatile solution adaptable to various discrete-based motion generation frameworks. Limitations. Since our method fundamentally builds on baseline models, it inherits certain limitations associated with these models. For instance, if the baseline model cannot generate variable motion lengths, our approach, which shares the same token generation model, may also be restricted by this limitation. Additionally, our current method requires pretrained model as starting point. However, our framework is inherently designed to support end-to-end training, allowing for the possibility of constructing an entirely new encoder from scratch. We leave this potential expansion as an avenue for future work."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 3 [2] Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gul Varol. Teach: Temporal action composition for 3d humans. In 2022 International Conference on 3D Vision (3DV), pages 414423. IEEE, 2022. 3 [3] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, and Etienne Burdet. robust and sensitive metric for quantifying movement smoothness. IEEE transactions on biomedical engineering, 59(8):21262136, 2011. 6 [4] German Barquero, Sergio Escalera, and Cristina Palmero. Seamless human motion composition with blended positional encodings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6 [5] Vighnesh Birodkar, Gabriel Barcik, James Lyon, Sergey Ioffe, David Minnen, and Joshua Dillon. Sample what you cant compress. arXiv preprint arXiv:2409.02529, 2024. 2 [6] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. 3, 7, [7] Seunggeun Chi, Hyung gun Chi, Hengbo Ma, Nakul Agarwal, Faizan Siddiqui, Karthik Ramani, and Kwonjoon Lee. M2d2m: Multi-motion generation from text with discrete diffusion models, 2024. 3, 6, 16 [8] Yann Desmarais, Denis Mottet, Pierre Slangen, and Philippe Montesinos. review of 3d human pose estimation algorithms for markerless motion capture, 2021. 2, 6 [9] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 99429952, 2023. 1, 7, 16 [10] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. 3 [11] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. 1, 5, [12] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts, 2022. 3 [13] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1900 1910, 2024. 1, 2, 3, 7, 12, 14, 16 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [15] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation, 2021. 5 [16] Rohollah Hosseyni, Ali Ahmad Rahmani, Jamal Seyedmohammadi, Sanaz Seyedin, and Arash Mohammadi. Bad: Bidirectional auto-regressive diffusion for text-to-motion generation. arXiv preprint arXiv:2409.10847, 2024. 3 [17] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [18] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, and Xinchao Wang. Priority-centric human motion generation in discrete latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14806 14816, 2023. 16 [19] Caroline Larboulette and Sylvie Gibet. review of computable expressive descriptors of human motion. In Proceedings of the 2nd International Workshop on Movement and Computing, pages 2128, 2015. 6 [20] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++, 2021. 1, 5, 12 [21] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15241534, 2024. 3 [22] Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, and Chang-wen Chen. Being comes from notbeing: Open-vocabulary text-to-motion generation with wordless training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2322223231, 2023. 3 [23] Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkanhalli, and Weidong Geng. Mcm: Multi-condition motion arXiv preprint synthesis framework for multi-scenario. arXiv:2309.03031, 2023. 3 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [25] Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, and Michael J. Black. Emage: Towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling, 2024. 1, 3 [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3 [27] Yifei Liu, Qiong Cao, Yandong Wen, Huaiguang Jiang, and Changxing Ding. Towards variable and coordinated holisIn Proceedings of the tic co-speech motion generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15661576, 2024. 1, 3, 7, 12, [28] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, and Yi Yang. Diversemotion: Towards diverse human motion generation via discrete diffusion. arXiv preprint arXiv:2309.01372, 2023. 3 [29] Zhenye Luo, Min Ren, Xuecai Hu, Yongzhen Huang, and Li Yao. Popdg: Popular 3d dance generation with popdanceset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2698426993, 2024. 3 [30] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes, 2019. 2 [31] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit. Evaluating the quality of synthesized motion with the frechet motion distance, 2022. 5, 12 [32] Masahiro Mori, Karl F. MacDorman, and Norri Kageki. The uncanny valley [from the field]. IEEE Robotics Automation Magazine, 19(2):98100, 2012. 2 [33] Mathis Petrovich, Michael J. Black, and Gul Varol. Actionconditioned 3D human motion synthesis with transformer In International Conference on Computer Vision VAE. (ICCV), 2021. 2 [34] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision, pages 480497. Springer, 2022. [35] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: Bidirectional autoregressive motion model. arXiv preprint arXiv:2403.19435, 2024. 3 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 2, 3, 7, 14, 16 [37] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4):236252, 2016. 5, 15 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 5 [40] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1105011059, 2022. 1, 3, 15 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, [42] Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Devon Hjelm, Zsolt Kira, and Alexander Toshev. Grounding multimodal large language models in actions, 2024. 2 [43] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, pages 358374. Springer, 2022. 3 [44] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. 1, 2, 5 [45] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. 3, 7, 16 [46] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448458, 2023. 3, 5, 12, 15 [47] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [48] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning, 2018. 2 [49] Jordan Voas, Yili Wang, Qixing Huang, and Raymond Mooney. What is the best automated metric for text to motion generation? In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. 1 [50] Haoru Wang, Wentao Zhu, Luyi Miao, Yishu Xu, Feng Gao, Qi Tian, and Yizhou Wang. Aligning motion generation with human perceptions. 2024. 6 [36] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In Proceed- [51] Yin Wang, Zhiying Leng, Frederick WB Li, Shun-Cheng Wu, and Xiaohui Liang. Fg-t2m: Fine-grained text-driven human 10 Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 73687376, 2024. 16 [64] Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, and Ting Liu. ϵ-VAE: Denoising as Visual Decoding. arXiv preprint arXiv:2410.04081, 2024. 2, 5 [65] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multiIn Proceedings of the perspective attention mechanism. IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. 16 motion generation via diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2203522044, 2023. 16 [52] Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. Motiongpt-2: general-purpose motion-language arXiv model for motion generation and understanding. preprint arXiv:2410.21747, 2024. [53] Marta Wilczkowiak, Ken Jakubzak, James Clemoes, Cornelia Treptow, Michaela Porubanova, Kerry Read, Daniel McDuff, Marina Kuznetsova, Sean Rintel, and Mar Gonzalez-Franco. Ecological validity and the evaluation of avatar facial animation noise. In 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), pages 7279, 2024. 6 [54] Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and Chi-Keung Tang. Motionllm: Multimodal motion-language learning with large language models. arXiv preprint arXiv:2405.17013, 2024. 16 [55] Chen Xin, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 3, 5 [56] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven 3d facial animation with discrete motion prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1278012790, 2023. 2 [57] Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, and Chuang Gan. Unimumo: Unified text, music and motion generation. arXiv preprint arXiv:2410.04534, 2024. 3 [58] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In CVPR, 2023. 1, 3, 5, 7, 12, 15, [59] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1601016021, 2023. 3 [60] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations, 2023. 2, 3, 7, 14, 16 [61] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Textdriven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2, 3, 7, 16 [62] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364373, 2023. 3, 7, 16 [63] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. 11 DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material is organized as follows: Section details the implementation of DisCoRD. Section provides additional information on the datasets and evaluation metrics. Section offers comprehensive analysis of sJPE. Section presents quantitative results excluded from the main paper. Section includes additional qualitative results. We highly recommend viewing the accompanying video, as static images are insufficient to fully convey the intricacies of motion. A. Implementation Details Table 6 provides an overview of the implementation details for our method. These configurations were employed to train the DisCoRD decoder using the pretrained Momask [13] quantizer. Specifically, the 512-dimensional codebook embeddings from MoMask are projected into the conditional channel dimension. This projection is concatenated with Gaussian noise of the same dimensionality as the output channel. The concatenated representation is subsequently projected into the input channel dimension of the U-Net architecture. The U-Net processes this input and transforms it back into the output channel dimension, generating the final output shape. For training, we used an input window size of 64 and trained the model for 35 hours on single NVIDIA RTX 4090 Ti GPU. B. Datasets and Evaluations In this section, we provide additional explanations regarding the co-speech gesture generation and music-to-dance generation tasks that we were unable to describe in detail in the main paper. B.1. Datasets. For the co-speech gesture generation task, we utilized the SHOW dataset [58], 3D holistic body dataset comprising 26.9 hours of in-the-wild talking videos. For the musicdriven dance generation task, we used mixed dataset combining AIST++ [20] and HumanML3D [11] where AIST++ is large-scale 3D dance dataset created from multi-camera videos accompanied by music of varying styles and tempos, containing 992 high-quality pose sequences in the SMPL format. Training Details"
        },
        {
            "title": "Batch Size\nWindow Size\nSteps\nEpochs",
            "content": "AdamW (0.9, 0.999) 0.0005 0 Cosine 20 1.0 0.999 MSE Loss 768 64 481896 200 Model Details Input Channels Output Channels Condition Channels Activation Dropout Width # Resnet / Block # Params 512 263 256 SiLU 0 (512, 1024) 2 66.9M Table 6. Implementation details for training the DisCoRD decoder on the HumanML3D dataset using the pretrained Momask quantizer. reported body FGD, which quantifies differences specifically for the body part, for ProbTalk [27]. For TalkSHOW [58], which only utilizes holistic FGDa metric that measures differences across the entire motion, including the face and handswe reported the holistic FGD. To evaluate the musicto-dance generation task, we utilized Distk, which quantifies the distributional spread of generated dances based on kinetic features, and Distg, which does the same for geometric features, as proposed in [46]. smaller difference between the distributions of the generated motion and the ground truth motion indicates that the Distk and Distg values of the generated motion align closely with those of the ground truth, reflecting similar level of distributional spread. B.2. Evaluations. C. Additional Analysis on sJPE To evaluate the co-speech gesture generation task, we used Frechet Gesture Distance (FGD) [31], which measures the difference between the latent distributions of generated and real motions. Since our focus is on body movements, we To evaluate the sample-wise naturalness of reconstructed motions, we introduce the symmetric Jerk Percentage Error (sJPE), as defined in Equation 5 of the main paper. We present detailed formulations of Noise sJPE and Static sJPE, supported by analysis using generated motion samples. Furthermore, qualitative comparisons highlight the effectiveness of DisCoRD against state-of-the-art discrete methods. Finally, we investigate the alignment of sJPE with human preference to validate its perceptual relevance. C.1. Visualization of Fine-Grained Motion To analyze fine-grained motion trajectories, we follow three-step procedure. First, we select joint for visualization, typically hand joints due to their high dynamism, and track their positional changes over time. Second, we apply Gaussian filter to smooth the trajectory, reducing noise. Finally, we compute the difference between the smoothed and original trajectories to isolate fine-grained motion components. This method allows for detailed evaluation of frame-wise noise and under-reconstructed regions in motion trajectories. The visualizations in Figure 5 of the main paper and the qualitative samples in the supplementary material are generated using this process. C.2. Details on sJPE. Within the symmetric Jerk Percentage Error (sJPE), we define two components: Noise sJPE and Static sJPE. These isolate the instances where the predicted jerk overestimates or underestimates the ground truth jerk, respectively. Noise sJPE and Static sJPE. Noise sJPE captures the average overestimation of jerk in the predicted motion signal, meaning frame-wise noise, corresponding to cases where Jpred,t > Jtrue,t. It is defined as: Noise sJPE = 1 (cid:88) t=1 max (0, Jpred,t Jtrue,t) Jtrue,t + Jpred,t . (6) The operator max(0, x) ensures that only positive differences contribute to Noise sJPE, separating overestimations from underestimations. Noise sJPE can be seen on the red box of Figure 9. Time steps where motion trajectory is noisy compared to ground truth motion show bigger jerk. The area under the predicted jerk and above the ground truth jerk, shown in blue area, is proportional to the Noise sJPE, meaning frame-wise noise. Static sJPE measures the average underestimation of jerk, meaning lack of dynamism in the predicted motion, corresponding to cases where Jpred,t Jtrue,t. It is defined as: Static sJPE = 1 (cid:88) t= max (0, Jtrue,t Jpred,t) Jtrue,t + Jpred,t . (7) Static sJPE can be seen on the green box of Figure 9. Time steps where motion trajectory is under-reconstructed compared to ground truth motion show smaller jerk. The area above the predicted jerk and under the ground truth jerk, shown in red area, is proportional to the Static sJPE, meaning under reconstructed motions."
        },
        {
            "title": "The overall sJPE can be expressed as the sum of Noise",
            "content": "sJPE and Static sJPE: sJPE = Noise sJPE + Static sJPE. (8) These formulations provide measure of prediction accuracy by separately accounting for the tendencies of the predictive model to overestimate or underestimate the true motion jerks. Figure 9. Relationship between fine-grained trajectory and jerk: Frame-wise noise in predicted motions, highlighted in the red box, results in higher jerk values compared to the ground truth, represented by the blue areas. The sum of the blue areas corresponds to Noise sJPE. Conversely, under-reconstruction in predicted motions, highlighted in the green box, leads to lower jerk values compared to the ground truth, represented by the red areas. The sum of the red areas corresponds to Static sJPE. C.3. Qualitative Results on Joint Trajectory and"
        },
        {
            "title": "Jerk",
            "content": "We present series of figures demonstrating the effectiveness of DisCoRD in reconstructing smooth and dynamic motion. For each sample, the first row visualizes the motion trajectory, while the second row plots the corresponding jerk at each time step, with the calculated sJPE displayed 13 Figure 10. Joint Trajectory and Jerk: Under-Reconstruction in Discrete Methods DisCoRD effectively reduces the red area, demonstrating its capability to reconstruct dynamic motion accurately. This improvement is also reflected in the lower sJPE value. Figure 11. Joint Trajectory and Jerk: Frame-Wise Noise in Discrete Methods DisCoRD significantly reduces the blue area, indicating its ability to generate smooth motions that closely resemble the ground truth. This improvement is further reflected in the lower sJPE value. at the top. This visualization enables detailed analysis of fine-grained trajectories in predicted motions and highlights the contributions of Noise sJPE and Static sJPE to the overall sJPE. We compare DisCoRD with recent discrete methods, including T2M-GPT [60], MMM [36], and Momask [13]. Motion samples that illustrate under-reconstruction in discrete methods are presented in Figure 10, while those that exhibit frame-wise noise are shown in Figure 11. Samples showing both issues in discrete models are displayed in Figure 12. DisCoRD effectively reduces frame-wise noise while accurately reconstructing dynamic, fast-paced motions. This is shown in both the visualizations and the sJPE results. C.4. Correlation between sJPE and human perception. To further verify that sJPE aligns with human judgment of naturalness, we conducted an additional user study. We 14 asked participants to rank three modelsMLD, MoMask, and oursin order of naturalness. The rankings were scored such that the first place received 1 point, the second place 2 points, and the third place 3 points. Using these human scores, we calculated Pearsons correlation between the human scores and two metricsMPJPE and sJPE for each sample. During this process, we excluded the lowest 10% of samples in terms of human score standard deviation among models, as these were considered indistinguishable by human evaluators. Our analysis revealed that the average Pearsons correlation between MPJPE and human scores was 0.181, whereas the correlation between sJPE and human scores was significantly higher at 0.483. This result demonstrates the effectiveness of sJPE in evaluating sample-wise naturalness. Figure 12. Joint Trajectory and Jerk: Both Frame-Wise Noise and Under-Reconstruction in Discrete Methods DisCoRD addresses both frame-wise noise and under-reconstruction by simultaneously reducing the blue and red areas. This demonstrates its ability to generate smooth and dynamic motions, closely aligning with the ground truth. This further supported by the lower sJPE values. D. Additional Quantitative Results D.1. Performance on Text-to-Motion Generation. In Table 7, we present comparison of our method against additional results from various text-to-motion models. Our method consistently achieves strong performance on the HumanML3D and KIT-ML [37] test sets, even when evaluated alongside these additional models. While ReMoDiffuse achieves particularly strong performance on KIT-ML, it is worth noting that its performance benefits from the use of specialized database for high-quality motion generation, which makes direct comparisons less appropriate. D.2. Performance on Various Tasks. In Table 8, we present additional evaluation results for cospeech gesture generation. Following [58], we additionally report Diversity, which measures the variance among multiple samples generated from the same condition, and Beat Consistency (BC), which evaluates the synchronization between the generated motion and the corresponding audio. In Table 9, we provide additional evaluation results for musicto-dance generation. Following [40], we report FIDk to measure differences in kinetic motion features and FIDg for geometric motion features. Additionally, we include the Beat Align Score (BAS) to assess the synchronization between motion and music. While [46] has shown that these metrics are not fully reliable and often fail to align with actual output quality, we include them to follow established conventions. E. Additional Qualitative Results In Figure 13, we present additional qualitative comparisons between our model and other leading approaches. In Figure 14, we additionally display more qualitative results of our method. We observed that our method effectively follows the text prompts while maintaining naturalness in the generated outputs. Again, we highly recommend viewing the accompanying video, as static images are insufficient to fully convey the intricacies of motion. Datasets Methods Human ML3D KITML MDM [45] MLD [6] MotionDiffuse [61] ReMoDiffuse [62] Fg-T2M [51] M2DM [18] M2D2M [7] MotionGPT [63] MotionLLM [54] MotionGPT-2 [52] AttT2M [65] MMM [36] T2M-GPT [60] + DisCoRD (Ours) MoMask [13] + DisCoRD (Ours) MDM [45] MLD [6] MotionDiffuse [61] ReMoDiffuse [62] Fg-T2M [51] M2DM [18] M2D2M [7] MotionGPT [63] MotionLLM [54] MotionGPT-2 [52] AttT2M [65] MMM [36] T2M-GPT [60] + DisCoRD (Ours) MoMask [13] + DisCoRD (Ours) Top 1 - 0.481.003 0.491.001 0.510.005 0.492.002 0.497.003 - 0.364.005 0.482.004 0.496.002 0.499.003 0.504.003 0.491.003 0.476.008 0.521.002 0.524.003 - 0.390.008 0.417.004 0.427.014 0.418.005 0.416.004 - 0.340.002 0.409.006 0.427.003 0.413.006 0.404.005 0.398.007 0.382.007 0.433.007 0.434.007 Precision Top 2 - 0.673.003 0.681.001 0.698.006 0.683.003 0.682.002 - 0.533.003 0.672.003 0.691.003 0.690.002 0.696.003 0.680.003 0.663.006 0.713.002 0.715.003 - 0.609.008 0.621.004 0.641.004 0.626.004 0.628.004 - 0.570.003 0.624.007 0.627.002 0.632.006 0.621.006 0.606.006 0.590.007 0.656.005 0.657. Top 3 0.611.007 0.772.002 0.782.001 0.795.004 0.783.024 0.763.003 0.799.002 0.629.004 0.770.002 0.782.004 0.786.002 0.794.002 0.775.002 0.760.007 0.807.002 0.809.002 0.396.004 0.734.007 0.739.004 0.765.055 0.745.004 0.743.004 0.753.006 0.660.004 0.750.005 0.764.003 0.751.006 0.744.005 0.729.005 0.715.004 0.781.005 0.775.004 FID MultiModal Dist MultiModality 0.544.044 0.473.013 0.630.001 0.103.004 0.243.019 0.352.005 0.087.004 0.805.002 0.491.019 0.191.004 0.112.006 0.080.003 0.116.004 0.095.011 0.045.002 0.032.002 0.497.021 0.404.027 1.954.062 0.155.006 0.571.047 0.515.029 0.378.023 0.868.032 0.781.026 0.614.005 0.870.039 0.316.019 0.718.038 0.541.038 0.204.011 0.169. 5.566.027 3.196.010 3.113.001 2.974.016 3.109.007 3.134.010 3.018.010 3.914.013 3.138.010 3.080.013 3.038.007 2.998.007 3.118.011 3.121.009 2.958.008 2.938.010 9.191.022 3.204.027 2.958.005 2.814.012 3.114.015 3.015.017 3.012.021 3.721.018 2.982.022 3.164.013 3.039.021 2.977.019 3.076.028 3.260.028 2.779.022 2.792.015 2.799.072 2.413.079 1.553.042 1.795.043 1.614.049 3.587.072 2.115.079 2.473.041 - 2.137.022 2.452.051 1.164.041 1.856.011 1.831.048 1.241.040 1.288.043 1.907.214 2.192.071 0.730.013 1.239.028 1.019.029 3.325.037 2.061.067 2.296.022 - 2.357.022 2.281.047 1.232.026 1.887.050 1.928.059 1.131.043 1.266.046 Table 7. Additional quantitative evaluation on the HumanML3D and KIT-ML test sets. indicates 95% confidence interval. +DisCoRD indicates that the baseline models decoder is replaced with our DisCoRD decoder. Bold indicates the best result, while underscore refers the second best. Methods TalkSHOW [58] +DisCoRD(Ours) ProbTalk [27] +DisCoRD(Ours) Diversity BC (0.868) 0.821 0.919 0.259 0.331 0.872 0.876 0.795 0. Table 8. Additional quantitative results on co-speech gesture generation. The results demonstrate that our method performs on par with, or surpasses, the baseline models. Methods Ground Truth FIDk FIDg BAS 0.2374 10.60 17.10 TM2D [9] +DisCoRD(Ours) 19.01 23.98 20.09 88. 0.2049 0.2190 Table 9. Additional quantitative results on music-to-dance generation. The results demonstrate that, although our method shows performance degradation on FIDk and FIDg, which are known to be unreliable, it achieves improvement in the Beat Align Score. 16 Figure 13. Additional qualitative comparisons on the HumanML3D test set. The continuous method, MLD, often fails to perfectly align with the text consistently, while the discrete method, MoMask, exhibits issues such as under-reconstruction, resulting in minimal hand movement, or unnatural leg jitter caused by frame-wise noise. Figure 14. Additional qualitative results of our method on the HumanML3D test set. Figure 15. Guidelines for user study in the Main paper: participants were asked to evaluate Faithfulness and Naturalness, excluding hand and facial movements that are not included in HumanML3D. Figure 16. Guidelines for User Study in the Supplementary: Participants were asked to evaluate Naturalness, excluding hand and facial movements that are not included in HumanML3D. 18 Figure 17. User evaluation interface for the user study in the Main paper: participants were presented with two randomly selected videos and asked to choose the better sample in terms of faithfulness and naturalness. 19 Figure 18. User evaluation interface for the user study in the supplementary: participants were presented with grid layout containing the GT video and three generated videos. Using the GT video as the upper bound, they were asked to rank the three generated videos in terms of naturalness."
        }
    ],
    "affiliations": [
        "POSTECH",
        "Sungkyunkwan University",
        "Yonsei University"
    ]
}