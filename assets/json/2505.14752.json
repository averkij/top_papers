{
    "paper_title": "Large Language Models for Data Synthesis",
    "authors": [
        "Yihong Tang",
        "Menglin Kong",
        "Lijun Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond."
        },
        {
            "title": "Start",
            "content": "Yihong Tang, Menglin Kong, Lijun Sun(cid:66)"
        },
        {
            "title": "McGill University",
            "content": "{yihong.tang,menglin.kong}@mail.mcgill.ca, lijun.sun@mcgill.ca"
        },
        {
            "title": "Abstract",
            "content": "Generating synthetic data that faithfully captures the statistical structure of realworld distributions is fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSYNTHOR, general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSYNTHOR treats the LLM as nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSYNTHOR in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSYNTHOR shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as valuable tool across economics, social science, urban studies, and beyond."
        },
        {
            "title": "Introduction",
            "content": "High-quality synthetic data has become an essential enabler for research, innovation, and policymaking, particularly in domains where access to real data is limited by privacy, legal, or logistical constraints [33, 6]. When statistically faithful, synthetic datasets can support meaningful analysis without compromising sensitive information. This is especially important in complex, high-dimensional domains such as human mobility, where detailed data is critical for urban planning and infrastructure design [42], yet poses serious privacy risks. For instance, privacy-preserving mobility traces can help simulate emergency scenarios while protecting individual identities. 5 2 0 2 0 2 ] . [ 1 2 5 7 4 1 . 5 0 5 2 : r Method Assumptions Manual Effort Efficiency Traditional methods for data synthesis, including parametric models and rule-based simulators, offer interpretability and control but rely on strong assumptions and often fail to scale to complex dependencies or high-dimensional data. Deep generative models such as GANs [11], VAEs [20], and diffusion models [15] have shown promise in capturing non-linear structure, particTable 1: Comparison of synthetic data generation approaches. Marginal / Joint (exact) Joint but unstable Varies Joint and marginal Classic Statistical Deep Learning Hybrid Models LLMSYNTHOR Strong Implicit Mixed Minimal Medium Medium Varies High High Medium High Low Low Medium Medium High Scalability Fidelity (cid:66)Corresponding Author. The code is available at https://github.com/YihongT/LLMSynthor.git Preprint. Under review. ularly in vision and text [38], but suffer from training instability, limited controllability, and poor generalization across formats [46]. Most existing approaches are tightly coupled to specific data types and require retraining or manual adaptation for new domains, limiting their practical utility [55]. Table 1 summarizes these trade-offs across different generative paradigms, highlighting the lack of unified, distribution-aware synthesis framework that is format-agnostic, scalable, and robust. These limitations call for an approach that unifies high-capacity generative priors with explicit statistical alignment, efficient sampling, and cross-domain applicability. Recent advances in Large Language Models (LLMs) suggest new direction. Pretrained on diverse, large-scale corpora, LLMs exhibit strong structural priors and can generate semantically coherent data across domains in zero-shot settings [1, 21, 23]. This makes them appealing as universal priors for synthetic data generation. However, standard LLM sampling remains fundamentally limited: it lacks global distributional alignment, generates samples inefficiently and independently, and cannot produce large datasets due to context length constraints [47]. These limitations prevent LLMs from serving as reliable tools for statistically aligned synthesis. To address these challenges, we propose LLMSYNTHOR, general framework that repurposes LLMs as structure-aware simulators embedded within distribution-guided inference loop. Rather than sampling directly from the model, LLMSYNTHOR leverages summary statistics to model the LLM as nonparametric copula simulator that captures latent dependencies and supports both marginal and joint alignment. To enable cross-domain use, LLMSYNTHOR maps continuous and discrete variables into unified, type-agnostic summary space, allowing interpretable comparison of statistical structures. This ensures consistent performance across diverse data types without retraining or redesign. To increase efficiency, we introduce LLM Proposal Sampling, in which the LLM outputs structured, sampleable distributions instead of individual records. To overcome context limitations and ensure full dataset coverage, we implement an iterative refinement process that guides generation using discrepancy signals between real and synthetic data in unified summary space. This theorygrounded mechanism enables scalable, type-agnostic generation across structured and unstructured formats without retraining. LLMSYNTHOR unifies the semantic flexibility of LLMs with rigorous statistical control, enabling fine-grained, high-fidelity synthesis across domains. It supports distributional alignment, structureaware generalization, and rejection-free sampling in flexible and scalable framework. To summarize, our contributions are: We introduce LLMSYNTHOR, the first framework to leverage LLMs for high-fidelity, distributionally aligned synthetic data generation, supporting seamless adaptation across data domains. We propose unified modeling paradigm that interprets the LLM as nonparametric copula simulator, and develop LLM Proposal Sampling to enable efficient, coherent synthesis that preserves both marginal and joint statistical properties. We establish theory-driven iterative synthesis loop that uses discrepancy-guided feedback to progressively align synthetic and real data distributions, providing rigorous statistical control over the generation process. We empirically demonstrate the versatility and utility of LLMSYNTHOR on e-commerce, population, and human mobility data, showing consistent gains in structural fidelity and downstream relevance, and highlighting its potential as general data synthesis tool for supporting data-driven research, simulation, and policy-making."
        },
        {
            "title": "2 Related Work",
            "content": "Data Synthesis Early data synthesis methods emphasized explicit statistical control, using approaches such as iterative proportional fitting (IPF) [5, 31], Bayesian networks [40, 41, 54], and copula-based models [32, 3, 34] to match marginals and preserve dependencies. While interpretable, these methods often require strong assumptions and struggle with scalability and heterogeneity. Deep generative models, including VAEs [2, 45], GANs [51, 4, 9, 27], and recent diffusionor flowbased architectures [22, 17, 19, 55], have greatly improved realism and high-dimensional modeling. However, they typically entangle marginal distributions with dependencies and require expensive retraining for new domains. LLM-based approaches, such as GReaT [7] and HARMONIC [48], represent new paradigm: treating structured data as natural language and synthesizing samples via autoregressive decoding. This allows for zero-shot transfer and broad domain coverage, but these 2 Figure 1: The proposed LLMSYNTHOR framework. Left right: statistical summarization, dependency inference, structure grounding, and LLM proposal sampling. methods lack direct control over marginal and joint distributions, sample inefficiently, and do not scale well to large or heterogeneous datasets. Large Language Models LLMs are sophisticated neural architectures trained on vast amounts of textual data, which equips them with the capacity to interpret, generate, and reason with natural language at human-like level [1, 13, 26]. These capabilities have enabled LLMs to become foundational tools across wide range of fields, including urban analytics [44, 25], social science [28], scientific research [50, 12], biomedical informatics [37], finance [56], legal reasoning [14], knowledge extraction [29], and creative content generation [10]. LLMs also serve as powerful agents for planning and large-scale simulation in scientific, technical, and policy domains [36, 43, 53]. Despite this remarkable versatility, standard LLM sampling methods face fundamental limitations when it comes to generating synthetic data that is faithful to real-world distributions. Specifically, standard LLMs cannot enforce global distributional alignment, generate samples efficiently, or overcome context length restrictions, making it impossible to directly produce large, statistically coherent datasets [47]. As result, outputs may excel at the individual sample level but lack rigorous control over marginal and joint statistical properties across samples, leading to synthetic data that is not reproducible, transparent, or statistically reliable. This shortcoming is especially critical in scientific, economic, and policy applications that demand high-fidelity, distribution-aligned data synthesis."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Problem Formulation Let the observed real-world dataset be denoted by Dreal = {xi}n i=1, where each sample xi consists of values of global set of variables = {vj}V j=1 that may include unstructured or nested fields (e.g., hierarchical structures). We assume that each xi is drawn i.i.d. from latent generative process, xi (x ϕ), ϕ Φ, where ϕ represents the true but unknown structural parameter governing data generation, drawn from known parameter space Φ. Due to the complexity of the generative model, the likelihood function (x ϕ) is analytically intractable. However, we assume access to an implicit generative simulator Gϕ, parameterized by ϕ, which induces simulated distribution ˆPϕ and enables drawing synthetic samples. Our goal is to use the simulator Gϕ to produce synthetic dataset Dsynth = {ˆxj}m j=1, where may differ from n, such that the distribution of Dsynth closely matches that of Dreal. In our framework, we instantiate Gϕ as pretrained LLM acting as nonparametric simulator, where the simulators behavior is implicitly controlled by prompt ϕ. At each iteration, new synthetic samples are generated based on the current ϕ and accumulated to form the synthetic dataset Dsynth. After each iteration, we update ϕ using feedback derived from discrepancies in summary statistics between Dreal and the evolving Dsynth, progressively guiding the synthetic data toward structural alignment with the real data in the summary statistics space, and thereby promoting overall distribution alignment. 3.2 Overview As illustrated in Figure 1, the method proceeds through an iterative synthesis loop composed of four key stages: (1) Statistical Summarization: Given dataset (real or synthetic), compute its summary 3 statistics s(D), including all marginal statistics and inferred joint statistics, which together represent the structural signal and serve as comparison basis; (2) Dependency Inference: This models the LLM as nonparametric copula simulator to infer joint structural components = {c1, c2, . . . }, where each ci specifies subset of relevant variables. (3) Structure Grounding: Compare summary statistics of real and synthetic data using discrepancy function Q(, ), yielding discrepancy signals δ = Q(cid:0)s(Dsynth), s(Dreal)(cid:1) and use δ to ground the generative process. (4) LLM Proposal Sampling: Conditioned on C, s(Dreal), and δ, the LLM generates set of proposal distributions {π(i)}k i=1. Samples drawn from these distributions are merged into the synthetic dataset Dsynth for use in the next iteration. Throughout, we use the notation D(t) synth to denote samples generated during the tth iteration, while omitting the superscript refers to the cumulative synthetic dataset Dsynth aggregated over all finished iterations. This iterative process refines and enriches Dsynth by directly minimizing discrepancy in the summary statistics space, resulting in synthetic data that faithfully reflects the statistical distribution of the real dataset Dreal. For clarity, all stages above are described at the level of single iteration, and the detailed iterative synthesis loop is presented in Section 3.5. 3.3 Inferring Structure via Statistics Summarization Statistics Summarization. Without tractable likelihoods, we cannot directly compare real and synthetic samples in the original data space. High-dimensional data often exhibits complex dependencies that are difficult to capture through raw comparison. To enable meaningful, structure-aware inference, we instead operate in lower-dimensional space defined by summary statistics. Formally, we define the summary statistic function s() that applies to any dataset and produces set of summary statistics s(D) that capture marginal or joint structure of the data, supporting conditioning on variable subsets, allowing discrepancy signals to be computed over marginals and joints during iterative synthesis. Importantly, in t-th iteration we apply s() to both the real dataset Dreal and the current synthetic dataset D(t) synth, so that we can directly compare them via discrepancy measures. To ensure general applicability across domains, summary statistics are computed separately for continuous and discrete variables. For continuous variables, we can extract empirical moments with quantile ranges. For discrete variables, we can compute frequency tables. We can also compute joint frequency distributions with proper binning strategies for continuous variables to capture joint dependencies across mixed variables. This guarantees that the summarization process is scalable, type-agnostic, and fully automated, while remaining adaptable to heterogeneous real-world data, interpretable by LLMs, and informative for downstream structural inference. Further implementation details are provided in Appendix C.1. LLM as Copula Simulator for Dependency Inference core challenge in high-dimensional generative modeling is uncovering latent statistical dependencies among variables. Pretrained on large-scale corpora, LLMs implicitly encode rich inductive biases over real-world relationships and co-occurrence patterns. This allows them to act as high-dimensional priors over plausible structures. When conditioned on summary statistics, LLMs can generalize beyond surface-level correlations to propose coherent and interpretable joint dependency patterns, without requiring explicit likelihood functions or parameter estimation. Given this, we treat the LLM as nonparametric copula simulator that infers structural dependencies directly from summary statistics s(D). We formalize this by modeling the joint structural components as samples from distribution implicitly defined by the LLM when conditioned on these statistics via prompting: LLM(cid:0)pcopula (s(Dreal)) (cid:1), where the prompt pcopula wraps inputs with task instructions, yielding joint structural components = {ci}C i=1. Each component defines local structural unit that captures latent dependency component of the joint distribution. The implemented prompt is provided in Appendix E. (1) Structure Grounding. While the joint structural components encode joint dependencies among selected variable subsets, it does not yet specify how these structures should inform the generative process. To integrate structural information, one approach is to prompt the LLM to estimate marginal and joint distributions and compare these to the real distribution to guide sampling. This is particularly useful when only aggregated statistics are available, such as in survey or census data. 4 In our setting, however, we have access to both the real dataset Dreal and the synthetic dataset Dsynth, allowing us to perform direct comparisons in the summary statistics space. Specifically, we apply the summary statistic function s() to each marginal variable and each joint structural component C. This yields two sets of summary statistics {sv(Dreal), sv(Dsynth)}vV and {sc(Dreal), sc(Dsynth)}cC, We then quantify discrepancies using the discrepancy function Q(, ): δv = Q(cid:0)sv(Dsynth), sv(Dreal)(cid:1), V; δc = Q(cid:0)sc(Dsynth), sc(Dreal)(cid:1), C. (2) These discrepancy signals collectively form local structure discrepancy δ = {δv}vV {δc}cC, which measures the degree to which the simulated distribution mismatch the observed structural signal both marginally and jointly. To be usable within the iterative loop, Q(, ) must be attributable, so each δv or δc can map to specific variable or subset and guide prompt updates. This non-differentiable signal drives iterative, structure-aligned sampling without retraining the LLM. Practically, we instantiate Q(, ) as discrepancy over discretized joint distributions estimated via empirical contingency tables. Continuous variables are discretized through fixed or quantile-based binning to ensure consistent treatment across variable types. This design enables scalable, typeagnostic structural comparison while preserving interpretability and enabling fine-grained structural correction signals. Through this structure-grounding mechanism, statistical discrepancies are transformed into LLM-interpretable prompts that iteratively guide the simulator to generate synthetic samples that reduce mismatch between Dreal and Dsynth. Details are provided in Appendix C.1. 3.4 Structure-conditioned LLM Sampling Once structural dependencies have been identified and grounded through summary statistics, we use the LLM as generative simulator to sample synthetic data conditioned on the inferred structure. Crucially, the LLM acts as strong prior over plausible high-dimensional configurations. This ensures that each synthetic sample ˆx forms coherent realization of valid joint distribution, even without explicit modeling of global density functions. Additionally, the LLM can condition its generation on structural signals, including summary statistics s(Dreal) (including variable semantics) and discrepancy signals δ, to modulate generation toward alignment at the distributional level. While the inductive prior ensures sample-level realism, the structural guidance enables dataset-level convergence. We define the structure-conditioned sampling process as: ˆx LLM(cid:0)psample(C, s(Dreal), δ)(cid:1), where ˆx denotes synthetic sample, and psample is the prompt constructed from C, s(Dreal) to guide generation process, and δ. This formulation allows the LLM to synthesize new samples that preserve semantic validity while iteratively reducing divergence from Dreal at the distributional level. (3) LLM Proposal Sampling. While the structure-conditioned generation process enables LLMs to align with target distributions through iterative refinement, sampling one data sample at time can be inefficient, particularly when generating large-scale synthetic datasets. To improve sampling efficiency without compromising statistical fidelity, we introduce LLM Proposal Sampling, general strategy in which the LLM outputs sampleable proposal distributions instead of directly generating data samples. Each proposal defines localized generative procedure that represents valid conditional or joint distribution, grounded in observed structure. This shifts the LLMs role from generating individual samples to planning or compiling distributional programs that external systems can execute efficiently. Formally, we reprogram Eq. 3 to let LLM produces proposals: {π1, . . . , πk} LLM(cid:0)pproposal(C, s(Dreal), δ)(cid:1), where each proposal πi describes sampleable distribution designed to reduce discrepancy between Dreal and Dsynth. Then, from each πi, we draw mi samples through (cid:83)k πi, resulting in total of (cid:80)k i=1 mi samples. The number of samples mi per proposal may be predefined or specified by the LLM. For iteration t, these samples collectively form D(t) (cid:9)mi j=1, ˆx(i) (cid:8)ˆx(i) i=1 (4) synth. Notably, each proposal πi may describe distribution through natural language, probabilistic rules, executable code, or references to external generators (e.g., diffusion models with ControlNet for image synthesis). This representational flexibility ensures that LLM Proposal Sampling remains practical and extensible across various real-world data generation scenarios. In this view, our framework can also serve as high-level distributional controller, guiding external generators toward structured and statistically aligned data synthesis. In our implementation (detailed in Appendix C.1), we adopt concrete realization of this idea tailored to heterogeneous data. Each proposal πi defines complete variable configuration over x, including optional nested structure for unstructured data. Discrete variables are assigned specific values, and continuous variables are specified by numeric ranges (e.g., quantile bins). Each proposal defines an interpretable, executable region of the joint space, enabling concrete sampling via standard routines and supporting scalable, generalizable synthesis across domains. 3.5 Iterative Data Synthesis Iterative Synthesis Loop We summarize the iterative data synthesis process of LLMSYNTHOR in Algorithm 1. At iteration t, the LLM infers updated structural dependencies C(t) based on the summary statistics of Dreal. Discrepancy signals δ(t) between the current synthetic dataset Dsynth and the real distribution are computed in the summary space, guiding the generation of new proposals {π(t) }. Samples from these proposals incrementally refine Dsynth, aligning it more closely with the real distribution over time. Algorithm 1: Iterative Synthesis Loop Require: Dreal, s(), Q(, ), iterations 1: for = 1 to do 2: (cid:1) C(t) LLM(cid:0)pcopula(s(Dreal))(cid:1) (cid:1) and s(cid:0)D(t1) Compute s(cid:0)Dreal synth ), s(Dreal)(cid:1) δ(t) Q(cid:0)s(D(t1) {π(t) D(t) Dsynth Dsynth D(t) } LLM(cid:0)pproposal synth (cid:83) i{ˆx(i)}, ˆx(i) π(t) synth synth 3: 4: 5: 6: 7: 8: end for 9: return Dsynth (cid:0)C(t), s(Dreal), δ(t)(cid:1)(cid:1) Theoretical Guarantee. Under mild conditions, the proposed iterative procedure progressively reduces the local-structure discrepancy between synthetic and real data. Let = be the set of structure units. For each and iteration t, define the local structure discrepancy δ(t) (ω) = Q(cid:0)su(Dsynth(ω)), su(Dreal)(cid:1), where Dsynth(ω) is the cumulative synthetic dataset after iteration t, and ω denotes the algorithmic randomness (e.g., the random seed). Expectations Eω[] are taken over ω, with the real dataset held fixed. structure unit is said to exhibit significant discrepancy at iteration t0 if δ(t0) Theorem 1 (Local Structural Consistency). Assume that Assumptions 13 (see Appendix D) hold. For any tolerance ε (0, τ ) and any with δ(t0) τ at some reference iteration t0, there exists finite t0 such that for all t, Eω τ , where τ > 0 is the dominance threshold. (ω)(cid:3) ε. (cid:2)δ(t) This result guarantees that, under the stated assumptions, the cumulative synthetic distribution aligns with the real data up to tolerance ε for all structure units that initially exhibit significant discrepancy. See Appendix for proof details and Appendix C, Figures 8 and 9, for convergence curves."
        },
        {
            "title": "4 Experiments",
            "content": "We propose three practical tasks across data formats, inherent distribution, and scientific domains to demonstrate the effectiveness of LLMSYNTHOR and its utilities. Unless otherwise stated, we perform experiments using Chat Completion mode GPT-4.1-nano [35]. All tasks use the same prompts and codes that are not directly used as data interfaces. 4.1 E-Commerce Transaction Synthesis E-commerce transaction data consists of both continuous and discrete variables with complex dependencies. These data are economically valuable, enabling applications such as dynamic pricing, recommendation, and fraud detection. To evaluate the controllability and fidelity of LLMSYNTHOR, we construct fully synthetic task based on known probabilistic process. Task Setup. We simulate controlled environment where each synthetic transaction is sampled from closed-form Bayesian network over six variables: {vA, vG, vL, vC, vX , vM }, representing 6 vA vG vL vC vX vM Gap Tvd Gap Tvd Gap Tvd Gap Gap Tvd Gap TVAE CTGAN CopulaGAN GReaT TabSyn LLMSYNTHOR 2.06 4.429 4.82 2.862 1.196 1.13 0.032 0.057 0.027 0.052 0.012 0.023 0.008 0.117 0.052 0.016 0.012 0.002 0.01 0.065 0.016 0.009 0.022 0.008 0.056 0.162 0.045 0.039 0.007 0.002 0.043 0.076 0.031 0.020 0.022 0. 0.054 0.080 0.057 0.045 0.045 0.010 0.02 0.028 0.024 0.027 0.01 0.022 113.194 138.998 151.239 169.866 114.12 12.762 0.085 0.059 0.047 0.104 0.067 0.011 0.017 0.088 0.045 0.009 0.028 0.003 0.013 0.022 0.014 0.012 0.005 0. Table 2: Marginal evaluation, lower values indicate better distribution alignment. user_age, gender, location_tier, product_category, price, and payment_method, respectively. The generative process follows structured probabilistic graphical model, with the joint distribution factorized as: p(vA, vG, vL, vC, vX , vM ) = p(vA) p(vG) p(vL) p(vC vA, vG) p(vX vC) p(vM vL). This setting enables precise control over dependency structures and allows rigorous evaluation of each models ability to capture both marginal and conditional distributions. Implementation details are provided in Section A.1. We generate reference dataset with 2000 samples as the target distribution for synthesis. All models are trained and evaluated on the same data, using multiple random seeds to report mean performance. Baselines. Given the structured tabular nature of this task, we compare LLMSYNTHOR to representative baselines across major generative paradigms: (1) TVAE and CTGAN (VAEand GAN-based models); (2) CopulaGAN (GAN with copula-based dependency modeling); (3) GReaT (autoregressive transformer for table generation); and (4) TabSyn (diffusion-based model). These methods cover diverse inductive biases and serve as strong baselines for evaluating fidelity and controllability. For fair comparison, we apply rejection sampling to ensure sample realism in baselines, while LLMSYNTHOR requires no such post-processing. [vA, vG, vC] [vC, vX ] [vL, vM ] Jsd Gap Jsd Gap Jsd Gap TVAE 0.23 0.074 0.245 0.106 0. 0.051 CTGAN 0.133 0.055 0.298 0. 0.145 0.076 CopulaGAN 0.133 0.057 0. 0.102 0.069 0.018 GReaT TabSyn 0. 0.058 0.382 0.177 0.038 0.020 0. 0.022 0.237 0.082 0.027 0.015 LLMSYNTHOR 0.071 0.022 0.134 0.020 0.007 0. Figure 2: Qualitative Distributions and Comparisons. Table 3: Joint evaluations. Results. We evaluate synthetic data quality from two perspectives: statistical fidelity and downstream utility. First, Tabs. 2 and 3 report marginal and joint distributional metrics, assessing how well each model preserves individual variable distributions and structured dependencies. We use Wasserstein distance (W) for continuous variables, total variation distance (TVD) for discrete variables, and classifier two-sample test (C2ST) Gap (acc 0.5) as general-purpose divergence measure. Joint subsets are selected based on the ground-truth Bayesian network. Figure 2 visualizes selected marginal and conditional distributions, including comparisons against GReaT. LLMSYNTHOR consistently achieves the lowest divergence and gap scores. Additional ablation studies, efficiency results, and visualizations are provided in Appendix C.2. We also assess the practical utility of synthetic data. We introduce two derived variables grounded in economic theory [39, 30]: discount_propensity (based on price elasticity of demand) and lifetime_value_band (a simplified proxy for customer lifetime value). Full definitions are provided in Appendix A.1. We train logistic regression, decision trees, and random forests on data generated by each method. Figure 2 shows that LLMSYNTHOR generalizes best to real data, demonstrating high fidelity and utility. We provide qualitative results tracking distributional divergence over iterations, demonstrating that LLMSYNTHOR effectively refines synthetic data toward the target distribution. Ablation studies further validate the role of the LLM as copula simulator, showing improved joint distribution learning. We also replace the backbone LLM with Qwen2.5-7B [52] and observe stable performance, highlighting the frameworks robustness. Full results and details are available in Appendix C.2. 7 4.2 Population Synthesis Population synthesis generates realistic microdata that preserves the joint distribution of demographic and household attributes. It is critical for applications like transportation planning, urban simulation, and policy analysis, where privacy-preserving alternatives to real population data are needed. The data is inherently unstructured, as households sizes vary. Task Setup. We use population microdata from the American Community Survey (ACS), focusing on households in Southern California. The dataset includes both householdand person-level attributes, resulting in unstructured records due to varying household sizes. After preprocessing, we obtain structured dataset of about 15,000 households with nine key variables. The task is to generate synthetic populations that preserve joint distributions across demographic and household features. To assess real-world utility, we define 16 policy-relevant queries across six categories: Equity, Vulnerability, Employment, Household, Demographics, and Mobility. Each query computes median or proportion capturing meaningful patterns (e.g., proportion of multigenerational households), which serves as proxy for distributional fidelity. Full data and query details are in Appendix A.2. Baselines. We compare LLMSYNTHOR with range of strong population synthesis baselines. (1) CP: Applies non-negative tensor factorization and normalizes into mixture-of-product-ofcategoricals. Household groups are inferred, with person models fitted per group. (2) HMM: hierarchical mixture model estimated via Expectation-Maximization (EM), using latent household and member classes with categorical distributions regularized by Dirichlet priors. (3) NVI: variational framework with amortized neural encoders and Gumbel-Softmax reparameterization, optimized via stochastic gradient variational inference (SGVI). These baselines span classical tensor methods, probabilistic generative models, and deep learning approaches, providing diverse points of comparison in fidelity, scalability, and structure-aware synthesis. 0.54 0.56 0.53 0.21 CP HMM NVI LLMSYNTHOR Demographics Employment Equity Household Mobility Vulnerability Results. Table 4 shows that LLMSYNTHOR achieves the lowest relative error in every category, often by large margin. For instance, on equity-related queries the error falls from 4.23 (HMM) to 0.25. Similar gains appear for demographics, employment, mobility, and vulnerability metrics. Although LLMSYNTHOR does not attain the top result on every single query, it outperforms all baselines on the majority of queries and on every aggregated category. Additional per-query results and distributional visualizations are available in Appendix C.3. These findings confirm that LLMSYNTHOR more accurately captures the high-order, non-linear joint dependencies present in real population data, producing synthetic populations with superior practical utility. Table 4: Category-wise mean relative error across queries. 1.47 0.48 0.24 0.35 5.79 4.23 5.49 0.25 2.34 2.01 2.06 0.13 0.86 0.91 1.06 0. 1.02 0.32 0.27 0.2 4.3 Mobility Synthesis Mobility synthesis generates realistic spatiotemporal trip and activity data while preserving privacy. It is essential for urban applications such as transport planning, demand forecasting, and emergency response, where access to real mobility traces is often restricted. Task Setup. We define the mobility synthesis task by integrating two complementary sources. From OpenPFLOW [18], we extract one day of trip records (origin, destination, timestamp) and assign transport modes using fixed distribution. Since OpenPFLOW lacks activity labels, we incorporate timeactivity patterns from LLMob [16] to model cross-source distribution. This task evaluates the ability to align heterogeneous spatiotemporal and behavioral data. As existing methods cannot handle mixed-source synthesis without significant modification, we focus on qualitative evaluation. We generate 30,000 trips over day in Tokyo to match both distributions. Further details are provided in Appendix A.3. Results. Figure 3 compares real and synthetic mobility patterns across three views. In (a), timeactivity heatmaps show close alignment: synthetic data accurately capture commuting peaks in Travel & Transport and midday rises in Shop & Service. notable difference appears at 6-9AM, where LLMSYNTHOR generates more Food and Shop & Service activities, likely reflecting LLM priors about morning routines. While this may seem like limitation, such deviations can also reveal or correct for censorship biases in real data. In (b), OD flow heatmaps during the morning peak show that synthetic trips reproduce key spatial patterns, matching high-density origin and destination areas in residential and commercial zones. Additional results are provided in Appendix C.4. Controllable Mobility Synthesis for Events Simulation We simulate concert at Tokyo Dome (20-24h) by adding the prompt There will be concert from 2024 at Tokyo Dome during proposal generation. As shown in Figure 3, this simple intervention causes LLMSYNTHOR to generate surge of trips to the event location, closely matching real-world patterns while preserving realistic background flows. The increased flow across other areas reflects broader spatialtemporal impacts on urban mobility. This demonstrates LLMSYNTHOR controllability and its potential for what-if scenario planning, allowing policymakers to simulate the effects of large events or interventions in privacy-preserving synthetic framework. Figure 3: Real vs. synthetic mobility patterns. 4.4 Discussion Our experiments demonstrate that LLMSYNTHOR consistently achieves high statistical fidelity, strong structural alignment, and practical utility across diverse domains. Despite its versatility, LLMSYNTHOR has several limitations. First, LLMs encode strong behavioral priors, which can occasionally introduce biases inconsistent with real-world data. This can be mitigated through stricter prompt design or the removal of semantic cues during generation. Second, the framework does not yet scale well to extremely high-dimensional settings (e.g., datasets with hundreds or thousands of variables), as performance depends on the context window and reasoning capabilities of the LLM, though this will improve with future model advances. Third, while LLMSYNTHOR is effective for mixed-type i.i.d. data, it is less suited for perceptual or sequential data such as images or time series. However, it can act as high-level controller to guide domain-specific generators for those modalities. Finally, although LLMSYNTHOR does not explicitly incorporate formal privacy guarantees such as differential privacy, its synthesis process is based on aligning summary statistics, rather than memorizing or replicating individual records, which inherently reduces the risk of direct re-identification and exposure of sensitive data."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present LLMSYNTHOR, general framework that transforms large language models into structure-aware simulators capable of producing high-fidelity, statistically grounded synthetic data across diverse domains. By unifying the semantic richness and reasoning capacity of LLMs with rigorous, distribution-guided inference, LLMSYNTHOR overcomes the persistent trade-offs between flexibility, statistical alignment, and scalability that have long challenged existing methods. Our results across e-commerce, population, and mobility data demonstrate that LLMSYNTHOR not only achieves superior fidelity and utility compared to baselines, but also establishes solid foundation for the next generation of adaptable and robust data synthesis frameworks. This work opens new avenues for scientific research, policy-making, and innovation in data-driven fields. As language models continue to evolve, the generality and extensibility of our approach will empower broad spectrum of applications where secure, reliable, and high-quality synthetic data are essential."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3 [2] Patricia Apellániz, Juan Parras, and Santiago Zazo. An improved tabular data generator with vae-gmm integration. In 2024 32nd European Signal Processing Conference (EUSIPCO), pages 18861890. IEEE, 2024. 2 [3] Athanassios Avramidis, Nabil Channouf, and Pierre LEcuyer. Efficient correlation matching for fitting discrete multivariate distributions with arbitrary marginals and normal-copula dependence. INFORMS Journal on Computing, 21(1):88106, 2009. 2 [4] Mrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. Synthesizing electronic health records using improved generative adversarial networks. Journal of the American Medical Informatics Association, 26(3):228241, 2019. 2 [5] Richard Beckman, Keith Baggerly, and Michael McKay. Creating synthetic baseline populations. Transportation Research Part A: Policy and Practice, 30(6):415429, 1996. 2 [6] Steven Bellovin, Preetam Dutta, and Nathan Reitinger. Privacy and synthetic datasets. Stan. Tech. L. Rev., 22:1, 2019. 1 [7] Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022. 2, 22 [8] Brian dAlessandro, Cathy ONeil, and Tom LaGatta. Conscientious classification: data scientists guide to discrimination-aware classification. Big data, 5(2):120134, 2017. 22 [9] Cristóbal Esteban, Stephanie Hyland, and Gunnar Rätsch. Real-valued (medical) time series generation with recurrent conditional gans. arXiv preprint arXiv:1706.02633, 2017. 2 [10] Carlos Gómez-Rodríguez and Paul Williams. confederacy of models: comprehensive evaluation of llms on creative writing. arXiv preprint arXiv:2310.08433, 2023. 3 [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1 [12] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. 3 [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3 [14] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems, 36:4412344279, 2023. 3 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [16] WANG JIAWEI, Renhe Jiang, Chuang Yang, Zengqing Wu, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao, et al. Large language models as urban residents: An llm agent framework for personal mobility generation. Advances in Neural Information Processing Systems, 37:124547124574, 2024. 8, 20 [17] Sanket Kamthe, Samuel Assefa, and Marc Deisenroth. Copula flows for synthetic data generation. arXiv preprint arXiv:2101.00598, 2021. 2 10 [18] Takehiro Kashiyama, Yanbo Pang, and Yoshihide Sekimoto. Open pflow: Creation and evaluation of an open dataset for typical people mass movement in urban areas. Transportation research part C: emerging technologies, 85:249267, 2017. 8, [19] Jayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. arXiv preprint arXiv:2210.04018, 2022. 2 [20] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 1 [21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 2 [22] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. In International Conference on Machine Learning, pages 1756417579. PMLR, 2023. 2 [23] Teyun Kwon, Norman Di Palo, and Edward Johns. Language models as zero-shot trajectory generators. IEEE Robotics and Automation Letters, 2024. 2 [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 24 [25] Yuebing Liang, Yichao Liu, Xiaohan Wang, and Zhan Zhao. Exploring large language models for human mobility prediction under public events. Computers, Environment and Urban Systems, 112:102153, 2024. 3 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 3 [27] Frederico Lopes, Carlos Soares, and Paulo Cortez. Privatectgan: Adapting gan for privacyaware tabular data sharing. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 169180. Springer, 2023. [28] Benjamin Manning, Kehang Zhu, and John Horton. Automated social science: Language models as scientist and subjects. Technical report, National Bureau of Economic Research, 2024. 3 [29] Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, and Stefano Ermon. Geollm: Extracting geospatial knowledge from large language models. arXiv preprint arXiv:2310.06213, 2023. 3 [30] Alfred Marshall. Principles of economics. Springer, 2013. 7 [31] Kirill Mueller and Kay Axhausen. Hierarchical ipf: Generating synthetic population for switzerland. Arbeitsberichte Verkehrs-und Raumplanung, 718, 2011. 2 [32] Roger Nelsen. An introduction to copulas. Springer, 2006. [33] Sergey Nikolenko et al. Synthetic data for deep learning, volume 174. Springer, 2021. 1 [34] Ostap Okhrin, Alexander Ristig, and Ya-Fei Xu. Copulae in high dimensions: an introduction. Applied quantitative finance, pages 247277, 2017. 2 [35] Openai. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. 6 [36] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. 3 [37] Jianing Qiu, Lin Li, Jiankai Sun, Jiachuan Peng, Peilun Shi, Ruiyang Zhang, Yinzhao Dong, Kyle Lam, Frank P-W Lo, Bo Xiao, et al. Large ai models in health informatics: Applications, challenges, and the future. IEEE Journal of Biomedical and Health Informatics, 27(12):6074 6087, 2023. 3 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [39] Roland Rust, Katherine Lemon, and Valarie Zeithaml. Return on marketing: Using customer equity to focus marketing strategy. Journal of marketing, 68(1):109127, 2004. 7 [40] Lijun Sun and Alexander Erath. bayesian network approach for population synthesis. Transportation Research Part C: Emerging Technologies, 61:4962, 2015. [41] Lijun Sun, Alexander Erath, and Ming Cai. hierarchical mixture modeling framework for population synthesis. Transportation Research Part B: Methodological, 114:199212, 2018. 2 [42] Yihong Tang, Junlin He, and Zhan Zhao. Activity-aware human mobility prediction with hierarchical graph attention recurrent network. IEEE Transactions on Intelligent Transportation Systems, 2024. 1 [43] Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, and Jinhua Zhao. Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to composite spatial reasoning. arXiv preprint arXiv:2410.16162, 2024. 3 [44] Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Zhaofeng Wu, Dingyi Zhuang, Jushi Kai, Kebing Hou, Xiaotong Guo, Jinhua Zhao, et al. Itinera: Integrating spatial optimization with large language models for open-domain urban itinerary planning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 14131432, 2024. 3 [45] Syed Mahir Tazwar, Max Knobbout, Enrique Hortal Quesada, and Mirela Popa. Tab-vae: novel vae for generating synthetic tabular data. In ICPRAM, pages 1726, 2024. [46] Hoang Thanh-Tung and Truyen Tran. Catastrophic forgetting and mode collapse in gans. In 2020 international joint conference on neural networks (ijcnn), pages 110. IEEE, 2020. 2 [47] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: survey of techniques to extend the context length in large language models. arXiv preprint arXiv:2402.02244, 2024. 2, 3 [48] Yuxin Wang, Duanyu Feng, Yongfu Dai, Zhengyu Chen, Jimin Huang, Sophia Ananiadou, Qianqian Xie, and Hao Wang. Harmonic: Harnessing llms for tabular data synthesis and privacy protection. arXiv preprint arXiv:2408.02927, 2024. 2 [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 34 [50] Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger Zimmermann, and Jinhua Zhao. Reimagining urban science: Scaling causal inference with large language models. arXiv preprint arXiv:2504.12345, 2025. 3 [51] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. Advances in neural information processing systems, 32, 2019. 2, [52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 7 12 [53] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. 3 [54] Danqing Zhang, Junyu Cao, Sid Feygin, Dounan Tang, Zuo-Jun Max Shen, and Alexei Pozdnoukhov. Connected population synthesis for transportation simulation. Transportation research part C: emerging technologies, 103:116, 2019. 2 [55] Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in latent space. arXiv preprint arXiv:2310.09656, 2023. 2, 22 [56] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. Revolutionizing finance with llms: An overview of applications and insights. arXiv preprint arXiv:2401.11641, 2024."
        },
        {
            "title": "Appendix",
            "content": "Table of Contents Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.1 E-Commerce Transactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 Population . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Mobility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 E-Commerce Transaction Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2 Population Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2.1 CP (Candecomp/Parafac) Tensor Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2.2 HMM and NVI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 E-Commerce Transaction Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3 Population Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.4 Mobility Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Data",
            "content": "A.1 E-Commerce Transactions vG vA vC vX vL vM Figure 4: Bayesian network representing the generative process of e-commerce transactions. Data Generating Process. Let vA, vG, vL, vC, vX , and vM denote the random variables for user age, gender, location tier, product category, price, and payment method, respectively. We assume the following generative mechanisms: (cid:16) 3 (cid:88) 3 (cid:88) (cid:17) 1. vA πi (µi, σ2 ) 1[18,90](vA), where πi = 1, πi > 0, i= i=1 vG discrete(cid:0)p(male) , p(female) (cid:1), p(male) + p(female) = 1, vL discrete(cid:0)p(1) , p(2) , p(3) (cid:1), 3 (cid:88) k=1 p(k) = 1, vC (vA, vG) discrete(cid:0)p(g(vA), vG) (cid:1), g(vA) = vA < 35, young, middle, 35 vA < 55, old, vA 55, vX vC = (µc, σ2 ) 1[ℓ,u](vX ), [ℓ, u] denotes the valid price range, vM vL = discrete(cid:0)p(k) (cid:1), p(k) (j) = 1 k. (cid:88) 2. 3. 4. 5. 6. Under this model, the joint distribution factorizes as p(vA, vG, vL, vC, vX , vM ) = p(vA) p(vG) p(vL) p(cid:0)vC vA, vG (cid:1) p(cid:0)vX vC (cid:1) p(cid:0)vM vL (cid:1) , enforcing the exact conditional independencies of the Bayesian network in Figure 4. Implementation Settings. parameter values: In our experiments, we instantiate the above model with the following User Age (vA): mixture of three Gaussians with weights π = [0.5, 0.35, 0.15], means µ = [24, 38, 55], standard deviations σ = [6, 15, 10], truncated to [18, 90]. User Gender (vG): two categories {Male, Female} with probabilities [0.45, 0.55]. Location Tier (vL): two categories {Developed, Developing} with probabilities [0.40, 0.60]. Product Category (vC): four categories {Electronics, Apparel, Food & Beverages, Furniture & for each age group {young, middle, old} Appliances}. Conditional probability tables p(g,vG) and gender vG are: young, Male: [0.50, 0.25, 0.05, 0.20], young, Female: [0.20, 0.50, 0.05, 0.25] middle, Male: [0.20, 0.10, 0.50, 0.20], middle, Female: [0.10, 0.20, 0.55, 0.15] old, Male: [0.10, 0.10, 0.65, 0.15], old, Female: [0.10, 0.10, 0.60, 0.20] Price (vX ): to {(800, 1000), (100, 500), (100, 400), (200, 500)} for {Electronics, Apparel, Food & Beverages, Furniture & Appliances}, respectively, then truncate to [0, 2000]. sample (µc, σ2 ) with (µc, σc) category each for set c, Payment Method (vM ): two categories {Online Payment, Cash on Delivery}, with p(Developed) = [0.70, 0.30] and p(Developing) = [0.40, 0.60]. 15 Derived Economic Variables. To evaluate the practical utility of synthetic data, we introduce two composite variables that simulate realistic segmentation and value estimation tasks. These targets are not included in the data generation process and are computed entirely post hoc. From machine learning perspective, they serve to test whether models trained on synthetic data can support meaningful discriminative tasks involving high-order, nonlinear feature interactions that resemble real-world decision logic. The variables are constructed to reflect common use cases in business and marketing analytics: discount targeting and customer lifetime value estimation. Both combine domain knowledge and structural dependenciesacross price, age, payment behavior, and product categoryinto proxy labels that require the generative model to faithfully reconstruct multiple conditional pathways. Discount Propensity (d). This variable approximates sensitivity to price, inspired by the concept of price elasticity of demand. Instead of modeling actual demand shifts, we simulate \"discount responsiveness score\" based on how far transaction price deviates from the expected norm and is modified by age and behavioral context. We first compute z-score for the transaction price within its category: = vX µvC σvC , where µvC and σvC are the category-wise mean and standard deviation. The composite score is then = tanh(z) + 0.01 (vA 35)2 100 + 0.5 1{vM = COD} + 0.3 1{vL = Developing}. Each term encodes an interpretable behavioral signal: tanh(z) models diminishing sensitivity to extreme price deviations, the quadratic age term increases discount likelihood for younger and older users, peaking at age 35, COD users (+0.5) and those in developing regions (+0.3) are assumed to be more price-conscious based on consumer behavior research. The 0.01 scale factor balances the influence of the age component, keeping it comparable to binary modifiers. The final categorization is: = High, > 1, Low, Mid, < 1, otherwise. Lifetime Value Band (ℓ). This variable acts as simplified proxy for customer lifetime value (CLV), an important metric for revenue forecasting and segmentation. CLV is typically defined as the discounted sum of future profits; we approximate this by incorporating transaction amount, demographic potential, and purchase channel/product effects. We compute proxy score ℓ0 that reflects expected customer value as function of transaction size, demographic potential, and behavioral modifiers: vX wm log(1 + vA 35) + 1 where wm is payment-method multiplier (1.2 for online, 0.85 for COD), encoding expected retention and conversion ease, wc is product-category multiplier: ℓ0 = wc, wc = 1.3, 1.1, 0.9, 1.4, vC = Electronics, vC = Apparel, vC = Food & Beverages, vC = Furniture & Appliances. The age-adjusted denominator reduces the projected value for extreme age groups, approximating long-term activity potential. We then discretize the proxy score ℓ0 into ordinal bands to reflect interpretable customer value segments: ℓ = High, Low, Mid, ℓ0 > 20, ℓ0 < 10, otherwise. The thresholds and coefficients used are fixed, interpretable, and manually defined. They are not optimized on the data and are instead chosen to encode plausible, domain-informed assumptions. All values are fixed across all experiments and applied identically to both real and synthetic datasets to ensure fair and reproducible evaluation of downstream discriminative utility. 16 A.2 Population Data We use the 2023 American Community Survey (ACS) 1-Year Public Use Microdata Sample (PUMS) , large-scale, nationally representative dataset released by the U.S. Census Bureau. The ACS is an ongoing survey program that provides detailed annual data on demographic, economic, housing, and social attributes of the U.S. population. Its microdata records offer rich insights that inform resource allocation, urban planning, public policy, and equity analysis at multiple geographic levels. The 2023 ACS 1-Year PUMS includes anonymized householdand person-level records, making it well-suited for population synthesis. For this study, we extract data from households in Southern California. We retain both household-level variables (e.g., number of persons, income, tenure, household type, vehicle ownership) and person-level attributes (e.g., age, race, education, employment, income). Since households vary in size and structure, the resulting data are inherently unstructured. To ensure consistency and quality, we apply the following preprocessing steps: Household filtering: We include only family households (record type H) with 2 to 5 members, drop entries with missing key fields, and normalize housing tenure and household type into interpretable categories. Person filtering: We retain individuals belonging to valid households and ensure non-missing values for core attributes like age and race. Records with anomalous or censored race categories or negative income are excluded. Variable recoding: Education levels are grouped into broader bands (e.g., high school, bachelor, master), employment status is collapsed, and race is mapped into simplified categories. Householdperson consistency: Only households whose member count matches the reported size (NP) are retained, ensuring alignment between household and person tables. Final dataset: We obtain cleaned population microdata sample consisting of 15,380 households, each associated with individual-level demographic and socioeconomic details. The resulting dataset contains both householdand person-level records. The variables are: Household-level variables: num_persons (25): Number of people in the household. housing_tenure (Owned, Rented): Indicates whether the household owns or rents their home. householder_type (Couple, Single Male, Single Female): Household composition based on adult structure. num_vehicles (06): Number of vehicles available to the household. Person-level variables: age (099): Age of the individual. race (White, Black, Indian, Asian, Combined): Simplified racial categories derived from ACS coding. education (Preschool, Elementary, High School, Bachelor, Master, Doctorate): Highest educational attainment. employment (Under 16, Employed, Unemployed, Military): Employment status grouped into meaningful categories. income (04,209,995): Individual annual income in U.S. dollars. Due to household size variation, the data is inherently unstructured, with each household containing variable number of individual records. This structure poses realistic and valuable challenge for generative models aimed at cross-level joint distribution synthesis. More detailed variable descriptions can be found at this link. https://www.census.gov/programs-surveys/acs/microdata/access.html https://www.census.gov/programs-surveys/acs/microdata/documentation.html 17 Query Definitions Accurate synthetic population data must enable meaningful analysis that informs real-world policy and planning. This includes identifying economic disparities, assessing social vulnerability, and supporting decisions on housing, mobility, and public services. To evaluate the practical value of such data, we define 16 policy-relevant queries grouped into six thematic categories. Each query includes description and an explanation of its social significance. Equity (1) median_income_black_households Description: Median total household income for households with at least one Black member. Social significance: This indicator measures racial income inequality and highlights the economic disparities that affect Black households. (2) prop_multigenerational_racial Description: Proportion of multigenerational households, defined as those with both children under 18 and elderly members aged 65 or older, that include at least one non-White individual. Social significance: This indicator sheds light on racial patterns in multigenerational living and provides insight into caregiving structures and household composition across different racial groups. Vulnerability (1) prop_elderly_poverty Description: Proportion of households that include at least one member aged 65 or older and have per-person income below 12,500. Social significance: This indicator reflects the economic vulnerability of older adults and helps identify households at risk of poverty in later life. (2) prop_female_headed_poverty Description: Proportion of single female-headed households with per-person income below 15,000. Social significance: This indicator highlights the financial challenges faced by single women who lead households, pointing to gendered dimensions of poverty. (3) prop_high_dependency_ratio Description: Proportion of households in which the combined number of children and elderly members exceeds the number of working-age members. Social significance: This indicator identifies households that carry high care burden and may be more susceptible to financial and caregiving stress. Employment (1) prop_high_edu_unemployed Description: Proportion of households where the highest level of education attained is Masters degree or Doctorate, and no household members are employed. Social significance: This indicator highlights potential underemployment or barriers to workforce participation among highly educated individuals. (2) prop_dual_earner_couples Description: Proportion of couple households where both adults are employed. Social significance: This indicator reflects the prevalence of dual-income households and offers insight into labor force participation within traditional family structures. (3) avg_income_per_person_dual_earner Description: Average per-person income in couple households where both adults are employed. Social significance: This indicator captures the financial outcomes associated with dual-earning and helps assess economic inequality among working families. 18 Household (1) age_gap_owners_vs_renters Description: Difference in the average age of household members between owner-occupied and renter-occupied households. Social significance: This indicator reflects how age distribution relates to housing tenure and supports analysis of life stage patterns in housing access. (2) prop_multigenerational Description: Proportion of households that include both children under the age of 18 and elderly members aged 65 or older. Social significance: This indicator highlights the prevalence of multigenerational living arrangements and the caregiving responsibilities they may involve. Demographics (1) median_avg_age Description: Median value of the average age of household members across all households. Social significance: This indicator provides summary measure of age distribution at the household level and supports analysis related to population aging and demographic representation. (2) prop_families_with_children Description: Proportion of households that include at least one child under the age of 18. Social significance: This indicator reflects the presence of families with children and helps identify demand for youth-oriented services and policies. Mobility (1) prop_no_vehicle_low_income Description: Proportion of households with no vehicles and per-person income below 15,000. Social significance: This indicator measures transportation disadvantage among low-income households and highlights potential barriers to employment, healthcare, and other essential services. (2) prop_child_no_vehicle Description: Proportion of households with at least one child under the age of 18 and no vehicles. Social significance: This indicator identifies families that may face challenges accessing child care, schools, and other child-related services due to limited transportation. (3) prop_elderly_no_vehicle Description: Proportion of households with at least one member aged 65 or older and no vehicles. Social significance: This indicator highlights potential mobility limitations and the risk of social isolation among older adults without access to private transportation. (4) prop_high_vehicle_high_income Description: Proportion of households that own three or more vehicles and have per-person income greater than 50,000. Social significance: This indicator reflects concentrations of material wealth and can provide insight into patterns of resource consumption and their potential environmental impact. 19 A.3 Mobility Data Sources. To construct our synthetic mobility task, we integrate two complementary datasets capturing different aspects of urban movement. The first is OpenPFLOW [18], high-resolution GPS trajectory dataset collected in the Tokyo metropolitan area. It provides detailed logs of individual movement traces, including agent ID, timestamp, location (latitude and longitude). However, OpenPFLOW lacks semantic labels indicating trip purpose or activity. To address this, we incorporate external behavioral data from LLMob [16], which uses Foursquare check-ins to infer timeactivity distributions across categories (e.g., Shop & Service, Food, Travel & Transport) aggregated by time-of-day. Combining these two sources allows us to model not only where and when people move, but also whya key requirement for realistic and policy-relevant mobility synthesis. and Geographic Scope and Temporal Binning. We focus on central urban rebounded by longitude gion in Tokyo, latitude [139.6726, 139.8896] [35.6004, 35.7788]. This area encompasses both dense commercial hubs and residential zones, offering diverse mobility Time is patterns, as shown in Figure 5. discretized into seven semantically meaningful intervals based on established guidelines in activity-based travel modeling: 06, 69, 912, 1214, 1417, 1720, 2024. These bins reflect common human routines (e.g., commuting peaks, lunch breaks, nighttime activity) and support alignment with LLMob activity distributions. Grid-Based Spatial Representation. To convert raw GPS points into spatially structured format, we partition the study area into uniform grid of square regions (2km resolution). Each trips origin and destination are mapped to regions via geometric projection. This discretization supports several goals: (1) privacy, by abstracting fine-grained trajectories; (2) compatibility with OD (origindestination) matrix modeling; and (3) alignment with spatial units used in real-world planning. Figure 5: Study area within the Tokyo metropolitan region. Trip Segmentation and Transport Mode Assignment. To construct trip-level records from raw trajectory data, we first segment each agents movement timeline into individual trips by identifying transition points where stationary periods are followed by motion. This segmentation is based on temporal gaps and changes in movement signals. From each trip, we extract its departure and arrival coordinates, time of occurrence, and associated transport mode. However, OpenPFLOWs original mode labels are often noisy, incomplete, or unreliable, which limits their use for structural modeling and evaluation. To address this, we reassign transport modes using distance-sensitive probabilistic model. Specifically, we calculate the great-circle distance between the origin and destination regions of each trip and assign transport mode based on calibrated distribution that reflects common behavioral patterns. Shorter trips are more likely to be labeled as walking, mid-range trips as biking, and longer trips as driving. This approach improves behavioral realism and produces mode variable that is both interpretable and structurally informative. Placeholder trips and intra-region trips are removed to ensure all retained trips exhibit spatial displacement and meaningful mode behavior. The resulting dataset serves as consistent and tractable basis for synthesis and evaluation. Activity. Since OpenPFLOW lacks activity labels, we do not annotate real trips with activity types. Instead, we leverage LLMobs time-conditioned activity distribution as external structural guidance. This distribution is used as target summary statistic during proposal generation, encouraging synthetic trips to match realistic time-activity patterns without directly assigning activity labels to observed data. Outputs. The final dataset includes: cleaned trip table with fields: time_str, activity, od, and transport. Grid region definitions and metadata (region size, bounds, projection). OD matrices per time bin, capturing trip flows between region pairs. regionregion distance matrix used for spatial modeling and transport inference. per-region top-k activity summary describing local land use patterns. Design Rationale. This mixed-source design reflects practical reality: in many urban contexts, movement traces (from GPS or transit data) and semantic behaviors (from points of interest or social platforms) are available from separate sources. Synthesizing mobility data that integrates both dimensions requires models to learn from partially aligned, heterogeneous inputs. Our preprocessing aims to bridge this gap, transforming fragmented empirical records into coherent, interpretable format that supports structure-aware synthesis. By modeling OD patterns, temporal rhythms, transport behavior, and activity semantics jointly, this setup enables robust evaluation of synthetic data quality from both spatial and behavioral perspectives."
        },
        {
            "title": "B Baselines",
            "content": "B.1 E-Commerce Transaction Synthesis We briefly summarize the baseline models used in our experiments: TVAE [51] Tabular Variational Autoencoder (TVAE) models tabular data by encoding mixed-type variables into continuous latent space via VAE framework. It supports conditional sampling and employs Gumbel-Softmax reparameterization for discrete features. TVAE captures global structure but tends to oversmooth discrete modes, especially under imbalanced categories. CTGAN [51] Conditional Tabular GAN (CTGAN) improves over TVAE by introducing conditional GAN architecture. It uses mode-specific normalization and balanced training sampler to enhance rare-category representation. While improving fidelity and diversity, CTGAN suffers from the inherent instability of GAN training and mode collapse. CopulaGAN [8] CopulaGAN integrates copula-based modeling into the GAN framework to separate marginal estimation from dependency modeling. By embedding copula regularization into the loss function, it improves the alignment between synthetic and real data in both marginal and joint distributions. This structure-aware regularization enhances statistical fidelity across variable combinations. GReaT [7] GReaT reformulates tabular synthesis as sequence modeling task, using an autoregressive Transformer trained on linearized table rows. It supports pretraining and zero-shot generation, demonstrating strong generalization. However, it lacks explicit distributional control, and statistical alignment with the real data is not guaranteed. TabSyn [55] TabSyn combines VAE encoder with score-based diffusion model in the latent space. Mixed-type tabular data are first embedded into structured continuous space, where denoising diffusion model performs generative sampling. This hybrid design ensures high fidelity, fast sampling, and better category preservation. TabSyn achieves strong performance on mixed-type datasets with improved statistical accuracy and diversity. B.2 Population Synthesis B.2.1 CP (Candecomp/Parafac) Tensor Factorization The CP baseline discretizes the joint household-person population into high-dimensional contingency tensor Rd1dK , where each axis corresponds to household or person attribute. non-negative CP decomposition is applied: ˆXi1,...,iK = (cid:88) λr (cid:89) A(k) ik,r, r=1 where λr denotes the weight of component r, and A(k) are factor matrices. After normalization, the model yields mixture-of-product-of-categoricals (MPC) distribution: k=1 QCP(i) = (cid:88) wr (cid:89) r=1 k=1 a(k) ik,r. Households are assigned to their most likely latent group via = arg maxr wr ik,r, and within each group, person-level non-negative CP factorization model Mg is fit for joint household-member sampling. (cid:81) a(k) B.2.2 HMM and NVI This section presents two hierarchical generative baselines used for structured population synthesis: HMM (Product Multinomial Hierarchical Mixture Model) and NVI (Neural Variational Inference). Both approaches are based on shared latent graphical model but differ in the inference procedure and parameterization. Hierarchical Generative Model The generative process assumes two-layer structure: zi Cat(λ), zi = Cat(ϕ(k) zij Cat(µg), zij = Cat(θ(ℓ) ), gm), x(k) x(ℓ) ij (5) (6) (7) (8) where zi is the latent household class and zij is the latent member class of person in household i. All conditional distributions are categorical, regularized by symmetric Dirichlet priors. HMM: Expectation-Maximization Estimation The HMM baseline employs the ExpectationMaximization (EM) algorithm for parameter estimation, alternating between posterior updates of latent variables and maximization steps. E-step (posterior responsibilities): γg = Pr(zi = data), ρm ij = Pr(zij = zi = g, data). M-step (parameter updates): λg µgm ϕ(k) (c) θ(ℓ) gm(c) (cid:88) γg , 1 (cid:80) , ij γg ρm ij (cid:80) ij γg γg I(x(k) = c) (cid:80) γg ij γg ρm ij (cid:80) I(x(ℓ) ρm ij ij γg (cid:80) (cid:80) , ij = c) . (9) (10) (11) (12) (13) (14) This formulation is particularly suited for moderate-scale datasets with clear latent groupings and yields interpretable household and person class structures. NVI: Amortized Variational Inference The NVI baseline implements amortized mean-field variational inference using neural networks to parameterize posterior distributions. The variational posterior is factorized as: q(z, z1:M ) = (cid:89) qϕ(zi xi) (cid:89) qψ(zij zi, xij), (15) Both qϕ and qψ are implemented using multilayer perceptrons (MLPs). Discrete latent variables zi and zij are reparameterized via the Gumbel-Softmax trick: zi = Softmax (cid:19) (cid:18) log γi + τ , Gumbel(0, 1), (16) with temperature τ gradually annealed to 0.1. The model is trained to maximize the evidence lower bound (ELBO): = Eq(z)[log p(x, z) log q(z x)], (17) where all categorical likelihoods are modeled using softmax-parameterized logits. Optimization is performed via mini-batch stochastic gradient descent, and missing values are handled via masking or learned probabilities. This variational method provides scalable and flexible alternative to classical EM, suitable for high-dimensional structured populations."
        },
        {
            "title": "C Experiments",
            "content": "C.1 Implementation Details Summary Statistics For continuous variables, we adopt hierarchical, quantile-based binning approach to construct robust and interpretable summary statistics. First, we determine the main bin boundaries using the quantiles of the real data distribution. Specifically, the observed value range is partitioned into fixed number of main bins (by default, 6 bins) such that each bin contains approximately equal numbers of real data samples. This avoids issues of bin sparsity and ensures fair representation of all regions of the distribution, including heavy tails and outliers. To further capture local distributional features, we perform secondary binning step: the main bin with the largest positive discrepancy between real and synthetic data (that is, where the real proportion exceeds the synthetic proportion by the greatest margin) is selected for further subdivision. Within this main bin, we create fixed number of sub-bins, again using uniform spacing within the main bins range. The frequencies for these sub-bins are then normalized so that their sum matches the original frequency of the parent main bin, ensuring consistency. This two-stage binning scheme is designed to highlight both global and local mismatches between real and synthetic data, enabling more sensitive detection and correction of model errors. The use of quantile-based main bins ensures statistical stability across diverse datasets and prevents dominance by densely populated intervals. Sub-bin refinement targets the most relevant region for further analysis or adjustment. All discrete variables (and discretized bins of continuous variables) are then summarized as frequency tables. For joint statistics, we compute empirical contingency tables over selected groups of variables, using the same binning strategy for continuous components. This unified, type-agnostic summarization supports scalable and LLM-interpretable structural inference. Discrepancy The statistical discrepancy Q(, ) between real and synthetic data is computed directly as the difference (typically total variation distance or L1 distance) between the categorical frequency tables defined above, at both marginal and joint levels. For marginals, this measures the maximum absolute difference in bin frequencies between datasets; for joints, it does the same over contingency tables of bin combinations. This approach is robust to scale, naturally handles missing or empty bins, and provides fine-grained, interpretable correction signals. Crucially, the construction of our summary statistics means that every entry in frequency table (i.e., every bin or category) can be individually attributed to specific value range (for continuous variables) or category (for discrete variables), or combination thereof for joint tables. This attributability property is essential for the iterative synthesis loop: it enables us to pinpoint where synthetic data diverges from the real distribution and directly guide the LLM to correct discrepancies in precisely those ranges or categories, rather than making global, undirected adjustments. Hyperparameters We generate 5 proposals per iteration for total of 100 iterations. In each iteration, 3 joint structural components are inferred for grounding. For LLM inference, we use GPT-4.1-nano with temperature of 0.8, and Qwen2.5-7B-Instruct deployed via the VLLM [24] framework. Generation hyperparameters are set as follows: max_new_tokens = 2048, temperature = 0.7, top_k = 20, and top_p = 0.98. For summary statistics and contingency calculations, we adopt two-level quantile-based binning scheme for continuous variables: 6 main bins and 8 sub-bins per main bin. This design captures both global and local structure in the distributions. All discrete variables and discretized bins are handled uniformly when computing marginal and joint frequency tables. All experiments are repeated at least three times, and we report the mean result. The uncertainty in our results directly originates from the stochasticity of LLM generation. Hardware All experiments are conducted on server equipped with an Intel Xeon E5-2698 v4 CPU (40 threads), 252 GB of RAM, and four NVIDIA Tesla V100 GPUs with 32 GB of memory each. 24 C.2 E-Commerce Transaction Synthesis In this subsection, we present additional visualizations comparing the distributions of real and synthetic data. Synthetic Distribution Visualization In Figure 6 and Figure 7, we visualize the full marginal distributions for all individual variables as well as selected joint distributions over correlated variable pairs, comparing synthetic data generated by LLMSYNTHOR and GReaT. These plots complement the quantitative metrics in the main paper by illustrating how well each method captures complex interactions and category-conditioned patterns. LLMSYNTHOR shows strong visual alignment with the real data across both marginal and conditional views, especially in structured relationships such as payment method by location tier and product category by age and gender. GReaT captures some trends but tends to over-smooth (Price by Product Category) or under-represent distributional variation (Price). These comparisons highlight the qualitative fidelity of LLMSYNTHOR in preserving both local and global statistical structure. Figure 6: Visualization of the synthetic distribution generated by LLMSYNTHOR. Figure 7: Visualization of the synthetic distribution generated by GReaT. 25 Convergence Analysis. Figure 8 presents empirical convergence curves tracking distributional discrepancy metrics across 100 synthesis iterations. We evaluate both marginal and joint distributions using suite of distances, including total variation distance (TVD), Wasserstein distance, energy distance, and Hellinger distance. All metrics consistently decrease over time, indicating that LLMSYNTHOR iteratively reduces structural divergence between real and synthetic data. This supports the theoretical result in Theorem 1, confirming that LLMSYNTHOR achieves local statistical alignment through guided refinement. Joint patterns such as product price by category and payment method by location tier also converge, demonstrating that high-order dependencies are faithfully preserved. Figure 8: Convergence curve showing distributional discrepancy metrics over iterations during synthesis. The plot depicts the overall mean distributional distance between real and synthetic data as function of synthesis iterations. To compute this curve, we aggregate several distributional metrics, including total variation distance (TVD), Wasserstein distance, energy distance, maximum mean discrepancy (MMD), Hellinger distance, Jensen-Shannon divergence (JSD), and Kullback-Leibler (KL) divergence, across both marginal and joint variables. For each iteration, the mean of all selected metrics over all variables is reported. The solid line indicates this average, while the shaded regions represent uncertainty intervals: 1, 2, and 3 standard deviations, along with the min-max range across repeated runs. The sharp initial decrease and rapid plateau show that most statistical alignment is achieved within the first 50 iterations, with diminishing returns beyond 200. This highlights the efficiency and stability of the iterative synthesis process and provides practical guidance for setting the number of iterations. Figure 9: Mean distributional distance across synthesis iterations. Efficiency Analysis. We compare the runtime efficiency of our method (LLMSYNTHOR) against GReaT, strong LLM-based baseline. Unlike GReaT, which requires training, LLMSYNTHOR performs zero-shot synthesis via LLM prompting. For this evaluation, we generate 2,000 samples in both methods. GReaT is run using batch size of 124, the maximum size recommended in its original paper to ensure performance, and trained over 200 epochs, which we find sufficient for convergence. In contrast, LLMSYNTHOR is evaluated using two backends: GPT-4.1-nano via API and Qwen2.5-7B-Instruct on single V100 GPU with 32GB memory. Our method achieves comparable or better runtime without any training overhead. The API version is notably faster due to optimized inference pipelines. In practice, further speedups are possible by increasing the number of samples generated per iteration or reducing the number of refinement steps. As LLMs continue to improve in both intelligence and inference speed, LLMSYNTHOR will become even more efficient and widely adaptable for practical data synthesis tasks. Figure 10: Efficiency comparison between GReaT and LLMSYNTHOR. 26 Ablation: LLM Backbone Robustness. Figure 11 shows the output of LLMSYNTHOR when using Qwen2.5-7B-Instruct as the LLM backbone instead of GPT-4.1. Despite architectural and training differences, the resulting synthetic distributions remain well-aligned with the real data across both marginal and joint views. This demonstrates the frameworks robustness to LLM choice, as long as the model possesses sufficient language understanding and compositional capacity. Figure 11: Ablation study: performance of LLMSYNTHOR using Qwen2.5-7B-Instruct as the LLM backbone. Ablation: Effect of Joint Structure Grounding. Figure 12 presents the outcome of disabling joint structure grounding and guiding generation using only marginal statistics. While marginal distributions remain accurate, joint relationships (e.g., agegenderproduct interactions) degrade noticeably. This highlights the importance of explicitly modeling structural dependencies for capturing realistic correlations, validating the need for our copula-based joint grounding strategy. Figure 12: Ablation study: performance of LLMSYNTHOR when guided only by marginal statistics, without joint structure grounding. https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 27 C.3 Population Synthesis Figure 13: Visualization of the synthetic distribution generated by NVI. Figure 14: Visualization of the synthetic distribution generated by LLMSYNTHOR. Distributional Comparison in Population Synthesis. Figure 13 and 14 compare the marginal distributions of real data against synthetic data generated by NVI and LLMSYNTHOR, respectively. Both methods maintain reasonable alignment on household-level and individual-level variables. However, LLMSYNTHOR more closely matches real distributions, particularly on skewed variables such as income and age, as well as on categorical distributions like employment and race. This suggests that the iterative structure-guided mechanism in LLMSYNTHOR improves fidelity even in challenging, high-variance settings. 28 Real CP Synth. HMM RE Synth. RE Synth. NVI LLMSYNTHOR RE Synth. median_income_black_households 58400 103128 44728 0.77 89355 0.53 79100 20700 0.35 54334.5 4065. prop_multigenerational_racial prop_elderly_poverty prop_female_headed_poverty prop_high_edu_unemployed prop_dual_earner_couples 0. 0.05 0.07 0.05 0.12 0.18 0. 0.06 0.15 0.01 0.17 0.07 0. 0.09 0.11 avg_income_per_person_dual_earner 77984.05 38113.62 39870. prop_families_with_children prop_multigenerational prop_high_dependency_ratio median_avg_age age_gap_owners_vs_renters prop_child_no_vehicle prop_elderly_no_vehicle prop_high_vehicle_high_income prop_no_vehicle_low_income 0.32 0.03 0. 48.67 14.92 0.01 0.01 0.1 0. 0.4 0.22 0.23 44 0.29 0. 0.05 0.08 0.02 0.08 0.19 0. 4.67 14.64 0.02 0.03 0.02 0. 10.81 1.56 0.16 1.67 0.88 0. 0.25 6.53 0.25 0.1 0.98 2. 3.17 0.17 0.5 0.18 0.15 0. 0.04 0.09 0.16 0.11 0.03 0. 0.03 39476.37 38507.68 0.41 0.17 0. 44.67 -0.61 0.01 0.02 0.08 0. 0.09 0.14 0.07 4 15.53 0.01 0.02 0.01 10.47 2.35 0. 0.18 0.27 0.49 0.29 4.92 0. 0.08 1.04 0.22 0.46 0.22 0. 0.18 0.12 0.04 0.04 0.16 0. 0.08 0.03 0.02 0.04 64007.69 13976. 0.42 0.2 0.3 43 0.11 0. 0.01 5.67 -0.11 15.03 0.01 0. 0.09 0.01 0 0 0.01 0. 10.63 1.68 0.43 0.34 0.3 0. 0.34 5.82 0.04 0.12 1.01 0. 0.16 0.11 0.51 RE 0.07 0. 0.3 0.44 0.15 0.43 0.01 0. 0.1 0.05 0.17 0.01 0.01 0. 0.01 0.05 79218.42 1234.37 0.02 0. 0.03 0.38 47 20.65 0.01 0. 0.08 0.02 0.05 0 0.07 1. 5.72 0 0 0.02 0 0. 0.02 0.21 0.03 0.38 0.5 0. 0.24 0.29 Table 5: Full utility results showing synthetic values (Synth.), absolute errors (), and relative errors. Full Utility Results. Table 5 reports detailed results for all 16 utility queries used in the population synthesis evaluation, including the real values, synthetic estimates, absolute error (), and relative error (RE). These queries span equity, vulnerability, employment, demographics, and mobility-related indicators. LLMSYNTHOR consistently achieves the lowest relative error across most metrics, confirming its ability to preserve both marginal statistics and complex structural dependencies. Notably, it significantly outperforms all baselines on sensitive or high-variance indicators such as median_income_black_households, avg_income_per_person_dual_earner, and prop_multigenerational. These results reinforce the models utility in supporting policyrelevant analysis with privacy-preserving synthetic data. 29 C.4 Mobility Synthesis Additional Mobility Visualizations. Figure 15 shows detailed comparison of spatialtemporal flow intensity between real and synthetic data across seven time intervals throughout the day. Each map captures the aggregate origin and destination activity within the Tokyo metropolitan area during specific time window. The synthetic data successfully preserves major spatial patterns such as morning and evening commute flows, while also capturing temporal variations in trip density. This highlights the models ability to maintain realistic spatiotemporal dynamics. Figure 15: Spatialtemporal flow intensity maps for real and synthetic mobility data across seven time intervals: 06, 69, 912, 1214, 1417, 1720, and 2024. Each map shows trip density aggregated by region to visualize commuting and activity patterns over the day. Figure 16 presents additional joint distribution visualizations across key mobility attributes. The left plot illustrates the correlation between transport mode and travel distance, showing that synthetic samples preserve realistic distance-dependent mode preferences (e.g., longer trips by car). The middle and right plots show marginal distributions for transport modes and time intervals, further confirming strong alignment between real and synthetic mobility behavior. Together, these results demonstrate that LLMSYNTHOR can faithfully reproduce both spatial structure and behavioral signals critical for urban simulation and mobility planning. Figure 16: Distribution comparisons for mobility variables."
        },
        {
            "title": "D Proofs",
            "content": "D.1 Proof of Theorem 1 To facilitate the proof, we use slightly different notation here than in the main text. Specifically, we make clear distinction between the batch of new samples at each iteration and the cumulative synthetic pool. The following symbols are local to this section. Notation. Let be the set of variables and Dreal = {xi}n let ω collect all algorithmic randomness (e.g., the random seed). Denote i=1 the observed data. At each iteration t, B(t)(ω) : batch of new samples at round t, D(t) synth(ω) := (cid:91) k=0 B(k)(ω) the cumulative synthetic pool up to and including iteration t, and set mt = D(t) For every structure unit define synth, bt = B(t+1). (ω) := Q(cid:0)su(D(t) δ(t) synth(ω)), su(Dreal)(cid:1), and for the new batch at round + 1 set u,batch(ω) := Q(cid:0)su(B(t+1)(ω)), su(Dreal)(cid:1). δ(t+1) where su() is the summary operator in Section 3.3 and Q(, ) non-negative divergence. All expectations are over ω with Dreal held fixed. Assumption 1 (Identifiability). If, for some dataset , we have Q(cid:0)s(X ), s(Dreal)(cid:1) = 0, then follows the true law (x ϕ). Why reasonable? We choose s() to include exactly the structural statistics that LLMSYNTHOR aims to preserve. Zero divergence therefore, implies perfect structural alignment, which is equivalent (within our target statistics) to match the ground-truth distribution. Assumption 2 (Exploration Coverage). For any density level ρ > 0 let Sρ = {x : (x ϕ) ρ}. For every ρ > 0 there exist finite and ω with Sρ supp(cid:0)D(t) Why reasonable? LLM-based proposal generation in LLMSYNTHOR uses temperature sampling and diversified prompts; empirically, every high-probability region of the real distribution is sampled with non-zero probability, and once sampled it remains in the cumulative pool. synth(ω)(cid:1). Assumption 3-a (Targeted Batch Improvement). There exist τ > 0, > 0, β (0, 1) such that for every and iteration t, Eω[δ(t+1) u,batch] (cid:40)Eω[δ(t) ] , Eω[δ(t) ] τ, β Eω[δ(t) ], otherwise. Assumption 3-b (Non-Vanishing Batch Weight). Let wt := bt/(mt + bt). We assume := inf wt > 0. Why reasonable? LLMSYNTHOR always re-prompts the LLM with the highest-discrepancy units, yielding batches that on average reduce the discrepancy by at least when it exceeds τ . With fixed (or proportional) batch size, the weight wt remains bounded below by positive constant c. Proof. Fix structure unit with δ(t0) τ . Claim 1 (Soundness). those of Dreal (Assump. 1). Hence δ(t) is faithful alignment metric. If δ(t) (ω) = 0, then the summary statistics of in D(t) synth(ω) coincide with Claim 2 (Eventual Coverage). By Assump. 2, every configuration of positive probability appears in the cumulative pool at finite iteration, so no structural aspect is permanently missed. 31 Claim 3 (Expected Descent of the Cumulative Pool). By linear decomposition, δ(t+1) = mt mt + bt δ(t) + bt mt + bt δ(t+1) u,batch. Taking expectation and applying Assumption 3-a (batch improvement) together with the weight bound in Assumption 3-b wt = bt gives mt+bt Eω[δ(t+1) ] Eω[δ(t) β Eω[δ(t) ], ] c, Eω[δ(t) ] τ, otherwise. Thus the expected sequence is monotone and, whenever above τ , decreases by at least c. Let d0 = Eω[δ(t0) ] and define T1 = (cid:108) d0 τ (cid:109) , T2 = (cid:108) log(ε/τ ) log β (cid:109) . Then Eω[δ(t0+T1) ] τ and Eω[δ(t0+T1+T2) ] ε. Claim 4 (Global Bound). Set = t0 + T1 + T2. For all t, Eω[δ(t) arbitrary among units with initial discrepancy τ , the theorem follows. ] ε. Because was"
        },
        {
            "title": "E Prompts",
            "content": "This section provides the exact prompts used in our framework to guide the LLM during proposal generation and structural inference. The first prompt, pproposal, instructs the LLM to generate sampleable distributional templates that align with target summary statistics. The second prompt, pcopula, is used to elicit joint dependency structures among variables by treating the LLM as nonparametric copula simulator. Both prompts are designed to be type-agnostic and modular, enabling effective alignment across heterogeneous datasets. LLM Proposal Sampling pproposal: LLM Proposal Sampling ## Output Format: ```json {{ \"n_proposals\": n, \"proposal1\": {{ \"reason\": \"...\" , \"proposal\": \"...\", \"num\": }} \"proposal2\": {{ \"reason\": \"...\", \"proposal\": \"...\", \"num\": n2 }}, \"...\" }} ``` ## Information: **Joint Guidance:** `{joint_guide}` **Marginal Guidance:** `{marginal_guide}` **Variables:** `{data_desc}` --- ## Rules: Create no more than {n_proposals} proposals totaling {n_samples} samples: 1. Each proposal must follow Joint and Marginal Guidance, do not improvise beyond the provided Guidance. 2. The reason for each proposal should explain the realistic meaning of this proposal and how it follows the provided Guidance by referencing frequencies one by one. 3. 'min', 'max', 'category' must use the actual values mentioned in **Variables**: Categorical variables must be single valid candidate string (case-sensitive), and numerical variables must be list of two integer or float numbers (e.g., [3.0, 5.1]). 4. Do not include multiple ranges or categories per variable within single proposal. 5. Structure of generated data must match the **Output Format**, and all variables must be included. 6. If variable value has high frequency, that value should be selected in multiple proposals. 7. Most generated samples should, as much as possible, prioritize satisfying components that are common across the Guidances, and the num for each proposal should be determined based on the frequencies specified in the Guidance. 8. Only list generation proposals; do not generate data or add extra text. 9. Do not return an empty JSON. Do not use escaped characters such as rn, t, or \". Do not include any comments, markdown formatting, or explanatory text. --- Now return pure, valid, and non-empty JSON in English that can be directly parsed by json.loads() in Python In our implementation, each proposal specifies well-defined distribution for every variable. For discrete variables, the proposal directly assigns single valid category value. For continuous variables, the proposal gives an explicit value range (for example, [3.0, 5.1]), from which values can be sampled uniformly or with another simple scheme (e.g., using numpy in Python). The num field in each proposal specifies how many samples to generate from that proposal, enabling the LLM to allocate sample counts across proposals based on the frequencies and guidance provided. This mechanism lets the LLM actively plan for diversity and statistical alignment among the generated samples by balancing the number and distribution of proposals. Importantly, to encourage transparent and high33 quality reasoning, we employ chain-of-thought [49] prompting: the LLM is instructed to explicitly explain the rationale behind each proposal, referencing frequency statistics and the provided guidance step by step. This makes the proposal process more interpretable and reliably aligned with the specified constraints. Importantly, this proposal format is just one practical instantiation of LLM Proposal Sampling. The framework is highly extensible. proposals distribution could be defined not just as value ranges or categories, but as executable code, tool calls, or pointers to external generators (such as diffusion models with ControlNet for images, or other LLM-based agents for specialized content). This flexibility allows LLM Proposal Sampling to serve as universal, high-level distributional controller, guiding external generators or hybrid pipelines toward statistically faithful and scenario-aligned synthetic data, regardless of data type or target domain. LLM as Copula Simulator for Dependency Inference pcopula: LLM as Copula Simulator for Dependency Inference ## Information **Variables:** ```{data_desc}``` ## Output Format: {{ \"1\": [\"var1\", \"var2\", ...], \"2\": [\"var1\", \"var2\", ...], ... }} ### Example Output JSON: {{ \"1\": [\"Temperature\", \"Humidity\"], \"2\": [\"Humidity\", \"WeatherCondition\"], }} --- Your task is to extract **at most {n_joints}** correlated variable groups based on the **Variables** summaries and present them in JSON format. 1. Ensure each group contains two or more variables. 2. Format the correlated variable groups according to the **Output Format**. 3. Output must be valid JSON."
        }
    ],
    "affiliations": [
        "McGill University"
    ]
}