{
    "paper_title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "authors": [
        "Kai-Po Chang",
        "Wei-Yuan Cheng",
        "Chi-Pin Huang",
        "Fu-En Yang",
        "Yu-Chiang Frank Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks."
        },
        {
            "title": "Start",
            "content": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment Kai-Po Chang1,, Wei-Yuan Cheng1, Chi-Pin Huang1, Fu-En Yang2, and Yu-Chiang Frank Wang1,2, 1 Graduate Institute of Communication Engineering, National Taiwan University 2 NVIDIA f11942093@ntu.edu.tw, frankwang@nvidia.com 5 2 0 2 4 ] . [ 1 6 5 3 4 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains challenging and unsolved task. To tackle this challenge, we propose Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks. 1. Introduction Recently, Large Language Models (LLMs) [6, 36] have demonstrated extensive reasoning capability in NLP tasks and exhibit great potential to be leveraged in visionlanguage models to handle multimodal understanding tasks, such as visual captioning [30, 35] and question answering [8, 38]. However, these multimodal LLMs (MLLMs) are prone to generate descriptions that are not grounded in the given visual input, causing severe hallucination issues [5, 18, 31, 37]. It would pose substantial risks in real-world applications that require high-standard robustFigure 1. Compared to existing MLLMs suffering from object and action hallucinations, our SANTA enhances faithfulness in describing both visual objects and temporal actions. ness and faithfulness, like healthcare and autonomous driving. As result, mitigating hallucinations for multimodal understanding draws significant attention from both academics and industry. To deal with this challenge, several works [4, 9, 12, 15, 22, 26, 29] have explored mitigating hallucinations in image understanding tasks, where the output captions might contain hallucinated objects that do not exist in the given inputs. For example, several works [9, 15, 29] enforce that the visual inputs can be sufficiently considered during the language decoding process. However, these methods induce extra computational overhead, substantially slowing down the inference process. On the other hand, HACL [12] and HALVA [26] align the visual tokens with textual ones via 1 contrastive learning to encourage the visual information to be interpretable by LLMs. While the object hallucinations are alleviated, the above methods only focus on the hallucinated objects that appear in static image but cannot handle the possible hallucinations of visual objects and temporal actions that commonly occur in video domains. With the goal of mitigating hallucinations of objects and actions, recent approaches [5, 20, 37] have designed learning strategies to capture spatial-temporal dynamics in video data. Vista-LLaMA [20] introduces an attention mechanism that equalizes the influence of visual and textual tokens on language outputs. Nevertheless, Vista-LLaMA [20] treats video data in frame-by-frame manner, leading to the difficulties of MLLMs in capturing the accurate visual appearance of objects and temporal dynamics of actions. Therefore, enabling the output descriptions of MLLMs to be sufficiently faithful in regard to both visual objects and temporal actions remains challenging yet unsolved problem. In this paper, we propose Self-Augmented Contrastive Alignment (SANTA) framework that mitigates object and action hallucinations occurred in video understanding tasks, as depicted in Fig. 1. To enforce the output descriptions to be exempted from the spurious correlation caused by language prior while adequately grounding into visual content, SANTA employs hallucinative self-augmentation scheme to enhance tracklet-phrase contrastive alignment. More specifically, the former observes the phrase-level spurious correlation of the MLLM, and then augments the original captions to serve as the contrasted negatives. The latter first represents visual objects and temporal actions by deriving visual tokens from region-level tracklets and object relations, respectively. With such spatial-temporal visual features obtained, we are able to bridge the modality gap by aligning object and action features with the corresponding language tokens while contrasting the hallucinative ones. By iteratively exploiting the above augmenting and training stages, our SANTA enables the alleviation of visual object and temporal action hallucinations in pre-trained MLLMs. We now summarize the contributions of this work below: We present SANTA, which jointly mitigates object and action hallucinations by suppressing hallucinative correlation from language prior while enhancing the emphasis of tracklet-level visual facts. SANTA employs self-augmentation hallucinative scheme to identify potential hallucinations in MLLMs and construct contrasted negatives to simulate this underlying hallucinative correlation. We develop tracklet-phrase contrastive alignment to match the regional objects and relation-aware actions with their associated visual and temporal phrases while expelling the hallucinative negatives. 2. Related Works 2.1. Mitigating Object Hallucination in MLLMs In recent years, multimodal LLMs (MLLMs) have demonstrated significant achievement in vision-language tasks (e.g., image captioning [28] and visual question answering [21]). However, MLLMs tend to generate descriptions based on LLMs language prior instead of grounding the facts in the visual condition, causing hallucinated objects that do not exist in the given images. For example, Visual Contrastive Decoding (VCD) [15] reduces hallucination by subtracting language-prior probabilities from the generated token distribution. DeCo [29] calibrates token selection by weighting logits from the final and an anchor layer that exhibits the highest confidence in token generation. While promising, the above methods incur additional computational burden during the language decoding process, resulting in large increase in inference time. On the other hand, another line of works [3, 4, 12, 22, 26, 41] aims to align visual and textual tokens, so that LLMs could sufficiently interpret and ground the visual facts. For instance, HACL [12] aligns the global image and caption feature to reduce the modality gap. Building upon this, HALVA [26] proposes an alignment-based training objective that aligns vision and corresponding textual representations at the phrase level. However, these approaches can only handle the object hallucinations in static image, limiting the applicability to alleviate the hallucinated temporal actions that are commonly encountered for video data. 2.2. Mitigating Action Hallucination in MLLMs Action hallucination occurs when models describe actions absent from the video, while most existing works instead study relationship hallucination, which focuses on static relationships between objects in single image. For example, Tri-HE [33] evaluates relations as edges between object nodes in image scene graphs, while R-Bench [34] assesses relationship hallucination with image-level and instancelevel questions derived from static image captions. These benchmarks focus on evaluation for static images and cannot address the multi-frame reasoning needed to detect action hallucinations. Mitigating action hallucination, in contrast, requires learning to track the evolving states of dynamic objects and to understand cross-frame interactions between multiple dynamic entities in order to infer motion, such as distinguishing person going up vs. down stairs. Building on this insight, recent works attempt to encourage captions to be grounded in the spatial-temporal visual content. For example, Vista-LLaMA [20] introduces model architecture that ensures consistent influence of visual tokens on each generated language token. Vriptor [37] builds on previous findings that detailed captions improve vision-language alignment [16, 19] by incorporating voice2 Figure 2. Overview of SANTA. We employ (a) Mitigating Video-Level Hallucination by applying Hallucinative Self-Augmentation to identify the highly potential hallucinated tokens in MLLM θM that deviate from ground truth words (e.g., synonyms or hypernyms) and then perform video-caption contrastive alignment. SANTA then (b) Mitigating Object nad Action-Level Hallucinations by Tracklet-Phrase Contrastive Alignment to align object and action tracklets with visual and temporal phrases while contrasting hallucinative negatives. over and audio transcriptions. Nevertheless, processing videos frame by frame cannot exploit the temporal dynamics needed for action faithfulness. To tackle this challenge, we employ SANTA framework that jointly aligns object and action tracklets with the corresponding textual tokens, enabling outputs with stronger object and action faithfulness. 3. Method 3.1. Problem Formulation Given an input video v, the goal of the MLLM θM is to generate textual caption = {wi}c i=1 with length c, where each token wi may describe an object, an action, or other contextual information. hallucination occurs when predicted token wi refers to visual concept that is not present in the video v. Formally, let Ov and Av denote the sets of objects and actions that actually appear in v, respectively. We define an object hallucination as the case where wi is an object noun but wi / Ov, and an action hallucination as the case where wi is an action verb but wi / Av. To mitigate object and action hallucinations, we propose SANTA, which jointly suppresses spurious correlations from language priors and enforces proper video grounding. Our framework leverages contrastive alignment to identify spurious correlations and construct hallucinative negative samples in bootstrapped manner (Sec. 3.2). Furthermore, instead of naively bridging the video-language gap at the frame level, we introduce novel tracklet-phrase contrastive learning that aligns tracklet-level object and action features with their corresponding object and action phrases in the caption (Sec. 3.3). 3.2. Mitigating Video-Level Hallucination To enforce the LLMs to properly focus on the visual facts rather than the superior correlation inherited in the language prior, we present contrastive learning framework that aligns the visual features to the token space of LLMs while exempting the potential hallucination descriptions. Instead of directly prompting off-the-shelf LLMs (e.g., GPT-4 [1]) to produce hallucinative captions as hard negative samples, we propose Hallucinative Self-Augmentation strategy to construct the contracted negatives that reflect the inherent spurious correlations of the MLLM θM . Hallucinative Self-Augmentation. As described in the Fig. 2(a), given video-caption pair (v, c), we reveal the potential hallucination of the MLLM θM by identifying the text tokens with the highest likelihood that do not belong to the set of ground truth tokens. More specifically, we generate hallucinative caption by constraining the frozen MLLM θM to select the token with the highest-probability 3 (at each decoding step) that falls outside the ground-truth tokens. The set of ground-truth tokens is constructed from the object nouns and action verbs in the ground-truth caption, together with their synonyms and hypernyms (defined by WordNet [7]). As result, the hallucinative captions Ch = {ci}Ch i=1 could be generated as follows: Ch (cid:89) t=1 sup θM (ct v, c<t), where (1) sup θM (ct v, c<t) = I(ct) PθM (ct v, c<t) /Ω PθM (c v, c<t) (cid:80) , where ct is the generated token at step , and I() is an indicator function defined such that I(ct) = 0 if ct (O A) and I(ct) = 1 otherwise. and are the set of groundtruth objects and action tokens, including their respective synonyms and hypernyms. Video-Caption Alignment with Hallucinative Negatives. With the hallucinative captions obtained, we aim at matching the video features to the LLMs token space and effectively suppressing the possible object and action hallucinations caused by language correlations. Concisely, we impose video-caption contrastive learning to attract the visual feature of input video with the textual token of the paired caption while pushing away the negative samples (V and ) that contain hallucinative caption as hard negatives and randomly sampled videos/captions. Formally, Lvideo is defined as follows: Lvideo = 1 2 (cid:2)Lt2v(c, v+, ) + Lv2t(v, c+, )(cid:3) , (2) where Lt2v(c, v+, ) = log (cid:34) ϕ(c, v+) ϕ(c, v+) + (cid:80)B i=1 ϕ(c, ) Lv2t(v, c+, ) = log (cid:34) ϕ(v, c+) ϕ(v, c+) + (cid:80)B i=1 ϕ(v, ) (cid:35) (cid:35) , , where ϕ(, ) = esim(,)/τ denotes the cosine similarity between pooled vision and text features. τ is temperature parameter. = {V 1 , ..., } are the sets of negative in-batch video and caption samples. is the input batch size. } and = {C 1 , ..., 3.3. Mitigating Object and Action-Level Hallucinations with Hallucinative Negatives While video-caption alignment bridges the gap between visual and textual modalities, it remains challenging to precisely locate the object regions and temporal actions in dynamic video. As illustrated in Fig. 2(b), we introduce novel Tracklet-Phrase Contrastive Learning paradigm to enforce the MLLM to accurately interpret the appeared objects and actions for generating faithful captions. Enhancing Object-Level Faithfulness. To ensure object faithfulness in the output descriptions of MLLM, we aim to associate object tracklets extracted from the video with their corresponding object phrases derived from the groundtruth caption c. More specifically, we derive the feature of regional object tracklets by employing existing visual grounding methods (e.g., Grounding-SAM2 [24, 25]), and obtain the phrase-level textual tokens by parsing with text-only LLMs (e.g., GPT-4 [1]). Subsequently, we define the regional-level object alignment loss Lobj as: 1 Lobj = (cid:2)Lt2v(T o, o, o) + Lv2t(P o, o, o )(cid:3) , (3) where and are the sets of negative in-batch object phrases and tracklets. Note that the object phrases parsed from the hallucinated caption from Eq. 1 serve as hard negatives to augment the region-level object alignment for enhancing object faithfulness. Enhancing Action-Level Faithfulness. To extract the temporal actions from dynamic video, we propose to derive relation-guided action features from visual object tracklets. To achieve this, we design perceiver-based [11, 17, 32] action squeezer θA to observe the interaction between object tracklets and extract the action tracklet features associated to the action verb in the ground truth caption. More concisely, θA employs set of learnable queries = {qk}NQ k=1 to derive from its corresponding set of related object tracklet features {T }N i=1 specified by the relationships in the ground-truth captions. Through the crossattention mechanism in θA, queries are able to interact and capture the action information lies in the involved object tracklet features {T i=1. Then, the action could be selected from the output of θA based on the highest similarity to the corresponding action phrase feature a. The above process is formulated as follows, }N = θM (qk ), where = argmax k{1,...,NQ} (cid:110) (cid:16) θA(qk, {T i }N i=1), a(cid:17)(cid:111) (4) . sim As result, the relation-guided action alignment Lact is computed as: 1 2 Lact = (cid:2)Lt2v(T a, a, a) + Lv2t(P a, a, a )(cid:3) , (5) where and are the sets of negative in-batch action phrases and tracklets. With our proposed tracklet-phrase contrastive alignment, the spatial-temporal visual tracklet features are effectively Algorithm 1 Training of SANTA Model: Targeted MLLM θM , Action-squeezer θA, Learnable queries Input: Given batched video-caption pairs = {vj , cj }B j=1, each caption cj contains multiple action instances. Each action instance is associated with multiple object instance {oi}N i=1, where a, oi cj , and the object tracklet features for each oi are denoted as . 1: for = 1, . . . , do 2: 3: 4: 5: 6: /* Hallucinative Self-Augmentation */ Freeze θM ; Ch Generate the hallucinative caption by Eq. 1; /* Contrastive Alignment */ Lvideo Compute video-caption alignment with and Ch by Eq. 2; Lobj Compute Region-Level Object Alignment with object tracklet-phrase features {T Get action phrase features with θM (a); Get action tracklet features with θA(Q, {T Lact Compute Relation-Guided Action Alignment with action tracklet-phrase features {T a, a} and Ch by Eq. 5; Update θM and θA by Ltotal in Eq. 6 Get object phrase features with θM (oi); } and Ch by Eq. 3; 7: 8: 9: 10: 11: , }N i=1); 12: 13: end for captured and aligned with their corresponding language phrase tokens. As result, the captions generated by MLLMs exhibit sufficient object and action faithfulness. 3.4. Summary of SANTA In Algorithm 1, we provide an overview of our training process for our proposed SANTA. SANTA employs hallucinative self-augmentation to generate the hallucinative captions serving as contrastive negatives during training. Subsequently, by jointly optimizing the proposed trackletphrase contrastive alignment Lobj + Lact along with Lvideo + Lg from previous work [12] , SANTA further achieves the enhanced object and action faithfulness. The overall training objective is as follows: Ltotal = α(Lobj + Lact) + βLvideo + Lg. (6) Here, Lg is the cross-entropy loss used for the next token prediction to ensure the model retains its original capability of generating fluent outputs. α and β are hyperparameters to control the relative contribution of the tracklet-phrase and video-caption contrastive alignments to Ltotal. 4. Experiments 4.1. Experimental Setup Training Data. We use MiraData [13] as our training data, which contains high-quality and detailed captions for each video. Our training data consists of 42715 videos with an average of 72.1 seconds per video. Each video is annotated with three types of captions: overall captions, main object captions, and background captions. The average cap5 tion length across all types is 318 words. More details are in the supplementary material. Evaluation Benchmarks. To assess both the object and action hallucination in the textual description generated by multimodal large language models (MLLMs), we evaluate performance on video captioning datasets (i.e., MiraData9k [13] and FactVC [18]) and multiple-choice video question answering datasets (i.e., VidHal [5]). Note that FactVC is composed of curated samples from ActivityNet [14] and YouCook2 [40], and that the MiraData-9k and the MiraData used during training do not overlap. Furthermore, we assess video description capability on Dream1k [30] following hallucination mitigation. FactVCF1 Hallucination Evaluation Metrics. To evaluate video hallucinations, existing metrics such as FactVC [18] and [18] utilizes HalFscore [4] are adopted. CLIP [23] embeddings to computes an F1 score to assess hallucination in generated captions, while HalFscore [4] computes an F1 score based on exact string matching. The latter comprises two sub-scores: hallucination-score (precision, denoted as Hal) and coverage-score (recall, denoted as Cov), reflecting the extent of hallucination and the comprehensiveness of detail coverage, respectively, with subscripts Obj and Act denoting objectand actionlevel evaluations (e.g., F1obj and F1act). However, HalFscore [4] fails to recognize synonyms and potentially misjudges hallucinations, while FactVCF1 [18] cannot be directly applied to MiraData-9k [13], due to the token length limitation of CLIP [23] (i.e., 78 tokens). To address these limitations, we extend HalFscore [4] by modifying its exact string matching to cosine-similarity for synonym awareness in Halobj and Halact while by incorporating tf-idf [27] weighting to better assess the importance of tokens in Covobj and Covact. We term this improved metric as weighted-HalFscore, comprising two sub-scores, formally defined as follows: Hal = 1 1 (cid:88) tiP max tj {cos(ti, tj)} , (7) Cov = (cid:80) ti(PG) tf-idf(ti) (cid:80) tj tf-idf(tj) (8) where and are the sets of predicted and ground-truth object (or action) tokens, and tf-idf(ti) is the tf-idf weight for token ti. The overall F1 score is then computed as: F1 = 2 (1 Hal) Cov (1 Hal) + Cov . (9) Implementation Details. We adopt the LLaVAas our base Video7B [39] and Qwen2.5-VL7B [2], models. Training is conducted with learning rate of 6e-5, batch size of 64, and 64 frames uniformly sampled from Table 1. Evaluation on both object and action hallucinations with existing methods on MiraData-9k [13] across three types of video captions: overall content, main object, and background. Bold and underline indicate the best and second best results, respectively. Method LLaVA-Video [39] +SFT [39] +DeCo [29] +HALVA [26] +HACL [12] +SANTA (Ours) LLaVA-Video [39] +SFT [39] +DeCo [29] +HALVA [26] +HACL [12] +SANTA (Ours) LLaVA-Video [39] +SFT [39] +DeCo [29] +HALVA [26] +HACL [12] +SANTA (Ours) HalFscore [4] weighted-HalFscore Halobj Halact Covobj Covact F1obj F1act Halobj Halact Covobj Covact F1obj F1act 79.4 72.0 74.2 78.9 70.0 69.6 83.1 79.5 80.9 84.9 79.4 77.7 78.7 63.3 77.1 81.6 60.3 62.2 76.5 67.6 73.4 75.0 65.2 64. 75.4 68.1 73.5 77.2 67.9 65.7 85.4 70.1 86.2 84.5 65.7 68.3 52.1 49.7 46.6 50.5 53.7 57.2 43.1 49.7 37.7 46.7 52.3 53.7 42.3 43.0 40.0 50.0 43.5 47.2 Overall Content Captioning 33.0 23.3 27.3 29.4 29.9 32. 27.5 27.1 26.9 27.0 32.2 34.0 29.5 35.8 33.2 29.8 38.4 39.7 49.0 44.9 48.4 48.7 42.7 40.9 Main Object Captioning 23.8 23.3 20.7 24.0 26.3 27.1 24.1 26.9 23.2 23.4 28.9 30.3 24.3 29.0 25.3 22.8 29.6 31. Background Captioning 19.4 18.2 16.2 22.9 18.8 22.8 22.3 22.6 14.9 18.5 24.3 25.8 38.9 39.6 29.0 26.9 41.5 42.3 53.2 44.2 53.6 55.0 45.6 43.3 65.1 63.4 65.2 67.0 60.4 60.2 58.4 54.6 56.6 58.0 51.6 50. 58.1 55.4 57.2 60.1 55.8 54.6 73.0 70.7 73.4 74.4 68.5 68.4 54.6 54.7 48.1 53.5 60.8 64.1 46.1 54.8 39.5 49.9 60.8 61.7 44.0 38.2 41.3 51.5 44.7 48.6 32.9 22.9 27.6 28.5 28.6 31. 24.3 21.4 20.9 23.5 25.7 26.6 19.0 14.3 15.9 22.8 18.4 22.3 52.7 54.9 49.8 52.4 59.0 61.5 46.4 55.2 42.7 47.4 57.5 59.1 38.9 37.4 37.8 40.2 42.0 43.8 36.7 30.4 33.8 34.0 35.9 38. 30.8 28.9 28.1 29.5 32.5 33.5 22.3 19.2 19.9 24.2 23.2 26.1 Table 2. Quantitative comparisons with hallucination mitigation methods on video captioning using FactVC [18]. Table 3. Quantitative evaluation of both object and action hallucinations on video question answering using VidHal [5]."
        },
        {
            "title": "Method",
            "content": "FactVCprec. FactVCrec. FactVCF"
        },
        {
            "title": "Method",
            "content": "Accobj Accact Avg. Acc LLaVA-Video [39] +SFT [39] +DeCo [29] +HALVA [26] +HACL [12] +SANTA (Ours) 36.4 38.2 36.1 38.2 38.9 39.4 39.9 41.7 39.6 41.6 42.4 42.9 37.9 39.7 37.6 39.7 40.5 40.9 LLaVA-Video [39] +SFT [39] +DeCo [29] +HALVA [26] +HACL [12] +SANTA (ours) 84.3 36.3 82.8 79.9 79.9 86.3 83.1 65.6 84.7 80.9 84.2 85.8 83.7 51.0 83.8 80.4 82.1 86.1 each video. All frames are resized to 384 pixels, and the models are trained for 2,000 steps. The action-squeezer is initialized from the perceiver model [32] pre-trained on video dynamics modeling. This module consists of single cross-attention layer followed by two-layer feedforward network, employing NQ = 16 learnable queries, adopted from the configuration of [32]. Hyperparameters α and β are set to 0.25 and 0.5, respectively. 4.2. Quantitative Evaluation In Video Hallucination Evaluation with Captioning. Tab. 1 and Tab. 2, we evaluate SANTA on MiraData-9k [13] and FactVC [18] against prior SOTA methods. As shown in Tab. 1, SANTA achieves the highest F1obj and F1act of Falscore [4] across three captioning subtasks, with average gains of +4.02%/+5.54% over the second-best method, and +3.77%/+7.7% for weighted-Falscore. Tab. 2 further confirms these improvements, w/ SANTA reaching the highest precision (FactVCprec.), recall (FactVCrec.), and F1 (FactVCF1). These consistent results highlight that SANTA effectively minimize hallucinations while maximizing coverage of relevant tokens from the provided videos. Video Hallucination Evaluation with QA. In Tab. 3, SANTA achieves the highest accuracy on both objectand action-level hallucination QA tasks, surpassing all baselines. This shows that beyond caption hallucination mitigation (Tab. 1 and 2), SANTA also provides the most accurate answers for hallucination-focused video QA. 4.3. tSNE Visualization In Fig. 3, we visualize the output visual and language features by the LLaVA-Video [39] under two conditions: with and without SANTA. Each tSNE subfigure uses 250 MiraData-9k [13] pairs: (b) object tracklet-phrase, and (c) action tracklet-phrase. Hallucinated captions are generated via hallucinative self-augmentation (introduced in Eq. 1) before SANTA training. Above the dotted line, clear vision-language gap and substantial over- (a) video-caption, 6 Figure 3. t-SNE visualization of the latent features of (a) video and caption, (b) object tracklets and phrases, and (c) action tracklets and phrases. For the w/o SANTA setting, we directly visualize features from LLaVA-Video [39]. Upon training with SANTA LLaVA-Video [39] improves the alignment between visual-language modalities while exempting from the hallucinative captions. Figure 4. Qualitative comparison of video captions predicted by HACL [12] and SANTA. Note that words highlighted in green indicate action faithfulness, while those in red indicate action hallucination. Similarly, words in blue represent object faithfulness, whereas those in orange denote object hallucination. The examples of (a) and (b) are sampled from MiraData-9k [13] and FactVC [18], respectively. lap between ground-truth and hallucinated language features are observed. After applying SANTA (below the line), the gap narrows and hallucinated language features become more clearly distinguishable from the ground-truth. 4.4. Qualitative Evaluation In Fig.4, we compare captions from SANTA, HACL[12], and the ground truth on MiraData-9k [13] (a) and In (a), SANTA generates an actionFactVC [18] (b). faithful caption closely matching the ground truth, while HACL [12] hallucinates the action occasionally glancing around (red), which is absent in the video. In (b), SANTA accurately captures both object and action (mop, mopping), whereas HACL hallucinates both an object (broom, orange) and an action (continues to sweep, red). 4.5. Analysis Ablation on Different Backbones. Beyond the strong performance of SANTA on LLaVA-Video [39] (in Tab. 1, 2, and 3), Tab. 4 shows that adding SANTA to LLaVAVideo consistently boosts both objectand action-level scores across all three MiraData-9k [13] captioning subtasks, demonstrating its effectiveness even on strong baseline. Moreover, applying SANTA to Qwen2.5-VL [2] yields further notable gains, particularly in action-level and weighted-HalFscore metrics, confirming the robustness and general applicability of our method across different MLLM architectures. Ablation on Key Components. introducing In Tab. 5, region-level object alignment into LLaVA-Video [39] boosts F1obj/F1act of HalFscore [4] by +5.5%/+13.3% 7 Table 4. Ablation of SANTA on different MLLM backbones. We report the averaged F1 score of MiraData-9k [13] (HalFscore [4] and Weighted-HalFscore) to evaluate the effectiveness in mitigating both object and action hallucinations. Method HalFscore [4] weighted-HalFscore Avg. F1Obj Avg. F1Act Avg. F1Obj Avg. F1Act Overall Content Captioning LLaVA-Video [19] +SANTA (Ours) Qwen2.5-VL [2] +SANTA (Ours) 29.5 39.7 44.3 42.1 27.5 34. 34.7 36.7 52.7 61.5 60.9 65.5 Main Object Captioning LLaVA-Video [19] +SANTA (Ours) Qwen2.5-VL [2] +SANTA (Ours) 24.3 31.6 34.4 32.8 24.1 30.3 29.7 32.3 46.4 59.1 55.3 63. Background Captioning LLaVA-Video [19] +SANTA (Ours) Qwen2.5-VL [2] +SANTA (Ours) 38.9 42.3 33.4 41.8 22.3 25. 23.3 27.4 38.9 43.8 45.4 45.9 36.7 38.6 40.7 41.1 30.8 33. 36.8 37.0 22.3 26.1 28.4 29.2 Table 5. Ablation of SANTA on key components. All metrics are used as described in Tab. 4. Lg+Lvideo Lobj Lact Ch HalFscore [4] weighted-HalFscore Avg. F1Obj 34.8 36.7 37.0 37.9 Avg. F1Act 25.5 28.9 29.9 30.0 Avg. F1Obj 51.1 53.3 53.9 54.8 Avg. F1Act 29.3 31.1 32.3 32.7 action (weighted-HalFscore +4.3%/+6.1%) over Lg + Lvideo. gains Adding of complete +0.8%/+3.5% (+1.1%/+3.9%), while SANTA achieves the best results, adding +2.4%/+0.3% (+1.7%/+1.2%). further the alignment bring Video Description Evaluation. To demonstrate the benefit of enhancing object and action faithfulness, we apply SANTA to LLaVA-Video [39] and observe improved video description performance on Dream1k [30]. The overall F1score improves from 32.5 to 32.7, confirming that increasing faithfulness reduces hallucinations and improves descriptive ability. Detailed results for each video type are in the supplementary material. Impact of Hallucinative Self-Augmentation Scheme. In Fig. 5, we present F1obj (a) and F1act (b) for hallucination evaluations across overall content, main object, and background captioning on Miradata-9k [13]. Our hallucinative self-augmentation scheme generates negative samples that better reflect the hallucinations that the MLLM itself is prone to producing. As result, our approach not only surpasses the simple supervised fine-tuning baseline (SFT), Figure 5. Ablation study of hallucinative self-augmentation scheme. We ablate this scheme by replacing it with negatives generated directly by text-only LLM (i.e., GPT-4 [1]), following the prompting setup of HACL [12]. We report F1Obj and F1Act of HalFscore [4] to evaluate the effectiveness of mitigating object and action hallucinations, respectively. but also outperforms the variant where SANTA uses hard negatives generated by GPT4o [10] (denoted as SANTAHallu. GPT4), which adopts exactly the same negatives generation strategy used in HACL [12] and HALVA [26]. This demonstrates that leveraging self-generated hallucinative samples enables more effective hallucination mitigation than relying solely on GPT4o-generated negatives. 5. Conclusion In this paper, we propose self-augmented contrastive alignment (SANTA) framework designed to mitigate hallucinations in multimodal LLMs for video understanding tasks. SANTA employs hallucinative self-augmentation scheme to expose and counteract spurious correlations in generated descriptions while encouraging the focus on visual facts through tracklet-phrase contrastive alignment. By explicitly linking regional objects and relation-guided actions with their corresponding visual and temporal phrases, SANTA enhances both object and action faithfulness in the output captions. Extensive experiments demonstrate that SANTA significantly reduces hallucinations compared to existing methods, achieving state-of-the-art performance on hallucination examination benchmarks. 6. Acknowledgments This work is supported in part by the National Science and Technology Council via grant NSTC 114-2221-E-002-056MY2 and NSTC 114-2640-E-002-006. We also thank the National Center for High-performance Computing (NCHC) resources. for providing computational and storage"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 4, 8 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 7, 8, 1 [3] Kai-Po Chang, Chi-Pin Huang, Wei-Yuan Cheng, FuEn Yang, Chien-Yi Wang, Yung-Hsuan Lai, and YuChiang Frank Wang. RAPPER: Reinforced rationaleprompted paradigm for natural language explanation in visual question answering. In The Twelfth International Conference on Learning Representations, 2024. 2 [4] Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao Chen, Bo Zhang, and Chunhua Shen. PerturboLLaVA: Reducing multimodal hallucinations with perturbative visual training. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, 5, 6, 7, 8 [5] Wey Yeh Choong, Yangyang Guo, and Mohan Kankanhalli. Vidhal: Benchmarking temporal hallucinations in vision llms. arXiv preprint arXiv:2411.16771, 2024. 1, 2, 5, 6 [6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [7] Christiane Fellbaum. Princeton university: About wordnet, 2010. 4, 1 [8] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 1, 2 [9] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and In Proceedings of the IEEE/CVF retrospection-allocation. Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. 1 [10] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 8, 1, 3, 5 [11] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 4 [12] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703627046, 2024. 1, 2, 5, 6, 7, 8, [13] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2025. 5, 6, 7, 8, 1, 2, 3, 4 [14] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 5 [15] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1387213882, 2024. 1, 2 [16] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 2 [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. 4 [18] Hui Liu and Xiaojun Wan. Models see hallucinations: Evaluating the factuality in video captioning. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 1, 5, 6, [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2, 8 [20] Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing hallucination in video language models via equal distance to visual tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1315113160, 2024. 2 [21] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [22] Dongmin Park, Zhaofang Qian, Guangxing Han, and SerNam Lim. Mitigating dialogue hallucination for large vision language models via adversarial instruction tuning, 2025. 1, 2 9 [35] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 1 [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [37] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. arXiv preprint arXiv:2406.06040, 2024. 1, 2 [38] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 1 [39] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 5, 6, 7, 8, 1, 2, 3 [40] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, 2018. 5 [41] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. 2 [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 4, 1, 2 [25] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, and Lei Zhang. Grounding dino 1.5: Advance the edge of open-set object detection, 2024. 4, 1, 2 [26] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Arik, and Tomas Pfister. Data-augmented phrase-level alignment for mitigating object hallucination. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, 6, 8 [27] Karen Sparck Jones. statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 28(1):1121, 1972. 5 [28] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating In Proceedings of clip-style models on dense captions. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2670026709, 2024. 2 [29] Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, and Huajun Chen. MLLM can see? dynamic correction decoding for hallucination mitigation. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2, [30] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. 1, 5, 8 [31] Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. 1 [32] Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu, Jaemin Cho, Zineng Tang, Mohit Bansal, and Heng Ji. Paxion: Patching action knowledge in video-language foundation models. Advances in Neural Information Processing Systems, 36:2072920749, 2023. 4, 6 [33] Junjie Wu, Tsz Ting Chung, Kai Chen, and Dit-Yan Yeung. Unified triplet-level hallucination evaluation for large visionlanguage models. Transactions on Machine Learning Research, 2025. 2 [34] Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, and Rongrong Ji. Evaluating and analyzing relationship hallucinations in large vision-language models. In Proceedings of the 41st International Conference on Machine Learning, pages 5355353570. PMLR, 2024. 2 Table 6. Quantitative comparison of hallucination mitigation methods on the Dream1k [30] benchmark. Bold and underline, respectively, indicate the best and second best results achieved by these methods over the LLaVA-Video [39]."
        },
        {
            "title": "Stock Youtube Overall",
            "content": "LLaVA-Video [39] +SFT [39] +HALVA [26] +HACL [12] +SANTA (Ours) 27.6 19.9 26.2 23.9 24.7 31.4 27.7 32.3 28.1 31. 33.4 28.3 34.6 30.0 31.8 36.7 36.0 38.2 37.5 41.2 33.0 33.0 31.8 33.2 33. 32.5 29.2 32.6 30.7 32.7 7. Additional Experiment Setups 7.1. Training Data Here, we provide more detailed explanation of the three captioning subtasks in MiraData-9k [13]. The first is overall captioning, which provides high-level summary of the videos main content. The second is main object captioning, which focuses on describing the primary objects present in the video. The last is background captioning, which provides contextual information about the surrounding environment. We describe the process of obtaining the ground-truth action and object token set using GPT-4o [10] as parser, following the prompting templates presented in Fig. 8. As mentioned in 3.2, we parse the ground-truth captions to extract structured relation triples. Specifically, we employ GPT-4o [10] as parser to derive each triple, consisting of an action verb and two associated object nouns, which we refer to as related object instances corresponding to the action verb. Thus, in our setup, is set to 2. After extracting these ground-truth token sets, we use WordNet [7] to expand them by incorporating both synonyms (e.g., kid for child and get up for stand) and hypernyms (e.g., person for child and move for stand). Since each video is described using three enriched captions, the extracted tokens comprehensively represent the visual facts depicted in the video. As result, this augmentation process allows self-augmented hallucinative captions to capture the hallucination tendencies of the generated MLLM while ensuring that no visual information outside the videos content is introduced. Tracklet-Phase Contrastive Alignment. As described in 3.3, to obtain the object tracklets, we employ GroundedSAM2 [24, 25] to generate segmentation masks for each object instance, based on object information available in ground-truth captions. Using these masks, we crop the corresponding object regions, allowing us to identify which visual tokens contain object-specific information. These identified visual tokens are then used for region-level object alignment and relation-guided action alignment. The entire process is conducted offline. 7.2. Additional Evaluation Setups In this subsection, we describe how we use MiraData9k [13], video-caption benchmark, to evaluate object and action hallucinations. As mentioned in Sec.7.1, we parse the ground-truth captions to extract sets of ground-truth object and action tokens. Using these sets, we then apply the hallucination evaluation metrics (i.e., HalAct, HalObj, CovAct, CovObj, F1Act, and F1Obj for both HalScore [4] and weighted-HalScore) introduced in Sec.4.1 to assess hallucination mitigation methods. 7.3. Comparisions We compare SANTA to simple baseline supervised fine-tuning baseline, and several the state-of-the-art visual including DeCo [29], hallucination mitigation methods, HALVA [26], and HACL [12]. For all methods, we utilize their publicly available codebases and adhere to their reported training configurations. To ensure fair comparison, we align their training dataset, training and inference hyperparameter settings for all the methods, matching those used by LLaVA-Video [39] and Qwen2.5-VL [2]. 8. Additional Quantitative Results More Detailed Results on Video Description. Tab. 6 presents quantitative comparison of various hallucination mitigation methods on Dream1k [30], challenging video description benchmark comprising videos from five diverse sources: animation movies, live-action movies, TikTokstyle videos, stock videos, and YouTube videos. The evaluation metric uses F1-score, calculated by the LLM-based evaluator, AutoDQ [30]. As shown in Tab. 6, SANTA achieves the highest overall F1-score, outperforming existing methods such as HALVA [26] and HACL [12]. This demonstrates that SANTA not only preserves but also enhances the video captioning capabilities of MLLMs. No1 Table 7. uantitative comparison of hallucination mitigation methods on the VideoMME [8] benchmark, with and without subtitles. Bold and underline, respectively, indicate the best and second best results achieved by these methods over the LLaVA-Video [39]."
        },
        {
            "title": "Overall",
            "content": "with without with without with without with without LLaVA-Video [39] +SFT [39] +HACL [12] +SANTA (Ours) 80.1 54.8 73.1 76.8 77. 56.8 69.9 72.3 66.6 45.4 62.3 64.6 61.8 45.9 57.9 59.8 58. 35.3 53.2 55.4 52.1 35.0 51.4 49.6 68.3 45.2 62.9 65.6 63. 45.9 59.7 60.6 Table 8. Ablation of SANTA w/ the quality of object tracklets. We report the averaged F1 score of MiraData-9k [13] (HalFscore [4] and Weighted-HalFscore) to evaluate the effectiveness in mitigating both object and action hallucinations. Method HalFscore [4] weighted-HalFscore Avg. F1Obj Avg. F1Act Avg. F1Obj Avg. F1Act HACL [12] SANTA w/ G-SAM2 (t=0.15) SANTA w/ G-SAM2 (t=0.25) 36.5 37.2 28.5 29.4 52.8 54. 37.9 30.0 54.8 30.5 31.6 32. tably, SANTA surpasses the original LLaVA-Video [39] baseline, further validating its effectiveness. Results on General Video Question Answering. In Tab. 7, we present quantitative comparison of different hallucination mitigation methods on the VideoMME [8] benchmark, evaluated under both with and without subtitles settings. VideoMME is multiple-choice QA benchmark that spans diverse range of video lengths, from short clips (11 seconds) to long-form content (up to 1 hour). The evaluation metric is accuracy, where higher scores indicate better video understanding performance. As presented in Tab. 7, when applied to the same LLaVA-Video backbone, SANTA achieves higher QA accuracy than HACL [12], improving the overall performance by +2.7 (with subtitles) and +0.9 (without subtitles). However, both methods still exhibit slight decrease compared to the original backbone, which is shared limitation of current hallucination mitigation approaches. We hypothesize that this stems from using only video captioning data (e.g., MiraData-9k [13]) during post-training, without leveraging any video QA data. Thus, Appropriately incorporating portion of this QA data could preserve the backbones QA capability and, when combined with the hallucination reduction achieved by SANTA, further enhance overall performance beyond the original backbone. Figure 6. Qualitative results on hallucinative and object/action tracklets from MiraData-9k [13]. Analysis of Robustness with the Impact of Object Tracklet Quality. We use the SOTA video tracker GroundedSAM2 [24, 25] to extract object tracklets. To analyze the robustness against possible tracking errors, we now ablate SANTA by training with noisy object tracklets, by lowering the detection confidence threshold (from default 0.25 to 0.15) to induce false positives on MiraData-9k [13]. As shown in Tab. 8, under the noisier setting (t=0.15) with more false positives, SANTA surpasses the existing SOTA hallucination mitigation method HACL [12] by clear margin. Specifically, it achieves +2.1 and +0.6 absolute improvements on Avg. F1Obj and Avg. F1Act under HalFscore, and +2.9 and +1.1 under weighted-HalFscore, respectively. These results demonstrate that SANTA remains robust and effectively mitigates both object and action hallucinations, even when trained with imperfect object tracking results. 9. Additional Qualitative Results Results on Hallucinative Negative and Object/Action Tracklets. Fig. 6 shows surfing scene video frame, ground-truth caption c, and the hallucinative negative caption (Ch). In the visualized video frame, the presence of sea wave elements implies hallucination of heap of sand..., in Ch. The object tracklets are denoted as colored etc. patches, and the action tracklets are derived from the associated two object tracklets (e.g., stand is associated with 2 surfer and pink surfboard). These relationships are determined by GPT4o [10] via prompting with the groundtruth caption, as described in Fig. 8. Additional Results on MiraData-9k. In Fig. 7, we present additional qualitative results of applying SANTA with LLaAV-Video [39] on MiraData-9k [13], which exhibits object and action faithfulness within its generated captions. Figure 7. Qualitative comparison of video captions predicted by HACL [12] and SANTA on MiraData-9k [13]. Note that words highlighted in green indicate action faithfulness, while those in red indicate action hallucination. Similarly, words in blue represent object faithfulness, whereas those in orange denote object hallucination. 4 Figure 8. The prompting template used to prompt GPT-4o [10] with few-shot demonstrations to perform the parsing task."
        }
    ],
    "affiliations": [
        "Graduate Institute of Communication Engineering, National Taiwan University",
        "NVIDIA"
    ]
}