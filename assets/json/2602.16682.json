{
    "paper_title": "Learning Situated Awareness in the Real World",
    "authors": [
        "Chuhan Li",
        "Ruilin Han",
        "Joy Hsu",
        "Yongyuan Liang",
        "Rajiv Dhawan",
        "Jiajun Wu",
        "Ming-Hsuan Yang",
        "Xin Eric Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics."
        },
        {
            "title": "Start",
            "content": "Chuhan Li1, Ruilin Han2, Joy Hsu3, Yongyuan Liang4, Rajiv Dhawan5, Jiajun Wu3, Ming-Hsuan Yang6, Xin Eric Wang1 1University of California, Santa Barbara, 2Yale University, 3Stanford University, 4University of Maryland, College Park, 5Amazon, 6University of California, Merced core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in scene), while largely overlooking observer-centric relationships that require reasoning relative to agents viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes models observercentric understanding with six different awareness tasks. Our comprehensive evaluation reveals human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics. Correspondence: chuhan_li@ucsb.edu, ericxwang@ucsb.edu Project Page: sawbench.github.io 6 2 0 2 8 ] . [ 1 2 8 6 6 1 . 2 0 6 2 : r Figure 1. (Left) Situated Awareness in the Real World. real-world example in which the observer walks along straight trajectory while frequently rotating their head. The resulting egocentric video exhibits substantial camera orientation changes despite linear translational motion. (Right) Reasoning Task Performance. Radar plot compares human performance with representative MFMs across six situated awareness tasks in SAW-Bench. Learning Situated Awareness in the Real World 1. Introduction Human perception of space is inherently situated. As people move through the world, they do not perceive scenes from detached, global viewpoint; instead, they experience the environment relative to their own body and perspective. At any given moment, person maintains an implicit sense of their location, orientation, and the intentional arc of their movements (Merleau-Ponty et al., 2013). Collectively, these observer-centric capabilities constitute situated awareness (Flach, 1995, Tversky, 2009, Sarter and Woods, 1991, Endsley, 1995), which operates continuously during everyday interaction and forms foundational layer that supports more complex spatial intelligence. Situated awareness is not only central to human perception, but also critical for autonomous systems operating in physical environments. In robotics, knowing what an object is is insufficient; an agent must also track precisely where it is relative to its own body to effectively plan grasping and navigation. Similarly, in augmented and virtual reality (AR/VR), systems must continuously synchronize virtual content with the users physical perspective to maintain immersion. In both domains, failure in this translation decouples the systems understanding from physical reality. Cognitive science studies suggest that such spatial intelligence relies on path integration, where local, situated updates are accumulated to larger observer-aware map (McNaughton et al., 2006). Consequently, failures in situated awareness do not merely result in local errors; they create cascading drift that significantly degrades the systems capability to understand space and time (Wolbers and Hegarty, 2010). Yet, despite the importance of situate awareness, the current evaluation landscape has largely overlooked these observer-centric capabilities. While there is growing interest in spatial reasoning for MFMs, existing benchmarks, such as VSI-Bench (Yang et al., 2025b) and MindCube (Yin et al., 2025), predominantly focus on observer-independent tasks. These benchmarks emphasize object-object interaction, discrete mental simulation, and metric distance estimation skills that define spatial reasoning from detached, static, and third person perspective. As result, models are evaluated under the assumption that they are passive spectators rather than active embodied agents with their own view point, motion, and position. This leaves observer-centric situated awareness, the ability to understand space relative to the observer and how it evolves overtime, largely unexplored. To bridge this gap, we introduce SAW-Bench, Situated Awareness in the Real World, novel video understanding benchmark designed to assess MFMs situated awareness capabilities. SAW-Bench comprises 2,071 carefully curated, human-annotated multiple-choice question-answer pairs spanning six distinct awareness tasks (localization, relative direction, route shape, reverse route plan, spatial memory, and spatial affordance), applied to 786 egocentric videos encompassing both indoor and outdoor environments (3.2). Videos in SAW-Bench are all self-recorded using Ray-Ban Meta (Gen 2) glasses. The question-answer pairs are carefully designed to ensure that models need to reason about the observer itself and the environment, making situated awareness essential for solving our tasks (3.1). We conduct comprehensive evaluation of 16 open-source and 8 proprietary MFMs. The best performing model, Gemini 3 Flash, yields performance of 53.89% on SAW-Bench, significantly below that of humanlevel performance (91.55%). Beyond this overall performance gap, we identify four systematic patterns in model behavior (5): (1) models often conflate egocentric camera rotation with translational movement; (2) model accuracy degrades significant as trajectory complexity increases; (3) models exhibit mismatch between view-level internal memory and persistent world-state memory; and (4) environment openness alone is an insufficient proxy for spatial reasoning difficulty. Learning Situated Awareness in the Real World Figure 2. Overview of SAW-Bench. We illustrate six representative tasks ( 3.1) evaluating different aspects of situated awareness: Self-Localization, Relative Direction, Route Shape, Reverse Route Plan, Spatial Memory, and Spatial Affordance. During data collection, human annotators follow pre-defined trajectories when recording egocentric videos ( 3.2); these trajectories are visualized as purple dashed arrows. For all tasks, the model input consists solely of egocentric video without access to any birds-eye or global scene representations; the visualizations shown here are provided for illustrative purposes only. 3 Learning Situated Awareness in the Real World Table 1. Summary of Visual-Spatial Reasoning Benchmarks. We compare existing works from: 1Egocentric setting, 2Self-collected data, 3Input modality, 4Self-localization tasks, 5Motion Understanding, 6Trajectory Reasoning, 7Spatial Memory, and 8Action Feasibility. Benchmark Ego. Self-Col. Mod. Self-Loc. Motion Traj. Mem. Act. Feas. Spatial-MM (Shiri et al., 2024) ViewSpatial Bench (Li et al., 2025b) Open3D-VQA (Zhang et al., 2025b) SpatialBench (Xu et al., 2025) MindCube (Yin et al., 2025) 3SDRBench (Ma et al., 2025) All-Angles Bench (Yeh et al., 2026) VSI-Super Recall (Yang et al., 2025c) VSI-Super Count (Yang et al., 2025c) OmniSpatial (Jia et al., 2025) SpaCE-10 (Gong et al., 2025) MomaGraph (Ju et al., 2025) VLM4D (Zhou et al., 2025) MMSI-Bench (mms, 2025) VSI-Bench (Yang et al., 2025b) MMSI-Video-Bench (Lin et al., 2025) SAW-Bench (Ours) Images Images Images Videos Images Images Images Videos Videos Images + Videos Images + Videos Images Videos Images Videos Videos Videos 2. Related Work General video understanding benchmarks. Video understanding has become central capability in MFMs, serving as key testbed for evaluating their ability to perceive, reason, and integrate visual and linguistic information over time. Recent benchmarks emphasize more complex forms of reasoning such as long-form video understanding (Chandrasegaran et al., 2024, Wu et al., 2025, Chen et al., 2025, Zhang et al., 2025a, Wang et al., 2025); visual-temporal reasoning (Liu et al., 2024, Shangguan et al., 2025, Xue et al., 2025); domain-specific reasoning (He et al., 2025, Zhao et al., 2025, Deng et al., 2025); and comprehensive video understanding (Ning et al., 2025, Li et al., 2024, Fu et al., 2025). Despite this growing complexity, these benchmarks remain largely allocentric, evaluating models as passive observers of scene-level events. Consequently, observer-centric spatial reasoning, understanding ones own position and relationship to the environment, remains largely unexamined, motivating the curation of SAW-Bench. 3D spatial intelligence. Research in 3D spatial intelligence has predominantly focused on reasoning over explicit, reconstructed geometric representations such as point clouds and meshes (Jain et al., 2022, Hsu et al., 2023, Huang et al., 2022, Abdelrahman et al., 2024). Early datasets like ReferIt3D (Achlioptas et al., 2020) and ScanRefer (Chen et al., 2020) evaluate the capability to ground natural language into specific 3D coordinates, while more recent datasets like 3D-VisTA (Ziyu et al., 2023), 3D-GRAND (Yang et al., 2025a), and EmbodiedScan (Wang et al., 2024a) assess more complex grounding and reasoning over holistic 3D scenes. Among these datasets, SQA3D (Ma et al., 2023) is the most relevant to our work, as it explicitly introduces situated questions in 3D scene (e.g., asking about the environment relative to specific location given 3D scene context). While prior work often relies on reconstructed 3D scenes, such data is costly to obtain and difficult to scale in real-world settings. In practice, self-recorded egocentric video is lower-friction input to MFMs, as it requires neither 3D capture nor externally mounted cameras. For that reason, SAW-Bench adopts egocentric video as its primary modality to evaluate situated awareness in realistic real-world settings. Visual-spatial intelligence benchmarks. In the visual domain, existing benchmarks often focus on highlevel semantic or commonsense reasoning, while overlooking fine-grained and precise spatial intelligence. 4 Learning Situated Awareness in the Real World Figure 3. Benchmark Curation Pipeline. We first pre-define 37 camera trajectories and annotate their metadata (details are provided in D). Human video collectors then record egocentric videos by following these trajectories in selected scenes. Low-quality recordings are filtered and re-captured to ensure consistent video quality. Evaluations that do focus on spatial awareness largely frame spatial reasoning through observer-independent tasks, overlooking the inherently observer-centric nature of embodied tasks. Benchmarks such as VSI-Bench (Yang et al., 2025b), VSI-Super (Yang et al., 2025c), SpatialBench (Xu et al., 2025), and SpaCE-10 (Gong et al., 2025) emphasize object-object interaction, counting, and distance estimation, which treat models as detached, external observer. Similarly, MindCube (Yin et al., 2025), MomaGraph (Ju et al., 2025) and All-Angles Bench (Yeh et al., 2026) assess spatial reasoning using discrete multi-view inference. While they evaluate models ability to integrate spatial information across viewpoints, they typically assume the observers state is given or static. As result, models capability for situated awareness, the continuous real time update from observers own pose and perspective relative to the environment, largely unexplored. more comprehensive comparison is provided in Table 1. 3. Situated Awareness Benchmark 3.1. Situated Awareness Tasks in SAW-Bench While cognitive science does not prescribe canonical task taxonomy for situated awareness, prior work across navigation (Tversky, 1993, Burgess, 2006), spatial updating (Franklin et al., 1992, Michon and Denis, 2001), spatial working memory (Luck and Vogel, 1997, Simons and Levin, 1997), and affordance (Gibson and Walk, 1960, Gibson, 2014) has studied these abilities as separable components, each capturing distinct aspect of how observers relates themselves to the environment. Motivated by this decomposition, we introduce six situated awareness tasks over 2,071 human-annotated question-answer pairs derived from 786 videos, each of which requires models to understand and reason over the relationship between itself and the environment: Self-Localization (9.92%): infer the observers position within the environment from an egocentric viewpoint; Relative Direction (40.27%): reason about the observers relative position across time by relating starting and ending viewpoints; Route Shape (26.36%): characterize the geometric shape of the observers movement trajectory; Reverse Route Plan (11.06%): infer sequence of movements that returns the observer to the starting location; Spatial Memory (4.83%): reason about changes in the environment by comparing spatial information across time; Spatial Affordance (7.82%): determine whether specific action is feasible under physical constraints from the observers viewpoint. Learning Situated Awareness in the Real World 3.2. Video Collection Egocentric video is natural sensing modality for studying situated awareness, as it captures the environment from the observers own viewpoint and preserves the observer-centric structure of spatial perception during real-world interaction. Unlike third-person footage, egocentric video directly encodes where objects appear relative to the camera wearer, how the field of view evolves with head and body movement, and how visibility of the environment changes over time. To reflect this setting, all videos in SAW-Bench are recorded from an egocentric perspective using Ray-Ban Meta (Gen 2) smart glass worn by human participants. Most videos are captured as single, continuous clips without interruption. For tasks involving Spatial Memory, we apply limited post-processing by concatenating two short clips recorded in the same physical scene: one before and one after controlled modification of the environment. No other temporal reordering or editing is performed. Audio is excluded from all videos to ensure that all reasoning is grounded solely in visual information. Our video collection process spans diverse set of real-world environments, including 10 outdoor scenes (e.g., courtyards, parking lots, lawns, and plazas) and 5 indoor scenes (e.g., lecture halls, classrooms, recreation rooms, and household environments). Within each scene, we collect approximately 40-60 distinct videos to support tasks that benefit from dense coverage of fixed environment, such as Self-Localization and Route Shape. For tasks that are more difficult to scale within limited set of scenes, particularly Spatial Memory and Spatial Affordance, we additionally collect set of videos across broader range of environments outside these core scenes. This supplemental collection prioritizes diversity over dense coverage, enabling evaluation of memory and action feasibility across varied layouts and physical constraints without requiring exhaustive sampling of each scene. During video collection, participants followed lightweight recording protocol, consisting of high-level guidelines intended to ensure consistency across scenes while preserving natural behavior. For tasks involving Self-Localization, participants were instructed to record videos from set of predefined reference locations (e.g., corners, side, or center) to ensure coverage of diverse viewpoints. Beyond these coverage requirements, the protocol did not prescribe specific paths, motions, or camera poses. Instead, participants were instructed to follow coarse trajectory shapes (e.g., zigzag or two consecutive turns), while retaining flexibility in how these shapes were executed within each environment. Recording protocols are provided in D.1. 3.3. Question-Answer Annotation and Quality Check Question-answer annotation. Question-answer (QA) pairs in SAW-Bench are annotated based on the predefined recording protocol and the known trajectory of each video. All QA pairs are annotated by the same human participants who recorded the videos, who followed predefined reference locations and coarse trajectory shapes during data collection. As result, annotation is restricted to these predefined configurations rather than open-ended interpretation of the video content. This design allows questions to target well-defined aspects of agent-centric situated awareness while minimizing ambiguity in ground-truth answers. Quality control. We perform quality check at both the video and annotation levels. For video quality control, each recorded video is manually reviewed by human reviewers. Videos exhibiting issues such as rapid head motion, poor visibility of key objects, or other factors that could impair spatial reasoning were discarded and re-filmed following the same recording protocol. To ensure high-quality QA annotation, each QA pair in the protocol is independently annotated by two annotators. We report inter-annotator agreement score in D.2. Disagreements were resolved through final review process following the same annotation guidelines. This 6 Learning Situated Awareness in the Real World Table 2. Evaluation Results on SAW-Bench. Unless otherwise specified, all models process videos at 2 fps (frames per second). Frame level sensitivity analyses are provided in F. Bold and underlined numbers indicate the best and second-best performance in each category, respectively. Model configurations are provided in Table 4. : Human baseline details are provided in C.3. : Models do not support fps-based sampling and process fixed total of 32 frames per video. : 8 frames per video sampling due to compute limitations. Model All SelfLocalization Relative Direction Route Shape Reverse Route Plan Spatial Memory Spatial Affordance Baselines Human Level Chance Level (Random) Chance Level (Frequent) Blind LLM (GPT-5.2) Socratic Model (GPT-5.2) 91.55 27.49 29.55 31.34 31.34 94.00 34.00 38.00 38.00 40.50 Proprietary Multimodal Foundation Models Gemini 3 Flash Gemini 2.5 Pro Gemini 3 Pro GPT-5.2 Gemini 2.5 Flash GPT-5 Mini 53.89 50.80 45.97 41.04 39.79 33.80 48.50 45.50 50.00 45.50 44.00 43.50 Open-Source Multimodal Foundation Models Qwen3-VL 235B-A22B Qwen3-VL 32B Qwen3-VL 30B-A3B Qwen2.5-VL 32B Qwen2.5-VL 72B Qwen3-VL 8B LLaVA OneVision 72B InternVL3 8B LLaVA-Video 72B InternVL3 14B Qwen2.5-VL 7B LLaVA-NeXT-Video 32B LLaVA-Video 7B InternVL2 40B InternVL2 8B LLaVA OneVision 7B InternVL3 38B 41.40 38.58 36.55 36.46 36.17 36.12 33.70 33.70 32.98 32.69 31.48 31.24 30.81 30.13 29.84 29.45 27.71 43.50 44.00 39.00 53.00 51.50 40.00 39.00 43.50 32.50 49.00 38.50 41.00 41.00 45.00 43.00 34.50 35.50 89.39 25.90 25.90 23.02 20. 41.13 37.05 38.61 25.78 25.30 27.46 33.41 29.14 29.62 28.06 26.74 27.82 22.30 26.86 23.86 17.27 19.06 24.46 25.06 17.75 14.99 20.26 23.50 97.62 21.43 27.11 36.63 41.58 64.84 66.12 52.01 50.55 57.33 36.08 53.11 48.35 43.04 41.03 41.76 46.70 46.15 36.45 43.04 45.05 43.59 35.35 32.78 38.28 41.94 34.80 37.55 93.01 27.51 27.51 24.02 24. 61.57 51.53 36.24 44.98 37.99 22.27 30.13 29.26 27.07 24.89 25.33 23.58 24.45 27.95 24.45 24.02 26.20 22.27 24.45 24.89 24.89 25.33 24.45 88.50 28.00 27.00 38.00 32.00 66.00 66.00 63.00 63.00 49.00 56.00 46.00 52.00 54.00 45.00 45.00 48.00 41.00 46.00 41.00 54.00 38.00 34.00 32.00 32.00 40.00 44.00 46.00 79.01 56.17 50.62 54.32 50. 70.99 66.05 61.73 62.96 46.91 49.38 54.32 52.47 50.00 54.94 56.79 48.77 52.47 48.15 53.70 49.38 49.38 51.23 49.38 54.32 50.00 49.38 51.23 rigorous approach ensures consistency and accuracy across all annotated QA pairs. 4. Experiments 4.1. Evaluation Protocol Models. We evaluate diverse set of general-purpose MFMs in zero-shot setting. For open-source MFMs, we test: Qwen2.5-VL (Bai et al., 2025b), Qwen3-VL (Bai et al., 2025a), InternVL 2 (Chen et al., 2024), InternVL 3 (Zhu et al., 2025), LLaVA-NeXT-Video (Zhang et al., 2025c), and LLaVA-OneVision (Li et al., 2025a). We also evaluate the following proprietary MFMs: GPT (Singh et al., 2026) and Gemini (Gemini Team, 2025, Comanici et al., 2025). We provide full model evaluation configuration in Table 4. Baselines. We include five baselines: (1) chance level (random), which is defined as the expected accuracy under uniform random answer selection; (2) chance level (frequent), which is defined as the accuracy achieved 7 Learning Situated Awareness in the Real World Figure 4. Error Case Analysis. (Left) Reverse Route Plan: Gemini 3 Flash successfully reconstructs the return path by systematically inverting the actions from the forward pass. In contrast, Qwen3-VL 235B attempts to exploit shortcut between the first and last frames, thereby neglecting the transitive dynamics and spatial transformations occurring throughout the frame sequence. (Right) Route Shape: While both Gemini 3 Flash and Qwen3-VL 235B effectively identify camera rotations, they falsely integrate these rotational pans into the observers physical movement trajectory, leading to incorrect shape understanding. by always selecting the most frequent answer; (3) blind LLM, which answer the multiple-choice questions without access to any visual information from the video. We use GPT-5.2 as our blind evaluation for this baseline. Details are provided in C.1; (4) socratic models (Zeng et al., 2023), which generate single holistic caption for each video using video captioner and use this caption as language-based representation of the video for downstream question answering. In evaluation, the model is provided with the question and the caption only. We use GPT-5.2 for both video captioning and question answering. Details are provided in C.2; (5) human level, which is measured based on the answers given by two graduate students, who have access to the full videos and no time constraints. Details are provided in C.3. Accuracy evaluation. We use accuracy as the primary metric to evaluate model performance on SAW-Bench. Following recent benchmarks for foundation model evaluation (Wang et al., 2024b, Shangguan et al., 2025, Zhao et al., 2025), we first apply regular-expressionbased parser to extract the predicted answer from each models raw response. If the parser fails, we additionally use GPT-4o-mini to extract the answer from the raw output. Prompt used for answer extraction is provided in B.3. 4.2. Main Results We provide quantitative results on SAW-Bench for all models in Table 2. To better understand where models fail, we select set of representative models (Gemini Team, 2025, Comanici et al., 2025, Singh et al., 2026, Bai et al., 2025a,b) and present examples of failure cases in G.1, G.2, G.3, G.4, G.5, and G.6. Widespread difficulty in situated awareness. Our evaluation reveals that situated awareness remains fundamental challenge for current MFMs. Even the top performing model, Gemini 3 Flash, achieves only 53.89% accuracy overall, leaving 37.66% performance gap compared to human accuracy of 91.55%. Notably, humans perform remarkably well on Self-Localization and Route Shape tasks, suggesting that 8 Learning Situated Awareness in the Real World these categories are naturally intuitive to observers. Interestingly, the performance gap between human and top-performing MFMs is much smaller on Spatial Memory and Spatial Affordance, highlighting that current models may be relatively strong on tasks that rely more heavily on spatial memorization and depth cues. Proprietary vs. open-source model. In general, proprietary MFMs outperform open-source MFMs, with the largest performance gap appearing on Reverse Route Plan, task that requires sustained reasoning over egocentric trajectories and explicit tracking of intermediate movements. We provide qualitative analysis on model responses in Figure 4 Right. In these cases, open-source model tends to rely on shortcuts cues from some key frames, typically the first and the last frame of the video; whereas proprietary model more consistently maintain coherent observer-centric representation across the full extent of the video. Blind LLM vs. socratic model. The blind LLM achieves an overall accuracy of 31.34%, which is only marginally above chance level baselines, indicating that effective performance on SAW-Bench requires access to visual information. Compared to Blind LLM, Socratic model does not yield significant performance gains, achieving the same overall accuracy of 31.34%. Although Socratic model has indirect access to visual content through video captioning, reducing egocentric video into static language-based representation discards critical observer-centric cues such as viewpoint changes, orientation, and temporal structure. Notably, the Socratic model exhibits slightly better performance on Route Shape than the Blind LLM, suggesting that captions can convey coarse trajectory-level information. However, this limited improvement does not extend to other tasks. As result, high-level semantic summaries alone are insufficient to situated awareness. 5. Analysis To better understand when and why models fail at situated awareness, we analyze representative error patterns that reflect core components of observer-centric reasoning. Camera rotation as source of trajectory errors. We identify systematic failure mode in Route Shape occurring when changes in camera rotation are decoupled from the observers translational movement. To isolate this effect, we compare three controlled scenarios: (1) straight path with stable head orientation (Figure 5 Left); (2) the same straight path with frequent head rotations (Figure 5 Middle); and (3) true zigzag trajectory (Figure 5 Right). Despite identical translational motion in cases (1) and (2), even top-performing models frequently misclassify case (2) as zigzag trajectory: Gemini 3 Flash does so in 60.0% of instances, while Qwen3-VL 235B misclassifies 53.3% of cases. As illustrated in Figure 4 Right, models justify these predictions by erroneously attributing camera orientation shifts to physical body displacement. This failure highlights fundamental limitation in current MFMs: the inability to maintain robust observer-centric coordinate system that distinguishes egocentric rotational pans from global positional updates. Figure 5. Camera Rotation and Observers Trajectory. Visualization of three controlled scenarios used to isolate the impact of head rotation on Route Shape. (Left) straight path with steady head orientation; (Middle) the same straight path with frequent left-and-right head rotations; and (Right) true zigzag trajectory. 9 Learning Situated Awareness in the Real World Finding 1. Current MFMs often conflate egocentric camera rotation with translational movement. Table 3. Accuracy (%) on Relative Direction Tasks Stratified by the Number of Turns. Performance for most models degrades significantly as geometric complexity increases. Models Straight Single Turn Two Turns Human Gemini 3 Flash Gemini 3 Pro Gemini 2.5 Pro GPT-5.2 Qwen3-VL 235B Qwen3-VL 32B 100.00 73.33 63.33 73.33 30.00 90.00 80.00 96.67 (-3.33%) 70.69 (-3.60%) 56.90 (-10.16%) 55.17 (-24.76%) 39.66 (+32.20%) 8.62 (-90.42%) 12.07 (-84.91%) 90.00 (-10.00%) 40.61 (-44.63%) 36.46 (-42.44%) 33.41 (-54.44%) 22.49 (-25.03%) 27.85 (-69.06%) 21.83 (-72.71%) Trajectory complexity and error accumulation. Spatial updating is an inherently accumulative process, where errors in estimating egocentric motion compound as the observer moves through an environment (McNaughton et al., 2006, Wolbers and Hegarty, 2010, Stangl et al., 2020). In human navigation, this integration is highly sensitive to noise introduced by changes in orientation (Cherep et al., 2020). To investigate whether MFMs exhibit similar sensitivity to trajectory complexity, we stratify results on the Relative Direction task by geometric complexity: (1) Straight (pure translation), (2) Single Turn (one rotational update), and (3) Two Turns (multiple rotational updates). As shown in Table 3, increasing geometric complexity is associated with substantial accuracy degradation for most models, particularly when trajectories involve multiple orientation changes. When quantified using relative performance drop with respective to straight trajectories, MFMs often exhibit significant degradation under multi-turn conditions, while human performance remains largely stable. This human-model gap suggests that current MFMs struggle to reliable integrate successive egocentric orientation changes over time, resulting in compounding errors as trajectories move away from simple translational motion. Finding 2. Model accuracy degrades significantly as trajectory complexity increases. Failure to maintain persistent object memory. recurring failure mode across Spatial Memory tasks arises from models difficulty in maintaining object persistence across egocentric motion. Although models often provide accurate descriptions of what is visible in individual frames or short temporal windows, they fail to reason about objects that leave the cameras field of view. As shown in Figure 6, models tend to infer that objects are absent in earlier frames simply because they are not visible, incorrectly treating first observation as object appearance rather than recognizing that the object may have existed outside the field of view. These errors suggest that current MFMs rely primarily on view-dependent evidence, rather than maintaining persistent world-state representation over time. Finding 3. Persistent tracking of objects across frames remains an open challenge across models. 10 Learning Situated Awareness in the Real World Figure 7. Indoor vs. Outdoor Performance. Comparison of zero-shot accuracy across six situated awareness tasks for Gemini 3 Flash, Gemini 2.5 Pro, GPT-5.2, and Qwen3-VL 235B. Effect of openness on situated awareness. Figure 7 summarizes model performance across indoor and outdoor environments. Contrary to the intuition that larger and more dynamic outdoor environments may increase spatial reasoning difficulty, no consistent performance degradation is observed in outdoor scenes. Across the four selected models, outdoor performance is often comparable to, and in several cases higher than, indoor performance. On average, the indooroutdoor performance gap remains small. These results suggest that environment scale alone does not determine spatial reasoning difficulty. While outdoor scenes typically span larger spatial extents, they often contain fewer objects and exhibit less structural clutter than indoor environments, which may reduce relational ambiguity. As result, spatial reasoning difficulty is not monotonically correlated with scene size or openness. Instead, indoor environments can pose equally, if not more, complex spatial challenges due to higher object density and more intricate layout structures. Figure 6. Model Responses in Spatial Memory. Across multiple models, non-visibility is incorrectly treated as non-existence: objects that exit the cameras field of view are inferred to have disappeared or changed, revealing gap between what is seen and what exists. Finding 4. Environment openness alone is an insufficient proxy for spatial reasoning difficulty. 6. Conclusion Situated awareness underlies how humans continuously perceive, navigate, and act in the physical world, yet it remains insufficiently captured by existing multimodal evaluation frameworks. In this work, we introduce SAW-Bench to explicitly evaluate observer-centric situated spatial understanding in MFMs using egocentric videos. Through systematic evaluation of 24 models, we uncover fundamental gaps in current MFMs ability to reason about observer-centric tasks. Our analysis identifies key factors underlying these limitations, offering insights for advancing MFMs toward more robust situated spatial intelligence. We hope this work sheds light on the development of AI systems that move beyond passive observation toward physically grounded, observer-centric, and interactive world understanding. 11 Learning Situated Awareness in the Real World"
        },
        {
            "title": "Impact Statement",
            "content": "This work introduces SAW-Bench, benchmark for evaluating observer-centric situated awareness in MFMs using egocentric videos. SAW-Bench provides diagnostic tool to measure spatial understanding capabilities that are currently underrepresented in existing evaluation frameworks. Potential impacts include improved reliability of AI systems deployed in robotics, augmented and virtual reality (AR/VR), and assistive technologies, where understanding spatial relationships from embodied agents or human wearers perspective is critical for safe and effective operation. While our benchmark does not introduce direct pathways to harm, we acknowledge potential downstream misuse, as models that perform well on these tasks could be integrated into applications that may be deployed in harmful ways. We encourage future work to study and adopt responsible deployment practices for systems built on or evaluated with our benchmark."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Ziyao Shangguan, Jiayuan Mao, Qianqi Yan, Gurusha Juneja, Mable Zhou, Mary Hegarty, Adina Roskies, and members of UCSB NLP Group for their helpful discussion and feedback. This project is partially sponsored by an Amazon gift award. 12 Learning Situated Awareness in the Real World"
        },
        {
            "title": "References",
            "content": "MMSI-Bench: Benchmark for Multi-Image Spatial Intelligence, author=Sihan Yang and Runsen Xu and Yiman Xie and Sizhe Yang and Mo Li and Jingli Lin and Chenming Zhu and Xiaochen Chen and Haodong Duan and Xiangyu Yue and Dahua Lin and Tai Wang and Jiangmiao Pang. ArXiv, abs/2505.23764, 2025. Eslam Abdelrahman, Mohamed Ayman, Mahmoud Ahmed, Habib Slim, and Mohamed Elhoseiny. Cot3DRef: Chain-of-Thoughts Data-efficient 3D Visual Grounding. In ICLR, 2024. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas J. Guibas. ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes. In ECCV, 2020. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-VL Technical Report. arXiv preprint arXiv:2511.21631, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025b. Neil Burgess. Spatial Memory: How Egocentric and Allocentric Combine. Trends in Cognitive Sciences, 10 (12):551557, 2006. Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Crist√≥bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. HourVideo: 1-Hour Video-Language Understanding. In NeurIPS, 2024. Dave Zhenyu Chen, Angel Chang, and Matthias Niessner. ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language. In ECCV, 2020. Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding. In ICLR, 2025. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. arXiv preprint arXiv:2404.16821, 2024. Lucia Cherep, Alex Lim, Jonathan Kelly, Devi Acharya, Alfredo Velasco, Emanuel Bustamante, Alec Ostrander, and Stephen Gilbert. Spatial Cognitive Implications of Teleporting through Virtual Environments. Journal of Experimental Psychology: Applied, 26(3):480, 2020. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint arXiv:2507.06261, 2025. Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, and Xiaohan Wang. SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models. arXiv preprint arXiv:2510.08559, 2025. Mica R. Endsley. Toward Theory of Situation Awareness in Dynamic Systems. Human Factors: The Journal of Human Factors and Ergonomics Society, 37:32 64, 1995. 13 Learning Situated Awareness in the Real World John M. Flach. Situation Awareness: Proceed with Caution. Human Factors: The Journal of Human Factors and Ergonomics Society, 37:149 157, 1995. Nancy Franklin, Barbara Tversky, and Vicky Coon. Switching Points of View in Spatial Mental Models. Memory & Cognition, 20(5):507518, 1992. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis. In CVPR, 2025. Gemini Team. New Era of Intelligence with Gemini 3. 2025. Accessed: 2026-01-16. Eleanor Gibson and Richard Walk. The\" Visual Cliff\". Scientific American, 202(4):6471, 1960. JJ Gibson. The Ecological Approach to Visual Perception. Psychology Press, 2014. Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Zhaokai Wang, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, and Rongrong Ji. SpaCE-10: Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence. arXiv preprint arXiv:2506.07966, 2025. Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, and Xin Eric Wang. MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos. In ICLR, 2025. Joy Hsu, Jiayuan Mao, and Jiajun Wu. NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations. In CVPR, 2023. Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view Transformer for 3D Visual Grounding. In CVPR, 2022. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom Up Top Down Detection Transformers for Language Grounding in Images and Point Clouds. In ECCV, 2022. Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models. arXiv preprint arXiv:2506.03135, 2025. Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, and Koushil Sreenath. MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning. arXiv preprint arXiv:2512.16909, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer. In TMLR, 2025a. Dingming Li, Hongxing Li, Zixuan Wang, Yuchen Yan, Hang Zhang, Siqi Chen, Guiyang Hou, Shengpei Jiang, Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models. arXiv preprint arXiv:2505.21500, 2025b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. MVBench: Comprehensive Multi-modal Video Understanding Benchmark. In CVPR, 2024. 14 Learning Situated Awareness in the Real World Jingli Lin, Runsen Xu, Shaohao Zhu, Sihan Yang, Peizhou Cao, Yunlong Ran, Miao Hu, Chenming Zhu, Yiman Xie, Yilin Long, Wenbo Hu, Dahua Lin, Tai Wang, and Jiangmiao Pang. MMSI-Video-Bench: Holistic Benchmark for Video-Based Spatial Intelligence. arXiv preprint arXiv:2512.10863, 2025. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. TempCompass: Do Video LLMs Really Understand Videos? In ACL, 2024. Steven Luck and Edward Vogel. The Capacity of Visual Working Memory for Features and Conjunctions. Nature, 390(6657):279281, 1997. Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Jieneng Chen, Celso de Melo, and Alan Yuille. 3DSRBench: Comprehensive 3D Spatial Reasoning Benchmark. In ICCV, 2025. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. SQA3D: Situated Question Answering in 3D Scenes. In ICLR, 2023. Bruce McNaughton, Francesco Battaglia, Ole Jensen, Edvard Moser, and May-Britt Moser. Path Integration and the Neural Basis of theCognitive Map. Nature Reviews Neuroscience, 7(8):663678, 2006. Maurice Merleau-Ponty, Donald Landes, Taylor Carman, and Claude Lefort. Phenomenology of Perception. Routledge, 2013. Pierre-Emmanuel Michon and Michel Denis. When and Why are Visual Landmarks Used in Giving Directions? In International Conference on Spatial Information Theory, pages 292305. Springer, 2001. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. VideoBench: Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models. In Computational Visual Media, 2025. Nadine B. Sarter and David D. Woods. Situation Awareness: Critical But Ill-Defined Phenomenon. The International Journal of Aviation Psychology, 1:4557, 1991. Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models. In ICLR, 2025. Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Gholamreza Haffari, and Yuan-Fang Li. An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models. In EMNLP, 2024. Daniel Simons and Daniel Levin. Change Blindness. Trends in cognitive sciences, 1(7):261267, 1997. Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. OpenAI GPT-5 System Card. arXiv preprint arXiv:2601.03267, 2026. Matthias Stangl, Ingmar Kanitscheider, Martin Riemer, Ila Fiete, and Thomas Wolbers. Sources of Path Integration Error in Young and Aging Humans. Nature Communications, 11(1):2626, 2020. Barbara Tversky. Cognitive Maps, Cognitive Collages, and Spatial Mental Models. In European Conference on Spatial Information Theory, pages 1424. Springer, 1993. Barbara Tversky. Spatial Cognition: Embodied and Situated. 2009. 15 Learning Situated Awareness in the Real World Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, and Jiangmiao Pang. EmbodiedScan: Holistic Multi-Modal 3D Perception Suite Towards Embodied AI. In CVPR, 2024a. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. LVBench: An Extreme Long Video Understanding Benchmark. In ICCV, 2025. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs. In NeurIPS, 2024b. Thomas Wolbers and Mary Hegarty. What Determines Our Navigational Abilities? Trends in Cognitive Sciences, 14(3):138146, 2010. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. LongVideoBench: Benchmark for Long-context Interleaved Video-Language Understanding. In NeurIPS, 2025. Peiran Xu, Sudong Wang, Yao Zhu, Jianing Li, and Yunjian Zhang. SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition. arXiv preprint arXiv:2511.21471, 2025. Zihui Xue, Mi Luo, and Kristen Grauman. Seeing the Arrow of Time in Large Multimodal Models. In NeurIPS, 2025. Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, and Joyce Chai. 3D-GRAND: Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination. In CVPR, 2025a. Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces. In CVPR, 2025b. Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Daohan Lu, Rob Fergus, Yann LeCun, Li Fei-Fei, and Saining Xie. Cambrian-S: Towards Spatial Supersensing in Video. arXiv preprint arXiv:2511.04670, 2025c. Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Ruoyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs. In AAAI, 2026. Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, and Li Fei-Fei. Spatial Mental Modeling from Limited Views. In NeurIPS, 2025. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. MMMU-Pro: More Robust Multi-discipline Multimodal Understanding Benchmark. In ACL, 2025. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S. Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. In ICLR, 2023. 16 Learning Situated Awareness in the Real World Hongjie Zhang, Lu Dong, Yi Liu, Yifei Huang, Yali Wang, Limin Wang, and Yu Qiao. LvBench: Benchmark for Long-form Video Understanding with Versatile Multi-modal Question Answering. In IJCV, 2025a. Weichen Zhang, Zile Zhou, Xin Zeng, Xuchen Liu, Jianjie Fang, Chen Gao, Yong Li, Jinqiang Cui, Xinlei Chen, and Xiao-Ping Zhang. Open3D-VQA: Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space. arXiv preprint arXiv:2503.11094, 2025b. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. LLaVA-Video: Video Instruction Tuning With Synthetic Data. In TMLR, 2025c. Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, and Arman Cohan. MMVU: Measuring Expert-Level Multi-Discipline Video Understanding. In CVPR, 2025. Shijie Zhou, Alexander Vilesov, Xuehai He, Ziyu Wan, Shuwang Zhang, Aditya Nagachandra, Di Chang, Dongdong Chen, Xin Eric Wang, and Achuta Kadambi. VLM4D: Towards Spatiotemporal Awareness in Vision Language Models. In ICCV, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, et al. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models. arXiv preprint arXiv:2504.10479, 2025. Zhu Ziyu, Ma Xiaojian, Chen Yixin, Deng Zhidong, Huang Siyuan, and Li Qing. 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment. In ICCV, 2023. 17 Learning Situated Awareness in the Real World"
        },
        {
            "title": "Contents",
            "content": "A Experiment Setup 20 A.1 Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 A.2 Implementation Details for Model Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Prompts B.1 System Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2 Evaluation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.3 Answer Extraction Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Baselines",
            "content": "24 C.1 Blind LLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Socratic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.2.1 Caption Generation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.2 Socratic Model Evaluation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 C.2.3 Example Inputs for the Socratic Model Baseline . . . . . . . . . . . . . . . . . . . . . 28 C.3 Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Video Filming Protocol and Meta Information Annotation",
            "content": "30 D.1 Video Filming Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 D.1.1 In-Place Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 D.1.2 Manhattan-Style Piecewise Linear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.1.3 Simple Shape Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 D.1.4 Extra Video Collections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Meta Information Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Data Analysis",
            "content": "35 E.1 Video Duration Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 E.2 Question Scene Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 E.3 Key Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Sensitivity Analysis",
            "content": "37 F.1 Sensitivity to Number of Input Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 18 F.2 Sensitivity to Frame Sampling Rate (FPS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Common Failure Cases 39 G.1 Self-Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.1.1 Example 36 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.1.2 Example 59 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.1.3 Example 103 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.2 Relative Direction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2.1 Example 10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 G.2.2 Example 413 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 G.2.3 Example 810 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 G.3 Route Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 G.3.1 Example 151 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 G.3.2 Example 225 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4 Reverse Route Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 G.4.1 Example 168 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 G.4.2 Example 196 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 G.5 Spatial Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 G.5.1 Example 53 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 G.5.2 Example 65 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.6 Spatial Affordance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 G.6.1 Example 105 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 G.6.2 Example 147 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "Back to the First Page",
            "content": "A. Experiment Setup A.1. Model Configuration Table 4. Model Configurations for Evaluation. Unset values indicate that their default values are being used. Configurations are based on official model repositories where available. Temp.: temperature. : GPT-5 mini only accepts temperature value of 1.0. : GPT-5.2 does not support adjusting the temperature or top-p parameters. : The thinking_level parameter for Gemini 3 Pro was set to low. When configured to dynamic, Gemini 3 Pro consistently generated reasoning traces that exceeded the output context window, resulting in incomplete or Null responses. Model API Checkpoint / HF Checkpoint Max New Tokens Temp. Top-P Top-K Sampling Rate (f ps) Thinking Level 1.0 1.0 1.0 1. Proprietary Multimodal Foundation Models Gemini 3 Pro Gemini 3 Flash Gemini 2.5 Pro Gemini 2.5 Flash GPT-5.2 GPT-5 mini gemini-3-pro-preview gemini-3-flash-preview gemini-2.5-pro gemini-2.5-flash gpt-5.2-2025-12-11 gpt-5-mini-2025-08-07 Open-Source Multimodal Foundation Models Qwen3-VL 235B Qwen3-VL 32B Qwen3-VL 30B Qwen3-VL 8B Qwen2.5-VL 72B Qwen2.5-VL 32B Qwen2.5-VL 7B LLaVA-NeXT-Video 32B LLaVA-Video 7B LLaVA-Video 72B LLaVA OneVision 72B LLaVA OneVision 7B InternVL3 38B InternVL3 14B InternVL3 8B InternVL2 40B InternVL2 8B qwen3-vl-235b-a22b-thinking qwen3-vl-32b-thinking qwen3-vl-30b-a3b-thinking qwen3-vl-8b-thinking qwen2.5-vl-72b-instruct qwen2.5-vl-32b-instruct qwen2.5-vl-32b-instruct LLaVA-NeXT-Video-32B-Qwen LLaVA-Video-7B-Qwen2 LLaVA-Video-72B-Qwen2 llava-onevision-qwen2-72b-ov llava-onevision-qwen2-7b-ov InternVL3-38B InternVL3-14B InternVL3-8B InternVL2-40B InternVL2-8B 16384 16384 16384 16384 16384 16384 16384 16384 16384 16384 8192 8192 8192 32768 32768 32768 32768 32768 32768 32768 32768 8192 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1. 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 low dynamic dynamic dynamic medium medium medium medium medium"
        },
        {
            "title": "Back to the First Page",
            "content": "A.2. Implementation Details for Model Inference All MFMs are evaluated in zero-shot setting across all tasks, consistent with prior work (Wang et al., 2024b, Shangguan et al., 2025, Zhao et al., 2025, Yue et al., 2025, Yang et al., 2025b). Whenever possible, we use the official code provided by each model for video preprocessing. Proprietary models and Qwen-series models are evaluated via their official API services1. In the evaluation prompt ( B.2), models are instructed to return responses in JSON-like format that includes both the selected multiple-choice option and the corresponding reasoning trace, enabling structured analysis and automatic parsing. When standard regularexpressionbased parsing fails, we employ GPT-4o-mini to extract the multiple-choice answers; the extraction prompt is provided in B.3. For thinking-enabled models, the thinking_level parameter for Gemini 3 Flash, Gemini 2.5 Pro, and Gemini 2.5 Flash is set to the default dynamic mode2, allowing the model to adapt its reasoning budget based on task complexity. For Gemini 3 Pro, thinking_level is set to low, as the dynamic setting consistently produces reasoning traces that exceed the output context window, leading to incomplete or null responses. For GPT-5.2, the thinking level is set to medium. For the Qwen3-VL series, the thinking variants are used instead of the instruct variants, with thinking_level set to medium. 1OpenAI, Gemini, and Qwen-series 2Gemini thinking mode"
        },
        {
            "title": "Back to the First Page",
            "content": "B. Prompts B.1. System Prompt System Prompt You are the person wearing the AR glasses. All videos are recorded from your first-person perspective. Treat the camera's movement as your own head and body movement. Only reason about what is visible or inferable from the egocentric video. Do not assume any external knowledge beyond what appears in the video. B.2. Evaluation Prompt We follow the evaluation prompt used in TOMATO (Shangguan et al., 2025)."
        },
        {
            "title": "Evaluation Prompt",
            "content": "You will be provided with sequence of frames uniformly sampled from video, the frames are provided in chronological order of the video. Analyze these frames and provide the answer to the question about the video content. Answer the multiple-choice question about the video content. You must use these frames to answer the multiple-choice question; do not rely on any external knowledge or commonsense. <question> {question} </question> <options> {index2ans} </options> PLEASE ANSWER THE QUESTION WITH ONLY THE OPTIONS PROVIDED. When answering, please follow the template provided: \"options\": <your choice> \"thinking_trace\": <your thinking trace>"
        },
        {
            "title": "Back to the First Page",
            "content": "B.3. Answer Extraction Prompt We follow the answer extraction prompt used in TOMATO (Shangguan et al., 2025). Answer Extraction Prompt You are given response, list of multiple-choice options, and index2answer mapping. You are required to extract the letter option from GPT. <response> {response} </response> <all_choices> {all_choices} </all_choices> <index2answer> {index2ans} </index2answer> Only output the single parsed letter from the response. No other texts are needed. If you think no options can match the index2answer dictionary, randomly select one letter. Your extracted letter is:"
        },
        {
            "title": "Back to the First Page",
            "content": "C. Baselines We summarize the baseline configurations in Table 2. Additional details for the Blind LLM baseline are provided in C.1, the Socratic model in C.2, and the human evaluation in C.3. C.1. Blind LLM Blind LLM refers language-only model that does not receive any visual input, such as images or videos, during the inference stage. Instead, the model is provided solely with the textual component of the task, including the question, instructions, and answer options. This setting isolates the contribution of linguistic priors and textual reasoning, serving as diagnostic baseline for identifying language-only shortcuts and estimating models performance without perceptual information. By comparison with vision-enabled settings, this baseline quantifies the extent to which visual information contributes to task performance. We follow the same blind LLM prompt used in HourVideo (Chandrasegaran et al., 2024)."
        },
        {
            "title": "Blind LLM Prompt",
            "content": "You are tasked with assisting in answering few difficult questions about short egocentric videos. The goal is to establish baseline for how many multiple-choice questions (MCQs) can be accurately answered without watching the videos. This may involve identifying poorly crafted distractor options or leveraging general knowledge and logical reasoning when the questions themselves are straightforward. You are STRICTLY expected to choose the correct MCQ answer based on your best judgment and provide one-line reason for your selection. <question> {question} </question> <options> {index2ans} </options> DO NOT GENERATE ANSWER SUCH AS 'NOT POSSIBLE TO DETERMINE.'"
        },
        {
            "title": "Back to the First Page",
            "content": "C.2. Socratic Model Socratic model (Zeng et al., 2023) is framework in which visual perception and textual reasoning are decoupled through two-stage pipeline. In the first stage, for video question-answering task, video captioning model converts the visual input into language-only caption. In the second stage, language model receives the generated caption together with the question and answer options, and performs reasoning entirely in the textual domain. Under this setting, the reasoning model does not access raw visual inputs; instead, all perceptual information is compressed through the caption. This framework evaluates how informative the visual input remains when mediated solely through textual descriptions. In our baseline evaluation, we use GPT-5.2 (Singh et al., 2026) for both video captioning and caption-based question answering. Videos are sampled at 2 fps, and the sampled frames are stitched into 4 4 image grids, with each grid containing 16 frames. For each video, multiple 4 4 grids are provided to the captioning model to generate textual description. We follow the same caption generation and evaluation prompts use in HourVideo (Chandrasegaran et al., 2024). Caption generation prompt is provided in C.2.1. Evaluation prompt using video caption is provided in C.2.2. Example input for caption generation is provided in C.2.3."
        },
        {
            "title": "Back to the First Page",
            "content": "C.2.1. Caption Generation Prompt Caption Generation Prompt MAIN INSTRUCTIONS: Your task is to analyze video frames extracted uniformly from short ego-centric video for detailed video understanding exercise. will provide sequence of images sampled at 2 frames per second (2 fps) from this video. Examine the video frames closely and generate comprehensive caption by strictly following the steps below: Step 1: **Scene Context** Observe the frames. What is the primary setting and activity in the video? Step 2: **Motion Description** Identify and describe any significant motion or actions taking place across the frames. Step 3: **Spatial Relationship Analysis** Examine and report the spatial relationships between key objects or entities. Describe the positioning and orientation of each element relative to others. Step 4: **Detailed Object Analysis** List the key objects and entities. Describe visible attributes such as color, shape, texture, and other notable features with precision (e.g., materials, signage text, tool types, object parts). Step 5: **Temporal Relationship Context** Describe any observable temporal progression or changes across the sequence (e.g., before/after changes, object movement, state changes, action sequences). If no meaningful change is visible, state that explicitly. Step 6: **Additional Detail-Oriented Observations** Add any other concrete, detail-oriented observations that could help answer fine-grained questions later (e.g., small objects, relative distances, occlusions, left/right placement, openings/closures, item locations), but do not speculate beyond what is visible. Step 7: **Summary** Provide concise yet comprehensive summary capturing the key elements from Steps 1-6. GUIDELINES: 1. Return your results in paragraph format with the following fields: - Scene Context - Motion Description - Spatial Relationship Analysis - Detailed Object Analysis - Temporal Relationship Context - Additional Details - Summary 2. The total length of the output must not exceed 200 words."
        },
        {
            "title": "Back to the First Page",
            "content": "C.2.2. Socratic Model Evaluation Prompt Socratic Model Evaluation Prompt You will be provided with textual caption that describes the content of video. The caption is derived from the video and reflects its observable visual and spatial information. Analyze the caption and answer the multiple-choice question about the video content. You must use only the information contained in the provided caption to answer the question; do not rely on any external knowledge, assumptions, or commonsense beyond what is explicitly stated in the caption. <video_caption> {video_caption} </video_caption> <question> {question} </question> <options> {index2ans} </options> PLEASE ANSWER THE QUESTION USING ONLY THE OPTIONS PROVIDED. When answering, strictly follow the template below: \"options\": <your choice> \"thinking_trace\": <your reasoning based solely on the caption>"
        },
        {
            "title": "Back to the First Page",
            "content": "C.2.3. Example Inputs for the Socratic Model Baseline We provide the example input for caption generation in Figure 8. Figure 8. Example Input to the Socratic Baseline. Each 4 4 grid contains 16 frames sampled at 2 fps, with frame indices shown in the bottom-left corner of each frame. C.3. Human Evaluation To establish human performance upper bound on the benchmark, two graduate students are recruited to complete the full set of evaluation tasks. Each annotator answers all questions independently, with unlimited time and full access to the corresponding videos. Annotators are instructed to rely solely on the visual information provided, without external tools or discussion. breakdown of individual annotator performance is reported in Table 11. Human evaluation interface in shown in Figure 9. Table 5. Human Baseline Performance on SAW-Bench. Model Annotator 1 Annotator 2 Average All 91.07 92.03 91.55 SelfLocalization Relative Direction 98.50 89.50 94.00 86.69 92.09 89.39 Route Shape 98.53 96.70 97.62 Reverse Route Plan 91.70 94.32 93. Spatial Memory 94.00 83.00 88.50 Spatial Affordance 76.54 81.48 79."
        },
        {
            "title": "Back to the First Page",
            "content": "Figure 9. Human Evaluation Interface. Each annotator answers all questions independently, with unlimited time and full access to the corresponding videos. Annotators are instructed to rely solely on the visual information provided, without external tools or discussion."
        },
        {
            "title": "Back to the First Page",
            "content": "D. Video Filming Protocol and Meta Information Annotation D.1. Video Filming Protocol We define structured recording protocol to ensure consistent coverage of observer-centric spatial reasoning primitives while maintaining controllable trajectory complexity. Each video is associated with predefined movement pattern, set of spatial queries, and deterministic ground-truth answers derived from the recording plan. We divide the recording protocol into four trajectory categories: (1) In-place orientation (D.1.1), in which the camera wearer remains at fixed spatial location and only rotates their viewpoint; (2) Manhattan-style piecewise linear trajectories (D.1.2), in which the camera wearer follows predefined path with two turns; (3) Simple geometric trajectories (D.1.3), in which the camera wearer moves along canonical geometric paths; and (4) Extra video collections (D.1.4), which include additional recordings designed to support the Spatial Memory and Spatial Affordance tasks."
        },
        {
            "title": "Back to the First Page",
            "content": "D.1.1. In-Place Orientation Table 6. Video Filming Protocol for In-Place Orientation. Camera wearers remain at fixed spatial location (i.e., corner, side, or center of scene) and only changes their viewing orientation, without introducing translational motion. Whenever possible, we include instances from each spatial location to ensure coverage across different positional contexts. For each video, we asked one Self-Localization and one Route Shape question. Spatial Location Video ID Q1 Corner Corner 2 Corner 3 Corner 4 Side 1 Side 2 Side Side 4 Center 0 1 2 4 5 6 7 8 Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? Self-Localization: Are you positioned near the corner, along the side, or near the center of the Scene Name? A1 Corner Corner Corner Corner Side Side Side Side Q2 Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Route Shape: Whats the shape of my moving trajectory? In-place orientation Center Route Shape: Whats the shape of my moving trajectory? In-place orientation"
        },
        {
            "title": "Back to the First Page",
            "content": "D.1.2. Manhattan-Style Piecewise Linear Table 7. Video Filming Protocol for Manhattan-Style Piecewise Linear. Observers moving trajectories are visualized using egocentric action arrows. For each video, we asked two Relative Direction questions, one Reverse Route Plan question, and one Route Shape question. Trajectory Video ID Q1 A1 Q2 Q3 A4 18 19 20 22 23 24 25 26 28 29 30 31 32 Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? left / front back right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, straight. straight, and continue straight. then go Turn left, go then turn right Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear left / left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear front right / back left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear right / right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? left / front back left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? front / right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? back left / back right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? back / left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear back / left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? front right / back right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? back / left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear back right / back left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear left / right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear back left / back left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn left and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear right / left Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn right, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? back right / back right Reverse Route Plan: From my viewpoint at the end of the video, how can go back to my starting point? Turn around, go straight. Turn left, go straight, then turn right and continue go straight. Route Shape: Whats the shape of my moving trajectory? Manhattan-style piecewise linear"
        },
        {
            "title": "Back to the First Page",
            "content": "D.1.3. Simple Shape Trajectories Table 8. Video Filming Protocol for Other Simple Trajectories. For each video, we asked two Relative Direction questions and one Route Shape question. Video ID Q1 A1 Trajectory Straight Straight w/ turning head frequently (L-Shape) (L-Shape) (U-Shape) (U-Shape) (U-Shape) (U-Shape) Circle (Clockwise) Circle (Counter-clockwise) Zigzag Square Triangle 35 36 37 38 39 41 42 43 44 45 Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? Relative Direction: From my viewpoint at the beginning / end of this video, where am located at the end / beginning of the video? left / right Route Shape: Whats the shape of my moving trajectory? Straight front / back Route Shape: Whats the shape of my moving trajectory? Straight front right / back right Route Shape: Whats the shape of my moving trajectory? L-Shape front left / back left Route Shape: Whats the shape of my moving trajectory? L-Shape front / right Route Shape: Whats the shape of my moving trajectory? U-Shape back / right Route Shape: Whats the shape of my moving trajectory? U-Shape right / left Route Shape: Whats the shape of my moving trajectory? U-Shape right / right Route Shape: Whats the shape of my moving trajectory? U-Shape same location / same location same location / same location front / back same location / same location same location / same location Route Shape: Whats the shape of my moving trajectory? Circular Route Shape: Whats the shape of my moving trajectory? Circular Route Shape: Whats the shape of my moving trajectory? Route Shape: Whats the shape of my moving trajectory? Zigzag Square Route Shape: Whats the shape of my moving trajectory? Triangle D.1.4. Extra Video Collections In addition to the predefined observer trajectories described in D.1.1, D.1.2, and D.1.3, we instruct video collectors to record additional videos in each scene to support the Spatial Memory and Spatial Affordance tasks. For Spatial Memory, video collectors identify objects that have been moved or displaced within the scene over the course of the trajectory. For Spatial Affordance, video collectors assess whether queried action is physically feasible given the scenes spatial and physical constraints."
        },
        {
            "title": "Back to the First Page",
            "content": "D.2. Meta Information Annotation Ground-truth answers are derived directly from the predefined trajectory and viewpoint sequence, independent of visual appearance. Screenshot of annotation interface is provided in Figure 9. Self-Localization. Ground-truth answers are deterministically defined by the filming location of the observer within the scene (i.e., corner, side, or center). Since observer positions are predefined during data collection, labels of Self-Localization do not require human annotation. Relative Direction. Ground-truth answers are annotated by human annotators based on the relative spatial relationship between the observers starting and ending viewpoints along the trajectory. Each question is independently annotated by two annotators, and inter-annotator agreement is reported to assess annotation consistency. We annotate only trajectory-level meta information; in this way, all videos that share the same trajectory share the same relative-direction labels. Inter-annotator agreement score is reported in Table 9. Any annotation disagreements are resolved through discussion at the end of the annotation process. Route Shape. Ground-truth answers are deterministically specified by the predefined movement trajectory (e.g., straight, L-shape, U-shape, circular). Similar to Self-Localization, these labels are directly derived from the filming protocol and do not depend on visual interpretation. Reverse Route Plan. Ground-truth answers for reverse route planning are defined analogously to Relative Direction, describing the sequence of actions required to return from the final viewpoint to the starting viewpoint. These answers are annotated by human annotators to account for egocentric action interpretation, following the same annotation protocol as relative direction. Similarly, we annotate only trajectory-level meta information. Inter-annotator agreement score is reported in Table 9. Any annotation disagreements are resolved through discussion at the end of the annotation process. Spatial Memory. Ground-truth answers are specified during the filming process by intentionally moving or displacing selected objects along the trajectory. Since object state changes are controlled and recorded at capture time, no additional human annotation is required. Spatial Affordance. Ground-truth answers are likewise determined during filming by designing queries whose feasibility is unambiguous given the scenes physical layout and constraint. Similar to Spatial Memory, no additional human annotation is required. Table 9. Inter-Annotator Agreement Measured by Cohens Œ∫. Task Relative Direction Reverse Route Plan Cohens Œ∫ 0.987 0."
        },
        {
            "title": "Back to the First Page",
            "content": "E. Data Analysis E.1. Video Duration Distribution We provide video duration distribution of SAW-Bench in Figure 10. Figure 10. Video duration distribution of SAW-Bench."
        },
        {
            "title": "Back to the First Page",
            "content": "E.2. Question Scene Distribution We provide question scene distribution of SAW-Bench in Figure 11. Figure 11. Question scene distribution of SAW-Bench. E.3. Key Statistics We summarize key statistics of SAW-Bench in Table 10. Table 10. Key statistics of SAW-Bench. Statistics Unique Videos Indoor Outdoor Number of Scenes Number of Videos per scene (avg) Number of Videos in Unassigned Scene Total Questions Value 786 317 (40.33%) 469 (59.67%) 15 42 155 2,"
        },
        {
            "title": "Back to the First Page",
            "content": "F. Sensitivity Analysis F.1. Sensitivity to Number of Input Frames Figure 12. Sensitivity to the Number of Input Frames Across Reasoning Types. Each subplot reports accuracy as function of the number of input frames (8, 16, 32) for fixed reasoning type. Lines correspond to different open-source MFMs evaluated with varying temporal context lengths. Tasks that require longer-horizon spatial integration, such as Spatial Memory and Route Shape, generally benefit from increased temporal context, whereas short-horizon tasks (e.g., Self-Localization and Relative Direction exhibit earlier saturation and more limited improvements beyond 16 frames. Nevertheless, additional frames yield only marginal gains across all tasks."
        },
        {
            "title": "Back to the First Page",
            "content": "F.2. Sensitivity to Frame Sampling Rate (FPS) Table 11. Sensitivity to Frame Sampling Rate (FPS) Across Reasoning Types. We evaluate the top-performing model, Gemini 3 Flash, on SAW-Bench by varying the number of input frames through different temporal sampling rates (FPS {1, 2, 3}), and report performance across reasoning tasks. All other configurations are held constant, as detailed in Table 4. Overall, increasing the frame sampling rate does not substantially improve situated spatial reasoning. Notably, performance on Spatial Memory decreases at FPS = 3, potentially due to redundant or temporally condensed visual information, while Reverse Route Plan performs worst at FPS = 1, suggesting greater reliance on temporal visual context for this reasoning type. Model FPS All SelfLocalization Relative Direction Gemini 3 Flash Gemini 3 Flash Gemini 3 Flash 1 2 3 53.55 53.89 51.57 45.00 48.50 42.50 39.93 41.13 40. Route Shape 69.05 64.84 60.99 Reverse Route Plan Spatial Memory Spatial Affordance 55.90 61.57 63. 64.00 66.00 53.00 72.22 70.99 70."
        },
        {
            "title": "Back to the First Page",
            "content": "G. Common Failure Cases G.1. Self-Localization G.1.1. Example 36 Self-Localization 36 Are you positioned near the corner, along the side, or near the center of the lecture hall? A. Side B. Center C. Corner Answer: C. Corner Gemini 3 Flash \"options\": \"thinking_trace\": The video begins with view of the lecture hall from the back, positioned towards the right side. As the camera moves, it approaches the right-side wall, where an exit door is located. The camera then pans across the room from this side position, showing the rows of seats and the front of the hall from side perspective. Throughout the video, the camera remains along the side or near the back corner, never moving toward the center of the hall. Therefore, the position is along the side. Gemini 3 Pro Based on the video frames, here is the analysis of my position: 1. **Initial View (00:00.000 - 00:06.500):** The video starts with view of the lecture hall seating. The rows of chairs are to my left, and wall is immediately to my right. am walking along an aisle that runs parallel to the wall on the right side of the room. This clearly places me on the \"side\" of the seating area, not in the center or corner. 2. **Walking Path (00:06.500 - 00:12.000):** continue walking straight along this side aisle, passing an exit door on my right. The perspective of the rows of seats confirms am at the edge of the seating arrangement. 3. **Turning and Looking Back (00:12.000 - 00:35.500):** approach set of glass doors, look through them into lobby/atrium area, and then turn around to face the lecture hall again. When turn back (around 00:27.000 onwards), am looking down the same side aisle just walked up. The screen is far to the left, and the wall is now on my left. This confirms my location is along the side wall of the room. Therefore, throughout the video, am positioned along the side aisle of the lecture hall."
        },
        {
            "title": "Back to the First Page",
            "content": "\"options\": \"thinking_trace\": The video begins with the camera moving along an aisle with wall immediately to the right and rows of seats to the left. This indicates position on the side of the room. The camera continues down this path, passing an exit sign on the right wall. Eventually, the camera turns around and looks back down the length of the room, still positioned next to the wall, confirming the location is along the side of the lecture hall rather than in the center or tucked into corner. Gemini 2.5 Pro \"options\": \"thinking_trace\": The video begins with view across many rows of empty seats in lecture hall, with large window wall on the far side. then pan my head to the right. As pan, it becomes clear that am standing in an aisle. To my left are the rows of seats, and to my right is solid wall. continue panning right along this wall until reach set of double doors with an EXIT sign above them. look through the glass doors for moment before panning back to the left, retracing my view along the side wall and back towards the front of the lecture hall. Since am in an aisle with seats on one side and wall with an exit door on the other, my position is along the side of the lecture hall. GPT-5.2 \"options\": \"thinking_trace\": The view stays next to the right wall/aisle (windows and side wall with EXIT door visible), looking across rows toward the screen, indicating side position rather than the center or corner. Qwen3-VL 235B \"options\": \"thinking_trace\": The camera movement along the side of the lecture hall, passing by rows of seats and windows, indicates side position. Qwen3-VL 32B \"options\": \"thinking_trace\": The camera moves along the side of the lecture hall, adjacent to the dark panel wall with the exit sign and doors, indicating position along the side. Qwen3-VL 30B \"options\": \"thinking_trace\": In the frames, the camera movement shows the perspective along the rows of seats in the lecture hall. The view captures the side view of the seating arrangement, indicating the position is along the side of the lecture hall. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video shows view of the lecture hall from position that is aligned with the side wall, as evidenced by the proximity to the exit sign and the glass door leading to another area. The perspective does not show central view of the hall or corner view where two walls would meet. Therefore, the position is along the side."
        },
        {
            "title": "Back to the First Page",
            "content": "G.1.2. Example 59 Self-Localization 59 Are you positioned near the corner, along the side, or near the center of the lecture hall? A. Center B. Side C. Corner Answer: B.Side Gemini 3 Flash \"options\": \"thinking_trace\": The video begins with view of squash court. can see two white walls meeting at corner, and wooden floor with red lines. black bag is visible on the floor near the corner. As the camera pans, it reveals more of the court, including glass walls and people outside, but the primary focus remains on the corner of the court. The red lines on the floor, which define the service box and other court areas, are clearly visible, indicating that am positioned within the playing area. My perspective is consistently showing the intersection of two walls and the floor, strongly suggesting am near corner of the court, rather than the center or just along side. Gemini 3 Pro Based on the video frames, here is the analysis of my position: 1. **Initial View (00:00.000 - 00:03.000):** The video starts with view of the front wall and the left side wall meeting at corner. black bag is on the floor near this corner. The perspective suggests am standing somewhat back from the front wall, looking towards the front-left corner. 2. **Panning Right (00:03.000 - 00:13.000):** As turn my head to the right, see the glass back wall of the court. Through the glass, can see gym area with exercise equipment. The camera pans across the back wall. 3. **Focus on the Wall (00:13.000 - 00:27.000):** The camera then moves very close to the right side wall (or possibly the front wall again, but the texture suggests side wall heavily marked with ball impacts). The view is extremely close-up, showing the scuff marks in detail. This indicates have moved right up to wall. 4. **Final Pan (00:27.000 - 00:32.000):** The camera pulls back and pans left again, revealing the front wall and the front-left corner where the bag is located. The angle of view clearly shows two walls meeting (front and left side). **Conclusion:** throughout the video, the cameras perspective is anchored near the intersection of two walls. Specifically, the view starts looking into corner, pans around, and ends looking back into that same corner"
        },
        {
            "title": "Back to the First Page",
            "content": "(front-left). The proximity to the walls and the angle of view into the intersection of the front and side walls strongly indicate position near the corner of the court. \"options\": \"thinking_trace\": The video begins with clear view of the intersection between the front wall and the left side wall, where black bag is placed on the floor. This immediately establishes perspective directed towards corner. As the video progresses, the camera pans right to show the glass back wall and then moves in very close to wall covered in ball marks, before panning back to the original corner view. The field of view captures the convergence of two perpendicular walls for significant portion of the clip, and the camera movement pivots around this location. Being able to see the two walls meeting at an angle directly in front or slightly to the side is characteristic of being positioned in or near corner. Therefore, the position is best described as near the corner. Gemini 2.5 Pro \"options\": \"thinking_trace\": The video begins with me looking at the front-left corner of the squash court. then pan my head to the right, sweeping my view across the front wall, past the glass door and the back glass wall, along the right-side wall, and then back towards the front-left corner. This wide, panoramic sweep, which captures view of all four walls from relatively stable standing position, indicates that am positioned somewhere in the middle of the court. If were in corner or along side wall, the perspective and the way the walls appear during the pan would be significantly different and more skewed. GPT-5.2 \"options\": \"thinking_trace\": The view repeatedly shows the junction of two walls and the floor at corner, and the camera moves very close alongside the wall, indicating position near the courts corner rather than centered or mid-side. Qwen3-VL 235B \"options\": \"thinking_trace\": The video frames show the camera moving towards and focusing on the corner where two walls of the squash court meet (evident in frames 2729 and 3032,which highlight the intersection of walls and the bag positioned in the corner). This indicates the person is near the corner of the court. Qwen3-VL 32B \"options\": \"thinking_trace\": The video frames show consistent view of the squash court from corner perspective. The walls and floor markings are visible in way that indicates the camera is positioned near the corner of the court, as opposed to the center or along the side. The angle and proximity to the walls suggest corner position. Qwen3-VL 30B \"options\": \"thinking_trace\": The video shows squash court with red lines on the walls. In the initial frames, bag is visible in the corner where two walls meet. As the camera moves, the corner area (where two walls intersect) is observed, indicating the person is near the corner of the squash court. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video starts with view of the corner of the squash court, showing two walls meeting at right angle. As the camera pans around, it captures different parts of the court but initially focuses on the corner area where bag is placed. This indicates that the starting position is near the corner of the squash court."
        },
        {
            "title": "Back to the First Page",
            "content": "G.1.3. Example 103 Self-Localization 103 Are you positioned near the corner, along the side, or near the center of the kitchen? A. Center B. Side C. Corner Answer: C. Corner Gemini 3 Flash \"options\": \"thinking_trace\": The video shows panoramic view of the kitchen, starting from the stove and range hood, panning across to dining area, then to kitchen island, then along the main countertop with sink and window, and finally back to the starting point. This 360-degree view indicates that the person is standing in the central open area of the kitchen, surrounded by the various functional zones (cooking, dining, prep, cleaning). Gemini 3 Pro Based on the video frames, here is the analysis of the cameras position: The video begins with close-up view of white cabinet and stainless steel range hood. As the camera pans to the right, it reveals kitchen island in the foreground, brick half-wall separating the kitchen from dining area, and refrigerator further back. The camera then continues to pan right, showing sink area with window covered by lace curtains. Finally, the camera pans back towards the left, ending on the cabinets above the stove area where it started. Throughout the video, the camera rotates to show almost 360-degree view of the kitchen. The perspective originates from point that allows viewing the stove on one side, the island in the middle, and the sink/window on the other side. The camera is clearly situated in the middle of the room, pivoting around to show the surrounding features (stove, island, fridge, sink). This central vantage point allows for comprehensive view of all the kitchens perimeter features. \"options\": \"thinking_trace\": The video starts facing stove and range hood. The camera then pans right, showing kitchen island directly in front of the viewer. Continuing the pan to the right, the camera shows refrigerator and then sink area with window. The camera continues to pan right, eventually showing the upper cabinets and returning near the starting point. The ability to pivot and see all these different walls and features (stove wall, island, fridge"
        },
        {
            "title": "Back to the First Page",
            "content": "wall, sink wall) from single stationary point strongly suggests the person is standing in the middle of the kitchen floor. Being in corner or on the side would restrict the field of view and make such panoramic rotation difficult without moving the body significantly across the room. Therefore, the position is near the center. Gemini 2.5 Pro \"options\": \"thinking_trace\": The video shows full 360-degree pan of the kitchen. start by looking at the stove and cabinets. As turn, see kitchen island, refrigerator, U-shaped counter with sink, and then back to the stove. The ability to perform complete rotation and see all the major elements of the kitchen surrounding me indicates that am standing in central position within the room, likely in the open space between the stove, the island, and the sink counter. GPT-5.2 \"options\": \"thinking_trace\": The view stays close to perimeter fixtures (stove/range hood, cabinets, counters, sink) along the wall rather than an open central area, indicating Im positioned along the side of the kitchen. Qwen3-VL 235B \"options\": \"thinking_trace\": The video shows kitchen with central island (visible in frames 9-15). The camera movement around the island and the layout of the kitchen (with the island as central feature) indicates the person is near the center of the kitchen. Thus, the correct answer is A. Qwen3-VL 32B \"options\": \"thinking_trace\": The video shows kitchen layout with central island (visible in frames 10-15). The camera movement captures the island as central point in the kitchen, indicating the person is positioned near the center. While the video starts near the stove (a side/corner), the overall movement and focus on the island suggest the primary position is near the center. Qwen3-VL 30B \"options\": \"thinking_trace\": The video shows the kitchen layout with central island (granite-topped, visible in frames 10-12). The perspective moves through the space, indicating proximity to this central feature. Thus, the position is near the center of the kitchen. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video starts with view of the kitchen cabinets and then pans to show the stove, dining area, and other parts of the kitchen. As the camera moves, it captures the side of the kitchen including the sink, window, and countertop. The perspective suggests that the camera is positioned along the side of the kitchen rather than in the center or near corner. Therefore, the answer is Side."
        },
        {
            "title": "Back to the First Page",
            "content": "G.2. Relative Direction G.2.1. Example 10 Relative Direction 10 From my viewing point at the beginning of this video, where am located at the end of the video? A. Front right B. Front left C. Front D. Same location Answer: C. Front Gemini 3 Flash \"options\": \"thinking_trace\": At the beginning of the video (00:00.000), am standing on lawn facing complex of residential buildings. begin by walking forward towards the buildings. Around 00:14.500, turn right and walk across the lawn, parallel to the buildings. At 00:30.000, turn right again, now facing away from the buildings and towards sports field with blue fence. continue walking in this direction until the end of the video (00:46.000). Relative to my starting position and initial orientation (facing the buildings), have moved forward and significantly to the right. Therefore, my final location is to the front right of my starting point. Gemini 3 Pro \"options\": \"thinking_trace\": At the beginning of the video, am standing on grassy lawn facing large beige apartment complex. The building is directly in front of me, and am positioned somewhat centrally relative to the courtyard area shown. As the video progresses, begin to move forward and slightly to the right, walking across the grass. By the middle of the video (around 00:15.000), have moved closer to the building and am now positioned more to the right side of the initial view, looking towards breezeway or passage between the buildings. Towards the end of the video (00:30.000 onwards), turn my head to the right and the view shifts completely away from the building to large open field with blue fence and mountains in the distance. However, relative to my starting position facing the building, my physical location has moved forward and to the right side of the initial scene before turned my head. Therefore, am located at the Front right relative to my starting point."
        },
        {
            "title": "Back to the First Page",
            "content": "Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. **Initial Position (00:00):** am standing on large grassy area, facing U-shaped, multi-story building. My position is relatively central on the lawn. 2. **Movement Analysis (00:00 - 00:14):** begin by panning my head left and then walk forward and to the left, moving closer to the left wing of the building complex. 3. **Movement Analysis (00:14 - 00:30):** then pan my head and body to the right, sweeping across the entire front of the building complex. As pan, the perspective changes, indicating am also moving physically to the right across the lawn. 4. **Movement Analysis (00:30 - 00:46):** My pan continues past the right side of the building, revealing large sports field behind fence. stop with this field in view. 5. **Final Position (00:46):** am now standing on the right side of the grassy area (relative to my starting point), looking at the sports field. 6. **Comparison:** started in central position on the lawn. ended up on the right side of the lawn. Therefore, relative to my initial viewing point, my final location is to the front and right. GPT-5.2 \"options\": \"thinking_trace\": start facing the building across the lawn, then move forward toward the landscaping/sculpture and continue shifting to the right side of the courtyard, ending near the right side looking out toward the fenced sports field, so my end position is in front-right of the starting viewpoint. Qwen3-VL 235B \"options\": \"thinking_trace\": The video starts with view of building and grass. As the frames progress, the camera moves and turns, eventually showing sports field in the final frames. From the initial viewing point, the final position corresponds to the front right direction, as the movement and rotation of the camera (from the first-person perspective) lead to perspective that is to the right of the original viewpoint. Qwen3-VL 32B \"options\": \"thinking_trace\": The video starts with view of the building on the left side of the frame. By the end, the camera has moved to the right, showing the blue fence and distant mountains, which is the front right area from the initial viewing point. Qwen3-VL 30B \"options\": \"thinking_trace\": The video starts with view of building on the right side (frame 0). As the sequence progresses, the camera moves across the grassy area, showing different angles of the neighborhood. By the end of the video (frame 46), the scene shifts to sports field with blue fence and mountains in the background. Comparing the initial viewpoint (building on the right) to the final viewpoint (sports field), the location at the end is positioned in the front-right relative to the starting point. Thus, the answer is A. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video starts with view of building and its surroundings. As the video progresses, the camera pans around the area but does not move to different location. The final frames show the same general area as the beginning, indicating that the viewing point has not changed significantly. Therefore, the correct answer is D: Same location."
        },
        {
            "title": "Back to the First Page",
            "content": "G.2.2. Example 413 Relative Direction 413 From my viewing point at the end of this video, where am located at the beginning of the video? A. Back right B. Right C. Same location D. Front right Answer: B. Right Gemini 3 Flash \"options\": \"thinking_trace\": At the beginning of the video (00:00.000), am standing under covered walkway, looking out towards large plaza with red sculpture and tall skyscraper. As the video progresses, walk out into the plaza, turn around, and then walk back under the covered walkway, but continue walking further into it. At the end of the video (00:39.000), am looking down the walkway towards street. From this final viewing point, the plaza (and thus my starting location) is to my right and further ahead towards the edge of the walkway. Therefore, my starting location is to the front right relative to my final position. Gemini 3 Pro The camera starts under covered walkway, looking out at plaza with red sculpture on the left. As the video progresses, the camera moves forward and turns to the right, walking along the covered walkway. By the end of the video, the camera is further down the walkway, looking towards street intersection. Therefore, relative to the final position, the starting position was behind and to the right. \"options\": \"thinking_trace\": The video begins with the camera positioned under covered walkway, facing plaza with prominent red sculpture to the left and tall building ahead. As the video progresses, the camera moves forward and pans right, following person walking away. By the end of the video (around 00:39.000), the camera has advanced significantly down the walkway and is now facing street scene with brick buildings. Looking back from this final viewpoint, the initial location (where the red sculpture was visible to the left) would be behind and to the right of the current position. Therefore, the starting location is Back right relative to the ending location.\""
        },
        {
            "title": "Back to the First Page",
            "content": "Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. **Analyze the initial position (00:00):** am standing under covered walkway, looking out into large, open plaza. There is tall building across the plaza and large red sculpture to my left. 2. **Analyze the movement:** walk forward, out from under the cover and into the plaza. move across the plaza towards the tall building. 3. **Identify the key turn (00:21):** After crossing part of the plaza, make distinct turn to my right. then begin walking along the side of the tall building, which is now on my right. 4. **Analyze the final position (00:39):** have continued walking forward from my right turn and am now under another covered walkway along the side of the tall building. My final view is looking forward down this walkway towards street. 5. **Determine the relative positions:** My path was an shape: walked forward, made 90-degree right turn, and then walked forward again. 6. **Relate the start to the end:** Imagine am at my final position, facing the direction am at the end of the video. To locate my starting point relative to this final position and orientation, have to consider the path took. The starting point is behind the corner turned. Since turned right to get to my final path, the initial path is to the right of my current backward direction. Therefore, from my final vantage point, the starting point is behind me and to my right. 7. **Evaluate the options:** * A: Back right - This matches the conclusion that the starting point is behind me and to my right, relative to my final position and direction of view. * B: Right - This would imply the starting point was directly to my side, which is incorrect as walked forward from it. * C: Same location - This is incorrect as clearly moved across plaza. * D: Front right - This is the opposite of the correct direction. Therefore, the correct option is Back right. GPT-5.2 \"options\": \"thinking_trace\": By the end Im facing down the covered walkway toward the street; earlier was at the plaza-side opening (with the red sculpture visible). Since walked/turned away from that area, the beginning location is behind me and slightly to the right from the final viewpoint. Qwen3-VL 235B \"options\": \"thinking_trace\": At the end of the video (frame 40), the viewer is in covered walkway facing street. The beginning of the video (frame 1) shows the viewer in different covered area (under an overhang) looking out toward plaza with red sculpture and tall building. As the video progresses, the viewer moves from the initial covered area through the plaza into the final covered walkway. From the end position, the beginning location is behind the viewer, and considering the spatial orientation (the red sculpture was on the left initially, but movement direction suggests the origin is to the back right), the correct answer is A: Back right. Qwen3-VL 32B \"options\": \"thinking_trace\": The video starts with view of covered walkway and large building in the background. As the camera moves, it reveals more of the surroundings, including red sculpture and other buildings. The camera then pans to the right, showing person walking away from the viewer. Finally, the camera returns to the initial position, indicating that the starting point is back to the right of the final viewing point. Qwen3-VL 30B \"options\": A"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": The video shows the camera moving from an initial position under covered structure (frame 0) toward the end of covered walkway (frame 39). At the end of the video, the viewing point is in the covered walkway facing the street. The beginning of the video was at the start of this journey, which corresponds to the back-right relative to the current end position due to the forward movement through the plaza and into the walkway, with the red sculptures position on the left in early frames indicating the direction of movement. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video starts with view from under an overhang looking out onto plaza. As the video progresses, the camera moves forward and turns to the right, revealing more of the plaza and surrounding buildings. The final frames show different perspective of the plaza and the street beyond. However, the starting point is still visible in the background of the final frames, indicating that the camera has not moved significantly from its original location. Therefore, the correct answer is C: Same location."
        },
        {
            "title": "Back to the First Page",
            "content": "G.2.3. Example 810 Relative Direction 810 From my viewing point at the beginning of this video, where am located at the end of the video? A. Front left B. Front right C. Front D. Same location Answer: C. Front Gemini 3 Flash \"options\": \"thinking_trace\": At the beginning of the video (00:00.000), the camera is positioned in courtyard facing forward. To the left is large building with an arched entrance. As the video progresses, the person moves forward into the courtyard and then turns towards the left, passing the San Clemente Centersign on the building that was originally on the left. By the end of the video (00:23.500), the person is at ramp/walkway between buildings. Relative to the initial viewing point and orientation, this final location is ahead and to the left. Therefore, the location is Front left. Gemini 3 Pro To determine the final location relative to the starting point, lets analyze the movement in the video: 1. **Start (00:00.000):** The view is of courtyard with building featuring large archway on the left and palm trees/benches in the center and right. 2. **Initial Movement (00:00.000 - 00:06.000):** The camera pans right, showing more of the courtyard, fountain, and the building on the right side. 3. **Return to Center (00:06.000 - 00:08.000):** The camera pans back to the left, re-centering on the archway building and the tree in front of it. 4. **Forward Movement (00:08.000 - 00:19.000):** The camera moves forward, walking past the initial benches and trees. The perspective shifts as the person walks deeper into the courtyard, towards the building in the background. 5. **Turn and Final Position (00:19.000 - 00:23.500):** The person turns slightly right and continues walking forward along path. The final view shows ramp and walkway leading between buildings."
        },
        {
            "title": "Back to the First Page",
            "content": "Comparing the start and end:* **Start:** Standing back in the open courtyard area.* **End:** The person has walked forward significantly, passing the initial trees and benches, and is now closer to the buildings that were originally in the background/middle ground. The path taken was generally forward and slightly to the right to follow the walkway. Therefore, the final location is in front of the starting point and slightly to the right. Among the options, Front right√Øs the most accurate description of the displacement. \"options\": \"thinking_trace\": The video starts with view of courtyard. The camera first pans around, then moves forward into the scene. The person walks past the initial set of benches and trees that were in the middle ground. As they move forward, they veer slightly to the right to follow path leading towards the buildings in the background. By the end of the video, the person is significantly ahead of their starting position and positioned to the right of the initial center line of sight, facing walkway between buildings. Therefore, the location is Front right relative to the start. Gemini 2.5 Pro \"options\": \"thinking_trace\": At the beginning of the video, am standing in paved courtyard. The main entrance with large arch is to my front-left. collection of benches and palm trees are in front of me, slightly to the right. As the video progresses, first pan around, then walk forward and to the right, moving past the central benches and trees. By the end of the video, am on concrete path looking at ramp, which is located to the right of the central courtyard area from my initial perspective. Therefore, my final location is to the front and right of where started. GPT-5.2 \"options\": \"thinking_trace\": From the initial courtyard viewpoint, move forward across the plaza and end up closer to the far-side buildings near the left wall/sign and ramp area, i.e., front-left relative to the start. Qwen3-VL 235B \"options\": \"thinking_trace\": The video starts with view of the courtyard including the fountain and buildings. As the frames progress, the camera moves around the courtyard. By the final frame, the position relative to the starting point is determined to be front left based on the directional movement through the courtyard elements observed in the sequence. Qwen3-VL 32B \"options\": \"thinking_trace\": The video starts with view of the courtyard. As the camera moves through the sequence, the final frame shows location that, from the initial viewing point, would be in the front left direction. The movement and orientation changes indicate that the end position relative to the start is front left. Qwen3-VL 30B \"options\": \"thinking_trace\": The video starts with view of the courtyard and builds, then moves through various angles of the campus. By analyzing the sequence, the movement suggests the person walked around the area but returned to the starting viewpoint by the end. The frames indicate circular path or return to the original position, so the location at the end matches the beginning. Qwen2.5-VL 72B \"options\": B"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": At the beginning of the video, am facing courtyard with building entrance on my left and palm trees in front. As the video progresses, turn to the right, moving past benches and more palm trees, eventually ending up facing different section of the building with ramp and additional greenery. This indicates that have moved from my initial position to the front right side of the original view."
        },
        {
            "title": "Back to the First Page",
            "content": "G.3. Route Shape G.3.1. Example 151 Route Shape 151 What's the shape of my moving trajectory? A. U-Shape B. Triangle C. L-Shape D. Straight line Answer: B. Triangle Gemini 3 Flash \"options\": \"thinking_trace\": The video starts with the camera moving forward along sidewalk next to parking lot (00:00.000 to 00:10.500). At around 00:11.000, the camera makes distinct right turn and continues moving forward across the parking lot (00:11.500 to 00:22.000). Then, at approximately 00:22.500, the camera makes another right turn and moves forward towards building with bike racks (00:23.000 to 00:36.000). This sequence of two right turns connecting three straight segments forms U-shape. Gemini 3 Pro The cameras movement can be broken down into two main segments based on the visual evidence: 1. **First Segment (00:00.000 - 00:22.000):** The video starts with the camera moving forward along sidewalk next to parking lot. We see red car, white car, and FedEx truck. The camera continues straight, passing parked cars on the right (a red sedan, white sedan, grey SUV with roof box, another white sedan). The movement is generally linear along the edge of the parking lot. 2. **Second Segment (00:22.500 - 00:36.000):** Around frame 00:22.500, there is distinct change. The camera view pivots or turns significantly to the right. After this turn, the camera begins moving forward again in new direction, perpendicular to the original path. The camera now moves towards beige building with brown awnings, passing gold sedan parked on the left and bike rack on the right. Combining these two segments:* The camera moves straight along one axis.* It then makes roughly 90-degree turn to the right.* It continues straight along new axis. This pattern of movements-straight, turn 90 degrees, straight-forms an \"L\" shape."
        },
        {
            "title": "Back to the First Page",
            "content": "\"options\": \"thinking_trace\": The video begins with the camera moving forward along sidewalk parallel to parking lot, passing several parked cars (red sedan, white sedan, grey SUV). This constitutes the first leg of the path. Around the 22-second mark, the camera stops its forward motion along this axis and turns approximately 90 degrees to the right. Following this turn, the camera proceeds forward again, now moving towards building and passing gold sedan and bike racks. This second leg is perpendicular to the first. The combination of straight path, 90-degree turn, and another straight path creates trajectory that resembles the letter L. Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. At the beginning of the video (0:00), am standing on sidewalk next to parking lot. start moving forward, parallel to row of parked cars on my right. This is the first straight segment of my movement. 2. Around 0:10, reach the end of the first row of cars and turn my body to the right, changing my direction of movement by approximately 90 degrees. 3. From 0:11 to 0:22, walk across the parking lot, perpendicular to my initial path. This is the second straight segment. 4. Around 0:22, turn my body to the left, changing my direction again by approximately 90 degrees. am now facing parallel to my original path, but on the opposite side of the parking lot. 5. From 0:23 to the end of the video, walk forward along this new path. This is the third straight segment. 6. The overall trajectory consists of three straight segments connected by two turns, forming shape that resembles the letter U. walk down one side, across the middle, and then up the other side. Therefore, the shape of my moving trajectory is U-shape. GPT-5.2 \"options\": \"thinking_trace\": The view moves forward along the left side of the parking lot toward the cars, then makes clear right turn to face and continue along the building/parking row, forming an L-shaped trajectory. Qwen3-VL 235B \"options\": \"thinking_trace\": The video shows movement starting straight, then right turn (evident from the shift in direction between frames 12-13), followed by continued movement along new straight path. This creates an L-shaped trajectory, matching option C. Qwen3-VL 32B \"options\": \"thinking_trace\": The camera starts near the sidewalk with the red car, moves forward, then turns and the path forms U-shape as it moves around the parking lot, indicating U-shaped trajectory. Qwen3-VL 30B \"options\": \"thinking_trace\": To determine the shape of the moving trajectory, analyze the cameras path through the parking lot. The video shows the camera moving forward, then turning (e.g., from moving along one row of parking spaces to another perpendicular row). This pattern of moving in two perpendicular directions forms an L-Shape. Other options are inconsistent: straight line would require no turning (not observed), U-Shape implies complete reversal (not seen), and Triangle would require three segments (not matching the movement). Thus, the trajectory is L-Shape. Qwen2.5-VL 72B \"options\": C"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": The video shows movement starting from the left side of the parking lot, moving forward and then turning right towards the building. This trajectory forms an L-shape as it progresses from one point to another with 90-degree turn."
        },
        {
            "title": "Back to the First Page",
            "content": "G.3.2. Example 225 Route Shape 225 What's the shape of my moving trajectory? A. Manhattan-style piecewise linear B. In-place orientation C. Straight line D. Zigzag Answer: D, Zigzag Gemini 3 Flash \"options\": \"thinking_trace\": The video shows the camera moving through an outdoor plaza. The movement consists of several distinct straight-line segments followed by sharp turns. For example, the camera moves forward, then turns left around 00:11.000, moves forward again, turns right around 00:16.000, moves forward, turns left around 00:21.000, moves forward, turns right around 00:26.000, and turns left again around 00:30.000. This pattern of moving in straight lines and making sharp, often near-right-angle turns is characteristic of Manhattan-style piecewise linear trajectory. Gemini 3 Pro \"options\": \"thinking_trace\": The video starts with the camera facing plaza with palm trees and people walking. From 00:00.000 to roughly 00:10.500, the camera seems relatively stationary or moving very slowly, mostly panning to observe the scene. Then, around 00:11.000, the camera turns left and begins to move forward along path. This forward movement continues until about 00:20.500. At this point, the camera stops moving forward and turns right (approx 90 degrees) to face different direction, looking towards building labeled Kerr Hall. Then, from 00:21.000 onwards, the camera moves forward again in this new direction. Finally, around 00:26.000, the camera turns left again (approx 90 degrees) and moves towards the building entrance. This pattern of moving straight, turning 90 degrees, moving straight, and turning 90 degrees again resembles Manhattan-style√∂r grid-like movement, which is characterized by piecewise linear segments connected by sharp turns. Therefore, the trajectory is best described as Manhattan-style piecewise linear."
        },
        {
            "title": "Back to the First Page",
            "content": "Gemini 2.5 Pro \"options\": \"thinking_trace\": The video shows first-person perspective. start by looking forward at group of people in plaza with palm trees. Then, my head turns to the right, panning across the scene to focus on large, modern, grey building. After moment, my head pans back to the left, looking past the initial view towards grassy area with market tent in the distance. The camera continues to pan left and right, observing different parts of the plaza. Throughout this entire sequence, my position relative to the surrounding objects (the palm trees, the buildings, the grassy area) does not seem to change significantly. The movement is primarily rotational, as if am standing in one spot and turning my head to look around. This is classic example of in-place orientation change, not trajectory involving walking or moving from one point to another. Therefore, the shape of my moving trajectory is best described as In-place orientation. GPT-5.2 options\": \"thinking_trace\": \"Across the frames, continuously advance forward across the plaza (objects like palm tree bases and bollards get closer), while my head pans left/right; the translation path itself appears roughly straight rather than turning in right angles or zigzagging. Qwen3-VL 235B \"option\": \"thinking_trace\": The video shows the perspective moving through an outdoor plaza with multiple turns at right angles, consistent with Manhattan-style piecewise linear trajectory (moving in straight segments with 90-degree turns). The path is not straight line (C), does not involve staying in place (B), and is not sharp zigzag (D). Qwen3-VL 32B \"options\": \"thinking_trace\": The movement in the video involves changing directions multiple times, forming path composed of several straight segments connected by turns. This matches the description of Manhattan-style piecewise linear trajectory, where movement is along straight lines with directional changes, even if the turns arent strictly 90-degree (as Manhattan-style can be interpreted broadly for piecewise linear paths with distinct segments). The other options dont fit: (in-place orientation) implies no movement, (straight line) is incorrect as direction changes, and (zigzag) suggests more frequent back-and-forth which isnt observed. Qwen3-VL 30B \"options\": \"thinking_trace\": The video shows the camera moving through space with multiple direction changes, indicating trajectory composed of straight segments with turns. This aligns with the definition of \"Manhattan-style piecewise linear\" (option A), where movement follows straight paths with directional shifts rather than single straight line (C), in-place orientation (B), or simple zigzag (D). Qwen2.5-VL 72B \"options\": \"thinking_trace\": The trajectory in the video appears to be series of straight segments that change direction at right angles, resembling Manhattan-style grid. This is evident from the way the camera moves forward and then turns sharply at intersections or corners, maintaining piecewise linear path throughout the sequence of frames. There are no indications of in-place orientation, continuous straight line, or zigzag pattern."
        },
        {
            "title": "Back to the First Page",
            "content": "G.4. Reverse Route Plan G.4.1. Example 168 Reverse Route Plan 168 From my viewpoint at the end of the video, how can go back to my starting point? A. Turn around, go straight. Turn right, go straight, then turn left and continue go straight B. Turn around, go straight. Turn left, go straight, then turn right and continue go straight C. Turn around, go straight. Turn right, go straight, then turn right and continue go straight D. Turn around, go straight. Turn left, go straight, then turn left and continue go straight Answer: D. Turn around, go straight. Turn left, go straight, then turn left and continue go straight Gemini 3 Flash \"options\": \"thinking_trace\": To determine the return path, first need to trace the forward path taken in the video. 1. Starting at 00:00.000, am walking straight across the soccer field. 2. Around 00:11.000, make right turn. 3. continue walking straight past soccer goal on my right until about 00:21.500. 4. Around 00:22.000, make left turn. 5. continue walking straight towards the center of the field until the end of the video. To return to the starting point from the end of the video, must reverse these steps: 1. Turn around (180 degrees) to face the direction just came from. 2. Go straight (reversing the final segment). 3. Since my last turn was left turn, must now turn right to backtrack. 4. Go straight (reversing the middle segment). 5. Since my first turn was right turn, must now turn left to backtrack. 6. Go straight to reach the original starting point. This sequence corresponds to Option A: Turn around, go straight. Turn right, go straight, then turn left and continue go straight."
        },
        {
            "title": "Back to the First Page",
            "content": "Gemini 3 Pro To determine the path back to the starting point, lets analyze the movement shown in the video: 1. **Start (00:00 - 00:11):** begin facing soccer goal on my right and fence/building area ahead. walk forward slightly and then turn left. 2. **First Turn (00:11 - 00:12):** turn left, now facing the sun and the length of the field. 3. **Movement (00:12 - 00:22):** walk straight along the sideline, passing the soccer goal on my right. 4. **Second Turn (00:22 - 00:23):** turn right, facing across the width of the field towards large building and mountains in the distance. 5. **Movement (00:23 - 00:35):** walk straight across the field towards the center circle. 6. **End (00:35.500):** am standing near the center circle, facing the building. To reverse this path and return to the start: 1. **Turn around:** need to face the opposite direction from where am currently looking. 2. **Go straight:** This retraces the path from the center circle back to the sideline where turned. 3. **Turn left:** Since originally turned right to face the center, reversing this means turning left to face back down the sideline. 4. **Go straight:** Walk back along the sideline towards the initial corner. 5. **Turn right:** Since originally turned left to start walking down the sideline, need to turn right to face the original starting orientation (looking at the fence/goal area). 6. **Go straight:** Walk the short distance back to the exact starting spot. Comparing this to the options:* Option A: Turn around, go straight. Turn right... (Incorrect turn direction).* Option B: Turn around, go straight. Turn left, go straight, then turn right and continue go straight. (Matches the reverse path: Turn around -> walk back across field -> Turn left at sideline -> walk down sideline -> Turn right to face original start).* Option C: Turn around, go straight. Turn right... (Incorrect turn direction).* Option D: Turn around, go straight. Turn left... then turn left... (Incorrect final turn). Therefore, the correct sequence is to turn around, go straight back to the sideline, turn left to walk along it, and then turn right to face the original starting position. \"options\": \"thinking_trace\": The video shows path where walk straight, turn left, walk straight for while, turn right, and walk straight again to the end point. To return, must reverse these actions in opposite order. First, turn around (180 degrees). Then walk straight, retracing the last leg of the journey. At the point where made the second turn (which was right turn), must now turn left. Then walk straight, retracing the middle leg. Finally, at the point where made the first turn (which was left turn), must turn right and walk straight to the original starting spot. This corresponds to the sequence: Turn around, go straight, turn left, go straight, turn right, go straight. Option matches this description. Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. **Analyze my starting position and initial movement:** The video begins with me standing on an artificial turf soccer field. am looking across the field towards goal in the distance. There is another goal to my right. From 0:00 to 0:11, walk forward. 2. **Analyze the first turn:** At 0:11, turn my head and body to the right, panning across the field. The sun, which was to my right, is now in front of me. This confirms right turn. 3. **Analyze the second movement:** From 0:12 to 0:19, walk straight, parallel to the goal that is now on my right side. 4. **Analyze the second turn:** At 0:19, pivot my body to the left and start walking away from the goal line, towards the center of the field. This is left turn. 5. **Analyze the final movement:** From 0:20 to the end of the video, walk straight towards the center of the field. The video ends with me facing the center circle and the far side of the field. 6. **Summarize the forward path:** * Go straight. * Turn right. * Go straight. * Turn left. * Go straight."
        },
        {
            "title": "Back to the First Page",
            "content": "7. **Determine the reverse path:** To return to my starting point from my final position, must reverse these actions in reverse order. * First, need to turn around (180 degrees) to face the direction came from. * Then, walk straight back, reversing the final \"Go straight\" action. * Next, need to reverse the Turn left. The opposite of left turn is **right turn**. * Then, walk straight back, reversing the second Go straight√§ction. * Next, need to reverse the \"Turn right\". The opposite of right turn is **left turn**. * Finally, walk straight back to my original starting point, reversing the first \"Go straight\" action. 8. **Construct the full reverse instruction:** Turn around, go straight. Turn right, go straight, then turn left and continue go straight. 9. **Match with the given options:** * A: Turn around, go straight. Turn right, go straight, then turn left and continue go straight. - This matches my derived reverse path. * B: Turn around, go straight. Turn left, go straight, then turn right and continue go straight. - Incorrect. The first turn on the way back should be right. * C: Turn around, go straight. Turn right, go straight, then turn right and continue go straight. - Incorrect. The second turn on the way back should be left. * D: Turn around, go straight. Turn left, go straight, then turn left and continue go straight. - Incorrect. Both turns are wrong. Therefore, option correctly describes the path back to the starting point. GPT-5.2 \"options\": \"thinking_trace\": From the start moved along the field, then made right turn to go along the side near the goal, and later made left turn to head toward the center/building. Reversing that route from the end: turn around and go straight, then turn right and go straight, then turn left and continue straight to reach the starting point. Qwen3-VL 235B \"options\": \"thinking_trace\": To determine the path back to the starting point, analyze the videos movement: the starting point is near the soccer goal visible in early frames, and the end point is on field with circular line facing building. The correct path requires reversing the trajectory. From the end, turning around (opposite the current view) and proceeding straight retraces the main path. Subsequent turns must counteract the forward-direction turns: turning right (instead of left) and then left (instead of right) aligns with retracing the route past goals and field markings. Option matches this sequence: turn around, go straight, turn right, go straight, then turn left to reach the starting goal. Qwen3-VL 32B \"options\": \"thinking_trace\": To determine the path back to the starting point, we reverse the movement sequence. The video shows the camera moving from the start, turning in certain directions. The correct reverse path would involve turning around first, then following the opposite of the original turns. Option describes sequence where after turning around and moving straight, left turn (opposite of previous right turn in the forward path) is followed by right turn to realign with the start, which matches the required reversal of directions observed in the video frames. Qwen3-VL 30B \"options\": \"thinking_trace\": To determine the path back to the starting point, we analyze the movement from the starting frame (0.2s) to the end frame (35s). The starting point is near the right-side goal. In the video, the camera moves leftward along the field, then across the central area. To return, we reverse this path: 1) Turn around (face the direction of the starting point), 2) Go straight, 3) Turn right (to align with the right-side goal area), 4) Go straight, 5) Turn left (to reach the exact starting spot near the goal). This matches option As sequence of \"Turn around, go straight. Turn right, go straight, then turn left and continue go straight.\" Qwen2.5-VL 72B \"options\": B"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": To return to the starting point from the viewpoint at the end of the video, need to retrace my steps. Initially, moved forward and then turned left, followed by another left turn. Therefore, to go back, should first turn around and go straight to undo the last movement. Then, need to turn left to align with the previous path and go straight. After that, should turn right to correct the direction and continue going straight to reach the starting point. This sequence matches option B."
        },
        {
            "title": "Back to the First Page",
            "content": "G.4.2. Example 196 Reverse Route Plan 196 From my viewpoint at the end of the video, how can go back to my starting point? A. Turn around, go straight. Turn left, go straight, then turn left and continue go straight B. Turn around, go straight. Turn right, go straight, then turn right and continue go straight C. Turn around, go straight. Turn left, go straight, then turn right and continue go straight D. Turn around, go straight. Turn right, go straight, then turn left and continue go straight Answer: C. Turn around, go straight. Turn left, go straight, then turn right and continue go straight Gemini 3 Flash The provided video starts with first-person view of kitchen, specifically focusing on white cabinet and stainless steel range hood above stove (00:00.000). As the camera pans to the right, it reveals an opening to dining area (00:01.500) and then stainless steel refrigerator (00:02.500). Further panning to the right shows more of the kitchen, including microwave, tiled countertop, and sink (00:03.500 - 00:09.000). The person then moves from the kitchen area into the dining room (00:10.500), which contains white table and chairs. After looking around the dining room, the person turns around (00:18.500), exits the dining room, and turns toward white door located next to the refrigerator (00:19.500 - 00:27.000), where the video ends. To return to the starting point (the stove/cabinet area) from the final position at the door: 1. **Turn around**: The person must turn away from the door to face back into the kitchen area. 2. **Go straight**: Move past the refrigerator toward the center of the kitchen. 3. **Turn right**: To align with the path toward the dining room opening or to navigate the kitchens layout as seen in the initial pan. 4. **Go straight**: Move toward the area between the dining room and the stove. 5. **Turn left**: This turn will orient the person to face the stove and the white cabinets where the video began. This sequence of movements corresponds to the directions provided in option D. options: {A: Turn around, go straight. Turn left, go straight, then turn left and continue go straight., B: Turn around, go straight. Turn right, go straight, then turn right and continue go straight., C: Turn around, go straight. Turn left, go straight, then turn right and continue go straight., D: Turn around, go straight. Turn right, go"
        },
        {
            "title": "Back to the First Page",
            "content": "straight, then turn left and continue go straight.} \"options\": \"thinking_trace\": The video starts at the stove/cabinet area. It pans right to show the dining room opening, then the fridge, then the sink. The person walks into the dining room, turns around, exits, and turns toward door near the fridge. To go back: from the door, turn around and go straight past the fridge. To reach the stove, which was to the left of the dining room opening (from the kitchens perspective), the person would need to turn right toward the opening/center area and then turn left to face the stove. This matches the Turn right, then turn left sequence in Option D. Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. **Analyze the starting and ending points:** The video starts with me facing stainless steel stove and range hood, with white cabinets to the left. The video ends with me standing directly in front of plain white door. 2. **Map the spatial layout:** By observing the pan-around shot, can construct mental map of the room. * The starting point is the stove * To the right of the stove area is kitchen island and an opening to dining room. * Opposite the stove and island is wall with stainless steel refrigerator, and to the left of the refrigerator is the white door (the ending point). * The counter continues from the refrigerator area and turns corner to the right, where sink is located under window. The kitchen has an L-shape or U-shape layout with central island. 3. **Determine the path taken from start to end:** The most logical path for me to have taken is from the stove to the door. This would involve: * Starting at the stove. * Turning right to face the main kitchen area. * Walking forward, likely passing the kitchen island. * Approaching the wall with the refrigerator and door. * Turning left to face the door. * Walking forward to the door. * The path is essentially: (From stove) Turn Right -> Go Straight -> Turn Left -> Go Straight -> Arrive at door. 4. **Reverse the path to find the way back:** To get from the ending point (the door) back to the starting point (the stove), must reverse these actions in reverse order. * **Start at the end point:** am facing the door. * **Step 1: Reverse the last action.** The last action was walking forward to the door. The reverse is to turn around (180 degrees) and walk straight away from the door. This corresponds to \"Turn around, go straight.\" * **Step 2: Reverse the second-to-last action.** The action before that was turning left to face the door. The reverse is to turn right. * **Step 3: Reverse the action before that.** This was walking straight past the island. The reverse is to walk straight again. This corresponds to \"Turn right, go straight.\" * **Step 4: Reverse the first action.** The first action was turning right from the stove. The reverse is to turn left, which will make me face the stove again. * **Step 5: Walk to the stove.** This corresponds to \"then turn left and continue go straight.\" 5. **Combine the reversed steps:** The full sequence to return to the starting point is: Turn around, go straight, turn right, go straight, turn left, go straight. 6. **Match the reversed path to the given options:** * A: Turn around, go straight. Turn left, go straight, then turn left and continue go straight. (Incorrect) * B: Turn around, go straight. Turn right, go straight, then turn right and continue go straight. (Incorrect) * C: Turn around, go straight. Turn left, go straight, then turn right and continue go straight. (Incorrect) * D: Turn around, go straight. Turn right, go straight, then turn left and continue go straight. (This matches the derived reverse path). Therefore, option correctly describes the path back to the starting point. GPT-5.2 \"options\": \"thinking_trace\": At the end Im facing white door beside the fridge. To return to where started (by the stove/range hood), need to turn around into the kitchen, go straight to the open area, then make two left turns to head back along the counters to the range hood area. Qwen3-VL 235B \"options\": A"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": The video starts at the stove area and ends near door. To return from the end point (door) to the start (stove), first turn around to face the dining area and go straight through it. Upon entering the kitchen, turn left to navigate toward the stove area. After going straight, another left turn aligns with the initial stove/cabinet position. This matches option As sequence of turns. Qwen3-VL 32B \"options\": \"thinking_trace\": The video starts near the stove (kitchen area) and ends at door. To return: 1. Turn around (face the direction from which you came, towards the dining area). 2. Go straight (through the dining area). 3. Turn left (to enter the kitchen, as the kitchen is to the left of the dining area). 4. Go straight (through the kitchen). 5. Turn left (to reach the starting point near the stove). This path matches option A. Qwen3-VL 30B \"options\": \"thinking_trace\": To determine the path back to the starting point, we analyze the videos sequence: 1. The starting point (frame 0-1) is near kitchen cabinets. 2. The end of the video (frame 26-27) shows white door. 3. Tracing the movement: the journey from start to end involved moving through the kitchen, dining area, and toward the door. To reverse this, from the door: - Turn around (face away from the door, toward the dining area). - Go straight (toward the dining area). - Turn right (to navigate toward the kitchen). - Go straight (toward the kitchen). - Turn left (to align with the starting point near the initial kitchen cabinets). This matches option D. Qwen2.5-VL 72B \"options\": \"thinking_trace\": To return to the starting point from the viewpoint at the end of the video, need to retrace my steps. The sequence of movements in the video shows path that involves turning and moving straight multiple times. By analyzing the frames, it is clear that need to turn around and go straight first, then make right turn, continue straight, make another right turn, and finally continue straight to reach the starting point. This matches option B."
        },
        {
            "title": "Back to the First Page",
            "content": "G.5. Spatial Memory G.5.1. Example 53 Spatial Memory 53 Which object changes between earlier and later in the video? A. Scooter B. Cart C. Trash bin D. Round table Answer: B. Cart Gemini 3 Flash The object that changes between the earlier and later parts of the video is the scooter. In the earlier frames (e.g., 00:06.500 to 00:10.000), silver kick scooter is clearly visible leaning against the \"GAUCHOS RECYCLE\" bin. However, when the camera returns to the same area later in the video (e.g., 00:21.500 to 00:22.500), the scooter is no longer present in that location. While other objects in the room also appear to have been moved or rearranged, the disappearance of the distinct scooter is clear and notable change. \"options\": \"thinking_trace\": The video shows room from first-person perspective. In the first half of the video (around 00:06.500 to 00:10.000), the camera pans past \"GAUCHOS RECYCLE\" bin where silver kick scooter is leaning. The camera then pans around the rest of the room, showing various lounge areas, tables, and ping pong table. When the camera eventually pans back to the area with the recycle bin and carts (around 00:21.500 to 00:22.500), the scooter is no longer there. This indicates that the scooter is the object that changed (disappeared) between the earlier and later parts of the video. Gemini 3 Pro \"options\": \"thinking_trace\": The video starts by showing room with large glass doors, blue recycling carts, and round table. Around 00:02.000, the camera focuses on recycling station labeled \"GAUCHOS RECYCLE\". Next to this station, there is scooter parked. The scooter is clearly visible from 00:05.000 to 00:08.000. Later in the video, the camera pans around the room showing seating areas and tables. Towards the end of the video, specifically around 00:21.000 and 00:22.000, the camera returns to view the area near the recycling station and the glass doors. In"
        },
        {
            "title": "Back to the First Page",
            "content": "these later frames, the scooter that was previously parked next to the recycling station is no longer there. The blue carts, trash bins, and round tables remain in their positions or are consistent with the rooms layout, but the scooter has been removed. Therefore, the object that changes (disappears) is the scooter. Gemini 2.5 Pro \"options\": \"thinking_trace\": The user wants me to identify an object that changes its position or state during the video. 1. **Initial Observation (00:00 - 00:03):** am in large common room. see glass doors, large wheeled carts (blue and red), round table with chairs, and recycling/trash bin station. 2. **Mid-Video Observation (00:04 - 00:08):** move closer to the recycling bin. silver and black scooter is clearly visible leaning against the front of the blue \"GAUCHOS RECYCLE\" bin. 3. **Panning Away (00:09 - 00:20):** pan away from the recycling bin area, looking around the rest of the room, which includes seating area with couches, fireplace, and ping pong table. 4. **Final Observation (00:21 - 00:24):** pan back across the room. When the area with the recycling bin comes back into view, the scooter that was previously leaning against it is no longer there. The recycling bin itself, the carts, and the round tables are all in the same positions as before. **Conclusion:** The scooter is present in the middle of the video but is absent at the end. This is change. The carts, trash bin, and round table do not change their positions. Therefore, the scooter is the object that changes. GPT-5.2 \"options\": \"thinking_trace\": scooter becomes visible later in the video (near the recycling station), whereas it is not present in the earlier frames. Qwen3-VL 235B \"options\": \"thinking_trace\": Analyzing the frames, the trash bin (recycling station labeled GAUCHOS RECYCLE) is visible in multiple frames. Comparing earlier and later segments, the trash bins presence and position change relative to the cameras movement, indicating it is the object that changes between earlier and later in the video. Other options like scooter, cart, and round table do not show significant change in their state or position as observed in the frames. Qwen3-VL 32B \"options\": \"thinking_trace\": In the video frames, scooter is visible near the GAUCHOS RECYCLE bin in some earlier frames (e.g., frames 7-9) but is not present in later frames (e.g., frames 22-24). The other options (Cart, Trash bin, Round table) remain consistently visible or unchanged throughout the video sequence. Qwen3-VL 30B \"options\": \"thinking_trace\": √Ñnalyzing the frames: (Scooter) is not visible in any frame. (Cart) shows blue carts in early frames (e.g., frame 3) and later frames (e.g., frame 24), so no change. (Trash bin) remains visible (e.g., GAUCHOS RECYCLE bin in frames 6-10 and later). (Round table) is visible in multiple segments: seen near the recycling bin (frame 6), in the mid-section (frames 11-12), and later (frames 20-24). While the cart and trash bin remain consistent, the round tables position or presence in different contexts suggests it is the object that changes in terms of spatial context or visibility across the sequence. Qwen2.5-VL 72B \"options\": C"
        },
        {
            "title": "Back to the First Page",
            "content": "\"thinking_trace\": In the earlier part of the video, there is blue trash bin visible near the entrance. As the video progresses and the camera moves around the room, the blue trash bin is no longer in view. This indicates that the position or visibility of the trash bin has changed between the earlier and later parts of the video. G.5.2. Example 65 Spatial Memory 65 Which object changes between earlier and later in the video? A. Outdoor coffee table B. Seat cushion C. Metal patio chair D. Throw pillow Answer: A. Outdoor coffee table Gemini 3 Flash \"options\": \"thinking_trace\": To determine which object changes between the earlier and later parts of the video, will examine each option against the provided frames. 1. **Outdoor coffee table**: This is visible in frames like 00:01.500, 00:03.000, 00:11.000, and 00:14.000. It remains in the same position and appearance throughout the video. 2. **Seat cushion**: The blue cushions on the wicker sofas are visible throughout the video (e.g., 00:01.500, 00:11.000, 00:14.000). They do not appear to change. 3. **Metal patio chair**: metal chair is visible in the bottom right of the frame at 00:00.000 and 00:00.500. As the camera moves, it goes out of view. There is no clear evidence of it changing√∂ther than the camera moving away from it. 4. **Throw pillow**: In frame 00:02.500 and 00:03.000, patterned throw pillow is clearly visible on the blue sofa. However, in later frames where the same sofa is visible, such as 00:11.000, 00:13.500, and 00:14.000, the patterned throw pillow is no longer there. This indicates clear change in the scene. Based on this analysis, the throw pillow is the object that changes (disappears) between the earlier and later parts of the video."
        },
        {
            "title": "Back to the First Page",
            "content": "Gemini 3 Pro The object that changes between earlier and later in the video is the throw pillow. \"options\": \"thinking_trace\": In the beginning of the video (00:00.000 to 00:03.000), there is patterned throw pillow on the blue sofa cushion closest to the camera. As the camera pans around the courtyard and returns to view the same sofa area later in the video (around 00:13.500 and onwards), the throw pillow is no longer present on the sofa. The other objects like the outdoor coffee table, seat cushions, and metal patio chair remain consistent in their presence or absence relative to the camera angle, but the pillow specifically disappears from its original location. Gemini 2.5 Pro \"options\": \"thinking_trace\": The user wants me to identify an object that changes its state or position during the video. 1. first examine the scene at the beginning of the video (00:00-00:02). see patio with outdoor furniture. Specifically, notice large L-shaped wicker sofa with blue cushions. In the corner of this sofa, there is single decorative throw pillow with pink, purple, and white swirly pattern. 2. then scan through the rest of the video, paying attention to the objects mentioned in the options. * The outdoor coffee table remains in the same position. * The blue seat cushions on the sofas remain in their places. * The metal patio chair, partially visible at the start, is out of frame for most of the video, but theres no indication it has changed. 3. look back at the L-shaped sofa in the later parts of the video. At timestamp 00:11 and again at 00:13, can see the corner of the sofa where the throw pillow was previously located. The pillow is no longer there. 4. Comparing the state of the sofa at the beginning (e.g., 00:02) with its state later on (e.g., 00:11), the most noticeable change is the disappearance of the throw pillow. 5. Therefore, the throw pillow is the object that changes between earlier and later in the video. GPT-5.2 \"options\": \"thinking_trace\": patterned throw pillow becomes visible on the blue outdoor sofa in later frames, whereas it is not present/visible in the earlier frames. Qwen3-VL 235B \"options\": \"thinking_trace\": The throw pillow is visible in some frames (e.g., frame 3) but not in others later in the video, indicating change. Other options like the outdoor coffee table, seat cushion, and metal patio chair remain consistent in appearance and presence across the frames. Qwen3-VL 32B \"options\": \"thinking_trace\": In the video frames, throw pillow is visible on the blue couch in earlier frames (e.g., frame 3 and 4) but is not present in later frames (e.g., frame 14 and 15). The other options (outdoor coffee table, seat cushion, metal patio chair) remain consistent in their presence or appearance throughout the video. Therefore, the throw pillow is the object that changes. Qwen3-VL 30B \"options\": \"thinking_trace\": Analyze each option: nA: Outdoor coffee table is consistently visible across frames. nB: Seat cushion (blue) remains unchanged on the couch. nC: Metal patio chair appears in some frames but does not undergo visible change (its present/repeatedly visible but not altered). nD: throw pillow (with striped pattern) appears on the couch in some frames (e.g., frame 3.2) but is absent in"
        },
        {
            "title": "Back to the First Page",
            "content": "others (e.g., frame 1.2). Thus, its presence/absence changes between frames. nOnly shows change in visibility between earlier and later frames.\" Qwen2.5-VL 72B \"options\": \"thinking_trace\": The throw pillow is visible on the seat cushion in the earlier part of the video. However, as the video progresses and the camera pans around, the throw pillow is no longer visible on the seat cushion. This indicates change in the presence or position of the throw pillow between the earlier and later parts of the video."
        },
        {
            "title": "Back to the First Page",
            "content": "G.6. Spatial Affordance G.6.1. Example 105 Spatial Affordance 105 Based on the video, can reach forward and touch the vending machine buttons using only my arm, without moving my feet? A. Yes B. No Answer: B. No Gemini 3 Flash \"options\": \"thinking_trace\": The video shows the person walking towards Pepsi vending machine. By the end of the video (around 00:08.500 to 00:12.500), the person has stopped walking and extends their right arm towards the vending machines control panel. The hand is clearly within reaching distance of the buttons while the camera (representing the persons head/body) remains stationary. This indicates that the buttons can be reached using only the arm without moving the feet further. Gemini 3 Pro \"options\": \"thinking_trace\": The video shows the person walking up to Pepsi vending machine. As the person gets closer, the camera angle shifts to focus on the control panel of the machine. At the end of the video (around 00:08.500 onwards), the person extends their right arm towards the machine. The hand is clearly visible reaching out and is very close to the buttons and the coin slot area. The perspective suggests that the person is standing right in front of the machine, within arms reach. The hand is extended and appears to be in position to interact with the machines interface. Therefore, it is possible to reach forward and touch the buttons using only the arm without moving the feet from that final position. Gemini 2.5 Pro \"options\": \"thinking_trace\": The video begins with me standing at distance from Pepsi vending machine. then walk towards it. By the end of the video, am standing directly in front of the machine. My hand enters the frame,"
        },
        {
            "title": "Back to the First Page",
            "content": "reaching out towards the control panel where the buttons and payment slots are located. Given my close proximity to the machine and the position of my outstretched hand, it is clear that can easily reach and touch the buttons without needing to take another step. GPT-5.2 \"options\": \"thinking_trace\": In the later frames you are standing directly in front of the vending machine, and your outstretched hand reaches the area next to the button/keypad panel, indicating you can touch the buttons without stepping forward. Qwen3-VL 235B \"options\": \"thinking_trace\": In the video, the persons hand is shown reaching toward the vending machine buttons. The distance between the person and the machine appears close enough that extending the arm without moving the feet would allow contact with the buttons. Therefore, the answer is Yes. Qwen3-VL 32B \"options\": \"thinking_trace\": In the 10th frame, the persons hand is extended towards the vending machine. The hand appears close enough to the machines control panel to touch the buttons without needing to move their feet. This suggests that the person can reach the buttons with just their arm. Therefore, the correct answer is B. Qwen3-VL 30B \"options\": \"thinking_trace\": In the video, the person moves towards the vending machine, and the hand extends towards the buttons. The frames show the hand reaching the vending machines button area without needing to move feet, as the distance is covered by arm extension. Thus, the answer is B. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The video shows that the persons hand is able to reach the vending machine buttons without any indication of moving their feet. The shadow and the position of the hand suggest that the person is standing close enough to touch the buttons with just an arm movement."
        },
        {
            "title": "Back to the First Page",
            "content": "G.6.2. Example 147 Spatial Affordance 147 Based on the video, can reach the hand sanitizer dispenser using only my arm after taking one small step leftward, without leaning or using anything? A. Yes B. No Answer: B. No Gemini 3 Flash \"options\": \"thinking_trace\": In the video, the person is standing short distance away from wall where hand sanitizer dispenser is mounted. The dispenser is located to the left of the persons current central line of sight. By taking one small step to the left, the person would be positioned directly in front of the dispenser. Given the proximity shown in the frames (especially around 00:03.000 to 00:06.000 where the hand is visible), the dispenser is well within normal arms reach from that position without needing to lean forward. Gemini 3 Pro \"options\": \"thinking_trace\": The video shows me standing in front of wall with hand sanitizer dispenser mounted on it. Initially, am few steps away. As the video progresses, move closer to the wall and the dispenser. By the end of the video (around frame 00:03.000 onwards), extend my right arm towards the dispenser. My hand is clearly visible in the foreground, reaching out. Although dont touch it in the video, the perspective and the extension of my arm suggest am within reaching distance. If were to take one small step to the left (towards the dispenser) from my final position, would be even closer and directly in front of it, making it easily reachable with just an arm extension without needing to lean excessively. Therefore, the answer is Yes. Gemini 2.5 Pro \"options\": \"thinking_trace\": 1. At the beginning of the video, am standing in hallway, facing concrete wall with hand sanitizer dispenser on it. 2. At 00:03, my right hand enters the frame, extended towards the dispenser."
        },
        {
            "title": "Back to the First Page",
            "content": "3. My hand is noticeable distance away from the dispenser, suggesting am standing few feet away from the wall. 4. The question asks if can reach the dispenser after taking one small step leftward. 5. Taking step leftward would move my body closer to the wall where the dispenser is located. 6. Given the initial distance shown by my outstretched arm, single small step would likely be sufficient to close the remaining gap, allowing my arm to reach the dispenser without needing to lean or use any other aids. GPT-5.2 \"options\": \"thinking_trace\": The dispenser is mounted on the concrete pillar very close to my position; my outstretched hand already appears near it in the later frames. small step left would bring my body closer/aligned with the pillar, making it reachable with just my arm without needing to lean or use anything. Qwen3-VL 235B \"options\": \"thinking_trace\": In frames 4-7, hand is extended toward the hand sanitizer dispenser, indicating the dispenser is within arms reach. Taking one small step leftward (as implied by the movement toward the dispensers location) allows the arm to reach the dispenser without leaning or using additional tools, as shown by the hands proximity to the dispenser in the video. Qwen3-VL 32B \"options\": \"thinking_trace\": In the video, the camera moves left from the initial position (facing the entrance doors) to reach the hand sanitizer dispenser. The frames show that after small leftward movement, the hand sanitizer is within arms reach. The visible hand in the later frames confirms that the arm can reach the dispenser without needing to lean or use additional support, as the distance is covered by the arms natural reach after the small step. Thus, the answer is Yes. Qwen3-VL 30B \"options\": \"thinking_trace\": In the frames, the hand sanitizer dispenser is on the wall. After small leftward step, the hand reaches towards the dispenser, indicating the arm can reach it without leaning or additional support. Thus, the answer is Yes. Qwen2.5-VL 72B \"options\": \"thinking_trace\": The hand sanitizer dispenser is within arms reach after taking one small step leftward. There is no need to lean or use anything else to reach it."
        }
    ],
    "affiliations": [
        "Amazon",
        "Stanford University",
        "University of California, Merced",
        "University of California, Santa Barbara",
        "University of Maryland, College Park",
        "Yale University"
    ]
}