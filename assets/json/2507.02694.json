{
    "paper_title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers",
    "authors": [
        "Zhijian Xu",
        "Yilun Zhao",
        "Manasi Patwardhan",
        "Lovekesh Vig",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback."
        },
        {
            "title": "Start",
            "content": "Can LLMs Identify Critical Limitations within Scientific Research? Systematic Evaluation on AI Research Papers Zhijian XuY Yilun Zhao Manasi Patwardhan T"
        },
        {
            "title": "Lovekesh Vig T Arman Cohan Y\nT TCS Research\nY Yale University",
            "content": "5 2 0 2 3 ] . [ 1 4 9 6 2 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertiseintensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present comprehensive taxonomy of limitation types in scientific research, with focus on AI. Guided by this taxonomy, for studying limitations, we present LIMITGEN, the first comprehensive benchmark for evaluating LLMs capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LIMITGEN-Syn, synthetic dataset carefully created through controlled perturbations of highquality papers, and LIMITGEN-Human, collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback. Data Code yale-nlp/LimitGen yale-nlp/LimitGen"
        },
        {
            "title": "Introduction",
            "content": "Peer review plays crucial role in ensuring the quality and integrity of scientific research. However, it is often time-consuming and expertise-intensive process, posing significant challenges, especially as the volume of published papers continues to grow. Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities across variety of scientific tasks, such as answering questions about scientific papers (Xu Equal Contributions. Figure 1: Overview of the research: the limitation generation task and three research questions investigated. et al., 2024), writing scientific papers (Chamoun et al., 2024; Lu et al., 2024), retrieving related works (Ajith et al., 2024; Press et al., 2024), improving idea generation (Wang et al., 2024; Zhou et al., 2024b; Si et al., 2024), and generating code to perform data-driven discovery (Huang et al., 2024; Tian et al., 2024). Meanwhile, there is increasing interest in exploring the potential of LLMs to assist with or generate peer reviews (Liang et al., 2024; Liu and Shah, 2023; DArcy et al., 2024; Lou et al., 2024), which can be used to provide quick and early feedback to researchers and potentially alleviate some of the burdens associated with traditional review processes. High-quality reviews are supposed to pinpoint the limitations of paper and provide concrete, actionable suggestions, assisting researchers in improving their work. Existing benchmarks for peerreview generation collect papers and their corresponding entire reviews from AI conferences (Du et al., 2024; Yu et al., 2024; Tan et al., 2024), but these benchmarks generally do not emphasize the importance of limitation identifications. Instead, they compare the overall quality of LLM-generated versus human-written reviews (Liang et al., 2024; Yu et al., 2024) and assess adherence to conference guidelines (Tyser et al., 2024). As opposed to other aspects of review generation, such as summary, strengths, syntactic or structural errors or request for elaboration, identification of limitations is the key aspect to facilitate future technical enhancement of the work and it is of utmonst importance from the research growth point of view. To address this gap, we present the first in-depth study and evaluation of LLM systems in identifying the limitations of scientific papers. To do so, we first provide comprehensive taxonomy of types of limitations in scientific fields of study, with focus on AI.1 Then guided by this taxonomy, we propose LIMITGEN-Syn, synthetic benchmark focusing on various categories of limitations. LIMITGEN-Syn systematically introduces controlled perturbations to high-quality papers to create scenarios where specific limitations are present. These perturbations include selective removal of crucial information such as experimental details, inadequate evaluation metrics, omission of key baseline comparisons, and constraints on datasets or methodologies. By carefully controlling these modifications, we can reliably evaluate how well LLM agents detect different types of limitations. In addition, LIMITGEN-Syn also allows capturing suggestions on how to resolve the identified limitation. To assess whether our taxonomy and the synthetic benchmark can effectively capture the diverse categories of limitations identified by humans in real-world peer review settings, we then collect human-written limitations from ICLR 2025 submissions as LIMITGEN-Human. We chose ICLR 2025 to mitigate contamination and also because ICLR peer reviews are often of high-quality due to the public nature of reviews and an extensive rebuttal process. Together, these two datasets form comprehensive benchmark for advancing identification of limitations in papers. Limitation identification is knowledgeintensive task, requiring years of expertise and staying current with rapidly evolving literature. In 1We chose AI as this is the field we are familiar with. In 6.3 we also perform user study to how our findings generalize to other domains. terms of modeling, in such settings, retrieval plays crucial role as it is challenging to keep LLM agents up-to-date in rapidly evolving fields. While Retrieval-augmented generation (RAG) has been applied to enhancing scientific workflow, such as conducting literature reviews (Agarwal et al., 2024; Asai et al., 2024) and answering domain-specific questions (Xu et al., 2024; Skarlinski et al., 2024), it has not yet been explored in the context of peerreviews. RAG also simulates how human identify limitations of papers by implicitly or explicitly referring to existing body of (often recent) literature and thus facilitate in grounding the generated limitations in existing scientific findings. We enhance limitation generation by leveraging RAG techniques. Specifically, we prompt LLMs to query the Semantic Scholar API to retrieve papers related to the one under review, extracting relevant content to enrich their domain understanding. Our results demonstrate that incorporating RAG improves the ability of LLM systems to generate limitations in research papers, providing more concrete feedback. We summarize our contributions as follows: We propose LIMITGEN, comprehensive benchmark specifically designed to assess the ability of models to identify and address limitations in scientific research, with reliable and systematic evaluation framework. We evaluate the performance of LLMs and agentbased systems in identifying limitations and demonstrate their shortcomings in providing constructive and actionable feedback. We explore the potential of RAG in review generation, demonstrating its ability to improve limitation identification and generate more contextually relevant and actionable suggestions."
        },
        {
            "title": "2.1 Peer-Review Generation",
            "content": "Recent advances in LLMs have significantly influenced scientific research, offering tools to streamline and enhance researchers workflows across various stages of the scientific pipeline (Xu et al., 2024; Lu et al., 2024; Ajith et al., 2024; Wang et al., 2024; Zhou et al., 2024b; Si et al., 2024; Tian et al., 2024). Researchers have also extensively explored the potential of LLMs in automated peer review generation, employing various approaches such as guiding LLMs with single prompts (Liang et al., 2024), adopting two-stage review generation frameworks with question-guided prompts (Gao et al., 2024), and leveraging multi-agent systems (DArcy et al., 2024). Other studies simulate the complete review process as multi-round dialogue (Tan et al., 2024). However, some research has found that LLM-generated reviews often suffer from generic and paper-unspecific content (Du et al., 2024), are seldom entirely accurate, lack critical analysis, and fail to provide technical details (Zhou et al., 2024a). During the peer review process, identifying limitations is crucial task as it helps highlight weaknesses in study, guiding authors toward improvements and fostering scientific progress. Current research primarily focuses on generating the entire review. Du et al. (2024) collected human and LLM-generated reviews, each annotated by experts with fine-grained deficiency labels and explanations. Tyser et al. (2024) compares generated reviews of papers with and without inserted errors by evaluating review scores. Other studies have constructed several exceptionally short computer science papers, each with an inserted error (Liu and Shah, 2023), or focused on identifying weaknesses within single paragraph rather than an entire paper (Chamoun et al., 2024). Lou et al. (2024) extracted human-written weaknesses from peer reviews. However, these studies do not thoroughly evaluate whether LLM systems can effectively detect specific limitations in scientific research. In this work, we present comprehensive benchmark to evaluate models ability to identify and address limitations in AI research papers, comprising synthetic subset created via controlled perturbations and set of human-written limitations."
        },
        {
            "title": "2.2 Retrieval Augmented Generation",
            "content": "Despite showing promise in various tasks, LLMs face significant challenges when adopted to specialized domains, including hallucinations (Mallen et al., 2023; Mishra et al., 2024), conflict between outdated pre-training data and latest domain knowledge (Kasai et al., 2024), and lack of transparent attribution (Ye et al., 2024). Retrieval augmented generation that integrates external knowledge has emerged as pivotal strategy to address these limitations (Lewis et al., 2020; Shuster et al., 2021; Izacard et al., 2023), enabling LLMs to produce more accurate and context-aware outputs. Recent studies use proprietary LLMs with external APIs (e.g., Semantic Scholar API & Google Search API) (Agarwal et al., 2024; Skarlinski et al., 2024; Chamoun et al., 2024) or develop new methodologies to train specialized open models (Asai et al., 2024) for tasks such as scientific literature review. Furthermore, multiple-round retrieval-enhanced reasoning methods have been developed to improve retrieval effectiveness (He et al., 2022; Shao et al., 2023; Jiang et al., 2023; Chen et al., 2024). In this work, we introduce novel approach that incorporates literature retrieval into the limitation generation process, enabling LLMs to utilize domain knowledge and produce more constructive feedback."
        },
        {
            "title": "3 LIMITGEN Benchmark",
            "content": "This section discusses the task formulation of LIMITGEN and details the data construction process used to curate its two subsets."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "We formally define the task of limitation generation in the context of LLMs as follows: Given: (1) scientific paper, which may either contain major limitation explicitly introduced (i.e., LIMITGEN-Syn discussed in 3.3), or exhibit limitations previously identified during peer review by human reviewers (i.e., LIMITGEN-Human discussed in 3.4); and (2) an aspect of limitations, which serves as focus point for the LLM to evaluate specific dimension of the papers quality. The LLM is tasked with generating the limitation for the given paper, reflecting its quality with respect to the specified aspect."
        },
        {
            "title": "3.2 Desiderata and Taxonomy of Limitations",
            "content": "The identification and categorization of limitations in scientific research require careful consideration of what constitutes meaningful limitation. Through our pilot analysis of peer reviews, we establish several key desiderata that guide our taxonomy of limitations. First, research limitation should represent substantive constraint or weakness that impacts the validity, generalizability, or reliability of the studys findings. These constraints may arise from methodological choices, resource limitations, or gaps in current scientific understanding. Importantly, limitations should be distinguished from superficial critiques of presentation style. Second, limitations should be actionable - they should point to specific aspects of the research that could be improved through concrete steps. This ensures that identifying limitations serves constructive purpose in advancing scientific knowledge, rather than simply highlighting unAspect Limitation Subtype Definition and Corresponding Data Example Low Data Quality Methodology Inappropriate Method The data collection method is unreliable, potentially introducing bias (Figure 4) and lacking adequate preprocessing Some methods in the paper are unsuitable for addressing this research (Figure 5) question and may lead to errors or oversimplifications Experimental Design Insufficient Baselines Fail to evaluate the proposed approach against broad range of well- (Figure 6) established methods Limited Datasets Rely on limited datasets, which may hinder the generalizability and (Figure 7) robustness of the proposed approach Inappropriate Datasets Use of inappropriate datasets, which may not accurately reflect the (Figure 8) target task or real-world scenarios Lack of Ablation Studies Fail to perform an ablation study, leaving the contribution of certain (Figure 9) component to the models performance unclear Limited Analysis Result Analysis Insufficient Metrics Limited Scope Irrelevant Citations Literature Review Inaccurate Description Rely on insufficient evaluation metrics, which may provide an incom- (Figure 10) plete assessment of the models overall performance Offer insufficient insights into the models behavior and failure cases (Figure 11) The review may focus on very specific subset of literature or methods, leaving out important studies or novel perspectives (Figure 12) Include irrelevant references or outdated methods, which distract from the main points and undermine the strength of conclusions (Figure 13) Provide an inaccurate description of existing methods, which can hinder readers understanding of the context and relevance of the (Figure 14) proposed approach Table 1: The types of scientific paper limitations included in the LIMITGEN-Syn subset. avoidable constraints. For instance, limitation regarding insufficient experimental validation should suggest specific additional experiments that would strengthen the work. Third, limitations should be grounded in established scientific principles and practices within the relevant domain. This requires domain expertise to properly identify and articulate limitations that reflect meaningful departures from best practices or gaps in scientific rigor. For instance, appropriate evaluation metrics for each task are well-known within each subfield. Based on these desiderata and our analysis of peer review comments from top AI conferences, we categorize research limitations into four primary aspects  (Table 1)  : (i) Methodological Limitations focus on the fundamental approaches and techniques employed in the research. These include issues such as inappropriate choice of methods, unstated assumptions that may not hold, and problems with data quality or preprocessing that could introduce bias. Such limitations directly impact the validity of the research findings. (ii) Experimental Design Limitations encompass weaknesses in how the research validates its claims. This category includes insufficient baseline comparisons, limited datasets that may not represent the full problem space, and lack of ablation studies to isolate the contribution of different components. These limitations affect the reliability and reproducibility of results. (iii) Results and Analysis Limitations relate to how findings are evaluated and interpreted. This includes using inadequate evaluation metrics that may not capture important aspects of performance, insufficient error analysis, and lack of statistical significance testing. These limitations impact the strength (iv) Literaand generalizability of conclusions. ture Related Limitations focus on how the research connects to and builds upon existing work. This includes missing citations of relevant prior work, mischaracterization of existing methods, and failure to properly contextualize contributions within the broader research landscape. These limitations affect both the novelty claims and the proper attribution of ideas. This taxonomy guides our creation of the LIMITGEN benchmark by ensuring we systematically evaluate different types of limitations that matter for scientific rigor. For each aspect, we identify specific subtypes of limitations that commonly appear in peer reviews and can be reliably assessed. The taxonomy also informs our evaluation criteria, as different types of limitations may require different forms of evidence and levels of domain knowledge to properly identify. Property (avg./max) Value"
        },
        {
            "title": "3.3 LIMITGEN-Syn Subset Collection",
            "content": "Source Paper Collection We collect scientific papers from arXiv under the Computation and Language category, focusing on those released between March 1, 2024, and May 31, 2024, period likely outside the pretraining data cut-off for most current LLMs. This selection helps minimize potential data memorization issues that affect model evaluation. To extract content, we use the tool2 by Lo et al. (2020), which converts LaTeX source files into JSON format, capturing elements including the title, abstract, main sections, and appendix of each paper. In total, we compile an initial pool of 1,408 NLP papers for further annotation. We exclude papers that do not focus on experimental work, such as surveys, position papers, and dissertations, as these lack the experimental designs required for our analysis. Additionally, papers of insufficient quality are omitted to ensure that the introduced limitation represents the most critical issue in each paper. This filtering process led us to 500 papers. Example Curation Following the taxonomy in Table 1, we design perturbation pipelines for each limitation subtype. For each paper, human experts determine the applicable perturbations and then apply all suitable perturbations accordingly. The annotators identify all the relevant sections in the paper based on the perturbation type. For each selected section, we employ GPT-4o to perturb the content according to the specific definitions and guidelines, such as removing relevant details or replacing particular dataset. The prompts are provided in Figure 4 to Figure 14. Alongside each perturbation, we generate brief description of the introduced limitation as the ground truth, which will serve as reference for later evaluations. Human Expert Validation To guarantee the reliability of our LIMITGEN-Syn dataset, each annotated example is evaluated by human annotator based on the following criteria: (1) The text within the paper must be grammatically correct and maintain clarity. (2) The introduced limitation must genuinely impact the quality and represent the most critical issue in the given aspect. (3) The generated ground truth limitation should clearly articulate the problem and be reasonable. Validators are tasked 2https://github.com/allenai/ s2orc-doc2json LIMITGEN-Syn Scientific Paper Word Length Limitation Word Length Paper Number Example Number 5,201.46 / 58,788 34.45 / 81 500 1,000 LIMITGEN-Human Scientific Paper Word Length Limitation Word Length Number of Limitations per Paper Paper Number 8,255.38 / 1,8910 61.97 / 795 6.05 / 20 1,000 Table 2: Data statistics of the LIMITGEN benchmark. Figure 2: The aspect distribution of human-written limitations in LIMITGEN-Human. with revising or removing examples that do not meet these standards. In practice, from 500 papers, total of 1,000 examples were retained, including 112 that were revised by human annotators. We provide the details of annotators involved in dataset construction in Table 7."
        },
        {
            "title": "3.4 LIMITGEN-Human Subset Collection",
            "content": "To assess whether our taxonomy and the synthetic benchmark can effectively capture the diverse categories of limitations identified by humans in realworld peer review settings, we then collect humanwritten limitations from ICLR 2025 submissions. We specifically focus on the weaknesses sections of each papers reviews and break them down into itemized limitations. To ensure quality, we use GPT-4o to exclude weaknesses that are too short (fewer than 20 words) or lack substantive suggestions, and then categorize the remaining limitations. The prompt is provided in Figure 16. We retained only the limitations related to methodology, experimental design, result analysis, and literature review, considering them as ground truth. We collect total of 9,844 papers and randomly sample 1,000 of them for experimentation."
        },
        {
            "title": "3.5 Data Statistics",
            "content": "Table 2 illustrates the data statistics of our benchmark. Figure 2 presents the detailed aspect distributions of the LIMITGEN-Syn subset. The full LIMITGEN benchmark consists of 2,000 examples and encompasses diverse range of aspect types commonly found in paper limitations."
        },
        {
            "title": "4 LIMITGEN Evaluation Protocol",
            "content": "Evaluating the quality of limitations generated by LLMs is inherently challenging due to the subjective and nuanced nature of research critique. Such assessments typically require expert-level judgment, making human evaluation labor-intensive. Moreover, comparing generated limitations with ground-truth ones is non-trivial, as valid limitations may differ in phrasing or granularity. These challenges motivate the careful design of our evaluation protocol to ensure both reliability and scalability."
        },
        {
            "title": "4.1 Human Evaluation Protocol",
            "content": "Human Evaluation Process. For LIMITGENSyn, we assess whether they correctly identify the intended subtype and calculate the accuracy. For LIMITGEN-Human, we assess the generated limitations across three dimensions: faithfulness, soundness, and importance. The detailed criteria are presented in the Appendix A.3. For each criterion, Likert-scale scores ranging from 1 to 5 are used. Given the paper and limitation generated by LLM, human evaluators are asked to assign scores for each dimension. Initially, ground truth references are not provided, minimizing potential bias from direct comparisons to the reference, as LLMs can generate limitations that are reasonable but not explicitly included in peer reviews. After submitting their initial scores, evaluators are then provided with the reference and asked to adjust their scores if they identify any aspects that may have been overlooked. Ensuring Reliable and Reproducible Human Evaluation. To ensure the reliability and reproducibility of our human evaluation, we develop detailed assessment guideline, provided in Appendix A.3. To measure inter-annotator agreement, we sample 50 fixed generated instances from LIMITGEN-Syn and LIMITGEN-Human, each independently assessed by two expert annotators. In LIMITGEN-Syn, the resulting Cohens Kappa score is 0.833. In LIMITGEN-Human, the scores for the criteria of importance, faithfulness, and soundness are 0.772, 0.735, and 0.717, respectively, indicating high level of consistency among evaluators."
        },
        {
            "title": "4.2 Automated Evaluation Protocol",
            "content": "To automatically evaluate the quality of the generated limitations, we compare them with the groundtruth limitations using two-step process. Coarse-grained Evaluation. For LIMITGENSyn, we use GPT-4o to classify the generated limitations and assess whether they correctly identify the intended subtype. Accuracy is used as the evaluation metric: sample is deemed correct in the coarse-grained evaluation if at least one generated limitation accurately matches the subtype. For LIMITGEN-Human, we refer to MARG (DArcy et al., 2024), evaluating recall, precision, and Jaccard Index to measure the overlap between generated and ground truth limitations for paper. These metrics are then averaged across all papers to produce single aggregated value for each metric. Fine-grained Evaluation. If generated limitation correctly identifies the subtype or has successful match in the ground truth limitations, we further evaluate the content to determine its alignment with the ground truth. This is achieved through reference-based evaluation using GPT-4o, which assigns scores to the generated limitations on from 1 to 5. These scores are based on two key criteria: relatedness to the ground truth and specificity in addressing the identified issue. Limitations that fail to determine the subtype or do not have match during the coarse-grained evaluation are excluded from fine-grained evaluation and assigned score of 0. For LIMITGEN-Syn, we calculate the average of the highest scores assigned to the limitations of each paper in fine-grained evaluation. For LIMITGEN-Human, we calculate the average of all limitations for each paper and then compute the overall average across all papers. This provides holistic measure of the systems performance across both accuracy and quality dimensions. Reliability Assessment. To validate the performance of our automated evaluation system, we also calculate the system correlation between the automated fine-grained evaluation and the human evaluation, using data presented in Table 3 and TaFigure 3: An overview of RAG pipeline. We prompt LLMs to query the Semantic Scholar API, retrieve recommended papers, and rerank them based on their abstracts. ble 4. In LIMITGEN-Syn, the correlation between the fine-grained score and accuracy is 0.96. In LIMITGEN-Human, the correlation between the fine-grained score and faithfulness, soundness, and importance scores are 0.77, 0.60, and 0.67. By comparing with ground truth, our automated evaluation system can effectively assess the quality of the generated limitations."
        },
        {
            "title": "5 Evaluated Systems",
            "content": "We next discuss the systems evaluated in our experiments, including LLMs, agent-based system, and RAG-enhanced pipeline."
        },
        {
            "title": "5.1 Evaluated LLMs",
            "content": "We evaluate the performance of 4 frontier LLMs across two distinct categories in our benchmark: (1) Proprietary LLMs, including GPT-4o and GPT4o-mini (OpenAI, 2024); and (2) Open-source LLMs, including Llama-3.3-70B (AI@Meta, 2024), Qwen2.5-72B (Yang et al., 2024). We require each model to generate the most significant limitations for an aspect of paper. In the LIMITGEN-Syn experiments, we measure whether models identify the single most prominent limitation in each paper within their top three generated limitations, ensuring fair comparison across systems."
        },
        {
            "title": "5.2 Evaluated Agent-based System.",
            "content": "We also present our multi-agent approach for generating limitations. Our architecture, following MARG (DArcy et al., 2024), consists of set of chat-based LLM agents (GPT-4o-mini in this study), each with its own chat history and prompt(s). The system includes three distinct agent roles: (1) leader agent, responsible for coordinating tasks among agents; (2) worker agent, which processes the full text of the paper; and (3) an expert agent, prompted to focus on specialized sub-task to support the leader. With task instructions for each aspect, the leader delegates specific sub-tasks to the other agents and synthesizes their responses to produce the final limitations."
        },
        {
            "title": "5.3 RAG-Enhanced Limitation Generation",
            "content": "In preliminary testing, we observed that LLMs often fail to detect limitations or provide substantive suggestions due to lack of knowledge in related areas. To address this, we enhanced the evaluated systems capabilities by incorporating the RAG module, method proven effective for knowledgeintensive tasks (Lewis et al., 2020; Shi et al., 2024), to ground limitation generation in the relevant literature. This method enables the LLMs to retrieve and consider related works when evaluating limitations in the given research paper. Specifically, the retrieval process leverages the Semantic Scholar API and adapts based on the input papers availability in the database. If the paper is available the database, we use its Semantic Scholar ID to fetch at most 20 recommended papers via the recommendation API3. If the paper is unavailable, we use GPT-4o-mini to generate query based on the papers abstract and use the relevance API4 to identify related papers. From this search, the top 3 results are treated as seed papers, and for each seed paper, 5 additional recommendations are retrieved through the recommendation API, yielding pool of 18 papers. These retrieved papers are then reranked by GPT-4o-mini, which assesses the similarity between the input paper and the candidates. The top 5 papers are selected. Due to LLMs context window constraints, directly providing all retrieved papers to them is impractical. We employ GPT-4o-mini to identify and 3https://api.semanticscholar.org/ api-docs/recommendations 4https://api.semanticscholar.org/ api-docs/graph#tag/Paper-Data/operation/ get_graph_paper_relevance_search Automated Eval. Human Eval. Systems"
        },
        {
            "title": "Coarse",
            "content": "Fine (0-5)"
        },
        {
            "title": "Accuracy",
            "content": "Automated Eval. Human Eval. (1-5) Jaccard Fine.(0-5) Faith. Sound. Import."
        },
        {
            "title": "Human",
            "content": "GPT-4o w/ RAG GPT-4o-mini w/RAG Llama-3.3-70B w/RAG Qwen-2.5-72B w/RAG"
        },
        {
            "title": "MARG",
            "content": "w/ RAG 86.0% 52.0% +12.2% 49.1% +4.2% 45.7% +2.4% 47.1% +1.2% 68.1% +9.8% 3.52 1.34 +0.37 1.25 +0.13 1.15 +0.05 1.20 +0. 1.83 +0.27 82.0% 45.9% +16.0% 37.8% +5.9% 32.7% +4.5% 31.5% +3.9% 54.8% +17.7% Table 3: Human and automated evaluation results of the LLMs and Agent-based system on LIMITGEN-Syn set averaged across all subtypes. For human evaluation, we randomly sample 100 examples from the dataset. extract content related to methodology, experimental design, result analysis, and literature review. This extracted content is then concatenated and used as concise reference to help the LLMs effectively identify limitations in these aspects. In experiments involving MARG, we enable the expert agent to retrieve related papers and provide specific suggestions based on the retrieved content while refining the initial limitation comments."
        },
        {
            "title": "6 Experiment Results",
            "content": "This section presents our main findings and indepth analysis."
        },
        {
            "title": "6.1 Results and Analysis",
            "content": "{ RQ1: How well do LLM-based systems perform in identifying limitations within scientific research? Table 3 shows the performance of the evaluated systems on LIMITGEN-Syn. In Appendix B, we provide more detailed results on their performance for each subtype of limitation. The results demonstrate that identifying limitations in scientific papers remains significant challenge for current LLMs. Even the best-performing LLM, GPT-4o, can only identify about half of the limitations that humans consider very obvious. Although MARG leverages multi-agent collaboration and generates more comments, successfully identifying more limitations, the feedback it provides still lacks specificity, GPT-4o w/ RAG GPT-4o-mini w/ RAG 15.9% +2.9% 15.5% +0.6% Llama-3.3-70B 16.3% +0.1% w/ RAG Qwen-2.5-72B 14.4% +1.0% w/ RAG MARG w/ RAG 15.2% +2.5% 0.42 +0.13 0.39 +0.01 0.39 +0.04 0.53 +0. 0.66 +0.24 3.19 2.84 +0.49 +1.13 2.78 3.03 +0.28 +0.77 2.98 2.85 +0.23 +0.70 2.86 2.91 +0.22 +0.35 3.60 3.19 +0.52 +0. 3.49 +0.60 2.97 +0.53 3.05 +0.21 2.94 +0.34 3.78 +0.43 Table 4: Human and automated evaluation results of the LLMs and Agent-based system on LIMITGEN-Human set averaged across all aspects. We randomly sample 100 examples from the dataset for human evaluation. which is reflected in the fine-grained scores. Table 4 shows the performance of the evaluated systems on LIMITGEN-Human, while the results on their performance for each aspect are illustrated in Appendix B. MARG outperforms all LLMs in terms of fine-grained scores and human evaluation but generates more comments than the other baselines, resulting in lower Jaccard scores. Consistent with the findings on LIMITGEN-Syn, the performance of all systems in LIMITGEN-Human remains quite poor. Their generated insights and feedback for top AI conference submissions lack depth and inspiration, especially when compared to those provided by experienced reviewers."
        },
        {
            "title": "6.2 Analysis of RAG Pipeline",
            "content": "{ RQ2: Can RAG enhance LLMs ability to identify limitations and provide constructive suggestions? Overall Results. While LLMs currently struggle to identify limitations in scientific papers and provide constructive advice, there is potential for them to offer better feedback if they can retrieve relevant literature to address their gaps in domain knowledge and understanding of the research context. We conducted experiments on all evaluated systems with the integration of the RAG pipeline. As shown in Table 3 and Table 4, incorporating the RAG method can enhance LLM performance in refining their outputs. Impact of Retrieved Content Quality on LLM Performance. We also investigate the impact of Systems Automated Eval. Human Eval. (1-5) Jaccard Fine.(0-5) Faith. Sound. Import. User Study GPT-4o GPT-4o-mini 15.0% 0.36 w/ RAG (Top 5) +1.4% +0.05 w/ RAG (Top 3) +1.3% +0.04 w/ RAG (Last 5) +0.8% +0.03 3.03 2.97 2.78 +0.28 +0.77 +0.53 +0.19 +0.56 +0.31 +0.07 +0.09 +0. Table 5: Human and automated evaluation results of different RAG settings from 18 retrieved papers on the subset of 100 examples from LIMITGEN-Human. the quality of retrieved content on LLM performance. In LIMITGEN-Human, we randomly sample 100 examples and conduct experiments by GPT4o-mini. For each example, we provide another two sets of retrieved papers as references: the top 3 ranked papers and the last 5 papers after re-ranking, from the 18 retrieved papers. The results, as shown in Table 5, demonstrate that providing broader set of relevant papers, as in the standard RAG method with the top 5 papers, improves the LLMs performance in generating accurate limitations compared to using only the top 3 or the last 5 papers. RAG consistently provides some benefits, even when the retrieved papers are not the most relevant. Case Study. We further conduct case study to analyze the impact of RAG on LLM systems ability to identify limitations. We select total of 20 examples from both subsets, each successfully matching the targeted limitation subtype or receiving all three ratings of 4 or higher in human evaluation. See the Appendix B.4 for some of the examples. Retrieved external knowledge provides LLMs with up-to-date domain information and offers standard practices for addressing specific issues. By comparing relevant papers with the examined paper, LLM systems are better equipped to identify problems. Systems with stronger reasoning capabilities, such as GPT-4o and MARG, benefit the most from RAG, as they can leverage external information to derive meaningful insights and improve their analysis."
        },
        {
            "title": "6.3 User Studies on Real-world Scenarios",
            "content": "{ RQ3: How can this research be applied in real-world scenarios to assist human researchers in improving their work? Our research focuses primarily on the AI domains. To investigate the applicability of our findings in more real-world scenarios, we design the following user studies to explore the domain generalization NLP Domain (as LIMITGEN-Syn) Biomedical Domain Computer Network Domain GPT-4o w/ RAG NLP Domain (as LIMITGEN-Syn) Biomedical Domain Computer Network Domain Llama-3.3-70B NLP Domain (as LIMITGEN-Syn) Biomedical Domain Computer Network Domain Llama-3.3-70B /RAG NLP Domain (as LIMITGEN-Syn) Biomedical Domain Computer Network Domain Acc. 45.9% 31.3% 37.5% 61.9% 50.0% 56.3% 32.7% 25.0% 31.3% 37.2% 31.3% 37.5% Table 6: Human evaluation result of the adaptability of our research across different scientific domains. of our research. Specifically, we examine the areas of biomedical sciences and computer networks. We first engage two experts in the two domains, each providing five research papers from their respective fields, focusing on those published after May 15, 2024, with which they are familiar. Following the annotation procedure outlined in LIMITGEN-Syn, the experts design perturbations across four aspects and annotate 32 examples in total. We then present another two experts with the perturbed papers and the generated limitations under two conditions: one utilizing our RAG pipeline and one without. As shown in Table 6, the human evaluation scores for GPT-4o and Llama-3.3-70B are consistent with the results observed in our main experiments. Our retrieval pipeline enhances the ability of LLMs to identify limitations. We believe that future work could further extend our research framework to encompass additional scientific domains."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper presents LIMITGEN, the first benchmark designed for systematically evaluating models on identifying and addressing scientific research limitations, supported by reliable and systematic evaluation framework. We also demonstrate how RAG enhances limitation generation, showcasing its ability to help models identify weaknesses and provide more constructive feedback. Through comprehensive analysis of LLM-based approaches for identifying different types of limitations, we offer key insights to guide future advancements."
        },
        {
            "title": "Ethical Considerations",
            "content": "This project is supported by Tata Sons Private Limited, Tata Consultancy Services Limited, and Titan. We are grateful to Nvidia Academic Grant Program for providing computing resources."
        },
        {
            "title": "Limitations",
            "content": "While our study provides valuable insights into the ability of LLM systems to identify limitations in scientific papers, several limitations remain that present opportunities for future work. First, our work does not include non-textual inputs such as figures, which are integral to many scientific papers. As figures often provide crucial evidence or highlight key findings, future extensions to our benchmark could incorporate multimodal inputs to better evaluate LLMs ability to identify limitations arising from inconsistencies or omissions in visual data. Second, this study does not explore advanced RAG techniques. Our focus is on assessing the potential of LLM systems in this context rather than optimizing retrieval methods. We encourage researchers to build upon our benchmark and investigate advanced retrieval methods to further improve limitation identification. Lastly, while our benchmark offers valuable insights into model performance, there are several limitations that should be considered. The current benchmark covers limited time span, including some parts of 2024 and ICLR 2025, which may not fully represent the evolving landscape of research in the field. Given the rapid advancements in NLP, it is important to regularly update the benchmark to incorporate the latest publications. Another potential limitation lies in the reliance on our automated evaluation method. Inherent biases in these systems could affect the accuracy and reliability of the overall evaluation. Additionally, our taxonomy and benchmark focus primarily on AI, as this is the field we are most familiar with. Although we conducted user study to assess its applicability to other domains, the nuances of different scientific disciplines may introduce challenges and limitation types that our framework does not fully address. Future work could expand this taxonomy by collaborating with experts from diverse fields, such as medicine, physics, and social sciences, to ensure broader generalizability. We have carefully considered the ethical implications of our work, which focuses on identifying limitations in scientific papers. Our approach is designed to assist human reviewers by offering complementary insights rather than replacing their essential role in the peer review process. We acknowledge potential risks, such as biases in LLMgenerated outputs and the potential to undermine the integrity of scientific evaluations if these systems are misused. Our study emphasizes that LLMs are far from achieving the level of expertise and nuanced understanding of human experts. Future developments in this field should prioritize transparency, fairness, and risk mitigation to ensure these tools are employed responsibly. Furthermore, the raw paper data used in our study is collected from arXiv with distributed under the CC BY 4.0 (Creative Commons Attribution 4.0 International) license. In alignment with this licensing framework, we will release our dataset under the same CC BY 4.0 license. This ensures that our dataset remains freely accessible while requiring proper attribution to the original sources, thereby maintaining legal and ethical compliance with the terms under which the original data was shared."
        },
        {
            "title": "References",
            "content": "Shubham Agarwal, Issam Laradji, Laurent Charlin, and Christopher Pal. 2024. Litllm: toolkit arXiv preprint for scientific literature review. arXiv:2402.01788. AI@Meta. 2024. The llama 3 herd of models. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. 2024. LitSearch: retrieval benchmark for scientific literature search. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1506815083, Miami, Florida, USA. Association for Computational Linguistics. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, et al. 2024. Openscholar: Synthesizing scientific literature with retrieval-augmented lms. arXiv preprint arXiv:2411.14199. Eric Chamoun, Michael Schlichtkrull, and Andreas Vlachos. 2024. Automated focused feedback generation for scientific writing assistance. In Findings of the Association for Computational Linguistics: ACL 2024, pages 97429763, Bangkok, Thailand. Association for Computational Linguistics. Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, and Ziliang Zhao. 2024. Generalizing conversational dense retrieval via LLM-cognition data augmentation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27002718, Bangkok, Thailand. Association for Computational Linguistics. Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. 2024. Marg: Multi-agent review arXiv preprint generation for scientific papers. arXiv:2401.04259. Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Cheng Jiayang, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, and Wenpeng Yin. 2024. LLMs assist NLP researchers: Critique paper (meta-)reviewing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 50815099, Miami, Florida, USA. Association for Computational Linguistics. Zhaolin Gao, Kianté Brantley, and Thorsten Joachims. Reviewer2: Optimizing review generaarXiv preprint 2024. tion through prompt generation. arXiv:2402.10886. Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. Mlagentbench: Evaluating language agents on machine learning experimentation. In Forty-first International Conference on Machine Learning. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992, Singapore. Association for Computational Linguistics. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah Smith, Yejin Choi, Kentaro Inui, et al. 2024. Realtime qa: whats the answer right now? Advances in Neural Information Processing Systems, 36. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. 2024. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196. Ryan Liu and Nihar Shah. 2023. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49694983, Online. Association for Computational Linguistics. Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, et al. 2024. Aaar-1.0: Assessing ais potential to assist research. arXiv preprint arXiv:2410.22394. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained hallucination detection and editing for language models. In First Conference on Language Modeling. OpenAI. 2024. Hello gpt-4o. Ori Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, and Matthias Bethge. 2024. CiteME: Can language models accurately cite scientific claims? In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 92489274, Singapore. Association for Computational Linguistics. Xi Ye, Ruoxi Sun, Sercan Arik, and Tomas Pfister. 2024. Effective large language model adaptation for improved grounding and citation generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62376251, Mexico City, Mexico. Association for Computational Linguistics. Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, RenJing Cui, Chengcheng Han, Qiushi Sun, et al. 2024. Automated peer reviewing in paper sea: Standardization, evaluation, and analysis. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1016410184. Ruiyang Zhou, Lu Chen, and Kai Yu. 2024a. Is llm reliable reviewer? comprehensive evaluation of llm on automatic paper reviewing tasks. In International Conference on Language Resources and Evaluation. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. 2024b. Hypothesis generation with large language models. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: Retrievalaugmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83718384, Mexico City, Mexico. Association for Computational Linguistics. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 37843803, Punta Cana, Dominican Republic. Association for Computational Linguistics. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can llms generate novel research ideas? largescale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109. Michael Skarlinski, Sam Cox, Jon Laurent, James Braza, Michaela Hinks, Michael Hammerling, Manvitha Ponnapati, Samuel Rodriques, and Andrew White. 2024. Language agents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740. Cheng Tan, Dongxin Lyu, Siyuan Li, Zhangyang Gao, Jingxuan Wei, Siqi Ma, Zicheng Liu, and Stan Li. 2024. Peer review as multi-turn and long-context dialogue with role-based interactions. arXiv preprint arXiv:2406.05688. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, et al. 2024. Scicode: research coding benchmark curated by scientists. arXiv preprint arXiv:2407.13168. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, et al. 2024. Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. arXiv preprint arXiv:2408.10365. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. 2024. Scimon: Scientific inspiration machines optimized for novelty. Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, and David Wadden. 2024. KIWI: dataset of knowledge-intensive writing instructions In Findings of for answering research questions. the Association for Computational Linguistics: ACL 2024, pages 1296912990, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671."
        },
        {
            "title": "A LIMITGEN Benchmark",
            "content": "Ensure that all relevant sections needing modifiA.1 LIMITGEN-Syn Table 8 illustrates the detailed distribution of the introduced limitation subtypes in our LIMITGENSyn. A.2 LIMITGEN-Human We randomly sample 1,000 papers from the ICLR 2025 submissions. We use GPT-4o to filter and classify the ground truth limitations, with the prompt provided in Figure 16 and Figure 15. A.3 Annotator Guidelines All annotators are experts with several NLP/ML publications as shown in Table 7. To ensure quality, they follow detailed annotation guidelines, which provide clear instructions for the annotation process. Source Paper Collection Our annotators follow the guidelines below to ensure only well-written arXiv papers are selected for perturbation: Exclude papers that do not focus on experimental work, such as surveys, position papers, and dissertations. Avoid papers with poorly written sections, lack of structure, or unprofessional presentation.. Ensure the methods are well-defined, reproducible, and grounded in established scientific principles. Avoid papers with vague or unsupported claims. Select papers that provide thorough experiments, proper baselines, and detailed evaluations. The results should be well-documented and statistically sound. The paper should present meaningful contribution to the field, such as novel approach, insights, or applications, rather than incremental work. Data Validation When validating the perturbation, annotators should follow these guidelines carefully to evaluate whether the perturbations meet the intended quality standards: Check that the generated perturbation aligns with the limitation type specified in the instruction and verify that GPT-4o strictly follows the provided instruction to introduce the intended limitation. cation are appropriately updated. Confirm that the perturbation does not compromise the clarity of the original text. Verify that the introduced limitation represents the most evident and significant limitation of the targeted aspect. Ensure the introduction of the limitation does not lead to unintended limitations elsewhere in the paper. The generated ground truth limitation should clearly articulate the problem and be reasonable. Human Evaluation For LIMITGEN-Human, we assess the generated limitations across three dimensions: Faithfulness: The generated limitations should accurately represent the papers content and findings, avoiding any introduction of misinformation or contradictions to the original concepts, methodologies or results presented. 5 points: Perfect alignment with the original content and findings, with no misinformation or contradictions. Fully reflects the papers concepts, methodologies, and results accurately. 4 points: Mostly aligns with the original content but contains minor inaccuracies or slight misinterpretations. These do not significantly affect the overall understanding of the papers concepts or results. 3 points: Generally aligns with the original content but includes several minor inaccuracies or contradictions. Some elements may not fully reflect the papers concepts or results, though the overall understanding is mostly intact. 2 points: Noticeable misalignment with the original content, with multiple inaccuracies or contradictions that could mislead readers. Some key aspects of the papers concepts or results are misrepresented. 1 point: Introduces significant misalignment by misrepresenting issues that do not exist in the paper. Creates considerable misinformation and contradictions that distort the original content, concepts, or results. Soundness: The generated limitations should be detailed and specific, with suggestions or critiques that are practical, logically coherent, and ID # NLP/AI Publication Data Annotation Data Validation Human Evaluation Human Performance 1 2 3 4 5 6 > 10 > 10 5-10 5-10 1-5 1-5 Table 7: Details of annotators involved in dataset construction and LLM performance evaluation. LIMITGEN is annotated by experts in NLP domains, ensuring both the accuracy of the benchmark and the reliability of the human evaluation. Property Methodology # Low Data Quality # Inappropriate Method Experimental Design # Insufficient Baseline # Limited Datasets # Inappropriate Datasets # Lack of Ablation Study Result Analysis # Limited Analysis # Insufficient Metrics Experimental Design # Limited Scope # Irrelevant Citations # Inaccurate Description Value 250 125 125 250 62 63 63 62 250 125 125 250 83 84 83 Table 8: Subtype distribution in the LIMITGEN-Syn subset. purposeful. It should clearly address relevant aspects of the paper and offer insights that can genuinely improve the research. 5 points: Highly detailed and specific, with practical, logically coherent, and purposeful suggestions. Clearly addresses relevant aspects and offers insights that substantially improve the research. 4 points: Detailed and mostly specific, with generally practical and logically sound suggestions. Addresses relevant aspects well but may lack depth or novelty in some areas. 3 points: Detailed and specific but with some issues in practicality or logical coherence. Suggestions are somewhat relevant and offer partial improvements. 2 points: Somewhat vague or lacking in specificity, with suggestions that have limited practicality or logical coherence. Addresses relevant aspects only partially and provides minimal improvement. 1 point: Lacks detail and specificity, with impractical or incoherent suggestions. Fails to effectively address relevant aspects or offer constructive insights for improvement. Importance: The generated limitations should address the most significant issues that impact the papers main findings and contributions. They should highlight key areas where improvements or further research are needed, emphasizing their potential to enhance the researchs relevance and overall impact. 5 points: Addresses critical issues that substantially impact the papers findings and contributions. Clearly identifies major areas for significant improvement or further research, enhancing the researchs relevance and overall impact. 4 points: Identifies meaningful issues that contribute to refining the papers findings and methodology. While the impact is notable, it does not reach the level of fundamentally shaping future research directions. 3 points: Highlights important issues that offer some improvement to the current work but do not significantly impact future research directions. Provides useful insights for refining the paper but lacks broader implications for further study. 2 points: Points out limitations with limited relevance to the papers overall findings and contributions. Suggestions offer marginal improvements but fail to address more substantial gaps in the research. 1 point: Focuses on trivial issues, such as minor errors or overly detailed aspects. Does not address substantive issues affecting the papers findings or contributions, limiting its overall relevance and impact. A.4 Human Baseline To obtain an informative estimate of expert-level performance on LIMITGEN, we randomly sample 50 examples from each subset. Two expert annotators (i.e., Annotators 1 and 6, as described in Table 7) independently solve these examples. During human evaluation, the expert evaluators are not informed of the sources of these generated limitations. We report the evaluation results on Table 3 and Table 4. A.5 Limitation Taxonomy Figure 4: An example of Low Data Quality and its perturbation implementation. Figure 5: An example of Inappropriate Method and its perturbation implementation. Figure 6: An example of Insufficient Baseline and its perturbation implementation. Figure 7: An example of Limited Datasets and its perturbation implementation. Figure 8: An example of Inappropriate Datasets and its perturbation implementation. Figure 9: An example of Lack of Ablation Study and its perturbation implementation. Figure 10: An example of Limited Analysis and its perturbation implementation. Figure 11: An example of Insufficient Metrics and its perturbation implementation. Figure 12: An example of Limited Scope and its perturbation implementation. Figure 13: An example of Irrelevant Citations and its perturbation implementation. Figure 14: An example of Inaccurate Description and its perturbation implementation. Limitation Aspect Classification [System Input]: Please classify the following limitation of scientific paper into one of the following aspects: clarity, methodology, experimental design, result analysis, literature review, or others. Output only the corresponding aspect. Classification Criteria: Clarity: Issues in the presentation, structure, or language that hinder readers understanding of the studys purpose, methods, results, or conclusions. Methodology: Problems with the selection, application, or justification of research methods, such as unreliable data collection techniques or limited novelty, affecting the robustness or interpretability of findings. Experimental Design: Shortcomings in the studys structure or execution, such as inappropriate dataset selection, lack of controls, or absence of ablation studies, which undermine the validity or generalizability of results. Result Analysis: Problems in interpreting or presenting data, such as overgeneralization, missing case studies, or using inappropriate metrics, compromising the accuracy or validity of conclusions. Literature Review: Issues with the literature review, such as omission of relevant studies, outdated sources, or incomplete synthesis of research, leading to biased conclusions or an incomplete understanding of the topic. [User Input]: Limitation: {limitation} Limitation Filter [System Input]: Based on the review comments of scientific paper below, retain only those comments that provide substantive suggestions. Merge comments discussing the same issue without omitting or summarizing any content. Present the final set of comments. [User Input]: Comments: {comments} Figure 15: Prompt for categorizing ground truth limitations in LIMITGEN-Human. Figure 16: Prompt for filtering ground truth limitations in LIMITGEN-Human."
        },
        {
            "title": "B Experiments",
            "content": "B.1 Experiment Setup Limitation Generation w/o RAG [System Input]: Read the following scientific paper and generate major limitations in this paper about its {aspect}. Do not include any limitation explicitly mentioned in the paper itself and return only the limitations. Limitation Generation w/ RAG [System Input]: Read the following content from several papers to gain knowledge in the relevant field. Using this knowledge, review new scientific paper in this field. Based on existing research, identify the limitations of the Paper to Review. Generate major limitations in this paper about its {aspect}. Do not include any limitation explicitly mentioned in the paper itself and return only the limitations. [User Input]: Paper to review: Title: {Title} {Paper} Figure 17: Prompt for limitation generation w/o RAG. B.1.1 Agent-based System the adopt structure fundamental We of MARG (DArcy et al., 2024), with modifications made better to align it with the requirements of our task. Each agent in the system begins with unique \"system\" message at the start of its message history to provide specific instructions tailored to its role. For example, the \"leader\" agent is instructed to act as the leader; its role includes coordinating other agents to fulfill the users requests. Additionally, the leader is guided to create high-level plan based on its task instructions before initiating communication or delegating sub-tasks. Only the worker agent has access to the full content of the It is prompted to follow paper being reviewed. the leaders instructions to locate and summarize relevant content. The \"expert\" agent receives detailed instructions specific to their expertise area, focusing on particular subtasks they are responsible for. Both the leader and the expert agents can view the workers responses, but the worker can only see the directives provided by the leader. When paper is input, the leader first organizes all agents to generate candidate initial comments collectively. Then, each comment is individually discussed and refined into detailed limitation or discarded. In the RAG setting experiments, experts can reference related papers during the refinement stage. This enables them to leverage the latest literature to acquire domain-specific knowledge, thereby enhancing the quality and relevance of the generated feedback. We modified the prompts for [User Input]: Relevant Paper 1: Title: {Title 1} {Retrieved Content 1} Relevant Paper 2: Title: {Title 2} {Retrieved Content 2} Relevant Paper 3: Title: {Title 3} {Retrieved Content 3} Relevant Paper 4: Title: {Title 4} {Retrieved Content 4} Relevant Paper 5: Title: {Title 5} {Retrieved Content 5} Paper to review: Title: {Title} {Paper} Figure 18: Prompt for limitation generation w/ RAG. each agent according to the specific requirements, which are presented from Figure 21 to Figure 33. B.2 LIMITGEN-Syn Experiments In this section, we discuss the detailed results for each subtype/aspect in LIMITGEN-Syn, as presented in Table 9 to Table Overall, LLMs perform best in identifying limitations within the Result Analysis aspect of scientific papers. This may be due to the fact that this aspect often involves more directly interpretable and quantifiable elements, such as statistical results and performance metrics, which LLMs are wellequipped to assess. As result, the integration of RAG provides minimal improvement in this aspect. In contrast, LLMs perform the weakest in identifying limitations within the Literature Review aspect, Limitation Aspect Check [System Input]: Please check whether the following limitation of scientific paper is related to the {aspect}. Output only \"yes\" or \"no\". [User Input]: Limitation:{limitation} Limitation Subtype Classification [System Input]: Please classify the following limitation of scientific paper into one of the following subtypes: {limitation subtypes & explanations in this aspect} [User Input]: Limitation:{limitation} Figure 19: Prompt for Coarse-grained Evaluation in LIMITGEN-Syn. as this aspect requires deeper understanding of the existing body of work and how it contextualizes the paper being reviewed. RAG demonstrates its greatest impact in the identification of limitations related to Experimental Design. This is likely because referencing relevant baseline methods or datasets from the retrieved papers helps enhance the specificity of the limitations. By providing more concrete examples or comparisons, RAG enables LLMs to offer more detailed and actionable suggestions, thereby improving the overall quality of the generated limitations in this area. Within the same aspect, LLMs demonstrate varying abilities to identify different limitation subtypes, and RAG also influences performance differently across these subtypes. For instance, in the Methodology aspect, the identification of limitations related to low data quality outperforms that of inappropriate methods. This discrepancy is likely due to the inherent complexity of the inappropriate method limitation, which requires deeper understanding of the papers core arguments and methodology. In contrast, low data quality limitations are more straightforward and are often supported by references from retrieved papers, which may include information on similar data preprocessing techniques. As result, RAG is particularly effective in assisting with the generation of limitations related to low data quality. Fine-grained Evaluation [System Input]: Compare the following pair of limitations of scientific paper: one generated and one from the ground truth. Assess the degree of relatedness and specificity of the generated limitation compared to the ground truth limitation. Rating Criteria: - 5 points: The generated limitation discusses exactly the same content as the ground truth and provides similar level of detail. - 4 points: The generated limitation discusses exactly the same content as the ground truth, but it is less detailed than the ground truth. - 3 points: The generated limitation is related to the ground truth, but not identical. - 2 points: The generated limitation is only loosely related to the ground truth. - 1 point: There is no connection between the generated limitation and the ground truth. Provide brief explanation, then assign rating (1-5). [User Input]: Ground truth limitation: {ground truth} Generated limitation: {generated limitation} Figure 20: Prompt for Fine-grained Evaluation in LIMITGEN. In contrast, LLMs perform the weakest in identifying limitations within the Literature Review aspect, which is consistent with the results observed in our LIMITGEN-Syn. And RAG proves to be most helpful in this aspect. By retrieving and incorporating relevant papers, RAG helps the model identify missing references, overlooked methodologies, or underexplored areas, leading to more comprehensive and informed limitations. B.3 LIMITGEN-Human Experiments Automated Evaluation Metrics Given set of generated limitations Cgen and set of ground truth limitations Cgt for paper, each generated limitation is paired with every ground truth limitation of the same aspect. GPT-4o assesses the degree of relatedness for each pair, categorizing them as \"none,\" \"weak,\" \"medium,\" or \"high.\" Pairs rated \"medium\" or \"high\" are counted as successful matches. Using the alignments between Cgen and Cgt, we evaluate several metrics, as described below. We refer to MARG (DArcy et al., 2024) and and define directional intersection operators to represent the set of aligned elements in the left or right operand, respectively. For example, Cgt is the set of elements of Cgen that align Cgen to any element in Cgt. Cgt Recall: Cgen Cgt , the fraction of real reviewer comments that are aligned to any generated limitations. Cgt Precision: Cgen , the fraction of generated limitations that are aligned to any ground truth limitation. Cgen (Pseudo-)Jaccard: The Jaccard index is commonly used measure of set overlap. Let Intersection = Cgen Cgt+Cgen 2 intersection Cgen+Cgtintersection . Jaccard Index is ; then the Cgt We adopt macro-averaging approach at the individual paper level. We generate several limitations for each paper in the test set and compare them with the corresponding human-written limitations, calculating the relevant metrics for each comparison. These metrics are then averaged across all the papers to produce single aggregated value for each metric. Result Analysis Table 18 to Table 21 show the detailed result for all the aspects in LIMITGENHuman. Overall, LLMs exhibit higher overlap and better quality in generating limitations related to experimental design compared to human reviewers. This may be because experimental design often receives the most feedback from human reviewers, providing clearer reference in the automated evaluation. Also, limitations in experimental design tend to be more structured and objective, which makes it easier for LLMs to identify and refine issues. Figure 21: System prompt for the leader agent in MARG. Figure 22: Prompt for the leader agent in MARG on methodology. Figure 23: Prompt for the leader agent in MARG on experimental design. Figure 24: Prompt for the leader agent in MARG on result analysis. Figure 25: Prompt for the leader agent in MARG on literature review. Figure 26: Prompt for the leader agent in MARG at the refinement stage w/o RAG. Figure 27: Prompt for the leader agent in MARG at the refinement stage w/ RAG. Figure 28: System prompt for the worker agent in MARG. Figure 29: Prompt for the expert agent in MARG on methodology. Figure 30: Prompt for the expert agent in MARG on experimental design. Figure 31: Prompt for the expert agent in MARG on result analysis. Figure 32: Prompt for the expert agent in MARG on literature review. Figure 33: Prompt for the expert agent in MARG at the refinement stage."
        },
        {
            "title": "Inappropriate Method",
            "content": "Automated Eval. Human Eval. Automated Eval. Human Eval."
        },
        {
            "title": "Coarse",
            "content": "Fine (0-5)"
        },
        {
            "title": "Coarse",
            "content": "Fine (0-5)"
        },
        {
            "title": "Accuracy",
            "content": "GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 44.0% +16.0% 61.6% +1.6% Llama-3.3-70B 56.8% Llama-3.3-70B w/RAG +2.7% Qwen-2.5-72B Qwen-2.5-72B w/RAG 45.5% +4.7% MARG MARG w/ RAG 83.3% +12.2% 1.00 +0.56 1.67 +0.07 1.38 +0. 1.16 +0.13 2.17 +0.28 38.5% +15.3% 46.2% +7.6% 46.2% +0.0% 23.1% +7.7% 69.2% +23.1% 48.0% +4.0% 56.0% +4.0% 54.8% +3.4% 43.1% +0.4% 82.4% -3.5% 1.24 +0.00 1.10 +0.08 1.01 +0.15 0.86 -0.03 1.71 -0.18 33.3% +8.4% 41.7% +8.3% 25.0% +0.0% 16.7% +8.3% 75.0% +0.0% Table 9: Human and automated evaluation results on Methodology in LIMITGEN-Syn set. Insufficient Baseline Limited Datasets Inappropriate Dataset Lack of Ablation Study Systems Automated Human Automated Human Automated Human Automated Human Coarse Fine Acc. Coarse Fine Acc. Coarse Fine Acc. Coarse Fine Acc. GPT-4o w/ RAG 1.00 41.7% +25.0% +0.50 1.83 58.3% 50.0% +16.7% +16.7% +0.34 2.08 75.0% 50.0% +16.7% +16.7% +0.92 0.33 16.7% 66.7% +33.3% +8.3% +0. 16.7% +16.6% GPT-4o-mini w/RAG 27.0% 0.62 +3.2% +0.16 33.3% +0.0% 64.5% 1.94 +8.1% +0.24 33.3% 1.71 73.0% +16.7% +4.8% +0. 66.7% +0.0% 4.8% 0.11 +0.0% +0.00 0.0% +0.0% Llama-3.3-70B w/RAG 0.88 29.8% +16.0% +0. 2.05 66.1% 16.7% +16.6% -2.8% -0.12 0.84 35.5% 66.7% +16.6% +2.6% +0.14 0.27 9.7% 28.6% +14.3% +5.1% +0.09 0.0% +16.7% Qwen-2.5-72B w/RAG 44.4% 1.27 +3.2% +0. 33.3% 2.10 70.5% +0.0% -27.9% -0.90 50.0% +0.0% 52.5% 1.48 +6.2% +0.01 42.9% +0.0% 21.0% 0.53 +9.6% +0.31 16.7% +16.6% MARG w/ RAG 41.7% 1.33 +16.6% +0.34 50.0% 1.38 38.5% +16.7% +30.7% +0.85 33.3% 0.38 15.4% +33.4% +30.8% +0.77 28.6% 1.67 58.3% +38.1% +16.7% +0. 50.0% +33.3% Table 10: Human and automated evaluation results on Experimental Design in LIMITGEN-Syn set."
        },
        {
            "title": "Insufficient Metrics",
            "content": "Automated Eval. Human Eval. Automated Eval. Human Eval."
        },
        {
            "title": "Coarse",
            "content": "Fine (0-5)"
        },
        {
            "title": "Coarse",
            "content": "Fine (0-5)"
        },
        {
            "title": "Accuracy",
            "content": "GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 72.0% +28.0% 46.4% +15.2% Llama-3.3-70B 59.0% Llama-3.3-70B w/RAG +20.2% Qwen-2.5-72B 71.0% Qwen-2.5-72B w/RAG +21.0% MARG MARG w/ RAG 84.2% +6.7% 1.88 +0.64 1.10 +0.39 1.42 +0.56 1.72 +0. 2.58 +0.10 84.6% +15.4% 46.2% +7.6% 53.8% +7.7% 61.5% +15.4% 84.6% +15.4% 52.0% +8.0% 50.4% +0.0% 47.9% -6.0% 47.2% -4.0% 88.0% +4.0% 1.36 +0. 1.43 +0.03 1.33 -0.18 1.26 -0.09 2.32 +0.12 50.0% +8.3% 41.7% +0.0% 41.7% -8.4% 41.7% +0.0% 75.0% +8.3% Table 11: Human and automated evaluation results on Result Analysis in LIMITGEN-Syn set. Systems Limited Scope Irrelevant Citations Inaccurate Description Automated Eval. Human Eval. Automated Eval. Human Eval. Automated Eval. Human Eval. Coarse Fine (0-5) Accuracy Coarse Fine (0-5) Accuracy Coarse Fine (0-5) Accuracy GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 100.0% +0.0% 100.0% +0.0% Llama-3.3-70B 97.4% Llama-3.3-70B w/RAG -15.9% Qwen-2.5-72B Qwen-2.5-72B w/RAG 90.0% +6.3% MARG MARG w/ RAG 100.0% +0.0% 2.69 +0.06 2.72 +0.03 2.73 -0.69 2.35 +0.16 3.00 +0. 87.5% +12.5% 75.0% +12.5% 75.0% +0.0% 87.5% +12.5% 100.0% +0.0% 0.0% +0.0% 0.0% +1.2% 2.5% -2.5% 0.0% +0.0% 37.5% +9.6% 0.00 +0.00 0.00 +0. 0.02 -0.02 0.00 +0.00 0.94 +0.24 0.0% +11.1% 56.2% +12.6% 0.0% +0.0% 7.7% -7.7% 0.0% +0.0% 22.2% +11.1% 41.0% +6.0% 15.4% +0.6% 23.5% -4.5% 57.1% +7.2% 1.25 +0.31 1.00 +0.12 0.32 +0.08 0.48 -0.04 1.29 +0. 50.0% +25.0% 37.5% +12.5% 12.5% +12.5% 25.0% +0.0% 37.5% +25.0% Table 12: Human and automated evaluation results on Literature Review in LIMITGEN-Syn set. Query Generation Generate tldr in 5 words: {Abstract} Figure 34: Prompt for query generation. Rerank [System Input]: Given the abstracts of {number} papers and the abstract of reference paper, rank the papers in order of relevance to the reference paper. Output the top 5. Extract Relevant Content (Experimental Design) [System Input]: Concatenate all the content from the experimental design sections of paper. Remove sentences that are irrelevant to the experiment setup, and keep details about the datasets, baselines, and main experimental, ablation studies. [User Input]: {sections} Figure 37: Prompt for extracting content relevant to experimental design from paper. [User Input]: Paper 1: {Title 1} {Abstract 1} Paper 2: {Title 2} {Abstract 2} Paper 3: {Title 3} {Abstract 3} ... Extract Relevant Content (Result Analysis) [System Input]: Concatenate all the content from the result analysis sections of paper. Remove sentences that are irrelevant to the result analysis of the experiments, and keep details about the metrics, case study and how the paper presents the results. Reference Paper: {Reference Paper Title} {Reference Paper Title} [User Input]: {sections} Figure 35: Prompt for reranking the retrieved papers and selecting the top 5. Figure 38: Prompt for extracting content relevant to result analysis from paper. Extract Relevant Content (Methodology) [System Input]: Concatenate all the content from the methodology sections of paper. Remove sentences that are irrelevant to the proposed methodology or models, and keep details about key components and innovations. Extract Relevant Content (Literature Review) [System Input]: Concatenate all the content from the literature review sections of paper. Remove sentences that are irrelevant to the literature review, and keep details about the related works. [User Input]: {sections} [User Input]: {sections} Figure 36: Prompt for extracting content relevant to methodology from paper. Figure 39: Prompt for extracting content relevant to result analysis from paper. Systems Automated Eval. Human Eval. Coarse Fine (0-5) Accuracy GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 46.0% +10.0% 58.8% +2.8% 55.8% Llama-3.3-70B Llama-3.3-70B w/RAG +3.1% Qwen-2.5-72B Qwen-2.5-72B w/RAG MARG MARG w/ RAG 44.3% -2.1% 82.8% +4.4% 1.12 +0.28 1.39 +0.07 1.20 +0.08 1.01 -0.08 1.94 +0.05 35.9% +11.9% 43.9% +8.0% 35.6% +0.0% 19.9% +8.0% 72.1% +11.5% Table 13: Human and automated evaluation results on Methodology in LIMITGEN-Syn set. Systems Automated Eval. Human Eval. Coarse Fine (0-5) Accuracy GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 52.1% +4.2% 47.0% +2.4% Llama-3.3-70B 38.4% Llama-3.3-70B w/RAG -5.9% Qwen-2.5-72B 37.8% Qwen-2.5-72B w/RAG +0.6% MARG MARG w/ RAG 64.9% +5.6% 1.31 +0.13 1.24 +0.06 1.02 -0.21 0.94 +0.04 1.74 +0.20 25.0% +18.1% 18.8% +6.2% 10.1% +2.4% 12.5% +0.0% 29.9% +18.0% Table 16: Human and automated evaluation results on Literature Review in LIMITGEN-Syn set. Systems Automated Eval. Human Eval. Coarse Fine (0-5) Accuracy GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 47.9% +16.7% 42.3% +4.0% Llama-3.3-70B 35.3% Llama-3.3-70B w/RAG +5.2% Qwen-2.5-72B Qwen-2.5-72B w/RAG 47.1% -2.2% MARG MARG w/ RAG 38.5% +23.7% 1.31 +0.54 1.09 +0.17 1.01 +0.12 1.34 -0. 1.19 +0.72 55.6% +22.2% 44.4% +5.6% 37.3% +15.9% 42.1% +0.0% 37.3% +29.4% Table 14: Human and automated evaluation results on Experimental Design in LIMITGEN-Syn set. Systems Automated Eval. Human Eval. Coarse Fine (0-5) Accuracy GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/RAG 62.0% +18.0% 48.4% +7.6% Llama-3.3-70B 53.5% Llama-3.3-70B w/RAG +7.1% Qwen-2.5-72B Qwen-2.5-72B w/RAG MARG MARG w/ RAG 59.1% +8.5% 86.1% +5.4% 1.62 +0.54 1.27 +0. 1.38 +0.19 1.49 +0.29 2.45 +0.11 67.3% +11.9% 43.9% +3.9% 47.8% -0.3% 51.6% +7.7% 79.8% +11.9% Table 15: Human and automated evaluation results on Result Analysis in LIMITGEN-Syn set. Overlap Evaluation [System Input]: Compare the following pair of limitations: one generated and one from the ground truth. Assess the degree of relatedness and the level of specificity of the generated limitation compared to the ground truth limitation. Start by providing brief explanation for each category, and then assign rating. Present your assessment in JSON format as follows: \"<Choose one of the following { \"relatedness_reason\": \"< Provide brief explanation of why you assessed the relatedness as you did>\", \"relatedness\": options: none, weak, medium, high>\", \"specificity_reason\": \"<Provide brief explanation of why you assessed the specificity as you did>\", \"specificity\": \"<Choose one of the following options: less, same, more>\" } [User Input]: Ground truth limitation: {ground truth} Generated limitation: {generated limitation} Figure 40: Prompt for measuring overlap in LIMITGENHuman. Systems Coarse. Fine. Accuracy Median Variance Median Variance Median Variance GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/ RAG Llama-3.3-70B Llama-3.3-70B w/ RAG Qwen-2.5-72B Qwen-2.5-72B w/ RAG MARG MARG w/ RAG 52.0% 66.7% 50.4% 60.0% 47.9% 45.8% 45.5% 43.2% 58.3% 75.0% 7.4% 9.3% 8.5% 9.0% 7.8% 7.1% 6.5% 7.9% 7.2% 3.6% 1.25 1. 1.10 1.46 1.01 1.16 1.26 1.17 1.67 2.23 0.60 0.78 0.64 0. 0.64 0.47 0.50 0.56 0.58 0.43 50.0% 66.7% 41.7% 50.0% 28.6% 33.3% 33.3% 33.3% 50.0% 75.0% 6.9% 8.4% 5.3% 6.7% 6.1% 6.3% 5.9% 7.3% 6.5% 3.8% Table 17: The median and variance of the results across subtypes in LIMITGEN-Syn set. Systems GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/ RAG Llama-3.3-70B Llama-3.3-70B w/ RAG Qwen-2.5-72B Qwen-2.5-72B w/ RAG MARG MARG w/ RAG Recall 46.7% +2.1% 42.2% +5.0% 56.7% +2.8% 23.9% +5.2% 51.9% +1.9% Automated Evaluation Human Evaluation (1-5) Precision Jaccard Fine (0-5) Faith. Sound. Import. 20.9% +2.8% 19.4% +1.3% 19.9% -0.1% 21.6% +0.9% 14.9% -1.1% 17.1% +2.6% 16.5% +1.3% 17.3% -0.4% 14.2% +1.0% 12.4% -0.7% 0.43 +0.12 0.38 +0.01 0.42 +0.03 0.49 +0. 0.64 +0.24 3.14 +0.66 2.92 +0.30 3.04 +0.28 2.52 +0.30 3.21 +0. 2.82 +1.17 2.66 +1.20 2.78 +1.09 2.78 +0.95 2.67 +1.41 3.62 +0. 3.10 +0.56 3.24 +0.20 2.94 +0.16 3.56 +0.33 Table 18: Human and automated evaluation results on Methodology in LIMITGEN-Human set. Systems GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/ RAG Llama-3.3-70B Llama-3.3-70B w/ RAG Qwen-2.5-72B Qwen-2.5-72B w/ RAG MARG MARG w/ RAG Recall 61.4% +4.5% 56.3% -1.4% 66.7% +2.6% 36.1% +0.4% 54.9% +1.5% Automated Evaluation Human Evaluation (1-5) Precision Jaccard Fine (0-5) Faith. Sound. Import. 34.1% +2.6% 33.3% -1.5% 31.9% +0.0% 42.0% +1.0% 22.3% +3.8% 28.0% +3.2% 27.6% -1.6% 27.6% +0.1% 24.5% +1.5% 18.8% +2.6% 0.70 +0. 0.70 -0.03 0.63 +0.06 0.94 +0.12 0.92 +0.21 3.70 +0.68 3.76 +0. 3.28 +0.19 3.79 +0.10 3.56 +0.49 3.41 +1.14 3.40 +0.77 3.63 +0. 3.63 +0.19 3.54 +0.70 4.12 +0.47 3.66 -0.17 3.56 -0.21 3.67 +0. 3.89 +0.34 Table 19: Human and automated evaluation results on Experiment Design in LIMITGEN-Human set. Systems GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/ RAG Llama-3.3-70B Llama-3.3-70B w/ RAG Qwen-2.5-72B Qwen-2.5-72B w/ RAG MARG MARG w/ RAG Recall 45.1% -1.5% 41.1% +3.3% 49.1% +4.5% 27.7% +1.6% 59.4% +4.2% Automated Evaluation Human Evaluation (1-5) Precision Jaccard Fine (0-5) Faith. Sound. Import. 14.8% +2.8% 15.5% +2.0% 15.5% +0.6% 18.9% +2.2% 12.5% +3.3% 12.6% +1.8% 12.7% +1.5% 12.9% +0.6% 12.9% +1.2% 11.1% +2.2% 0.36 +0.09 0.32 +0.05 0.32 +0.03 0.48 +0. 0.45 +0.16 3.21 +0.05 3.07 +0.46 3.00 +0.38 2.86 +0.32 3.80 +0. 2.73 +1.15 2.93 +0.00 2.73 +0.12 2.81 +0.14 2.84 +1.08 3.41 +0. 2.87 +1.14 2.81 +0.51 2.91 +0.48 3.95 +0.19 Table 20: Human and automated evaluation results on Result Analysis in LIMITGEN-Human set. Systems Automated Evaluation Human Evaluation (1-5) Recall Precision Jaccard Fine (0-5) Faith. Sound. Import. GPT-4o GPT-4o w/ RAG GPT-4o-mini GPT-4o-mini w/ RAG Llama-3.3-70B Llama-3.3-70B w/ RAG Qwen-2.5-72B Qwen-2.5-72B w/ RAG MARG MARG w/ RAG 31.8% +10.9% 21.1% +7.5% 39.5% +2.8% 16.9% -0.6% 65.9% +4.3% 6.6% +5.2% 6.3% +1.0% 8.6% +0.1% 7.8% +1.3% 22.1% +5.3% 5.8% +4.1% 5.2% +1.1% 7.5% +0.1% 6.0% +0.2% 18.4% +5.9% 0.18 +0. 0.14 +0.05 0.20 +0.01 0.22 +0.06 0.62 +0.36 2.71 +0.56 2.37 +0. 2.60 +0.07 2.46 +0.18 3.84 +0.19 2.38 +1.10 2.14 +1.11 2.25 +1. 2.24 +0.11 3.71 +0.71 2.81 +1.15 2.24 +0.61 2.59 +0.34 2.22 +0. 3.72 +0.84 Table 21: Human and automated evaluation results on Literature Review in LIMITGEN-Human set. Systems GPT-4o w/ RAG GPT-4o-mini w/ RAG Llama-3.3-70B w/ RAG Qwen-2.5-72B w/ RAG MARG w/ RAG Automated Eval. Human Eval. (1-5) Jaccard Fine.(0-5) Faith. Sound. Import. 14.9% +2.2% 14.6% +1.4% 15.1% +0.1% 13.6% +1.1% 15.4% +1.9% 0.39 +0.11 0.35 +0. 0.37 +0.04 0.48 +0.12 0.63 +0.30 3.17 +0.37 3.00 +0.37 3.02 +0. 2.69 +0.31 3.68 +0.44 2.78 +1.15 2.80 +0.76 2.75 +0.83 2.79 +0. 3.19 +0.97 3.52 +0.47 2.99 +0.58 3.02 +0.32 2.93 +0.32 3.81 +0. Table 22: The median of the results across aspects in LIMITGEN-Human set. Systems GPT-4o w/ RAG GPT-4o-mini w/ RAG Llama-3.3-70B w/ RAG Qwen-2.5-72B w/ RAG MARG w/ RAG Automated Eval. Human Eval. (1-5) Jaccard Fine.(0-5) Faith. Sound. Import. 0.9% 0.8% 0.9% 0.7% 0.7% 0.7% 0.6% 0.7% 0.2% 0.4% 0.05 0.04 0.06 0.04 0.03 0. 0.09 0.10 0.04 0.05 0.16 0.28 0.33 0.44 0.08 0.13 0.38 0. 0.08 0.01 0.18 0.20 0.28 0.32 0.33 0.36 0.33 0.48 0.26 0. 0.29 0.12 0.34 0.24 0.19 0.05 0.35 0.25 0.03 0.08 Table 23: The variance of the results across aspects in LIMITGEN-Human set. B.4 Case study Figure 41: An example of GPT-4o w/ RAG generated limitation in LIMITGEN-Syn. Figure 42: An example of GPT-4o w/ RAG generated limitation in LIMITGEN-Syn. Figure 43: An example of GPT-4o w/ RAG generated limitation in LIMITGEN-Human. Figure 44: An example of GPT-4o w/ RAG generated limitation in LIMITGEN-Human."
        }
    ],
    "affiliations": []
}