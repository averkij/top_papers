{
    "paper_title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation",
    "authors": [
        "Wei Li",
        "Xin Zhang",
        "Zhongxin Guo",
        "Shaoguang Mao",
        "Wen Luo",
        "Guangyue Peng",
        "Yangyu Huang",
        "Houfeng Wang",
        "Scarlett Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development."
        },
        {
            "title": "Start",
            "content": "FEA-Bench: Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation Wei Li*, Xin Zhang, Zhongxin Guo, Shaoguang Mao Wen Luo, Guangyue Peng, Yangyu Huang, Houfeng Wang , Scarlett Li State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University Microsoft Research Asia weili22@stu.pku.edu.cn xinzhang3@microsoft.com, wanghf@pku.edu.cn 5 2 0 2 9 ] . [ 1 0 8 6 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Implementing new features in repository-level codebases is crucial application of code generation models. However, current benchmarks lack dedicated evaluation framework for this capability. To fill this gap, we introduce FEABench, benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing more comprehensive evaluation method of LLMs automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development. Our code will soon be publicly available at https://github.com/microsoft/FEA-Bench."
        },
        {
            "title": "Introduction",
            "content": "The remarkable text generation capabilities of large language models (LLMs) (Achiam et al., 2023) have extended their impact into the domain of code generation (Xu et al., 2022), which has led to the emergence of developer assistants such as Copilot, Cursor, Devin, etc. An important research topic to evaluate the effectiveness of LLMs in generating code across diverse scenarios. Many of the existing benchmarks focus on evaluating standalone programming problems, such as HumanEval, MBPP, *Work done during internship at MSRA. Corresponding author. Project leader. 1 Figure 1: The proposed FEA-Bench aims to evaluate incremental repository development, while SWE-bench (Jimenez et al., 2024) focuses on repairing issues. and LiveCodeBench (Chen et al., 2021; Austin et al., 2021; Jain et al., 2024). These benchmarks offer little insight into the challenges developers face in real-world projects, where codebases are composed of multiple interconnected files. In such projects, modifications in one part of the code often necessitate corresponding edits elsewhere. This type of collaborative, large-scale development is referred to as repository-level code development. In the realm of repository-level code generation, much of the current evaluation effort is centered around code completion (Li et al., 2024b; Yang et al., 2024a). Code completion refers to generating correct code snippets at specified locations within given code context. However, this task is inherently limitedit typically targets localized generation and does not account for broader implications beyond the scope of completion. Recent advancements in the capabilities of LLMs have expanded their potential role from merely suggesting code snippets to managing the full lifecycle of repository development. prominent benchmark in this domain is SWE-bench (Jimenez et al., 2024), which evaluates LLMs on resolving issues, primarily focusing on bug fixes within repositories. In practice, as shown in Figure 1, more critical aspect of software engineering is the launch of new features, which often entails introducing new functions or even entire files into the repository. The continuous implementation of new features drives software growth and is key focus of automated software engineering. We define such tasks as repository-level incremental code development. In this work, to bridge the gap in benchmarks for this domain, we construct dataset derived from pull requests in GitHub repositories that specifically focus on adding new components, with the overarching goal of implementing new features. Each task instance in our dataset is paired with its corresponding unit test files, culminating in the creation of the Feature Implementation Benchmark (FEA-Bench), which comprises 1,401 task instances sourced from 83 diverse GitHub repositories. Statistically, our dataset exhibits characteristics that significantly differ from the bug-fix-oriented SWE-bench. Task instances in FEA-Bench require the implementation of new functions and classes in Python and involve substantially longer code generation compared to SWE-Bench. Experimental results demonstrate that current LLMs perform poorly on the proposed benchmark. According to our execution-based metrics, the best-performing LLM, DeepSeek-R1, successfully resolves only about 10% of the task instances. The key contributions of this paper are as follows: We introduce the task of repository-level incremental code development, addressing critical challenge in real-world software engineering where the continuous implementation of new features is essential for sustaining software growth. We construct the first benchmark to evaluate repository-level incremental code development. By employing parsing and other filtering methods, we constructed dataset composed of feature implementation tasks, offering execution-based evaluation. Using an automated pipeline, we scale the test data to include 83 diverse code repositories, ensuring high diversity. We will publicly release our data collection and evaluation codebase, allowing FEA-Bench to be continuously updated and expanded."
        },
        {
            "title": "2.1 Code Large Language Models",
            "content": "Large language models (LLMs) have revolutionized software engineering by enabling code generation, debugging, and translation capabilities (Pan et al., 2024; Li et al., 2023a; Joshi et al., 2023; Shi et al., 2024). Large-scale pre-trained LLMs such as GPT-4 (Achiam et al., 2023), CodeLlama (Roziere et al., 2023), DeepSeek-Coder (Guo et al., 2024; Zhu et al., 2024), and Qwen2.5-Coder (Hui et al., 2024) have demonstrated proficiency in generating functional code across multiple programming languages. Recent advancements also include instruction-tuned models like Starcoder (Li et al., 2023b), WizardCoder (Luo et al., 2023), WaveCoder (Yu et al., 2024b), Magicoder (Wei et al., 2024) and EpiCoder (Wang et al., 2025). With these advancements, code LLMs are poised to further revolutionize how developers interact with code, promising increased efficiency in software creation. Additionally, the integration of agents has further enhanced the performance of LLMs in software engineering tasks (Yang et al., 2024b; Xia et al., 2024; Zhang et al., 2024). In this paper, we further evaluate the performance of current LLMs in the incremental code development scenarios at the repository level. This investigation aims to drive the research on code LLMs toward addressing more intricate software engineering challenges, thereby advancing the capabilities of these models in handling sophisticated development tasks."
        },
        {
            "title": "2.2 Code Generation Benchmarks",
            "content": "Recent code generation benchmarks, such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), have primarily focused on synthesizing standalone functions or scripts from natural language, while subsequent efforts like APPS (Hendrycks et al., 2021), EvalPlus (Liu et al., 2023a), CoderEval (Yu et al., 2024a), ClassEval (Du et al., 2023), BigCodeBench (Zhuo et al., 2024), and FullStackBench (Liu et al., 2024b) have expanded evaluation to more complex scenarios. However, these benchmarks largely overlook the repository-level challenges of real-world software development, gap addressed by recent works (Bairi et al., 2024; Zhang et al., 2023). Because of the wild applications of auto code completion tools like Github Copilot (Dakhel et al., 2023), most repository-level code generation benchmarks aim to evaluate the code completion capabilities of LLMs (Liu et al., 2023b). Repository-level code completion aims to generate code for incomplete code snippets within repository (Wang et al., 2024). Benchmarks such as DevEval (Li et al., 2024b), EvoCodeBench (Li et al., 2024a), Codev2 Bench (Pan et al., 2024), and ExecRepoBench (Yang et al., 2024a) evaluate this capability. However, code completion data constructed by removing single line or function body can suffer from future context leakage issues (Zheng et al., 2024). In practice, autonomous development of code repository should include all edits to the code, rather than simply completing the specified code under perfect context. SWE-bench (Jimenez et al., 2024) focuses on repairing repositories issues by revising existing programs. This highlights the need for models to effectively modify and integrate changes within the repository. Our work fills another gap of incremental repository-level development by introducing benchmark that evaluates LLMs on the implementation of new features, further bridging the divide between code completion and real-world software engineering."
        },
        {
            "title": "3.1 Overview",
            "content": "The task instances in FEA-Bench are constructed based on existing pull request (PR) data from GitHub. As illustrated in Figure 2, each task instance contains the following elements: ❶ Feature Request Content of the pull request and corresponding issues (if any) provide essential information regarding the new feature or functionality to be developed. ❷ Definition of New Components This includes the signatures and documentation of newly added functions and classes. The name of the new component must be consistent with that in PR, which is the prerequisite for completing the task, because unit tests are written based on the specified name. ❸ Environment Setup This includes the relevant information for the repository and specifies the base commit of the code repository for each task instance. Besides, the configurations of the building execution environment are also included. ❹ Patch It describes the changes made to the code in the repository and can be processed by the unidiff standard library1. Additionally, the changes can be applied to the repository using the git apply tool. patch can be divided into test patch and gold patch; the former pertains to changes in test codes, while the latter involves the other changes affecting the software itself. Figure 2: An example of the task instances from the FEA-Bench. During the inference of LLMs, the first two items: feature request and new components are considered as known information. The environment setup serves as prerequisite for creating the testbed and environment. Python file patches and unit tests are used as labels and evaluation metrics and should not be leaked during the inference of LLMs. ❺ Unit Test The correctness of the code changes is verified based on the result of running these tests. We get the ground truth status by actually running pytest before and after applying the gold patch. Our data collection pipeline is developed based on SWE-bench. As shown in Figure 3, the proposed collection pipeline ensures data diversity and comprehensiveness, enabling robust evaluation of LLMs capabilities in implementing new features at the repository level. By making our data collection and evaluation codes publicly available, we aim to facilitate continuous updates and the creation of new versions of FEA-Bench. In the rest of this section, we will discuss the construction and the characteristics of FEA-Bench."
        },
        {
            "title": "3.2 Repository Collection",
            "content": "To determine the scope of GitHub repositories for data collection, we initially focus on the GitHub repositories corresponding to Python packages listed on the Top PyPI website 2. Packages appearing on this list are generally influential Python 1https://github.com/matiasb/python-unidiff 2https://hugovk.github.io/top-pypi-packages/ 3 Figure 3: The data collection pipeline for the FEA-Bench. First, determine the scope of GitHub repositories from which task instances will be collected. Next, gather pull requests as task instances and apply filtering criteria to select instances that meet the purpose of adding new features. Finally, use the included test files to execute unit tests, ensuring that only task instances with reproducible test results are included in FEA-Bench. For more detailed construction process, please refer to Appendix A.2. software with high data quality, and most repositories use unified pytest format for testing, which facilitates later execution. This leaderboard contains 8,000 Python packages. We filter out repositories that have license and more than 1,000 pull requests, resulting in approximately 600 repositories meeting these criteria. Fast Validation Except for repositories already included in SWE-bench, the remaining repositories do not have customized installation procedures or testing methods. While environment configurations vary among different Python packages, pip offers unified installation approach3. And we adopt pytest as the default method for unit testing. Based on these settings, for each repository, we extract the first 20 pull requests that include changes to test files and observe the unit test status. Repositories that have at least one task instance where the unit tests passed with the default configuration are retained, as they possess the potential to generate task instances that meet our criteria. After this fast validation process, in addition to the 18 Python packages included in the SWE-bench dataset, another 101 Python packages are identified as sources for extracting repository-level data for our benchmark."
        },
        {
            "title": "3.3 Task Collection and Filtering",
            "content": "Based on existing repository data, we crawl all pull requests and filter out those that include changes to test files as valid task instances. Given our focus on the incremental feature development task, we introduce the following steps to filter out final task instances for FEA-Bench: Extraction of new components For each task instance candidate, we perform parsing on all Python scripts involved in the gold patch. We compare the state before and after applying the patch to identify newly added namespaces, including classes 3pip install -e . and functions, and extract their signatures and docstrings as metadata. Filtering based on new components We retain only those task instances that contain at least one new component. To ensure that implementing new features is the primary purpose of the pull request, we further restrict the new components to occupy more than 25% of all edited lines in gold patch. This threshold is set relatively low because code changes often need to include modifications to other related code in addition to the new components themselves. Intent-based filtering The pull requests filtered using the above rule-based approach still include some that are not primarily aimed at feature implementation. Therefore, we use GPT-4o to classify the intent based on the pull request description. Only pull requests classified as \"new feature\" are retained. Verification by running unit tests For each instance, we first set up the environment and testbed. We then apply the test patch and run the unit test files involved in the test patch. Theoretically, some unit tests should fail at this stage. After applying the gold patch, we rerun the same unit tests. If the configuration is correct, all unit tests should pass. Unlike SWE-bench, we do not impose restrictions on whether ImportError or AttributeError occurs before applying the gold patch, as these errors are almost inevitable before new components are implemented. Instead, to ensure data quality, we exclude samples where tests remain in the failed status after applying the gold patch. The status of each test function before and after applying the gold patch is recorded. After task collection and filtering, 83 out of 119 repositories collected by the method in Section 3.2 produce 1,401 task instances for new feature implementation. 4 Repositories Tasks # Lines of oracle files # Files edited # Lines edited # Lines of added components % Added components # Functions added # Classes added FEA-Bench Lite Full SWE-bench Full Verified 83 1401 2115.5 2.62 128. 87.1 67.8 4.49 0.78 48 200 1366.8 1.54 68.1 47.2 69.3 2.02 0 12 2294 1961.3 1.66 37. 10.9 28.9 0.73 0.064 12 500 1488.2 1.24 14.32 2.1 14.7 0.25 0.006 Table 1: Statistics for FEA-Bench and its lite subset, as well as for SWE-bench and its verified subset. The metrics include: 1) the number of task instances and involved repositories; 2) the average total number of lines in all Python files involved in code changes (i.e., the Oracle setting described in Section 4.2), and the average number of edited files and lines; 3) the average number of lines of new components, the average percentage of new component lines relative to all edited lines, and the average number of added functions and classes."
        },
        {
            "title": "3.4 Benchmark Characteristics",
            "content": "Semi-guided software engineering task Although the signatures for new components are provided, the primary objective of the FEA-Bench is to evaluate the whole solution of new feature implementation. This is comprehensive real-world software engineering task, distinct from code completion. As shown in Table 1, each instance in FEA-Bench involves an average of 128.5 modified lines, with 87.1 lines attributed to the new components themselves. This indicates that approximately 41.4 lines of changes are made elsewhere in the repository. Implementing new features not only requires the ability to generate code for specified new components but also necessitates making complementary changes within the existing repository. Lite version We have also curated subset to serve as lite version of our dataset. This subset was filtered based on criteria including higher quality and lower difficulty. This lite version is particularly useful for evaluating systems that are computationally intensive and time-consuming. Detailed information can be found in Section A.3. New components driven generation task From Table 1, we can observe that, on average, the number of lines for new components in each FEABench task instance is more than 8 that of SWEbench. Furthermore, the new components account for approximately 67.8% of all edited lines in FEABench, compared to just 28.9% in SWE-bench. The difference in this metric between FEA-Bench lite and SWE-bench verified is even more pronounced. These statistics indicate that the task instances in FEA-Bench are primarily aimed at implementing new features. In SWE-bench, the average number of new functions is 0.73, indicating that its task instances mainly involve editing existing code rather than incremental development. Complex solutions While SWE-bench focuses on fixing issues, which generally involve simpler problems, the task instances in FEA-Bench exhibit greater complexity. From Table 1, whether measured by the number of edited lines or edited files, the solutions of task instances in FEA-Bench are notably more complex than those in SWE-bench."
        },
        {
            "title": "4.1 Models",
            "content": "Due to its repository-level code generation characteristics, FEA-Bench requires models to have long context window. We evaluate representative code LLMs and general-purpose LLMs with strong foundational capabilities on the FEA-Bench. The code LLMs used in our evaluation include CodeLlama, Codestral4, Qwen2.5-Coder, and DeepSeekCoder-V2 (Roziere et al., 2023; Hui et al., 2024; Zhu et al., 2024). For general-purpose LLMs, we evaluate the performance of GPT-4, GPT-4o (Achiam et al., 2023; Hurst et al., 2024) and DeepSeek-V3(Liu et al., 2024a), as well as models with long chain-of-thought (CoT) capabilities such as o1 and DeepSeek-R1 (Jaech et al., 2024; Guo et al., 2025)."
        },
        {
            "title": "4.2 Context",
            "content": "To explore the capabilities and potential limits of LLMs in implementing new features within code repositories, we construct different prompts from several perspectives based on our collected data. Each task instance is evaluated using various context settings to provide comprehensive understanding of model performance. New component hints In FEA-Bench, information about new components can be derived from two sources: 1) signatures and documentation of newly extracted functions and classes, and 2) changes in non-Python files within the patch, which often contain relevant information about the new components. Based on this, we have two settings: Brief. Only provides the signatures of new compo4https://mistral.ai/en/news/codestral 5 Model Size Window Oracle BM25 (27K) Oracle BM25 (27K) Detailed Brief Detailed Brief Detailed Brief Detailed Brief FEA-Bench FEA-Bench lite CodeLlama Qwen2.5-Coder Codestral-22B 13B 34B 14B 32B 22B 16K 16K 32K 32K 32K DeepSeek-Coder-V2 DeepSeek-R1-Distill DeepSeek-V3 DeepSeek-R1 128K 16B 32B 128K 671B 64K 671B 64K GPT-4 GPT-4o o1-mini o1 128K 128K 128K 200K 0.14 0.57 3.57 4.43 0.86 0.21 3.78 8.14 9. 4.71 6.14 1.93 7.28 0.43 0.57 3.57 3.64 0.93 0.29 4.07 6.92 8.35 4.21 5.57 1.86 6. 3.71 3.85 1.43 0.57 4.78 8.21 10.49 3.14 5.28 2.28 6.78 2.93 2.78 1.36 0.36 4.21 7.64 9.85 2.86 4.50 2.57 6.64 0.0 0.0 3.5 6. 0.5 0.0 5.5 14.5 14.5 6.0 5.0 2.0 10.0 0.0 0.0 4.5 5.5 1. 0.5 7.5 10.5 14.5 6.5 5.0 3.5 12.5 3.5 2.0 0.0 0.0 7.0 13.0 12. 2.0 4.0 1.0 5.0 2.5 2.5 0.0 0.5 5.5 12.0 13.5 1.0 3.5 2.0 7. Table 2: The resolved ratios on FEA-Bench (lite) task instances. The evaluation is conducted on single-round generation outputs by each model and task instance is considered resolved only if all unit tests are passed. The prompt using BM25 retrieved files is limited to length of 27K tokens. This ensures that, with maximum generation of 4K tokens, the total length will not exceed 32K tokens, which is the context window limits of most tested models. \"Detailed\" and \"Brief\" refer to the levels of hints regarding new components in prompt, as mentioned in Section 4.2. nents. Detailed. Includes all the aforementioned information. Retrieval method Given the extensive amount of code across multiple files in repository, selective inclusion of file contents as context is necessary. Firstly, the README file and files containing new components are always included in the context. For other files, similar to SWE-bench, we divide retrieval methods into: Oracle. Includes all files involved in the patch in the context. BM25. Retrieves relevant files across the entire repository by BM25 algorithm (Robertson et al., 2009) based on the content of the pull request, and ranks them by relevance and filling the context until reaching specified length. Output format Generating an entire file can lead to interruptions due to generation limits and is costly. Therefore, our experiments offer two edit-based generation settings: Natural. Generates code edits in natural format as pairs of beforeand-after snippets, which can be converted into patches applicable to the code repository through post-processing. Patch. Directly generates edits in patch format. Since patches use line numbers for fragment location, this setting includes line numbers in the contexts code content."
        },
        {
            "title": "The details of prompt and experimental settings",
            "content": "are shown in Appendix B."
        },
        {
            "title": "5 Evaluation Results",
            "content": "The performance of LLMs The evaluation results of FEA-Bench are presented in Table 2. It is observed that in Oracle and Detailed prompt settings, the best resolved ratio of task instances is 9. 92%, indicating the poor performance of LLMs in the incremental development task at the repository level. Generally, the models with larger parameter sizes demonstrate better results. Among code LLMs, Qwen2.5-Coder exhibited performance comparable to that of GPT-4, highlighting its superiority in the domain of code generation. Despite this, general-purpose LLMs with stronger foundational capabilities can approach the performance of specialized code LLMs. For example, R1-Distill, which shares the same underlying architecture as Qwen2.5-Coder, showed competitive performance. Among the models evaluated, the latest DeepSeekV3 and R1 models achieve the best performance, significantly outperforming OpenAIs GPT-4 and o1 series. This underscores the importance of foundational capabilities in LLMs for repository-level development tasks. Additionally, the performance on the lite version is slightly higher, but the relative trends remain largely consistent. When computational resources are limited, the metrics from FEABench lite can be used to reflect the performance on the full benchmark."
        },
        {
            "title": "The performance under different contexts As",
            "content": "6 Precision Avg. Recall Avg. Recall All BM25 (27K) BM25 (40K) Retrieval Metrics (%) 31.85 40.26 77.14 76.04 53.03 51.61 Resolved Ratio (%) Detailed Brief Detailed Brief GPT-4 GPT-4o 3.14 5.28 2.86 4.50 3.14 4.78 2.78 4.64 Table 3: The retrieval metrics and the instance resolved ratios under 27K and 40K token length limits of the prompt in BM25 retrieval mode. Model Qwen2.5-Coder(32B) R1-Distill(32B) GPT-4 GPT-4o o1 DeepSeek-V3 DeepSeek-R1 %Apply %Resolved Natural Patch Natural Patch 44.82 55.75 59.10 66.38 57.03 69.09 73. 12.92 19.06 33.26 19.49 - - - 4.43 3.78 4.71 6.14 7.28 8.14 9.92 1.71 1.71 3.07 1.86 - - - Table 4: The impact of output formats Natural and Patch. We show the success rates of applying code edits to the code repository and the final resolved ratios. The experiments on directly generating patches are conducted only on the models shown in the first four columns. shown in Table 2, we evaluate the performance of LLMs under different settings of new component hints and retrieval methods. Overall, detailed new component hints lead to better model performance. However, in the results from FEA-Bench lite, brief hints that only provide signatures performed better. This discrepancy could be attributed to the lack of structured presentation of new components and their documentation within the prompt, as illustrated in Figure 6. Regarding retrieval methods, the Oracle setting generally outperforms the BM25 setting, although the difference is not substantial. This may be because code files containing new components, which are regarded as known conditions in the FEA-Bench, ensure certain baseline performance. Considering the simplest instances in the dataset: those involving modifications to only one code file. Since instances in FEA-Bench always include new components, the unique code file is known information. In this case, BM25 retrieved files supplement additional information, which can lead to better model performance compared to the Oracle setting."
        },
        {
            "title": "6.1 Retrieved Files in Context",
            "content": "We aim to investigate whether providing more code context helps LLMs improve new feature development. Therefore, we conducted experiments by increasing the prompt limit from 27K to 40K tokens, which surpasses the original context window limit of Qwen2.5-Coder but can contain more retrieved files. The results are presented in Table 3. The evaluated models include GPT-4 and GPT-4o, and we also report the average precision and recall of Python files in the context relative to those in the gold patch, as well as the proportion of instances where all involved files are recalled. Although the recalls slightly improve at the limit of 40K tokens, model performance decreases. This indicates that current LLMs still struggle to extract useful information from long contexts in repositorylevel code development tasks. To improve performance on FEA-Bench, enhancing the precision of retrieval may be an effective approach than simply increasing the context length."
        },
        {
            "title": "6.2 Output Format of Edits",
            "content": "Direct repository-level code development outputs edits rather than new code itself. How LLMs can better generate edits remains an open question. Therefore, we analyze the impact of output formats using the two configurations described in Section 4.2. The two rightmost columns of Table 4 illustrate comparison between the Natural and the Patch generation method, with the former demonstrating significantly higher performance. The possible reason is that generating patches imposes stricter formatting requirements, which current LLMs find challenging to adhere to accurately. Therefore, the main results presented in Table 2 adopt the performance by Natural generation method. Whether using patches converted by converting before-and-after code snippets in Natural mode or directly generating patches in Patch mode, during evaluation, these patches must be applied to the repository. The success rates of the git apply are shown in the middle two columns of Table 4. It can be observed that the success rate in the Natural prompt mode is significantly higher, contributing to the superior performance of LLMs in Natural mode. Further observation reveals significant positive correlation between the success rate of applying patches and the resolved ratio, regardless of whether Natural or Patch mode is used. DeepSeek7 Figure 4: The resolved ratios grouped by the categories of the repositories. R1, which performs the best on FEA-Bench, has the highest values for both metrics. This indicates that the format of code edits is critical factor limiting the performance of LLMs on such tasks."
        },
        {
            "title": "6.3 Performances across Repositories",
            "content": "To further analyze LLMs performance at finer granularity, we examine resolved ratios across different categories of repositories. We classify repositories into several categories, as shown in Appendix A.1. The performance of DeepSeek-R1, o1, and GPT-4o across different categories is illustrated in Figure 4. The resolved ratios of different models vary across categories; task instances in the Testing category (from the repo: joke2k/faker) have the highest resolved ratio, followed by the Internet category. For the remaining categories, the pass ratio of the three models are at similar level. Among the three LLMs, GPT-4o shows slightly weaker performance compared to the other models, but its trend is largely consistent with o1. Notably, DeepSeek-R1 exhibits weaker performance in the Testing category but significantly outperforms both GPT-4o and o1 in all other categories. This suggests that integrating different models might further enhance overall performance on FEA-Bench."
        },
        {
            "title": "6.4 Performances under Different Complexity",
            "content": "of New Components To further investigate whether the implementation of new components adds pressure on LLMs in completing feature implementation tasks, we examine the relationship between the number of new functions and the number of resolved instances. The number of added functions is more indicative of the complexity of new components, as functions are more atomic compared to classes. Figure 5 illustrates the distribution of task instances and those solved by R1 with respect to the number of added functions. Excluding instances where the number Figure 5: Histogram of the number of added functions both in all task instances and resolved task instances by DeepSeek-R1 (under Natural and Detailed prompt settings). of added functions is zero (i.e., adding only classes for storing variables), it is evident that the resolved ratio decreases as the number of added functions increases. The resolved ratio is 18.96% when the number of added functions is 1, 8.24% when it is 2, and 5.47% when it is greater than or equal to 3. This indicates that implementing new features is more challenging task than fixing bugs, which constitute the majority of instances in SWE-bench. Moreover, the more complex the new components, the higher the difficulty of successful implementation."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce FEA-Bench, novel benchmark for evaluating the repository-level incremental code development capabilities of large language models (LLMs). Our benchmark focuses on the critical task of implementing new features by adding new components to existing code repositories. Through our comprehensive dataset and rigorous evaluation, we demonstrate that current LLMs face significant challenges in this domain. We also analyzed that the retrieval method of files, 8 the output format, the repository itself, and the complexity of new components all impact the implementation of new features. Our work highlights the need for further advancements in LLMs reasoning and generation capabilities to better address real-world software engineering tasks. We hope that FEA-Bench will serve as valuable resource for the research community, driving progress in this important area. method will be made publicly available for academic research. However, we should note that the inference results of the task instances from the benchmark may contain code that is harmful to computer systems. Evaluation by docker is recommended, just like SWE-bench. Additionally, the ChatGPT platform was used as an AI assistant for refining the paper writing."
        },
        {
            "title": "References",
            "content": "Our constructed data and experiments have certain limitations. Firstly, the quantity of high-quality data suitable for repository-level incremental development is limited. High-quality and usable pull requests for new feature development are relatively scarce. Many repository-level code developments for implementing new functionalities were committed during the early stages of repositories, without going through the rigorous code review process typical of the open-source community, resulting in lower data quality that cannot be utilized. Furthermore, the softwares early-stage developments might not even have been conducted using the GitHub platform, posing challenge for data collection and utilization. Consequently, FEA-Bench, which is built on publicly available data and subjected to stringent filtering, may exhibit certain scenario limitations. Secondly, due to the long context involved in repository-level code development, the cost of conducting experiments using LLMs is relatively high. Therefore, the experimental results are based on single round generation, akin to Pass@1, which may introduce certain level of bias into the results. Additionally, given the scarcity of API resources for models like DeepSeek-V3 and R1, some results in the main experiments presented in Table 2 are missing. We hope that more affordable models similar to DeepSeek can be further developed to facilitate research and applications in the field of repository-level code development."
        },
        {
            "title": "Ethics Statement",
            "content": "We collected the data from publicly available Github repositories only for research purposes. All the repositories have licenses that allow free software use. LLMs are used only for classification during the construction of the FEA-Bench dataset, so no harmful information can be created in the dataset. The dataset and code for our proposed Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv preprint, abs/2303.08774. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. ArXiv preprint, abs/2108.07732. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. 2024. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374. Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot ai pair programmer: Asset or liability? Journal of Systems and Software, 203:111734. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. 2023. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation. ArXiv preprint, abs/2308.01861. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming ArXiv preprint, the rise of code intelligence. abs/2401.14196. 9 Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. ArXiv preprint, abs/2105.09938. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023b. Starcoder: may the source be with you! ArXiv preprint, abs/2305.06161. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. ArXiv preprint, abs/2409.12186. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. ArXiv preprint, abs/2410.21276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. ArXiv preprint, abs/2412.16720. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. ArXiv preprint, abs/2403.07974. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Harshit Joshi, José Pablo Cambronero Sánchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radicek. 2023. Repair is nearly generation: Multilingual program repair with llms. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, ThirtyFifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 51315140. AAAI Press. Jia Li, Ge Li, Zhuo Li, Zhi Jin, Xing Hu, Kechi Zhang, and Zhiyi Fu. 2023a. Codeeditor: Learning to edit source code with pre-trained models. ACM Transactions on Software Engineering and Methodology, 32(6):122. Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024a. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. ArXiv preprint, abs/2404.00599. Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, et al. 2024b. Deveval: manually-annotated code generation benchmark aligned with real-world code repositories. ArXiv preprint, abs/2405.19856. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. ArXiv preprint, Deepseek-v3 technical report. abs/2412.19437. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023a. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, ZY Peng, et al. 2024b. Fullstack bench: Evaluating llms as full stack coder. ArXiv preprint, abs/2412.00535. Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench: Benchmarking repository-level ArXiv preprint, code auto-completion systems. abs/2306.03091. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. ArXiv preprint, abs/2306.08568. Zhenyu Pan, Rongyu Cao, Yongchang Cao, Yingwei Ma, Binhua Li, Fei Huang, Han Liu, and Yongbin Li. 2024. Codev-bench: How do llms understand developer-centric code completion? ArXiv preprint, abs/2410.01353. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950. Yuling Shi, Songsong Wang, Chengcheng Wan, and Xiaodong Gu. 2024. From code to correctness: Closing the last mile of code generation with hierarchical debugging. ArXiv preprint, abs/2410.01215. Yanlin Wang, Yanli Wang, Daya Guo, Jiachi Chen, Ruikai Zhang, Yuchi Ma, and Zibin Zheng. 2024. Rlcoder: Reinforcement learning for repository-level code completion. ArXiv preprint, abs/2407.19487. 10 Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, et al. 2025. Epicoder: Encompassing diversity and complexity in code generation. arXiv preprint arXiv:2501.04694. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. ArXiv preprint, abs/2407.01489. Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, MAPS 2022, page 110, New York, NY, USA. Association for Computing Machinery. Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke Jin, Lei Zhang, Qiyao Peng, Ken Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, et al. 2024a. Execrepobench: Multi-level executable code completion evaluation. ArXiv preprint, abs/2412.11990. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024b. Swe-agent: Agent-computer interfaces enable automated software engineering. ArXiv preprint, abs/2405.15793. Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. 2024a. Codereval: benchmark of pragmatic code generation with generative In Proceedings of the 46th pre-trained models. IEEE/ACM International Conference on Software Engineering, pages 112. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2024b. WaveCoder: Widespread and versatile enhancement for code large language models by instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51405153, Bangkok, Thailand. Association for Computational Linguistics. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. RepoCoder: Repository-level code completion through iterative retrieval and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 24712484, Singapore. Association for Computational Linguistics. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. 2024. Autocoderover: AuIn Proceedings tonomous program improvement. of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, page 15921604, New York, NY, USA. Association for Computing Machinery. Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu Zhang, and Zibin Zheng. 2024. Towards more realistic evaluation of llm-based code generation: an experimental study and beyond. ArXiv preprint, abs/2406.06918. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. ArXiv preprint, abs/2406.11931. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. ArXiv preprint, abs/2406.15877."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Repositories The task instances in FEA-Bench are derived from 83 Python packages corresponding to GitHub repositories. For each package, we obtained its license information and topic classification from the PyPI website. For packages missing the topic attribute, we utilized their README files and employed GPT-4o to classify them into topics available on the PyPI website5. Finally, considering the distribution of the topics of task instances in FEA-Bench, we further simplify the topics to several categories to facilitate data visualization and analysis, as presented in Section 6.3."
        },
        {
            "title": "The relevant information regarding the involved",
            "content": "code repositories is summarized in Table 5. A.2 Construction Details Table 6 shows the number of remaining task instances for each stage during the data collection process. In Section 3.2, we mentioned that after the fast validation step, including the 18 repositories from SWE-bench, total of 119 repositories (18 + 101) are available for further data collection. On this basis, we crawl the pull requests from GitHub to obtain code changes. Files with names containing words like test are identified as unit test files executable by pytest. Initially, we filter pull requests (PRs) based on whether they were merged, resulting in the number 5https://pypi.org/classifiers/ Figure 6: The prompt for the inference of the task instances. of PRs shown in the \"# PR\" column of Table 6. Next, we excluded PRs without any test files, leaving only those PRs with the necessary conditions to be considered valid task instances, as indicated by the \"# All tasks\" column. Further filtering was conducted according to the third and fourth stages illustrated in Figure 3, retaining only those PRs that introduce at least one new component and are classified as \"new feature\" types by GPT-4o based on the PR content. Additionally, instances with patch lengths exceeding 8K (8192) tokens are excluded to remove long-tail distributions and noise. The remaining candidate task instances are listed under the \"# Candidates\" column. This filtering step does not exist in SWE-bench, leading to smaller number of candidate task instances in our benchmark dataset. Finally, for each instance, we apply the test patch to the repository in the base commit state to verify that unit tests could accurately evaluate the corresponding code edits. Similar to SWEbench, our pipeline annotates each instance with the corresponding repository version and provides environment and installation configurations based on versions. For repositories included in SWEbench, we directly utilize their environment setup and test configurations grouped by versions. For other repositories, minimal installation instructions 12 (pip install -e .) and basic pytest testing configurations are used. Initially, we run the unit tests directly and observe the pass status. Subsequently, we apply the gold patch from the PR and execute the unit tests again. We allowed errors such as AttributeError and ImportError in the first test, which are common when new components are not yet implemented, but these errors are not permitted in SWE-bench data collection pipeline. For the second test after applying the gold patch, any FAILED status is unacceptable to ensure data quality. After this execution-based filtering process, the final task instances constitute the FEA-Bench task instances, as shown in the \"# in Full\" column of Table 6. We only present information for repositories that have at least one instance in FEA-Bench. Specifically, out of the 119 repositories identified during the repository collection phase, only 83 repositories contain at least one task instance. A.3 FEA-Bench lite The feature implementation task proposed in this study, as one primary type of repository-level incremental code development tasks, require LLMs with long context capabilities to perform reasoning over extensive file contents. Such inference is computationally expensive. Considering the possible evaluation of multi-round code generation systems, it is necessary to select high-quality subset for more efficient evaluation. Therefore, we establish stricter criteria to curate higher-quality, lowerdifficulty FEA-Bench lite subset. Instances meeting any of the following lowquality criteria are excluded: The feature request descriptions contain fewer than 40 words. The instance involves cascading issues or commit SHA-256 references. The descriptions contain images. Additionally, to limit task difficulty, instances meeting any of the following criteria are also excluded: Involve deleting code files. Involve more than three code files. Natural-formatted code change content exceeding 4K(4096) tokens. Contain new class(es). Contain more than ten added functions."
        },
        {
            "title": "B Inference",
            "content": "B.1 Prompt In Section 4.2, we present different prompt settings for the context of inference. more detailed prompt structure is illustrated in Figure 6. The prompt is constructed using the first two items shown in Figure 2 as known input information. The feature request includes both pull request content and issue content. Information about new components and related code files is listed straightforwardly within the prompt. In Figure 6, the italicized text indicates placeholders that need to be filled with specific data of each task instance, while the other text represents standard prompt content. Red text highlights additional information for detailed hints of new components, and blue text indicates parts that need to be modified when directly generating patches as results. In this study, we provide two formats for generating code edits: Natural and Patch. The corresponding output instructions are illustrated in Figure 7. To ensure that the models produce outputs in the correct format, both modes are accompanied by detailed instructions and examples."
        },
        {
            "title": "We include several different prompts mentioned",
            "content": "above for each task instance in the dataset files. B.2 Generation Configurations For models with fewer than 32 billion parameters, we utilized the vLLM framework 6 on an 8-GPU NVIDIA A100 workstation, employing tensor parallelism for inference. The maximum number of generated tokens is limited to 4096, matching the generation length limits of used OpenAI GPT-4 and GPT-4o in our experiments. For larger open-source and all closed-source models, specifically DeepSeek-V3, R1, and OpenAI series models, we invoke their APIs for inference. The versions of the OpenAI models used are as follows: The gold patch contains More than 10 code change hunks. GPT-4: gpt-4-turbo-2024-04-09 6https://github.com/vllm-project/vllm 13 GPT-4o: gpt-4o-2024-05-13 o1: o1-2024-12-17 o1-mini: o1-mini-2024-09When possible, the temperature and top-p settings are fixed at 0.2 and 0.95, respectively. For DeepSeek-V3 and R1, the max output tokens are 8K (8192). For o1 and o1-mini, the max output tokens is 100,000 and 65,536 (64K), respectively. During the inference process, LLMs perform single generation for each task instance in FEABench, and the output is converted into patch for evaluation. The evaluation tools are adapted from the SWE-bench evaluation scripts which are based on docker, ensuring safety and easy use for the evaluation process. 14 Figure 7: The output instructions at the rear of the inference prompt. 15 Source SWE-Bench Physics SWE-Bench Internet SWE-Bench Other SWE-Bench Other SWE-Bench Internet SWE-Bench Physics SWE-Bench Other SWE-Bench Medical SWE-Bench Libraries SWE-Bench Other SWE-Bench Other SWE-Bench AI SWE-Bench Other Other SWE-Bench Mathematics SWE-Bench Other Other AI AI Medical Other Libraries Libraries Other Libraries Build Tools Internet Other Libraries AI Other Libraries Libraries License Category Repo Name Internet::WWW/HTTP Software Development::Quality Assurance Scientific/Engineering::Mathematics Software Development::Code Generators Communications::Chat Scientific/Engineering::Artificial Intelligence Scientific/Engineering::Artificial Intelligence Scientific/Engineering::Medical Science Apps. Text Processing::Linguistic Topic BSD-3-Clause Scientific/Engineering::Astronomy astropy/astropy Internet::WWW/HTTP BSD-3-Clause django/django Other Scientific/Engineering::Visualization matplotlib/matplotlib BSD-3-Clause Scientific/Engineering::Visualization mwaskom/seaborn BSD-3-Clause pallets/flask BSD-3-Clause Scientific/Engineering::Physics pvlib/pvlib-python Scientific/Engineering::Information Analysis Apache-2.0 pydata/xarray Scientific/Engineering::Medical Science Apps. Others pydicom/pydicom Software Development::Libraries LGPL-2.1 pylint-dev/astroid Software Development::Quality Assurance GPL-2.0 pylint-dev/pylint MIT Scientific/Engineering::Information Analysis pyvista/pyvista BSD-3-Clause Scientific/Engineering::Artificial Intelligence scikit-learn/scikit-learn BSD-2-Clause Text Processing::Markup sphinx-doc/sphinx MIT sqlfluff/sqlfluff Others sympy/sympy Apache-2.0 Aider-AI/aider GPL-3.0 Cog-Creators/Red-DiscordBot MIT DLR-RM/stable-baselines3 MIT EleutherAI/lm-evaluation-harness Apache-2.0 Project-MONAI/MONAI Apache-2.0 PyThaiNLP/pythainlp BSD-3-Clause Software Development::Libraries RDFLib/rdflib Software Development::Libraries MIT Textualize/rich Software Development::User Interfaces MIT Textualize/textual Software Development::Libraries MIT TileDB-Inc/TileDB-Py Software Development::Build Tools Apache-2.0 astronomer/astronomer-cosmos Internet::WWW/HTTP Apache-2.0 atlassian-api/atlassian-python-api Software Development::Quality Assurance aws-cloudformation/cfn-lint MIT-0 Software Development::Libraries aws-powertools/powertools-lambda-python MIT-0 Scientific/Engineering::Artificial Intelligence aws/sagemaker-python-sdk Scientific/Engineering::Bio-Informatics biopragmatics/bioregistry Software Development::Libraries boto/boto3 boto/botocore Software Development::Libraries cocotb/cocotb conan-io/conan deepset-ai/haystack docker/docker-py dpkp/kafka-python embeddings-benchmark/mteb facebookresearch/hydra fairlearn/fairlearn falconry/falcon google-deepmind/optax googleapis/python-aiplatform googleapis/python-bigquery gradio-app/gradio graphql-python/graphene huggingface/accelerate huggingface/datasets huggingface/huggingface_hub huggingface/pytorch-image-models huggingface/trl joblib/joblib joke2k/faker lark-parser/lark minio/minio-py open-mmlab/mmengine openvinotoolkit/datumaro pgmpy/pgmpy pre-commit/pre-commit prometheus/client_python prompt-toolkit/python-prompt-toolkit pygments/pygments pyocd/pyOCD pypa/hatch pyro-ppl/pyro python-hyper/h2 roboflow/supervision rytilahti/python-miio saleweaver/python-amazon-sp-api scrapy/scrapy scverse/scanpy slackapi/bolt-python slackapi/python-slack-sdk snowflakedb/snowflake-connector-python softlayer/softlayer-python spec-first/connexion statsmodels/statsmodels tfranzel/drf-spectacular tobymao/sqlglot tornadoweb/tornado tortoise/tortoise-orm wagtail/wagtail Apache-2.0 MIT Apache-2.0 Apache-2.0 BSD-3-Clause Scientific/Engineering::Electronic Design Automation (EDA) Other MIT Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 MIT MIT Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 MIT Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 Apache-2.0 BSD-3-Clause Software Development::Libraries Software Development::Testing MIT Text Processing::Linguistic MIT Software Development::Libraries Apache-2.0 Utilities Apache-2.0 Scientific/Engineering::Image Processing MIT Scientific/Engineering::Artificial Intelligence MIT Software Development::Quality Assurance MIT Apache-2.0 System::Monitoring BSD-3-Clause Software Development::User Interfaces BSD-2-Clause Software Development::Documentation Apache-2.0 MIT Apache-2.0 MIT MIT GPL-3.0 MIT BSD-3-Clause Software Development::Libraries BSD-3-Clause Scientific/Engineering::Bio-Informatics Communications::Chat MIT Communications::Chat MIT Software Development::Libraries Apache-2.0 Software Development::Libraries MIT Internet::WWW/HTTP Apache-2.0 BSD-3-Clause Scientific/Engineering::Information Analysis BSD-3-Clause Software Development::Documentation MIT Apache-2.0 Apache-2.0 BSD-3-Clause Software Development::Build Tools Scientific/Engineering::Artificial Intelligence Software Development::Libraries Software Development::Libraries Scientific/Engineering::Artificial Intelligence Software Development::Libraries Scientific/Engineering::Artificial Intelligence Internet::WWW/HTTP Scientific/Engineering::Artificial Intelligence Scientific/Engineering::Artificial Intelligence Internet::WWW/HTTP Scientific/Engineering::Human Machine Interfaces Software Development::Libraries Scientific/Engineering::Artificial Intelligence Scientific/Engineering::Artificial Intelligence Scientific/Engineering::Artificial Intelligence Software Development::Libraries Scientific/Engineering::Artificial Intelligence Software Development::Debuggers Software Development::Build Tools Scientific/Engineering::Artificial Intelligence Internet::WWW/HTTP Scientific/Engineering::Image Processing Home Automation Internet::WWW/HTTP Database::Database Engines/Servers Internet::WWW/HTTP Database::Front-Ends Internet::WWW/HTTP Build Tools AI Libraries Libraries AI Libraries AI Internet AI AI Internet Other Libraries AI AI AI Libraries AI Libraries Testing Other Libraries Other Other AI Other Other Other Other Other Build Tools AI Internet Other Other Internet Libraries Other Other Other Libraries Libraries Internet Other Other Database Internet Database Internet Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Fast-Validation Table 5: The information of the repositories involved in FEABench. 16 Repository Name sympy/sympy joke2k/faker conan-io/conan tobymao/sqlglot scikit-learn/scikit-learn pvlib/pvlib-python deepset-ai/haystack Project-MONAI/MONAI matplotlib/matplotlib sphinx-doc/sphinx googleapis/python-aiplatform astropy/astropy Textualize/textual falconry/falcon softlayer/softlayer-python Textualize/rich rytilahti/python-miio sqlfluff/sqlfluff google-deepmind/optax pydata/xarray boto/boto3 roboflow/supervision RDFLib/rdflib huggingface/datasets aws-cloudformation/cfn-lint boto/botocore pgmpy/pgmpy huggingface/huggingface_hub prometheus/client_python pypa/hatch scrapy/scrapy slackapi/python-slack-sdk django/django pydicom/pydicom pylint-dev/pylint embeddings-benchmark/mteb python-hyper/h2 mwaskom/seaborn pyvista/pyvista dpkp/kafka-python lark-parser/lark astronomer/astronomer-cosmos fairlearn/fairlearn huggingface/accelerate docker/docker-py huggingface/trl joblib/joblib open-mmlab/mmengine openvinotoolkit/datumaro pygments/pygments pyocd/pyOCD pyro-ppl/pyro tortoise/tortoise-orm DLR-RM/stable-baselines3 EleutherAI/lm-evaluation-harness PyThaiNLP/pythainlp TileDB-Inc/TileDB-Py atlassian-api/atlassian-python-api aws/sagemaker-python-sdk googleapis/python-bigquery gradio-app/gradio graphql-python/graphene prompt-toolkit/python-prompt-toolkit snowflakedb/snowflake-connector-python spec-first/connexion statsmodels/statsmodels tornadoweb/tornado pallets/flask pylint-dev/astroid Aider-AI/aider Cog-Creators/Red-DiscordBot aws-powertools/powertools-lambda-python Libraries biopragmatics/bioregistry cocotb/cocotb facebookresearch/hydra huggingface/pytorch-image-models minio/minio-py pre-commit/pre-commit saleweaver/python-amazon-sp-api scverse/scanpy slackapi/bolt-python tfranzel/drf-spectacular wagtail/wagtail Class (Short) Max PR No. 27454 Mathematics Testing 2144 17534 Build Tools Database 4597 30289 AI 2336 Physics 8669 AI Medical 8275 29140 Other 13196 Other AI 4830 17525 Physics 5444 Other 2425 Internet 2207 Libraries 3548 Libraries 1993 Other 6534 Other 1164 AI 9879 Other 4371 Libraries 1773 Other 3025 Libraries 7342 AI 3898 Other 3331 Libraries 1887 AI 2683 AI 1080 Other 1860 Build Tools 6598 Libraries 1627 Other 18807 Internet Medical 2195 10168 Other 1730 AI 1291 Internet 3798 Other 7045 Other 2442 Libraries 1503 Other 1439 Build Tools 1472 AI 3293 AI 3297 Libraries 2550 AI 1641 Libraries 1620 Other 1689 Other 2837 Other 1734 Other 3413 AI 1840 Database 2069 AI 2609 AI 1056 Other 2128 Libraries 1476 Internet 4987 AI Internet 2102 10271 Other 1583 Libraries 1949 Other 2127 Libraries 2011 Internet 9462 Other 3452 Internet 5640 Internet 2669 Libraries 2767 Other 6499 Other 5814 1346 4345 3005 2398 1472 3382 1638 3427 1234 1362 12732 Other Other Libraries Libraries Libraries Other Internet Other Other Other Internet # PR # All tasks 6034 12857 518 1386 3715 6185 1996 2463 4174 17489 444 1072 1438 4132 1809 3468 4057 18163 1597 5798 826 2927 4292 11205 819 2205 499 1279 661 1277 283 1142 249 979 2079 3448 174 805 1838 4310 130 757 94 1021 370 1562 792 4118 790 2583 777 2145 368 902 586 1559 134 429 313 721 846 3253 266 764 725 18482 494 979 1859 4431 137 1050 144 1072 469 1093 1149 3837 212 937 135 467 270 754 229 937 352 1607 538 1554 248 1181 250 701 350 1013 521 1388 346 886 175 954 1038 2302 283 607 199 534 52 961 269 667 488 1184 49 849 715 3219 358 1372 1210 4707 19 68 49 713 466 1311 327 879 1444 3793 341 1573 404 2604 301 1810 31 346 157 4060 517 4398 156 784 493 2137 442 1301 85 551 242 890 527 1222 21 1027 516 1535 159 446 135 345 1531 # Candidates 819 186 473 185 186 102 317 463 164 126 241 405 139 67 105 41 46 296 54 234 38 30 21 99 92 83 76 130 14 34 53 29 32 42 79 15 11 40 292 17 10 39 29 67 24 29 11 59 98 79 11 369 46 23 9 39 34 14 116 65 81 2 3 22 31 121 16 17 8 1 18 115 28 34 35 17 14 23 9 35 9 12 213 # in Full 239 126 124 116 83 52 49 37 34 30 29 27 27 27 26 24 23 19 19 17 17 14 13 12 11 11 11 10 10 9 9 9 7 7 7 7 6 5 5 5 5 4 4 4 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # in Lite 46 8 23 14 9 10 8 1 7 4 1 8 1 3 2 4 1 2 3 2 2 7 2 2 0 0 2 1 1 1 2 0 1 1 1 1 0 0 2 0 2 1 1 2 1 0 0 1 0 1 0 0 0 0 1 1 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 Table 6: Statistics of how many PRs (task instances) are left during the data collection procedures."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}