{
    "paper_title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models",
    "authors": [
        "Pei Wang",
        "Yanan Wu",
        "Zekun Wang",
        "Jiaheng Liu",
        "Xiaoshuai Song",
        "Zhongyuan Peng",
        "Ken Deng",
        "Chenchen Zhang",
        "Jiakai Wang",
        "Junran Peng",
        "Ge Zhang",
        "Hangyu Guo",
        "Zhaoxiang Zhang",
        "Wenbo Su",
        "Bo Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 0 1 7 1 1 . 0 1 4 2 : r Preprint MTU-BENCH: MULTI-GRANULARITY TOOL-USE BENCHMARK FOR LARGE LANGUAGE MODELS Pei Wang1, Yanan Wu1, Zekun Wang1, Jiaheng Liu1, Xiaoshuai Song1, Zhongyuan Peng1,2, Ken Deng1, Chenchen Zhang1, Jiakai Wang1, Junran Peng2, Ge Zhang3, Hangyu Guo1, Zhaoxiang Zhang2, Wenbo Su1, Bo Zheng1 1Alibaba Group, 2University of Chinese Academy of Sciences, 3University of Waterloo {yupei.wp, lixing.wyn, ljh411989}@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose multi-granularity tool-use benchmark for large language models called MTUBench. For the multi-granularity property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing highquality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git."
        },
        {
            "title": "INTRODUCTION",
            "content": "Since the release of large language models (LLMs) such as GPT-4 (OpenAI, 2023), Natural Language Processing (NLP) has entered new wave of advancements, even being considered as the spark of Artificial General Intelligence (AGI) (Bubeck et al., 2023). Recently, there has been surge of research focused on enabling LLMs to interface with external tools, such as calculators (Cobbe et al., 2021), search engines (Schick et al., 2023), and booking service APIs (Qin et al., 2023b). This approach, referred to as Tool Learning (Schick et al., 2023; Qin et al., 2023a;b; Wang et al., 2023), allows LLMs to not only accurately perform precise calculations, but also maintain up-to-date information. Furthermore, it enables LLMs to function as end-to-end AI assistants that are capable of fulfilling real-world user needs such as booking hotels or ordering food. Thus, Tool Learning is critical step to transform LLMs into general AI agents. Previous works have explored to stimulate the ability to call tools for LLMs (Schick et al., 2023; Qin et al., 2023b; Zhuang et al., 2023; Tang et al., 2023; Paranjape et al., Figure 1: The circles from inside to outside represent the data source, scenes included in MTU-Instruct, and the automatic evaluation metrics in MTU-Eval. * First three authors contributed equally. Corresponding Author: Jiaheng Liu. Preprint Table 1: Comparison of various tool-use benchmark datasets. Auto. Eval. denotes automatic evaluation without GPT. S-S, S-M, M-S, and M-M denote single-turn single-tool, singleturn multi-tool, multi-turn single-tool, and multi-turn multi-tool, respectively. OOD refers to whether the benchmark accounts for an out-of-distribution setting, where the test set consists of domains different from those in the training set. Real-World means whether the dialogues in the benchmark are sampled from real-world scenarios. The numbers in the evaluation range mean: 1 tool selection, 2 parameter selection, 3 dialogue-level success rate, 4 turn-based success rate, 5 tool number, and 6 tool order. Dataset #Dialogues #Tools #Turn-#Tool RealWorld Auto. Eval. Eval. Range Train Test OOD MetaTool (Huang et al., 2024) API-Bank (Li et al., 2023) ToolLLM (Qin et al., 2023b) API-Bench (Patil et al., 2023) ToolAlpaca (Tang et al., 2023) ToolQA (Zhuang et al., 2023) MTU-Bench (Ours) 21,127 199 S-S, S-M 2,202 2,211 S-S, S-M, M-S, M-M 12,657 16,464 S-S, S-M 17,002 1,645 S-S 3,938 1,530 400 13 159,061 S-S, S-M, M-S, M-M S-S, S-M S-S, S-M, M-S, M-M 1 2 1 2 3 2 3 2 3 1 2 3 4 2 3 1 2 3 4 5 6 2023; Li et al., 2023). For example, (Schick et al., 2023) propose to convert tool calls into text spans, such as <API>SOME PARAMETER KEY-VALUE PAIRS</API> to denote the tool name and parameters with an additional special token (i.e., </API>) to show the initiation of tool execution. Moreover, the recent works (i.e., ToolBench (Xu et al., 2023; Qin et al., 2023b), APIBench (Patil et al., 2023), and API-Bank (Li et al., 2023)) have investigated instruction tuning data or evaluation for tool-use. However, we observe that exhibit several limitations to varying degrees as shown in Table 1: (1) some do not consider multi-turn dialogue scenarios (Patil et al., 2023; Xu et al., 2023; Zhuang et al., 2023); (2) some do not address multi-tool usage scenarios (Tang et al., 2023; Patil et al., 2023; Li et al., 2023; Xu et al., 2023); (3) several works use external API tools to deduce user instructions, but these synthesized instructions often do not accurately align with actual real-world user needs (Qin et al., 2023b); (4) many of them rely on GPT for the evaluation, leading to heavy evaluation costs (Qin et al., 2023b; Tang et al., 2023); and (5) many do not comprehensively assess fine-grained aspects of tool-use (Li et al., 2023; Qin et al., 2023b; Patil et al., 2023), such as the accuracy of tool call orders, complex tool calls involving inheritance relationships, per-dialogue turn accuracy of tool and parameter selection, four-quadrant analysis of single/multi-turn and single/multi-tool settings, etc. To remedy these issues, in Figure 1, we introduce MTU-Bench (Multi-Granularity Tool-Use Benchmark), which comprises both MTU-Instruct for training and MTU-Eval for evaluation. As illustrated in Figure 2, we sample real-world user instructions from various existing open-source dialogue datasets such as MultiWOZ (Budzianowski et al., 2018) and SGD (Rastogi et al., 2020a; Lee et al., 2022). After instruction clustering, the detected user intents and slot filling are leveraged to synthesize API calls using GPT-4 (OpenAI, 2023). The synthesized data includes the thoughts, the actions (i.e., tool names), the action parameters, and the observations (i.e., the generated API execution results). This data forms our MTU-Bench dataset. Following meticulous quality verification by GPT-4 and manual check, we split the MTU-Bench data into training and testing splits, involving 54798 dialogues in total, as well as 136 tools. In our MTU-Eval, we propose series of fine-grained metrics such as tool selection accuracy, parameter selection accuracy, success rate, turn success rate, task process rate, tool number accuracy, tool order accuracy, etc., to evaluate the tool-use abilities in comprehensive manner, where the GPT API costs are not needed for evaluation. Moreover, we also pick out hard subset from the test split to include more complex tool-use scenarios such as easily confusable tools, nonsensical or noisy tools, tool parameter updating, etc. Preprint Finally, by fine-tuning LLaMA3 (Dubey et al., 2024) on MTU-Bench, we find that our resulting model, MTU-LLaMA, performs the best in various scenarios and metrics, demonstrating the effectiveness of our MTU-Instruct. In summary, our contributions are as follows: (1). MTU-Bench: We introduce novel automated data synthesis pipeline designed to derive high-quality, fine-grained tool-use datasets from pre-existing task-oriented dialogue datasets. This pipeline facilitates the creation of MTU-Bench, comprising MTU-Instruct for training purposes and MTU-Eval for evaluation. (2). MTU-Instruct and MTUEval: We introduce the high-quality and diverse instruction tuning dataset, MTU-Instruct, to improve models tool-use capabilities in real-world scenarios. Additionally, we propose novel automatic evaluation framework, MTU-Eval, which assesses various tool-use settings through comprehensive and fine-grained metrics, free of GPT-based evaluators. (3). MTU-LLaMA and Experimental Findings: After instruction tuning on MTU-Instruct, we obtain strong open-source model for tooluse, MTU-LLaMA. Our comprehensive experiments reveal several findings regarding the tool-use capabilities of LLMs, particularly in terms of multi-turn dialogue scenarios, multi-tool settings, and error cases. These findings offer valuable insights for advancing tool-use in LLMs."
        },
        {
            "title": "2 MTU-BENCH",
            "content": "The MTU-Bench involves both MTU-Instruct for training and MTU-Eval for evaluation. We first present the data construction and analysis in 2.1, and then show the evaluation procedure in 2.2. 2.1 MTU-INSTRUCT Figure 2: The workflow for MTU-Instruct construction. It involves five steps: (1) Data Collection, (2) Tool Creation, (3) Tool Clustering, (4) Tool Documentation, and (5) Tool-Use Synthesis. 2.1.1 DATA CONSTRUCTION Many previous methods based on scraped API documentation and GPT-4 inspired synthesis faced limitations due to lack of data diversity, resulting in weak generalization capabilities. Inspired by the mapping relationships between intents and APIs, as well as between slots and tool parameters, we designed an automated data synthesis pipeline that transforms traditional dialogue datasets into tool-use datasets. To enhance data diversity, we collected datasets from multiple NLP datasets and standardized them into unified tool documentation. The diversity is illustrated in Figure 3. As shown in Figure 2, the construction of MTU-Bench involves five primary steps: (1) collecting task-oriented dialogue datasets, particularly those containing intents and slots, (2) creating tools through grammar-based transformations or GPT-4 synthesis, (3) clustering the tools based on their similarities, (4) generating tool documentation using GPT-4, and (5) synthesizing tool-use samples consisting of thoughts, actions(tool calls), action inputs, observations, and adjusted responses based on the dialogue and tool documentation, followed by holistic validation process. Data Collection. To improve the diversity of our dataset, we collect several open-source taskoriented dialogue datasets as our data sources. These datasets focus on dialogues for specific tasks such as flight reservations or movie bookings, which are highly suitable for synthesizing tool-use data. The multi-turn dialogue datasets include MultiWOZ (Budzianowski et al., 2018), SGD (Rastogi 3 Preprint et al., 2020b), TaskMaster (Byrne et al., 2019) and MetaLWOZ (Shalyminov et al., 2020). The single-turn dialogue datasets include ATIS (Hemphill et al., 1990) and SNIPS (Siddhant et al., 2018). They provide diverse task-oriented dialogues across various domains, real-world conversation and fine-grained annotation, encompassing both single-turn and multi-turn dialogues. Tool Creation. We employ two approaches to create tools. (1) Grammar-based creation. For dialogue datasets that already have detected intents and filled slots, we directly convert the intents into tool names and the slots into tool parameters. For example, in the user query find flight from charlotte to las vegas, the intent Flight will convert to the tool name, and the slot from_location=charlotte, to_location=las vegas will convert to the parameters, resulting in the tool call FLIGHT(FROM_LOCATION=CHARLOTTE,TO_LOCATION=LAS VEGAS). (2) LLM-based creation. For dialogue datasets without predefined intents or slots, we utilize GPT-4 to make the tools. Based on the contextual situation, we categorize it into five scenarios: information missing, information exists, information confirmed, aimless chatting and specific API call. Provided with the historical dialogue context and the current round of conversation, especially the assistants response, LLM needs to determine which situation belongs to the current situation. Information missing. If the response is asking for important information, it should be the situation of missing information, no tool call should be made, and necessary parameters should be accumulated for related tools. Information exists. When the LLM can provide response based solely on the information from the dialogue history, no tool call will be made and the model can directly reply. Information confirmed. When the assistant is confirming information (e.g.,Would you like to confirm this flight reservation?), this is classified as information to be confirmed. Aimless chatting. If the scenario pertains to aimless chatting or situations that do not necessitate tool invocation, no tool call should be made. Specific API call. Only if the LLM determines that an API call is necessary to fulfill the users request, it is encouraged to generate an appropriate pseudo-tool for invocation, along with corresponding description and parameters of this tool. Tool Clustering. Due to the diversity in both the intent detection and slot-filling strategies, as well as the creation of specific tools based on LLM, the synthesized tool set can be highly redundant. For example, tools like search_movie and find_movie may have different names but essentially perform the same function. To address this redundancy, we introduce tool clustering phase. Specifically, we cluster the tool names based on InsTag(Lu et al., 2023) with fixed distance threshold. Then, all tool names and their parameter names are standardized to the centroid of their respective cluster, resulting in reduction ratio of 20:1. Tool Documentation. To enable models to use specific tools effectively, we compile all tool usage into comprehensive tool document. This document allows the model to determine the appropriate tool names and their usage. We prompt the GPT-4 model to write description for each tool generated in the previous step, along with information about its parameters(required and optional) and returns. The collection of these tool entries forms the final tool document, which is included as part of the LLMs context. Please refer to Appendix for the details. Tool-Use Data Synthesis. In this step, we convert all samples from task-oriented dialogue datasets into tool-use dataset with GPT-4, following the format of ReAct (Yao et al., 2023). We provide the dialogue history and the tool document generated in the previous steps as context for GPT-4, and then prompt it to generate three key components: (1) thought: the reasoning process behind the tool selection, (2) action: the name of the tool being invoked, and (3) action input: the parameters used in the tool call along with their values. This chain of thought prompting technique enhances the models ability to reason over the most appropriate tool and accurately input the parameter values. We also allow the model to generate any additional parameters needed that are beyond those listed in the tool document, to ensure completeness and flexibility of tool-use. We further ask the GPT-4 to simulate tool execution, generating observation (i.e., the results of the simulated tool execution) and then produce the final response for the current dialogue turn. The observations are aligned with the return information in the tool document and are generated in structured format, such as JSON dictionary. The model then formulates response based on the observations, either to report the status of the tool execution or to complete the dialogue turn. 4 Preprint To ensure data quality, we apply various quality filters and adjustments, including heuristic rules, GPT-4, and manual annotation. For example, we split some multi-turn dialogues into single-turn dialogues to balance the data distribution. For the training set, we use GPT-4 to verify the accuracy and necessity of tool selection, check parameter matching, adjust thoughts, rewrite response and ensure consistency in the tool definitions. Through GPT-4, we filter out about 10% of defective samples. For the test set, we hired multiple experts to conduct manual quality checks based on similar principles. Each sample was checked by three experts, and the differences in labeling were determined by the fourth expert. Based on the number of dialogue turns and the number of tools in each dialogue, the synthesized data can be categorized into four types: (1) Single-turn Single-tool (S-S), (2) Single-turn Multi-tool (S-M), (3) Multi-turn Single-tool (M-S), and (4) Multi-turn Multi-tool (M-M). For more detailed information about the construction of MTU-Bench, including prompt templates and the tool documentation, please refer to the Appendix B. 2.1.2 DATA ANALYSIS Domain #Topics #Tools Place Media Traffic Daily Life Money Information Disease Others 4 2 5 5 2 4 5 Additional Statistics 27 18 31 10 12 2 14 #Dialogues (Train/Test) 29,667 / 150 13,730 / 92 17,720 / 89 23,706 / 163 1,740 / 32 9,125 / 3,371 / 125 - / 68 #Dialogues (Train/Test) #Tools (Train/Test) Avg. Turns per Dialogue Avg. Tools per Dialogue Avg. Tools per Turn 54,367 / 431 122 / 14 2.6 5.6 2. Figure 3: The domain distribution of tools. Figure 4: Statistics of MTU-Bench. Statistics. Figure 4 delineates the statistical metrics of our MTU-Bench, underscoring its substantial scale and diversity. It is particularly designed to encompass multi-tool and multi-turn settings, as well as real-world domains and topics. Diversity. Figure 3 illustrates the domain and topic distribution of the tools within our MTU-Bench. Figures 9 and 10 depict the distributions of dialogue turn counts, word counts, tool numbers, and other length-related metrics across both training and testing splits. These figures underscore the diversity of our MTU-Bench in terms of length and topic distribution. We refer the readers to Appendix for more detailed data analysis and statistics. 2.2 MTU-EVAL We propose MTU-Evalthe first evaluation framework that encompasses multiple levels of difficulty, diverse domains and cases of tool-use, varying numbers of dialogue turns and tools, as well as multifaceted evaluation by considering various granularities and aspects of LLM tool-use. We present MTU-Eval in two parts: (1) Test Set Splitting and (2) Evaluation Metrics. 5 Preprint 2.2.1 TEST SET SPLITTING We construct two distinct test sets from MTU-Bench through manual sampling: (1) the normal test set and (2) the more challenging hard test set. Normal/Hard Test Set. MTU-Bench includes data from 31 different topics, such as weather-related and calendar-related tasks. Initially, we select data from 5 topics as an Out-of-Distribution (OOD) test split. From the remaining 26 topics, we further split the data into training set and an in-domain test set. For more challenging evaluations, we manually curate hard test set, which includes more complex tool-use cases, such as those involving long tool parameters, easily confusable tools, parameter value updating, scenarios with large number of tools, etc., as listed in Appendix D. 2.2.2 METRICS For Single-Tool Scenarios (S-S & M-S). For scenarios where there is just single tool involved, we evaluate two metrics: Tool Selection Accuracy (TS) and Parameter Selection Accuracy (PS). For Multi-Turn Scenarios (M-S & M-M). In multi-turn dialogues, the following metrics are introduced: (1) Success Rate (SR): binary metric where the entire dialogue is considered successful for tool-use: (=1) only if there are no errors throughout all turns; otherwise, it is considered unsuccessful (=0). (2) Averaged Turn Success Rate (ATS): We first evaluate each dialogue turn with the tool-use success rate (where each turn is marked as either 0 or 1), then average the binary scores with the total dialogue turns of dialogue session. This score takes into account the finer-grained success rate of tool-use at the turn level. (3) Soft Averaged Turn Success Rate (SATS): This metric adjusts the ATS based on the proximity of errors to the current turn. Specifically: If turn is incorrect, the score is 0. If turn is correct, given as the index of this turn and as the index of the last incorrect turn, the score is 1 when < and 1 e(ji) when > i. This design is based on the intuition that closer incorrect turn can negatively impact subsequent turns. Moreover, the closer the turn becomes incorrect, the lower the overall accumulated score, even if the remaining turns are correct. (4) Task Process Rate (TPR): This is calculated as the ratio of the first incorrect turn to the total number of turns. This metric is included to capture how early in the dialogue the first mistake occurs, as earlier errors tend to disrupt the overall task flow more significantly. For Multi-Tool Scenarios (S-M & M-M). For scenarios involving multiple tools, the following metrics are introduced: (1) Tool Number Accuracy (TN): Denote the predicted tool list as Pred and the ground truth tool list as GT, TN = Pred GT / Pred GT , where denotes the number of tools. (2) Tool Order Accuracy (TO): This metric evaluates the correctness of the tool sequence, adjusted by decay factor: TO = LCR(GT, Pred) / GT , where LCR is the longest common subsequence, and is decay coefficient calculated as: = cos ((π/2) (i/Pred), where is the starting position of the longest common subsequence. The value of ranges from 0 to 1, with faster decay for positions later in the sequence. These metrics offer comprehensive and fine-grained evaluation of LLMs tool-use capabilities. Unlike conventional approaches that focus only on overall success rates (Qin et al., 2023b), our metrics account for the dynamics along dialogue turns and the dependencies between multiple tools. We refer the reader to Appendix for more examples of how to compute these metrics."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "Experimental Setup. We evaluate 5 closed-source LLMs such as GPT-3.5 (OpenAI, 2023), GPT-4 (OpenAI, 2023), Qwen-Max (Team, 2024), GLM-4-Plus1 and DeepSeek2.5 (DeepSeekAI, 2024). We also evaluate numerous open-source LLMs such as LLaMA2 (Touvron et al., 2023) and LLaMA3 (AI@Meta, 2024) series, Qwen1.5 (Team, 2024) and Qwen2 (Yang et al., 2024) series, Mistral (Jiang et al., 2023), ChatGLM3 and GLM-4 (GLM et al., 2024; Du et al., 2022; Zeng et al., 2022) series. Then, we also compare 2 models specifically enhanced for tool-use: ToolLLaMA (Qin et al., 2023b) and our MTU-LLaMA, which isfine-tuned on MTU-Instruct based on LLaMA3-8B1https://bigmodel.cn/dev/api/normal-model/glm-4 6 Preprint Table 2: Results of different models on the normal set of MTU-Eval (S-S & M-S). S-S and M-S denote Single-Turn Single-Tool and Multi-Turn Single-Tool settings, respectively. We utilize green (1st), blue (2nd), and yellow (3rd) backgrounds to distinguish the top three results within both open-source and tool-use-specific models. We employ bold and underlined text to denote the top and second-best results across all model categories (same markers for the other tables). All the baselines are instruction-tuned models. Models GPT-4 GPT-3.5 Qwen-Max GLM-4-Plus DeepSeek V2.5 LLaMA2-7B LLaMA2-70B LLaMA3-8B LLaMA3-70B Qwen1.5-14B Qwen1.5-72B Qwen2-7B Qwen2-72B Mistral-7B ChatGLM3-6B GLM-4-9B S-S PS 52.08 46.15 50.00 50.00 43.75 3.85 33.33 30.77 57. 34.62 38.46 38.46 48.08 25.00 0.00 45.83 TS 95.83 84.62 91.67 95.83 93. 15.38 70.59 65.38 86.54 75.00 78.85 73.08 86.54 60.42 10.00 91. Avg. TS PS ATS SATS SR TPR Avg. M-S Closed-Source Large Language Models 73.96 65.38 70.83 72.92 68.75 88.10 69.05 86.73 85.03 86. 74.49 50.68 66.67 71.43 69.39 73.67 50.51 64.96 72.01 68.09 Open-Source Large Language Models 9.62 47.28 48.08 72.12 54.81 58. 55.77 67.31 42.71 5.00 68.75 33.67 47.28 35.71 79.25 62.93 80. 71.09 79.93 61.22 22.90 63.95 28.91 30.95 17.35 61.90 36.73 61. 49.66 61.22 37.07 5.99 42.18 26.78 30.18 17.53 62.18 35.17 59. 49.59 58.52 37.33 5.79 42.72 67.36 40.81 57.88 65.19 60.20 20.67 23.78 12.34 54. 27.37 50.88 40.14 50.28 29.14 3.66 35.98 29.63 1.85 20.37 24.07 18.52 0.00 0. 0.00 14.81 1.85 16.67 5.56 14.81 3.70 0.00 3.70 45.35 12.70 35.53 44.18 38. 7.12 9.82 1.72 32.33 6.34 28.75 13.30 25.15 9.98 0.00 19. 63.10 37.60 55.36 60.32 56.77 19.53 23.67 14.11 50.88 28.40 48.75 38.22 48.32 29. 6.39 34.59 Tool-Use-Specific Large Language Models ToolLLaMA2-7B MTU-LLaMA (Ours) 85.42 92.31 18.75 50.00 52.08 71. 31.97 81.63 7.82 67.69 7.6 66.94 5.20 58.74 0.0 9.26 5.73 32. 9.72 52.79 Instruct. Note that all baselines are instruction-tuned models. We refer the readers to Appendix for more details on the hard cases, the prompt templates, and the metric computation. 3.1 MAIN RESULTS Overall Performance. The experimental results for the normal set are presented in Table 2 (S-S & M-S) and Table 3 (S-M & M-M). The results on the hard set are illustrated in Table 4. These results reveal several key findings: (1) Open-source models typically exhibit inferior performance compared to closed-source models in nearly all metrics, with the exception of GPT-3.5. However, certain models, including LLaMA3-70B and Qwen2-72B, demonstrate results comparable to those achieved by closed-source models. (2) GPT-4 consistently exhibits superior performance on the normal set; however, its performance decreases on the hard set compared to GLM-4-Plus. Qwen-Max demonstrates exceptional performance in the M-M setting, with its advantages becoming more pronounced in the hard setting, even surpassing GPT-4. Similarly, GLM-4-Plus exhibits outstanding performance in the S-S and M-S settings, and its superiority is further amplified in the hard setting, also exceeding that of GPT-4. DeepSeek V2.5 performs admirably in the S-M setting. (3) Our MTU-LLaMA exhibits substantial advancements over its initialization, i.e., LLaMA3-8B-Instruct, across all settings and metrics. It is also competitive with some closed-source models, underscoring the effectiveness of our MTU-Instruct. (4) Generally, all the models perform better on the normal set than on the hard set, indicating LLMs limitations in handling more challenging tool-use scenarios. (5) Notably, despite being fine-tuned specifically for tool-use, ToolLLaMA exhibits poor performance across all settings and metrics, suggesting its limited generalizability. Effect of Multi-Turn. We compare the single-turn (S-S, S-M) and multi-turn (M-S, M-M) settings across the Tables 2, 3, and 4, and have following findings: (1) Both closed-source and open-source models tend to perform worse in multi-turn settings (M-S and M-M) compared to single-turn settings (S-S and S-M). (2) Our MTU-LLaMA shows relatively better adaptation and robustness to multi-turn settings. (3) Based on our novel TPR metric, we can observe that LLMs typically experience tool-use Preprint Table 3: Results of different models on the normal set of MTU-Eval (S-M & M-M). S-M and M-M denote Single-Turn Multi-Tool and Multi-Turn Multi-Tool settings, respectively. S-M TO Avg. TN TO ATS SATS SR TPR Avg. Closed-Source Large Language Models M-M Models GPT-4 GPT-3.5 Qwen-Max GLM-4-Plus DeepSeek V2.5 LLaMA2-7B LLaMA2-70B LLaMA3-8B LLaMA3-70B Qwen1.5-14B Qwen1.5-72B Qwen2-7B Qwen2-72B TN 66.85 32.64 39.76 45.76 56.88 3.39 6.82 14.79 26. 22.12 28.04 24.52 52.76 70.52 38.22 48.82 48.48 60.28 3.94 8.48 20.30 32.68 28.22 30. 29.59 59.98 68.68 35.43 39.29 47.12 58.58 72.10 24.72 62.00 53.95 50.15 73.38 25.46 64.07 54.58 51.79 68.77 16.11 56.27 49.17 44.84 Open-Source Large Language Models 3.67 7.65 17.55 29.76 25.17 29.32 27.05 56.37 22.22 30.12 9.43 33. 27.78 23.00 21.04 45.93 22.22 30.49 10.04 35.71 28.67 23.31 22.75 47. 22.90 28.77 4.44 26.94 21.07 52.94 15.24 42.02 5.44 9.21 9. 66.07 12.22 55.27 45.72 41.26 21.80 28.77 2.81 23.82 19.04 51.86 11.52 38.07 3. 8.01 6.13 30.95 0.00 4.76 4.76 7.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7. 0.00 2.38 0.00 0.00 7.14 59.52 3.97 52.38 39.48 34.72 19.92 28.77 0.00 17. 14.88 7.34 4.76 29.76 0.60 5.95 0.00 61.80 13.75 49.13 41.28 38.32 19.92 28. 4.46 22.99 18.57 26.41 12.55 29.76 5.16 7.88 8.04 1.59 19. 2.48 30.70 Mistral-7B 14.21 18.22 16.22 10. 11.11 ChatGLM3-6B GLM-4-9B 6.53 23.64 8.56 27.58 7.55 25.61 10.64 16. 11.11 16.45 ToolLLaMA2-7B MTU-LLaMA (Ours) 11.52 55.39 11.52 58.55 11.51 56.97 4.07 42. 4.07 43.42 2.78 39.64 2.34 32.50 Tool-Use-Specific Large Language Models errors within the initial 30%-50% turns for closed-source models, and within the first 0%-30% turns for open-source models. (4) Most models such as Qwen2-72B have significantly higher ATS scores than TPR scores. This implies that while LLMs frequently encounter tool-use errors in the initial turns, they can still correctly use tools in subsequent turns in most cases. However, this correctness does not account for the cascading effect of previous errors, but solely considers the success rate of independent tool usage. (5) Fortunately, the SATS scores can be treated as an equilibrium between ATS and TPR metrics, which simultaneously account for the positions at which tool-use errors occur and the subsequent impact on later turns. Effect of Multi-Tool. Based on multi-tool settings (S-M and M-M) across Tables 2, 3, and 4, we derive the following findings: (1) Multitool settings (S-M and M-M) show significant complexity, leading to noticeable performance drops for most models. (2) Despite the complexity, models like GPT-4, Qwen2-72B and our MTU-LLaMA show stronger robustness. (3) In contrast, the good models in single-tool settings such as GLM-4-Plus (closed-source) and LLaMA3-70B (open-source), are surpassed by Qwen-Max (closed-source) and Qwen2-72B (open-source), respectively, indicating the superior performance of Qwen series in multi-tool settings. (4) The model rankings by TN and TO are highly consistent, implying that models with better control over the number of tools also tend to manage tool sequences effectively, suggesting strong correlation between these capabilities. Table 4: Average scores on the hard set of MTUEval. (Detailed results are shown in Appendix E). Models S-S M-S S-M M-M Closed-Source Large Language Models GPT-4 GPT-3.5 Qwen-Max GLM-4-Plus DeepSeek V2.5 77.88 41.96 77.88 82.69 80.77 44.61 30.86 42.11 47.61 44.94 58.07 18.39 24.01 30.90 40.01 Open-Source Large Language Models LLaMA2-7B LLaMA2-70B LLaMA3-8B LLaMA3-70B Qwen1.5-14B Qwen1.5-72B Qwen2-7B Qwen2-72B Mistral-7B ChatGLM3-6B GLM-4-9B 28.57 28.57 25.89 71.43 44.64 56.73 58.93 68.40 26.92 9.09 47. 17.13 23.46 12.85 40.40 29.39 29.92 28.73 38.42 26.04 5.57 30. 2.35 1.74 9.91 20.67 12.81 18.85 17.50 37.14 11.48 18.89 18. 41.36 11.87 45.08 39.53 30.62 11.76 16.79 5.89 20.56 9.37 17.93 10.17 25.13 10. 9.52 9.52 Tool-Use-Specific Large Language Models ToolLLaMA2-7B MTU-LLaMA (Ours) 18.27 37.5 10.19 43.10 0.51 39. 2.34 24.70 8 Preprint Figure 5: (a) Effect of dialogue turns. (b) Effect of different number of tools. 3.2 ANALYSIS OOD Performance. To evaluate the generality of MTU-LLaMA, we measure its performance on the OOD test split of MTU-Bench and two other OOD tool-use benchmarks, i.e., API-Bank (Li et al., 2023) and ToolTalk (Farn & Shin, 2023). in Table 5, we compare the performance of MTU-LLaMA, LLaMA3-8B-Instruct, and GPT-4 on these benchmarks under the M-S setting, and We observe that MTU-LLaMA outperforms LLaMA3-8B-Instruct on all three OOD benchmarks. Notably, MTU-LLaMA achieves performance comparable to that of GPT-4 on API-Bank, which show strong generalizability of MTU-LLaMA. Effect of Dialogue Turns. We illustrate the impact of dialogue turns on tool selection accuracy in Figure 5. We observe that performance slightly declines as the number of dialogue turns increases. Our MTU-LLaMA exhibits the most gradual decrease in performance, demonstrating its robustness to higher dialogue turns. Table 5: OOD Performance of our MTU-LLaMA. Models MTU-Bench (OOD) TS PS ATS SATS SR TPR Avg. 67.28 GPT-4 36.71 LLaMA3-8B MTU-LLaMA 47. 67.57 36.71 37.88 65.19 35.53 62.05 Models 47.23 23.00 54.58 ToolTalk 32.31 7.69 9.26 37.72 11.31 35. 52.88 25.16 41.10 TS PS ATS SATS SR TPR Avg. 51.08 GPT-4 LLaMA3-8B 30.00 MTU-LLaMA 30.25 51.60 30.86 31.23 Effect of Tool Numbers. Figure 5 shows the impact of tool numbers on tool selection accuracy. As the number of tools increases, both GPT-3.5 and LLaMA3-70B experience notable declines in performance, with LLaMA3-70B In contrast, MTUshowing sharper drop. LLaMA maintains relatively stable accuracy, demonstrating its superior handling of multiple tool calls. 48.56 GPT-4 LLaMA3-8B 14.03 MTU-LLaMA 51. 44.45 11.71 45.66 48.56 14.03 51.80 45.56 13.86 48.58 44.59 11.94 45.57 38.32 7.74 38.10 41.10 8.63 38. Models SATS TPR Avg. ATS SR TS PS 44.90 26.72 30.87 39.74 22.66 28.51 API-Bank 6.90 0.00 3.45 27.13 15.44 22. 36.89 20.95 24.52 Error Analysis. In Figure 7, we use five LLMs to analyze the different types of errors (i.e., Action Error, Parameter Error and Format Error). Specifically, the Action Error and Parameter Error denotes to select wrong tools and wrong parameters, respectively, and the Format Error means that the model cannot follow instructions well and outputs wrong formats, which cannot be resolved well. Figure 7 illustrates two primary findings: (1) Action Error occurs more often than other errors, specifically in challenge M-M setting, and stronger models show fewer errors. (2) Format Error usually exists in weaker LLMs (e.g., LLaMA38B-Instruct and ChatGLM4-9B), and our fine-tuned version MTU-LLaMA greatly reduces format issues, which shows the effectiveness of MTU-Instruct. See Appendix for detailed error cases. Figure 6: Scaling Law of LLMs on MTU-Eval. Scaling Law We evaluate the performance of MTU-LLaMA across different model sizes, using LLaMA2 models with 7B, 13B, and 70B parameters as initialization, which are fine-tuned with 9 Preprint Figure 7: Analysis of different error types for different LLMs under M-S and M-M settings. MTU-Instruct. The results in Figure 6 show that the performance of MTU-LLaMA improves as the model size increases, suggesting its scalability. Consistency Between Our Proposed Metrics and Human Evaluation. To show the validity of our proposed metrics (SATS, TN, and TO), we evaluate the consistency between these novel metrics and the human evaluation results. We randomly sample 50 instances from the M-S subset for the SATS metric and 50 instances from the S-M subset for the TN and TO metrics to compare two models: GPT-3.5 and LLaMA3-8B. Human annotators are also asked to compare the two models. In Table 6, we report the Pearson correlation coefficient between our metrics and human evaluation results, as well as the Pearson correlation among human annotators, which shows excellent consistency between these metrics and human evaluations. More details can be found in Appendix E."
        },
        {
            "title": "4 RELATED WORKS",
            "content": "Metric Annotation Count Table 6: Consistency for our proposed metrics. Instruction Tuning for Tool Learning. The objective of tool learning is to equip large language models (LLMs) with human-like tool usage capabilities (Yang et al., 2023; Shen et al., 2023; Qu et al., 2024). To achieve this, LLMs are typically fine-tuned with tool instruction data to improve their performance in tool planning, selection, calling, and response generation (Schick et al., 2023; Liang et al., 2023; Kong et al., 2023). However, existing tool instruction datasets either have limitations in multi-turn dialogue and multi-tool usage scenarios, or are based on synthetic data, resulting in misalignment with real-world user needs (Huang et al., 2024; Li et al., 2023; Patil et al., 2023; Zhuang et al., 2023). In this paper, we introduce MTU-Instruct, large-scale instruction dataset to improve LLMs performance in diverse real-world tool-use scenarios. Consistency Metric-Human Human-Human 0.9245 0. 0.9482 0.8497 0.9344 0.8280 SATS TN TO 50 50 50 Evaluation Benchmarks for Tool Learning. Many tool-use benchmarks have been proposed, but they still have many limitations. Firstly, in Table 1, these benchmarks have limited capabilities to assess complex scenarios (e.g., multi-turn dialogues, multiple tools, and cross-domain tool generalization) (Huang et al., 2024; Li et al., 2023; Patil et al., 2023; Tang et al., 2023). Secondly, some benchmarks rely excessively on GPT models, potentially leading to subjective and unstable results with heavy costs (Qin et al., 2023b; Tang et al., 2023). Finally, existing assessments often overlook critical dimensions, such as the order of multi-tool invocation, the impact of erroneous calls on subsequent interactions, and the accuracy of tool parameter selection, resulting in evaluations lacking comprehensiveness and depth (Zhuang et al., 2023; Li et al., 2023; Patil et al., 2023). In contrast, MTU-Bench not only includes extensive multi-turn dialogues and multiple tool scenarios but also introduces testing for OOD tool generalization. By employing automated evaluation and incorporating metrics like SATS, TN, and TO, MTU-Eval achieves more comprehensive assessment."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose multi-granularity tool-use benchmark for LLMs called MTU-Bench, which consists of MTU-Instruct and MTU-Eval. Specifically, first, the MTU-Instruct dataset is 10 Preprint used to enhance the tool-use abilities of existing LLMs, and the MTU-Eval with multiple tooluse scenes is applied to benchmark the tool-use abilities comprehensively. Notably, all evaluation metrics of our MTU-Eval are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Finally, we hope MTU-Bench can guide developers and researchers in understanding the tool-use capabilities of LLMs and facilitate the growth of foundation models."
        },
        {
            "title": "ETHICS",
            "content": "In developing MTU-Bench and MTU-LLaMA, we recognize several ethical considerations that arise from the broader context of integrating tool-use capabilities into large language models (LLMs). As these models become more capable of interacting with real-world systemssuch as those involving financial services, healthcare, and other critical domainswe must consider the potential risks associated with misuse. For instance, there is the possibility that LLMs could be exploited to access sensitive tools or manipulate information in ways that could harm individuals or organizations. While our work aims to improve the accuracy and efficiency of tool use, we are mindful of the importance of ensuring that these technologies are deployed responsibly. We advocate for the implementation of robust safeguards, including transparency in decision-making processes, fairness in how tools are applied, and accountability in real-world usage. Furthermore, we encourage future research and development efforts to focus on mitigating potential biases and ensuring that these systems are secure and trustworthy when handling sensitive tasks."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. Peter Lee, Yin Tat Lee, Yuanzhi Li, Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Eric Scott Lundberg, Sparks March 2023. https://www.microsoft.com/en-us/research/publication/ Horvitz, Ece Kamar, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, intelligence: of artificial general URL sparks-of-artificial-general-intelligence-early-experiments-/ with-gpt-4/. Early experiments with gpt-4. Johannes Gehrke, and Yi Zhang. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašic. Multiwoz - large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv: 1810.00278, 2018. Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy Cedilnik, and Kyu-Young Kim. Taskmaster-1: Toward realistic and diverse dialog dataset. arXiv preprint arXiv: 1909.05358, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv: Arxiv-2110.14168, 2021. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320335, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston 11 Preprint Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender 12 Preprint A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. Nicholas Farn and Richard Shin. Tooltalk: Evaluating tool-usage in conversation setting. arXiv preprint arXiv:2311.10775, 2023. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. C. T. Hemphill, J. J. Godfrey, and G. Doddington. The atis spoken language systems pilot corpus. In HLT, 1990. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. Metatool benchmark for large language models: Deciding whether to use tools and which to use. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=R0c2qtalgG. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, et al. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems. arXiv preprint arXiv:2311.11315, 2023. Harrison Lee, Raghav Gupta, Abhinav Rastogi, Yuan Cao, Bin Zhang, and Yonghui Wu. Sgd-x: benchmark for robust generalization in schema-guided dialogue systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 1093810946, 2022. 13 Preprint Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: benchmark for tool-augmented llms, 2023. Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint arXiv: Arxiv2303.16434, 2023. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations, 2023. OpenAI. Gpt-4 technical report. PREPRINT, 2023. Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv: Arxiv-2303.09014, 2023. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv: 2305.15334, 2023. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Y. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Jun-Han Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ARXIV.ORG, 2023a. doi: 10.48550/arXiv.2304.08354. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023b. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and JiRong Wen. Tool learning with large language models: survey. arXiv preprint arXiv:2405.17935, 2024. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 86898696, 2020a. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 86898696, 2020b. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761, 2023. doi: 10.48550/arXiv.2302.04761. URL https: //doi.org/10.48550/arXiv.2302.04761. Igor Shalyminov, Alessandro Sordoni, Adam Atkinson, and Hannes Schulz. Fast domain adaptation In ICASSP 2020for goal-oriented dialogue using hybrid generative-retrieval transformer. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 80398043. IEEE, 2020. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv: Arxiv-2303.17580, 2023. Aditya Siddhant, Anuj Goyal, and A. Metallinou. Unsupervised transfer learning for spoken language understanding in intelligent agents. AAAI Conference on Artificial Intelligence, 2018. doi: 10.1609/AAAI.V33I01.33014959. URL https://arxiv.org/abs/1811.05370v1. 14 Preprint Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. Qwen Team. Introducing qwen1.5, February 2024. URL https://qwenlm.github.io/ blog/qwen1.5/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. Interactive natural language processing. arXiv preprint arXiv: 2305.13246, 2023. URL https://arxiv.org/abs/2305.13246v1. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv: 2305.16504, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: synergizing reasoning and acting in language models (2022). arXiv preprint arXiv:2210.03629, 2023. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for llm question answering with external tools. arXiv preprint arXiv:2306.13304, 2023. Preprint"
        },
        {
            "title": "A LIMITATIONS",
            "content": "Despite the strengths of our proposed MTU-Bench, there are still several limitations to consider. First, although MTU-Bench incorporates diverse range of real-world user instructions, it may not fully capture all potential edge cases or highly complex interactions that can occur in dynamic, real-world environments. Second, while MTU-Eval provides comprehensive fine-grained metrics for evaluating tool-use abilities, these metrics are based on predefined benchmarks, which may not account for every possible tool-use challenge in evolving real-world applications. Although MTU-LLaMA demonstrates strong generalization across various metrics and scenarios, further research is needed to explore its adaptability to increasingly complex and emerging tool functionalities in real-time settings. Future work should focus on broadening the benchmarks coverage and exploring more dynamic and complex real-world use cases. DETAILS OF MTU-BENCH CONSTRUCTION Prompt Templates for MTU-Bench Construction. The construction procedures of MTU-Bench involves numerous prompt templates, as listed below, for tool making (tool name synthesis and tool parameter synthesis), thought synthesis, observation simulation, and data quality check. Prompt Template (Tool Name Synthesis) will give you dialogue, including historical dialogue information and current round dialogue information. Please help me determine whether Assistant needs to call an \"API\" to obtain specific information or perform certain operations in order to solve the user issue in the current round of conversation. 1. If yes, and Assistants response shows that the users query has been solved, then return me with \"api_name\" in \"tags\". The name of the API should be concise and easy to understand, such as search_restaurant, book_restaurant, etc. The API name should start with verb and include the specific domain name. Related domains includes [domain_info ], etc. Note that if multiple APIs need to be called, return me with \"api_name, api_name, ...\" in \"tags\". 2. If Assistant can provide the current response without calling the API, return me \"no need to call\"in \"tags\". 3. If Assistant needs to call an API but is unable to do so due to lack of necessary information, inform me of \"lack of necessary information\" in \"tags\". For example, due to the lack of \"restaurant name\", the API for \"book_restaurant\" cannot be called. 4. If Assistant is only confirming existing information with the User, inform me of the \" confirmation information\" in \"tags\". 6. If some information in the historical conversation can help Assistant respond to the current issue without calling APIs, inform me that \"information already exists\" in \"tags\". Please reply to me in JSON format: {\"Analysis\": str, \"tags\": str}. Preprint Prompt Template (Tool Parameter Synthesis) will provide you with conversation segment that includes both historical dialogue information and current round dialogue information. Based on this information, please help me determine whether, to solve the problem presented by the User in the current round of dialogue, the Assistant calls specific API to obtain the necessary information or to perform related actions. Please respond according to the following guidelines: 1. If an API call is required and the Assistants response solves the Users problem, please specify which API was called and reply to me in the following JSON format: \"Action\": \" api_name\". Also, provide the parameters required for calling that API in the format: \" Action Input\": {\"parameter_name\": \"value\", ...}\". 2. If it is impossible to call an API due to missing necessary parameter information, please explain in the \"Thought\" section due to the absence of which parameters, which API cannot be called. 3. If answering the Users question does not require calling an API, please explain in the \" Thought\" section why there is no need to use API. 4. Please strictly use the API names and parameter names provide, and refrain from fabricating any. If the required parameter is not defined within our list, you are allowed to introduce new parameter names. Beyond this allowance, do not utilize any API names and parameter names that are beyond the specified range, to ensure consistency and accuracy. 5. Please include section called \"Thought\" in your answer where you clearly and unambiguously demonstrate your thought process when solving or answering the question. Below are the APIs and their parameters information you can use: [apis_information] Please reply in the following JSON format: {\"Thought\": \"str\", \"Action\": \"api_name\", \"Action Input\": {\"parameter_name\": \"value\", ...}}. If there is no information for \" Action\", \"Action Input\", or \"Thought\", please return an empty string. 17 Preprint Prompt Template (Thought Synthesis) will provide you with multiround conversation and API call information for each round. Your task is to add or verify the Assistants thinking process of API call to the dialog. Your need to return the modified conversation with API call results added to me, strictly following the following format: nModified Dialogue:n[Round 0]nUser: xxx.nThought: xxx.nAction: xxx.nAction Input : xxx.nObservation: xxx.nAssistant: xxxx. n[Round 1]nUser: xxx.nThought: xxx. nAssistant: xxx.n[Round 2]... That is to say, after each round of the users question, the Assistant must think about whether to call the API and how to call it, and present this in \"Thought\". For example: Modified Dialogue: [Round 0] User: Find me some fantasy movies can watch online, something directed by Neil Marshall. Thought: The user is looking for fantasy movies directed by Neil Marshall to watch online. need to query database or service for movies that fit these criteria. Action: FindMovies Action Input: {directed_by: Neil Marshall, genre: Fantasy} Observation: {directed_by: Neil Marshall, genre: Fantasy, subtitles: False, title: Hellboy} Assistant: Are you interested in Hellboy? [Round 1] User: would enjoy watching Hellboy. Thought: The user has not expressed any request, so the tool is not invoked here. Assistant: Shall play the movie? [Round 2] User: Yes, begin playing the movie. Yes, play it without subtitles. Thought: The user has confirmed they want to start watching the movie \"Hellboy\" and specifically requested to play it without subtitles. Ill proceed to execute the play command with subtitles disabled. Action: PlayMovie Action Input: {subtitles: False, title: Hellboy} Observation: {directed_by: Neil Marshall, genre: Fantasy, subtitles: False, title: Hellboy} Assistant: The movie is now playing. Requirements: 1. After \"Action\", it shows which API to call. If not, it indicates that API calls are not required for this round. The parameters passed in by calling the API are displayed after \" Action Input\". You just need to supplement the API call return results for the rounds that have called the API, and place them after \"Observation\". \"Observation\" shows the return result of the API. 2. Please return the thinking process in \"Thought\". 5. will provide you with reference APIs and parameter names. 6. Except for adding \"Thought\" to the round of API calls, all other information remains in its original content. 7. Please return the complete conversation. The following is list of APIs and their parameters for your reference: [apis information] 18 Preprint Prompt Template (Observation Simulation) will provide you with multiround conversation and API call information for each round. Your task is to add the return result of an API call to the dialog based on Assistants response. You need to return the modified conversation with API call results added to me, strictly following the following format: nModified Dialogue:n[Round 0]nUser: xxx.nThought: xxx.nAction: xxx.nAction Input : xxx.nObservation: xxx.nAssistant: xxxx. n[Round 1]nUser: xxx.nThought: xxx. nAssistant: xxx.n[Round 2]... For example: Modified Dialogue: [Round 0] User: Find me some fantasy movies can watch online, something directed by Neil Marshall. Thought: The user is looking for fantasy movies directed by Neil Marshall to watch online. need to query database or service for movies that fit these criteria. Action: FindMovies Action Input: {directed_by: Neil Marshall, genre: Fantasy} Observation: {directed_by: Neil Marshall, genre: Fantasy, subtitles: False, title: Hellboy} Assistant: Are you interested in Hellboy? [Round 1] User: would enjoy watching Hellboy. Assistant: Shall play the movie? [Round 2] User: Yes, begin playing the movie. Yes, play it without subtitles. Thought: The user has confirmed they want to start watching the movie \"Hellboy\" and specifically requested to play it without subtitles. Ill proceed to execute the play command with subtitles disabled. Action: PlayMovie Action Input: {subtitles: False, title: Hellboy} Observation: {directed_by: Neil Marshall, genre: Fantasy, subtitles: False, title: Hellboy} Assistant: The movie is now playing. Requirements: 1. After \"Action\", it shows which API to call. If not, it indicates that API calls are not required for this round. The parameters passed in by calling the API are displayed after \" Action Input\". You just need to supplement the API call return results for the rounds that have called the API, and place them after \"Observation\". 2. Please return the information in \"Observation\" in JSON format, for example: {\" parameter_name\": \"value\", \"parametername\": \"value\"...}. Specifically, in every round with \"Action\" and \"Action Input\", you should add an \"Observation\" after the \"Action Input\", which should fill in the information returned by the API. For example: nAction: FindMovies nAction Input: {directed_by: Neil Marshall, genre: Fantasy} nObservation: {directed_by: Neil Marshall, genre: Fantasy, subtitles: False, title : Hellboy}. 3. If Assistants response shows that it has not yet received the specific information returned by the API tool call, that is \"nObservation\": {\"error\": \"Time out.\"} 4. If the result returned by Assistant shows that no relevant information is found, the API call returns an empty dict, such as: \"nObservation\": {}. 5. will provide you with reference APIs and parameter names. Ensure that all parameter names used are defined and should not be fabricated. 6. Except for adding API return results to the round of API calls, all other information remains in its original content. 7. Please return the complete conversation. The following is list of APIs and their parameters for your reference: [apis information] 19 Preprint Prompt Template (Data Quality Check) Please review the provided conversation snippet, which includes historical dialogue, current round dialogue, and the API call made in this round. Your task is to verify the accuracy of the API and parameters used in this round of dialogue. Assume the Assistant does not have knowledge of realworld information such as cinemas or restaurants; it relies on API calls to access information or carry out actions such as making reservation. Use the guidelines below to correct any inaccuracies. 1. Check and correct the API selection in the \"Action\" field. Common errors include: a. Assistants response indicates that an API call was made, but the \"Action\" field is empty. b. Assistant dont need to call any API to reply to the users current round of conversation. In this case, calling is not necessary, but there is an API name in the \"Action\" field. c. The assistants response shows that the necessary information required for API calls is missing. Therefore, the assistant is asking the user for additional information, but there is an API name in the \"Action\". d. The API listed in the \"Action\" is incorrect. 2. Verify and correct the parameters listed in \"Action Input\" and ensure they correctly match the API call. The \"Action Input\" should be formatted as {\"parameter_name\": \" value\", \"parameter_name\": \"value\", ...}. 3. Revise the content in \"Thought\" to include the correct rationale for selecting tools and parameters. The thought should only consider historical conversations and current user issues, assuming that the assistants response is unknown. 4. Ensure all API and parameter names used are as defined and should not be invented. Here is list of APIs and their parameters for your reference: [apis_information] Please respond in the following JSON format: {\"Thought\": str, \"Action\": \"api_name\", \"Action Input\": {\"parameter_name\": \"value\", ...}}. If there is no \"Action\" or \"Action Input\" information, please return an empty string. Example of Tool Document. As shown in Figure 8, the tool document allows the model to determine the appropriate tool names and their usages. It contains all the tools we synthesized, each tool including its corresponding tool description, necessary parameters, optional parameters, parameter description and data type, as well as the returns. MORE MTU-BENCH DATA ANALYSIS. Length Distribution. We illustrate the length distributions in Figure 9, and Figure 10 for the training and evaluation data, respectively. Table 7: The number of dialogues under different settings. More Statistics. Table 7 provides comprehensive summary of the dialogue statistics across diverse settings, distinct splits and subsets. DETAILS OF MTU-EVAL Hard cases in the hard test set encompass extensive parameters, nonsensical tool names, determination of specific parameter values, inability to call tools, interaction among multiple tools, and multi-turn parameter inheritance, as delineated in Table 8. Setting Train Test normal hard S-S 14277 S-M M-S 19007 M-M 7442 OOD - 39 31 37 52 55 42 65 Prompt Templates for MTU-Eval. During the evaluation, the models are provided with distinct system prompts for various settings, which encompass both the comprehensive task specifications and the tool documentation. We list the system prompts used for evaluation in Boxes D. 20 Preprint Figure 8: The JSON structure of the tool setAlarm, which is used to create new alarm based on user-specified parameters. The structure includes required, optional and result (i.e., return) parameters, along with their corresponding data types, descriptions, formats and default values. 21 Preprint Figure 9: Length distributions of the training data. Figure 10: Length distributions of the evaluation data. 22 Preprint Table 8: Hard cases in the hard test set. Type Description Case Extensive Parameters Nonsensical Tool Names The quantity of parameters required to specified be exceeds six. The designation of the tool lacks significance, for instance, abc. Determination of Specific Parameter Values Parameter values must conform to specific criteria. User: Can you book me non-refundable, one-stop flight on Delta Airlines from New York to Chicago, leaving on the 5th and returning on the 11th of this month in Economy class? Target: Action: ReserveRoundtripFlights Action Input: {airlines: Delta Airlines, departure_date: 2024-01-05, destination_city: Chicago, origin_city: New York, return_date: 2024-01-11, seating_class: Economy, number_stops: 1, refundable: False} System Prompt: ...The following is list of APIs and their parameters that you can use: { name: eee , description: Book an appointment at dentist for given time and date, required_parameters: [dentist_name, appointment_time, appointment_date]... User: would like to book an appointment with Yvonne Yang at Greenview Dental Care on the 11th at 11:45. Do they offer cosmetic services? Target: Action: eee Action Input: {appointment_date: 2024-01-11, appointment_time: 11:45, dentist_name: Yvonne Yang} Tool Definition: name: searchHotel, description: To search for type, amenities, hotels based on set of criteria including rating, location, and price range., optional_parameters: [...name: priceRange, choices = [cheap, moderate, expensive] User: Can you help me find moderately priced hotel in the centre with parking and internet availability? Target: Action: SearchHotel Action Input: area: centre, parking: True, internet: True, priceRange: moderate , return_info: [all] Inability Call Tools to list of does contain suitable The tools not any options. System Prompt: You are ...The following is list of APIs and their parameters that you can use... Prompt: Can you find me drama movie with subtitles directed by Claire Denis? Target: Thought: The user wants to find drama movie with subtithere is no available API in tles directed by Claire Denis. However, the provided list Therefore, that supports the searching for movies. its not possible to meet the users request with the current tools. Interaction among Multiple Tools In single turn of dialogue, more than four tools are utilized. Multi-Turn Parameter Inheritance The parameters for the current turn must be extracted from the historical dialogue. User: am looking for the date of concert in New York, followed by the weather forecast for that day. And want to find dentist, book an appointment at 10:00 on Febrary 2nd. And please add the dentists phone to my contact list. Thank you! Target Action: FindEvents Action Input: {event_type: concert, city: New York} Action : GetWeather Action Input: {city: New York, date: FindEvents.date} Action: BookAppointment Action Input: {dentist_name: FindProvider.dentist_name, appointment_time: 10:00, appointment_date: 2024-02-02} Action: AddNewContact Action Input: {contact_name: FindProvider.dentist_name, phone_number: FindProvider.phone_number} User: Find some music please. Assistant: Action: LookupMusic Observation: {artist: Ariana Grande, song_name: Be My Baby} Assistant: Ok, how about Be My Baby by Ariana Grande? User: That sounds good. Assistant: Should play this now? User: Yes. Play on kitchen speaker please. Target: Action: PlayMedia Action Input: { playback_device: kitchen speaker, song_name: Be My Baby } 23 Preprint System Prompt (S-S) Please reply to the user based on their input and historical conversation information. You can choose to call external tools to implement it. Here are the call requirements and information about available APIs. 1. Please provide your thought process in \"Thought\", including user intent analysis, whether to call APIs, and how to call APIs. 2. When users request can be satisfied by calling an API, please provide the required calling information in the following format: nAction: The name of the API to be called. nAction Input: The parameter information required to call the API, in Json format. 3. If the users needs can be met without calling the API, then no API call action will be made. 4. If there is lack of mandatory information that makes it impossible to call specific API , then no API call action will be made. 5. The name and parameter name of the API must be consistent with the provided API information. The value of the parameter should be extracted from the context, and the information should not be fictional. 6. If none of the apis provided are available to meet the users requirements, no Action is taken. 7. If you need to call the API, your output format should be: Thought: xxxxnAction: xxxxnAction Input: xxxx For example: Thought: The user is looking for oneway flight for three people from Las Vegas to Atlanta on specific date, March 5th. Lets call the SearchOnewayFlight API to find the available flights.nAction: SearchOnewayFlightnAction Input: {\"origin_city\": \"Las Vegas\", \"destination_city\": \"Atlanta\", \"departure_date\": \"20240305\", \" seating_class\": \"Economy\"} If you dont need to call the API, your output format should be: Thought: xxxx The following is list of APIs and their parameters that you can use: [apis information] 24 Preprint System Prompt (S-M) Please reply to the user based on their input. You can choose to call external APIs to implement it. Here are the call requirements and information about available APIs. 1. Please provide your thought process in \"Thought\", including user intent analysis, whether to call APIs, and how to call APIs. 2. When users request can be satisfied by calling APIs, please provide the required calling information in the following format: nAction: The name of the APIs.nAction Input: the parameter information required to call APIs, in Json format. For example, nAction: \"api_name_A\"nAction Input: {\"parameter_name_A.1\": \"parameter_value_A .1\", ...}nAction: \"api_name_B\"nAction Input: {\"parameter_name_B.1\": \" parameter_value_B.2\", ...}, ... 3. There may be an interaction relationship between APIs, where the parameter value returned by the previous API call needs to be used as the parameter value for the next API call. Please use \"previous_API_name.return_parameter_name\" as the parameter value for the new API call. 4. Multiple APIs may need to be called to meet the users needs. Please pay attention to the order of APIs call. 5. There may be an interaction relationship between APIs, where the parameter value returned by the previous API call needs to be used as the parameter value for the next API call. Please use \"previous_API_name.return_parameter_name\" as the parameter value for the new API call. 6. The name and parameter name of the API must be consistent with the provided API information. The value of the parameter should be extracted from the context, and the information should not be fictional. 7. If you need to call the API, your output format should be: nThought: xxxxnAction: xxxxnAction Input: xxxxnAction: xxxxnAction Input:xxxx nAction: xxxxnAction Input: xxxx... If you dont need to call the API, your output format should be: nThought: xxxx The following is list of APIs and their parameters that you can use: [apis information] 25 Preprint System Prompt (M-S) Please reply to the user based on their input and historical conversation information. You can choose to call external tools to implement it. Here are the call requirements and information about available APIs. 1. Please provide your thought process in \"Thought\", including user intent analysis, whether to call APIs, and how to call APIs. 2. When users request can be satisfied by calling an API, please provide the required calling information in the following format: nAction: The name of the API to be called. nAction Input: The parameter information required to call the API, in Json format. 3. \"Observation\" is the information returned by API calls. 4. If the users needs can be met without calling the API, then no API call action will be made. 5. If there is lack of mandatory information that makes it impossible to call specific API , then no API call action will be made. 6. Note that if the dialogue history already contains the required information, there is no need to call the tool again. 7. The name and parameter name of the API must be consistent with the provided API information. The value of the parameter should be extracted from the context, and the information should not be fictional. 8. If you need to call the API, your output format should be: nThought: xxxxnAction: xxxxnAction Input: xxxx Therefore, if you dont need to call the API, your output format should be: nThought: xxxx The following is list of APIs and their parameters that you can use: [apis information] 26 Preprint System Prompt (M-M) Please reply to the user based on their input and history conversation. You can choose to call external APIs to implement it. Here are the call requirements and information about available APIs. 1. Please provide your thought process in \"Thought\", including user intent analysis, whether to call APIs, and how to call APIs. 2. When users request can be satisfied by calling APIs, please provide the required calling information in the following format: nAction: The name of the APIs.nAction Input: the parameter information required to call APIs, in Json format. For example, nAction: \"api_name_A\"nAction Input: {\"parameter_name_A.1\": \"parameter_value_A .1\", ...}nAction: \"api_name_B\"nAction Input: {\"parameter_name_B.1\": \" parameter_value_B.2\", ...}, ... 3. There may be an interaction relationship between APIs, where the parameter value returned by the previous API call needs to be used as the parameter value for the next API call. Please use \"previous_API_name.return_parameter_name\" as the parameter value for the new API call. 4. Multiple APIs may need to be called to meet the users needs. Please pay attention to the order of APIs call. 5. The name and parameter name of the API must be consistent with the provided API information. The value of the parameter should be extracted from the context, and the information should not be fictional. 6. If you need to call the API, your output format should be: Thought: xxxxnAction: xxxxnAction Input: xxxxnAction: xxxxnAction Input:xxxx nAction: xxxxnAction Input: xxxx... If you dont need to call the API, your output format should be: Thought: xxxx The following is list of APIs and their parameters that you can use: [apis information] 27 Preprint Examples for computing metrics. The examples are listed below: Tool Selection Accuracy (abbr. TS) (for S-S, M-S) Positive Example: - User: Search for the latest news on artificial intelligence. - Assistant: OK! Let me search the web. search_web(query=\"latest news on AI\") - Ground Truth Tool: search_web - Assistants Tool: search_web - Score: TS = 1 - Explanation: The assistant correctly selected the tool search_web, matching the ground truth. Negative Example: - User: Calculate the square root of 16. - Assistant: OK! Let me use the calculator. search_web(query=\"square root of 16\") - Ground Truth Tool: calculate - Assistants Tool: search_web - Score: TS = 0 - Explanation: The assistant incorrectly selected search_web instead of the correct tool calculate. Parameter Selection Accuracy (abbr. PS) (for S-S, M-S) Positive Example: - User: Translate good evening to German. - Assistant: Sure, translating now. translate(text=\"good evening\", target_language=\"German\") - Ground Truth Parameters: {text=good evening, target_language=German} - Assistants Parameters: {text=good evening, target_language=German} - Score: PS = 1 - Explanation: The assistant provided the correct parameters matching the ground truth. Negative Example: - User: Set an alarm for 6 AM tomorrow. - Assistant: Setting your alarm. set_alarm(time=\"6 PM\", name=\"tomorrow\") - Ground Truth Parameters: {time=6 AM, date=tomorrow} - Assistants Parameters: {time=6 PM, name=tomorrow} - Score: PS = 0 - Explanation: The assistant incorrectly set the value for the time parameter. The assistant incorrectly set the key for the date parameter. Success Rate (abbr. SR) (for M-S, M-M) Preprint Positive Example: - User: need to send an email to Alex. - Assistant: Sure, composing an email. compose_email(recipient=\"Alex\", subject=\"Hello\", body=\"Hi Alex, how are you?\") - User: Also, schedule meeting with him next Monday at 2 PM. - Assistant: Scheduling the meeting. schedule_meeting(participant=\"Alex\", date=\"next Monday\", time=\"2 PM\") - Ground Truth: No errors throughout all turns. - Score: SR = 1 - Explanation: The assistant correctly used the tools in all dialogue turns without any errors. Negative Example: - User: Find the fastest route to the airport. - Assistant: Finding the fastest route. get_directions(destination=\"airport\") - User: Book taxi to pick me up in 30 minutes. - Assistant: Booking taxi. book_taxi(pickup_time=\"in 3 minutes\") - Ground Truth: Assistant should have set the pickup time to in 30 minutes. - Score: SR = 0 - Explanation: The assistant made an error in the second turn (incorrect pickup time), so the entire dialogue is considered unsuccessful. Averaged Turn Success Rate (abbr. ATS) (for M-S, M-M) Positive Example: - Turn 1: - User: Whats on my schedule today? - Assistant: Checking your schedule. get_schedule(date=\"today\") - Turn Success: - Turn 2: - User: Add dentist appointment at 4 PM. - Assistant: Adding the appointment. add_event(event=\"dentist appointment\", time=\"4 PM\", date=\"today\") - Turn Success: 1 - Turn 3: - User: Remind me an hour before. - Assistant: Setting reminder. set_reminder(event=\"dentist appointment\", time=\"3 PM\", date=\"today\") - Turn Success: 1 - Score: ATS = (1 + 1 + 1)/3 = 1.0 - Explanation: All turns were executed correctly. Negative Example: Preprint - Turn 1: - User: Play my workout playlist. - Assistant: Playing your playlist. play_music(playlist=\"workout\") - Turn Success: 1 - Turn 2: - User: Skip to the next song. - Assistant: Restarting the song. play_music(action=\"restart\") - Turn Success: 0 (Incorrect action parameter) - Turn 3: - User: Increase the volume. - Assistant: Increasing volume. adjust_volume(level=\"up\") - Turn Success: 1 - Score: ATS = (1 + 0 + 1)/3 = 0.6667 - Explanation: One out of three turns was unsuccessful, leading to an ATS of approximately 0.6667. Soft Averaged Turn Success Rate (abbr. SATS) (for M-S, M-M) Positive Example: - Turn 1: - User: Whats the current time in Tokyo? - Assistant: The current time is... get_time(location=\"Tokyo\") 3:00 PM - Turn Success: 1 - Turn 2: - User: Schedule call with the Japan office at that time tomorrow. - Assistant: Scheduling the call. schedule_call(participant=\"Japan office\", time=\"3:00 PM\", date=\"tomorrow\") - Turn Success: 1 - Score: SATS = (1 + 1)/2 = 1.0 - Explanation: Both turns were successful, so the SATS is 1. Negative Example: - Turn 1: - User: Check the weather in New York. - Assistant: Checking the weather. get_weather(location=\"New York\") - Turn Success: 1 - Turn 2: - User: Schedule meeting tomorrow at 10 AM. - Assistant: Scheduling the meeting. schedule_meeting(date=\"tomorrow\", time=\"10 AM\") - Turn Success: 1 30 Preprint - Turn 3: - User: Set an alarm for 9 AM. - Assistant: Setting an alarm. set_alarm(time=\"9 PM\") - Turn Success: 0 (Incorrect time parameter) - Turn 4: - User: Remind me to call John at 11:00 AM. - Assistant: Setting reminder. set_reminder(event=\"call John\", time=\"11:00 AM\") - Turn Success: 1 - Turn 5: - User: Send an email to Sarah. - Assistant: Composing an email. compose_email(recipient=\"Sarah\", subject=\"Hello\", body=\"Hi Sarah, ...\") - Turn Success: 1 Calculations: - Last incorrect turn index = 3 - For = 1 and = 2: - Since < i, Score = 1 - For = 3: - Turn is incorrect, Score = 0 - For = 4: - > i, Score = 1 e(43) = 1 e1 0.6321 - For = 5: - > i, Score = 1 e(53) = 1 e2 0.8647 - Score: SATS = 1+1+0+0.6321+0.8647 3. 5 0.6994 - Explanation: The error in the third turn reduces the scores of subsequent turns due to the exponential decay, resulting in SATS of approximately 0.6994. 5 Task Process Rate (abbr. TPR) (for M-S, M-M) Positive Example: - Total Turns: = - First Incorrect Turn Index: No incorrect turns. - Score: TPR = = 1 - Explanation: Since there are no errors, the task was processed completely. Negative Example: - Total Turns: = 5 - First Incorrect Turn Index: = 3 (error occurs at turn 3) - Score: TPR = n = 2 5 = 0.4 31 Preprint - Explanation: The task process rate indicates the proportion of the task completed before the first error. Tool Number Accuracy (abbr. TN) (for S-M, M-M) Positive Example: - Ground Truth Tool List (GT): {search_web, summarize_text, translate_text} - Assistants Predicted Tool List (Pred): {search_web, summarize_text, translate_text} - Score: TN = PredGT PredGT = 3 3 = 1. - Explanation: All tools predicted by the assistant match the ground truth. Negative Example: - Ground Truth Tool List (GT): {search_web, translate_text} - Assistants Predicted Tool List (Pred): {search_web, play_music, set_alarm} - Score: TN = PredGT PredGT = 1 4 = 0. - Explanation: Only search_web is common between the predicted and ground truth lists, out of four unique tools. Tool Order Accuracy (abbr. TO) (for S-M, M-M) Positive Example: - Ground Truth Tool Sequence (GT): [search_web, extract_data, generate_report] - Assistants Predicted Tool Sequence (Pred): [search_web, extract_data, generate_report] - Longest Common Subsequence (LCS): [search_web, extract_data, generate_report] - Starting Position (i): 1 - Total Tools in Pred: Pred = 3 - Decay Coefficient (t): = cos (cid:16) π 2 Pred (cid:17) = cos (cid:0) π 2 1 (cid:1) 0.8660 - Score: TO = LCS GT = 0.8660 3 3 = 0.8660 1 = 0.8660 - Explanation: The assistants sequence perfectly matches the ground truth, resulting in high TO score. Negative Example: - Ground Truth Tool Sequence (GT): [get_weather, plan_route, book_hotel] - Assistants Predicted Tool Sequence (Pred): [book_hotel, plan_route, get_weather] - Longest Common Subsequence (LCS): [plan_route] - Starting Position (i): 2 - Total Tools in Pred: Pred = 3 - Decay Coefficient (t): Preprint = cos (cid:16) π 2 Pred (cid:17) = cos (cid:0) π 2 2 3 (cid:1) 0.5000 - Score: TO = LCS GT = 0.5000 3 0.1667 - Explanation: Only one tool matches in sequence, and it starts at the second position, leading to low TO score. 33 Preprint Table 9: Results of different models on the hard set of MTU-Eval (S-S & M-S). S-S and MS denote Single-Turn Single-Tool and Multi-Turn Single-Tool settings, respectively. All the baselines are instruction-tuned models. Avg. TS PS ATS SATS SR TPR Avg. M-S Models GPT-4 GPT-3.5 Qwen-Max GLM-4-Plus DeepSeek V2.5 LLaMA2-7B LLaMA2-70B LLaMA3-8B LLaMA3-70B Qwen1.5-14B Qwen1.5-72B Qwen2-7B Qwen2-72B Mistral-7B ChatGLM3-6B GLM-4-9B S-S PS 67.31 33.93 67.31 73.08 71.15 25.00 25.00 17.86 60.71 44.64 55.77 50.00 57.86 17. 4.55 34.62 TS 88.46 50.00 88.46 92.31 90.38 32.14 32.14 33.93 82.14 44.64 57. 67.86 78.93 36.54 13.64 59.62 Closed-Source Large Language Models 77.88 41.96 77.88 82.69 80.77 85.47 71.51 80.45 84.92 83. 62.01 43.02 56.98 63.69 58.10 55.77 39.45 50.54 58.40 56.10 Open-Source Large Language Models 28.57 28.57 25.89 71.43 44.64 56. 58.93 68.40 26.92 9.09 47.12 34.64 52.51 37.99 75.42 45.81 48. 64.80 78.77 63.69 21.44 64.80 26.82 30.17 13.97 52.51 45.81 48. 36.31 50.84 34.64 5.21 37.99 22.32 28.45 11.64 51.68 40.28 40. 32.90 47.19 31.29 4.13 36.56 46.57 29.33 41.77 48.79 47.28 16.32 21.62 8.15 42. 32.05 31.62 24.91 37.20 23.90 2.61 29.42 ToolLLaMA2-7B MTU-LLaMA(ours) 32.69 51. 3.85 28.57 18.27 40.18 25.70 78.21 3.35 59.22 3.50 57.35 2.60 46. Tool-Use-Specific Large Language Models 0.00 0.00 3.23 6.45 3.23 0.00 0.00 0.00 3.23 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 17.80 1.83 19.67 23.44 21.66 2.70 8.03 5.38 17. 12.40 11.57 13.48 16.50 2.74 0.00 12.52 44.61 30.86 42.11 47.61 44.94 17.13 23. 12.85 40.40 29.39 29.92 28.73 38.42 26.04 5.57 30.22 0.65 16. 10.19 43."
        },
        {
            "title": "E MORE EXPERIMENTAL ANALYSIS",
            "content": "Detailed Evaluation Results on Hard Set. In Table 9 and Table 10, we present the performance of various LLMs evaluated on the hard subset of MTU-Eval. Our analysis indicates that among closed-source models, GLM-4-Plus excels in handling single-tool scenarios, surpassing even GPT-4. Conversely, Qwen-Max demonstrates superior performance in multi-turn and multi-tool scenarios, also outperforming GPT-4. In single-turn multi-tool scenarios, GPT-4 and DeepSeek V2.5 exhibit relatively better performance. Furthermore, there remains significant gap between open-source models and their closed-source counterparts. Among the open-source models, our MTU-LLaMA, Qwen2-72B, and LLaMA3-70B lead the performance metrics. Specifically, Qwen2-72B shows slight advantage in multi-tool scenarios, while LLaMA3-70B excels in single-tool scenarios. MTU-LLaMA, however, demonstrates balanced ability across almost all evaluated settings. Error Analysis. The error cases of GPT-4 results on S-S, M-S, and S-M are enumerated in Tables 11, 12, and 13, respectively. Additionally, comprehensive breakdown of error frequencies across different models and error categories is illustrated in Table 14. Details on the Human Evaluation. Five annotators with extensive expertise in large language models (LLM) are engaged to assess the prediction outcomes. The annotators assign win/lose scores where score of 1 represents victory for GPT-3.5, score of 0 indicates tie, and score of -1 signifies win for LLaMA3-8B. 34 Preprint Table 10: Results of different models on the hard set of MTU-Eval (S-M & M-M). S-M and M-M denote Single-Turn Multi-Tool and Multi-Turn Multi-Tool settings, respectively. All the baselines are instruction-tuned models. Models GPT-4 GPT-3.5 Qwen-Max GLM-4-Plus DeepSeek V2.5 LLaMA2-7B LLaMA2-70B LLaMA3-8B LLaMA3-70B Qwen1.5-14B Qwen1.5-72B Qwen2-7B Qwen2-72B Mistral-7B ChatGLM3-6B GLM-4-9B TN 57.77 17.75 23.75 32.43 40.06 2.14 1.39 9.25 19. 12.23 19.27 15.24 35.12 10.18 4.58 20.00 S-M TO Avg. TN TO ATS SATS SR TPR Avg. M-M Closed-Source Large Language Models 58.37 19.04 24.28 29.36 39.96 2.56 2. 10.57 21.64 13.39 18.42 16.37 36.13 12.79 6.57 17.96 58.07 18.39 24.01 30.90 40. 57.08 21.31 57.04 52.74 43.51 58.31 22.39 58.56 53.74 45.21 49.28 15.09 49.96 47.03 35.45 Open-Source Large Language Models 2.35 1.74 9.91 20. 12.81 18.85 17.50 37.14 11.48 5.58 18.98 15.14 20.23 11.45 31. 15.14 26.92 16.03 35.05 17.05 16.54 17.43 15.27 20.74 12.60 33. 16.92 27.07 16.41 36.83 17.81 16.67 17.94 15.00 20.54 6.26 24. 11.98 21.71 13.92 30.72 13.38 13.92 12.75 Tool-Use-Specific Large Language Models 45.44 10.41 49.13 43.11 32. 13.69 20.04 4.33 20.63 8.82 18.61 10.37 27.52 10.69 12.32 8. 0.00 0.00 8.11 5.41 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 38.06 2.03 47.70 35.18 27.07 11.44 19.19 0.68 13.02 3.38 13.29 4.28 20. 6.08 9.23 0.68 41.36 11.87 45.08 39.53 30.62 11.76 16.79 5.89 20.56 9.37 17. 10.17 25.13 10.84 11.45 9.52 ToolLLaMA2-7B MTU-LLaMA(ours) 0.51 39.77 0.51 38. 0.51 39.31 3.82 35.90 3.82 36.67 3.15 34.14 2.32 27.37 0.00 0. 0.90 14.14 2.34 24.70 Type Description Case Table 11: Error cases of GPT-4 results on S-S. Tool Selection The model incorrectly selects an unintended tool. Paramete Omission The model omits nece ssary parameters in the results. Parameter Misalignment The model did not populate the parameters according to the specified criteria. User: Please book 5 economy bus tickets from Sacramento to San Diego forthe 10th of this month. Predict: Action: FindBus Action Input: {departure_date : 2024-01-10, ... Target: Action: BuyBusTicket Action Input: {departure_date : 2024-01-10, ... User: need to find an affordable round trip flight from Washington to Mexico for 4 people, departing on the 10th and returning on the 13th of this month. Predict: Action: SearchRoundtripFlights Action Input: {origin_city : washington, destination_city:mexico, departure_date: 2024-01-10, return_date: 2024-01-13, passengers : 4} Target: Action: SearchRoundtripFlights Action Input: {origin_city : washington, destination_city:mexico, departure_date: 2024-01-10, return_date: 2024-01-13, passengers : 4, seating_class : economy } User: need to find round trip flight to NYC from London, leaving day after tomorrow and returning on the 9th, in Economy class. Predict: Action: SearchRoundtripFlights Action Input: {departure_date : 2024-01-06, return_date : 2024-01-09, seating_class : Economy, origin_airport : London, destination_airport : nyc } Target: Action: SearchRoundtripFlights Action Input: {departure_date : 2024-01-06, return_date : 2024-01-09, seating_class : Economy, origin_airport : London, destination_airport : New York } 35 Preprint Table 12: Error cases of GPT-4 results on M-S. Type Description Case Repeated Call When the information already exists in the previous conversation history, the tool should not be called repeatedly. Parameter Hallucination The model incorrectly assumes parameter without sufficient context. Parameter Inheritance The model should inherit the information parameters from the conversation history and update the current state in timely manner. User: Milton Country Park Assistant: ...Action: SearchAttraction... Observation: {name: Milton Country Park, address: milton county park, milton, type: park, entrance_fee: free, phone: 765-987554} Assistant: Its free park on the north side of town. User: Could get the phone number for that? Predict: Thought: The user wants the phone about Milton Country Park, so need to use the searchAttraction API to get the details. Action: SearchAttraction Action Input: ... Target: Thought: The phone number has be received in the history, so there is no need to call any tool. The phone number is 765987554. ...(Dialogue history)... User: ... need bus for two people that would be leaving Portland on the 5th. ...(Dialogue history)... Assistant: ...Action: FindBus... Observation: {fare: 39, from_location: Portland, from_station: Portland Bus Station, leaving_date: 2024-01-05, leaving_time: 10:20, to_location: Vancouver, to_station:Pacific Central Station, transfers: 0, travelers: 2} User: want to reserve the hotel room. Assistant: ...You would like to check in on 5th? User: want to check in on the 8th. Predict: Thought: The user has provided check-in date, which, combined with information from past queries (hotel name and destination), enables the ReserveHotel API to reserve hotel for the user. Action: ReserveHotel Action Input: {hotel_name: Atrium Inn Vancouver, check_in_date:2024-01-08, number_of_days: 1 , destination: Vancouver} Target: Thought: specific check-in date has been provided, but the duration of the stay is still unknown. For how many days? User: Is Dumbo playing today in Santa Rosa? Assistant: ...Action: GetTimesForMovie... User: Could you get me taxi there? How long is that? Observation: {genre: Fantasy, location: Santa Rosa, movie_name: Dumbo, price: 9,show_date: 2024-01-04, show_time: 22:00, show_type: 3d, street_address: 409 Aviation Boulevard, theater_name: Airport Stadium 12} Predict: Thought: The user want to get taxi to go to the movie theater. Action: GetRide Action Input: {destination: theater , number_of_riders: 1} Target: Thought: The user want to get taxi to go to the movie which is at 409 Aviation Boulevard. can call GetRide. Action: GetRide Action Input: {destination: 409 Aviation Boulevard , number_of_riders: 1} Table 13: Error cases of GPT-4 results on S-M. Type Description Case Call Fewer Tools The model fails to call all the necessary tools. Call More Tools The model calls additional unnecessary tools. Call Wrong Tool The model selects an inappropriate tool for the task, potentially leading to incorrect results. User: need to set up an appointment with dentist in Los Angeles who offers cosmetic services on June 15th at 2:00 pm. Afterwards, set me reminder two hours before the appointment named Dentist Appointment. Predict: Action: FindProvider... Action: SetAppointmentReminder... Target: Action: FindProvider... Action: BookAppointment ... Action: SetAppointmentReminder... User: Hi, can you help me find song of 2019. And modify it as my 3pm alarms sound? Predict: Action: LookupSong... Action: GetAlarm ... Action: ChangeAlarmSound... Target: Action: LookupSong... Action: ChangeAlarmSound... User: Im looking for pop music to play and set as my alarm sound for my 3pm alarm today. Can you help me with that? Predict: Action: LookupSong... Action: SetAlarm ... Target: Action: LookupSong... Action: ChangeAlarmSound 36 Preprint Table 14: Error Frequencies across Various Models and Error Types. Bold number signifies the most critical error type for each model. Underlined number represents the most critical model for each error type. Models Action Error Missed Excessive Incorrect SUM Parameter Error Format Error SUM GPT4 LLaMA3-70B-Instruct ChatGLM4-9B LLaMA3-8B-Instruct MTU-LLaMA(Ours) GPT4 LLaMA3-70B-Instruct ChatGLM4-9B LLaMA3-8B-Instruct MTU-LLaMA(Ours) GPT4 LLaMA3-70B-Instruct ChatGLM4-9B LLaMA3-8B-Instruct MTU-LLaMA(Ours) GPT4 LLaMA3-70B-Instruct ChatGLM4-9B LLaMA3-8B-Instruct MTU-LLaMA(Ours) SUM 0 2 16 9 8 8 9 28 8 25 69 111 91 56 11 8 28 2 14 511 10 9 4 3 6 6 9 2 25 4 18 22 9 24 38 114 145 207 103 782 S-S 3 5 8 10 10 S-M 4 13 10 11 4 3 9 17 3 M-S M-M 4 7 10 4 3 13 16 28 22 24 18 31 40 44 47 94 129 132 83 53 129 183 213 120 26 27 35 38 26 36 56 52 22 49 96 92 112 113 73 44 62 38 6 4 1 1 12 0 0 4 1 28 0 14 11 40 101 30 0 5 15 46 1 43 44 64 72 50 54 91 93 94 157 197 281 346 186 97 196 236 265 149 143 1436 1031"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "University of Chinese Academy of Sciences",
        "University of Waterloo"
    ]
}