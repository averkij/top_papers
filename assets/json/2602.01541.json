{
    "paper_title": "Toward Cognitive Supersensing in Multimodal Large Language Model",
    "authors": [
        "Boyi Li",
        "Yifan Shen",
        "Yuanzhe Liu",
        "Yifan Xu",
        "Jiateng Liu",
        "Xinzhuo Li",
        "Zhengyuan Li",
        "Jingyuan Zhu",
        "Yunhan Zhong",
        "Fangzhou Lan",
        "Jianguo Cao",
        "James M. Rehg",
        "Heng Ji",
        "Ismini Lourentzou",
        "Xu Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 1 4 5 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Toward Cognitive Supersensing in Multimodal Large\nLanguage Model",
            "content": "Boyi Li1,, Yifan Shen1,,, Yuanzhe Liu1,, Yifan Xu1, Jiateng Liu1, Xinzhuo Li1, Zhengyuan Li1, Jingyuan Zhu2, Yunhan Zhong1, Fangzhou Lan2, Jianguo Cao2, James M. Rehg1, Heng Ji1, Ismini Lourentzou1,, Xu Cao1,2, 1University of Illinois Urbana-Champaign, 2PediaMed AI Equal contribution, Project lead, Corresponding author Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights. GitHub: https://github.com/PediaMedAI/Cognition-MLLM HuggingFace: https://huggingface.co/datasets/PediaMedAI/CogSense-Bench Correspondence to: xucao@pediamed.ai"
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have rapidly advanced open-vocabulary visual understanding, enabling strong performance in recognition, grounding, and many compositional perception tasks (Yang et al., 2025a). Yet, existing models can describe what is present but struggle to mentally operate on visual information, e.g., by explaining abstract layouts, simulating future transformations, or inferring visual rules in diagrams (Schulze Buschoff et al., 2025). These failures expose gap between high-level cognitive reasoning and low-level perception, and are becoming key bottlenecks in modern VQA and visual reasoning benchmarks (Lu et al., 2024; Cao et al., 2025). Current approaches often attempt to narrow this gap by eliciting Chain-of-Thought (CoT) reasoning as natural-language reasoning traces, encouraging models to articulate intermediate steps explicitly (Wei et al., 2022; Zhang et al., 2024c), or relying on help from additional tools (Hu et al., 2024). However, even in multimodal settings, the intermediate computation is externalized in text, which is poor interface for many visuospatial operations. Many subroutines underlying abstract visual reasoning, such as mentally rotating shapes, simulating dynamics, or inducing rules in pattern matrices, are most naturally expressed as geometric transformations, continuous states, or structured visual relations rather than as sequence of discrete tokens. Expressing these intermediate states solely in linear text can introduce representational 1 bottlenecks, where spatial relations are compressed into discrete tokens, increasing the risk of information loss and brittle reasoning. To quantify this cognitive gap, we introduce CogSense-Bench, comprehensive benchmark that operationalizes visual cognition along five core dimensions: fluid intelligence, crystallized intelligence, visuospatial cognition, mental simulation, and visual routines, where these dimensions are the foundation of intuitive theories of physics and psychology (Lake et al., 2017). Using CogSense-Bench, we systematically re-evaluate SoTA MLLMs and find that substantial weaknesses persist across all dimensions, even when models are equipped with CoT prompting, suggesting that text-only reasoning is indeed often brittle interface for tasks that require multi-step visual transformation and manipulation. We therefore seek to shift part of the intermediate reasoning from discrete tokens to representation space that better preserves geometry, continuity, and structured visual relations. This direction is consistent with cognitive science accounts of visuospatial sketchpad (the minds eye) that supports maintaining and transforming internal visual representations during problem solving (Ganis & Schendan, 2011; Xie et al., 2020; Tabi et al., 2022), and with recent progress showing that latent visual states can serve as effective substrates for prediction and world modeling (Yang et al., 2025b). Motivated by these insights, we explore whether equipping MLLMs with an internal visual reasoning substrate can better support multi-step, non-linguistic manipulation of visual information. To this end, we introduce Cognitive Supersensing, training paradigm that encourages MLLMs to construct latent image-based internal reasoning chains. Concretely, we augment an MLLM with Latent Visual Imagery Prediction (LVIP) head that predicts sequence of visual imagery latent embeddings, latent imagery states that encode intermediate visual reasoning, and aligns them with image representations derived from the target answer and supervision signal, thereby grounding the latent chain toward the correct solution. This design enables multi-step reasoning in representation space that better preserves visual structure, while still allowing language to provide high-level semantic guidance and final explanations. To train models under this paradigm, we further curate CogSense-Dataset, targeted training set spanning five categories and eleven sub-tasks, and adopt multi-stage training pipeline. First, in supervised fine-tuning stage, we optimize LVIP objective that provides an auxiliary learning signal encouraging intermediate latent visual states to stay predictive of the final answers visual representation, tightly coupling the reasoning process with the solution. Second, we introduce reinforcement learning stage that optimizes rollout trajectories in latent-space conditioned on these visual cognitive latents, encouraging coherent latent dynamics and discouraging brittle text-only reasoning paths. Across extensive experiments, CogSense-8B achieves SoTA performance in all tasks and the strongest overall accuracy (73.8%), surpassing GPT-5.2 by +33.5. It also has improvement in out-of-domain (OOD) generalization, outperforming strong baselines on challenging broader science VQA benchmarks. Taken together, our results suggest that latent visual imagery can serve as an effective internal scaffold that bridges perceptual recognition and cognitive-level multimodal reasoning. Overall, our contributions are: We propose Cognitive Supersensing, latent-space reasoning-and-learning framework with supervised finetuning and reinforcement learning stages that endows MLLMs with visual imagery capability by aligning semantic reasoning with latent visual world modeling. We introduce CogSense-Bench, comprehensive benchmark spanning five cognitive dimensions, fluid intelligence, crystallized intelligence, visuospatial cognition, mental simulation, and visual routines, providing systematic testbed for evaluating visual cognition beyond perceptual recognition and supporting future research in this direction. Through extensive experiments, we show that reasoning and planning purely in text space is often insufficient for visual cognition: CogSense-8B achieves SoTA performance on CogSense-Bench and exhibits strong OOD generalization on challenging mathematics and science VQA benchmarks, outperforming competitive baselines that rely on text-only planning. Figure 1 CogSense-Dataset Examples. Samples across each category from the CogSense-Dataset. CogSense-Dataset comprises various visual cognitive questions classified into five categories: Fluid Intelligence, Crystallized Intelligence, Visuospatial Cognition, Mental Simulation, and Visual Routines, which require visual imagery and cognitive supersensing with deep thinking and reasoning. 3 Figure 2 CogSense-Dataset Distribution. The data distribution of our CogSense-Dataset-105K."
        },
        {
            "title": "2 CogSense Dataset and Benchmark",
            "content": "While numerous VQA benchmarks evaluate MLLMs across various tasks, they predominantly focus on semantic recognition or description (Li et al., 2025e; Chia et al., 2024). There remains lack of systematic evaluation protocols specifically designed to assess high-level visual cognition, i.e. the ability to reason over visual inputs through distinct cognitive mechanisms such as abstract reasoning, spatial structuring, mental simulation, and attention-driven operations. To address this gap, we introduce CogSense-Dataset-105K, comprehensive and large-scale dataset, together with unified multi-task benchmark, CogSense-Bench. Visual cognition supports high-level visual reasoning and inference processes, such as causal reasoning, intuitive physics, and intuitive psychology (Schulze Buschoff et al., 2025; Battaglia et al., 2013). These capacities reflect central dimensions through which humans interpret, predict, and reason about visual environments. To systematically evaluate whether MLLMs exhibit analogous competencies, we assess visual cognitive intelligence across five theory-grounded cognitive categories. These categories are selected to align with established constructs in cognitive science and psychology, and to capture complementary mechanisms underlying human visual reasoning as articulated in prior theoretical and empirical work. First, Fluid Intelligence (Gf ) evaluates the capacity to solve unseen reasoning problems independently of prior knowledge (Cattell, 1963). Grounded in Structure Mapping Theory (Gentner, 1983), this requires the model to transcend surface attributes and map high-order logical rules. Crystallized Intelligence (Gc) targets the utilization of learned world knowledge (Cattell, 1963), relying on Inductive Reasoning and Prototype Theory to abstract semantic concepts from visual variance (Rosch, 1973). To assess 3D spatial understanding, Visuospatial Cognition tests the reconstruction of structural relationships, requiring the ability to group discrete visual elements into coherent, holistic structures based on laws (Wertheimer, 1923) and the decomposition of shapes into geometric primitives (geons) consistent with Recognition-by-Components Theory (Biederman, 1987). Distinct from static recognition, Mental Simulation demands that models act as \"simulation engines,\" employing Hypothetico-Deductive Reasoning to synthesize programs and infer hidden dynamics (Battaglia et al., 2013). Finally, Visual Routines evaluates efficiency in visual search, requiring the composition of elementary operations and Focused Attention to bind features and exercise inhibitory control (Ullman, 1984). Fig. 1 depicts examples of each category, and Fig. 2 shows the data distribution of CogSense-Dataset. Detailed statistics are provided in Appendix A. Appendix provides the implementation details for the data pipeline. CogSense Dataset and Benchmark are designed to probe high-level visual cognition beyond recognition, including abstract rule induction, spatial structuring, mental simulation, and attention control. These categories often require (i) explicit multi-step reasoning that composes elementary operations, and (ii) maintaining and manipulating answer-oriented internal visual states during inference. Motivated by these requirements, we propose Cognitive Supersensing that couples chain-of-thought style rationale generation with latent visual imagery prediction, enabling MLLMs to better support simulation-like and stateful reasoning demanded by CogSense-Bench."
        },
        {
            "title": "3.1 Preliminaries\nLet V and Q denote the visual input and the textual prompt, respectively, and denote the multimodal input\nas X = (V, Q). Our goal is to learn a multimodal model parameterized by θ, which induces a conditional\ngeneration distribution qθ(· | X) over an output consisting of a reasoning rationale Z (a token sequence) and\na final answer y.\nSpecifically, V is processed by a pre-trained visual encoder Encvis(·) to extract visual features, denoted as\n, where T is the number of images and vi corresponds to the i-th image. These features are\nVV = {vi}T\nmapped into the language embedding space via a projection layer P(·), yielding projected visual tokens\nhV = P(VV ). Simultaneously, the prompt Q is tokenized and embedded using the LLM embedding layer to\nobtain textual tokens hQ. The LLM backbone Enctxt(·) takes the concatenated token sequence [hV , hQ] as\ninput and produces hidden states used by (i) a text decoder to generate (Z, y) autoregressively and (ii) a\nLatent Visual Imagery Prediction (LVIP) head to predict answer-oriented latent imagery.",
            "content": "i=1 The training follows three-stage pipeline, shown in Fig. 3: (1) Reasoning Chain Generation to synthesize high-quality rationales, (2) SFT with Latent Visual Imagery Prediction to jointly learn text generation and latent imagery prediction, and (3) RL with Latent Rationales to further optimize rationale sampling."
        },
        {
            "title": "3.2 Stage I: Reasoning Chain Generation",
            "content": "To address the scarcity of high-quality cognitive training data for visual CoT reasoning, we synthesize rationales using powerful MLLM as teacher model, denoted as MT . Given multimodal input pair (V, Q), MT generates reasoning rationale that demonstrates the logical deduction steps, along with predicted answer ˆy, which we then filter to ensure alignment with the ground-truth answer y. Specifically, for each piece of data, we construct task-specific generation prompt Pgen. The teacher model is instructed to analyze the visual input and produce step-by-step reasoning chain based on (V, Q, Pgen): (Z, ˆy) MT ( V, Q, Pgen). (1) We filter out generated reasoning chains that fail to reach the correct conclusion (ˆyi = yi) or contain hallucinated content. This yields an augmented dataset Dchain = {(Vi, Qi, Zi, yi)}N for SFT. Additional details regarding Pgen are available in Section B.3. i="
        },
        {
            "title": "3.3 Stage II: SFT with Latent Visual Imagery Prediction\nWe propose Cognitive Supersensing as an approach for acquiring visual cognitive capabilities. Motivated by the\nhuman constructive matching process, we introduce an auxiliary module, Latent Visual Imagery Prediction\n(LVIP), to predict hy, the latent representation of the ground-truth answer option image. LVIP is trained\njointly with the standard supervised fine-tuning objective.",
            "content": "LVIP Head. The LVIP head gψ() is two-layer MLP attached to the shared LLM backbone, in addition to the text decoder. It predicts the latent representation of the answer option using the backbone hidden states corresponding to visual tokens. We assume the visual input contains the question image together with candidate option images. Let HV RN be the sequence of hidden states for all visual tokens output by Enctxt(). We extract the subset corresponding to the option images, denoted as Hopt RM with . We apply average pooling over Hopt to obtain an aggregated representation hopt, and compute the predicted latent imagery ˆhy = gψ(hopt). Learning Objectives. Let Vy denote the candidate option image indexed by the ground-truth answer y. The supervision target for LVIP is the embedding of Vy extracted by the (frozen) visual encoder, where hy = Encvis(Vy). We optimize LVIP with an MSE loss between ˆhy and hy, jointly with the standard 5 Figure 3 The framework of Cognitive Surpersensing. Left: Architecture Overview. CogSense-8B is VLM that takes images and prompts as input with text decoder to generate the answer and Latent Visual Imagery Prediction (LVIP) head to generate latent visual imagery of the option-image in parallel. Right: Method Overview. To train CogSense-8B, we (1) generate reasoning paths via LLMs, (2) implement SFT to jointly optimize the LVIP head and the model weights, and (3) implement RL to further optimize reasoning paths with Latent Rationales. autoregressive cross-entropy loss over the target text sequence = (Z, y) conditioned on = (V, Q): LSFT = (cid:88) t=1 log qθ(xt X, x<t) + β MSE(ˆhy, hy), (2) where β balances the two objectives. At inference time, the final answer is generated by the text decoder; the LVIP head can be kept as frozen auxiliary module to produce ˆhy for optional answer-oriented grounding."
        },
        {
            "title": "3.4 Stage III: RL with Latent Rationales",
            "content": "In this stage, we leverage diversity-seeking RL by performing amortized variational inference over latent rationales using Generative Flow Network (Bengio et al., 2021, 2023; Lahlou et al., 2023; Zhang et al., 2022). Concretely, we refine the rationale policy qθ(Z X) to sample rationale trajectories in proportion to an unnormalized trajectory score, rather than committing to single deterministic chain. We define the corresponding reward-induced target posterior over rationales as p(Z X, y) exp(R(Z; X, y)) (3) and train qθ(Z X) to approximate this posterior via the flow-matching objective. We define the trajectory score as weighted combination of answer evidence and LVIP-based representation grounding: R(Z; X, y) = α Rans(Z; X, y) + γ Rlvip(Z; X, y). (4) Importantly, the LVIP term provides answer-oriented grounding in representation space, which complements discrete answer supervision and facilitates exploration over long rationale trajectories. 6 Table 1 Cognitive Ability Results. Performance comparison on CogSense-Bench with CogSense-8B and different MLLMs and VLMs. We bold the best results and underline the runner-ups. Human baseline is also listed on the first row. Model"
        },
        {
            "title": "Human",
            "content": "Gemini 2.5 Flash GPT-o3 GPT-5.2 Claude Sonnet 4 Grok 4 Fast Llama-4-Scout-17B Gemma-3-27B Ministral-14B Intern-S1 Qwen3-VL-30B CogSense-8B (Ours) Fluid Intel. Crystallized Intel. Visuospatial Cog. Mental Simu. Visual Rout. Avg. 82.7 23.2 4.7 29.4 22.5 13.0 20.3 18.5 9.42 6.6 30.8 63. 91.3 40.2 51.4 35.9 31.3 45.4 29.9 29.4 10.6 21.0 34.0 91.0 88.5 31.0 20.4 57.5 26.6 41. 35.4 39.8 10.6 12.4 37.2 69.0 97.9 40.2 38.7 60.0 58.0 21.3 48.7 55.3 39.3 17.5 56.0 68. 78.7 45.3 43.0 37.6 34.4 37.6 41.9 43.0 31.2 42.6 40.9 50.5 88.4 36.3 32.3 40.3 32.6 31. 31.8 32.7 16.5 17.4 37.4 73.8 Token-Wise Marginal Reward Estimation. Let = (z1, . . . , zn, ) be an autoregressive rationale, with denoting end of sequence. We define the answer-evidence component using frozen scorer to avoid moving-target reward: Rans(Z; X, y) = log qθ0 (y X, Z) , (5) where qθ0 is frozen scorer (a fixed copy of the supervised model). To incorporate LVIP grounding during RL while keeping it stationary, we keep the LVIP head gψ frozen and compute representation-level reward that depends on through the backbone conditioning on [X; Z]. Let Vy be the option image indexed by the ground-truth answer y, and let hy = Encvis(Vy) be its embedding from frozen visual encoder. Let hopt(X, Z; θ) denote the average-pooled final-layer hidden state of the option-image visual tokens produced by the backbone when conditioned on [X; Z]. We define Rlvip(Z; X, y) = (cid:13) (cid:13)gψ (cid:0)hopt(X, Z; θ)(cid:1) hy (cid:13) 2 2 . (cid:13) (6) To provide prefix-level training signals required by flow-based objectives, we define the prefix state at step as τt = (z1:t, ), where indicates the availability of termination action rather than an emitted token. Instead of evaluating the scorer at every token, we compute (cid:101)R(τt) only at sparse anchor indices with stride λ and linearly interpolate within each segment. Specifically, let anchors be {0, λ, 2λ, . . .} with < n, and define t+ = min(t + λ, n). For any integer {0, 1, . . . , t+ t}, (cid:101)R(τt+i) = (cid:101)R(τt) + (cid:16) t+ (cid:101)R(τt+) (cid:101)R(τt) (cid:17) . At anchor positions, we set (cid:101)R(τt) R(τt; X, y) with R(τt; X, y) = α log qθ0 (y X, z1:t) + γ Rlvip(τt; X, y), (7) (8) where Rlvip(τt; X, y) is computed analogously by conditioning the backbone on [X; z1:t]. We then plug (cid:101)R into standard SubTB loss (Madan et al., 2023) to train qθ(Z X) toward the reward-induced target distribution. Reference-Guided GFlowNet Fine-tuning. To reduce variance from low-quality samples, we anchor exploration with reference rationale Zref . For each X, we sample candidates {Zi}m i=1 qθ( X) and keep only those that meet relative evidence threshold: I(Zi) = 1[ R(Zi; X, y) R(Zref ; X, y) + log δs ] , (9) 7 Table 2 General Ability Results. Performance comparison in general tasks with CogSense-8B and the base model. CogSense-8B demonstrates similar general ability performance to the base model. Model HallusionBench AI2D GQA ScienceQA RealWorldQA ChartQA BLINK MMStar Qwen3-VL-8B(base) CogSense-8B (Ours) 61.1 60. 85.4 85.1 71.4 71.8 92.6 92.6 71. 71.9 88.6 84.7 64.7 65.3 70.9 66.8 Table 3 Ablation Study Results. We compare the base model and three variants: SFT w/o LVIP, SFT w/ LVIP, SFT w/o LVIP + GRPO, SFT w/ LVIP + GRPO and CogSense-8B. We bold the best results and underline the runner-ups. Variant Fluid Intel. Crystallized Intel. Visuospatial Cog. Mental Simu. Visual Rout. Avg. Qwen3-VL-8B (base) Qwen3-VL-8B SFT w/o LVIP Qwen3-VL-8B SFT w/ LVIP Qwen3-VL-8B SFT w/o LVIP + GRPO Qwen3-VL-8B SFT w/ LVIP + GRPO CogSense-8B (Ours) 31.2 51.1 55.4 55.8 59.1 63.8 34.8 76.6 88.6 79.9 89. 91.0 31.0 63.7 61.1 63.7 64.6 69.0 45.3 59.3 61.3 63.3 65.3 68.0 40.9 41.9 44.1 43.0 46. 50.5 35.5 62.3 68.0 65.5 70.8 73.8 where δs (0, 1] and is the index of the current training step (so log δs 0 controls the allowed slack in log-space relative to the reference). We optimize SubTB only on accepted trajectories: L(θ) = (cid:88) i=1 I(Zi) LsubTB(Zi; θ), (10) where LsubTB is instantiated with the densified prefix scores (cid:101)R. Bayesian Posterior over Latent Rationales. At inference time, we treat as latent and aggregate evidence across i=1 qθ(Z X), decode an answer yi conditioned multiple sampled rationales. We sample rationales {Zi}N on (X, Zi), and compute length-normalized evidence score with the frozen scorer Si = 1 Zi + yi log qθ0(yi X, Zi). (11) We then output ˆy = yi , where = arg maxi Si, which serves as simple MAP-style selection over sampled latent rationales and reduces sensitivity to any single brittle chain under reasoning ambiguity."
        },
        {
            "title": "4.1 Experimental Settings\nBaselines and Metrics. We compare CogSense-8B with several mainstream open and closed-source MLLMs\nand VLMs with strong multimodal performance: Gemini 2.5 Flash (Comanici et al., 2025), GPT-o3 (OpenAI,\n2025b), GPT-5.2 (OpenAI, 2025a), Claude-Sonnet-4 (Anthropic, 2025), Grok 4 Fast (xAI, 2025), Llama-4-\nScout-17B (Meta, 2025), Gemma-3-27B (Team, 2025), Ministral-3-14B (MistralAI, 2025), Intern-S1 (Bai\net al., 2025a), and Qwen3-VL-30B (Bai et al., 2025b). The accuracy rate is used as the metric to evaluate the\nperformance in all tasks. We also let 20 participants complete 100 questions selected from CogSense-Bench\nthrough stratified sampling, reporting the comparison results of human-accurate rates for CogSense-Bench\nin Tab. 1 as a human baseline. This study has been approved by the Institutional Review Board (IRB). The\ndetailed information of the human study can be found in Appendix C.",
            "content": "Implementation Details. All experiments are conducted with 8 NVIDIA H200 GPUs. We use Qwen3-VL-8B as the backbone to train CogSense-8B. We use Adam as the optimizer with η = 105 and weight decay 105."
        },
        {
            "title": "4.2 Cognitive Ability Results\nQuantitative Results. As Tab. 1 illustrates, despite being an 8B model, CogSense-8B achieves SoTA performance\nin all tasks and delivers the strongest overall accuracy (73.8%), surpassing GPT-5.2 by +33.5. Notably, when",
            "content": "8 Figure 4 Qualitative Example of Visual Cognition Reasoning Across Models. We underline decisive sentences in the reasoning chain. CogSense-8B demonstrates coherent, multi-step logical chain that closely matches the ground truth, while other models exhibit less precise or less interpretable reasoning paths compared against human performance, CogSense-8B substantially narrows the performance gap relative to existing MLLMs, outperforming the next strongest baseline by large margin across all cognitive categories. This reduction in the humanmodel gap suggests that CogSense-8B more effectively leverages visual imagery to support cognitive reasoning, rather than relying primarily on superficial or text-biased heuristics. Qualitative Comparison. Fig. 4 shows qualitative comparison between CogSense-8B and mainstream models, demonstrating that Gemini 2.5 Flash and GPT-5.2 both fail in extracting the correct underlying rule of the pattern, leading to wrong answer. In contrast, CogSense-8B demonstrates high-quality reasoning with concise and clear expression of the underlying pattern. This comparison suggests that CogSense-8B more effectively captures the underlying visual regularities required for abstract pattern reasoning. Additional qualitative examples can be found in Appendix D."
        },
        {
            "title": "4.3 General Ability Results",
            "content": "We also evaluate the general vision-language understanding ability of CogSense-8B. As shown in Tab. 2, we evaluate on vision-language datasets such as HallusionBench (Guan et al., 2024), AI2D (Kembhavi et al., 2016), GQA (Hudson & Manning, 2019), ScienceQA (Lu et al., 2022), RealWorldQA (xAI, 2024), ChartQA (Masry et al., 2022), BLINK (Fu et al., 2024), MMStar (Chen et al., 2024). CogSense-8B maintains robust performance on these general benchmarks, comparable to the base model. This balance proves that the visual cognitive abilities of Cognitive Supersensing are not due to overfitting of data, but successfully injects high-level visual cognitive knowledge while preserving the foundational knowledge and instruction-following capabilities inherent in the pre-trained backbone."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "As summarized in Tab. 3, to isolate the contributions of LVIP and our proposed reinforcement learning mechanism, we perform hierarchical evaluation across five variants: the standard SFT without LVIP, SFT with LVIP, SFT without LVIP plus GRPO, SFT with LVIP plus GRPO, and CogSense-8B that combines LVIPsupervised SFT and the RL method we proposed. Compared to the vanilla Qwen3-VL-8B, applying standard 9 SFT nearly doubled the average performance. The integration of LVIP introduces further improvement, raising the average accuracy to 68.0%. This suggests that LVIP helps the model align semantic reasoning and abstract world modeling, enabling the model to gain initial supersensing\" capabilities. To validate the necessity of our specific RL design, we introduce GRPO as reinforcement learning baseline. Although applying GRPO to the SFT models (w/ or w/o LVIP) yields observable gains, achieving improvement by +3.2 and +2.8 compared to models without GRPO, it serves primarily to refine the policy within standard optimization scope. Finally, CogSense-8B achieves the SoTA performance of 73.8% on average, outperforming the strongest baseline (SFT w/ LVIP + GRPO) by distinct margin. This result highlights that our elaborately designed RL method is not only an optimization trick, but specialized reasoning refiner that is more effective than general GRPO in leveraging the visual supersensing established by LVIP."
        },
        {
            "title": "4.5 Out-of-Domain Evaluation",
            "content": "To further verify the generalization capability of CogSense-8B, we extend our evaluation to out-of-domain scenarios using the Chemistry and Math subsets of the EMMA benchmark (Hao et al., 2025), with both the question and the options having images. Examples for selected data from EMMA are displayed in Fig. 5. As shown in Tab. 4, CogSense-8B achieves substantial gains in the Chemistry and Math subsets, respectively, by +6.2 and +8.8, confirming that our method learns generalized visual cognition patterns rather than overfitting to specific training data."
        },
        {
            "title": "5 Related Work",
            "content": "Our work is closely related to Abstract Reasoning, Visual Cognition, VLMs for Visual Reasoning, and Latent Visual Reasoning. comprehensive discussion is provided in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "Figure 5 EMMA Benchmark Sample Problems. Model Table 4 Out-of-Domain Evaluation. We evaluate the generalization ability of CogSense-8B compared with the backbone on the EMMA benchmark. In this work, we propose Cognitive Supersensing, paradigm designed to bridge the gap between perceptual processing and complex cognitive reasoning in MLLMs. By integrating Latent Visual Imagery Prediction (LVIP) head and employing training strategy that combines SFT and RL with Latent Rationales, we enable CogSense-8B to simulate internal visual imagery aligned with semantic reasoning chains. We further introduce CogSense-Bench, comprehensive evaluation suite targeting five dimensions of visual cognition. Our experiments demonstrate that CogSense-8B achieves superior performance compared to SoTA MLLM baselines on these cognitive tasks. These results indicate that modeling the interaction between visual simulation and logical deduction is an effective direction for advancing the reasoning abilities of multimodal systems. Qwen3-VL-8B (base) CogSense-8B (Ours) Chemistry Mathematics 45.46.2 34.88.8 26.0 39."
        },
        {
            "title": "Appendix",
            "content": "A Statistics and Samples of Dataset and Benchmark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Implementation Details for Data Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.1 Data Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.2 Data Reformatting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 B.3 Reasoning Chain Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Human Study Design and Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 More Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Statistics and Samples of Dataset and Benchmark",
            "content": "We conclude CogSense Dataset and Benchmark statistics in Tab. A1 and Tab. A2. We ensured zero data leakage by randomly sampling instances from each category at the same proportion as in the CogSense-Dataset and subsequently removing the sampled data from it. Table A1 CogSense-Dataset Descriptions and Statistics. We have listed the number of VQA pairs corresponding to each category. Category Fluid Intelligence Crystallized Intelligence Dataset MaRs-VQA PGM RAVEN Bongard-RWR+ Bongard-HOI Visuospatial Cognition Bongard-LOGO Mental Simulation KiVA STARE ARC-AGI ARC-AGI-2 Visual Routines CVR Sum #Test 1.4K 10K 18K 16K 23K 12K 1.4K 4K 1.6K 8K 10K 105.4K License CC-BY-NC-3.0 Unspecified GPL-3.0 License CC-BY-4.0 NVIDIA Source Code License MIT license Apache-2.0 License Unspecified Apache-2.0 License Apache-2.0 License Apache-2.0 License Table A2 CogSense-Bench descriptions and statistics. We have listed the number of VQA pairs corresponding to each category. Category Fluid Intelligence Dataset MaRs-VQA PGM RAVEN #Test 13 94 License CC-BY-NC-3.0 Unspecified GPL-3.0 License Crystallized Intelligence Bongard-RWR+ Bongard-HOI 152 216 CC-BY-4.0 NVIDIA Source Code License Visuospatial Cognition Bongard-LOGO 113 MIT license Mental Simulation KiVA STARE ARC-AGI ARC-AGI-2 Visual Routines CVR Sum Apache-2.0 License Unspecified Apache-2.0 License Apache-2.0 License Apache-2.0 License 13 26 31 80"
        },
        {
            "title": "B Implementation Details for Data Pipeline",
            "content": "B.1 Data Extraction We searched for valuable raw data related to visual cognition from large number of datasets with various tasks, such as Bongard-OpenWorld (Wu et al., 2024), Bongard-HOI (Jiang et al., 2022), Bongard-LOGO (Nie et al., 2020), Bongard-RWR+ (Pawlonka et al., 2025), CVR (Zerroug et al., 2022), KiVA (Yiu et al., 2025), MaRs-VQA (Cao et al., 2025), PGM (Barrett et al., 2018), RAVEN (Zhang et al., 2019), I-RAVEN (Hu et al., 2021), RAVEN-FAIR (Benny et al., 2021), A-I-RAVEN (Małkiński & Mańdziuk, 2025), I-RAVENMesh (Małkiński & Mańdziuk, 2025), I-RAVEN-X (Camposampiero et al., 2025), STARE (Li et al., 2025e), ARC-AGI (Chollet, 2019), ARC-AGI-2 (Chollet et al., 2025), etc, and manually selected valuable data that fit pre-defined five categories from datasets mentioned above. These sources were strictly selected to ensure high relevance to the downstream tasks. Building upon these seed sources, we developed automated extraction and cleaning scripts to perform large-scale expanded collection and finally got the raw dataset. B.2 Data Reformatting In order to standardize the format of the dataset, we reformatted those questions where the metric was not originally multiple-choice to become multiple-choice. Reformatting for Bongard Problems. We randomly selected one image from the positive side and mixed it with the negative samples, shuffling them as options. The remaining positive samples were used as the question. Reformatting for ARC-AGI Problems. We perform data augmentation on the ground-truth images, including methods such as color modification. Both the ground-truth images and the augmented images are provided as options. B.3 Reasoning Chain Generation As stated in the main paper, we designed to transform traditional short QA pairs into high-quality reasoning data containing an explicit reasoning chain. We customized different prompts for different types of original questions and used LLMs to generate corresponding reasoning chains based on these different prompts. The model was required not only to output the final answer but also to elaborate on its reasoning process. Specifically, the model was guided to analyze visual cues within the images, infer underlying rules, and explain causal relationships, finally generating detailed reasoning process text. To ensure quality, we filter out generated reasoning chains that fail to reach the correct conclusion or exhibit hallucinated content. Fig. B1 shows the customized prompts."
        },
        {
            "title": "C Human Study Design and Setup",
            "content": "Participant Recruitment. To establish human performance baseline, we recruited total of 20 participants through online platforms using random sampling strategy. Participation in the study was entirely voluntary, and no financial compensation or monetary incentives were provided to the subjects. Questionnaire Design. The questionnaire design involved rigorous selection process to ensure representativeness. We constructed test set comprising 100 multiple-choice questions derived from the CogSense-Bench. These questions were manually selected by stratified sampling to strictly maintain the category proportions of the original benchmark. The study was implemented and distributed digitally via the Google Forms platform, allowing for remote administration. Compliance. Regarding experimental standards and ethical compliance, all protocols were reviewed and approved by the Institutional Review Board (IRB) prior to the study. 13 Reasoning Chain Generation Prompts for CogSense-Dataset You are careful visual reasoning assistant. Your task is to generate CoT and the final answer for the provided questions. Your response must consist of ONLY the following two parts in JSON format: 1. reasoning: reasoning steps (3-12 concise steps from high-level to details). 2. final_answer: the final answer of the question. Keep reasoning concise (3-12 steps). Do NOT include internal deliberation beyond the steps. Output must be in JSON format and ONLY with the reasoning steps and final answer with nothing else. For Crystallized Intelligence and Visuospatial Cognition Questions: You will be given set of images that share common pattern. Analyze these examples to understand the pattern. Then, select the image that belongs to the same category as the images in the question set. Question images (these images share common pattern): [question_images] Option images: [option_images] For Fluid Intelligence Questions: You will be given an image that contains eight subimages and blank space. Then, given some option images, predict which one is the right option, following the same pattern to complete the sequence based on the pattern shown in the question image. Question image: <question_image> Option images: [option_images] For ARC-AGI in Mental Simulation: You will be given some pairs of input-output images as examples. The output is obtained by coloring the grid according to specific pattern based on the input. Analyze these examples to understand the underlying grid color pattern. Then, given new input image and option images, predict which image is generated by applying the same pattern to the input. Examples: [<input_image, output_image>] Question image: <new_input_image> Option images: [option_images] For KiVA in Mental Simulation: You will be given some pairs of input-output images or transform patterns as examples. Analyze the pattern and select the correct answer from the given options. Then, given new input image and some options that indicate possible outputs, predict the right option following the same pattern. Examples: [pattern_image] Question image: <new_input_image> Option images: [option_images] For STARE in Mental Simulation: You will be given some images and blank space. Then, given some option images, predict which one is the right option, following the same pattern to complete the sequence based on the pattern shown in the question image. Question image: <question_image> Option images: [option_images] For Visual Routines Questions: You will be given some images, in which one of them differs from the rest of the images. Find the odd one out. Question images: [images] Option images: [images] Input: [images] and <prompt> Output: <reasoning_chain> and <answer> Figure B1 Reasoning Chain Generation Prompts. We customize different prompts for different types of original questions and use LLMs to generate corresponding reasoning chains based on these different prompts."
        },
        {
            "title": "D More Qualitative Examples",
            "content": "More qualitative examples are shown in Fig. D1. Figure D1 More Qualitative Examples of Visual Cognition Reasoning Across Models. CogSense-8B demonstrates coherent, multi-step logical chain that closely matches the ground truth, while other models exhibit less precise or less interpretable reasoning paths"
        },
        {
            "title": "E Related Works",
            "content": "Our work is closely related to Abstract Reasoning, Visual Cognition, VLMs for Visual Reasoning, and Latent Visual Reasoning. Bongard Problems. The Bongard Problems (BPs) are introduced by M. M. Bongard (Bongard, 1968, 1970) as novel challenge to machine vision. typical BP includes two sets, positive and negative, with each side consisting of six images that share common pattern. It requires system to induce the logical rule that distinguishes set of positive examples from negative ones (Bongard, 1968, 1970). This task evaluates the core properties of human cognition, identifying the underlying rule that differentiates the sides and articulating it in natural language (Małkiński et al., 2025). To modernize this benchmark for deep learning, the Bongard-LOGO dataset was introduced, transforming the concept learning challenge into few-shot binary classification problem (Nie et al., 2020). Recent works have expanded the field of BPs to include real-world images, such as Bongard-HOI (Jiang et al., 2022), Bongard-OpenWorld (Wu et al., 2024), Bongard-RWR (Małkiński et al., 2025), Bongard-RWR+ (Pawlonka et al., 2025). These works expand the range of presented objects, attributes, and relationships, illustrating human-object interactions and incorporating free-form concepts in the real world, thus increasing the diversity of featured scenes. Matrix Reasoning. Matrix reasoning tasks play critical role in assessing human intelligence, particularly in relation to visual cognition and working memory. (Salthouse, 1993; Jaeggi et al., 2010; Fleuret et al., 2011). These tasks are widely used in the psychology field through Ravens Progressive Matrices (RPMs) (Raven, 1936; Raven & Court, 1998; John & Raven, 2003) and the Wechsler Intelligence Scale (WISC) (Wechsler, 1949, 2008; Kaufman et al., 2015) to evaluate human fluid intelligence and abstract visual reasoning. Early works, such as PGM (Barrett et al., 2018) and RAVEN (Zhang et al., 2019), are proposed to test if neural networks can learn abstract reasoning and to lift machine intelligence by associating vision with structural, relational, and analogical reasoning in hierarchical representation. The claim that deep learning models can be trained to solve simple matrix reasoning was then proved by other works (Małkiński & Mańdziuk, 2025; Xu et al., 2023; Małkiński & Mańdziuk, 2024). Several datasets and benchmarks are also introduced to enlarge this field, such as I-RAVEN (Hu et al., 2021), RAVEN-FAIR (Benny et al., 2021), and CVR (Zerroug et al., 2022). However, these studies tend to overlook the ability of humans to solve such puzzles in zero-shot manner, without the need for explicit training on extensive datasets. Therefore, some useful zero-shot visual reasoning inference datasets such as RAVEN-IQ (Huang et al., 2023) and Visual Reasoning Benchmark (Zhang et al., 2024b) are proposed to deal with the issue. The latest MaRs-VQA (Cao et al., 2025) overcomes previous works limitations of lacking rigorous human experiments as reference and conducting experiments on relatively small datasets without psychometric validation. Abstraction and Reasoning Corpus. Abstraction and Reasoning Corpus for Artificial General Intelligence (ARCAGI) is novel general AI benchmark designed to measure human-like form of general fluid intelligence and enable fair general intelligence comparisons between AI systems and humans (Chollet, 2019). Its measurement standard is based on the skill-acquisition efficiency and out-of-domain generalization over mere task performance. The tasks require systems to solve grid-based visual program synthesis problems by abstracting the underlying rules from only minimal set of input-output examples (typically three to five pairs). robust estimate of human performance (LeGris et al., 2024) on the full ARC public evaluation set places the average human accuracy at 64.2%, which greatly exceeds the current state-of-the-art AI methods. Its updated version, ARC-AGI-2 (Chollet et al., 2025), is more challenging. Even the most recent and powerful models at that time, such as OpenAIs o3 (OpenAI, 2025b), achieve only negligible performance on the newly released, adversarially selected ARC-AGI-2 challenge. This reinforces that modern frontier reasoning models fundamentally lack the structural inductive biases that are necessary for human-like skill-acquisition and generalization. VLMs for Vision Reasoning. Visual recognition-related Vision Language Model (VLM) studies have made great progress since the development of CLIP (Radford et al., 2021). It has now been proven that VLMs have the ability to address vision reasoning tasks (Zellers et al., 2019; Bordes et al., 2024). VLMs are the dominant architecture for bridging perception and language, typically integrating visual encoder (e.g., CLIP) with pretrained LLMs via cross-modal connector to align visual features with text space (Radford et al., 2021; Gupta & Kembhavi, 2023; Li et al., 2023; Liu et al., 2023; Zhang et al., 2024a; Shao et al., 2024; Fu et al., 2024). The training pipeline for VLMs often involves pre-training for modality alignment, followed by instruction tuning or Supervised Fine-Tuning (SFT) to enhance general multimodal capabilities (Zhu et al., 2023; Li et al., 2023; Liu et al., 2023; Ye et al., 2024). Methodologies such as Qwen-VL (Bai et al., 2023; Wang et al., 2024a; Bai et al., 2025c,b), LLaVA (Liu et al., 2023, 2024), MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), CogVLM (Wang et al., 2024b), etc., highlight the significance of employing high-quality visual instruction tuning data. However, current VLMs still face challenges in adapting to high-resolution (Carvalho & Martins, 2025; Li et al., 2025d) and visually complex images in vision reasoning. This is because most current vision reasoning approaches are primarily text-level, with the LLM exploring textual tokens while the visual input remains static (Wu & Xie, 2024; Izadi et al., 2025; Li et al., 2025a; Bai et al., 2025d; Liu et al., 2025). This absence of dynamic visual search mechanism limits the models ability to selectively acquire fine-grained visual cues. Other reasons stem from few-shot reasoning (Guo et al., 2023), compositional understanding (Yuksekgonul et al., 2023), and the constrained visual grounding capabilities inherent in CLIP (Tong et al., 2024), etc. Our work overcomes these problems, providing novel approach for high-performing vision reasoning. Latent Visual Reasoning. While standard VLMs rely on static visual encoding, new paradigm of Latent Visual Reasoning has emerged, shifting the focus from passive perception to active, internal simulation. This approach draws inspiration from World Models (Zhu et al., 2025; Sun et al., 2025), enabling systems to \"think\" in compressed latent space before generating response. Rather than mapping pixels directly to text, these models perform intermediate reasoning steps, like System-2 cognition, by predicting future states or manipulating visual abstractions (Zhu et al., 2025; Sun et al., 2025). For instance, recent frameworks introduce \"visual scratchpads\" or latent tokens that allow the model to sketch out reasoning traces implicitly (Zhang et al., 2025; Li et al., 2025b). Technologies such as Mirage (Yang et al., 2025c) and Latent Sketchpad (Zhang et al., 2025) exemplify this by empowering models to generate and refine mental imagery during the inference process, effectively bridging the gap between visual grounding and abstract logic. Further advancements include Latent Visual Reasoning (LVR) (Li et al., 2025a) and implicit reasoning tokens (Li et al., 2025c), which allow models to perform autoregressive planning in the visual embedding space without explicit supervision. These methods overcome the limitations of text-centric reasoning by maintaining rich, high-dimensional visual information throughout the decision-making chain (Li et al., 2025a,c)."
        },
        {
            "title": "References",
            "content": "Anthropic. System card: Claude opus 4 & claude sonnet 4. System card (technical report), May 2025. URL https://www.anthropic.com/claude-4-system-card. Document shows changelog updates on July 16, 2025 and September 2, 2025. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, et al. Intern-s1: scientific multimodal foundation model, 2025a. URL https://arxiv.org/abs/ 2508.15763. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report, 2025b. URL https://arxiv.org/abs/2511.21631. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report, 2025c. URL https://arxiv.org/abs/2502.13923. Tianyi Bai, Zengjie Hu, Fupeng Sun, Jiantao Qiu, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, and Wentao Zhang. Multi-step visual reasoning with visual tokens scaling and verification, 2025d. URL https://arxiv.org/abs/2506.07235. David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International Conference on Machine Learning, pp. 511520. PMLR, 2018. Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34: 2738127394, 2021. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):155, 2023. Yaniv Benny, Niv Pekar, and Lior Wolf. Scale-localized abstract reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1255712565, 2021. Irving Biederman. Recognition-by-components: theory of human image understanding. Psychological Review, 94(2): 115, 1987. Mikhail Moiseevich Bongard. The recognition problem. Technical report, Foreign Technology Div Wright-Patterson AFB Ohio, 1968. Mikhail Moiseevich Bongard. Pattern Recognition. Spartan Books, 1970. ISBN 978-0-87671-118-7. Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, et al. An introduction to vision-language modeling, 2024. URL https://arxiv.org/abs/2405.17247. Giacomo Camposampiero, Michael Hersche, Roger Wattenhofer, Abu Sebastian, and Abbas Rahimi. I-raven-x: Benchmarking generalization and robustness of analogical and mathematical reasoning in large language and reasoning models, 2025. URL https://arxiv.org/abs/2510.17496. Xu Cao, Yifan Shen, Bolin Lai, Wenqian Ye, Yunsheng Ma, Joerg Heintz, Jintai Chen, Meihuan Huang, Jianguo Cao, Aidong Zhang, and James M. Rehg. What is the visual cognition gap between humans and multimodal llms?, 2025. URL https://arxiv.org/abs/2406.10424. Miguel Carvalho and Bruno Martins. Efficient architectures for high resolution vision-language models. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 1052010530, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. Raymond Cattell. Theory of fluid and crystallized intelligence: critical experiment. Journal of Educational Psychology, 54(1):1, 1963. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. Yew Ken Chia, Vernon Toh, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 1625916273, 2024. François Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/1911.01547. François Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems, 2025. URL https://arxiv.org/abs/2505.11831. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36:4925049267, 2023. François Fleuret, Ting Li, Charles Dubout, Emma Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on visual categorization test. Proceedings of the National Academy of Sciences, 108(43): 1762117625, 2011. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. Giorgio Ganis and Haline Schendan. Visual imagery. Wiley Interdisciplinary Reviews: Cognitive Science, 2(3): 239252, 2011. Dedre Gentner. Structure-mapping: theoretical framework for analogy. Cognitive Science, 7(2):155170, 1983. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Qing Guo, Prashan Wanigasekara, Jian Zheng, Jacob Zhiyuan Fang, Xinwei Deng, and Chenyang Tao. How do large multimodal models really fare in classical vision few-shot challenges? deep dive. In R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1495314962, 2023. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark, 2025. URL https: //arxiv.org/abs/2501.05444. Sheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and Shihao Bai. Stratified rule-aware network for abstract visual reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 15671574, 2021. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36:7209672109, 2023. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 67006709, 2019. Amirmohammad Izadi, Mohammad Ali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi, Hosein Hasani, and Mahdieh Soleymani Baghshah. Visual structures helps visual reasoning: Addressing the binding problem in vlms, 2025. URL https://arxiv.org/abs/2506.22146. 19 Susanne Jaeggi, Barbara Studer-Luethi, Martin Buschkuehl, Yi-Fen Su, John Jonides, and Walter Perrig. The relationship between n-back performance and matrix reasoningimplications for training and transfer. Intelligence, 38(6):625635, 2010. Huaizu Jiang, Xiaojian Ma, Weili Nie, Zhiding Yu, Yuke Zhu, and Anima Anandkumar. Bongard-HOI: Benchmarking few-shot visual reasoning for human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1905619065, 2022. John and Jean Raven. Raven progressive matrices. In Handbook of nonverbal assessment, pp. 223237. Springer, 2003. Alan Kaufman, Susan Engi Raiford, and Diane Coalson. Intelligent testing with the WISC-V. John Wiley & Sons, 2015. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European Conference on Computer Vision, pp. 235251. Springer, 2016. Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernández-Garcıa, Léna Néhale Ezzine, Yoshua Bengio, and Nikolay Malkin. theory of continuous generative flow networks. In International Conference on Machine Learning, pp. 1826918300. PMLR, 2023. Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. Solim LeGris, Wai Keen Vong, Brenden M. Lake, and Todd M. Gureckis. H-arc: robust estimate of human performance on the abstraction and reasoning corpus benchmark, 2024. URL https://arxiv.org/abs/2409.01374. Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. Latent visual reasoning, 2025a. URL https://arxiv.org/abs/2509.24251. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulić, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought, 2025b. URL https://arxiv.org/abs/2501.07542. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, pp. 1973019742. PMLR, 2023. Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, and Roei Herzig. Latent implicit visual reasoning, 2025c. URL https://arxiv.org/abs/2512.21218. Kevin Y. Li, Sachin Goyal, Joao D. Semedo, and J. Zico Kolter. Inference optimal vlms need fewer visual tokens and more parameters, 2025d. URL https://arxiv.org/abs/2411.03312. Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations, 2025e. URL https://arxiv.org/abs/2506. 04633. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, volume 36, pp. 3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Zhining Liu, Ziyi Chen, Hui Liu, Chen Luo, Xianfeng Tang, Suhang Wang, Joy Zeng, Zhenwei Dai, Zhan Shi, Tianxin Wei, Benoit Dumoulin, and Hanghang Tong. Seeing but not believing: Probing the disconnect between visual attention and answer correctness in vlms, 2025. URL https://arxiv.org/abs/2510.17771. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. URL https://arxiv.org/abs/2310.02255. Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, pp. 2346723483. PMLR, 2023. 20 Mikołaj Małkiński and Jacek Mańdziuk. One self-configurable model to solve many abstract visual reasoning problems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1429714305, 2024. Mikołaj Małkiński and Jacek Mańdziuk. Deep learning methods for abstract visual reasoning: survey on ravens progressive matrices. ACM Computing Surveys, 57(7):136, 2025. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question In Findings of the Association for Computational answering about charts with visual and logical reasoning. Linguistics: ACL 2022, pp. 22632279, 2022. Mikołaj Małkiński and Jacek Mańdziuk. A-i-raven and i-raven-mesh: Two new benchmarks for abstract visual reasoning, 2025. URL https://arxiv.org/abs/2406.11061. Mikołaj Małkiński, Szymon Pawlonka, and Jacek Mańdziuk. Reasoning limitations of multimodal large language models. case study of bongard problems, 2025. URL https://arxiv.org/abs/2411.01173. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. Online, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. MistralAI. Introducing mistral 3. Online, 2025. URL https://mistral.ai/news/mistral-3. Weili Nie, Zhiding Yu, Lei Mao, Ankit B. Patel, Yuke Zhu, and Animashree Anandkumar. Bongard-LOGO: new benchmark for human-level concept learning and reasoning. In Advances in Neural Information Processing Systems, volume 33, pp. 1646816480, 2020. OpenAI. Introducing gpt-5.2. Online, 2025a. URL https://openai.com/index/introducing-gpt-5-2/. OpenAI. Introducing gpt-o3 and gpt-o4-mini. Online, 2025b. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Szymon Pawlonka, Mikołaj Małkiński, and Jacek Mańdziuk. Bongard-rwr+: Real-world representations of fine-grained concepts in bongard problems, 2025. URL https://arxiv.org/abs/2508.12026. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763. PmLR, 2021. James Raven. Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive. Masters thesis, University of London, 1936. John Raven and John Hugh Court. Ravens progressive matrices and vocabulary scales. Oxford pyschologists Press Oxford, England, 1998. Eleanor Rosch. Natural categories. Cognitive Psychology, 4(3):328350, 1973. Timothy Salthouse. Influence of working memory on adult age differences in matrix reasoning. British Journal of Psychology, 84(2):171199, 1993. Luca Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models. Nature Machine Intelligence, 7(1):96106, 2025. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning, 2024. URL https://arxiv.org/abs/2403.16999. Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Latent chain-of-thought for visual reasoning, 2025. URL https://arxiv.org/abs/2510.23925. Younes Adam Tabi, Maria Raquel Maio, Bahaaeddin Attaallah, Shannon Dickson, Daniel Drew, Mohamad Imran Idris, Annika Kienast, Verena Klar, Lisa Nobis, Olivia Plant, et al. Vividness of visual imagery questionnaire scores and their relationship to visual short-term memory performance. Cortex, 146:186199, 2022. Gemma Team. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 95689578, June 2024. Shimon Ullman. Visual routines. Cognition, 18(1):97159, 1984. ISSN 0010-0277. doi: https://doi.org/10.1016/ 0010-0277(84)90023-4. URL https://www.sciencedirect.com/science/article/pii/0010027784900234. 21 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024a. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2024b. URL https://arxiv.org/abs/2311.03079. David Wechsler. Wechsler intelligence scale for children. Psychological corporation, 1949. David Wechsler. WAIS-IV Administration and Scoring Manual. PsychCorp, 2008. URL https://books.google.co.il/ books?id=Bf-DswEACAAJ. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. Max Wertheimer. Untersuchungen zur Lehre von der Gestalt. II. Psychologische Forschung, 4(1):301350, January 1923. ISSN 1430-2772. doi: 10.1007/BF00410640. URL https://doi.org/10.1007/BF00410640. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun Zhu, and Yizhou Wang. BongardOpenWorld: Few-shot reasoning for free-form visual concepts in the real world. In International Conference on Learning Representations, 2024. xAI. Grok-1.5 vision preview, 2024. URL https://x.ai/blog/grok-1.5v. xAI. Grok 4 fast model card. System card (technical report), September 2025. URL https://data.x.ai/ 2025-09-19-grok-4-fast-model-card.pdf. Siying Xie, Daniel Kaiser, and Radoslaw Cichy. Visual imagery and perception share neural representations in the alpha frequency band. Current Biology, 30(13):26212627, 2020. Jingyi Xu, Tushar Vaidya, Yufei Wu, Saket Chandra, Zhangsheng Lai, and Kai Fong Ernest Chong. Abstract visual In Proceedings of the IEEE/CVF reasoning: An algebraic approach for solving ravens progressive matrices. Conference on Computer Vision and Pattern Recognition, pp. 67156724, 2023. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1063210643, 2025a. Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, et al. Cambrian-s: Towards spatial supersensing in video, 2025b. URL https://arxiv.org/abs/ 2511.04670. Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens, 2025c. URL https://arxiv.org/abs/2506.17218. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality, 2024. URL https://arxiv.org/abs/2304.14178. Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. KiVA: Kid-inspired visual analogies for testing large multimodal models. In International Conference on Learning Representations, 2025. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it?, 2023. URL https://arxiv.org/abs/2210.01936. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 67206731, 2019. Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. benchmark for compositional visual reasoning. In Advances in Neural Information Processing Systems, volume 35, pp. 2977629788, 2022. 22 Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. RAVEN: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 53175327, 2019. Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling, 2022. URL https://arxiv.org/abs/2202.01361. Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models, 2024a. URL https://arxiv.org/abs/2401.13601. Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, and Furu Wei. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms, 2025. URL https://arxiv.org/abs/2510.24514. Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, and Navdeep Jaitly. How far are we from intelligent visual deductive reasoning?, 2024b. URL https://arxiv.org/abs/2403.04732. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models, 2024c. URL https://arxiv.org/abs/2302.00923. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. URL https://arxiv.org/abs/2304.10592. Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. survey on latent reasoning, 2025. URL https://arxiv.org/abs/2507.06203."
        }
    ],
    "affiliations": [
        "PediaMed AI",
        "University of Illinois Urbana-Champaign"
    ]
}