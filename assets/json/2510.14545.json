{
    "paper_title": "Agentic Entropy-Balanced Policy Optimization",
    "authors": [
        "Guanting Dong",
        "Licheng Bao",
        "Zhongyuan Wang",
        "Kangzhi Zhao",
        "Xiaoxi Li",
        "Jiajie Jin",
        "Jinghan Yang",
        "Hangyu Mao",
        "Fuzheng Zhang",
        "Kun Gai",
        "Guorui Zhou",
        "Yutao Zhu",
        "Ji-Rong Wen",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training."
        },
        {
            "title": "Start",
            "content": "Agentic Entropy-Balanced Policy Optimization Guanting Dong1, Licheng Bao2, Zhongyuan Wang2, Kangzhi Zhao2, Xiaoxi Li1, Jiajie Jin1, Jinghan Yang2, Hangyu Mao2, Fuzheng Zhang2, Kun Gai2, Guorui Zhou2(cid:66), Yutao Zhu1, Ji-Rong Wen1, Zhicheng Dou1(cid:66) 1Renmin University of China 2Kuaishou Technology {dongguanting, dou}@ruc.edu.cn (cid:135) GitHub: https://github.com/dongguanting/ARPO 5 2 0 2 6 1 ] . [ 1 5 4 5 4 1 . 0 1 5 2 : r Abstract Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1ùêæ RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanitys Last Exam, and 43.0% on WebWalkerQA for Pass@1; 65.0% on GAIA, 26.0% on Humanitys Last Exam, and 70.0% on WebWalkerQA for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training. Keywords Agentic Reinforcement Learning, Agentic Search, Web Agent, Tool Learning, Large Language Model"
        },
        {
            "title": "1 Introduction\nThe emergence of large language models (LLMs) have profoundly\nrevolutionized a wide range of natural language reasoning tasks [3,",
            "content": "Work done during internship at Kuaishou. (cid:66) Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. WWW 26, Dubai, UAE 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/xxxxxxx.xxxxxxx Figure 1: Performance overview of AEPO algorithm. 29, 64, 9193, 113, 119]. Despite their impressive capabilities, the static nature of their internal knowledge often leads LLMs to experience hallucinations and information staleness in knowledgeintensive scenarios [123]. Retrieval-augmented generation (RAG) addresses these limitations by empowering LLMs to reason with retrieved relevant knowledge, thereby improving the reliability of generated answers [15, 17, 31, 42, 45, 52, 87]. However, with the explosive growth of web information, the static RAG workflow limits effective interaction between LLMs and search engines, resulting in significant bottlenecks in open-domain web exploration. To overcome these challenges, series of LLM-based web agents have emerged [22, 50, 51]. These agents perform on-demand web searches during reasoning and strategically interact with external tool environments, achieving reliable, in-depth web information seeking [6, 16, 30, 40, 48, 82, 127]. To strive for efficient training of web agents, early implementations focus on distill tool-use trajectories from stronger models and guide weaker models through supervised fine-tuning (SFT) [23, 28, 46, 116]. However, relying solely on SFT struggles to discover autonomous and generalizable tool-use capability [10]. As largescale reinforcement learning with verifiable rewards (RLVR) demonstrates the potential to unlock frontier LLM capabilities [29, 91, 94], several web-search agents adopt trajectory-level RL [76, 118, 124] combined with carefully designed reward functions to elicit agentic reasoning in LLMs [13, 105]. While effective to some extent, this line of work consistently overlooks the multi-turn interactive nature between LLMs and tool environments [122], making it difficult to discover step-level tool-use behaviors during RL training. To mitigate this limitation, recent efforts in agentic RL have shown that web agents often display high entropy in their output tokens due to uncertainty about the external tool-call results [14]. Drawing on this finding, they introduce tree-structured rollout method that adaptively branches at high-entropy tool-call steps, effectively broadening sampling diversity and coverage [25, 38, 54, 57]. WWW 26, April 1317, 2026, Dubai, UAE Dong et al. Figure 2: Two high-entropy challenges in agentic RL. (1) High-Entropy Rollout Collapse: Over-branching at high-entropy steps along specific paths, limiting exploration of other potential correct branches; (2) High-Entropy Token Gradient Clipping: Consistent clipping of high-entropy token gradients during policy updates hinders learning effective exploration behaviors. Although these entropy-driven agentic RL algorithms stimulate exploration of latent tool-use behaviors, such high-entropy signals further pose two extra challenges for web agent training: (1) High-entropy Rollput Collapse: During the rollout phase, high-entropy tool-call steps often occur consecutively, leading the LLM to over-branch along single trajectory under highentropy guidance. This situation depletes the branching budget for other trajectories at high-entropy steps, ultimately limiting the diversity and scope of rollout sampling (see Figure 2 (left)). (2) High-entropy Token Gradient Clipping: The tree-structured rollout strategy encourages LLMs to explore step-level tool-use behaviors, thus preserving valuable high-entropy tokens. However, vanilla RL algorithms aggressively clip high-entropy tokens gradient during policy update phase, leading to the premature termination of the LLMs exploration (see Figure 2 (right)). Consequently, efficiently balancing entropy in agentic RL remains fundamental challenge in the pursuit of generalized agent training. To address these challenges, we propose Agentic EntropyBalanced Policy Optimization (AEPO), an entropy-balanced agentic RL algorithm designed for training multi-turn web agents. Unlike traditional entropy-driven RL approaches [14, 55], AEPO focuses on balancing and rationalizing rollout branching and policy updates under the guidance of high-entropy tool calls, thereby achieving more stable RL training. Specifically, we pioneer the quantification of two inherent entropy-driven challenges on agentic RL. Building on these insights, AEPO introduces two key algorithmic optimizations: (1) Dynamic Entropy-balanced Rollout Mechanism: To mitigate High-entropy Rollout Collapse issue, AEPO initially proposes the entropy pre-monitoring to adaptively allocate global and branch sampling budget, ensuring balanced exploration across the tree-structured rollout. Moreover, it incorporates branch penalty strategy for consecutive high-entropy tool-call steps to effectively address over-branching issues in specific chains. (2) Entropy-Balanced Policy Optimization: Draw inspiration from recent clipping-optimized works [3, 84], we intuitively integrate stop-gradient operation into the high-entropy clipping term during policy updates to tackle the High-Entropy Token Gradient Clipping. This preserves and properly rescales gradients of high-entropy tokens during backpropagation while leaving the forward pass unchanged. Furthermore, AEPO proposes entropy-aware advantage estimation, integrating entropy advantage into vanilla advantage estimation, enabling the model to prioritize learning on high-uncertainty tokens. We conduct comprehensive evaluations across 14 datasets covering deep information seeking, knowledge-intensive reasoning, and computational reasoning. As shown in Figure 1, the results show that AEPO consistently outperforms mainstream RL algorithms in generalized reasoning tasks. Remarkably, with only 1ùëò RL training samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on HLE and 43.0% on WebWalkerQA for Pass@1; and 65.0% on GAIA, 26.0% on Humanitys Last Exam and 70.0% on WebWalkerQA for Pass@5. Further analysis confirms that AEPO effectively broadens sampling diversity during rollouts while maintaining high and stable policy entropy throughout RL training, providing promising solution for developing general web agents. In summary, the key contributions are as follows: We systematically reveal two entropy-driven issues inherent to agentic RL: High-Entropy Rollout Collapse and High-Entropy Token Gradient Clipping. Through preliminary experiments, we quantify their impact on multi-turn web-agent training, offering empirical evidence for further research into entropy balancing. We propose Dynamic Entropy-Balanced Rollout mechanism, which adaptively allocates rollout sampling budgets via entropy pre-monitoring, while imposing branch penalty on consecutive high-entropy steps to prevent over-branching issues. We introduce Entropy-Balanced Policy Optimization, which intuitively integrates stop-gradient operation into the highentropy clipping term to preserve and rescale gradients on highentropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Experiments on 14 challenging benchmarks demonstrate that AEPO consistently outperforms mainstream RL algorithms in web agent training. Quantitative analyses across dimensions such as Pass@k sampling, rollout diversity, tool-call efficiency and entropy dynamics verify AEPOs strong scalability and stability, offering valuable insights for developing general web agents."
        },
        {
            "title": "2 Preliminary\nBefore introducing the AEPO algorithm, we will briefly outline key\ntask definitions and illustrate preliminary entropy-based experi-\nments to reveal key limitations of web agent RL training.",
            "content": "Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE Figure 3: Quantitative statistics of two entropy-based challenges in web agent RL training."
        },
        {
            "title": "2.1 Problem Definition\n2.1.1 Agentic Reinforcement Learning. In this section, we define\nthe training objective for agentic reinforcement learning as follows:",
            "content": "max ùúãùúÉ Eùë• D,ùë¶ùúãùúÉ (ùë• ;ùëá ) (cid:2)ùëüùúô (ùë•, ùë¶) (cid:3) ùõΩ DKL [ùúãùúÉ (ùë¶ ùë•;ùëá ) ùúãref (ùë¶ ùë•;ùëá ) ] , (1) Here, ùëá is the available tool set, ùúãùúÉ and ùúãref denote the policy and the reference LLM. The symbols ùëüùúô represent the reward functio. The input ùë• is drawn from the dataset D, and ùë¶ is the corresponding output contain tool-call results. 2.1.2 Token Entropy Calculation. Building on recent studies in entropybased RL efforts [9, 101, 103, 125], we determine the entropy of token generation at step ùë° using the formula: ùëâ ùêªùë° = ùëó =1 ùëùùë°,ùëó log ùëùùë°,ùëó , where ùíëùë° = ùúãùúÉ ( R<ùë° , ùë•;ùëá ) = Softmax (cid:16) ùíõùíï ùúè (cid:17) . (2) In this context, ùëâ represents the size of the vocabulary, ùíõùíï Rùëâ are the logits before applying the softmax function, and ùúè is the temperature parameter for decoding. This entropy quantifies the level of uncertainty in the distribution of token generation."
        },
        {
            "title": "2.2 Entropy-based Pilot Experiments\nIn this section, we delves into two high-entropy challenges in web agent\ntraining and quantifies their limitations.",
            "content": "2.2.1 Problem-1: High-Entropy Rollout Collapse. We select ARPO [14] as the backbone algorithm, representative entropy-guided agentic RL method, and train with its default 1ùëò training samples. We further quantify: (i) the steps in each sampled trajectory that exhibit consecutive high-entropy tool usage; (ii) within each rollout batch (branching budget is 8), the number and probability of trajectories that contain high-entropy branches. As shown in Figure 3 (left), our key findings are: (i) High-entropy toolcall turns exhibit transitivity: the proportion of consecutive high-entropy tool-call turns (56.5%) exceeds isolated high-entropy turns (43.5%), with trajectories reaching up to 6 consecutive high-entropy turns. This indicates that high-entropy tool-call rounds often occur consecutively. (ii) Rollout branch collapse: 93.4% of branches concentrate on 13 trajectories, while the remaining trajectories receive virtually no budget for high-entropy branch sampling. This shows an imbalanced allocation of rollout branching resources. We argue two observations are tightly coupled: Due to an excessive number of consecutive high-entropy rounds in specific samples, the model tends to over-branch on few trajectories during the rollout phase. We define this issue as the High-Entropy Rollout Collapse. 2.2.2 Problem-2: High-Entropy Token Gradient Clipping. Under the same setup as previous experiment, we further visualize the policy update phase of ARPO, including (i) the importance sampling ratio of tokens in trajectories;1 (ii) comparison of the Top-10 gradient-clipped tokens between ARPO and DAPO during training step, with clipping thresholds of 0.2 and 0.28. As illustrated in Figure 3 (right), we identify the following insights: (i) Consistent with findings in single-turn RL efforts [9, 58], tokens related to logical transitions, connections, and reflections typically exhibit high entropy. Beyond this, specific tool-call tokens also show high entropy. These tokens are highly functional and have low contextual dependency, incentivizing the model to explore diverse reasoning paths and tool-use patterns. (ii) Vanilla RL method uniformly clip the gradients of high-entropy tokens without distinguishing whether they include valuable exploratory behaviors. Although DAPO adopt clip-higher strategy [118] to alleviates this by increasing the threshold, the clipping distribution remains similar and the clipped token count is still substantial. Moreover, we empirically find that significant gradient clipping emerges in the very first policy update, resulting in lack of gradient support for high-entropy exploratory tokens in early training. This leads to fixed paradigmatic reasoning, hindering the LLM to explore tooluse patterns. We define this issue as the High-entropy Token Gradient Clipping."
        },
        {
            "title": "3 Methodology\nThis section introduces Agentic Entropy-Balanced Policy Optimization (AEPO),\nan agentic RL algorithm proposed to balance entropy during both the rollout\nand policy update phases. As shown in Figure 4, AEPO comprises two core\ncomponents:",
            "content": "(1) Dynamic Entropy-Balanced Rollout: To mitigate the High-Entropy Rollout Collapse identified in pilot experiments (2.2), we adaptively allocate the sampling budget between global and branch sampling via 1The token-level importance sampling ratio correlate positively with entropy in RL WWW 26, April 1317, 2026, Dubai, UAE Dong et al. Figure 4: The overview of Agentic Entropy-Balanced Policy Optimization. entropy pre-monitoring (3.1.1), and penalize consecutive high-entropy tool-call steps during rollout to avoid over-branching (3.1.2). (2) Entropy-Balanced Policy Optimization: To further address HighEntropy Token Gradient Clipping, we insert stop-gradient operation into the clipping term to preserve and properly rescale gradients on highentropy tokens (3.2.1), while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens (3.2.2). Below, we will we delve into the specifics of our approach."
        },
        {
            "title": "3.1 Dynamic Entropy-Balanced Rollout\nIn this section, we address the ‚ÄúHigh-Entropy Rollout Collapse‚Äù by naturally\nbreaking it down into two sub-goals: (1) Providing more reasonable re-\nsource allocation for global and branch sampling; (2) Penalizing continuous\nhigh-entropy branch sampling within single trajectories. Consequently, we\npropose the following two algorithmic solutions.",
            "content": "3.1.1 Entropy Pre-Monitoring. Traditional tree-based rollout empirically allocate resources for global and branch exploration without theoretical support [14, 38, 55, 114]. Inspired by information bottleneck theory [96], we advocate the allocation of global and partial branching exploration resources from the perspective of maximizing information gain. Specifically, given total rollout sampling budget of ùëò, which includes ùëö global samples and ùëò ùëö high-entropy partial branch samples, we simply model the sampling information gain ùêºGain per rollout step as: ùêºGain = ùëö ùêºroot (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) Gloabal + (ùëò ùëö) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) Partial (cid:124) ùêºtool. (3) Here, ùêºroot and ùêºtool represent the information gain from the input question and external tool-call result. In the autoregressive decoding process of language model, the information gain of the question is typically measured by the token entropy decoded by the model, with informative questions generally leading to greater uncertainty [8, 130]. Therefore, we derive the following positive correlation: ùêºGain ùëö ùêªroot (cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) Gloabal , where ùêª avg + (ùëò ùëö) ùêª avg tool (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) Partial (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) tool = 1 ùëÅ ùëÅ ùëñ=1 ùêª ùëñ tool, (4) where ùêªroot and ùêª ùëñ represent the entropy of the question and the entropy introduced by the ùëñ-th tool call, respectively. Based on the formula, we tool reveal that: (1) When ùêªroot ùêª ùëéùë£ùëî > 0, the uncertainty from the initial question surpasses that from the subsequent tools. In this case, we should increase ùëö to enhance global exploration, thereby boosting the information gain ùêºGain. (2) Conversely, when ùêªroot ùêª ùëéùë£ùëî < 0, ùëö should be decreased to allocate more budget to branch exploration via tool calls. tool tool Based on the above theoretical analysis, we propose the entropy premonitoring phase. As shown in Figure 4(a), we first allow the LLM to generate complete tool-integrated trajectory for the input ùëû. Following ARPOs entropy calculation [14], we compute the question and tool average entropies for each trajectory according to Equation (4), forming the entropy matrices ùêªroot and ùêª ùëéùë£ùëî tool R1ùëò . Subsequently, by comparing the ùêªroot and ùêªtool, we dynamically determine the global sampling count ùëö as: ùëö = ùëò ùúé (cid:16) ùõΩ (ùêªroot ùêª ùëéùë£ùëî tool ) (cid:17) , (5) where ùúé (ùë• ) is the sigmoid function, and ùõΩ controls sensitivity. The value of ùëö is positively correlated with the entropy gap between the question and the tools. As result, AEPO dynamically allocates rollout sampling resources, thereby enabling efficient sampling. 3.1.2 Entropy-Balanced Adaptive Rollout. After entropy pre-monitoring, we introduce the main entropy-balanced adaptive rollout phase to penalize consecutive high-entropy branch sampling, which comprises three core steps: (1) Entropy Variation Monitoring: Following resource allocation from the pre-monitoring phase, the LLM generates ùëö global trajectory-level samples for the query ùëû, recording the initial entropy matrix ùêªroot for each trajectory. After each tool-call step ùë° , the real-time entropy of the models output is continuously monitored and represented as step-level entropy matrix ùêªùë° R1ùëò . The standardized entropy variation relative to the initial entropy is then computed as Œîùêªùë° = Normalize(ùêªùë° ùêªroot ), where the normalization involves dividing the sum of all values in Œîùêª by the vocabulary size ùëâ . (2) Entropy-Balanced Beaming: Unlike traditional entropy-guided branch sampling [14, 126], AEPO promotes adaptive exploration that showcases beneficial entropy changes in tool-call steps while also constraining consecutive high-entropy branch sampling in specific chains. As shown in Figure 4(b), we introduce consecutive branch penalty strategy. Given tool-call step ùë° , the number of consecutive high-entropy branches ùëô prior to Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE step ùë° for each chain is tracked, then defining the branch sampling probability at step ùë° as follows: ùëÉùë° = (ùõº + ùõæ Œîùêªùë° ) (1 ÀÜùëÉ (ùëô ) ), where ùõº is the base sampling probability and ùõæ is the entropy stabilization factor. ÀÜùëÉ (ùëô ) is linear function related to ùëô. ùëÉùë° decreases as the number of consecutive branching steps ùëô increases, implementing consecutive branching penalty. This design makes the tree-structured rollout sampling more diverse, allowing for more comprehensive coverage of the problem-solving space. We then define the rollout branching action at step ùë° as: (6) Action(ùëÉùë° ) = (cid:40)Branch(ùëç ), Continue, if ùëÉùë° > ùúè, otherwise. (7) When ùëÉùë° exceeds the predetermined threshold ùúè, Branch(ùëç ) is initiated, creating ùëç partial braching reasoning paths from the current node; otherwise, the current trajectory continues. (3) Termination Conditions: Finally, our iterative rollout process terminates when one of the following conditions is met: (a) If the total number of branch paths ùëç reaches the partial sampling budget ùëò ùëö, branching stops and sampling continues until the final answer is generated; (b) If all paths terminate before reaching ùëò ùëö, then ùëò ùëö ùëç additional trajectory-level samples are added to satisfy condition (a). Through the dynamic entropy-balanced rollout, AEPO ensures the diversity of sampling branches while adaptively allocating exploration resources, thus addressing the \"High-entropy Rollout Collapse\" issue. The algorithm of dynamic entropy-balanced rollout is detailed in Algorithm 1."
        },
        {
            "title": "3.2 Entropy-Balanced Policy Optimization\nAEPO preserves a considerable number of exploratory tokens via entropy-\nbalanced rollouts, presenting a challenge in effectively updating these to-\nkens‚Äô gradients. This section aims to improve targeted learning for these\ntokens by implementing the following designs:",
            "content": "3.2.1 Entropy Clipping-Balanced Mechanism. Unlike traditional methods that entirely discard gradients outside the clipping range [14, 76], AEPO introduces an innovative high-entropy clipping-balanced mechanism. The core idea is to retain high-entropy gradients that exceed the clipping interval, allowing the model to learn valuable exploratory token signals. Motivated by GPPO [84], we integrate stop-gradient operation into the high-entropy clipping term of the policy update phrase, decoupling forward and backward propagation. Our mechanism ensures that forward propagation remains unchanged, while protecting the gradient backward of high-entropy tokens from clipping constraints. For instance, in GRPO [76], given an input question ùë• and policy model ùë¶, GRPO enables the reference policy ùúãref to generate group of ùê∫ outputs {ùë¶1, ùë¶2, . . . , ùë¶ùê∫ } and optimizes the policy by maximizing: ùê∫ ùëáùëó (cid:19) ùëó = ùë° =1 min = Eùë• 1 + ùúñ‚Ñé sg(ùõø ) (cid:18) ùõø, 1 ùúñùëô , (cid:18) ùõø ùê¥ (ùë° ) , clip 1 (cid:205)ùê∫ ùëó =1 ùëáùëó (cid:19) (8) where ùõø = ùëü ( ùëó ) (ùúÉ ) represents the importance sampling ratio, and ùë†ùëî () denotes the stop-gradient operation. It is noteworthy that the value of the term ùõø ùë†ùëî (ùõø ) always equals 1, ensuring that AEPOs forward computation remains unchanged. For the backpropagation, AEPOs gradient update process is formulated as: ùê¥(ùë° ) ùõø , ùë° ùúÉ = Eùë• (cid:20) 1 ùëó =1 ùëáùëó (cid:205)ùê∫ ùëó =1 (cid:205)ùëáùëó (cid:205)ùê∫ ùë° =1 Fùëó,ùë° (ùúÉ ) ùúôùúÉ (cid:0)ùëé ùëó,ùë° , ùë† ùëó,ùë° (cid:1) ùê¥ (ùë° ) (cid:21) , where Fùëó,ùë° (ùúÉ ) = 1 + ùúñ‚Ñé, 0, ùõø, if ùõø > 1 + ùúñ‚Ñé and ùê¥ (ùë° ) > 0, if ùõø < 1 ùúñ‚Ñé and ùê¥(ùë° ) < 0, otherwise . (9) Algorithm 1 Dynamic Entropy-Balanced Rollout Require: Reasoning model ùúãùúÉ , external tools ùëá , total rollout size ùëò, entropy sensitivity ùõΩ, branch penalty slope ùõæ 1: Input: Dataset ùê∑ 2: Initialize reference model: ùúã old 3: for ùëñ = 1 to do 4: Sample mini-batch ùê∑ùëè ùê∑ for each query ùëû ùê∑ùëè do ùúÉ ùúãùúÉ // Entropy Pre-Monitoring Generate 1 complete trajectory ùëü to obtain ùêªroot and ùêª avg tool Global rollout size ùëö ùëò ùúé(cid:0)ùõΩ (ùêªroot ùêª avg Branch rollout size ùëè ùëò ùëö // Entropy-Balanced Adaptive Rollout Initialize rollout pool Consecutive-high-entropy counter ùëô 0 while < ùëö do tool)(cid:1) Sample trajectory ùëü ; add to end while while ùëè > 0 and ùëü ùëó not terminated do // (1) Entropy Variation Monitoring Select trajectory ùëü at tool-call step ùë° Œîùêªùë° Normalize(ùêªùë° ùêªinitial) // (2) Entropy-Balanced Beaming Consecutive penalty ÀÜùëÉ (ùëô) ùõæ ùëô Branch probability ùëÉùë° (ùõº + ùõΩŒîùêªùë° )(1 ÀÜùëÉ (ùëô)) // (3) Termination Conditions if ùëÉùë° > ùúè then Branch ùëç sub-trajectories; ùëè ùëè ùëç else ùëô ùëô + 1 if Œîùêªùë° > 0 end if end while if ùëè > 0 then Sample ùëè additional trajectories and add to 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: end if end for 33: 34: end for 35: Output: rollout trajectory set During backpropagation, the gradient of high-entropy token is retained and appropriately rescaled to 1 + ùúñ‚Ñé only when ùõø > 1 + ùúñ‚Ñé and ùê¥ (ùë° ) > 0. In other cases, the gradient update rule aligns with vanilla clipping mechanism of GRPO. This controlled rescaling ensures that the model learns balanced exploratory behavior without completely ignoring high-entropy tokens. To more clearly articulate the theoretical aspects of AEPO compared to clippingoptimized RL methods [3, 85], we discuss their differences in Appendix 2. 3.2.2 Entropy-aware Advantage Estimation. Owing to the clippingbalanced mechanism, we retain the gradients of high-entropy tokens. However, challenge arises in training the model to better distinguish between exploratory and non-exploratory tokens. Traditional outcome-based RL algorithms assign the same advantage to all tokens in sequence based on the answer correctness, neglecting the models confidence levels across different tokens [35, 76]. To this end, we propose an entropy-aware advantage estimation that incorporates token entropy calculation into advantage shaping. This approach allows the model to assign greater rewards to exploratory tokens 2For detailed proof of the gradient form of AEPO, please refer to Appendix WWW 26, April 1317, 2026, Dubai, UAE Dong et al. that are correct but exhibit high uncertainty. natural way is to calculate an accuracy-based advantage while integrating an entropy-based advantage term, defined as follows: ùëüùë° mean (cid:16) std (cid:16) ùêªùë° mean (cid:16) std (cid:16) {ùëÖùëñ }ùê∫ ùëñ=1 (cid:17) {ùêªùë° }ùëá (cid:17) Œîùêª = Acc = ùê¥ (ùë° ) ùê¥ (ùë° ) (10) ùë° =1 (cid:17) (cid:17) , , {ùêªùë° }ùëá ùë° =1 {ùëÖùëñ }ùê∫ ùëñ=1 where ùêªùë° represents the ùë° -th token entropy, and ùëá is the total number of tokens across all trajectories in the group. . We estimate the entropy advantage for each token based on the average token entropy within the same trajectory. Furthermore, we treat the entropy advantage as regularization term in the advantage estimation to reshape ùê¥acc as: (cid:17) Acc ùê¥ (ùë° ) Œîùêª ùê¥ (ùë° ) = ùê¥(ùë° ) (cid:16)1 + ùëé This step is computed before the policy update. Notably, our entropyaware advantage estimation can be seamlessly integrates with existing agentic RL algorithms to further enhance the models emphasis on learning exploratory tokens during training. The full algorithm of AEPO is detailed in Algorithm 2. (11) ."
        },
        {
            "title": "4 Experiment Settings\n4.1 Datasets\nWe assess the effectiveness of AEPO in web agents RL training through\nthree long-term reasoning tasks:\n(1) Deep Information Seeking Tasks: This includes challenging evalua-\ntions for web agents: General AI Assistant (GAIA) [60] and the Human\nLast Exam (HLE) [66], as well as deep information seeking: WebWalk-\nerQA [107], XBench [5], and Frames [43].",
            "content": "(2) Knowledge-Intensive Reasoning: This covers 3 multi-hop complex open-domain question-answering tasks: 2WikiMultihopQA [33], Musique [97], and Bamboogle [67], along with the web multi-hop task WebWalkerQA [108]. (3) Computational Reasoning: This includes simple math reasoning tasks like GSM8K[11], MATH [32], and competition-level math challenges: MATH500 [56], AIME2024, and AIME2025.3 All dataset splits align with the standard settings established by previous works [14, 41, 51]."
        },
        {
            "title": "4.2 Baselines\nWe consider the following strong baseline methods:\n(1) Advanced RL Algorithms: We select three categories of RL algorithms:\n(1) Vanilla RL: GRPO[76] and Reinforce++[35]; (2) Clipping-optimized RL:\nDAPO[118], CISPO[61] and GPPO[84]; and (3) Agentic RL: GIGPO[26]\nand ARPO[14].",
            "content": "(2) Advanced Backbone Models: For challenging reasoning benchmarks, we evaluate the instruction-tuned versions of Qwen2.5[74] and Llama3.1[18]. For deep information seeking tasks, we also report results for QwQ[95], DeepSeek-R1[29], GPT-4o[37], and o1-preview[37], using Qwen3-32B[113] as reference. (3) Advanced Web Agents: We introduce series of open-source workflowbased search agents as references, including vanilla RAG[44], Search o1[50], Webthinker[51], and ReAct[115]. The detailed introduction of baselines are listed in Appendix C"
        },
        {
            "title": "4.3 Evaluation Metric\nConsistent with previous work, we use the F1 score to evaluate four question-\nanswering tasks that require intensive knowledge reasoning. For other tasks,\nwe employ the VLLM framework to serve Qwen2.5-72B-instruct, using LLM-\nas-Judge to assess the answers. In all tasks, answers are extracted from the\nbox in the respons. By default, the temperature is set to 0.6 and top-p to\n0.95, and we evaluate using the Pass@1 score.",
            "content": "3https://huggingface.co/datasets/AI-MO/aimo-validation-aime"
        },
        {
            "title": "4.4 Implementation Details\nIn the AEPO phase, we implement the AEPO algorithm using the VERL\nframework [77], excluding tool-call results from the loss calculation to avoid\nbias. Our setup includes a training batch size of 128, a PPO mini-batch size\nof 16, and a context length of 20K. For AEPO rollout, the global rollout size\nis 16, with ùëé, ùõΩ set to 0.2. Resource allocation follows Equation 5, with a\nconsecutive branch penalty probability of ùëÉ (ùëô ) = 0.2 ¬∑ ùëô. Other settings align\nwith ARPO for fair comparison. To stabilize RL training, the KL divergence\ncoefficient in GRPO is set to 0. The RL for reasoning and deep information\nseeking is 2 and 5 epochs. All experiments use 16 NVIDIA H800 GPUs.",
            "content": "During training and evaluation, we use the Bing Search API (US-EN region) as the search engine. Following RAG-related work [41, 51], we retrieve 10 web pages per query. For reasoning tasks, we use the top 10 snippets; for deep information seeking, we extract up to 6000 tokens per page and use same size model as browser agent."
        },
        {
            "title": "5 Experiment Results\n5.1 Main Result on Deep Information Seeking\nTo validate the effectiveness of AEPO in challenging deep web information\nseeking tasks, we train the Qwen3 series models combined with AEPO using\n1ùêæ open-source samples and compared them with advanced web agents\nand RL algorithms. As shown in Table 1, we derived the following insights:\n(1) Limitations of Advanced Large Models: Both advanced closed-\nsource LLMs and large-parameter open-source LLMs (e.g. GPT-4o and\nDeepSeek-R1-671B) perform poorly in challenging deep information seek-\ning scenarios, particularly on the GAIA (<30%) and HLE (<10%) benchmarks.\nThis indicates that relying solely on model internal knowledge is insufficient\nfor complex agentic search tasks.",
            "content": "(2) Strong Generalization Ability of AEPO in Deep Information Seeking: Compared to robust web agents and advanced RL algorithms, the Qwen3-8B and 14B models combined with AEPO demonstrate exceptional performance, achieving pass@1 scores of 11.2%, 47.6% and 43% on the HLE, GAIA and WebWalkerQA benchmarks, respectively. Notably, our model is trained solely on 1k samples from an open-source web search dataset, without any data synthesis or filtering, showcasing its efficiency in training web agents. (3) Importance of Dual Entropy Balancing Optimization: AEPO consistently outperforms ARPO in both average performance and individual benchmarks, with Qwen3-8B showing significant 6% improvement on the GAIA benchmark and WebWalkerQA. This highlights the importance of AEPOs algorithmic design, which implements dual entropy balancing in both the Rollout and policy update phases, effectively facilitating LLMs exploratory tool behavior and addressing two high-entropy challenges. This is crucial for deep information seeking scenarios involving frequent tool invocation."
        },
        {
            "title": "5.2 Main Result on Generalized Reasoning\nTo further validate the effectiveness of AEPO in web agent training, we\nconduct a comparision of AEPO with 7 RL algorithms across 10 challenging\nreasoning tasks. As shown in Table 2, our key insights are as follows:",
            "content": "(1) Instability of Clipping RL Algorithms in Web Agent Training: Using GRPO as baseline, clipping-optimized RL algorithms perform well on Qwen 2.5-7B-instruct, with GPPO and CISPO achieving average scores above 57%. However, in Llama3-8B, they do not show significant improvement over GRPO. Furthermore, practical experiments reveal that clipping-optimized RL algorithms often lead to entropy collapse, disrupting training performance. This indicates that clipping-optimized RL algorithms are sensitive to the architecture of the backbone model and often show instability during web agent training. (2) Generalization Ability of Agentic RL Algorithms: Agentic RL algorithms, represented by ARPO and GIGPO, demonstrate stable and robust Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE Table 1: Overall performance on deep information seeking tasks. The best results are indicated in bold, and the second-best results are underlined. Results from larger or closed-source models are presented in gray for reference. Method General AI Assistant WebWalkerQA Humanitys Last Exam XBench-DR FRAMES Lv. Lv.2 Lv.3 Avg. Easy Med. Hard Avg. NS CE SF Avg. Avg. Avg. Direct Reasoning (>=32B) Qwen3-32B-thinking DeepSeek-R1-32B QwQ-32B GPT-4o DeepSeek-R1-671B o1-preview 26.2 21.5 30.9 23.1 40.5 - 12.1 13.6 6.5 15.4 21.2 - Single-Enhanced Method (Qwen3-8B) 15.4 Vanilla RAG 15.4 Search-o1 11.5 WebThinker 17.3 ReAct 28.2 35.9 43.6 35.9 RL-based Method (Qwen3-8B) 28.1 Qwen3-8B 48.7 + GRPO 53.9 + ARPO + AEPO (Ours) 61.5 15.4 25.0 32.7 42.3 Single-Enhanced Method (Qwen3-14B) Vanilla RAG Search-o1 WebThinker ReAct 19.2 23.1 26.9 25.0 38.5 48.7 48.7 48.7 RL-based Method (Qwen3-14B) Qwen3-14B + GRPO + ARPO + AEPO (Ours) 33.3 51.3 56.4 61.5 13.5 34.6 40.4 44.2 0 0.0 5.2 8.3 5.2 - 16.7 0.0 0.0 8.3 16.7 8.3 16.7 8.3 8.3 0.0 8.3 8.3 0.0 0.0 16.7 16.7 14.9 14.2 18.9 17.5 25.2 - 20.4 21.4 22.3 23. 20.4 32.0 38.8 45.6 25.2 30.1 33.0 32.0 19.4 36.9 43.7 47.6 6.9 7.5 7.5 6.7 5.0 11.9 8.9 6.7 6.7 8.9 0.0 28.9 31.1 40. 17.8 11.1 13.3 11.1 6.7 35.6 40.0 40.0 1.1 1.4 2.1 6.0 11.8 10.4 10.7 15.5 13.1 16.7 2.4 32.1 35.7 39.3 13.1 21.4 23.8 20. 2.4 42.9 44.1 50.0 2.9 4.2 4.6 4.2 11.3 7.9 9.9 9.7 16.9 18.3 2.8 28.2 28.2 35.2 11.3 16.9 18.3 12.7 4.2 35.2 36.6 40. 3.1 3.8 4.3 5.5 10.0 9.9 10.0 11.5 13.0 15.5 2.0 30.0 32.0 38.0 13.5 17.5 19.5 15.5 4.0 38.5 40.5 44.5 14.6 6.6 11.5 2.7 8.5 12. 5.1 7.6 7.3 4.2 3.9 7.9 7.3 12.1 5.5 6.4 7.0 5.8 5.5 7.9 10.3 10.6 9.8 5.1 7.3 1.2 8.1 8.1 1.6 2.7 4.0 4. 2.7 4.0 6.7 5.3 6.3 4.0 4.0 5.3 6.7 6.7 10.7 14.7 8.4 6.5 5.2 3.2 9.3 6.6 12.9 5.3 6.3 6.3 8.4 10.5 15.8 11. 9.4 10.5 9.5 10.5 11.6 12.6 13.7 10.5 12.6 6.4 9.6 2.6 8.6 11.1 5.8 6.4 6.6 4.6 4.6 7.8 8.8 11.0 6.0 6.8 7.0 6. 6.8 8.6 10.0 11.2 14.0 10.0 10.7 18.0 32.7 - 8.0 10.0 13.0 16.0 9.0 20.0 25.0 28.0 15.0 21.0 23.0 20.0 14.0 27.0 32.0 35. 26.0 23.8 28.8 44.6 45.6 - 18.8 19.2 21.4 21.1 19.0 46.2 47.8 50.2 31.4 39.8 40.8 37.6 23.8 54.6 55.4 58.8 Figure 5: The comparison analysis of Qwen3-14B using ARPO and AEPO across Pass@1 to Pass@5 metrics. performance across both backbone models, with ARPO achieving average performance consistency above 55%. Notably, these methods attempt tree-structured rollout during the rollout phase, further confirming the effectiveness of branching exploration in high-entropy tool-call steps. (3) Effectiveness of AEPO: AEPO consistently outperforms other reinforcement learning algorithms across 10 datasets and backbone models, achieving an average accuracy improvement of nearly 5% over GRPO while maintaining competitiveness across fine-grained domains. These results highlight AEPOs efficiency and strong adaptability across different model architectures and tasks, making it more suitable than other RL algorithms for training multi-turn web agents."
        },
        {
            "title": "5.3 Pass@K Sampling Analysis\nDue to the dynamic multi-turn interactions and complexity of tool environ-\nments in web agent training, we conduct a sampling analysis of the model‚Äôs\nPass@3 and Pass@5 to accurately assess its true problem-solving abilities.\nAs illustrated in Figure 5, AEPO demonstrates significant performance\nimprovements with larger-scale sampling. Notably, the Qwen3-14B model\ncombined with AEPO achieves remarkable results: GAIA at 65%, HLE at 26%,\nand XBench-DR at 65%. Compared to the robust agentic RL algorithm ARPO,\nAEPO consistently excels across five datasets. This stable improvement in\nPass@K can be primarily attributed to AEPO‚Äôs entropy balancing optimiza-\ntions, which allows the model to explore fine-grained tool usage behaviors\nmore efficiently, thereby enhancing reasoning and sampling efficiency.",
            "content": "WWW 26, April 1317, 2026, Dubai, UAE Dong et al. Table 2: Overall performance on ten challenging reasoning tasks are presented. The top two outcomes are bolded and underlined. Method Mathematical Reasoning Knowledge-Intensive Reasoning AIME24 AIME25 MATH500 GSM8K MATH WebWalker HotpotQA 2Wiki. MuSiQue. Bamboogle Classical RL Method + GRPO + Reinforce ++ 13.3 13.3 Clipping-optimized RL Method 16.7 16.7 13.3 + DAPO + GPPO + CISPO Agentic RL Method + GIGPO + ARPO + AEPO (Ours) Classical RL Method + GRPO + Reinforce ++ 20.0 23.3 26.7 23.3 26. Clipping-optimized RL Method 20.0 26.7 26.7 + DAPO + GPPO + CISPO Agentic RL Method + GIGPO + ARPO + AEPO (Ours) 30.0 30.0 33.3 13.3 16. 13.3 6.7 6.7 13.3 16.7 16.7 26.7 23.3 23.3 23.3 30.0 20.0 30.0 30.0 Backbone Model: Llama3.1-8B-Instruct 62.4 61.4 61.2 61.8 62.2 62.4 64.6 65.8 87.4 87.0 87.4 86.6 87.0 87.4 88.0 87. 79.2 77.2 76.4 79.4 78.2 77.2 80.2 80.6 26.5 27.5 25.5 27.5 26.0 31.5 30.5 33. Backbone Model: Qwen2.5-7B-Instruct 78.0 78.0 80.4 76.2 77.8 78.4 78.8 80.4 92.8 92.2 91.0 91.6 91. 91.6 92.2 92.2 87.8 88.8 88.8 87.6 86.2 87.6 88.8 90.0 22.0 26.0 24.0 31.0 29. 30.5 26.0 31.5 57.8 57.1 56.6 61.8 57.3 61.8 65.4 64.7 59.0 55.1 57.7 60.7 59. 58.1 58.8 62.5 71.8 71.6 70.3 72.8 75.6 74.6 75.5 79.0 76.1 68.9 68.4 74.2 72. 73.5 76.1 77.1 31.0 29.9 29.2 29.8 32.2 31.8 34.8 33.0 30.6 25.2 28.6 31.5 29. 31.1 31.1 31.1 68.2 69.1 67.3 71.9 71.8 72.1 73.8 75.8 68.4 64.9 65.5 72.4 70. 70.1 71.5 73.4 Avg. 51.1 51.1 50.4 51.5 51.0 53.2 55.3 56.3 56.5 54. 54.8 57.5 57.2 57.1 58.3 60.1 Figure 6: Visualization of Rollout diversity: ARPO (left) and AEPO (right)"
        },
        {
            "title": "5.4 Does AEPO Mitigate Rollout Collapse?",
            "content": "(1) Diversity Analysis. To investigate whether AEPOs dynamic entropybalanced rollout improves sampling diversity, we follow the setup of the preliminary experiment (2.2) and randomly selected samples from 10 rollout steps, encompassing 640 distinct problems and approximately 7.6ùëò trajectories. We further employ BGEM3 [4] as the semantic embedding model, applied the PCA method for dimensionality reduction, and used DBSCAN [19] for clustering to visualize the representation of rollout sampling. As shown in Figure 6, the results indicate that compared to ARPO, AEPOs sampling trajectories form more distinct cluster centers (54 vs. 62) and exhibit tighter intra-cluster distances with larger inter-cluster gaps. This demonstrates that AEPO improves the scope of rollout diversity and provides clearer differentiation in the sampling path distribution. We attribute this to AEPOs entropy pre-monitoring and continuous entropy Figure 7: The comparison of branch sampling distribution in rollout (left); The comparison of tool-call efficiency across four RL algorithms (right). penalty branches, which effectively address the continuity of high-entropy branches to achieve comprehensive coverage of the problem-solving space. (2) Statistics Analysis. To quantitatively analyze AEPOs effectiveness in addressing rollout collapse, we measure the branch distribution of ARPO and AEPO over 10 steps during rollout. As shown in Figure 7 (left), with both the global and partial branch sampling budgets set to 8, ARPO typically branches into 2-3 trajectories. In contrast, AEPO exhibits more diverse branching pattern, potentially covering all 8 paths with different branches. This highlights AEPOs dynamic resource allocation and continuous branch penalty mechanism enable the model to explore potential high-entropy tool-call steps across different trajectories, effectively mitigating bias in specific path branches. Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE Research [21, 47, 72, 83, 90, 106, 109, 121] aims to fully leverage the posttraining paradigm. This includes data synthesis, RL algorithm optimization, and report generation, thereby broadening the scope of web agent training. To minimize resource consumption during training, another line of research seeks to simulate search engines using the generative capabilities of large models for self-alignment [20, 86]. Recently, agentic RL methods [14, 26, 34, 55] have focused on optimizing foundational RL algorithms for web agents, employing tree-structured rollouts for autonomous branch sampling under high entropy. While these methods have advanced web agent training, they often overlook the challenges posed by high-entropy tokens. Several single-turn RL studies [9, 58, 85, 126] have emphasized that stable entropy training is crucial for enhancing model performance. However, this aspect remains largely unexplored in multi-turn agentic RL. In this paper, we introduce AEPO to achieve entropy-balanced web agent RL training."
        },
        {
            "title": "6.2 Agentic Reinforcement Learning.\nReinforcement learning (RL) plays a crucial role in helping large language\nmodel (LLM) agents adapt to dynamic and open environments [59, 60, 78].\nFoundational studies such as DQN [62] and AlphaZero [79] have shown\nthat self-play-based RL can endow agents with skills ranging from natural\nlanguage understanding to strategic gameplay [63]. Building on these foun-\ndations, value-based RL methods have been applied to improve embodied\nintelligence in hardware control and complex gaming tasks [1, 65, 75, 89,\n102, 120]. Recent advancements, like RAGEN [104, 129], incorporate reason-\ning states and environmental interactions into turn-level responses using\ntrajectory-level RL. To enhance tool-integrated reasoning, several stud-\nies [6, 24, 24, 40, 40, 51, 53, 80, 81, 86] utilize rule-based RL to enable LLMs\nto autonomously invoke external tools (e.g., search engines, Python compil-\ners) to improve reasoning accuracy. Further research, including ToolRL [68],\nTool-Star [13], and OTC [99], explores the integration of multiple tools and\nenhances tool-use efficiency. Efforts by Kimi Deepresearcher 4 and Web-\nsailor [48] focus on optimizing RL algorithms to better handle deepsearch‚Äôs\nlong context scenarios. With the surge in reasoning capabilities of Mul-\ntimodal large language models (MLLMs), several works have effectively\nbroadened the scope of this field by combining agentic RL in the multimodal\ndomain with external tools [2, 69‚Äì71, 73, 100, 111].",
            "content": "Although many studies enhance tool invocation through reward shaping and rollout mechanisms, trajectory-level RL alone often struggles to effectively capture the multi-turn, long-horizon characteristics of LLM-based agent behavior. This challenge has led to the development of ARPO, which aims to learn step-level tool-use behavior patterns."
        },
        {
            "title": "7 Conclusion\nIn this paper, we introduce Agentic Entropy-Balanced Policy Optimization\n(AEPO), an agentic RL algorithm that effectively balances entropy during\nboth rollout and policy update phases. Initially, we quantify two inherent\nentropy-driven challenges in preliminary experiments. AEPO comprises\ntwo core components: (1) a dynamic entropy-balanced rollout mechanism\nthat adaptively allocates the sampling budget between global and branch\nsampling through entropy pre-monitoring, while imposing a branch penalty\non consecutive high-entropy tool-call steps to prevent oversampling; (2)\nEntropy-Balanced Policy Optimization, which incorporates a stop-gradient\noperation in the high-entropy clipping term to preserve and rescale gradi-\nents on high-entropy tokens, alongside entropy-aware advantage estimation\nto focus learning on high-uncertainty tokens. Experiments across 14 bench-\nmarks demonstrate that AEPO consistently outperforms seven mainstream\nagentic RL algorithms. Quantitative analyses confirm its scalability and\nstability, offering valuable insights for training general web agents.",
            "content": "4https://moonshotai.github.io/Kimi-Researcher/ Figure 8: Visualization of training dynamics, including entropy loss(left) and accuracy (right) across training steps"
        },
        {
            "title": "5.5 Does AEPO Achieve Entropy-Balanced and",
            "content": "Efficient RL Training? (1) Tool-call Efficiency Analysis. In agentic RL training, effectively controlling the frequency of tool usage can significantly reduce financial costs. To confirm the efficiency of AEPOs tool usage, we quantify the tool consumption of AEPO compared to other RL algorithms in the deep information seeking task. As shown in Figure 7 (right), AEPO requires only about half the number of tool calls to achieve superior performance compared to vanilla and clipping-optimized RL algorithms. Additionally, compared to the agentic RL algorithm ARPO, AEPO consistently reduces the number of tool calls. We attribute this enhanced efficiency to the entropy pre-monitoring phase, which balances the allocation of rollout exploration resources based on the information gain from the problem and tool usage. This ensures that AEPO not only broadens the rollout exploration space but also achieves efficient web agent training. (2) Entropy Stability Analysis. To better quantify the effectiveness of entropy-balanced policy optimization during policy updates, we present the RL training curves for 10 reasoning tasks. Figure 8 illustrates the dynamic visualization of entropy loss and validation set accuracy across 10 reasoning tasks throughout the training steps. We observe that using clippingoptimized RL often encounters entropy instability during training, leading to performance collapse. In contrast, AEPO demonstrates more stable entropy curve compared to other reinforcement learning algorithms. Interestingly, sharp fluctuations in entropy loss do not improve training stability and effectiveness. Instead, maintaining consistently high and stable entropy dynamic is generally advantageous for ongoing performance enhancement. This observation supports our initial motivation, as AEPO employs entropybalanced policy optimizations to foster more reasonable and stable entropy dynamics."
        },
        {
            "title": "6 Related Work\n6.1 Reinforcement Learning for Web Agent.\nThe emergence of agent reinforcement learning (RL)[122] has set the stage\nfor the development of general-purpose web agents, a pursuit shared by\nboth academia and industry. Initial efforts[6, 24, 40, 53, 81] established\na foundation by enabling models to autonomously interact with search\nengines or code interpreters using rule-based RL. Building on this ground-\nwork, subsequent innovations have emerged: Tool-star [13] incorporates\nmulti-tool usage within agentic RL, while other studies [7, 12, 36, 82, 88,\n98, 99, 110, 128] enhance efficiency and stability through redesigned re-\nward functions. MemAgent introduces memory mechanisms during the RL\nphase to better manage contextual information [117]. Additionally, recent\nresearch [27, 39, 49, 112] explores comprehensive asynchronous training\nframeworks for web agents. Building on these advancements, Tongyi Deep",
            "content": "WWW 26, April 1317, 2026, Dubai, UAE Dong et al."
        },
        {
            "title": "References",
            "content": "[1] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024. DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/ 2024/hash/1704ddd0bb89f159dfe609b32c889995-Abstract-Conference.html [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923 (2025). [3] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, and Yonghong Duan. 2025. MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention. CoRR abs/2506.13585 (2025). https://doi.org/10.48550/ARXIV.2506. 13585 arXiv:2506.13585 [4] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. CoRR abs/2402.03216 (2024). https://doi.org/10.48550/ARXIV.2402.03216 arXiv:2402.03216 [5] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. 2025. xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations. arXiv preprint arXiv:2506.13651 (2025). [6] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning. arXiv:cs.AI/2503.19470 https://arxiv.org/abs/2503. 19470 [7] Yifei Chen, Guanting Dong, and Zhicheng Dou. 2025. Toward Effective ToolIntegrated Reasoning via Self-Evolved Preference Learning. arXiv preprint arXiv:2509.23285 (2025). [8] Yifei Chen, Guanting Dong, Yutao Zhu, and Zhicheng Dou. 2025. Revisiting RAG Ensemble: Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration. arXiv preprint arXiv:2508.13828 (2025). [9] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. 2025. Reasoning with Exploration: An Entropy Perspective. CoRR abs/2506.14758 (2025). https://doi.org/10.48550/ARXIV.2506. 14758 arXiv:2506.14758 [10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. 2025. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training. CoRR abs/2501.17161 (2025). https://doi.org/10.48550/ARXIV.2501.17161 arXiv:2501.17161 [11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168 (2021). [12] Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Yuan Wang, Quanxing Zha, Sunhao Dai, and Changhua Meng. 2025. Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward. CoRR abs/2508.12800 (2025). https://doi.org/10.48550/ARXIV.2508.12800 arXiv:2508.12800 [13] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. ToolStar: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. CoRR abs/2505.16410 (2025). https://doi.org/10.48550/ARXIV.2505. arXiv:2505.16410 [14] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. 2025. Agentic Reinforced Policy Optimization. CoRR abs/2507.19849 (2025). https://doi.org/10.48550/ ARXIV.2507.19849 arXiv:2507.19849 [15] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. 2025. Toward Verifiable Instruction-Following Alignment for Retrieval Augmented Generation. In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, Toby Walsh, Julie Shah, and Zico Kolter (Eds.). AAAI Press, 2379623804. https://doi.org/10.1609/AAAI.V39I22.34551 [16] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. 2024. Progressive Multimodal Reasoning via Active Retrieval. arXiv preprint arXiv:2412.14835 (2024). [17] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. 2024. Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation. CoRR abs/2406.18676 (2024). https: //doi.org/10.48550/ARXIV.2406.18676 arXiv:2406.18676 [18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [19] Martin Ester, Hans-Peter Kriegel, J√∂rg Sander, and Xiaowei Xu. 1996. densitybased algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD96). AAAI Press, 226231. [20] Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, Lei Bai, Ning Ding, and Bowen Zhou. 2025. SSRL: Self-Search Reinforcement Learning. CoRR abs/2508.10874 (2025). https://doi.org/10.48550/ARXIV.2508.10874 arXiv:2508.10874 [21] Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, et al. 2025. Towards General Agentic Intelligence via Environment Scaling. arXiv preprint arXiv:2509.13311 (2025). [22] Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. 2025. WebEvolver: Enhancing Web Agent SelfImprovement with Coevolving World Model. CoRR abs/2504.21024 (2025). https://doi.org/10.48550/ARXIV.2504.21024 arXiv:2504.21024 [23] Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, and Dong Yu. 2025. Cognitive Kernel-Pro: Framework for Deep Research Agents and Agent Foundation Models Training. CoRR abs/2508.00414 (2025). https://doi.org/10.48550/ARXIV.2508.00414 arXiv:2508.00414 [24] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. arXiv:cs.CL/2504.11536 https://arxiv. org/abs/2504.11536 [25] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025. Group-in-Group Policy Optimization for LLM Agent Training. CoRR abs/2505.10978 (2025). https://doi.org/10.48550/ARXIV.2505.10978 arXiv:2505. [26] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025. Group-in-Group Policy Optimization for LLM Agent Training. arXiv preprint arXiv:2505.10978 (2025). [27] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. 2025. Beyond Ten Turns: Unlocking Long-Horizon Agentic arXiv:cs.CL/2508.07976 https: Search with Large-Scale Asynchronous RL. //arxiv.org/abs/2508.07976 [28] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2024. ToRA: Tool-Integrated Reasoning Agent for Mathematical Problem Solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=Ep0TtjVoap [29] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [30] Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, and Yong Dai. 2025. Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents. CoRR abs/2508.01858 (2025). https://doi.org/10.48550/ARXIV.2508.01858 arXiv:2508.01858 [31] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-Augmented Language Model Pre-Training. CoRR abs/2002.08909 (2020). arXiv:2002.08909 https://arxiv.org/abs/2002.08909 [32] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In Proceedings of the Neural Information Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html [33] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, Donia Scott, N√∫ria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, 66096625. https://doi.org/10.18653/V1/2020.COLINGMAIN.580 [34] Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. 2025. TreeRL: LLM Reinforcement Learning with On-Policy Tree Search. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 12355 12369. https://aclanthology.org/2025.acl-long.604/ [35] Jian Hu. 2025. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262 (2025). [36] Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu Chen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li, and Junbo Zhao. 2025. Reinforcement Learning with Rubric Anchors. CoRR abs/2508.12790 (2025). https://doi.org/10.48550/ARXIV.2508. 12790 arXiv:2508.12790 [37] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [38] Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, and Liaoni Wu. 2025. Tree Search for LLM Agent Reinforcement Learning. arXiv:cs.LG/2509.21240 https://arxiv.org/abs/2509.21240 [39] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. 2025. VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use. arXiv preprint arXiv:2509.01055 (2025). [40] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. CoRR abs/2503.09516 (2025). https://doi.org/10. 48550/ARXIV.2503.09516 arXiv:2503.09516 [41] Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, and Zhicheng Dou. 2025. Decoupled Planning and Execution: Hierarchical Reasoning Framework for Deep Search. arXiv:cs.AI/2507.02652 https://arxiv.org/abs/2507. [42] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024. FlashRAG: Modular Toolkit for Efficient Retrieval-Augmented Generation Research. CoRR abs/2405.13576 (2024). https://doi.org/10.48550/ARXIV.2405. 13576 arXiv:2405.13576 [43] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2024. Fact, Fetch, and Reason: Unified Evaluation of Retrieval-Augmented Generation. arXiv:cs.CL/2409.12941 https://arxiv.org/abs/2409.12941 [44] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html [45] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html [46] Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu. 2024. DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning. CoRR abs/2407.04078 (2024). https://doi.org/10.48550/ARXIV.2407.04078 arXiv:2407.04078 [47] Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, et al. 2025. WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning. arXiv preprint arXiv:2509.13305 (2025). [48] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebSailor: Navigating Super-human Reasoning for Web Agent. arXiv:cs.CL/2507.02592 https://arxiv.org/abs/2507.02592 [49] Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang, Hongxuan Lu, Tianrui Qin, Chenghao Zhu, Yi Yao, Shuying Fan, Xiaowan Li, Tiannan Wang, Pai Liu, King Zhu, He Zhu, Dingfeng Shi, Piaohong Wang, Yeyi Guan, Xiangru Tang, Minghao Liu, Yuchen Eleanor Jiang, Jian Yang, Jiaheng Liu, Ge Zhang, and Wangchunshu Zhou. 2025. Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL. CoRR abs/2508.13167 (2025). https://doi.org/10.48550/ARXIV.2508.13167 arXiv:2508.13167 [50] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic Search-Enhanced Large Reasoning Models. CoRR abs/2501.05366 (2025). https://doi.org/10.48550/ ARXIV.2501.05366 arXiv:2501.05366 [51] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, JiRong Wen, and Zhicheng Dou. 2025. WebThinker: Empowering Large Reasoning Models with Deep Research Capability. arXiv preprint arXiv:2504.21776 (2025). [52] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. 2024. RetroLLM: Empowering Large Language Models to Retrieve Finegrained Evidence within Generation. CoRR abs/2412.11919 (2024). https: //doi.org/10.48550/ARXIV.2412.11919 arXiv:2412.11919 [53] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. ToRL: Scaling Tool-Integrated RL. CoRR abs/2503.23383 (2025). https://doi.org/10.48550/ARXIV.2503.23383 arXiv:2503.23383 [54] Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, et al. 2025. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic tree-based modeling. arXiv preprint arXiv:2508.17445 (2025). [55] Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, et al. 2025. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic tree-based modeling. arXiv preprint arXiv:2508.17445 (2025). [56] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets Verify Step by Step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=v8L0pN6EOi [57] Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, and ShaoGuo Liu. 2025. Ettrl: Balancing exploration and exploitation in llm test-time reinforcement learning via entropy mechanism. arXiv preprint arXiv:2508.11356 (2025). [58] Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, et al. 2025. Part I: Tricks or traps? deep dive into RL for LLM reasoning. arXiv preprint arXiv:2508.08221 (2025). [59] Xing Han L√π, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher J. Pal, and Siva Reddy. 2025. AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories. CoRR abs/2504.08942 (2025). https://doi.org/10. 48550/ARXIV.2504.08942 arXiv:2504.08942 [60] Gr√©goire Mialon, Cl√©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for General AI Assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=fibxvahvs3 [61] MiniMax. 2025. MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention. arXiv:cs.CL/2506.13585 https://arxiv.org/abs/2506.13585 [62] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nat. 518, 7540 (2015), 529533. https://doi.org/10.1038/NATURE14236 [63] Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. 2015. Language Understanding for Text-based Games using Deep Reinforcement Learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, Llu√≠s M√†rquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computational Linguistics, 111. https://doi.org/10.18653/V1/ D15-1001 [64] OpenAI. 2024. Learning to Reason with LLMs. https://openai.com/index/ learning-to-reason-with-llms [65] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. 2019. AdvantageWeighted Regression: Simple and Scalable Off-Policy Reinforcement Learning. CoRR abs/1910.00177 (2019). arXiv:1910.00177 http://arxiv.org/abs/1910.00177 [66] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. 2025. WWW 26, April 1317, 2026, Dubai, UAE Dong et al. Humanitys last exam. arXiv preprint arXiv:2501.14249 (2025). [67] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap in Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 56875711. https: //doi.org/10.18653/V1/2023.FINDINGS-EMNLP.378 [68] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T√ºr, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958 (2025). [69] Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Jiapeng Wang, Yifan Zhang, Zhuoma GongQue, Chong Sun, Yida Xu, Yadong Xue, et al. 2025. V-oracle: Making progressive reasoning in deciphering oracle bones for you and me. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2012420150. [70] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. 2024. We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning? CoRR abs/2407.01284 (2024). https://doi.org/10.48550/ARXIV.2407.01284 arXiv:2407.01284 [71] Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, Jie Wang, Chong Sun, Chen Li, and Honggang Zhang. 2025. We-Math 2.0: Versatile MathBook System for Incentivizing Visual Mathematical Reasoning. CoRR abs/2508.10433 (2025). https://doi.org/10.48550/ARXIV.2508.10433 arXiv:2508.10433 [72] Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. 2025. WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents. arXiv preprint arXiv:2509.13309 (2025). [73] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. 2025. UI-TARS: Pioneering Automated GUI Interaction with Native Agents. arXiv preprint arXiv:2501.12326 (2025). [74] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 Technical Report. arXiv:cs.CL/2412.15115 https://arxiv.org/abs/2412.15115 [75] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization Algorithms. CoRR abs/1707.06347 (2017). arXiv:1707.06347 http://arxiv.org/abs/1707.06347 [76] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. CoRR abs/2402.03300 (2024). https://doi.org/10.48550/ARXIV.2402.03300 arXiv:2402.03300 [77] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. HybridFlow: Flexible and Efficient RLHF Framework. arXiv preprint arXiv: 2409.19256 (2024). [78] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768 (2020). [79] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen Simonyan, and Demis Hassabis. 2017. Mastering Chess and Shogi by Self-Play with General Reinforcement Learning Algorithm. CoRR abs/1712.01815 (2017). arXiv:1712.01815 http://arxiv.org/abs/1712.01815 [80] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. 2025. Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning. arXiv preprint arXiv:2505.01441 (2025). [81] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. CoRR abs/2503.05592 (2025). https://doi.org/10.48550/ARXIV.2503.05592 arXiv:2503.05592 [82] Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning. CoRR abs/2505.17005 (2025). https://doi.org/10.48550/ ARXIV.2505.17005 arXiv:2505.17005 [83] Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, et al. 2025. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310 (2025). [84] Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang, Kun Gai, and Guorui Zhou. 2025. Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization. CoRR abs/2508.07629 (2025). https://doi.org/10.48550/ARXIV.2508. 07629 arXiv:2508.07629 [85] Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, and Guorui Zhou. 2025. CE-GPPO: Controlling Entropy via GradientPreserving Clipping Policy Optimization in Reinforcement Learning. arXiv preprint arXiv:2509.20712 (2025). [86] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. 2025. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. arXiv:cs.CL/2505.04588 https://arxiv.org/abs/2505.04588 [87] Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and JiRong Wen. 2024. HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems. CoRR abs/2411.02959 (2024). https: //doi.org/10.48550/ARXIV.2411.02959 arXiv:2411.02959 [88] Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, and Ji-Rong Wen. 2025. HierSearch: Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches. arXiv preprint arXiv:2508.08088 (2025). [89] Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, and Bo An. 2024. True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum? id=hILVmJ4Uvu [90] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization. CoRR abs/2507.15061 (2025). https: //doi.org/10.48550/ARXIV.2507.15061 arXiv:2507.15061 [91] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534 (2025). [92] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 (2025). [93] Meituan LongCat Team, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, et al. 2025. LongCat-Flash Technical Report. arXiv preprint arXiv:2509.01322 (2025). [94] Qwen Team. 2024. QwQ: Reflect Deeply on the Boundaries of the Unknown. https://qwenlm.github.io/blog/qwq-32b-preview/ [95] Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face (2024). [96] Naftali Tishby, Fernando Pereira, and William Bialek. 2000. The information bottleneck method. arXiv preprint physics/0004057 (2000). [97] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics 10 (2022), 539554. [98] Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, and Kam-Fai Wong. 2025. Toward Theory of Agents as Tool-Use Decision-Makers. CoRR abs/2506.00886 (2025). https://doi.org/10.48550/ARXIV. 2506.00886 arXiv:2506.00886 [99] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. 2025. OTC: Optimal Tool Calls via Reinforcement Learning. arXiv preprint arXiv:2504.14870 (2025). [100] Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. 2025. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544 (2025). [101] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. 2025. Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning. CoRR abs/2506.01939 (2025). https: //doi.org/10.48550/ARXIV.2506.01939 arXiv:2506.01939 [102] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. 2024. DistRL: An Asynchronous Distributed Reinforcement Learning Framehttps: work for On-Device Control Agents. CoRR abs/2410.14803 (2024). //doi.org/10.48550/ARXIV.2410.14803 arXiv:2410. [103] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. 2025. Reinforcement Learning for Reasoning in Large Language Models with One Training Example. arXiv preprint arXiv:2504.20571 (2025). [104] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Agentic Entropy-Balanced Policy Optimization WWW 26, April 1317, 2026, Dubai, UAE Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, and Mingshu Zhai. 2025. GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models. CoRR abs/2508.06471 (2025). https://doi.org/10.48550/ARXIV.2508.06471 arXiv:2508.06471 [120] Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. 2024. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ c848b7d3adc08fcd0bf1df3101ba6728-Abstract-Conference.html [121] Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, and Fei Huang. 2025. EvolveSearch: An Iterative Self-Evolving Search Agent. CoRR abs/2505.22501 (2025). https://doi.org/10.48550/ARXIV.2505.22501 arXiv:2505.22501 [122] Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. 2025. The Landscape of Agentic Reinforcement Learning for LLMs: Survey. arXiv preprint arXiv:2509.02547 (2025). [123] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Sirens Song in the AI Ocean: Survey on Hallucination in Large Language Models. CoRR abs/2309.01219 (2023). https://doi.org/10.48550/ARXIV.2309.01219 arXiv:2309.01219 [124] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. 2025. Group Sequence Policy Optimization. CoRR abs/2507.18071 (2025). https://doi.org/10.48550/ARXIV.2507.18071 arXiv:2507.18071 [125] Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, et al. 2025. First Return, Entropy-Eliciting Explore. arXiv preprint arXiv:2507.07017 (2025). [126] Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, et al. 2025. First return, entropy-eliciting explore. arXiv preprint arXiv:2507.07017 (2025). [127] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, et al. 2025. Agentfly: Finetuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153 (2025). [128] Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. 2025. Fin-PRM: Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models. CoRR abs/2508.15202 (2025). https://doi.org/10.48550/ARXIV.2508.15202 arXiv:2508. [129] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. 2024. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446 (2024). [130] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024. An information bottleneck perspective for effective noise filtering on retrieval-augmented generation. arXiv preprint arXiv:2406.01549 (2024). Yejin Choi, and Manling Li. 2025. RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning. arXiv:cs.LG/2504.20073 https://arxiv.org/abs/2504.20073 [105] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebDancer: Towards Autonomous Information Seeking Agency. arXiv:cs.CL/2505.22648 https://arxiv.org/abs/2505.22648 [106] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebDancer: Towards Autonomous Information Seeking Agency. CoRR abs/2505.22648 (2025). https://doi.org/10.48550/ARXIV.2505. 22648 arXiv:2505.22648 [107] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025. WebWalker: Benchmarking LLMs in Web Traversal. CoRR abs/2501.07572 (2025). https://doi.org/10.48550/ARXIV.2501.07572 arXiv:2501. [108] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025. WebWalker: Benchmarking LLMs in Web Traversal. arXiv:cs.CL/2501.07572 https://arxiv.org/abs/2501.07572 [109] Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, and Jingren Zhou. 2025. ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization. arXiv preprint arXiv:2509.13313 (2025). [110] Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, et al. 2025. LIMI: Less is More for Agency. arXiv preprint arXiv:2509.17567 (2025). [111] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. 2024. LLaVA-o1: Let Vision Language Models Reason Step-by-Step. arXiv preprint arXiv:2411.10440 (2024). [112] Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. 2025. SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning. arXiv preprint arXiv:2509.02479 (2025). [113] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report. CoRR abs/2505.09388 (2025). https://doi.org/10.48550/ARXIV.2505.09388 arXiv:2505.09388 [114] Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. 2025. TreeRPO: Tree Relative Policy Optimization. arXiv preprint arXiv:2506.05183 (2025). [115] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). [116] Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, and Dong Yu. 2024. SIaM: Self-Improving Code-Assisted Mathematical Reasoning of Large Language Models. CoRR abs/2408.15565 (2024). https://doi.org/10.48550/ARXIV.2408. 15565 arXiv:2408.15565 [117] Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, and Hao Zhou. 2025. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent. CoRR abs/2507.02259 (2025). https://doi.org/10.48550/ARXIV.2507.02259 arXiv:2507.02259 [118] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. 2025. DAPO: An Open-Source LLM Reinforcement Learning System at Scale. CoRR abs/2503.14476 (2025). https://doi.org/10.48550/ARXIV.2503.14476 arXiv:2503.14476 [119] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, WWW 26, April 1317, 2026, Dubai, UAE Dong et al."
        },
        {
            "title": "Appendix",
            "content": "A Proof of the Gradient of AEPO In this section, we will comprehensively detail the theoretical derivation of AEPOs forward propagation formulas and how they lead to the backward propagation formulas. Specifically: We begin with the loss function: region, propagating fixed penalty regardless of sample reliability. AEPO, instead, sets ùêπùë° (ùúÉ ) = 0, effectively filtering out low-confidence negative advantages. This simple but principled change prevents unstable gradient signals from low-likelihood rollouts and reduces the variance introduced by symmetric updates. Consequently, AEPO achieves smoother optimization dynamics and more stable convergence, especially under high-entropy exploration regimes where CISPO often exhibits oscillatory behavior. Figure 8 provides experimental evidence for this discussion. ùê∫ ùëáùëó min = Eùë• (cid:18) ùõø, 1 ùúñùëô , (cid:18) ùõø ùê¥ (ùë° ) , clip 1 (cid:205)ùê∫ ùëó =1 ùëáùëó (cid:19) (12) (ùúÉ ) represents the importance ratio, and sg() is the stopgradient operator. Given that ùê¥(ùë° ) is constant and ùúÉ ùõø = ùõø ùúôùúÉ (ùëé ùëó,ùë° , ùë† ùëó,ùë° ), the gradient of ùëì (ùõø ) can be expressed as: where ùõø = ùëü ( ùëó ) 1 + ùúñ‚Ñé sg(ùõø ) ùê¥(ùë° ) ùë° =1 ùëó =1 ùõø (cid:19) , ùë° B.2 GPPO where ùúÉ ùëì (ùõø ) = ùê¥ (ùë° ) ùë† (ùõø ) ùõø ùúôùúÉ (ùëé ùëó,ùë° , ùë† ùëó,ùë° ), (13) where ùë† (ùõø ) depends on the range of ùõø. Therefore, we consider three ‚Ñì (ùë° ) = scenarios: (1) If ùê¥ (ùë° ) > 0 and ùõø > 1 + ùúñ‚Ñé: The upper clipping boundary is active, so ùëì /ùõø = (1 + ùúñ‚Ñé )/sg(ùõø ), effectively simplifying to (1 + ùúñ‚Ñé ). (2) If ùê¥(ùë° ) < 0 and ùõø < 1 ùúñ‚Ñì : The lower clipping boundary dominates, leading to ùëì /ùõø = 0, causing the gradient to vanish. The region is unclipped, resulting in ùëì /ùõø = ùõø. By combining all cases, we derive: Fùëó,ùë° (ùúÉ ) = 1 + ùúñ‚Ñé, 0, ùõø, ùê¥ (ùë° ) > 0, ùõø > 1 + ùúñ‚Ñé, ùê¥ (ùë° ) < 0, ùõø < 1 ùúñ‚Ñì , otherwise. Thus, the gradient update is given by: ùúÉ = Eùë• 1 (cid:205)ùê∫ ùëó =1 ùëáùëó ùê∫ ùëáùëó ùëó = ùë° =1 Fùëó,ùë° (ùúÉ ) ùúôùúÉ (ùëé ùëó,ùë° , ùë† ùëó,ùë° ) ùê¥ (ùë° ) (14) . (15)"
        },
        {
            "title": "B Discussion of the Gradient Forms in",
            "content": "Clipping-optimized RL In this section, we discuss the gradient differences between AEPO and clipping-optimized RL algorithms to gain insight into the differences in their policy update stages [85]. B.1 CISPO = Eùë• 1 (cid:205)ùê∫ ùëó =1 ùëáùëó ùê∫ ùëáùëó ùõø ùê¥(ùë° ) log ùúãùúÉ (ùëé ( ùëó ) ùë° ùë† ( ùëó ) ùë° ) ùëó =1 By expanding the gradient of the loss function, we obtain: ùë° =1 ùúÉ = Eùë• 1 (cid:205)ùê∫ ùëó =1 ùëáùëó ùê∫ ùëáùëó ùëó =1 ùë° =1 Fùëó,ùë° (ùúÉ ) ùúôùúÉ (ùëé ùëó,ùë° , ùë† ùëó,ùë° ) ùê¥ (ùë° ) where Fùëó,ùë° (ùúÉ ) = 1 ùúñ‚Ñì , 1 + ùúñ‚Ñé, 1 ùúñ‚Ñì , 1 + ùúñ‚Ñé, ùê¥ (ùë° ) < 0, ùõø < 1 ùúñ‚Ñì , ùê¥ (ùë° ) > 0, ùõø > 1 + ùúñ‚Ñé, ùê¥ (ùë° ) > 0, ùõø < 1 ùúñ‚Ñì , ùê¥ (ùë° ) < 0, ùõø > 1 + ùúñ‚Ñé, ùõø, otherwise. (16) . , (17) (18) As shown in Eq. (14), AEPO modifies the CISPO objective by introducing an asymmetric clipping rule that deactivates gradient flow when both ùê¥ (ùë° ) < 0 and ùõø < 1 ùúñ‚Ñì . In CISPO, the gradient factor remains ùêπùë° (ùúÉ ) = 1 ùúñ‚Ñì for this LGPPO (ùúÉ ) = Eùë• 1 (cid:205)ùê∫ ùëó =1 ùëáùëó ùê∫ ùëáùëó ùëó =1 ùë° =1 ‚Ñì (ùë° ) , (19) ùõΩ1 ùõΩ2 1 ùúñ‚Ñì sg(ùõø ) 1 + ùúñ‚Ñé sg(ùõø ) ùõø ùê¥ (ùë° ) , ùõø ùê¥ (ùë° ) , ùê¥ (ùë° ) < 0, ùõø < 1 ùúñ‚Ñì , ùõø ùê¥ (ùë° ) , ùê¥(ùë° ) > 0, ùõø > 1 + ùúñ‚Ñé, (20) otherwise. By expanding its gradient, we have: ùúÉ LGPPO = Eùë• where 1 (cid:205)ùê∫ ùëó =1 ùëáùëó ùê∫ ùëáùëó ùëó =1 ùë° = Fùëó,ùë° (ùúÉ ) ùúôùúÉ (ùëé ùëó,ùë° , ùë† ùëó,ùë° ) ùê¥ (ùë° ) Fùëó,ùë° (ùúÉ ) = ùõΩ1 (1 ùúñ‚Ñì ), ùõΩ2 (1 + ùúñ‚Ñé ), ùê¥ (ùë° ) < 0, ùõø < 1 ùúñ‚Ñì , ùê¥ (ùë° ) > 0, ùõø > 1 + ùúñ‚Ñé, ùõø, otherwise. , (21) (22) Compared with GPPO, which retains bounded (non-zero) gradients inside clipped regions via its ùõΩ-scaled correction terms, AEPO enforces stricter rule: residual gradients in the region ùê¥ (ùë° ) < 0, ùõø < 1 ùúñ‚Ñì are discarded (i.e., ùêπùë° (ùúÉ ) = 0). Empirically, we find that agentic RL training is particularly sensitive to GPPO-style pessimistic suppression. Since our goal is to exploit high-entropy tokens with positive rewards, excessive pessimism can hamper effective credit assignment for these tokens. AEPO therefore removes residual negative updates while allowing high-entropy, positively rewarded tokens to fully contribute to the gradient, improving both stability and credit propagation in long-horizon agentic tasks. Baselines In this section, we provide detailed overview of the baseline models involved in all experiments, as follows: C.1 RL algorithms (1) Classical RL Method: GRPO [76] is reinforcement learning algorithm for fine-tuning large language models via group-based policy optimization. It optimizes model behaviors by comparing responses within sampled groups and assigning relative rewards, enabling more stable and sample-efficient policy updates. Reinforce++ [35] extends the classic policy-gradient algorithm by incorporating variance reduction and adaptive normalization techniques. It improves training stability and sample efficiency when fine-tuning language models with scalar rewards, while keeping the overall objective aligned with standard REINFORCE. (2) Clipping-optimized RL Method DAPO [118] decouples the clipping operation from the policy update to achieve more stable optimization, and introduces dynamic sampling strategy that adaptively selects training examples to maintain effective Agentic Entropy-Balanced Policy Optimization Hangyu Mao2, Fuzheng Zhang2, Kun Gai2, Guorui Zhou266, Yutao Zhu1, Ji-Rong Wen1, Zhicheng Dou166 WWW 26, April 1317, 2026, Dubai, UAE Algorithm 2 Agentic Entropy-Balanced Policy Optimization Require: Reasoning model ùúãùúÉ , external tools ùëá , total rollout size ùëò, entropy sensitivity ùõΩ, branch penalty slope ùõæ, clipping bounds ùúñùëô, ùúñ‚Ñé, entropy-aware weight ùõº 1: Input: Dataset ùê∑ 2: Initialize reference model: ùúã old 3: for ùëñ = 1 to do 4: Sample mini-batch ùê∑ùëè ùê∑ ùúÉ ùúãùúÉ 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: // Dynamic Entropy-Balanced Rollout for each query ùëû ùê∑ùëè do Generate 1 complete trajectory ùëü to obtain ùêªroot and ùêª avg tool Global rollout size ùëö ùëò ùúé(cid:0)ùõΩ (ùêªroot ùêª avg Branch rollout size ùëè ùëò ùëö Initialize rollout pool Consecutive-high-entropy counter ùëô 0 while < ùëö do tool)(cid:1) Sample trajectory ùëü ; add to end while while ùëè > 0 and ùëü ùëó not terminated do Select trajectory ùëü at tool-call step ùë° Œîùêªùë° Normalize(ùêªùë° ùêªinitial) Consecutive penalty ÀÜùëÉ (ùëô) ùõæ ùëô Branch probability ùëÉùë° (ùõº + ùõΩŒîùêªùë° )(1 ÀÜùëÉ (ùëô)) if ùëÉùë° > ùúè then Branch ùëç sub-trajectories; ùëè ùëè ùëç else ùëô ùëô + 1 if Œîùêªùë° > 0 end if end while if ùëè > 0 then Sample ùëè additional trajectories and add to end if end for // Entropy-Balanced Policy Optimization for step = 1 to do Compute standard advantage ÀÜùê¥Acc and entropy advantage ÀÜùê¥Œîùêª via Eq. (10) Entropy-aware advantage ÀÜùê¥ ÀÜùê¥Acc (1 + ÀÜùê¥Œîùêª )ùõº for each token ùë° in trajectory ùëó do Importance ratio ùõø ùúãùúÉ /ùúãùúÉold if ùõø > 1 + ùúñ‚Ñé and ÀÜùê¥ > 0 then Gradient scaler Fùëó,ùë° 1 + ùúñ‚Ñé else if ùõø < 1 ùúñùëô and ÀÜùê¥ < 0 then Gradient scaler Fùëó,ùë° 0 else Gradient scaler Fùëó,ùë° ùõø end if end for Update parameters via Eq. 15 end for 45: 46: end for 47: Output: Fine-tuned model ùúãùúÉ gradient signals. These techniques together improve training efficiency and prevent performance degradation in long-horizon reasoning tasks. GPPO [84] extends the PPO framework by decoupling the clipping operation between the forward and backward passes. During optimization, the policy ratio is clipped in the forward computation to ensure bounded updates, while the original, unclipped ratio is used in the backward path to preserve complete gradient information. CISPO [61] reformulates ratio clipping by applying the constraint to importance sampling weights instead of policy ratios. It bounds update magnitudes in expectation while preserving token-level gradient information through unclipped policy ratios. (3) Agentic RL Method GIGPO [26] groups complete trajectories at episode level to compute macro-relative advantages, and also retroactively groups actions sharing anchor states across trajectories at step level to compute micro-relative advantages. Both levels are combined without using critic, preserving the critic-free nature while enabling per-step credit signals. ARPO [14] is an RL method tailored for multi-turn LLM agents. It introduces an entropy-based adaptive rollout scheme that increases sampling in steps with high uncertainty, and incorporates an advantage attribution mechanism to assign credit across branching tool-use interactions. C.2 Web Search Agent RAG [44] (Retrieval-Augmented Generation) combines information retrieval with generative modeling to enhance the accuracy, reliability, and timeliness of outputs. It retrieves relevant information from an external knowledge base before generating responses, addressing internal knowledge gaps and reducing hallucinations. Search-o1 [50] is framework designed to enhance reasoning by integrating agentic RAG mechanisms with Reason-in-Documents module. It improves accuracy, coherence, and reliability in reasoning tasks, outperforming native reasoning and traditional RAG methods in complex scenarios. WebThinker [51] is an open-source framework developed by Renmin University of China, enabling LRMs to autonomously search, explore web pages, and generate research reports. It employs direct preference optimization and iterative synthesis tools to enhance tool utilization capabilities. ReAct [115] combines reasoning and action to tackle complex tasks effectively. It allows models to generate reasoning steps and use external tools, such as search engines and databases, during decision-making, optimizing results through iterative processes. The Overall Algorithm Workflow of AEPO In this section, we delve into the overall workflow of the Agentic EntropyBalanced Policy Optimization (AEPO) algorithm, as depicted in Algorithm Diagram 2. The AEPO algorithm integrates dynamic entropy-balanced rollouts with entropy-balanced policy optimization to enhance multi-turn tool-use capabilities in large language models."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Renmin University of China"
    ]
}