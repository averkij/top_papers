{
    "paper_title": "Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition",
    "authors": [
        "Jinlong Ma",
        "Yu Zhang",
        "Xuefeng Bai",
        "Kehai Chen",
        "Yuwei Wang",
        "Zeming Liu",
        "Jun Yu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit $\\textbf{modality bias}$, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning ($\\textbf{MCR}$), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines."
        },
        {
            "title": "Start",
            "content": "Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition Jinlong Ma1, Yu Zhang1, Xuefeng Bai1, Kehai Chen1, Yuwei Wang2, Zeming Liu3, Jun Yu1, Min Zhang 1 1Harbin Institute of Technology, Shenzhen, China, 2Institute of Computing Technology Chinese Academy of Sciences, 3Beijing University of Aeronautics and Astronautics Correspondence: chenkehai@hit.edu.cn 6 2 0 2 4 ] . [ 1 6 8 4 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals fundamental challenge: MLLMs exhibit modality bias, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines."
        },
        {
            "title": "Introduction",
            "content": "Grounded Multimodal Named Entity Recognition (GMNER, Yu et al., 2023) aims to organize information into structured key multimodal representations, which simultaneously identifies named entities in the text and grounds them to their corresponding visual bounding boxes. As foundational task, GMNER facilitates various downstream applications, such as recommendation systems (Acharya et al., 2023) and knowledgebased question answering (Zhang et al., 2024; Sun et al., 2023). The code and data are released at https://github.com/ aaaalonga/MCR. 1 Figure 1: Error patterns caused by modality bias in GMNER due to the models tendency to hallucinate correlations based on unimodal heuristics rather than rigorous cross-modal verification. Recently, Multimodal Large Language Models (MLLMs, Bai et al., 2025; Yue et al., 2025) have achieved remarkable performance on various vision-language tasks. This progress has motivated researchers to explore their use for GMNER (Tang et al., 2025c,a). However, these approaches typically employ MLLMs as auxiliary tools like image descriptors within cascaded pipelines, which inevitably introduce cumulative error propagation (Tang et al., 2025b) and incur additional computational costs (Ok et al., 2024). In this work, we take the first step toward exploring the potential of MLLMs for end-toend GMNER by reformulating it as generative reasoning task. Our investigation reveals that direct application of MLLMs to GMNER faces critical pathology: modality bias, characterized by the models tendency to hallucinate correlations based on unimodal heuristics rather than rigorous crossmodal verification. As shown in Figure 1 (a), textual bias causes the model to disregard visual evidence: despite correctly recognizing Kevin Durant with image-only input, incorrectly grounds the text-only entity Iggy to the bounding it box of Kevin Durant. Symmetrically, visual bias leads to the neglect of textual semantics. In Figure 1 (b), the model overrides textual context, erroneously recalling Manchester United as named entity driven by visual cues, ignoring its absence in the textual context. We further conduct quantitative analyses that empirically confirm the severity and prevalence of modality bias for different MLLMs in Table 4. These reveal that MLLMs are prone to taking cognitive shortcuts rather than engaging in rigorous deduction, required for strict cross-modal grounding. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning to mitigate modality bias through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). Specifically, MRSI transforms abstract constraints into executable reasoning chains by synthesizing and injecting diverse reasoning templates to explicitly model the structural dependencies. Furthermore, to empower the model to autonomously explore reasoning trajectories within these structural bounds, CVO is proposed to dynamically align intermediate reasoning process with Group Relative Policy Optimization (GRPO) (Guo et al., 2025). This optimization mechanism punishes unimodal shortcuts and encourages the model to generate constraint-faithful rationales, effectively rectifying the intrinsic modality bias. Extensive experiments on Multimodal Named Entity Recognition (Huang et al., 2024; Yu et al., 2023) and Visual Grounding (He et al., 2023) benchmarks verify that our method, applied to Qwen2.5-VL and MimoVL, achieves superior performance compared to existing baselines. In-depth analyses confirm that our design explicitly facilitates cross-modal reasoning, effectively mitigating modality bias. In summary, our contributions are as follows: We identify modality bias in MLLM-based endto-end GMNER, revealing that models are prone to unimodal cognitive shortcuts. We propose MCR framework, which enforces explicit, constraint-faithful reasoning through schema injection and verifiable optimization against modality bias. We achieve superior performance on multiple benchmarks, demonstrating that structured reasoning is essential for precise cross-modal grounding."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Multimodal Named Entity Recognition (MNER) MNER (Moon et al., 2018) extracts and classifies named entities from image-text pairs. As fine-grained extension, Grounded MNER (GMNER) (Yu et al., 2023) requires the model to simultaneously recognize the named entities and localize visually present entities via bounding boxes. Existing studies primarily focus on refining cross-modal alignment to suppress visual noise (Liu et al., 2024; Bao et al., 2024) and enhancing generalization for unseen entities (Wang et al., 2024b). With the emergence of Multimodal Large Language Models (MLLMs) (Bai et al., 2025; Yue et al., 2025), recent studies (Tang et al., 2025a,c; Ok et al., 2024) have started to integrate them into GMNER, pivotal prerequisite for constructing knowledge graphs (Zhong et al., 2023; Li et al., 2020, 2024b) and facilitating knowledge graph question answering (Xu et al., 2025; Sun et al., 2023). These approaches primarily exploit the vast semantic priors inherent in MLLMs to refine and align multimodal feature representations, thereby facilitating more accurate entity-image association. In this work, we move beyond feature alignment to fully exploit the cross-modal reasoning potential of MLLMs, enabling holistic multimodal interplay for rigorous consistency verification."
        },
        {
            "title": "2.2 Reasoning in MLLMs",
            "content": "MLLMs have achieved remarkable success across wide range of domains (Zhu et al., 2025; Bai et al., 2025; Zhang et al., 2025a; Zheng et al., 2025; Zuo et al., 2025), demonstrating exceptional capabilities in integrating and reasoning over heterogeneous data. Recent advancements in MLLMs have catalyzed paradigm shift in complex reasoning tasks (Li et al., 2025b; Chen et al., 2025; ?). By introducing explicit reasoning processes into language-level Chains of Thought (CoT), these models decompose intricate problems into granular, sequential sub-steps (Wei et al., 2022a; Wang et al., 2022b; Gao et al., 2023). This reasoningcentric paradigm has proven instrumental in mitigating hallucinations arising from modality misalignment (Zhang et al., 2023; Li et al., 2025a; Zhang et al., 2025b), significantly enhancing both the accuracy and stability of multi-step inference. 2 2.3 Modality bias in MLLMs Recent works identify modality bias (Zhang et al., 2025d, 2026; Leng et al., 2024; Zhang et al., 2025c) in MLLMs, observing that models often exhibit intrinsic inclinations toward specific modalities. To address this, prevailing strategies typically employ Reinforcement Learning from Human Feedback (RLHF) by curating extensive preference datasets (Ouyang et al., 2022; Wang et al., 2024a) to enable MLLMs to distinguish between hallucinated and grounded content, effectively mitigating bias and hallucinations. In this work, we attribute modality bias in GMNER to cognitive shortcuts, where models bypass rigorous verification in favor of unimodal heuristics. To rectify this, we propose Modalityaware Consistency Reasoning (MCR) to explicitly model the interplay between modalities to verify entity existence and spatial alignment, thereby enforcing rigorous cross-modal consistency."
        },
        {
            "title": "3 Task Formulation",
            "content": "Given sentence and its associated image v, Grounded Multimodal Named Entity Recognition (GMNER) can be decomposed into two subtasks: Multimodal Named Entity Recognition (MNER). MNER recognizes entities in and assigns each entity predefined type. And it produces pairs (ei, ti), where ei is an entity span in and ti denotes its corresponding type. Entity Extraction & Grounding (EEG). EEG is parallels generalized Visual Grounding (VG). For each textual entity ei, decide whether it is visually present in v. If present, output its bounding box bi; otherwise, output None. Accordingly, the GMNER output can be formulated as: = {(ei, ti, li)}k1 i=1, (1) where k1 indicate the numbers of output triples in sample, and li is formed as: li = (cid:40) bi = (x1, y1, x2, y2), ei is grounded, None, ei is ungrounded, (2) where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners."
        },
        {
            "title": "4 Methodology",
            "content": "To fully leverage multimodal evidence and ensure cross-modal consistency, we propose Modalityaware Consistency Reasoning (MCR) including Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). The framework of MCR is illustrated in Figure 2. MRSI organizes the diverse reasoning schema with modality-specific constraints and enforces explicit reasoning, while CVO leverages the reasoning schema together with GRPO to further strengthen the models reasoning capability. 4.1 Multi-style Reasoning Schema Injection To address the modality bias caused by insufficient cross-modal consistency reasoning, we propose MRSI, which injects constraint-centered and diverse reasoning schema into the inference process (Zhou et al., 2025; Zhoubian et al., 2025) to strengthen cross-modal verification. Specifically, MRSI is guided by four core constraints, covering entity recognition Cs, type classification Ct, visual entailment Ce and visual grounding Cu: = {Cs, Ct, Ce, Cu}. (3) Each constraint aligns with the task and its relevant modality. See Appendix for an example. The resulting reasoning schema in Figure 2 reflects both taskand modalityspecific considerations. As shown in Appendix C, through templates, LLMs, or MLLMs, we transform (s, v, C, Yτ ) into programmatic reasoning steps with multiple styles on the labeled set DG: DR = (cid:91) (s,v,Y)DG Γϕ(z s, v, C, Y) , (4) where Γϕ denotes template extractors, LLMs or MLLMs, and DR is the obtained CoT training dataset. The diversity of reasoning schema prevents the sampled trajectories from collapsing into overly similar outputs, avoiding the negligible advantages and gradient vanishing (Xiong et al., 2025; Yao et al., 2025). We use D1 (a subset of DR) to inject reasoning schema into MLLMs (Tang et al., 2025d; Koksal and Alatan, 2025) via: LMRSI = E(x,v,z,y)D1 (cid:2) log πMLLM(z x, v) + log πMLLM(y x, v, z)(cid:3), (5) where πMLLM(z x, v) and πMLLM(y x, v, z) respectively denote the probability that MLLMs generate reasoning path given the imagetext pair and predict the answer based on the generated path. Through explicitly introducing and z, MRSI compels the model to retain reasoning path for cross-modal consistency checking. 3 Figure 2: The Framework of MCR. The framework consists of two stages: (1) Multi-style Reasoning Schema Injection constructs diverse reasoning schema DR by treating the core constraints as reasoning criteria and generating multiple reasoning styles from templates, LLMs, and MLLMs based on the imagetext inputs and labels. subset of DR is injected into MLLMs through supervised fine-tuning. (2) Constraint-guided Verifiable Optimization uses the remaining of DR and optimizes the model with verifiable reward functions derived from the core constraints, together with the GRPO algorithm, to enhance cross-modal consistency reasoning."
        },
        {
            "title": "4.2 Constraint-guided Verifiable Optimization",
            "content": "Reinforcement Learning with Verifiable Rewards (Guo et al., 2025) replaces reward models with reward functions and has shown strong performance on reasoning tasks. Following this, we introduce CVO to enhance cross-modal reasoning."
        },
        {
            "title": "4.2.1 Constraint-guided Verifiable Reward\nInspired by prior similar reward functions (Liu\net al., 2025; Roit et al., 2023), we anchor on C\nand design rule-based verifiable reward functions\nfor GMNER, including entity count, entity span,\nentity type, entailment, and localization rewards.\nEntity Count, Span and Type Rewards. The\nreward Rc encourages broader\nentity count\nexploration while still maintaining precision,\npreventing the model from becoming overly\nconservative. It assigns a score by comparing the\nnumber of predicted entity triples with the ground-\ntruth count. See Appendix D.1 for details.",
            "content": "In computing the entity span reward Rs, we compute the token-level F1 score for every predictedgold entity pair in sample and perform optimal matching using the Hungarian algorithm. See Appendix D.2 for details. The entity span reward for sample is defined as the average tokenlevel F1 over all matched pairs: Rs = 1 (cid:88) Fij, (i,j)N (6) 4 where Fij denotes the token-level F1 score between the i-th predicted entity and the j-th gold entity, denotes the set of successfully matched entity pairs, and = is the number of matched pairs. And the type reward Rtype is formed as: Rt = 1 (cid:88) 1{ˆti = ti}, (7) where 1 denotes the indicator function, which equals 1 if the predicted type ˆti matches the gold type ti and 0 otherwise. The sample-level reward Rt is the average over the matched pairs. Visual Grounding and Entailment Rewards. The grounding reward function Ru considers the Intersection-over-Union (IoU) metric, which measures the overlap between predicted bbox and gold bbox as the intersection area divided by their union. Ru is formed as: Ru = 1 k (cid:88) max(0, IoUi σ 1 σ ), (8) where IoUi denotes the IoU between the i-th predicted and gold bbox, and we apply threshold σ and bounding boxes with an IoU below the threshold σ are set 0 of IoU (Liu et al., 2025), while bounding boxes with an IoU above the threshold σ are linearly mapped into [0, 1]. The sample-level reward Ru is the average over the matched pairs. And the entailment reward Re is formed as: Re = 1 (cid:88) i= 1{ˆvi = vi}, (9) vi = 1{li = None}, ˆvi = 1{ˆli = None}, where 1 denotes the indicator function, which returns 1 if the condition inside the braces is true and 0 otherwise, vi and ˆvi respectively indicate whether the gold and predicted entities are visible. For each matched entity, the reward is set to 1 if the predicted and gold locations are both None or both non-None; otherwise, it is 0. The sample-level reward Re is the average over the matched pairs. Finally, our overall reward is weighted combination of the above rewards: = λ1Rc + λ2Rs + λ3Rt + λ4Ru + λ5Re, (10) where λj denotes the weight of each reward term."
        },
        {
            "title": "4.2.2 Optimization with Verifiable Rewards\nGiven verifiable rewards R and remaining data\nD2 = DR\\D1, CVO optimizes the policy to align\ncross-modal verification with the core constraints.\nFor each query q sampled, the current policy πθold\ngenerates G diverse responses {o1, o2, . . . , oG}.\nThe verifiable reward functions compute scores\n{r1, r2, . . . , rG} for each response by R. We then\nobtain the group advantage Ai:\nri − µG\nσG",
            "content": "Ai = (11) , where µG denotes the empirical mean of the group rewards computed over {r1, r2, . . . , rG}, σG denotes their empirical standard deviation. To further improve training efficiency and reduce collapse risk, we apply sampling-based filtering to D2 based on reward distribution statistics. See the Appendix D.3 for details. CVO updates the policy with GRPO style objective. The learning objective uses clipped importance ratio to prevent overly aggressive updates and length normalization to keep responses comparable. The objective function is defined as follows: JCVO(θ) = E[q (Q), πθold(O q)] (cid:88) (cid:32) min 1 πθ(ot q, o<t) πθold(ot q, o<t) At, t=1 (cid:18) πθ(ot q, o<t) πθold(ot q, o<t) clip (cid:19) , 1 ε, 1 + ε At (cid:33) , (12) where clipping with threshold ε prevents overly aggressive updates and length normalization ensures comparability across responses. The design yields stable group preference optimization without critic and supports constraint anchored reasoning in multimodal settings."
        },
        {
            "title": "5 Experiments",
            "content": "We briefly introduce the datasets, baselines, and evaluation metrics used in our experiments, with further details provided in the Appendix E. 5.1 Experimental Setup Datasets. Training datasets are from three sources: Twitter-GMNER (Yu et al., 2023) for GMNER, multi-image MNER dataset MNER-MI (Huang et al., 2024), and visual grounding dataset, GREC (He et al., 2023). We evaluate TwitterGMNER along with its two subtasks (MNER and EEG) in the main experiments, and conduct additional evaluations on MNER-MI and GREC. Baselines. Following prior work (Tang et al., 2025b), we categorize existing approaches into pipeline and unified methods based on whether textual entity extraction and visual region prediction are executed within single pass. Furthermore, we investigate the applicability of open-source and close-source MLLMs within an end-to-end paradigm. To establish robust benchmarks, we implement Chain-of-Thought (CoT) (Wei et al., 2022b), Few-shot prompting (Brown et al., 2020), and Supervised Fine-tuning (SFT) (Ouyang et al., 2022) as strong baselines for comparison. Evaluation Metrics. Following Yu et al., 2023, we evaluate GMNER, MNER and VG using Precision, Recall and F1 score. For the subtasks in GMNER, MNER identifies and classifies entities, while EEG grounds named entities in the image."
        },
        {
            "title": "5.2 Main Results",
            "content": "regarding trainingAs presented in Table 1, free approaches, the introduction of explicit reasoning via Chain-of-Thought (CoT) or Fewshot prompting yields notable performance gains compared to the direct application of MLLMs. Upon integrating our proposed MCR framework (with MRSI and CVO), all MLLMs consistently outperform existing baselines. Specifically, on GMNER, the proposed method improves over the previous best unified method MQSPN (Tang et al., 2025b) by 11.87% F1 scores and over the best Type Methods GMNER MNER EEG Pre Rec F1 Pre Rec F1 Pre Rec Pipeline Unified End-to-End ITA-VinVL-EVG (Wang et al., 2022a) 52.4 50.8 51.6 80.4 78.4 79.4 56.6 54.8 55.7 BARTMNER-VinVL-EVG (Yu et al., 2023) 52.5 52.4 52.5 80.7 80.1 80.4 55.7 55.6 55.7 SCANNER (Ok et al., 2024) ReFineG (Tang et al., 2025a) UnCo (Tang et al., 2025c) 68.3 68.7 68.5 54.1 60.2 57.0 64.6 - - - 69. - - 81.7 - - - - - - - - - - - - MNER-QG (Jia et al., 2023) H-Index (Yu et al., 2023) TIGER (Wang et al., 2023) MQSPN (Tang et al., 2025b) GLM4.5VL (Hong et al., 2025) +CoT +CoT+3-Shot Qwen2.5VL-72B (Bai et al., 2025) +CoT +CoT+3-Shot Qwen2.5VL-7B (Bai et al., 2025) +CoT +CoT+3-Shot +SFT +MRSI (ours) +MRSI+CVO (ours) MimoVL-7B (Yue et al., 2025) +CoT +CoT+3-Shot +SFT +MRSI (ours) +MRSI+CVO (ours) 53.0 54.8 53.9 78.2 78.6 78.4 58.5 56.6 57.5 56.2 56.7 56.4 79.4 80.1 79.7 60.9 61.5 61.2 55.8 57.5 56.6 79.9 80.1 80.3 60.7 61.8 61.3 59.0 58.5 58.8 81.2 79.7 80.4 61.9 62.9 62.4 33.0 44.4 37.8 43.0 57.9 49.4 36.2 48.7 41.6 40.9 50.3 45.1 53.7 66.1 59.3 44.6 54.8 49.2 43.2 55.5 48.5 53.1 68.3 59.7 47.2 60.7 53.1 24.0 44.5 31.2 32.2 59.8 41.9 26.3 48.9 34.2 30.4 45.2 36.3 40.5 60.3 48.4 33.7 50.2 40.3 33.0 52.3 40.5 47.0 74.4 57.6 37.2 58.8 45.6 5.40 14.1 7.80 9.80 25.3 14.1 6.10 15.9 8.80 11.4 13.6 12.4 20.2 24.0 21.9 12.9 15.4 14.1 16.5 33.7 22.2 27.5 56.2 37.0 18.3 37.3 24.5 63.3 62.0 62.7 83.0 81.3 82.2 65.8 64.4 65.1 69.1 68.1 68.6 82.4 81.2 81.8 72.1 71.0 71.5 70.5 70.8 70.6 82.6 82.9 82.8 73.2 73.5 73.4 9.60 10.9 10.2 20.5 23.3 21.8 10.6 12.0 11.2 11.9 17.3 14.1 22.1 32.2 26.2 13.1 19.0 15.5 15.0 21.4 17.7 29.6 42.1 34.8 17.8 25.3 20.9 63.5 60.6 62.0 81.7 78.0 79.8 67.0 63.9 65.4 66.1 65.5 65.8 81.5 80.8 81.1 69.8 69.2 69.5 69.4 69.7 69.6 82.2 82.5 82.3 72.8 73.1 72. Table 1: Comparison on GMNER, MNER and EEG. Pre, Rec and F1 respectively denote Precision, Recall and F1 score. Best in each block is bold. pipeline method SCANNER (Ok et al., 2024) by 2.11% F1 scores. Moreover, using Qwen2.5VL7B, the proposed method respectively outperforms Qwen2.5VL-72B and. And the proposed method respectively surpasses direct Supervised FineTuning (SFT) by 8.05% and 7.57% F1 scores on Qwen2.5VL-7B and MimoVL-7B. On MNER, the proposed method outperforms all methods at least 2.33% F1 scores. On EEG, the proposed method exceeds the best unified method MQSPN by 10.97% F1 scores."
        },
        {
            "title": "5.3 Performance on uni-modal bias dataset",
            "content": "Pipeline methods often decompose GMNER into MNER and VG, where the bidirectional modality biases in GMNER also manifest separately. Beyond GMNER, MCR further leverages datasets from these tasks for training and evaluation, using MNER-MI (Huang et al., 2024) for MNER and GREC (He et al., 2023) for VG MNER-MI features weak textimage correlation, making visual bias more likely, while GREC may induce textual bias when description corresponds to zero region. We evaluate SFT and MCR on MimoVL-7B and Qwen2.5VL-7B across these datasets. Performance on MNER. Table 2 show that MCR outperforms SFT on both tasks. Within MCR, the second-stage CVO generally surpasses the firststage MRSI. This indicates that proposed method effectively leverages visual information to support entity extraction and classification while reducing the impact of irrelevant noise in the image. Performance on VG. In GREC, N-acc and Precision (He et al., 2023) respectively evaluate grounding accuracy for cases with zero target region and with target region. N-acc reflects the models ability to judge entailment between text and image and thus partially characterizes textual bias. As shown in Table 2, N-acc improves across models, indicating that our method effectively reduces text bias in MLLMs. 6 Methods MNER-MI GREC-testA GREC-testB Model Method N-Rate (%) N-Count Pre Rec F1 N-acc Pre N-acc Pre Qwen2.5VL-7B - 84.0 +SFT +MRSI 82.1 +MRSI+CVO 84. MimoVL-7B - 81.8 +SFT +MRSI 83.2 +MRSI+CVO 84.7 - 83.6 81.8 86.2 - 80.8 83.4 85.0 - 83.8 82.0 85.1 - 81.23 83.3 84. - 70.6 74.2 74.7 - 65.7 72.6 75.7 - 81.1 89.1 90.4 - 66.1 89.4 90.4 - 70.2 70.3 69.7 - 69.6 68.6 71. - 62.5 71.5 72.8 - 45.6 71.6 73.0 Table 2: Results on MNER-MI and GREC. For GREC dataset, we remove cases where single textual description corresponds to multiple regions in the image. 5.4 Component Analysis Methods GMNER MNER EEG Pre Rec F1 Pre Rec Pre Rec F1 Ours w/o MRSI w/o CVO w/o DR w/o Inst 70.5 50.2 68.9 69.5 67.3 70.8 53.2 68.6 68.8 66.1 70.6 51.7 68.8 69.2 66. 82.6 64.3 81.9 81.6 81.7 82.9 68.1 81.5 80.7 80.2 82.8 66.2 81.7 81.1 80.9 73.2 54.2 72.2 72.8 70.8 73.5 57.5 71.9 72.1 69.5 73.4 55.8 72.0 72.5 70. Table 3: Ablation results on GMNER, MNER, and EEG. w/o DR means removing diverse reasoning styles and training MCR with single style. w/o Inst means removing the components that specify stepwise crossmodal verification and cautionary guidelines from the original instructions. Compelling the model to reason is essential. In Table 3, w/o MRSI directly applying CVO on DR while skipping MRSI, and we observe that the F1 score drops by 18.95%. This indicates the stepwise cross-modal verification path established by MRSI is necessary prerequisite. Verifiable rewards can further improve the models reasoning capability. As shown by w/o CVO in Table 3, using the CVO data to continue MRSI yields only marginal 0.18% improvement. In contrast, strengthening the models reasoning with CVO adds to 2.04% gain. This contrast highlights the necessity of CVO for further enhancing the models cross-modal verification capability. Multi-style reasoning schema enhance training performance. w/o DR eliminates reasoning diversity, and training MCR with single reasoning style. It slows down the CVO stage and results in smaller performance gains. This degradation is caused by reduced coverage and exploration of reasoning trajectories and shortcut reliance. Qwen2.5VL-72B GLM4.5VL Qwen2.5VL-7B MimoVL-7B Direct Prompt CoT CoT+3-Shot Direct Prompt CoT CoT+3-Shot CoT+3-Shot MCR(ours) CoT+3-Shot MCR(ours) 29.2 15.6 5.8 26.9 24.3 13.8 13.2 0.1 24.3 0.2 1372 610 241 898 820 363 3 637 5 Table 4: Quantitative results of visual bias. N-Count means the number of recalled entities that are absent from the sentence and N-Rate means the proportion of such entities among all recalled entities. Direct Prompt denotes concise task instruction. Figure 3: Quantitative results of textual bias. MCR effectively improves the models ability to determine whether an entity is present, which in turn indicates that MCR mitigates textual bias. Reasoning-related instruction components effectively guide the model. w/o Inst removes the components that specify stepwise cross-modal verification and cautionary guidelines for MCR training from the original instructions. Without this constraint-aware guidance, the model struggles to establish clear intermediate goals and consistent verification criteria, which weakens execution fidelity at key steps and results in 3.90% drop in F1 score under the same training conditions."
        },
        {
            "title": "5.5 Further Analysis",
            "content": "MCR effectively mitigates visual bias in GMNER. Directly inspecting every test image to quantify how MCR handles visual bias is impractical, so we introduce two indirect metrics. Based on whether recalled entity appears in the input sentence, we define N-Count as the number of recalled entities that are absent from the sentence, and N-Rate as the proportion of such entities among all recalled entities. As shown in Table 4, training-free approaches such as CoT and few-shot prompting can partially alleviate visual bias in GMNER. In contrast, MCR reduces visual 7 Figure 4: Effect of Multi-style vs. Single-style Reasoning Schema on F1 and Reward Scores in CVO on Qwen2.5VL-7B (Left) and MimoVL-7B (Right). Single means single-style reasoning schema and Multi means multi-style reasoning schema. Figure 5: Effect of Multi-style vs. Single-style Reasoning Schema on Cross Entropy (Left) and Mean Completion Length (Right) in CVO on MimoVL-7B. bias for Qwen2.5VL-7B and MimoVL-7B to nearnegligible level. MCR effectively mitigates textual bias in GMNER. Inspired by the N-acc metric in GREC, we introduce three metrics to quantify textual bias in GMNER. N-Pre measures the fraction of predicted text-only triples with location None that correctly match gold text-only triples, while N-Rec measures the proportion of gold text-only triples that the model correctly predicts as having no location. N-F1 is the harmonic mean of NPre and N-Rec. As shown in Figure 3, MCR on Qwen2.5VL-7B improves over SFT by nearly +14% across all three metrics, indicating effective mitigation of textual bias. Notably, N-Pre is substantially higher than N-Rec, suggesting that the model conservatively recalls text-only entities to achieve more accurate entailment judgments. Multi-style reasoning schema help improve the training effectiveness of CVO. Figure 4 compares single-style and multi-style reasoning schema during CVO training in terms of both reward and F1 score. At the early stage, singlestyle reasoning achieves higher rewards and F1 scores due to more focused supervision from MRSI. However, we observe that multi-style 8 Figure 6: Case Studies of MCR Mitigating Modality Bias. reasoning yields higher rewards and F1 scores finally, indicating its stronger ability to stimulate exploration and support more effective policy optimization. Moreover, it leads to more stable optimization, while single-style reasoning suffers from larger fluctuations. Controlled Policy Exploration in CVO. As shown in Figure 5, when multi-style reasoning schema is used in CVO, the cross-entropy increases gradually and then stabilizes, indicating that the model performs controlled and effective exploration over diverse reasoning strategies. Meanwhile, the mean completion length first decreases and then converges, suggesting that the model gradually settles into more concise and consistent reasoning patterns. In contrast, when CVO is trained with single-style reasoning schema, the cross-entropy exhibits only very slow increase, while the mean completion length fluctuates without clear convergence. Case Study. As shown in Figure 6, Naive Endto-end methods may incorrect grounding due to insufficient cross-modal verification, such as grounding the NBA logo to the textual entity NFL. MCR mitigates this by explicitly reasoning over imagetext consistency. More cases are provided in Appendix E.5."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we advance GMNER by reformulating it as an end-to-end generative reasoning task. We diagnose critical pathologymodality biasrevealing that MLLMs often rely on unimodal cognitive shortcuts rather than rigTo address orous cross-modal verification. this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning to mitigate modality bias. Comprehensive evaluations on GMNER, MNER, and Visual Grounding benchmarks demonstrate that MCR effectively mitigates modality biases, enabling rigorous cross-modal verification and superior performance compared to achieves existing baselines. Besides, comprehensive suite of ablation experiments validates the necessity of our design choices and the stability of the optimization mechanism."
        },
        {
            "title": "7 Limitations",
            "content": "Despite the promising performance of MCR in mitigating modality bias across GMNER, MNER, and VG tasks, our framework remains constrained by the inherent parametric knowledge limits of the underlying MLLMs. Specifically, MCR relies on the models internal knowledge base for entity recognition; consequently, it may struggle to generalize to unseen entities that are absent from the pre-training corpus."
        },
        {
            "title": "References",
            "content": "Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023. Llm based generation of item-description for recommendation system. In Proceedings of the 17th ACM conference on recommender systems, pages 12041207. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Xigang Bao, Mengyuan Tian, Luyao Wang, Zhiyuan Zha, and Biao Qin. 2024. Contrastive pre-training with multi-level alignment for grounded multimodal named entity recognition. In Proceedings of the 2024 international conference on multimedia retrieval, pages 795803. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language In International Conference on Machine models. Learning, pages 1076410799. PMLR. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Shuting He, Henghui Ding, Chang Liu, and Xudong Jiang. 2023. Grec: Generalized referring expression comprehension. arXiv preprint arXiv:2308.16182. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, and 1 others. 2025. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Shizhou Huang, Bo Xu, Changqun Li, Jiabo Ye, and Xin Lin. 2024. Mner-mi: multi-image dataset for multimodal named entity recognition in social media. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1145211462. Meihuizi Jia, Lei Shen, Xin Shen, Lejian Liao, Meng Chen, Xiaodong He, Zhendong Chen, and Jiaqi Li. 2023. Mner-qg: An end-to-end mrc framework for multimodal named entity recognition with query grounding. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 80328040. Aybora Koksal and Aydin Alatan. 2025. Milchat: Introducing chain of thought reasoning and grpo to multimodal small language model for remote sensing. arXiv preprint arXiv:2505.07984. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626. Sicong Leng, Yun Xing, Zesen Cheng, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, and Lidong Bing. 2024. The curse of multi-modalities: Evaluating hallucinations of large multimodal models across language, visual, and audio. arXiv preprint arXiv:2410.12787. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. 2025a. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542. Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, and Gang Pan. 2024a. Llms as bridges: Reformulating grounded multimodal named entity recognition. arXiv preprint arXiv:2402.09989. Linfeng Li, Peng Wang, Jun Yan, Yao Wang, Simin Li, Jinpeng Jiang, Zhe Sun, Buzhou Tang, Tsung-Hui Chang, Shenghui Wang, and 1 others. 2020. Realworld data medical knowledge graph: construction and applications. Artificial intelligence in medicine, 103:101817. Xingzuo Li, Kehai Chen, Yunfei Long, and Min Zhang. 2024b. Llm with relation classifier for document-level relation extraction. arXiv preprint arXiv:2408.13889. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, and 1 others. 2025b. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Peipei Liu, Hong Li, Yimo Ren, Jie Liu, Shuaizong Si, Hongsong Zhu, and Limin Sun. 2024. Hierarchical aligned multimodal learning for ner on tweet posts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1868018688. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785. Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. Multimodal named entity recognition arXiv preprint 2018. for short social media posts. arXiv:1802.07862. Hyunjong Ok, Taeho Kil, Sukmin Seo, and Jaeho Lee. 2024. Scanner: Knowledge-enhanced approach for robust multi-modal named entity recognition of unseen entities. arXiv preprint arXiv:2404.01914. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744. Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Léonard Hussenot, Orgad Keller, and 1 others. 2023. Factually consistent summarization via reinforcement learning with textual entailment feedback. In Proceedings of the 61st annual meeting linguistics the association for computational of (volume 1: long papers), pages 62526272. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, HeungYeung Shum, and Jian Guo. 2023. Think-ongraph: Deep and responsible reasoning of large language model on knowledge graph. arXiv preprint arXiv:2307.07697. 10 Jielong Tang, Shuang Wang, Zhenxing Wang, Jianxing Yu, and Jian Yin. 2025a. Refineg: Synergizing small lowresource grounded multimodal ner. arXiv preprint arXiv:2509.10975. supervised models and llms for Jielong Tang, Zhenxing Wang, Ziyang Gong, Jianxing Yu, Xiangwei Zhu, and Jian Yin. 2025b. Multigrained query-guided set prediction network for grounded multimodal named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2524625254. Jielong Tang, Yang Yang, Jianxing Yu, Zhen-Xing Wang, Haoyuan Liang, Liang Yao, and Jian Yin. 2025c. Unco: Uncertainty-driven collaborative framework of large and small models for grounded the 2025 multimodal ner. Conference on Empirical Methods in Natural Language Processing, pages 76447662. In Proceedings of Yihong Tang, Kehai Chen, Muyun Yang, Zhengyu Niu, Jing Li, Tiejun Zhao, and Min Zhang. 2025d. Thinking in character: Advancing role-playing agents with role-aware reasoning. arXiv preprint arXiv:2506.01748. Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2024a. mdpo: Conditional preference optimization arXiv for multimodal preprint arXiv:2406.11839. large language models. Jieming Wang, Ziyan Li, Jianfei Yu, Li Yang, and Rui Xia. 2023. Fine-grained multimodal named entity recognition and grounding with generative the 31st ACM In Proceedings of framework. International Conference on Multimedia, pages 3934 3943. Xinyu Wang, Min Gui, Yong Jiang, Zixia Jia, Nguyen Bach, Tao Wang, Zhongqiang Huang, and Kewei Tu. 2022a. Ita: Image-text alignments for multimodal named entity recognition. In Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: human language technologies, pages 31763189. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Ziqi Wang, Chen Zhu, Zhi Zheng, Xinhang Li, Tong Xu, Yongyi He, Qi Liu, Ying Yu, and Enhong Chen. 2024b. Granular entity mapper: Advancing fine-grained multimodal named entity recognition and grounding. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 32113226. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022a. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. in multimodal large language model. arXiv preprint arXiv:2505.20977. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, and Tong Zhang. 2025. Reinforce-ada: An adaptive sampling framework for reinforce-style llm training. arXiv preprint arXiv:2510.04996. Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, and Min Zhang. 2025. Memory-augmented query reconstruction for llm-based knowledge graph reasoning. arXiv preprint arXiv:2503.05193. Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, and 1 others. 2025. R1-sharevl: Incentivizing reasoning capability of multimodal large language models via share-grpo. arXiv preprint arXiv:2505.16673. Jianfei Yu, Ziyan Li, Jieming Wang, and Rui Xia. 2023. Grounded multimodal named entity recognition the 61st on social media. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9141 9154. In Proceedings of Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, and oothers. 2025. Mimo-vl technical report. arXiv preprint arXiv:2506.03569. Pingrui Zhang, Xianqiang Gao, Yuhan Wu, Kehui Liu, Dong Wang, Zhigang Wang, Bin Zhao, Yan Ding, and Xuelong Li. 2025a. Moma-kitchen: 100k+ benchmark for affordance-grounded last-mile navigation in mobile manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 63156326. Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. 2024. Questionguided knowledge graph re-scoring and injection for knowledge graph question answering. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89728985, Miami, Florida, USA. Association for Computational Linguistics. Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, and Lili Qiu. 2025b. Reasongen-r1: Cot for autoregressive image generation models through sft and rl. arXiv preprint arXiv:2505.24875. Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. 2025c. Evaluating and steering modality preferences 11 Yu Zhang, Mufan Xu, Xuefeng Bai, Kehai Chen, Pengfei Zhang, Yang Xiang, and Min Zhang. 2026. Instruction anchors: Dissecting the causal dynamics of modality arbitration. Zefeng Zhang, Hengzhu Tang, Jiawei Sheng, Zhenyu Zhang, Yiming Ren, Zhenyang Li, Dawei Yin, Duohe Ma, and Tingwen Liu. 2025d. Debiasing multimodal large language models via noise-aware preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 9423 9433. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, and 1 others. 2025. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2973329735. Xiangqing Zheng, Chengyue Wu, Kehai Chen, and Min Zhang. 2025. Locot2v-bench: benchmark for longform and complex text-to-video generation. arXiv preprint arXiv:2510.26412. Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. 2023. comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys, 56(4):162. Ruiyang Zhou, Shuozhe Li, Amy Zhang, and Liu Leqi. 2025. Expo: Unlocking hard reasoning with selfexplanation-guided reinforcement learning. arXiv preprint arXiv:2507.02834. Sining Zhoubian, Dan Zhang, and Jie Tang. 2025. Restrl: Achieving accurate code reasoning of llms with optimized self-training and decoding. arXiv preprint arXiv:2508.19576. Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. 2025. Benchmarking and improving large vision-language models for fundamental visual graph understanding and reasoning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3067830701, Vienna, Austria. Association for Computational Linguistics. Fei Zuo, Kehai Chen, Yu Zhang, Zhengshan Xue, and Min Zhang. 2025. InImageTrans: Multimodal LLMbased text image machine translation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2025620277, Vienna, Austria. Association for Computational Linguistics."
        },
        {
            "title": "A Ethical Considerations",
            "content": "Modalityand Taskspecific Constraints A.1 Potential Risks Potential risks associated with our work include the misuse of entity grounding capabilities for surveillance purposes and the possibility of model hallucinations leading to misinformation. We mitigate these risks by using only publicly available datasets and strictly filtering harmful content, but we urge practitioners to exercise caution and respect user privacy when deploying these models in real-world applications. A.2 Use of LLM In the preparation of this manuscript, we utilized Large Language Models (LLMs) for grammatical error correction and polishing to improve readability. A.3 Code and Data All images and generated text contexts, which we use to train MCR, strictly follow guidelines designed to exclude any harmful, unethical, or offensive content. Furthermore, the data used in MCR does not involve any comparisons of harmful, ethical, or offensive content between image pairs. Our code and curated dataset annotations will be released under an open-source license (e.g., MIT or CC-BY 4.0) upon acceptance, and we strictly adhere to the licensing terms and usage policies of the original datasets (Twitter-GMNER, MNER-MI, GREC) and backbone models used in this work. Grounded Multimodal Named Entity Recognition (GMNER) task requires, given text and paired image, recognize the meaningful and specific . Entity entities from the text that are ... type classification is primarily based on textual information, with visual cues considered when the text is insufficient to make confident determination. When you predict each entitys location, if the entity appears in the image, the corresponding location is bounding box (bbox); if the entity does not appear in the image, the corresponding location is None ... Given piece of age:<image><sentence>. The ground-truth labels for this image-text pair in the GMNER task are: <label>. Now generate the reasoning process that leads from the image-text pair to the ground-truth labels. The content of the thought process should be your reasoning on how to obtain the true label based on the image and text input. text and its paired im-"
        },
        {
            "title": "C Multiple Styles Reasoning Schema",
            "content": "We construct diverse reasoning styles and paths using templates, LLMs, and MLLMs, and we design corresponding prompts for each style. We next illustrate the procedure with the GMNER task. C."
        },
        {
            "title": "Instruction",
            "content": "To ensure the model understands the task, we first design task-introduction instruction and use it across all experiments."
        },
        {
            "title": "Instruction for GMNER",
            "content": "Here is Grounded Multimodal Named Entity Recognition task. Given text and paired image, You need to identify all entities in the given text, assign each entity category, and locate the corresponding entities in the image during the entity prediction. Modality-specific Constraints Here is one of the instructions used for distillation, which requires the model to consider the relevant modalities during execution and to produce results consistent with the labels. Instructions to format the thought process. To ensure that the generated reasoning is produced in formatted style constrained by fixed tags, we design two instruction prompts for distinct reasoning routes, and we next present one of the instructions. 1 Formal Type Prompt for GMNER Distill Type Prompt for GMNER To accomplish this task, follow the steps below and place your reasoning and the results of each step inside the <process></process> tags. 1.First, prioritize the textual information; use the image only as supplement. From the text, identify how many entities are present (zero or more) and list them. Important: do NOT extract entities that appear only in the image but not in the text, and do NOT omit entities that appear in the text but not in the image. Put the number of entities inside <entity_num></entity_num> tags. 2.Second, determine the type of each extracted entity. Use one of these labels: person, organization, location, miscellaneous. Put each entity and its type inside <mner></mner> tag in the format: (entity text, entity type). 3.Third, decide whether each entity is visible For entities not visible in the in the image. inside an invisible) image, place (entity text, <entailment></entailment> tag. For entities that are visible, place (entity text, visible) inside an <entailment></entailment> tag. 4.Fourth, provide location information: for visible entities, give their bounding box as (x1, y1, x2, y2); for invisible entities, use None. Put each item inside <location></location> tag in the format:(entity text, (x1, y1, x2, y2)) for visible entities;(entity text, None) for invisible entities. After completing the steps above, synthesize your findings and produce the final answer of the task inside <answer></answer> tags. Instructions to output thought process from LLMs. To elicit reasoning in questionanswer or few-conclusion style, we design two instruction prompts for distinct reasoning routes, and we next present one of the instructions. The LLMaugmented reasoning paths are generated using similar prompts."
        },
        {
            "title": "Conclusion Type Prompt for GMNER",
            "content": "To accomplish this task, you need to follow the reasoning rules below to carry out step-by-step reasoning and accomplish the task objectives. Goal and Reasoning rules: 1. Extract all special entities that appear in the TEXT (do not invent entities that only appear in the image but not in the text). 2. Assign type to each special entity from (person, organization, location, miscellaneous). 3. Decide if each special entity is visible in the IMAGE. 4. If visible, provide the bbox coordinates as (x1, y1, x2, y2) of the special entity in the IMAGE; if not visible, use None as location information. Put your reasoning in <process></process> tags and put the final answer in format of ( entity text, entity type, None or bounding box ) in <answer></answer>. Instructions to output distilled thought process. We use the follow instruction to make MLLMs output distilled style reasoning paths. 2 To accomplish this task, you need to carefully analyze this task step by step from the perspectives of the meaning of special entities, text semantics, image content, whether the special entities appear in the image, and the corresponding location information, and place the thought process within <process></process> tags, and put the triplet results in format of ( entity text, entity type, None or bounding box ) within <answer></answer> tags. C.2 Reasoning Styles and Paths The above diversified instructions yield varied reasoning styles and paths, and we next illustrate two example reasoning processes on single sample. Different reasoning styles or paths produce different output formats, and even within the same sample, the ordering of entity triples as well as the ordering of the entity, type, and location within each triple can vary. Thought Process v1 Determine Entity Types: <process> Lets analyze both the text and the image step by step: 1. Identify Entities in the Text: The text mentions \"Spurs\" and \"Premier League\". 2. \"Spurs\" refers to Tottenham Hotspur Football Club, which is \"Premier League\" is also an an organization. organization, specifically professional football league. 3. Check Visibility in the Image: In the image, there is visible logo and text related to \"Spurs\". The logo of horse and the word \"SPURS\" can be clearly seen on the cake box. There is no visible mention or depiction of \"Premier League\" in the image. 4. Locate Entities in the Image: For \"Spurs\", the logo and text are present. By examining the image, the bounding box coordinates for \"Spurs\" can be determined as (405, 216, 558, 324), which encompasses the area where the \"Spurs\" logo. Since \"Premier League\" is not visible in the image, its location is marked as None. 5. Compile the Labels: (Spurs, organization, (405, 216, 558, 324)) (Premier League, organization, None) </process> Thought Process v2 <process> In the text, Ive identified \"Spurs\" as an entity, which appears to be visible in the image at the coordinates (405, 216, 558, 324). Based on my understanding, \"Spurs\" refers to Tottenham Hotspur Football Club, an organization in the English football league system. also recognize \"Premier League\" as an entity in the text, though it doesnt seem to be visually present in the image. The Premier League is the top level of the English football league system. In total, there are two entities in the text. </process>"
        },
        {
            "title": "D More Details about Rewards",
            "content": "D.1 Entity Count Rewards to avoid visual bias. After MRSI, the model tends to conservatively recall entities To increase recall while preventing visual bias from reappearing, we introduce entity count reward defined by the difference between the predicted and gold entity counts. The penalty is scaled by the true count: harsher penalties are applied when the true count is small, while penalties are more lenient when the true count is large. The reward is formed as: wo(q) = wu(p) = 0.4, 0.2, 0.1, 0, 1 2, 3 4, 5, otherwise. 0 2, 3 4, 0.5, 0.3, 0.2, 5, 0, otherwise. (13) (14) Rc = = q, 1, max(cid:0)0, 1 (p q)wo(q)(cid:1), > > 0, max(cid:0)0, 1 (q p)wu(p)(cid:1), 0 < < q, 0, otherwise. (15) where and respectively denote the numbers of predicted and gold entities in sample, wo and wu are the penalty weights for excessive recall and insufficient recall. The reward Rcount is computed as function of the relationship between and q. ˆei,k and ej,k denote the k-th token in the predicted and gold spans respectively, we first compute the length of their longest contiguous token overlap wij between them. Then, we define the token-level precision and recall as: Pij = wij , Rij = wij (16) where and respectively denote the numbers of tokens in the predicted and gold spans. The tokenlevel F1 score for this pair is finally computed as: Fij = 2PijRij Pij + Rij . (17) Given the token-level F1 matrix {Fij} over all predictedgold span pairs, we apply the Hungarian algorithm to obtain an optimal one-to-one matching between predicted and gold entities. Let denote the set of matched index pairs (i, j) and = be the number of matched pairs. The entity span reward for sample is then defined as the average token-level F1 over all matched pairs: Rs = 1 (cid:88) Fij. (i,j)N (18) D.3 Data Preparation To prevent over-reliance on fixed templates and mitigate training collapse (Xiong et al., 2025) in CVO, we construct multi-style set of reasoning schema DR during MRSI. We then use only subset D1 for MRSI, and allocate the remainder D2 = DRD1 to CVO for calibrating and optimizing cross-modal verification on core constraints. To further improve training efficiency and reduce collapse risk, we apply sampling-based filtering to D2. For each samples responses {o1, o2, . . . , oG} with rewards {r1, r2, . . . , rG}, we compute the standard deviation, maximum reward, and median reward, and impose preset thresholds on these statistics. Specifically, only samples with standard deviation of at least 0.1, maximum group reward of at least 0.8, and median group reward between 0.08 and 0.6 are retained for training. (Zhoubian et al., 2025)."
        },
        {
            "title": "E More Experiment Details",
            "content": "E.1 Datasets D.2 Token-level F1 score For each predicted entity span ˆei = {ˆei,1, . . . , ˆei,n} and gold entity span ej = {ej,1, . . . , ej,m}, where Our training data are drawn from three sources: Twitter-GMNER (Yu et al., 2023) for GMNER and NER, multi-image multimodal NER dataset 3 Dataset Train Val Test GMNER MNER-MI GREC 7000 6856 14000 1500 860 1500 860 19066 Table 5: Dataset statistics used in our experiments. MNER-MI (Huang et al., 2024), and generalized visual grounding dataset GREC (He et al., 2023). Because GREC includes cases where one textual description corresponds to multiple image regions, which is incompatible with the GMNER setting, we exclude samples in which single textual description corresponds to multiple image regions. We evaluate Twitter-GMNER in the main experiments, and conduct additional evaluations on MNER-MI and GREC in Section 5.3. As shown in Table 5, we also report the number of raw datasets used during training and evaluation. For GMNER and MNER-MI, we use the full datasets. For GREC, we first filter out multi-target cases, then select 14,000 samples from the remaining data for training, while retaining all remaining validation and test samples. In total, we use 55,712 samples annotated with multi-style reasoning schema for training. E.2 Baselines Following prior work (Tang et al., 2025b), we categorize GMNER approaches into unified and pipeline methods. Unified methods use pretrained language models to extract entitytypelocation triples in single pass, while pipeline methods decompose the process into multiple stages handled by different models. Unified approaches reduce error propagation and improve over early pipelines (Wang et al., 2022a; Yu et al., 2023), but recent pipeline methods that incorporate LLMs as knowledge bases achieve substantially better performance (Li et al., 2024a; Ok et al., 2024). In contrast to prior unified methods that still rely on auxiliary components and employ LLMs as auxiliary tools, we propose an end-to-end unified approach that uses MLLMs to complete all steps in single inference. type pairs, and then uses the Entity Extraction & Grounding (EEG) model to predict the bounding box for each pair. (3) Scanner (Ok et al., 2024) first identifies textual and visual entities using NER and visual grounding models, enriches entity semantics with LLMs and external knowledge bases, and finally matches textual entities to visual locations via trained module. (4) UnCo (Tang et al., 2025c) adopts an uncertainty-aware collaboration between small models and large multimodal language models to refine grounded multimodal named entity recognition predictions. (5) ReFineG (Tang et al., 2025a) combines small supervised models and large language models to enhance low-resource grounded multimodal named entity recognition through refinement and knowledge transfer. Unified Methods. (1) MNER-QG (Jia et al., 2023) formulates multimodal named entity recognition as an unified machine reading comprehension task, where entity queries are grounded to both (2) Htextual context and visual evidence. index (Yu et al., 2023) formulates GMNER to sequence generation task with multimodal (3) TIGER (Wang et al., 2023) BART model. formulates fine-grained named entity recognition and grounding as sequence generation task by converting entity-type-object triples into target text and employing T5 model to jointly predict entity spans, fine-grained types, and corresponding (4) MQSPN (Tang et al., image objects. 2025b) formulates grounded multimodal named entity recognition as set prediction problem that employs multi-grained learnable queries to explicitly align textual entities with visual regions. End-to-end Methods. GLM4.5VL, Qwen2.5VL, and MimoVL are multimodal large language models with strong capabilities in multimodal understanding, reasoning, and visual grounding. We evaluate these models under different prompting and training settings, including direct instruction prompting, Chain-of-Thought (CoT) prompting, and CoT with 3-shot demonstrations. In addition, we include supervised fine-tuning (SFT) baseline, where the models are fine-tuned on GMNER training data. Pipeline Methods. (1) ITA-VinVL-EVG (Wang et al., 2022a) formulates multimodal named entity recognition as an imagetext alignment problem. (2) BARTMNER-VinVL-EVG (Yu et al., 2023) first uses generative model BART to identify entity E.3 Evaluation Metrics GMNER. For GMNER and its two subtasks, MNER and EEG, we follow prior work (Yu et al., 2023) and evaluate performance using Precision (Pre), Recall (Rec), and the F1 score. Each sample contains zero or more entity triples {(ei, ti, li)}k1 i=1, and we compute the correctness of each entity triple as follow: correct = (cid:40) 1, Ce Ct Cl, 0, otherwise, Ce/Ct = (cid:40) 1, ei/ti = ˆei/ˆti, 0, otherwise, Cl = 1, 1, li = ˆli = None, IoU (li, ˆli) 0.5, 0, otherwise, IoU (li, ˆli) = area(li ˆli) area(li ˆli) , (19) (20) (21) (22) where Ce, Ct and Cl represent the correctness of entity, type and location; ei, ti and li represent the gold entity, type and location; ˆei, ˆti and ˆli represent the predicted entity, type and location; IoU denotes the IoU score between li and ˆti; and area refers to the amount of two-dimensional space enclosed by region. predicted entity triple is regarded as correct only when the entity, type and location are all correct. Then Precision (Pre), Recall (Rec), and F1 score are used to evaluate the performance: Pre = #correct #predict , Rec = #correct #gold , F1 = 2 Pre Rec Pre + Rec , (23) (24) where #correct, #predict and #gold respectively represent the number of triples of correct predictions and gold labels. VG. Following prior work (He et al., 2023), we use no-target accuracy (N-acc) to measure localization accuracy when no target entity is present and Precision to measure accuracy when target entity is present, and use Precision (Pre) to measure one-target entity localization. For no-target sample, it considered true positive (TP) when predicted bounding box is None, otherwise false negative (FN). Then N-acc is computed as follow: N-acc = . (25) TP TP + FN For one-target sample, prediction is counted as correct localization only if IoU 0.5. 5 Quantitative metrics for textual bias. Inspired by N-acc, we introduce N-Pre, N-Rec, and N-F1 to quantify textual bias in GMNER, particularly cases where the model assigns bounding boxes to entities absent from the image. For an entity triple whose location is None (li = None), we determine its correctness as follows: n-correct = (cid:40) 1, Ce Cn, 0, otherwise, Cn = (cid:40) 1, 0, li = ˆli = None, otherwise, (26) (27) where Cn represent the correctness of no-target entity location. We compute the above metrics over all no-target entity triples: N-Pre = #n-correct #n-predict , N-Rec = #n-correct #n-gold , (28) N-F1 = 2 N-Pre N-Rec N-Pre + N-Rec , (29) where #n-correct, #n-predict and #n-gold respectively represent the number of triples of correct predictions and gold labels. Quantitative metrics for visual bias. Directly inspecting every test image to quantify how MCR handles visual bias is impractical, so we introduce two indirect metrics that measure image-only entity recall. Based on whether recalled entity appears in the input sentence, we define N-Count as the number of recalled entities that are absent from the sentence, and N-Rate as the proportion of such entities among all recalled entities. Specifically, for input sentence and the model predicted entity triples ˆY = {( ˆei, ˆti, ˆli)}k2 i=1 where k2 is the number of all entity triples, we compute N-Count and N-Rate as follow: N-Count = 1 k2 k2(cid:88) 1{ ˆei / s}, (30) N-Rate = N-Count k2 , (31) where 1 denotes an indicator function that returns 1 if the predicted entity is mentioned in the sentence and 0 otherwise. lower N-Count and N-Rate indicate weaker visual bias, as the model is less likely to hallucinate image-only entities as text mentions. instance, the model erroneously recalls the imageonly entity NBA, and misclassifies the human entity Rory Calhoun due to the presence of cat in the image. By explicitly prompting the model to surface multimodal evidence and reinforcing the principled use of such evidence, MCR effectively alleviates the model clarifies these issues. Specifically, the modality source of each entity and relies on internal knowledge and sentence semantics when determining entity types, rather than being misled by superficial visual cues. Limitations of knowledge and entity span. As illustrated in Figure 8, (a) and (b) present two failure cases of MCR. Although MLLMs incorporate substantial knowledge during training, GMNER requires broad, cross-domain knowledge that inevitably includes entities beyond the models coverage or cases where the model has acquired incorrect knowledge. As result, MLLMs may still fail in such scenarios. For example, even though MCR possesses knowledge about Lady Gaga, its limited visual knowledge of her appearance causes it to be misled by visually similar cues. Similarly, due to the lack of prior knowledge about Ay Ziggy Zoomba, the model makes an incorrect judgment from the outset. These cases illustrate that MCR remains constrained by the underlying models knowledge coverage and visual familiarity. Figure 8(b) further shows that MLLMs still struggle with entity span detection. E.4 Implementation Details We conduct all experiments on 8 NVIDIA Tesla L20 GPUs. Training and inference use the msswift (Zhao et al., 2025) framework, and decoding and sampling use the vLLM (Kwon et al., 2023) engine. All training procedures are conducted using LoRA (Hu et al., 2022). MRSI. we generate diverse reasoning schema using combination of template-based extraction, DeepSeek (Guo et al., 2025), Qwen2.5VL-72B, and Qwen3VL-30B-A3B (Bai et al., 2025). We train Qwen2.5VL for 2 epochs and MimoVL for 5 epochs with learning rate of 0.0001 and cosine learning schedule. We process the data in batches of 16. We trained the model for 4 hours on 8 L20 GPUs. CVO. During the CVO phase, we train for 2 epochs with learning rate of 0.000005 and batch size of 64. We use warmup ratio of 0.05, sample 8 generations per input, set GRPO clipping thresholds to 0.15 and 0.25, and apply temperature 1.5 with top-k sampling (k = 200), top-p sampling (p = 0.95), and β = 0.005. We trained the model for 20 hours on 8 L20 GPUs. E.5 Case Study MCR effectively mitigates modality bias. As illustrated in Figure 7, (a) and (b) present two cases where MCR successfully mitigates textual bias. Naive End-to-end methods lead the model to assign incorrect image regions to textual entities. For example, the model incorrectly grounds the NBA logo to the textual entity NFL, and assigns an elderly male to Donald Trump. By explicitly generating reasoning paths and reinforcing cross-modal consistency verification, MCR effectively alleviates these issues. Specifically, the model recognizes the distinct semantics of the NBA logo in the image and the NFL entity in the text and correctly concludes that they do not match. Similarly, it explicitly verifies whether the person in the image corresponds to Donald Trump, thereby avoiding erroneous grounding. (c) and (d) present two cases where MCR successfully mitigates visual bias. When MLLMs perform entity recognition and classification, they can be distracted by irrelevant visual elements, leading to the spurious recall of image-only entities or incorrect classification of textual entities. For Figure 7: Case Studies of MCR Mitigating Modality Bias. 7 Figure 8: Failure Cases of MCR."
        }
    ],
    "affiliations": [
        "Beijing University of Aeronautics and Astronautics",
        "Harbin Institute of Technology, Shenzhen, China",
        "Institute of Computing Technology Chinese Academy of Sciences"
    ]
}