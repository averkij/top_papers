{
    "paper_title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "authors": [
        "Hyunjong Ok",
        "Jaeho Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options."
        },
        {
            "title": "Start",
            "content": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models Hyunjong Ok1,2 1POSTECH Jaeho Lee1 2HJ AILAB hyunjong.ok@gmail.com, jaeho.lee@postech.ac.kr 6 2 0 2 J 0 2 ] . [ 1 2 5 1 4 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are highly sensitive to prompt structure. Even minor changes in surface formssuch as instruction phrasing, example placements, or reasoning elicitationcan lead to substantial differences in the prediction quality of the models (Wei et al., 2022; Kojima et al., 2022). Despite the importance of this sensitivity for the practical reliability of LLMs, our current understanding remains largely descriptive. We know what LLMs are sensitive to, but not why. For instance, Lu et al. (2022) has shown that permuting the order of demonstrations in in-context learning (ICL) can dramatically affect accuracy, yet offers limited insight into what makes certain orders preferable. Likewise, recent studies on multiplechoice question answering (MCQA) report significant performance fluctuations under option permutations, but stop short of identifying the mechanisms underlying this phenomenon (Pezeshkpour and Hruschka, 2024; Zheng et al., 2024). In this work, we undertake an in-depth study of less obvious but consequential form of promptsensitivity: the ordering of components in MCQA prompts. typical MCQA prompt consists of three Figure 1: Performance gap between CQO and QOC. We measure the average accuracies of 21 decoder-only LLMs on 4 different datasets, when prompted in two distinct structures; CQO (context-question-option) and QOC (question-option-context). elements: context passage (C), question (Q), and set of options (O). Intuitively, reordering these components should have little effect, as their semantic content remains unchanged. Contrary to this expectation, we find that placing the context before the questions and options (CQO) consistently and substantially outperforms the reverse ordering (QOC) across wide range of setups (Figure 1).1 To explain this phenomenon, we formulate three competing hypotheses and evaluate them through series of carefully controlled experiments. By systematically validating (or invalidating) each hypothesis, we aim to uncover the underlying factors that drive context-order sensitivity in MCQA. Specifically, we consider the following hypotheses: Hypothesis 1: Biased training data. CQO-style prompts may be more prevalent in training data, making QOC an unfamiliar format for the model. Hypothesis 2: Failures in option recall. QOC structure makes it difficult for the model to recall the options located in the middle of the prompt, a.k.a. lost-in-the-middle (Liu et al., 2024). Hypothesis 3: Causal attention (winner). The causal attention structure in decoder-only trans1A similar phenomenon has also been noted by Shaier et al. (2024) outside the MCQA context, which also does not demystify why such phenomenon happens. 1 (a) Decoder model (b) Encoder model Figure 2: Decoder vs. Encoder attention. In QOC (QuestionOptionsContext), causal masking prevents decoder models from attending to the context while selecting among options, so they often answer from option priors rather than evidence. Encoder models use bidirectional attention and can condition on the context when scoring the options. formers makes it impossible for the option tokens to attend to the context tokens, creating an information bottleneck (Figure 2). Our experimentsconducted on wide range of datasets and models support the third hypothesis, and rule out the other two hypotheses. Stepping further, we identify several factors that can affect the impact of causal masking, namely the context length and the option positions. Finally, building on the causal-attention-based explanation, we design targeted interventions that can improve the performance of LLMs on QOCstructured prompts, or conversely, degrade the performance on CQO-structured prompts. These results provide additional evidence that causal attention is the driving mechanism of the sensitivity."
        },
        {
            "title": "2 Experimental setup",
            "content": "Datasets. We evaluate on four reading comprehension benchmarks that require context-based reasoning: LogiQA (Liu et al., 2020); RACEH/M (Lai et al., 2017); SciQ (Welbl et al., 2017). All tasks are in MCQA format with four options. Models. We conduct experiments on 21 decoderonly LLMs from four model families: LLaMA 3 (Grattafiori et al., 2024), Qwen 2.5/3 (Yang et al., 2024), and Gemma 2 (Team et al., 2024). Model sizes range from 0.5B to 9B parameters, including both base and instruction-tuned variants. For architecture comparison experiments (3.3), we additionally test Flan-T5 (Chung et al., 2024) family (encoder-decoder) and BERT, RoBERTa, ALBERT (Devlin et al., 2019) family (encoder-only). 2 (a) Performance gap (b) ICL performance Figure 3: Hypothesis 1: Effect of instruction tuning and in-context learning. (a) We compare the performance gaps between base and instruct models, and find they remains remarkably consistent. (b) Few-shot prompting yields marginal gains. These results suggest that the friendly formatting is not the primary driver. Evaluation metrics. We measure accuracy under two prompt orderings: CQO and QOC, and utilize the performance gap ( = AccCQO AccQOC), which quantifies context order sensitivity. Other details. Further details on datasets, models, prompting templates, and evaluation/scoring protocols are provided in Appendix B."
        },
        {
            "title": "3 Hypotheses and analysis",
            "content": "As shown in Figure 1, permuting the prompt order leads to sharp performance drop across all benchmarks. To understand the cause, we formulate and test set of hypotheses. All detailed experiments for each model and dataset are in Section C."
        },
        {
            "title": "3.1 Hypothesis 1: Bias in training samples",
            "content": "Hypothesis. CQO is simply more frequent in the training data than QOC. Thus, in the QOC format, the model may fail to process an unfamiliar format. Experiment. We test the training-distribution hypothesis in two ways. First, CQO-like prompts are more common in instruction data; this hypothesis predicts larger CQO-QOC gap for instruction models. We therefore compare nine matched baseinstruct pairs. Second, we use in-context learning to familiarize models with the QOC format, varying the number of demonstrations up to 5-shot. Results. As shown in Figure 3a, the CQO-QOC gaps are nearly identical, suggesting the phenomenon is not driven by instruction tuning. Moreover, even with 5-shot demonstrations shown in Figure 3b, QOC accuracy improves by only 3.1% and remains far below CQO. Together, these results rule out training distribution as the primary cause. Figure 4: Hypothesis 2: Option recall analysis. To investigate \"forgetting\" the options due to the long context intervention, we evaluated option recall accuracy. Results show that accuracy is consistently higher than CQO, indicating that the models retain option information and ruling out memory loss as the primary cause."
        },
        {
            "title": "3.2 Hypothesis 2: Failure to recall options",
            "content": "Hypothesis. LLMs often fail to recall the information located in the middle of the context (Liu et al., 2024). Thus, in the QOC format, the model may fail to correctly recall the options, which are located between the question and the context. Experiment. After presenting the prompt, we ask the LLMs to recall each option. Precisely, we measure the chance of an exact match for each option. Results. As shown in Figure 4, QOC achieves similar, or even higher, recall accuracy than CQO. This indicates that the failure of option recall may not be the cause of accuracy drops in QOC."
        },
        {
            "title": "3.3 Hypothesis 3: Causal attention",
            "content": "Hypothesis. Causal attention mask prevents option tokens from attending to context in QOC. In decoder-only, each token can only attend to preceding tokens. Thus, in QOC, where options appear before context, option representations are computed without information from the context. Experiment 1: Architecture comparison. If causal masking is the root cause, models with bidirectional attention should exhibit no performance gap. We compare three architecture types: decoder-only (causal), encoder-decoder (bidirectional encoder), and encoder-only (bidirectional). For encoderdecoder models, we feed the entire prompt to the encoder and let the decoder generate the answer. Results 1. As shown in Figure 5a, decoder-only models show 14.72% gap, encoder-decoder models (Flan-T5) show 2.30%, and encoder-only models (BERT, RoBERTa, ALBERT) show nearzero gap (0.02%). This pattern strongly implicates causal masking as the underlying mechanism. Experiment 2: Context removal test. If the context (a) Model design comparison (b) Context position Figure 5: Hypothesis 3, Exp# 1: Architecture comparison. (a) Decoder-only LLMs show large gap. In contrast, encoder-only and encoder-decoder LLMs have minimal accuracy gap, confirming that the causal mask is the primary factor. (b) For decoder-only LLMs, QOC performance drops to nearly QO, indicating that the information inside the context is ignored. (a) Attention decay (b) Context attribution ratio Figure 6: Hypothesis 3, Exp# 2 & 3: Analysis of context utilization. (a) Layer-wise option attention: In CQO, attention to options declines as context is integrated, whereas in QOC it rises. (b) Gradient-based attribution: context tokens contribute substantially more to CQO predictions than to QOC. is effectively inaccessible in QOC, then removing it entirely should not materially change performance. We therefore compare QOC against QO (QuestionOptions only), where the context is omitted. Results 2. As shown in Figure 5b, QOC accuracy is nearly identical to QO, consistent with the model failing to use the context in QOC. This supports the hypothesis that the issue is not forgetting but restricted access induced by the causal mask. Experiment 3: Attention analysis. We analyze how attention to option tokens evolves across layers. Also, we measure token contributions using GradientInput attribution (Shrikumar et al., 2017), aggregating per-token scores over the context span and normalizing by total attribution. Results 3. Option tokens receive exactly zero attention from context tokens in QOC by construction of the causal mask, confirming that the context-tooption pathway is blocked. Across layers, attention to options increases with depth in QOC but decreases in CQO, suggesting that QOC increasingly relies on the options themselves while CQO 3 Short context Long context Dataset LogiQA SciQ RACE-M RACE-H Avg. Length Gap () 70 6.2% 70 7.3% 195 24.8% 305 20.8% (a) Effect of context length."
        },
        {
            "title": "Position",
            "content": "A Gap () 22.4% 20.5% 19.2% 9.9% (b) Effect of answer position. Table 1: Factors influencing causal masking impact. (a) Datasets with longer contexts suffer more from causal masking. (b) The correct answers position affects the gap, with option being the most robust. more effectively integrates contextual information (see Figure 6a). Also, context attribution is 0.797 in CQO but only 0.335 in QOC (see Figure 6b). Why doesnt the final attention fix it? One may argue that the final answer token can still attend to the context in QOC. However, causal masking prevents the options themselves from being contextconditioned: all option hidden states are computed before any context is seen, so they cannot encode evidenceoption alignment. Since later tokens cannot retroactively update earlier states (they can only read them), the model must do evidenceoption comparison only at the final decoding step using context-blind option representations. Modulating factors. Two factors modulate the gap severity. First, longer contexts show larger gaps (Table 1a): more context means more information becomes inaccessible under causal masking. Second, earlier answer positions suffer greater accuracy drops than later ones in QOC (Table 1b), as later options are near the context. Methods CQO + Attention pruning QOC + Activation patching + QOCO (Repeat) LogiQA SciQ RACE-M RACE-H Avg 39.08 30.47 32.94 35.18 35.62 94.16 60.94 86.89 87.64 91.80 74.32 39.52 49.57 62.22 63. 69.48 38.89 48.76 56.94 59.65 69.26 42.46 (-26.8) 54.54 60.49 (+6.0) 62.76 (+8.2) Table 2: Accuracy results on MCQA benchmarks. Performances are the average of decoder-only models. changed. CQO accuracy drops from 69.26% to 42.46% (26.8), with consistent decreases across model families (Qwen: 28.1%, LLaMA: 25.3%, Gemma: 26.9%), indicating that option access to contextual evidence is critical. Activation patching (improving QOC). We restore context-aware option representations in QOC by patching option hidden states with those computed under CQO. For each sample, we run both templates and align corresponding option token positions by exact string matching. At layers in the middle-to-late half of the network (e.g., layers 1223 for 24-layer models, normalized by depth), we replace hQOC opt with hCQO . We patch only opopt tion tokens (not context or question) to isolate the mechanism; patched states come from different template on the same sample, and token alignment is verified by exact string match. This increases QOC accuracy by 6.0 points on average, with larger gains for models exhibiting larger baseline gaps. Option repetition (improving QOC). As simple prompt, we repeat the options after the context (QOCO). The repeated option tokens can attend to the context under the causal mask, requiring no internal model intervention. Repeating options improves QOC by 8.2 points, partially closing the gap without modifying model internals."
        },
        {
            "title": "5 Conclusion",
            "content": "We design targeted interventions that directly manipulate the option and context attention pathway (see Table 2). We apply each intervention to all 21 decoder-only models across four datasets. Attention pruning (degrading CQO). To simulate the constraint imposed by causal masking in QOC, we block option-to-context attention in CQO. For each sample, we identify token spans for the context and the options, and set mask[i, j] = for all pairs where Options and Context. This prevents option tokens from attending to context tokens while leaving all other attention unWe investigated why decoder-only LLMs exhibit huge performance gap between CQO and QOC orderings in MCQA tasks. After ruling out training distribution bias and memory decay, we identified causal attention as the core mechanism: in QOC, the causal mask prevents options from attending to context, rendering it effectively inaccessible. This finding is supported by architecture comparisons (encoder models show no gap), attention analysis, and targeted interventions that successfully degrade CQO or improve QOC performance. Our work provides mechanistic insight into prompt sensitivity and offers practical guidance."
        },
        {
            "title": "Limitations",
            "content": "While our study provides mechanistic insights into prompt order sensitivity, our evaluations are primarily focused on MCQA tasks with four-option formats. However, considering that the identified root causecausal attention maskingis fundamental to decoder-only architectures, we believe our findings regarding the information bottleneck are likely applicable to broader QA formats where context placement varies. Additionally, although we evaluate models up to 9B parameters due to computational constraints, the consistent performance gap observed across 21 diverse models suggests that this behavior is intrinsic to the causal decoder design rather than scale-dependent artifact. Our work establishes rigorous empirical foundation for understanding prompt sensitivity."
        },
        {
            "title": "References",
            "content": "Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 41904197. Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2024. Make your llm fully utilize the context. Advances in Neural Information Processing Systems, 37:6216062188. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, and 1 others. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. 2019. What does bert look at? an analysis of berts attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276286. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language underIn Proceedings of the 2019 Conference standing. of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 4171 4186. Shuoyang Ding and Philipp Koehn. 2021. Evaluating saliency methods for neural language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50345052. Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. 2021. Causal abstractions of neural networks. Advances in Neural Information Processing Systems, 34:95749586. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 785794. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2024. Loogle: Can long-context language models understand long contexts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1630416333. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence (IJCAI), pages 36223628. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems, volume 35, pages 1735917372. Pouya Pezeshkpour and Estevam Hruschka. 2024. Large language models sensitivity to the order of options in multiple-choice questions. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 20062017, Mexico City, Mexico. Association for Computational Linguistics. Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, and Fanny Jourdan. 2025. ConSim: Measuring concept-based explanations effectiveness with automated simulatability. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 55945615, Vienna, Austria. Association for Computational Linguistics. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations. Sagi Shaier, Lawrence Hunter, and Katharina Wense. 2024. It is not about what you say, it is about how you say it: surprisingly simple approach for improving reading comprehension. In Findings of the Association for Computational Linguistics: ACL 2024, pages 82928305. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In International conference on machine learning, pages 31453153. PMlR. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, and 1 others. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in neural information processing systems, 33:12388 12401. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106. Sarah Wiegreffe, Oyvind Tafjord, Yonatan Belinkov, Hannaneh Hajishirzi, and Ashish Sabharwal. 2025. Answer, assemble, ace: Understanding how LMs answer multiple choice questions. In The Thirteenth International Conference on Learning Representations. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations."
        },
        {
            "title": "A Related work",
            "content": "Prompt sensitivity. growing body of work has established that LLMs can be sensitive to prompt design choices, including input order and formatting. In the in-context learning (ICL) setting, Lu et al. (2022) show that reordering the same set of demonstrations can substantially change accuracy and propose strategies to mitigate order sensitivity, but do not explore the underlying internal mechanism. In MCQA, prior work reports large variation under option permutations and develops mitigation methods (e.g., Pezeshkpour and Hruschka (2024); Zheng et al. (2024)), yet largely remains at the level of behavioral characterization. Prompt formatting studies further show that semantically preserving changespunctuation, spacing, labeling, or layoutcan induce sizable performance swings (e.g., Sclar et al. (2024)). More mechanistic MCQA analyses, such as Wiegreffe et al. (2025), use activation patching-style interventions to localize where answer selection is determined and how it is amplified across layers. However, these lines of work do not directly address block-order question that arises with long contexts: whether and how the accessibility of context information to option representations changes with prompt order. Relatedly, Shaier et al. (2024) reports large order effects in reading comprehension, but stops short of providing structural account of the failure mode. In contrast, we study striking order effect in MCQA between CQO and QOC and identify an architectural mechanism: under causal attention, option tokens in QOC are structurally prevented from integrating information from the subsequent context. We then test this mechanism with targeted causal interventions that manipulate the optioncontext information pathway. Mechanistic analysis tools. To connect promptorder sensitivity to internal computation, we draw on set of mechanistic interpretability tools. First, we use attention analysis to characterize how models route information under different prompt orders; prior work has shown the utility of interpreting attention weights as explanations (Clark et al., 2019; Abnar and Zuidema, 2020). We compute attention statistics for each prompt order, use them as descriptive diagnostics, and also perform gradient-based analyses. We employ gradientbased input attribution as an auxiliary diagnostic: GradientInput and related attribution methods quantify how sensitive the prediction is to perturbations of each input token representation, and have been widely used as simple, interpretable featureimportance baselines (Shrikumar et al., 2017; Ding and Koehn, 2021; Poché et al., 2025). We also use causal interventions on internal pathways. In particular, we (i) ablate the option-to-context attention pathway by masking attention edges between token groups (a targeted path/edge ablation), and (ii) apply activation patching (causal tracing/interchange interventions), which swaps hidden states from clean run into corrupted run to test whether specific representation is causally responsible for the downstream behavior (Vig et al., 2020; Geiger et al., 2021; Meng et al., 2022). We adapt these tools to the prompt-order setting by isolating the optioncontext information pathway: attention ablation tests necessity, and activation patching tests sufficiency of context-conditioned option representations for closing the CQO-QOC gap. Long-context failure modes. Prior work shows that LLMs can underutilize long contexts and are sensitive to where relevant evidence appears (e.g., lost-in-the-middle; Liu et al. (2024); Li et al. (2024); An et al. (2024)). In our setting, we use long contexts to test specific hypothesis: QOC may underperform because the model forgets the options after reading long context. Our optionrecall results reject this explanationQOC retains options at least as well as CQOsuggesting that the CQOQOC gap is not primarily long-context memory failure, but distinct prompt-order effect."
        },
        {
            "title": "B Detailed information",
            "content": "To demonstrate the robustness of our findings, we conduct experiments with various models and datasets. Details are below. B.1 Model information Detailed information on the model is in Table 3, which contains the Hugging Face ID of each model. B.2 Dataset information We infer using test sets for all benchmarks. LogiQA is logical reasoning dataset sourced from the Chinese Civil Service Examination. It contains 651 test samples, each requiring multi-step logical inference over given context. Questions cover various reasoning types, including categorical reasoning, conditional reasoning, and disjunctive reasoning. 7 SciQ is science question answering dataset containing 1,000 crowdsourced multiple-choice questions spanning physics, chemistry, and biology. Each question is paired with short supporting passage (averaging 80 words) that provides the necessary context to answer correctly. RACE (Reading comprehension from examinations) consists of English reading comprehension questions collected from Chinese middle and high school exams. We utilize both subsets: RACE-M: Middle school level with 1,436 test samples and simpler passages (250 words on average) RACE-H: High school level with 3,498 test samples and more complex texts (350 words on average) Questions span various types, including detail retrieval, inference, vocabulary, and main idea identification. B.3 Prompting templates and evaluation protocols We employ distinct prompting strategies and evaluation protocols tailored to each model architectureDecoder-only LM), and EncoderEncoder-only (Masked LM), Decoder (Seq2Seq)to ensure fair and optimal performance assessment. (Causal B.3.1 Prompting templates We evaluate two primary template orderings to test context robustness: CQO (Context Question Options) and QOC (Question Options Context). Decoder-only models For standard causal language models (e.g., Llama-3, Qwen, Gemma, Mistral), we use an open-ended completion format that guides the model to predict the next token as the answer label. CQO template Prompt: Context: {Context} Question: {Question} Options: A: {Option A} B: {Option B} C: {Option C} D: {Option D} Among to D, the answer is: QOC template Prompt: Question: {Question} Options: A: {Option A} B: {Option B} C: {Option C} D: {Option D} Context: {Context} Among to D, the answer is: Encoder-only models For masked language models (e.g., BERT, RoBERTa), we utilize Clozestyle task where the model must predict the token at the [MASK] position. CQO template Prompt: Context: {Context} Question: {Question} Options: A. {Option A} B. {Option B} C. {Option C} D. {Option D} Among A, B, C, D, [MASK]. the answer is QOC template Prompt: Question: {Question} Options: A. {Option A} B. {Option B} C. {Option C} D. {Option D} Context: {Context} Among A, B, C, D, [MASK]. the answer is Encoder-Decoder models For sequence-tosequence models (e.g., FLAN-T5), the full prompt is provided to the encoder. The decoder is initialized to generate the answer. We use the 8 \"Full\" variant where the entire content resides in the encoder. CQO template Encoder Input: Context: {Context} Question: {Question} Options: A. {Option A} B. {Option B} C. {Option C} D. {Option D}"
        },
        {
            "title": "The answer is",
            "content": "QOC template Encoder Input: Question: {Question} Options: A. {Option A} B. {Option B} C. {Option C} D. {Option D} Context: {Context}"
        },
        {
            "title": "The answer is",
            "content": "B.3.2 Evaluation protocols Decoder-only models (likelihood scoring). Instead of relying on unconstrained text generation, which requires complex parsing and may yield invalid outputs, we evaluate models using constrained likelihood approach. Given the input prompt, we compute the logits for the next token prediction. We extract the logits corresponding to the valid option tokens (A, B, C, D) and their token variations. We apply Softmax operation over these four values to obtain normalized probability distribution: (Correct) = elogitanswer k{A,B,C,D} elogitk (cid:80) Encoder-only models (masked prediction). We formulate the task as Masked Language Modeling (MLM). The model processes the bidirectional context and predicts the token at the [MASK] position. Similar to the decoder approach, we extract the probabilities for the tokens corresponding to the options A, B, C, and D. To handle tokenizer differences, we select the maximum logit across case variations (e.g., and a) for each option. These maximum logits are then normalized via Softmax to obtain the final probability distribution. Encoder-Decoder models. For T5-based models, we employ the same evaluation protocol as decoderonly models. B."
        },
        {
            "title": "Inference details",
            "content": "All experiments are conducted with greedy decoding single runs, maximum 16 output tokens, bfloat16 precision, and NVIDIA A6000 GPUs."
        },
        {
            "title": "C Additional result",
            "content": "This section provides detailed per-model and perdataset results for all experiments discussed in the main paper. C.1 Full model-dataset results Table 4 shows CQO and QOC accuracy for all 21 decoder-only models across 4 datasets. The average gap is +14.7%. C.2 Base vs instruction-tuned models Table 5 compares CQO-QOC gaps between base and instruction-tuned model pairs. Both variants show consistent gaps (Base: 14.70%, Instruct: 14.12%), indicating that instruction tuning does not mitigate order sensitivity. C. In-context learning results Table 6 shows QOC performance with 0, 1, 3, and 5 in-context examples. ICL provides marginal improvement (+3.1% from 0-shot to 5-shot) but cannot close the gap to CQO (69.26%). C.4 Option recall accuracy The models prediction is the option with the highest probability. This method allows for precise measurement of the models preference even when differences are subtle. Table 7 shows option recall accuracy by dataset. High recall rates in both CQO (93.5%) and QOC (94.7%) confirm that the QOC performance drop is not due to memory failure. Figure 7: Context attribution ratio (CQO vs QOC) by dataset. CQO consistently utilizes context more effectively across all benchmarks. C.5 Encoder and encoder-decoder model results Table 8 shows results for encoder-only models and encoder-decoder models. C.6 Gradient attribution Figure 7 shows context attribution ratios by dataset. Across all datasets, CQO receives significantly more gradient flow from context tokens than QOC (average ratio: 2.38). We also show details in Table 9. C."
        },
        {
            "title": "Intervention results",
            "content": "Attention pruning (CQO Degradation). Table 10 shows the effect of blocking option-tocontext attention in CQO prompts. The average accuracy drop is -26.8%, confirming this attention pathway is essential. Activation patching (QOC Improvement). Table 11 shows the effect of replacing QOC option hidden states with CQO representations. Average improvement: +6.0%. Option repetition (QOCO). Table 12 shows the effect of repeating options after context (Q-O-CO format). This simple modification allows the repeated options to attend to context, improving QOC accuracy by an average of +8.2%."
        },
        {
            "title": "Hugging Face ID",
            "content": "Qwen 2.5 Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen 3 Family Qwen3-0.6B Qwen3-1.7B Qwen3-4B LLaMA 3 Family Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Llama-3.1-8B Llama-3.1-8B-Instruct Gemma 2 Family Qwen/Qwen2.5-0.5B Qwen/Qwen2.5-0.5B-Instruct Qwen/Qwen2.5-1.5B Qwen/Qwen2.5-1.5B-Instruct Qwen/Qwen2.5-3B-Instruct Qwen/Qwen2.5-7B Qwen/Qwen2.5-7B-Instruct Qwen/Qwen3-0.6B Qwen/Qwen3-1.7B Qwen/Qwen3-4B meta-llama/Llama-3.2-1B meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-8B-Instruct gemma-2-2b gemma-2-2b-it gemma-2-9b gemma-2-9b-it google/gemma-2-2b google/gemma-2-2b-it google/gemma-2-9b google/gemma-2-9b-it Encoder-Decoder (Architecture Comparison) Flan-T5-small Flan-T5-base Flan-T5-large Flan-T5-xl google/flan-t5-small google/flan-t5-base google/flan-t5-large google/flan-t5-xl Encoder-only (Architecture Comparison) BERT-base BERT-large RoBERTa-base RoBERTa-large ALBERT-base-v2 ALBERT-xlarge-v2 google-bert/bert-base-uncased google-bert/bert-large-uncased FacebookAI/roberta-base FacebookAI/roberta-large albert/albert-base-v2 albert/albert-xlarge-v Table 3: Model information used in our experiments. We also provide the Hugging Face ID."
        },
        {
            "title": "Model",
            "content": "LLaMA Family Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B Gemma Family gemma-2-2b gemma-2-2b-it gemma-2-9b gemma-2-9b-it"
        },
        {
            "title": "Avg\nCQO QOC CQO QOC CQO QOC CQO QOC Gap",
            "content": "RACE-M RACE-H"
        },
        {
            "title": "SciQ",
            "content": "39.8 42.1 29.6 31.0 27.3 34.9 28.7 27.0 40.4 42.2 42.5 42.9 51.0 53.1 37.2 41.0 52.1 29.0 38.9 39.2 50.7 39.1 33.8 36.6 27.0 29.6 23.7 32.6 29.2 27.5 33.9 34.3 33.0 34.4 38.2 39.8 26.4 36.7 39. 28.4 33.9 35.8 38.1 33.0 96.3 97.8 75.8 91.9 93.4 96.6 83.6 93.6 97.7 97.0 97.7 97.8 98.3 98.6 87.4 94.7 98.1 88.1 96.5 98.0 98.4 94. 91.8 97.1 56.0 83.5 84.3 94.0 69.0 78.9 91.9 90.6 93.2 93.8 96.2 97.4 69.8 86.3 95.5 73.3 90.3 94.9 97.0 86.9 77.6 86.1 38.4 60.7 64.0 80.6 58.4 58.6 81.5 80.8 87.3 86.8 90.6 90.1 51.2 74.9 86. 55.4 75.4 84.8 90.9 74.3 47.1 64.9 29.5 43.2 39.3 54.2 40.0 38.6 52.4 47.8 55.9 57.6 62.8 68.1 33.9 44.4 55.4 32.1 47.6 52.4 73.8 49. 73.9 82.1 36.0 57.3 60.0 75.6 51.4 52.8 76.3 75.8 82.4 82.2 87.7 87.3 46.9 70.9 80.8 45.5 67.0 80.2 86.8 69.5 48.2 61.0 27.2 41.2 37.6 52.2 41.3 39.5 53.4 48.7 57.0 56.8 64.7 65.2 32.2 45.6 55. 30.6 45.1 54.1 66.9 +16.7 +12.1 +10.0 +10.8 +15.0 +13.7 +10.7 +11.9 +16.1 +18.6 +17.7 +16.8 +16.4 +14.7 +15.1 +17.1 +18.1 +13.4 +15.2 +16.3 +12.7 48.8 +14. Table 4: Full accuracy results. Performance of 21 decoder-only models on 4 datasets. Gap = CQO QOC."
        },
        {
            "title": "LLaMA Family",
            "content": "Llama-3.1-8B Llama-3.2-1B Llama-3.2-3B"
        },
        {
            "title": "Qwen Family",
            "content": "Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B"
        },
        {
            "title": "Gemma Family",
            "content": "Gemma-2-2B Gemma-2-9B +5.99 LogiQA SciQ +4.50 RACE-M +30.57 +25.64 RACE-H +16.68 Average LogiQA SciQ RACE-M RACE-H Average +2.61 +19.80 +8.91 +8.75 +10.02 +3.69 LogiQA SciQ +9.10 RACE-M +24.65 +22.41 RACE-H +14.96 Average LogiQA -0.46 SciQ +14.60 RACE-M +18.31 +10.15 RACE-H +10.65 Average +6.45 LogiQA SciQ +5.80 RACE-M +29.11 +22.90 RACE-H Average +16.06 +9.52 LogiQA SciQ +4.50 RACE-M +31.96 RACE-H +25.24 +17.81 Average +12.75 LogiQA SciQ +2.10 RACE-M +27.79 +23.04 RACE-H +16.42 Average LogiQA +0.61 +14.80 SciQ RACE-M +23.26 +14.89 RACE-H +13.39 Average +3.38 LogiQA SciQ +3.10 RACE-M +32.45 +26.13 RACE-H +16.26 Average +5.53 +0.70 +21.17 +21.10 +12.12 +1.38 +8.40 +17.48 +16.09 +10.84 +2.30 +2.60 +26.39 +23.41 +13.68 -0.46 +14.70 +19.99 +13.32 +11.89 +7.99 +6.40 +32.94 +27.10 +18.61 +10.29 +4.20 +29.25 +25.36 +17. +13.36 +1.20 +22.01 +22.10 +14.67 +4.92 +6.20 +27.79 +21.98 +15.22 +12.60 +1.40 +17.06 +19.93 +12.75 -0.46 -3.80 -9.40 -4.55 -4.55 -1.23 -11.40 +8.57 +7.35 +0.82 -1.38 -6.50 +1.74 +1.00 -1. +0.00 +0.10 +1.67 +3.17 +1.24 +1.54 +0.60 +3.83 +4.20 +2.54 +0.77 -0.30 -2.72 +0.11 -0.53 +0.61 -0.90 -5.78 -0.94 -1.75 +4.30 -8.60 +4.53 +7.09 +1.83 +9.22 -1.70 -15.39 -6.20 -3."
        },
        {
            "title": "Grand Average",
            "content": "+14.70 +14.12 -0.58 Table 5: CQO-QOC gap comparison: Base vs Instruction-tuned models (breakdown by dataset). Both Base and Instruct models show similar ordering sensitivity. Diff = Instruct Gap Base Gap."
        },
        {
            "title": "Model",
            "content": "0-shot 1-shot 3-shot 5-shot LLaMA Family Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B Gemma Family Gemma-2-2B Gemma-2-2B-Instruct Gemma-2-9B Gemma-2-9B-Instruct 55.22 64.90 34.93 49.39 46.22 58.24 44.87 46.13 57.90 55.36 59.75 60.49 65.49 67.62 40.60 53.25 61.36 41.10 54.23 59.29 68.95 60.35 64.30 28.05 46.59 50.18 57.23 44.29 41.46 57.60 54.70 59.49 59.22 65.95 67.19 44.08 55.13 64.86 47.84 59.89 62.33 67. 62.29 65.67 31.93 50.68 53.24 58.78 45.06 40.30 58.45 57.26 61.26 61.36 66.52 67.71 50.17 58.51 67.25 49.94 61.01 64.46 70.09 62.78 64.80 32.64 51.54 53.27 59.60 45.37 42.45 58.55 58.14 61.06 61.92 65.82 68.50 51.65 58.79 67.15 49.89 61.26 65.28 69. +7.56 -0.10 -2.30 +2.15 +7.04 +1.35 +0.50 -3.68 +0.65 +2.78 +1.31 +1.42 +0.33 +0.88 +11.05 +5.54 +5.79 +8.80 +7.03 +5.99 +0."
        },
        {
            "title": "Average",
            "content": "54.54 55.16 57.24 57.63 +3.09 Table 6: In-context learning results per model (QOC format). Accuracy with 0, 1, 3, and 5 in-context examples. = 5-shot 0-shot. ICL provides marginal improvement (+3.1%) but cannot close the gap to CQO baseline (69.3%). Model Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B gemma-2-2b-it gemma-2-9b-it Average LogiQA RACE-M CQO QOC CQO QOC CQO QOC CQO QOC CQO QOC RACE-H SciQ Avg 93.3 96.7 89.9 89.7 94.0 93.1 89.6 85.9 88.7 86.9 88.9 88.7 89.7 86.9 87.7 91.0 88.9 84.0 94.8 89.9 94.5 96.8 85.3 91.2 92.9 93.7 90.0 86.1 87.7 85.6 89.0 89.3 89.6 87.3 89.6 92.7 89.2 84.3 95.1 90. 99.8 99.8 97.8 98.3 99.6 95.2 99.2 94.9 93.8 95.0 99.2 99.1 99.5 99.2 96.5 99.3 99.7 93.5 98.8 97.8 99.8 99.8 97.6 99.1 98.0 99.1 98.4 96.0 96.2 97.0 99.2 99.2 99.4 99.4 97.0 99.6 99.6 89.6 99.2 98.1 97.3 97.0 87.2 92.0 95.4 91.2 96.5 92.5 93.4 91.4 95.9 96.5 96.4 96.2 77.3 97.0 97.5 76.7 97.1 92. 97.2 98.3 88.1 97.6 94.0 97.9 96.3 95.7 95.5 94.4 96.8 97.4 96.6 96.8 91.5 97.9 97.7 84.3 98.0 95.4 96.9 96.5 87.4 91.9 94.5 88.4 96.8 92.9 94.9 92.3 95.8 96.2 96.2 95.7 84.2 97.3 97.2 85.0 96.4 93.5 96.9 98.5 88.2 97.7 92.8 97.8 96.1 95.1 95.2 95.0 96.5 96.9 96.4 96.1 93.3 98.1 97.4 89.5 97.8 95. 96.8 97.5 90.6 93.0 95.9 92.0 95.5 91.6 92.7 91.4 95.0 95.1 95.5 94.5 86.4 96.1 95.8 84.8 96.8 93.5 97.1 98.3 89.8 96.4 94.4 97.1 95.2 93.2 93.6 93.0 95.4 95.7 95.5 94.9 92.8 97.1 96.0 86.9 97.5 94.7 Table 7: Option recall accuracy (%) by prompt order. High recall rates across both CQO and QOC templates confirm that memory is not the bottleneck for the QOC performance drop."
        },
        {
            "title": "Encoder Only Models",
            "content": "BERT-Base BERT-Large RoBERTa-Base RoBERTa-Large ALBERT-Base ALBERT-XLarge"
        },
        {
            "title": "20.74\nLogiQA\nSciQ\n23.90\nRACE-M 23.68\n21.04\nRACE-H\n22.34\nAverage",
            "content": "LogiQA 20.74 28.90 SciQ RACE-M 21.17 16.67 RACE-H 21.87 Average"
        },
        {
            "title": "21.97\nLogiQA\n28.60\nSciQ\nRACE-M 21.73\n16.87\nRACE-H\n22.29\nAverage",
            "content": "LogiQA 20.28 25.60 SciQ RACE-M 20.68 17.21 RACE-H 20.94 Average"
        },
        {
            "title": "19.97\nLogiQA\nSciQ\n26.30\nRACE-M 20.82\n17.30\nRACE-H\n21.10\nAverage",
            "content": "19.20 25.60 21.38 17.92 21.03 19.82 26.50 21.73 18.78 21.71 20.74 27.80 20.89 16.41 21.46 25.19 28.90 23.47 17.50 23.76 20.12 25.90 20.82 17.21 21.01 20.12 26.10 20.61 17.27 21."
        },
        {
            "title": "Encoder Avg",
            "content": "+1.69 -1.00 -0.28 +1.03 +0.36 +0.92 -2.60 +1.95 +2.26 +0.63 +0.00 +1.10 +0.28 +0.26 +0. -3.23 -0.30 -1.74 -0.63 -1.47 +0.15 -0.30 -0.14 +0.00 -0.07 -0.15 +0.20 +0.21 +0.03 +0.07 -0."
        },
        {
            "title": "Gap",
            "content": "Encoder-Decoder Models 27.80 LogiQA SciQ 88.70 RACE-M 51.74 42.71 RACE-H 52.74 Average Flan-T5-Small Flan-T5-Base Flan-T5-Large Flan-T5-XL LogiQA 32.10 94.60 SciQ RACE-M 76.25 66.52 RACE-H 67.37 Average"
        },
        {
            "title": "36.41\nLogiQA\n96.40\nSciQ\nRACE-M 85.31\n80.45\nRACE-H\n74.64\nAverage",
            "content": "LogiQA 35.33 97.40 SciQ RACE-M 90.18 86.68 RACE-H 77.40 Average 29.95 84.20 44.64 38.02 49.20 30.41 93.60 72.91 63.15 65.02 34.10 96.00 84.19 76.56 72.71 33.33 97.10 88.86 84.73 76.01 Enc-Dec Avg -2.15 +4.50 +7.10 +4.69 +3.54 +1.69 +1.00 +3.34 +3.37 +2.35 +2.30 +0.40 +1.11 +3.89 +1. +2.00 +0.30 +1.32 +1.94 +1.39 +2.30 Table 8: Encoder-only and encoder-decoder model results (breakdown by dataset). Bidirectional attention in encoder models and cross-attention in encoder-decoder models eliminate ordering sensitivity. Gap = CQO QOC."
        },
        {
            "title": "Ratio",
            "content": "Qwen Family (cont.)"
        },
        {
            "title": "LLaMA Family",
            "content": "Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct"
        },
        {
            "title": "Qwen Family",
            "content": "Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct"
        },
        {
            "title": "Ratio",
            "content": "Qwen2.5-3B"
        },
        {
            "title": "0.841\nLogiQA\nSciQ\n0.808\nRACE-M 0.889\n0.906\nRACE-H\n0.861\nAverage",
            "content": "LogiQA 0.836 0.804 SciQ RACE-M 0.868 0.893 RACE-H 0.850 Average"
        },
        {
            "title": "0.877\nLogiQA\nSciQ\n0.815\nRACE-M 0.901\n0.917\nRACE-H\n0.877\nAverage",
            "content": "LogiQA 0.855 0.835 SciQ RACE-M 0.881 0.890 RACE-H 0.865 Average LogiQA 0.781 0.816 SciQ RACE-M 0.843 0.850 RACE-H 0.823 Average"
        },
        {
            "title": "0.825\nLogiQA\nSciQ\n0.775\nRACE-M 0.805\n0.820\nRACE-H\n0.806\nAverage",
            "content": "0.257 0.269 0.368 0.400 0.323 0.248 0.294 0.403 0.432 0.344 0.255 0.220 0.386 0.414 0.319 0.293 0.299 0.413 0.424 0.357 0.275 0.267 0.405 0.429 0.344 0.298 0.319 0.460 0.485 0. 0.306 0.282 0.375 0.407 0.342 0.263 0.250 0.399 0.428 0.335 0.250 0.219 0.312 0.325 0.277 0.210 0.236 0.355 0.373 0.293 3.133 2.859 2.335 2.205 2.562 3.364 2.612 2.157 2.060 2.441 3.129 3.697 2.251 2.128 2.637 2.869 2.700 2.154 2.137 2.410 3.037 3.007 2.142 2.080 2.470 2.942 2.556 1.957 1.892 2.247 2.798 2.958 2.350 2.187 2.526 2.973 3.260 2.111 1.987 2.455 3.505 3.752 2.834 2.745 3.142 3.926 3.285 2.269 2.197 2.747 Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B"
        },
        {
            "title": "Gemma Family",
            "content": "Gemma-2-2B Gemma-2-2B-Instruct Gemma-2-9B Gemma-2-9B-Instruct"
        },
        {
            "title": "0.519\nLogiQA\nSciQ\n0.519\nRACE-M 0.617\n0.633\nRACE-H\n0.572\nAverage",
            "content": "LogiQA 0.524 SciQ 0.558 RACE-M 0.654 0.655 RACE-H 0.598 Average 0.224 0.276 0.351 0.342 0.298 0.194 0.281 0.374 0.387 0.309 0.338 0.368 0.479 0.500 0.421 0.256 0.366 0.473 0.491 0.396 0.189 0.219 0.316 0.341 0. 0.197 0.256 0.357 0.364 0.294 0.179 0.318 0.446 0.430 0.343 0.276 0.233 0.361 0.396 0.316 0.199 0.294 0.406 0.441 0.335 0.247 0.267 0.373 0.396 0.321 0.262 0.332 0.499 0.553 0. 3.886 2.889 2.421 2.548 2.841 4.532 2.965 2.333 2.298 2.811 2.564 2.360 1.859 1.814 2.097 3.350 2.310 1.840 1.781 2.174 4.553 3.806 2.695 2.547 3.206 4.301 3.083 2.408 2.390 2.868 5.178 2.658 2.010 2.097 2.601 1.861 2.061 1.633 1.524 1.727 2.233 1.570 1.433 1.345 1.553 2.095 1.947 1.655 1.600 1.784 2.000 1.681 1.310 1.186 1.453 Table 9: Gradient attribution (context ratio) per model (breakdown by dataset). CQO consistently allocates more gradient flow to context tokens than QOC. Ratio = CQO / QOC."
        },
        {
            "title": "Grand Average",
            "content": "0.797 0.335 2.380 16 Model LLaMA Family Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B Gemma Family Gemma-2-2B Gemma-2-2B-Instruct Gemma-2-9B Gemma-2-9B-Instruct LogiQA CQO Pruned SciQ CQO Pruned RACE-M RACE-H CQO Pruned CQO Pruned 39.78 42.09 29.65 31.03 27.34 34.87 28.73 27.04 40.40 42.24 42.55 44.09 51.00 53.15 37.17 41.01 52.07 29.03 38.86 39.17 50.69 30.88 35.79 29.34 31.18 30.41 33.64 27.34 26.27 27.80 25.81 27.34 28.57 31.03 35.33 21.97 26.57 26. 27.65 37.79 35.64 43.32 -8.91 -6.30 -0.31 +0.15 +3.07 -1.23 -1.38 -0.77 -12.60 -16.44 -15.21 -15.51 -19.97 -17.82 -15.21 -14.44 -25.65 -1.38 -1.08 -3.53 -7.37 96.30 97.80 75.80 91.90 93.40 96.60 83.60 93.60 97.70 97.00 97.80 97.80 98.30 98.60 87.40 94.87 98. 88.10 96.50 98.00 98.40 89.90 93.70 61.50 83.30 76.70 90.00 25.60 26.30 29.70 31.50 24.50 25.60 79.90 81.30 27.30 30.40 43.10 80.60 89.90 94.40 94.90 -6.40 -4.10 -14.30 -8.60 -16.70 -6.60 -58.00 -67.30 -68.00 -65.50 -73.30 -72.20 -18.40 -17.30 -60.10 -64.47 -55. -7.50 -6.60 -3.60 -3.50 77.65 86.07 38.37 60.72 64.00 80.57 58.36 58.64 81.48 80.78 87.47 86.91 90.60 90.11 51.18 74.86 86.84 55.36 75.42 84.82 90.88 51.95 58.98 32.03 42.34 42.69 53.83 26.60 25.77 26.74 27.09 26.32 26.67 48.89 46.87 21.59 26.32 26. 39.55 52.37 58.70 68.25 -25.70 -27.09 -6.34 -18.38 -21.31 -26.74 -31.75 -32.87 -54.74 -53.69 -61.14 -60.24 -41.71 -43.25 -29.60 -48.54 -60.45 -15.81 -23.05 -26.11 -22.63 73.87 82.13 35.99 57.26 60.01 75.64 51.40 52.80 76.27 75.84 82.42 82.28 87.74 87.31 46.88 70.87 80. 45.45 67.04 80.25 86.82 49.60 58.75 31.30 42.65 41.80 53.72 25.96 25.07 25.53 26.44 24.87 25.90 45.03 46.20 21.47 27.90 28.67 40.42 53.06 57.86 66.92 -24.27 -23.38 -4.69 -14.61 -18.21 -21.93 -25.44 -27.73 -50.74 -49.40 -57.55 -56.38 -42.71 -41.11 -25.41 -42.97 -52. -5.03 -13.98 -22.38 -19.90 Avg -16.32 -15.22 -6.41 -10.36 -13.29 -14.12 -29.15 -32.17 -46.52 -46.26 -51.80 -51.08 -30.70 -29.87 -32.58 -42.60 -48.32 -7.43 -11.18 -13.91 -13."
        },
        {
            "title": "Average",
            "content": "39.14 30.48 -8.66 94.17 60.96 -33. 74.34 39.52 -34.82 69.48 39.01 -30. -26.79 Table 10: Attention pruning results per model. Blocking option-to-context attention in CQO prompts causes significant accuracy drop, confirming context utilization is essential. = Zeroed CQO."
        },
        {
            "title": "Model",
            "content": "LLaMA Family Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B Gemma Family Gemma-2-2B Gemma-2-2B-Instruct Gemma-2-9B Gemma-2-9B-Instruct"
        },
        {
            "title": "SciQ",
            "content": "RACE-M RACE-H"
        },
        {
            "title": "QOC Patched QOC Patched QOC Patched QOC Patched",
            "content": "33.79 36.56 27.04 29.65 23.66 32.57 29.19 27.50 33.95 34.25 33.03 33.79 38.25 39.78 26.42 36.71 39.32 28.42 33.95 35.79 38.10 30.57 28.42 26.73 30.11 30.72 31.80 26.42 23.96 37.02 37.48 34.72 31.34 46.08 52.38 31.95 38.86 50.23 28.26 38.56 32.72 50. 91.80 97.10 56.00 83.50 84.30 94.00 69.00 78.90 91.90 90.60 93.30 93.60 96.20 97.40 69.80 86.30 95.50 73.30 90.30 94.90 97.00 87.20 94.70 57.00 82.40 73.20 93.40 76.60 78.70 95.00 92.10 93.30 96.20 98.30 98.10 83.20 92.00 97.90 89.10 94.50 71.80 95. 47.08 64.90 29.46 43.25 39.35 54.18 40.04 38.65 52.37 47.84 55.50 57.66 62.81 68.11 33.91 44.43 55.43 32.10 47.63 52.37 73.82 52.58 61.98 29.53 47.08 41.09 55.78 41.71 36.98 71.24 67.34 70.96 72.70 88.72 87.67 56.27 71.80 84.96 53.06 72.70 62.12 80. 48.23 61.03 27.24 41.17 37.59 52.23 41.25 39.48 53.37 48.74 57.18 56.92 64.69 65.21 32.25 45.57 55.17 30.56 45.05 54.12 66.90 48.77 51.97 28.70 43.77 36.45 47.97 37.14 32.76 64.69 61.61 66.98 66.21 83.28 83.13 50.71 68.21 78.73 45.28 67.64 59.23 72. Avg -0.45 -5.63 +0.56 +1.45 -0.86 -1.01 +0.60 -3.03 +9.09 +9.27 +6.74 +6.12 +13.61 +12.70 +14.94 +14.46 +16.60 +12.83 +14.12 -2.83 +5."
        },
        {
            "title": "Average",
            "content": "33.42 34.76 89.62 90.96 49.72 65. 48.78 61.48 +5.96 Table 11: Activation patching results. Replacing QOC option hidden states with CQO representations improves accuracy, demonstrating that context-aware representations enhance prediction. = Patched QOC. 17 LogiQA QOC QOCO SciQ QOC QOCO RACE-M RACE-H QOC QOCO QOC QOCO Model LLaMA Family Llama-3.1-8B Llama-3.1-8B-Instruct Llama-3.2-1B Llama-3.2-1B-Instruct Llama-3.2-3B Llama-3.2-3B-Instruct Qwen Family Qwen2.5-0.5B Qwen2.5-0.5B-Instruct Qwen2.5-1.5B Qwen2.5-1.5B-Instruct Qwen2.5-3B Qwen2.5-3B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Qwen3-0.6B Qwen3-1.7B Qwen3-4B Gemma Family Gemma-2-2B Gemma-2-2B-Instruct Gemma-2-9B Gemma-2-9B-Instruct 33.79 36.56 27.04 29.65 23.66 32.57 29.19 27.50 33.95 34.25 33.03 33.79 38.25 39.78 26.42 36.71 39.32 28.42 33.95 35.79 38.10 38.40 39.48 27.19 29.03 30.57 35.48 27.50 24.27 33.95 35.02 35.94 38.56 42.70 45.78 34.56 36.10 48. 26.11 41.01 38.25 48.08 +4.61 +2.92 +0.15 -0.62 +6.91 +2.91 -1.69 -3.23 0.00 +0.77 +2.91 +4.77 +4.45 +6.00 +8.14 -0.61 +9.22 -2.31 +7.06 +2.46 +9.98 91.80 97.10 56.00 83.50 84.30 94.00 69.00 78.90 91.90 90.60 93.30 93.60 96.20 97.40 69.80 86.30 95. 73.30 90.30 94.90 97.00 86.90 97.40 68.50 94.20 92.40 96.50 85.10 88.70 95.60 91.40 97.10 96.80 98.00 97.50 80.10 82.10 98.20 91.60 94.90 97.60 97.30 -4.90 +0.30 +12.50 +10.70 +8.10 +2.50 +16.10 +9.80 +3.70 +0.80 +3.80 +3.20 +1.80 +0.10 +10.30 -4.20 +2. +18.30 +4.60 +2.70 +0.30 47.08 64.90 29.46 43.25 39.35 54.18 40.04 38.65 52.37 47.84 55.50 57.66 62.81 68.11 33.91 44.43 55.43 32.10 47.63 52.37 73.82 59.19 78.55 34.26 55.85 53.76 72.14 43.31 48.40 63.58 60.38 71.31 73.75 79.18 80.99 49.86 55.99 76. 48.05 71.80 71.80 85.65 +12.11 +13.65 +4.80 +12.60 +14.41 +17.96 +3.27 +9.75 +11.21 +12.54 +15.81 +16.09 +16.37 +12.88 +15.95 +11.56 +21.31 +15.95 +24.17 +19.43 +11.83 48.23 61.03 27.24 41.17 37.59 52.23 41.25 39.48 53.37 48.74 57.18 56.92 64.69 65.21 32.25 45.57 55. 30.56 45.05 54.12 66.90 58.98 73.79 32.08 51.43 47.66 66.47 38.74 44.57 56.43 59.75 66.18 69.01 74.76 76.90 45.20 56.98 72.41 45.25 65.84 69.07 81.25 Avg +5.64 +7.40 +5.57 +8.23 +9.87 +9. +10.75 +12.76 +4.84 +10.26 +10.07 +14.24 +3.79 -2.51 +5.35 +5.09 +4.49 +3.06 +6.28 +11.01 +7.88 +9.00 +9.04 +12.09 +8.17 +10.07 +7.66 +11.69 +12.95 +11.83 +4.54 +11.41 +17.24 +12.61 +14.69 +11.65 +20.79 +14.15 +9.88 +14.95 +9.11 +14.35 Average 32.93 36. +3.08 86.89 91.80 +4.91 49.56 63. +13.98 48.75 59.65 +10.89 +8.21 Table 12: QOCO results per model (breakdown by dataset). QOCO prompts (repeat variant) achieve improved accuracy to QOC, indicated by small . = QOCO QOC."
        }
    ],
    "affiliations": [
        "HJ AILAB",
        "POSTECH"
    ]
}