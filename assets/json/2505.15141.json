{
    "paper_title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms",
    "authors": [
        "Yunlong Hou",
        "Fengzhuo Zhang",
        "Cunxiao Du",
        "Xuan Zhang",
        "Jiachun Pan",
        "Tianyu Pang",
        "Chao Du",
        "Vincent Y. F. Tan",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding has emerged as a popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt a fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes a training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as a Multi-Armed Bandit problem and provide a general speculative decoding framework BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSpec is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts."
        },
        {
            "title": "Start",
            "content": "BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Yunlong Hou * 1 Fengzhuo Zhang * 1 Cunxiao Du * 2 Xuan Zhang * 3 Jiachun Pan 1 Tianyu Pang 2 Chao Du 2 Vincent Y. F. Tan 1 Zhuoran Yang 4 Abstract Speculative decoding has emerged as popular method to accelerate the inference of Large Language Models (LLMs) while retaining their superior text generation performance. Previous methods either adopt fixed speculative decoding configuration regardless of the prefix tokens, or train draft models in an offline or online manner to align them with the context. This paper proposes training-free online learning framework to adaptively choose the configuration of the hyperparameters for speculative decoding as text is being generated. We first formulate this hyperparameter selection problem as Multi-Armed Bandit problem and provide general speculative decoding framework BANDITSPEC. Furthermore, two bandit-based hyperparameter selection algorithms, UCBSPEC and EXP3SPEC, are designed and analyzed in terms of novel quantity, the stopping time regret. We upper bound this regret under both stochastic and adversarial reward settings. By deriving an information-theoretic impossibility result, it is shown that the regret performance of UCBSPEC is optimal up to universal constants. Finally, extensive empirical experiments with LLaMA3 and Qwen2 demonstrate that our algorithms are effective compared to existing methods, and the throughput is close to the oracle best hyperparameter in simulated real-life LLM serving scenarios with diverse input prompts. 5 2 0 2 1 2 ] . [ 1 1 4 1 5 1 . 5 0 5 2 : r 1. Introduction Large Language Model (LLM) is trained to predict the probability of the next token conditioned on all previous *Equal contribution Work done as an associate member at Sea AI Lab Project Lead 1National University of Singapore 2Sea AI Lab 3Singapore Management University 4Yale University. Correspondence to: Cunxiao Du <cnsdunm@gmail.com>, Zhuoran Yang <zhuoran.yang@yale.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 Figure 1. Given the prefix tokens and the candidate hyperparameter configurations (e.g., models), which configuration should be selected to decode the next tokens? We formulate this problem as bandit problem and propose general framework BANDITSPEC. tokens (Brown et al., 2020; Touvron et al., 2023). This autoregressive decoding approach involves multiple forward passes, with each pass generating one token sequentially. Consequently, this process can lead to significant latency during inference. Speculative decoding was introduced by Leviathan et al. (2023); Chen et al. (2023) to accelerate the inference of LLMs. The standard speculative decoding framework has been extended with improved performance since then. thorough overview is presented at Appendix A. While the existing speculative decoding methods are diverse, most previous works adopt fixed one across tasks, severely limiting their potential. For instance, when dealing with code debugging or grammar-checking tasks, the generated tokens are expected to resemble most of the input tokens. Therefore, retrieval-based speculative decoding techniques are preferred (Hu et al., 2024). In contrast, for story generation tasks, we expect the generated tokens to be more creative. Thus, draft model with high-temperature parameter is preferred over retrieval-based methods. The potential of these speculative decoding methods can only be exploited when the configuration of hyperparameters is well-aligned to the given task. There are existing works that attempt to achieve this goal, e.g., Zhou et al. (2024) distills the draft BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms model during inference. Furthermore, even when the choice of the draft model is optimized, the associated hyperparameters can still be refined. For instance, Liu et al. (2024) and Huang et al. (2024) aim to optimize the speculation length in training and training-free manner. Based on these observations, we ask the questions (see Figure 1) : given prefix prompts and candidate configurations of hyperparameters, is there theoretically sound framework to model and solve the hyperparameters selection problem? Is there any training-free method that can adaptively choose the hyperparameters such that the latency of speculative decoding can be minimized? In this paper, we answer these questions affirmatively. We adopt bandit framework to leverage its adaptivity in unknown environments to achieve this goal. Our contributions can be summarized as follows. We formulate the hyperparameter selection problem in speculative decoding as bandit problem and propose general speculative decoding framework BANDITSPEC(ALG) (see Algorithm 3), where the hyperparameter selection algorithm ALG selects the hyperparameters to be deployed in each round of speculative decoding. The objective is to minimize the stopping time regret, which measures the latency of ALG compared to that of the best hyperparameter. Under mild stochastic and adversarial reward assumptions, we devise two hyperparameter selection algorithms, UCBSPEC and EXP3SPEC, respectively. By deriving upper bounds on the stopping time regret, we prove that the inference latency between the proposed algorithms and the best hyperparameter under given initial prompt vanishes asymptotically. In addition, we show, via deriving an informationtheoretic impossibility result, that the regret performance of UCBSPEC is optimal up to constants. Extensive empirical experiments with LLaMA3 and Qwen2 are conducted to demonstrate the efficacy of the proposed framework. When the batch size is 1, the adaptive selection of models via UCBSPEC and EXP3SPEC can greatly improve the latency, exhibiting competitive performance against the existing methods. Under simulated real-life scenarios where LLMs are implemented for diverse prompts simultaneously, the adaptive selection of speculation length via UCBSPEC achieves comparable throughput with the oracle best. 2. Preliminaries LLM Decoding We denote an LLM as : , where and are the space of tokens and the space of all token sequences, respectively. Most LLMs predict this conditional probability via predicting the logits of the next token. Concretely, the LLMs predict log (xt x1:t1), where xt and x1:t1 are Algorithm 1 CANONICAL DECODING Inputs: initial prompt pt0 = pt , target model . Procedures: 1: Set = 0. 2: while = 0 and xt = EOS do 3: 4: 5: 6: end while 7: return t, ptt = + 1. xt ( ptt1). ptt = concat(ptt1, xt). respectively the t-th token and first 1 tokens. In the inference stage, LLMs use an additional temperature parameter γ > 0 to predict the next tokens probability as softmax(γ1 log ( x1:t1)), where softmax is the softmax operator. The results in our work hold for any γ > 0, and we just denote softmax(γ1 log ) as for ease of notation. When γ > 0, we sample the next token from the predicted distribution, which is called sampling (sampling decoding). When γ 0, the next token will be the token that corresponds to the highest logit value; this is called greedy decoding. We note that the greedy decoding is deterministic, i.e., the token is sampled from degenerate distribution. These two families of decoding methods can be unified as Algorithm 1, where LLMs autoregressively generate tokens until the EOS token. Speculative Decoding As shown in Algorithm 1, the autoregressive decoding feature requires multiple forward inferences of LLMs sequentially. To reduce the number of forward inferences, Leviathan et al. (2023); Chen et al. (2023) proposed the vanilla speculative decoding algorithm which implements draft model to generate draft tokens and let the target model verify them in parallel. For completeness, we present and describe the vanilla speculative decoding algorithm in Appendix C.1. This vanilla specualtive decoding is then extended by some existing works, e.g., Miao et al. (2023); Cai et al. (2024) organizes the draft tokens as tree, which improves the number of accepted tokens. The speculative decoding algorithm contains many hyperparameters, e.g., the draft model Q, and the tree structure in Miao et al. (2023); Cai et al. (2024). Most existing works keep these hyperparameters fixed for all the tasks. Some other works optimize the draft model in an online or offline manner (Liu et al., 2023) and the size of the tree (Chen et al., 2024), which are designed for specific considerations. In contrast, our work aims to derive unified online hyperparameter selection algorithm that can be applied for any type of hyperparameters. Multi-Armed Bandits The Multi-Armed Bandit (MAB) is fundamental online decision-making problem (see Algorithm 7 for its dynamics). In its classical stochastic form, an agent chooses from arms, each of which delivers reBANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Algorithm 2 SPECUATIVE DECODING SUBROUTINE (SPECDECSUB) Inputs: pt , target model , the hyperparameters S, maximum speculation length L. Procedures: 1: Call standard speculative decoding algorithm with (pt, P, S, L). 2: return the accepted and bonus tokens x1:τ , where τ 1. ward sampled i.i.d. from an unknown but fixed distribution when pulled (Lattimore & Szepesvari, 2020). The goal is to select arms over rounds to maximize cumulative rewards. Two primary classes of algorithmsUCB-type (Auer et al., 2002b) and sampling-based methods (Russo et al., 2017) have been developed and proven optimal in this setting. In the adversarial formulation, there are no assumptions on the reward distributions; rewards can evolve arbitrarily over time and may be correlated across arms (Auer et al., 2002a). Several algorithms, such as EXP3 and EXP4 (Auer et al., 2002a), are known to achieve optimal performance under these conditions. In this work, we frame the hyperparameter selection problem as an MAB problem and develop algorithms tailored to both stochastic and adversarial settings. Notations: Let [N ] := {1, , }. For finite set , we denote the set of distributions supported on it as = {P : [0, 1] (cid:80) xX (x) = 1, (x) 0 for all }. The space of all finite length sequences whose components belong to is denoted as , and we use x1:L to denote length-L sequence. The KullbackLeibler (KL) divergence between two distributions and is denoted as KL(P, Q). 3. Bandits for Adaptive Speculative Decoding In the section, we formally formulate the hyperparameter selection problem in speculative decoding using the parlance of multi-armed bandits. The goal of this online decisionmaking process is to decode as soon as possible, i.e., minimizing the latency of the LLM decoding. Different from the classical multi-armed bandit problem, this problem involves two stochastic processes that march at various paces. In fact, as described in Appendix C.1, each (vanilla) speculative decoding subroutine produces several tokens, where the number of accepted tokens itself is also random variable. Thus, the selection of hyperparameters of each speculative decoding subroutine and the token generation processes are evolving at different paces. To put the problem in mathematically sound way, we first specify general speculative decoding subroutine (SPECDECSUB) in Algorithm 2. The input of this subroutine is prompt pt , target model , specification of hyperparameters S, and the maximum speculation Algorithm 3 SPECULATIVE DECODING WITH BANDITS (BANDITSPEC) Inputs: arm selection algorithm ALG, initial prompt pt0 = pt , bandit configuration ν = (P, = {Si}i[K], L). Procedures: 1: = 0, H0 = , I0 = 1, xI0,0 = . 2: while EOS / xIt,t do = + 1. 3: Select hyperparameter index It = ALG(Ht1). 4: xIt,t = SPECDECSUB(ptt1, P, SIt, L). 5: ptt = concat(ptt1, xIt,t). 6: 7: Ht = concat(Ht1, (It, xIt,t)). 8: end while 9: return ST(ALG, pt, ν) = t, ptST(ALG,pt,ν) = ptt. length L, and the output is the accepted token sequence x1:τ . We provide two examples of the hyperparameter sets here. (1) If we adopt the vanilla speculative decoding (Algorithm 6) as Line 1, can be different draft models : , and is the set of all the provided draft models. We would like to choose draft model according to its training context, e.g. math, creative writing, to decode the current prefix. Then the problem we consider is how to adaptively select proper draft model for speculative decoding via bandit algorithms. (2) If we adopt Medusa (Cai et al., 2024) as Line 1, can be different tree structures, and is the set of plausible tree structures. In this problem, we would like to adaptively adjust the speculation tree structure according to the context. With the help of SPECDECSUB, the speculative decoding with bandit framework, BANDITSPEC, can be specified in Algorithm 3 and as illustrated in Figure 1. The bandit configuration ν = (P, = {Si}i[K], L) consists of three components: the target model , the set of candidate hyperparameter specifications S, and the maximum speculation length L. Each hyperparameter specification Si corresponds to an arm in the bandit problem. Given prompt pt and an arm selection algorithm ALG, hyperparameter specification is chosen according to the history Ht1 in Line 4. Then SPECDECSUB is invoked with selected hyperparameters SIt as input. The output of SPECDECSUB, xIt,t,1 is then adopted to update the prompt (Line 6) and the history information (Line 7). The whole process stops when the EOS token appears in the prompt. We denote the number of calls to SPECDECSUB (the stopping time) and the generated token sequence as ST(ALG, pt, ν) and ptST(ALG,pt,ν), respectively. To minimize the decoding latency, we aim to design ALG to minimize ST(ALG, pt, ν). Since the position of the EOS token itself is random variable, we would like 1We abbreviate the notation xIt,t,1:τt as xIt,t to represent the accepted tokens generated by SIt at time step t. 3 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms to minimize the expectation of ST(ALG, pt, ν). The performance of ALG is measured via the stopping time regret Reg(ALG, pt, ν) := E(cid:2)ST(ALG, pt, ν) pt, ν(cid:3) (1) E(cid:2)ST(ALGi(pt,ν), pt, ν) pt, ν(cid:3), where ALGi is the arm selection algorithm which adopts Si in all rounds, i.e., = ALGi(Ht) for all Ht and t, [ST(ALGi, pt, ν) pt, ν] and i(pt, ν) = argmini[K] denotes the index of the best hyperparameter for prompt pt under bandit configuration ν. For ease of notation, when ν and pt are clear from the context, ST(ALG, pt, ν), Reg(ALG, pt, ν), and i(pt, ν) will be abbreviated as ST(ALG), Reg(ALG), and i, respectively. We will use BANDITSPEC(ALG) to specify the choice of ALG in Algorithm 3. For simplicity, we regard the bonus token as the last accepted token. Thus, the length of the accepted tokens xIt,t at each round, yIt,t, is between [1, L+1]. Before going to the algorithm design and the theoretical analysis, we would like to specify some important properties that are shared for any arm selection algorithm ALG and clarify the intuitions about our theoretical analysis. We denote the stopping time of the canonical decoding (Algorithm 1) and the generated sequence as τc and ptτc, respectively. Proposition 3.1. For any arm selection algorithm ALG that selects an arm according to the history, the generated prompt ptST(ALG) is equal to ptτc in distribution, i.e., ptST(ALG) d= ptτc, and len(ptST(ALG)) d= len(ptτc ). (2) The stopping time ST(ALG) can be bounded as len(ptST(ALG)) + 1 ST(ALG) len(ptST(ALG)), a.s. (3) The proof of Proposition 3.1 is provided in Appendix D.1. This proposition states that the distribution of the generated prompt is the same as that of the prompt generated by Algorithm 1. The stopping time ST(ALG) is equal to the length of the generated prompt up to constant. To facilitate our theoretical understanding, we pose the following question. Question: Whether it is possible to devise an arm selection algorithm ALG to achieve sublinear regret in terms of the length of the generated token sequence, i.e., is Reg(ALG, pt, ν) = o(E[len(ptST(ALG))])? Interpretation of the Desired Result. Given prompt pt and bandit configuration ν, BANDITSPEC adpatively selects the hyperparameter via ALG and learns the context. The stopping time regret (1) measures how the stopping time of BANDITSPEC(ALG) compared to that of the (agnostic) best one BANDITSPEC(ALGi ). By minimizing Figure 2. Illustration of our bandit model for choosing configurations to decode the next token, where UCB and EXP3 refer to UCBSPEC and EXP3SPEC, repectively. Reg(ALG, pt, ν), we want to devise an ALG to (approxiIn particular, mately) match the performance of ALGi . if Reg(ALG, pt, ν) = o(E[len(ptST(ALG))]), it implies that BANDITSPEC(ALG) requires the same number of speculative decoding rounds as BANDITSPEC(ALGi ) asymptotically even though the information about Si is not revealed at the beginning. In order words, BANDITSPEC(ALG) learns the identity of Si quickly and the price for this learning process can be amortized over time. Additionally, when BANDITSPEC(ALG) is deployed over diverse prompt inputs, we expect significant acceleration of token generation compared to any fixed single speculative decoding method. Why do we consider stochastic and adversarial settings? To derive efficient algorithms and meaningful theoretical analysis, it is necessary to make certain plausible assumptions of the problem. For the BANDITSPEC problem, we need to model the stochasticity of the number of accepted tokens for each hyperparameter specification. We highlight that in real-world applications, they are far from identically and independently distributed. The stochastic case (Section 4) models it as random variables and only assumes that each hyperparameter will have stationary mean acceptance length (Assumption 4.1) without the independence assumption. The adversarial case (Section 5) removes this stationarity assumption and does not make any distributional assumption of the number of accepted tokens for each hyperparameter. We highlight that there is no explicit adversary in the speculative decoding, but we model the stochasticity of the number of accepted tokens as the randomness from an (imaginary) adversary. 4. Modeling Tokens Stochastically In this section, we model the length of the accepted tokens as random variables. Assumption 4.1 (Stationary Mean Values). There exist values {µi}i[K] [1, L+1], such that conditioned on the BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Algorithm 4 UCBSPEC Inputs: number of hyperparameter specifications K, history Ht = (cid:0)(Is, XIs,s)(cid:1)t Procedures: 1: if 1 then return It+1 = + 1. 2: Compute the lengths YIs,s = len(XIs,s) for all [t]. 3: Set the statistics {ˆµi,t}i[K], {UCBi,t}i[K], where s=1, confidence parameter δ. ni,t = (cid:88) s= 1{Is = i}, ˆµi,t = (cid:80)t s=1 Yi,s1{Is = i} ni,t , cri,t = (cid:115) 2 (cid:18) 1 + ni,t n2 i,t 1 + 2 log Kt2(1 + ni,t) 1 δ (cid:19) , UCBi,t = ˆµi,t + cri,t. 4: return index It+1 = argmaxi[K] UCBi,t. history Ht1 and the chosen arm It at time t, the expected number of the accepted tokens E[YIt,t Ht1, It] = µIt. This assumption assumes that the conditional expectation of the number of accepted tokens for each hyperparameter is equal to fixed number conditioned on the previous tokens. We emphasize that this assumption does not require independence between the number of accepted tokens across implementations of SPECDECSUB, which would be unrealistic in real-world applications. More discussions are provided in Appendix B.1. 4.1. Upper Bounds for the Stochastic Case Algorithm Design We design UCB-type arm selection algorithm UCBSPEC, as shown in Algorithm 4. To avoid additional terms, we call the aggregated algorithm, BANDITSPEC(UCBSPEC), as UCBSPEC. The full version of UCBSPEC is detailed in Algorithm 8. This aggregated algorithm, UCBSPEC, is adapted from the classical UCB-1 algorithm in Auer et al. (2002b). The main differences are the confidence radius design cri,t and the stopping rule. We highlight that the form of cri,t is designed to fit the weak assumption of the number of accepted tokens. In fact, the proof of the regret of UCB-1 assumes that the values of each arm are generated before the pull of arms (Auer et al., 2002b; Lattimore & Szepesvari, 2020), which bifurcates from practical LLM inference scenarios. In contrast, we remove this strong restriction. The stopping rule of UCBSPEC makes the analysis of our algorithm rather different from that of UCB-1. The stochasticity of the total number of arm pulls requires novel regret decomposition analysis that is not presented in previous works. Theoretical Analysis We first state an assumption. Assumption 4.2 (Finite Generation Length). Given any prompt pt , the expected length of the output sequence of the canonical decoding algorithm (Algorithm 1) is finite, i.e., E[len(ptτc)] < . This assumption states that the expected length of the generated prompt is finite. In real-world applications, the length of the generated prompt is always finite due to the limits of computation and storage. To state our main result, we denote the suboptimality gap between the best arm := argmaxi[K] µi and arm := µi µi. Define the hardness parameter as H(pt, ν) := (cid:80) i=i 1/(µi i), which captures the difficulty of acceleration given the initial prompt pt and bandit configuration ν. Theorem 4.3 (Upper Bound). Under Assumptions 4.1 and 4.2, given any prompt pt and bandit configuration ν = (P, = {Si}i[K], L), the expected stopping time regret of Algorithm 3 with ALG = Algorithm 4 (UCBSPEC) is upper bounded as Reg(ALG, pt, ν) = (cid:16) (cid:17) H(pt, ν) L2 log E[len(ptτc)] . Theorem 4.3 answers the proposed question in Section 3 in the affirmative under Assumptions 4.1 and 4.2. To interpret the results of the theorem, for each hyperparameter Si, it requires ni = O(L2 log E[len(ptST(ALG))]/2 )) pulls to identify that Si is suboptimal under the current prompt pt and bandit configuration ν, resulting in nii token loss compared to the case where Si had been adopted. Additionally, this loss could be compensated by nii/µi pulls of Si , which constitutes the final stopping time regret bound. The proof is postponed to Appendix D.2 with more discussions in Appendix B.2. 4.2. Lower Bound for the Stochastic Case We further provide an information-theoretic lower bound of the regret under the greedy decoding strategy to indicate how the upper bound is in Theorem 4.3. More details and the proof of Theorem 4.4 are deferred to Appendix D.4. m=1 init with len(ptm τc Theorem 4.4 (Lower Bound). Given any sequence of initial prompts (ptm) ) , and bandit configuration ν = (P, = {Si}i[K], L), under Assumption D.4, the greedy decoding strategy and the dynamics represented in Algorithm 3, for any nonanticipatory and consistent arm selection algorithm ALG, the expected regret satisfies lim inf Reg(ALG, pt, ν) log(len(ptm )) τc (cid:88) i=i µi 1 kli , where kli := inf SS {KL(PSi, PS) : EXPS [X] > µi }. 5 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms To provide more concrete example of the lower bound, consider the truncated geometric distribution (TGD) on [1, + 1] with parameter (0, 1), i.e., PS(x) = (cid:40)px1(1 p), = 1, 2, . . . , L, pL, = + 1. (4) Algorithm 5 EXP3SPEC Inputs: number of hyperparameter specifications K, history Ht = (cid:0)(Is, XIs,s)(cid:1)t Procedures: 1: Compute the lengths YIs,s = len(XIs,s) for all [t]. s=1. This TGD was considered in the seminal works on speculative decoding (Leviathan et al., 2023; Chen et al., 2023). Proposition 4.5 (Tightness Result). Let STGD = {S : PS satisfies (4)}. Let {Si}K i=1 STGD and Si satisfies (4) with pi (Line 5 in Algorithm 2), then lim inf Reg(ALG, ptm, ν) log(len(ptm τc )) H(pt, ν) pi (1 pL ) (1 pi ) . Therefore, the upper and lower bound match up absolute constants and L2(1pi ) ) factor. In particular, if pi pi (1pL (cid:0)21/L, 1(cid:1), they match up to absolute constants and L. The proof is deferred to Appendix E.3. Proposition 4.5 indicates UCBSPEC is optimal up to constants and when considering the TGD. In other words, the additional speculative decoding rounds of UCBSPEC not only achieves O(log E[len(ptτc )]) compared to ALGi , but is also among the best possible for any arm selection algorithm (up to constants). For the tightness of our algorithm, according to Note 15.3 in Lattimore & Szepesvari (2020), kli = O(2 ) when is small. This indicates the dominating terms in the upper bound in Theorem 4.3 match the lower bound in Theorem 4.4 up to (possibly instance-dependent) constants. Futherfmore, because the truncated geometric distribution is more close to sub-exponential family distribution, especially when is large, bandit algorithms built upon UCB1 (Auer et al., 2002b) are generally loose in some factors. In order to close the gap between the upper and lower bounds completely, KL-UCB (Garivier & Cappe, 2011) can possibly be adapted to this problem out of theoretical interest. However, on the practical side, KL-UCB demands solving an optimization problem at each round, which can be time-consuming during implementations. Thus, it does not perfectly align with our ultimate goal of LLM inference acceleration. 5. Modeling Tokens Adversarially In this section, we weaken Assumption 4.1 in Section 4 and consider more general case. Specifically, we make the following assumption on the number of accepted tokens. 2: Set the statistics for all [K] (cid:98)Zi,t = 1{i = It} + 1 Yi,t pt,i . (6) 3: Set learning rate ηt = (cid:112)log K/(t K). 4: Set probability vector pt [K] with for all [K] pt,i = (cid:16) exp (cid:17) ηt (cid:16) (cid:80)t1 s=1 (cid:98)Zi,s (cid:80)t1 ηt s=1 (cid:98)Zj,s (cid:17) . (cid:80)K j=1 exp 5: return hyperparameter index It+1 pt. time step be yi,t = len(Xi,t). We assume {yi,t}i[K],tN is fixed by the environment before the algorithm starts. The bandits problem with this assumption is often referred to as the oblivious adversarial bandits in the online learning works (Auer et al., 2002a; Lattimore & Szepesvari, 2020). It admits more general and practical setups compared to the stochastic MAB. We find the greedy decoding strategy aligns more closely to this setup in the sense that the generated tokens by the models are (potentially) fixed given the initial prompt. Hence, we present our result under the greedy decoding strategy in this section.2 Given prompt pt and bandits configuration ν, the stopping time regret (1) of an arm selection algorithm ALG becomes Reg(ALG) := E[ST(ALG)] min i[K] ST(ALGi) (5) It is worth pointing out that under the greedy decoding strategy, the stopping time of any proposed algorithm can still be random due to the internal randomness embedded in the algorithm. For instance, the choice of hyperparameter SIt in Line 5 in Algorithm 5. Algorithm Design We present our arm selection algorithm in Algorithm 5, which is an abridged version of the full version BANDITSPEC(EXP3SPEC) delineated in Algorithm 9. This algorithm modifies the anytime EXP3 algorithm (Lattimore & Szepesvari, 2020) to suit the speculative decoding application. In terms of the algorithm design, the main difference lies in the change of the stopping rule. We highlight that while the stopping time of the algorithm is random, Assumption 5.1 (Adversarial Mean Values). Let the number of accepted tokens generated by hyperparameter Si at 2Our analysis can also be extended to cover the sampling decoding strategy (see Remark D.3). 6 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms the anytime feature of Algorithm 5 does not require any information about the time horizon. This is achieved by the vanishing sequence of learning rates {ηt}tN which can be elegantly adapted to the unknown stopping time. With regard to the analysis, previous works (Auer et al., 2002a; Bubeck et al., 2012; Lattimore & Szepesvari, 2020) only consider the gap between the cumulated rewards over the same fixed horizon , i.e., maxi[K] E[(cid:80) t[T ] yi,t yIt,t]. In contrast, we need to upper bound the stopping time regret in (5) where the baseline ALGi and any proposed algorithm ALG have different termination times in general. Thus, the analysis is much more involved. Theoretical Analysis To ease the analysis, we make an assumption on the stopping time of Algorithm 5. Assumption 5.2 (Stopping Time assumption). Given prompt pt and configuration ν, := argmini[K] ST(ALGi). We assume that ST(ALG) > ST(ALGi ) almost surely. let In speculative decoding, when the initial prompt is given, there generally exists hyperparameter that has the highest acceptance rate in most rounds compared to the rest of the hyperparameters. As bandit algorithms will explore those suboptimal hyperparameters, the termination time falls behind that of the optimal hyperparameter. Therefore, Assumption 5.2 is satisfied in practical applications. Theorem 5.3. Under Assumptions 4.2, 5.1 and 5.2, given any prompt pt and bandit configuration ν = (P, = {Si}i[K], L), the expected stopping time regret of Algorithm 3 with ALG = Algorithm 5 (EXP3SPEC), Reg(ALG, pt, ν) 2L min 2LK log + (cid:114) min i[K] (cid:26)(cid:113) len(ptτc)K log K, (cid:27) ST(ALGi)K log . Theorem 5.3 also provides an affirmative answer to the question posed in Section 3. The first term in the minimum provides worst-case guarantee. Even if all hyperparameters in are not good or is large, EXP3SPEC will stop at no more than O((cid:112)len(ptτc)) time steps after Si terminates. The second term is an instance-dependent bound in terms of hyperparameters S. Specifically, when the best hyperparameter Si has small stopping time, EXP3SPEC will scale as ST(ALGi ) + O((cid:112)ST(ALGi )). This upper bound suggests that the number of speculative decoding rounds of EXP3SPEC is almost the same as that of the best hyperparameter configuration ALGi . 6. Experiments In this section, we conduct two sets of experiments to demonstrate the efficacy of the proposed bandit framework 7 BANDITSPEC, along with UCBSPEC and EXP3SPEC. In the first experiment, the candidate hyperparameters are different draft models. In the second experiment, the candidate hyperparameters are different speculation lengths, where real-life LLM serving scenarios are simulated with diverse input prompts. Additional experimental results on memory utilization and additional experiments on larger models and different hardwares are provided in Appendix G. The code is accessible via https://github.com/sail-sg/ BanditSpec. 6.1. Experiment with Draft Models Experimental Setups We adopt the open-sourced LLaMA3-8B-Instruct (Dubey et al., 2024) and Qwen27B-Instruct (Yang et al., 2024) as the target models. The commonly-used existing speculative decoding methods PLD (Saxena, 2023), Rest (He et al., 2024), Suffix Tree (Oliaro et al., 2024; Hu et al., 2024) and Eagle-2 (Li et al., 2024b) are adopted as the baselines. Among these baselines, PLD, Rest, and Suffix Tree represent the nonparametric (or model-free) speculative decoding methods, whereas Eagle-2 represents the speculative decoding methods that utilize smaller draft models. Each of these methods corresponds to an arm in our problem. The experiments are carried out on Spec Bench (Xia et al., 2024), Alpaca (Taori et al., 2023), Code Editor (Guo et al., 2024) and Debug Bench (Tian et al., 2024). Among these benchmarks, Spec Bench and Alpaca encompass multiple topics, while Code Editor and Debug Bench focus on coding tasks, representative scenario for specialized models. We record the number of accepted tokens for each speculative decoding step, as well as the wall-time for generating each complete response. The Mean Accepted Tokens (MAT) and the throughput (Tokens/s) are computed. These two metrics are widely adopted in the speculative decoding community and are positively correlated (Xia et al., 2024). In particular, Tokens/s measures the actual latency during decoding. The experiments are conducted on single A100 and set batch size as 1. Experimental Results We report the results of our experiments in Table 1. The proposed adaptive speculative decoding framework BANDITSPEC exhibits superior performance compared to existing methods in the datasets we consider. In particular, the best performance measured by Token/s is always achieved by the proposed framework. We note that although the non-parametric methods are worse than Eagle2 in average, they are effective on portion of prompts. Our proposed methods, UCBSPEC and EXP3SPEC, automatically adapt to different prompts, i.e., suffering from small stopping time regret on each prompt. Thus, they achieve better performance than all the methods that only use fixed model. On Debug Bench, UCBSPEC can even achieve BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Table 1. Empirical Comparison between the proposed algorithms and the existing works, measured by Mean Accepted Tokens (MAT) () and Tokens/s (). The best result is highlighted in bold, while the second best result is underlined. The proposed algorithms demonstrate unequivocal superior performance compared with the existing methods. Methods Spec Bench Alpaca Code Editor Debug Bench MAT() Tokens/s() MAT() Tokens/s() MAT() Tokens/s() MAT() Tokens/s() LLaMA3-8B-Instruct Vanilla PLD Rest Suffix Tree Eagle-2 EXP3SPEC UCBSPEC 1.00 1.46 1.29 1.83 3.94 3.65 3.98 Qwen2-7B-Instruct Vanilla PLD Rest Suffix Tree Eagle-2 EXP3SPEC UCBSPEC 1.00 1.55 1.31 1.96 3.64 3.76 4. 35.73 43.96 40.67 55.10 98.15 102.10 105.72 38.71 52.44 46.42 68.42 97.82 107.36 112.33 1.00 1.53 1.48 1.71 4.04 4.23 4.35 1.00 1.42 1.47 1.46 3.61 3.83 3.93 35.92 53.06 52.40 64.02 110.00 120.38 125.78 39.32 58.41 59.01 62.60 104.43 113.90 114. 1.00 2.13 1.33 2.30 4.79 4.36 4.83 1.00 1.89 1.31 2.18 4.88 4.90 4.92 36.32 82.61 51.32 90.21 128.76 137.29 138.27 39.30 64.56 53.79 85.75 138.58 160.41 161.35 1.00 1.67 1.29 2.13 4.78 4.50 4.60 1.00 2.15 1.22 2.49 4.79 4.86 5. 36.89 82.76 48.49 77.56 119.12 132.25 135.34 39.57 70.49 50.51 101.47 126.01 151.73 151.37 improvements of 13% for LLaMA3 and 19% for Qwen2. Moreover, as UCBSPEC demonstrates better performance under almost all benchmarks with both two target models, this suggests that speculative decoding in real-life environments tends to be closer to the stochastic (Assumption 4.1) compared to the adversarial reward case (Assumption 5.1). Remark 6.1. The adversarial setting can be regarded as means of comparison to the stationary setting. Prior to this work, it was priori unclear how to use MAB to improve speculative decoding. Should one employ stochastic, adversarial or even more generalized model? We consider range of such MAB models and do comparison among them to provide the community with guide on which MAB model is best suited to the speculative decoding problem. As the empirical performance of UCBSPEC is better than EXP3SPEC  (Table 1)  , it implies that real-life scenario tends to be benign and may be more aligned with the stationary mean assumption. 6.2. Experiment with Speculation Lengths Experimental Setups In addition to improving the latency when batch size is 1, our proposed algorithms also improve the throughput in real LLM serving scenarios with different batch sizes. In practical serving environments, speculative decoding does not always yield performance gains due to variations in batch size and acceptance rate. As the batch size increases, the system rapidly becomes compute-bound, while lower acceptance rate can lead to wasted computation resources of GPU. Additionally, the execution time of the draft model contributes to an overall decrease in throughput. Given these confoundingly interrelated factors, along with latent variables such as the acceptance rate (which is unknown before verification and depends on the input prompts), we adopt bandit-based approach to model the current throughput as the reward. Specifically, we employ UCBSPEC to dynamically adjust the hyperparameter γ, the speculation length, to maximize the throughput, i.e., the number of generated tokens per second. We set the maximum speculation length as 4, and γ takes values in {0, . . . , 4} where γ = 0 corresponds to the canonical decoding (Algorithm 1). As the first experiment suggests UCBSPEC is more in line with the real-life speculative decoding environment than EXP3SPEC, we only evaluate UCBSPEC in this experiment. The experiments are conducted on single A100. Specifically, we use LLaMA3-8B-Instruct and Qwen2-7BInstruct as the target models and adopt Eagle-1 (Li et al., 2024a), the current state-of-the-art model, as the draft model. We do not use Eagle-2 (Li et al., 2024b) because it does not support batch inference. For evaluation, we adopt Alpaca (Taori et al., 2023) as the test set, as it covers various topics, thereby simulating realistic setting with diverse acceptance rates. To approximate real-world conditions, we randomly sample prompts from the test set to form batch for inference, with batch sizes ranging from 1 to 50. As 8 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms choose appropriate hyperparameters to accelerate LLM inference under realistic assumptions. Both theoretical guarantees and extensive experiments are provided to demonstrate that adaptive speculative decoding via bandit algorithms can boost the performance of existing methods in training-free manner. For future work, we would like to point some directions, improving the performance of the current algorithms. Therefore, another direction is to design hyperparameter selection algorithms that can achieve the (near) optimal balance between these two goals based on practical needs. Structured Bandits Our current framework is based on the standard K-armed bandit model. However, broader classes of bandit problems with additional structuressuch as linear bandits (Abbasi-yadkori et al., 2011) and Lipschitz bandits (Magureanu et al., 2014)can also be considered. This aligns more closely with practical scenarios, where the number of hyperparameters can be large, and the value of may be very high when modeling the problem as K-armed MAB. By leveraging such structures in MABs, we can expect to identify better hyperparameters more efficiently, thereby further accelerating the optimization process. Robust bandits and Non-stationary bandits As indicated by the experimental result, the real-life speculative decoding environment is closer to the stochastic reward case (Assumption 4.1) than the adversarial reward case (Assumption 5.1). Therefore, one direction for future work is to consider the settings in between, e.g., robust bandits in the presence of adversarial corruptions (Ding et al., 2022; Zhong et al., 2021), or non-stationary bandits (Cao et al., 2019; Besbes et al., 2014; Hou et al., 2024) where the mean number of accepted tokens can vary across time. These settings are more benign than the adversarial reward assumption and can be exploited to accelerate the inference. Contextual Bandits Another direction is to explore contextual bandits, where the environment reveals additional information that can be leveraged to reduce the learning burden (Luo et al., 2018; Kato & Ariu, 2021). Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Acknowledgements: This work is supported by funding from the Singapore Ministry of Education Academic Research Fund (AcRF) Tier 1 grants under grant numbers A-8002934-00-00 and A-8000980-00-00. This research is also supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2023-08-044T-J), and is part of the programme DesCartes which is supported by the National Research (a) Target model: LLaMA (b) Target model: Qwen2 Figure 3. We compare throughtput improvements with different speculative decoding lengths γ [4] and the canonical decoding (γ = 0). The performance of UCBSPEC approaches that of the best hyperparameter across all samples for both target models LLaMA3 and Qwen2. The sample indices are sorted according to the best arm improvement for clear demonstration. our evaluation metric, we measure the throughput improvement relative to the canonical decoding (non-speculative) baseline. Our result is averaged over 16 independent runs to smoothen the hardware-dependent factors. Experimental Results The results are presented in Figure 3, where we reorder the 500 sample indices in ascending order of the performance of the best hyperparameter (blue line) for easy comparison. Otherwise, the lines in this figure will not be largely monotonic. Here the worst and best lines are calculated among results of γ {1, , 4} in hindsight. Thus, we call the best line as the oracle best. Firstly, since the optimal hyperparameter γ varies with different input prompts for either target model, fixing single hyperparameter is suboptimal, e.g., in Figure 3 (b), the best hyperparameter changes from γ = 1 (light green) to γ = 2 (green) at sample index around 80; and the original Eagle1 (Li et al., 2024a) (γ = 4 in purple) is even inferior to the canonical decoding (γ = 0 in grey) for sample indices less than 80. This necessitates the use of adaptive hyperparameter selection. Next, UCBSPEC demonstrates competitive throughput performance, outperforming the second-best hyperparameter in most cases and closely approaching the (varying) oracle best across experiments. These benefits are obtained thanks to the adaptivity of BANDITSPEC. 7. Conclusions and Discussions In this work, we propose MAB framework together with two hyperparameter selection algorithms that adaptively 9 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Foundation, Prime Ministers Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme."
        },
        {
            "title": "References",
            "content": "Abbasi-yadkori, Y., Pal, D., and Szepesvari, C. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, volume 24, pp. 23122320, 2011. Agrawal, S. and Goyal, N. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on learning theory, pp. 391. JMLR Workshop and Conference Proceedings, 2012. Agrawal, S. and Goyal, N. Near-optimal regret bounds for thompson sampling. Journal of the ACM (JACM), 64(5): 124, 2017. Antos, A., Grover, V., and Szepesvari, C. Active learning in heteroscedastic noise. Theoretical Computer Science, 411(29-30):27122728, 2010. Arora, R., Dekel, O., and Tewari, A. Online bandit learning against an adaptive adversary: from regret to policy regret. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 1747 1754, 2012. Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):4877, 2002a. Auer, P., Fischer, P., and Cesa-Bianchi, N. Finite-time analysis of the multi-armed bandit problem. Machine Learning, 47:235256, 2002b. Besbes, O., Gur, Y., and Zeevi, A. Stochastic multi-armedbandit problem with non-stationary rewards. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K. (eds.), Proceedings of the 27th Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., teusz Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language modArXiv, abs/2005.14165, els are few-shot 2020. URL https://api.semanticscholar. org/CorpusID:218971783. learners. Bubeck, S., Cesa-Bianchi, N., et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5 (1):1122, 2012. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Cao, Y., Wen, Z., Kveton, B., and Xie, Y. Nearly optimal adaptive procedure with change detection for piecewisestationary bandit. In Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, pp. 418427. PMLR, 2019. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Chen, Z., May, A., Svirschevski, R., Huang, Y., Sequoia: Ryabinin, M., Jia, Z., and Chen, B. and hardware-aware speculative Scalable, decoding. ArXiv, abs/2402.12374, 2024. URL https: //api.semanticscholar.org/CorpusID: 267751265. robust, Ding, Q., Hsieh, C.-J., and Sharpnack, J. Robust stochastic linear contextual bandits under adversarial attacks. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pp. 71117123. PMLR, 2830 Mar 2022. Du, C., Jiang, J., Yuanchen, X., Wu, J., Yu, S., Li, Y., Li, S., Xu, K., Nie, L., Tu, Z., et al. Glide with cape: lowhassle method to accelerate speculative decoding. arXiv preprint arXiv:2402.02082, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Garivier, A. and Cappe, O. The kl-ucb algorithm for bounded stochastic bandits and beyond. In Proceedings of the 24th Annual Conference on Learning Theory, volume 19 of Proceedings of Machine Learning Research, pp. 359376. PMLR, 0911 Jun 2011. Gordon, G. J. Regret bounds for prediction problems. In Proceedings of the 12th Conference on Learning Theory, pp. 2940, 1999. Guo, J., Li, Z., Liu, X., Ma, K., Zheng, T., Yu, Z., Pan, D., LI, Y., Liu, R., Wang, Y., Guo, S., Qu, X., Yue, X., Zhang, G., Chen, W., and Fu, J. Codeeditorbench: Evaluating code editing capability of large language models, 2024. 10 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms He, Z., Zhong, Z., Cai, T., Lee, J., and He, D. REST: Retrieval-based speculative decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 15821595. Association for Computational Linguistics, June 2024. Hou, Y., Tan, V., and Zhong, Z. Almost minimax optimal best arm identification in piecewise stationary linear bandits. Advances in Neural Information Processing Systems, 37:128967129041, 2024. Hu, Y., Wang, K., Zhang, J., Li, C., and Chen, H. Sam decoding: Speculative decoding via suffix automaton. arXiv preprint arXiv:2411.10666, 2024. Hu, Z. and Huang, H. Accelerated speculative sampling based on tree monte carlo. In Forty-first International Conference on Machine Learning, 2024. Huang, K., Guo, X., and Wang, M. Specdec++: Boosting speculative decoding via adaptive candidate lengths. arXiv preprint arXiv:2405.19715, 2024. Kato, M. and Ariu, K. The role of contextual information in best arm identification, 2021. arXiv:2106.14077. Khisti, A., Ebrahimi, M. R., Dbouk, H., Behboodi, A., Memisevic, R., and Louizos, C. Multi-draft speculative sampling: Canonical architectures and theoretical limits. In The Thirteenth International Conference on Learning Representations, 2025. Kveton, B., Szepesvari, C., Vaswani, S., Wen, Z., Lattimore, T., and Ghavamzadeh, M. Garbage in, reward out: Bootstrapping exploration in multi-armed bandits. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 36013610. PMLR, 0915 Jun 2019. Lai, T. L. and Robbins, H. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1): 422, 1985. Lattimore, T. and Szepesvari, C. Bandit algorithms. Cambridge University Press, 2020. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference In Interfrom transformers via speculative decoding. national Conference on Machine Learning, pp. 19274 19286. PMLR, 2023. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024a. Li, Y., Wei, F., Zhang, C., and Zhang, H. EAGLE-2: Faster inference of language models with dynamic draft trees. In Empirical Methods in Natural Language Processing, 2024b. Liu, X., Hu, L., Bailis, P. D., Stoica, I., Deng, Z., Cheung, A., and Zhang, H. Online speculative decoding. ArXiv, abs/2310.07177, 2023. URL https: //api.semanticscholar.org/CorpusID: 263835233. Liu, X., Daniel, C., Hu, L., Kwon, W., Li, Z., Mo, X., Cheung, A., Deng, Z., Stoica, I., and Zhang, H. Optimizing speculative decoding for serving large language models using goodput. arXiv preprint arXiv:2406.14066, 2024. Luo, H., Wei, C.-Y., Agarwal, A., and Langford, J. Efficient contextual bandits in non-stationary worlds. In Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 17391776. PMLR, 0609 Jul 2018. Magureanu, S., Combes, R., and Proutiere, A. Lipschitz bandits: Regret lower bound and optimal algorithms. In Conference on Learning Theory, pp. 975999. PMLR, 2014. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y. Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. ArXiv, abs/2305.09781, 2023. URL https://api.semanticscholar. org/CorpusID:258740799. Oliaro, G., Jia, Z., Campos, D., and Qiao, A. Suffixdecoding: model-free approach to speeding up large language model inference. arXiv preprint arXiv:2411.04975, 2024. Robbins, H. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527535, 1952. Russo, D., Roy, B. V., Kazerouni, A., and Osband, I. tutorial on thompson sampling. ArXiv, abs/1707.02038, 2017. URL https://api.semanticscholar. org/CorpusID:3929917. Saxena, A. Prompt lookup decoding, November 2023. https://github.com/apoorvumang/ URL prompt-lookup-decoding/. Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024a. 11 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms findings-acl.456. URL https://aclanthology. org/2024.findings-acl.456. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Yin, M., Chen, M., Huang, K., and Wang, M. theoretical perspective for speculative decoding algorithm. arXiv preprint arXiv:2411.00841, 2024. Zhong, Z., Cheung, W. C., and Tan, V. Probabilistic sequential shrinking: best arm identification algorithm for stochastic bandits with corruptions. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1277212781. PMLR, 1824 Jul 2021. Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Rostamizadeh, A., Kumar, S., Kagy, J.-F., and Agarwal, R. Distillspec: Improving speculative decoding via knowledge distillation. In The Twelfth International Conference on Learning Representations, 2024. Sun, Z., Mendlovic, U., Leviathan, Y., Aharoni, A., Beirami, A., Ro, J. H., and Suresh, A. T. Block verification accelerates speculative decoding. In Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024b. Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and Yu, F. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024c. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023. Tian, R., Ye, Y., Qin, Y., Cong, X., Lin, Y., Liu, Z., and Sun, M. Debugbench: Evaluating debugging capability of large language models, 2024. Touvron, H., Martin, L., Stone, K. R., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A. S., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I. M., Korenev, A. V., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M. H. M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. URL https://api.semanticscholar. org/CorpusID:259950998. Wan, R., Wei, H., Kveton, B., and Song, R. Multiplier bootstrap-based exploration. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3544435490. PMLR, 2329 Jul 2023. Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., and Zou, J. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Xia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T., Li, W., and Sui, Z. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 76557671, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms A. Related Works Speculative Decoding Speculative decoding is proposed in Leviathan et al. (2023); Chen et al. (2023), where the draft model only generates single chain of draft tokens. Then line of works extends the chain structure to the tree structure (Miao et al., 2023; Cai et al., 2024; Du et al., 2024; Li et al., 2024a; Hu & Huang, 2024). In these works, the draft tokens are organized as connected tree. To further improve the number of accepted tokens, previous works propose to generate tokens in batch manner, i.e., the draft tokens are organized as multiple disconnected parts. SpecTr (Sun et al., 2024c) views this problem from the optimal transport perspective and derives the algorithm that is optimal up to multiplicative constant. Khisti et al. (2025) derives the canonical form of this problem and design the relaxed optimization algorithms. All these algorithms verify the draft tokens in token by token manner. Sun et al. (2024b) proposes to verify all the draft tokens as whole block, which further boosts the acceleration ratio. Sun et al. (2024a) proposes to fit the speculative deciding into hierarchical structure, which multiple draft models with various sizes are generating and verifying tokens. The smaller model will generate more tokens. This fine-grained behavior improve the overall performance of the system. Liu et al. (2023) design algorithms to update the draft model parameters in an online manner, which makes the draft model adaptive to the current context. Liu et al. (2024) and Huang et al. (2024) aim to optimize the speculative length in training and training-free manner (more discussions on SpecDec++ (Huang et al., 2024) are provided in Appendix B.4). Chen et al. (2024) optimizes the hyperparameters related to the hardware by dynamic programming in an offline manner. We also note that there are series of non-parametric speculative decoding algorithms (Hu et al., 2024; Oliaro et al., 2024), i.e., the draft model itself does not require any training procedures. Yin et al. (2024) derives the theoretical analysis of the speculative decoding. Multi-Armed Bandit The multi-armed bandit problem is fundamental topic in decision theory and reinforcement learning, with various algorithms developed to address the exploration-exploitation trade-off. The standard stochastic K-armed bandit problem is firstly introduced by Robbins (1952) and then studied by Lai & Robbins (1985). There has been major theoretical advancement with the introduction of Upper Confidence Bound (UCB) algorithms (Auer et al., 2002b). Various algorithms have been proposed to achieve improved theoretical guarantees and practical performance since then (Garivier & Cappe, 2011; Bubeck et al., 2012). Beyond UCB-type algorithms, sampling-based algorithms, such as Thompson Sampling (Agrawal & Goyal, 2012; 2017; Russo et al., 2017) and sampling via bootstrap (Kveton et al., 2019; Wan et al., 2023), have also exhibited strong empirical performance with provable regret bounds. Furthermore, the problem has been extended to the adversarial settings where the rewards are no longer stochastic (Auer et al., 2002a; Bubeck et al., 2012). We refer to Lattimore & Szepesvari (2020) for comprehensive introduction of the Multi-Armed Bandit problem. B. Additional Discussions and Remarks B.1. Discussion on the Assumptions On the theoretical side, the stationary mean assumption (Assumption 4.1) is strictly weaker than the i.i.d. assumption. In particular, the number of accepted tokens can depend on the generated tokens. Therefore, this assumption is aligned with real-world scenarios in which different decoding steps are correlated. Furthermore, the basic Multi-Armed Bandits (MAB) model can be generalized to contextual bandits and non-stationary bandits. The proposed BANDITSPEC framework provides basic template to apply these more general MAB setups to speculative decoding. Our formulations under the stationary/adversarial mean assumptions are just basic setups and we leave the more general/elaborate setups as future research. On the experimental side, our experimental results  (Table 1)  indicate, the performance of UCBSPEC significantly outperforms one of the best speculative decoding methods, Eagle-2 (Li et al., 2024). This corroborates the stationary mean assumption in our formulation. B.2. Discussion on UCBSPEC We comment that UCBSPEC is among the simplest UCB-type algorithms in the sense that only the empirical means and UCBs need to be maintained, and the hyperparameter to be selected can be directly determined via the UCBs. In contrast, Thompson Sampling (Agrawal & Goyal, 2012) and KL-UCB (Garivier & Cappe, 2011) generally achieve better empirical regret bounds than UCB1 (Auer et al., 2002b). However, Thompson Sampling requires maintaining the posterior distribution and sampling from it to select the arm to pull; whereas KL-UCB involves solving an optimization problem for arm selection. These steps add additional complexity to the algorithms. BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Given our goal of accelerating LLM inference, the simplicity of UCB1 is more in line with this objective. Therefore, we propose UCBSPEC, which redesigns the confidence radius and the stopping rule of UCB1 to adapt specifically to the speculative decoding application. B.3. Discussion on Adaptive Adversary We consider the oblivious adversary in this paper where the numbers of accepted tokens generated by all hyperparameters at all time steps, i.e., {yi,t}i[K],tN, are fixed before the decoding process starts. One may be interested in considering the adapative adversary, where the environment (adversary) can choose the number of accepted tokens generated by SIt based on (part of) the history information Ht1 and It (Arora et al., 2012). This adversary is more malicious than the oblivious one and the regret is expected to be even larger than the current one in Theorem 5.3. As our empirical experiments suggest that the practical scenario aligns more closely with the stochastic reward assumption (Assumption 4.1) and deviates from the oblivious adversarial reward assumption (Assumption 5.1), we believe it is unnecessary to consider the adaptive adversarial reward. B.4. Discussion on SpecDec++ (Huang et al., 2024) We compare the proposed methods with an existing adaptive speculative decoding method, SpecDec++ (Huang et al., 2024), in this section. SpecDec++ (Huang et al., 2024) is adaptive in choosing the speculation length, achieving good performance compared to the vanilla speculative decoding method (Leviathan et al., 2023; Chen et al., 2023). It trains an acceptance probability prediction head and stops drafting new tokens when the predicted rejection probability reaches certain threshold. We compare it with the proposed methods as follows: Firstly, we highlight that our proposed method is training-free which can be deployed easily along with existing off-the-shelf methods. In contrast, SpecDec++ focuses on training of an acceptance prediction head. Currently, SpecDec++ is only available when using LLaMA-2-Chat-7B as the draft model and LLaMA-2-Chat-70B as the target model (bfloat 16). Secondly, the proposed BANDITSPEC framework considers the more general hyperparameter selection problem that goes beyond merely the speculation length. Therefore, it is orthogonal to SpecDec++ in the sense that any methods with (or without) SpecDec++ can also be candidates for the hyperparameter in our framework, e.g., {Eagle-2, LLaMA-2-Chat-7B} with SpecDec++ can also be regarded as arms (if they are available). C. Additional Details In this section, we provide more details that complement the main paper. C.1. Vanilla Speculative Decoding For completeness, we present and describe the vanilla speculative decoding algorithm (Leviathan et al., 2023; Chen et al., 2023) in Algorithm 6 in this section. We introduce some notations first. For any nonnegative function : R+ with (cid:80) the distribution induced by it as Norm(f ()) = ()/ (cid:80) [f ()]+ = max{0, ()}. xX (x) > 0, we define xX (x). The positive part of function is denoted as Speculative decoding implements draft model to generate draft tokens and let the target model verify them in parallel. In practice, the draft model is much smaller than the target model. Thus, the draft token generation (Line 1) can be achieved in short time. Then we let the target model only forward inference once with these draft tokens as inputs (Line 2). The verification procedures (Lines 4 to 9) are designed to guarantee that the output tokens x1:τ +1 is distributed as the target model . Here, the additional (τ + 1)-st accepted token is also called the bonus token. 2 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Sample ri Unif([0, 1]). if ri min{1, (xi pt, x1:i1)/Q(xi pt, x1:i1)} then Algorithm 6 Vanilla Speculative Decoding Inputs: base model , draft model Q, prefix pt, maximum speculation length Procedures: 1: Generate draft tokens x1:L via xi Q( pt, x1:i1) for [L]. 2: Set τ = 0, and calculate the values of (xi pt, x1:i1) for [L] in parallel. 3: for = 1, . . . , do 4: 5: 6: 7: 8: 9: end if 10: 11: end for 12: if τ = then sample xL+1 ( pt, x1:L). 13: return x1:τ +1. Sample xi Norm(cid:0)(cid:2)P ( pt, x1:i1)Q( pt, x1:i1)(cid:3) break. Set τ = and xi = xi. else (cid:1). + C.2. Dynamics of MAB We provide description of the dynamics of MAB in Algorithm 7. Algorithm 7 Dynamics of MAB Inputs: arms, time horizon . 1: H0 = . 2: for = 1, 2, . . . , do 3: 4: 5: Ht = concat(Ht1, (It, XIt,t)). 6: end for Agent adopts an algorithm to select It based on Ht1. Environment reveals the reward XIt,t to the agent. The goal is to minimize the cumulative regret max i[K] (cid:20) (cid:88) (cid:21) Xi,t (cid:20) (cid:88) (cid:21) , XIt,t t= t=1 where the expectation is taken w.r.t. the randomness in the rewards (for the stochastic setup) and the possible internal randomness in the arm selection algorithm. C.3. Full Description of BANDITSPEC with UCBSPEC We provide the full description of BANDITSPEC(ALG) with ALG = UCBSPEC in Algorithm 8. 3 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms = + 1 if then Algorithm 8 BANDITSPEC(UCBSPEC) (Full version of UCBSPEC) Inputs: initial prompt pt0 = pt , bandit configuration ν = (P, = {Si}i[K], L). Procedures: 1: = 0, H0 = , I0 = 1, xI0,0 = . 2: while EOS / xIt,t do 3: 4: 5: 6: 7: 8: 9: 10: 11: end if Observe XIt,t = xIt,t = SPECDECSUB(ptt1, P, SIt, L) ptt = concat(ptt1, XIt,t), Ht = concat(Ht1, (It, XIt,t)). Update the statistics {ˆµi,t}i[K], {cri,t}i[K], where Select index It = argmaxi[K] CBi,t1. It = t. (Round-Robin) else and YIt,t = yIt,t = len(XIt,t). ni,t = (cid:88) s=1 1{Is = i}, ˆµi,t = (cid:80)t s=1 Yi,s1{Is = i} ni,t , (cid:115) 2 cri,t = 1 + ni,t n2 i,t UCBi,t = ˆµi,t + cri,t. (cid:18) 1 + 2 log Kt2(1 + ni,t) 1 δ 2 (cid:19) , 12: end while 13: return t, ptt C.4. Full Description of BANDITSPEC with EXP3SPEC We provide the full description of BANDITSPEC(ALG) with ALG = EXP3SPEC in Algorithm 9. Algorithm 9 BANDITSPEC(EXP3SPEC) (Full version of EXP3SPEC) Inputs: initial prompt pt0 = pt , bandit configuration ν = (P, = {Si}i[K], L), learning rates ηt = Procedures: 1: = 0, H0 = , I0 = 1, xI0,0 = . 2: while EOS / xIt,t do 3: 4: = + 1 Set probability vector pt [K] with (cid:113) log tK , N. pt,i = (cid:16) exp (cid:17) ηt (cid:16) (cid:80)t1 s=1 (cid:98)Zi,s (cid:80)t1 ηt s=1 (cid:98)Zj,s (cid:80)K j=1 exp (cid:17) , [K]. 5: 6: 7: 8: Select hyperparameter index It pt. Observe XIt,t = xIt,t = SPECDECSUB(ptt1, P, SIt, L) and yIt,t = len(XIt,t). ptt = concat(ptt1, XIt,t), Ht = concat(Ht1, (It, XIt,t)). Set the statistics (cid:98)Zi,t = 1{i = It} + 1 yi,t pt,i , [K]. 9: end while 10: return t, ptt 4 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms D. Proofs of Main Results D.1. Proof of Proposition 3. To prove Proposition 3.1, we note that we only need to prove ptST(ALG) d= ptτc, and len(ptST(ALG)) + 1 ST(ALG) len(ptST(ALG)), a.s. The other results are implied by these two results. For equality, we note that this is already proved by Theorem 1 in Yin et al. (2024), where the equality holds for any specification of the hyperparameters. For the inequality, we note that each implementation of SPECDECSUB generates at least one token and at most + 1 tokens. Thus, the inequality holds almost surely. D.2. Proof of Theorem 4.3 Theorem 4.3 (Upper Bound). Under Assumptions 4.1 and 4.2, given any prompt pt and bandit configuration ν = (P, = {Si}i[K], L), the expected stopping time regret of Algorithm 3 with ALG = Algorithm 4 (UCBSPEC) is upper bounded as Reg(ALG, pt, ν) = (cid:16) (cid:17) H(pt, ν) L2 log E[len(ptτc)] . Proof of Theorem 4.3. Our proof of Theorem 4.3 consists of three steps. Reward and Stopping time decomposition. Construction of the high probability event. Concluding the proof. As we mentioned in Section 4, the main difference lies at the two aspects: Firstly, the stopping time is now random, which depends on the generated tokens. This cause trouble when we decompose the reward/regret, as both the rewards and time horizon depend on the history. We tackle this problem in Step 1 by making use of the martingale structure of the rewards sequence. Secondly, we consider the problem under Assumption 4.1, where the number of accepted tokens can be dependent. This is practical as LLM generates tokens in an autoregressive manner. In contrast, under the commonly seen assumption for the K-armed MAB, the rewards are i.i.d. and can be regarded as they have been sampled before the algorithm starts (see Chapter 4 in Lattimore & Szepesvari (2020)). Thus, Chernoff-Hoeffding bound (Lemma F.2) can be directly applied, which cannot be used under Assumption 4.1. We solve this problem in Step 2, by adopting the so-called self-normalized confidence bounds (Abbasi-yadkori et al., 2011). Step 1: Reward and Stopping time decomposition. By the property of speculative decoding in (2) and (11), for any algorithm ALG, 3 E(cid:2)len(ptτc)(cid:3) = E[len(ptST(ALG))] = (cid:20) ST(ALG) (cid:88) (cid:21) YIt,t t=1 We wish to decompose the expected total token sequence in terms of each hyperparameter [K] in the first step, i.e., (cid:20) ST(ALG) (cid:88) (cid:21) YIt,t = t=1 (cid:88) i=1 µi E(cid:2)ni,ST(ALG) (cid:3). 3We clarify that only the tokens up to the EOS token will be appended to the prefix token sequence in practice. Therefore, the actual number of accepted tokens is between [1, YIST(ALG),ST(ALG)]. While this mismatch can be solved, using YIST(ALG),ST(ALG) as the number of accepted tokens in the last round will introduce an error of at most to the token sequence length len(ptST(ALG)), which is an error of at most 1 to the stopping time ST(ALG). This error is negligible compared to the other values in the stopping time regret. Therefore, we assume the EOS token only appears at the end of accepted tokens for the sake of simplicity. 5 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms The standard regret analysis adopts Walds equation to decompose the expected cumulative regret, or equivalently the stopping time. However, as both the stopping time ST(ALG) and YIt,t depends on the history under our problem setup, Walds equation fails. We propose new and general approach to decompose the reward. Step 1.1: We first prove that Mn := (cid:80)n M0 := 0, Fn := σ(Hn). By the definition of martingale, we only need to show (1) E[Mn] < , and (2) E[Mn+1Fn] = Mn. (1) E[Mn] < : As the number of the accepted tokens at each round is bounded as YIt,t [1, + 1] almost surely and µIt [1, + 1], we have YIt,t µIt L. Then the triangular inequality Mn (cid:80)n t=1 YIt,t µIt < indicates that t=1 YIt,t µIt, = 0, 1, 2, . . . is martingale with respect to {Fn} n=0, where E[Mn] < . (2) E[Mn+1Fn] = Mn: The conditional expectation of Mn+1 can be calculated via tower property as E[Mn+1 Fn] = Mn + (cid:2)E (cid:2)YIn+1,n+1 µIn+1 Hn, In+1 (cid:3) Hn (cid:3) = Mn, where the last equality results from Assumption 4.1. (7) (8) n=0. t=1 YIt,t µIt, = 1, 2, . . . is martingale with respect to {Hn} Based on (7) and (8), Mn, = 0, 1, 2, . . . is martingale with respect to {Fn} Step 1.2: We then prove E[MST(ALG)] = 0 via Doobs optional stopping lemma (Lemma F.1). We have already showed that (cid:80)n n=0. In order to apply Lemma F.1, we firstly verify the prerequisites listed in Lemma F.1 condition (b): (1) E[ST(ALG)] < , and (2) there exists R, such that E[Mt Mt1 Ft1] almost surely for ST(ALG). (1) E[ST(ALG)] < : According to the property of speculative decoding (3) and Assumption 4.2, we have that E[ST(ALG)] E[len(ptτc)] < . (2) E[Mt Mt1 Ft1] c: As Mt Mt1 = YIt,t µIt and YIt,t µIt almost surely, it holds that E[Mt Mt1 Ft1] L. Taking = finishes the verification. Therefore, condition (b) in Lemma F.1 is satisfied and we obtain (cid:20) ST(ALG) (cid:88) t=1 (cid:21) YIt,t µIt = 0. Step 1.3: We show that (cid:104) (cid:80)ST(ALG) t=1 (cid:105) YIt,t = (cid:80)K i=1 µi E(cid:2)ni,ST(ALG) (cid:3). We firstly note that (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) ST(ALG) (cid:88) t= YIt,t (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) (L + 1)E[ST(ALG)] < and (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) ST(ALG) (cid:88) t=1 µIt (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) (L + 1)E[ST(ALG)] < , so the expectations of (cid:80)ST(ALG) t=1 YIt,t and (cid:80)ST(ALG) t=1 µIt exist and are finite (integrable). Furthermore, we have (cid:20) ST(ALG) (cid:88) (cid:21) µIt (cid:20) (cid:88) = ST(ALG) (cid:88) t=1 i=1 t=1 1{It = i} µi (cid:21) = (cid:88) i=1 µi E(cid:2)ni,ST(ALG) (cid:3), (9) (10) where ni,ST(ALG) = (cid:80)ST(ALG) (10) imply t=1 1{It = i} by definition. Because (cid:80)ST(ALG) t=1 YIt,t and (cid:80)ST(ALG) t= µIt are integrable, (9) and (cid:20) ST(ALG) (cid:88) (cid:21) YIt,t (cid:20) ST(ALG) (cid:88) = YIt,t µIt (cid:21) (cid:20) ST(ALG) (cid:88) + (cid:21) µIt = t=1 t=1 t=1 (cid:88) i=1 µi E(cid:2)ni,ST(ALG) (cid:3). (11) 6 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms This equality decomposes the cumulative reward (and stopping time) in terms of the arms. Step 2: Construction of the high probability event. We then derive the concentration property for the number of accepted tokens. Define the good events: Et := (cid:8)ˆµi,t [µi cri,t, µi + cri,t], [K] at round t(cid:9). Since random variables supported on [a, b] is (b a)2/4-sub-Gaussian and 1 µIt YIt,t µIt + 1 µIt under our problem setup. According to Lemma F.3, we obtain P(cid:0)Et (cid:1) 1 δ t2 and (cid:88) t=K+1 P(cid:0)E (cid:1) π2δ 6 , (12) where δ is confidence parameter that will be specified later. We remark that Lemma F.3 from Abbasi-yadkori et al. (2011) adopts self-normalized concentration bound for the martingale sequence, which generalizes the standard i.i.d. reward assumption in the K-armed MAB problem. As result, we can now bound the number of times arm is pulled at any round K. Conditional on the good event Et, we have ˆµi,t [µi cri,t, µi + cri,t] and arm will not be pulled if cri,t < i/2. By adopting Lemma F.4, when arm is selected at time + 1, it must hold that ni,t 4 + (cid:16) 2L2 2 1 + 2 log LK t2 iδ (cid:17) . Step 3: Concluding the proof. According to Step 1, E(cid:2)len(ptτc)(cid:3) = E[len(ptST(ALG))] = (cid:20) ST(ALG) (cid:88) t= (cid:21) YIt,t = (cid:88) i=1 µi E(cid:2)ni,ST(ALG) (cid:3). Therefore, Reg(ALG) = 1 µi = (cid:88) i=i (cid:18) (cid:88) µi (cid:2)ni,ST(ALG) (cid:3) + i[K] µi E[ni,ST(ALG)]. (cid:88) i[K] Under the UCBSPEC algorithm, we have (cid:88) i=i µi (cid:2)ni,ST(ALG) (cid:3) (cid:2)ni,ST(ALG) (cid:3) µi [ST(ALGi )] (cid:19) = (cid:88) i=i µi (cid:20) ST(ALG) (cid:88) (cid:26) 1 t=K+1 It = i, ni,t1 4 + (cid:16) 2L2 2 1 + 2 log (cid:17)(cid:27)(cid:21) LKt2 iδ + (cid:88) i=i µi (cid:20) ST(ALG) (cid:88) (cid:26) 1 t=K+1 It = i, ni,t1 > 4 + (cid:16) 2L2 2 1 + 2 log LK(t 1)2 iδ (cid:17)(cid:27)(cid:21) + (13) (14) (cid:88) i=i (cid:88) i=i (cid:88) i=i µi µi µi (cid:20) 4 + (cid:16) 2L2 2 1 + 2 log LK (ST(ALG))2 iδ (cid:18) (cid:18) 4 + 4 + (cid:16) (cid:16) 2L2 2 2L2 2 1 + 2 log 1 + 2 log LK (E[ST(ALG)])2 iδ LK (E[len(ptτc])2 iδ (cid:17)(cid:21) (cid:20) ST(ALG) (cid:88) + (cid:21) 1(cid:8)E (cid:9) + t=K+ (cid:17)(cid:19) + (cid:88) P(cid:0)E (cid:1) + K, (cid:17)(cid:19) t=K+1 π2δ 6 + K, + 7 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms where the first inequality results from (13), the second inequality utilizes Jensens inequality, and the last inequality adopts the property of speculative decoding (3) and the upper bound on the error probability of good event (12) in Step 2. Taking δ = 1/2 in the above bound concludes the proof of this theorem. D.3. Proof of Theorem 5.3 Fix any [K], the baseline algorithm is set to be ALGb, i.e., Algorithm 3 implements Line 4 of Algorithm 3 with hyperparameter Sb only. Let ALG = EXP3SPEC. we assume the BANDITSPEC(ALG) repeats the while loop in Algorithm 3 until max{ST(ALG), ST(ALGb)}. To avoid any confusion, we restate the algorithm for the purpose of analysis in Algorithm 10. Algorithm 10 takes ALG and ALGb as an input and stops until ptST(ALG) and ptST(ALGb) are generated. The two token sequences up to EOS token are output at the end of the algorithm. 0 = pt, stopping time τ = . Algorithm 10 BANDITSPEC(EXP3SPEC) (For analysis purpose) Inputs: initial prompt pt0 = pt , speculative decoding configuration ν = (P, = {Si}i[K], L), stopping time τ = , baseline hyperparameter Sb, initial prompt ptb Procedures: 1: = 0, H0 = , I0 = 1, xI0,0 = , xb 2: while τ = or τ = do 3: 4: 5: 6: 7: 8: = + 1 // Procedures of the original EXP3SPEC if τ = and EOS xIt1,t1 then end if Set probability vector pt [K] with τ = 1 and ptτ = ptt1. b,0 = . pt,i = (cid:16) exp (cid:17) ηt (cid:16) (cid:80)t1 s=1 (cid:98)Zi,s (cid:80)t1 ηt s=1 (cid:98)Zj,s (cid:80)K j=1 exp (cid:17) with learning rate ηt = (cid:114) log , [K]. (15) Select hyperparameter index It pt. Observe XIt,t = xIt,t = SPECDECSUB(ptt1, P, SIt, L) and yIt,t = len(XIt,t). ptt = concat(ptt1, XIt,t), Ht = concat(Ht1, (It, XIt,t)). Set the statistics (cid:98)Zi,t = 1{i = It} + 1 yi,t pt,i , [K]. (16) // Procedures of the baseline ALGb if τ = and EOS / xb τ = 1 and ptb It1,t1 then t1. τ = ptb b,t = xb = concat(ptb end if Observe ptb 18: 19: end while 20: return ST(ALG) = τ, ptST(ALG) = ptτ and ST(ALGb) = τ b, ptST(ALGb) = ptb It,t = SPECDECSUB(ptb t1, t1, P, Sb, L) and yb,t = len(X b,t). τ . b,t). 9: 10: 11: 12: 13: 14: 15: 16: 17: Theorem 5.3. Under Assumptions 4.2, 5.1 and 5.2, given any prompt pt and bandit configuration ν = (P, = {Si}i[K], L), the expected stopping time regret of Algorithm 3 with ALG = Algorithm 5 (EXP3SPEC), (cid:26)(cid:113) len(ptτc)K log K, (cid:27) ST(ALGi)K log . Reg(ALG, pt, ν) 2L min 2LK log + (cid:114) min i[K] BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Algorithm 11 Dynamics of the OLO Problem with Full Information Feedback 1: H0 = . 2: for = 1, 2, . . . , do 3: 4: 5: Ht = concat(Ht1, (pt, ℓt)). 6: end for Selects pt [K] based on Ht1. Observes the loss vector ℓt and suffers loss ℓt. Proof of Theorem 5.3. For ease of presentation, we present Algorithm 10, where the while loop in BANDITSPEC is repeated until max{ST(ALG), ST(ALGi)}. Our analysis of Algorithm 10 is novel compared to the standard analysis of EXP3 algorithm (Auer et al., 2002a; Lattimore & Szepesvari, 2020). It requires more technical manipulations due to the fact that the termination times of the baseline algorithm ALGi and ALG are different and random, and that our goal is to minimize the stopping time regret (5). For theoretical analysis, we regard Algorithm 10 as an instantiation of the Follow-the-Regularized-Leader (FTRL) algorithm (Gordon, 1999; Lattimore & Szepesvari, 2020). The proof is decomposed into 5 steps: Connection between FTRL and Algorithm 10: we firstly introduce FTRL and the problem where it is applicapable. The shared features and differences are highlighted. Transformation of the stopping time regret: the stopping time regret is related to the regret under FTRL framework. In this case, the minimized regret by FTRL can be translated to the stopping time regret. Regret decomposition: the FTRL regret is decomposed for easier processing. Upper bound each term in the decomposed regret: we upper bound each term in the decomposed regret. The main difficulty is to deal with the difference in the time scales ST(ALG) and ST(ALGi) and the randomness in ST(ALG). Specifically, because both the loss vectors { (cid:98)Zt}tN and the stopping time ST(ALG) are random, taking expectation of the cumulative loss within [ST(ALGi), ST(ALG)] is non-trivial. We devise Lemma D.1 and Lemma D.2 to deal with this problem. Conclusion of the stopping time regret: we aggregate the results in the previous steps and derive the final bound for the stopping time regret. Step 1: Connection between FTRL and Algorithm 5. We denote [K] as the K-dimensional probability simplex. FTRL is often used in the Online Learning Optimization Problem (OLO). We firstly provide brief introduction to OLO that operates on [K]. Let ℓ1, ℓ2, . . . RK be sequence of unknown loss vectors. The dynamics of OLO problem is stated in Algorithm 11. Given time horizon N, the agent (or algorithm) aims to minimize the (loss-based) regret max p[K] (cid:88) t=1 (pt p)ℓt, (17) where pt is the action taken by the agent at time step t, is some fixed baseline action in [K] and the maximum operator indicates the agent is competing with the best fixed baseline. The FTRL algorithm minimizes (17) by taking the action pt = argminp[K] Φt(p) at time step t, where Φt : [K] is defined as Φt(x) := Ft(x) + t1 (cid:88) s=1 xℓs and Ft : [K] R, N, are some convex functions. 9 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms In the following, we illustrate the connection between FTRL and Algorithm 5 (or Algorithm 10). We let ℓt = (cid:98)Zt := (cid:0) , Ft(x) = (x)/ηt with (x) : [K] and (x) := (cid:80)K i=1(xi log xi xi)+log +1. Furthermore, (cid:98)Z1,t . . . , (cid:98)ZK,t some calculation indicates the action taken by FTRL is exactly pt as in (15), i.e., (cid:1) pt = argmin p[K] Φt(p). Therefore, Algorithm 5 is indeed an instantiation of FTRL in terms of the algorithm design. Under our problem setup, the difference lies at the target of the algorithm. Instead of minimizing the corresponding regret max p[K] (cid:20) (cid:88) (pt p) (cid:98)Zt (cid:21) , t=1 we aim at minimizing the (loss-based) regret defined on two different time scales (cid:103)Reg(ALG) := (cid:20) ST(ALG) (cid:88) t=1 (cid:21) (cid:98)Zt min i[K] (cid:20) ST(ALGi) (cid:88) (cid:21) , (cid:98)Zt t=1 (18) where ei RK is an one-hot vector with the ith coordinate being 1, and the expectation is taken w.r.t. the internal randomness within ALG and (cid:98)Zt. We highlight again that ST(ALG) is random whereas ST(ALGi) is fixed under the greedy decoding strategy. Step 2: Transformation of the stopping time regret. The stopping time regret (5) and (cid:103)Reg(ALG) in (18) may look different at first sight. We demonstrate that these two notions of regret can be transformed into one another up to some constant factors. We firstly simplify (cid:103)Reg(ALG). Let zi,t = 1 (yi,t 1)/L. According to the definition of pt in (15) and (cid:98)Zt in (16), ST(ALG) (cid:88) (cid:98)Zt = ST(ALG) (cid:88) t=1 t=1 + 1 yi,t = + 1 ST(ALG) len(ptST(ALG)) . Furthermore, note that (cid:98)Zt is an unbiased estimator for zt and ST(ALGi) is fixed real number, ST(ALGi) (cid:88) (cid:104) (cid:98)Zt (cid:105) = ST(ALGi) (cid:88) zi,t = t=1 t= + 1 ST(ALGi) len(ptST(ALGi)) . Hence, (cid:103)Reg(ALG) is simplified as (cid:103)Reg(ALG) = (cid:20) + 1 + 1 = ST(ALG) len(ptST(ALG)) (cid:0)E[ST(ALG)] min ST(ALGi)(cid:1) i[K] (cid:21) min i[K] (cid:16) + 1 ST(ALGi) len(ptST(ALGi)) (cid:17) where we adopt the property of speculative decoding (2) in the last equality. This indicates that (cid:103)Reg(ALG) = + 1 Reg(ALG). (19) (20) Step 3: Regret decomposition. Based on the previous two steps, we now upper bound (cid:103)Reg(ALG). This notion of regret distinguishes from the standard regret analysis due to the difference in the time scales ST(ALG) and ST(ALGi). 10 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms For simplicity, we define the Bregman divergence induced by convex function : [K] R+ as Df (, ) : [K] [K] with Df (a, b) := (a) (b) (a b)f (b). Fix [K], we now decompose this empirical regret w.r.t. i. ST(ALG) (cid:88) t=1 (cid:98)Zt ST(ALGi) (cid:88) t=1 (cid:98)Zt. ST(ALG) (cid:88) (cid:16) t=1 ST(ALG) (cid:88) (cid:16) = = t=1 (pt pt+1) (cid:98)Zt ST(ALG) (cid:88) (cid:17) + t+1 (cid:98)Zt ST(ALGi) (cid:88) (cid:98)Zt t=1 t=1 (pt pt+1) (cid:98)Zt ST(ALG) (cid:88) (cid:16) (cid:17) + t=1 Φt+1(pt+1) Ft+1(pt+1) Φt(pt+1) + Ft(pt+1) (cid:17) (cid:16) ST(ALG) (cid:88) (cid:98)Zt ST(ALG) (cid:88) (cid:98)Zt + ST(ALGi) (cid:88) (cid:17) (cid:98)Zt t=1 t=1 t= ST(ALG) (cid:88) (cid:16) = t=1 (pt pt+1) (cid:98)Zt ST(ALG)1 (cid:88) (cid:16) (cid:17) + t=0 Φt+1(pt+1) Φt+1(pt+2) (cid:17) ST(ALG) (cid:88) (cid:16) + t=1 Ft(pt+1) Ft+1(pt+1) (cid:17) + FST(ALG)+1(ei) F1(p1) + ΦST(ALG)+1(pST(ALG)+1) ΦST(ALG)+1(ei) + (cid:16) ST(ALG) (cid:88) (cid:98)Zt ST(ALGi) (cid:88) (cid:17) (cid:98)Zt t=1 t=1 ST(ALG) (cid:88) t=1 (cid:124) (cid:18) (pt pt+1) (cid:98)Zt (cid:19) DF (pt+1, pt) ηt + FST(ALG)+1(ei) F1(p1) (cid:123)(cid:122) ((cid:50)) (cid:125) (cid:124) (cid:123)(cid:122) () (cid:125) ST(ALG) (cid:88) (cid:16) + t=1 (cid:124) Ft(pt+1) Ft+1(pt+1) (cid:17) + ΦST(ALG)+1(pST(ALG)+1) ΦST(ALGi)+1(ei) (cid:123)(cid:122) () (cid:125) (cid:124) (cid:123)(cid:122) () (cid:125) (cid:16) ST(ALG) (cid:88) + (cid:98)Zt ST(ALGi) (cid:88) (cid:98)Zt t=1 (cid:124) t=1 (cid:123)(cid:122) () (cid:17) (cid:125) (21) where the last inequality adopts the fact that DΦt(a, b) = DFt(a, b) = DF (a, b)/ηt and the inequality Φt(pt) Φt(pt+1) = DΦt(pt+1, pt) (pt+1 pt)Φt(pt) DΦt(pt+1, pt). Here (pt+1 pt)Φt(pt) 0 results from the choice of pt = argminp[K] condition. Φt(p) and the first-order optimization Step 4: Upper bound each term in the decomposed regret. In this step, we upper bound each term in (21). We comment that ((cid:50)) and () require us to attend to the randomness in ST(ALG) and the different time indeces. This issue will not be encountered in the conventional scenario where ST(ALG) = ST(ALGi). Upper bound ((cid:50)): we will show that E[((cid:50))] E[K (cid:80)ST(ALG) ηt/2] almost surely. t=1 Recall the definition of (cid:98)Zt in (6), (cid:98)Zt only has positive value at the It coordinate. We divide the problem into two cases: (1) pt,It pt+1,It < 0, and (2) pt,It pt+1,It 0. 11 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Case (1): pt,It pt+1,It < 0. Because the Bregman divergence is always non-negative, hence, ((cid:50)) (pt,It pt+1,It) (cid:98)ZIt,t 0 0 ηt 2pt,It . Case (2): pt,It pt+1,It 0. Note that (x) is Legendre function on [K]. By invoking Lemma F.8, (pt pt+1) (cid:98)Zt DF (pt+1, pt) ηt ηt 2 (cid:98)Zt2 H1 , where Ht = 2F (qt) and qt = α pt + (1 α) pt+1 for some α [0, 1]. Furthermore, 2F (qt) is diagonal matrix with (cid:0)2F (qt)(cid:1) i,i = 1/qt,i, [K]. Therefore, ηt 2 (cid:98)Zt2 H1 = ηt 2 z2 It,t p2 t,It qt,It ηt 2 1 p2 t,It pt,It = ηt 2pt,It . To conclude the two cases, it holds almost surely that ((cid:50)) ST(ALG) (cid:88) t=1 ηt 2pt,It . Lastly, by adopting Lemma D.1 which is proved in Appendix E.1, we obtain E[((cid:50))] 2 (cid:20) ST(ALG) (cid:88) (cid:21) . ηt t= Lemma D.1. Under Assumption 4.2, consider the learning rates ηt defined in Algorithm 5 (or Algorithm 10), it holds that (cid:20) ST(ALG) (cid:88) t=1 (cid:21) ηt pt,It (cid:20) ST(ALG) (cid:88) = (cid:21) . ηt t=1 Upper bound (): Because (x) is non-negative on [K] and (ei) = log K, hence, () (ei) ηST(ALG+1) = log ηST(ALG)+1 = log ηST(ALG) . where we manually set ηST(ALG)+1 = ηST(ALG). Upper bound (): Recall that (x) = (cid:80)K Additionally, ηt = (cid:112)log K/(K t), [ST(ALG)] is decreasing sequence. Therefore, i=1(xi log xi xi) + log + 1, so (x) is non-negative for any [K]. () = ST(ALG) (cid:88) t= (cid:16) (pt+1) ηt (cid:17) (pt+1) ηt+1 0, a.s. Upper bound (): Since pST(ALG)+1 is the minimizer of ΦST(ALG)+1(p), we have () 0. Upper bound (): Under Assumption 5.2, ST(ALG) ST(ALG1) almost surely. We further prove that E[()] E[ST(ALG)] ST(ALGi) for = i, which we summarize in the following lemma with proof postponed to Appendix E.1. Lemma D.2. Under Assumption 4.2 and 5.2, consider (cid:98)Zt defined in Algorithm 5, it holds that (cid:20) ST(ALG) (cid:88) (cid:21) i (cid:98)Zt t=ST(ALGi )+1 E[ST(ALG)] ST(ALGi ). 12 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Step 5: Conclusion of the stopping time regret. Aggregating the upper bounds for each terms in (21) and taking expectation, (cid:103)Reg(ALG) 2 (cid:20) ST(ALG) (cid:88) (cid:21) ηt t=1 + (cid:20) log ηST(ALG) (cid:21) + E[ST(ALG)] ST(ALGi ). By substituting the learning rate values into the equation, (cid:104)(cid:112)ST(ALG) log (cid:103)Reg(ALG) 2 (cid:113) (cid:105) + E[ST(ALG)] ST(ALGi ) 2 E(cid:2)ST(ALG)(cid:3) log + E[ST(ALG)] ST(ALGi ) (cid:113) 2 len(ptτc) log + E[ST(ALG)] ST(ALGi ). where the second inequality adopts Jensens inequality and the last equality holds due to ST(ALG) len(ptτc) almost surely as in (3). According to the regret transformation in (20), Reg(ALG) 2L (cid:113) E(cid:2)ST(ALG)(cid:3) log 2L (cid:113) len(ptτc ) log K. (22) Furthermore, by solving the quadratic function in terms of Reg(ALG), i.e., Reg(ALG) 2L (cid:113)(cid:0)Reg(ALG) + ST(ALGi )(cid:1) log K, we obtain Reg(ALG) 4L2 log + 2L (cid:114) min i[K] ST(ALGi) log K. (23) Aggregating (22) and (23) concludes the proof of this theorem. Remark D.3 (Sampling Decoding under Adversarial Mean Values). Since the tokens can be regarded as fixed given the initial prompt and the hyperparameter configurations under the greedy decoding strategy, we consider the greedy decoding strategy under the adversarial mean values assumption (Assumption 5.1). If one wishes to consider the sampling decoding strategy, the proof of Theorem 5.3 can be adapted to it. Specifically, this switch of decoding strategy mainly influences (19), the proofs of Lemma D.1 and Lemma D.2. We can depend on Doobs Optional Stopping Theorem (Lemma F.1) to solve this problem, just like what we have done to prove (28) and replacing the condition (1) therein by E[ST(ALG)] E[len(ptτc )] < . The rest of the proof can go through in similar manner. In the end, we can arrive at similar result, i.e., Reg(ALG, pt, ν) 2L min 2LK log + (cid:26) (cid:114) min i[K] E[ST(ALGi)]K log K, (cid:113) E[len(ptτc)]K log (cid:27) . D.4. Proof of Theorem 4.4 Under the greedy decoding strategy, the problem is alleviated in two aspects. Firstly, given the target model and the initial prompt pt, the total length len(ptτc) is (potentially) determined. While the total length is determined, it is worth noting that the number of accepted tokens at each round (Line 5 in Algorithm 3) is still random. Additionally, under the dynamics presented in Algorithm 3 and given the history Ht, there is one-to-one mapping between the accepted tokens XIt,t and its length YIt,t. Since the lower bound is established in terms of class of algorithms over set of initial prompts, we adopt initial to denote the set of initial prompts and adopt Sall to denote the set of all hyperparameter specifications that can be selected to constitute S. To further ease the problem, we augment Assumption 4.1. Assumption D.4. We assume that Given any bandit configuration ν = (P, = {Si}i[K], L) and initial prompt pt initial, conditional on the history Ht1 and the selected arm It at round t, the distribution of the length of the accepted tokens P( pt, Ht1, It = i) = PSi( ), [K]. For any two hyperparameter specifications S, Sall, we have KL(PS, PS) < . 13 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms We consider the class of arm selection algorithms which are non-anticipatory and consistent. Definition D.5 (Non-anticipartory Algorithm). An arm selection algorithm ALG is non-anticipatory if ALG( Ht) σ(Ht), N. Definition D.6 (Consistent Algoirthm). An arm selection algorithm ALG is consistent over class of bandit configurations Λ and prompt set ) , , and for all (0, 1], initial if for all ν Λ and any sequence of initial prompts (ptm) inital with len(ptm τc m=1 lim Reg(ALG, ptm, ν) len(ptm τc )a = 0. Theorem 4.4 (Lower Bound). Given any sequence of initial prompts (ptm) ) , and bandit configuration ν = (P, = {Si}i[K], L), under Assumption D.4, the greedy decoding strategy and the dynamics represented in Algorithm 3, for any non-anticipatory and consistent arm selection algorithm ALG, the expected regret satisfies init with len(ptm τc m=1 lim inf Reg(ALG, pt, ν) log(len(ptm )) τc (cid:88) i=i µi 1 kli , where kli := inf SS {KL(PSi, PS) : EXPS [X] > µi }. Proof of Theorem 4.4. The proof consists of three steps: Divergence decomposition: similar to the reward decomposition in the upper bound proof, the divergence decomposition cannot be done as the time horizon ST(ALG) is random stopping time. We tackle this problem in the first step. Lower bound on Eν[ni,ST(ALG)]: we adapt the standard trick to lower bound the expected number of times arm has been chosen. Conclusion of the proof. Step 1: Divergence decomposition. The divergence decomposition suffers from the same issue as the reward decomposition, i.e., the stopping time ST(ALG) depends on the history. We adopt the same trick as in the reward decomposition step to overcome this issue and the result is summarized in Lemma D.7 whose proof is postponed to App. E. Lemma D.7. Under Assumption D.4, given two bandit configurations ν = (P, = {Si}K {S i=1, L) which only differ in the hyperparameter specifications, for any pt i}K initial and algorithm ALG, i=1, L) and ν = (P, = KL(PALG,pt,ν, PALG,pt,ν) = (cid:88) i= EALG,pt,ν[ni,ST(ALG)]KL(PSi, PS ) where PALG,pt,ν (resp. PALG,pt,ν) is the probability measure induced by (ALG, pt, ν) (resp. (ALG, pt, ν)) defined on the σ-algebra {σ(Ht)} t=1. Step 2: Establishment for the lower bound of Eν[ni,ST(ALG)]. Given algorithm ALG, bandit configuration ν Λ, prompt pt (ptm) j=1, L) for = with m=1 and any ε > 0, construct 1 alternative bandit configurations νi = (P, Si = {Si,j}K Si,j = Sj 1{j = i} + 1{j = i}, in νi satisfies that its mean µS where ) kli + ε. In other words, under bandit configuration νi, only Si changes into and arm becomes the best arm. As the bandit configurations only differ in the hyperparameter selection, we adopt the shorthand notation Pν, Eν and Reg(ν) for PALG,pt,ν, EALG,pt,ν and Reg(ALG, pt, ν) respectively when there is no risk of confusion. > µi with KL(PSi, PS 14 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms According to Lemma D.7, for any pt (ptm) m=1, KL(Pν, Pνi ) = Eν[ni,ST(ALG)] KL(PSi, PS ) Eν[ni,ST(ALG)] (kli + ε). By Lemma F.6, with Ai = {ni,ST(ALG) > ST(ALG)/2}, Pν[Ai] + Pνi[Ac ] exp (cid:0) Eν[ni,ST(ALG)] KL(PSi, PS )(cid:1) exp (cid:0) Eν[ni,ST(ALG)] (kli + ε)(cid:1) 1 2 1 Thus, by adopting (14), the expected reward under ν can be lower bounded as Reg(ν) µi µi Eν[ni,ST(ALG)] µi Eν[ni,ST(ALG) Ai] Pν[Ai] 1 2 Eν[ST(ALG) Ai] Pν[Ai] µi len(ptτc ) 2(L + 1) Pν[Ai]. Similarly, under the alternative bandit configuration νi, Reg(νi) µS µi µS Eνi[ni,ST(ALG)] Combining (24), (25) and (26), Reg(ν) + Reg(νi) µS µi µS len(ptτc) 2(L + 1) Pνi[Ac ]. (24) (25) (26) min min (cid:110) µi (cid:110) µi , , µS µS i µi µS µi µS (cid:111) (cid:111) len(ptτc) 2(L + 1) len(ptτc) 4(L + 1) (Pν[Ai] + Pνi [Ac ]) exp (cid:0) Eν[ni,ST(ALG)] (kli + ε)(cid:1) which holds for any pt (ptm) m=1. Rearranging the terms, we have lim inf Eν[ni,ST(ALG)] log(len(ptm )) τc + lim inf = 1 kli + ε 1 kli + ε log (cid:0) min (cid:8) µi , µS µi µS (cid:9)(cid:1) log (cid:0)4(L + 1)(cid:1) log(Reg(ν) + Reg(νi)) (kli + ε) log(len(ptm τc )) where the last equality follows from the definition of consistent algorithm. Because ε > 0 is arbitrarily chosen, by sending ε 0, we obtain the lower bound for Eν[ni,ST(ALG)] lim inf Eν[ni,ST(ALG)] log(len(ptm )) τc 1 kli . (27) Step 3: Conclusion of the proof. Aggregating (14) and (27), lim inf Reg(ALG, ptm, ν) log(len(ptm τc )) (cid:88) i=i µi 1 kli . This concludes the proof. 15 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms E. Supporting Propositions E.1. Supporting Lemmas for Theorem 5.3 Lemma D.1. Under Assumption 4.2, consider the learning rates ηt defined in Algorithm 5 (or Algorithm 10), it holds that (cid:20) ST(ALG) (cid:88) t=1 (cid:21) ηt pt,It (cid:20) ST(ALG) (cid:88) = (cid:21) . ηt t=1 Proof of Lemma D.1. Similar to the proof of Lemma D.2, we adopt Doobs Optional Stopping Theorem (Lemma F.1) to show (cid:20) ST(ALG) (cid:88) t=1 ηt pt,It (cid:21) ηt = 0. (28) According to condition (b) in Lemma F.1, we only need to show that (1) E[ST(ALG)] < , and (2) there exists such that for all < ST(ALG), E(cid:2)(cid:12) (cid:12)ηt/pt,It ηt(cid:12) (cid:3) < c. (cid:12)Ht1 Condition (1): Given Assumption 4.2, it holds that len(ptτc) < under the greedy decoding strategy. Therefore, we have E[ST(ALG)] len(ptτc) < . Condition (2): Note that (cid:104)(cid:12) (cid:12) ηt pt,It ηt (cid:12) (cid:12) (cid:105) (cid:12) (cid:12) (cid:12) Ht (cid:105) (cid:104) ηt pt,It (cid:12) (cid:12) (cid:12) Ht1 2K ηt 2(cid:112)K log K. + ηt (cid:104) (cid:88) i=1 ηt pt,i 1{It = i} (cid:105) (cid:12) (cid:12) (cid:12) Ht1 + ηt Therefore, Condition (2) is satisfied and (28) is established. In addition, by using Assumption 4.2, (cid:20)(cid:12) (cid:12) (cid:12) ST(ALG) (cid:88) t= ηt (cid:21) (cid:12) (cid:12) (cid:12) E(cid:2)ST(ALG)(cid:3)(cid:112)K log < . So E(cid:2) (cid:80)ST(ALG) t= ηt (cid:3) exists. Lastly, (cid:20) ST(ALG) (cid:88) t=1 (cid:21) ηt pt,It (cid:20) ST(ALG) (cid:88) = t=1 ηt pt,It ηt (cid:21) (cid:20) ST(ALG) (cid:88) + (cid:21) ηt t= which indicates E(cid:2) (cid:80)ST(ALG) In conclusion, by adding E(cid:2) (cid:80)ST(ALG) ηt/pt,It (cid:3) exists. t=1 t=1 (cid:3) on both sides of (28), the desired result is obtained. ηt Lemma D.2. Under Assumption 4.2 and 5.2, consider (cid:98)Zt defined in Algorithm 5, it holds that (cid:20) ST(ALG) (cid:88) (cid:21) i (cid:98)Zt t=ST(ALGi )+1 E[ST(ALG)] ST(ALGi ). Proof of Lemma D.2. If the two expectations below exist and (cid:20) ST(ALG) (cid:88) (cid:21) (cid:98)Zt (cid:20) ST(ALG) (cid:88) = (cid:21) , zi,t t=1 t=1 16 (29) then it holds that BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms (cid:20) ST(ALG) (cid:88) (cid:21) (cid:98)Zt (cid:20) ST(ALGi) (cid:88) (cid:21) (cid:98)Zt (cid:20) ST(ALG) (cid:88) = (cid:21) zi,t ST(ALGi) (cid:88) zi,t t=1 (cid:20) t= t=1 t=1 ST(ALG) (cid:88) (cid:21) zi,t t=ST(ALGi)+ E[ST(ALG)] ST(ALGi). The desired result can be obtained. Therefore, we aim to prove (29). Since both ST(ALG) and (cid:98)Zt are random, the obstacle is that we cannot directly take expectation of the summand. We will firstly prove (cid:20) ST(ALG) (cid:88) t= (cid:98)Zt zi,t (cid:21) = 0 (30) by Doobs Optional Stopping Theorem (Lemma F.1). According to condition (b) in Lemma F.1, we only need to show that (1) E[ST(ALG)] < , and (2) there exists such that for all < ST(ALG), E(cid:2)e (cid:3) < c. ( (cid:98)Zt zt)(cid:12) (cid:12)Ht1 Condition (1) holds as shown in the proof Lemma D.1. We only need to show Condition (2). Note that E[ (cid:98)Z] = zt, E(cid:2)e ( (cid:98)Zt zt) (cid:12) (cid:12) Ht (cid:104) E(cid:2)e (cid:3) (cid:98)Zt Ht1, It (cid:3) (cid:12) (cid:12) (cid:12) Ht1 (cid:105) + zt = 2zi,t 2. Therefore, (cid:80)ST(ALG) t=1 (cid:98)Zt zi,t is well-defined and (30) is established. We then prove the two expectations exist. Note that all involved variables are positive, we only need to show t=1 This can be obtained by noticing that zi,t [0, 1] for N, (cid:20) ST(ALG) (cid:88) (cid:21) zi,t < . (cid:20) ST(ALG) (cid:88) (cid:21) zi,t t=1 E(cid:2)ST(ALG)(cid:3) < . Combining the above with Condition 2, we have (cid:20) ST(ALG) (cid:88) (cid:21) (cid:98)Zt (cid:20) ST(ALG) (cid:88) = t= t=1 (cid:98)Zt pzt (cid:21) (cid:20) ST(ALG) (cid:88) + (cid:21) zt t=1 which means E(cid:2) (cid:80)ST(ALG) (cid:3) exists. In conclusion, by adding E(cid:2) (cid:80)ST(ALG) (cid:98)Zt t=1 t= zi,t (cid:3) on both sides of (30), (cid:20) ST(ALG) (cid:88) (cid:21) (cid:98)Zt (cid:20) ST(ALG) (cid:88) = (cid:21) . zi,t t=1 t= This finishes the proof of (29). E.2. Proof of Lemma D.7 Lemma D.7. Under Assumption D.4, given two bandit configurations ν = (P, = {Si}K {S i=1, L) which only differ in the hyperparameter specifications, for any pt i}K initial and algorithm ALG, i=1, L) and ν = (P, = KL(PALG,pt,ν, PALG,pt,ν) = (cid:88) i=1 EALG,pt,ν[ni,ST(ALG)]KL(PSi, PS ) 17 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms where PALG,pt,ν (resp. PALG,pt,ν) is the probability measure induced by (ALG, pt, ν) (resp. (ALG, pt, ν)) defined on the σ-algebra {σ(Ht)} t=1. Proof. As the two bandit configurations only differ in the hyperparameter specifications, we adopt the abbreviated notation Pν and Pν for the induced probability PALG,pt,ν and PALG,pt,ν, respectively. We use PALG() to denote the output distribution of the arm selection algorithm ALG in Line 4 in Algorithm 3. With the bandit configuration ν = (P, S, L), the probability of HST(ALG) is Pν(HST(ALG)) = ST(ALG) (cid:89) t=1 PALG(It Ht1, pt)P(YIt,t Ht1, pt, It) = ST(ALG) (cid:89) t=1 PALG(It Ht1, pt)PSIt (YIt,t). Similarly, under the bandit configuration ν = (P, , L), Therefore, it holds that Pν(HST(ALG)) = ST(ALG) (cid:89) t=1 PALG(It Ht1, pt)PS It (YIt,t). log Pν(HST(ALG)) Pν(HST(ALG)) = ST(ALG) (cid:88) log t=1 PSIt PS It (YIt,t) (YIt,t) . Because KL(PSi, PS ) < , [K] under Assumption D.4, the divergence between Pν and Pν can be rewritten as KL(Pν, Pν) = Eν (cid:20) log Pν(HST(ALG)) Pν(HST(ALG)) (cid:21) (cid:20) ST(ALG) (cid:88) log = Eν t=1 (cid:21) PSIt PS It (YIt,t) (YIt,t) < . We then prove that (cid:20) ST(ALG) (cid:88) Eν log t=1 PSIt PS It (YIt,t) (YIt,t) (cid:21) = (cid:88) i=1 Eν[ni,ST(ALG)]KL(PSi, PS ). The proof is composed by two arguments: Argument 1: Argument 2: (cid:20) ST(ALG) (cid:88) (cid:16) Eν t=1 log PSIt PS It (YIt,t) (YIt,t) KL(PSIt , PS It ) (cid:17)(cid:21) = 0. (cid:20) ST(ALG) (cid:88) Eν t=1 (cid:21) KL(PSIt , PS It ) = (cid:88) i=1 Eν[ni,ST(ALG)]KL(PSi, PS ). If the two arguments are true, by summing up (32) and (33), we can obtain the desired result (31). We prove the two above Arguments. Argument 1. Let Mn := (cid:88) t=1 log PSIt PS It (XIt,t) (XIt,t) KL(PSIt , PS It ), = 1, 2, . . . (31) (32) (33) BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms and M0 := 0. We firstly prove that (Mn) (1) Eν[Mn] < . According to Assumption D.4, for any [K], KL(PSi , PS such that n=0 is martingale w.r.t. (Hn) n=0: (1) Eν[Mn] < , and (2) Eν (cid:2)Mn+1 Hn ) < , this indicates there exists (cid:3) = Mn. L+1 (cid:88) x= (cid:12) (cid:12) (cid:12) log PSi(x) PSi(x) (x) PS (cid:12) (cid:12) (cid:12) < < , [K]."
        },
        {
            "title": "This indicates",
            "content": "Eν[Mn] (cid:20)(cid:12) (cid:12) (cid:12) log Eν (cid:88) t=1 PSIt PS It (XIt,t) (XIt,t) (cid:12) (cid:12) (cid:12) + (cid:12) (cid:12) (cid:12)KL(PSIt , PS It ) (cid:21) (cid:12) (cid:12) (cid:12) < . (2) Eν (cid:2)Mn+1 Hn (cid:3) = Mn. By adopting the tower property. Eν (cid:2)Mn+1 Hn (cid:3) = Mn + Eν (cid:20) Eν (cid:104) log PSIn+1 PS In+1 (YIn+1,n+1) (YIn+1,n+1) KL(PSIn+1 , PS In+1 (cid:12) (cid:12) (cid:12) Hn, In+ (cid:105) (cid:12) (cid:12) (cid:12) Hn ) (cid:21) = Mn. (34) (35) (36) From (35) and (36), (Mn)nN is martingale w.r.t. (Hn)nN. Additionally, we will adopt Doobs Optional Stopping Theorem (Lemma F.1) on MST(ALG). The prerequisites are verified as follows: (1) E[ST(ALG)] < : By Assumption 4.2, ST(ALG) is stopping time w.r.t. (Hn)nN with E[ST(ALG)] len(ptτc < . (2) there exists R, such that E[Mn+1 Mn Hn] for any ST(ALG): According to (34), E[Mn+1 Mn Hn] Eν (cid:20)(cid:12) (cid:12) (cid:12) log PSIt PS It (XIt,t) (XIt,t) (cid:12) (cid:12) (cid:12) (cid:21) (cid:12) (cid:12) (cid:12) Hn + Eν (cid:20) KL(PSIt , PS It ) (cid:21) (cid:12) (cid:12) (cid:12) 2c < . Taking = 2c finishes the verification. Therefore, the prerequisites in Lemma F.1 (b) are satisfied. By invoking Lemma F.1, (32) is established. Argument 2. Firstly, by (34), (cid:20)(cid:12) (cid:12) (cid:12) Eν ST(ALG) (cid:88) t= KL(PSIt , PS It ) (cid:21) (cid:12) (cid:12) (cid:12) Eν (cid:2)ST(ALG)(cid:3) < . So (cid:80)ST(ALG) t=1 KL(PSIt , PS It ) has finite expectation. Furthermore, it can be observed that (cid:20) ST(ALG) (cid:88) Eν t=1 KL(PSIt , PS It ) (cid:21) = Eν (cid:20) (cid:88) ST(ALG) (cid:88) i=1 t=1 1{It = i}KL(PSi, PS (cid:21) ) = (cid:88) i=1 Eν[ni,ST(ALG)]KL(PSi, PS ). Therefore, (33) is proved. This concludes the proof of this divergence decomposition lemma. 19 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms E.3. Proof of Proposition 4.5 Proposition 4.5 (Tightness Result). Let STGD = {S : PS satisfies (4)}. Let {Si}K (Line 5 in Algorithm 2), then i=1 STGD and Si satisfies (4) with pi lim inf Reg(ALG, ptm, ν) log(len(ptm τc )) H(pt, ν) pi (1 pL ) (1 pi ) . Therefore, the upper and lower bound match up absolute constants and L2(1pi ) pi (1pL they match up to absolute constants and L. ) factor. In particular, if pi (cid:0)21/L, 1(cid:1), Proof of Proposition 4.5. Given any with parameter p, µS = L+1 (cid:88) x=1 PS(x) = (cid:88) x= px1(1 p) + (L + 1) pL = 1 pL+1 1 . Note that if µS µi, we have > pi. Therefore, µS µi = 1 pL+1 1 = 1 pL+1 1 pi pL+1) (p pi) + (pL+1 (1 p)(1 pi) (1 p)(1 pi) (p pi)(1 pL) (1 p)(1 pi) . (p pi) + (pL+1 pL+1 + pipL+1 ppL+1 ) (37) In addition, for any [K], KL(PSi , PS) = L+1 (cid:88) x=1 PSi(x) log PSi(x) PS(x) = pi pL+1 1 pi log pi + (1 pL ) log 1 pi 1 . By utilizing log 1 for > 0, KL(PSi, PS) pi pL+1 1 pi pi + (1 pL ) pi 1 = (1 pL )(pi p)2 p(1 pi)(1 p) (38) Combining (38) and (37), KL(PSi, PS) (µS µi)2(1 pi)(1 p)(1 pL ) p(1 pL) (µS µi)2 p(1 pL)/(1 p) . According to the definition of kli in Theorem 4.4, kli (µi µi) pi (1 pL )/(1 pi ) = pi (1 pL 2 )/(1 pi ) . Thus, the regret is lower bounded by lim inf Reg(ALG, ptm, ν) log(len(ptm τc )) (cid:88) i=i 1 µi pi (1 pL ) (1 pi ) . Furthermore, if pi (cid:0)21/L, 1(cid:1), lim inf Reg(ALG, ptm, ν) log(len(ptm τc )) (cid:88) i=i L/2 µi . 20 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms F. Supporting Lemmas Lemma F.1 (Doobs optional stopping, Theorem 3.8 in Lattimore & Szepesvari (2020)). Let = (Ft)tN be filtration and (Xt)tN be an F-adapted martingale and τ an F-stopping time such that at least one of the following holds: (a) There exists an such that P[τ > n] = 0. (b) E[τ ] < , and there exits constant such that for all N, E[Xt+1 Xt Ft] almost surely on the event that τ > t. (c) There exists constant such that Xtτ almost surely for all N. Then Xτ is almost surely well-defined, and E[Xτ ] = E[X0]. Furthermore, when (Xt) is super/sub-martingale rather than martingale, then equality is replaced with less/greater-than, respectively. Lemma F.2 (Chernoff-Hoeffding bound, Fact 1 in Auer et al. (2002b)). Let X1, . . . , Xn be random variables with common range [0, 1] and E[Xn X1, . . . , Xn1] = µ. Let Sn = X1 + . . . + Xn. Then for all 0, (cid:18) (cid:18) (cid:19) (cid:19) P[Sn nµ + a] exp and P[Sn nµ a] exp 2a2 2a2 Lemma F.3 (Confidence Intervals, Lemma 6 in Abbasi-yadkori et al. (2011)). Assuming that the noise ηt is conditionally 1-sub-Gaussian. With probability at least 1 δ, {1, 2, . . . , K}, 0, ˆµi,t µi (cid:115) (1 + ni,t) n2 i,t (cid:18) 1 + 2 log (cid:18) K(1 + ni,t)1/2 δ (cid:19)(cid:19) . Lemma F.4 (Lemma 8 in Antos et al. (2010)). Let > 0. For any (2/a)[log(1/a) b]+, at + > log t. Lemma F.5 (Exercise 3.7 in Lattimore & Szepesvari (2020)). Let = (Ft)tN be filtration, and τ be stopping time with respect to F. Then Fτ is σ-algebra. Lemma F.6 (Bretagnolle-Huber inequality, Theorem 14.2 in Lattimore & Szepesvari (2020)). Let and be probability measures on the same measurable space (Ω, F), and let be an arbitrary event. Then (A) + Q(Ac) exp (cid:0)KL(P, Q)(cid:1), 1 2 where Ac = Ω is the complement of A. Lemma F.7 (Pinskers inequality, Equation (14.12) in Lattimore & Szepesvari (2020)). For measures and on the same probability space (Ω, F) that dT (P, Q) (cid:114) 1 2 KL(P, Q). Lemma F.8 (Theorem 26.13 in Lattimore & Szepesvari (2020)). Let η > 0 and be Legendre and twice differentiable with positive definite Hessian in = int(dom(f )). Then for all x, A, there exists [x, y] = {(1α)x+αy : α [0, 1]} such that y, Df (x, y) η η 2 u2 2f (z)1 . G. Additional Experimental Results G.1. Additional Experimental Details For the experiments stated in Section 6.1, we report the memory utilization in this section. As Ealge-2 (Li et al., 2024b) is one of the best speculative decoding methods, we adopt it as the baseline. The Normalized Memory (NM) and Normalized Memory Bandwidth (NMB) are presented in Table G.1. The result shows that the proposed methods do not incur additional memory consumption compared to the baseline method. We further remark that this result is achieved by our superior algorithm design, where several non-parametric models (PLD, REST, Suffix Tree) to enhance parametric SOTA model (Eagle-2). Specifically, 21 BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Table 2. The memory and memory bandwidth utilized by our method. As Eagle-2 is one of the best SD methods, we adopt it as the baseline to normalize the results of other methods. NM=Normalized Memory and NMB=Normalized Memory Bandwidth. Methods Spec Bench Alpaca Code Editor Debug Bench NM"
        },
        {
            "title": "NMB",
            "content": "NM"
        },
        {
            "title": "NMB",
            "content": "NM"
        },
        {
            "title": "NMB",
            "content": "NM"
        },
        {
            "title": "NMB",
            "content": "LLaMA3-8B-Instruct Eagle-2 EXP3SPEC UCBSPEC 1.0000 0.9981 1.0059 1.0000 1.0171 1.0093 1.0000 0.9950 1.0130 1.0000 1.0170 1. 1.0000 1.0200 0.9990 1.0000 0.9980 0.9820 1.0000 1.0100 1.0150 1.0000 0.9960 1.0020 Qwen2-7B-Instruct Eagle-2 EXP3SPEC UCBSPEC 1.0000 1.0043 0.9929 1.0000 1.0095 1.0036 1.0000 1.0050 1.0080 1.0000 0.9980 0.9900 1.0000 1.0400 1.0270 1.0000 0.9850 0. 1.0000 0.9890 1.0320 1.0000 0.9960 0.9950 Non-parametric means that these methods do not have any parameters in GPU, and directly predict the future tokens based on the past tokens according to the data structures like Trie Tree, which are python objects and stored in CPU RAM. All these show that the storage of the draft models will not increase the GPU memory. Our model only requires approximately an additional 100MB of CPU RAM. Since CPU memory is typically much larger (1TB in our server) and cheaper than GPU memory (40 GB in our server), this cost is negligible. All the draft models share the same verifier model, which is the target model (LLaMA3-8B-Instruct (Dubey et al., 2024) and Qwen2-7B-Instruct (Yang et al., 2024) in our experiments). So that the storage of the verifier does not increase the GPU memory. The reduction in memory usage comes from the fact that non-parametric models require fewer verification tokens (e.g., 40 for Suffix Tree) compared to the baseline Eagle-2 (e.g., 64). As result, when invoking these models, slight decrease in activation memory usage may be observed. Additionally, slight differences in GPU memory may be observed, arising randomly from the short-lived activation tensors rather than from the method itself. We note that the size of SpecBench is not large enough, i.e., the number of arms pulls is not large, to derive statistically sound result. We enable Mixture-of-Agent (Wang et al., 2024) on the prompts whose responses are shorter than 100 tokens to increase the number of arm pulls. G.2. Experiments on Larger Models In addition to the two models in the main paper, we conduct an addtional set of experiment on larger target model, namely LLaMA-2-13B (Touvron et al., 2023). As Table 1 indicates Eagle-2 (Li et al., 2024b) is one of the best speculative decoding methods, we adopt it as the baseline. The other setups are the same as the ones in Section 6.1. From the result reported in Table G.2, the proposed methods, UCBSPEC and EXP3SPEC, demonstrate their efficacy on larger models. G.3. Experiments on Different Hardwares In the main paper, the experiments are conducted on single A100 GPU. In this section, we conduct an additional set of experiment on GeForce RTX 4090. We adopt Eagle-2 (Li et al., 2024b) as the baseline and Spec Bench (Xia et al., 2024) as the benchmark. The result is presented in Table G.3. We observe similar trend as the result presented in Table 1. The proposed method remains useful with different hardware setup. BANDITSPEC: Adaptive Speculative Decoding via Bandit Algorithms Table 3. Empirical Comparison between the proposed algorithms and Eagle-2 (Li et al., 2024b) with LLaMA-2-13B as the target model, measured by Mean Accepted Tokens (MAT) () and Tokens/s (). The best result is highlighted in bold, while the second best result is underlined. The proposed algorithms remain effective on larger models. Methods Spec Bench Alpaca Code Editor Debug Bench MAT() Tokens/s() MAT() Tokens/s() MAT() Tokens/s() MAT() Tokens/s() LLaMA-2-13B Eagle-2 EXP3SPEC UCBSPEC 4.35 4.05 4.43 91.94 95.52 97. 4.32 4.32 4.36 96.59 99.64 102.29 5.19 5.22 5.27 107.57 115.65 113.97 5.16 5.03 5.27 108.45 116.65 118. Table 4. Empirical comparison between Eagle-2 and the proposed algorithms on GeForce RTX 4090. We observe similar trend as the result presented in Table 1. Methods Spec Bench MAT Tokens/s LLaMA3-8B-Instruct Eagle-2 EXP3SPEC UCBSPEC 4.14 3.95 4.16 Qwen2-7B-Instruct Eagle-2 EXP3SPEC UCBSPEC 3.65 3.96 4.17 97.01 102.24 107.38 94.16 111.74 112."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab",
        "Singapore Management University",
        "Yale University"
    ]
}