{
    "paper_title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data",
    "authors": [
        "Zhong Guan",
        "Likang Wu",
        "Hongke Zhao",
        "Ming He",
        "Jianpin Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}"
        },
        {
            "title": "Start",
            "content": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Zhong Guan * 1 2 Likang Wu * 1 2 3 Hongke Zhao 1 2 3 Ming He 4 Jianping Fan 4 5 2 0 2 4 ] . [ 1 0 3 1 2 0 . 5 0 5 2 : r Abstract Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises question: Does attention fail for graphs in natural language settings? Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: LLM4Exploration *Equal contribution 1 College of Management and Economics, Tianjin University, Tianjin, China 2 Laboratory of Computation and Analytics of Complex Management Systems ,Tianjin University,Tianjin, China 3ai-deepcube 4AI Lab at Lenovo Research, Beijing, China. Correspondence to: Hongke Zhao <hongke@tju.edu.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 1. Introduction LLMs have garnered significant attention, achieving remarkable success in language processing and demonstrating effective transferability to various other domains (Goyal et al., 2024; Ouyang et al., 2022; Pi et al., 2024; Singh et al., 2023; Wu et al., 2024; Zhao et al., 2024; Wu et al., 2023). This trend has inspired the graph machine learning community to delve into the application of LLMs within their field (He et al., 2024; Chen et al., 2024b; Huang et al., 2024; Kong et al., 2024; He & Hooi, 2024; Liu et al., 2025; Guan et al., 2024). However, recent studies reveal that existing LLMs for graphs fail to deliver satisfactory performance on graphstructured data, pointing to undiscovered challenges that hinder the deployment of LLMs in this context (Luo et al., 2024). Attention mechanisms are critical component of LLMs success, effectively linking tokens to enable models to comprehend complex contexts and domain-specific knowledge (Xiao et al., 2024; Hsieh et al., 2024; Yu et al., 2024). Despite the vast potential of attention mechanisms, research into their application on graph-structured data remains largely unexplored, lacking systematic analysis. Therefore, we embarked on an investigation from the perspective of attention analysis, aiming to uncover unique phenomena of LLMs attention on graph data structures and to validate our hypotheses regarding LLMs behavior on such data. Our work seeks to fill this research gap and establish clear direction for future studies in the field. In this paper, we conducted our study based on the following hypotheses, and uncovered new issues and phenomena. Q1: Do the attention distribution of LLMs change before and after training with finetuning ? Can LLMs correctly utilize graph structures? We first compared the distribution curves of attention scores for node tokens and text tokens before and after LLMs training. We then conducted hypothesis testing to clarify the following points: whether the attention on node tokens has shifted; whether the attention on text tokens has shifted; and whether the attention distributions between node tokens and text tokens are consistent. Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data The results showed that after training, the LLMs attention towards node tokens indeed underwent significant shift. This suggests that the LLMs have developed an initial capability to recognize graph-structured data. Simultaneously, our hypothesis testing revealed that the attention distribution of LLMs within nodes exhibits extreme tendency. However, in subsequent experiments where we disrupted the connectivity information, we found that even when the topological connection information was randomly shuffled, it had almost no effect on the LLMs performance. This indicates that the LLMs did not effectively utilize the correct connectivity information. Q2: Can LLMs allocate attention to different types of graph nodes in manner consistent with the structural properties of the graph? When processing graph data inputs, LLMs calculate attention scores between node tokens to weigh the importance of different nodes relative to each other. Through our visualization experiments, we found that the attention scores between different node tokens in LLMs do not adequately match the graph structure. Specifically, under sequential conditions, the attention distribution of node tokens exhibits U-shaped or long tail, which deviated from our idealized assumptions. Ideally, the model should focus more on central nodes and allocate attention in hierarchical, diminishing manner. Meanwhile, our analysis revealed that the attention paid by text tokens to node tokens more closely matches our ideal expectations. This indicates that the current limitation in LLMs lies not in the interaction between text and nodes but in the modeling of connections between nodes. Q3: Which is more suitable for LLMs graph-structured tasks: the fully connected perspective of LLMs or the fixed-link perspective of GNNs? We introduce specific metric, the Global Linkage Horizon (GLH), to measure the visibility range between nodes in LLMs. Through extensive experiments adjusting the GLH, we found that neither the fully connected view of LLMs nor the fixed-linkage view of GNNs represented the optimal attention perspective for LLMs. Intermediate perspectives that incorporate certain topological link information achieve superior performance during training and yield unexpected improvements when used solely for inference. Specifically, models trained with smaller linkage horizon can be effectively deployed with larger linkage horizon. This transferability from small to large perspectives addresses practical deployment challenges while enhancing model performance. In this work, our primary objective is to identify unique phenomena and explore the causes and potential impacts of these phenomena, aiming to provide new perspectives on how LLMs process graph-structured data, thereby guiding the direction of future discoveries for the community. Our contributions in this work are summarized as follows: We conducted visualization and analysis of LLM attention on graph-structured data. To our knowledge, this is the first empirical study to investigate LLMs for graph machine learning from the perspective of attention. Our analysis reveals that LLMs fail to effectively leverage the connectivity information in graphs. We delve into this issue by examining two major aspects: the distribution of attention scores and the scope of attention windows. We identify Attention Sink issues similar to those observed in natural language tasks, as well as unique phenomenon we term Skewed Line Sink specific to graph data. Drawing on the experience of the NLP community, we can explore methods to correct these biases to improve model performance. Through series of experiments, we identified multiple phenomena and current challenges faced by LLMs in graph machine learning applications. Our work provides valuable insights and guides future research efforts in this field. 2. Related Work 2.1. Analysis of Attention in LLMs With the remarkable attention that LLMs have garnered across various communities, pioneering studies have begun to focus on the attention mechanisms within LLMs and their distribution (Ruan & Zhang, 2024; Acharya et al., 2024; Makkuva et al., 2024; Liang et al., 2024). StreamLLM (Xiao et al., 2024) was among the first to identify the phenomenon of Attention sinks, where semantically limited initial tokens can receive disproportionately higher attention scores. The study suggested maintaining these special tokens during long-text inference for better performance. Building on StreamLLM, Yu et al. (2024) found that correcting partial attention sinks can yield performance improvements without additional training. Duan et al. (2024) explored the relationship between attention and sentence uncertainty, while Hsieh et al. (2024) investigated the lostin-the-middle phenomenon in RAG, focusing on the interaction between retrieved document ranking and attention. 2.2. Attention Window We define the attention window as the visibility range between tokens within single layer of neural network. In models like BERT (Kenton & Toutanova, 2019), the attention view is bidirectionally fully connected, allowing each 2 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 1. Attention distribution of different types of tokens before and after training. With Amazon-Ratings on the left, Roman-Empire in the middle, and Wikics on the right. The attention values have undergone log scaling and are plotted as density distribution Figure. token to attend to all other tokens. Conversely, in GPT series models (Brown et al., 2020), unidirectional causal mask is used, restricting each token to only see preceding tokens, resulting in lower triangular attention matrix. Similarly, in GNNs, the visibility is defined by fixed connections, where each node can only see its directly linked neighbors (Kipf & Welling, 2022). In the context of applying LLMs to graph data mining, some works have simply adapted LLMs to use either fixed-link or bidirectionally fully connected attention window (Yang et al., 2024; Zhu et al., 2024b). However, these adaptations lack thorough experimental analysis and deeper exploration of the attention windows impact on model performance. 3. Empirical Study Our experiment and analysis summary are based on multiple datasets. In addition to the presentation part, some detailed settings and results can be specifically seen in Appendix. A1. Changes in Attention Distribution Fine-tuned LLMs(LLaGA (Chen et al., 2024b)) show relative improvements on graph tasks. To investigate whether LLMs can recognize the differences between graphstructured data and natural language data, as well as effectively utilize graph structural information (Q1), we compare the changes in token attention focus before and after training. Setting. H0: The attention focused on node tokens does not change before and after fine-tuning. H1: The attention focused on text tokens changes before and after fine-tuning. H2: The attention distributions for node tokens and text tokens are not consistent. It is worth noting that we follow the recognized LLaGa guidelines. For more detailed information, see Appendix J. We compared the attention score distributions for node tokens and text tokens before and after training in Figure 1. Analysis Table 1. Statistical Score Distributions(Roman-Empire Dataset). Note: ** indicates p-value <0.01, otherwise >0.05 for t-test and KS test. JS Divergence values are provided as this. Attention of Comparison T-Test KS Test JS Before vs After (Text) Before vs After (Node) Node vs Text (Before) Node vs Text (After) 44.061** 0.461 -60.455** -78.651** 0.092** 0.049** 0.279** 0.316** 0.0064 0.0099 0.0753 0.0791 Our analysis reveals marked shift in the attention distribution for node tokens, demonstrating that LLMs begin to recognize node tokens. In nutshell, it is observed that LLMs tend to align the attention distribution of node tokens and text tokens. To statistically validate these observations, we performed T-tests, Kolmogorov-Smirnov (KS), and Jensen-Shannon Divergence (JS) on the attention score distributions, with the results summarized in Table 1. Our statistical analysis reveals distinctive outcome when comparing the attention score distributions before and after training, specifically for node tokens. The KS test indicates significant change in the distribution of attention scores posttraining, while the t-test suggests that the mean attention scores remain unchanged. Given that the KS test focuses on the overall distribution and the t-test emphasizes the mean value, this discrepancy highlights the nuanced changes in attention patterns. Additionally, the JS distance further supports these findings by showing that the overall distribution of attention scores for nodes changes more significantly compared to text tokens. This implies that while the average attention levels remain consistent, the distribution of attention scores exhibits bimodal trend, indicating that LLMs develop distinct attention patterns for certain nodes, leading to 3 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data polarization in attention allocation. A2. Structure Information Disruption Experiment After discovering that LLMs can perceive graph-structured data, we further investigated whether they can effectively utilize this type of data. To explore this, we designed disruption experiments with varying levels of connectivity information alteration and observed changes in model performance. Ideally, as the level of disruption increases, the models performance should deteriorate. Additionally, we collected attention scores from the model to provide new insights into its behavior in graph data. Setting. We divided the disruption experiments into four levels: (I) Swapping entire sets of child nodes between pairs of first-order nodes. (II) Exchanging random numbers of child nodes between first-order nodes, and further disrupting the information structure. (III) Randomly shuffling the positions of first-order and second-order nodes, even allowing original second-order nodes to become first-order nodes. (IV) Incorporating unrelated nodes and performing random substitutions. And (Raw) keeping information structure. The results of these experiments are shown in Table 2. Disappointingly, the LLMs showed no significant reaction to the perturbation of graph-structured connectivity information across half of the datasets. Specifically, in the majority of cases(Wikics, Pubmed), there was no significant change in model performance under disruption levels (I) and (II). Only at higher levels of disruption (III) or (IV) did we observe decline in performance. However, GNNs that passed the WL-test have been continuously decreasing. In contrast, only the Roman dataset presented an ideal scenario where the models performance degraded progressively with increasing levels of disruption. This behavior aligns with our hypothesis that the model effectively leverages graph-structured information. The consistent degradation of performance on the Roman dataset suggests that LLMs can indeed exploit structural information when it is sufficiently represented and not overly disrupted. In other datasets, the model failed to demonstrate such clear pattern, indicating limitations in its ability to utilize graph structure. To gain deeper insights, we meticulously examined changes in the distribution of attention scores assigned by neighboring nodes to central nodes before and after training, under varying perturbations. The results are illustrated in Figure 2. Figure 2 delineates the distribution of attention preand post-training, revealing diminished focus on graph structure through reduced attention from neighbor to central nodes in datasets where graph structure information was not effectively utilized. Conversely, on the Roman-Empire dataset, there is an observed increase in such attention, signifying the models learned utilization of graph structure information. Overall(Q1), while LLMs exhibit an awareness of graphstructured data, their current mechanisms limit their ability to effectively utilize this information. In the subsequent subsection, we will delve deeper into the distribution of attention scores to further explore how LLMs process graph-structured data. B. Adaptability of Attention Distribution to Graph Data When the graph structure is input to the LLMs in natural language form, the model assigns different attention scores to each graph node. By analyzing the attention scores of different types of nodes, we aim to examine whether the LLM can effectively adapt to the graph data structure (Q2) and assess whether its attention distribution aligns with the ideal statethat is, whether it can reasonably allocate attention according to the topological structure of the graph. Setting. We emulate the most common instruction construction methods (such as InstrutGLM (Ye et al., 2023) and LLaGA (Chen et al., 2024b)) to describe graph link structures in natural language form shown in Figure 4. During this process, the number of neighbors for each central node varies, meaning that fixed positions in the input sequence may be occupied by different types of nodes or text tokens. To eliminate interference from factors such as sequence length and position, which could affect the attention scores assigned to node tokens, we introduce two operations: padding and random shuffling. These operations ensure that the input instructions and nodes have fixed length and position, as illustrated in Figure 4. B1. Position Bias Interferes with Attention Adaptation Through our experiments, we recorded attention scores for different nodes at fixed positions. As shown in Figure 3 Upper, node tokens exhibit slash trend or U-shaped distribution of attention towards node tokens, with attention scores sharply increasing for the final nodes. This distribution aligns with the common attention distribution curve observed in language models. Additionally, we attribute the lower attention scores for the initial nodes to their not being positioned at the forefront of the entire text. However, it reveals an issue: under the graph data structure, the LLMs attention to important nodes does not adequately adapt to the graph structure. Specifically, in the ideal state of graph machine learning, the model should prioritize central nodes and gradually decrease attention to neighboring nodes in hierarchical order. Alternatively, if there are super-nodes, random arrangement should yield an average trend. Instead, the curve we obtained is entirely different from the ideal state, with the importance of central nodes significantly lagging behind other nodes. In our exploration of this issue, we conducted more detailed analysis of the attention patterns between text tokens 4 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Table 2. Connectivity Information Disruption Performance. Node classification results (accuracy(%) std) for 4 runs on four real-world datasets and five levels of disruption. The model used is LLama2-7B, with node sampling configured as 8x8. Dataset Raw (I) (II) (III) (IV) Wikics Pubmed Amazon-Ratings Roman-Empire 0.7862 0.007 0.83670.003 0.44860.002 0.80890.001 0.7847 0.004 0.83630.001 0.39800.003 0.79180.001 0.7907 0.0035 0.83640.002 0.39770.002 0.79100.002 0.7670 0.003 0.76980.003 0.39750.003 0.62900.002 0.7080 0.005 0.78350.001 0.39130.001 0.57840. Figure 2. The attention scores from neighboring nodes to the central node, both before and after training, were presented as mean values with standard deviations, using 1:8 sampling ratio. and node tokens in Figure 3(Lower). The attention paid by text tokens to node tokens aligns more closely with the ideal scenario: first-order nodes receive higher attention scores compared to their surrounding second-order nodes. The attention scores exhibit fluctuating upward trend as function of node position, with peaks occurring at positions corresponding to structurally significant nodes. This indicates that the ability of LLMs to capture relationships between text and nodes is already quite robust; the aspect that requires further development is the connectivity among nodes within LLMs. When exploring whether there are inconsistencies in the attention paid by node tokens and text tokens to text tokens, the results showed no significant differences between the two. The locations of attention sinks or minor fluctuations were largely consistent, indicating high degree of similarity. This suggests that both node tokens and text tokens exhibit consistent attention patterns towards text tokens. Remark(Q2). We found that the attention mechanism does not adapt well to sequentially input graph-structured data, as its distribution does not meet the ideal scenario. Unlike NLP tasks, where the mismatch distribution is acceptable due to uncertain positions of important text, graph-structured data contains prior importance information. As result, LLMs with this issue cannot match the performance of GNNs that focus on graph structure. Furthermore, some studies on RAG indicate that the placement of tokens in different positions significantly affects attention (Hsieh et al., 2024). B2. Nodes Interaction Attention Score To delve deeper into the nuances of attention distribution among node tokens, we conducted heatmap visualization analysis of the attention score matrix, uncovering several novel phenomena. More precisely, Figure 5 shows the attention interaction matrix across all nodes. Based on this, we observed the so-called Attention sink phenomenon (Xiao et al., 2024), which manifests in two distinct patterns across most graph datasets. The first pattern is simple Attention sink as shown in Figure 5(Left), where certain positions consistently attract higher attention scores without significant topological or sequential information. The second pattern exhibits unique diagonal characteristic specific to graph data. Typically, diagonals near the main diagonal exhibit higher attention values due to nodes paying more attention to their neighbors; however, as depicted in Figures 5(Right), there exists diagonal with notably higher attention scores compared to surrounding diagonals. This pattern was not found in textual inspections and suggests that the model may be learning to capture unexpected spatial patterns or path dependencies, possibly arising from the inherent properties of the graph structure. Given its distinction from simple adjacency relationships, we term this pattern 5 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 3. Illustration of all tokens to nodes attention. The x-coordinate refers to the relative position of the node token in the entire node list. We collected the attention scores of all tokens towards node tokens and plotted them in line graph according to the relative position of nodes within the instructions. Upper: the mean values of attention scores from nodes(Querys) to nodes(Keys). Lower: the mean values of attention scores from texts(Querys) to nodes(Keys). The plot annotates nodes at different hierarchical levels. Our node sampling is 8*8. In recent works (Kim et al., 2022; Joshi, 2020), it has been shown that transformers can be viewed as fully connected graph-attention models, while contemporary decoder-only LLMs function as unidirectional fully connected Graph Transformer (GT) models. For given graph-structured data, the attention window of LLMs manifests as lower triangular matrix representing full connectivity, whereas the attention window of GNNs is fixed-linkage adjacency matrix. These two extremes represent the spectrum of attention windows in graph-structured data. The attention window determines how the model captures relationships between nodes and is critical factor influencing the effectiveness of graph structure learning. In this section, we explore how LLMs learn and utilize graph structures from different visible perspectives, seeking the optimal linkage perspective between fully connected and fixed-linkage views(Q3). Setting. We introduce global linkage horizon to indicate the visibility range of node tokens under the attention window. ranges from 0 to 2L, where represents the number of hops or neighborhood radius from the central node. As shown in Figure 6, when sampling subgraphs with = 2 hops: = 0 means all nodes can only see themselves, = 1 indicates all nodes can see their first-order neighbors, and = 4 means nodes can see all other nodes. Among these, = 0 and = 4 represent Attention Windows without effective link information, while = 1, 2, 3 include partial graph link information. Additionally, considering that Figure 4. Padding and Random Shuffling to ensure fixed sequence length and position. This minimizes interference with attention scores for other reasons. Skewed Line Sink. The emergence of Attention sink and Skewed Line Sink phenomena interferes with the proper allocation of attention between nodes in LLMs, preventing them from effectively utilizing graph structural information. However, based on these findings, we can engage more effectively with the NLP community to correct these biases. C. Attention Window for Graph-Structured Data 6 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 5. Illustration of attention score interaction matrix(Nodes). Left: The left panel displays the attention interaction matrix between all nodes (1 central node + 8 first-order nodes + 8*8 second-order nodes), averaged across all heads and layers. Right: The right panel shows the attention interaction matrices between first-order and second-order nodes, with four selected groups highlighted for detailed analysis. We highlight the attention sink with gray box and the Skewed Line Sink with black box. Table 3. Model performance(accuracy(%) std) for 4 runs on four real-world datasets and under different values of Amazon Pubmed Roman Wikics k4 unidi 77.490.21 80.730.0 45.19 0.2 82.920.1 k4 67.810.06 81.380.0 43.79 0.4 82.470.4 bidi k3 unidi 77.960.13 81.680.02 45.46 0.0 83.360.1 k3 74.930.26 81.610.0 45.63 0.1 81.980.1 bidi k2 unidi 78.900.17 82.870.02 44.67 0.0 82.890.3 k2 77.420.19 82.050.02 45.18 0.2 82.670.1 bidi k1 unidi 78.15 0.02 83.120.0 45.95 0.1 80.490.2 k1 78.43 0.09 83.050.01 46.17 0.0 79.810.2 bidi LLMs are mostly unidirectional decoders, while GNN links are bidirectional, we incorporate both unidirectional (kunidi) and bidirectional (kbidi) links into our empirical study. C1. Optimal Visible Perspective We conducted multiple experiments with different values, and the results are summarized in Table 3. From the table, it is evident that when the training and inference values remain unchanged, the fully connected view (k = 4) of LLMs cannot achieve the best performance. In contrast, intermediate views (k = 2, 3), which contain certain topological link information, achieve better results. The fixed-linkage view 7 (k = 1) is also considered to perform well. However, settings with = 4 require pre-labeling node tokens and modifying the attention window, posing deployment challenges in real-world scenarios. Therefore, we continue our exploration. C2. Unidirectional vs. Bidirectional Attention To further investigate the impact of unidirectional and bidirectional attention, we conducted comparative experiments in Table 4. The mutual transfer between unidirectional and bidirectional attention is difficult to achieve better results, and the loss caused by transfer increases as the value of grows. C3. Transferability across Different Visible Perspectives We also tested the transfer ability of models trained at one value and inferred at others in Table 4. Small-to-large: Surprisingly, switching from smaller to larger values does not weaken model performance but rather enhances it. Specifically, we can achieve better performance at inference time with k=4 by training the model at = 2 or = 3, compared to training directly at = 4. Since = 4 is the fully connected view of LLMs, no modification is required during real-world inference, addressing the deployment difficulties mentioned in subsection C1. Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 6. Illustration of different global linkage horizon k. From left to right, the images represent k=1 to 4. To demonstrate the field of view of GNNs, the image for k=1 is depicted bidirectionally, while the rest are shown unidirectionally. Table 4. Transfer Performance of Models across Different Values. Rows indicate the value used during training, Columns represent the value utilized during testing. The best performance for each value transfer is highlighted in gray, with the overall best performance bolded. The best result of LLM testing under normal window (k4 unidi) conditions is indicated with an underline. Test Train k4 bidi k4 unidi k3 bidi k3 unidi k2 bidi k2 unidi k1 bidi k1 unidi k4 bidi k4 unidi k3 bidi k3 unidi k2 bidi k2 unidi k1 bidi k1 unidi 67.810.06 66.870.19 68.090.34 65.950.04 67.450.13 66.720.13 65.100.21 64.820.23 61.020.23 77.490.21 72.430.02 78.410.06 74.750.38 78.190.23 77.000.36 77.870.34 74.631.15 72.170.66 74.930.26 72.750.26 75.870.04 72.830.17 72.660.17 71.230.11 57.730.49 77.820.30 72.550.19 77.960.13 76.480.11 79.220.15 77.380.66 77.740.04 64.030.34 73.710.49 75.140.21 74.880.34 77.420.19 75.220.47 76.590.51 74.480.41 52.580.04 74.970.68 68.800.53 77.510.02 73.300.56 78.900.17 76.080.38 76.330.09 49.170.30 74.330.04 69.200.09 75.220.34 75.250.02 76.780.45 78.320.49 78.430.09 34.050.01 72.080.79 55.750.21 74.630.26 66.190.02 76.910.28 75.500.53 78.150. Large-to-small: Transitioning from broader to narrower window results in improved model performance (e.g., from 4 to 3, or from 3 to 2). We attribute this phenomenon to the fact that training LLMs is more challenging from wider perspectives, where the model must contend with greater amount of context and potential noise. When shifting to narrower perspective, the model benefits from reduced level of complexity and fewer distractions, leading to better. Overall(Q3), our empirical studies reveal that the Attention Window with connection information significantly impacts LLMs understanding of graph structure. Training with nonfully connected views containing certain topological link information aids LLMs in understanding graph structures and facilitates transfer from small to large perspectives. Other work in Appendix detailed description of the setup is provided in Appendix A, with information about the dataset in Appendix B. Additional related work is discussed in Appendix C. Attention plots and experimental results for other datasets (base LLMs) are presented in Appendices H, G, I, and F. 4. Findings and Conclusion Findings. We found that although LLMs gradually become aware of graph data during training, they fail to effectively leverage the connectivity information within these graphs. It was also discovered that fine-tuned LLMs possess the ability to capture relationships between nodes and text, but they lack the capability to model relationships among nodes. This limitation manifests as Attention Sink and Skewed Line Sink phenomena in attention interactions. Additionally, we found that fully connected attention windows of LLMs are not suitable for training on graph data. 8 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data better approach is to train under smaller perspective that incorporates graph topology information and then perform transfer learning. healthcare llm for effective medical documentation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pp. 11671168, 2024. Conclusion and Limitation. We explored the attention mechanisms of LLMs on graph data and discovered numerous phenomena, providing directional guidance for further research by the graph learning community. At the same time, due to limitations in length and resources, we only analyzed portion of the discovered phenomena. There remains wealth of unexplored phenomena awaiting further investigation by the research community."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 5. Acknowledgements This study was partially funded by the supports of National Natural Science Foundation of China (72471165) and Natural Science Foundation of Tianjin (No. 24JCQNJC01560)."
        },
        {
            "title": "References",
            "content": "Acharya, S., Jia, F., and Ginsburg, B. Star attention: Efficient llm inference over long sequences. arXiv preprint arXiv:2411.17116, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, R., Zhao, T., Jaiswal, A., Shah, N., and Wang, Z. Llaga: Large language and graph assistant. arXiv preprint arXiv:2402.08170, 2024a. Chen, R., Zhao, T., JAISWAL, A. K., Shah, N., and Wang, Z. Llaga: Large language and graph assistant. In Forty-first International Conference on Machine Learning, 2024b. Duan, J., Cheng, H., Wang, S., Zavalny, A., Wang, C., Xu, R., Kailkhura, B., and Xu, K. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5050 5063, 2024. Goyal, S., Rastogi, E., Rajagopal, S. P., Yuan, D., Zhao, F., Chintagunta, J., Naik, G., and Ward, J. Healai: Guan, Z., Zhao, H., Wu, L., He, M., and Fan, J. Langtopo: aligning language descriptions of graphs with tokenized topological modeling. arXiv preprint arXiv:2406.13250, 2024. He, X., Bresson, X., Laurent, T., Perold, A., LeCun, Y., and Hooi, B. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. In The Twelfth International Conference on Learning Representations, 2023. He, X., Bresson, X., Laurent, T., Perold, A., LeCun, Y., and Hooi, B. Harnessing explanations: Llm-to-lm interpreter for enhanced text-attributed graph representation learning. In The Twelfth International Conference on Learning Representations, 2024. He, Y. and Hooi, B. Unigraph: Learning cross-domain graph foundation model from natural language. arXiv preprint arXiv:2402.13630, 2024. Hsieh, C.-Y., Chuang, Y.-S., Li, C.-L., Wang, Z., Le, L. T., Kumar, A., Glass, J., Ratner, A., Lee, C.-Y., Krishna, R., et al. Found in the middle: Calibrating positional attention bias improves long context utilization. arXiv preprint arXiv:2406.16008, 2024. Huang, X., Han, K., Yang, Y., Bao, D., Tao, Q., Chai, Z., and Zhu, Q. Can gnn be good adapter for llms? In Proceedings of the ACM on Web Conference 2024, pp. 893904, 2024. Joshi, C. Transformers are graph neural networks. The Gradient, 12:17, 2020. Kenton, J. D. M.-W. C. and Toutanova, L. K. Bert: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, pp. 2. Minneapolis, Minnesota, 2019. Kim, J., Nguyen, D., Min, S., Cho, S., Lee, M., Lee, H., and Hong, S. Pure transformers are powerful graph learners. Advances in Neural Information Processing Systems, 35: 1458214595, 2022. Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2022. Kong, L., Feng, J., Liu, H., Huang, C., Huang, J., Chen, Y., and Zhang, M. Gofa: generative one-for-all model for joint graph language modeling. arXiv preprint arXiv:2407.09709, 2024. 9 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Liang, Y., Liu, H., Shi, Z., Song, Z., Xu, Z., and Yin, J. Conv-basis: new paradigm for efficient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024. Liu, Z., Wu, L., He, M., Guan, Z., Zhao, H., and Feng, N. Multi-view empowered structural graph wordification for language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2471424722, 2025. Luo, Y., Shi, L., and Wu, X.-M. Classic gnns are strong baselines: Reassessing gnns for node classification. arXiv e-prints, pp. arXiv2406, 2024. Makkuva, A. V., Bondaschi, M., Nagle, A., Girish, A., Kim, H., Jaggi, M., and Gastpar, M. Attention with markov: curious case of single-layer transformers. In ICML 2024 Workshop on Mechanistic Interpretability, 2024. Mernyei, P. and Cangea, C. Wiki-cs: wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901, 2020. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Pi, R., Yao, L., Gao, J., Zhang, J., and Zhang, T. Perceptiongpt: Effectively fusing visual perception into llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2712427133, 2024. Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. critical look at the evaluation of gnns under heterophily: Are we really making progress? In The Eleventh International Conference on Learning Representations, 2023. Qiao, Y., Ao, X., Liu, Y., Xu, J., Sun, X., and He, Q. Login: large language model consulted graph neural network training framework. arXiv preprint arXiv:2405.13902, 2024. Ruan, T. and Zhang, S. Towards understanding how attention mechanism works in deep learning. arXiv preprint arXiv:2412.18288, 2024. Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 1152311530. IEEE, 2023. Sun, S., Ren, Y., Ma, C., and Zhang, X. Large language models as topological structure enhancers for text-attributed graphs. arXiv preprint arXiv:2311.14324, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wu, L., Zhao, H., Li, Z., Huang, Z., Liu, Q., and Chen, E. Learning the explainable semantic relations via unified graph topic-disentangled neural networks. ACM Transactions on Knowledge Discovery from Data, 17(8):123, 2023. Wu, L., Zheng, Z., Qiu, Z., Wang, H., Gu, H., Shen, T., Qin, C., Zhu, C., Zhu, H., Liu, Q., et al. survey on large language models for recommendation. World Wide Web, 27(5):60, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. Yang, H., Wang, X., Tao, Q., Hu, S., Lin, Z., and Zhang, M. Gl-fusion: Rethinking the combination of graph neural network and large language model. arXiv e-prints, pp. arXiv2412, 2024. Yang, J., Liu, Z., Xiao, S., Li, C., Lian, D., Agrawal, S., Singh, A., Sun, G., and Xie, X. Graphformers: Gnnnested transformers for representation learning on textual graph. Advances in Neural Information Processing Systems, 34:2879828810, 2021. Ye, R., Zhang, C., Wang, R., Xu, S., and Zhang, Y. Natural language is all graph needs. arXiv preprint arXiv:2308.07134, 2023. Yu, Z., Wang, Z., Fu, Y., Shi, H., Shaikh, K., and Lin, Y. C. Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration. arXiv preprint arXiv:2406.15765, 2024. Zhang, S., Zheng, D., Zhang, J., Zhu, Q., Adeshina, S., Faloutsos, C., Karypis, G., Sun, Y., et al. Hierarchical compression of text-rich graphs via large language models. arXiv preprint arXiv:2406.11884, 2024. Zhao, H., Wu, L., Shan, Y., Jin, Z., Sui, Y., Liu, Z., Feng, N., Li, M., and Zhang, W. comprehensive survey of large language models in management: Applications, challenges, and opportunities. Challenges, and Opportunities (August 14, 2024), 2024. 10 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Zhao, J., Qu, M., Li, C., Yan, H., Liu, Q., Li, R., Xie, X., and Tang, J. Learning on large-scale text-attributed graphs via variational inference. arXiv preprint arXiv:2210.14709, 2022. Zhu, X., Xue, H., Zhao, Z., Jin, M., Xu, W., Huang, J., Wang, Q., Zhou, K., and Zhang, Y. Llm as gnn: Graph vocabulary learning for graph foundation model. openreview, 2024a. Zhu, Y., Wang, Y., Shi, H., and Tang, S. Efficient tuning and inference for large language models on textual graphs. arXiv preprint arXiv:2401.15569, 2024b. Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data A. Implementation Details Throughout the experiments, we maintained LLama2-7B (Touvron et al., 2023) as our base model and used an 8x8 scheme for neighbor sampling. For each dataset, during the experimental process, we processed the raw text from the datasets using the base model to serve as the embedding features for the nodes, treating each node akin to token in the manner of LLama. Regarding the data collected as described in A.1, we gathered the attention scores for each node across different layers and heads, scaling them using logarithmic function. For the perturbed data in A.2: (II) We performed random swaps between two nodes, repeating this procedure ten times consecutively. (IV) We conducted random exchanges of nodes within the same batch to introduce disorder. More details are shown in Table 5. Table 5. Hyperparameters for the Roman-Empire, Amazon-Ratings, and Pubmed, Wikics."
        },
        {
            "title": "Hyperparameters",
            "content": "Amazon-Ratings"
        },
        {
            "title": "Pubmed Wikcis Roman",
            "content": "learning rate warmup gradient accumulation steps batch size epoch num beams use embedding 1e-4 0.05 8 4 1 2 Ture 1e-4 0.05 8 4 1 2 True 1e-4 0.05 8 4 1 2 False 1e-4 0.05 8 4 1 2 False B. Datasets When selecting datasets, we considered broad spectrum and chose heterogeneous graph datasets Amazon-Rating (Platonov et al., 2023) and Roman-Empire (Platonov et al., 2023), as well as homogeneous graph datasets Pubmed and WikiCS (Mernyei & Cangea, 2020). The statistical metrics of each dataset are shown in the following Table 6. Table 6. Statistics of datasets Roman-Empire Amazon-Ratings Wikics Pubmed nodes edges avg degree node features classes edge homophily adjusted homophily 22,662 32,927 2.91 4096 18 0.05 -0. 24,492 93,050 7.60 4096 5 0.38 0.14 11,701 216,123 36.89 4096 10 - - 19,717 44,338 4.49 4096 3 - - C. Additional Related Work Recent advancements have delved into leveraging Large Language Models (LLMs) within graph structure domains. Studies such as GLEM (Zhao et al., 2022), along with other works (Yang et al., 2021), have probed into the integration of LLMs and Graph Neural Networks (GNNs) through joint training frameworks. The approach taken by (He et al., 2023) employs LLMs to forecast node ranking classifications and offers comprehensive insights to enrich the quality of GNN embeddings. Meanwhile, Sun et al. (2023) has exploited LLMs for generating pseudo-labels aimed at enhancing the representation of graph topologies. Moreover, there has been an emphasis on advancing the direct processing capabilities of LLMs for textual graphs. For instance, InstructGLM (Ye et al., 2023) pioneers the use of instruction tuning based on LLMs to articulate graph structures and node characteristics, effectively addressing graph-related tasks. An interactive fusion of LLMs and GNNs is also presented by (Qiao et al., 2024). Despite these strides, LLMs face challenges when dealing with structured data that has been converted into natural language, frequently leading to less than optimal outcomes. To address this issue, LLaGA (Chen et al., 2024a) reformulates node-link information into sequential data, thereby applying instruction tuning that enhances LLM comprehension while preserving structural node information. UniGraph (He & Hooi, 2024) implements masked strategy for co-training LLMs and GNNs together, achieving robust generalization across diverse graphs and Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data datasets. Additionally, Kong et al. (2024) investigates the creation of graph-based foundational models that merge LLMs with GNNs. GraphAdapter (Huang et al., 2024) utilizes GNN model as an adapter working alongside LLMs for TAG tasks, which aids in task-specific fine-tuning via external access. Moreover, recent efforts have increasingly focused on designing modules from more comprehensive perspective to achieve better performance. In the context of Zhu et al. (2024a), mimicking the aggregation process of GNNs through instructions forces LLMs to learn how to aggregate node information effectively. The work by Zhang et al. (2024) employs LLMs to compress lengthy raw texts hierarchically, ultimately condensing them into small number of tokens suitable for graph tasks. This hierarchical compression allows for efficient representation of complex textual data in form that can be readily processed by graph-based models. Zhang et al. (2024) approach incorporates the information from neighboring nodes when performing attention calculations on tokens within its own text, while also reducing the number of tokens. During the final inference phase, it refines predictions by re-evaluating the surrounding neighbor nodes, thereby enhancing the accuracy and relevance of the outcomes. D. Other Base Models for Disruption Performance Table 7. Disruption Performance. Node classification results (accuracy(%) std) for 4 runs on four real-world datasets and five levels of disruption. The model used is Vicuna-7B, with node sampling configured as 8x8. Dataset Raw (I) (II) (III) (IV) Wikics Pubmed Amazon-Ratings Roman-Empire 0.7899 0.004 0.83220.002 0.45410.001 0.80940.001 0.7898 0.002 0.83450.001 0.45440.001 0.80010.001 0.7919 0.001 0.83000.001 0.44980.002 0.78830.003 0.7773 0.002 0.74340.003 0.41350.001 0.65430.001 0.7101 0.005 0.74530.001 0.39430.001 0.60160. Table 8. Disruption Performance. Node classification results (accuracy(%) std) for 4 runs on four real-world datasets and five levels of disruption. The model used is LLama3-7B, with node sampling configured as 8x8. Dataset Raw (I) (II) (III) (IV) Wikics Pubmed Amazon-Ratings Roman-Empire 0.7988 0.001 0.84480.001 0.45150.001 0.81080.002 0.7981 0.006 0.84490.003 0.45230.002 0.80070.002 0.7988 0.005 0.84420.003 0.39850.003 0.79220.002 0.7487 0.002 0.80670.003 0.38890.002 0.63240. 0.7209 0.001 0.77530.002 0.37440.005 0.56180.002 E. Statistical Analysis of Attention Score Distributions It can be seen that most of them are consistent with our presentation and our conclusions. Table 9. Statistical Analysis of Attention Score Distributions. Note: ** indicates p-value 0.01, otherwise 0.05 for t-test and KS test. JS Divergence values are provided as is. Comparison Amazon WikiCS T-Test KS Test JS T-Test KS Test JS Before vs After (Text) Before vs After (Node) Node vs Text (Before) Node vs Text (After) 95.066 69.334 337.116 318.054 0.118 0.116 0.428 0.438 61.764 0.0066 118.293 0.0113 0.1423 268.998 0.1123 184.605 0.107 0.216 0.362 0.285 0.0038 0.0307 0.0766 0.0264 13 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data F. Disruption Attention Score In Figure 2, we illustrate the attention scores from neighboring nodes to the central node before and after training, without altering the structural information. Here, we show the attention scores from neighboring nodes to the central node under various perturbation conditions. Table 10. Disruption Attention Score (Amazon-Ratings). The attention scores from neighboring nodes to the central node, were presented as mean values with standard deviations, using 1:8 sampling ratio. Node 1 2 3 4 6 7 8 (raw) 0.0170.025 0.0180.025 0.0120.019 0.0080.011 0.0070.011 0.0060.009 0.0040.005 0.0040.005 (I) (II) 0.0180.025 0.0180.025 0.0120.020 0.0080.012 0.0070.010 0.0060.008 0.0040.005 0.0040.005 0.0170.024 0.0170.025 0.0120.019 0.0080.010 0.0070.010 0.0060.009 0.0040.005 0.0040.005 (III) 0.0190.026 0.0180.026 0.0130.020 0.0080.011 0.0070.010 0.0060.008 0.0040.005 0.0040.005 (IV) 0.0170.025 0.0180.025 0.0120.019 0.0080.011 0.0070.011 0.0060.009 0.0040.005 0.0040.005 Table 11. Disruption Attention Score (Wikics). The attention scores from neighboring nodes to the central node, were presented as mean values with standard deviations, using 1:8 sampling ratio. Node 1 2 3 4 5 7 8 (raw) 0.0170.021 0.0120.017 0.0090.014 0.0060.011 0.0060.011 0.0050.010 0.0040.009 0.0040.009 (I) (II) 0.0180.022 0.0130.018 0.0090.015 0.0070.013 0.0060.012 0.0050.012 0.0040.011 0.0040. 0.0170.020 0.0120.016 0.0080.013 0.0060.010 0.0060.010 0.0050.009 0.0040.008 0.0040.007 (III) 0.0160.019 0.0110.013 0.0070.009 0.0050.006 0.0050.006 0.0040.005 0.0030.004 0.0030.003 (IV) 0.0180.022 0.0130.018 0.0090.015 0.0070.012 0.0060.012 0.0050.011 0.0040.010 0.0040.010 Table 12. Disruption Attention Score (Roman). The attention scores from neighboring nodes to the central node, were presented as mean values with standard deviations, using 1:8 sampling ratio. Node 2 3 4 5 6 8 (raw) 0.0230.021 0.0160.016 0.0140.014 0.0120.012 0.0100.010 0.0090.010 0.0110.011 0.0040.002 (I) (II) 0.0240.021 0.0160.016 0.0130.013 0.0110.011 0.0090.010 0.0090.009 0.0110.011 0.0040.003 0.0240.021 0.0160.016 0.0140.014 0.0120.013 0.0100.011 0.0100.010 0.0110.011 0.0040. (III) 0.0220.020 0.0150.014 0.0110.011 0.0100.011 0.0090.009 0.0080.008 0.0060.010 0.0040.001 (IV) 0.0230.021 0.0160.016 0.0140.014 0.0120.012 0.0100.010 0.0090.010 0.0110.012 0.0040.003 14 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data G. Attention Score Matrix(Tokens) G.1. Token to Token Figure 7. Attention score interaction matrix(Nodes) in Wikics. 15 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 8. Attention score interaction matrix(Nodes) in Roman-Empire. 16 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 9. Attention score interaction matrix(Nodes) in Amazon-Ratings. 17 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data G.2. Text to Text Figure 10. Attention score interaction matrix(Text) in Wikics. Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 11. Attention score interaction matrix(Text) in Roman-Empire. 19 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 12. Attention score interaction matrix(Text) in Amazon-Ratings. Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data H. Attention Score among First Nodes(with child nodes) Figure 13. Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Amazon-Ratings((1+8)*2). 21 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 14. Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Roman-Empire((1+8)*2). 22 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 15. Visualization of the average attention(Attention Score among First Nodes(with child nodes)) in Wikics((1+8)*2). 23 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data I. Different Layer Attention Score between First Nodes and child nodes Figure 16. Visualization of the average attention(Center nodes and First-order nodes) in Amazon-Ratings. 24 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 17. Visualization of the average attention(Center nodes and First-order nodes) in Roman-Empire. 25 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data Figure 18. Visualization of the average attention(Center nodes and First-order nodes) in Wikics. 26 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data J. Prompt Here, we list all the prompts we used in this paper on different datasets, we use the following prompt:. Roman-Empire: <User >: In an article, words that have dependency relationships (where one word depends on another) are connected, forming dependency graph. Based on the connections between words, determine the syntactic role of each word. Given that word [] that connect [], what is the word [] syntactic role? <Assistant >: Amazon-Ratings: <User >: In product graph dataset, edges connect products that are frequently purchased together. Based on the connections between products (books, music CDs, DVDs, VHS tapes), predict the average rating given by reviewers for the products. Given that product [] that connect [], what is the product [] rating? <Assistant >: Pubmed: <User >: In medical paper dataset, papers that cite each other form linkage relationship. Based on the linkage relationships among papers, the research directions of medical papers can be predicted. Given that paper [] that connect [], What is the category of the paper []? <Assistant >:: Wikics: <User >: In paper dataset, papers that cite each other form linkage relationship. Based on the linkage relationships among papers, the research directions of papers can be predicted. Given that paper [] that connect [], What is the category of the paper []? <Assistant >:: K. Different Templates We designed various Templates for analysis and discussion, inspired by the the paper (TALK LIKE GRAPH). The results of the disruption experiment are as follows. From the Table K, it can be seen that although different templates have varying model performances, the overall trend is consistent, showing utilization of link information specifically on the Roman dataset, while link information did not play role on most other datasets. Our initial template (1) remains the best performing and makes the best use of link information. Dataset Raw (I) (II) (III) (IV) Table 13. Comparison of different datasets under various templates. (1) Wikics Pubmed Amazon-Ratings Roman-Empire (2) Wikics Pubmed Amazon-Ratings Roman-Empire (3) Wikics Pubmed Amazon-Ratings Roman-Empire (4) Wikics Pubmed Amazon-Ratings Roman-Empire 0.7862 0.007 0.8367 0.003 0.4486 0.002 0.8089 0. 0.7847 0.004 0.8363 0.001 0.3980 0.003 0.7918 0.001 0.7907 0.0035 0.8364 0.002 0.3977 0.002 0.7910 0.002 0.7670 0.003 0.7698 0.003 0.3915 0.003 0.6290 0.002 0.7087 0.005 0.7835 0.001 0.3813 0.001 0.5784 0.002 0.7795 0.006 0.8112 0.004 0.4231 0.003 0.7932 0.002 0.7789 0.005 0.8108 0.002 0.4038 0.002 0.7961 0. 0.7778 0.003 0.8050 0.001 0.3923 0.003 0.7935 0.001 0.7512 0.004 0.7745 0.002 0.3869 0.002 0.7228 0.003 0.6871 0.006 0.7783 0.002 0.3768 0.002 0.6130 0.001 0.7806 0.007 0.8315 0.003 0.4438 0.002 0.8030 0.001 0.7793 0.004 0.8310 0.001 0.3932 0.003 0.7864 0.001 0.7851 0.0035 0.8312 0.002 0.3929 0.002 0.7856 0. 0.7618 0.003 0.7643 0.003 0.3867 0.003 0.6234 0.002 0.7025 0.005 0.7781 0.001 0.3765 0.001 0.5728 0.002 0.7811 0.006 0.8325 0.004 0.4379 0.003 0.8091 0.002 0.7833 0.005 0.8322 0.002 0.4085 0.002 0.7915 0.002 0.7812 0.0030 0.8317 0.001 0.4072 0.003 0.7913 0.001 0.7665 0.004 0.7897 0.002 0.3978 0.002 0.6285 0. 0.7092 0.006 0.7064 0.002 0.3885 0.002 0.5788 0.001 27 Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data L. Transfer Performance of Models across Different Values. Table 14. Transfer Performance of Models across Different Values(Roman). Rows indicate the value used during training, Columns represent the value utilized during testing. The best performance for each value transfer is highlighted in gray, with the overall best performance bolded. k= k=3 k=2 k=1 bidi unidi bidi unidi bidi unidi bidi unidi k4 bidi k4 unidi k3 bidi k3 unidi k2 bidi k2 unidi k1 bidi k1 unidi 73.720.04 81.380.00 68.510.04 80.730.00 80.960.02 75.960.05 67.410.06 81.790.01 78.650.01 76.090.03 67.750.03 82.660.01 77.260.01 77.300.04 82.540.01 64.810.05 80.960.02 73.840.04 70.510.04 80.370.01 81.610.00 75.670.01 69.500.05 81.680.02 79.700.00 76.100.00 69.710.04 82.630.02 78.250.03 77.440.02 66.550.01 82.560.00 79.820.01 73.040.04 80.800.02 72.620.01 82.050.02 74.370.02 79.920.02 71.170.01 74.280.06 79.660.01 75.600.01 81.600.01 75.760.01 82.870.01 77.370.00 82.780.01 79.100.00 72.480.01 78.640.02 72.740.03 81.260.00 76.540.03 83.050.01 73.530.00 73.910.04 79.730.01 74.930.08 81.590.02 75.940.01 82.110.02 77.890.04 83.120. Table 15. Transfer Performance of Models across Different Values(Amazon-Ratings). Rows indicate the value used during training, Columns represent the value utilized during testing. The best performance for each value transfer is highlighted in gray, with the overall best performance bolded. k=4 k=3 k=2 k=1 bidi unidi bidi unidi bidi unidi bidi unidi k4 bidi k4 unidi k3 bidi k3 unidi k2 bidi k2 unidi k1 bidi k1 unidi 35.990.51 43.790.43 41.270.16 45.190.20 45.280.01 39.310.33 43.200.15 45.660.11 44.310.10 44.400.04 40.450.16 44.330.12 42.300.11 32.020.09 42.960.09 41.730.15 42.260.11 35.190.11 44.360.08 44.950.08 45.630.08 38.460.47 44.640.09 45.460.04 45.090.05 44.240.03 41.240.03 44.400.20 37.660.11 44.630.07 42.630.10 44.330.17 38.810.16 45.430.19 43.560.02 45.300.01 45.180.20 42.840.28 40.680.03 43.080.19 32.810.21 44.590.18 35.240.52 45.110.16 43.660.04 44.670.05 45.140.00 44.760. 30.080.07 44.470.07 35.550.25 44.780.02 42.780.11 43.460.10 46.170.03 46.270.08 28.980.07 42.970.36 29.100.20 42.830.09 41.420.16 43.010.06 45.300.34 45.950."
        }
    ],
    "affiliations": [
        "AI Lab at Lenovo",
        "College of Management and Economics, Tianjin University, Tianjin, China",
        "Laboratory of Computation and Analytics of Complex Management Systems, Tianjin University, Tianjin, China",
        "ai-deepcube"
    ]
}