{
    "paper_title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking",
    "authors": [
        "Ravi Ghadia",
        "Maksim Abraham",
        "Sergei Vorobyov",
        "Max Ryabinin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\\times$H100 node, improving upon prior methods by over 25$\\%$."
        },
        {
            "title": "Start",
            "content": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Ravi Ghadia 1 Maksim Abraham 1 Sergei Vorobyov 1 Max Ryabinin 1 6 2 0 2 4 2 ] . [ 1 6 9 1 1 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5% for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on single 8H100 node, improving upon prior methods by over 25%. 1. Introduction The Transformer architecture (Vaswani et al., 2017) has powered significant advances in AI in recent years, ranging from language models with agentic and reasoning capabilities (Gemini Team, 2025; Kimi Team et al., 2025; Chen et al., 2025) to video generation (Team Wan et al., 2025; Wu et al., 2025; Sand.ai et al., 2025). As the field continues to progress, the demand for longer context lengths in AI models continues to grow due to applications such as code generation (Li et al., 2023a; Hui et al., 2024), long document understanding (Chia et al., 2024; Jiang et al., 2024), or 1Together AI. Correspondence to: Ravi Ghadia <rghadia@together.ai>, Max Ryabinin <mryab@together.ai>. Preprint. February 25, 2026. 1 Figure 1. Comparison of context parallelism approaches on longsequence training for Llama 3-8B using 8 H100s. UPipe provides maximum efficiency, resulting in longer maximum context length (5M tokens) while retaining throughput. even audio processing (Hori et al., 2021). However, training models to effectively process such long sequences is limited by the accelerator hardware: beyond certain limit, even keeping the activations necessary for self-attention becomes bottleneck. As result, methods that reduce the memory requirements of long-context training have recently attracted surge of research interest. The most scalable approaches for increasing the context size beyond single accelerator leverage distributed training, splitting the computations and memory allocations across multiple devices. In particular, the context parallelism (Li et al., 2022; 2023b; Jacobs et al., 2023) family of methods (also known as sequence parallelism) focuses on sharding model operations across the sequence axis. These methods enable effective scaling in context length with the number of accelerators, but the activation memory per device still scales linearly with the sequence length. Therefore, at very long sequence lengths (>2M), the activation memory starts to become bottleneck, limiting the training capacity. In this paper, we propose UPipe, context parallelism method that focuses on improving the memory efficiency of long-context training while maintaining the performance on par with current approaches. Our method is designed on the principle that for long-context training, processing subset of heads at once is enough to saturate the GPU. Therefore, serializing the execution in the attention layer by grouping heads into smaller chunks allows for much better memory reuse. UPipe is agnostic to the underlying attention Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Figure 2. Memory usage breakdown when training Llama3-8B with sequence length of 3M tokens across 8 H100 GPUs. AC stands for Activation Checkpointing, AO denotes AC with offloading, OOM stands for Out of Memory. algorithm: similarly to DeepSpeed Ulysses, it uses the same kernels to compute attention as non-distributed training. On Llama 3-8B, our method is able to fit context lengths of up to 5 million tokens on single H100 node, and up to 8M on two H100 nodes with unified sequence parallelism (Fang & Zhao, 2024), outperforming prior works by 25% and 33% respectively in terms of maximum context length supported. At the same time, UPipes training throughput is comparable to other context parallelism techniques. Our contributions are as follows: 1. We propose UPipe, new context parallelism method that executes the attention layer in multiple stages, processing attention heads in chunks. This method is easy to implement and can work as plug-and-play replacement for existing techniques. 2. We analyze the memory usage of long-context Transformer training, identifying the major bottleneck left unaddressed by prior works and showing how UPipe mitigates this bottleneck. 3. We present GQA-compatible schedule for UPipe, which processes heads out of order to avoid redundant communication while retaining the memory benefits of standard GQA architecture. 4. We compare UPipe with prior approaches to contextparallel training, measuring the speed and memory usage for 8B and 32B dense Transformer models across sequence lengths ranging from 128k to 5M tokens1. UPipe can support longer sequence lengths (up to 5 million tokens on single 8H100 node) with negligible performance differences compared to other techniques. 1The code for our experiments is available at github.com/togethercomputer/Untied-Ulysses. 2. Background Large language models (LLMs) excel at processing textual information and solving wide variety of tasks. The research community has also become significantly interested in improving the long context understanding abilities of Transformer-based models for various use cases, such as voice (Yang et al., 2024) and video generation (Sand.ai et al., 2025). However, the paradigm of using regular selfattention for long-sequence data has several obstacles to further scaling. For instance, Team Wan et al. (2025) mention that the memory bottleneck due to activation memory when training 14B Diffusion Transformer on sequence length of 1M tokens is 8 TB. It is impossible for single accelerator to hold such tensors, which necessitates the use of scalable approaches that rely on distributed training. 2.1. Context Parallelism To mitigate the problem of large activation memory footprint, context parallelism was first introduced by Li et al. (2023b) and later an online softmax compatible version by Liu et al. (2023) via Ring Attention. This allows sharding the context across devices. Since attention computation requires every shard to attend to other shards, this method first performs local attention using the local Q, K, shards present on the device, and then exchanges the K, shards among the devices in ring fashion via peer-to-peer communication. Because of this, Ring Attention requires O(C) communication calls per attention operation, which is expensive in terms of run time. DeepSpeed-Ulysses (Jacobs et al., 2023) is another context parallelism technique built upon Megatron-SP (Korthikanti et al., 2022) that mitigates the runtime overhead by rearranging the tensors via all-toall, having constant communication volume and unlocking the use of optimized self-attention kernels. We discuss this technique in more detail in Section 3.1. 2 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Table 1. Theoretical peak memory usage breakdown across different stages of the forward pass for Transformer model. All floating point tensors use BFloat16 precision by default, except cross-entropy loss, which uses fp32 precision. Stage Inputs Embedding 4 (int32) Attention 2 dmodel Type QKV all-to-all Intermediate tensors Memory 6 dhead 6 dhead Ratio Outputs Total 2 dmodel 2 dmodel = dmodel/dhead 2 dmodel 16 dmodel Feed-forward 2 dmodel Intermediate 8 df df 2.67 dmodel 2 dmodel 25 dmodel Cross-Entropy 2 dmodel Logits + LogSoftmax 8 30 dmodel Loss 240 dmodel 1 2 3 4 Further works introduce additional optimizations for memory and throughput. For instance, USP (Unified Sequence Parallelism, Fang & Zhao, 2024) uses DeepSpeed-Ulysses within the node and Ring Attention across nodes, running all-to-all communication over the faster NVLink fabric and ring communication over the slower one. To address the memory bottleneck of very long contexts, other papers have explored tiling or chunking techniques. In particular, Arctic Long Sequence Training (ALST, Bekman et al., 2025) uses tiling for feed-forward layer and cross-entropy loss calculations, lowering the memory pressure of intermediate tensors. However, it does not address the memory overhead in the attention phase. Fully Pipelined Distributed Transformer (FPDT, Yao et al., 2025) addresses the memory overhead in self-attention by chunking computations along the sequence length dimension and offloading to CPU. However, it suffers from reduced performance due to CPU overhead and additional memory transfers. UPipe addresses the memory overhead while maintaining performance and extends to hybrid schemes such as USP. Also, our technique performs chunking along the head dimension, which is orthogonal and complementary to the optimizations of FPDT. 2.2. Activation memory for Transformer model Next, we quantify and analyze the memory usage in each phase of long-context Transformer training step. We identify the factors that impose the biggest memory bottlenecks and cover the techniques to address them. Consider decoder-only Transformer with layers, with heads per layer, and GQA group size of G. Let the models hidden size be dmodel, per-head dimension dhead, the intermediate dimension of the feed-forward network df and the vocab size of the model . Finally, let sequence of length be the input to the model. For now, we keep the batch size equal to 1, since our focus is to establish how memory scales with respect to the sequence length S. We assume mixed precision setup with parameters, activations and gradients in bfloat16 precision, thus requiring 2 bytes per parameter for each tensor. However, some intermediate tensors still require 4 bytes, as discussed below. For decoder-only Transformer model, the sequence of operations during the forward pass can be broken down into 4 phases, as shown in Table 1. 1 Embedding: The input sequence of tokens is converted into embedding vectors of size dmodel, thus requiring 2 dmodel bytes of memory. 2 Attention: The input gets transformed into query Q, key and value vectors, requiring 6 dhead bytes. As we will discuss in Section 3.1, all-to-all requires the same amount of additional memory. Assuming Flash Attention (Dao et al., 2022) is used, the attention computation would not require any additional GPU HBM memory, except for storing the final output (and the final log-sum-exponent) vectors of size 2 dmodel (and 2 respectively). Typically, for most Transformer models, = dmodel/dhead, therefore this phase has total memory usage of 2 + (6 + 6) + 2 = 16 dmodel bytes. 3 FFN: Next, the attention output is fed to the feed-forward network with two layers, which projects the input with dimension dmodel into four intermediate tensors (assuming SwiGLU) of dimension df and then projects the intermediate tensors into the output of dimension dmodel. Typically, df 2.67 dmodel, which results in the total memory usage of 25 dmodel bytes. 4 Cross-Entropy Loss: At the end of the final layer, the output is converted into logits of shape , which are then used by the cross-entropy loss function. This phase has the most critical memory constraints, because the vocabulary size is typically very large ( 30 dmodel). Moreover, the cross-entropy calculation requires logits and intermediate log-softmax values to be casted to fp32, making the overall memory consumption 240 dmodel bytes. 2.3. Mitigating memory overheads From the above analysis, we can identify the most critical factors that become memory bottleneck when scaling the context length. Due to its memory consumption, the cross-entropy loss is the first obstacle. We can overcome it by computing the loss in tiled manner, materializing 3 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Table 2. Peak activation memory within the forward attention block under GQA. π represents the number of chunks in FPDT, ν represents the number of chunks in UPipe. UPipe consumes ν times less intermediate (QKV + all-to-all) tensor memory than Ulysses + offloading. FPDT has lower memory usage due to arbitrary chunk size, but suffers from performance degradation. Method"
        },
        {
            "title": "Ulysses",
            "content": "Ulysses + offloading"
        },
        {
            "title": "Untied Ulysses",
            "content": "Before attn block During inp all to all During attn kernel During out all to all C Cπ S C + (γ + 1) + (γ + 1) Cπ + (γ + 1) + (γ + 1) 2 Cν Cπ C + (γ + 1) + (γ + 1) (2γ + 1) Cπ + γ 2 Cν S + 2 3 2 Cπ + 2 Cν the intermediate tensors only one tile at time. Tiled loss kernels are available in libraries such as Liger-Kernel (Hsu et al., 2025), which we leverage in our experiments. To deal with the memory usage of the feedforward phase, we adopt similar approach to ALST by using tiled MLP function. Furthermore, we also use tiling for RMSNorm, since we found that to be more memory-efficient compared to using torch.compile on the RMSNorm module. Notably, we find that Rotary Positional Encoding (Su et al., 2023) also incurs memory overhead due to fp32 casting, which is why we use the fused RoPE implementation from the Flash Attention API which performs in-place operations to avoid huge memory spikes. Similarly to other prior works and libraries like Unsloth (Han et al., 2023), we manage the activation memory across layers via full activation checkpointing with CPU offloading. Finally, for the attention phase, we propose UPipe, as described in Section 3.3. Table 2 provides the memory consumption of different stages of the attention layer during the forward pass. In grouped-query attention, the size of key (K) and value (V ) tensors is reduced by factor of (the GQA ratio, = H/G). We define γ = 1 + 2 as the combined size of Q, K, (relative to S/C), and β = 4 + 4 as the combined size of the eight tensors required for the backward pass of the attention (Q, K, V, Out, dOut, dQ, dK, dV ). The constant factor of hidden size is omitted in the calculations for brevity. Table 6 presents similar breakdown for the backward pass of an attention layer. 3. Untied Ulysses As discussed above, the majority of activation tensors during training that contribute to peak memory usage can be tiled, recomputed or offloaded. In this section, we outline the design of UPipe, highlighting the key ideas that allow it to break the memory wall faced by current state-of-the-art methods in the attention stage. Moreover, our technique is more performant than prior methods when training on multi-million token sequences. Since our method builds on top of DeepSpeed-Ulysses (Jacobs et al., 2023), we will first elaborate on the mechanics of this algorithm, highlighting the opportunity for memory optimization. 3.1. DeepSpeed-Ulysses DeepSpeed-Ulysses (DS-Ulysses) is context parallelism technique that allows training of LLMs in distributed setup, particularly for long-context data. The technique works by uniformly sharding sequence of length across devices, so that every device has sequence of length S/C. For Transformer-based model, token-wise operations (feedforward layer, RMSNorm, cross-entropy) can be applied to every sequence shard without communication. However, the attention layer requires access to the entire sequence, so it cannot be executed independently on each shard. Ulysses solves this problem by rearranging the shards: afterwards, the memory occupied by the shards is the same as before, but now every device has access to the full sequence. Let us walk through how DeepSpeed-Ulysses works using an example from Figure 3a with = 2 devices. Initially, the sequence of size [S, dmodel] is sharded on two devices: X0 and X1 with size [ 2 , dmodel] each. These shards then undergo the QKV projection to produce the query, key and value tensors. In this example, let us consider the number of heads = 4, which means the query, key and value tensors have 4 heads: H0, H1, H2, H3 with dhead = dmodel/4. Hence, the size of these tensors is [ 2 , 4, dhead]. Next, DS-Ulysses performs an all-to-all operation inp all to all on the QKV tensors to reshard them: switching the sharding from the sequence length dimension to the head dimension. Thus, QKV tensors are resharded from [ 2 , 4, dhead] to [S, 2, dhead]. As result, device 0 now has access to the full sequence for heads H0, H1, while device 1 has access to the full sequence for heads H2 and H3. After attention, device 0 computes outputs for heads H0, H1 (and device 1 computes outputs for H2 and H3), which are then sharded back to the original shape. Therefore, another all-to-all operation out all to all is performed, so that the final outputs O0 and O1 are of shape [ 2 , dmodel] each. Because of all-to-all communication, DS-Ulysses has constant communication volume with respect to the number of devices, which makes it more scalable than Ring Attention. However, it faces memory bottleneck for long-context training, because the overhead from the communication buffers can grow to become significant. 4 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Figure 3. Illustration of (a) DeepSpeed-Ulysses and (b) UPipe designs. UPipe processes attention in headwise untied manner, so that in each stage, attention is performed only on subset of heads. This allows memory reuse across different stages, significantly reducing the peak memory usage due to attention activations. HBM (High-Bandwidth Memory) usage illustrates memory utilization due to the intermediate buffers and omits other components for brevity. 3.2. Memory Usage The memory usage peak in DeepSpeed-Ulysses occurs during the inp all to all operation because of the memory buffers for query, key, and value tensors and similarly sized all-to-all buffers. From Table 1, we can see that the memory usage due to these intermediate tensors is proportional to H. Our key insight is that for long sequences and large enough models, subset of heads is enough to reach the compute-bound regime. Hence, we process only heads at time (U < H) so that after the input all-to-all, each device has U/C heads. Because the memory usage of intermediate tensors is proportional to the number of heads, the memory overhead reduces from O(H) to O(U ). As we show later, changing allows for natural runtime-memory tradeoff. 3.3. Untied Ulysses: UPipe The biggest memory overhead in DS-Ulysses comes from storing the full-head (i.e., all heads) QKV tensors, along with the additional full-head all-to-all buffers. UPipe addresses this by untying the entire attention execution in an end-to-end manner. We propose headwise chunking scheme for performing attention, which processes only subset of heads at time, reducing the peak activation memory. To realize the memory benefits in practice, we also pro5 pose GQA-scheduling technique which processes attention heads out-of-order and prevents redundant communication. , 0 , and 0 Formally, given model with attention heads and number of context parallel devices C, UPipe chunks the attention execution into H/U stages, processing heads per stage. During forward pass, the execution begins by projecting the input into Q0 i.e., the first heads. This is followed by inp all to all, resulting in U/C heads per device. Note that must be divisible by to ensure that each device processes an integer number of heads. In the next stage, we process the next heads while reusing the memory buffers from the previous stage (i.e., use Q0 buffers to store Q1 and similarly for other tensors). Therefore, the memory usage remains O(U ) throughout the execution. Let us now walk through the example in Figure 3b. With = 4, = 2, and = 2. UPipe performs attention over H/U = 2 stages: processing 2 heads per stage. Consider the execution on Device 0: in stage 0, the input X0 is projected into the first two heads H0, H1 of QKV . Next, inp all to all is performed on H0, H1 so that device 0 has H0 for the entire sequence. Notice that during allto-all, we only need buffers for 2 heads (as opposed to 4 heads in DS-Ulysses). We then perform attention on H0, Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Figure 4. Illustration of UPipes GQA scheduling algorithm. We communicate as many unique key/value heads as possible along with the corresponding queries in stage-0. In the subsequent stages, we only communicate the next queries of the corresponding groups, reusing the key/value tensors from stage-0 until stage-G, where is the group size. generate output for head 0, and perform out all to all. In the second stage, X0 is projected into the next two heads H2, H3. At this point, heads H0, H1 are already processed so we reuse their HBM buffers to store H2, H3. Next, we perform all-to-all and similarly reuse the all-to-all buffers from stage-0. For the final output, we initialize the buffers in the beginning and fill them during execution. This avoids the concatentation of individual chunks, which otherwise degrades performance. 3.4. Memory Savings For heads and devices used for context parallelism, DS-Ulysses has total memory usage of: 6 dhead bytes for the QKV tensors and the same number of bytes for the all-to-all communication buffers, resulting in total of 12 dhead bytes of intermediate tensors. In case of UPipe, we can replace with , because single stage processes heads. Thus, the memory usage is 12 dhead bytes. To maximize memory savings, we want to be as small as possible, and the smallest valid value is = C. In this setting, the maximum memory usage of UPipe during the attention stage becomes 12 dhead. Thus, when = C, the peak activation memory usage of our method is independent of the number of heads. Taking Qwen3-32B as an example with = 64 and using single 8H100 node (i.e., = 8), DS-Ulysses requires 96 dhead. By contrast, UPipe requires 12 dhead, lowering the memory usage of intermediate activations for self-attention by 87.5%. 4. Implementation We use TorchTitan (Liang et al., 2025) as the training framework to integrate the implementation of UPipe, due to its flexibility and performance. We use the same framework for Unified Sequence Parallelism (USP) baselines to ensure fair comparison with respect to the training setup. For the core Transformer functionality, we override the attention module with our implementation, and we replace FeedForward (FFN), RMSNorm and CELoss modules with publicly available optimized implementations (ALST, Liger-Kernel). We integrate the core transformer modules inside TorchTitan via simple drop-in replacement of the existing modules. We use the hybrid attention implementation from USP as the baselines for Ulysses and Ring Attention, which includes load balanced zig-zag Ring Attention implementation. UPipe uses the same code structure as USP, so it can be easily extended for hybrid (Ulysses+Ring) setup. For allto-all communication, we use the non-QKVPacked variant from USP, which communicates queries, keys, and values sequentially to avoid memory overhead from simultaneous communication. For operations independent along the sequence length, such as FFN and RMSNorm, we use TiledCompute, tiling mechanism introduced by ALST. To determine the tile size, we use the same method as the authors of ALST by using square tile of size [dmodel dmodel]. We use FusedLinearCrossEntropyLoss from LigerKernel for memory-efficient loss computation. This kernel fuses the final linear projection with the cross-entropy loss, computing logits and loss in tiled manner to avoid materializing the full fp32 logits tensor in memory. 4.1. GQA Scheduling Because of its memory benefits, Grouped Query Attention (GQA, Ainslie et al., 2023) is an important architectural component of modern Transformer models (Grattafiori et al., 2024; Jiang et al., 2023; Yang et al., 2025). To ensure the GQA benefits are retained, we make our design GQAcompatible. This requires rearranging the query tensors to reuse the KV tensors communicated in the previous stage. Assuming = C, recall that UPipe processes heads per stage, so that after all-to-all, every device performs attention on single head. For GQA model with group size G, only C/G of the first heads are unique. For example, 6 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking consider = 4 and = 4. With naive processing, stage0 would process query heads Q0, Q1, Q2, Q3 with the corresponding key heads K0, K0, K0, K0. Since each device communicates 1 query, key and value heads per stage, the total communication volume across H/C stages is O(3 1). Referring to Figure 4, one might observe that we could communicate K0, K1, K2, K3 (i.e., all the unique key tensors) in stage 0. We would then need to send the corresponding query tensors (i.e., Q0, Q4, Q8, Q12). However, in the next stage, we do not need to communicate any key (or value) tensors. We can now choose the next query tensors from the groups (i.e., Q1, Q5, Q9, Q13), because the corresponding key (and value) tensors were already communicated in stage 0. In this case, for every stages, we communicate 1 query, key and value heads in the first stage, and only 1 query heads in the subsequent stages. Thus, the total communication volume is O((3 + 1) 1) which is always less than the naive processing (since > 1). 5. Experiments We now present the experimental results using UPipe, comparing it with multiple SOTA baselines, as well as standard context parallelism approaches. We run the experiments on two different model families across multiple context lengths, comparing the maximum context length supported by different techniques and their throughput in each setup. For all experiments with UPipe, we set the chunk size to be the same as the number of context-parallel devices (U = C), to demonstrate the highest memory efficiency. 5.1. Setup System Specifications: We run our experiments on NVIDIA 8H100 nodes with 80GiB of HBM3 on-chip DRAM per GPU, connected via 4th generation NVLink with 900GBps bidirectional bandwidth for intra-node communication, and Mellanox Infiniband with 400 Gbps bidirectional bandwidth for inter-node communication. We use 64-core Intel x86 CPU per node with 1.9TiB RAM. Models: We use Llama3-8B (Grattafiori et al., 2024) and Qwen3-32B (Yang et al., 2025) models to run our experiments. Llama3-8B has = 32 query heads (and 8 keyvalue heads), thus allowing UPipe to process attention in H/C = 4 stages. Qwen3-32B has 64 query heads (and 8 key-value heads), so UPipe processes it in 8 stages. Note that we use = 8 here, because we always restrict Ulysses context parallelism degree to 8 and use rest for ring (in USP-Hybrid style setup as discussed in 5.2.1). Framework: We use TorchTitan as the distributed training framework, and Flash Attention-3 (FA3) (Shah et al., 2024) for all the experiments. Note that FPDT does not natively support FA3, so we patched their implementation with FA3 support for fair comparison against UPipe. Distributed Setup: We use context parallelism over 8H100s for our training. We use PyTorchs Fully Sharded Data Parallel (Zhao et al., 2023) to distribute the parameters, gradients and optimizer states across all the GPUs. For efficient management of activation memory, we use full activation-checkpointing with CPU offloading, to maintain consistency with FPDT. For all sequence lengths except 5M, we allow the CPU offloaded activations to reside on the non-swappable CPU RAM by setting PIN MEMORY to True. For 5M, we set this to False due to the CPU RAM constraints (1.9TB). For better memory management, we set PYTORCH CUDA ALLOC CONF =expandable segments:True similar to ALST. 5.2. Baselines We compare UPipes performance and memory efficiency against multiple baselines. Fully Pipelined Distributed Transformer (FPDT) is long-context training recipe which processes attention by chunking along the sequence length dimension and using online softmax to perform full attention. It mitigates the prohibitive memory requirements by asynchronously offloading chunks to CPU, keeping only the necessary chunks on GPU, and using double buffer mechanism to overlap communication with computation. It can support maximum sequence of 4M tokens when training Llama3-8B on single 8H100. For standard Ulysses and Ring attention baselines, we use USP implementation, as it provides an easy-to-use interface for modifications. Additionally, we also compare against the ring implementation in native PyTorch. We omit Arctic Long Sequence Training (ALST) in our experiments because our modified version of USP-Ulysses (which uses DS-Ulysses, offloaded activation checkpointing, and tiling for MLP / CELoss) resembles with the ALST design. Moreover, unlike ALST, we do not offload optimizer states onto the CPU in any of the experiments to avoid throughput degradation. 5.2.1. MULTI-NODE TRAINING For multi-node experiments, we use similar setup as USPHybrid, using 8-ulysses-2-ring, that is, Ulysses over 8 GPUs within the node, and Ring across 2 nodes. This is common setup for hybrid context parallelism (e.g. Team Wan et al., 2025) to allow faster all-to-all for Ulysses within the node, and slower ring communication across nodes. Note that FPDT does not support hybrid sequence parallelism, so we use the standard 16-ulysses-1-ring setup. We also compare against ring-only baselines: USP-Ring and native PyTorch ring. Both these baselines use zigzag load balancing, ensuring fair work distribution across all ranks. 7 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Table 3. Throughput comparison (Tokens/second/GPU) for Llama3-8B (8H100s) and Qwen3-32B (16H100s) across varying sequence lengths. By reducing the peak memory usage and eliminating CUDA allocation retries, our approach achieves the highest throughput for sequence lengths 2M. Notably, on single 8H100 node, our method scales Llama3-8B to 5M-token sequence lengtha 25% improvement over the previous state-of-the-art. OOM: Out of Memory. Note: FPDT execution fails at lengths > 4M."
        },
        {
            "title": "Model",
            "content": "128K 256K 512K 1M 2M 3M 4M 5M Native PyTorch 1373.87 2064.90 2320.47 1171.68 2281."
        },
        {
            "title": "Ring\nUlysses\nFPDT\nUPipe",
            "content": "8 - 3 l Native PyTorch 127.03 418.39 545.29 286.40 483."
        },
        {
            "title": "Ring\nUlysses\nFPDT\nUPipe",
            "content": "2 3 - 3 Q 845.99 1387.67 1503.80 884.75 1487.29 112.20 308.88 370.70 217.85 339.56 474.30 841.05 878.63 621.20 867.17 91.39 194.44 217.04 151.91 204.46 249.85 458.51 475.33 382.42 472. OOM 110.27 117.02 95.88 113.26 OOM 237.99 246.05 219.53 246.07 58.45 59.98 55.41 59.56 159.96 162.41 153.48 166.32 OOM OOM 38.86 40.42 OOM OOM 119.76 125. 27.66 29.97 98.25 OOM 5.3. Performance 5.3.1. SINGLE-NODE TRAINING Llama3-8B: Table 3 (top) reports single-node throughput (tokens/second/GPU) for Llama3-8B across increasing sequence lengths. Ulysses delivers strong throughput by using single all-to-all per attention, but its full-head QKV and communication buffers make activation memory the primary limiter at multi-million-token contexts. FPDT can push context length further by tiling attention and offloading to CPU, but it pays substantial throughput penalty from frequent CPUGPU transfers. UPipe bridges this gap: at shorter sequences it is slightly slower than Ulysses due to additional stage launches, but this overhead is amortized as context grows (as shown in Table 5). At longer sequences ( 2M), UPipe matches Ulysses throughput while using much lower GPU memory, enabling training at 5M tokens on single 8H100 node. In effect, UPipe preserves the throughput of Ulysses while delivering memory efficiency closer to offloading-based approaches, improving the maximum single-node context length over FPDT (4M) by 25%. 5.3.2. MULTI-NODE TRAINING Llama3-8B: Figure 5 shows the comparison of UPipe and USP-Hybrid on 16H100 setup. UPipe is more memory efficient than USP-Hybrid at all sequence lengths from 512K to 6M tokens, and supports context lengths upto 8M tokens, improving upon USP-Hybrid by 33%. The throughput of UPipe is comparable to USP-Hybrid at all sequence lengths, highlighting UPipes runtime efficiency. Qwen3-32B: Table 3 (bottom) summarizes the throughput comparison of different methods when training Qwen3-32B on 16H100 setup. UPipe outperforms all other methods Figure 5. Llama3-8B: Peak GPU memory usage and throughput (normalized w.r.t USP-Hybrid) comparison of UPipe and USPHybrid at different sequence lengths on 16H100s. Our method significantly outperforms USP-Hybrid in terms of memory efficiency, and allows maximum context size of 8M, improving upon USP-Hybrid (6M tokens) by 33%, while maintaining throughput. for sequences 2M. Notably, UPipe always outperforms FPDT across all sequence lengths in terms of throughput. In terms of maximum sequence length, our method can support 4M token sequences, 2 more than Ulysses (2M tokens), while delivering 8.3% better performance than FPDT. Additionally, we also report the maximum allocated memory in Appendix Table 4. UPipe provides best memory efficiency compared to other methods except FPDT. However, FPDT suffers from performance degradation due to frequent CPU transfers. Nevertheless, our method should be composable with FPDT due to orthogonal chunking dimensions, allowing benefits from both the methods. For the above experiments, our aim was to show the maximum memory efficiency of UPipe so we chose = C. 8 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking"
        },
        {
            "title": "References",
            "content": "Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https://arxiv.org/abs/ 2305.13245. Bekman, S., Rajbhandari, S., Wyatt, M., Rasley, J., Ruwase, T., Yao, Z., Qiao, A., and He, Y. Arctic long sequence training: Scalable and efficient training for multi-million token sequences, 2025. URL https://arxiv.org/ abs/2506.13996. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Chia, Y. K., Cheng, L., Chan, H. P., Liu, C., Song, M., Aljunied, S. M., Poria, S., and Bing, L. M-longdoc: benchmark for multimodal super-long document understanding and retrieval-aware tuning framework, 2024. URL https://arxiv.org/abs/2411.06176. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Re, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Fang, J. and Zhao, S. Usp: unified sequence parallelism approach for long context generative ai, 2024. URL https://arxiv.org/abs/2405.07719. Gemini Team. Gemini 3.0: new era of intelligence with gemini 3. Google DeepMind, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, Figure 6. Llama3-8B: Ablation analysis of UPipes head-chunk size , with 512K context size on = 4 GPUs. Smaller chunk size yields better memory efficiency at the cost of throughput and vice versa, allowing trade-off between memory and throughput. Next, we present an ablation on discussing the associated tradeoffs between memory and throughput. 5.4. Ablation on the head-chunk size As shown in Figure 6, we perform an ablation study on UPipes hyperparameter i.e., the number of heads processed per stage. It allows memory-runtime tradeoff, where larger corresponds to more heads processed per stage, resulting in higher memory usage and lower runtime. Conversely, when = C, UPipe provides maximum memory benefits, at the cost of slight performance degradation due to kernel launch overhead (though at longer context lengths, this overhead is amortized by the increased length as shown in Table 5). We perform the ablation using Llama3-8B on 4H100 GPUs, with context size of 512K tokens. 6. Conclusion In this paper, we introduced UPipe, an untied variant of Ulysses that reduces attention activation memory by chunking attention end-to-end at the head level. Our experimental evaluation demonstrates that these memory gains translate into practical context scaling without sacrificing throughput. On Llama3-8B, UPipe reaches 5M tokens on single 8 H100 node (25% beyond FPDT), and scales to 8M tokens on 16 H100 while maintaining throughput comparable to widely deployed baselines such as DeepSpeedUlysses and Unified Sequence Parallelism. On larger models, UPipe reduces attention intermediate tensor memory by up to 87.5% (e.g., Qwen3-32B at = 8), helping avoid allocation retries that otherwise degrades performance. Overall, UPipe provides practical path to pushing the context frontier: it is simple, compatible with existing contextparallel training pipelines, and delivers substantial memory efficiency improvements while preserving the throughput. We expect UPipe to serve as useful building block for future long-context systems, especially as models and modalities continue to demand larger effective context windows. 9 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Han, D., Han, M., and team, U. Unsloth, 2023. URL https://github.com/unslothai/unsloth. Hori, T., Moritz, N., Hori, C., and Roux, J. L. Advanced long-context end-to-end speech recognition using context-expanded transformers. ArXiv, abs/2104.09426, 2021. URL https://api.semanticscholar. org/CorpusID:233296591. 10 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking Hsu, P.-L., Dai, Y., Kothapalli, V., Song, Q., Tang, S., Zhu, S., Shimizu, S., Sahni, S., Ning, H., Chen, Y., and Wang, Z. Liger-kernel: Efficient triton kernels for LLM training. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. URL https: //openreview.net/forum?id=36SjAIT42G. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., Dang, K., Fan, Y., Zhang, Y., Yang, A., Men, R., Huang, F., Zheng, B., Miao, Y., Quan, S., Feng, Y., Ren, X., Ren, X., Zhou, J., and Lin, J. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. Jacobs, S. A., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. URL https: //arxiv.org/abs/2309.14509. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Jiang, Z., Ma, X., and Chen, W. Longrag: Enhancing retrieval-augmented generation with long-context URL https://arxiv.org/abs/ llms, 2024. 2406.15319. Kimi Team, Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., Feng, Y., Fu, K., Gao, B., Gao, H., Gao, P., Gao, T., Gu, X., Guan, L., Guo, H., Guo, J., Hu, H., Hao, X., He, T., He, W., He, W., Hong, C., Hu, Y., Hu, Z., Huang, W., Huang, Z., Huang, Z., Jiang, T., Jiang, Z., Jin, X., Kang, Y., Lai, G., Li, C., Li, F., Li, H., Li, M., Li, W., Li, Y., Li, Y., Li, Z., Li, Z., Lin, H., Lin, X., Lin, Z., Liu, C., Liu, C., Liu, H., Liu, J., Liu, J., Liu, L., Liu, S., Liu, T. Y., Liu, T., Liu, W., Liu, Y., Liu, Y., Liu, Y., Liu, Y., Liu, Z., Lu, E., Lu, L., Ma, S., Ma, X., Ma, Y., Mao, S., Mei, J., Men, X., Miao, Y., Pan, S., Peng, Y., Qin, R., Qu, B., Shang, Z., Shi, L., Shi, S., Song, F., Su, J., Su, Z., Sun, X., Sung, F., Tang, H., Tao, J., Teng, Q., Wang, C., Wang, D., Wang, F., Wang, H., Wang, J., Wang, J., Wang, J., Wang, S., Wang, S., Wang, Y., Wang, Y., Wang, Y., Wang, Y., Wang, Y., Wang, Z., Wang, Z., Wang, Z., Wei, C., Wei, Q., Wu, W., Wu, X., Wu, Y., Xiao, C., Xie, X., Xiong, W., Xu, B., Xu, J., Xu, J., Xu, L. H., Xu, L., Xu, S., Xu, W., Xu, X., Xu, Y., Xu, Z., Yan, J., Yan, Y., Yang, X., Yang, Y., Yang, Z., Yang, Z., Yang, Z., Yao, H., Yao, X., Ye, W., Ye, Z., Yin, B., Yu, L., Yuan, E., Yuan, H., Yuan, M., Zhan, H., Zhang, D., Zhang, H., Zhang, W., Zhang, X., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Y., Zhang, Z., Zhao, H., Zhao, Y., Zheng, H., Zheng, S., Zhou, J., Zhou, X., Zhou, Z., Zhu, Z., Zhuang, W., and Zu, X. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022. URL https://arxiv.org/abs/2205.05198. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., Gontier, N., Meade, N., Zebaze, A., Yee, M.-H., Umapathi, L. K., Zhu, J., Lipkin, B., Oblokulov, M., Wang, Z., Murthy, R., Stillerman, J., Patel, S. S., Abulkhanov, D., Zocca, M., Dey, M., Zhang, Z., Fahmy, N., Bhattacharyya, U., Yu, W., Singh, S., Luccioni, S., Villegas, P., Kunakov, M., Zhdanov, F., Romero, M., Lee, T., Timor, N., Ding, J., Schlesinger, C., Schoelkopf, H., Ebert, J., Dao, T., Mishra, M., Gu, A., Robinson, J., Anderson, C. J., Dolan-Gavitt, B., Contractor, D., Reddy, S., Fried, D., Bahdanau, D., Jernite, Y., Ferrandis, C. M., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder: may the source be with you!, 2023a. URL https://arxiv.org/abs/2305.06161. Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective, 2022. URL https://arxiv.org/abs/2105. 13120. Li, S., Xue, F., Baranwal, C., Li, Y., and You, Y. Sequence parallelism: Long sequence training from system perspective. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23912404, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.134. URL https: //aclanthology.org/2023.acl-long.134/. Liang, W., Liu, T., Wright, L., Constable, W., Gu, A., Huang, C.-C., Zhang, I., Feng, W., Huang, H., Wang, J., Purandare, S., Nadathur, G., and Idreos, S. Torchtitan: One-stop pytorch native solution for production ready LLM pretraining. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=SFN6Wm7YBI. Liu, H., Zaharia, M., and Abbeel, P. Ring attention with blockwise transformers for near-infinite context, 2023. URL https://arxiv.org/abs/2310.01889. 11 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yang, D., Wang, D., Guo, H., Chen, X., Wu, X., and Meng, H. M. Simplespeech: Towards simple and efficient text-to-speech with scalar latent transArXiv, abs/2406.02328, former diffusion models. 2024. URL https://api.semanticscholar. org/CorpusID:270226637. Yao, J., Jacobs, S. A., Tanaka, M., Ruwase, O., Subramoni, H., and Panda, D. K. Training ultra long context language model with fully pipelined distributed transformer, 2025. URL https://arxiv.org/abs/2408.16978. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304.11277. Sand.ai, Teng, H., Jia, H., Sun, L., Li, L., Li, M., Tang, M., Han, S., Zhang, T., Zhang, W. Q., Luo, W., Kang, X., Sun, Y., Cao, Y., Huang, Y., Lin, Y., Fang, Y., Tao, Z., Zhang, Z., Wang, Z., Liu, Z., Shi, D., Su, G., Sun, H., Pan, H., Wang, J., Sheng, J., Cui, M., Hu, M., Yan, M., Yin, S., Zhang, S., Liu, T., Yin, X., Yang, X., Song, X., Hu, X., Zhang, Y., and Li, Y. Magi-1: Autoregressive video generation at scale, 2025. URL https://arxiv.org/abs/2505.13211. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL https: //arxiv.org/abs/2407.08608. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Team Wan, Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models, 2025. URL https: //arxiv.org/abs/2503.20314. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wu, B., Zou, C., Li, C., Huang, D., Yang, F., Tan, H., Peng, J., Wu, J., Xiong, J., Jiang, J., Linus, Patrol, Zhang, P., Chen, P., Zhao, P., Tian, Q., Liu, S., Kong, W., Wang, W., He, X., Li, X., Deng, X., Zhe, X., Li, Y., Long, Y., Peng, Y., Wu, Y., Liu, Y., Wang, Z., Dai, Z., Peng, B., Li, C., Gong, G., Xiao, G., Tian, J., Lin, J., Liu, J., Zhang, J., Lian, J., Pan, K., Wang, L., Niu, L., Chen, M., Chen, M., Zheng, M., Yang, M., Hu, Q., Yang, Q., Xiao, Q., Wu, R., Xu, R., Yuan, R., Sang, S., Huang, S., Gong, S., Huang, S., Guo, W., Yuan, X., Chen, X., Hu, X., Sun, W., Wu, X., Ren, X., Yuan, X., Mi, X., Zhang, Y., Sun, Y., Lu, Y., Li, Y., Huang, Y., Tang, Y., Li, Y., Deng, Y., Zhou, Y., Hu, Z., Liu, Z., Yang, Z., Yang, Z., Lu, Z., Zhou, Z., and Zhong, Z. Hunyuanvideo 1.5 technical report, 2025. URL https://arxiv.org/abs/2511.18870. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, 12 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking A. Appendix A.1. Single Node Memory Comparisons Table 4. Memory comparison (GiB) for Llama3-8B (8H100s) and Qwen3-32B (16H100s) across varying sequence lengths. UPipe processes attention in headwise chunked manner, reducing peak memory usage during training. This leads to practical advantage of longer sequence length support on Llama3-8B (upto 5M tokens), improving by 25% over prior SOTA (FPDT: 4M tokens). OOM: Out of Memory. Note: FPDT execution fails at lengths > 4M."
        },
        {
            "title": "Model",
            "content": "128K 256K 512K 1M 2M 3M 4M 5M Native-PyTorch 25.32 21.32 21.26 21.73 21."
        },
        {
            "title": "Ring\nUlysses\nFPDT\nUPipe",
            "content": "8 - 3 l Native-PyTorch 45.81 40.14 40.13 38.94 39."
        },
        {
            "title": "Ring\nUlysses\nFPDT\nUPipe",
            "content": "2 3 - 3 Q 31.40 23.40 23.02 22.50 22.30 53.69 41.16 41.16 39.47 40.84 43.55 27.58 26.80 24.03 24.70 69.47 44.22 44.10 40.54 42.72 67.86 35.86 34.35 27.09 29. OOM 50.51 50.27 42.66 46.84 OOM 52.49 49.49 35.17 40.50 63.11 62.60 46.91 55.65 69.11 64.55 43.35 51.10 OOM OOM 52.27 64.47 OOM OOM 51.42 61. 57.77 73.28 72.30 OOM Table 4 shows the memory comparison of various context parallelism schemes on Llama3-8B and Qwen3-32B models across different context lengths. FPDT shows best memory usage due to arbitrary chunk size setting, but performs poorer due to CPU overheads. UPipe has better memory efficiency than all other methods, while also matching the throughput with respect to Ulysses. Note that while FPDT reports lower allocated memory, it is unable to run with context length > 4M. A.2. Runtime comparison between DS-Ulysses and UPipe Table 5. Runtime comparison (in seconds) for Llama3-8B on 8H100 between DS-Ulysses and UPipe across varying sequence lengths. FA3-Fwd/Bwd: Total flash-attention-3 forward and backward kernel time. The runtime refers to the total time for single training step. 128K 256K 512K 1M 2M 3M All-to-All 0.40 FA3-Fwd 1.58 FA3-Bwd 2.40 3.03 Other 7.40 Total y - p All-to-All 0.46 FA3-Fwd 1.51 FA3-Bwd 2.41 2.82 Other 7.20 Total 0.90 6.35 9.13 5.33 21.72 1.10 6.38 9.25 5.23 21. 1.68 25.71 36.74 10.08 74.21 2.43 25.93 36.99 10.10 75.45 4.93 103.49 146.86 19.78 275.06 5.52 103.92 147.37 19.58 276.39 16.30 421.67 588.73 41.30 1068.00 17.12 417.55 590.79 37.76 1063. 42.21 995.92 1324.71 56.31 2419.14 34.34 940.62 1330.76 55.52 2361.24 Table 5 shows the runtime comparison for Llama3-8B on single 8H100 node for DS-Ulysses and UPipe. It shows the breakdown of runtime into major components of single training step: flash-attention-3 forward time, flash-attention-3 backward time, and All-to-All communication time. Note that UPipe has higher runtime at lower sequence lengths due to multiple kernel launches. However, this is amortized at higher sequence lengths due to enough work per kernel launch saturating the GPU. 13 Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking A.3. GQA backward-attention memory Table 6. Peak activation memory within the backward attention block under GQA. Note: π represents the number of chunks in FPDT, ν represents the number of chunks in UPipe. As shown, UPipe consumes ν times lesser intermediate (QKV + all-to-all) tensor memory than Ulysses + offloading. FPDT has lower memory usage due to arbitrary chunk size, but suffers from performance degradation."
        },
        {
            "title": "Ulysses",
            "content": "Ulysses + offloading"
        },
        {
            "title": "Before Bwd Attn During out all to all During Bwd Attn Kernel During inp all to all",
            "content": "(L + 1) C"
        },
        {
            "title": "2 S\nC\nS\nC·π\n2 S\nC",
            "content": "(L + 2) C"
        },
        {
            "title": "3 S\nC\n3 S\nC·π\nC + 2 S\n2 S",
            "content": "Cν (L + β + 1) (β + 2) (β + 2) Cπ + (β + 1) 2 Cν (L + γ + 1) (γ + 2) (γ + 2) Cπ + 2(γ + 1) 2 Cν"
        }
    ],
    "affiliations": [
        "Together AI"
    ]
}