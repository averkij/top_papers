{
    "paper_title": "EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation",
    "authors": [
        "Zongyang Qiu",
        "Bingyuan Wang",
        "Xingbei Chen",
        "Yingqing He",
        "Zeyu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation."
        },
        {
            "title": "Start",
            "content": "EmoVid: Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation Zongyang Qiu1,2*, Bingyuan Wang1*, Xingbei Chen1, Yingqing He3, Zeyu Wang1,3 1The Hong Kong University of Science and Technology (Guangzhou), China 2Fudan University, China 3The Hong Kong University of Science and Technology, Hong Kong SAR, China zyqiu22@m.fudan.edu.cn, bwang667@connect.hkust-gz.edu.cn, xchen053@connect.hkust-gz.edu.cn, yhebm@connect.ust.hk, zeyuwang@ust.hk 5 2 0 2 4 1 ] . [ 1 2 0 0 1 1 . 1 1 5 2 : r Abstract Emotion plays pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotionannotated video dataset specifically designed for artistic media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes new benchmark and protocol for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation. Project Page https://zane-zyqiu.github.io/EmoVid Introduction Video is powerful medium for storytelling and expression, with emotion playing key role in viewer engagement (Cao et al. 2022). While recent video generation models have improved in visual coherence and motion, they have paid limited attention to emotional expressiveness (Kalateh et al. 2024). This is especially true in creative applications like comic portrait animation, sticker (meme) creation, and cinematic editing, where emotional expressiveness is essential but underexplored (Wang, Chen, and Wang 2025). In recent years, emotional content analysis and generation have achieved great progress in language, speech, and *These authors contributed equally. Corresponding author. images, and have gained considerable attention within multimodal contexts (Kalateh et al. 2024). In video, multimodal approaches have shown promising results in improving tasks such as sentiment analysis, emotion-driven content creation, and interactive video generation (Pandeya and Bhattarai 2021). However, most of these studies focused on human dialogue and realistic styles. The integration of emotion in stylized video understanding and generationtaking into account both creative context, stylistic features, and the emotional undercurrentsremains underexplored. In this paper, we introduce EmoVid, the first large-scale and emotion-labeled video dataset focusing on stylized and non-realistic content. As shown in Figure 1, EmoVid consists of videos in three categories: cartoon animation, movie clips, and animated stickers (GIFs). We adopt the Mikels eight-emotion scheme (Mikels et al. 2005) (amusement, awe, contentment, excitement, anger, disgust, fear, sadness), widely used discrete set originally curated for affective image studies. Each clip is annotated with emotion labels, color attributes, and captioned via vision-language model (VLM). Through this dataset, we explored the emotional patterns in videos, such as emotion distributions, coloremotion correlations, temporal transitions of emotion, and semantic links. We believe that these insights can enhance tasks like comic animation and stylized video generation. To demonstrate the effectiveness of EmoVid, we propose benchmark for both T2V and I2V tasks, evaluating the visual quality and emotion accuracy of AI-generated videos. We also fine-tune the Wan2.1 (Wan et al. 2025) model on our data. Experiments show significant gain in emotional expressiveness when emotion is explicitly incorporated as prior into video generation tasks. We also designed video generation pipeline, which can generate animated stickers of any character with any emotion. This is of great value for todays network communication and can also be further used in the production of animations and movies. Together, EmoVid contributes to both affective computing and stylized video generation by linking emotion understanding with practical generative tasks. In summary, we make the following contributions: We introduce EmoVid, large-scale, emotion-labeled video dataset focusing on stylized and non-realistic content, and present scalable benchmark, evaluation metFigure 1: Overview of the EmoVid dataset. The dataset spans eight emotion categoriesContentment, Awe, Amusement, Excitement, Sadness, Disgust, Fear, and Angerand three content domains: Animation, Movie, and Sticker. The dataset captures diverse emotional expressions in various visual styles and contexts, demonstrating both multimodal richness (with associated text and audio) and cross-domain generality. rics, and protocol for emotional enhancement in video generation. We explore both spatial and temporal emotional patterns in the EmoVid dataset, as well as their relationship with text captions or other visual attributes. We demonstrate EmoVids utility in generation and editing tasks by fine-tuning the Wan2.1 model, which shows significant improvement in emotional expression. modeling transitions and nuances in affect. Consequently, video-based affective computing has gained traction, with datasets such as AffectNet and LIRIS-ACCEDE supporting sequence-based modeling (Mollahosseini, Hasani, and Mahoor 2017; Baveye et al. 2015). Despite progress, few efforts have been made to recognize video emotions within creative domains, such as film, performance, or storytelling, where affect is most deeply embedded and semantically rich."
        },
        {
            "title": "Video Generation and Editing",
            "content": "Emotion Analysis and Affective Computing Affective computing has gained increasing attention in recent years, particularly in textual, auditory, and visual modalities. Early research mostly focused on text sentiment analysis using lexical features and semantic understanding (Wang et al. 2025b). Subsequently, auditory emotion recognition emerged as reliable signal through speech prosody, pitch, and tone analysis (Zadeh et al. 2016). Visual emotion recognition, especially via facial expression and gesture, has gained momentum more recently but remains less mature in comparison to text and audio (Zhu et al. 2024). More recently, multimodal affective computing has become key focus (Das and Singh 2023), but comprehensive multimodal tasks remain scarce due to challenges in data alignment, modality imbalance, and dataset availability. Within the visual modality, affective computing first progressed on static images, exploring the affective content of images via color histograms, facial attributes, and compositional cues (Pang, Zhu, and Ngo 2015). Nevertheless, static images lack temporal dynamics, which is essential for Recent advances in video generation have shown remarkable capability across diverse domains, such as human motion synthesis (Tulyakov et al. 2018), natural scene rendering (Wang, Liu et al. 2018), and short video creation (Ho, Saharia et al. 2022). In creative applicationssuch as animation, film production, and meme/sticker creation generative models like VideoCrafter (Yang, Xu et al. 2023) have demonstrated strong visual coherence and temporal smoothness, but current research primarily focuses on visual quality, realism, or aesthetic control, with little emphasis on affective expressiveness (Ma et al. 2025). Notably, meme or sticker generation inherently carries affective signals, yet emotional intent is typically implicit and lacks formal integration into generation frameworks. While affect has been discussed in video synthesis through domains such as facial expression transfer (Zakharov et al. 2019), and gesture-guided animation (Cao, Yang et al. 2022), these discussions are often limited to human-centric or conversational tasks. Affective conditioning has been explored via latent space alignment (Ji, Wang Dataset Modalities Size Content Emotion Labels CAER (Lee et al. 2019) MELD (Poria et al. 2019) DEAP (Koelstra et al. 2011) VEATIC (Ren et al. 2024) MEAD (Wang et al. 2020) DH-FaceEmoVid-150 (Liu et al. 2025) EmoVid (Ours) v, v, a, v, v, v, v, a, TV shows 12h (13k clips) 1.4h (1.4k clips) Human dialogue Music videos 2h (120 clips) In-the-wild 3h (124 clips) 40h (10k clips) Human face 150h (18k clips) Human face 39h (22k clips) Animation, Movie, Sticker 7 cls 7 cls V-A-D V-A 7 cls + 3 intensity levels 6 cls + 4 compound 8 cls (Mikels) Table 1: Comparison of EmoVid with other emotional video datasets. We focus on modalities, size, content types, and emotion label schemes. Modalities are abbreviated as follows: = Video, = Audio, = Text. Emotion labels include either discrete categories (e.g., 7 cls = 7 emotion classes) or dimensional annotations such as ValenceArousal (VA) and ValenceArousalDominance (VAD). et al. 2023) or emotion labels in prompt-based generation (Guo, Huang et al. 2023), yet these approaches rarely address stylized and non-realistic domains where emotion is central to narrative structure, such as animated films or cinematic scene generation (Wang et al. 2025a). The gap between affective modeling and creative video generation suggests pressing need to bridge semantic emotion representation with generative visual storytelling. Emotion-related Datasets Affective computing has been supported by growing number of emotion datasets across textual, auditory, and visual modalities. Text-based corpora such as SemEval (Rosenthal, Farra, and Nakov 2017) and GoEmotions (Demszky, Movshovitz-Attias et al. 2020) provide fine-grained emotion labels for sentiment and intent understanding. In the auditory domain, datasets like RAVDESS (Livingstone and Russo 2018) and IEMOCAP (Busso et al. 2008) include speech with emotion expressions. Visual emotional datasets evolved from static-image datasets such as Emotion6 (Peng, Wang et al. 2015) and EmoSet (Yang et al. 2023). However, static images lack temporal continuity, limiting their utility in studying emotional dynamics and transitions. This has motivated the development of video-based datasets for affective modeling. Several datasets have attempted to bridge the gap between affective labeling and video modality. MELD (Poria et al. 2019), DEAP (Koelstra et al. 2011), and VEATIC (Ren et al. 2024) offer emotion-annotated videos, but are limited in either modality (e.g., lacking audio and text), domain focus (e.g., only dialogues or music videos), or size. Facial datasets such as MEAD (Wang et al. 2020) and DHFaceEmoVid-150 (Liu et al. 2025) focus on constrained emotional expressions, offering high precision for facial affect but limited diversity in content and setting. These datasets are suitable for emotion recognition but not ideal for video generation, where varied visuals, rich context, and narrative emotional arcs are critical. In contrast, EmoVid introduces large-scale, multimodal (video, audio, text) dataset with 22,758 clips covering animation, film scenes, and stickersdomains where emotional content is not only embedded but essential to semantics. The EmoVid Dataset As discussed in the former sections, the primary challenge of emotion-enhanced video generation lies in the lack of stylized emotional video datasets. We analyzed previous datasets specifically in emotion and video-related fields, and summarized their features in terms of size, category, content, and emotion labels, as shown in Table 1. Through the analyses, we find that existing datasets either lack in scale or fail to include all necessary modalities, which hinders the progress of multimodal emotion analysis. Moreover, they all focus exclusively on real-world scenarios (primarily human facial expressions), limiting the effective transfer of emotional priors to general video generation tasks. Data Collection To fill this gap, we created the EmoVid dataset, the first large-scale multimodal video dataset with fine-grained emotion labels, which comprises 2,807 animation face clips, 13,255 movie clips, and 6,696 animated stickers. The dataset includes high-quality emotional annotations, as well as other relevant visual attributes such as brightness, colorfulness, and hue, along with textual caption for each video clip. Basic information is provided in Table 2, and more detailed information can be found in the appendix. Type Total Animation Movie Sticker Clips 22758 2807 13255 6696 Avg 6.18 5.12 8.75 2.91 SD Vid Aud Cap 4.53 2.65 4.71 2.18 Table 2: Basic statistics of the EmoVid dataset. Avg is the average duration (in seconds), and SD is the standard deviation. Vid, Aud, and Cap indicate whether each clip includes video, audio, and textual caption, respectively. For the animation clips, we source data from the MagicAnime dataset, which contains 3,000 clips of cartoon faces from American, Chinese, and Japanese cartoons (Xu et al. 2025). The movie clips are retrieved using the metadata and code provided by Condensed Movies (Bain et al. 2020). As movie videos are mostly several minutes long, we segment them using the PySceneDetect tool (Castellano 2025). Only clips within 430 seconds were retained for further analysis. For the animated stickers, we used the Tenor API (Tenor, Inc. 2025) to search for GIFs based on the eight primary emotion labels and their synonyms summarized by Yang et al. (2023). Each clip is manually verified to ensure it accurately expresses the intended emotion. More details are included in the appendix. Data Labeling Videos in EmoVid are annotated at the clip level, and the annotation includes the following aspects: Emotion. We employ the widely-used Mikels emotion model (Mikels et al. 2005), which categorizes emotions into eight types: amusement, awe, contentment, excitement, anger, disgust, fear, and sadness. As the Valence-Arousal Model (Russell 1980) also plays an important role in intuitive understanding of emotions, we sorted the valence and arousal of the eight emotions as in Figure 2 according to the work of Warriner, Kuperman, and Brysbaert (2013). Given the trade-off between labeling accuracy and resource consumption, we adopt human-machine collaborative method to obtain the labels. We first conducted comparative experiment on the EmoSet dataset (Yang et al. 2023), the results of which are detailed in the appendix. We find that fine-tuning VLMs on the same data domain significantly improves emotion labeling accuracy, and NVILALite-2B (Liu et al. 2024b) exhibits classification performance comparable to that of humans. We pick 20% of the animation and movie data to be annotated by human annotators, with each clip tagged as one of the eight emotions or as no specific emotion. As emotions are ambiguous and open to interpretation, each video is annotated by three people, and the video is retained only when at least two annotators provide the same result. For the remaining 80% of the data, we use the NVILA-Lite-2B model (fine-tuned on the manually labeled data) to annotate the clips. To assess annotation quality, we randomly select 1% of the videos as validation set. Three human annotators independently annotate the same set. We then calculated pairwise Cohens kappa scores across the four annotation sources (three humans and the VLM). The results indicate small difference (< 4%) between the inter-human kappas and the human-VLM kappas, indicating that the VLM provides labels of similar quality to humans. Attributes and Captions. We computed three low-level visual attributes for each video clip: colorfulness, brightness, and hue, based on the HSV color space. Specifically, we sampled every 20 frames from each clip, and every pixel is represented by triplet (hi, si, vi). For hue H, each pixel value hi is defined as an angle on the color wheel in the range [0, 360), and the overall is defined as the angle of the vector resulting from the sum of these vectors: ϕ = atan2 (cid:32) (cid:88) sin(hi), (cid:88) (cid:33) cos(hi) 180 π = round ((ϕ + 360) mod 360) (1) (2) The modulo operation ensures that lies within the range [0, 360). Colorfulness (C) was defined as the normalized average of the saturation channel (S), and brightness (B) was calculated using the value channel (V ): = round (cid:32) 1 (cid:88) (cid:33) Si, , = round i=1 (cid:32) 1 (cid:88) (cid:33) Vi, 1 . i=1 (3) where is the total number of pixels sampled from selected frames. Both and were normalized between 0 and 1 and rounded to one decimal place. Additionally, we generated high-quality captions for each clip using the NVILA-8B-Video model to facilitate further training and evaluation based on the dataset. To sum up, beyond categorical emotion labels, EmoVid provides rich multimodal annotations including: Audio tracks aligned with each video to enable audiovisual emotion fusion. Low-level visual features including brightness, colorfulness, and hue, quantitatively extracted to support emotion attribution analysis. Free-form captions generated by VLM, describing the perceived content and sentiment of each video."
        },
        {
            "title": "Analysis of EmoVid",
            "content": "Properties of EmoVid EmoVid is large-scale, multimodal video dataset designed for emotion-aware video understanding and generFigure 2: Relationship between different emotions. We refer to Warriner, Kuperman, and Brysbaert (2013) to arrange emotion categories on the valence-arousal model. Figure 3: Emotion distribution across three video categories. Notably, the imbalance of animation and movie videos reflects the real-world emotional landscape of these domains. Figure 4: Video features and color-emotion correlations. (a) t-SNE visualization of video features. Animation and Movie clusters are separated, with Sticker samples overlapping both, reflecting their hybrid content characteristics. (b) Positive-tototal emotion ratio across bins of colorfulness and brightness, exhibiting distinct upward trend. (c) Emotion transition matrix from consecutive movie clips. Diagonal dominance indicates strong emotional persistence. ation tasks. The dataset consists of 22,758 videos, with total duration of 140,580 seconds. Among them, 10,049 clips have human-annotated emotion labels (282 animation clips, 2,771 movie clips, and 6,996 sticker clips). The emotion labels of the rest are generated by fine-tuned VLM, the quality of which is verified in the above experiments. As depicted in Figure 4(a), the t-SNE visualization illustrates the clear distribution among the three data types. We observe significant differentiations between Animation and Movie types, while Sticker data points are intermediate, exhibiting overlaps with both Animation and Movie clusters. This aligns intuitively with expectations regarding content similarity across these categories. EmoVid covers eight discrete emotion categories aligned with the Mikels model, including amusement, anger, awe, contentment, disgust, excitement, fear, and sadness. The videos are sourced from three representative domains animation, movie, and sticker contenteach contributing different emotional intensities, stylistic traits, and temporal structures. Figure 3 summarizes the distribution of emotion labels across domains. From the figure, we observe that the emotion distribution in animation and movie video types is relatively imbalanced, with emotions like anger and sadness appearing more frequently, while amusement and awe are underrepresented. This reflects the natural distribution of emotions in real-life settings because these videos are all collected from real-world contexts and annotated. Together, these properties make EmoVid an interpretable and extensible benchmark for video-based emotion understanding and generation. The dataset serves as foundation for multimodal tasks such as emotional storytelling, text-tovideo (T2V) or image-to-video (I2V) generation, and emotionally grounded video editing. Emotional Structure and Dynamics Visual attributes. The inclusion of visual attributes in the dataset is intended to facilitate further research on the relationship between emotion and color expression. In Figure 4(b), the positive-to-total emotion ratio demonstrates general upward trend concerning colorfulness and brightness attributes, which is consistent with our conventional knowledge. We also calculate the average value of colorfulness, brightness, and hue for each of the eight emotions. Positive-valence categories are brighter and slightly more colorful than negative-valence ones, while high-arousal emotions tend to be darker but more colorful than lowarousal emotions. ANOVA (St, Wold et al. 1989) confirms statistically significant (p < 0.01) but small effects (η2 < 1%), and the details are in the appendix. Temporal analysis. Different from static data, key advantage of video datasets lies in the additional temporal dimension, which enables the exploration of how emotions evolve over time. Since the movie clips are extracted from continuous segments of films, we are able to perform temporal analysis based on them. As illustrated in Figure 4(c), the first-order Markov transition matrix derived from consecutive movie clips unveils three-stage emotional dynamic. First, all eight emotions exhibit strong self-persistenceparticularly fear (0.53), anger (0.46), and amusement (0.46)indicating that once an affective state is established, the visual stream tends to maintain it over short temporal windows. Second, transitions are markedly more frequent within the same valence polarity than across it (typically 0.080.18 versus < 0.08). Third, negative emotions reveal chain-like escalation pattern: sadness fear/anger and fear angersuggesting possible defense-attack progression that makes negative sequences harder to dissolve (Blanchard et al. 1977). Together, these findings corroborate hold, intra-valence drift, arousal leap trajectory, providing explicit understandings for emotion-aware video generation, editing, and pacing strategies. Text Caption. To further explore the relationship between content and emotion, we extracted the five most frequently occurring 24 word phrases from video captions associated with each emotion, after filtering out redundant or noisy expressions. These phrases provide concrete semanTask Method FVD CLIP SD Flicker EA-2cls EA-8cls T2V I2V VideoCrafter-V2 HunyuanVideo CogVideoX WanVideo (before) WanVideo (after) DynamiCrafter512 HunyuanVideo CogVideoX WanVideo (before) WanVideo (after) 610.1 552.6 584.0 594.3 573.7 512.3 544.6 528.4 517.9 517.8 0.3012 0.2776 0.3013 0.2982 0.3021 0.7288 0.7244 0.7214 0.7146 0. 0.0184 0.0116 0.0213 0.0091 0.0143 0.0280 0.0233 0.0331 0.0325 0.0324 80.42 76.87 82.91 84.17 88.33 90.41 89.17 90.83 91.25 94.58 42.50 40.41 44.58 44.16 48.33 71.25 70.00 70.83 71.30 76. Table 3: Quantitative results on the EmoVid benchmark. Evaluation covers T2V and I2V tasks. Emotion Accuracy (EA) is assessed using binary (EA-2cls) and 8-category (EA-8cls) emotion classification metrics, highlighting the improved capability of finetuned models in capturing and reproducing emotional content. tic cues reflective of emotional content. For instance, under amusement we found frequent phrases such as funny reaction, merry moment, and laughing together. In contrast, fear often included phrases like dark tunnel, screaming sound, and approaching danger. The detailed information can be found in the appendix. Evaluation of EmoVid To rigorously evaluate the effectiveness and utility of EmoVid, we construct comprehensive benchmark and perform both quantitative and qualitative analyses. Specifically, we sample 240 videos across 3 video types with 8 distinct emotion labels, selecting 10 representative videos for each category. The test videos are all sampled from humanannotated ones and have been double-checked to guarantee the best quality. For each video, we use its corresponding caption and modify it by appending an explicit emotional label (e.g., The video is in the amusement emotion). We test four SOTA T2V modelsVideoCrafter-V2 (Chen et al. 2024), HunyuanVideo (Kong et al. 2024), CogVideoX5B (Yang et al. 2024), and Wan2.1-T2V-14B (Wan et al. 2025). We also conduct experiments on four I2V models DynamiCrafter512 (Xing et al. 2024), HunyuanVideo-I2V, CogVideoX-I2V, and Wan2.1-I2V-480P. Furthermore, we fine-tune both T2V and I2V Wan2.1 models on EmoVid, excluding the data in the benchmark with the LoRA technique (Hu et al. 2022). We conducted fine-tuning using the DiffSynth Studio framework (ModelScope 2025) on an H20 GPU with 96 GB memory. To balance the distribution of training data, we did not use the entire set of movie clips. Instead, the final training dataset consisted of 2,727 animation clips, 8,000 movie clips, and 6,616 sticker clips. The LoRA configuration was set with rank=32, learning rate=1e-4, training epoch=3, and batch size=1, the same as default settings in DiffSynth Studio. Quantitative Results To measure the emotional accuracy of generated videos, we use the fine-tuned VLM employed during dataset annotation to classify the generated videos and adopt the same metrics as Yang et al. (2023). EA-2cls measures binary accuracy by checking whether the predicted emotion matches the ground-truth valence, while EA-8cls measures top-1 accuracy across the eight discrete emotions. We evaluated model performance using diverse set of metrics: FVD (Ge et al. 2024) measures the overall visual fidelity. CLIP Score (Hessel et al. 2021) quantifies semantic alignment between text and video. SD Score (Liu et al. 2024a) measures consistency between video and its first frame. Temporal Flicker (Huang et al. 2024) captures temporal instability across frames. EA-2cls and EA-8cls measure binary and full-class emotion accuracy. Table 3 shows that our fine-tuned model WanVideo (after) has better performance in both tasks over the baseline WanVideo (before). In the T2V setting, while general metrics like FVD and CLIP show comparative performance, the emotion alignment metrics (EA-2cls and EA-8cls) exhibit clear gains, indicating stronger emotional expression fidelity in the generated videos. Similar trends are observed in the I2V scenario, where the fine-tuned model surpasses all competitors, achieving the highest emotion classification accuracy of 92.08% (2-class) and 72.92% (8-class)."
        },
        {
            "title": "Qualitative Results",
            "content": "To further assess the effectiveness of fine-tuning pretrained models on our dataset, we qualitatively compare video outputs of the original Wan2.1-I2V-480P model and those of the fine-tuned version. As shown in Figure 5(a), the baseline model often fails to capture the correct emotional tonee.g., generating neutral or mismatched expressionswhereas the fine-tuned model exhibits more precise emotional articulation, such as heightened facial expressions, contextual cues, and mood-consistent motion patterns. These improvements highlight the value of EmoVid not only as benchmarking dataset, but also as valuable resource for emotion-specific downstream tasks. We also utilize the LoRA-trained I2V model to generate animated stickers conditioned on different characters and Figure 5: Qualitative results. (a) Comparison between the original Wan2.1 I2V model and our fine-tuned one. The indicates better emotional alignment. (b) Emotion-conditioned animated sticker generation using the fine-tuned Wan2.1 I2V model. emotions, as illustrated in Figure 5(b). The results demonstrate that our model is capable of producing vivid emotional expressions, which can be applied to social media platforms. In addition, we employ multi-LoRA approach on the T2V model, combining our emotion-aware LoRA with other LoRAs that encode character identity or visual style priors, to generate videos with specific emotional attributes. More results can be found in the appendix. Discussion Through comparative experiments, we validate the effectiveness of the EmoVid benchmark. The quantitative results demonstrate that models fine-tuned with EmoVid data exhibit superior emotional accuracy, and the qualitative comparisons further support this conclusion. Such improvement mainly comes from cases where the baseline model failed to express the intended emotion, while the fine-tuned version captured it more precisely. Importantly, EmoVid is designed for artistic and creative scenarios where emotional expression is central. Such contexts include films and operas, miniseries, and emotionally evocative social media content such as expressive memes. In these applications, emotional clarity is often more important than realism or temporal fidelity. Our benchmark captures this focus by emphasizing emotional accuracy as the primary evaluation criterion. Finally, we anticipate that EmoVid will be useful far beyond the scope of model benchmarking. Potential downstream applications include emotion-aware avatar generation, expressive media content synthesis, and controllable video editing based on emotional cues. As generative models become increasingly capable, datasets like EmoVid can help ground their outputs in meaningful human affect."
        },
        {
            "title": "Conclusion",
            "content": "This paper has introduced EmoVid, the first large-scale emotion-annotated video dataset tailored specifically for creative contexts, including animations, movie clips, and animated stickers. EmoVid fills critical gap by providing high-quality multimodal annotationsemotion labels, visual attributes, and textual captionsenabling deeper analysis of the interplay between visual features, temporal dynamics, and emotional perception. Through extensive experiments, we demonstrated EmoVids capability in finetuning state-of-the-art generative models, resulting in significant improvements in emotional expressiveness for both T2V and I2V tasks. By establishing new benchmark for affective video computing, EmoVid not only advances fundamental research in emotion-driven video understanding and generation but also supports practical applications in fields such as animation, filmmaking, and social media communication. Our work is based on the assumption that each clip conveys specific emotion. However, due to the complex nature of human emotion, the real-world expressions can be highly detailed and composite. In addition, the audio component of the dataset can be better leveraged to build truly unified video-audio-text multimodal model. We will continue to explore these directions in our future work. Ethics Statement In recognition of the importance of copyright and privacy protection, we will only provide access to our dataset strictly for non-commercial research use by academic institutions. Any use of the dataset must comply with relevant intellectual property laws and ethical research standards. Redistribution or commercial use is prohibited. Acknowledgments We thank Shuolin Xu and Dr. Xian Xu for helpful discussions and support on dataset preprocessing. We also thank Xiaochun Wang for preparing the figures and Yihan Wu for assistance in conducting the user study. References Bain, M.; Nagrani, A.; Brown, A.; and Zisserman, A. 2020. Condensed Movies: Story-Based Retrieval with Contextual In Proceedings of the Asian Conference on Embeddings. Computer Vision. Baveye, Y.; Dellandrea, E.; Chamaret, C.; and Chen, L. 2015. LIRIS-ACCEDE: Video Database for Affective Content Analysis. IEEE Transactions on Affective Computing, 6(1): 4355. Blanchard, R. J.; Blanchard, D. C.; Takahashi, T.; and Kelley, M. J. 1977. Attack and Defensive Behaviour in the Albino Rat. Animal Behaviour, 25: 622634. Busso, C.; Bulut, M.; Lee, C.-C.; Kazemzadeh, A.; Mower, E.; Kim, S.; Chang, J. N.; Lee, S.; and Narayanan, S. S. 2008. IEMOCAP: Interactive Emotional Dyadic Motion Capture Database. Language Resources and Evaluation, 42(4): 335359. Cao, W.; Yang, W.; et al. 2022. GestureDiffuCLIP: GestureConditioned Diffusion Model for Zero-Shot Emotive Body Animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Cao, W.; Zhang, K.; Wu, H.; Xu, T.; Chen, E.; Lv, G.; and He, M. 2022. Video Emotion Analysis Enhanced by Recognizing Emotion in Video Comments. International Journal of Data Science and Analytics, 14(2): 175189. Castellano, B. 2025. PySceneDetect. Chen, H.; Zhang, Y.; Cun, X.; Xia, M.; Wang, X.; Weng, C.; and Shan, Y. 2024. VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models. arXiv:2401.09047. Das, R.; and Singh, T. D. 2023. Multimodal Sentiment Analysis: Survey of Methods, Trends, and Challenges. ACM Computing Surveys, 55(13s): 138. Demszky, D.; Movshovitz-Attias, D.; et al. 2020. GoEmotions: Dataset of Fine-Grained Emotions. arXiv preprint arXiv:2005.00547. Ge, S.; Mahapatra, A.; Parmar, G.; Zhu, J.-Y.; and Huang, J.-B. 2024. On the Content Bias in Frechet Video Distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Guo, R.; Huang, X.; et al. 2023. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In arXiv preprint arXiv:2307.04725. Hessel, J.; Holtzman, A.; Forbes, M.; Bras, R. L.; and Choi, Y. 2021. CLIPScore: Reference-Free Evaluation Metric for Image Captioning. arXiv preprint arXiv:2104.08718. Imagen Video: HighHo, J.; Saharia, C.; et al. 2022. Definition Video Generation with Diffusion Models. arXiv preprint arXiv:2210.02303. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. International Conference on Learning Representations, 1(2): 3. Huang, Z.; He, Y.; Yu, J.; Zhang, F.; Si, C.; Jiang, Y.; Zhang, Y.; Wu, T.; Jin, Q.; Chanpaisit, N.; et al. 2024. VBench: Comprehensive Benchmark Suite for Video Generative Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2180721818. EmoVideo: EmotionJi, Z.; Wang, W.; et al. 2023. Controllable Text-to-Video Generation via Affective Prompt Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Kalateh, S.; Estrada-Jimenez, L. A.; Hojjati, S. N.; and Barata, J. 2024. Systematic Review on Multimodal Emotion Recognition: Building Blocks, Current State, Applications, and Challenges. IEEE Access. Koelstra, S.; Muhl, C.; Soleymani, M.; Lee, J.-S.; Yazdani, A.; Ebrahimi, T.; Pun, T.; Nijholt, A.; and Patras, I. 2011. DEAP: Database for Emotion Analysis Using PhysiologIEEE Transactions on Affective Computing, ical Signals. 3(1): 1831. Kong, W.; Tian, Q.; Zhang, Z.; Min, R.; Dai, Z.; Zhou, J.; Xiong, J.; Li, X.; Wu, B.; Zhang, J.; et al. 2024. HunyuanVideo: Systematic Framework for Large Video Generative Models. arXiv preprint arXiv:2412.03603. Lee, J.; Kim, S.; Kim, S.; Park, J.; and Sohn, K. 2019. In ProContext-Aware Emotion Recognition Networks. ceedings of the IEEE/CVF International Conference on Computer Vision, 1014310152. Liu, H.; Sun, W.; Di, D.; Sun, S.; Yang, J.; Zou, C.; and Bao, H. 2025. MoEE: Mixture of Emotion Experts for AudioDriven Portrait Animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2622226231. Liu, Y.; Cun, X.; Liu, X.; Wang, X.; Zhang, Y.; Chen, H.; Liu, Y.; Zeng, T.; Chan, R.; and Shan, Y. 2024a. EvalCrafter: Benchmarking and Evaluating Large Video Generation Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2213922149. Liu, Z.; Zhu, L.; Shi, B.; Zhang, Z.; Lou, Y.; Yang, S.; Xi, H.; Cao, S.; Gu, Y.; Li, D.; Li, X.; Fang, Y.; Chen, Y.; Hsieh, C.-Y.; Huang, D.-A.; Cheng, A.-C.; Nath, V.; Hu, J.; Liu, S.; Krishna, R.; Xu, D.; Wang, X.; Molchanov, P.; Kautz, J.; Yin, H.; Han, S.; and Lu, Y. 2024b. NVILA: Efficient Frontier Visual Language Models. arXiv:2412.04468. Livingstone, S. R.; and Russo, F. A. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). PLOS ONE. Ma, Y.; Feng, K.; Hu, Z.; Wang, X.; Wang, Y.; Zheng, M.; He, X.; Zhu, C.; Liu, H.; He, Y.; et al. 2025. ConarXiv preprint trollable Video Generation: Survey. arXiv:2507.16869. Mikels, J. A.; Fredrickson, B. L.; Larkin, G. R.; Lindberg, C. M.; Maglio, S. J.; and Reuter-Lorenz, P. A. 2005. Emotional Category Data on Images from the International Affective Picture System. Behavior Research Methods, 37(4): 626630. ModelScope. 2025. DiffSynth-Studio. https://github.com/ modelscope/DiffSynth-Studio/. Apache-2.0 license. Mollahosseini, A.; Hasani, B.; and Mahoor, M. H. 2017. AffectNet: Database for Facial Expression, Valence, and Arousal Computing in the Wild. IEEE Transactions on Affective Computing, 10(1): 1831. Pandeya, Y. R.; and Bhattarai, B. 2021. Deep-LearningBased Multimodal Emotion Classification for Music Videos. Sensors. Pang, L.; Zhu, S.; and Ngo, C.-W. 2015. Deep Multimodal Learning for Affective Analysis and Retrieval. IEEE Transactions on Multimedia, 17(11): 20082020. Peng, K.; Wang, J.; et al. 2015. Mixed Emotional Image Dataset with Subjective and Objective Labels. In Proceedings of the ACM International Conference on Multimodal Interaction. Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria, E.; and Mihalcea, R. 2019. MELD: Multimodal MultiParty Dataset for Emotion Recognition in Conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 527536. Ren, Z.; Ortega, J.; Wang, Y.; Chen, Z.; Guo, Y.; Yu, S. X.; and Whitney, D. 2024. VEATIC: Video-Based Emotion and Affect Tracking in Context Dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 44674477. Rosenthal, S.; Farra, N.; and Nakov, P. 2017. SemEval-2017 In Proceedings of Task 4: Sentiment Analysis in Twitter. SemEval. Russell, J. A. 1980. Circumplex Model of Affect. Journal of Personality and Social Psychology, 39(6): 1161. St, L.; Wold, S.; et al. 1989. Analysis of Variance (ANOVA). Chemometrics and Intelligent Laboratory Systems, 6(4): 259272. Tenor, Inc. 2025. Tenor Animated GIF Search Engine. Tulyakov, S.; Liu, M.-Y.; Yang, X.; and Kautz, J. 2018. MoCoGAN: Decomposing motion and content for video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition. Wan, T.; Wang, A.; Ai, B.; Wen, B.; Mao, C.; Xie, C.-W.; Chen, D.; Yu, F.; Zhao, H.; Yang, J.; et al. 2025. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314. Wang, B.; Chen, Q.; and Wang, Z. 2025. Diffusion-Based Visual Art Creation: Survey and New Perspectives. ACM Computing Surveys, 57(10): 137. Wang, B.; Meng, H.; Cao, R.; Cai, Z.; Li, L.; Ma, Y.; Chen, Q.; and Wang, Z. 2025a. MagicScroll: Enhancing Immersive Storytelling with Controllable Scroll Image Generation. In 2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR), 431441. IEEE. Wang, B.; Shi, Q.; Wang, X.; Zhou, Y.; Zeng, W.; and Wang, Z. 2025b. EmotionLens: Interactive Visual Exploration of the Circumplex Emotion Space in Literary Works via Affective Word Clouds. Visual Informatics, 9(1): 8498. Wang, K.; Wu, Q.; Song, L.; Yang, Z.; Wu, W.; Qian, C.; He, R.; Qiao, Y.; and Loy, C. C. 2020. MEAD: Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation. In Proceedings of the European Conference on Computer Vision, 700717. Springer. Wang, T.-C.; Liu, M.-Y.; et al. 2018. Video-to-Video Synthesis. In Advances in Neural Information Processing Systems. Warriner, A. B.; Kuperman, V.; and Brysbaert, M. 2013. Norms of Valence, Arousal, and Dominance for 13,915 English Lemmas. Behavior Research Methods, 45(4): 1191 1207. Xing, J.; Xia, M.; Zhang, Y.; Chen, H.; Yu, W.; Liu, H.; Liu, G.; Wang, X.; Shan, Y.; and Wong, T.-T. 2024. DynamiCrafter: Animating Open-Domain Images with Video Diffusion Priors. In Proceedings of the European Conference on Computer Vision, 399417. Springer. Xu, S.; Wang, B.; Cai, Z.; Fu, F.; Ma, Y.; Lee, T.; Yu, H.; and Wang, Z. 2025. MagicAnime: Hierarchically Annotated, Multimodal and Multitasking Dataset with Benchmarks for Cartoon Animation Generation. arXiv:2507.20368. Yang, J.; Huang, Q.; Ding, T.; Lischinski, D.; Cohen-Or, D.; and Huang, H. 2023. EmoSet: Large-Scale Visual EmoIn Proceedings of the tion Dataset with Rich Attributes. IEEE/CVF International Conference on Computer Vision, 2038320394. Yang, Y.; Xu, Y.; et al. 2023. VideoCrafter: Open Diffusion Models for High-Quality Video Generation. arXiv preprint arXiv:2310.19512. Yang, Z.; Teng, J.; Zheng, W.; Ding, M.; Huang, S.; Xu, J.; Yang, Y.; Hong, W.; Zhang, X.; Feng, G.; et al. 2024. CogVideoX: Text-to-Video Diffusion Models with an Expert Transformer. arXiv preprint arXiv:2408.06072. Zadeh, A.; Zellers, R.; Pincus, E.; and Morency, L.-P. 2016. MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos. arXiv preprint arXiv:1606.06259. Zakharov, E.; Shysheya, A.; Burkov, E.; and Lempitsky, V. 2019. Few-Shot Adversarial Learning of Realistic Neural Talking Head Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Zhu, X.; Guo, C.; Feng, H.; Huang, Y.; Feng, Y.; Wang, X.; and Wang, R. 2024. Review of Key Technologies for Emotion Analysis Using Multimodal Information. Cognitive Computation, 16(4): 15041530. APPENDIX Detailed Statistics The following section provides additional statistical insights and detailed analyses of the EmoVid dataset, further complementing the main manuscript. Duration Distribution across Styles Figure 6 illustrates the duration distribution of videos in the EmoVid dataset. The dataset encompasses total duration of 140,580 seconds with mean duration of 6.18 seconds per clip. The shortest video clip is 0.18 seconds, and the longest is 29.98 seconds. Most videos are concentrated between approximately 2 to 8 seconds, supporting both the capture of brief emotional moments and the exploration of extended emotional arcs. Table 4 provides breakdown of emotion categories across different video stylesAnimation, Movie, and Sticker. Movies constitute the largest share of data (58.24%) and have the longest average video duration (8.75 seconds), followed by Stickers (29.42%) with shorter clips (average 2.91 seconds) and Animations (12.33%) averaging 5.12 seconds. This distribution enables comprehensive coverage of emotional expression across varying content styles. Color Attributes To explore the relationship between color attributes and video emotions, we calculated the mean and standard deviation of each color parameter for every emotion category, displayed in Table 5. To properly handle the circular nature of hue values (ranging from 0 to 360), we compute the circular mean and circular variance. We observe that positivevalence emotions are generally brighter and more colorful, while high-arousal emotions tend to be darker but also more colorful than their low-arousal counterparts. ANOVA reveals statistically significant yet small effects (η2 < 1%), suggesting color cues are helpful for stylistic guidance or weak supervision, though insufficient for standalone classification. Figure 8 presents average values of colorfulness and brightness for each of the eight emotions. Notably, the spatial layout aligns well with valence and arousal dimensions, which form near-orthogonal directions, supporting the VA models robustness and highlighting clear link between perceived emotion and color properties."
        },
        {
            "title": "Caption Analysis",
            "content": "Given the intrinsic link between the semantic content of video and its expressed emotion, we conducted series of analyses on the captions associated with each video. As first step, we employed the Natural Language Toolkit (NLTK) to assess the sentiment polarity of each caption. Prior to analysis, all captions were tokenized after removing URLs, numbers, punctuation marks, and standard English stopwords. Figure 7 presents the proportion of positive and negative captions for each emotion category. The results reveal clear pattern: videos labeled with positive emotions tend to have significantly higher proportion of positively valenced captions compared to those labeled with negative emotions. Beyond sentiment polarity, we also performed keyword analysis to uncover representative semantic patterns across different emotional categories. Using the CountVectorizer with n-grams ranging from 2 to 4 words, we extracted the most frequent short phrases for each emotion class. Table 6 lists the top five frequent caption-level phrases corresponding to both positive and negative emotions. Interestingly, the extracted keywords reflect the affective semantics of each category. For instance, positive emotions such as contentment and amusement are frequently associated with calm, festive, or joyful expressions (e.g., Peace written, Merry Christmas), while negative emotions like fear and anger feature more intense or threatening phrases (e.g., Man holding gun aiming, Boxing glove). Construction Details To ensure high-quality emotion labels across diverse video types, we employed hybrid labeling strategy that combines manual annotation with classifier-assisted verification. Figure 6: Duration distribution of EmoVid clips. Most clips cluster between roughly 28 seconds, and overall total duration is 140,580 seconds. Mean clip length is 6.18 seconds (min 0.18 s, max 29.98 s). Figure 7: Caption polarity by emotion category. The results indicate good alignment between caption semantics and emotion labels. Style Total"
        },
        {
            "title": "Animation\nMovie\nSticker",
            "content": "Amusement Anger Awe Contentment Disgust Excitement Fear Sadness Total Avg Len SD 2008 294 1152 562 4756 1768 827 90 2901 1097 581 2424 409 1351 664 2458 260 1297 901 2706 3315 3323 6.18 4.53 266 63 1497 2401 851 943 598 2807 1559 13255 6696 1166 5.12 2.65 8.75 4.71 2.91 2.18 Table 4: Distribution of emotions across styles. Avg Len denotes the mean clip duration in seconds; SD is the standard deviation of clip length. Emotion Brightness Colorfulness Hue () Contentment Awe Amusement Excitement Sadness Fear Disgust Anger 0.443 0.212 0.416 0.193 0.432 0.208 0.416 0.202 0.421 0.211 0.395 0.205 0.416 0.198 0.398 0.186 0.373 0.195 0.394 0.199 0.388 0.197 0.406 0.200 0.355 0.200 0.374 0.197 0.365 0.192 0.406 0. 34.50 61.34 38.47 57.37 32.92 54.42 38.21 60.12 33.98 64.69 38.31 60.23 43.06 59.60 37.74 52.05 Table 5: Per-emotion visual feature analysis. We include statistics of brightness, colorfulness, and hue. Figure 8: Distribution of emotion categories in color space. Each point represents an emotion category, plotted by average brightness (x-axis) and colorfulness (y-axis)."
        },
        {
            "title": "Manual Annotation Protocol",
            "content": "To retrieve emotion-specific stickers, we used the eight emotion categories along with their synonyms given by EmoSet. Based on this, we further merged words with the same rootfor example, amusement/amusing/amused were all mapped to amuse. The full list of keywords is in Table 8. We recruited total of 12 annotators to perform finegrained emotion labeling. Among them, two annotators were tasked with validating the pre-labeled emotion sticker dataset. They reviewed each clip and retained only those whose emotional expressions were consistent with the assigned label. After this filtering process, 6,696 out of the original 9,633 clips were preserved. animation and movie datasetsnamely, 3,000 clips from movies and 600 from animation. Each clip was independently labeled by three annotators, who were asked to select one category from eight emotions plus no specific emotion option. clip was retained only if at least two annotators reached consensus on the same label. Given the critical role of audio in emotional expression, annotators watched each clip with its audio during the labeling process. Classifier Comparison To further scale up the labeling process, we evaluate both traditional visual classifiers and recent Vision-Language Models (VLMs) on EmoSet-118k. Specifically, we compared two convolutional baselinesResNet-50 and VGG16with two lightweight VLM pipelines: TinyLLaVAPhi-2-SigLIP-3.1B and NVILA-Lite-2B. All models were trained on randomly selected 80% subset of the EmoSet118k dataset and evaluated on held-out set of 160 images. Detailed results are in the Table 9. Based on small-scale human evaluation, the fine-tuned NVILA-Lite-2B model achieved an accuracy of 87.5% on EmoSet-118k, closely matching human-level performance. Notably, its misclassifications typically involved semantically adjacent emotions (e.g., awe vs. contentment), which are inherently difficult to distinguish. In addition, NVILALite-2B exhibited low computational footprint, making it practical choice for scalable labeling. Figure 9 showcases examples from the EmoVid animation subset, highlighting that NVILA-Lite-2B consistently produces semantically coherent emotion labelsoften outperforming the other baselines by significant margin. To further assess the labeling performance of the VLM on EmoVid, we randomly selected 1% of the VLM-labeled videos as validation set. Three human annotators were invited to independently annotate these videos. The overall Fleiss kappa across all annotations was 0.3704. We also computed pairwise Cohens kappa scores among the four annotation sources (three humans and the VLM). The average inter-human kappa was 0.311, while the average human-VLM kappa was 0.301, indicating minimal gap. These results suggest that the VLM achieves humancomparable annotation ability on the EmoVid dataset."
        },
        {
            "title": "Metrics and Implementation Details",
            "content": "Notation. Let generated video be = {xt}T t=1 with frames, and let denote the corresponding text prompt. We report FVD, CLIP, SD, Flicker, and EA. The remaining 10 annotators focused on 20% of the CLIP score (textvideo alignment, ). We use OpenCLIP Contentment"
        },
        {
            "title": "Brown hugging\nPeace written\nExpect it\nStone lantern\nBlonde boy",
            "content": "Sadness Awe Amusement Excitement"
        },
        {
            "title": "Adventure Displayed\nExcited written\nYour enthusiasm\nRunning through lush\nGoing adventure",
            "content": "Disgust Fear Anger"
        },
        {
            "title": "Intense emotion\nShows man flaming\nRed angry character\nMan flaming skill\nBoxing glove",
            "content": "Table 6: Top-5 phrases for the caption of each emotion. The table presents the most frequent short phrases associated with each emotion category, extracted from video captions. Model Accuracy (%) ResNet50 VGG-16 TinyLLaVA TinyLLaVA (finetuned) NVILA NVILA (finetuned) 66.68 45.46 26. 38.75 57.5 87.5 Table 7: Emotion classification accuracy of different models on EmoSet. TinyLLaVA refers to TinyLLaVA-Phi-2-SigLIP3.1B, and NVILA refers to NVILA-Lite-2B. The CLIP score is the cosine similarity CLIP(V, y) = cos(cid:0)f , g(cid:1). We report the mean over the evaluation set. SD score (first-frame consistency in latent space, ). This metric measures how consistent the content of all frames is with the first frame in strong image latent space. We use the VAE encoder from stabilityai/sd-vae-ft-mse (Diffusers). Each frame is resized to 512512 and normalized to [1, 1]. Let zt denote the flattened latent sampled from the encoders posterior for frame xt; we compute SD(V ) = 1 (cid:88) t=1 cos(cid:0)z1, zt (cid:1), and report the mean across videos. Figure 9: Qualitative comparison of automatic labelers on animation samples. For each clip, we show the predictions from VGG-16, ResNet-50, TinyLLaVA-Phi-2-SigLIP3.1B, and NVILA-Lite-2B. NVILA-Lite-2B produces the most semantically coherent labels. Temporal Flicker (perceptual instability, ). We quantify frame-to-frame perceptual change using LPIPS with the alex backbone. All frames are center-cropped to 224224 and normalized to [1, 1]. For consecutive frames (xt, xt+1), with ViT-L/14 (pretrained=openai) and its default preprocessing. For each video, we uniformly sample up to = min(8, ) frames and encode them with the image encoder to unit-normalized vectors {ft}K t=1. The video embedding is the average of per-frame features, = 1 (cid:88) t=1 ft. The prompt is tokenized and encoded to unit vector g. Flicker(V ) = 1 1 1 (cid:88) t=1 LPIPS(cid:0)xt, xt+1 (cid:1). Lower is better; we report the average across videos. Content-debiased Frechet Video Distance (FVD, ). We follow the content-debiased FVD implementation and extract video features with either VideoMAE-v2 or I3D (as configured in our code). Given Gaussian fits to the real and generated feature sets, (µr, Σr) and (µg, Σg), (cid:16) FVD = µr µg2 2 +Tr Σr +Σg 2(cid:0)Σ1/2 Σg Σ1/2 (cid:1)1/2(cid:17) . Figure 10: Examples of generated videos using our fine-tuned Wan2.1-T2V model. An extra LoRA module is included to generate Studio Ghibli style videos. Real-set statistics can be cached; generated features are computed on-the-fly. Emotion Accuracy (EA, ). We evaluate whether the predicted emotion of generated video matches the groundtruth (GT) label. EA 8cls (8-way accuracy). Let ˆei and ei be the predicted and GT emotion for sample i, where = 8. The 8-class top-1 accuracy is EA 8cls = 1 (cid:88) i=1 [ˆei = ei] . EA 2cls (valence accuracy). We partition the eight emotions into positive set and negative set with = and = (four classes per side, fixed by our annotation protocol). Let ν : {pos, neg} be the induced valence mapping: EA 2cls = 1 (cid:88) i=1 [ν(ˆei) = ν(ei)] . Intuitively, EA 2cls counts prediction as correct whenever the predicted emotion has the same polarity (positive vs. negative) as the ground truth. Implementation notes. Videos are decoded with decord. For CLIP, we uniformly sample up to 8 frames; SD and Flicker use all frames. All cosine similarities and means are computed per video and averaged over the benchmark set. More Experiment Results We used the fine-tuned Wan2.1-I2V model to generate animated expressions of characters with specific emotions, as shown in Figure 11. We then applied the Multiple LoRA technique with the Wan2.1-T2V model to generate videos that combine specific visual styles with targeted emotional expressions, as shown in Figure 10. User Study We conducted controlled perceptual study to compare our Wan-Finetuned model against two baselines, Wan-Original and CogVideoX. We generated videos for eight emotion categories, using both text-to-video (T2V) and image-to-video (I2V) prompts for each. Fifteen participants were presented with the outputs from all three models side-by-side and asked to rank them on two criteria: Emotion Expression (the accuracy and salience of the emotion) and Aesthetic Quality (coherence, plausibility, and visual appeal). This process yielded 240 discrete rankings per criterion (15 raters 16 prompts). We analyzed the rankings using mean rank, Top-1 preference rate, and pairwise win rates. We also performed statistical significance tests on the comparisons and computed inter-rater reliability using Kendalls to validate findings. The results, summarized in Figure 12, show clear and significant preference for Wan-Finetuned. For Emotion Expression, participants ranked Wan-Finetuned first in 66.2% of comparisons, far exceeding Wan-Original (16.7%) and CogVideoX (17.1%) (Fig. 12b). This strong preference is mirrored in the mean rank (Fig. 12a) and the decisive pairwise win rates against both baselines (Fig. 12c). We observed similar strong preference for Wan-Finetuned on Aesthetic Quality (57.9% Top-1 rate). As shown in Fig. 12d, this advantage is consistent across all individual emotion categories. All pairwise preferences for Wan-Finetuned were statistically significant (p 0.001), and moderate inter-rater agreement (mean Kendalls = 0.371 for Emotion, 0.333 for Aesthetics) confirms the reliability of these judgments. Figure 11: Example results of emotion-conditioned animated sticker generation using our fine-tuned Wan2.1-I2V model. We demonstrate EmoVids potential in efficiently adapting existing general-purpose video models for emotional content generation tailored to stylized, creative, and social media applications. Figure 12: Perceptual user study results comparing Wan-Finetuned with Wan-Original and CogVideoX. (a) Mean rank for Emotion Expression and Aesthetic Quality (lower is better). (b) Top-1 preference rate (higher is better). (c) Pairwise win rate of Wan-Finetuned against the two baselines. (d) per-category breakdown of Top-1 preference rates, demonstrating consistent advantages for Wan-Finetuned across all eight emotions. Emotion Keyword Amusement amuse, entertain, delight, enjoy, pleasure, laughter, mirth, hilarity, merry, glad, recreation, extravaganza, cheer, delectation, ravishment Awe awe, wonder, revere, venerate, inspire, respect, hallowed, exalt, amaze, astonish, impress, marvel, astound, startle, surprise, worship Contentment content, happy, satisfy, fulfill, need, expect, long, complacence, smug, gloat, peace, ease, comfort, gratify, serenity, equanimity, replete, warmth Excitement excite, pride, glee, exhilarate, fervor, lively, joy, rouse, agitate, passion, thrill, adventure, enthusiasm, flurry, furore, commotion, elate, kick, nightlife, show, frisson, hysteria Anger Disgust Fear Sadness anger, choler, ire, grievance, fury, rage, wrath, infuriate, enrage, umbrage, offend, indignation, outrage, dudgeon, irascible, annoy, chafe, vex, pique, irritate, aggravate, exasperate, harass, torment, displease, resent, antagonize, provoke, hassle, burn, explode, fume, seethe, aggressive disgust, dislike, distaste, flush, revolt, revulsion, vomit, repel, sicken, abhor, abomination, detest, execration, loathe, odium, repugnance, repulse, nausea, hate, averse, antipathy fear, horror, afraid, scare, frighten, panic, terror, affright, dread, anxious, apprehensive, alarm, dismay, consternation, shiver, chill, quiver, shudder, worry, concern, trouble, uneasy, tremor, qualm, trepidation, timid, craven, funk, creep, attack, intimidate sad, depress, sorrow, unhappy, bitter, doleful, mourn, melancholy, pensive, wistful, tired, despair, desperate, deplore, distress, lament, pity, sorry, gloom, grieve, dismal, sombre, glum, deject, downcast, tear, lugubrious, disconsolate, cheerless, lachrymose, woebegone, triste, tragic, upset, disastrous, pathetic, poignant, harrow, miserable, heartbreak, wretched, desolate, dispirit Table 8: Emotion categories and keywords. Curated keyword lists (lemmas and common synonyms) used for retrieval and normalization during data collection."
        }
    ],
    "affiliations": [
        "Fudan University",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}