{
    "paper_title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
    "authors": [
        "Jianshu Zhang",
        "Dongyu Yao",
        "Renjie Pi",
        "Paul Pu Liang",
        "Yi R.",
        "Fung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues."
        },
        {
            "title": "Start",
            "content": "VLM2-Bench: Closer Look at How Well VLMs Implicitly Link"
        },
        {
            "title": "Explicit Matching Visual Cues",
            "content": "Jianshu Zhang*, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R. (May) Fung HKUST CMU MIT jianshu.zhang777@gmail.com ppliang@mit.edu raindy@cmu.edu yrfung@ust.hk rpi@ust.hk 5 2 0 2 7 1 ] . [ 1 4 8 0 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visually linking matching cues is crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various languageside and vision-side prompting methods, leads to total of eight key findings. We identify critical challenges in models ability to link visual cues, highlighting significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models ability to independently structure and infer relationships among visual cues."
        },
        {
            "title": "Introduction",
            "content": "Humans constantly link matching visual cues to navigate and understand their environment. For instance, we can determine whether objects, and individuals are the same simply by comparing their distinguishing visual features (Bruce and Young, 1986; Palermo and Rhodes, 2007; Treisman and Gelade, 1980). This ability, often without needing *These authors contribute to this work equally. 1Project page: https://vlm2-bench.github.io/. Work was done while student was an intern at HKUST. 1 Figure 1: Previous benchmarks fail to assess the ability to link matching visual cues, whereas our VLM2-Bench explicitly tests this ability, as shown in the example where the model need to identify the reappearance of the same person by linking visual cues, like facial features or clothing, across non-adjacent frames. additional background knowledge, is fundamental in our daily interactions with the world around us. However, while current vision-language models (VLMs) (Chen et al., 2024b; Li et al., 2024b; Zhang et al., 2024b; Team, 2025) have demonstrated extensive knowledge and expanded their capabilities from single-image understanding to handling multiple images and videos, whether thay can effectively link matching visual cues across images or framesan essential skill for coherent multimodal reasoningremains an open question. As shown in Figure 1, existing benchmarks on multiple images and videos fall short in exploring this fundamental ability as they: (a) do not require explicitly linking visual cues across images or frames (Liu et al., 2024c; Yu et al., 2019); (b) rely on external knowledge rather than assessing models ability to link explicitly visual cues (Zhao et al., Figure 2: Overview of VLM2-Bench. The benchmark is categorized into three subsets based on visual cues: GC (General Cue), OC (Object-centric Cue), and PC (Person-centric Cue), each comprising multiple subtasks. To comprehensively evaluate VLMs ability to visually link matching cues, the benchmark includes diverse question formatsT/F , and open-ended ensuring comprehensive evaluation. , multiple-choice , numerical 2024; Liu et al., 2024a); (c) emphasize broad and abstract visual comparisons rather than specific cue matching (Wu et al., 2025; Liu et al., 2024b); and (d) focus on retrieval-based tasks rather than evaluating the direct association of visual cues across different visual contexts (Wang et al., 2024a). To bridge this gap, we introduce VLM2-Bench, benchmark specifically designed to evaluate how well VLMs visually link matching cues. VLM2Bench is structured around three types of visual cue connection: general cue, person-centric cue, and object-centric cue, encompassing total of eight subtasks. To balance scalability and quality, we design semi-automated pipeline with human verification for further refinement. Additionally, our subtasks cover variety of QA formatsincluding T/F, multi-choice, numerical, and open-ended questionstotaling over 3,000 question-answer pairs. To better evaluate model performance, we also design specific metrics tailored to various task. We conduct comprehensive evaluation of 8 open-source models and GPT-4o on our VLM2Bench. Despite VLMs generally possessing extensive knowledge, some models perform on par with, or even worse than, the chance-level baseline on our vision-centric tasks. Notably, GPT-4o also underperforms, lagging behind human-level accuracy by 34.80%. This highlights the significant room for improvement in VLMs ability to link visual cues. Furthermore, we introduce various language-side and vision-side prompting techniques to explore whether they can enhance the models performance on the benchmark. Through experimental results and case studies, we present eight key observations, hoping that these insights will guide future improvements in VLMs for vision-centric tasks."
        },
        {
            "title": "2 VLM2-Bench",
            "content": "VLM2-Bench is benchmark designed to assess models ability to visually link matching cues when processing multiple images or videos. This section introduces the three main categories of VLM2-Benchgeneral cue (2.1), object-centric cue (2.2), and person-centric cue (2.3)detailing their associated subtasks, data collection process, and QA pair construction."
        },
        {
            "title": "2.1 General Cue (GC)",
            "content": "GC is designed to assess models ability to link matching cues across diverse contexts, encompassing broad range of general cues. Given two images containing both matched and mismatched cues, an ideal model should accurately identify mismatched ones and associate matched ones. Subtasks. Here we introduce two subtasks: (i) Matching (Mat) evaluates models ability to link corresponding visual cues across two images to determine whether they match. Instead of merely identifying differences, the model must associate identical visual elements in both images to recognize what has remained the same and what has 2 Figure 3: Construction of GC: (i) We start by manually verifying the edited image data based on three key criteria. (ii) VLM is then prompted to generate captions for each image, followed by salient score-based filtering to retain the challenging cases. (iii) Finally, visual cues are extracted from two sources and incorporated into QA prompt, guiding an LLM to generate both positive and negative answer pairs. changed. (ii) Tracking (Trk) focuses on models ability to track specific visual cue that appears in only one of the two images and determine how it has changed. Rather than simply detecting difference, the model must link the cue across contexts to understand the transformation process. Data Collection. We repurpose data from two image editing datasets (Wei et al., 2024; Ku et al., 2023), where each data sample includes an original image Iori, an edited image with subtle modifications Iedit, and corresponding edit instruction describing the changes. Our data collection is carried out across two dimensions. First, to ensure diversity in the mismatched cues, GC encompasses various types of changes, such as instance-level modifications (e.g., add/remove, swap, attribute change), which focus on specific items, as well as environment-level changes. QA Construction. We predefine T/F question template for Mat and Trk with placeholder for the candidate answer (refer to Appendix E). Figure 3 illustrates the construction process, which follows three-stage approach. Manual Screening & Refinement: We ensure that accurately reflects the changes (correctness), corresponds uniquely to the modified cues (uniqueness), and is unambiguous (clarity). Salient Sampling: Here, we automate the removal of overly simple cases (e.g., mismatched cues are too salient). To achieve this, VLM first generates separate descriptions for Iori and Iedit, denoted as Capori and Capedit. These descriptions are then combined with into single passage using predefined template (see Table 5 for details). The probability assigned by language model (e.g., Llama3-8B (Dubey et al., 2024)) to given this text-based information is used to compute the salient score, formulated as: Ssalient = 1 P (cid:88) i=1 log Pθ(pi p<i), (1) where = {p1, p2, ..., pP} represents the tokenized P, and = (Capori, Capedit) denotes the context filled with template . Samples with scores below θ (-2.0 here) are retained, ensuring that the benchmark includes more challenging examples requiring nuanced visual cue association. Pair-wise Answer Generation: Finally, we extract visual cues using dual-level approach. First, cues parsed from VLM-generated descriptions compensate for the limitations of open-set detectors when handling out-of-distribution scenes. Meanwhile, the open-set detector (Wu et al., 2022) extracts fine-grained cues that VLMs might overlook. With these extracted cues, we prompt an LLM to generate pair of answers for Mat and Trk, each consisting of one positive and one negative answer."
        },
        {
            "title": "2.2 Object-centric Cue (OC)",
            "content": "OC aims to assess models ability to link matching cues associated with everyday objects using object-centric cues. Even when encountering an object for the first time, well-aligned model should be able to leverage its unique visual cues to establish associations, enabling it to recognize and track the object across different scenes. This capability is essential for coherent perception and interaction in real-world deployments. Subtasks. Based on the complexity of linking cues to solve the problem, we define three subtasks in OC. (i) Comparison (Cpr) requires the 3 model to determine whether the objects appearing in different images are the same. This task primarily assesses the models ability to perceive visual consistency or change. Notably, we observe that models exhibit significant model-specific bias when making binary decision (Goyal et al., 2017; Ye et al., 2024b; Song et al., 2024; Li et al., 2024a), leading to discrepancies between results and their actual capabilities. To mitigate this, we introduce consistency-pair validation, where for each statement (e.g., is Y, with the answer being T), we generate corresponding negation (e.g., is not Y, with the answer being F). The model is only considered correct if it correctly answers both statements, ensuring consistency in its decisionmaking. (ii) Counting (Cnt) involves identifying the number of unique objects, requiring the model not only to recognize variations or consistencies but also to track distinct cues to avoid double-counting the same object. (iii) Grouping (Grp), the most challenging one, requires the model to identify all instances of the same object, building on precise cue matching across multiple images. Data Collection. We manually collect various categories of everyday objects (e.g., pets, cups). For each category, we define multiple subcategories and collect set of images IOifour images that depict the same object in different scenarios. Additionally, we also collect set IOi, consisting of four images of different objects, each containing some matching visual cues with IOi, which are used as distractors. QA Construction. For each subtask, we define question template that includes placeholder for IOi, which allows us to tailor the question based on different objects (see Appendix E). For answer generation, we first curate the multi-image sequences according to predefined rules. For each specific sequence, we generate the ground truth answers for the questions related to Cpr, Cnt, and Grp."
        },
        {
            "title": "2.3 Person-centric Cue (PC)",
            "content": "PC aims to evaluate models ability to link personcentric cues. While model cannot memorize every individual, it should possess the capability to associate the same person across different images or frames by leveraging distinctive visual cues such as facial features, clothing, or body posture. This ability is essential for ensuring coherent perception of human actions and is fundamental requirement for real-world VLM applications. Subtasks. Similar to OCs subtasks (refer to 2.2), PC includes (i) Comparison (Cpr), (ii) Counting (Cnt), and (iii) Grouping (Grp). However, unlike objects, individuals can be observed through their actions in videos. Therefore, we introduce (iv) Video Identity Describing (VID). This subtask assesses whether model can correctly link the same person by analyzing its description of video containing that person. Data Collection. We manually select several individuals, each denoted as Pi. For each individual, we collect IPi4 images depicting the same individual. For each image Ii IPi, we select the distractor images Ii / IPi that has the highest CLIP similarity (Hessel et al., 2021). This allows us to obtain images of different individuals where most cues are matched. For the subtask of VID, we collect videos of different individuals, denoted as VPi, and pair each with another video VPi featuring different individual with highly similar cues (e.g., actions, scene, clothing). We then construct two video sequences: (i) Pi Pi, assessing the models ability to distinguish individuals. (ii) Pi Pi Pi, evaluating whether the model detects changes and links the final occurrence of Pi to its first appearance. QA Construction. The construction for the overall QA in PCs Cpr, Cnt, and Grp subtasks follows similar approach to OC. For the VID task, we emphasize the models ability to describe individuals when designing open-ended questions, aiming to better test the models capacity to link individuals appearing in different scenes. Figure 4: Statistical overview of VLM2-Bench. The pie chart shows the distribution of 9 subtasks across the 3 main categories of visual cues. The bar plot illustrates the percentage breakdown by question format."
        },
        {
            "title": "2.4 Benchmark Statistics",
            "content": "Our benchmark is organized into three main categories, comprising total of 9 subtasks. After careful verification, it contains 3,060 question-answer 4 Baselines or Models GC Chance-Level Human-Level LLaVA-OneVision-7B LLaVA-Video-7B LongVA-7B mPLUG-Owl3-7B Qwen2-VL-7B Qwen2.5-VL-7B InternVL2.5-8B InternVL2.5-26B Mat Trk Cpr 25.00 95.06 16.60 18.53 14.29 17.37 27.80 35.91 21.24 30.50 25.00 98. 13.70 12.79 19.18 18.26 19.18 43.38 26.03 30.59 50.00 96.02 47.22 54.72 26.67 49.17 68.06 71.39 53.33 43.33 OC Cnt 34.88 94.23 56.17 62.47 42.53 62.97 45.99 41.72 55.23 51. Grp Cpr Cnt Grp VID PC Overall* Avg human 25.00 91.92 27.50 28.50 18.50 31.00 35.00 47.50 46.50 52.50 50.00 97.08 62.00 62.00 21.50 63.50 61.50 80.00 51.50 59.50 34.87 92. 46.67 66.91 38.90 58.86 58.59 57.98 60.00 59.70 25.00 91.17 37.00 25.00 18.00 26.00 49.00 69.00 52.00 61.00 - 100.00 47.25 59.00 3.75 13.50 16.25 46.50 5.25 21.75 33.72 95. 39.35 43.32 22.59 37.85 42.37 54.82 41.23 45.59 -61.44 0.00 -55.81 -51.84 -72.57 -57.31 -52.79 -40.34 -53.93 -49.57 GPT-4o 37.45 39. 74.17 80.62 57.50 50.00 90.50 47. 66.75 60.36 -34.80 Table 1: Evaluation results on VLM2-Bench, covering Mat (Matching), Trk (Tracking), Cpr (Comparison), Cnt (Counting), Grp (Grouping), and VID (Video Identity Describing). The highest , second , and third highest scores are highlighted. *: Overall excludes the VID due to the lack of chance-level baseline for open-ended tasks. pairs, with varying formats including T/F, multichoice (MC), numerical (Nu), and open-ended (Oe). To ensure the quality of the annotations, we perform an inter-annotator agreement (IAA) evaluation (Thorne et al., 2018) involving three annotators, resulting in high Fleiss Kappa score (Fleiss, 1971) of 0.983. Figure 4 presents the distribution of these subtasks across the three categories, along with the breakdown of different question formats. For additional details, refer to Appendix C."
        },
        {
            "title": "3.1 Metric Design",
            "content": "T/F (Matching, Tracking, Comparison): Accuracy is computed based on paired evaluation, where response is correct only if it answers (groundtruth True) and (ground-truth False) correctly. The overall accuracy across test pairs is: Accpair = (cid:80)N i=1 (cid:1) (cid:0)T + , (2) where + and denote correct predictions for and , respectively. (Counting): Absolute matching alone Numerical does not effectively reflect the severity of errors in numerical responses. To measure the extent of the error between the predicted count ˆNi and ground truth Ni, we introduce Accnum. The first step is to calculate the normalized error: ϵi = (cid:16) max (cid:12) ˆNi Ni (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Ni 1, img Ni (cid:17) , (3) 5 where img is the number of input images. We define wi = max({N img i=1)/N img }n to penalize errors in cases with fewer images and introduce α as an error amplification factor. The final accuracy over cases is: Accnum = 1 1 (cid:88) i=1 wi ϵα . (4) Multi-choice portion of correctly predicted choices. (Grouping): Accuracy is the proOpen-ended (Video Identity Describing): We use GPT-4o to score models descriptions, in combination with rule-based scoring prompts. The final accuracy Accoe is obtained by averaging the scores of all open-ended responses and rescaling them to the range of [0,1]. Additionally, we perform manual verification of GPT-4os scoring. For each model, we randomly sample 20 scored responses for review, and find only 2 instances with discrepancies, resulting in an accuracy rate of 98.89% (178/180). Refer to Appendix for more details."
        },
        {
            "title": "3.2 Evaluation Setup",
            "content": "that Evaluated Models. We evaluate eight opensource VLMs support multiple-image or video input: LLaVA-OneVision (Li et al., 2024b), LLaVA-Video (Zhang et al., 2024b), LongVA (Zhang et al., 2024a), mPLUGOwl3 (Ye et al., 2024a), Qwen2-VL (Wang et al., 2024b), Qwen2.5-VL (Team, 2025), and InternVL2.5 (Chen et al., 2024b). Additionally, we include the commercial model GPT-4o (Hurst et al., 2024) for comparison. Model LV-OV LV-Vid LongVA Owl3 Qw2-VL Qw2.5-VL In2.5-8B In2.5-26B GPT-4o Matching (Mat) Swp 49.15 49.15 46.58 52.56 52.56 55.62 51.28 51.71 61.97 Attr 53.45 53.45 53.45 55.17 55.17 74.14 52.07 58.62 56. Env 52.50 51.25 46.25 50.00 68.75 67.50 66.25 61.25 70.00 A/R 50.68 56.08 37.84 54.73 53.68 64.19 64.86 60.81 75.00 Tracking (Trk) Attr Swp 57.50 45.51 52.50 48.88 42.50 49.44 55.00 48.88 77.50 62.90 55.00 69.10 62.50 67.42 47.50 62.92 67.50 67.98 A/R 27.27 46.75 46.10 41.56 65.58 61.69 54.55 56.49 68.83 Env 70.59 67.65 60.29 73.53 63.93 64.71 60.65 66.18 64.71 Table 2: Breakdown of four mis-matched cue types in two subtasks of GC. For each model, the highest and second highest error (%) per subtask are highlighted. Baselines. We and human-level baselines (details are in Appendix D). chance-level introduce 3.3 Results and Findings Results. Table 1 presents the comprehensive performance of various models across the three categories General Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC) of our VLM2Bench, covering total of nine subtasks. Finding I: Simple tasks for humans pose significant challenges for VLMs. We observe that humans achieve near-perfect accuracy across most tasks in our VLM2-Bench. In contrast, even GPT4o, state-of-the-art model, performs significantly lower than humans, with an overall performance gap of 34.80%. For open-source models, many show performance comparable to the chance-level baseline or only slightly outperform it. Specifically, for the VID, humans can easily achieve 100% accuracy in distinguishing and linking individuals in video. However, even the best-performing model, GPT-4o, reaches only 66.75%. Errors mainly arise from failing to recognize individuals after changes or misidentifying reappearing persons as new. Finding II: Relatively consistent error patterns in Mat and Trk of GC. Table 2 shows that models struggle with mismatched cues due to swap in Mat, which requires linking two completely different cues. To identify what has changed, models must first link and match all the other cues in the context before they can determine that the swapped cue has been transformed. This task requires deeper understanding of how cues relate to each other across different instances. In contrast, Trk challenges models with mismatched cues due to add/remove, which focuses on tracking how specific cue changes. This suggests that when there is cue that appears only once, the model struggles to link 6 the non-appearing cue with the appearing cue to track the transformation process effectively. This limitation reveals models difficulty in handling cases where certain cues are missing but still need to be linked to understand the dynamic changes. Finding III: Models perform better in linking person-centric cues than object-centric cues. We selected the top three open-source models (Qwen2.5-VL-8B, InternVL2.5-8B, InternVL2.526B) and compared their performance on the three shared tasks (Cpr, Cnt, Grp) in both OC and PC. Results show that, on average, the performance on PC is higher than on OC by 7.65%, 9.75%, and 11.83% for the tasks of Cpr, Cnt, Grp, respectively. This could be due to the fact that, during training on person-related data, models are likely provided with explicit person names as anchors to person-centric cues, which helps the models better distinguish different individuals. In contrast, objects are typically trained using general category names, which may not provide such clear distinctions. Additionally, these models might have been specifically trained on large datasets that emphasize differentiating and linking individuals (Pi et al., 2024a; Dai et al., 2024), thereby enhancing their ability to link person-centric cues."
        },
        {
            "title": "4 How Prompting Methods affect VLMs",
            "content": "In this section2, we investigate various prompting methods (language-side and vision-side) to evaluate their impact on performance in VLM2-Bench. We select the top 3 performing open-source models (Qwen2.5-VL-8B, InternVL2.5-8B, InternVL2.526B), along with GPT-4o, and explore different approaches of CoT (Kojima et al., 2022; Wei et al., 2023) and visual prompting (VP) (Lei et al., 2024; Yang et al., 2023) (refer to Appendix for details). The goal is to investigate whether these techniques can improve performance across the benchmark and to identify the underlying factors that contribute to their success or failure."
        },
        {
            "title": "4.1 Probing for General Cue (GC)",
            "content": "Methods. (i) CoT-normal  (Table 20)  encourages the model to solve the task step by step, allowing it to reason through the problem. (ii) CoT-special  (Table 21)  guides the model to solve the task using thought process closer to how humans typically approach it. (iii) VP-grid (Figure 12) is adapted 2Due to space limits, we reference most case studies, figures, and details in the Appendix within this section. correctly under the vanilla setting. However, as shown in Figure 17, GPT-4o successfully resolves previously incorrect case by effectively leveraging the cues introduced through visual prompting while utilizing its strong visual perception abilities. (a) Results of CoT-normal, CoT-special, and VP-grid on GC. 4.2 Probing for Object-centric Cue (OC) (i) CoT  (Table 20)  . (ii) VP-zoom-o Methods. (Figure 13) uses an open-set detector (Ren et al., 2024) to obtain bounding boxes, which are then cropped to focus the models attention on objectcentric cues. By eliminating irrelevant non-object cues and emphasizing the object-centric cues, it enhances the models ability to better focus on the most relevant visual information. Finding VI: The open-ended nature of language may hinder object grouping. Unlike GC that link instance-level cues, OC requires grouping similar objects based on fine-grained visual details. As shown in Figure 5b, InternVL2.5 using CoT struggles with this task because the open-ended nature of language leads to both limited coverage of subtle visual cues (see Figure 18) and inconsistent representations of the same cues, introducing ambiguity, making it harder for models to reliably align and group matching objects. Finding VII: Amplifying object cues benefits stronger models while having minimal impact on others. From Figure 5b, we observe that for models with strong vision capabilities like GPT-4o, our VP-zoom-o method further enhances performance. For other models, this method at least ensures that the performance remains on par with the vanilla approach, without causing any degradation."
        },
        {
            "title": "4.3 Probing for Person-centric Cue (PC)",
            "content": "(i) CoT  (Table 20)  . (ii) VP-zoom-p Methods. (Figure 14) utilizes face detector (Geitgey, 2016) to obtain bounding boxes of faces-the most distinguishing feature of different individuals. It then crops the image to focus only on the face, thereby minimizing the interference from distractor cues such as clothing and other background elements. Finding VIII: CoT and visual prompting fail to improve linking on highly abstract personcentric cues, leading to performance drop. From Figure 5c, we observe that for almost all models, neither CoT (language-based) nor VP-zoom-p (vision-based) lead to improved performance. This is because facial features are highly abstract, and (b) Results of CoT and VP-zoom-o on OC. (c) Results of CoT and VP-zoom-p on PC. Figure 5: Performance gains or losses (%) when applying different prompting methods on VLM2-Bench. from previous work (Lei et al., 2024) for our tasks, overlaying dot matrix on the image as visual anchors to provide positional references and enhance the models performance. Finding IV: Reasoning in language aids models in logically linking visual cues. From Figure 5a, it is evident that both CoT-normal and CoT-special, which reasoning in language, positively impact model performance in most cases. As demonstrated in Figure 15, CoT-special improves performance by first having the model explicitly write out the cues present in each image, followed by using language to make inferences. This process helps reduce the models error rate by structuring the task and providing clearer logical guidance. This suggests that when models are linking general visual cues, using language to help structure the logical flow of the process can be beneficial. Finding V: Effectiveness of visual prompting depends on models ability to interpret both prompting cues and the visual content. As shown in Figure 5a, VP-grid negatively impacts GC performance for QwenVL2.5, causing significant drop compared to the vanilla approach. Figure 16 reveals that this decline stems from the models difficulty in interpreting the visual coordinates within the prompt, leading to misinterpretation of the cues and causing it to fail cases it originally answered 7 CoT methods struggle to effectively describe them in words. Additionally, VP-zoom-p fails because current models visual capabilities are insufficient to accurately perceive facial features."
        },
        {
            "title": "5 Related Work",
            "content": "Advancements in vision-language models (Hurst et al., 2024; Team, 2025; Zhang et al., 2024a; Li et al., 2024b; Ye et al., 2024a; Chen et al., 2024b; Liang et al., 2024b) have significantly broadened their capabilities. Previously restricted to processing single-image inputs, many VLMs can now handle multi-image and even video inputs, allowing them to capture richer and more dynamic visual contexts. Additionally, with access to growing volume of high-quality visual-textual paired training data (Pi et al., 2024b; Garg et al., 2024; Chen et al., 2023; Zhang et al., 2024c; Wang et al., 2024c), these models have shown substantial improvements in perceiving subtle visual cues and their relationships, enabling them to engage in more nuanced reasoning about visual content. Furthermore, VLMs are increasingly applied in real-world scenarios, including navigation (Weerakoon et al., 2024), planning (Yang et al., 2024), and autonomous driving (Jiang et al., 2024), solidifying their role in bridging vision and language for practical applications. However, to truly integrate into everyday life, VLMs still have significant room for improvement when it comes to more fundamental but common visual tasks, such as those assessed in our benchmark. Benchmarking vision-language models plays critical role in guiding their future development (Liang et al., 2024a; Yin et al., 2023; Chen et al., 2024a). These benchmarks typically focus on assessing the models fine-grained perception (Li et al., 2024a; Tong et al., 2024), reasoning abilities (Lu et al., 2022; Yu et al., 2023; Wu et al., 2024), commonsense knowledge (Yue et al., 2024). In addition, evaluations targeting multi-image and video inputs are designed to measure the new competencies that VLMs require as their visual context extends. These tasks include captioning (Yue et al., 2024; Yu et al., 2019), retrieval (Wang et al., 2024a; Li et al., 2025), comparison (Wu et al., 2025; Jiao et al., 2024), and temporal reasoning (Liu et al., 2024b). However, existing benchmarks focus on evaluating VLMs ability to interpret visual cues based on their knowledge. In contrast, humans typically solve such tasks by explicitly matching visual cues without relying on extensive background knowledge. To better assess whether they can replicate this human-like ability, we propose VLM2Bench, which focuses on linking and matching explicit visual cues."
        },
        {
            "title": "6 Takeaways",
            "content": "Based on our findings, we highlight three key areas for future improvements: Strengthening Fundamental Visual Capabilities. Improving core visual abilities not only enhances overall performance but also increases adaptability. stronger visual foundation maximizes the effectiveness of visual prompting and reduces reliance on prior knowledge, enabling models to operate more independently in vision-centric tasks. Balancing Language-Based Reasoning in Vision-Centric Tasks. Integrating language into vision-centric tasks requires careful calibration. Future research should establish clearer principles on when language-based reasoning aids visual understanding and when it introduces unnecessary biases, ensuring models leverage language appropriately. Evolving Vision-Text Training Paradigms. Current training paradigms focus heavily on emphasizing vision-language associations. However, as models expand their visual context window, their ability to reason purely within the visual domain becomes increasingly crucial. We should prioritize developing models that can structure, organize, and infer relationships among visual cues."
        },
        {
            "title": "7 Conclusion",
            "content": "In summary, we introduce VLM2-Bench, novel benchmark designed to probe the capability of vision-language models (VLMs) in visually linking matching cues, an essential yet underexplored skill for models in everyday visual reasoning. Through extensive evaluations and further analysis of prompting techniques applied on our benchmark, we identify 8 key findings. Notably, even GPT-4o falls 34.80% behind human performance. Based on these insights, we advocate for advancements in fundamental visual capabilities, better integration of language-based reasoning, and the evolution of vision-text training paradigms to improve VLMs performance in vision-centric tasks."
        },
        {
            "title": "Limitations",
            "content": "VLM2-Bench focuses on evaluating visual cue linking but does not cover all possible scenarios. Additionally, while it provides valuable insights, its scale is limited, and model performance may not fully generalize to all real-world settings. Automated evaluation constraints limit the inclusion of open-ended questions in our benchmark, impacting the assessment of models vision-centric reasoning abilities. Expanding task diversity and refining evaluation methods remain important directions for future work."
        },
        {
            "title": "References",
            "content": "Vicki Bruce and Andrew Young. 1986. Understanding face recognition. British journal of psychology, 77 ( Pt 3):30527. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Improving large multiLin. 2023. Sharegpt4v: Preprint, modal models with better captions. arXiv:2311.12793. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. 2024a. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. 2024b. Internvl: Scaling up vision foundation models and aligning for In Proceedings of generic visual-linguistic tasks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198. Dawei Dai, Xu Long, Li Yutang, Zhang Yuanhui, and Shuyin Xia. 2024. Humanvlm: Foundation for human-scene vision-language model. Preprint, arXiv:2411.03034. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Joseph Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378. Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. 2024. Imageinwords: Unlocking hyper-detailed image descriptions. Preprint, arXiv:2405.02793. Adam Geitgey. 2016. Machine learning is fun! part 4: Modern face recognition with deep learning. Medium. Medium Corporation, 24:2016. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. 2024. Senna: Bridging large vision-language models and end-to-end autonomous driving. Preprint, arXiv:2410.22313. Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, and Ying Shen. 2024. Img-diff: Contrastive data synthesis for multimodal large language models. arXiv preprint arXiv:2408.04594. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. 2023. Imagenhub: Standardizing the evaluation of condiarXiv preprint tional image generation models. arXiv:2310.01596. Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. 2024. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. Preprint, arXiv:2402.12058. Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. 2024a. Naturalbench: Evaluating visionlanguage models on natural adversarial samples. arXiv preprint arXiv:2410.14669. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024b. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, et al. 2025. Migician: Revealing 9 the magic of free-form multi-image grounding in multimodal large language models. arXiv preprint arXiv:2501.05767. Jongyoon Song, Sangwon Yu, and Sungroh Yoon. 2024. Large language models are skeptics: False negative problem of input-conflicting hallucination. arXiv preprint arXiv:2406.13929. Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, and LouisPhilippe Morency. 2024a. Hemm: Holistic evaluation of multimodal foundation models. arXiv preprint arXiv:2407.03418. Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2024b. Foundations & trends in multimodal machine learning: Principles, challenges, and open questions. ACM Computing Surveys, 56(10):1 42. Haowei Liu, Xi Zhang, Haiyang Xu, Yaya Shi, Chaoya Jiang, Ming Yan, Ji Zhang, Fei Huang, Chunfeng Yuan, Bing Li, et al. 2024a. Mibench: Evaluating multimodal large language models over multiple images. arXiv preprint arXiv:2407.15272. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. 2024b. Tempcompass: Do video arXiv preprint llms really understand videos? arXiv:2403.00476. Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. 2024c. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521. Romina Palermo and Gillian Rhodes. 2007. Are you always on my mind? review of how face perception and attention interact. Neuropsychologia, 45:7592. Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. 2024a. Personalized visual instruction tuning. arXiv preprint arXiv:2410.07113. Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. 2024b. Image textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv preprint arXiv:2406.07502. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. 2024. Grounded sam: Assembling openworld models for diverse visual tasks. Preprint, arXiv:2401.14159. Qwen Team. 2025. Qwen2.5-vl. James Andreas Vlachos, Christos Thorne, Christodoulopoulos, and Arpit Mittal. 2018. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578. Anne Treisman and Garry A. Gelade. 1980. featureintegration theory of attention. Cognitive Psychology, 12:97136. Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. 2024a. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. 2024c. Internvid: large-scale video-text dataset for multimodal understanding and generation. Preprint, arXiv:2307.06942. Kasun Weerakoon, Mohamed Elnoor, Gershom Seneviratne, Vignesh Rajagopal, Senthil Hariharan Arul, Jing Liang, Mohamed Khalid Jaffar, and Dinesh Manocha. 2024. Behav: Behavioral rule guided autonomy using vlms for robot navigation in outdoor scenes. Preprint, arXiv:2409.16484. Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. 2024. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. 10 In Proand reasoning benchmark for expert agi. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024a. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024b. Video instruction tuning with synthetic data. Preprint, arXiv:2410.02713. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024c. Video instruction tuning with synthetic data. Preprint, arXiv:2410.02713. Bingchen Zhao, Yongshuo Zong, Letian Zhang, and Timothy Hospedales. 2024. Benchmarking multiimage understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning. arXiv preprint arXiv:2406.12742. Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, et al. 2025. Towards open-ended visual quality comparison. In European Conference on Computer Vision, pages 360377. Springer. Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. 2022. Grit: generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280. Shujin Wu, Yi Fung, Sha Li, Yixin Wan, Kai-Wei Chang, and Heng Ji. 2024. MACAROON: Training visionlanguage models to be your engaged partners. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 77157731, Miami, Florida, USA. Association for Computational Linguistics. Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. 2023. Fine-grained visual prompting. Preprint, arXiv:2306.04356. Zhutian Yang, Caelan Garrett, Dieter Fox, Tomás Lozano-Pérez, and Leslie Pack Kaelbling. 2024. Guiding long-horizon task and motion planPreprint, ning with vision language models. arXiv:2410.02193. Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024a. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. Preprint, arXiv:2408.04840. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. 2024b. Justice or prejudice? quantifying biases in llm-as-a-judge. arXiv preprint arXiv:2410.02736. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. survey on multimodal large language models. arXiv preprint arXiv:2306.13549. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2024. Mmmu: massive multi-discipline multimodal understanding"
        },
        {
            "title": "A Appendix Outline",
            "content": "In the appendix, we provide: Appendix provides details on the licensing terms and usage rights for our benchmark. Appendix presents the statistical analysis of the VLM2-Bench. Appendix details on how we obtain the chance-level and human-level baselines. Appendix elaborates more details on the construction of the VLM2-Bench. Appendix provides deeper dive into the various prompting techniques we use. Appendix detailed breakdown and analysis of failure and success examples regarding different prompting methods."
        },
        {
            "title": "B Licencing and Intended Use",
            "content": "Our VLM2-Bench is available under the CC-BY 4.0 license for academic use with proper attribution. The images, videos, and annotations in this benchmark are intended solely for research purposes. These data were sourced from publicly available online platforms, and while efforts were made to use them responsibly, explicit permissions may not have been obtained for all content. Users are responsible for ensuring that their use of the data complies with applicable intellectual property laws and ethical guidelines. We encourage users to verify the sources and ensure compliance with any terms of service or licensing agreements. VLM2-Bench Statistics Here we provide additional details regarding the construction and statistics of our VLM2-Bench benchmark. As described in the main paper ( 2.4), our benchmark comprises three main categoriesGeneral Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC)with total of 3,060 visual-text query pairs. Below, we elaborate on the specific data composition, including the distribution of question types (T/F, multiple-choice (MC), numerical (Nu), and open-ended (Oe)) and the rationale behind each subtask. C.1 Overall Composition Table 6 provides detailed summary of the total query counts across different categories and subtasks in our benchmark. The dataset is structured into three primary categories: General Cue (GC), Object-centric Cue (OC), and Person-centric Cue (PC), comprising total of 3,060 visual-text query pairs. The General Cue (GC) category consists of 960 queries, which include 260 Matching (Mat) true/false pairs, resulting in 520 queries, and 220 Tracking (Trk) true/false pairs, leading to 440 queries. Category T/F MC Nu Oe Total GC OC PC 960 720 400 200 360 100 120 200 960 1,280 820 Total 2,080 300 480 200 3,060 Figure 6: Overview of query distribution across the three categories of VLM2-Bench. T/F = True/False, MC = multiple-choice, Nu = numerical, Oe = open-ended. The Object-centric Cue (OC) category contains 1,280 queries, covering three subtasks: Comparison (Cpr) with 360 true/false pairs (720 queries), Counting (Cnt) with 360 numerical queries, and Grouping (Grp) with 200 multiple-choice questions. Lastly, the Person-centric Cue (PC) category includes 820 queries, comprising 200 Comparison (Cpr) true/false pairs (400 queries), 120 Counting (Cnt) numerical queries, 100 Grouping (Grp) multiple-choice questions, and 200 Free-form (VID) open-ended queries. Overall, these components collectively sum up to 3,060 visual-text query pairs, offering comprehensive benchmark for evaluating vision-language models across various types of contextual cues. 12 C.2 Details per Subtask and Question Type General Cue (GC). Matching (Mat). We collect 260 True/False (T/F) pairs focused on verifying the alignment between visual instance and textual description (e.g., object presence, basic attributes). Each T/F pair forms two distinct queries (one True, one False), yielding 520 queries in total. Tracking (Trk). We design 220 T/F pairs that test an understanding of object or entity continuity across frames. For example, question might ask whether the same object reappears in subsequent frames. Each T/F pair similarly results in two queries, totaling 440. Object-centric Cue (OC). All the visual query cases are built upon the 360 image sequences we construct. Details about image sequences can be found in Section E.2. Comparison (Cpr). This subtask examines the models ability to compare object properties (e.g., size, color, quantity) across different frames. We produce 360 T/F pairs, each yielding two queries (720 total). Among these 360 pairs, we maintain 1:2 ratio of True to False for ground-truth answers (i.e., 120 True vs. 240 False). Counting (Cnt). We provide 360 numerical questions, each asking for count of objects in given scene or sequence. Possible numeric answers are typically small integers (e.g., 1, 2, 3), reflecting the number of relevant objects. Grouping (Grp). We generate 200 multiple-choice (MC) questions that ask about grouping objects according to certain criteria (e.g., AAB, ABC, AAAB, AABC, ABCD). Each question presents multiple group-configuration options plus None option, which can serve as either correct or distractor choice. For image sequences of length 4, the options include various plausible groupings (two-of-a-kind, three-ofa-kind, etc.) along with at least one additional distractor grouping that also involves three-of-a-kind to ensure sufficient challenge. Person-centric Cue (PC). Similar to OC, the construction of 260 image sequences as well as 200 video clips for PC is detailed in Section E.3. Comparison (Cpr). We create 200 T/F pairs (400 queries total) focusing on comparing attributes or actions related to one or more human individuals across multiple images in sequence. The ground truth is balanced at 100 True vs. 100 False. Counting (Cnt). This subtask involves 120 numerical questions asking for the number of people present or the frequency of certain actions in sequence. Typical numeric answers range from 1 to 4, given the scope of each visual sequence. Grouping (Grp). We provide 100 MC questions based on sequences containing at least three images, with at least two images featuring the same main meta-human. The goal is to identify correct groupings of persons based on appearance, role, or action. As with OC-Grp, each question includes None option as either the correct or distractor choice. Open-ended (VID). We introduce 200 open-ended queries that focus on various person-centric aspects, such as identifying roles or describing activities. These questions allow more flexibility in model responses and assess the ability to generate context-relevant answers. C.3 Annotation Quality and Agreement As noted in the main text, three annotators reviewed all 3,060 question-answer pairs. An inter-annotator agreement study showed high consensus rate of 98.74%, ensuring that the data is both accurate and consistent. C.4 Summary Our construction methodology ensures balanced coverage of both object-centric and person-centric reasoning, as well as basic general cues such as element matching and tracking. The inclusion of multiple question types (T/F, MC, numerical, and open-ended) further promotes comprehensive evaluation of vision-language models. Figure 4 in the main paper illustrates the distribution of these subtasks and their question-format breakdown. We believe that the richness and diversity of VLM2-Bench make it robust platform for advancing multimodal research."
        },
        {
            "title": "D Baselines",
            "content": "D.1 Chance-level In this part, we explain the calculation of chance-level accuracy for all subtasks in Table 1. GC-Mat, GC-Trk. The Matching (Mat) and Tracking (Trk) tasks in General Cue (GC) follow TrueFalse (TF) paired-question format, where each pair consists of positive question and negative question: Positive Question: Derive from the correct element or change. The ground truth (GT) answer is True (T). Negative Question: Derive from the distractor element or change. The ground truth (GT) answer is False (F). question pair example is shown in Table 3. Positive Question: \"Is the answer the salad correct for the given question: What object that was present in the first image is no longer visible in the second?\" GT Answer: Negative Question: \"Is the answer the ciabatta roll correct for the given question: What object that was present in the first image is no longer visible in the second?\" GT Answer: Table 3: Example of True-False paired questions in GC-Mat, with positive and negative question. During the construction of these questions, we ensure that the queried content originates from either the correct answer or distractor answer. These elements are designed to be independent and identically distributed. Since each question in the pair has an independent 50% chance of being answered correctly, the expected accuracy under random guessing would be (correct answer) = 2 1 2 = 1 4 = 25%. OC-Cpr, PC-Cpr. The OC-Cpr and PC-Cpr tasks utilize True-False (TF) paired-question format where both questions in pair originate from the same correct answer but are framed in two different ways: Positive Question: direct affirmative statement that correctly represents the ground truth. Negative Question: negated version of the positive question, often by inserting \"not\" after the verb. An example is shown in Table 4. This construction aims to eliminate language bias by ensuring that the model does not favor one phrasing over another. For language model that is free from bias, these two questions are logically equivalentanswering one correctly implies answering the other correctly as well. Consequently, under random guessing, the expectation is (correct answer) = 1 2 = 50%. OC-Cnt, PC-Cnt. The calculation formulas for the accuracy of the chance-level accuracy are the same as in Section 3.1. Under pure random guessing strategy, the predicted answer ˆNi is uniformly sampled from the set {1, 2, . . . , L}, where is the number of images (i.e., the sequence length for that instance). For fixed Positive Question: \"Given the images, the claim The pets in these images are the same pet. is right.\" GT Answer: Negative Question: \"Given the images, the claim The pets in these images are not the same pet. is right.\" GT Answer: Table 4: Example of True-False paired questions in OC-Cpr, with positive and negative question. sequence length L, we can compute the expected normalized accuracy E(L) by averaging over all possible ground-truth and guess pairs: E(L) = 1 1 L (cid:88) (cid:88) =1 ˆN =1 w(L) ϵ(N, ˆN )α, where and the weight is defined as ϵ(N, ˆN ) = ˆN max(N 1, ) w(L) ="
        },
        {
            "title": "Lmax\nL",
            "content": ", with Lmax = 4 being the maximum sequence length in our dataset. OC-Cnt Task: The OC-Cnt task exhibits the following distribution: Length 2: 80 sequences (22.2%) Length 3: 120 sequences (33.3%) Length 4: 160 sequences (44.4%) the overall chance level accuracy is obtained as the weighted average: AccOC-Cnt = Thus, 80 E(2)+120 E(3)+160 E(4) 360 34.88%. PC-Cnt Task: For the PC-Cnt task, the sequence distribution is: Length 2: 30 sequences (25.0%) Length 3: 25 sequences (20.8%) Length 4: 65 sequences (54.2%) Accordingly, the overall chance level accuracy is given by: AccPC-Cnt = 30 E(2)+25 E(3)+65 E(4) 34.87%. 120 D.2 Human-level To facilitate human participants in providing responses to our questions, we integrated all model-prompted questions and answer choices into graphical user interface (GUI), as illustrated in Figure 7. This interface enabled participants to select their answers conveniently, ensuring consistency in data collection. We then gathered all responses and conducted statistical analysis on the collected human evaluations. 15 Figure 7: The GUI used for human-level testing."
        },
        {
            "title": "E More details on Benchmark Construction",
            "content": "E.1 GC (General Cue) Manual Screening and Refine. Figure 8 demonstrates the Graphic User Interface (GUI) we build for manually screening image editing data. Figure 8: The GUI used for manually screening image editing data and refining edited prompts in General Cue (GC). Salient Sampling. The pseudocode in Figure 9 and Table 5 displays the calculation process for the salient sampling score mentioned in Section 2.1. Supposed you are looking at two images: Image 1: <Cap_src> Image 2: <Cap_edit> From Image 1 to Image 2, the change can be summarized as: <P> Table 5: Template for salient-score calculation, which contain three placeholders for each sample. Prompts for Pair-wise Answer Generation. Table 6 and 7 provides the complete prompts used to generate pair-wise answers for our evaluation tasks. The prompts were designed to instruct the language model to produce two distinct answersa positive (T) answer and negative (F) answerfor each task. The dual-answer format is intended to capture both the expected response and its direct opposite, thereby offering more balanced insight into the models understanding. 17 #Task Description Given the change between the first image and the second image, you need to generate four choices to the question What new element can be observed in the second image that was not present in the first?\" (this question varies based on the mis-matched cue types, here shows the question for add\" from the Add/Remove\" category). Remember, the choices lengths should be similar. Additionally, your response should start with \"Choices:\". #Pair Design In these two choices, you need to contain *only* the names of objects, but be specific: 1. Correct Answer (You need to infer the *only* from the Editing Information) 2. Distractor (You need to pick random object *only* in the Description, but differ from the correct answer object) #In-context example Editing Information: Add katana held in the figures left hand, angled downwards. Description: The image depicts person dressed in traditional Japanese armor, standing in misty, snowy landscape. The armor is detailed and appears to be made of metal, with various straps and buckles. The person is wearing black mask that covers their entire face, adding to the mysterious and stealthy appearance. The background features stone lanterns and other traditional Japanese structures, which are partially obscured by the mist. The overall atmosphere is serene yet somewhat eerie, with the mist adding sense of mystery and isolation. The scene suggests historical or fantasy setting, possibly samurai or ninja in snowy, misty environment. Choices: Correct Answer: katana held Distractor: black mask #Task Editing Information: <Edit Prompt> Description: <Description> Table 6: Prompt for generating paired answers in the Matching (Mat) subtask of General Cue (GC). 18 Task Description Given the change between the first image and the second image, you need to generate four choices to the question \"What key visual difference can be observed from the first image to the second image?\". Remember, the choices lengths should be similar. Additionally, your response should start with Choices:\" and must contain Correct Answer and Direct Reverse Answer. Pair Design In the two choices, you need to contain: 1. Correct Answer (You need to infer from the Editing Information) 2. Direct Reverse Answer (You need to infer from the Editing Information and change it to the opposite) In-context example Editing Information: Swap the black ninja gloves with clean white gloves appropriate for serving. Description: The image depicts person dressed in formal attire, standing in doorway. The individual is wearing black tuxedo with white dress shirt and black bow tie. They are holding tray with several items on it. The tray contains small glass container, bottle, and small white object, possibly salt shaker or similar item. The person is also wearing black gloves, which are typical for serving or formal dining scenarios. The background shows wooden door with brass hinge and light-colored wall. The setting appears to be indoors, possibly in house or formal establishment. Choices: Correct Answer: The black ninja gloves were replaced with clean white gloves. Direct Reverse Answer: The clean white gloves were replaced with black ninja gloves. #Task Editing Information: <Edit Prompt> Description: <Description> Table 7: Prompt for generating paired answers in the Tracking (Trk) subtask of General Cue (GC). 19 Algorithm 1 Salient Score Computation 1 # cap_src : caption for the source image 2 # cap_edit : caption for the edited image 3 # T: template for constructing paragraph 4 # P: editing prompt 5 input_text = concat ( cap_src , cap_edit , T) 6 in_tokens = tokenizer . encode ( input_text ) 7 out_tokens = tokenizer . encode (P ) 8 log_sum = 0 9 tokens = in_tokens 10 11 # Model Forward Pass 12 for in range (1 , len ( out_tokens )): outputs = model ( tokens ) 13 logits = outputs . logits 14 15 16 17 18 19 20 21 22 23 24 # Normalize the total log probability as the salient_score 25 salient_score = log_sum / len ( out_tokens ) 26 27 # Return : salient_score # Extract log probability of next token probs = log_softmax ( logits [0 , -1, :]) prob = probs [ out_tokens [i ]] log_sum += prob # Update Input Sequence tokens = concat ( tokens , out_tokens [i ]) Figure 9: Pseudocode for salient score computation in the phrase of Salient Sampling in the construction of GC. E.2 OC (Object-centric Cue) Data Collection. To construct the dataset, we follow structured approach to collect object-centric images, as illustrated in Figure 10. In total, we manually collected 320 images for objects. Main Meta-Object Selection. We predefine 8 types of common objects, with each type containing 5 meta-objects. For each meta-object, we collect four images that represent the same object from different angles and scene conditions. Distractor Meta-Object Selection. To build meaningful object image sequences, we introduce visually distractive elements for each main meta-object, referred to as distractor meta-objects. Specifically, for each main meta-object, we collect four additional images that belong to different but visually similar meta-objects within the same object category. These images are selected following predefined visual cue confusion principles, ensuring that they provide meaningful challenges for vision language models. We ensure that each distractor image belongs to different distractor meta-object, fundamentally guaranteeing that the count of different meta-objects in the final constructed sequence strictly follows our design. The principle of selecting distractor meta-objects is illustrated in the outer ring of Figure 10. Image Sources. The images are gathered from various sources based on the nature of the objects: Plush Objects: Images of plush toys are entirely sourced from the Jellycat website and its review sections, where diverse user-uploaded images provide wide variety of object angles and scenes. Pet Objects: For the pet category of meta-objects, we source images from combination of social media accounts of popular pet influencers pet photography. We also include images of ragdoll cat owned by one of the authors. As result, this approach guarantees that each pet meta-object within the dataset belongs to the same individual cat or dog, minimizing variability unrelated to visual cue confusion. Other Objects: Most images are collected from Amazon product listings and review sections containing user-uploaded photos. smaller portion of the dataset is curated using Google Lens image search, where specific visual distractive cues are used to retrieve and manually select images. The detailed visual cue principles guiding this selection process can be found in Figure 10. 20 Figure 10: The overview of the structured design of the Object-centric Cue (OC) images. Central Layer (Main Meta-Objects): The innermost circle represents the predefined 8 object categories, which serve as the foundation for our dataset. These categories include Pet, Plush, Bag, Book, Cup, Shirt, Shoes, and Toy. Each category consists of 4 main meta-objects. Middle Layer (Example Meta-Objects within Each Category): Each segment surrounding the center showcases representative main meta-object within its category. These meta-objects serve as core instances for data collection. For example, the Pet category includes Cat and Dog, while the Bag category includes Backpack, Schoolbag and Fashion Bag. Outer Layer (Distractor Meta-Objects & Visual Cue Distraction Principles): The outermost ring presents 1 out of 4 distractor meta-objects specifically selected to create challenging image sequences. Each distractor meta-object shares one or more distractive visual cues with its corresponding main meta-object. Images Sequence Construction. The construction of image sequences in OC (a total of 360 sequences) follows the structure in Table 8. More specific details are listed below: Two-Image Sequences (image_seq_len = 2) 1. Main Meta-Object Only (AA): Two images are randomly sampled from the same main meta-object. 40 sequences are constructed (one for each main meta-object). 2. Main Meta-Object + Distractor Meta-Object (AB): One image is randomly selected from the main meta-object, and one from the corresponding distractor meta-object. 40 sequences are constructed. Three-Image Sequences (image_seq_len = 3) 1. Main Meta-Object Only (AAA): Three images are randomly sampled from the same main metaobject. 40 sequences are constructed. 2. Main Meta-Object + Distractor Meta-Object (AAB): Two images are selected from the main meta-object, and one from the distractor meta-object. The order of images is shuffled. 40 sequences are constructed. 3. Main Meta-Object + Distractor Meta-Objects (ABC): One image is selected from the main metaobject, while two are selected from different distractor meta-objects. 40 sequences are constructed. Four-Image Sequences (image_seq_len = 4) 1. Main Meta-Object Only (AAAA): All four images are sampled from the same main meta-object and shuffled. 40 sequences are constructed. 2. Main Meta-Object + Distractor Meta-Object (AAAB): Three images are sampled from the same main meta-object, while one is selected from distractor meta-object. 40 sequences are constructed. 3. Main Meta-Object + Distractor Meta-Objects (AABC): Two images are selected from the main meta-object, while two are selected from different distractor meta-objects. 40 sequences are constructed. 4. Main Meta-Object + Distractor Meta-Objects (ABCD): One image is selected from the main meta-object, while three are selected from different distractor meta-objects. 40 sequences are constructed. Question Templates. Table 9, 10 and 11 list detailed standard question templates (with format instructions) for the Object-centric Cue task, including 3 subtasks: Comparison (cpr), Counting (Cnt), and Grouping (Grp). E.3 PC (Person-centric Cue) Data Collection. We collect images of meta-humans mainly from https://www.imdb.com/ and some are from the actor or actresss social media. Main Meta-human Selection. Our dataset is evenly distributed across different racial groups (Asian, Black, and White) and genders (Male and Female). For every race-gender combination, we select five main meta-humans, each contributing four images, yielding total of 120 images. To ensure consistency, all selected individuals are within similar age range, preventing significant agerelated facial changes that could interfere with identity recognition. Additionally, each actors appearance remains relatively consistent in terms of makeup and overall styling, ensuring that different images of the same meta-human retain distinct yet comparable visual cues (e.g. face shape, eye spacing, nose structure, and lip contours). By preserving these features, we avoid manipulating single individuals visual cues that could potentially mislead VLMs. Rather, we ensure that the evaluation genuinely tests whether the model can visually link matching cues to recognize the same or different individuals without prior identity knowledge. 22 Num 2 3 3 3 4 4 4 Src AA AB AAA AAB ABC AAAA AAAB"
        },
        {
            "title": "ABCD",
            "content": "Process of Image Sequences Construction 2 images from the same object Oi, randomly sampled as IOi = {Ii, Ij}, and shuffled. 1 image Ii from IOi and 1 image Ii from distractor set IOi, randomly shuffled. 3 images from the same object Oi, randomly sampled as IOi = {Ii, Ij, Ik}, and shuffled. 2 images from the same object Oi, randomly sampled as IOi = {Ii, Ij} and 1 Ii from distractor set IOi, randomly shuffled. 1 images from the same object Oi, randomly sampled as IOi = {Ii} and 2 images {Ii, Ij} from distractor set IOi, randomly shuffled. 4 images from the same object Oi, randomly sampled as IOi = {Ii, Ij, Ik, Ip}, and shuffled. 3 images from the same object Oi, randomly sampled as IOi = {Ii, Ij, Ik} and 1 image Ii from distractor set IOi, randomly shuffled. 2 images from the same object Oi, randomly sampled as IOi = {Ii, Ij} and 2 images {Ii, Ij} from distractor set IOi, randomly shuffled. 1 images from the same object Oi, randomly sampled as Ii and 3 images {Ii, Ij, Ik} from distractor set IOi, randomly shuffled. Cpr cnt Grp T F F 2 1 3 2 4 2 3 3 - - - [Ii, Ij] [] - [Ii, Ij, Ik] [Ii, Ij] [] Table 8: Summary of multi-images sequence construction for Object-centric Cue (OC) tasks. Distractor Meta-human Selection. To introduce challenging distractors in our sequences, we compute the CLIP embedding for every image and store these embeddings in reference base. When distractor image is needed, we perform an image-to-image similarity search within this base to identify the most visually similar image that originates from different meta-human. This fine-grained matching ensures that the distractor image closely resembles the main meta-humans image, leading to more challenging image sequences. Discussion on Why Objects Require Dedicated Distractors, While Humans Do Not. In objectcentric tasks, objects are categorized into eight distinct types, with substantial differences among different types (e.g. pets and bags). Therefore, each main meta-object requires dedicated distractors from the same object type to ensure meaningful comparisons. In contrast, humans belong to single category, meaning that any meta-human can serve as distractor for another. Given that we compute CLIP embeddings to select visually similar distractors, the constructed image sequences already present significant challenge without the need for type-specific distractors. We also ensure diversity by selecting five main meta-humans for each race-gender pair, providing sufficiently large pool from which to choose suitable distractors. Corresponding to our hypothesis, in the final curated sequences, most distractor meta-humans chosen were of the same race or gender as the main meta-human. Additionally, as shown in Table 1, these curated image sequences along with our designed questions effectively challenge tested models, revealing their limited performances in visually linking matching cues on person-centric data. Images Sequence Construction. The construction of image sequences in PC (a total of 260 sequences) follows the structure in Table 12. More specific details are listed below: Two-Image Sequences (image_seq_len = 2) 1. Main Meta-Human Only (PP): Two images are randomly selected from the same main meta-human, 23 OC-Cpr Positive Question: Judge the following statement based on the images: The {obj}s in these images are the same {obj}. Provide only one correct answer: (True) or (False). Respond with either or F. GT Answer: OC-Cpr Negative Question: Judge the following statement based on the images: The {obj}s in these images are not the same {obj}. Provide only one correct answer: (True) or (False). Respond with either or F. GT Answer: Table 9: Question templates used for consistency-pair evaluation in the Comparison (Cpr) subtask of Object-centric Cue (OC). OC-Cnt Question: Answer the following question according to this rule: You only need to provide *ONE* correct numerical answer. For example, if you think the answer is 1, your response should only be 1. The Question is: How many different {obj}s are there in the input images? GT Answer: 3 (Example Answer) Table 10: The question template used for the counting (Cnt) subtask of Object-centric Cue (OC). OC-Grp Question: Answer the following question based on this rule: You only need to provide *ONE* correct answer, selecting from the options listed below. For example, if you think the correct answer is B) 1 and 2, your response should be B) 1 and 2. The Question is: Which images show the same {obj} in the input images? Choices: A) 1 and 3; B) None; C) 2 and 3; D) 1 and 2. GT Answer: A) 1 and 3 (Example Answer) Table 11: The question template used for the grouping (Grp) subtask of Object-centric Cue (OC). 24 resulting in 50 sequences. 2. Main Meta-Human + Distractor Meta-Human (PQ): One image is randomly selected from the main meta-human, and the other from distractor meta-human. The order of the images is shuffled. This results in 50 sequences. Three-Image Sequences (image_seq_len = 3) 1. Main Meta-Human Only (PPP): Three images are randomly sampled from the same main metahuman. 20 sequences are constructed. 2. Main Meta-Human + Distractor Meta-Human (PPQ): Two images are selected from the main meta-human, and one from single distractor meta-human. The order of images is shuffled. 30 sequences are constructed. 3. Main Meta-Human + Distractor Meta-Humans (PQR): One image is selected from the main meta-human, while the other two come from distinct distractor meta-humans. The order is shuffled. 10 sequences are constructed. Four-Image Sequences (image_seq_len = 4) 1. Main Meta-Human Only (PPPP): All four images are sampled from the same main meta-human. The order is shuffled. 30 sequences are constructed. 2. Main Meta-Human + Distractor Meta-Human (PPPQ): Three images are sampled from the main meta-human, while one is selected from single distractor meta-human. The order is shuffled. 20 sequences are constructed. 3. Main Meta-Human + Distractor Meta-Humans (PPQR): Two images are selected from the main meta-human, while two are selected from distinct distractor meta-humans. The order is shuffled. 20 sequences are constructed. 4. Main Meta-Human + Distractor Meta-Humans (PQRS): One image is selected from the main meta-human, while three are selected from distinct distractor meta-humans. The order is shuffled. 30 sequences are constructed. Video Construction. The video data for this benchmark is manually collected from Shutterstock3. We selected ten common activity categories that an individual can perform: clean, cook, drink, exercise, listen, play, read, ride, walk, and work. For each category, we curated 10 sets of candidate video pairs, and each set consists of two videos. To ensure motion consistency and length diversity, we carefully structured the final videos by concatenating clips while keeping the total duration within the 0-100s time range. Figure 11 displays the sketch of concatenated video length distribution. The final compositions followed two formats: ->P format (same length for each clip). : direct concatenation of two distinct clips 3https://www.shutterstock.com 25 Figure 11: Distribution of video duration in the subtask of Video Identity Description (VID) in Person-centric Cue (PC). Num 2 2 3 3 4 4 4 4 Src PP PQ PPP"
        },
        {
            "title": "PQRV",
            "content": "Process of Image Sequences Construction 2 images from the same person Pi, randomly sampled as IPi = {Ii, Ij}, and shuffled. 1 image Ii from IPi and 1 image Ii from distractor set IPi, randomly shuffled. 3 images from the same person Pi, randomly sampled as IPi = {Ii, Ij, Ik}, and shuffled. 2 images from the same person Pi, randomly sampled as IPi = {Ii, Ij} and 1 Ii from distractor set IPi, randomly shuffled. 1 image from the same person Pi, randomly sampled as IPi = {Ii} and 2 images {Ii, Ij} from distractor set IPi, randomly shuffled. 4 images from the same person Pi, randomly sampled as IPi = {Ii, Ij, Ik, Ip}, and shuffled. 3 images from the same person Pi, randomly sampled as IPi = {Ii, Ij, Ik} and 1 image Ii from distractor set IPi, randomly shuffled. 2 images from the same person Pi, randomly sampled as IPi = {Ii, Ij} and 2 images {Ii, Ij} from distractor set IPi, randomly shuffled. 1 image from the same person Pi, randomly sampled as Ii and 3 images {Ii, Ij, Ik} from distractor set IPi, randomly shuffled. Cpr cnt Grp T F F 2 1 3 2 4 2 3 3 - - - [Ii, Ij] [] - [Ii, Ij, Ik] [Ii, Ij] [] Table 12: Summary of multi-images sequence construction for Person-centric Cue (PC) tasks. 26 ->P ->P format : sequence where the first clip and the third clip are sampled from the same candidate video, while the second clip is sampled from the second candidate video (same length for the three clips). Regardless of the different default sampling methods for our baseline models in Table 13, both ->P and ->P ->P formats ensure that every video clip has frames included in the sampling process: Uniform Sampling (8/16 frame): Each clip contributes proportionate number of frames based on the total video length. Since in one concatenated video, all the sampled clips are the same length, this method guarantees at least 2 frames for each clip can be sampled as model input frames. FPS Sampling (1fps): Since frames are sampled at fixed rate, the structure of ->P and ->P - >P ensures that each clip is present long enough for multiple frames to be captured, regardless of its placement in the sequence. Model Name Uniform (8/16) FPS (1fps) LLaVA-OneVision-7B LLaVA-Video-7B LongVA-7B mPLUG-Owl3-7B Qwen2-VL-7B Qwen2.5-VL-7B InternVL2.5-8B InternVL2.5-26B GPT-4o Table 13: Comparison of different video sampling methods of VLMs. Thus, by maintaining the integrity of each clips temporal structure, both ->P and ->P ->P formats effectively ensure that every clip contributes frames to the final sampled frame input for all models. Question Templates. Table 14, Table 15, Table 16, and Table 17 present the detailed standard question templates for the Person-centric Cue task, covering the four subtasks: Comparison (PC-Cpr), Counting (PC-Cnt), Grouping (PC-Grp), and Video Identity Description (PC-VID). PC-Cpr Positive Question: Judge the following statement based on the images: The individuals in these images are the same person. Provide only one correct answer: (True) or (False). Respond with either or F. GT Answer: PC-Cpr Negative Question: Judge the following statement based on the images: The individuals in these images are not the same person. Provide only one correct answer: (True) or (False). Respond with either or F. GT Answer: Table 14: Question templates used for consistency-pair evaluation in the Comparison (Cpr) subtask of Person-centric Cue (PC). 27 PC-Cnt Question: \"Answer the following question according to this rule: You only need to provide *ONE* correct numerical answer. For example, if you think the answer is 1, your response should only be 1. The Question is: How many distinct individuals are in the input images?\" GT Answer: 2 (Example Answer) Table 15: The question template used for the counting (Cnt) subtask of Person-centric Cue (PC). PC-Grp Question: Answer the following question according to this rule: You only need to provide *ONE* correct answer, selecting from the options listed below. For example, if you think the correct answer is B) 2 and 3, your response should only be B) 2 and 3. The Question is: Which images correspond to the same person in the input images? Choices: A) None; B) 2 and 3; C) 1 and 3; D) 1 and 2.\" GT Answer: D) 1 and 2 (Example Answer) Table 16: The question template used for the grouping (Grp) subtask of Person-centric Cue (PC). PC-VID Question: \"Give comprehensive description of the whole video, prioritizing details about the individuals in the video.\" Table 17: The question template used for the Video Identity Description (VID) subtask of Person-centric Cue (PC)."
        },
        {
            "title": "F More details on Prompting Approaches",
            "content": "F.1 Prompts for LLM-as-Evaluator When models answer our free-form PC-VID questions, their responses are evaluated by GPT-4o using the scoring prompts detailed in Tables 18 and 19. Specifically, for videos following P sequence, GPT-4o assesses whether the model explicitly distinguishes that the first individual (P) and the second individual (P) are different. In this case, if the model successfully makes this distinction, it receives score of 1; otherwise, it is given score of 0. For videos that exhibit P (PQP) pattern, the evaluation is more nuanced. The evaluator model (GPT-4o) checks two aspects: (1) whether the model correctly identifies that there are two distinct individuals (i.e., and P), and (2) whether the model explicitly recognizes that the final appearance belongs to the same individual as the first (P). perfect identification of both aspects yields score of 2, while correctly distinguishing the individuals without explicitly linking the final appearance to the first results in score of 1. If the model fails to distinguish between the individuals, score of 0 is assigned. #Task You are evaluating models ability to accurately distinguish between two different individuals, and Q, who appear sequentially in video (first P, then Q). Given description, your task is to determine if the model explicitly identifies that the first person (P) and the second person (Q) are different individuals. #Return Format You only need return number after \"Score:\". If you think the model correctly identifies that the two appearances belong to different individuals, return \"Score: 1\". If you think the model fails to explicitly state that there are two different individuals, return \"Score: 0\". #Description <Models Description> Table 18: Scoring prompt for VID (when video belongs to category of ->P ). #Task You are evaluating models ability to accurately distinguish between two different individuals, and Q, who appear sequentially in video following an PQP pattern (first P, then Q, then again). Given description, your task is to determine whether the model explicitly identifies that: (1) and are different individuals, and (2) The person in the final scene is the same as the first (P). #Return Format You only need return number after \"Score:\". (1) If the model correctly describes that the video follows an PQP sequence, explicitly recognizing that the first and last appearances belong to the same person (P), while the middle appearance is different person (Q), return \"Score: 2\". (2) If the model correctly identifies that there are two different people in the video (P and Q) but does not explicitly mention that the last scene returns to P, return \"Score: 1\". (3) If the model fails to recognize that two different individuals appear (e.g., treats all appearances as the same person or does not distinguish between and Q), return \"Score: 0\". #Description <Models Description> Table 19: Scoring prompt for VID (when video belongs to category of ->P ->P ). 29 F.2 Prompting Approaches for Probing on VLM2-Bench CoT (CoT-normal). The normal version of the Chain-of-Thought prompt is shown in Table 20. We simply require the model to think step-by-step to ensure self-reflection and self-correction, as well as the transparent thinking process. CoT-special for GC. Table 21 shows special version of the Chain-of-Thought prompt. According to the task features, we carefully analyze how human being approaches and visually links matching cues for questions in GC, then curate this prompt as an imitation of the human visual linking process. VP-grid for GC. Figure 12 displays complete version of Visual Prompting with Grid assistance (VP-grid). Here we follow (Lei et al., 2024) to print set of dot matrix onto the input image, accompanied by the image order dimension concatenated with Cartesian coordinates as (image order index, colum index), row index). In the detailed textual prompt design, we also integrated references and explanations for the grids, allowing VLMs to leverage this visual assistance as spatial and visual matching references. VP-zoom-o for OC. In Figure 13, we demonstrate the visual prompting process for OC. We leverage the Grounded-SAM (Ren et al., 2024) model to detect bounding boxes for objects based on their types then crop the zoomed-in objects as the image input for further VQA pairs. VP-zoom-p for PC. The visual prompting process in similar to that of OC (Figure 14). We use face detection model (Geitgey, 2016) to zoom in on the individuals face and occlude other irrelevant information. <Question> Lets think step by step to answer this question, you need to output the thinking process of how you get the answer. Table 20: CoT prompt for GC (here we denote as CoT-normal to distinguish it from the CoT-special in Table 21 that specifically designed for GC), OC, and PC. <Question> Use the following 4 steps to answer the question: Step 1. Understand the Question - Identify the questions purpose. - Check for any format requirements. Step 2. Perceive (List Elements) - List every details in each image respectively. - Note positions and attributes of elements. Step 3. Connect (Compare & Reason) - Compare corresponding elements in each image. - List all the unchanged elements and the changed element. Step 4. Conclude (Answer the Question) Table 21: CoT-special specifically designed for GC. 30 Figure 12: An illustration of how VP-grid works for GC. Figure 13: An illustration of how VP-zoom-o works for OC."
        },
        {
            "title": "G Case Study",
            "content": "This section focuses on how various prompting techniques influence model performance, highlighting their successes and limitations across different models. G.1 Case for CoT-special prompting in General Cue (GC) Task We observe that the CoT-special prompt boosts InternVL2.5-8Bs performance by over 25% than the standard query in both Matching and Tracking tasks for General Cue. While for the traditional CoT-normal prompting technique, this boost is only 13%. The CoT-special prompt  (Table 21)  directs the model through four explicit steps: understanding the question, perceiving (listing elements), connecting (comparing and reasoning), and concluding. This structured approach mirrors the human process of visual matching and is effective even for rather smaller model like InternVL2.5-8B, which might otherwise struggle with the ambiguity of complex generic step-by-step instruction (which we will discuss later in the next Subsection G.2). For example, in the provided InternVL2.5-8B response Figure 15, the model correctly executes the following: In Step 2, it identifies critical details such as \"Vase with flowers on the table\" and \"Chandelier above\" in Image 1, while noting the absence of the vase in Image 2. In Step 3, it systematically compares the two images, highlighting that while many elements remain unchanged (e.g., the chandelier, kitchen area, bowl of fruit, window), the removal of the vase is the key difference. Finally, in Step 4, the model concludes that the statement \"The vase on top of the table was removed\" accurately describes the visual change, thereby arriving at the correct answer. This detailed, multi-step breakdown not only ensures that all pertinent visual cues are captured and Figure 14: An illustration of how VP-zoom-p works for PC. processed but also reduces errors by structuring the logical flow of reasoning. The CoT-special prompts explicit instructions help InternVL2.5-8B align visual information with textual descriptions more effectively, thus enhancing overall performance. Compared to the less specific CoT-normal promptwhich may leave the model with gaps in reasoningthe CoT-special prompt provides clear, task-specific guidance that is essential for complex visual reasoning tasks, as evidenced by the substantial performance improvement. G.2 Case for VP-grid in General Cue Task The VP-grid (Visual Prompting with Grid assistance) method enhances visual matching in General Cue tasks by overlaying dot matrix grid onto the input image. Each dot is annotated with three-dimensional coordinate tuple, (image order index, column index, row index), where the first dimension distinguishes the sequence of images (e.g., the first image is indexed as 1 and the second as 2). This grid is further supported by detailed textual descriptions that clarify the coordinate system, enabling Vision-Language Models (VLMs) to use these cues for spatial and visual matching. example failure case in VP-grid. However, this approach does not yield consistent improvements across all models. For instance, the Qwen2.5-VL-7B model demonstrates significant performance dropnearly 20%when using VP-grid. An example failure case is in Figure 16. Our analysis reveals that although the model correctly identifies visual elements (e.g., pedestrian with high-visibility vest at coordinates (2, 5, 3)), it fails to properly interpret the image sequence. Specifically, the model incorrectly associates the coordinates (2, 5, 3) with the first image, rather than the second, despite the explicit definition provided in the textual prompt. This misinterpretation leads to erroneous linking of visual matching cues and subsequent faulty reasoning. We suspect that the underlying issue is the limited semantic comprehension capability of the relatively smaller 7B model, which struggles with complex, predefined spatial instructions and visual assistance. example of success case in VP-grid. In contrast to models that often misinterpret or neglect spatial cues provided by VP-gridleading to errors such as mismatching image indicesGPT-4o successfully leverages these visual prompts to achieve correct visual-textual alignment. In the example at Figure 17, the model identifies the cats nose at coordinates (1, 2, 4) in the first image and at (2, 2, 4) in the second image, enabling it to accurately capture the change in the visual attribute (from lighter pink to darker black). This success stems from several key aspects of GPT-4os processing capabilities: 1. Precise Disambiguation of Image Order: The VP-grid explicitly encodes image order, which GPT-4o uses to differentiate between multiple images. This prevents the common error of conflating spatial information from distinct imagesa problem seen in smaller models. 32 Figure 15: Case study illustrating how CoT-special improves performance of the subtask of Tracking (Trk) in General Cue (GC). The model, InternVL2.5-8B, demonstrates step-by-step reasoning process: In Step 2, it identifies key details such as Vase with flowers on the table\" and \"Chandelier above\" in Image 1, while noting the absence of the vase in Image 2. In Step 3, it compares the images, recognizing that while many elements remain unchanged (e.g., chandelier, kitchen area, fruit bowl, window), the vases removal is the primary difference. In Step 4, the model concludes that the statement \"The vase on top of the table was removed\" accurately reflects the visual change, leading to the correct answer. 33 2. Robust Visual Matching in space: With clear coordinate annotations, the model effectively locates and compares the same physical regions across images. In this case, the exact correspondence between the cats nose in different images is recognized, which is crucial for detecting subtle visual changes. 3. Structured Reasoning Process: GPT-4o adheres to well-defined reasoning sequence in our textual guidance(perception, connection, and conclusion). By systematically linking the provided grid coordinates with the textual descriptions, it is able to deduce the key visual change accurately. Implications on Model Scale. Our analysis suggests that the enhanced performance of GPT-4o with VP-grid can be attributed to its larger model capacity. Although the detailed architecture of GPT-4o is proprietary, its ability to process complex multi-modal prompts implies that: Enhanced Semantic Understanding: Larger models are inherently better at comprehending intricate, structured prompts that combine visual and textual information. This results in more nuanced interpretation of spatial cues. Superior Visual-Textual Alignment: With greater capacity, GPT-4o can integrate and correlate the detailed spatial data (visual assistance) from the VP-grid with the corresponding textual descriptions, minimizing the risk of mis-association or errors. Effective Handling of Complexity: The advanced reasoning capabilities of larger models enable them to navigate the additional complexity introduced by VP-grid without suffering from the side effects seen in smaller models. This ensures that the additional spatial guidance improves performance rather than causing confusion. The success of GPT-4o in utilizing the VP-grid approach demonstrates that model scale plays critical role in effectively integrating complex visual and textual cues. By accurately disambiguating image order and performing precise spatial matching, GPT-4o not only avoids the pitfalls encountered by smaller models but also benefits significantly from the additional visual assistance, leading to an overall performance improvement of approximately 10%. G.3 Case for CoT prompting in Object-centric Cue Task The task design for Object-centric cue (OC) and person-centric cue (PC) requires multiple images (more than 2) as sequence input. We observe that, unlike General Cue (GC) tasks where models are required to link instance-level cues, OC tasks demand that models group similar objects based on fine-grained visual features. As illustrated in Figure 5b, models using the CoT approach sometimes struggle to provide comprehensive overview of vision-based cues across sequence of images. detailed case in Figure 18 is provided by InternVL2.5-26Bs response. The ground truth and Vanilla responses correctly identify that there is no grouping for the same meta-object in the sequence, with the answer D) None. In the CoT response, the model states: \"The second and third images both have dinosaurs wearing sunglasses\". Although the description here is true, its ambiguity and lack of detailed coverage lead the model to incorrectly select option C) 2 and 3, rather than the correct option D) None. Because if we take closer look at the design on the backpack in image 3, the dinosaur with sunglasses is actually holding keyboard instead of skateboard in image 2. This is distractive visual matching cue we intend to capture during the distractor meta-object selection. This major difference should have prevented models from grouping image 2 and image 3 together. According to our findings, this misgrouping occurs for two main reasons: 1. Insufficient Overview of Visual Cues: The CoT prompt does not force the model to systematically verify all critical details across multiple images. As result, the model overlooks nuanced differences, such as the design discrepancy on the backpack in image 3, where the dinosaur holds keyboard rather than skateboard. Figure 16: Case study illustrating why VP-grid leads to performance degradation in Qwen2.5-VL-7B. The model correctly identifies visual elements (e.g., pedestrian with high-visibility vest at coordinates (2, 5, 3)) but fails to interpret the image sequence correctly. It mistakenly associates the coordinates with the first image instead of the second, despite the explicit definition in the textual prompt. This misinterpretation results in incorrect visual cue linking and faulty reasoning, highlighting the models difficulty in handling structured spatial instructions and visual prompts. 2. Variability in Descriptive Language: The open-ended language generated by the CoT approach can lead to inconsistent descriptions. In this case, the model generalized the visual cue of \"dinosaur design\" without capturing the specific attribute (i.e., the object the dinosaur is holding), which is crucial for correct grouping. Thus, the lack of structured guidance in the CoT prompt leads to the dropping or misinterpretation of critical cues, resulting in incorrect grouping decisions for multi-image sequences in OC tasks. This analysis underscores the importance of more detailed structured intermediate reasoning strategies, such as those provided by tailored CoT-special prompt, to ensure that all relevant visual details are captured and compared accurately. 35 Figure 17: Case study demonstrating why VP-grid leads to performance improvement for GPT-4o. Unlike models that often misinterpret or overlook spatial cues, GPT-4o effectively uses VP-grid to align visual and textual information. In the example shown in Figure 17, the model correctly identifies the cats nose at coordinates (1, 2, 4) in the first image and (2, 2, 4) in the second, accurately capturing the visual change in the attribute (from lighter pink to darker black). This success highlights GPT-4os ability to handle structured spatial prompts and improve performance through visual prompting. Figure 18: Case study illustrating why CoT leads to performance degradation. In the example shown in Figure 18, InternVL2.5-26Bs response correctly identifies that no grouping occurs for the same meta-object in the sequence, with the correct answer being D) None. However, in the CoT response, the model incorrectly selects option C) 2 and 3. While it correctly states that the second and third images both have dinosaurs wearing sunglasses,\" the lack of detailed analysis leads to an inaccurate conclusion. closer examination reveals key difference between the imagesthe dinosaur in image 3 is holding keyboard instead of skateboard, which should have prevented the grouping of the two images. This highlights the importance of providing more detailed and unambiguous cues in CoT reasoning."
        }
    ],
    "affiliations": [
        "CMU",
        "HKUST",
        "MIT"
    ]
}