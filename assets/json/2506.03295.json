{
    "paper_title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem",
    "authors": [
        "Yubo Wang",
        "Ping Nie",
        "Kai Zou",
        "Lijun Wu",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs."
        },
        {
            "title": "Start",
            "content": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem Yubo Wang1,2, Ping Nie5, Kai Zou3, Lijun Wu4, Wenhu Chen1,2 1University of Waterloo, 2Vector Institute, 3Netmind.AI, 4Shanghai AI Lab, 5Independent 5 2 0 2 3 ] . [ 1 5 9 2 3 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on single problem (Wang et al., 2025a) can unleash these models reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises critical question: Is there more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to single problem and using teacher LLMs to provide detailed critiques. We finetune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight oneshot CFT as simple, general, and computeefficient approach to unleashing the reasoning capabilities of modern LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have recently achieved impressive results on mathematical and scientific reasoning tasks (Achiam et al., 2023; Yang et al., 2025; Hendrycks et al., 2021; Lewkowycz et al., 2022; Wang et al., 2024; Du et al., 2025), demonstrating strong generalization 1 and reasoning capabilities. Among various posttraining methods, reinforcement learning with verifiable rewards (RLVR) (Guo et al., 2025) has shown particular promise in enhancing reasoning ability by enabling models to learn through trial-and-error exploration (Zeng et al., 2025; Ma et al., 2025). Interestingly, recent studies reveal that even single training example can significantly improve model performance through 1-shot RLVR (Wang et al., 2025a). These findings suggest that base models inherently possess substantial reasoning potential, which can be effectively unleashed with minimal and targeted training signals. However, RLVR methods suffer from two major drawbacks. First, it is highly resource-intensive, often requiring over 100 GPU hours to fine-tune even 7B model on single problem (Wang et al., 2025a). Second, due to issues like transient nonstationarity and plasticity loss, RLVR often exhibits unstable or unreliable training dynamics (Dang and Ngo, 2025; Goldie et al., 2024; Igl et al., 2020). Another popular post-training method is supervised fine-tuning (SFT). While SFT is more stable than RLVR, it typically depends on large volumes of high-quality data to prevent overfitting. For instance, Phi-4-Reasoning (Abdin et al., 2025) was fine-tuned on 16B tokens (over million examples), and AceMath (Liu et al., 2024) used 1.6M curated examples. However, in many reasoning tasks beyond mathematics, large-scale, highquality datasets are scarce, which can hinder the effectiveness of SFT and increase the risk of overfitting on small datasets. Recently, Critique Fine-Tuning (CFT) has emerged as promising alternative (Wang et al., 2025b). By enabling models to learn from critiques of diverse incorrect solutions, CFT can enhance the models exposure to varied reasoning patterns and mitigates overfitting. Specifically, CFT introduces diversity by allowing teacher models to critique wide range of candidate answers to given problem. Figure 1: One-shot CFT consistently improves mathematical and logical reasoning. Left: Average accuracy (%) on six mathematical reasoning benchmarks for Qwen and Llama models, comparing base, SFT, RLVR, and CFT with only one training example. Right: In-domain accuracy (%) on three logic reasoning benchmarks (BBEH subtasks) for Qwen2.5-Math-7B. Across both domains, CFT with single problem significantly outperforms standard supervised fine-tuning and matches or exceeds reinforcement learning with much lower compute. This exposes the LLM to multiple perspectives and error types, thereby more effectively unleashing its reasoning potential. This leads to the question: simple, robust, and compute-efficient paradigm for unleashing the reasoning capabilities of modern LLMs in both mathematical and logical domains. Can critiques from single problem suffice to unleash LLMs reasoning potential, achieving RLVRlevel effectiveness with minimum cost? In this work, we systematically investigate oneshot CFT as general and compute-efficient posttraining approach for both mathematical and logical reasoning tasks. As shown in Figure 2, we first generate diverse candidate solutions to single problem using multiple open-source models, then employ strong teacher LLMs to provide detailed critiques for each solution. These high-quality critiques are filtered and used to fine-tune Qwen and Llama models (1.5B14B parameters) on compact yet information-rich datasets. Our experiments demonstrate that one-shot CFT yields substantial performance gains on both fronts. On six standard math reasoning benchmarks, our approach achieves up to 15% absolute improvement in average accuracy (e.g., boosting Qwen2.5Math-7B from 27% to 42%) with only 5 GPU hours. Notably, on Minerva (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024) and AMC23, one-shot CFT can improve the accuracy by more than 20%. On logic reasoning, we observe 16% average accuracy gain across three representative subtasks from BIG-Bench Extra Hard (BBEH) (Kazemi et al., 2025), including Causal Understanding, DisambiguationQA, and Time Arithmetic. Further ablation studies confirm the robustness of one-shot CFT across different seed problems and model combinations. Overall, our findings highlight one-shot CFT as"
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Advances in Post-Training for Reasoning Post-training (Ouyang et al., 2022) is crucial step in aligning pre-trained language models (LLMs) to solve specific tasks. Recently, there has been surge of interest in post-training methods for improving reasoning performance, particularly in mathematical and coding problems (Toshniwal et al., 2024; Yue et al., 2024; Shao et al., 2024; Guo et al., 2025). These methods have demonstrated significant improvements, with approaches like DeepSeekMath (Shao et al., 2024) achieving strong performance on benchmarks such as MATH-500 (Hendrycks et al., 2021) and AMC (Lewkowycz et al., 2022). To make post-training more efficient, researchers have explored methods that require minimal data. S1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025) showed notable advances by leveraging 1000 training examples to enhance reasoning capabilities. Building on this, 1-shot RLVR (Wang et al., 2025a) reduced data requirements to single example, demonstrating that base LLMs possess latent reasoning abilities that can be efficiently activated with minimal supervision. These methods highlight the potential of leveraging small amounts of data to unlock reasoning capabilities in LLMs. Our work builds on this paradigm of efficient post-training but introduces critique fine-tuning (CFT) (Wang et al., 2025b) as an alternative to supervised fine-tuning (SFT) and reinforcement 2 Figure 2: Overview of the 1-shot CFT dataset construction and the key difference between SFT and CFT training. Top: Candidate solutions to single math problem are generated, critiqued, and filtered to form the training set. Bottom: Comparison of training paradigms: (left) SFT supervises the model to generate the reference solution; (right) CFT trains the model to critique candidate solution, encouraging deeper reasoning and error analysis. learning (RL). Unlike SFT, which risks overfitting to reference solutions, CFT encourages models to analyze errors and reason critically by exposing them to diverse perspectives and error types. This approach has been shown to generalize better, especially in data-scarce scenarios. 2.2 Challenges of RLVR Recent studies have raised concerns about the robustness and reproducibility of RLVR-based methods. Hochlehnert et al. (2025) emphasized that many reported RLVR improvements rely on unstable evaluation practices, such as single-seed evaluations or small benchmark sizes, which can lead to misleading conclusions about reasoning progress. They also highlighted the importance of reproducible evaluation frameworks to ensure fair comparisons across methods. Similarly, Shao et al. (2025) demonstrated that RLVR gains often depend on spurious reward signals or model-specific priors rather than genuine reasoning improvements. For example, random or incorrect rewards were shown to produce significant performance boosts in Qwen2.5-Math models, but these effects failed to generalize to other model families like Llama or OLMo. This suggests that RLVR often amplifies pre-existing capabilities rather than teaching new reasoning skills, raising questions about its generalizability and scalability. In this context, CFT provides an alternative approach by leveraging critiques as training signals. It shows strong performance not only on mathematical reasoning tasks like MATH-500 (Hendrycks et al., 2021) and Minerva (Lewkowycz et al., 2022), but also on logic reasoning tasks from the BIGBench Extra Hard (BBEH) benchmark (Kazemi et al., 2025), demonstrating potential for generalizing across diverse reasoning domains."
        },
        {
            "title": "3 Method",
            "content": "In this section, we will detail our dataset construction and training scheme. 3.1 One-shot CFT Dataset Construction To systematically assess one-shot CFT, we construct suite of critique datasets derived from single seed problem, following the one-shot RLVR research protocol. Our process is as follows: Seed Problem Selection. We select seed math problems from the DeepScaleR subset, following the setting of previous one-shot RLVR studies. For ease of comparison, we focus on four representative problems, π1, π2, π13, and π1209, which were also analyzed in prior work. The full content of these seed problems is provided in Appendix A.3. Candidate Solutions Generation. For each seed problem, we use 10 open-source models as solution generators, including Qwen2.5Math-7B-Instruct (Yang et al., 2024), Qwen34B/8B/14B/32B (Yang et al., 2025), MiMo-7BSFT (Xia et al., 2025), MiMo-7B-RL, DeepSeekR1-Distill-Qwen-32B (Guo et al., 2025), Phi4-reasoning (Abdin et al., 2025), and Phi-4reasoning-plus. Each generator provides 10 solutions for the seed problem, resulting in 100 various candidate solutions. (see Fig. 1) Teacher Critique Annotation. We then solicit critiques for each candidate solution from 7 high3 performing, proprietary teacher models: Claude3-7-Sonnet (Anthropic, 2025), Claude-3-5-Sonnet, GPT-4.1-Mini (OpenAI, 2025a), GPT-4.1, GPT4o (Achiam et al., 2023), O3-Mini (OpenAI, 2025b), and O1-2024 (Jaech et al., 2024). rate of 5 106, using cosine learning rate schedule and warmup ratio of 0.1. The global batch size is set to 512. To ensure fair comparison, all hyperparameters are kept consistent across different model architectures and problem seeds. 3.2 Dataset Statistics"
        },
        {
            "title": "4 Experiments on Math Reasoning",
            "content": "For each seed problem, we start with 700 critiques (100 candidate solutions 7 teacher critiques). After filtering out incorrect or inconsistent critiques, we remove 43, 16, 22, and 68 examples for π1, π2, π13, and π1209, respectively. To ensure fair comparison across different seeds, we further remove the longest and shortest samples by length and subsample the remaining data to construct unified training set of 600 critiques per seed problem. Table 1 summarizes the dataset statistics, including average input and output token counts and difficulty ratings. The problems vary in difficulty: π2 and π13 are easy, π1 is medium, and π1209 is hard. Training Dataset dsr-cft-p0 dsr-cft-p1 dsr-cft-p2 dsr-cft-p3 Problem Input ID Tokens Tokens Level Output Difficulty π1 π2 π13 π1209 736 779 1763 1136 614 621 1024 992 Medium Easy Easy Hard Table 1: Statistics of the training datasets for one-shot CFT. Each dataset contains 600 critique examples per seed problem. Token counts are averaged across all examples in the dataset. 3.3 Training Following the Critique Fine-Tuning (CFT) (Wang et al., 2025b), each training instance is constructed by concatenating the original problem and candidate solution as the model input, with the corresponding teacher-provided critique serving as the target output. Specifically, each sample takes the form (x, y) c, where denotes the seed problem, is candidate solution from student model, and is the critique provided by teacher model. During training, the model learns to generate given the concatenated (x, y) as input. As illustrated in Figure 3, unlike Supervised Fine-Tuning (SFT), which directly trains the model to generate solutions, CFT emphasizes critiquing candidate solutions for correctness and step-by-step reasoning. Detailed templates and training examples are provided in Appendix A.1. We adopt full-parameter instruction tuning for all experiments. Models are trained with learning 4 4.1 Setup We conduct our experiments on four backbone models: Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Llama-3.2-3B-Instruct, and Qwen2.5-14B. For seed question selection, we follow the protocol established in one-shot RLVR studies and choose the same four representative problems: π1, π2, π13, and π1209. The corresponding CFT training datasets are denoted as dsr-cft-p0, dsr-cft-p1, dsr-cft-p2, and dsr-cft-p3. To facilitate fair comparison with supervised fine-tuning (SFT) methods, we employ the full DeepScaleR dataset (40.9K examples) as the training data for our Full SFT baseline. Additionally, for the one-example SFT (SFT-1ex) condition, we select π1 as the seed problem and use the same 7 closed-source API models to generate 100 diverse solutions. We then verify all 700 generated solutions against the ground-truth answer, retaining 600 correct responses for our final SFT (1 ex) dataset. For evaluation, we include sober baseline scores provided by Hochlehnert et al. (2025), which reassess prior backbone results under standardized framework. These baselines highlight underestimation in earlier evaluations of Qwen models, providing more rigorous reference point for comparison. We evaluate all models on six standard mathematical reasoning benchmarks: MATH-500, Minerva Math, OlympiadBench, AIME25, AIME24, and AMC23. To ensure statistical stability for the smaller benchmarks (AIME25, AIME24, AMC23), we repeat each evaluation 32 times and report the average result as the final score. 4.2 Main Results Table 2 presents the main performance comparison across different training methods, including oneshot Critique Fine-Tuning (CFT), supervised finetuning (SFT), and one-shot Reinforcement Learning with Verifiable Reward (RLVR). For validation, we randomly select 500 math problems from the MATH dataset (excluding those in the MATH-500 benchmark) to construct the validation set. During training, all models are checkpointed every 10 Figure 3: Comparison between Supervised Fine-Tuning (SFT) and Critique Fine-Tuning (CFT). SFT generates solutions directly, while CFT critiques candidate solutions for correctness. steps. The checkpoint with the highest validation score is selected for final evaluation. CFT significantly improves upon the backbone. Across all model scales, one-shot CFT consistently improves reasoning accuracy over the base models. Even when evaluated against the more rigorous sober baseline scores by Hochlehnert et al. (2025), CFT demonstrates substantial gains. For instance, on Qwen2.5-Math-7B, the backbone accuracy is revised to 32.2%, and one-shot CFT still achieves 42.2%, delivering +10.0 point improvement. CFT outperforms SFT even with full data. Under the same one-shot setting, CFT substantially outperforms SFT. For Qwen2.5-Math-7B, one-shot SFT achieves 22.9%, while one-shot CFT reaches 42.2%. Notably, one-shot CFT also surpasses SFT trained on the full dataset (25.6%), highlighting the superior generalization and reasoning gains from the critique supervision signal. CFT is competitive with or superior to one-shot RLVR. CFT demonstrates stronger performance than RLVR across most settings. On Qwen2.5Math-7B and Llama-3.2-3B-Instruct, one-shot CFT outperforms RLVR by +2.0 and +2.1 points, respectively. On Qwen2.5-Math-1.5B, CFT is slightly behind RLVR (by 1 point). 4.3 Training Efficiency Comparison 75% accuracy on the Math-500 and quickly stabilizes. In contrast, RLVR requires over 120 GPU hours to reach similar level of performance and exhibits greater fluctuations during training. This efficiency advantage is primarily due to the high computational cost of reinforcement learning, which requires many iterations to propagate reward signals. In contrast, CFT benefits from direct and dense critique supervision, enabling much faster and more stable training. Consequently, oneshot CFT matches or surpasses RLVR performance while using only about 1/15 to 1/20 of the compute. 4.4 Effectiveness of Seed Examples Table 3 compares one-shot CFT performance on datasets from different seed problems. While all seeds are effective, dsr-cft-p0 (from seed problem π1) achieves the highest average accuracy. To understand this, we assess the difficulty of each seed by prompting Qwen3-32B to grade 100 candidate solutions from Qwen2.5-Math-7B, using the grading prompt provided in Appendix A.2. Scores of 1 (correct), 0.5 (partially correct), or 0 (incorrect) are assigned and summed. Seeds of moderate difficulty, such as π1, yield balanced mix of correct and incorrect solutions, enabling richer critiques and more effective learning. As shown in Figure 4, one-shot CFT achieves significantly higher training efficiency than one-shot RLVR. With only 5 GPU hours, CFT surpasses Overall, one-shot CFT is robust to the seed choice, with moderate-difficulty seeds providing the strongest learning signal. 5 Model Method Math-500 Minerva Olympiad AIME24 AIME25 AMC23 AVG Qwen2.5-Math-1.5B Llama3.2-3B-Instruct Qwen2.5-Math-7B Qwen2.5-14B base base (sober) SFT (1 ex) SFT (full) RL (1 ex) CFT (1 ex) = CFT - base base SFT (1 ex) SFT (full) RL (1 ex) CFT (1 ex) = CFT - base base base (sober) SFT (1 ex) SFT (full) RL (1 ex) CFT (1 ex) = CFT - base base SFT (1 ex) SFT (full) CFT (1 ex) = CFT - base 35.8 51.7 37.2 49 72.4 66.6 +30.8 40.8 41.4 43.2 45.8 49.0 +8.2 58.6 64.3 53.8 58.6 79.2 76.4 +17.8 60.4 63.8 65.2 71.2 +10.8 11.0 11.3 9.6 14.3 26.8 30.1 +19. 15.8 13.2 14.7 16.5 21.0 +5.2 17.3 17.3 14.3 24.6 27.9 40.4 +23.1 22.4 19.5 24.2 43.8 +21.4 22.1 26.0 22.7 23.2 33.3 30.4 +8.3 13.2 11.7 12.1 17.0 15.3 +2.1 17.5 26.0 18.2 27.6 39.1 39.3 +21. 27.9 20.9 22.7 34.8 +6.9 15.0 11.3 3.1 7.9 11.7 10.4 -4.6 8.3 2.7 3.1 7.9 9.2 +0.9 16.7 20.7 12.1 10.0 23.8 18.8 +2.1 3.8 5.0 2.6 12.5 +8.7 2.5 5.7 0.0 2.1 7.1 8.8 +6. 1.7 0.0 1.7 1.2 2.9 +1.2 10.8 8.7 6.7 7.1 10.8 14.6 +3.8 3.8 1.2 1.7 8.3 +4.5 40.0 44.0 38.3 35.8 51.6 50.6 +10.6 25.3 23.2 24.3 25.3 32.5 +7.2 43.1 56.2 32.5 45.3 60.3 63.4 +20. 44.1 36.9 38.3 45.3 +1.2 21.1 25.0 18.5 22.2 33.8 32.8 +11.7 17.5 15.4 16.5 19.0 21.7 +4.2 27.3 32.2 22.9 28.9 40.2 42.2 +14.9 27.1 24.6 25.8 36.0 +8.9 Table 2: Performance (%) on mathematical benchmarks. The base results are measured using the same prompt and evaluation setting with SFT and CFT. The base (sober) is taken from Hochlehnert et al. (2025) with more comprehensive evaluation. The RL (1 ex) results are from Wang et al. (2025a). The delta rows show the performance difference between CFT (1 ex) and the base. Training Data Seed Score (/100) Math-500 Minerva Math Olympiad AIME25 AIME24 AMC23 AVG baseline dsr-cft-p0 dsr-cft-p1 dsr-cft-p2 dsr-cft-p3 dsr-cft-p0,p1,p2,p3 - 49.0 93.0 83.0 10.0 58. 52.6 77.0 72.4 77.0 72.6 74.6 17.3 40.4 35.7 33.1 32.4 34.6 17.5 39.3 32.1 39.1 35.4 35. 10.8 14.6 15.8 12.1 7.1 13.3 16.7 18.8 20.0 13.8 10.4 17.1 43.1 63.4 51.6 57.2 59.7 65. 26.3 42.2 37.9 38.7 36.3 40.1 Table 3: Comparison of performance (%) with different seed math problems on Qwen-2.5-Math-7B 4.5 Diversity of Candidate Solutions To analyze the effect of candidate solution diversity, we compare three settings on the seed problem π1. We use single strong generator (Phi4-Reasoning-Plus) and single weaker generator (Qwen2.5-Math-7B-Instruct) to each produce 100 candidate solutions, generate critiques, and perform CFT. Our main method, by contrast, mixes 100 candidate solutions from 10 different generators before collecting critiques and fine-tuning. As shown in Table 4, using single generator yields average benchmark scores of 38.7 (Phi4-Reasoning-Plus) and 37.6 (Qwen2.5-Math-7BInstruct). In comparison, our mixed-generator approach achieves higher average of 42.2. This demonstrates that greater diversity in candidate solutions leads to richer error types and reasoning patterns, enabling more effective critique fine-tuning."
        },
        {
            "title": "5 Experiments on Logic Reasoning",
            "content": "To further evaluate the effectiveness of one-shot CFT beyond mathematical reasoning, we conducted experiments on logic reasoning tasks from the BIG-Bench Extra Hard (BBEH) benchmark. BBEH is challenging benchmark designed to test advanced reasoning capabilities of large language models. It consists of 23 subtasks, each containing 200 questions, except for the DisambiguationQA subtask, which has 120 questions. In this section, we present one-shot CFT results on three logic reasoning subtasks: Causal Understanding, DisambiguationQA, and Time Arithmetic. 6 Solution Generators Math-500 Minerva Olympiad AIME25 AIME24 AMC23 Avg 1 generator (Phi-4) 1 generator (Qwen2.5) 10 generators (mixed) 75.8 74.4 76. 32.0 30.5 40.4 35.4 35.6 39.3 7.1 9.6 14.6 16.7 17.1 18.8 58.8 64.7 63.4 37.6 38.7 42. Table 4: Full ablation results on the diversity of solution generators in one-shot CFT. Model Causal Understanding DisambiguationQA Time Arithmetic Average Qwen2.5-Math-7B 24.0 5. SFT (1 ex) CFT (1 ex) SFT (1 ex) CFT (1 ex) SFT (1 ex) CFT (1 ex) SFT (3 ex) CFT (3 ex) Fine-tuned on 1 problem from Causal Understanding 27.5 41. 11.7 25.0 Fine-tuned on 1 problem from DisambiguationQA 20.5 34.5 9.2 24.2 Fine-tuned on 1 problem from Time Arithmetic 24.5 37. 10.8 28.3 Fine-tuned on 3 problems from the above tasks. 29.5 36.5 11.7 28.3 2.5 2.0 9. 2.0 2.5 5.0 14.0 6.5 15.5 10.5 13.7 25.2 10.6 20. 13.4 26.4 15.9 26.8 Table 5: Performance of Qwen2.5-Math-7B on three BIG-Bench Extra Hard (BBEH) logic reasoning subtasks. For each subtask, SFT and CFT are performed using single example from that subtask, and evaluated on all three tasks. The in-domain (diagonal) results and all results in the last two rows (merged CFT/SFT) are highlighted. The last two rows show results when merging the three problems into single three-example training set, evaluating generalization across all three subtasks. Best results in each column are in bold. experiments, we used the provided few-shot examples to generate critiques from teacher models and applied these critiques for fine-tuning. The models were evaluated on three subtasks to measure both in-domain and cross-domain performance. The evaluation metric is the accuracy percentage over the test sets provided by BBEH. 5.2 Main Results Table 5 summarizes the performance of one-shot CFT and SFT on the three BBEH subtasks. In-Domain Performance. One-shot CFT consistently outperforms one-shot SFT across all three subtasks when fine-tuned on an in-domain example. For instance, on Causal Understanding, one-shot CFT achieves an accuracy of 41.5%, compared to 27.5% for one-shot SFT. Similarly, for DisambiguationQA and Time Arithmetic, one-shot CFT achieves 25.0% and 14.0%, respectively, outperforming their SFT counterparts. Cross-Task Generalization. One-shot CFT demonstrates strong cross-task generalization. When fine-tuned on single example from Causal Understanding, the model achieves 25.0% on DisambiguationQA and 9.0% on Time Arithmetic, Figure 4: Comparing Model accuracy on Math-500, v.s. the training cost. For the Qwen2.5-Math-7B trained with 1-shot RL and 1-shot CFT. These tasks cover diverse reasoning types and evaluate CFTs generalization across logic domains. 5.1 Experimental Setup For each subtask, we fine-tuned Qwen2.5-Math-7B using one example from the corresponding task (1-shot CFT) and compared it to supervised finetuning (1-shot SFT). Additionally, we evaluated the cross-task generalization by fine-tuning on merged dataset containing one example from each of the three subtasks (total of 3 examples). Following the protocol used in our mathematical reasoning 7 significantly surpassing the performance of SFT. Similar trends are observed when the model is fine-tuned on examples from the other two tasks. Multi-Task Fine-Tuning. Fine-tuning on merged dataset containing one example from each subtask further boosts model performance. Oneshot CFT achieves 36.5%, 28.3%, and 15.5% accuracy on Causal Understanding, DisambiguationQA, and Time Arithmetic, respectively, outperforming one-shot SFT, which achieves 29.5%, 11.7%, and 6.5%. 5.3 Ablation Study: Impact of Model Scale on Logic Reasoning Model DisambiguationQA Qwen3-4B-Base SFT (1 ex) CFT (1 ex) Qwen2.5-Math-7B SFT (1 ex) CFT (1 ex) Qwen2.5-14B SFT (1 ex) CFT (1 ex) 15.0 18.3 33. 5.0 9.2 24.2 9.2 13.3 36.6 Table 6: Ablation study on the impact of model scale for the DisambiguationQA task. CFT consistently outperforms both base and SFT across all model sizes. To further understand the effectiveness of CFT at different model scales, we conduct an ablation study on the DisambiguationQA logic reasoning task, comparing base, SFT (1 ex), and CFT (1 ex) settings across several model sizes. As shown in Table 6, one-shot CFT delivers substantial improvements over both the base and SFT (1 ex) settings, regardless of model size. Notably, the performance boost from CFT is more pronounced as model size increases. For example, Qwen2.5-14B-CFT (1 ex) achieves 36.6% accuracy, compared to 9.2% for the base and 13.3% for SFT (1 ex). This pattern holds for smaller models as well, with CFT (1 ex) providing consistent and significant gain. These results demonstrate that critique-based supervision is highly effective at unleashing reasoning capabilities in LLMs of various scales, further validating the generality and robustness of the CFT paradigm in logic reasoning tasks. 5.4 Impact of Different Seed Problems Similar to the findings in mathematical reasoning, the choice of seed problems significantly influences Model Causal Understanding Qwen2.5-Math-7B SFT-p0 (1 ex) SFT-p1 (1 ex) CFT-p0 (1 ex) CFT-p1 (1 ex) 24.0 27.5 26.0 41.5 39.5 Model DisambiguationQA Qwen2.5-Math-7B SFT-p0 (1 ex) SFT-p1 (1 ex) CFT-p0 (1 ex) CFT-p1 (1 ex) 5.0 9.2 10.8 23.3 29.1 Table 7: Impact of Different Seed Problems on 1-shot CFT Performance in BBEH. BBEH Mini consists of 20 problems randomly sampled from each of the 23 subtasks in the full BBEH dataset. the performance of 1-shot CFT. As shown in Table 7, CFT consistently outperforms SFT for both Causal Understanding and DisambiguationQA, regardless of the specific seed problem (p0 or p1). This demonstrates the robustness of CFT across different subtasks and problem instances. In summary, our experiments on the BBEH benchmark demonstrate the effectiveness of oneshot CFT in logic reasoning tasks across both indomain and cross-domain scenarios. One-shot CFT consistently outperforms SFT in three diverse subtasks, achieving substantial improvements in accuracy. Moreover, the method exhibits strong crosstask generalization and robustness to different seed problems, aligning with our findings in mathematical reasoning. These results highlight the potential of CFT to enhance reasoning capabilities in challenging benchmarks with minimal data."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces and investigates one-shot Critique Fine-Tuning (CFT) as an efficient and effective method for unleashing the reasoning capabilities of LLMs. Using diverse student-teacher interactions on single math problem, one-shot CFT surpasses both traditional supervised fine-tuning and one-shot RLVR in accuracy, while offering up to 20 higher training efficiency. Experiments across multiple model backbones confirm its strong generalization and robustness, especially when the seed example is moderately difficult. One-shot CFT offers practical post-training solution for LLMs in computeand data-limited scenarios."
        },
        {
            "title": "Limitations",
            "content": "Our study is primarily limited to relatively weaker language models, particularly base models prior to supervised fine-tuning or distillation. When applied to already strong reasoning-oriented LLMs, our method yields mixed results. This suggests that our approach is most effective at unleashing latent capabilities in less-aligned models. However, for models that have undergone extensive alignment, our algorithm does not consistently yield further improvements. Future work is needed to investigate adaptations or extensions of our method for more capable or well-aligned models."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, and 1 others. 2025. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anthropic. 2025. Claude 3.7 sonnet. https://www. anthropic.com/claude/sonnet. Accessed: 202505-18. Quy-Anh Dang and Chris Ngo. 2025. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, and 1 others. 2025. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739. Alexander Goldie, Chris Lu, Matthew Jackson, Shimon Whiteson, and Jakob Foerster. 2024. Can learned optimization make reinforcement learning less difficult? Advances in Neural Information Processing Systems, 37:54545497. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal 9 scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. 2025. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086. Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. 2020. Transient non-stationarity and generalisation in arXiv preprint deep reinforcement learning. arXiv:2006.05826. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, and 1 others. 2025. Big-bench extra hard. arXiv preprint arXiv:2502.19187. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. 2025. General-reasoner: Advancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. OpenAI. 2025a. Gpt-4.1. https://openai.com/ index/gpt-4-1/. Accessed: 2025-05-18. 2025b. OpenAI. mini. introducing-o3-and-o4-mini/. 2025-05-18. o4 https://openai.com/index/ Accessed: Introducing and o3 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. 2025. Spurious rewards: Rethinking training signals in rlvr. Notion Blog. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. 2024. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, and 1 others. 2025a. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yubo Wang, Xiang Yue, and Wenhu Chen. 2025b. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703. Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, and 1 others. 2025. Mimo: Unlocking the reasoning potential of language model arXiv preprint from pretraining to posttraining. arXiv:2505.07608. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, and 1 others. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. 10 Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. 2025. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Training Data Format and Example Each training sample follows conversational format, consisting of user prompt (including math problem and candidate solution) and teacherprovided critique as the target output. The structure is as follows: Instruction Template: role: user content: Instruction, question and candidate solution role: assistant content: Teacher critique Concrete Example: User message: critique Please following solution question is correct. whether to the the Question: The pressure exerted by wind on sail varies jointly as the area of the sail and the cube of the winds velocity . When the velocity is 8 miles per hour, the pressure on sail of 2 square feet is 4 pounds. Find the wind velocity when the pressure on 4 square feet of sail is 32 pounds. Solution: To solve the problem, we start by expressing the relationship between the area A, and the winds velocity mathematically. Since the pressure varies jointly as the area of the sail and the cube pressure , the is of the winds velocity, we can write: = kAV 3 where constant of We are given proportionality. that when the velocity is 8 miles per hour, the pressure on sail of 2 square feet is 4 pounds. We can use this information to find the value of k. Substituting = 4, = 2, and = 8 into the equation, we get: 4 = 2 83 83 = 512 4 = 2 512 4 = 1024 = 4/1024 = 1/256 Now that we have the value of k, we can use it to find the wind velocity when the pressure on 4 square feet of sail is 32 pounds. Substituting = 32, = 4, and = 1/256 into the equation = kAV 3, we get: 32 = (1/256) 4 3 32 = (4V 3)/256 = 3/64 3 = 32 64 3 = 2048 = 3 Therefore, the wind velocity when the pressure on 4 square feet of sail is 32 pounds is: 2048 = 12 Assistant message: Critique: Let me analyze this solution: 1. Key Mathematical Concepts: - Joint variation - Cube relationship - Solving for constants using given conditions - Cube roots 2. Step-by-Step Check: Initial equation setup = kAV 3 is correct First substitution with = 4, = 2, = 8 is correct Calculation of 83 = 512 is correct Finding = 4/(2 512) = 1/256 is correct Second equation setup with = 32, = 4 is correct Algebra to solve for 3 is correct Final calculation of correct 2048 = 12 is 3 3. Solution Verification: Lets verify the final answer: When = 12, = 4, and = 1/256: = (1/256)(4)(123) = (1/256)(4)(1728) = (1728 4)/256 = 32 The students solution is completely correct. They followed logical approach, performed all calculations accurately, and arrived at the correct answer. 12 miles per hour Conclusion: right [END] This format is used for all training examples in our CFT experiments. A.2 Prompts This section provides all prompts used for dataset construction, including those for solution generation, critique generation, and grading. Solution Generation Prompts We used different prompts for each solution generator model: Qwen3 and MiMo: <im_start>user Please reason step by step to find solution to the following question, and put your final answer within boxed{}. {question}<im_end> <im_start>assistant Qwen2.5: <im_start>system Please reason step by step, put final boxed{}.<im_end> <im_start>user answer your and within {question}<im_end> <im_start>assistant Phi-4: your <im_start>system<im_sep> Please reason step by step, put final boxed{}.<im_end> <im_start>user<im_sep> {question}<im_end> <im_start>assistant<im_sep> answer and within Critique Generation Prompt You are mathematics expert. Analyze if the students solution to the given question is correct. Follow these steps: 1. Identify the key mathematical concepts and correct approach. 2. Check each step of the students solution. 3. If incorrect, point out errors and provide the correct solution, putting your final answer within boxed{}. Conclude with \"Conclusion: right/wrong [END]\" {question} {solution} Grading Prompt Below is the English prompt used for grading student answers with three discrete scores: You are grader for mathematics exam. Given the following question and reference answer, grade the students exam answer. Only give one of three possible scores: 1 point (mostly correct), 0.5 points (partially correct), or 0 points (seriously incorrect). Put your score in Final Grade: boxed{}. A.3 Seed Problem Descriptions Here we provide the full statements of the four seed math problems used in our experiments. π1: The pressure exerted by wind on sail varies jointly as the area of the sail and the cube of the winds velocity . When the velocity is 8 miles per hour, the pressure on sail of 2 square feet is 4 pounds. Find the wind velocity when the pressure on 4 square feet of sail is 32 pounds. 12 π2: How many positive divisors do 9240 and 13860 have in common? π13: Given that circle passes through points (0, 4), Q(2, 0), and R(3, 1). (1) Find the equation of circle C. (2) If the line : mx + 1 = 0 intersects circle at points and B, and AB = 4, find the value of m. π1209: Define the derivative of the (n 1)th derivative as the nth derivative (n , 2), that is, (n)(x) = [f (n1)(x)]. They are denoted as (x), (x), (4)(x), ..., (n)(x). If (x) = xex, then the 2023rd derivative of the function (x) at the point (0, (2023)(0)) has y-intercept on the x-axis of . A.4 Use of AI Assistance We used ChatGPT to capture grammar errors in the manuscript. A.5 Potential Risks Our work focuses on improving mathematical reasoning in large language models. Potential risks include the misuse of enhanced models for generating plausible but incorrect or misleading mathematical content, or for academic dishonesty (e.g., automated solution generation in educational settings). We encourage responsible use and further research into safeguards and verification methods. A.6 License for Artifacts All code and data released with this work are provided under the MIT License. Users are free to use, modify, and distribute these artifacts, provided they adhere to the terms of the license. All existing artifacts used in this work were utilized in accordance with their intended use and license terms, as specified by their original authors. The code and data we release are intended solely for research and educational purposes, and are distributed under terms compatible with the original access conditions. Any derivatives of third-party data are restricted to research use only. A.7 Data Privacy and Offensive Content All data used in this work were sourced from publicly available mathematical problem sets and do not contain any personally identifying information or offensive content. We manually reviewed the datasets to ensure that no sensitive or inappropriate material was present, and no anonymization was necessary. A.8 Documentation of Artifacts We provide documentation for all released artifacts, including descriptions of the datasets, code usage instructions, and details of supported benchmarks. The datasets consist of English mathematical problems covering range of domains such as algebra, geometry, and mathematical olympiad questions. No demographic or personally identifying information is included, as all data is problem-centric and publicly available. A.9 Parameters for Packages We report all packages and libraries used for data preprocessing and evaluation in our experiments. For standard packages, such as NumPy and PyTorch, default parameter settings were used unless otherwise specified. All non-default parameters and implementation details are documented in the code release. A.10 Case Study This case study highlights crucial difference between the base model and the CFT-enhanced model in solving modular equations. The base model (step-0) attempts to find integer solutions to 14u 46 (mod 100) by directly checking possible values, but the solutions it finds (u = 34 and = 79) do not actually satisfy the original congruence. This reflects reasoning error: the model fails to verify its candidate solutions. As result, the final answer is incorrect. In contrast, after one-shot Critique Fine-Tuning (step-20 and step-50), the model correctly applies the Extended Euclidean Algorithm to solve the congruence, finds the true solutions (u = 39 and = 895), and accurately computes the average as 64. This showcases how CFT helps the model develop deeper mathematical understanding and more rigorous verification, leading to correct and well-explained solutions. 13 Question:What is the average of the two smallest positive integer solutions to the congruence $$14u equiv 46pmod100 ?$$ Answer: Output (step0): To solve the congruence $14u equiv 46 pmod100$, we need to find the integer solutions for $u$ [...]We can use the Extended Euclidean Algorithm to find the modular inverse. Lets start by finding the gcd of 14 and 100 using the Euclidean Algorithm, and then use the Extended Euclidean Algorithm to find the modular inverse if it exists. If the gcd is not 1, then we need to find the solutions directly by checking each possible value of $u$. Lets implement this in Python. [Python Code] outputn34.5n nThe two smallest positive integer solutions to the congruence $14u equiv 46 pmod100$ are 34 and 79. The average of these two solutions is $frac34 + 792 = 56.5$. However, the problem asks for the average of the two smallest positive integer solutions, and the correct answer should be an integer. Lets verify the solutions and the average again.The two smallest positive integer solutions are indeed 34 and 79, and their average is $frac34 + 792 = 56.5$. Since the problem asks for the average, and the average is decimal, we should consider the integer part of the average, which is 56.Therefore, the final answer is $boxed56$. Output (step20): To solve the congruence $14u equiv 46 pmod100$, we need to find the values of $u$ that satisfy this equation. Lets go through the steps: [...] We can use the Extended Euclidean Algorithm to find this inverse.3. **Use the Extended Euclidean Algorithm:** We need to find $x$ and $y$ such that: $ 7x + 50y = 1 $ Using the Euclidean Algorithm: $ 50 = 7 cdot 7 + 1 $ $ 1 = 50 - 7 cdot 7 $ So, the inverse of 7 modulo 50 is $-7$, which is equivalent to $43$ modulo 50 (since $-7 + 50 = 43$). 4. **Solve for $u$:** Now we can multiply both sides of the congruence $7u equiv 23 pmod50$ by 43: $ equiv 23 cdot 43 pmod50 $ Lets calculate $23 cdot 43 mod 50$: $ 23 cdot 43 = 989 $ $ 989 div 50 = 19 text remainder 39 $ So, $u equiv 39 pmod50$.5. **Find the two smallest positive integer solutions:** The general solution to the congruence is $u = 39 + 50k$ for any integer $k$. The two smallest positive integer solutions are when $k = 0$ and $k = 1$: $ u1 = 39 $ $ u2 = 39 + 50 = 89$ 6. **Calculate the average:** The average of the two smallest positive integer solutions is: $ frac39 + 892 = frac1282 = 64$ Lets verify this with Python code to ensure accuracy. [Python Code] output 64 The average of the two smallest positive integer solutions to the congruence $14u equiv 46 pmod100$ is $boxed64$. Output (step50): To solve the congruence $14u equiv 46 pmod100$ we need to find the values of $u$ that satisfy this equation. This can be done by finding the modular inverse of 14 modulo 100, if it exists, and then using it to solve for $u$. First, we need to check if 14 has an inverse modulo 100. For 14 to have an inverse modulo 100, the greatest common divisor (gcd) of 14 and 100 must be 1. However, since $gcd(14, 100) = 2$, 14 does not have an inverse modulo 100. Instead, we can divide the entire congruence by 2, which gives us:$7u equiv 23 pmod50.$ Now, we need to find the modular inverse of 7 modulo 50. The modular inverse $x$ of 7 modulo 50 is number such that $7x equiv 1 pmod50$. We can use the Extended Euclidean Algorithm to find this inverse. Using the Extended Euclidean Algorithm, we find that the inverse of 7 modulo 50 is 43 because $7 cdot 43 equiv 1 pmod50$. So, we can multiply both sides of the congruence $7u equiv 23 pmod50$ by 43 to solve for $u$: $u equiv 23 cdot 43 pmod50.$ Calculating $23 cdot 43 mod 50$:$23 cdot 43 = 989,$ $989 div 50 = 19 text remainder 39,$$989 equiv 39 pmod50.$ So, $u equiv 39 pmod50$. This means that the general solution for $u$ is $u = 39 + 50k$ for any integer $k$. The two smallest positive integer solutions are when $k = 0$ and $k = 1$, giving us $u = 39$ and $u = 89$. The average of these two solutions is:$frac39 + 892 = frac1282 = 64.$ So, the final answer is $boxed64$.Conclusion: right\" Table 8: Case Study: Question, Answer, and Progressive Outputs"
        }
    ],
    "affiliations": [
        "Independent",
        "Netmind.AI",
        "Shanghai AI Lab",
        "University of Waterloo",
        "Vector Institute"
    ]
}