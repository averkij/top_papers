{
    "paper_title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
    "authors": [
        "Kwan Yun",
        "Seokhyeon Hong",
        "Chaelin Kim",
        "Junyong Noh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advancements in learning-based motion in-betweening, a key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, a novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs a two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, a fine-tuning technique for video diffusion models. Additionally, we propose a ``motion-video mimicking'' optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to a wide range of motion in-betweening tasks."
        },
        {
            "title": "Start",
            "content": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models"
        },
        {
            "title": "Seokhyeon Hong Chaelin Kim Junyong Noh",
            "content": "KAIST, Visual Media Lab { yunandy, ghd3079, chaelin.kim, junyongnoh}@kaist.ac.kr 5 2 0 2 1 1 ] . [ 1 7 1 4 8 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite recent advancements in learning-based motion inbetweening, key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, fine-tuning technique for video diffusion models. Additionally, we propose motion-video mimicking optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to wide range of motion in-betweening tasks. The code and videos are available at project page. 1. Introduction Keyframe interpolation is essential in the animation creation process, providing smooth and natural transitions between keyframes to achieve lifelike character movements. Traditionally, this process has been highly labor-intensive, often involving extensive manual adjustments to refine the In recent years, while the adoption of moanimation. tion capture (MOCAP) technologies has helped alleviate this manual burden, its high costs and infeasibility for certain characters and animals present significant challenges. To address these limitations, deep learning-based motion in-betweening methods have emerged, making notable advancements [12, 18]. These efforts have focused on developing sophisticated architectures [9, 13, 32, 33] and utilizing large datasets [18] to generate realistic motion sequences over long durations. Nevertheless, major hurdle persists: training existing in-betweening models still demands vast amounts of MOCAP or manually keyframed Figure 1. AnyMoLe generates in-between motion from context frames and keyframes without requiring external training data. data for each character. This is crucial issue, as motion inbetweening is not only valuable for well-documented characters but is even more essential for those that are difficult to capture with MOCAP or lack extensive keyframed data. Recent advancements in video generation models, trained on web-scale video datasets, have enabled the generation of realistic videos from arbitrary text [46, 16] or images [46, 56]. Additionally, some models have demonstrated the ability to interpolate between two images, naturally generating range of transitioning scene elements or camera movements [10, 46, 47]. These capabilities align closely with the goal of motion in-betweening, as video generation models excel at creating smooth transitions and continuous motion. Therefore, these models can address data scarcity issues often encountered in 3D motion inbetweening and leverage the generation of realistic motions from rendered keyframes, which become the essential foundation of our motion in-betweening process. We propose novel motion in-betweening method, AnyMoLe, which addresses the problem of scarcity of character-specific datasets. AnyMoLe leverages video diffusion models to generate in-betweening motion through sequential process of rendering, video generation, and motion optimization. In this process, we observed that naively generating an interpolated video and reconstructing 3D character often yielded unsatisfactory results due to: 1) limited contextual understanding, 2) domain gaps between real-world and rendered scenes, and 3) difficulties in accurately tracking motion from generated video. To address these issues, we propose components specifically designed to overcome each challenge. 1 Limited contextual understanding arises from the fixed input of video diffusion models, which only utilize the first and last frames, without direct knowledge of the intermediate or previous frames. To address this lack of contextual awareness, we condition frame generation on previous frames as motion context, treating the problem as frame inpainting by incorporating masked noise during the video generation process. Additionally, we employ two-stage approach for smooth video generation: first, generating sparse frames to establish the motion structure, followed by dense frame generation to fill in the details. The domain gap stems from differences in the distributions between real-world videos and rendered scenes. Video diffusion models are typically trained on real-world videos, which often feature elements like motion blur and complex organic environments. In contrast, rendered scenes usually depict synthetic environments and virtual characters. To bridge this gap, we employ Inference-stage Context Adaptation (ICAdapt). This approach fine-tunes the video diffusion model by using only short, 2-second segment of context motion. During this process, the spatial module is overfitted to accurately represent specific virtual characters, while the temporal module remains frozen to preserve the learned motion dynamics. To address the difficulty in tracking the motion of arbitrary rigged characters in the generated video, we propose novel motion-video mimicking method. Motion-video mimicking is similar to 3D reconstruction with difference in allowed degree of freedom, as it is for rigged characters with an arbitrary joint structure. For motion-video mimicking, we propose sequential optimization approach that predicts the states of 3D joints to generate smooth transitions. The joint prediction is performed by our newly proposed scene-specific joint estimator, specifically designed for motion in-betweening in few-shot setting, using only the context frames and keyframes for training. Table 1. Difference between AnyMoLe and baseline methods. Methods AnyMoLe TS [33] Deciwatch [53] Characters Arbitrary Human Human Training data None 3D motion Dataset video-3D poses Dataset Output 3D motion 3D motion 3D poses By integrating these techniques, AnyMoLe successfully performs the first motion in-betweening for arbitrary characters as shown in Figure 1. The differences between AnyMoLe and previous methods are highlighted in Table 1. Our contributions can be summarized as follows: By utilizing video diffusion model with two stage inference for contextual understanding, we propose the first motion in-betweening method for arbitrary characters without external data. We present ICAdapt, to bridge the domain gaps between real-world videos and rendered scenes by finetuning video diffusion model. We introduce novel optimization technique for motionvideo mimicking, which enables sequential motion optimization for arbitrary characters. We propose new scene-specific joint estimator, specialized for specific rendering scene, which utilizes 2D and 3D-aware features for effective 3D joint estimation. 2. Related Work 2.1. Motion In-betweening Motion in-betweening aims to generate smooth and natural motion transitions by 3D character conditioned on sparse set of keyframes. Early approaches mainly utilized optimization with space-time constraints [35, 45] or radial basis functions [36, 37] to interpolate keyframes. Other approaches leveraged motion graphs [26, 27] constructed from given motion dataset, where optimal paths connecting two keyframes are found within these graphs [25]. Statistical models such as maximum posteriori [7] and geostatistical models [30] were also employed for transition synthesis. With the rise of learning-based methods, neural network models have produced promising results for motion in-betweening. Specifically, Recurrent Neural Networks [17, 18, 23, 39, 40], Convolutional Neural Networks [19, 22, 28, 58], and Transformers [21, 32, 33] have been widely adopted. Recently, diffusion-based models have gained popularity for their ability to generate intermediate transitions by leveraging their generative capabilities [9, 24, 41]. Despite these advancements, performing motion in-betweening for diverse characters within single model remains challenging, because preparing extensive animation data for each specific character is infeasible. In contrast, our method harnesses the capabilities of video diffusion models to create in-between frames for any characters with arbitrary joint structures, bypassing the need for preparing extensive motion data. 2.2. Diffusion Based Video Interpolation Models Recent advancements in image-conditioned video diffusion models enable the generation of promising results for video frame interpolation, particularly addressing the limitations of traditional methods. Diffusion models, originally applied to large-scale text-to-video (T2V) generation [5, 20, 38, 44, 48], have been extended to image-tovideo (I2V) synthesis [4, 15, 43, 46, 56]. Techniques such as SEINE [8] and PixelDance [54] leverage these models to generate image transitions, while DynamiCrafter [46] utilize them for frame interpolation between two input images. Most recently, ToonCrafter [47] finetuned general interpolation model, to become cartoon interpolation model using relatively small dataset of cartoon videos (a few hundred thousand samples compared to the original dataset of ten millions). Similarly, we finetune video diffusion model but with only two seconds of context motion for single motion adaptation. 2 Figure 2. Overview of AnyMoLe: First, the video diffusion model is fine-tuned without using any external data (Sec. 3.1) while the scenespecific joint estimator is trained (Sec. 3.3.1). Next, the fine-tuned video generation model produces an in-between video (Sec. 3.2), which is then refined through motion video mimicking to generate the final in-between motion (Sec. 3.3). 3. Methods Given two seconds of context motion and target key frames for 3D character, AnyMoLe performs 3D motion inbetweening using video diffusion model, as illustrated in Figure 2. The process can be described as follows: 1. Using the two seconds of context motion and target keyframes, render each frame from diverse views. 2. From the multi-view rendered images, finetune the video diffusion model and train scene-specific joint estimator concurrently using both 2D and 3D-aware features. 3. The finetuned video diffusion model generates an inbetween video in two stage auto-regressive manner: first, generate coarse frames from the given context and key frames, second, fill in the remaining frames. 4. Optimize character motion sequentially to align it motion with the generated video using differentiable renderer and the trained joint estimator. In the following subsections, we will describe the ICAdapt (Sec. 3.1), two stage conditional video generation process (Sec. 3.2), and progressive optimization process for motionvideo mimicking (Sec. 3.3). 3.1. Inference-stage Context Adaptation We use DynamiCrafter [46], state-of-the art open-source video interpolation model as our baseline. While this video diffusion model has demonstrated robust motion understanding capability for live-action videos, there is domain gap when applying it to rendered character animation. For example, rendered scenes contain virtual avatars and synthetic backgrounds that differ from the scenes observed in real-world videos, and they lack camera effects such as motion blur. Therefore, we finetune the video diffusion model at the start of inference, using the videos rendered from two seconds of given context motions. If we train the whole video diffusion model using single motion, it will lead to catastrophic forgetting which was evident in Xing et al. [47]. Therefore, we finetune the spatial module and image projector of DynamiCrafter while preserving the original temporal module and fps embedding. This process ensures that finetuned model Dadp faithfully generates video as it is rendered while keeping its ability to generate new motions. Figure 3. Overview of the ICAdapt training process. The spatial module and image injection module are trained, while the others are frozen. Specifically, for the input, we render two seconds of motion at 30 fps from different views, resulting in videos. Then, we sample images with intervals of 1, 2, and 3 frames, which becomes 30, 15, and 10 fps videos, respectively. We segment each video into 16 frame intervals using sliding window to align with the output dimension of Dadp. As result, the two seconds motion is converted to multiple video clips, each containing 16 frames, aligning with the output dimension of the diffusion model. Finetuning the spatial module and image projector with these clips ensures that the generated videos fall in the domain of rendering scene while preserving the details of the input character. Figure 3 shows the process for ICAdapt, whose objective function is defined as follows: EEvae(x),t,ϵ (cid:2)ϵ ϵθ (zt; I0, IN , txt, t, fps) 2 2 (cid:3) , (1) min θ where ϵ (0, I) represents noise, I0 and IN denote the first and last frames, txt is text condition automatically generated from pretrained video captioning model [51], 3 ing frames, the resulting motion is unlikely to be smooth. To address this, we perform the second stage for fine video generation. Similar to the first stage, we use keyframes as input but with smaller time interval. As we additionally have the frames generated in the first stage, they can be effectively used as guidance for the second stage. This process is shown in Figure 5. Blue boxes represent given frames, green boxes represent generated frames, and gray boxes represent the frames that are yet to be generated. Figure 4. Context frames guided video generation process. and fps refer to the frame rate. This training aims to capture the spatial information that remains constant (e.g., scene environment and character) while avoiding overfitting to the temporal information that may change (e.g., walking to running). The training is conducted in latent space, as Dadp is latent diffusion model. 3.2. Two-stage Video Generation With the finetuned video diffusion model Dadp, we can now generate an interpolated video for the target character, recipe for our final in-betweened motion. Here, naively generating video from keyframes would produce results with wrong context, as interpolation lacks contextual information about the previous motion. To generate video that naturally reflects the given context, we first produce coarse frames by using context frames as guidance in an auto-regressive manner. Because Dadp is video interpolation model that originally takes only the first and last images as input and cannot utilize additional context frames, we reformulate the task of interpolation with context frame guidance as problem of latent inpainting during the diffusion process [3, 29]. Specifically, Dadp takes the rendered images I0 and IN as input and generates In where {n 1 1}. With guidance frames Im where {m M1 M2}, M2 < , we iteratively use these frames Im during the backward denoising process. For example, at timestep during denoising, noisy latents znt are replaced with the encoded guidance frames combined with noise ϵt, which is equivalent to producing them through the forward diffusion process. Please refer to Figure 4 for illustration that replaces corresponding parts of znt with Evae(Im) + ϵt to produce nt. This process is carried out iteratively, regenerating Im while ensuring that the interpolated results are correctly aligned with the distribution of Dadp. After the iterative generation, the interpolated sparse video is generated. Because this video is generated with low frame-rate with large semantic jumps across neighborFigure 5. Two stage inference of Dadp. First, at coarse stage, low frame-rate video is generated in auto regressive manner. Next, high frame-rate video is generated from low frame-rate video. 3.3. Motion-video Mimicking The purpose of motion video mimicking is to elevate the generated video into 3D motion data, guided by keyframes, to produce the final motion output. We achieve this by optimizing the root position and per-joint rotation R, starting from keyframes using our scene-specific 3D joint estimator Escene. We will first discuss the Escene in Sec 3.3.1 and the optimization process in Sec 3.3.2. 3.3.1. Scene-specific Joint Estimator Accurate joint position estimation is fundamental objective for optimizing motion in video analysis. Although there are few general-purpose 2D keypoint detection methods [49, 50], we observed that these approaches, typically trained on real images, often fail to generalize to unseen rendered characters. To address this limitation, we propose 3D joint estimation network Escene, specialized in specific scene. The training process is shown in Figure 6. We use the same dataset as in ICAdapt, supplemented with additional images for target keyframes, applying weight > 1. This weight is used to avoid overfitting to context frames, because the context frames are extracted at 30 fps, while the keyframes are at 1 fps. Moreover, we exclude images rendered from back views because only the front views will be used for motion-video mimicking. Training Escene with back views would confuse left-right information. Despite incorporating two seconds of context frames and keyframes, the available data remains relatively limited in size to train 3D estimator from scratch. To bolster performance of Escene with limited data, we leverage 4 spectively. From these keyframes, we iteratively optimize the motion parameters for interpolated frames by moving inwards toward the center. Specifically, we first optimize the parameters for the frames adjacent to the keyframes: Pk1+1, Rk1+1 and Pk21, Rk21 and for each subsequent frame, we initialize the optimization using the parameters from the nearest previously optimized frames. This optimization process continues as long as k1 + < k2 , where is the number of frames optimized from each keyframe. This sequential optimization ensures fast convergence and maintains temporal coherence with adjacent frames. Because the keyframes are the only ground truth data available, they serve as the initial poses during the optimization of the surrounding frames. In addition to 3D joint positions estimated using Escene, we utilize an image loss to minimize the image-wise difference between the rendered character and the generated video. This image loss captures subtle movements that the joint loss might miss. We also utilize two regularization losses, Lreg. Position regularization enforces the root position to be similar to the position of two nearest keyframes, while rotation regularization enforces the current rotation to be similar to that of the previous frame to stabilize the optimization process. To this end, the objective for Motion-Video Mimicking is defined as follows: (M(P, R), pcam) Escene( ˆI)2 argmin P,R where Lreg = λposPj Pintp2 2 + λimgIP,R ˆI2 2 + Lreg 2 + λrotRj Rprev2 2. (3) Here, ˆI represents single frame from the generated video, M(P, R) represents the 3D positions of joints in the global coordinate system given the local rotation and global position, pcam and follows the notations of Eq. 2. The terms λimg, λpos, and λrot denote the weights, and IP,R represents the rendered image given the root position and rotation. Through this optimization process, we can obtain the final 3D positions and rotations of the target character, resembling the generated 2D video. 4. Experiments 4.1. Implementation Details We implemented AnyMoLe and conducted all training and inference on computer with an Nvidia A6000 GPU. For ICAdapt, frames are rendered in four different views (N = 4; front, left, right, and back), while the back view is discarded for Escene. ICAdapt was conducted for 500 steps with batch size of 16, while Escene was trained for 3,500 steps with batch size of 32. Weight for keyframes in Figure 6 was set to 3. For two-stage inference, the first stage generated 5fps video, while the second stage generated 15fps video. After optimization on the 15fps video, we upsampled the video to 30fps for evaluation by apply5 Figure 6. Overview of joint estimator training process. features from DINOv2 [31], which are trained with register tokens [11]. These features encapsulate rich semantic information through self-supervised training on extensive image datasets. Recognizing that DINOv2 effectively captures 2D semantic features but lacks an inherent understanding of 3D structures, we supplement DINOv2 features with FiT3D [52] features, 3D-aware variant that provides structural guidance. The feature merger is achieved using lightweight convolutional layers and concatenation to prevent overfitting. The resultant fused feature is subsequently decoded into heatmaps, facilitating precise 2D pose estimation. We pass the estimated 2D joint positions through MLP layers to predict depth for each joint. Building on recent findings [57] that emphasize the superiority of contextual features over large datasets for 3D pose estimation, we leverage also for per joint depth estimation. Specifically, we sample the feature fx,y R11C from , which corresponds to the estimated 2D joint in spatial dimension. Then, the sampled feature fx,y is concatenated with the estimated 2D joint position and passed through Depth MLP for the estimation of per joint depth value. The training objective of Escene is based on MSE loss, defined as follows: Ljoint = (Gjoint, pcam) Jest2 2, (2) where Gjoint indicates the ground-truth 3D joint positions in global space, automatically derived using context frames and keyframes, pcam denotes the camera parameters, is an affine transformation that projects global positions onto screen space, and Jest indicates the estimated joint positions. Given that the depth is represented in normalized device coordinates (NDC) space, we denormalize it using the half of height to match the range of width-height and depth information. 3.3.2. Optimization Process We initiate the optimization process using two keyframes, denoted as Pk1, Rk1 and Pk2 , Rk2 , which represent the root positions and per-joint rotations at frames k1 and k2, reFigure 7. Results of baseline comparison. Our method generated in-between frames similar to the ground truth, while SinMDM and SinMDM* generated out-of-context motion (blue box). TST generated partially out-of-context footsteps (blue box) and out-of-style motion (red box). ERD-QV generated motion that has different style, with stiff back and sharp hand positions (red box). Table 2. Quantitative results compared with the baselines. Ours outperformed all competitors with by margin. Humanoid Character HL2Q L2Q L2P NPSS LPIPS CLIP SSIM HL2Q L2Q L2P NPSS LPIPS CLIP SSIM Methods 0.0015 0.0011 0.0063 0.0726 0.0547 0.9711 0.9343 0.0019 0.0021 0.0299 0.1863 0.0433 0.9675 0.9531 Ours SinMDM 0.0971 0.0386 0.3733 1.7148 0.1007 0.9393 0.9185 0.2465 0.1710 0.1382 4.4324 0.0816 0.9536 0.9362 SinMDM* 0.0981 0.0390 0.3991 1.7584 0.1053 0.9287 0.9161 0.2467 0.1713 0.1973 4.4076 0.0943 0.9495 0.9332 TST 0.0028 0.0106 0.0209 4.3509 0.0953 0.9606 0.9148 ERD-QV 0.0028 0.0109 0.0098 4.3801 0.0693 0.9656 0.9272 Non-humanoid - - - - - - - - - - - - - - ing Gaussian filter and slerp. Weights λimg, λpos, and λrot in Eq.3 were set to 50, 7,000, and 30,000, respectively. For all of our experiments, we used the first two seconds of the input motion as the context motion and sampled the remaining portion of the motion at one-second intervals for the target keyframes. 4.2. Evaluation Metrics For evaluation metrics, we measured the rotation, position, and rendered image similarity, all compared with groundtruth motion. For rotation, we used L2Q and NPSS metrics. L2Q is the L2 distance in local quaternion space compared to ground truth, while NPSS is the angular frequency similarity proposed in Gopalakrishnan et al. [14]. Because the joints that are near the root have larger impact on overall character motion compared to leaf joints, we also propose to use an adjunctive metric, Hierarchy-filtered L2Q (HL2Q). We conducted the filtering process using the known skeletal hierarchy. In our experiments, we used the child joints only up to 50% of the depth from the root joint in the skeleton hierarchy. Additional results with varying filtering thresholds are presented in the supplementary material. For positional differences, we used L2P, which measures the L2 distance between the global joint position and ground truth. Lastly, we used LPIPS [55] for perceptual similarity, CLIP [34] for semantic similarity, and SSIM for structural similarity of rendered characters, compared with the ground truth under the same camera setting. 4.3. Comparison with Baselines We conducted comparisons with three different baselines: ERD-QV [18], TST [33], and SinMDM [57]. ERD-QV and TST are motion in-betweening methods trained on large MOCAP datasets, while SinMDM is motion generation method trained on single motions, which demonstrated motion in-betweening as its application. For ERD-QV and TST, their original MOCAP datasets [18] were used, while SinMDM was trained using the same context frames as our method. We also made variant of SinMDM, SinMDM* that preserves keyframes by omitting region-ofinterest (ROI) blending. This adjustment was made because ROI blending was originally designed for settings involving multiple target frames. In our experiments, we used 20 different motions of diverse characters, including both humanoid characters and non-humanoid characters (e.g., birds, snakes, and dinosaurs). The humanoid characters were sourced from Mixamo [1], while the non-humanoid characters were obtained from Truebones Zoo [42]. Because the ERD-QV and TST methods were trained for single character, we retargeted 6 Figure 8. Ablation results on video generation. Without applying ICAdapt, each frame of the video exhibited inconsistencies, such as generating noticeable style shifts (blue box). Omitting the fine-stage process resulted in low frame rate, making identical or significant jumps between frames. their motions to target humanoid characters using the commercial software MotionBuilder [2]. However, as these methods cannot generate motions for non-humanoid characters such as snakes, comparison for non-humanoid characters was not performed. Comparison results are shown in Figure 7. While our method faithfully generated frames following the style of the input keyframes, SinMDM and SinMDM* resulted in motion that was out of context due to the method overfitting to the given context frames. This is because the original SinMDM trains the model with full motion; the context frames may not contain all the motions that will be used for the motion in-betweening task. For ERD-QV and TST, while these models generated the foot motion better than SinMDM, they failed to follow the style of the given motion, such as upper body tilt or hand positions. Quantitative results are presented in Table 2. In all metrics, our method outperformed competitors by large margin. 4.4. Ablation Study Video Generation The first stage of AnyMoLe is to generate video by filling in the motion in 2D using the knowledge of large video diffusion model. In this paper, we proposed ICAdapt to bridge the gap between real-world videos and rendered scenes. Additionally, we implemented two-stage generation process to achieve higher frame rate, ensuring smooth motion optimization, as frame-wise similarity aids in optimizing between neighboring frames. We conducted an ablation study without these components. As shown in Figure 8, when ICAdapt was not used, spatial informationsuch as the style of horse-changed (blue box). When the fine-stage was not used, low frame rate video was produced, with identical nearby frames. In contrast, our method produced results with smooth transitions and no visual artifacts. Table 3. Quantitative results of ablation study. Methods HL2Q L2Q L2P NPSS LPIPS CLIP SSIM Ours 0.00169 0.00158 0.01811 0.1295 0.04901 0.9693 0.9437 w/o ICAdapt 0.00210 0.00181 0.02640 0.1413 0.07115 0.9619 0.9340 w/o fine stage 0.00200 0.00171 0.02204 0.1345 0.06037 0.9653 0.9385 XPose 0.00174 0.00167 0.01976 0.1288 0.06004 0.9678 0. w/o data select 0.00799 0.00421 0.15460 0.2434 0.12735 0.9061 0.9211 To verify whether degraded video quality indeed negatively impacts the generated motion, we conducted quantitative evaluation on the full process without these components. For this quantitative ablation study, we used the same dataset and metrics as in the baseline comparison, but this time the resulting values for humanoid and non-humanoid characters were averaged for simplification. As shown in the results for w/o ICAdapt and w/o fine stage in Table 3, the final motion extracted from lower-quality videos led to lower performance on all metrics. This occurred because the degraded video quality negatively affects the optimization process by making joint estimation more challenging for w/o ICAdapt and causing larger motion gaps between adjacent frames for w/o fine stage. Motion Video Mimicking For Motion Video Mimicking, we first trained our scene-specific joint estimator and sequentially optimized 3D joint positions to estimate the character motion. We compared our joint estimator with general pose esitimation method XPose [50]. As shown in Table 3, ours outperformed in all the metrics except NPSS. This is because XPose, trained with real images, sometimes failed to estimate the positions correctly when the appearance of the character is somewhat stylized. We additionally conducted study on our data selection process, which differs from the training dataset used for ICAdapt. Specifically, we 7 Table 4. Perceptual study results on ground-truth similarity, target keyframe faithfulness, and motion naturalness. The number indicates the percentage of selections. Humanoid Non-humanoid Character Similar Faithful Natural Similar Faithful Natural Methods 91.67 60.12 Ours 3.57 SinMDM 13.10 4.76 5.36 SinMDM* - 16.07 TST - 5.36 ERD-QV 92.46 3.17 4.37 - - 90.48 3.97 5.56 - - 63.10 14.29 4.17 12.50 5.95 64.88 7.74 5.95 16.67 4. included rendered keyframes with weight but excluded back-view images when training the scene-specific joint estimator. This data choice was crucial because excluding keyframes led to worse results due to overfitting to context frames as indicated by the last row in Table 3. Additionally, because motion video mimicking was performed only in the frontal view, training with an in-domain dataset helped improve performance. 4.5. User Study We conducted user study with 21 participants, 11 female and 10 male aged 27.1 on average to evaluate motion inbetweening results based on human perception. Each participant was presented with three questions on each of 20 motions and asked to choose the one that is: 1) most similar to the ground-truth motion (Similar), 2) best follows the last target keyframe (Faithful), and 3) most smooth and natural (Natural). The scores reported in Table 4 represent the percentage of selections for each category. Our method received higher scores compared to the baseline methods for both humanoid and non-humanoid characters. 5. Applications AnyMoLe can extend its capabilities from single character motion in-betweening to handling multi-object scenarios, such as two ball simulation. By leveraging video diffusion models with contextual understanding, it ensures smooth transitions between objects while maintaining coherent spatial relationships. This is achieved with simple modification to Escene and optimization process, to estimate and optimize all positions of objects instead of the joints. As shown in Figure 9, AnyMoLe smoothly generates two ball simulation with different bounciness. 6. Discussion and Conclusion We proposed novel motion in-betweening method to overcome the limitations posed by requirements of datasets. While our method requires training only lightweight pose estimator using pretrained feature extractors and finetuning the video diffusion model on small dataset, the overall process still necessitates five to six hours to complete the 8 Figure 9. From understanding of the context, AnyMoLe can naturally generate in-between frames in multi-object scenario. Figure 10. Visualization of generated frame from fast turnaround, showing ambiguity that can hinder joint estimation. entire process. Additionally, for motions with fast or complex dynamics as shown in Figure 10, rapid movements can blur the generated videos, causing ambiguityespecially in distinguishing left from right jointsand thus hampering motion estimation. One possible direction to mitigate time requirements and improve robustness against few ambiguous frames in generated videos is to train context-informed, character-agnostic 3D joint estimator. If such pretraining can be achieved without sacrificing performance, it would significantly reduce computational time for training Scene-specific joint estimator and ensure robustness, even when encountering blurry frames, due to its contextual understanding. In this paper, we introduced novel method for motion in-betweening that effectively leverages video diffusion models to overcome the limitation of requiring characterspecific dataset, which has been overlooked. To acheive this, we first identified three primary challenges associated with the naive application of these video diffusion models to motion in-betweening tasks: 1) limited contextual understanding, 2) domain gaps, and 3) difficulties in tracking motion from generated videos. To address these challenges, we introduced two-stage video generation process using context frames, the ICAdapt method, motion-video mimicking, and scene-specific joint estimator. By addressing these key challenges, our approach is the first to successfully achieve motion in-betweening for arbitrary characters without the need for external data. We believe that our contributions broaden the use of video generation models and will stimulate further research in 3D character motion synthesis, particularly for characters that are challenging to capture with MOCAP or to animate manually."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (RS-2024-00333478)."
        },
        {
            "title": "References",
            "content": "[1] Adobe Systems Inc. Mixamo, 2024. Accessed: 2024-06-20. 6 [2] Autodesk, INC. Motionbuilder, 2022. 7 [3] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 4 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 [7] Jinxiang Chai and Jessica Hodgins. Constraint-based motion optimization using statistical dynamic model. In ACM SIGGRAPH 2007 papers, pages 8es. 2007. 2 [8] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion In ICLR, model for generative transition and prediction. 2024. [9] Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, and Michiel van de Panne. Flexible motion in-betweening with diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 19, 2024. 1, 2 [10] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 14721480, 2024. 1 [11] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 5 [12] Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin, Zhehui Qian, Bohan Zhang, and Yi Yuan. Singleshot motion completion with transformer. arXiv preprint arXiv:2103.00776, 2021. 1 [13] Yinglin Duan, Yue Lin, Zhengxia Zou, Yi Yuan, Zhehui Qian, and Bohan Zhang. unified framework for real time motion completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 44594467, 2022. 1 [14] Anand Gopalakrishnan, Ankur Mali, Dan Kifer, Lee Giles, and Alexander Ororbia. neural temporal model for huIn Proceedings of the IEEE/CVF man motion prediction. Conference on Computer Vision and Pattern Recognition, pages 1211612125, 2019. [15] Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. In ICLR, 2024. 2 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1 [17] Felix Harvey and Christopher Pal. Recurrent transition networks for character locomotion. In SIGGRAPH Asia 2018 Technical Briefs, pages 14. 2018. 2 [18] Felix Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. ACM Transactions on Graphics (TOG), 39(4):601, 2020. 1, 2, 6 [19] Alejandro Hernandez, Jurgen Gall, and Francesc MorenoNoguer. Human motion prediction via spatio-temporal inIn Proceedings of the IEEE/CVF International painting. Conference on Computer Vision, pages 71347143, 2019. 2 [20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [21] Seokhyeon Hong, Haemin Kim, Kyungmin Cho, and Junyong Noh. Long-term motion in-betweening via keyframe prediction. In Computer Graphics Forum, page e15171. Wiley Online Library, 2024. 2 [22] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler, and Otmar Hilliges. Convolutional autoenIn 2020 International coders for human motion infilling. Conference on 3D Vision (3DV), pages 918927. IEEE, 2020. [23] Haemin Kim, Kyungmin Cho, Seokhyeon Hong, and Junyong Noh. Recurrent motion refiner for locomotion stitching. In Computer Graphics Forum, page e14920. Wiley Online Library, 2023. 2 [24] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: FreearXiv form language-based motion synthesis & editing. preprint arXiv:2209.00349, 2022. 2 [25] Lucas Kovar and Michael Gleicher. Automated extraction and parameterization of motions in large data sets. ACM Transactions on Graphics (ToG), 23(3):559568, 2004. 2 [26] Lucas Kovar, Michael Gleicher, and Frederic Pighin. Motion graphs. pages 473482. 2002. [27] Jehee Lee, Jinxiang Chai, Paul SA Reitsma, Jessica HodInteractive control of avatars gins, and Nancy Pollard. In Proceedings of the animated with human motion data. 29th annual conference on Computer graphics and interactive techniques, pages 491500, 2002. 2 [28] Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang, Hao Li, and Yajie Zhao. Task-generic hiIn 2021 Intererarchical human motion prior using vaes. 9 national Conference on 3D Vision (3DV), pages 771781. IEEE, 2021. 2 [29] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146111471, 2022. 4 [30] Tomohiko Mukai and Shigeru Kuriyama. Geostatistical motion interpolation. In ACM SIGGRAPH 2005 Papers, pages 10621070. 2005. [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5 [32] Boris Oreshkin, Antonios Valkanas, Felix Harvey, Louis-Simon Menard, Florent Bocquelet, and Mark Coates. Motion in-betweening via deep δ-interpolator. IEEE Transactions on Visualization and Computer Graphics, 2023. 1, 2 [33] Jia Qin, Youyi Zheng, and Kun Zhou. Motion in-betweening via two-stage transformers. ACM Transactions on Graphics (TOG), 41(6):116, 2022. 1, 2, 6 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. 2021. 6 [35] Charles Rose, Brian Guenter, Bobby Bodenheimer, and Michael Cohen. Efficient generation of motion transitions using spacetime constraints. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 147154, 1996. 2 [36] Charles Rose, Michael Cohen, and Bobby Bodenheimer. Verbs and adverbs: Multidimensional motion interpolation. IEEE Computer Graphics and Applications, 18(5):3240, 1998. [37] Charles Rose III, Peter-Pike Sloan, and Michael Cohen. Artist-directed inverse-kinematics using radial basis function interpolation. In Computer graphics forum, pages 239250. Wiley Online Library, 2001. 2 [38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. 2 [39] Xiangjun Tang, He Wang, Bo Hu, Xu Gong, Ruifan Yi, Qilong Kou, and Xiaogang Jin. Real-time controllable motion transition for characters. ACM Transactions on Graphics (TOG), 41(4):110, 2022. 2 [40] Xiangjun Tang, Linjun Wu, He Wang, Bo Hu, Xu Gong, Yuchen Liao, Songnan Li, Qilong Kou, and Xiaogang Jin. Rsmt: Real-time stylized motion transition for characters. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1 10, 2023. 2 [41] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 2 [42] Truebones Motions Animation Studios. Truebones, 2024. Accessed: 2024-02-05. 6 [43] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. In NeurIPS, 2024. 2 [44] Xiang Wang, Shiwei Zhang, Hang jie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation with text-free videos. In CVPR, 2024. 2 [45] Andrew Witkin and Michael Kass. Spacetime constraints. ACM Siggraph Computer Graphics, 22(4):159168, 1988. 2 [46] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain arXiv preprint images with video diffusion priors. arXiv:2310.12190, 2023. 1, 2, 3 [47] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. ToonarXiv preprint crafter: Generative cartoon interpolation. arXiv:2405.17933, 2024. 1, 2, 3 [48] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, He, Liu, Chen, Cun, Wang, Shan, et al. Makeyour-video: Customized video generation using textual and structural guidance. IEEE TVCG, 2024. [49] Lumin Xu, Sheng Jin, Wang Zeng, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, and Xiaogang Wang. Pose for everything: Towards category-agnostic pose estimation. In European conference on computer vision, pages 398416. Springer, 2022. 4 [50] Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Xpose: Detecting any keypoints. In European Conference on Computer Vision, pages 249268. Springer, 2025. 4, 7 [51] Keunwoo Yu, Zheyuan Zhang, Fengyuan Hu, Shane Storks, and Joyce Chai. Eliciting in-context learning in visionlanguage models for videos through curated data distributional properties. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2041620431, 2024. 3 [52] Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, and Jan Eric Lenssen. Improving 2d feature representations by 3d-aware fine-tuning. In European Conference on Computer Vision, pages 5774. Springer, 2025. 5 [53] Ailing Zeng, Xuan Ju, Lei Yang, Ruiyuan Gao, Xizhou Zhu, Bo Dai, and Qiang Xu. Deciwatch: simple baseline for 10 efficient 2d and 3d pose estimation. In European Conference on Computer Vision, pages 607624. Springer, 2022. 2 [54] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In CVPR, 2024. 2 [55] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [56] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. 10 synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 1, 2 arXiv preprint [57] Qitao Zhao, Ce Zheng, Mengyuan Liu, and Chen Chen. single 2d pose with context is worth hundreds for 3d human pose estimation. Advances in Neural Information Processing Systems, 36, 2024. 5, 6 [58] Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, et al. Generative tweening: Long-term inbetweening of 3d human motions. arXiv preprint arXiv:2005.08891, 2020."
        }
    ],
    "affiliations": [
        "KAIST, Visual Media Lab"
    ]
}