{
    "paper_title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression",
    "authors": [
        "Saibo Geng",
        "Nathan Ranchin",
        "Yunzhen yao",
        "Maxime Peyrard",
        "Chris Wendler",
        "Michael Gastpar",
        "Robert West"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable \"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 8 0 1 0 . 6 0 5 2 : r zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression Saibo Geng1 Nathan Ranchin1 Yunzhen Yao1 Maxime Peyrard4 Chris Wendler1,2 Michael Gastpar1 Robert West1,3 1EPFL 2Northeastern University 3Microsoft 4Universit√© Grenoble Alpes, CNRS, Grenoble INP, LIG {saibo.geng, nathan.ranchin, yunzhen.yao, michael.gastpar, robert.west}@epfl.ch maxime.peyrard@univ-grenoble-alpes.fr ch.wendler@northeastern.edu"
        },
        {
            "title": "Abstract",
            "content": "Tokenization efficiency plays critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized on general-purpose corpora. These tokenizers fixed vocabularies often fail to adapt to domainor language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, framework that enables LLMs to dynamically adjust the token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally merges co-occurring tokens into reusable hypertokens on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 2060%, with significant improvements in inference latency. Code will be released at epfl-dlab/zip2zip."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have shown impressive versatility across broad spectrum of tasks and domains [Brown et al., 2020, Bubeck et al., 2023], including biomedical tests [Nori et al., 2023], mathematical reasoning [Frieder et al., 2023], programming [Jiang et al., 2024], and multiple human languages. critical underlying component of this flexibility is the tokenizer, which defines the models vocabulary and governs how raw text is converted into token sequence fed to the model. The efficiency of the tokenization schemei.e., how compactly text is represented as tokenshas significant impact on model performance. In particular, more compact tokenization yields three key benefits: (1) larger effective context window; (2) lower computational (and thus monetary) cost; and (3) shorter response times. Despite its importance, the tokenizer used in most LLMs produces fixed, static vocabulary using algorithms such as Byte Pair Encoding [Sennrich et al., 2016] over large-scale, general-purpose web corpora. While this globally optimized vocabulary performs reasonably well on average, it often fails to adapt to domain-specific or language-specific distributions [Ahia et al., 2023, Petrov et al., 2023], where the text distribution diverges significantly from the pretraining data. The resulting mismatch leads to longer token sequences, increasing both memory and compute demands, as well as the end Equal contribution. Preprint. Under review. users cost by factor of 2-3x when processing domain-specific text [Ahia et al., 2023]. To mitigate this issue, prior work has explored expanding the token vocabulary during domain or language adaptation to improve tokenization efficiency [Wang et al., 2019, Zhao et al., 2024, Kim et al., 2024, Liu et al., 2023, 2024]. While effective, this approach needs to be repeated for each target domain or language and requires maintaining separate tokenizers. Meanwhile, commercial LLM providers trend toward increasing the size of token vocabulariesgrowing from 32K to 128K [Grattafiori et al., 2024] and even up to 200K [Abdin et al., 2024] tokensto improve overall tokenization efficiency. However, prior work [Dagan et al., 2024, Liang et al., 2023] shows that simply enlarging the vocabulary yields diminishing returns in domain adaptation, and vocabularies past certain size can potentially degrade model performance [Liang et al., 2023]. These limitations point to compelling need for an adaptive tokenization mechanismone that can dynamically tailor the vocabulary to the input text at inference time, without retraining the model or maintaining separate tokenizers. Such mechanism would allow the model to construct new domain-specific tokens on-the-fly, so to enhance tokenization efficiency. However, adaptive tokenization poses architectural challenges, as both the embedding layer and the language modeling head in transformer models [Vaswani et al., 2017] are static matrices tied to fixed vocabulary size. In this paper, we propose zip2zip (with hat-tip to seq2seq [Sutskever et al., 2014]), method that equips LLMs with dynamic token vocabulary, enabling inference-time token adaptation. zip2zip achieves adaptive tokenization through continuous vocabulary expansion at runtime, allowing the model to represent repeated or domain-specific pattern with single long token rather than inefficient short tokens. This requires modest modifications to both the transformer architecture and the language modeling objective. zip2zip comprises three key components: (1) Tokenizer: an integration of Lempel-Ziv-Welch (LZW) compression2 [Welch, 1984] into the tokenization process, which continuously merges frequently co-occurring token sequences into reusable longer tokens (hypertokens) at runtime; (2) Architecture: lightweight encoder added to the transformer that computes embeddings for newly formed tokens on the fly; (3) Training: compression-aware causal language modeling variant that trains the model directly on LZW-compressed sequences, aligning learning with the inference-time token distribution. The name zip2zip reflects its dual role in achieving compression of both the input tokens (the first zip) and output tokens (the second zip), thereby jointly improving the efficiency of input encoding and output decoding. We finetune Phi-3-4B and Phi-3-14B to support zip2zip using as few as 100M tokensrequiring only 10 and 40 H100 GPU hours, respectivelyfor effective adaptation. The resulting models demonstrate strong inference-time compression capabilities and achieve 2060% reductions in both input and output sequence lengths, translating to up to 60% improvements in end-to-end latency. To make it easy to upgrade existing LLMs to zip2zip, we release an efficient, open-source implementation of the framework. It includes (1) fast Rust-based LZW tokenizer, (2) drop-in model architecture compatible with Hugging Face Transformers and vLLM, (3) training pipeline for LZWcompression-based finetuning. Existing LLMs can be seamlessly extended with zip2zip, gaining adaptive tokenization capabilities through parameter-efficient finetuningwithout any changes to the base model or tokenizer. 2 zip2zip 2.1 Dynamic Token Vocabulary To enable dynamic tokenization at inference time, we associate each LLM with hyper-vocabulary Vh that augments the models static token vocabulary. Tokens from the original vocabulary are referred to as base tokens. Each entry in the hyper-vocabulary is hypertoken, representing merged sequence of base tokens. The total vocabulary for zip2zip model is the union Vh. At the beginning of each inference session, Vh is initialized as an empty set, and is incrementally populated during decoding by identifying and merging recurring token subsequences in the context window, as illustrated in Figure 1. Continuous Vocabulary Expansion. As decoding proceeds, zip2zip continuously merge cooccurring tokens as new hypertokens to Vh and recurrently apply merging on newly generated tokens. This continual expansion allows the model to represent longer, recurring sequences of base tokens 2LZW is the algorithm used in zip compression tool, which inspired the name zip2zip. 2 Figure 1: Overview of the zip2zip inference pipeline. At each decoding step, the model has growing context composed of both base tokens (blue) and hypertokens (green). The static vocabulary of size 6 remains fixed, while the dynamic vocabulary is continuously expanded by merging co-occurring tokens using LZW compression. The codebook (right) maps hypertoken IDs to their corresponding base tokens. As decoding progresses, new hypertokens created at step (e.g., to be, or not) become immediately available for reuse at step t+. Additionally, output tokens, once generated, instantly become eligible for compression. Hypertokens are also eligible for merging, enabling the formation of nested hypertokens. The final output sequence (bottom) is reconstructed via LZW decompression. compactly. Hypertokens are treated as first-class tokens within the model, used interchangeably with base tokens throughout the decoding process. Importantly, this process occurs entirely during inference, without modifying the underlying tokenizer or requiring model retraining. LZW Algorithm. We implement vocabulary expansion using the Lempel-Ziv-Welch (LZW) compression algorithma dictionary-based, lossless compression method that incrementally builds codebook of variable-length sequences. In our setting, the codebook is initialized with the base token vocabulary and expands by adding new hypertokens on the fly as recurring token patterns are encountered. To control the growth of the dynamically expanding vocabulary, we impose maximum merge size that restricts how many base tokens single hypertoken can represent. LZW is particularly well-suited for zip2zip due to the following properties: (1) it is streaminghypertokens created at step can be immediately reusable at step + 1; in contrast, methods like BPE require access to the full sequence and operate offline; (2) it is self-containedinput base tokens can be perfectly reconstructed from the compressed token sequence alone3; (3) it is unambiguouswhen both base tokens and hypertokens are available, which one to use is consistently determined by the LZW algorithm without ambiguity. 2.2 Hyper-Embedding and Hyper-Projection Hypertokens do not have fixed embedding vectors in the original models embedding layer (and projection layer), as they are not part of the original vocabulary. To compute the embedding of hypertoken, we learn mapping from the base token embeddings to the hypertoken embedding. We achieve this by introducing hyper-encoder, which is neural network that takes the embeddings of the constituent base tokens as input and outputs the corresponding hypertoken embedding. Specifically, for sequence of base tokens y1:M := y1 . . . yM , the hyper-encoder fœï : Rd produces the hypertoken embedding = fœï(y1:M ) Rd, where is the maximum merge size and is the embedding dimension. For hypertokens composed of fewer than base tokens, we pad the input sequence to length . Since the embedding map for base tokens remains unchanged, the hyper-encoder fœï essentially maps the concatenated base token embeddings from (M d)-dimensional space to d-dimensional hypertoken embedding vector, performing nonlinear dimensionality reduction. For the output projection layer, if the underlying transformer ties the embedding and the projection matrices, one can reuse the same hyper-encoder to compute the representation used for projection. Otherwise, separate hyper-encoder is trained to produce the hypertoken projection vectors. 3There is no need to persist or transmit the codebook across inference calls, preserving compatibility with existing LLM libraries and interfaces. 2.3 Architecture We illustrate the architecture of zip2zip in Figure 2. The input text is first tokenized into base tokens (STEP 1), which are then passed through an online LZW compressing module that compresses the token sequence into stream of hypertokens (STEP 2). Since hypertokens are not part of the models original embedding layer, their embedding vectors are computed on-the-fly using the hyper-encoder during inference (STEP 34). Once embedded, both base token embeddings and hypertokens embeddings are passed through the standard transformer layers of the base model, producing contextualized hidden states (STEP 56). This step is identical to vanilla transformer, with hypertokens and base tokens treated equally. At the output projection layer, hypertoken projection vectors (same as the hypertoken embedding vectors in the tied case, and computed by separate hyperencoder otherwise) are appended to the original projection matrix in the language modeling head (STEP 7). This allows the model to compute joint softmax over the union of the base vocabulary and the hyper vocabulary Vh (STEP 8). The resulting probability distribution is over Vh, and the sampled token may be either base token or hypertoken (STEP 9). In the next cycle, the newly generated token (STEP 10) whether base or hyperis appended to the input sequence, and the process repeats (back to STEP 1). At the end of generation, the hypertoken sequence is decompressed via the LZW decoding function into sequence of base tokens (STEP 1112).The whole process works in fully autoregressive way, where newly generated tokens will also be merged into hypertokens for future steps. Furthermore, we highlight two points: Vocabulary UpConsistent dates. The expanding vocabularycomprising created newly hypertokensmust be updated in consistent manner across both the input embedding layer and the output projection layer, maintaining consistent view of the hypertoken set. Failure to update both sides consistently can result in two types of errors: (1) hypertokens that cannot be decoded, or (2) the model attempting to decode non-existing hypertoken. Hyper-Embedding Cache. Although hypertoken embeddings are computed on-the-fly, they are contextindependent and can thus be cached across inference steps. Similar to the transformers KV-cache, this enables incremental updates: only newly created hypertokens need to be processed at each step. Since the codebook grows linearly with the number of tokens in the context n, the total cache size grows also linearly in memory. Thus, the computational cost for hypertoken embeddings remains constant per stepi.e., one token embedding is computed per step. 2.4 Training zip2zip models Figure 2: zip2zip architecture. At inference time, base tokens are compressed into hypertokens using LZW (STEPS 12). hyper-encoder computes embeddings for hypertokens (STEP 3 4), which are processed by the base LLM (STEPS 56). Output representations are projected jointly on base and hyper-projection layers (STEP 7), producing joint logits and sampled tokens (STEPS 810), which can be decoded back to base tokens (STEPS 1112). Objective. Let denote the target text distribution. Given language model œÄŒ∏ parameterized by Œ∏, standard pretraining seeks to minimize the causal language modeling (CLM) objective, which corresponds to the expected negative log-probability of data sequences under the model: EyD [ log œÄŒ∏(y)] , min Œ∏ (1) where œÄŒ∏(y) denotes the probability of the token sequence under the model œÄŒ∏. Let be an online compression algorithm (e.g., LZW), and œï be the parameters of the hyper-encoder. Given sequence D, let = C(y) be its compressed form. In zip2zip, we aim to optimize the same CLM loss, but over the compressed sequences z. The training objective becomes: min Œ∏,œï EyD [ log œÄŒ∏,œï(C(y))] = min Œ∏,œï EzC(D) [ log œÄŒ∏,œï(z)] . (2) Here, we slightly abuse the notation to let œÄŒ∏,œï(z) denote the probability assigned to the compressed sequence z, parameterized by the base model weights Œ∏ and the hyper-encoder parameters œï. To construct the compressed dataset C(D), we first tokenize the corpus using standard tokenizer, and then apply the LZW compression algorithm. This preprocessing step is performed once prior to training and can be efficiently parallelized through batching. Parallelizable Training via Causal Masking. Although hypertokens introduce additional vocabulary dynamics, training remains fully parallelizable. We leverage the standard causal masking mechanism used in language models, allowing the model to predict the next tokenwhether base token or hypertokenat each position in parallel. To eliminate the need for sequential codebook updates during inference, we precompute fixed codebook by applying LZW compression to the entire input sequence. This precomputed codebook is then used consistently throughout training to condition token predictions, ensuring efficiency and compatibility with standard training pipelines. Auxiliary Reconstruction Loss. We introduce an auxiliary reconstruction objective that encourages hypertoken embedding to retain sufficient information about its underlying base token sequence. Specifically, the model is trained to reconstruct the original base token embeddings from the hypertoken embedding. We jointly optimize the language model and the hyper-encoder using combined loss that includes both the standard next-token prediction loss and the auxiliary reconstruction loss. Formally, we optimize: min Œ∏,œï,œà EyD [ log œÄŒ∏,œï(C(y))] + Œª Ey1:M [ (y1:M , fœà (fœï(y1:M )))] , (3) where fœï : Rd is the hyper-encoder, fœà : Rd is the decoder aiming to reconstruct the corresponding base tokens from their hyper-embedding, and : V is the reconstruction loss function, such as the cross-entropy loss, between the base tokens y1:M and the reconstructed base tokens fœà (fœï(y1:M )). The hyperparameter Œª controls the trade-off between the prediction error of the language model and the reconstruction error of the autoencoder. This joint optimization objective encourages the hyper-encoder to learn compact d-dimensional manifold embedded in the higher-dimensional (M d) space of base token embeddings, while the language model œÄŒ∏,œï learns to predict the next (hyper)token given the preceding context. The reconstruction loss can be viewed as form of auto-encoding, where the hypertoken acts as compressed latent representation and reconstruction encourages the preservation of semantic content and the compression to be lossless. Adapting Pretrained Language Models. The proposed objectives (Equation 2, 3) integrate naturally with pretrained language models. In this setting, the base model can be frozen while training only the hyper-encoder to adapt to compressed token sequences. Parameter-efficient methods such as LoRA [Hu et al., 2022] may also be used to adapt select components of the base model, enabling effective adaption with minimal computes. 2.5 Efficiency Advantage zip2zip improves efficiency by increasing the average token length, thereby reducing the number of tokens required to represent the same text. This compression applies to both inputs (e.g., prompts) and outputs (e.g., completions), leading to shorter effective context lengths. As result, the model performs fewer computationsboth in the attention mechanism and the feedforward layersand, more importantly, requires fewer autoregressive decoding steps during inference. Since the latency of large language models is primarily driven by the cost of sequential decoding, reducing the number of output tokens by n% leads to an approximate n% speedup in decoding latency, which we will demonstrate empirically in Section 3.6. more detailed discussion of FLOPs is provided in Appendix for completeness."
        },
        {
            "title": "3 Experiments",
            "content": "To evaluate the effectiveness of zip2zip, we adapt the Phi-3 models (3B and 14B) within the zip2zip framework. We evaluate our adapted models across four dimensions: (1) token efficiency, (2) language modeling perplexity, (3) downstream task performance, (4) inference efficiency. For perplexity and downstream benchmarks, we use the widely adopted lm-evaluation-harness framework [Gao et al., 2024]. 3.1 Training Setup Rather than updating the full model weights, we adopt parameter-efficient finetuning using LoRA [Hu et al., 2022]. In addition, we train the hyper-embedding and hyper-projection modules. We set the maximum merge size to = 3 and use two-layer transformer encoder as the hyper-encoder. Ablation studies on and hyper-encoder architecture can be found in Appendix A. For comparison, we also perform continual finetuning of the base model using LoRA under identical training conditions, serving as baseline (denoted as Cont. Finetune in the Tables) The fine-tuning process is highly efficient, requiring approximately 10 H100-GPU hours for 4B-parameter model and up to 40 H100-GPU hours for 14B-parameter model, using only 0.1 billion training tokens. Interestingly, the reconstruction loss converges to near zero during training, indicating that the model can almost perfectly recover the original base token sequences from the hypertoken representations. This highlights the learned compression is highly information-preserving. Details of the training setup, compute infrastructure, and dataset curation are provided in Appendices and E. 3.2 Sample Outputs and Hypertoken Patterns We present several examples to provide intuition into how the zip2zip model generates text. Figure 3: Zip2Zip output examples. Blue: base tokens; Yellow: hypertokens (composed of 2 base tokens); Orange: hypertokens (composed of 3+ base tokens). We see that the model successfully generates mixture of hypertokens and base tokens in the output (see Figure 3). The hypertoken ratio is as high as 40% in the Python code generation example, and 20% in the biomedical text generation example. Many of the hypertokens correspond to semantically meaningful units or domain-specific terms as shown in Table 1. For more fine-grained visualization of hypertoken with zip2zip , we provide visualizations of token streams in Appendix 8. Table 1: Examples of hypertokens formed by zip2zip across three domains Code Generation tor + ch = torch Att + ention = Attention Multi + Head = MultiHead Biomedical + + NA = mRNA trans + cribed = transcribed synth + esis = synthesis French + iff + el = Eiffel de + la = de la Gust + ave = Gustave + dim = kdim cell + ular = cellular comm + enc + √© = commenc√© 3.3 Token Efficiency Given an input text and tokenizer, we define token efficiency Œ∑ := Bytes(x) Tokens(x) as the average number of bytes represented by each token, where Bytes(x) refers to the number of bytes in the UTF-8 6 Table 2: Token efficiency (bytes/token) across domains for different tokenizers w/wo zip2zip. Tokenizer Multilingual Web Math Code Chat Llama-3-128K [Grattafiori et al., 2024] 4.1 +zip2zip Qwen-2-150K [Yang et al., 2024] +zip2zip Phi-4-200K [Abdin et al., 2024] +zip2zip Gemma-3-256K [Team et al., 2025] +zip2zip 2. 5.1 3.8 5.1 2.3 6.3 (+54%) 4.0 (+48%) 6.4 (+25%) 4.7 (+24%) 4.0 6.2 (+55%) 3.7 (+61%) 6.4 (+25%) 4.6 (+24%) 4.1 6.3 (+54%) 4.1 (+52%) 6.7 (+24%) 5.5 (+20%) 3.3 5.6 (+70%) 3.7 (+61%) 6.4 (+28%) 5.4 (+23%) 2. 5.0 5.4 4.4 3.7 4.6 2. 4.6 5.4 (+17%) 4.4 5.2 (+18%) 4.7 5.4 (+15%) 4.5 5.4 (+20%) encoding of x. This measures how compactly tokenizer encodes input texthigher values of Œ∑ indicate more efficient tokenization. We evaluate token efficiency using the tokenizers of four LLMsLlama-3 [Grattafiori et al., 2024], Qwen-2 [Yang et al., 2024], Phi-4 [Abdin et al., 2024], and Gemma-3 [Team et al., 2025]each associated with different base vocabulary size ranging from 128K to 256K. Token efficiency is measured across five representative domains, sampled from publicly available datasets: code [Lozhkov et al., 2024b], math [LI et al., 2024], chat [Ding et al., 2023], multilingual [Penedo et al., 2024], and web [Lozhkov et al., 2024a]. Table 2 shows that applying LZW zip2zip consistently improves token efficiency across all tokenizer and domains. Gains are particularly strong in structured domains like code and math50% higher than the base tokenizer. Interestingly, models with larger vocabulary sizes do not always achieve better token efficiency, suggesting that simply enlarging the vocabulary size is not sufficient to improve it. 3.4 Perplexity We evaluate the perplexity of zip2zip models on four corpora: Wikitext [Merity et al., 2016], the Pile [Gao et al., 2020], and two subsets of Paloma [Magnusson et al., 2023]: mC4, multilingual subset of C4, and dC4 (aka C4-100D), subset of C4 spanning 100 domains. Given token sequence = x1, . . . , xN , and model q, perplexity and byte-level perplexity [Radford et al., 2019, Magnusson et al., 2023] are deTable 3: Byte-perplexity () on four corpora using 1024-token context window. Model Method Wiki Pile mC4 dC4 Phi-3.5-4B Base 1.58 1.79 1.88 1.74 Cont. finetune 1.59 1.81 1.88 1.74 1.69 1.95 2.00 1.82 zip2zip 1.43 1.72 1.82 1.67 Cont. finetune 1.47 1.79 1.86 1.68 1.56 1.90 1.96 1.75 zip2zip fined as: PPL := (cid:16)(cid:81)N (cid:17)1/B (cid:16)(cid:81)N i=1 q(xi) (cid:17)1/N , Byte-PPL := Phi-3-14B Base i=1 q(xi) = PPL1/Œ∑, where is the number of UTF-8 bytes of the text, and Œ∑ denotes the token efficiency (i.e., bytes per token). Token-level perplexity depends on the tokenization scheme and is unsuitable for cross-tokenizer comparison. We instead report byte-level perplexity, vocabularyagnostic metric that normalizes for tokenization differences. Table 3 shows that zip2zip model has modest increase in Byte-perplexity, indicating drop in language modeling performance. 3.5 Evaluation on NLP Benchmarks We next evaluate zip2zips performance on real-world tasks. We evaluate on seven widely used NLP benchmarks, including ARC-[Challenge, Easy] [Clark et al., 2018], HellaSwag [Zellers et al., 2019], LAMBADA [Paperno et al., 2016], OpenbookQA [Mihaylov et al., 2018], PIQA [Bisk et al., 2019], Winogrande [Sakaguchi et al., 2019] and GSM8K [Cobbe et al., 2021]. As shown in Table 4, the model finetuned with zip2zip performs similarly to the baseline on most tasks. However, on GSM8K, where the primary task involves numerical computation, the model exhibits significant degradation. Due to the sensitivity of such tasks to tokenization, it occasionally generates malformed or repeated numbers. While token-level operations are already known to be challenging for LLMs [Singh and Strouse, 2024], adaptive tokenization appears to exacerbate this issue. Table 4: Two-shot accuracy (in %) across 7 NLP benchmarks, higher is better. Standard deviations (bootstrapped) 0.02 across all tasks. Model Method ARC-c ARC-e HS OBQA PIQA WG GSM8K Phi-3.5-4B Base Phi-3-14B Base 0.60 Cont. finetune 0.60 0.57 zip2zip 0.62 Cont. finetune 0.62 0.62 zip2zip 0.83 0.66 0.46 0.82 0.63 0.47 0.83 0.61 0.46 0.80 0.70 0.51 0.88 0.66 0.52 0.86 0.68 0.51 0.79 0.75 0.82 0.75 0.82 0.75 0.83 0.76 0.87 0.80 0.85 0.79 0.82 0.40 0.15 0.84 0.52 0.25 To validate the effectiveness of zip2zip on non-English languages, we evaluate the model on machine translation tasks, including WMT14 [Mach√°Àácek and Bojar, 2014], WMT16 [Bojar et al., 2016]. The results, shown in Table 5, indicate small performance degradation across BLEU, CHRF, and TER metrics when using zip2zip. However, the drop is relatively minor, suggesting that the model retains strong multilingual capabilities even in the compressed representation. Table 5: Machine translation performance on WMT benchmarks. Scores are averaged across both translation directions. Standard deviations (approximately 1.0 2.0) are reported in Table 10 in Appendix C. Model Method WMT14 En-Fr WMT16 En-De BLEU CHRF TER BLEU CHRF TER BLEU CHRF TER WMT16 En-Ro Phi-3.5-4B Base Cont. finetune zip2zip Phi-3-14B Base Cont. finetune zip2zip 33.6 36.5 34.1 39.1 38.9 36.4 58.3 61.0 59.4 62.6 63.2 62.8 53.0 51.5 54.5 49.3 48.8 51.2 39.2 42.3 39.7 43.1 48.4 44. 63.2 65.4 64.5 65.6 70.1 68.1 47.9 44.9 48.0 44.1 39.8 42.9 17.7 16.7 14.3 21.3 21.8 19.5 45.5 45.8 44.2 51.0 52.0 50.1 73.4 79.7 93.5 70.5 68.3 72.9 3. Inference Efficiency zip2zip reduces decoding time by lowering the number of tokens that need to be generated. However, it introduces additional FLOPs due to the on-the-fly computation of hyper-embeddings by the hyperencoder. To address this overhead, we implement hyper-embedding caching and optimize the computation using custom Triton kernel. We report separate timings for prefill and decoding across multiple models, with and without zip2zip, in Table 6. Table 6: Throughput (tokens/sec) comparison of the zip2zip framework against the baseline Hugging Face Transformers generate and MLX generate implementation. Performance is detailed for prefill and decode phases across various context lengths (first value in column headers) combined with 256-token generation length. zip2zip demonstrates notable throughput improvements, in both prefill and decoding phase. Setting Method 256+ 512+256 1024+256 2048+256 Prefill Decode Prefill Decode Prefill Decode Prefill Decode Hardware: Apple M1 (16GB RAM) Phi-3-4B 7.3 Base model zip2zip 7.9 Relative % -11.8% +7.5% 165.0 145.5 Hardware: NVIDIA H100 80GB GPU 6.8 211.3 231.4 7.3 +9.5% +34.8% -6.6% +3.9% +18.9% +7.5% 200.9 189.6 196.6 233.8 7.5 10.1 7.1 7.4 Phi-3.5-4B Phi-3-14B 53.1 Base model zip2zip 61.9 Relative % +33.6% +9.3% +102.6% +46.6% +60.9% +16.6% +85.4% +16.5% 4993.2 9258.1 2689.4 4326.7 1347.2 2722.1 700.9 936. 54.4 79.8 52.8 61.5 56.2 61.4 42.2 Base model zip2zip 46.3 Relative % +41.5% +23.0% +45.5% +39.5% +57.0% +48.1% +88.1% +9.6% 2328.6 3657.0 1356.3 1973. 3849.5 7239.1 724.4 1024.6 43.8 61.1 45.1 66.8 44.6 54.9 As we show in Table 6, zip2zip achieves significant speedup in all four settings. Both prefill and decoding times are significantly reduced, with the most substantial gains observed in the 512+256 setting with the Phi-3.5-4B model. Improvements are significantly stronger on datacenter-grade GPUs like the NVIDIA H100 and more modest on consumer hardware (e.g., Apple M1). Efficient LZW Tokenization. zip2zip introduces an additional LZW compression step during inference and decompression step at the end of generation. As result, the efficiency of LZW-integrated tokenization is important to overall performance. To minimize overhead, we implemented Rust-based zip2zip tokenizer that outperforms the Python version (see Figure 4) and matches the latency of HuggingFaces fast BPE tokenizer."
        },
        {
            "title": "4 Related Work",
            "content": "Figure 4: zip2zip tokenizer latency (ms) vs. HF tokenizer. Vocabulary Expansion. Several works have explored expanding the tokenizer vocabulary to better support specific domains or languages. Zhao et al. [2024], Kim et al. [2024], Liu et al. [2023, 2024] adapt LLaMA to Chinese, Korean, and specialized domains such as mental health and law by appending new tokens. Wang et al. [2025], Liu et al. [2024] conducted studies on how to effectively expand the vocabulary by better selecting the subset of tokens to add. In contrast, zip2zip is the first to enable dynamic vocabulary expansion at inference time, constructing new tokens based on the input context without requiring retraining or modifying the tokenizer ahead of time. Prompt Compression. Prompt compression methods include GistTokens [Mu et al., 2023], Selective Context [Li et al., 2023], LLMLingua [Jiang et al., 2023], Summary Vectors [Chevalier et al., 2023], In-context Autoencoder [Ge et al., 2024], and others [Wingate et al., 2022] reduce the input token length and but do not impact the number of output tokens, which often dominates overall generation time. In contrast, zip2zip compresses both the input and output token sequences. Latent Tokens Representation. The concept of latent token representations, or patches, has been mostly explored in computer vision, with methods like Token Merging [Bolya et al., 2023] and Token Pooling [Marin et al., 2023] aiming to reduce sequence length while preserving semantic content. Recently, Byte Latent Transformer (BLT) [Pagnoni et al., 2024] extended this concept to language modeling by discarding tokens entirely and operating directly at the byte level. Both BLT and zip2zip adopt hierarchical modeling of input for LLMs, but they differ in three key ways: (1) Goal: BLT aims to replace the tokenizer, whereas zip2zip seeks to expand and improve it; (2) Algorithm: BLT uses entropy-based segmentation, while zip2zip applies LZW-based token compression; (3) Training: BLT requires training from scratch, whereas zip2zip enables continued adaptation of pretrained models. Lester et al. [2024] propose improving language model efficiency by training LLMs directly on text compressed with arithmetic coding. While both approaches leverage compression to enhance efficiency, zip2zip emphasizes dynamic vocabulary expansion to enable uptraining of existing models. In contrast, Lester et al. [2024] requires training from scratch."
        },
        {
            "title": "5 Discussion and Limitations",
            "content": "Beyond LZW. While we adopt LZW for dynamic construction of hypertokens, zip2zip is broadly compatible with any online compression algorithm. Future work may explore alternative schemes that provide different trade-offs between compression efficiency and model performance. Codebook Management Strategy. The LZW algorithm grows the codebook linearly with the number of tokens in the context window. Empirical results show that only about 25% of hypertokens are reused during generation, leaving substantial room for optimization. Two potential improvements are: (1) pruning or selective retention strategies to reduce unused entries, and (2) codebook prefilling, which could be beneficial if likely tokens can be anticipated before input processing. CompressionQuality Trade-off. There is an inherent trade-off between compression and modeling: as the token space is compressed more aggressively, redundancy is reducedbut so is predictabilitymaking it harder for the model to forecast the next (hyper)token. In the extreme, optimal 9 compression schemes such as arithmetic coding produce sequences that are statistically indistinguishable from random noise, rendering them unlearnable by language models [Lester et al., 2024]. Empirically, we observe this effect as increased perplexity under higher compression levels  (Table 3)  , which can undermine the benefits of compression by degrading generation quality (though minor in the tasks in Table 4 and Table 5). Striking the right balance between compression and model performance remains an important direction for future research."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced zip2zip, framework for inference-time vocabulary adaptation with LLMs. By integrating LZW-based token compression with dynamic hypertoken embedding mechanism, zip2zip enables substantial reductions in sequence length and decoding steps, leading to improved inference efficiency with minimal architectural modifications. Our experiments demonstrate that zip2zip maintains strong performance across range of tasks while achieving significant gains in inference efficiency. These findings highlight the promise of integrating dynamic tokenization into LLMs, opening up new directions for research in LLM efficiency."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, S√©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905. Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 models. Conference on Empirical Methods in Natural Language Processing, pages 99049923, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 614. URL https://aclanthology.org/2023.emnlp-main.614/. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID:208290939. OndÀárej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur√©lie N√©v√©ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. Findings of the 2016 conference on machine translation. In OndÀárej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aur√©lie N√©v√©ol, Mariana Neves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, J√∂rg Tiedemann, and Marco Turchi, editors, Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301/. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster, 2023. URL https://arxiv.org/abs/2210.09461. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/ paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. 10 S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. URL https://arxiv.org/abs/2303.12712. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232/. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozi√®re. Getting the most out of your tokenizer for pre-training and domain adaptation, 2024. URL https://arxiv.org/abs/2402.01035. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023. Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. Mathematical capabilities of chatgpt, 2023. URL https://arxiv.org/abs/2301.13867. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. URL https://arxiv.org/abs/2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=uREj4ZuGJE. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm√°n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, 11 Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V√≠tor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin 12 Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1335813376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https: //aclanthology.org/2023.emnlp-main.825. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation, 2024. URL https://arxiv.org/abs/2406.00515. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Seungduk Kim, Seungtaek Choi, and Myeongho Jeong. Efficient and effective vocabulary expansion towards multilingual large language models, 2024. URL https://arxiv.org/abs/2402.14714. Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, and Noah Constant. Training llms over neurally compressed text, 2024. URL https://arxiv. org/abs/2404.03626. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, [https: Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. //huggingface.co/AI-MO/NuminaMath-1.5](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 63426353, Singapore, December 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.emnlp-main.391. URL https://aclanthology.org/2023.emnlp-main.391/. Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1314213152, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.813. URL https://aclanthology.org/2023.emnlp-main.813/. 13 Chengyuan Liu, Shihang Wang, Lizhi Qing, Kun Kuang, Yangyang Kang, Changlong Sun, and Fei Wu. Gold panning in vocabulary: An adaptive method for vocabulary expansion of domain-specific LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 74427459, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.emnlp-main.424. URL https://aclanthology.org/2024.emnlp-main.424/. Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, and Rada Mihalcea. Taskadaptive tokenization: Enhancing long-form text generation efficacy in mental health and beyond. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1526415281, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.944. URL https://aclanthology.org/2023.emnlp-main.944/. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024a. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation, 2024b. Matou≈° Mach√°Àácek and OndÀárej Bojar. Results of the WMT14 metrics shared task. In OndÀárej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, and Lucia Specia, editors, Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293301, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-3336. URL https://aclanthology.org/W14-3336/. Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A. Jha, Oyvind Tafjord, Dustin Schwenk, Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hanna Hajishirzi, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: benchmark for evaluating language model fit. ArXiv, abs/2312.10523, 2023. URL https://api.semanticscholar.org/CorpusID: 266348815. Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token pooling in vision transformers for image classification. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1221, 2023. doi: 10.1109/ WACV56688.2023.00010. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1932719352. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf. 14 Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems, 2023. URL https://arxiv.org/abs/2303.13375. Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, and Srinivasan Iyer. Byte latent transformer: Patches scale better than tokens, 2024. URL https://arxiv.org/abs/2412.09871. Denis Paperno, Germ√°n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern√°ndez. The LAMBADA dataset: Word prediction requiring broad discourse context. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144/. Guilherme Penedo, Hynek Kydl√≠Àácek, Vinko SabolÀácec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: sparkling update with 1000s of languages, 2024. URL https://huggingface.co/datasets/HuggingFaceFW/fineweb-2. Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. Language model tokenizers introduce unfairness between languages. In Advances in Neural Information Processing Systems, 2023. URL https://arxiv.org/abs/2305.15425. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. URL https://api. semanticscholar.org/CorpusID:160025533. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande. Communications of the ACM, 64:99 106, 2019. URL https://api.semanticscholar.org/CorpusID: 198893658. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. Aaditya K. Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms, 2024. URL https://arxiv.org/abs/2402.14903. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS14, page 31043112, Cambridge, MA, USA, 2014. MIT Press. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram√©, Morgane Rivi√®re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga√´l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, JanThorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, Andr√°s Gy√∂rgy, Andr√© Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna KlimczakPlucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim P√µder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and L√©onard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong Yu. Improving pre-trained multilingual model with vocabulary expansion. In Mohit Bansal and Aline Villavicencio, editors, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 316327, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ K19-1030. URL https://aclanthology.org/K19-1030/. Shumin Wang, Yuexiang Xie, Bolin Ding, Jinyang Gao, and Yanyong Zhang. Language adaptation of large language models: An empirical study on LLaMA2. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics, pages 7195 7208, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https: //aclanthology.org/2025.coling-main.480/. Welch. technique for high-performance data compression. Computer, 17(6):819, 1984. doi: 10.1109/MC.1984.1659158. David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, pages 56215634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.412. URL https://aclanthology.org/2022.findings-emnlp.412/. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can In Anna Korhonen, David Traum, and Llu√≠s M√†rquez, machine really finish your sentence? 16 editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond english: An empirical study on language capability transfer, 2024. URL https://arxiv.org/ abs/2401.01055."
        },
        {
            "title": "A Ablation Studies",
            "content": "Definition A.1 (Compression Rate). We define the compression rate as the ratio between the number of tokens after compression (Ncomp) and the number of tokens in the original uncompressed text (Norig), expressed as percentage: Compression Rate ="
        },
        {
            "title": "Ncomp\nNorig",
            "content": "100%. lower compression rate indicates greater reduction in token count, and thus more effective compression. A.1 LZW Maximum Merge Size The last column of Table 7 shows how the maximum merge size affects compression rate when the context window length is 2048. As increases, compression rate improves significantly, especially from = 1 to = 3. Beyond that, gains diminish, suggesting = 3 strikes good balance between efficiency and compression rate. Table 7: Effect of maximum merge size (M ) on bytelevel perplexity and compression rate. Perplexity is measured for Phi-3.5-4B across four corpora with 1024-token context window. Compression rate is evaluated over the training corpus with 2048-token context. = 1 corresponds to no compression. Interestingly, the relationship between maximum merge size and training loss in Figure 5 as well as perplexity in Table 7 is non-monotonic. The baseline case with = 1 (i.e., no zip2zip compression) yields the lowest perplexity overall, which is expected and consistent with prior findings that compression typically incurs trade-off in model performance. Among the compressed settings, the case = 2 performs the worst, with noticeably slower convergence and higher final loss. In contrast, the case = 3 achieves the best performance within the compressed configurations, striking favorable balance between compression and prediction performance. While = 4 and = 5 also perform reasonably well, they exhibit slightly higher loss than = 3, suggesting diminishing returns or possible over-compression at larger maximum merge sizes (see Figure 5). 1 1.62 1.70 2.00 1.91 2 1.96 2.21 2.55 2.22 3 1.72 1.84 2.15 2.00 4 1.71 1.84 2.14 1.99 5 1.72 1.84 2.14 1.99 Wiki Pile mC4 dC4 Compression Rate(%) 100.00 75.30 71.21 68.93 68.41 Table 7 reports the byte-level perplexity across four corpora using 1024-token context window. The results align closely with the training loss trends observed earlier. Setting = 1 (i.e., no compression) consistently achieves the lowest perplexity across all datasets, reaffirming that compression introduces performance trade-off. Notably, = 2 performs the worst across all corpora, exhibiting the highest perplexity values. For merge sizes = 3, = 4, and = 5, perplexity scores are nearly identical, suggesting that moderate compression can be achieved without significantly sacrificing language modeling qualityprovided = 2 is avoided. This consistency across loss and perplexity metrics further supports the robustness of maximum merge size = 3 as the most effective trade-off point. A.2 Hyper-encoder architecture We ablate the architecture of the hyper-encoder to evaluate its effect on language modeling performance, as shown in Table 8. We compare increasingly expressive architectures, starting with simple 17 Figure 5: Effect of maximum merge size on zip2zip training loss: = 1 (no compression) achieves the lowest loss overall. Among compressed settings, = 3 performs best, while = 2 shows the worst convergence. Larger (4 and 5) yield slightly worse results than = 3. Table 8: Ablation of hyper-encoder architecture on byte-perplexity () across four corpora using 1024-token context window. Performance improves with increasingly expressive architectures. Model Method Wiki Pile mC4 dC Phi-3.5-4B averaging 1.81 1.97 2.29 2.08 1-attention-layer 1.73 1.86 2.16 2.01 1-transformer-layer 1.71 1.83 2.13 1.99 2-transformer-layer 1.72 1.84 2.15 2.00 averaging method that introduces no additional parameters. This baseline yields the highest perplexity, highlighting its limited capacity. Adding single attention layer significantly improves performance, and further gains are observed with 1-layer transformer encoder. The 2-layer transformer offers marginal additional benefit, suggesting that lightweight transformer (12 layers) is sufficient for effective hypertoken modeling. Figure 6 illustrates the effect of hyper-encoder architecture on zip2zip training loss. We observe that the simple averaging method converges the fastest but plateaus at relatively high loss, reflecting its limited capacity. As model complexity increaseswith attention and transformer layersthe convergence becomes slower, yet the final loss is significantly lower. Notably, the 1-layer and 2-layer transformer encoders yield the best performance, demonstrating that additional parameters enable the model to better capture structure, albeit at the cost of slower training dynamics. Figure 6: Effect of hyper-encoder architecture on zip2zip training loss. Averaging (no additional parameters) converges quickly but to higher loss. As architectural complexity increasesfrom attention to transformer layersconvergence becomes slower, but the final loss is lower. This highlights trade-off between training speed and modeling capacity. FLOPs Estimation for zip2zip Following the assumptions of Kaplan et al. [2020], we estimate training FLOPs (Œì) as: 18 Œì 6 Ntokens Nparams, where Ntokens is the total number of processed tokens and Nparams is the number of trainable parameters. This estimate ignores the quadratic attention cost, assuming: 12 dmodel sequence length. For zip2zip, this becomes: Œìz2z 6 Ntokens œÅ Nparams(1 + Œ±), where œÅ is the compression ratio, and Œ± accounts for the overhead of the hyper-encoder applied at the embedding and LM head. The relative FLOPs ratio is then: Œìz2z Œì = œÅ (1 + Œ±). Assuming the hyper-encoder mirrors the base models configuration, we estimate: Œ± lM , where is the number of hyper-encoder layers, is the maximum merge size, and is the number of base model layers. We illustrate this estimate across several model scales in Table 9, showing that the relative FLOPs overhead from the hyper-module remains modest (typically under 15%). Model 1 2 14 LLM-4B 2 2 32 LLM-7B 3 3 LLM-70B 80 4 3 LLM-400B 128 Œ± = lM 0.14 0.13 0.11 0.09 Table 9: Relative FLOPs overhead from the hyper-module across different model sizes."
        },
        {
            "title": "C Additional Results",
            "content": "Machine Translation We report standard deviations for machine translation results across WMT benchmarks in Table 10, computed using the lm-evaluation-harness codebase. Table 10: Standard deviations (bootstrapped) for machine translation scores across WMT benchmarks. Model Method WMT14 En-Fr WMT16 En-De BLEU CHRF TER BLEU CHRF TER BLEU CHRF TER WMT16 En-Ro Phi-3-4B +Raw +Finetune +zip2zip Phi-3-14B +Raw +Finetune +zip2zip 2.1 2.2 1.9 2.0 2.2 2.1 1.4 1.6 1.5 1.4 1.4 1.5 1.7 1.8 2.0 1.9 1.9 1.8 1.9 1.8 1.7 2.0 2.0 2.1 1.6 1.4 1.6 1.5 1.3 1.6 1.8 1.7 1.9 1.7 1.9 1. 1.5 1.4 1.6 1.5 1.4 1.5 1.3 1.5 1.4 1.4 1.3 1.3 2.4 2.3 2.5 2.2 2.9 2."
        },
        {
            "title": "D Technical Details",
            "content": "Model and Training Configuration Pretrained Model: microsoft/Phi-3-medium-4k-instruct Sequence Length: 1024 Total Batch Size: 32,768 tokens Learning Rate Schedule: Cosine decay Learning Rate Range: Max = 3e-4, Min = 1e-5 LoRA rank and alpha value: Both are Training Steps: 10,000 Validation Interval: Every 100 steps Checkpoint Interval: Every 500 steps Pytorch Model Compilation: Enabled LoRA Configuration Rank: Alpha: 16 Target Modules: qkv_proj, o_proj, gate_proj, down_proj, up_proj System and Libraries Hardware: 4 NVIDIA A100-SXM4-80GB GPUs, 64-core CPU (128 threads) Key Libraries: PyTorch >= 2.5.0 Transformers >= 4.47.0 Datasets <= 3.1.0 Accelerate >= 0.26.0 Compute Resources We report the compute resources used for training our models in Table 11. All training was conducted on internal servers equipped with NVIDIA H100 GPUs. We estimate GPU-hours by multiplying wall-clock training time by the number of GPUs used. No additional compute was used beyond the reported experiments; we did not perform parameter grid search, large-scale hyperparameter tuning, or exploratory runs that were excluded from the paper. Table 11: Training compute resources for zip2zip experiments. Model GPUs Time GPU Type GPU-Hours Phi-3.5-Medium (14B) Phi-3.5-Mini (4B) 4 2 15h 46m NVIDIA H100 80GB NVIDIA H100 80GB 7h 0m 63.0 14.0 Inference. All evaluations complete within 1 hour on single A100 GPU, demonstrating the runtime efficiency of zip2zip."
        },
        {
            "title": "E Data Mixture",
            "content": "To support effective fine-tuning, we construct curated dataset with balanced representation across diverse domains, including code, mathematics, dialogue, general web content, and multilingual text. The final dataset contains approximately 1 billion compressed tokens. 20 Dataset Domain Proportion (%) HuggingFaceFW/fineweb-edu[Lozhkov et al., 2024a] devngho/the-stack-llm-annotations-v2[Lozhkov et al., 2024b] Code AI-MO/NuminaMath-1.5[LI et al., 2024] Math HuggingFaceH4/ultrachat_200k[Ding et al., 2023] Chat / Dialogue HuggingFaceFW/fineweb-2[Penedo et al., 2024] Multilingual Web / Knowledge 20% 25% 20% 20% 15% Table 12: Training data composition across domains. Table 12 summarizes the constituent datasets and their respective proportions. visualization of the dataset composition and sequence length characteristics is shown in Figure 7. The multilingual subset in fineweb-2 includes the following languages: Mandarin Chinese (cmn_Hani), German (deu_Latn), Japanese (jpn_Jpan), Spanish (spa_Latn), French (fra_Latn), Italian (ita_Latn), Portuguese (por_Latn), Dutch (nld_Latn), and Arabic (arb_Arab). Figure 7: Left: Proportional breakdown of the fine-tuning dataset across five domains. Right: Cumulative distribution of input sequence lengths per domain (log scale). Code and multilingual data exhibit longer tail distributions, indicating greater variability in sequence lengths."
        },
        {
            "title": "F Token Stream Visualization",
            "content": "21 (a) Default Tokenization of some Python code. (b) The same code with adaptive tokenization. (c) Default Tokenization of some biomedical text. (d) The same text with adaptive tokenization. (e) Default Tokenization of text in French. (f) The same text with adaptive tokenization. Figure 8: Examples comparing default and adaptive tokenization. Dotted-line frames highlight where the differences are most noticeable."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: We have clearly stated the claims, assumption and contributions we have made in the abstract and introduction. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have included section called Discussion and Limitations in the main text. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: While we dont theory results. We have tried to formalize our method with formulas but its not really theoretical contribution. And we have tried to make it clear with empirical results. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have described how our methods works in details and we also provide code and specify dataset and experimental details in the supplementary material as well as on this anonymous repository. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide code and data, as well as detailed experimental settings, in the supplementary material, as well as on this anonymous repository. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: It can be found in the code included in the supplemental material; also available on this anonymous repository. We also include the settings of key hyperparameters in Appendix D. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For brevity, in the main text we report experimental results in tables, and we leave the corresponding standard deviations in Table 10. For the two-shot accuracy, we report the standard deviations in the title of Table 4, which are approximately always 0.02. 23 Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide detailed information on the computer resources in Appendix D. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics, and we confirm that the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? 24 Answer: [NA] Justification: This work does not involve any societal impacts. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve any high-risk components such as pretrained generative models or web-scraped data. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original paper that produced the code package, dataset, and models, and included the url links when applicable. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have provided our code package and detailed settings, as well as license, in this anonymous link. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. 26 Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method development in this paper does not involve any LLM as any important, original, or non-standard components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "EPFL",
        "Microsoft",
        "Northeastern University",
        "Universit√© Grenoble Alpes, CNRS, Grenoble INP, LIG"
    ]
}