{
    "paper_title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
    "authors": [
        "Genglin Liu",
        "Shijie Geng",
        "Sha Li",
        "Hejie Cui",
        "Sarah Zhang",
        "Xin Liu",
        "Tianyi Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 7 9 9 2 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "WEBCOACH: SELF-EVOLVING WEB AGENTS WITH CROSS-SESSION MEMORY GUIDANCE Genglin Liu University of California, Los Angeles"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, modelagnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have recently shown impressive skills in web and GUI navigation, enabling agents to fill forms, book flights, or compare shops across complex interfaces (Nakano et al., 2021; Wei et al., 2025; Wu et al., 2025b; Zhang et al., 2025e; Qin et al., 2025). This rapid progress spans both desktop-style pages and mobile apps, with techniques ranging from coordinatefree visual grounding to reinforcement fine-tuning and multimodal tutorial mining (Luo et al., 2025; Zhang et al., 2025a). Despite these advances, web agents still waste many steps: they revisit the same links, stall at login gates, or trigger CAPTCHAs across sessions (Li et al., 2025; Lyu et al., 2025; Huang et al., 2025). Recent work introduces back-tracking or progress rewards to mitigate single-episode errors (Wu et al., 2025c; Zhang et al., 2025b), but agents rarely remember mistakes or successes beyond the current task. In short, most systems navigate without long-term memory an ability humans rely on to internalize past experiences and anticipate pitfalls. The absence of memory fundamentally limits sample efficiency and robustness. Even lightweight history compression or paging enhances web automation accuracy (Zhu et al., 2025; Kang et al., Work done while interning at Amazon. Code available at https://github.com/genglinliu/"
        },
        {
            "title": "WebCoach",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of the WebCoach framework. WebCoach augments web-browsing agents with persistent, cross-session memory through an External Memory Store (EMS) and retrievalaugmented coaching mechanism. The Condenser converts raw navigation histories into standardized summaries stored in EMS, from which the Coach retrieves relevant prior experiences to provide taskspecific guidance to the main web agent. This design enables long-term planning, reflection, and continual improvement across browsing sessions. 2025), while episodic reflection and contextual replay are known to improve adaptation in other domains (Shinn et al., 2023; Liu et al., 2025b). However, these ideas have not been fully integrated into mainstream web-navigation pipelines. We introduce WebCoach, lightweight, model-agnostic framework that layers memory-aware guidance onto any existing web agent. WebCoach follows simple premise: agents should learn from their own trajectoriessuccesses, failures, and edge caseswithout retraining the base policy. It realizes this through three modules: (1) WebCondenser that converts raw interaction traces into compact semantic summaries; (2) an External Memory Store that indexes and retrieves relevant episodes; and (3) Coach trainable LLM that decides when and how to intervene mid-episode. This architecture is framework-independent, as it wraps agents such as BROWSER-USE via simple trajectory hooks. Memories can be written online, shared across agents, and bootstrapped from curated traces, which avoids the lengthy reinforcement cycles demanded by prior improvements (Yin et al., 2025; Xie et al., 2025). As result, even small seed corpus yields useful advice from the first run. On the WebVoyager benchmark (He et al., 2024), which spans 643 live browsing tasks across 15 web domains, WebCoach consistently improves long-horizon success rates and efficiency across diverse base models. When paired with the SKYWORK-38B agent, WebCoach raises the overall success rate from 47% to 61% 14-point gain while maintaining or reducing the average number of steps per task. Other models, such as QWEN-VL-32B, exhibit similar gains, increasing their success rate from 49% to 57% and thereby achieving performance comparable to GPT-4o with open-source backbones. Notably, dynamic self-experience memory (where agents iteratively"
        },
        {
            "title": "Preprint",
            "content": "expand their own memory store) outperforms externally seeded memories, highlighting that agents learn most effectively from their own trajectories rather than borrowed ones. In summary, this work advocates for new, memory-centered paradigm in web agent design one where guidance is not hardcoded, but derived from experience. WebCoach is step towards that goal, offering practical and extensible architecture for building agents that truly remember and learn from their interactions."
        },
        {
            "title": "2 WEBCOACH: A MODEL-AGNOSTIC FRAMEWORK FOR",
            "content": "MEMORY-AUGMENTED WEB NAVIGATION WebCoach is lightweight, plug-and-play layer that augments any existing web-navigation agent (the actor) with memory-aware guidance. It operates through three decoupled components: WebCondenser, an External Memory Store (EMS), and the Coach. These components communicate via function calls and therefore require no modifications to the actors internal agentic workflow, enabling model-agnostic improvements on web browsing tasks."
        },
        {
            "title": "2.1 WEBCONDENSER",
            "content": "The WebCondenser is the first stage in the WebCoach framework and serves as the bridge between raw agent trajectories and memory-aware reasoning. Its role is deliberately narrow but essential: to convert low-level environment traces into semantically meaningful summaries. After every environment step, the actor agent logs JSON file describing its current trajectory. This includes its observations, actions, and intermediate rewards (not necessarily numerical score but self-evaluations on the status of the task completion). The WebCondenser parses these raw traces and condenses them into structured schema that includes concise natural language summary, dense embedding, and key metadata such as whether the task was successfully completed. More details of the WebCondensers operations and output formats are provided in Appendix A. Input. After every environment step, the actor writes structured log describing the current partial trajectory T1:t = (cid:8)(oi, ai, ri)(cid:9)t i=1 (observation, action, reward). WebCondenser is agnostic to the exact schema, allowing compatibility with frameworks like BROWSER-USE, Nova-Act, or any other framework. Processing. small LLM ( 8B) converts the raw trace into fixed schema: summary text: 3-5 sentences capturing the high-level outcome so far. embedding: 1536-d OpenAI embedding vector of the summary. This block could be replaced by other high quality embedding models as long as we keep the consistency across items in the EMS. final success: true/false/null (task still running). fail modes or success workflows: Evidence with key steps for error analysis. If complete trajectory successfully executed the user query and satisfied all the specified conditions, then the WebCondenser should highlight the workflows that led to this success; otherwise, it should summarize the errors or challenges that led to an unsuccessful attempt. The model also detects whether the trace is partial or complete. This process ensures fast runtime performance and compatibility across different frameworks. Importantly, the WebCondenser is not designed to perform any reasoning or intervention itself. Instead, it acts as lightweight, schemanormalizing filter that prepares trajectory data for downstream modules. Routing. crucial design decision is the distinction between partial and completed trajectories. While partial episodes (i.e., tasks still in progress) are streamed to the Coach for real-time decisionmaking, they are not stored. Only completed episodes that reach natural stopping point are persisted to memory. This routing ensures the memory is populated with finalized examples that reflect either success or failure, preventing the accumulation of noise or transient decision states."
        },
        {
            "title": "Preprint",
            "content": "If the current step does not terminate the task, the record is streamed to the Coach for immediate use while skipping storage. Only once the task ends, the Condenser flags the trace as complete and persists it in the EMS; this prevents polluting memory with half-finished noise yet still gives the Coach live context."
        },
        {
            "title": "2.2 EXTERNAL MEMORY STORE (EMS)",
            "content": "The External Memory Store (EMS) is the long-term memory backbone of the WebCoach framework. It accumulates all completed browsing episodes, storing semantic embedding of each agent experience alongside the natural language summaries and metadata generated by the WebCondenser. This memory structure allows new browsing tasks to benefit from past experience, enabling efficient retrieval of relevant trajectories that reflect similar goals, domains, or interaction patterns. Data schema. Each record stores (cid:10)embedding, summary text, meta(cid:11), where meta information contains high-level identifications including episode_id, domain / URL root, user goal, model name, total steps, and timestamp. Retrieval. The EMS is implemented as vector database using FAISS with HNSW indexing Douze et al. (2025), which supports efficient top-K similarity search even as the memory grows to millions of episodes. Once candidate memories are retrieved, they are ranked by their similarity to the current context embedding, which is computed via normalized dot product between the embedding vectors. HNSW builds multi-layer navigable small-world graph that enables logarithmicIn our time approximate nearest-neighbor search with high recall (Malkov & Yashunin, 2018). implementation, we use FAISS with the HNSW-128. Given the Condenser embedding et for the current partial trace, EMS returns the top-K past experiences using score(et, ei) = ei et2 ei . Cold start. key feature of the EMS is its generality: it is agnostic to the model, dataset, or domain an episode originated from. This universal design allows it to serve as cross-actor, cross-task knowledge repository and enables bootstrapping. During cold start phase, the EMS can be seeded with highquality episodes from previously trained web agents, ensuring the Coach has relevant experience to draw upon from the very first online episode."
        },
        {
            "title": "2.3 COACH",
            "content": "Figure 2: Retrieval speed at for the EMS with 600 trajectories. Repeat each 200 times to measure the consistency. Most runs end up averaging between 9.0 and 9.5 ms for ranging from 1 to 10. The Coach is the reasoning engine of WebCoach. Unlike the WebCondenser and EMS, which deal with summarization and storage respectively, the Coach operates at runtime to enhance decisionmaking for the actor agent. At each step, the Coach receives real-time summary of the actors current partial trajectory along with set of retrieved memories from the EMS. It then decides whether to intervene by providing the actor with additional guidance. It is implemented by 8B LLM that decides whether to inject additional context into the actors next prompt. Inputs. The Coach leverages both the content and the outcome labels (success/failure) of past experiences to issue advice that is grounded in evidence. For example, it might say: Avoid clicking Nextprevious agents got stuck in loop here, or Try clicking the first Used filter on the left sidebar, it worked for others. The input will come from two branches simultaneously at each call: 1. The Condenser summary of the current partial trace (summary textt). 2. The top-K (K = 5) EMS summaries {summary texti}K i=1."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Asynchronous Evaluation of WebVoyager. WebCoachs asynchronous evaluation pipeline distributes the 15 subdomains in WebVoyager (e.g., Amazon, Apple, ArXiv) across parallel evaluation queues to maximize throughput and GPU utilization. Yellow boxes indicates in-progress tasks, green indicates completed tasks, and blue indicates tasks that are waiting in the queue. Our limited compute supports running 5 tasks in parallel, and once task finishes earlier than the others in batch, we immediately start another task from the wait list instead of waiting for the entire batch to finish. This asynchronous queueing strategy reduces total evaluation time by over 80%, enabling scalable benchmarking of web agents at large scale. With 600 trajectories stored in the EMS, timings showed the cosine loop costs 10 ms per query regardless of k, so latency doesnt concern how we choose ks default value, as shown in Fig 2. We pick k=5 because presenting five closest past experiences gives the coach enough varied examples to detect patterns without overwhelming the LLM context window or drowning out the current state. Decision rule. Intervention is selective. The Coach is explicitly trained or prompted to remain silent unless it detects high likelihood of failure (e.g., encountering CAPTCHAs, loops, dead ends) or recognizes better workflow from memory. This ensures that interventions are timely and meaningful, rather than overwhelming the actor with irrelevant advice. The guidance itself is concise, usually sentence or two. It is injected into the actors prompt as system message. This injection is entirely non-invasive: no gradients are backpropagated, and the actors internal policy remains untouched. Injection mechanism. The LLM is instructed to intervene when it predicts either high failure probability (looping, CAPTCHA, HTTP 4xx), or faster path exists in memory. Otherwise it returns \"intervene\": false. For intervene=true, the advice JSON is synchronously appended to the actors message history as system message before the next action selection. This requires no change to the actors policy network. The Coach offers scalable way to improve agent reliability without any modifications to the actors architecture. It enables continual improvement over time, as more episodes are accumulated and more patterns are learnedall without retraining the original navigation model. By separating summarization (WebCondenser), storage/retrieval (EMS), and decision-making (Coach), WebCoach remains modeland framework-agnostic. Each module is replaceable, enabling rapid experimentation with larger memories, improved retrieval, or custom intervention policies without disturbing the underlying navigation agent."
        },
        {
            "title": "3.1 DATA",
            "content": "We conduct all evaluations in real browser environment, using WebVoyager He et al. (2024). In contrast to prior work that relies on cached website snapshots (Deng et al., 2023; Lu et al., 2024) or sandboxed environments with limited set of simulated sites (Yao et al., 2022; Zhou et al., 2023; Koh"
        },
        {
            "title": "Preprint",
            "content": "et al., 2024), this benchmark involves live interaction with real-world webpages. This setup reduces the simulation-to-reality gap and better reflects the challenges faced by real users, such as dynamic content, login gates, and UI drift. WebVoyager complements this by enabling large-scale crawling and evaluation across wide web surface, and remains one of the most widely used platforms for benchmarking web agents under online conditions (He et al., 2024). By adopting an online benchmark, we ensure that our evaluation aligns with the deployment setting of web agents, where robustness to interface changes, session-level memory, and adaptation to unfamiliar page layouts are crucial. For evaluation, we utilize browser-use agents evaluation capability by checking the resulting state of their last action against the initial user query, and determine the success of the task."
        },
        {
            "title": "3.2 SETUP",
            "content": "All experiments are conducted in real Chromium browser environment running inside Docker containers. dedicated image is built for each run, and all dependencies are initialized at container startup to ensure consistent execution across checkpoints and models. Evaluations are performed on an NVIDIA A100 machine. To bound runtime and avoid degenerate navigation loops, we enforce 30-second timeout per action step and hard cap of 50 steps per task. Asynchronous Evaluation with Dynamic Batching To increase throughput over WebVoyagers 15 subdomains, we adopt two-level parallelization strategy. First, Docker containerization provides isolated browser instances. Second, Python subprocess scheduler groups subdomains into batches and executes several in parallel, while tasks within each subdomain are processed sequentially to preserve browser determinism. Webvoyager has 15 distinct subdomains (Amazon, Apple.com, Google Flights, etc) and each subdomain contains 30-50 tasks. In order to maximize evaluation speed and GPU utilization, we serve the open-source VLMs on vLLM and SGLang, and deploy the browser-use agent on chromium browser environment in groups of 5 subdomains at once. As illustrated in Fig 3, we maintain an evaluation queue of 5 subdomains, and whenever all tasks from one domain finished running, new domain will enter the queue and be evaluated, until we process all tasks. We treat each WebVoyager task as job with an estimated runtime and schedule them on up to 5 parallel browser workers using an LPT-style (Longest Processing Time first) heuristic: longer tasks are launched earlier, and whenever worker becomes free it pulls the longest remaining job from global evaluation queue. This reduces tail latency and shortens the overall makespan compared to naive FIFO batching. Take Qwen-VL-32b for example. Every task on average took 460s to complete, meaning that for all 643 tasks, sequential evaluation would take 460*643/3600 = 82 hours to complete. With our parallelization queue strategy the actual running time for this evaluation turned out to be less than 14 hours, yielding 83% reduction. Base-Agent Configuration. The base web agent is vision-language model (Qwen2.5-VL-7B, Qwen2.5-VL-32B, or Skywork-r1v3-38B) served on vLLM or SGLang. Optional chain-of-thought or verbose thinking modes are disabled unless explicitly required by an ablation, as they increase latency without observable gains in navigation performance. Apart from WebCoachs injected system messages, we use the default browseruse system prompt without modification. Prompt Usage. While the functional roles of the WebCondenser and Coach are described in Section 2, here we note only the prompting behavior relevant to evaluation. The Condenser is used strictly for summarizing partial and complete trajectories under fixed schema, and the Coach produces lightweight JSON-formatted intervention decision. All prompts follow fixed template to ensure deterministic behavior across runs. Memory Retrieval and Leakage Control. Only completed trajectories are written to the External Memory Store (EMS). During evaluation, retrieval explicitly excludes any episode whose WebVoyager task ID matches that of the current task, preventing leakage of same-task experience. The EMS returns only experiences from distinct tasks or from prior runs of different subtasks. Embedding"
        },
        {
            "title": "Preprint",
            "content": "similarity is computed using normalized dot products, and HNSW-128 indexing supports scalable nearest-neighbor search as the memory grows in the dynamic setting. Reproducibility. Each run stores its configuration, Condenser outputs, Coach outputs, and (optionally) intermediate browser states. These logs enable reproducible replays, ablations, and crossmodel comparisons."
        },
        {
            "title": "4 RESULTS",
            "content": "Performance comparison across base models. WebCoach consistently improves Figure 4: browser-use agents reasoning and robustness across different backbones. The framework achieves higher success rates with equal or fewer average steps, while maintaining efficient completion times."
        },
        {
            "title": "Base Model",
            "content": "Avg. Time (s) Avg. Steps"
        },
        {
            "title": "None\nNone\nNone\nNone",
            "content": "External (Frozen) GPT-4o External (Frozen) GPT-4o External (Frozen) GPT-4o External (Frozen) Qwen3-8B External (Frozen) Qwen3-8B External (Frozen) Qwen3-8B Self (Dynamic) Self (Dynamic) Self (Dynamic) Qwen3-8B Qwen3-8B Qwen3-8B GPT-4o Qwen-VL-7B Qwen-VL-32B Skywork-38B Qwen-VL-7B Qwen-VL-32B Skywork-38B Qwen-VL-7B Qwen-VL-32B Skywork-38B Qwen-VL-7B Qwen-VL-32B Skywork-38B 118.4 144.0 200.9 215.0 332.6 460.1 519.6 369.4 406.4 475.0 200.4 367.4 395. 10.9 16.4 13.3 10.7 16.6 10.9 10.7 16.4 12.2 10.3 17.4 11.9 10.2 0.653 0.328 0.495 0.473 0.288 0.547 0. 0.291 0.565 0.574 0.311 0.571 0.614 Table 1: Success rate, average time, and average number of steps to completion across WebVoyager benchmark experiments. Columns reorganized to show memory type and coach model explicitly. We conducted four sets of experiments on WebVoyager (He et al., 2024) across three base models: Qwen2.5-VL-7B, Qwen2.5-VL-32B, and Skywork-r1v3-38B, alongside ceiling baseline with GPT-4o. Each run covered the full WebVoyager benchmark of 643 online tasks. The experiment configurations were: 1. Baseline: no coaching enabled. 2. Frozen EMS (GPT-4o coach): memory initialized with GPT-4o trajectories and GPT-4o acting as the coach. 3. Frozen EMS (Qwen3-8B coach): memory initialized with GPT-4o trajectories but coached by Qwen3-8B."
        },
        {
            "title": "Preprint",
            "content": "4. Dynamic EMS (Qwen3-8B coach): each main agent iteratively updates its own trajectories into the memory store, coached by Qwen3-8B. Interestingly, GPT-4o did not consistently outperform Qwen3-8B as coach, which led us to decide against DPO-based finetuning of Qwen3. We believe that Qwen3-8B is sufficiently capable to act as condenser and coach in the zero-shot setting, since this task primarily depends on trajectory-level reasoning rather than complex instruction following. Quantitative analysis. Figure 4 and Table 1 reveal consistent pattern of improvement across both success rate and efficiency metrics. The largest jump occurs in the Skywork-38B model, which improves from 47.3% to 61.4% success rate when equipped with WebCoach, 14.4-point gain, comparable to the GPT-4o ceiling baseline. The Qwen-VL-32B also improves by over 7 points (49.5% 57.1%), confirming that experience-guided coaching scales effectively with model capacity. In all cases, the number of action steps per task remains flat or decreases, suggesting that the observed gains stem not from brute-force exploration but from better-informed decision paths. While average completion time rises due to the additional inference overhead of the Coach and Condenser, the efficiency gain in navigation behavior outweighs the latency cost. For instance, dynamic EMS with self-expansion adds 150 seconds of average runtime relative to baseline, yet reduces redundant actions by 12 steps per episode. The overhead is also transient: as the EMS grows, HNSWs logarithmic search complexity ensures that retrieval remains highly efficient. Effect of memory source and self-experience. We observe that self-generated experiences yield more transferable and actionable knowledge than foreign demonstrations. When agents iteratively grow their own EMS, they encode decision patterns consistent with their own inductive biases and representation space. For example, Qwen-32B agent retrieves embeddings closer to its own prior successful action embeddings, resulting in smoother reasoning continuity across tasks. In contrast, GPT-4o-derived trajectories, while high-quality, occasionally inject stylistic mismatches. This explains why the dynamic EMS variant achieves higher success with lower step counts than frozen EMS, despite starting from an empty database. Scaling effects and cognitive thresholds. The benefits of WebCoach are strongly correlated with model scale. Both Skywork-38B and Qwen-VL-32B show pronounced gains, while the 7B backbone does not benefit, with its success rate slightly decreasing (0.328-0.311). We hypothesize that there is cognitive threshold, where smaller models lack the grounding and reasoning depth to exploit cross-episode memory effectively. Once models approach the reasoning frontierable to nearly solve task on their ownthe coaching signal serves as crucial disambiguator that helps avoid high-level decision errors (e.g., repeated scrolling, redundant navigation loops). Thus, memory guidance becomes more valuable at the boundary of partial competence rather than total ignorance. Behavioral analysis. Qualitative inspection of logged traces further supports these trends. Agents equipped with WebCoach exhibit clearer high-level planning and fewer repetitive page visits. For instance, on the Apple.com and Amazon subtasks, coached agents learned to skip redundant login or trade-in prompts that previously caused deadlocks. In dynamic EMS mode, the coach often cited earlier self-generated traces when warning about such pitfallsdemonstrating retrieval-based reflection. In contrast, baseline agents frequently cycled between similar UI elements without learning from past errors. We have more visual recap of what our pipeline does at different steps of web navigation task in Appendix A. At the subdomain level, the largest improvements appear in semantically complex sites like Apple, ArXiv, and BBC News, where reasoning and element disambiguation are key. Simpler domains such as Booking.com and Google Flights show smaller or negligible gains, suggesting that memory is most beneficial for multi-step information extraction tasks rather than atomic button-clicking ones. Summary. Overall, the results highlight that (1) WebCoach enhances task robustness through evidence-grounded reflection, (2) self-evolving EMS accelerates adaptation without external demonstrations, and (3) scaling the base agent amplifies the value of memory-guided reasoning. Together, these findings demonstrate that persistent, cross-session memory can close much of the performance gap between open-source and proprietary LLM agents on complex, dynamic web environments."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Reasoning-Centric Web and GUI Agents Research on interactive agents has advanced along three converging directions. First, GUI-centric studies combine reward shaping, curriculum design, and self-reflection to turn small or open-source VLMs into capable mobile or desktop controllers (Lu et al., 2025; Lian et al., 2025; Shi et al., 2025; Bai et al., 2024; Liu et al., 2025c). These studies often leverage automated supervision through goal synthesis, self-critique, and cross-platform action libraries to reduce annotation cost and boost generalization (Yang et al., 2025; Wu et al., 2025a; Xu et al., 2024; Wu et al., 2024). Second, work on web navigation integrates multi-turn RL, structured exploration, and hierarchical planning, improving performance on benchmarks like WebArena and WebShop while underscoring the importance of strong visual grounding (Wei et al., 2025; Zhang et al., 2025d; Gandhi & Neubig, 2025; Li et al., 2025; Yang et al., 2024; Putta et al., 2024; Zheng et al., 2024; Nakano et al., 2021; Gur et al., 2023; Lee et al., 2025). Finally, platform-level systems pursue generality by unifying planning, tool use, and self-evolution to automate complex desktop or Windows workflows with minimal task-specific engineering (Agashe et al., 2024; Zhang et al., 2024; Qiu et al., 2025). Agentic Memory and Context Management Work on memory has evolved from simple history compression to scalable, structured substrates. While even simple summarization of past states improves automation accuracy, more advanced multimodal agents employ dual episodic-semantic stores that reduce storage while increasing precision (Zhu et al., 2025; Zhang et al., 2025c; Wang & Chen, 2025). Other approaches draw on operating-system analogies to organize information into short-, mid-, and long-term layers or action logs, enhancing performance across language, code, and GUI tasks (Kang et al., 2025; Tang et al., 2025; Gao et al., 2025b). Techniques like latent-space retrieval and Zettelkasten-style linking further extend reasoning across hundreds of thousands of tokens (Wang et al., 2025a; Yu et al., 2025; Xu et al., 2025). Workflow-oriented stores mine reusable action sequences to improve web and multi-agent performance (Wang et al., 2024; Rahman et al., 2025; Liu et al., 2025a), while production systems achieve constant-memory operation with lower latency and strong downstream gains (Zhou et al., 2025; Chhikara et al., 2025). These directions together indicate shift toward adaptive, long-horizon memory for multimodal and collaborative agents. Self-Evolving Agents Self-evolving agents refine themselves through experience rather than retraining. Foundational techniques include replay buffers, natural-language reflection, and automatic curricula, which improve performance in both web and embodied environments (Liu et al., 2025b; Shinn et al., 2023; Wang et al., 2023; Ouyang et al., 2025). Subsequent efforts discover reusable skills or APIs via program synthesis for cross-task transfer (Wang et al., 2025c; Zheng et al., 2025), while self-generated trajectories and scalable curricula close the gap between open-source and proprietary models (Qi et al., 2024; Patel et al., 2024). Recent extensions integrate world-model imagination and transition abstraction for long-horizon or mobile settings (Fang et al., 2025; Chae et al., 2024; Wang et al., 2025b). comprehensive survey summarizes these evolving strategies and highlights open challenges for continual self-improvement (Gao et al., 2025a)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented WebCoach, model-agnostic, self-evolving framework that augments web browsing agents with persistent, cross-session memory. By integrating lightweight WebCondenser, scalable External Memory Store (EMS), and retrieval-based Coach, WebCoach enables existing agents to reflect on prior trajectories, generalize from experience, and recover from repeated errors without retraining. Through large-scale online evaluations on WebVoyager, WebCoach demonstrated consistent gains across three open-source LLM backbones, while maintaining comparable or lower average step counts. These results underscore the value of experience-driven memory in improving robustness and efficiency in web navigation. Future work will (1) integrate the external memory directly into the agents internal policy to eliminate multi-LLM dependency, and (2) explore reinforcement-based optimization over long-term reward signals. More broadly, our findings highlight that memory-centric design is an essential step towards building continually improving, self-reflective web agents."
        },
        {
            "title": "REFERENCES",
            "content": "Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. IEEE Transactions on Big Data, 2025. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint arXiv:2504.21024, 2025. Apurva Gandhi and Graham Neubig. Go-browse: Training web agents with structured exploration. arXiv preprint arXiv:2506.03533, 2025. Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025a. Xinzge Gao, Chuanrui Hu, Bin Chen, and Teng Li. Chain-of-memory: Enhancing gui agents for cross-application navigation. arXiv preprint arXiv:2506.18158, 2025b. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Jing Huang, Zhixiong Zeng, Wenkang Han, Yufeng Zhong, Liming Zheng, Shuai Fu, Jingyuan Chen, and Lin Ma. Scaletrack: Scaling and back-tracking automated gui agents. arXiv preprint arXiv:2505.00416, 2025. Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Dongjun Lee, Juyong Lee, Kyuyoung Kim, Jihoon Tack, Jinwoo Shin, Yee Whye Teh, and Kimin Lee. Learning to contextualize web pages for enhanced decision making by llm agents. arXiv preprint arXiv:2503.10689, 2025."
        },
        {
            "title": "Preprint",
            "content": "Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. Shuquan Lian, Yuhang Wu, Jia Ma, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inference-time grounding. arXiv preprint arXiv:2507.22025, 2025. Genglin Liu, Vivian Le, Salman Rahman, Elisa Kreiss, Marzyeh Ghassemi, and Saadia Gabriel. Mosaic: Modeling social ai for content dissemination and regulation in multi-agent simulations. arXiv preprint arXiv:2504.07830, 2025a. Yitao Liu, Chenglei Si, Karthik Narasimhan, and Shunyu Yao. Contextual experience replay for self-improvement of language agents. arXiv preprint arXiv:2506.06698, 2025b. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025c. Xing Han Lu, Zdenek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with In ICML, 2024. URL https://openreview.net/forum?id= multi-turn dialogue. mUSPhG4uDW. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop: benchmark for deep research shopping agents. arXiv preprint arXiv:2506.02839, 2025. Yu Malkov and Dmitry Yashunin. Efficient and robust approximate nearest neighbor search IEEE transactions on pattern analysis and using hierarchical navigable small world graphs. machine intelligence, 42(4):824836, 2018. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Siru Ouyang, Jun Yan, Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long Le, Samira Daruki, Xiangru Tang, et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025. Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025."
        },
        {
            "title": "Preprint",
            "content": "Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025. Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei Chang, Yejin Choi, and Saadia Gabriel. X-teaming: Multi-turn jailbreaks and defenses with adaptive multi-agents. arXiv preprint arXiv:2504.13203, 2025. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic problem solving. arXiv preprint arXiv:2507.06229, 2025. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957, 2025. Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory. arXiv preprint arXiv:2502.00592, 2025a. Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks. arXiv preprint arXiv:2501.11733, 2025b. Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, and Daniel Fried."
        },
        {
            "title": "Inducing programmatic",
            "content": "skills for agentic tasks. arXiv preprint arXiv:2504.06821, 2025c. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025. Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, and Ziwei Liu. Gui-reflection: Empowering multimodal gui models with self-reflection behavior. arXiv preprint arXiv:2506.08012, 2025a. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025b. Qinzhuo Wu, Pengzhi Gao, Wei Liu, and Jian Luan. Backtrackagent: Enhancing gui agent with error detection and backtracking mechanism. arXiv preprint arXiv:2505.20660, 2025c. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Bin Xie, Rui Shao, Gongwei Chen, Kaiwen Zhou, Yinchuan Li, Jie Liu, Min Zhang, and Liqiang Nie. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for gui agent. arXiv preprint arXiv:2505.16827, 2025."
        },
        {
            "title": "Preprint",
            "content": "Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, et al. Zerogui: Automating online gui learning at zero human cost. arXiv preprint arXiv:2505.23762, 2025. Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: simple yet strong baseline for llm-based web agents. arXiv preprint arXiv:2410.13825, 2024. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. Xiaoran Yin, Xu Luo, Hao Wu, Lianli Gao, and Jingkuan Song. Unlocking smarter device control: Foresighted planning with world model-driven code execution approach. arXiv preprint arXiv:2505.16422, 2025. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025a. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. Ufo: ui-focused agent for windows os interaction. arXiv preprint arXiv:2402.07939, 2024. Danyang Zhang, Situo Zhang, Ziyue Yang, Zichen Zhu, Zihan Zhao, Ruisheng Cao, Lu Chen, and Kai Yu. Progrm: Build better gui agents with progress rewards. arXiv preprint arXiv:2505.18121, 2025b. Hongxin Zhang, Zheyuan Zhang, Zeyuan Wang, Zunzhe Zhang, Lixing Fang, Qinhong Zhou, arXiv preprint Ella: Embodied social agents with lifelong memory. and Chuang Gan. arXiv:2506.24019, 2025c. Yimeng Zhang, Tian Wang, Jiri Gesi, Ziyi Wang, Yuxuan Lu, Jiacheng Lin, Sinong Zhan, Vianne Gao, Ruochen Jiao, Junze Liu, et al. Shop-r1: Rewarding llms to simulate human behavior in online shopping via reinforcement learning. arXiv preprint arXiv:2507.17842, 2025d. Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement finetuning. arXiv preprint arXiv:2506.01391, 2025e. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. Boyuan Zheng, Michael Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, et al. Skillweaver: Web agents can self-improve by discovering and honing skills. arXiv preprint arXiv:2504.07079, 2025. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "Preprint",
            "content": "Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025. Xiyue Zhu, Peng Tang, Haofu Liao, and Srikar Appalaraju. Turbocharging web automation: The impact of compressed history states. arXiv preprint arXiv:2507.21369, 2025. APPENDIX A: STEP-BY-STEP VISUAL ILLUSTRION OF WEBCOACHS"
        },
        {
            "title": "OPERATION",
            "content": "Task Description: On Apples website, check if the HomePod mini in store is available in multiple colors and list them. Website: https://www.apple.com/ Coaching Coaching output is json file including whether or not to intervene, and if so, the exact actionable advice message. For debugging purposes, we also present the retrieved experiences that include meta information, summary, and current patterns with the specific web elements that the agent interacted with. At this step the coach detected some challenges and decides to issue an advice: [Coaching advice output] Based on past experiences, its important to avoid getting stuck in loops or repeatedly failing to apply specific filters. Instead of navigating through the site without clear path, look for Choose your color section or dropdown menu on the HomePod mini product page. If you cant find it, consider using the search function with specific terms like HomePod mini color options or HomePod mini available colors to directly locate the information. If you encounter any navigation issues or loops, immediately return to the product page and check for color options there. [Retrieved Relevant Experience 1] Summary: The user is attempting to find information about the MacBook Air color options on Apples website. They started by scrolling the homepage, clicked on MacBook Pro link, and then navigated through several unrelated pages like AirPods and iPhone. After using the search bar to find MacBook Air, they reached the product page but have been scrolling and searching without success in locating the color options section. The user has attempted various methods, including using the search function and looking for Customize button, but has not yet found the relevant information. [Current Patterns] 1. Repeated Navigation to MacBook Air Page: The user repeatedly navigates to the MacBook Air product page, likely due to misstep or need to verify the correct page. 2. Persistent Scrolling on MacBook Air Page: The user continues to scroll down the MacBook Air product page in an effort to find the color options section, indicating that they havent found the information yet. 3. Repeated Search Bar Interactions: The user frequently interacts with the search bar, possibly due to confusion about the page structure or uncertainty about the location of the color options. Similarity score: 0.8895191550254822 [2 more relevant retrieved experiences omitted] [Main agent returned results:] is done: true, success: true, long-term memory: Task completed: True - The available colors for the HomePod mini are: Blue, Yellow, Midnight, White, and Orange. [Condenser output (to be stored in the EMS)] Summary: The user successfully completed the task of checking the availability of the HomePod mini in multiple colors. They navigated from the Apple homepage to the TV and Home section, then to the HomePod mini product page, and finally identified the color options. Al-"
        },
        {
            "title": "Preprint",
            "content": "though there were some steps where the user scrolled and searched for color options, they eventually found the color selection on the product page. Success workflows: 1. name: Navigation to Product Page, description: The user navigated from the Apple homepage to the TV and Home section and then to the HomePod mini product page, which is key step in accessing the product information. 2. name: Color Option Identification, description: The user successfully identified the available colors (Blue, Yellow, Midnight, White, and Orange) on the product page, which is the main goal of the task. 3. name: Scrolling and Searching for Color Options, description: The user attempted to locate the color options by scrolling through the product page, which eventually led them to the color selection section. embedding: (...omitted...) Figure 5: Step 1 Screenshot"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Step 2 Screenshot Figure 7: Step 3 Screenshot"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Step 4 Screenshot APPENDIX B: FULL PER-SUBTASK RESULTS"
        },
        {
            "title": "Subtask",
            "content": "GPT-4o Qwen-VL-32B Qwen-VL-7B Skywork-r1v3 SR S/T Time Steps SR S/T Time Steps SR S/T Time Steps SR S/T Time Steps 0.889 Allrecipes 0.951 Amazon 0.605 Apple 0.837 ArXiv 0.714 BBC News Booking 0.136 Cambridge Dict. 0.953 0.595 Coursera 0.659 ESPN 0.854 GitHub 0.071 Google Flights 0.878 Google Map 0.233 Google Search 0.674 Huggingface 0.761 Wolfram Alpha 40/45 39/41 26/43 36/43 30/42 6/44 41/43 25/42 29/44 35/41 3/42 36/41 10/43 29/43 35/46 67 92 119 77 87 303 48 109 114 82 200 81 192 123 79 0.644 7.4 6.5 0.707 11.7 0.558 0.628 8.0 10.3 0.571 18.4 0.068 5.3 0.837 12.9 0.714 11.7 0.432 0.537 8.5 19.3 0.095 8.2 0.561 15.6 0.070 11.3 0.512 0.522 8.3 29/45 29/41 24/43 27/43 24/42 3/44 36/43 30/42 19/44 22/41 4/42 23/41 3/43 22/43 24/ 254 210 264 153 159 398 92 177 205 157 249 141 249 169 130 12.3 0.356 11.1 0.659 12.9 0.209 10.8 0.233 12.0 0.190 18.6 0.182 7.6 0.628 13.3 0.452 14.2 0.182 13.8 0.317 18.6 0.310 11.6 0.463 17.5 0.279 13.9 0.279 11.9 0.435 16/45 27/41 9/43 10/43 8/42 8/44 27/43 19/42 8/44 13/41 13/42 19/41 12/43 12/43 20/46 142 193 187 166 141 162 115 149 178 118 169 112 114 107 108 16.1 0.556 17.1 0.463 15.6 0.488 15.0 0.698 14.3 0.690 17.5 0.045 13.9 0.721 16.3 0.571 17.9 0.455 16.9 0.610 18.9 0.071 15.8 0.610 19.0 0.279 15.0 0.581 17.0 0.283 25/45 19/41 21/43 30/43 29/42 2/44 31/43 24/42 20/44 25/41 3/42 25/41 12/43 25/43 13/ 204 195 209 150 181 297 119 254 201 274 370 205 181 214 178 11.0 9.1 10.5 7.2 9.4 12.7 6.4 13.5 9.5 12.9 16.5 10.4 9.9 11.1 9.9 Overall 0.653 420/643 118.4 10.9 0.495 319/643 200.9 13.3 0.344 221/643 144.0 16.4 0.473 304/643 215.0 10.7 Table 2: Baseline: Without WebCoach, per-subtask comparison across base models on the WebVoyager benchmark. Each models subcolumns report Success Rate (SR), number of successful vs. total tasks (S/T), average completion time (s), and average action steps. GPT-4o serves as the ceiling reference."
        },
        {
            "title": "Preprint",
            "content": "Subtask Allrecipes Amazon Apple ArXiv BBC News Booking Cambridge Dict. Coursera ESPN GitHub Google Flights Google Map Google Search Huggingface Wolfram Alpha Qwen-VL-32B Qwen-VL-7B Skywork-r1v3 SR S/T Time Steps SR S/T Time Steps SR S/T Time Steps 0.644 0.805 0.395 0.698 0.595 0.045 0.907 0.690 0.591 0.829 0.071 0.732 0.233 0.674 0. 29/45 33/41 17/43 30/43 25/42 2/44 39/43 29/42 26/44 34/41 3/42 30/41 10/43 29/43 16/46 337 370 353 282 311 910 261 637 467 520 926 461 314 425 341 12.2 11.8 13.0 9.9 12.8 15.5 6.1 13.2 8.8 10.4 17.5 9.7 7.0 8.9 7.5 10.9 0.311 0.585 0.233 0.209 0.143 0.182 0.605 0.357 0.114 0.195 0.119 0.463 0.140 0.163 0.500 14/45 24/41 10/43 9/43 6/42 8/44 26/43 15/42 5/44 8/41 5/42 19/41 6/43 7/43 23/ 292 370 347 312 346 378 261 348 382 294 367 309 336 317 330 0.288 185/643 332.6 16.5 17.4 14.8 14.4 16.7 17.7 13.9 17.7 17.6 17.0 18.0 16.1 18.3 15.3 17.0 16. 0.600 0.829 0.395 0.721 0.667 0.091 0.953 0.738 0.614 0.829 0.095 0.707 0.302 0.605 0.239 27/45 34/41 17/43 31/43 28/42 4/44 41/43 31/42 27/44 34/41 4/42 29/41 13/43 26/43 11/46 561 556 667 322 479 858 273 535 460 526 882 426 413 460 383 0.555 357/643 519. 12.5 11.1 13.7 7.1 10.4 14.8 6.1 11.3 9.1 10.6 17.8 9.5 9.0 10.1 8.3 10.7 Overall 0.547 352/643 460. Table 3: Per-subtask results with WebCoach (GPT-4o experiences, frozen EMS, GPT-4o as coach). Each subtask corresponds to specific website domain. Metrics include success rate (SR), successful vs. total tasks (S/T), average completion time (s), and average action steps."
        },
        {
            "title": "Subtask",
            "content": "Allrecipes Amazon Apple ArXiv BBC News Booking Cambridge Dict. Coursera ESPN GitHub Google Flights Google Map Google Search Huggingface Wolfram Alpha Qwen-VL-32B Qwen-VL-7B Skywork-r1v3 SR S/T Time Steps SR S/T Time Steps SR S/T Time Steps 0.644 0.805 0.488 0.721 0.571 0.045 0.930 0.690 0.614 0.805 0.119 0.732 0.070 0.535 0.717 29/45 33/41 21/43 31/43 24/42 2/44 40/43 29/42 27/44 33/41 5/42 30/41 3/43 23/43 33/ 305 276 402 261 259 869 253 553 507 447 538 279 504 359 277 11.7 9.9 14.5 10.0 12.5 17.2 6.0 12.1 10.5 9.3 18.4 10.8 17.3 12.6 9.8 12.2 0.267 0.488 0.233 0.233 0.190 0.182 0.558 0.333 0.159 0.220 0.214 0.390 0.116 0.233 0.543 12/45 20/41 10/43 10/43 8/42 8/44 24/43 14/42 7/44 9/41 9/42 16/41 5/43 10/43 25/46 328 349 354 290 320 485 372 510 437 445 377 318 342 320 0.291 187/643 369.4 16.3 15.0 15.8 14.3 15.5 18.1 13.5 18.0 16.7 18.4 18.2 16.0 18.5 16.1 15.9 16.4 0.711 0.732 0.605 0.791 0.619 0.068 0.837 0.690 0.545 0.878 0.095 0.756 0.279 0.651 0. 32/45 30/41 26/43 34/43 26/42 3/44 36/43 29/42 24/44 36/41 4/42 31/41 12/43 28/43 18/46 406 477 460 268 411 870 241 589 456 412 892 412 401 435 400 0.574 369/643 475.0 10.0 9.7 10.5 5.9 9.5 15.6 6.1 12.6 9.9 9.2 18.0 9.3 9.1 10.0 8. 10."
        },
        {
            "title": "Overall",
            "content": "0.565 363/643 406.4 Table 4: Per-subtask results with WebCoach (GPT-4o experiences, frozen EMS, Qwen3-8B as coach). Each models subcolumns report success rate (SR), successful vs. total tasks (S/T), average completion time (s), and average action steps."
        },
        {
            "title": "Subtask",
            "content": "Allrecipes Amazon Apple ArXiv BBC News Booking Cambridge Dict. Coursera ESPN GitHub Google Flights Google Map Google Search Huggingface Wolfram Alpha Qwen-VL-32B Qwen-VL-7B Skywork-38B SR S/T"
        },
        {
            "title": "Steps",
            "content": "SR S/T"
        },
        {
            "title": "Steps",
            "content": "SR S/T"
        },
        {
            "title": "Steps",
            "content": "0.644 0.805 0.512 0.721 0.571 0.068 0.930 0.690 0.614 0.805 0.143 0.732 0.093 0.535 0.717 29/45 33/41 22/43 31/43 24/42 3/44 40/43 29/42 27/44 33/41 6/42 30/41 4/43 23/43 33/46 276 250 363 236 234 786 229 500 458 404 486 252 456 325 250 11.4 9.7 14.2 9.8 12.2 16.8 5.9 11.8 10.3 9.1 18.0 10.6 16.9 12.3 9.6 11.9 0.289 0.488 0.256 0.256 0.214 0.205 0.558 0.357 0.182 0.244 0.238 0.415 0.140 0.256 0. 13/45 20/41 11/43 11/43 9/42 9/44 24/43 15/42 8/44 10/41 10/42 17/41 6/43 11/43 26/46 178 189 192 157 174 263 202 277 237 241 205 173 186 174 161 0.311 200/643 200.4 17.3 15.9 16.7 15.2 16.4 19.2 14.3 19.1 17.7 19.5 19.3 17.0 19.6 17.1 16. 17.4 0.733 0.756 0.651 0.814 0.643 0.159 0.860 0.714 0.591 0.878 0.190 0.780 0.349 0.674 0.457 33/45 31/41 28/43 35/43 27/42 7/44 37/43 30/42 26/44 36/41 8/42 32/41 15/43 29/43 21/46 338 397 383 223 342 724 201 490 380 343 742 343 334 362 333 0.614 395/ 395.4 9.9 9.6 10.4 5.9 9.4 15.5 6.1 12.5 9.8 9.1 17.9 9.2 9.0 9.9 8.6 10."
        },
        {
            "title": "Overall",
            "content": "0.571 367/643 367.4 Table 5: Per-subtask results with WebCoach (self experience, dynamic EMS updates, Qwen3-8B as coach). Each models subcolumns report success rate (SR), successful vs. total tasks (S/T), average completion time (s), and average action steps."
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}