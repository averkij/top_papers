{
    "paper_title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
    "authors": [
        "Ming Li",
        "Shitian Zhao",
        "Jike Zhong",
        "Yuxiang Lai",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 8 1 6 1 . 3 0 5 2 : r CLS-RL: Image Classification with Rule-Based Reinforcement Learning Ming Li1 Shitian Zhao1 Jike Zhong2 Yuxiang Lai3 Kaipeng Zhang1 1Shanghai AI Laboratory 2University of Southern California 3 Emory University"
        },
        {
            "title": "Abstract",
            "content": "Classification is core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to state-ofthe-art classification models. However, acquiring large-scale labeled data can be expensive, and fine-tuning on it may also require considerable amount of time. In this paper, we explore few-shot MLLM classification fine-tuning, where only few labeled data samples for each class are used for fine-tuning. We found that supervised fine-tuning (SFT) can cause severe catastrophic forgetting issues and may even degrade performance compared to the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning (RL), we propose CLS-RL, which uses verifiable signals (class names) as reward to fine-tune MLLMs and format reward to encourage models to think before answering. We conducted extensive experiments on eleven datasets and discovered that CLS-RL outperforms SFT in most datasets and has much higher average accuracy on both base-to-new generalization and few-shot learning setting. Additionally, we observed free-lunch phenomenon for CLS-RL; specifically, when models are fine-tuned on particular dataset, their performance on other distinct datasets may also improve compared to zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of image classification. Lastly, inspired by recent works in inference time thinking, we re-examine the thinking process during fine-tuning, critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL approach, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, the No-ThinkingCLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL. We hope that our findings related to the thinking process during fine-tuning could offer insights for other visual tasks of MLLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have shown significant progress in tasks including scientific and visual question answering [24, 21, 5, 6, 28, 3]. However, their capabilities in image classification, fundamental task in machine learning, remain largely Corresponding Author: zhangkaipeng@pjlab.org.cn Preprint. (a) StanfordCars (b) SUN397 (c) DescribableTextures Figure 1: Changes in response length across different fine-tuning steps. The response length experiences sudden decrease at specific steps across all three datasets and then fluctuates. unexplored. Recent research suggests that their suboptimal performance in this area is primarily due to lack of specific data during pre-training [47]. Supervised fine-tuning of MLLMs with sufficient in-domain data has been demonstrated to enhance their performance, aligning them more closely with state-of-the-art classification models [47]. Although fine-tuning MLLMs with adequate data can boost their performance, obtaining such labeled data can often be laborious and costly, and fine-tuning on such large dataset may also require significant amount of time [48, 49]. Therefore, few-shot fine-tuning has been proposed to reduce labeling costs and training time, which has proven successful in previous contrastive vision-language model fine-tuning [48, 49, 22, 17], but has not yet been examined in auto-regressive MLLMs. In this paper, we investigate closed-form few-shot fine-tuning for Multimodal Large Language Models (MLLMs) classification, wherein subset of class names is provided for selection. Following [49, 48], we conducted experiments on eleven datasets and two settings: few-shot and base-to-new. Our initial finding show that supervised fine-tuning (SFT) may cause severe catastropic forgetting problems and lead to even lower performance compared with zero-shot performance. To address these challenges, motivated by the recent success of rule-based reinforcement learning and Deepseek R1 [11, 42, 46], we propose CLS-RL, reinforcement learning framework for classification fine-tuning. Instead of guiding the fine-tuning process with token-level loss as done in SFT, CLS-RL fine-tunes MLLMs using verifiable reward losses and guides models to explore diverse reasoning thoughts. Specifically, CLS-RL employs rule-based verifiable reward functions to guide policy optimization (GRPO) and encourage models to think before answering. Our extensive experiments on few-shot and base-to-new settings show that CLS-RL can perform much better than SFT on both in-domain learning and new class generalization. Additionally, we found an interesting \"free-lunch\" phenomenon in CLS-RL fine-tuning. Previous work in few-shot contrastive vision-language model (VLM) fine-tuning has demonstrated that, when fine-tuned on specific dataset, the performance of VLMs on other datasets is drastically degraded, phenomenon known as catastrophic forgetting. In MLLM few-shot fine-tuning, we discovered that when fine-tuned on one specific dataset using CLS-RL, the performance of MLLMs on other datasets can also be improved, phenomenon we refer to as the \"free-lunch\" phenomenon. For instance, when fine-tuned on 4-shot ImageNet dataset [7], the models performance improves by 13.6% on OxfordFlowers [27], 15.89% on StanfordCars [18], and 10.59% on FGVCAircraft [25], despite the distributions of these three datasets being markedly different from ImageNet and the class names being entirely distinct. This phenomenon validates that, rule-based fine-tuning methods can not only relieve catastrophic forgetting problems in few-shot fine-tuning, but also effectively teach models the fundamentals of image classification. Lastly, we revisit the thinking process in rule-based reinforcement learning (RL), factor considered crucial for the success of Deepseek R1 [11]. Unlike in the math or scientific problems of Deepseek R1, where the model response length gradually increases, we found that in CLS-RL, the response length drastically decreases in some steps and then fluctuates, as shown in Figure 1. This implies that the thinking process in classification may not be as critical as in scientific problems, gradually leading models to adopt the simplest possible thinking approach to arrive at the final answers (refer to Sec. 5.4). Moreover, recent studies on LLMs and MLLMs [16, 39] have demonstrated that during inference, common sense questions may not require reasoning, and overthinking can even be detrimental to certain visual tasks. Motivated by these findings, we explore rule-based RL fine-tuning 2 without thinking process and propose novel No-Thinking-CLS-RL method. In the No-ThinkingCLS-RL, for the instruction prompt part, we shift from thinking-encouraging prompt in CLS-RL to thinking-discouraging prompt, stated as: Question Please directly output the answer, where {Question} will be replaced by each specific question. For the reward component, we eliminate the format reward in CLS-RL and compel the model not to think by introducing an equality accuracy reward. Here, the accuracy reward is set to 1 only if the models output is the same as the ground truth label exactly, thereby forcing the model to bypass any thinking process during inference. Surprisingly, we found that No-Thinking-CLS-RL performs better than CLS-RL. Furthermore, by compelling the model to only output the answer, the training time of no-thinking-CLS-RL is significantly shorter than CLS-R1 (refer to Sec. 5.2). These findings demonstrate that the thinking process during RL fine-tuning may not be important, or even detrimental, for simple visual tasks such as classification. Our contributions can be summarized as follows: We explored few-shot image classification fine-tuning for MLLMs and proposed the CLS-RL method, inspired by recent success of Deepseek R1 [11]. In comparison to SFT, CLS-RL optimizes the model using verifiable reward loss, yielding significantly improved results in both base-to-new and few-shot settings. We identified \"free-lunch\" phenomenon associated with CLS-RL fine-tuning, stark contrast to the catastrophic forgetting issues observed with contrastive VLM fine-tuning. Remarkably, models fine-tuned on one specific dataset exhibited enhanced performance on other, significantly different datasets, despite variations in data distribution and even completely distinct class names. This underscores the capability of CLS-RL to genuinely instruct models in acquiring classification knowledge rather than relying on memorization. Inspired by recent investigations into the inference reasoning process and the observation that response length significantly diminishes in CLS-RL fine-tuning, we introduce No-ThinkingCLS-RL. This approach removes the format reward and introduces novel equality reward for fine-tuning, compelling the model to bypass the thinking process during both training and inference. Ultimately, No-Thinking-CLS-RL achieves superior performance compared to CLS-RL, while also substantially reducing training time."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 MLLM for Image Classification Image classification is at the core of machine learning research, driving many fundamental advancements in theoretical understanding and practical applications. Early CNN-based [20, 19, 37, 40, 12, 14, 41] models perform classification by hierarchically extracting spatial features through convolutional layers and mapping them to class probabilities. Vision transformers [8] largely follow the same pipeline but replace convolution with self-attention [43] for global feature modeling. CLIP [33] classifies images by computing the similarity between the image embedding and text embeddings of category descriptions and then selecting the closest match. More recently, the advent of MLLM enabled new paradigm for image classification that leverages both vision encoders and LLMs. Unlike traditional classifiers, MLLMs take images and natural language prompts as input and generate text-based outputs, making them more interpretable, flexible, and user-friendly. However, MLLM for image classification is still an underexplored area. Recent work [47] shows that MLLMs perform poorly at classification but can be improved with SFT. In this work, we take different angle and investigate novel approach: using rule-based RL to fine-tune MLLM for classification. We systematically study its effectiveness compared to SFT and show its advantage and potential improvements. 2.2 RL for Post Training Reinforcement learning (RL) has become an important technique used in LLM and MLLM posttraining. Introduced by InstructGPT [29] as Reinforcement Learning from Human Feedback (RLHF), RL was used to align LLM and MLLM output with humans preference. Due to the computation inefficiency of PPO [35] used in RLHF, some offline RL algorithms [26, 34] and value model free RL algorithms [36, 15] were introduced in the post training stage. Besides, researchers also explored how to build good reward model in terms of robustness and reward density [44, 10, 23]. Recently, 3 DeepSeek-R1 [11] applied the rule-based reward to the reinforcement training of LLM, proving the huge potential of RL in terms of incentivising LLMs reasoning ability. Inspired by DeepSeek-R1, we applied reinforcement learning to the classification task with rule-based reward function, and analyzed its generalizability and learning efficiency compared to supervised fine-tuning."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce the proposed CLS-RL and no-thinking-CLS-RL. 3.1 Group Relative Policy Optimization We follow Deepseek R1 [11] to employ Group Relative Policy Optimization (GRPO) as the RL algorithm for optimization. Unlike supervised fine-tuning (SFT), which optimizes models through token-level losses, RL-based methods like GRPO utilize policy gradients, calculated from reward loss, for optimization. This encourages reasoning by exploring much larger solution space [11]. Let be the qestion set, πθold be the policy model and {o1, o2, , oG} be group of response from πθold for question q. Let πθref denote the frozen reference model. The GRPO algorithms aim to optimize model πθ by the following objective: JGRPO(θ) = qQ,{oi}G i=1πθold (cid:32) (cid:34) 1 (cid:88) i=1 min πθ(oiq) πθold (oiq) Ai, clip (cid:18) πθ(oiq) πθold(oiq) (cid:19) (cid:33) (cid:35) , 1 ϵ, 1 + ϵ Ai βDKL(πθπref) , where ϵ and β are clipping hyper-parameter and the coefficient controlling the KullbackLeibler (KL) penalty, respectively. Here, Ai = rimean({r1,r2,...,rG}) is the advantage using the group reward std({r1,r2,...,rG}) {r1, r2, . . . , rG}, and DKL(πθπref) = πref(oiq) πθ(oiq) log 1 is the KL divergence loss to prevent current model πθ deviating too much from reference model πθref. GRPO eliminates the critic model in PPO by estimating the relative advantage by sampling group of responses {oi}G i=1 and normalizing their rewards within the group to compute relative advantage, which is more computationally efficient [36]. (cid:16) πref(oiq) πθ(oiq) (cid:17) 3.2 CLS-RL We then introduce the instruction prompt and reward function of CLS-RL. Following [36], we designed prompt Instruction prompt. that encourages models to first engage in thinking process before ultimately delivering the answer. The prompt is designed as: {Question} Please output the thinking process in <think> </think> and final answer in <answer> </answer> tags. Here {Question} will be replaced by each specific question. Reward Function. The reward function is composed with two parts: format reward and accuracy reward. The format reward is designed to encourage models to think before answering and extract verifiable answers for computing accuracy reward, which is designed as follow: Rformat = (cid:26)1, 0, if format is correct if format is incorrect The accuracy reward is rule-based reward that checks if the answers in <answer> </answer> tags matches with the ground truth (gt) labels, which is designed as follow: Raccuracy = (cid:26)1, if answer tag exists and extracted answer matches gt label 0, otherwise Combining format reward and accuracy reward is the final reward function of CLS-RL. 4 3.3 No-Thinking-CLS-RL As mentioned in Sec. 1, unlike the scientific problem solving in Deepseek R1 [11], we found that in CLS-RL fine-tuning, the response length fluctuates or even drastically decreases. Additionally, we observed that the rationale does not appear to be beneficial in arriving at the final answers, as shown in Sec. 5.4. These findings suggest that models progressively learn to simplify their thinking process and recognize that extensive deliberation may not be advantageous for arriving at correct answers. Moreover, recent studies on LLMs and MLLMs [16, 39] have demonstrated that common sense questions may not require chain-of-thought reasoning, and overthinking can even be detrimental to certain visual tasks, which are consistent to previous findings. These findings suggest that for simple visual tasks may not require complex reasoning, and the thinking process may not be as critical as it is for more complex tasks, such as math problems. Motivated by these insights, we explore rule-based RL without thinking process and propose novel No-Thinking-CLS-RL method. Instruction prompt. Instead of the prompt in CLS-RL, which encourages models to think, the prompt in the No-Thinking-CLS-RL method discourages or even prohibits the model from thinking. The prompt is designed as: {Question} Please directly output the answer. Here {Question} will be replaced by each specific question. Reward Function. Unlike the reward function in CLS-RL, which is composed of both format reward and an accuracy reward, the No-Thinking-CLS-RL method eliminates the format reward and retains only the accuracy reward. The accuracy reward is an equality-based matching reward that checks whether the models output matches the ground truth exactly. It is defined as follows: Raccuracy = (cid:26)1, if model output=gt label 0, otherwise The equality-based matching reward forces the model to bypass any thinking process and output only the answers, which are significantly shorter than the typical reasoning responses in CLS-RL. As result, the training and inference time of No-Thinking-CLS-RL is much shorter than that of CLS-RL. We will show that with much less training time, No-Thinking-CLS-RL can outperform CLS-RL in Section 4."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we introduce the experimental results. 4.1 Experimental Setup Setting. In this paper, we mainly focus on closed-form classification for MLLMs, where subset of class names is provided for selection. The question format is What kind of object in this {Instruction prompt}, where {Instruction prompt} replaced by each method. phote? Datasets. Following [49, 48], we conducted extensive experiments on 11 public classification benchmark datasets to evaluate SFT, CLS-RL, and No-Thinking-CLS-Rl. The datasets are ImageNet [7], Caltech101 [9], OxfordPets [30], StanfordCars [18], Flowers102 [27], Food101 [2], FGVCAircraft [25], SUN397 [45], DTD [4], EuroSAT [13], and UCF101 [38]. For closed-form classification, we sample 40% of labels (80% for base-to-new) including ground truth labels to form the classification list in question. Implementation Details. We implement our code in Pytorch [31]. We utilize Qwen2-VL-2BInstruct [1] as the base model, and fine-tune all parameters during training. All training is conducted in 8 A100 GPUs. The batch size is set to 1 per GPU and we use 2-step gradient accumulation during training. All images are resized to 328328 resolution. We first extract answers from <answer> ...</answer> tag and then verify if class names are in answers. If <answer> </answer> tag does not exist in model responses we directly verify if class names are in model responses, following [47]. 4.2 Base-to-New Results In this subsection, we introduce the results on the base-to-new generalization setting. Following [49], We split each dateset into two disjoint groups: the base class dataset and the new class dataset. This 5 Table 1: Comparison of Qwen2VL instruct, SFT, CLS-RL, and no-thinking-RL in the base-to-new generalization setting. No-Thinking: no-thinking-RL. Base: base class accuracy. New: new class accuracy. H: harmonic mean accuracy. no-thinking: No-Thinking-CLS-RL. (a) Average over 11 datasets. (b) ImageNet. (c) Caltech101. Base New Base New Base New 62.1 66.27 64.12 Qwen2VL 67.4 70.73 69.03 SFT CLS-RL 81.17 79.15 80.15 no-thinking 83.42 81.88 82. Qwen2VL 61.56 74.9 67.58 27.78 47.78 35.13 SFT CLS-RL 88.12 90.01 89.05 no-thinking 88.97 90.66 89.81 Qwen2VL 88.83 92.9 90.82 93.87 93.01 93.44 SFT CLS-RL 97.74 95.2 96.45 no-thinking 97.93 95.63 96.77 (d) DescribableTextures. (e) EuroSAT. (f) Food101. Base New Base New Base New Qwen2VL 60.99 61.34 61.17 71.98 71.41 71.7 SFT CLS-RL 77.42 67.82 72.3 no-thinking 77.42 70.37 73. Qwen2VL 54.52 63.54 58.69 91.55 77.87 84.16 SFT CLS-RL 58.09 69.33 63.22 no-thinking 66.43 74.13 70.07 Qwen2VL 74.07 80.42 77.12 74.27 77.82 76.0 SFT CLS-RL 87.29 87.56 87.42 no-thinking 88.77 88.93 88.85 (g) OxfordFlowers. (h) OxfordPets. (i) StanfordCars. Base New Base New Base New Qwen2VL 61.1 60.99 61.05 97.77 94.95 96.34 SFT 87.78 74.97 80.87 CLS-RL no-thinking 88.71 76.73 82. Qwen2VL 75.59 91.79 82.9 SFT 84.06 86.28 85.15 83.28 94.49 88.53 CLS-RL no-thinking 86.64 95.5 90.85 Qwen2VL 43.81 33.15 37.74 SFT 74.54 69.68 72.03 82.08 75.74 78.78 CLS-RL no-thinking 91.13 87.04 89.04 (j) SUN397. (k) UCF101. (l) FGVCAircraft. Base New Base New Base New Qwen2VL 56.97 65.8 61.07 27.39 37.8 31.77 SFT CLS-RL 81.03 82.52 81.77 no-thinking 83.18 84.14 83. 69.6 64.62 67.02 Qwen2VL 59.95 63.93 61.87 SFT CLS-RL 79.47 74.95 77.14 no-thinking 80.47 79.18 79.82 Qwen2VL 36.07 39.47 37.7 38.23 57.53 45.94 SFT 70.53 58.07 63.69 CLS-RL no-thinking 68.01 58.31 62.79 experimental setup is designed to assess the models ability to acquire downstream knowledge while simultaneously demonstrating generalization to novel information. All the methods are fine-tuned on the base class dataset and evaluated on both the base and new class test dataset. We conduct 4-shot experiments. The results are shown in Table 1. As demonstrated in Table 1, our proposed CLS-RL framework notably surpasses SFT in performance. In detail, CLS-RL exhibits an approximately 14% higher accuracy for base classes and 9% increase for new class accuracy, leading to an aggregate improvement of 11% in the harmonic mean accuracy. These results demonstrate the effectiveness of rule-base reinforcemnet fine-tuning in image classification. However, we also find that SFT can surpass CLS-RL in certain datasets, such as OxfordFlowers and EuroSAT, indicating that SFT may hold advantages in specific scenarios. We also observed that SFT significantly underperforms on the ImageNet and SUN397 datasets. The likely reason is that the prompts in these two datasets are very lengthy, preventing SFT from effectively memorizing classification knowledge, thereby resulting in poor performance. The proposed No-Thinking-CLS-RL demonstrates superior performance, in both base class and new class average accuracy, resulting in 2.5% enhancement in average harmonic mean accuracy, compared with CLS-RL. These findings suggest that omitting the thinking process during fine-tuning allows rule-based RL to achieve improved classification performance and enhanced generalization capabilities than with thinking process. 4.3 Few-Shot Learning Results In this subsection, we present the results of few-shot learning. Few-shot learning aims to verify whether method can learn task-specific knowledge effectively. We train SFT and CLS-RL on 6 Table 2: Few-shot learning results. S.C.: StanfordCars dataset. F.A.: FGVCAircraft dataset. ImageNet 70.8 41.60 92.24 92.31 Caltech 88.56 93.91 98.09 98.46 DTD 54.79 71.336 69.92 73.52 EuroSAT 45.68 75.16 49.46 58.02 Food 77.54 75.75 88.94 90.78 Flowers102 64.43 96.87 86.56 91.6 OxfordPets 73.89 85.80 87.24 86.13 SU 63.83 41.66 84.57 86.72 UCF101 66.22 63.81 82.1 83.82 Average 62.21 70.65 81.25 84.39 F.A. 42.75 60.15 74.41 74. S.C. 35.77 71.13 80.24 92.5 Qwen2VL SFT CLS-RL No-Thinking-RL (a) CLS-RL (b) No-Thinking-CLS-RL Figure 2: Visualization of improvement of CLS-RL and No-Thinking-CLS-RL on different datastes compared with zero-shot Qwen2 VL. Left: improvement of CLS-RL. Right: improvement of NoThinking-CLS-RL. The x-axis represents the fine-tuning dataset, and the y-axis represents the test dataset. The accuracy improvement is marked as red, and decrease is marked as blue 4-shot setting and report the accuracy results in Table 2. From Table 2, we observe that CLS-RL markedly surpasses SFT in most datasets, resulting in notably higher average accuracy than SFT, which demonstates that rule-base reinforcement fine-tuning can let model learn better downstream knowledge than SFT. Similar to the base-to-new setting, we also note that SFT can exceed CLSRLs performance in specific datasets. We further discover that No-Thinking-CLS-RL outperforms CLS-RL in 10 out of 11 datasets, ultimately achieving 3.14% higher average accuracy compared to CLS-RL. These results indicate that conducting RL fine-tuning without the thinking process can effectively enhance the models performance on downstream tasks compared with fine-tuning with thinking. 4.4 Free-Lunch Phenomenon In this section, we discuss the free-luch phenomenon. Previous work in few-shot contrastive VLM (CLIP [32]) fine-tuning has demonstrated that, when fine-tuned on specific dataset, the performance of VLMs on other datasets is drastically degraded, phenomenon known as catastrophic forgetting [48, 17]. However, we will show that CLS-RL and No-Thinking-CLS-RL fine-tuning can enhance the performance of MLLMs on other datasets when fine-tuned on one specific dataset. We visualize the improvement of CLS-RL and No-Thinking-CLS-RL fine-tuning compared with zero-shot Qwen2VL instruct in Figure 2. Here, we present the results of the Qwen2VL-Instruct model repeatedly fine-tuned on single dataset and subsequently tested across all 11 datasets. From Figure 2, it is observed that when fine-tuned on specific dataset, both CLS-RL and NoThinking-CLS-RL tend to yield improvements on other datasets in most instances, despite variations 7 (a) DescribableTextures Acc Reward (b) DescribableTextures Test Acc (c) OxfordFlowers Acc Reward (D) OxfordFlowers Test Acc Figure 3: Comparison of accuracy reward convergence speed and test accuracy over steps of CLS-RL and No-Thinking-CLS-RL. Table 3: Training and inference efficiency comparison between CLS-RL and No-Thinking-CLS-RL."
        },
        {
            "title": "Accuracy Training Time",
            "content": "SFT CLS-RL No-Thinking-CLS-RL 41.60 92.24 92.31 35 min 1587 min 94 min Inference Time 20min 30 min 26 min in data distribution and even completely different class lists. For example, when fine-tuned on 4-shot SUN397 dataset [45], the models performance improves by 16.98% on ImageNet [7], 15.88% on StanfordCars [18], and 11.10% on UCF101 [38]. These results indicate that the application of rule-based verifiable signals and reward loss for model fine-tuning can effectively compel models to acquire essential classification knowledge instead of memorizing. Consequently, this equips them to achieve superior performance on entirely new datasets. However, it is also noted that such improvements can be negative in certain instances. For example, fine-tuning on the EuroSAT dataset could result in diminished performance on the OxfordPets dataset. This outcome is understandable, given that the knowledge required for the EuroSAT dataset is significantly divergent from that of the OxfordPets dataset, and the classification knowledge from the EuroSAT dataset may even be detrimental to the classification of the OxfordPets dataset."
        },
        {
            "title": "5 More Analysis About Thinking Process",
            "content": "In this section, we present more discussions about the thinking process in rule-based reinforcement fine-tuning for visual classification. 5.1 Convergence Comparison In this subsection, we explore the differences in convergence rates between CLS-RL and No-ThinkingCLS-RL. We illustrate the accuracy reward at each training step and examine the test accuracy at select intervals on the Describable Textures and Oxford Flowers datasets. The results are shown in Figure 3. We observe that No-Thinking-CLS-RL exhibits faster convergence speed compared to CLS-RL, as evidenced by higher accuracy reward at most steps and significantly higher test accuracy in the early stages of training (within the first 30 steps). possible explanation for this could be that in CLS-RL, the format of the reward loss may introduce some noise during fine-tuning, potentially leading to instability in accuracy reward at the beginning of the fine-tuning process. 5.2 Efficiency Comparison In this subsection, we compare the training and inference efficiency of CLS-RL and No-ThinkingCLS-RL, using the ImageNet dataset as case study. The runtime for both training and inference is measured. The results are presented in Table 3. It is found that CLS-RL requires significantly more time for both training and inference compared to SFT and No-Thinking-CLS-RL, attributable to the necessity of generating multiple lengthy responses duringthe fine-tuning and long reasoning response to ontain final answer during inference phases. In contrast, SFT optimizes only the label tokens during fine-tuning, and No-Thinking-CLS-RL compels the model to output only the ground truth labels during this phase, which significantly reduces the time required. During the inference phase, both methods are designed to output solely class labels, resulting in considerably reduced processing (a) No-Thinking-CLS-RL vs CLS-RL (b) Normal prompt vs No-Thinking prompt Figure 4: Visualization of improvement of no-thinking-CLS-RL on different datastes compared with CLS-RL (Left) and improvement of CLS-RL compared with CLS-RL with no-thinking prompt (Right). The accuracy improvement is marked as red, and decrease is marked as blue time. Indeed, No-Thinking-CLS-RL can be viewed as hybrid of SFT and CLS-RL. The aim of No-Thinking-CLS-RL aligns with that of SFT, which is to compel the model to produce responses identical to the labels. However, while SFT employs token-level supervised loss for optimization, No-Thinking-CLS-RL leverages an equality accuracy reward to fine-tune the model. In summary, No-Thinking-CLS-RL has the potential to not only improve the performance of CLS-RL but also to significantly reduce both training and inference times. 5.3 Which datasets require thinking? In this subsection, we examine the cross-dataset generalization capability, often referred to as the \"free-lunch phenomenon,\" between No-Thinking-CLS-RL and CLS-RL. We illustrate the comparative improvement of No-Thinking-CLS-RL over CLS-RL by testing on 11 datasets, using model that was fine-tuned on one specific dataset. We also showcase the comparative improvement achieved by using standard training prompt over No-Thinking Prompt (which directs CLS-RL to immediately produce the answer during inference). The results are shown in Figure 4. We can find that NoThinking-CLS-RL has better cross-dataset generalization ability than CLS-RL, except OxfordPets and FGVCAircraft datasets. This suggests that utilizing an equality reward without engaging in any thinking process can enhance the models cross-dataset generalization ability, with the exceptions being the OxfordPets and FGVCAircraft datasets. For these datasets, the thinking process during fine-tuning appears to be important for cross-dataset performance. These findings are consistent to the results of using different inference prompt strategies of CLS-RL, as shown in Figure 4 (b). We find that using normal prompt for in-domain fewshot test performs better than No-Thinking prompt among all datasets. This is reasonable given that the normal prompt is the one used for fine-tuning. However, it is surprising to discover that using the normal prompt for cross-dataset testing results in lower performance in many cases compared to the no-thinking prompt, except OxfordPets and FGVCAircraft datasets. These findings suggest that while CLS-RL fine-tuning may enable the model to learn good cross-dataset classification generalization ability, over-thinking during inference can potentially diminish this ability. 5.4 Does the thinking process really help reasoning? In this subsection, we delve into the content of the thought process in CLS-RL. We showcase examples of model responses from CLS-RL across six datasets in Figure 5. Typically, the content found in the \"thinking\" tags are somewhat trivial, such as \"This is photo of <class>\" which offers little to 9 Figure 5: Examples of CLS-RL fine-tuned model responses across six datasets. The thinking content appears not very beneficial to deriving the final answers. Table 4: Comparison on open-set Few-shot learning results. ImageNet Caltech101 Food101 Flowers102 OxfordPets Average Qwen2VL CLS-RL No-Thinking-CLS-RL 46.57 54.84 56.45 62.96 79.07 86.29 57.79 73.51 71.99 48.44 67.64 71.21 47.40 89.94 86. 52.63 73.0 74.40 no benefit towards arriving at the final answer, or they might already represent the final answers themselves. This observation aligns with the phenomenon observed in Figure 1, where there is decrease in response length during fine-tuning. This suggests that the model progressively learns it can achieve better performance by directly answering the questions and engaging in less \"thinking\" to arrive at the final answers. 5.5 Open-set Classification Comparison We present the results of open-set classification using CLS-RL and No-Thinking-CLS-RL in this subsection. Unlike closed-form classification, open-set classification is not well-defined problem, which is much harder or even unrealistic for some datasets, since synonyms, plural forms, and name partially missing will be judged as incorrect. For example, in the StanfordCars dataset [18], the model can hardly output the correct year of the car in images. Therefore, we selected five datasets with class names that are relatively straightforward for the model to output and compare the few-shot learning performance between CLS-RL and No-Thinking-CLS-RL. The results are shown in Table 4. No-Thinking-CLS-RL outperforms CLS-RL on three datasets among five datasets, ultimately achieving 1.4% improvement in average accuracy over CLS-RL."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we explore the problems of few-shot image classification fine-tuning in MLLMs. We found that SFT tends to induce severe overfitting and catastrophic forgetting issues, resulting in performance that is even inferior to that of the zero-shot model. To tackle this challenge, inspired by the recent successes in rule-based RL fine-tuning, we propose CLS-RL to fine-tune MLLMs for classification tasks. CLS-RL employs format reward to prompt the model to engage in thinking process before answering during training, along with verifiable accuracy reward for model 10 optimization. As result, CLS-RL performs much better than SFT on both base-to-new and few-shot settings. Furthermore, we observed \"free-lunch\" phenomenon, wherein the performance of model fine-tuned on one specific dataset improves on other distinct datasets. This demonstrates that CLS-RL enables the model to genuinely learn classification knowledge, rather than merely memorizing information. Lastly, we delve into the thinking process of CLS-RL. Inspired by recent research on varying inference strategies and observations regarding the decrease in response length by CLS-RL, we introduce No-Thinking-CLS-RL. No-Thinking-CLS-RL eliminates the format reward and employs an equality reward to compel the model to forego the thinking process and directly output the final answer. No-Thinking-CLS-RL outperforms CLS-RL in both few-shot and base-to-new settings, while also significantly reducing training and inference times. We hope our exploration of the thinking process in rule-based reinforcement learning for classification can provide insights for other visual tasks in MLLMs."
        },
        {
            "title": "References",
            "content": "[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [2] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101mining discriminative components with random forests. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446461. Springer, 2014. [3] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36063613, 2014. [5] DeepMind. Gemini 2.0 flash experimental. https://deepmind.google/technologies/ gemini/flash/, 2024. Accessed: 2024-12-25. [6] DeepMind. Gemini 2.0 flash thinking. https://deepmind.google/technologies/ gemini/flash-thinking/, 2025. Accessed: 2025-01-21. [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [9] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. [10] X. Guan, L. L. Zhang, Y. Liu, N. Shang, Y. Sun, Y. Zhu, F. Yang, and M. Yang. rstarmath: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [11] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2016. [13] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. In arXiv preprint arXiv:1704.04861, 2017. 11 [15] J. Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [16] D. Jiang, R. Zhang, Z. Guo, Y. Li, Y. Qi, X. Chen, L. Wang, J. Jin, C. Guo, S. Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [17] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1911319122, 2023. [18] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained In Proceedings of the IEEE international conference on computer vision categorization. workshops, pages 554561, 2013. [19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 10971105, 2012. [20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [21] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. [22] M. Li, J. Zhong, C. Li, L. Li, N. Lin, and M. Sugiyama. Vision-language model fine-tuning via simple parameter-efficient modification. arXiv preprint arXiv:2409.16718, 2024. [23] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [24] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, towards real-world vision-language understanding. arXiv preprint et al. Deepseek-vl: arXiv:2403.05525, 2024. [25] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [26] Y. Meng, M. Xia, and D. Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [27] M.-E. Nilsback and A. Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. [28] OpenAI. Gpt-4o system card. https://cdn.openai.com/gpt-4o-system-card.pdf, 2024. Accessed: 2024-09-26. [29] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [30] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. [31] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:80268037, 2019. [32] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [34] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 12 [35] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [36] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [37] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [38] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [39] Z. Sprague, F. Yin, J. D. Rodriguez, D. Jiang, M. Wadhwa, P. Singhal, X. Zhao, X. Ye, K. Mahowald, and G. Durrett. To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. [40] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 19, 2015. [41] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (ICML), pages 61056114, 2019. [42] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023. [44] P. Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [45] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. [46] T. Xie, Z. Gao, Q. Ren, H. Luo, Y. Hong, B. Dai, J. Zhou, K. Qiu, Z. Wu, and C. Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [47] Y. Zhang, A. Unell, X. Wang, D. Ghosh, Y. Su, L. Schmidt, and S. Yeung-Levy. Why are visuallygrounded language models bad at image classification? arXiv preprint arXiv:2405.18415, 2024. [48] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1681616825, 2022. [49] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):23372348, 2022."
        }
    ],
    "affiliations": [
        "Emory University",
        "Shanghai AI Laboratory",
        "University of Southern California"
    ]
}