{
    "paper_title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
    "authors": [
        "Chunshi Wang",
        "Junliang Ye",
        "Yunhan Yang",
        "Yang Li",
        "Zizhuo Lin",
        "Jun Zhu",
        "Zhuo Chen",
        "Yawei Luo",
        "Chunchao Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 7 4 6 3 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "PART-X-MLLM: PART-AWARE LARGE LANGUAGE MODEL 3D MULTIMODAL Chunshi Wang,1,2, Junliang Ye,,2,3, Yunhan Yang,2,4, Yang Li2, Zizhuo Lin1 Jun Zhu3, Zhuo Chen2, Yawei Luo1,, Chunchao Guo2, 1Zhejiang University, 2Tencent Hunyuan, 3Tsinghua University, 4The University of Hong Kong Figure 1: Part-X-MLLM is natively 3D, part-aware multimodal large language model that provides comprehensive understanding of 3D shapes and supports wide range of 3D understanding tasks. It also seamlessly integrates with diffusion-based pipelines, enabling semantically precise part-aware 3D shape generation and editing."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Part-X-MLLM, native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in structured, executable grammar. Given an RGB point cloud and natural language prompt, our model autoregressively generates single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as versatile interface to drive downstream geometryaware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through single, language-native frontend. We pre-train dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/ Equal contribution. Corresponding Author. Project Leader."
        },
        {
            "title": "Tencent Hunyuan",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "The creation of rich, interactive 3D worlds is cornerstone of modern visual computing. While recent advances in generative AI have solved the creation of holistic 3D shapes, they largely treat assets as static, monolithic forms. This structural opaqueness limits their use in essential downstream tasks like fine-grained semantic understanding, compositional editing and procedural animation. Real-world objects are inherently assemblies of meaningful parts; unlocking 3D interaction thus demands native LLM-based tool that can reason about and manipulate this part structure. Current 3D Multimodal Large Models (MLLMs) fall short of this goal. Scene-level 3D MLLMs align point clouds with language and perform captioning or Q&A Xu et al. (2024); Hong et al. (2023); Qi et al. (2024b;a), but they largely treat objects as monolithic and lack persistent part identifiers, grounded references, and executable outputs. On the generative side, geometry-oriented models offer high-fidelity asset synthesis via structured 3D latents Xiang et al. (2024); Zhao et al. (2025c); Hunyuan3D et al. (2025) or tokenized 3D representations Wang et al. (2024); Ye et al. (2025b), yet expose limited semantic addressability. Part pipelines either lift 2D segmentations to 3D Liu et al. (2024a); Chen et al. (2025a); Yang et al. (2024); Liu et al. (2025c); Yang et al. (2025a)prone to view inconsistencies and weak 3D constraintsor generate parts natively in 3D Chen et al. (2025b); Zhang et al. (2025a); Yang et al. (2025b) without unified language interface. Editing methods increasingly operate in 3D space Ye et al. (2025c); Li et al. (2025a), but are not themselves language-native frontends. There is still no model that (i) understands and names parts, (ii) grounds references to persistent bounding box (BBox), and (iii) compiles executable add/delete/modify programs while delegating to strong geometry engineswith controllable semantic granularity (from coarse labels to fine descriptions)through single instruction-following interface. We address this challenge with Part-X-MLLM, native 3D part-aware Multimodal Large Language Model that reframes 3D interaction as language modeling problem. Our core insight is that spectrum of disparate tasksgeneration, editing, and question answeringcan be unified under single, geometry-aware grammar of parts. Part-X-MLLM translates user instructions and 3D visual input into structured program, emitting single token sequence of part-level bounding boxes, persistent references, semantic descriptions, and edit operators. This discrete, language-native interface provides three concrete benefits. (1) Stable part identity and grounding: tokens carry persistent references to parts via BBox symbols, enabling precise, auditable reasoning and manipulation across steps and tasks. (2) Controllable semantic granularity: the same program can surface either coarse labels or fine descriptions on demand, and our post-hoc clustering supports user-controlled merging of parts. (3) Separation of structure and semantics: dual-encoder design decouples geometry (XYZ+normals) from appearance (RGB), avoiding the representational conflict observed in singleencoder ablations and yielding consistent gains on box listing, multi-part grounding, and part Q&A. Because the output program is model-agnostic, any geometry module can be driven by this token interfaceturning language into universal control surface for 3D assets. Empirically, the resulting plans enable strong part grounding, compositional generation, and localized editing across 11 task families on our UniPart-Bench, establishing general paradigm for part-centric 3D intelligence. Our contributions are summarized as follows: We introduce Part-X-MLLM, native 3D part-aware MLLM that unifies generation, editing, and reasoning as single geometry-aware program in part grammar with persistent BBox tokensproviding language-native, model-agnostic control surface for 3D assets. We propose dual-encoder architecture that decouples structure (XYZ+normals) from appearance (RGB), avoiding representational conflicts and delivering consistent gains over single-encoder baseline across grounding, captioning, and part Q&A. We enable semantic granularity control by clustering part bounding boxes using text semantics, allowing seamless transition between coarse components and fine-grained parts under the same programmatic interface. We establish UniPart-Bench, 30k-entry part-centric benchmark spanning 11 task families with geometric and linguistic metrics, and use it to rigorously evaluate plan quality and downstream performance."
        },
        {
            "title": "2.1 MULTIMODAL UNDERSTANDING",
            "content": "Early 3D MLLMs align point clouds with language for 3D captioning, QA, and reasoning, including PointLLM Xu et al. (2024), 3D-LLM Hong et al. (2023), Point-BERT Yu et al. (2022), GPT4Point Qi et al. (2024b), and ShapeLLM Qi et al. (2024a). However, point clouds sparsity and limited detail constrain high-fidelity, editable asset creation. Beyond object-level pointlanguage alignment, scene-level 3D-LLMs ground dialogue and reasoning in reconstructed scenes or videos, leveraging object identifiers, superpoints, or position-aware video features, e.g., Chat-Scene Huang et al. (2024), Chat-3D Wang et al. (2023), LL3DA Chen et al. (2024c), 3D-LLAVA Deng et al. (2025), and Video-3D LLM Zheng et al. (2025). Parallel efforts extend LM reasoning to 3D tasks and programmatic scene abstractions, such as Scene-LLM Fu et al. (2024) and the Scene Language Zhang et al. (2025b). Language-embedded scene reconstruction and spatial intelligence further improve grounding and generalization, as in LangScene-X Liu et al. (2025a) and SpatialMLLM Wu et al. (2025a). 2.2 3D GENERATION Recent advances in 3D generation have followed three primary paradigms. First, optimizationbased score distillation sampling (SDS) optimizes 3D representation under the guidance of 2D diffusion model, originating from DreamFusion Poole et al. (2022) and further aligned with human preferences Ye et al. (2024); Liu et al. (2025b); Miao et al. (2025). Second, the sparse voxel-based approach, as exemplified by TRELLIS Zhang et al. (2023; 2024); Lai et al. (2025); Zhao et al. (2023); Li et al. (2024b), encodes geometry using sparse 3D voxel grids with associated feature vectors. While this approach excels at capturing fine geometric details, it incurs substantial computational costs due to the large number of tokens required at high resolutions. Third, vector set representations, as in 3DShape2VecSet Li et al. (2025b); Chen et al. (2025c); Xiang et al. (2025); Ye et al. (2025a); Wu et al. (2025b); Zhao et al. (2025a), compress 3D shapes into compact latent codes, offering computational efficiency and scalability but often sacrificing surface detail fidelity. 2.3 PART GENERATION 2D-driven pipelines extract multi-view cues then lift to 3D: Part123 Liu et al. (2024a) and PhyCAGE Yan et al. (2024b) uses SAM Kirillov et al. (2023) masks, PartGen Chen et al. (2025a) segments/inpaints with inconsistency issues, SAMPart3D Yang et al. (2024) and PartField Liu et al. (2025c) distill priors, and HoloPart Yang et al. (2025a) completes parts with diffusion. These methods suffer from weak 3D constraints. Direct 3D approaches include: PASTA Li et al. (2024a) for primitive composition, AutoPartGen Chen et al. (2025b) for autoregressive generation, PartPacker Tang et al. (2025) and Frankenstein Yan et al. (2024a) for efficient part representation with constrained space usage, BANG Zhang et al. (2025a) for exploded views, and Assembler Zhao et al. (2025b) for assembly sampling. OmniPart Yang et al. (2025b) unifies these approaches via autoregressive box planning followed by TRELLIS-based synthesis. X-Part Yan et al. (2025) scale up vecset-based part generation conditioned on semantics provided by Ma et al. (2025). 2.4 3D EDITING Optimization-based editing utilizes SDS: DreamFusion Poole et al. (2022) enables text-to-3D generation, Vox-E Sella et al. (2023) adds volumetric regularization, and Instruct-NeRF2NeRF Haque et al. (2023) edits multi-views using InstructPix2Pix Brooks et al. (2023) while optimizing NeRF Mildenhall et al. (2021). Faster alternatives include: Shap-Editor Chen et al. (2024b) for feed-forward latent editing, MVEdit Chen et al. (2024a) as training-free 3D adapter, and PrEditor3D Erkoc et al. (2025) using DDPM inversion with 2D-to-3D lifting. FocalDreamer Li et al. (2024c) enables part-wise assembly, Nano3D Ye et al. (2025c) and VoxHammer Li et al. (2025a) performs training-free latent editing, and Make-Your-3D Liu et al. (2024b) customizes subjects via model co-evolution. Yet these methods are typically tool-side: they do not provide language-native model that reasons about parts and emits executable edit programs with precise spatial grounding. We target this gap by coupling part-aware planning interface with strong geometry backends."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 2: The Part-X-MLLM Framework. Our pipeline begins by encoding geometry and appearance features separately using dual-encoder architecture, which are then fused together with text prompts. These combined features are passed to an autoregressive decoder that generates program-like token sequence representing plan (e.g., bounding boxes, edit commands). Finally, specialized geometry heads execute this plan to enable part-aware generation and editing."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "An overview of our framework is shown in Figure 2. Our methodology centers on three key design choices: unified architecture that processes geometry and language, multi-stage training curriculum that systematically builds model capabilities, and the use of powerful, pre-existing geometry engines as execution backends. 3.1 MOTIVATION Modern 3D applications demand more than holistic shape synthesisthey require precise, languagedriven control over semantically meaningful parts. For example, artists want to swap handles without touching the body; roboticists need to reason about graspable subcomponents; and downstream pipelines rely on consistent, addressable structure for animation and simulation. Prior systems either focus on scene-level understanding or provide powerful but siloed generators/editors with bespoke interfaces. Our goal is native, part-centric MLLM that treats parts as first-class citizens and exposes single, executable interface that is intuitive, auditable, and robust across categories. 3.2 UNIFIED ARCHITECTURE FOR PART-AWARE PLANNING Dual 3D Encoders. To capture both geometric structure and visual appearance, we employ dualpathway encoder. Structure Encoder processes the raw point cloud geometry (XYZ and normals) to extract structural tokens. parallel Semantic Encoder processes RGB color information to produce appearance tokens. This dual representation allows the model to disambiguate parts that may be structurally similar but visually distinct (e.g., two identical chair legs of different colors). Structured Planning Language and Autoregressive Decoder. decoder-only transformer, initialized from pretrained LLM, takes the fused sequence of structural, semantic, and text tokens as input. It is trained to autoregressively generate program-like output that follows our structured planning language. This language defines special tokens for part representation (e.g., <boxs>...<boxe> wrapping six quantized coordinate tokens) and edit operations (e.g., <adds>, <dels>, <mods>). By formulating the output as program, we unify diverse tasks into single instruction-following problem, where the models goal is always to generate the correct token sequence representing the plan. 3.3 DOWNSTREAM GEOMETRY INTERFACES Our models structured output is designed to be consumed by downstream modules capable of interpreting its geometric and semantic content."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 3: Task realization with planning language. decoder outputs program tokens that unify diverse interactions: (Top) part-aware generation guided by bounding boxes; (Middle) grounded Q&A whose answers embed BBox tokens; (Bottom) auto-located 3D editing executed via cuboid masks and commands. The numbered circles (e.g., ) denote the corresponding task types. Part-Aware Synthesis. For generation, the planned bounding boxes and optional part text are passed to synthesis module, which treats the boxes as spatial guides to generate high-fidelity, part-based assets (e.g., in mesh, 3DGS, or NeRF format). Localized Editing. For editing, the emitted program and associated bounding boxes are used to define cuboid masks for localized manipulation, enabling precise edits while preserving untouched regions. 3.4 END-TO-END TASK REALIZATION To make the workflow concrete, Figure 3 illustrates how our structured planning language realizes four representative tasks. Part-aware Mesh Generation: The decoder generates program containing set of bounding boxes and optional part text. synthesis module then uses these boxes as spatial guides to generate part-based asset. Q&A with Grounding: Answers are augmented with BBox tokens, yielding language outputs that carry explicit, persistent references to parts. Auto-located 3D Editing: The model localizes the instruction by generating bounding boxes and an edit command (e.g., <adds>). downstream editing module then uses this program to apply masked edit. Semantic Granularity Control. Beyond these core tasks, our box-and-text representation enables dynamic control over semantic granularity. By clustering part bounding boxes based on the similarity of their associated text descriptions (using CLIP embeddings), we can progressively merge fine-grained parts into coarser semantic components. This allows users to control the level of detail in the generated output without manual intervention, such as pre-defining the number of parts (cf. PartPacker) or manually merging masks (cf. OmniPart). qualitative example is shown in Figure 6, with the full algorithm detailed in the appendix. 3.5 MULTI-STAGE INSTRUCTION TUNING We adopt two-stage curriculum. The first stage pretrains structure-aware encoder for robust geometry understanding. The second stage performs full instruction tuning, integrating semantic encoder and aligning powerful LLM with our specialized task grammar. Stage 1: Geometry-Only BBox Pretraining. We initialize the structure encoder with the Hunyuan 2.1 3D Shape VAE Encoder. Each training sample is fixed-size RGB-less point cloud of shape (40960, 6) containing (x, y, z) coordinates and surface normals. The encoder downsamples features"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "by 20 to produce latent of length 2048. To force bounding-box knowledge into the encoder, we pair it with lightweight autoregressive decoder whose task is to predict part-level bounding boxes from these latent features, with no textual semantics involved. After pretraining on 3.6M objects for 10 epochs, we retain the specialized structure encoder weights and discard the lightweight decoder. This stage domain-specializes the 3D encoder to reliably disentangle and localize part BBoxes. Stage 2: Full Instruction Tuning with Dual-Encoder LLM. After pretraining the structure encoder, we proceed directly to full instruction tuning with more powerful Qwen 2.5 VL model. In this stage, we introduce the Semantic (RGB) Encoder, which has the same architecture as the structure encoder and processes point cloud of shape (10240, 6) with (x, y, z) and (r, g, b) data to capture appearance. We also extend the vocabulary with our task-specific special tokens (e.g., <boxs>/<boxe>, <adds>/<adde>). During this stage, we freeze the pretrained Structure Encoder from Stage 1 and the original Qwen 2.5 VL token embeddings. We then train only the new Semantic Encoder, the AR transformer layers of the Qwen 2.5 VL decoder, and the embeddings for our newly added special tokens. This approach efficiently aligns the powerful language model with our dual-stream (geometry and appearance) conditioning and executable grammar, preserving its strong prior while adapting it for our specialized tasks."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS We train on 64 A100 GPUs. Stage 1 runs for 2 days and Stage 2 runs for 5 days. We use AdamW as the optimizer with learning rates of 2 104 (Stage 1) and 5 108 (Stage 2). The batch size is 256 in Stage 1 and 64 in Stage 2. All our training data is annotated using Vision-Language Models (VLMs). To translate plans into high-fidelity geometry, we use powerful, off-the-shelf models as execution backends. For part-aware generation, we use the synthesis module from OmniPart Yang et al. (2025b), feeding it our generated bounding boxes. For editing, we use the training-free volumetric editor Nano3D Ye et al. (2025c) and VoxHammer Li et al. (2025a), providing it with cuboid mask derived from our planned BBox and the users instruction. This modular approach allows Part-X-MLLM to serve as universal, language-driven frontend for various SOTA geometry engines. The rich information encoded in the generated token probabilities also enables advanced downstream tasks, such as confidence-aware face segmentation (see Appendix A.4). 4.2 DATASET AND EVALUATION PROTOCOL We curate high-quality, part-centric 3D dataset comprising 85,771 distinct objects with an average of 23 parts per object. Each object is annotated with axis-aligned part bounding boxes (AABBs) and paired natural language annotations at two granularities: coarse part label (Q1) and finegrained part description (Q2). At the object level, we include an overall caption and small set of instructionanswer pairs for part-aware Q&A. All annotations follow the unified box-token grammar introduced in Section 3, enabling consistent serialization of AABBs and edit programs. Data construction follows two-step pipeline: (1) structured labeling stage collecting object-level and part-level texts and (2) data building stage converting annotations into instruction-following samples across multiple task families (grounding, captioning, QA, editing). Concretely, we instantiate eleven task templates (Types 010) covering pure box listing, multi-part grounding with coarse/fine text, single-part grounding from name or description, box-to-text captioning, part-aware Q&A, and edit programs for deletion/modification/addition. The train/test split is obtained by deterministic file list partition ( 99.5/0.5). Full details, prompt templates, sampling rules, and dataset statistics are provided in the supplementary material (Tables 5 and figures therein). Since existing benchmarks do not test for structured, part-aware, and executable program generation from language, we introduce UniPart-Bench, held-out set of 400 objects, to evaluate our models core capabilities. Our evaluation focuses on the quality of the structured plans generated by the model, as measured by the accuracy of the predicted BBox layouts. For downstream tasks, the generated plans are passed to external geometry modules. For generation, we forward the BBoxes to synthesis head; for editing, we provide the instruction and cuboid mask derived from the planned BBox."
        },
        {
            "title": "4.3 PART-AWARE GENERATION AND EDITING",
            "content": "Bounding Box Generation. To evaluate the quality of our structured generation, we report BBox IoU, Voxel Recall, and Voxel IoU. Matching pairs each ground-truth box with its nearest predicted box. As baselines, we include PartField Liu et al. (2025c) by treating the voxel set as point cloud and extracting BBox per predicted segment, and the generation model from OmniPart Yang et al. (2025b). Our model consumes RGB point cloud tokens and text prompt and autoregressively emits an ordered list of bounding boxes following the box grammar of Section 3. For the PartField baseline, we treat voxels derived from the asset as point cloud and segment them at the groundtruth part count, then compute bounding boxes per segment for comparison. For OmniPart, we obtain masks from SAM Kirillov et al. (2023) and filter out very small masks with area ratio less than 1600/10242 to ensure meaningful part segmentation. Table 1: Quantitative results for bounding box generation (%)."
        },
        {
            "title": "Method",
            "content": "Voxel recall Voxel IoU Bbox IoU PartField Liu et al. (2025c) OmniPart (SAM Mask) Yang et al. (2025b) Part-X-MLLM (Ours) 69.65 68.33 74.11 46.04 43. 48.74 37.33 34.33 42.55 Qualitative Generation and Editing Results. Figure 4 visualizes our qualitative shape decomposition results, where our model demonstrates superior performance in generating semantically coherent and geometrically accurate part segmentations. It successfully captures fine-grained details and maintains structural integrity, outperforming baselines that often produce fragmented or inaccurate decompositions. We also evaluate the models ability to perform localized, language-driven edits. As shown in Figure 5, Part-X-MLLM successfully interprets user instructions to add, remove, or modify specific parts, executing the edits while preserving the rest of the objects structure. Figure 4: Qualitative shape decomposition results. Semantic Granularity Control. As introduced in Section 3, our framework supports controlling part granularity by semantically clustering bounding boxes. Figure 6 demonstrates this process, where our algorithm progressively merges components based on the CLIP similarity of their textual descriptions, reducing the part count from 22 down to 2. This automated process allows for flexible control over the level of detail without manual intervention."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 5: Qualitative results for part-aware editing. Our model successfully interprets natural language instructions to perform localized edits, while preserving the integrity of the original object. Figure 6: Semantic granularity control via part clustering. By clustering parts based on the semantic similarity of their descriptions, we can progressively merge fine-grained components into coarser structures. The number of components is automatically reduced from 22 to 2. Confidence-Aware Face Segmentation. Beyond generating structured plans for synthesis and editing, our models autoregressive outputs enable additional downstream applications. One such use case is fine-grained, confidence-aware face segmentation. As shown in Figure 7, by leveraging the generated bounding boxes and their associated token probabilities during decoding, we can achieve high-quality, face-level segmentation of 3D objects without any additional training. The detailed algorithm is provided in the appendix (Section A.4). Figure 7: Confidence-aware face segmentation. By leveraging the generated bounding boxes and their associated confidence scores, we can achieve high-quality, fine-grained face-level segmentation of 3D objects without any additional training."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Ablation Study: Dual vs. Single Encoder. We conduct an ablation study to validate our dualencoder design, which processes geometric structure and visual appearance in separate pathways. We compare our full model against single-encoder variant that consumes unified point cloud with fused geometry (XYZ) and color (RGB) information. As shown in Table 2, the dual-encoder architecture consistently outperforms the single-encoder baseline across all evaluated tasks. For pure geometric tasks like box listing, the dual encoder improves IoU by significant margin (+7.06). For language-intensive tasks such as Part QA and Multi-Part Grounding, we observe uniform gains across all metrics. This suggests that forcing single encoder to handle both structural and semantic information creates conflict, whereas decoupling these responsibilities into two specialized encoders is more effective and robust design choice. Table 2: Ablation study on the dual-encoder architecture. We compare our full model against single-encoder variant. All metrics are reported on UniPart-Bench. Task Model IoU SBERT SimCSE BLEU-1 ROUGE-L METEOR Pure Box Listing Multi-Part Grounding Part QA Dual Encoder (Ours) Single Encoder Gain Dual Encoder (Ours) Single Encoder Gain Dual Encoder (Ours) Single Encoder Gain 75.53 68.47 +7.06 72.82 69.78 +3.04 55.44 54.24 +1.20 - - - 55.60 54.18 +1. 78.98 78.44 +0.54 - - - 54.19 53.53 +0.66 84.25 83.13 +1.12 - - - 35.55 33.95 +1. 40.54 39.29 +1.25 - - - 35.58 33.97 +1.61 42.26 41.31 +0.95 - - - 18.09 17.27 +0. 34.24 33.06 +1.18 4.4 PART AND OBJECT UNDERSTANDING Part Understanding Q&A. To evaluate part-level understanding and reasoning, we test on UniPart-Bench. We report sentence-level similarities (SBERT, SimCSE) and token-level metrics (BLEU-1, ROUGE-L, METEOR). Results in Table 3 show consistent gains of our method on partlevel Q&A. We observe substantial gains over the strongest baseline across all metrics: compared to the best non-ours scores, Part-X-MLLM improves by +18.7 SBERT, +25.5 SimCSE, +21.3 BLEU1, +13.0 ROUGE-L, and +14.2 METEOR. These gains reflect stronger part-level grounding and reasoning enabled by our box grammar and instruction tuning. Table 3: Part understanding Q&A on UniPart-Bench. Model SBERT SimCSE BLEU-1 ROUGE-L METEOR GPT4Point Qi et al. (2024b) PointLLM-7B Xu et al. (2024) PointLLM-13B Xu et al. (2024) ShapeLLM-13B Qi et al. (2024a) ShapeLLM-Omni-7B Ye et al. (2025b) Part-X-MLLM (Ours) 48.32 61.30 56.36 61.19 57.35 78. 45.17 58.48 51.47 57.26 51.16 84.25 15.16 21.78 21.40 23.32 22.77 40.54 22.55 29.26 29.16 32.56 29.57 42. 16.19 22.45 21.80 24.45 23.24 34.24 Overall 3D Object Captioning. Unlike part-level captioning, this benchmark probes holistic object understanding on UniPart-Bench. We report SBERT, SimCSE, BLEU-1, ROUGE-L, and METEOR following PointLLM. On overall object captioning, our model also outperforms the best prior scores, with absolute improvements of +10.4 SBERT, +9.4 SimCSE, +18.8 BLEU-1, +20.2 ROUGE-L, and +15.2 METEOR. The large gains on token-based metrics suggest stronger lexical coverage and structure in object-level descriptions. Qualitative Understanding Results. Figure 8 provides qualitative examples for overall object captioning. Our model generates more accurate and detailed descriptions compared to baselines. For instance, our model correctly identifies an object as pink teddy bear mascot costume with purple bow tie, while other models provide less specific or incorrect descriptions. Additional"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Table 4: Overall 3D object captioning on UniPart-Bench. Model SBERT SimCSE BLEU-1 ROUGE-L METEOR GPT4Point Qi et al. (2024b) PointLLM-7B Xu et al. (2024) PointLLM-13B Xu et al. (2024) ShapeLLM-13B Qi et al. (2024a) ShapeLLM-Omni-7B Ye et al. (2025b) Part-X-MLLM (Ours) 25.60 42.79 43.51 25.15 31.18 53.82 27.00 42.44 43.12 27.14 31.93 51.97 11.50 11.58 13.54 11.77 17. 36.04 12.00 14.39 15.74 12.14 19.04 38.11 12.70 16.90 17.45 12.84 14.30 30.71 qualitative results for part-aware Q&A, demonstrating our models strong grounding capabilities, are provided in the appendix (Figure 9). Figure 8: Qualitative results for overall object captioning."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Part-X-MLLM casts 3D interaction as executable program generation: from RGB point clouds and text it emits single sequence of part AABBs that geometry engines execute, unifying generation, QA, and localized editing, and improving Voxel Recall/IoU and BBox IoU on UniPart-Bench. Appendix A.2.1 supports controllable granularity. Limitations. Longer sequences slow inference; simple compaction and hierarchical grouping mitigate latency. Our confidence-based segmentation from BBoxes remains relatively shallow; incorporating stronger features could improve segmentation quality. Fine-tuning on 3D tasks may reduce the base LLMs general language capabilities."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We gratefully acknowledge the invaluable feedback and support from Xianghui Yang, Jian Liu, Haohan Weng, and Biwen Lei. We also thank the Tencent Hunyuan 3D Team for their generous assistance and collaboration throughout this project."
        },
        {
            "title": "REFERENCES",
            "content": "Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, and Leonidas Guibas. Generic 3d diffusion adapter using controlled multi-view editing. arXiv preprint arXiv:2403.12032, 2024a. Minghao Chen, Junyu Xie, Iro Laina, and Andrea Vedaldi. Shap-editor: Instruction-guided latent 3d editing in seconds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2645626466, 2024b. Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 5881 5892, 2025a. Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: Autogressive 3d part generation and discovery. arXiv preprint arXiv:2507.13346, 2025b. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2642826438, 2024c. Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, and Guosheng Lin. Ultra3d: Efficient and high-fidelity 3d generation with part attention. arXiv preprint arXiv:2507.17745, 2025c. Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, and Ian Reid. 3d-llava: Towards generalist 3d lmms with omni superpoint transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 37723782, 2025. Ziya Erkoc, Can Gumeli, Chaoyang Wang, Matthias Nießner, Angela Dai, Peter Wonka, Hsin-Ying Lee, and Peiye Zhuang. Preditor3d: Fast and precise 3d shape editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 640649, 2025. Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024. Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. In Proceedings of the IEEE/CVF inInstruct-nerf2nerf: Editing 3d scenes with instructions. ternational conference on computer vision, pp. 1974019750, 2023. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3d scene and large language models with object identifiers. Advances in Neural Information Processing Systems, 37:113991 114017, 2024. Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, and Lu Sheng. Voxhammer: Training-free precise and coherent 3d editing in native 3d space. arXiv preprint arXiv:2508.19247, 2025a. Songlin Li, Despoina Paschalidou, and Leonidas Guibas. Pasta: Controllable part-aware shape generation with autoregressive transformers. arXiv preprint arXiv:2407.13677, 2024a. Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman3d: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024b. Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-driven 3d editing via focal-fusion assembly. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 32793287, 2024c. Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025b. Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH 2024 Conference Papers, pp. 112, 2024a. Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3d: Fast and consistent subject-driven 3d content generation. In European Conference on Computer Vision, pp. 389406. Springer, 2024b. Fangfu Liu, Hao Li, Jiawei Chi, Hanyang Wang, Minghui Yang, Fudong Wang, and Yueqi Duan. Langscene-x: Reconstruct generalizable 3d language-embedded scenes with trimap video diffusion. arXiv preprint arXiv:2507.02813, 2025a. Fangfu Liu, Junliang Ye, Yikai Wang, Hanyang Wang, Zhengyi Wang, Jun Zhu, and Yueqi Duan. Dreamreward-x: Boosting high-quality 3d generation with human preference alignment. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 114, 2025b. doi: 10.1109/ TPAMI.2025.3609680. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025c. Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. Qiaowei Miao, Kehan Li, Jinsheng Quan, Zhiyuan Min, Shaojie Ma, Yichao Xu, Yi Yang, Ping Liu, and Yawei Luo. Advances in 4d generation: survey, 2025. URL https://arxiv.org/ abs/2503.14501. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In European Conference on Computer Vision, pp. 214238. Springer, 2024a. Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: unified framework for point-language understanding and generation. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 26417 26427, 2024b."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel In Proceedings of the IEEE/CVF international conference on computer editing of 3d objects. vision, pp. 430440, 2023. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Dataefficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023. Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui arXiv preprint Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv:2411.09595, 2024. Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025a. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation made easy with spatial sparse attention. arXiv preprint arXiv:2505.17412, 2025b. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: In European Conference on Empowering large language models to understand point clouds. Computer Vision, pp. 131147. Springer, 2024. Han Yan, Yang Li, Zhennan Wu, Shenzhou Chen, Weixuan Sun, Taizhang Shang, Weizhe Liu, Tian Chen, Xiaqiang Dai, Chao Ma, et al. Frankenstein: Generating semantic-compositional 3d scenes in one tri-plane. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024a. Han Yan, Mingrui Zhang, Yang Li, Chao Ma, and Pan Ji. Phycage: Physically plausible compositional 3d asset generation from single image. arXiv preprint arXiv:2411.18548, 2024b. Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, YanPei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025a. Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025b. Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 3:2, 2025a."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to-3d generation with human preference, 2024. URL https: //arxiv.org/abs/2403.14613. Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, and Jun Zhu. Shapellm-omni: native multimodal llm for 3d generation and understanding. arXiv preprint arXiv:2506.01853, 2025b. Junliang Ye, Shenghao Xie, Ruowen Zhao, Zhengyi Wang, Hongyu Yan, Wenqiang Zu, Lei Ma, and Jun Zhu. Nano3d: training-free approach for efficient 3d editing without masks, 2025c. URL https://arxiv.org/abs/2510.15019. Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: In Proceedings of the Pre-training 3d point cloud transformers with masked point modeling. IEEE/CVF conference on computer vision and pattern recognition, pp. 1931319322, 2022. Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. Bang: Dividing 3d assets via generative exploded dynamics. ACM Transactions on Graphics (TOG), 44 (4):121, 2025a. Yunzhi Zhang, Zizhang Li, Matt Zhou, Shangzhe Wu, and Jiajun Wu. The scene language: Representing scenes with programs, words, and embeddings, 2025b. URL https://arxiv.org/ abs/2410.16770. Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. Deepmesh: Auto-regressive artist-mesh creation with reinforcement learning, 2025a. URL https://arxiv.org/abs/2503.15265. Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, and Ying Shan. Assembler: Scalable 3d part assembly via anchor point diffusion. arXiv preprint arXiv:2506.17074, 2025b. Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:73969 73982, 2023. Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025c. Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 89959006, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THE USE OF LARGE LANGUAGE MODELS (LLMS) Large Language Models (LLMs) are used exclusively for minor language editingsuch as improving grammar and readabilityand not for method design or experimental work. All technical contributions, including the methodology, equations, and results, are solely the work of the authors. A.2 MORE EXPERIMENTAL RESULTS A.2.1 SEMANTIC PART CLUSTERING ALGORITHM To enable dynamic control over semantic granularity, we introduce post-processing algorithm that clusters fine-grained part bounding boxes into coarser, semantically meaningful components. This process, illustrated in Figure 6, operates without requiring manual intervention or predefined number of target clusters. The algorithm follows three-step pipeline: feature extraction, clustering, and merging. 1. Feature Extraction. For each predicted part pi, we extract its bounding box bi = (xmin, xmax)i and textual description di. hybrid feature vector fi is then generated. First, the semantic feature vector fsem,i is obtained by encoding the description with pretrained CLIP model: fsem,i = CLIP-Encode(di). (1) Next, we compute the spatial feature vector fspat,i from the bounding boxs center ci = (xmin + xmax)/2 and size si = xmax xmin. The raw spatial vector is normalized across all parts in the object to produce ˆfspat,i: fspat,i = [ci, si], ˆfspat,i = Normalize({fspat,j}N j=1)i. (2) Finally, the semantic and spatial features are combined using weighting factor α [0, 1], and the resulting vector is L2-normalized: fi = (1 α)fsem,i αˆfspat,i (1 α)fsem,i αˆfspat,i2 , (3) where denotes concatenation. 2. Clustering. We apply DBSCAN to the set of feature vectors {fi}N i=1. DBSCAN groups points based on two parameters: distance threshold ϵ and minimum number of points minPts. point fi is core point if its ϵ-neighborhood contains at least minPts points. cluster is formed by set of density-connected points, starting from core point and recursively expanding to all reachable neighbors. This approach allows us to automatically identify variable number of clusters without prior specification, returning set of clusters = {C1, . . . , CK} and set of noise points . 3. Merging. For each cluster Ck C, we compute single merged bounding box Bk = (Xmin,k, Xmax,k). This is done by taking the component-wise minimum and maximum over all bounding boxes bi Ck: Xmin,k = min ibiCk (xmin,i), Xmax,k = max ibiCk (xmax,i). (4) The final output is set of merged bounding boxes, representing coarser, semantically-grouped decomposition of the object. This automated approach provides flexible and powerful way to adjust the granularity of the generated 3D assets, bridging the gap between fine-grained part generation and high-level semantic understanding."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "A.3 ADDITIONAL QUALITATIVE RESULTS Figure 9 provides qualitative examples for part-aware question answering. Our model demonstrates strong grounding capabilities by providing detailed, box-annotated answers that accurately describe object parts in response to user queries. Figure 9: Qualitative results for part-aware Q&A. Our model provides more accurate and descriptive answers, with precise part grounding indicated by bounding box tokens. A.4 CONFIDENCE-AWARE FACE SEGMENTATION FROM BOUNDING BOXES As mentioned in Section 3, the rich information encoded in our models autoregressive output can be leveraged for advanced downstream tasks beyond simple generation or editing. One such application is fine-grained, confidence-aware face segmentation, as shown in Figure 7. This process requires no additional training and relies solely on the generated bounding boxes and the token probabilities from the decoding process. The algorithm follows three-step process: 1. Confidence-Aware BBox Inference. During autoregressive decoding, the model generates sequence of tokens = (t1, t2, . . . , tL) that represent series of bounding boxes. For each token ti, the model also outputs probability distribution over the entire vocabulary, from which we derive confidence score. The confidence of bounding box Bj, which is composed of sequence of tokens (typically 6), is calculated as the arithmetic mean of the probabilities of its constituent tokens: Conf(Bj) = 1 (cid:88) i=1 (tit<i) (5) This provides per-box confidence score that reflects the models certainty in its prediction. 2. Face-to-Box Assignment. Given mesh with set of faces = {f1, f2, . . . , fM } and set of inferred bounding boxes = {B1, B2, . . . , BN }, we first determine which faces belong to which boxes. face fm is considered candidate for Bj if its centroid cm lies within the volume of Bj: cm Bj (cm xmin,j) (cm xmax,j) (6) where xmin,j and xmax,j are the minimum and maximum coordinates of box Bj, and the comparison is element-wise."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "3. Conflict Resolution. faces centroid may lie within multiple overlapping bounding boxes, creating an ambiguity. We resolve this using two-tiered rule system: Containment Rule: If face fm is candidate for two boxes, Bi and Bj, and one box is strictly contained within the other (e.g., Bi Bj), the face is assigned to the box with the smallest volume. This prioritizes more specific, fine-grained predictions. Confidence Rule: If the boxes overlap but neither contains the other, the face is assigned to the box with the highest confidence score, Conf(Bj). This leverages the models own uncertainty estimate to make the most likely assignment. This process results in deterministic assignment of each face to single bounding box, producing high-quality, fine-grained segmentation of the object, as shown in Figure 7. A.5 ANALYSIS OF SPECIAL TOKEN EMBEDDINGS To better understand how our model interprets the specialized grammar, we visualize the embeddings of our newly added special tokens using t-SNE, as shown in Figure 10. The visualization reveals highly structured and semantically meaningful latent space. Figure 10: t-SNE visualization of special token embeddings. The tokens form distinct, wellstructured clusters based on their function, indicating meaningful learned representation. We observe three key phenomena. First, the tokens form distinct clusters based on their function: Point, Box, and Edit tokens occupy separate regions of the embedding space. Second, the 128 box tokens, which represent quantized coordinates, form continuous, ordered manifold. This demonstrates that the model has learned the ordinal nature of spatial coordinates rather than treating them as independent categorical variables. Third, tokens with similar functions, such as the start/end pairs for edits (e.g., <adds>/<adde>), are positioned closely together. This structured organization confirms that the model has successfully learned robust and interpretable representation of our executable grammar, which is crucial for precise, language-driven 3D planning. A.6 DATASET CONSTRUCTION AND LABELING Scope. We build high-quality, part-centric dataset tailored for Part-X-MLLM. The corpus contains 85,771 unique 3D objects with an average of 23 parts per object. Each part is annotated"
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Figure 11: Model-assisted labeling pipeline. Left: inputs (full-asset + per-part crops). Middle: structured tool schema drives the LMM to output object-level and part-level JSON. Right: validated JSON is stored and used by the data builder. with an axis-aligned bounding box (AABB) and two levels of text: coarse name (Q1) and finegrained description (Q2). At the object level, we include concise overall caption and small set of instructionanswer pairs for part-aware Q&A. All annotations are serialized using the unified box-token grammar described in Section 3. A.6.1 MODEL-ASSISTED LABELING To scale high-quality labels consistently, we adopt model-assisted pipeline guided by structured tool schema. Given full-asset render and sequence of part close-ups, we collect: Q1: short part name. Q2: fine-grained natural description ( 15 words; avoid irrelevant rendering terms). Q3: confidence flag (Yes/No). Concretely, we follow the schema implemented in our labeling tool, which calls an external LMM with JSON response format and deterministic field ordering. For each object, we provide: (1) one full-asset image (front view); and (2) part crops (one per part). A.6.2 BUILDING INSTRUCTION-FOLLOWING SAMPLES We convert raw labels into diverse instruction-following pairs covering grounding, captioning, QA, and editing. central convenience is box-token grammar with opening/closing tokens <boxs> and <boxe> wrapping six quantized coordinates, and edit verbs <adds>/<adde>, <dels>/<dele>, and <mods>/<mode>. Quantization and serialization. Each coordinate [1, 1] is quantized into = 128 bins as 2 (K 1)(cid:1) , q(x) = round(cid:0) x+1 = 2 q(x) K1 1, (7) then serialized as six tokens inside <boxs>...<boxe>. For reproducibility, parts in list are deterministically ordered by (q(zmin), q(ymin), q(xmin)). Task families. We instantiate eleven templates (Types 010): Type 0: pure box listing from point cloud (detect all bounding boxes). Type 1: multi-part grounding with coarse text (AABBs + Q1 per part). Type 2: multi-part grounding with fine text (overall description first, then AABBs + Q2). Type 3: single-part grounding from coarse text (locate all Q1 parts; return AABBs + description). Type 4: single-part grounding from fine text (locate part by Q2; return single AABB). Type 5: box-to-text (given box, answer Q1). Type 6: box-to-text (given box, answer Q2). Type 7: part-aware QA (replace textual part references <Part i> with the corresponding box tokens in answers). Type 8: deletion program (emit <dels> [boxes] <dele>). Type 9: modification program (emit <mods> [box] new text <mode>). Type 10: addition program (emit <adds> [box] text <adde>). Train/test split and balancing. We partition the file list deterministically at 0.5% for test and 99.5% for train. Templates 12 are lightly duplicated to increase multi-part coverage; for templates 37 we downsample to fixed budget; for edit templates (810) we cap the number per shard. See Table 5."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Serialize each part AABB to tokens; sort by (zmin, ymin, xmin) for each template {0, . . . , 10} do Algorithm 1 Data building (simplified) 1: Load datas 2: for each object do 3: 4: 5: 6: 7: 8: Shuffle and save shards; optionally balance per-template counts Instantiate natural-language prompt from template pool Emit the target sequence (boxes, text, or edit program) Append conversation pair to the corpus Table 5: Task families and sizes. Raw denotes counts before optional balancing; Final denotes the target budget after balancing. Name Input Output Raw % Type Final Single-Part Grounding Single-Part Grounding Multi-Part Grounding Multi-Part Grounding Box-to-Text (coarse) Box-to-Text (fine) Part QA EditAdd EditRemove EditReplace Pure box listing Total 1 box + fine text 1 box all boxes + Q1 all boxes + Q2 point + coarse text point + fine text point + text point + text point + box + text Q1 point + box + text Q2 text point + text program (box + text) point + text program (boxes) point + text program (box + text) point + text all boxes point + text 506,755 887,590 85,771 85,771 887,590 887,590 577,369 247,998 1,394,345 883,941 500,000 7.30 T3 12.78 T4 1.24 T1 1.24 T2 12.78 T5 12.78 T6 8.31 T7 3.57 T10 20.08 T8 12.73 T9 7.20 T0 506,755 506,755 257,313 257,313 506,755 506,755 506,755 247,998 247,998 247,998 500, 6,944,720 100.00 4,292,395 A.7 DATASET STATISTICS Task families and sizes. Table 5 summarizes per-task counts before/after balancing. Counts follow our build scripts. Category distribution. Our corpus spans everyday objects and scenes. Table 6 lists the main categories (top-12 by frequency). A.8 COMPREHENSIVE RESULTS ON UNIPART-BENCH We report per-task results on UniPart-Bench. Note that UniPart-Bench is held-out subset of our 85,771-object training dataset, ensuring identical data construction pipeline and distribution characteristics. Following our data construction, each ground-truth (GT) item may contain both BBox tokens and text. When both are present, we evaluate BBoxes with IoU and text with SBERT/SimCSE/BLEU-1/ROUGE-L/METEOR. If GT contains only BBoxes or only text, we evaluate the available modality and leave the other columns blank. Table 7 summarizes results for Tasks 010 while mapping each task to its template Type and name as in Table 5. Discussion. Language-intensive tasks (T7 Part QA, T10 EditAdd) obtain the highest SBERT/SimCSE and strong lexical metrics, indicating robust alignment between our planned box-conditioned answers/programs and textual GT. Among IoU-based tasks, T0/T2/T10 show the strongest geometric alignment, reflecting reliable planning for pure detection, fine grounding, and edit addition respectively. Blank text or IoU entries arise by design when tasks GT lacks the corresponding modality. A.9 PROMPT TEMPLATES FOR DATA CONSTRUCTION To ensure the reproducibility of our dataset construction, this section provides the complete set of English prompt templates used to generate the instruction-following samples for each of the 11 task types, as described in Section A.6.2. These templates are presented in the tables below."
        },
        {
            "title": "Tencent Hunyuan",
            "content": "Table 6: Category distribution (top-19)."
        },
        {
            "title": "Count",
            "content": "Share (%)"
        },
        {
            "title": "Food",
            "content": "20,426 7,139 7,010 6,909 6,730 6,582 6,406 5,996 5,995 5,885 5,183 1,774 23.74 8.30 8.15 8.03 7.82 7.65 7.45 6.97 6.97 6.84 6.02 2.06 Table 7: All-task results on UniPart-Bench benchmark. Type/Name follows the template definitions in Table 5. Blank entries indicate that the GT for that task does not contain the corresponding modality. Task Type Name IoU SBERT SimCSE BLEU-1 ROUGE-L METEOR 0 T0 1 T1 2 T2 3 T3 4 T4 5 T5 6 T6 7 T7 8 T8 9 T9 10 T10 Pure box listing Multi-Part Grounding (Q1) Multi-Part Grounding (Q2) Single-Part Grounding (Q1) Single-Part Grounding (Q2) Box-to-Text (Q1) Box-to-Text (Q2) Part QA EditRemove (program) EditReplace (program) EditAdd (program) 0.755 0.728 0.736 0.528 0.443 0.554 0.473 0.409 0.700 55.60 63.68 73.28 57.35 64.64 78. 54.19 60.68 71.70 56.49 61.96 84.25 35.55 31.01 36.29 38.12 31.35 40.54 35.58 33.68 38.94 38.14 33.73 42. 18.09 27.72 33.21 19.49 28.13 34.24 80.38 79.71 47.62 51. 46.63 A.9.1 TYPE 0: PURE BOX LISTING ID Prompt Template 1 \"Detect all bounding boxes in this point cloud\" 2 \"Show me all the bounding boxes\" 3 \"Generate bounding boxes for all objects\" 4 \"Find all object boundaries\" 5 \"Extract all bounding boxes from this scene\" 6 \"Locate all object bounding boxes\" 7 \"Output all detected bounding boxes\" 8 \"Provide bounding boxes for all components\" 9 \"Identify all object boundaries in this model\" 10 \"Return all bounding box coordinates\" 11 \"Detect and output all object boxes\" 12 \"Find all rectangular boundaries\" 13 \"Generate all object bounding boxes\" 14 \"Show all detection boxes\" 15 \"Output bounding box coordinates for all objects\" 16 \"Detect all objects and return their boxes\" 17 \"Find every bounding box in this point cloud\" 18 \"Extract object boundaries from this 3D data\" 19 \"Provide all object detection boxes\" 20 \"Return coordinates of all detected objects\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "A.9.2 TYPE 1: MULTI-PART GROUNDING (COARSE TEXT) ID Prompt Template 1 \"What distinct components does this contain? Please annotate with bounding boxes and provide short labels\" 2 \"What functional parts make up this object? provide 6 box-tokens then write the name\" 3 \"What structural elements can be decomposed? First Output in the specified format\" 4 \"What key components does this have? Please locate and name them\" 5 \"What identifiable parts are there? Mark with AABB tokens\" 6 \"What construction units can be distinguished? Please list them\" 7 \"What parts need to be annotated in this?\" 8 \"What basic components does this contain? Please output bounding box + label\" 9 \"What main parts is this composed of? Please enumerate using token format\" 10 \"What recognizable sub-parts are there? Use the specified format for output\" 11 \"Which distinct parts exist here? box-tokens and short labels\" Provide 12 \"Identify every component and prepend its 6 quantized box tokens\" 13 \"List all separable elements; each line starts with tokens\" 14 \"Locate and name each part of the object\" 15 \"Enumerate all components with their bounding-box tokens\" 16 \"Break the shape into parts, output AABB tokens then concise tag\" 17 \"Mark every structural unit. Format: tokens followed by NAME\" 18 \"Point out all functional pieces and give their tokenized boxes\" 19 \"Provide the set of parts and their six token indices\" 20 \"Give every recognized section together with its AABB tokens\" 21 \"List all structural elements using 6 box-tokens + name format\" 22 \"Return the quantized bounding box and short name for each part\" 23 \"Please enumerate in the format of tokens followed by NAME\" 24 \"Output part AABB (tokens) and their names\" 25 \"Give the list of components together with their quantized boxes\" 26 \"Return each element as six tokens followed by short label\" 27 \"Provide AABB tokens plus name for every distinguishable component\" 28 \"Enumerate all parts with their bounding-box tokens and brief tag\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 29 \"Please identify all parts and output bounding box tokens + short name\" 30 \"After completion, only return the parts list without extra explanation\" 31 \"Output strictly according to the specified format, no additional text\" 32 \"No extra description at the end, only list the parts\" 33 \"List the token AABB and name for each part\" 34 \"Give tokens and labels in order of appearance\" 35 \"Use six tokens followed by space and name\" 36 \"Example line: tokens label, please output according to this example\" 37 \"Return all components and their quantized coordinate indices\" A.9.3 TYPE 2: MULTI-PART GROUNDING (FINE TEXT) ID Prompt Template 1 \"Please describe the overall appearance of this point cloud in detail, then introduce each part one by one (with AABB tokens)\" 2 \"First give an overall impression, then explain each part in turn with bounding box tokens\" 3 \"Please provide an overview of this model, and describe each component with tokens\" 4 \"What is the overall shape like? What are the materials and functions of each part?\" 5 \"Please first introduce the complete structure, then list parts with tokens + detailed explanations\" 6 \"From this point cloud, give an overall description then detail each part with its bounding box\" 7 \"Describe the complete object, followed by part-wise details using quantized tokens\" 8 \"Provide holistic view and then list all elements with 6 box tokens and properties\" 9 \"Summarize the scene, then output each component in the required token format\" 10 \"Give full description first, then annotate every part with its box tokens and long caption\" 11 \"Please first present the overall features, then elaborate on each functional component\" 12 \"After summarizing the appearance, list each part item by item (format: tokens description)\" 13 \"Give the global appearance, then each part line starts with 6 tokens\" 14 \"Present the overall structure and afterwards the detailed attributes of all components\" 15 \"Explain the general design; afterwards specify each element with its tokens and features\" 16 \"First output an overall description, then write detailed explanation for each part with tokens\" 17 \"Describe holistically, then provide component-wise explanations with bounding-box indices\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 18 \"Begin with the object overview; subsequently list parts and their detailed properties\" 19 \"Offer complete summary and then enumerate parts with tokenized boxes\" 20 \"Return the overall description and AABB + detailed explanation for each part\" 21 \"Finally, please list all components and their features in the specified format\" 22 \"Please output in the format of overall description tokens description\" 23 \"Provide each part in turn (including token bounding box and function/material description)\" 24 \"Provide the overall description followed by every part in the required tokenized box format\" 25 \"Please finish by listing each components six box tokens and an informative sentence\" 26 \"Return first the global description, then each element as tokens LONG DESCRIPTION\" 27 \"Include holistic summary, then annotate each part with its quantized AABB and details\" 28 \"Conclude with the part-wise list using bounding-box tokens plus their detailed attributes\" 29 \"Output the parts list, each line starting with tokens\" 30 \"Please output the description of this object or scene and its parts BBox information, overall first then parts, format and order cannot be changed\" 31 \"End by outputting all parts and their respective detailed features\" 32 \"Summary first, then component lines with tokens and descriptions\" 33 \"Output strictly in two sections: overview + per-part details\" 34 \"After the overview, enumerate every part with its quantized box tokens\" 35 \"Overall + parts format example: tokens The left handle is ...\" A.9.4 TYPE 3: SINGLE-PART GROUNDING (FROM COARSE TEXT) ID Prompt Template 1 \"Find the {part name} in this model\" 2 \"Locate the {part name} in this model\" 3 \"Point out the {part name} in this point cloud\" 4 \"Mark the {part name} in this object\" 5 \"Where is the {part name} in this 3D model?\" 6 \"Identify the {part name} in this point cloud\" 7 \"Please show all {part name} in this object\" 8 \"Where is the position of {part name} in this scene?\" 9 \"Locate the {part name} in this model\" 10 \"Find the {part name} in this point cloud\" 11 \"Point out the {part name} in this object\" 12 \"Where is the {part name} in this 3D shape?\" 13 \"Mark the {part name} in this model\" 14 \"Show all {part name} in this object\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 15 \"Identify the {part name} in this point cloud\" 16 \"Highlight the position of {part name}\" A.9.5 TYPE 4: SINGLE-PART GROUNDING (FROM FINE TEXT) ID Prompt Template 1 \"Where is the part corresponding to this description: {part description}\" 2 \"Help me locate this part: 3 \"Find the corresponding part based on this {part description}\" description: {part description}\" 4 \"In this point cloud, which part does {part description} refer to?\" 5 \"Mark the position of this part: 6 \"Please provide the bounding box for the {part description}\" part corresponding to this description: {part description}\" 7 \"Find the part that matches this description: {part description}\" 8 \"Locate the component described as: {part description}\" 9 \"Which part is this referring to: {part description}\" 10 \"Mark the boundary of: 11 \"Show the box coordinates for: 12 \"Provide the bounding box for this described {part description}\" {part description}\" element: {part description}\" 13 \"Where exactly is: 14 \"Given this description, locate the corresponding {part description}\" part: {part description}\" 15 \"Locate the part based on this text and provide its AABB: {part description}\" 16 \"Which specific part does this description correspond to? {part description}\" A.9.6 TYPE 5: BOX-TO-TEXT (COARSE) ID Prompt Template 1 \"What is this part?\" 2 \"What is this marked area?\" 3 \"What is contained in this box?\" 4 \"What is this marked portion called?\" 5 \"What part is inside this bounding box?\" 6 \"What is this part called?\" 7 \"Name this highlighted component\" 8 \"What is contained in this bounding box?\" 9 \"Identify this marked region\" 10 \"Give the name of this part\" 11 \"What is inside this AABB box?\" 12 \"Name this area with one word\" 13 \"Whats the simple label for this bounded area?\" 14 \"What would you call this boxed element?\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 15 \"What part does this bounding box point to? Please answer briefly\" 16 \"What is this outlined section?\" 17 \"Provide the name for this demarcated part\" A.9.7 TYPE 6: BOX-TO-TEXT (FINE) ID Prompt Template 1 \"Describe this part in detail\" 2 \"What does this area contain? Please explain in detail\" 3 \"Please describe the part within this bounding box, including appearance, material and function\" 4 \"What is in this box? Please provide detailed information\" 5 \"What is the marked portion? Please provide complete description\" 6 \"Describe this part in detail\" 7 \"What can you tell me about this highlighted component?\" 8 \"Provide comprehensive description of whats in this box\" 9 \"Explain the appearance, material and function of this marked area\" 10 \"Give details about this bounded region\" 11 \"What are the characteristics of this marked area? Please describe comprehensively\" 12 \"Elaborate on the appearance and purpose of this part\" 13 \"What is contained in this bounding box? Elaborate on its features\" 14 \"Tell me everything about this outlined element\" 15 \"What is the material, shape and function of the object in this box?\" 16 \"Please characterize this demarcated component thoroughly\" 17 \"Whats inside this box? Include all relevant details\" A.9.8 TYPE 7: PART-AWARE Q&A This task reuses the questions from the QA field in the raw annotations and replaces textual part references with box tokens in the answer. No new templates are generated for the questions themselves. A.9.9 TYPE 8: DELETION PROGRAM ID Prompt Template By part name 1 \"Please remove the {part name} from this object\" 2 \"Get rid of every {part name}\" 3 \"I want to delete the {part name} here\" 4 \"Can you erase all instances of the {part name}?\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 5 \"Show me this model but without the {part name}\" 6 \"Take out the {part name}\" 7 \"The {part name} needs to be removed\" 8 \"Omit the {part name} from this scene\" 9 \"I dont want to see the {part name} anymore\" 10 \"Could you proceed with deleting the {part name}?\" 11 \"Lets see what it looks like if we remove the {part name}\" 12 \"Exclude the {part name} from the final output\" 13 \"The task is to get rid of the {part name}\" 14 \"Wipe out the {part name} from the 3D model\" 15 \"Please filter out the {part name}\" 16 \"Delete the component identified as {part name}\" 17 \"I require the removal of the {part name}\" 18 \"Make the {part name} disappear\" 19 \"This model would be better without the {part name}\" 20 \"Execute the deletion of the {part name}\" By part description 21 \"Please remove this specific part: {part description} 22 \"I dont want the component described as {part description}\" 23 \"Delete the part that is {part description}\" 24 \"Get rid of this particular element: {part description}\" 25 \"Find the part matching {part description} and remove it\" 26 \"The element characterized by {part description} should be deleted\" 27 \"Erase the component with this description: {part description}\" 28 \"I want to exclude the part that is {part description}\" 29 \"Locate and then delete this item: {part description}\" 30 \"Take out the part that looks like this: {part description}\" 31 \"The target for deletion is the part described as: {part description}\" 32 \"Can you remove the part with these features: {part description}\" 33 \"Please omit this from the model: {part description}\" 34 \"Based on the description {part description}, remove the corresponding part\" 35 \"Ive identified part to remove: {part description}\" 36 \"Wipe the following item from the scene: {part description}\" 37 \"The part to be erased is: 38 \"Remove the object that fits this profile: {part description}\" {part description}\" 39 \"Please execute deletion on the component identified as {part description}\" 40 \"Lets remove one specific part: {part description}\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "A.9.10 TYPE 9: MODIFICATION PROGRAM ID Prompt Template 1 \"Please edit the {part name} to be {new description}\" 2 \"Change the {part name} into {new description}\" 3 \"Replace the {part name} with this: {new description}\" 4 \"I want the {part name} to look like this: {new description}\" 5 \"Modify the {part name} to become {new description}\" 6 \"Update the {part name} so it is now {new description}\" 7 \"Lets alter the {part name}. It should be {new description}\" 8 \"Transform the {part name} into {new description}\" 9 \"Could you make the {part name} to be {new description}\" 10 \"My instruction is to change the {part name} to {new description}\" 11 \"The {part name} needs an update. details: {new description}\" Here are the new 12 \"Lets swap the current {part name} with new one: {new description}\" 13 \"The {part name} should be revised to be {new description}\" 14 \"Please perform an edit on the {part name}. It should now be {new description}\" 15 \"Adjust the {part name} to match this description: {new description}\" A.9.11 TYPE 10: ADDITION PROGRAM ID Prompt Template 1 \"Add the {part name} to this 3D asset.\" 2 \"Please add {part name} to the model.\" 3 \"Insert the {part name} component.\" 4 \"Attach the {part name} to this object.\" 5 \"Place the {part name} on this model.\" 6 \"Include the {part name} in this design.\" 7 \"Incorporate the {part name} into this structure.\" 8 \"This model is missing its {part name}. Please add it.\" Add it back.\" 9 \"Complete this 3D model by adding the {part name}.\" 10 \"The {part name} is missing. 11 \"Restore the {part name} to this object.\" 12 \"Fill in the missing {part name}.\" 13 \"This asset needs {part name}. 14 \"Enhance this model with {part name}.\" 15 \"Improve this design by adding the {part name}.\" 16 \"Augment this object with the {part name}.\" 17 \"Extend this model to include the {part name}.\" 18 \"Could you add the {part name} to complete this Add it.\" model?\" 19 \"I need you to add the {part name} to this 3D object.\" 20 \"Would you please attach the {part name}?\""
        },
        {
            "title": "Tencent Hunyuan",
            "content": "ID Prompt Template 21 \"Can you help me add the {part name} component?\" 22 \"Mount the {part name} in the appropriate position.\" 23 \"Install the {part name} where it belongs.\" 24 \"Position the {part name} correctly on this model.\" 25 \"Generate and add the {part name} to this asset.\" 26 \"Create the {part name} component for this model.\" 27 \"Design and attach the {part name}.\" 28 \"This looks incomplete without the {part name}. Add it.\" 29 \"To make this functional, add the {part name}.\" 30 \"The model requires {part name} to be complete.\" Part-specific templates Add the head.\" 31 \"Add the head section to complete this figure.\" 32 \"This model needs its head. Please attach it.\" 33 \"The top part is missing. 34 \"Install the wheels to make this vehicle complete.\" 35 \"Add wheels for mobility.\" 36 \"Mount the wheels on this vehicle.\" 37 \"Install the door to complete the entrance.\" 38 \"Add door for access.\" 39 \"Place the door in the opening.\" 40 \"Attach the handle for better grip.\" 41 \"Add the handle component.\" 42 \"Install the handle mechanism.\" 43 \"Add the legs to support this structure.\" 44 \"Attach the leg components.\" 45 \"Install the supporting legs.\" 46 \"Add wings to complete this model.\" 47 \"Attach the wing components.\" 48 \"Install the wings on both sides.\""
        }
    ],
    "affiliations": [
        "Tencent Hunyuan",
        "The University of Hong Kong",
        "Tsinghua University",
        "Zhejiang University"
    ]
}