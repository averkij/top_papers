{
    "paper_title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models",
    "authors": [
        "Aditya Tanna",
        "Pratinav Seth",
        "Mohamed Bouadi",
        "Utsav Avaiya",
        "Vinay Kumar Sankarapu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 3 2 0 8 2 0 . 1 1 5 2 : r Aditya Tanna, Pratinav Seth, Mohamed Bouadi Utsav Avaiya, Vinay Kumar Sankarapu Lexsi Labs, India & France Abstract Tabular foundation models represent growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TABTUNE, unified library that standardizes the complete workflow for tabular foundation models through single interface. TabTune provides consistent access to multiple state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune . Keywords: Tabular Foundation Models, Meta-Learning, PEFT, Calibration, Fairness, Benchmarking."
        },
        {
            "title": "1 Introduction",
            "content": "Tabular data i.e. structured collections of records (rows) and attributes (columns)underpins vast array of real-world applications, from healthcare and finance to industrial analytics and scientific research. These datasets form the backbone of modern decision-making systems: hospitals rely on patient records for diagnostic risk prediction, financial institutions assess creditworthiness and fraud using structured features, and scientific studies aggregate multi-sensor measurements to discover new insights. Yet despite this ubiquity, deep learning on tabular data has long lagged behind the breakthroughs achieved in domains such as natural language processing and computer vision. Traditional approaches like gradient-boosted decision trees (e.g., XGBoost [1], LightGBM [2], CatBoost [3]) remain dominant because they reliably handle heterogeneous features, variable sample sizes, and small datasetschallenges that standard neural networks often struggle with. Deep models frequently underperform on tabular benchmarks due to the lack of spatial or temporal structure, making feature extraction less amenable to convolutional or sequential architectures. Moreover, tabular datasets often exhibit high heterogeneitymixing numerical, categorical, and ordinal variablesand suffer from missing values and skewed distributions, which violate many of the inductive biases that make deep learning successful in other modalities. As result, practitioners continue to favor tree ensembles and gradient boosting for their robustness, interpretability, and low data requirements. Recently, the advent of tabular foundation models (TFMs) has begun to reshape this landscape. These models extend the foundation model paradigmpretraining on large, diverse corpora followed by downstream adaptationto structured data domains. Models such as TABPFN [4, 5, 6], TABICL [7], CONTEXTTAB [8], ORIONMSP [9], and ORIONBIX [10] leverage large-scale pre-training on synthetic or massive tabular datasets to provide generic, TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models out-of-the-box learning capabilities. These developments mark significant step toward general-purpose tabular learners that can adapt rapidly to new datasets with minimal supervision. However, despite their promise, adopting these models in real-world practice remains hampered by several frictions: DIVERSE PREPROCESSING REQUIREMENTS: Each model demands its own data encoding, feature normalization, and missing-value strategyrequiring practitioners to build bespoke pipelines for each model family. For example, TabPFN expects numerically encoded categorical features consistent with its synthetic priors, whereas TabICL and Orion architectures rely on learned embeddings or set-transformer encoders for categorical attributes. FRAGMENTED APIS AND TRAINING PROTOCOLS: Some models operate purely in zero-shot inference mode (zero fine-tuning), others support supervised fine-tuning (SFT) or parameter-efficient fine-tuning (PEFT), specifically Low-Rank Adaptation (LoRA; see Section 2 for details). Managing these disparate workflows is laborious and error-prone, particularly when comparing multiple model families under consistent experimental settings. EVALUATION GAPS IN DEPLOYMENT-RELEVANT METRICS: While accuracy improvements are well documented, aspects such as calibration of probability estimates, fairness across subgroups, and resource-efficiency trade-offs are under-explored in unified framework. This absence of standardization makes it difficult to assess whether performance gains translate to trustworthy or deployable behavior. MODEL SELECTION COMPLEXITY: With multiple models and tuning strategies available, users face nontrivial questions: Which model best suits small vs. large dataset? What is the memory/latency trade-off? How do calibration and fairness behave under different tuning regimes? In practice, this uncertainty often discourages the adoption of TFMs despite their potential benefits. To bridge these gaps, we introduce TABTUNE, unified, scikit-learn-compatible toolkit that standardizes the full modelling workflow for tabular foundation models. TabTune offers: SINGLE INTERFACE for multiple TFM families, handling model-specific preprocessing internally and exposing consistent fitpredict semantics. Support for SPECTRUM OF ADAPTATION STRATEGIES: zero-shot inference, meta-learning, full supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). This design allows practitioners to switch between inference paradigms seamlessly using unified API. Built-in diagnostics for CALIBRATION (Expected Calibration Error, Maximum Calibration Error, Brier score) and FAIRNESS (Statistical Parity Difference, Equalised Odds Difference, Equalised Opportunity Difference), enabling holistic assessment of model trustworthiness and risk. systematic benchmarking module across standard tabular suites (e.g., TALENT [11], OpenML-CC18 [12]) that evaluates accuracy, calibration, fairness, and resource efficiency in unified ranking framework, facilitating reproducible and comparative studies of TFM performance. With TabTune, practitioners can quickly experiment with multiple model/tuning configurations using consistent functions (e.g., .fit(), .predict(), .evaluate()), without rewriting preprocessing logic or training loops. Moreover, TabTune serves as an experimental bed for studying the interplay between learning paradigmssuch as zero-shot inference versus parameter-efficient tuningand their downstream effects on calibration, fairness, and computational efficiency. For detailed system architecture, see Section 3.2. Through comprehensive experiments, we demonstrate that TabTune not only simplifies workflows, but also provides new insights into the trade-off space of accuracy, calibration, fairness and efficiency across tabular foundation models. In the remainder of the paper, Section 2 reviews prior work on tabular foundation models and adaptation techniques. Section 3.2 details the system design of TabTune, Section 6 describes our experimental setup and benchmark results, and Section 7 offers practical guidance on model selection, discusses limitations, and outlines future directions."
        },
        {
            "title": "2.1 Traditional Machine Learning and Deep Learning for Tabular Data",
            "content": "Tabular datastructured as records (rows) and features (columns)is pervasive across applications such as healthcare, finance, industrial analytics, and scientific research. For decades, models based on gradient-boosted decision trees (GBDTs) such as XGBoost, LightGBM, and CatBoost have dominated supervised learning on tabular tasks. These methods reliably handle heterogeneous feature types, missing values, and small to medium sample sizes, while requiring 2 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models only modest hyper-parameter tuning. Their robustness and simplicity have made them the de facto standard for tabular prediction tasks in both research and production settings. In recent years, deep neural networkssuch as feed-forward multilayer perceptrons, attention-based architectures, and hybrid treeneural modelshave been applied to tabular data [13, 14]. Despite significant progress, these models have often struggled to consistently outperform GBDTs in real-world benchmarks. This gap arises from several challenges: the heterogeneity of tabular data, limited dataset sizes, complex featuretarget relationships, and the absence of large-scale pre-training strategies analogous to those that revolutionized vision and language modeling."
        },
        {
            "title": "2.2 Tabular Foundation Models",
            "content": "A major paradigm shift has recently emerged with the advent of tabular foundation models (TFMs)large pretrained architectures that aim to generalize across diverse tasks, often through in-context learning (ICL) or single forwardpass inference without task-specific gradient updates. In the context of TFMs, zero-shot inference refers to the \"fit then predict\" paradigm: during the fit phase, the model receives training samples as context (without gradient-based parameter updates), and during the predict phase, it performs in-context learning through forward passes alone to make predictions on test samples. Unlike traditional machine learning where \"fit\" implies gradient updates and training, in TFMs the fit step simply provides context samples to the pretrained model, enabling rapid adaptation without requiring training time. TABPFN (Tabular Prior-data Fitted Network) represents one of the earliest and most influential TFMs. It employs transformer architecture trained on vast collections of synthetic tabular datasets, effectively learning to approximate Bayesian inference across varying generative priors. This pre-training strategy enables TabPFN to achieve strong fewshot and zero-shot performance on small to medium datasets (typically up to 10,000 samples), offering competitive accuracy without explicit fine-tuning [4, 5, 6]. Building on this foundation, TABICL extends the in-context learning paradigm to larger-scale tabular datasets by introducing novel two-stage column-then-row attention mechanism. Through large-scale synthetic pre-training (up to 60K samples) and robust scaling to half million samples at inference time, TabICL demonstrates that transformer-based ICL can challenge traditional tree ensembles even on large tabular domains [7]. Another line of work, TABDPT, introduces an in-context learning (ICL)-based tabular foundation model trained with denoising-style, self-supervised objectives on real-world tables [15]. In parallel, other approaches explore the role of mixed synthetic priors, showing that combining causal, statistical, and randomized priors during pre-training can substantially improve downstream generalization and calibration performance across classification and regression tasks. Collectively, these developments illustrate that, with sufficient architectural bias and large-scale pre-training, transformerbased TFMs can achieve broad generalization on structured datachallenging long-held assumptions about the superiority of tree-based ensembles in tabular learning. Recent comprehensive surveys have catalogued the landscape of tabular foundation models [16, 17], highlighting their potential to transform tabular learning. Recent work has demonstrated that careful fine-tuning strategies can yield significant improvements for TFMs [18]. Meanwhile, researchers have argued that tabular foundation models should be research priority [19], given their potential impact across domains. ORIONMSP [9] introduces transformer architecture for tabular in-context learning that combines multi-scale sparse attentioncapturing both local and global feature dependencies efficientlywith Perceiver-style latent memory that enables safe bidirectional communication between training and test samples. It uses column-wise embeddings via Set Transformers to model feature distributions and split-masked causal attention mechanism for proper ICL reasoning. The design achieves near-linear attention scaling while maintaining high accuracy across diverse datasets, rivaling or outperforming TabICL and TabPFNespecially on heterogeneous, high-dimensional, and imbalanced datasetsthus demonstrating robust, efficient, and generalizable tabular learning without gradient updates. Similarly, ORIONBIX [10] employs biaxial attention mechanism for tabular in-context learning, enabling bidirectional context modeling across both rows and columns of tabular data. This dual-axis attention pattern allows the model to capture complex feature interactions and sample relationships simultaneously, providing more comprehensive representation of tabular structure. The biaxial design facilitates effective information flow between training and test samples, enabling strong in-context learning performance across diverse tabular datasets while maintaining computational efficiency."
        },
        {
            "title": "2.3 Representation Learning and Alternative Approaches",
            "content": "While transformer-based TFMs dominate recent work, alternative representation learning paradigms have been explored. Graph neural networks have been adapted for tabular data, modeling feature relationships through graph structures [20]. Extensions of foundation models to time series forecasting demonstrate their versatility; for instance, TabPFN-v2 has 3 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models shown competitive performance on temporal tasks [21], suggesting that tabular foundation models may generalize beyond traditional classification settings."
        },
        {
            "title": "2.4 Tabular ML Toolkits and Frameworks",
            "content": "Several frameworks have emerged to facilitate deep learning with tabular data, each addressing different aspects of the modeling pipeline. AUTOGLUON [22] is an AutoML framework that automates model selection and hyperparameter tuning for tabular data, employing neural architecture search and ensemble methods to achieve strong predictive performance with minimal manual configuration. PYTORCH TABULAR [23] provides modular framework for deep learning with tabular data, offering reusable components for neural network architectures tailored to structured data. However, these frameworks primarily focus on training models from scratch or performing architecture search, rather than working with pretrained tabular foundation models. In contrast, TABTUNE is the first unified framework specifically designed for tabular foundation models, bridging the gap between pretrained TFM architectures and practical deployment workflows. While existing toolkits excel at discovering or building models from scratch, TabTune addresses the unique challenges of working with pretrained TFMs: standardizing diverse preprocessing requirements, unifying adaptation strategies (zero-shot inference, meta-learning, supervised fine-tuning, and parameter-efficient fine-tuning), and providing built-in evaluation diagnostics for calibration and fairness. This specialization enables practitioners to leverage the powerful generalization capabilities of pretrained TFMs without the complexity of managing model-specific interfaces and workflows."
        },
        {
            "title": "2.5 Adaptation Techniques: Meta-Learning, Fine-Tuning, and PEFT",
            "content": "Although inference-only TFMs exhibit strong generalization, many real-world applications benefit from adapting to target distributions. Three key adaptation paradigms have emerged. Meta-learning (or episodic fine-tuning) uses supportquery splits from training data to retain in-context generalization while improving task-specific performance [24]. This approach has proven effective for tabular foundation models such as TabICL, where episodic training enhances accuracy and supports few-shot adaptation to new distributions [7, 15]. Supervised fine-tuning (SFT) updates all model parameters on labeled data when sufficient resources are available [25]. In tabular domains, full fine-tuning often yields notable gains over inference-only use, especially on large datasets where models learn domain-specific patterns [7, 15]. However, it remains computationally and storage intensive, as every parameter must be updated per task. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA) [26], limit updates to lowdimensional subspaces, greatly reducing compute and memory costs while preserving performance. Surveys categorize PEFT into additive (adapters), selective (partial updates), and reparameterization-based (low-rank) approaches [27]. Empirical studies show that PEFT matches or surpasses full fine-tuning with far lower resource demands [28]. For tabular foundation models, PEFT enables efficient domain adaptation under tight resource budgets, supporting rapid experimentation and scalable deployment. Together, these paradigms bridge the gap between zero-shot inference and fully supervised modeling, promoting adaptable TFMs for diverse applications."
        },
        {
            "title": "2.6 Calibration, Fairness, and Responsible Deployment",
            "content": "As tabular foundation models progress from research to deployment, challenges of trustworthiness, calibration, fairness, and robustness have become increasingly central. Calibration reliabilitythe alignment between predicted probabilities and actual outcomesis vital for highstakes decision-making. Guo et al. [29] showed that modern neural networks often exhibit systematic miscalibration, motivating metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE) to measure these gaps. For tabular foundation models, early results suggest that pretrained transformers yield better-calibrated confidence estimates than classical ensembles, especially in low-data settings where in-context learning supports more nuanced uncertainty estimation. Fairness requires that models avoid systematic bias across demographic groups [30]. Fairness frameworks propose criteria such as equalized odds [31]requiring equal true and false positive ratesand equal opportunity, focusing on true positive rate parity. [32] revealed fundamental tension between perfect calibration and strict fairness, showing that these objectives can conflict. In-context learning setups may further amplify biases present in demonstration data. Mitigation strategies include group-balanced sampling, fair demonstration selection, and uncertainty-based data curation. 4 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Figure 1 Overview of the TabTune architecture. Raw tabular data are processed through model-aware pre-processing engine and modular tuning pipeline, coordinated by central orchestration engine with integrated evaluation assessment modules. Robustnessthe stability of model behavior under distributional shift, perturbations, and adversarial attacksremains an open challenge. While tabular foundation models benefit from large-scale, heterogeneous pretraining, their behavior under shift and noise is not yet fully characterized. These considerations highlight the need for unified evaluation frameworks that jointly assess predictive performance, calibration, fairness, and robustness to ensure the responsible deployment of tabular foundation models."
        },
        {
            "title": "3 System Design",
            "content": "The design of TabTune emphasizes abstraction of the operational complexity associated with modern tabular foundation models (TFMs), while preserving flexibility for advanced adaptation, benchmarking, and experimentation. The framework adopts strictly modular architecture in which preprocessing, tuning, and orchestration layers are decoupled yet interoperable, ensuring extensibility and transparency across research and production workflows (Figure 2)."
        },
        {
            "title": "3.1 Design Principles",
            "content": "Unified Interface. TabTune exposes consistent, high-level API that provides unified access to all supported TFMs, including TabPFN, TabDPT, TabICL, Mitra, ContextTab, OrionMSP and OrionBiX . Through shared syntax for model initialization, training, and evaluation, it eliminates the need for users to adapt to model-specific conventions or data representations, enabling seamless experimentation across architectures. Model-Aware Automation. The framework automatically identifies the chosen model and configures preprocessing, hyperparameters, and adaptation procedures accordingly. Whether deploying zero-shot TabPFN, performing episodic meta-learning with TabICL, or applying parameter-efficient LoRA fine-tuning, TabTune dynamically selects optimized defaults aligned with the models internal design and capacity. This automation reduces manual configuration overhead and ensures consistency across runs. Unified Workflow Compatibility. All models in TabTune follow consistent workflow based on standardized method calls such as fit(), predict(), and evaluate(). This design allows easy integration into broader machine learning pipelines and benchmarking frameworks, enabling reproducible and interpretable experimentation without requiring specialized wrappers or retraining scripts."
        },
        {
            "title": "3.2 Modular Architecture",
            "content": "TabTunes architecture is composed of four interdependent modules that collectively handle data processing, model adaptation, inference, and evaluation. These components are designed to remain logically independent yet harmonized through unified orchestration layer. 5 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Figure 2 Modular architecture of TabTune. The TabularPipeline orchestrates the workflow by chaining the model-aware DataProcessor, the encapsulated TFM, and the adaptive TuningManager, forming cohesive and control framework. TabularPipeline module, which serves as the main user interface and orchestrator of the modeling workflow. It encapsulates the selected TFM, manages persistence through save() and load() utilities, and coordinates the complete pipelinefrom preprocessing and fine-tuning to evaluation and model comparison. The DataProcessor component is responsible for handling model-specific preprocessing logic. Upon initialization, it dynamically loads specialized processors and performs imputation, normalization, encoding, and embedding generation as required. The TuningManager module constitutes the computational backbone of TabTune. It executes the selected adaptation strategyranging from zero-shot inference to supervised fine-tuning, meta-learning, and parameterefficient fine-tuning (PEFT). The manager implements training routines in unified format, abstracts gradient updates, and integrates lightweight adaptation methods such as LoRA to reduce computational and memory costs while maintaining model generalization. For meta-learning configurations, episodic training is supported through dynamic supportquery sampling, preserving the in-context adaptability of transformer-based models. Complementing these components, the TabularLeaderboard module enables systematic evaluation and large-scale comparison of model variants. It automates training and evaluation over consistent data partitions and produces ranked summaries across metrics such as AUC, F1-score, and inference efficiency. This functionality transforms comparative model assessment into reproducible and configurable experiment suite."
        },
        {
            "title": "3.3 Supported Models",
            "content": "TabTune includes built-in support for seven prominent tabular foundation models, default tuning configurations, and aligned preprocessing pipelines. This design ensures that every model can be instantiated, fine-tuned, and evaluated with minimal configuration effort, while preserving model-specific fidelity."
        },
        {
            "title": "4 Implementation",
            "content": "This section describes how TabTunes implementation is organized around three core componentsthe DataProcessor, TuningManager, and TabularPipelinewhich together enable seamless preprocessing, adaptation, and inference through standardized interface. 6 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 1 Tabular foundation models supported in TabTune. Paradigm indicates the learning approachICL, Prior-Fitted Network, Denoising Transformer, or Scalable ICL. Key Feature summarizes each models core architectural element (e.g., attention)."
        },
        {
            "title": "OrionMSP\nOrionBiX\nTabICL\nMitra\nContextTab\nTabDPT\nTabPFN",
            "content": "In-Context Learning In-Context Learning In-Context Learning Scalable ICL Semantics-Aware ICL Utilizes modality-specific semantic embeddings In-Context Learning Prior-Fitted Network Multi-scale sparse attention with Perceiver-style latent memory Biaxial attention with bidirectional context modeling Two-stage column-then-row attention for large-scale tabular ICL 2D attention (row and column) with mixed synthetic priors Employs diffusion-based self-supervised pre-training Approximates Bayesian inference on synthetic priors"
        },
        {
            "title": "4.1 Model-Aware Preprocessing",
            "content": "The DataProcessor dynamically loads model-specific routines to format data correctly with minimal user setup. For TABPFN, features are mapped to numerical or integer-encoded tensors to align with its synthetic priors. CONTEXTTAB [8] adds semantic vectors for column names and categorical values. TABICL and MITRA apply normalization and model-tailored transformations. For ORIONMSP [9] and ORIONBIX [10], preprocessing is distributionand hierarchy-aware: columns are embedded with Set Transformer-based induced set attention on train rows only, numerical features are standardized and categorical values embedded, multi-scale row inputs and block-sparse attention masks are constructed, Perceiver-style memory is written by train rows and read by all rows, and label injection uses split-masked attention to prevent test-to-train leakage. Compared to TABICL [7], ORION adds distribution-aware column embeddings and multi-scale sparsity for scalable, ICL-safe processing. Together, these routines ensure each TFM receives inputs in its expected schema."
        },
        {
            "title": "4.2 Flexible Tuning Strategies",
            "content": "The TuningManager module governs model adaptation and optimization, executing the selected strategy specified during pipeline initialization. It provides unified training controller that supports four complementary regimesinference, supervised fine-tuning, meta-learning, and parameter-efficient fine-tuning (PEFT)under common abstraction. Zero-shot inference, TabTune performs zero-shot prediction using pretrained weights, exploiting the generalization capacity of models such as TabPFN for small datasets without parameter updates. Supervised fine-tuning extends this by optimizing all model parameters through gradient-based learning on labeled data, allowing users to specify training details (e.g., number of epochs or learning rate) via the configuration dictionary. For in-context learning models such as TabICL and Mitra, batches are split into pseudo-episodes where the first half serves as support context and the second half as query targets, maintaining the episodic structure required by the model architecture. Meta-learning fine-tuning introduces episodic adaptation, where models are trained on dynamically sampled supportquery pairs to maintain in-context generalization while improving task-specific accuracy. Each episode randomly samples disjoint support and query sets from the training data, with class labels normalized to contiguous indices within each episode to ensure compatibility with fixed-size classification heads. This approach is particularly effective for transformer-based architectures like TabICL and TabDPT that rely on in-context reasoning. Parameter-Efficient fine-tuning (PEFT)implemented via Low-Rank Adaptation (LoRA)reduces computational cost by updating only small low-rank matrices within attention layers. LoRA is configured with rank = 8 and scaling factor Î± = 16, injecting trainable adapters into attention projection layers. When LoRA application fails due to architectural constraints, the framework automatically falls back to standard fine-tuning. This technique significantly lowers memory requirements while preserving performance and is supported for both supervised and meta-learning modes."
        },
        {
            "title": "4.2.1 PEFT Implementation Details:",
            "content": "The fine-tuning framework determines target layers for parameter-efficient adaptation automatically based on model architecture. Updates are typically restricted to attention projections and core transformer components, while dynamic or task-specific heads are excluded. If no predefined targets are detected, adapters are applied to all eligible linear layers. When adapter injection is incompatible with the underlying architecture, the system reverts to standard full-parameter fine-tuning. 7 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 2 Fine-tuning strategy support matrix across tabular foundation models. = full support; = experimental (may revert to base fine-tuning); = not supported. SFT = Supervised Fine-Tuning; Meta = Meta-Learning; PEFT SFT = Parameter-Efficient Supervised Fine-Tuning; PEFT Meta = Parameter-Efficient Meta-Learning."
        },
        {
            "title": "OrionMSP\nOrionBiX\nTabPFN\nTabICL\nTabDPT\nMitra\nContextTab",
            "content": "SFT Meta-Learning PEFT SFT PEFT Meta-Learning"
        },
        {
            "title": "4.2.2 Episodic Training Mechanics:",
            "content": "For meta-learning adaptation, episodes are formed by sampling disjoint support and query sets from the training data. Each episode contains labeled support examples and query examples used for evaluation. The model is conditioned on the support labels and optimized to predict the query labels within each episode. To ensure compatibility with fixed-size classification heads, class labels are remapped to contiguous indices {0, 1, . . . , 1} according to the support set. Episodes in which query samples include unseen classes are excluded to preserve consistency."
        },
        {
            "title": "4.2.3 Data Sampling Strategies",
            "content": "The framework incorporates optional resampling techniques to address label imbalance and promote well-diversified dataset for model context, while remaining independent of the core modeling workflow. Resampling is applied exclusively to the training split to avoid any data leakage into the validation or test sets. Supported methods : The available options include: smote (Synthetic Minority Over-sampling Technique), random_over (random oversampling), random_under (random undersampling), tomek (Tomek links removal), kmeans (cluster centroids undersampling), and knn (neighborhood cleaning rule). These methods enable flexible control over class balance and support reproducible experimentation across diverse tabular datasets."
        },
        {
            "title": "4.3 Evaluation Utilities",
            "content": "TabTune provides comprehensive evaluation capabilities across performance, calibration, and fairness. Beyond these standard metrics, TabTune also includes built-in utilities for assessing model calibration and fairnesscritical dimensions for responsible deployment in high-stakes applications. The evaluate() method computes standard classification metrics including accuracy, precision, recall, F1score, and AUC-ROC, enabling rapid assessment of predictive performance across models and tuning strategies (Listing 4.5). The evaluate_calibration() method computes Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Brier score to quantify the reliability of predicted probabilities. These metrics reveal whether models confidence estimates align with actual prediction accuracy, essential for decision-making systems in healthcare, finance, and autonomous applications. The evaluate_fairness() method assesses demographic parity and equalized odds across sensitive attributes, measuring whether predictions exhibit systematic bias across subgroups. Users specify sensitive features (e.g., gender, race, age) and receive Statistical Parity Difference (SPD), Equalised Odds Difference (EOD), and more, enabling practitioners to identify and mitigate algorithmic bias before deployment."
        },
        {
            "title": "4.4 Installation",
            "content": "TabTune is designed to be easily integrated into existing Python environments. The library is open-source and available on the Python Package Index (PyPI), ensuring straightforward installation for practitioners. To install the latest stable release, users can execute the following command: 1 pip install tabtune 8 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Alternatively, for the latest development version, the package can be installed directly from the source repository: 1 git clone https://github.com/Lexsi-Labs/TabTune.git 2 cd TabTune 3 pip install -r requirements.txt 4 pip install -e ."
        },
        {
            "title": "4.5 Pipeline Abstraction and Workflow",
            "content": "The TabularPipeline serves as the primary orchestration layer for integrating the preprocessing and tuning components. It manages the end-to-end flowdata ingestion, preprocessing, adaptation, and predictionthrough unified API that abstracts low-level operations. The following demonstrates the core workflow for model initialization, training, and inference. The pipeline detects the model type, loads the corresponding preprocessing backend, and delegates adaptation to the TuningManager. In this example, the user specifies only the model and adaptation mode. TabTune internally triggers three automated steps: (1) the DataProcessor formats the dataset to match the models expected schema, (2) the TuningManager selects and executes the correct adaptation strategy, and (3) the TabularPipeline aggregates predictions and handles evaluation. This design illustrates how the framework abstracts operational complexity behind minimal interface, enabling quick prototyping and reproducible experimentation. 1 from tabtune import TabularPipeline 2 # Initialize the pipeline with model configuration 3 pipeline = TabularPipeline(model_name=\"OrionMSP\",tuning_strategy=\"inference\") 4 # Train and predict 5 pipeline.fit(X_train, y_train) 6 predictions = pipeline.predict(X_test)"
        },
        {
            "title": "4.6 Benchmarking with TabularLeaderboard",
            "content": "To facilitate large-scale model comparison and reproducibility, TabTune includes the TabularLeaderboard module. This component automates benchmarking across multiple TFMs and adaptation modes under consistent data splits, producing ranked summaries based on selected metrics such as AUC, F1-score, and training efficiency. Listing 4.6 shows representative use case where several models and tuning regimes are benchmarked on the same dataset. Each configuration defines its own adaptation strategy and hyperparameters, while TabTune ensures standardized preprocessing and evaluation across all runs.The leaderboard system illustrates the high-level orchestration capabilities of TabTunes architecture. Internally, it constructs multiple TabularPipeline instances, each managing its respective preprocessing and tuning configuration, and executes them in parallel under uniform conditions. The results are aggregated and ranked automatically, providing reproducible, interpretable, and scalable benchmarking framework for evaluating tabular foundation models. Following demonstrates both evaluation modes, which can be applied to any TabTune model after training. 9 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models 1 from tabtune import TabularLeaderboard 2 # Initialize leaderboard 3 leaderboard = TabularLeaderboard(X_train, X_test, y_train, y_test) 4 # Add model configurations 5 leaderboard.add_model(model_name=TabPFN,tuning_strategy=inference) 6 leaderboard.add_model(model_name=OrionBix,tuning_strategy=finetune, 7 8 leaderboard.add_model(model_name=OrionMSP,tuning_strategy=inference) 9 # Execute and rank models 10 results = leaderboard.run(rank_by=roc_auc_score) tuning_params={epochs: 5})"
        },
        {
            "title": "5 Usage Scenarios",
            "content": "This section demonstrates TabTunes capabilities through several scenarios, showcasing the frameworks functionality. These examples provide hands-on illustrations of the system components described in Section 4."
        },
        {
            "title": "5.1 Basic Usage and Multi-Model Switching",
            "content": "TabTunes unified API allows seamless experimentation across foundation models with identical workflows. Switching between the seven supported models requires only parameter changeno need to learn new APIs or preprocessing steps. This consistency enables rapid prototyping, performance comparison, and deployment without rewriting data or training code. Model persistence ensures reproducibility and version control. 1 from tabtune import TabularPipeline 2 # Example 1: TabPFN Model 3 pipeline_pfn = TabularPipeline(model_name=\"TabPFN\",tuning_strategy=\"inference\") 4 pipeline_pfn.fit(X_train, y_train) 5 predictions_pfn = pipeline_pfn.predict(X_test) 6 # Example 2: OrionMSP Model with semantic features 7 pipeline_OrionMSP = TabularPipeline(model_name=\"OrionMSP\",tuning_strategy=\"inference\") 8 pipeline_OrionMSP.fit(X_train, y_train) 9 predictions_OrionMSP = pipeline_OrionMSP.predict(X_test)"
        },
        {
            "title": "5.2 Model Comparison and Results Analysis",
            "content": "This scenario showcases systematic benchmarking of multiple models and tuning strategies with detailed performance insights. The leaderboard automates cross-validation, metric computation, and ranking across all configurations. Results can be exported as structured DataFrames for analysis, visualization, or automated model selectionenabling data-driven decisions on the best modeltuning combination for given dataset and deployment setting. 1 from tabtune import TabularLeaderboard 2 leaderboard = TabularLeaderboard(X_train, X_test,y_train, y_test) 3 # Add multiple model configurations 4 leaderboard.add_model(model_name=TabPFN,tuning_strategy=inference) 5 leaderboard.add_model(model_name=TabICL,tuning_strategy=finetune,tuning_params={epochs:5}) 6 leaderboard.add_model(model_name=OrionMSP,tuning_strategy=inference) 7 # Run comparison and get detailed results 8 results = leaderboard.run(rank_by=roc_auc_score)"
        },
        {
            "title": "5.3 Fine-Tuning Demonstration",
            "content": "This scenario demonstrates the different fine-tuning strategies available in TabTune. Each approach offers distinct trade-offs between performance, and generalization capability. 10 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "5.3.1 Supervised Fine-Tuning (SFT)",
            "content": "Supervised fine-tuning optimizes all model parameters using standard mini-batch gradient descent. This approach typically achieves the highest accuracy when sufficient training data is available. 1 from tabtune import TabularPipeline 2 # Full supervised fine-tuning 3 sft_pipeline = TabularPipeline( model_name=OrionMSP, 4 tuning_strategy=finetune, tuning_params={finetune_mode: sft,epochs: 5,learning_rate: 1e-4} 5 6 7 ) 8 sft_pipeline.fit(X_train, y_train) 9 sft_metrics = sft_pipeline.evaluate(X_test, y_test)"
        },
        {
            "title": "5.3.2 Meta-Learning Fine-Tuning",
            "content": "Meta-learning fine-tuning uses episodic training, where the model learns to adapt from support sets to query sets within each episode. This approach preserves in-context learning capabilities while improving task-specific performance. 1 from tabtune import TabularPipeline 2 # Meta-learning fine-tuning 3 meta_pipeline = TabularPipeline( model_name=OrionMSP, 4 tuning_strategy=finetune, tuning_params={ 6 5 finetune_mode: meta-learning, epochs: 3, learning_rate: 5e-5, support_size: 48, query_size: 32 } 9 10 ) 11 meta_pipeline.fit(X_train, y_train) 12 meta_metrics = meta_pipeline.evaluate(X_test, y_test)"
        },
        {
            "title": "5.3.3 Parameter-Efficient Fine-Tuning with SFT (PEFT SFT)",
            "content": "PEFT SFT combines LoRA adapters with supervised fine-tuning, achieving comparable performance to full fine-tuning while reducing memory usage by 6080%. 1 from tabtune import TabularPipeline 2 # PEFT with supervised fine-tuning 3 peft_sft_pipeline = TabularPipeline( 4 model_name=TabICL, tuning_strategy=peft, tuning_params={ finetune_mode: sft, epochs: 5, peft_config: {r: 8,lora_alpha: 16,lora_dropout: 0.05} } 9 10 ) 11 peft_sft_pipeline.fit(X_train, y_train) 12 peft_sft_metrics = peft_sft_pipeline.evaluate(X_test, y_test)"
        },
        {
            "title": "5.3.4 Parameter-Efficient Fine-Tuning with Meta-Learning (PEFT Meta-Learning)",
            "content": "PEFT meta-learning combines LoRA adapters with episodic training, offering the benefits of both parameter efficiency and in-context generalization. 11 7 8 5 7 8 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models 1 from tabtune import TabularPipeline 2 # PEFT with meta-learning 3 peft_meta_pipeline = TabularPipeline( 4 5 7 model_name=TabICL, tuning_strategy=peft, tuning_params={finetune_mode: meta-learning, epochs: 3, support_size: 48, query_size: 32, peft_config: {r: 8, lora_alpha: 16, lora_dropout: 0.05}} 8 9 ) 10 peft_meta_pipeline.fit(X_train, y_train) 11 peft_meta_metrics = peft_meta_pipeline.evaluate(X_test, y_test)"
        },
        {
            "title": "5.4 Comprehensive Model Evaluation",
            "content": "This scenario demonstrates TabTunes built-in evaluation utilities across three dimensions: standard performance metrics, probability calibration, and fairness analysis."
        },
        {
            "title": "5.4.1 Performance Metrics Evaluation",
            "content": "Standard classification metrics provide the foundation for model assessment. TabTunes evaluate() method computes accuracy, precision, recall, F1-score, and AUC-ROC in unified call. 1 from tabtune import TabularPipeline 2 pipeline = TabularPipeline(model_name=TabICL,tuning_strategy=finetune) 3 pipeline.fit(X_train, y_train) 4 # Comprehensive performance evaluation 5 performance = pipeline.evaluate(X_test, y_test) 6 print(f\"Accuracy: {performance[accuracy]:.4f}\") 7 print(f\"AUC-ROC: {performance[roc_auc_score]:.4f}\") 8 print(f\"F1-Score: {performance[f1_score]:.4f}\")"
        },
        {
            "title": "5.4.2 Calibration Evaluation",
            "content": "Calibration assessment quantifies the reliability of predicted probabilitiesessential for deployment in decision-critical applications. The evaluate_calibration() method returns ECE, MCE, and Brier score metrics. 1 # Assess probability calibration quality 2 calibration_metrics = pipeline.evaluate_calibration(X_test, y_test, n_bins=15) 3 print(f\"Expected Calibration Error: {calibration_metrics[expected_calibration_error]:.4f}\") 4 print(f\"Maximum Calibration Error: {calibration_metrics[maximum_calibration_error]:.4f}\") 5 print(f\"Brier Score: {calibration_metrics[brier_score_loss]:.4f}\")"
        },
        {
            "title": "5.4.3 Fairness Evaluation",
            "content": "Fairness analysis detects systematic bias across demographic groups, enabling responsible model deployment. Users specify sensitive attributes, and TabTune computes demographic parity and equalized odds metrics. 1 # Evaluate fairness across sensitive attributes 2 fairness_metrics = pipeline.evaluate_fairness(X_test,y_test,sensitive_features=gender_column) 3 print(f\"Statistical Parity Difference: {fairness_metrics[statistical_parity_difference]:.4f}\") 4 print(f\"Equalized Odds Difference: {fairness_metrics[equalized_odds_difference]:.4f}\")"
        },
        {
            "title": "5.5 Model Persistence and Checkpointing",
            "content": "TabTune supports end-to-end pipeline persistence and checkpointing for reproducible experiments and deployment. 12 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models 1 from tabtune import TabularPipeline 2 # Train and save fine-tuned model 3 pipeline = TabularPipeline( model_name=TabICL,tuning_strategy=finetune,tuning_params={epochs: 5} 4 ) 5 pipeline.fit(X_train, y_train) 6 performance = pipeline.evaluate(X_test, y_test) 7 print(performance) 8 # Save the entire pipeline (preprocessor + model + config) 9 pipeline.save(my_model.joblib) 10 # Load and use for predictions 11 loaded_pipeline = TabularPipeline.load(my_model.joblib) 12 performance = loaded_pipeline.evaluate(X_test, y_test) 13 print(performance)"
        },
        {
            "title": "6 Experimental Results",
            "content": "We conduct evaluation of TabTune across multiple dimensions: predictive performance, probability calibration, fairness, and scalability. This section presents our experimental design and detailed results across diverse scenarios."
        },
        {
            "title": "6.1 Experimental Design",
            "content": "Benchmark Suites and Datasets : Our experiments encompass three well-established benchmark suites i.e. TALENT [11] (181 datasets), OPENML-CC18 [12] (72 datasets), and TABZILLA [33] (36 datasets). Facilitating systematic comparison across broad spectrum of tabular learning settings. We further conduct DOMAIN-SPECIFIC EVALUATIONS in high-impact areas such as MEDICAL and FINANCIAL domains to assess the practical applicability of the methods. All experiments adhere to the STANDARDIZED DATASET SPLITS defined by each benchmark, ensuring both reproducibility and fairness in comparison. To maintain consistency and fairness across model families, benchmark results are computed only on the common subset of datasets available for all evaluated models within each benchmark suite. This unified evaluation protocol ensures that performance rankings and metrics reflect genuine methodological differences rather than disparities in dataset coverage. In total, our evaluations span 155/181 datasets for TALENT, 27/36 datasets for TabZilla, and 63/72 datasets for OpenML-CC18. Some datasets were excluded due to OUT-OF-MEMORY (OOM) ERRORS even on H200 GPUs or CUDA-RELATED ISSUES, particularly affecting TabPFN-based models. Models and Adaptation Strategies: We evaluate recent tabular foundation modelsTabPFN, TabICL, OrionMSP, including zero-shot OrionBiX, Mitra, ContextTab, and TabDPTunder multiple adaptation paradigms, inference, meta-learning fine-tuning, supervised fine-tuning, parameter-efficient supervised fine-tuning, and parameter-efficient meta-learning. In addition, we include established traditional baselines using AutoGluon [22] such as XGBoost, LightGBM, CatBoost, and Random Forest as strong reference models for comparison. Evaluation Metrics : Our assessment covers four complementary dimensions: PERFORMANCE: Classification Accuracy (ACC), AUC-ROC, and weighted F1-score (F1) quantify predictive capability across standard benchmark datasets (TALENT, OpenML-CC18, TabZilla) covering diverse dataset characteristics including small and large sample sizes, narrow and wide feature spaces, and balanced and imbalanced class distributions. It is important to clarify how MEAN RANK values are derived. Within each benchmark suite, models are ranked by accuracy on every dataset (lower rank = better performance), and these per-dataset ranks are averaged to obtain the overall mean rank. Thus, lower mean rank indicates stronger and more consistent performance across datasets, rather than the highest score on any single task. While absolute metrics (ACCURACY, F1) reflect peak task-level performance, mean rank provides normalized measure of cross-dataset generalization consistency. SCALABILITY: Analysis of performance variation across dataset sizes, feature dimensionality, and class imbalance provides practical guidance for model selection. This analysis uses the same benchmark datasets as performance evaluation, aggregated and summarized across these dimensions to reveal scalability patterns. CALIBRATION: We evaluate how fine-tuning impacts calibration metrics across different models and adaptation strategies, measuring the alignment between predicted probabilities and actual outcomes, which is critical for decisionmaking systems. Our evaluation uses three metrics: (I) Expected Calibration Error (ECE), which measures the average 13 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models deviation between predicted confidence and observed accuracy across binned predictions; (II) Maximum Calibration Error (MCE), which captures the worst-case calibration gap, identifying regions of systematic miscalibration; and (III) Brier score, which quantifies both calibration and refinement by penalizing squared deviations between predicted probabilities and binary outcomes. FAIRNESS: We assess equitable treatment across demographic subgroups using established fairness-oriented datasets with sensitive attributes (gender, race). We measure three fairness criteria that capture different notions of equitable treatment: (I) Statistical Parity Difference (SPD) quantifies the absolute difference in positive prediction rates across groups, testing whether the model provides equal opportunity independent of sensitive attributes. model satisfies demographic parity when SPD approaches zero, indicating that positive predictions are distributed proportionally across demographic groups. (II) Equalized Odds Difference (EOD) measures disparities in both true positive and false positive rates across groups, ensuring consistent error profiles across subgroups. This criterion is more stringent than statistical parity, requiring not only proportional positive predictions but also equal accuracy within each group. (III) Equalized Opportunity Difference (EOpD) focuses specifically on true positive rate parity, ensuring that qualified individuals receive equal treatment regardless of group membership. This metric is particularly relevant in settings where false negatives have severe consequences, such as loan approvals or hiring decisions. Analysis Focus : central goal of our evaluation is to analyze how different adaptation strategieszero-shot inference, meta-learning fine-tuning, and supervised fine-tuningaffect model behavior across multiple dimensions. We examine whether fine-tuning improves performance, influences calibration, and impacts fairness across domains. This comparative analysis offers practical guidance on selecting appropriate adaptation strategies for tabular foundation models in different application contexts. Detailed dataset statistics and information refer to Appendix 10. All Experiments are executed on NVIDIA L40S (48Gb VRAM), with H200 (141Gb VRAM) used for memory-intensive cases. This ensures consistent execution across all experiments while handling the computational demands of models."
        },
        {
            "title": "6.2.1 Overall Leaderboards",
            "content": "Table 3 presents comprehensive results across the TALENT, OpenML-CC18, and TabZilla benchmark suites, reporting mean rank, accuracy (ACC), and weighted F1-score (F1) for each model and adaptation strategy. The aggregated findings reveal clear and consistent trends in how adaptation mechanisms influence predictive performance and cross-dataset generalization. Baselines and Zero-Shot Inference : Classical Machine Learning methods remain strong reference baselines, attaining mean accuracies between 0.833 and 0.861 with aggregated ranks around 6.0. In contrast, pretrained tabular foundation models (TFMs) exhibit markedly stronger generalization even without task-specific training. rank (4.13), with acAmong pretrained TFMs, TabPFN-2.5 now achieves the best overall zero-shot curacies of (ACC=0.8540/F1=0.8440) on TALENT, (ACC=0.8726/F1=0.8672) on OpenML-CC18, and (ACC=0.8824/F1=0.8791) on TabZilla. OrionMSP follows closely (overall rank 4.61), reaching (ACC=0.8455 / 0.8353) on TALENT, (ACC=0.8722/F1=0.8676) on OpenML-CC18, and (ACC=0.8821/F1=0.8786) on TabZilla. TabICL and TabDPT remain competitive (ranks 5.92 and 6.45, ACC 0.840.88), while Mitra and ContextTab lag behind. Overall, zero-shot TFMs exceed traditional machine-learning baselines by roughly 24 percentage points in accuracy and three to four rank positions, demonstrating the strength of pretrained priors. Episodic meta-learning substantially improves both accuracy and rank Meta-Learning Fine-Tuning : stability relative to zero-shot inference. ORIONMSP attains the lowest aggregate rank of 2.26, achieving (ACC=0.8401/F1=0.8310) on TALENT, 0.8548/0.8516 on OpenML-CC18, and (0.8735/0.8672) on TabZilla. TABPFN ranks 2.42 overall with accuracies of (0.8517/0.8414) on TALENT, (0.8842/0.8784) on OpenMLCC18, and (0.8663/0.8603) on TabZilla. TABDPT (rank 3.95) maintains consistent performance near (0.83/0.82). By contrast, architectures such as TabICL and OrionBiX exhibit instability on TabZilla, where TabICLs F1 drops to 0.6845. These results indicate that meta-learning offers the most balanced compromise between generalization and task-specific adaptation. Supervised Fine-Tuning (SFT) : Full parameter fine-tuning yields divergent outcomes across architectures. TABPFN benefits most, achieving the best overall rank (1.97) and maintaining high accuracy across all benchmarks (ACC = 0.84590.8697, F1 = 0.83500.8617). TABDPT ranks 2.79 overall and performs particularly well on TabZilla (rank 1.81), with (ACC = 0.8337, F1 = 0.8260). ORIONMSP remains competitive (rank 2.88) with (ACC 0.79, F1 0.76), whereas TabICL and OrionBiX suffer severe degradationTabICLs accuracy on TabZilla falls from 0.873 to 0.567 and F1 from 0.870 to 0.473 indicating overfitting and loss of pretrained priors. These findings suggest that SFT is advantageous for Bayesian or probabilistic architectures such as TabPFN. TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 3 Overall leaderboard across three benchmark suitesTALENT, OpenML-CC18, and TabZilla. Ranks denote the mean rank per benchmark suite (lower is better). Metrics: ACC = Accuracy, F1 = Weighted F1. The All column reports the aggregated rank across all suites for strategy. Models are grouped by adaptation strategy. Formatting: 1st place; 2nd place within each group. Models / Strategy All TALENT OpenML-CC TabZilla Rank Rank ACC F1 Rank ACC F1 Rank ACC F1 Baselines + Inference (Zero-Shot) XGBoost CatBoost Random Forest LightGBM TabICL OrionBix OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 7.94 7.69 8.71 8.03 5.92 6.52 4.61 5.45 4.13 13.07 7.75 6.45 FineTune Meta Learning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 3.66 3.96 2.26 2.42 6.09 3.95 7.43 6.90 7.56 7.51 5.09 5.87 4.06 4.48 3.62 11.63 6.52 6. 3.33 4.25 1.80 2.07 5.79 3.72 FineTune Supervised Fine Tuning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 5.05 4.21 2.88 1.97 5.98 2.79 4.74 3.54 2.27 1.83 5.62 2.98 FineTune PEFT - Meta Learning TabICL OrionBiX OrionMSP Mitra TabDPT 4.32 2.77 2.21 4.64 2.28 4.07 2.39 2.25 4.17 2.11 0.8395 0.8329 0.8278 0.8324 0.8465 0.8339 0.8455 0.8508 0.8540 0.3935 0.8373 0.8402 0.8344 0.8158 0.8401 0.8517 0.6416 0.8255 0.7668 0.7698 0.7908 0.8459 0.5460 0. 0.7017 0.7854 0.7879 0.6369 0.8002 FineTune PEFT - Supervised Fine Tuning TabICL OrionBiX OrionMSP Mitra TabDPT 3.70 3.99 2.01 4.62 1.91 3.72 3.57 2.01 4.01 1.65 0.5692 0.6358 0.6749 0.5294 0. 0.8351 0.8252 0.8201 0.8238 0.8372 0.8253 0.8353 0.8406 0.8440 0.2884 0.8269 0.8311 0.8253 0.8060 0.8310 0.8414 0.5874 0.8167 0.7439 0.7469 0.7653 0.8350 0.4382 0.8094 0.6867 0.7762 0.7728 0.5905 0.7910 0.4638 0.5570 0.5956 0.4303 0.7842 6.57 6.98 7.10 6.89 5.30 5.63 4.58 5.48 4.36 11.50 7.01 5. 3.05 2.86 2.82 2.42 5.42 4.40 4.36 4.30 3.18 1.89 5.02 2.25 4.07 2.50 1.93 4.32 2.17 3.06 3.67 1.73 4.49 2.03 0.8558 0.8588 0.8547 0.8581 0.8667 0.8653 0.8722 0.8714 0.8726 0.3614 0.8639 0.8672 0.8664 0.8548 0.8548 0.8842 0.6164 0. 0.6838 0.7126 0.7995 0.8697 0.5408 0.8499 0.7773 0.8471 0.8566 0.6000 0.8600 0.8174 0.7380 0.8241 0.4965 0.8500 0.8537 0.8520 0.8497 0.8493 0.8623 0.8596 0.8676 0.8663 0.8672 0.2522 0.8581 0.8625 0.8597 0.8516 0.8516 0.8784 0.5651 0.8501 0.6299 0.6595 0.7668 0.8617 0.4309 0. 0.7605 0.8430 0.8453 0.5426 0.8539 0.7965 0.6789 0.8033 0.3917 0.8398 6.82 6.70 9.27 5.80 6.65 5.47 4.36 5.65 3.97 12.21 8.02 4.57 4.15 2.00 1.73 2.42 4.70 2.62 4.65 4.25 2.88 1.86 5.52 1.81 3.45 3.52 1.33 4.36 2. 2.92 3.95 1.47 4.70 1.95 0.8612 0.8579 0.8358 0.8618 0.8734 0.8728 0.8821 0.8752 0.8824 0.3152 0.8389 0.8814 0.6956 0.8726 0.8735 0.8663 0.5592 0.8500 0.5670 0.6476 0.7454 0.8433 0.4608 0.8337 0.7116 0.7370 0.8594 0.5461 0.8495 0.7920 0.6707 0.8214 0.4606 0. 0.8326 0.8384 0.8399 0.8211 0.8698 0.8628 0.8786 0.8716 0.8791 0.1830 0.8334 0.8775 0.6845 0.8662 0.8672 0.8603 0.5147 0.8456 0.4733 0.5782 0.7222 0.8327 0.3467 0.8260 0.7003 0.7200 0.8581 0.4960 0.8481 0.7776 0.6071 0.8071 0.3597 0.8461 Parameter-Efficient Fine-Tuning (PEFT) : PEFT achieves near-SFT accuracy with significantly reduced computational overhead. In the meta-learning configuration, ORIONMSP attains the best overall rank (2.21), recording (ACC = 0.7879/F1 = 0.7728) on TALENT, (0.8566/0.8453) on OpenML-CC18, and (0.8594/0.8581) on TabZilla. TABDPT follows closely (rank 2.28) with (0.8002/0.7910) on TALENT and (0.8600/0.8539) on OpenML-CC18. Under the supervised PEFT variant, TABDPT achieves the overall best rank (1.91) with (ACC 0.85 and F1 0.84), while ORIONMSP (rank 2.01) attains (0.821 / 0.807) on TabZilla. These results show that PEFT retains roughly 95% of full-fine-tuning accuracy while substantially lowering resource demands. Discussion : Across all adaptation regimes, TFMs consistently outperform traditional baselines by 2 % points in accuracy and approximately four rank positions. TABPFN proves most resilient, achieving the top overall rank under SFT, whereas ORIONMSP demonstrates superior cross-dataset generalization in meta-learning. TABDPT delivers near-optimal performance under PEFT, offering the most favorable efficiencyaccuracy balance. Conversely, models such as TabICL and OrionBiX are highly sensitive to full parameter updates and prone to overfitting. Collectively, these observations establish that careful alignment between model architecture and adaptation strategy is critical for maximizing the performance of tabular foundation models. 15 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Key Takeaways: OVERALL LEADERBOARD: Across all benchmark suites, the strongest overall performers are TABPFN and ORIONMSP. The TABPFN FAMILY (including TabPFN-2.0 and TabPFN-2.5) achieves the best aggregate results, with mean accuracies between 0.850.88 and weighted F1 scores of 0.840.88, reflecting exceptional cross-benchmark consistency. ORIONMSP follows closely (overall rank 4.61 in zero-shot and 2.26 in meta-learning), reaching ACC 0.840.88 and F1 0.830.88, with particularly strong generalization on OpenML-CC18 and TabZilla. ZERO-SHOT PERFORMANCE: The TABPFN family and ORIONMSP are the strongest zero-shot models. TABPFN-2.5 attains the best aggregate zero-shot rank (4.13), with ACC 0.850.88 across TALENT, OpenML-CC18, and TabZilla. ORIONMSP is close second (rank 4.61), matching TabPFNs accuracy on OpenML-CC18 and TabZilla while slightly trailing on TALENT. Both substantially outperform classical baselines by about 24 ACCURACY POINTS. SFT TRADE-OFFS: Full supervised fine-tuning benefits probabilistic architectures such as TABPFN, which achieves the highest accuracies (ACC 0.850.87, F1 0.830.86) across all benchmarks. In contrast, models such as TABICL and ORIONBIX exhibit severe degradation under SFT, reflecting overfitting and loss of pretrained priors. PEFT PERFORMANCE: Parameter-efficient fine-tuning attains nearfull-finetuning accuracy. TABDPT achieves top results (ACC 0.85, F1 0.84) and maintains strong stability across benchmarks, while ORIONMSP delivers competitive results (ACC 0.820.86, F1 0.800.85) with cross-dataset generalization."
        },
        {
            "title": "6.2.2 Scalability Analysis",
            "content": "We analyze how model performance varies with dataset size, feature dimensionality, and class imbalance to inform model selection under different data regimes to highlight which models and strategies perform best across varying data constraints."
        },
        {
            "title": "Based on Dataset Size",
            "content": "Table 4 shows how fine-tuning influences performance across dataset scales. On small datasets (<1K samples), TABPFN-2.5 achieves the highest zero-shot accuracy (ACC=0.8349, F1=0.8194), marginally ahead of TABPFN-2.0 under meta-learning (0.8336/0.8256) and TABDPT (0.8333/0.8271). ORIONMSP (PEFTMeta, 0.8275/0.8213) follows closely, confirming that transformer backbones still generalize well under constrained data. Fine-tuning TABICL or ORIONBIX on small datasets causes sharp degradation (ACC drops >15%), revealing overfitting risk. For medium-sized datasets (1K10K), TABPFN in meta-learning configuration leads (ACC=0.8638, F1=0.8548), followed by its SFT variant (0.8580/0.8485); both outperform all classical baselines. On datasets large-scale reaches (0.8911/0.8961)but competitively, with ORIONMSP zero-shot scale (0.8754/0.8831) and TABDPT zero-shot (0.8821/0.8754). Across all sizes, MITRA consistently underperform (ACC < 0.65), reflecting limited scalability. transformer TFMs strongXGBOOST ensembles tree-based (>10K), remain"
        },
        {
            "title": "Based on Dataset Width",
            "content": "Table 5 reveals that feature dimensionality strongly shapes model behaviour. For narrow datasets (<10 features), performance converges across models: ORIONMSP zero-shot (0.8376/0.8295) slightly leads, followed by TABDPT and TABPFN, while classical baselines remain competitive. On medium-width data (10100 features), TABPFN dominates under both zero-shot (0.8376/0.8589) and meta-learning (0.8683/0.8579), outperforming all baselines and transformer variants. The advantage of TFMs becomes most pronounced on wide feature spaces (>100 features): TABPFN-2.0 achieves exceptional results under SFT (ACC=0.9346, F1=0.9335) and meta-learning (0.9172/0.9533), far exceeding other models. TabICL and OrionBiX degrade when fully fine-tuned, confirming that dense feature spaces accentuate overfitting. Traditional models (XGBoost) remain competitive but trail by 23 %."
        },
        {
            "title": "Based on Class Imbalance",
            "content": "Table 6 compares balanced ( 0.6) and imbalanced (< 0.6) scenarios. Under balanced conditions, TABPFN (SFT) achieves the best rank (1.96) with ACC=0.8336, F1=0.8267, followed by ORIONMSP (Meta Learning, rank 2.27). 16 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 4 Performance variation by dataset size across all benchmark suites. ACC = Accuracy; F1 = Weighted F1-score, averaged across datasets within each size category: Small (<1K samples), Medium (1K10K), and Large (>10K). Values are on 01 scale (higher is better). Models are grouped by adaptation strategy. Formatting: 1st place ; 2nd place within each group. Models Small (<1K) Medium (1K-10K) Large (>10K) Rank ACC F1 Rank ACC F1 Rank ACC F1 Baselines + Zero-Shot Inference XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 8.66 8.88 9.48 8.80 6.96 7.34 6.76 7.42 5.78 15.24 9.76 6.28 FineTune Meta Learning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 4.68 2.78 3.21 3.31 6.39 3.85 0.8168 0.8124 0.7988 0.8143 0.8301 0.8330 0.8232 0.8325 0.8349 0.4334 0.8179 0.8333 0.7912 0.8149 0.8211 0.8336 0.6417 0.8224 FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 5.65 5.20 2.90 2.68 6.48 2. 0.6746 0.7158 0.8135 0.8216 0.5625 0.8227 FineTune PEFT - Meta Learrning TabICL OrionBiX OrionMSP Mitra TabDPT 4.80 3.33 2.28 5.21 2.92 0.7518 0.8093 0.8275 0.6510 0.8191 0.7964 0.7935 0.8187 0.7789 0.8338 0.8150 0.8194 0.8131 0.8194 0.3236 0.8085 0. 0.7806 0.8123 0.8151 0.8256 0.5988 0.8188 0.6225 0.6792 0.7920 0.8109 0.4408 0.8153 0.7397 0.8042 0.8213 0.6012 0.8157 FineTune - PEFT - Supervised FineTuning TabICL OrionBiX OrionMSP Mitra TabDPT 3.53 4.38 1.73 5.53 2. 0.7957 0.7576 0.8298 0.5244 0.8269 0.7710 0.7102 0.8132 0.4133 0.8182 8.28 7.89 8.57 8.30 5.86 6.71 4.53 4.63 4.07 12.89 6.82 6.58 3.64 3.99 2.34 2.13 6.10 4.09 4.86 4.04 2.85 1.93 6.05 2.87 4.25 2.86 2.10 4.57 2. 3.73 3.99 2.06 4.59 1.67 0.8363 0.8340 0.8285 0.8314 0.8486 0.8348 0.8494 0.8557 0.8566 0.3600 0.8430 0.8424 0.8342 0.8283 0.8413 0.8638 0.6083 0.8289 0.7553 0.7718 0.8036 0.8580 0.5133 0.8377 0.7251 0.7995 0.8134 0.6089 0.8121 0.6424 0.6585 0.7190 0.5008 0. 0.8314 0.8264 0.8221 0.8226 0.8398 0.8260 0.8402 0.8462 0.8470 0.2553 0.8334 0.8339 0.8256 0.8204 0.8330 0.8548 0.5516 0.8212 0.7187 0.7386 0.7798 0.8485 0.4116 0.8284 0.7114 0.7939 0.8019 0.5599 0.8049 0.5686 0.5892 0.6597 0.4013 0.8245 6.52 6.43 8.68 6.82 5.42 5.54 3.66 6.75 3.42 12.40 9.34 6. 3.06 4.50 1.38 2.76 5.82 3.54 5.25 4.03 2.96 1.57 5.42 2.65 4.29 2.29 2.45 4.55 1.83 3.71 3.75 2.04 3.98 2.02 0.8961 0.8786 0.8680 0.8817 0.8790 0.8716 0.8831 0.8771 0.8887 0.3879 0.8548 0.8821 0.8431 0.8410 0.8787 0.8748 0.6585 0. 0.6693 0.6697 0.7231 0.8572 0.5720 0.8120 0.6962 0.7848 0.8013 0.6304 0.8407 0.6084 0.6327 0.6931 0.5459 0.7686 0.8911 0.8721 0.8614 0.8753 0.8731 0.8656 0.8754 0.8701 0.8831 0.2801 0.8464 0.8754 0.8360 0.8306 0.8721 0.8668 0.6132 0.8613 0.6212 0.6272 0.6828 0.8459 0.4501 0. 0.6799 0.7665 0.7817 0.5795 0.8299 0.5020 0.5381 0.6152 0.4476 0.7418 When class ratios become highly skewed, meta-learning fine-tuning provides the greatest stability: ORIONMSP achieves rank 2.16 (ACC=0.8735, F1=0.8636), with TABPFN close behind (rank=2.50, ACC=0.8784, F1=0.8664). These results confirm that episodic adaptation enhances calibration and minority-class reliability, while tree-based models lose 35% in accuracy under severe imbalance. Discussion : Scalability analysis underscores that TABPFN, ORIONMSP, and TABDPT each occupy distinct strengths. TABPFN excels on small, medium, and high-dimensional data where Bayesian priors and strong supervision prevail; ORIONMSP scales best with large datasets and imbalanced distributions due to meta-learning robustness; and TABDPT sustains near-top performance across all settings with minimal computational overhead. Classical baselines remain reliable but consistently trail the best TFMs by 24% in accuracy. Key Takeaways: DATASET SIZE: Small datasets favor the TABPFN FAMILY (TabPFN-2.5 zero-shot, TabPFN-2.0 metalearning), with TABDPT as close, efficient alternative; medium datasets favor TABPFN (meta & SFT); large datasets see XGBOOST slightly leading in raw accuracy, with ORIONMSP and TABDPT close behind. 17 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 5 Performance variation by feature dimensionality (dataset width) across all benchmark suites. ACC = Accuracy; F1 = Weighted F1-score, averaged across datasets within each width category. Values are on 01 scale (higher is better). Models are grouped by adaptation strategy. Formatting: 1st place ; 2nd place within each group. Models Narrow (<10) Medium (10-100) Wide (>100) Rank ACC F1 Rank ACC F1 Rank ACC F1 Baselines + Zero-Shot Inference XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 7.79 6.57 8.31 7.17 6.01 5.56 3.16 6.08 4.63 12.52 8.64 5.35 FineTune - Meta Learning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 3.66 3.92 1.90 2.73 5.67 3.56 0.8199 0.8124 0.7981 0.8107 0.8188 0.8089 0.8376 0.8167 0.8243 0.3769 0.7965 0.8243 0.7867 0.7956 0.8253 0.8264 0.6646 0. FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 5.28 4.03 2.63 1.86 5.81 2.50 0.6609 0.6922 0.7540 0.8258 0.5619 0.8078 FineTune PEFT - Meta Learrning TabICL OrionBiX OrionMSP Mitra TabDPT 4.58 2.51 2.18 4.15 2.22 0.6375 0.7584 0.7836 0.6471 0.7979 0.8134 0.8045 0.7919 0.8004 0.8098 0.8020 0.8295 0.8071 0.8120 0.2720 0.7862 0.8169 0.7779 0.7900 0.8190 0.8171 0.6247 0.8071 0.6195 0.6576 0.7297 0.8162 0.4478 0.7948 0.6375 0.7584 0.7836 0.6471 0. FineTune - PEFT - Supervised FineTuning TabICL OrionBiX OrionMSP Mitra TabDPT 3.73 3.81 2.10 4.28 1.72 0.6196 0.6278 0.6795 0.5518 0.7821 0.5287 0.5444 0.6113 0.4481 0.7631 8.27 8.26 8.82 8.27 5.62 6.70 4.98 4.93 3.92 13.14 7.29 6. 3.48 3.89 2.21 2.37 6.12 3.82 5.04 4.33 2.99 2.06 6.09 2.88 4.21 2.92 2.20 4.83 2.32 3.74 4.08 1.97 4.78 2.02 0.8482 0.8441 0.8410 0.8458 0.8627 0.8510 0.8572 0.8676 0.8697 0.3886 0.8441 0.8566 0.8383 0.8316 0.8514 0.8683 0.6004 0. 0.7507 0.7643 0.8025 0.8578 0.5349 0.8378 0.7601 0.8147 0.8254 0.6177 0.8299 0.6596 0.6851 0.7477 0.5106 0.8305 0.8410 0.8344 0.8235 0.8326 0.8549 0.8417 0.8478 0.8589 0.8609 0.2781 0.8452 0.8483 0.8338 0.8278 0.8457 0.8579 0.5763 0.8350 0.7101 0.7294 0.7750 0.8469 0.4268 0. 0.7444 0.8057 0.8107 0.5650 0.8209 0.5902 0.6177 0.6939 0.4108 0.8201 6.14 6.85 9.11 9.00 7.93 8.20 7.00 7.44 4.20 14.46 8.38 8.64 3.86 3.58 2.86 2.28 6.17 3.48 3.95 3.75 2.90 1.40 5.60 3.40 4.05 2.50 2.66 4.61 2. 2.77 4.00 1.88 4.77 1.55 0.9101 0.9127 0.8987 0.8957 0.8948 0.8859 0.8860 0.9129 0.9248 0.2521 0.9008 0.8845 0.8319 0.8429 0.8564 0.9172 0.4090 0.8400 0.7757 0.7560 0.8148 0.9346 0.3307 0.8623 0.7714 0.8251 0.8293 0.4129 0.8416 0.8760 0.6737 0.8042 0.2831 0. 0.9039 0.9084 0.8936 0.8908 0.8936 0.8849 0.8837 0.9111 0.9240 0.1497 0.8999 0.8820 0.8667 0.8640 0.8737 0.9533 0.3224 0.8497 0.7269 0.7046 0.7808 0.9335 0.2436 0.8587 0.7622 0.8243 0.8256 0.3338 0.8376 0.8502 0.6166 0.7663 0.1874 0.8901 FEATURE DIMENSIONALITY: Wide datasets (>100 features) favor TABPFN (SFT, ACC 0.92, F1 0.92), confirming its strength for high-dimensional problems; medium-width data favor TABPFN (Meta Learning), while narrow data show minimal gaps between TFMs and baselines. CLASS IMBALANCE: Meta-learning fine-tuning yields the best robustness under imbalance, with ORIONMSP (Rank-2.16, ACC = 0.8735, F1 = 0.8636) outperforming other TFMs; TABPFN (rank 2.50) remains second. OVERALL TREND: TFMs scale smoothly with data size, width, and balance, maintaining >24% advantage and >3 rank improvement over traditional baselines."
        },
        {
            "title": "6.2.3 Domain-Specific Analysis",
            "content": "To assess real-world applicability beyond general scalability patterns, we evaluate model performance across high-stakes domains including Medical and Finance  (Table 7)  . Within domain-specific suites, different fine-tuning strategies yield distinct advantages depending on the domain. Medical Domain Performance : On medical datasets, TABPFN with supervised fine-tuning (SFT) achieves the best overall results (Rank = 1.86, ACC = 0.8094, F1 = 0.7958), followed by its meta-learning variant (Rank = 2.28, ACC = 0.8133, F1 = 0.8000). ORIONMSP performs strongly in zero-shot (ACC = 0.8045, F1 = 0.7916) and meta-learning settings (Rank = 2.20, ACC = 0.7744, F1 = 0.7645), but exhibits minor degrada18 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 6 Performance variation by class imbalance across all benchmark suites. ACC = Accuracy; F1 = Weighted F1-score, averaged within each imbalance category. Rank denotes the mean rank within each category (lower is better). Values are on 01 scale (higher is better). Models are grouped by adaptation strategy. Formatting: 1st place ; 2nd place within each group. Models / Strategy Balanced ( 0.6) Imbalanced (<0.6) Rank ACC F1 Rank ACC F1 Baselines + Zero-Shot Inference XGBoost CatBoost Random Forest LightGBM TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 8.44 8.60 9.36 8.70 5.81 6.95 5.20 4.73 3.81 13.61 7.33 6.27 FineTune - Meta Learning TabICL OrionBiX OrionMSP TabPFN Mitra TabDPT 3.78 4.02 2.27 2.31 6.40 4.09 0.8175 0.8076 0.7983 0.8071 0.8279 0.8096 0.8265 0.8367 0.8406 0.2763 0.8152 0.8233 0.7943 0.7947 0.8231 0.8461 0.4699 0.8066 FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN Mitra TabDPT 4.84 4.11 3.04 1.96 6.40 3.02 0.6708 0.6917 0.7467 0.8336 0.3216 0.8025 FineTune PEFT - Meta Learrning TabICL OrionBiX OrionMSP Mitra TabDPT 4.24 2.73 2.28 5.02 2.30 0.6530 0.7504 0.7714 0.4683 0. FineTune - PEFT - Supervised FineTuning TabICL OrionBiX OrionMSP Mitra TabDPT 3.56 4.02 2.14 4.97 1.76 0.6003 0.5974 0.6753 0.3189 0.7907 0.8110 0.8020 0.7955 0.7977 0.8233 0.8040 0.8202 0.8309 0.8348 0.1540 0.8095 0.8189 0.7884 0.7881 0.8183 0.8407 0.4142 0. 0.6317 0.6550 0.7287 0.8267 0.2037 0.7949 0.6460 0.7436 0.7623 0.4173 0.7801 0.5233 0.5164 0.6105 0.2171 0.7798 7.28 6.69 7.99 7.31 6.00 6.07 3.96 6.23 4.49 12.48 8.19 6.65 3.58 3.94 2.16 2.50 5.83 4.01 5.26 4.32 2.72 1.98 5.57 2. 4.43 2.84 2.14 4.21 2.33 3.87 3.96 1.87 4.21 2.10 0.8853 0.8780 0.8736 0.8769 0.8801 0.8782 0.8835 0.8803 0.8835 0.4824 0.8722 0.8793 0.8662 0.8622 0.8735 0.8784 0.7885 0.8650 0.7794 0.7934 0.8307 0.8709 0.7413 0.8578 0.8026 0.8528 0.8608 0.8001 0. 0.7223 0.7490 0.7915 0.7495 0.8496 0.8779 0.8669 0.8639 0.8626 0.8692 0.8677 0.8725 0.8691 0.8727 0.3891 0.8601 0.8684 0.8547 0.8532 0.8636 0.8664 0.7391 0.8552 0.7362 0.7590 0.7953 0.8578 0.6415 0.8464 0.7800 0.8420 0.8433 0.7524 0.8511 0.6507 0.6875 0.7423 0.6497 0. tion under full SFT. TABDPT (PEFTSFT) also performs competitively (Rank = 1.96, ACC = 0.7680, F1 = 0.7531), achieving near-optimal results with reduced fine-tuning cost. In contrast, models such as TABICL and ORIONBIX lose up to 710 % accuracy under SFT, confirming overfitting risk in small, noisy clinical datasets. Traditional baselines (XGBOOST, LIGHTGBM) remain competitive (ACC 0.790.80) but trail the best TFMs by 23 . Overall, the medical results demonstrate that meta-learning TFMs generalize more reliably under limited data and high noise. Finance Domain Performance : Financial datasets are typically larger and more imbalanced, amplifying the value of episodic and parameter-efficient adaptation. ORIONMSP under meta-learning attains the best overall rank (2.26) and strong accuracy (ACC = 0.8209, F1 = 0.8089), narrowly surpassing TABPFN (meta, Rank = 3.00, ACC = 0.8220, F1 = 0.8056) and its SFT counterpart (Rank = 2.00, ACC = 0.8222, F1 = 0.8058). TABDPT (PEFTSFT) maintains competitive accuracy (Rank = 2.82, ACC = 0.7782, F1 = 0.7543), while ORIONMSP (PEFTSFT) achieves the strongest overall efficiencyaccuracy balance (Rank = 1.93, ACC = 0.7854, F1 = 0.7272). Transformer models such as TABICL degrade sharply under SFT (ACC drop > 12 %), whereas classical baselines remain robust (ACC 0.81) but underperform in F1 due to poor minority calibration. The finance domain thus emphasizes meta-learning as the most stable strategy for large-scale, imbalanced tabular data. Discussion : Across both domains, pretrained TFMs consistently outperform traditional baselines when appropriate fine-tuning strategies are used. TABPFN achieves the best reliability and calibration on small, noisy medical datasets, 19 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 7 Domain-specific leaderboards for Medical and Finance datasets from the benchmark suites. Rank denotes the mean rank within each domain (lower is better). ACC = Accuracy; F1 = Weighted F1-score (01 scale, higher is better). Models are grouped by adaptation strategy. Formatting: 1st place ; 2nd place within each group. Models / Strategy Medical Finance Rank ACC Rank ACC F1 Baselines + Zero-Shot Inference XGBoost RandomForest CatBoost LightGBM TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 7.32 7.30 7.48 6.36 6.66 4.98 5.31 5.82 5.04 12.00 6.40 7. 0.7834 0.7779 0.7784 0.7949 0.7819 0.7893 0.8045 0.7984 0.7965 0.3935 0.8003 0.7764 FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN Mitra TabDPT 4.64 3.60 2.24 1.86 5.54 3.12 FineTune - Meta Learning TabICL OrionBiX OrionMSP TabPFN Mitra TabDPT 3.40 3.58 2.20 2.28 5.06 4.48 0.7081 0.7400 0.7481 0.8094 0.5600 0.7717 0.7833 0.7623 0.7744 0.8133 0.6499 0.7535 FineTune PEFT - Meta Learrning TabICL OrionBiX OrionMSP Mitra TabDPT 4.19 2.37 2.02 3.62 2. 0.7031 0.7628 0.7779 0.6539 0.7390 FineTune - PEFT - Supervised FineTuning TabICL OrionBiX OrionMSP Mitra TabDPT 3.68 3.26 2.02 4.08 1.96 0.5759 0.5926 0.6353 0.5557 0.7680 0.7669 0.7752 0.7594 0.7614 0.7696 0.7759 0.7916 0.7857 0.7835 0.2863 0.7881 0. 0.6565 0.6961 0.7116 0.7958 0.4343 0.7573 0.7695 0.7536 0.7645 0.8000 0.5913 0.7477 0.6873 0.7613 0.7658 0.6055 0.7312 0.5044 0.5189 0.5767 0.4357 0.7531 7.62 8.46 6.75 7.32 7.82 6.46 5.25 8.21 4.57 15.14 10.85 9.46 6.50 5.33 3.00 2.00 6.10 2. 3.43 4.26 2.26 3.00 6.73 5.06 5.07 3.07 2.57 5.07 2.79 3.57 5.07 1.93 5.18 2.82 0.7958 0.8052 0.8117 0.8095 0.8125 0.8206 0.8158 0.8094 0.8169 0.5340 0.8000 0.8080 0.6863 0.7389 0.7896 0.8222 0.6823 0.7922 0.8171 0.8106 0.8209 0.8220 0.7559 0. 0.7351 0.7958 0.7977 0.7492 0.7934 0.7533 0.7279 0.7854 0.6665 0.7782 0.7885 0.8001 0.8015 0.7974 0.7942 0.8125 0.8047 0.7919 0.8000 0.4250 0.7830 0.7960 0.6335 0.7118 0.7523 0.8058 0.5748 0.7763 0.8019 0.8022 0.8089 0.8056 0.7044 0.7930 0.7118 0.7846 0.7842 0.7085 0. 0.6833 0.6469 0.7272 0.5487 0.7543 whereas ORIONMSP dominates in large, high-imbalance financial contexts. TABDPT maintains strong performance in both settings through efficient PEFT adaptation, confirming that resource-efficient methods can match full SFT accuracy within 2 %. These trends highlight the need for strategydomain alignment: Bayesian priors excel under data scarcity, while meta-learning transformers scale better with complex, heterogeneous features. Key Takeaways: MEDICAL DOMAIN: TABPFN (SFT, rank 1.86) achieves best overall accuracy, with its meta-learning variant (rank 2.28) maintaining robustness; TABDPT (PEFTSFT, rank 1.96) offers strong low-cost alternatives. FINANCE DOMAIN: ORIONMSP (meta, rank 2.26, ACC 0.82) achieves top results, closely followed by TABPFN (meta/SFT, ranks 2.003.00); TABDPT (PEFTSFT) remains efficient and competitive. CROSS-DOMAIN: TABPFN provides most consistent cross-domain performance; ORIONMSP generalizes best to large-scale financial data; meta-learning and PEFT yield stable, domain-adaptive transfer."
        },
        {
            "title": "6.3 Calibration Evaluation",
            "content": "Reliable probability calibration is essential for deploying tabular models in real-world decision-making systems. Calibration measures how well predicted probabilities align with observed outcomes, ensuring that model confidence 20 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 8 Probability calibration metrics across models and tuning strategies on the TALENT, OpenML-CC18, and TabZilla benchmark suites. ECE = Expected Calibration Error; MCE = Maximum Calibration Error; BS = Brier Score. All metrics range from 01 (lower is better). Values are averaged across datasets within each suite. Models are grouped by adaptation strategy. Formatting: 1st place ; 2nd place within each group. TALENT OpenML-CC TabZilla ECE MCE BS ECE MCE BS ECE MCE BS Models / Strategy Zero-Shot Inference TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 0.0219 0.0324 0.0219 0.0276 0.0266 0.2238 0.02585 0.0308 FineTune - Meta Learning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 0.0355 0.0409 0.0340 0.0271 0.0334 0.0562 FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 0.0768 0.0753 0.0471 0.0287 0.1723 0.0548 PEFT FineTune - Meta Learning TabICL OrionBiX OrionMSP Mitra TabDPT 0.0355 0.0409 0.0340 0.0334 0.0562 0.2421 0.2245 0.2098 0.2470 0.2484 0.3115 0.2525 0. 0.2570 0.2693 0.2570 0.2440 0.1912 0.3235 0.3219 0.3204 0.1886 0.2509 0.2620 0.3019 0.2570 0.2693 0.2570 0.1912 0.3235 PEFT FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP Mitra TabDPT 0.0768 0.0753 0.0471 0.1723 0. 0.3219 0.3204 0.1886 0.2620 0.3019 0.1533 0.1787 0.1589 0.1514 0.1483 0.5291 0.1619 0.1586 0.1671 0.1841 0.1656 0.1502 0.3479 0.1791 0.2349 0.2334 0.2113 0.1580 0.4677 0.1867 0.1671 0.1841 0.1656 0.3479 0.1791 0.2349 0.2334 0.2113 0.4677 0. 0.0371 0.0325 0.0319 0.0375 0.0380 0.2138 0.0352 0.0443 0.0426 0.0731 0.0520 0.0346 0.0533 0.0745 0.1156 0.1131 0.0550 0.0381 0.1539 0.0497 0.0426 0.0731 0.0520 0.0533 0.0745 0.1156 0.1131 0.0550 0.1539 0.0497 0.3404 0.3230 0.2902 0.2880 0.2945 0.2952 0.3260 0. 0.3387 0.4164 0.3458 0.2989 0.2258 0.4178 0.3610 0.3374 0.3155 0.2957 0.2269 0.3397 0.3387 0.4164 0.3458 0.2258 0.4178 0.3610 0.3374 0.3155 0.2269 0.3397 0.1267 0.1325 0.1262 0.1253 0.1270 0.5307 0.1353 0.1351 0.1346 0.1607 0.1606 0.1178 0.3932 0. 0.3039 0.2923 0.1995 0.1246 0.4784 0.1443 0.1346 0.1607 0.1606 0.3932 0.1620 0.3039 0.2923 0.1995 0.4784 0.1443 0.0369 0.0385 0.0310 0.0431 0.0530 0.1946 0.0389 0.0435 0.2033 0.0744 0.0365 0.0388 0.0891 0.0664 0.1534 0.1581 0.0773 0.0469 0.2023 0. 0.2033 0.0744 0.0365 0.0891 0.0664 0.1534 0.1581 0.0773 0.2023 0.0713 0.2863 0.2512 0.2805 0.2745 0.2750 0.2995 0.2815 0.2997 0.4926 0.3964 0.2574 0.2636 0.2086 0.2838 0.3280 0.3801 0.3321 0.2528 0.2991 0.3183 0.4926 0.3964 0.2574 0.2086 0. 0.3280 0.3801 0.3321 0.2991 0.3183 0.1301 0.2419 0.1243 0.1283 0.1272 0.5733 0.1605 0.1227 0.3732 0.1416 0.1250 0.1344 0.4239 0.1591 0.4078 0.3530 0.2594 0.1454 0.5254 0.1801 0.3732 0.1416 0.1250 0.4239 0.1591 0.4078 0.3530 0.2594 0.5254 0. reflects actual likelihoods. We evaluate Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Brier Score (BS) across all benchmark suitesTALENT, OpenML-CC18, and TabZillaunder different adaptation strategies. Lower values indicate better calibration. Table 8 summarizes the results and highlights how fine-tuning affects probabilistic reliability. Zero-shot inference Pretrained tabular foundation models (TFMs) achieve the strongest overall calibration without fine-tuning. ORIONMSP and TABICL obtain the lowest ECE (0.0219 on TALENT) and maintain low BS values ( 0.15). ORIONMSP further sustains excellent calibration on OpenML-CC18 (ECE=0.0319, BS=0.1262) and TabZilla (ECE=0.0310, BS=0.1243), ranking first or tied across suites. TABPFN-2.0 remains consistently well-calibrated (ECE=0.02760.0431, BS=0.1250.151), confirming the reliability of its Bayesian uncertainty modeling. TABDPT performs respectably (ECE0.030.04, BS0.120.16), while MITRA shows extreme miscalibration (ECE>0.19, BS>0.52), indicating unreliable confidence outputs. Meta-learning fine-tuning Episodic meta-learning largely preserves calibration quality. TABPFN achieves the best overall calibration across suites (ECE=0.02710.0388, BS=0.1180.150), improving slightly over zero-shot results. ORIONMSP remains competitive (ECE=0.03400.0520, BS=0.1250.166), maintaining stable confidence estimates even after adaptation. In contrast, transformer-heavy models such as TABICL exhibit severe degradation on TabZilla (ECE rises from TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models 0.0369 to 0.2033), showing that full episodic updates can destabilize latent priors when domain variability is high. Interestingly, MITRA improves calibration dramatically under meta-learning (ECE drops from 0.22 to 0.030.05), although its Brier scores remain poor (> 0.34), suggesting partial correction without overall reliability gains. Supervised fine-tuning (SFT) Full SFT substantially worsens calibration for most transformer models. TABICLs ECE triples to quadruples (0.020.04 0.080.15) and its Brier Score deteriorates from 0.15 to 0.230.41. ORIONBIX follows similar pattern. ORIONMSP maintains moderate calibration (ECE=0.0470.077, BS=0.210.26)still degraded but markedly better than other transformers. TABPFN, however, preserves excellent reliability: across all suites, it records ECE=0.02870.0469, MCE<0.30, and BS=0.1240.158, closely matching or outperforming its zero-shot performance. TABDPT also retains good calibration (ECE=0.04970.0713, BS=0.1440.186). MITRA again performs worst (ECE > 0.15, BS > 0.47), underscoring structural limitations in its confidence estimation. Discussion : Calibration analysis reveals clear hierarchy of reliability. TABPFN consistently maintains AMONG THE LOWEST ECE and Brier scores across all adaptation strategies (ECE 0.0270.047, BS 0.120.16), validating its Bayesian design and robustness to parameter updates. Meta-learning preserves calibration better than full SFT, confirming that episodic adaptation reinforces pretrained uncertainty priors. ORIONMSP achieves strong calibration, especially under zero-shot and meta-learning, while transformer-centric models like TABICL and ORIONBIX show pronounced drift once fully fine-tuned. Overall, TFMs maintain lower calibration error than ensemble baselines and degrade far less under fine-tuning, making them more trustworthy for risk-sensitive deployment. Key Takeaways: TABPFN maintains excellent calibration across all strategies (ECE 0.0270.047, BS 0.120.15), confirming its Bayesian reliability; TABDPT (SFT) and ORIONMSP (zero-shot/meta-learning) rank next best (ECE 0.02190.0365). Supervised fine-tuning degrades calibration for transformer models: TABICLs ECE increases 34 and BS deteriorates sharply, while ORIONMSP degrades moderately. Zero-shot inference yields the best calibration overall, with ORIONMSP and TABPFN showing ECE < 0.04 across all suites."
        },
        {
            "title": "6.4 Fairness Evaluation",
            "content": "Fairness analysis evaluates whether model predictions exhibit systematic bias across demographic subgroups, complementing accuracy and calibration assessments by addressing equity in high-stakes applications such as lending, healthcare, and criminal justice. Unlike performance metrics that can be computed automatically, fairness requires explicit identification of sensitive attributes (e.g., race, gender). We use datasets with demographic labelsAdult Census Income, German Credit, and COMPAS Recidivismto measure bias propagation and mitigation across strategies. Following standard fairness literature, we report Statistical Parity Difference (SPD), Equalized Odds Difference (EOD), and Equalized Opportunity Difference (EOpD). All metrics are absolute differences in the 01 range (lower = fairer). Table 9 summarizes fairnessaccuracy trade-offs across zero-shot, meta-learning, and fine-tuning strategies. Zero-shot inference. clear fairnessaccuracy trade-off emerges. MITRA achieves the lowest bias (SPD = 0.0193, EOD = 0.0590, EOpD = 0.0982) but with very low predictive accuracy (ACC = 0.6902). ORIONMSP and ORIONBIX balance fairness and accuracy best, maintaining high accuracy (ACC = 0.875-0.878) and moderate fairness (EOD = 0.270.28). TABPFN shows consistent behavior (ACC = 0.871, SPD = 0.307), while CONTEXTTAB ranks second in fairness (SPD = 0.208) at slightly lower accuracy. These results suggest that transformer-based models preserve competitive fairness even without task-specific adaptation. Meta-learning fine-tuning. Episodic adaptation largely preserves fairness while sustaining accuracy. ORIONBIX achieves the lowest overall bias (SPD = 0.2021, EOD = 0.1624, EOpD = 0.2010) while maintaining high accuracy (ACC = 0.8743). ORIONMSP follows closely (EOD = 0.2798, EOpD = 0.2907),a close second with slightly higher bias but similar accuracy. TABPFN achieves the highest accuracy (ACC = 0.8733) with slightly higher disparity (SPD 0.31). MITRA remains highly fair but again sacrifices accuracy. Overall, meta-learning fine-tuning maintains fairness levels similar to zero-shot inference while modestly improving accuracy consistency. TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 9 Fairness metrics across models and tuning strategies on datasets with explicit demographic attributes. All metrics are absolute differences (01 range; lower is better, 0 = perfect equity). SPD = Statistical Parity Difference; EOD = Equalized Odds Difference; EOpD = Equalized Opportunity Difference. ACC = Accuracy; F1 = Weighted F1-score. Values are averaged across fairness evaluation datasets. Models are grouped by adaptation strategy. Sensitive features (e.g., race, gender, age) are manually specified per dataset. Formatting: 1st place ; 2nd place within each group. Models / Strategy SPD EOD EOpD ACC F1 Zero-Shot Inference TabICL OrionBiX OrionMSP TabPFN-2.0 TabPFN-2.5 Mitra ContextTab TabDPT 0.2900 0.3328 0.3380 0.3070 0.3067 0.0193 0.2082 0.2990 FineTune - Meta Learning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 0.3140 0.2021 0.3038 0.3070 0.2586 0.2648 0.3168 0.2698 0.2827 0.3114 0.3204 0.0590 0.1860 0.3197 0.3100 0.1624 0.2798 0.3115 0.2748 0.3180 FineTune - Supervised FineTuning TabICL OrionBiX OrionMSP TabPFN-2.0 Mitra TabDPT 0.0550 0.1128 0.1227 0.3070 0.0161 0.3075 0.3225 0.1761 0.2208 0.3115 0.0170 0.3153 FineTune PeFT - Meta Learning TabICL OrionBiX OrionMSP Mitra TabDPT 0.3140 0.3149 0.3038 0.2595 0.2778 FineTune PeFT - SFT TabICL OrionBiX OrionMSP Mitra TabDPT 0.1572 0.0349 0.1132 0.0029 0.2960 0.3092 0.2722 0.2798 0.2391 0.2792 0.4041 0.1564 0.2140 0.0370 0.3190 0.3278 0.2855 0.2983 0.3176 0.3144 0.0982 0.1951 0.3338 0.3332 0.2010 0.2907 0.3177 0.3301 0. 0.3319 0.1817 0.2353 0.3177 0.0555 0.3284 0.3332 0.2875 0.2907 0.3154 0.3051 0.4089 0.2234 0.2420 0.0740 0.3342 0.8743 0.8779 0.8752 0.8708 0.8756 0.6902 0.8580 0.8674 0.8678 0.8743 0.8680 0.8733 0.7967 0.8614 0.4584 0.6180 0.4753 0.8733 0.7268 0. 0.8678 0.8748 0.867 0.8104 0.8651 0.4881 0.6401 0.5732 0.7133 0.8595 0.8680 0.8727 0.8706 0.8631 0.8679 0.5847 0.8345 0.8582 0.8603 0.8692 0.8650 0.8668 0.7546 0.8503 0.3625 0.5498 0.4039 0.8668 0.6277 0.8435 0.8602 0.8717 0.8632 0.7721 0. 0.4269 0.5759 0.5080 0.6028 0.8505 Supervised fine-tuning (SFT). Full SFT produces mixed fairness outcomes. MITRA again records the lowest bias (SPD = 0.0161, EOD = 0.0170) but with only ACC = 0.7268. TABICL exhibits striking fairness gains (SPD drops from 0.29 to 0.055) yet loses over 40% in accuracy (ACC = 0.4584). ORIONBIX improves fairness (EOD = 0.1761) but suffers large accuracy loss (ACC = 0.618). In contrast, TABPFN maintains both high accuracy (ACC = 0.8733) and moderate, stable fairness metrics across all strategies, confirming robustness. TABDPT ranks second in accuracy (ACC = 0.8529) with fairness levels comparable to TABPFN. These results underscore that SFT can reduce group disparity for some models but typically harms predictive reliability. Parameter-efficient fine-tuning (PEFT). PEFT variants show similar patterns while retaining higher stability. Under PEFT meta-learning, ORIONBIX and ORIONMSP achieve balanced fairness (EOD 0.270.28) and strong accuracy (ACC = 0.8670.875), outperforming full SFT counterparts. Under PEFT SFT, MITRA again minimizes bias (SPD = 0.0029) but accuracy remains low (ACC = 0.7133). TABDPT delivers the best accuracyfairness compromise (ACC = 0.8595, SPD 0.30). Overall, PEFT preserves fairness trends while mitigating the accuracy collapse observed in full fine-tuning. Discussion. Fairness evaluation confirms fundamental accuracyequity trade-off. Models achieving near-perfect demographic parity (MITRA, SPD < 0.02) incur large accuracy losses, while TABPFN and ORIONMSP consistently achieve 23 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models balanced performance. Meta-learning proves most stable, preserving both fairness and accuracy across suites. SFT can occasionally improve parity for specific transformer architectures but typically at the expense of predictive fidelity. PEFT provides an effective compromise, maintaining high accuracy and moderate bias without significant fairness degradation. Key Takeaways: ACCURACYFAIRNESS TRADE-OFF: MITRA attains best fairness (SPD = 0.00290.0193) but suffers low accuracy (ACC = 0.690.73). BALANCED MODELS: TABPFN and ORIONMSP offer the best fairnessaccuracy equilibrium (ACC = 0.870.88, EOD = 0.270.29), outperforming other TFMs. EFFECT OF FINE-TUNING: SFT improves fairness for some models (e.g., TABICL SPD 0.055 vs 0.29 zero-shot) but drastically reduces accuracy; meta-learning preserves both aspects better. PEFT STABILITY: Parameter-efficient strategies maintain fairness trends while retaining high accuracy, offering pragmatic balance for equitable deployment."
        },
        {
            "title": "7 Discussion",
            "content": "Our comprehensive analysis across three benchmark suites reveals clear patterns that guide model selection, adaptation strategy, and deployment trade-offs."
        },
        {
            "title": "7.1 Fine-Tuning Strategy Recommendations",
            "content": "Fine-tuning strategy choice is critical and highly model-dependent. Zero-shot inference achieves excellent calibration (ECE <0.04 for top models) and competitive predictive performance. It is ideal when computational resources are limited or calibration reliability is priority. TABPFN-2.5 attains the highest overall zero-shot standing (Rank = 4.13), establishing new state-of-the-art for inference-only tabular tasks. ORIONMSP follows closely, achieving strong accuracy on large datasets (ACC = 0.87220.8821) and ranking among the top configurations on OpenML-CC18 and TabZilla without requiring additional training. Meta-learning fine-tuning provides the best balance between adaptation and generalization. It improves rank stability for models such as ORIONMSP (aggregate rank 2.26 in Table 3) while preserving calibration (ECE <0.06). Meta-learning also excels in high-imbalance settings (ORIONMSP rank 2.16, TABPFN rank 2.50 in Table 6), confirming its cross-domain robustness. However, instability can occur for some models such as TABICL on TabZilla (ECE increases to 0.2033), requiring careful validation. Supervised fine-tuning (SFT) delivers the strongest results for TABPFN (ranks 1.831.89, ACC up to 0.9346 on wide datasets). In contrast, SFT causes substantial degradation for TABICL and ORIONBIX: for instance, TABICLs accuracy on TabZilla falls from 0.8734 to 0.5670, indicating severe overfitting. Calibration also deteriorates (ECE 0.080.15 vs. 0.020.04 zero-shot). SFT should therefore be used selectivelybeneficial for TABPFN, acceptable for TABDPT and ORIONMSP, but risky for TABICL and ORIONBIX. Parameter-efficient fine-tuning (PEFT) attains nearly full fine-tuning accuracy while greatly reducing computational cost. TABDPT achieves high performance (ACC 0.85, F1 0.84) across all benchmarks, and ORIONMSP maintains competitive accuracy (ACC 0.820.86, F1 0.800.85) with strong crossdataset generalization."
        },
        {
            "title": "7.2 Mechanisms Behind Fine-Tuning Performance",
            "content": "The varying outcomes of different fine-tuning strategies can be explained by the underlying adaptation mechanisms. Why SFT Underperforms for TabICL and OrionBiX? Full supervised fine-tuning updates all model parameters, erasing pretrained representationsespecially detrimental on smaller datasets. The performance collapse of TABICL on TabZilla (ACC 0.8734 0.5670) exemplifies overfitting and representation drift, whereas optimization distorts pretrained priors. Both TABICL and ORIONBIX rely on high-capacity transformer layers optimized for diverse pretraining data; constraining them to narrow distribution leads to loss of generality and inflated variance in predictions. 24 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Why Meta-Learning Preserves Generalization? Episodic training in ORIONMSP and TABPFN aligns closely with the few-shot adaptation process. Each metaepisode simulates the supportquery dynamics of deployment, encouraging transfer of cross-task features rather than memorization. This prevents catastrophic forgetting and reinforces pretrained inductive biases. The improvement in rank stability for ORIONMSP (from 2.903.84 in zero-shot to 1.732.82 in meta-learning) confirms that episodic optimization promotes stable generalization. When Fine-Tuning is Beneficial? Fine-tuning is most useful when: 1. Datasets are sufficiently large (>10K samples), mitigating overfitting. 2. Domain distribution shifts significantly from pretraining data (e.g., medical, finance). 3. Architecture maintains calibration under parameter updates (as in TABPFNs Bayesian prior design). For small or balanced datasets, zero-shot or meta-learning is more reliable and cost-efficient. Parameter-efficient fine-tuning (PEFT) provides strong middle ground, retaining 95% of full SFT accuracy while reducing memory."
        },
        {
            "title": "7.3 Calibration Insights",
            "content": "Our calibration analysis shows that TABPFN uniquely maintains excellent calibration across all fine-tuning strategies (ECE 0.0270.047, BS 0.120.16), validating its Bayesian uncertainty formulation. This makes TABPFN ideal for high-stakes domains such as healthcare and finance. Meta-learning further stabilizes calibrationTABPFNs ECE improves slightly under episodic updates, and ORIONMSP remains well-calibrated (ECE <0.06). In contrast, TABICL exhibits the largest degradation under SFT (ECE increases 34, BS rises to 0.230.41), confirming that full parameter updates distort uncertainty estimates. For calibration-critical applications, we recommend TABPFN (any regime) or ORIONMSP (meta-learning); SFT should be avoided for TABICL and ORIONBIX."
        },
        {
            "title": "7.4 Model Selection Guidelines",
            "content": "Model selection should be guided by dataset scale, balance, feature dimensionality, and available computational resources. All recommendations below are derived from the common subset of datasets evaluated across all models to ensure consistent statistical comparison. Reported values combine mean accuracy (ACC), weighted F1-score (F1), and mean rank (based on accuracy) as presented in Tables 36. General Guidelines: When dataset characteristics are unknown, zero-shot inference with ORIONMSP or meta-learning fine-tuning with TABPFN serve as the most reliable defaults. Both models consistently achieve top-three average ranks across all benchmark suites while maintaining strong calibration and fairness. In general: TABPFN provides the most consistent accuracy and F1 across diverse data regimes, excelling in calibration and few-shot generalization. ORIONMSP scales best on large, complex, and imbalanced datasets, particularly under meta-learning. Specific Regime Recommendations. ON SMALL DATASETS (<1K SAMPLES), TABPFN-2.5 achieves the highest zero-shot performance (ACC = 0.8349, F1 = 0.8194), slightly ahead of TABDPT (0.8333/0.8271) and boosted-tree baselines. Under meta-learning, TABPFN-2.0 also leads (0.8336/0.8256), while ORIONMSP (PEFTMETA) provides strong performance at lower fine-tuning cost (0.8275/0.8213). Fine-tuning TABICL or ORIONBIX on small datasets still causes sharp degradation (ACC drops > 15%), revealing overfitting risk. MEDIUM DATASETS (1K10K SAMPLES): TABPFN with meta-learning achieves the highest mean accuracy (ACC = 0.8638, F1 = 0.8548, rank 2.0), followed closely by its supervised fine-tuning variant (ACC = 0.8580, F1 = 0.8485). When computational constraints are present, TABDPT (PEFTMeta) maintains competitive accuracy (ACC = 0.81210.8332) with minimal resource overhead. Zero-shot inference remains practical option for quick deployment, trailing fine-tuned variants by only about 23% in accuracy. LARGE DATASETS (>10K SAMPLES): ORIONMSP in zero-shot mode delivers the highest mean performance among TFMs (ACC = 0.8843, F1 = 0.8768, rank 2.0). TABDPT (zero-shot or PEFTSFT) achieves comparable accuracy (ACC = 0.84960.8831) while offering lower adaptation cost. Classical models such as XGBOOST and CATBOOST reach strong absolute accuracies (ACC = 0.8969, F1 = 0.8920) but 25 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models exhibit poorer calibration and higher cross-dataset variance. TFMs are therefore more advantageous when generalization consistency or uncertainty estimation is prioritized. BALANCED DATASETS: TABPFN under supervised fine-tuning yields the best average rank (1.96) with ACC = 0.8336, F1 = 0.8267, followed by ORIONMSP under meta-learning (rank 2.27). Balanced data favor Bayesian or episodically fine-tuned architectures, which maximize calibration and stability. IMBALANCED DATASETS: Meta-learning strategies provide the most robust adaptation under label imbalance. ORIONMSP achieves the best overall performance (rank = 2.16, ACC = 0.8735, F1 = 0.8636), with TABPFN ranking second (rank = 2.50, ACC = 0.8784, F1 = 0.8664). These results confirm that episodic training improves minority-class calibration. In contrast, tree-based baselines lose roughly 35% in F1 when class ratios fall below 0.6. LOW-TO-MEDIUM FEATURE DIMENSIONALITY (<100 FEATURES): Model performance converges across methods (ACC = 0.840.86), with TABPFN (Meta Learning) and ORIONMSP (Zero-Shot) performing most consistently. In this regime, TFMs primarily outperform classical methods in calibration and rank stability rather than raw accuracy. HIGH-DIMENSIONAL DATASETS (>100 FEATURES): TABPFN under supervised fine-tuning achieves the highest overall scores (ACC = 0.9346, F1 = 0.9335), significantly surpassing other models. TABDPT and ORIONMSP maintain strong accuracy (ACC 0.840.89) while offering superior efficiency. Architectures such as TABICL and ORIONBIX exhibit sharp performance drops under full fine-tuning, confirming their susceptibility to overfitting in wide-feature settings."
        },
        {
            "title": "Key Takeaways",
            "content": "For inference-only deployment, use ORIONMSP (Zero-Shot) or TABPFN (Zero-Shot) for balanced accuracy and calibration. For adaptive fine-tuning, prefer TABPFN (SFT/Meta) on smalltomedium datasets and ORIONMSP (Meta) on large-scale or imbalanced data."
        },
        {
            "title": "7.5 Domain-Specific Guidance",
            "content": "MEDICAL DOMAIN: TABPFN achieves the strongest performance in this domain, with its supervised fine-tuning configuration attaining Rank = 1.86 and ACC = 0.8094, closely followed by the meta-learning variant (Rank = 2.28, ACC = 0.8133). TABDPT under PEFTSFT (Rank = 1.96) provides competitive accuracy at substantially lower computational cost. Transformer-based architectures such as TABICL and ORIONBIX exhibit notable degradation under full fine-tuning, reflecting sensitivity to small and noisy clinical datasets. Overall, Bayesian and episodically trained models generalize more reliably in data-scarce medical settings. FINANCE DOMAIN: ORIONMSP under meta-learning achieves strong results (Rank = 2.26, ACC = 0.8209, F1 = 0.8089), while TABPFN performs exceptionally well in supervised and meta-learning modes (Ranks = 2.003.00, ACC = 0.82200.8222). TABDPT remains competitive (PEFTSFT, Rank = 2.82) and offers an efficient alternative with only minor loss in accuracy. These outcomes confirm that episodic and parameter-efficient adaptation strategies are particularly effective for large-scale, class-imbalanced financial data, where they improve both calibration and minority-class performance."
        },
        {
            "title": "7.6 Practical Value of Tabular Foundation Models",
            "content": "Tabular Foundation Models (TFMs) deliver consistent but moderate absolute accuracy improvementstypically 24 percentage pointsover strong classic baselines such as XGBOOST, CATBOOST, and LIGHTGBM. Across comparable settings, TFMs achieve mean accuracies around 0.850.88 versus 0.830.86 for boosted trees. However, their primary advantage lies in reliability and robustness rather than raw accuracy. Key strengths include: (I) ZERO-SHOT ADAPTABILITY: The TABPFN family and ORIONMSP provide top-tier zero-shot performance. TABPFN-2.5 attains the best aggregate zero-shot rank (4.13) with ACC 0.850.88, while ORIONMSP remains highly competitive (rank 4.61, ACC = 0.84550.8821). Both outperform boosted-tree baselines by roughly 1.52.5 PERCENTAGE POINTS in accuracy without any task-specific training. (II) CALIBRATION: TABPFN maintains low calibration error (ECE < 0.05) across both zero-shot and fine-tuned regimes, outperforming ensemble baselines whose ECE often exceeds 0.08. (III) FAIRNESS STABILITY: TFMs achieve moderate statistical parity differences (SPD 0.280.30) while sustaining high accuracy (> 0.85), indicating balanced performance across sensitive groups. 26 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Consequently, TFMs should be regarded as reliable, general-purpose learners well suited for rapid deployment, few-shot adaptation, and risk-sensitive applications where calibration and fairness are critical. For large, well-labeled, single-domain problems, gradient-boosted trees may still be preferable for simplicity and computational efficiency, but TFMs offer distinct advantages in uncertainty estimation, cross-domain generalization, and calibration consistency."
        },
        {
            "title": "7.7 Limitations",
            "content": "While TABTUNE provides broad support for evaluating and fine-tuning TFMs, several limitations remain. TASK SCOPE: Current support is limited to binary and multi-class classification tasks; regression and multi-label extensions are planned for future releases. MODEL CONSTRAINTS: DATASET SIZE: TABPFN performs optimally on datasets with fewer than 10,000 samples; larger datasets may significantly increase inference latency or exceed memory limits. DIMENSIONALITY: All models can degrade when the feature space exceeds 1000 dimensions without preprocessing or feature selection. PEFT COMPATIBILITY: Parameter-efficient fine-tuning is fully supported for ORIONMSP and TABDPT, but only partially implemented for TABPFN and CONTEXTTAB. CONTEXTTAB EVALUATION: Due to excessive GPU memory requirements, CONTEXTTAB was evaluated only on smaller benchmark subsets. EVALUATION AND ANALYSIS: MANUAL FAIRNESS SPECIFICATION: Sensitive attributes are manually defined per dataset, introducing potential bias in fairness evaluation. CALIBRATION INTERPRETATION: Optimal threshold tuning remains domain-dependent and may vary across metrics or class distributions. BEYOND-ACCURACY METRICS: Advanced interpretability, reliability, and uncertainty quantification metrics are not yet fully integrated into the current release."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce TABTUNE, unified Python library that standardizes the workflow for tabular foundation models (TFMs). By abstracting model-specific preprocessing, consolidating adaptation strategies, and integrating evaluation modules for performance, calibration, and fairness, TABTUNE addresses the fragmentation that has limited the broader adoption of TFMs. Inspired by AUTOML, TABTUNE focuses on managing and optimizing pretrained TFMs rather than building models from scratch, offering coherent framework for efficient adaptation and assessment. We conducted comprehensive study on the performance of tabular foundation models (TFMs) and the impact of fine-tuning strategies on their effectiveness. The empirical analysis demonstrates that the choice of adaptation strategy is highly model-dependent. TABPFN performs optimally under supervised fine-tuning or meta-learning, ORIONMSP exhibits strong generalization and robustness particularly with meta-learning, and TABDPT using parameter-efficient fine-tuning (PEFT) achieves favorable balance between predictive accuracy and computational efficiency. Beyond raw performance, our findings emphasize the reliability and fairness of TFMs. TABPFN consistently attains excellent calibration, while both ORIONMSP and TABPFN maintain strong fairnessaccuracy balance, reinforcing their potential for responsible and trustworthy deployment in high-stakes domains."
        },
        {
            "title": "9 Future Work",
            "content": "As an extensible and reproducible platform for TFM research, TABTUNE continues to evolve to support emerging methodologies and broader evaluation dimensions. Future work will focus on integrating more advanced fine-tuning and meta-learning approaches to enhance few-shot and domain adaptation capabilities. In parallel, we aim to strengthen the evaluation framework by incorporating modules for advanced fairness assessment, interpretability, and uncertainty quantification. Additionally, we plan to extend task coverage beyond binary and multi-class classification to include regression, multi-label learning, and time-series modeling. By standardizing the TFM workflow, TABTUNE enables practitioners to transition efficiently from experimentation to reliable deployment, fostering evidence-based model selection and accelerating the responsible adoption of foundation models in structured data domains. 27 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "References",
            "content": "[1] Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785794, 2016. [2] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. [3] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018. [4] Noah Hollmann, Samuel MÃ¼ller, Katharina Eggensperger, and Frank Hutter. Tabpfn: transformer that solves In The Eleventh International Conference on Learning small tabular classification problems in second. Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [5] Noah Hollmann, Samuel MÃ¼ller, Lennart Purucker, Arjun Krishnakumar, Max KÃ¶rfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with tabular foundation model. Nat., 637(8044):319326, 2025. [6] LÃ©o Grinsztajn, Klemens FlÃ¶ge, Oscar Key, Felix Birkel, Philipp Jund, Brendan Roof, Benjamin JÃ¤ger, Dominik Safaric, Simone Alessi, Adrian Hayler, Mihir Manium, Rosen Yu, Felix Jablonski, Shi Bin Hoo, Anurag Garg, Jake Robertson, Magnus BÃ¼hler, Vladyslav Moroshan, Lennart Purucker, Clara Cornu, Lilly Charlotte Wehrhahn, Alessandro Bonetto, Bernhard SchÃ¶lkopf, Sauraj Gambhir, Noah Hollmann, and Frank Hutter. Tabpfn-2.5: Advancing the state of the art in tabular foundation models, 2025. [7] Jingang Qu, David HolzmÃ¼ller, GaÃ«l Varoquaux, and Marine Le Morvan. Tabicl: tabular foundation model for in-context learning on large data. CoRR, abs/2502.05564, 2025. [8] Marco Spinaci, Marek Polewczyk, Maximilian Schambach, and Sam Thelin. Contexttab: semantics-aware tabular in-context learner. CoRR, abs/2506.10707, 2025. [9] Mohamed Bouadi, Pratinav Seth, Aditya Tanna, and Vinay Kumar Sankarapu. Orion-msp: Multi-scale sparse attention for tabular in-context learning, 2025. [10] Mohamed Bouadi, Pratinav Seth, Aditya Tanna, and Vinay Kumar Sankarapu. Orion-bix: Bi-axial attention for tabular in-context learning, 2025. [11] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. closer look at deep learning methods on tabular datasets. arXiv preprint arXiv:2407.00956, 2024. [12] Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael Mantovani, Jan van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. arXiv preprint arXiv:1708.03731, 2017. [13] Vadim Borisov, Tobias Leemann, Kathrin SeÃler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: survey. IEEE Transactions on Neural Networks and Learning Systems, 35(6):74997519, 2024. [14] Shriyank Somvanshi, Subasish Das, Syed Aaqib Javed, Gian Antariksa, and Ahmed Hossain. survey on deep tabular learning. arXiv preprint arXiv:2410.12034, 2024. [15] Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Hamidreza Kamkari, Alex Labach, Jesse C. Cresswell, Keyvan Golestan, Guangwei Yu, Maksims Volkovs, and Anthony L. Caterini. Tabdpt: Scaling tabular foundation models. CoRR, abs/2410.18164, 2024. [16] Jun-Peng Jiang, Si-Yang Liu, Hao-Run Cai, Qile Zhou, and Han-Jia Ye. Representation learning for tabular data: comprehensive survey. arXiv preprint arXiv:2504.16109, 2025. [17] Gilbert Badaro, Mohammed Saeed, and Paolo Papotti. Transformers for tabular data representation: survey of models and applications. volume 11, pages 227249, 2023. [18] Ivan Rubachev, Akim Kotelnikov, Nikolay Kartashev, and Artem Babenko. On finetuning tabular foundation models. arXiv preprint arXiv:2506.08982, 2025. [19] Boris van Breugel and Mihaela van der SchÃ¤r. Position: Why tabular foundation models should be research priority. arXiv preprint arXiv:2405.01147, 2024. [20] Cheng-Te Li, Yu-Che Tsai, Chih-Yao Chen, and Jay Chiehen Liao. Graph neural networks for tabular data learning: survey with taxonomy and directions. arXiv preprint arXiv:2401.02143, 2024. [21] Shi Bin Hoo, Samuel MÃ¼ller, David Salinas, and Frank Hutter. From tables to time: How tabpfn-v2 outperforms specialized time series forecasting models. 2025. [22] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505, 2020. TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models [23] Manuj Joseph. Pytorch tabular: framework for deep learning with tabular data. arXiv preprint arXiv:2104.13638, 2021. [24] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML), pages 11261135. PMLR, 2017. [25] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146, 2018. [26] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2021. [27] Haonan Liu et al. Parameter-efficient fine-tuning: comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. Note: Verify exact authors and details. [28] Olga Razuvayevskaya et al. Empirical analysis of parameter-efficient fine-tuning methods. arXiv preprint, 2024. Note: Verify exact title, authors, and arXiv number. [29] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 13211330. PMLR, 2017. [30] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and machine learning. fairmlbook. org, 2019. [31] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. Advances in neural information processing systems, 29, 2016. [32] Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Weinberger. On fairness and calibration. Advances in neural information processing systems, 30, 2017. [33] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Vishak Prasad, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In Advances in Neural Information Processing Systems, 2023. 29 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "10 Appendix A: Default Hyperparameters",
            "content": "This appendix provides detailed default hyperparameters for all tuning strategies supported in TabTune. These values are optimized for general-purpose use and can be overridden via the tuning_params configuration dictionary."
        },
        {
            "title": "10.1 Zero-Shot Inference",
            "content": "Zero-shot inference uses pretrained model weights without any parameter updates. No training hyperparameters are required for this strategy. The model receives training samples as context during inference and performs predictions through forward passes alone. However, each model has specific inference parameters that control prediction behavior: Parameter Descriptions: n_estimators / n_ensembles: Number of ensemble runs with different configurations for robust predictions softmax_temperature / temperature: Scaling factor for logits before softmax (lower values yield sharper predictions) context_size: Number of training samples used as context for each test prediction average_before_softmax: Whether to average logits or probabilities across ensemble runs safety_factor: Memory safety factor for batch size estimation (TabICL/OrionMSP/OrionBiX) offload: Whether to offload intermediate computations to CPU for memory efficiency use_amp: Automatic mixed precision for faster inference with reduced memory Table 10 Default inference hyperparameters for zero-shot inference across models. Values shown are default settings used when no custom parameters are specified. All parameters can be customized via model-specific configuration dictionaries."
        },
        {
            "title": "TabPFN\nTabICL\nOrionMSP\nOrionBiX\nTabDPT\nMitra\nContextTab",
            "content": "n_estimators=8, softmax_temperature=0.9, average_before_softmax=False Inference config: min_batch_size=1, safety_factor=0.8, offload=auto (COL), False (ROW/ICL), use_amp=True Inference config: min_batch_size=1, safety_factor=0.8, offload=auto (COL), False (ROW/ICL), use_amp=True Inference config: min_batch_size=1, safety_factor=0.8, offload=auto (COL), False (ROW/ICL), use_amp=True n_ensembles=8, temperature=0.8, context_size=512, permute_classes=True d_model=64, num_heads=4, num_layers=2, use_synthetic_prior=True (No specific inference parameters)"
        },
        {
            "title": "10.2 Supervised Fine-Tuning (SFT)",
            "content": "Supervised Fine-Tuning (SFT) is standard adaptation approach where the entire pretrained model is updated using gradient descent on task-specific labeled data. Unlike meta-learning, SFT treats the target dataset as single supervised learning task, training the model end-to-end to minimize cross-entropy loss over multiple epochs. Key Characteristics: Training Paradigm: Standard supervised learning with full-batch or mini-batch optimization Parameter Updates: All model parameters are updated during training Objective: Minimize classification loss on the target tasks training set Use Cases: Optimal for large datasets (>10K samples) where task-specific optimization is beneficial Advantages: Can achieve highest task-specific accuracy, full model capacity utilization Considerations: Higher memory requirements, risk of overfitting on small datasets, may reduce generalization to new tasks General Hyperparameter Guidelines: Learning Rate: Typically ranges from 1e-5 to 1e-4, with lower rates preserving pretrained knowledge Epochs: 3-25 epochs depending on dataset size and convergence behavior Batch Size: 16-128, adjusted based on dataset size and memory constraints Regularization: Weight decay (1e-4 to 1e-5) and learning rate warmup help stabilize training Optimizer: Adam or AdamW are standard choices for tabular foundation models 30 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 11 Default hyperparameters for Supervised Fine-Tuning (SFT) across models. SFT updates all model parameters using gradient descent on task-specific labeled data. All parameters can be overridden via the tuning_params configuration dictionary. Optimal for large datasets (>10K samples)."
        },
        {
            "title": "TabPFN\nTabICL\nOrionMSP\nOrionBiX\nMitra\nTabDPT\nContextTab",
            "content": "epochs=25, learning_rate=1e-5, max_episode_size=len(X), query_set_ratio=0.3, weight_decay=1e-4, optimizer=AdamW epochs=5, learning_rate=1e-5, batch_size=16, optimizer=Adam epochs=5, learning_rate=1e-5, batch_size=16, optimizer=Adam epochs=5, learning_rate=1e-5, batch_size=16, optimizer=Adam epochs=5, learning_rate=1e-5, batch_size=128, weight_decay=1e-4, warmup_epochs=1, optimizer=Adam epochs=5, learning_rate=2e-5, batch_size=32, weight_decay=1e-4, warmup_epochs=1, optimizer=Adam epochs=5, learning_rate=1e-4, batch_size=128, optimizer=Adam"
        },
        {
            "title": "10.3 Meta-Learning Fine-Tuning",
            "content": "Meta-learning fine-tuning, also known as episodic fine-tuning or few-shot adaptation, mimics the few-shot learning scenario during training. The model is trained using episodic data sampling, where each episode consists of support set (context) and query set (target predictions). This approach trains the model to quickly adapt to new tasks from limited examples, improving generalization. Key Characteristics: Training Paradigm: Episodic learning with support-query splits Episode Structure: Each training step samples support and query sets from the target dataset Objective: Learn to make accurate predictions on query sets given support set context Use Cases: Ideal for small to medium datasets (<10K samples), maintains generalization to new tasks Advantages: Better generalization, mimics actual inference scenarios for in-context learning models Considerations: Requires careful episode sampling, support/query size tuning, more epochs needed General Hyperparameter Guidelines: Learning Rate: Typically lower than SFT (2e-6 to 1e-5) to preserve pretrained representations Support Size: 32-512 samples per episode, depending on dataset size and model architecture Query Size: Usually 20-50% of support size, determines episode difficulty Episodes per Epoch: 100-1000 episodes ensure diverse task exposure Epochs: 3-5 epochs often sufficient due to episodic diversity Batch Size: 1-8 episodes per batch for memory-efficient training Episode Filtering: Episodes where query classes are absent from the support set are automatically filtered to maintain class consistency. This ensures the model always has context examples for all classes it must predict. Table 12 Default hyperparameters for Meta-Learning Fine-Tuning across models. Meta-learning uses episodic training with support-query splits. Episodes where query classes are absent from support set are automatically filtered. All parameters can be customized via tuning_params. Ideal for small to medium datasets (<10K samples)."
        },
        {
            "title": "TabPFN\nTabICL\nOrionMSP\nOrionBiX\nMitra\nTabDPT",
            "content": "epochs=3, learning_rate=1e-5, batch_size=256, optimizer=AdamW epochs=5, learning_rate=2e-6, support_size=48, query_size=32, n_episodes=1000, optimizer=Adam epochs=5, learning_rate=2e-6, support_size=48, query_size=32, n_episodes=1000, optimizer=Adam epochs=5, learning_rate=2e-6, support_size=48, query_size=32, n_episodes=1000, optimizer=Adam epochs=3, learning_rate=1e-5, batch_size=4, support_size=128, query_size=128, steps_per_epoch=50, optimizer=Adam epochs=5, learning_rate=1e-5, batch_size=8, support_size=512, query_size=256, steps_per_epoch=100, optimizer=Adam"
        },
        {
            "title": "10.4 Parameter-Efficient Fine-Tuning (PEFT)",
            "content": "Parameter-Efficient Fine-Tuning (PEFT) reduces memory and computational requirements by updating only small subset of model parameters through low-rank adaptation techniques. PEFT can be combined with either SFT or meta-learning strategies, providing efficient alternatives to full parameter updates. 31 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Key Characteristics: Training Paradigm: Updates only adapter parameters, base model weights remain frozen Memory Efficiency: Reduces fine-tuning memory by 60-80% compared to full fine-tuning Parameter Updates: Only low-rank adapter matrices are trained, not the full model Use Cases: Memory-constrained environments, rapid experimentation, multiple task adaptations Advantages: Fast training, lower memory footprint, easy task switching (swap adapters) Considerations: May achieve slightly lower accuracy than full fine-tuning, model compatibility varies Low-Rank Adaptation (LoRA) Mechanism: LoRA introduces trainable low-rank matrices that approximate full weight updates. For linear layer with weight matrix Rmn, LoRA adds trainable matrices Rmr and Rrn where min(m, n) is the rank. The forward pass becomes: = + Î± (BA)x where Î± is scaling factor and BA is the low-rank update. Only and are updated during training, while remains frozen. General Hyperparameter Guidelines: Rank (r): Controls adapter capacity; typically 4-16, with 8 providing good balance LoRA Alpha (Î±): Scaling factor; typically Î± = 2r (e.g., Î± = 16 for = 8) LoRA Dropout: 0.05-0.2 for regularization, prevents adapter overfitting Target Modules: Attention projections, feed-forward layers, and encoders are common targets Base Strategy: Uses same epochs, learning rate, and batch size as SFT or meta-learning PEFT-SFT vs PEFT Meta-Learning: PEFT can be applied to either strategy: PEFT-SFT: Combines full-batch supervised learning with LoRA adapters; best for large datasets PEFT Meta-Learning: Combines episodic training with LoRA adapters; maintains generalization benefits PEFT-SFT: Uses SFT hyperparameters  (Table 11)  with LoRA adapters applied. Compatible with: [TabICL, OrionMSP, OrionBiX, TabDPT, Mitra]. PEFT Meta-Learning: Uses Meta-Learning hyperparameters  (Table 12)  with LoRA adapters applied. Compatible with: [TabICL, OrionMSP, OrionBiX, TabDPT, Mitra]. Note: TabPFN and ContextTab have limited or experimental PEFT support. TabPFN PEFT is unstable due to architectural constraints with batched inference, and ContextTab PEFT may conflict with its embedding pipeline. These models automatically fall back to base fine-tuning when PEFT is requested. Table 13 Default LoRA (Low-Rank Adaptation) configuration for Parameter-Efficient Fine-Tuning (PEFT). PEFT reduces memory by 60-80% compared to full fine-tuning. These values apply to both PEFT-SFT and PEFT Meta-Learning strategies. All parameters can be customized via peft_config dictionary."
        },
        {
            "title": "Value",
            "content": "rank (r) LoRA alpha (Î±) LoRA dropout Target modules Model-specific (attention projections, encoders, transformers) 8 16 0."
        },
        {
            "title": "10.5 Additional Notes",
            "content": "All training strategies use CrossEntropyLoss for classification tasks. For meta-learning, episodes with query classes absent from support sets are automatically filtered to maintain consistency. Sampling strategies follow the guidelines in Section 4.2.3. Gradient clipping and learning rate schedulers (e.g., warmup) are applied where specified in the configuration above. 32 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "11.1 Hardware and Software Configuration",
            "content": "All experiments were executed on NVIDIA L40S GPUs. For some experiments we get OOM issues for those cases we use NVIDIA H200 GPUs."
        },
        {
            "title": "11.2 Data Splitting and Reproducibility",
            "content": "All benchmarks follow their respective standardized dataset splits. Random seeds are fixed for reproducibility across experiments. Cross-validation is performed where specified by the benchmark protocols (TALENT uses auto-discovered splits, OpenML-CC18 uses predefined splits, TabZilla follows its standard evaluation protocol)."
        },
        {
            "title": "11.3 Dataset Statistics",
            "content": "The following subsections provide comprehensive statistics for all datasets used in our evaluation across different benchmark suites. Each table includes dataset names, domains, sample counts, feature counts, number of classes, and task types (binary classification or multiclass classification). OpenML-CC18 Benchmark Datasets Table 14 lists all datasets from the OpenML-CC18 benchmark suite used in our evaluation."
        },
        {
            "title": "TALENT Benchmark Datasets",
            "content": "Table 15 lists all datasets from the TALENT benchmark suite used in our evaluation."
        },
        {
            "title": "TabZilla Benchmark Datasets",
            "content": "Table 16 lists all datasets from the TabZilla benchmark suite. TabZilla uses OpenML dataset IDs, and these datasets are specifically selected for evaluating neural network performance on tabular data."
        },
        {
            "title": "Fairness Evaluation Datasets",
            "content": "Fairness evaluation requires datasets with sensitive features (e.g., demographic attributes). Table 17 lists commonly used datasets for fairness evaluation. These datasets contain demographic or other sensitive attributes that enable fairness metric computation (Statistical Parity Difference, Equalized Odds Difference, Equalized Opportunity Difference)."
        },
        {
            "title": "11.4 Evaluation Metrics",
            "content": "All experiments report the following metrics: Performance: Accuracy, AUC-ROC, weighted F1-score Calibration: Expected Calibration Error (ECE), Maximum Calibration Error (MCE), Brier score Fairness: Statistical Parity Difference (SPD), Equalized Odds Difference (EOD), Equalized Opportunity Difference (EOpD) All metrics are computed using standard implementations and follow established benchmark protocols for each suite."
        },
        {
            "title": "OpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML",
            "content": "OpenML-ID-3 OpenML-ID-6 OpenML-ID-11 OpenML-ID-12 OpenML-ID-14 OpenML-ID-15 Table 14 OpenML-CC18 benchmark datasets (72 datasets)."
        },
        {
            "title": "Other\nHandwriting\nOther\nOther\nOther\nHealthcare",
            "content": "3196 20000 625 2000 2000 699 37 17 5 217 77 10 33 binclass"
        },
        {
            "title": "2\nYes\n26 multiclass No\n3 multiclass Yes\n10 multiclass Yes\n10 multiclass Yes\nYes",
            "content": "binclass"
        },
        {
            "title": "Continued on next page",
            "content": "TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "Table 14 Details of OpenML-CC18 benchmark datasets."
        },
        {
            "title": "OpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML",
            "content": "OpenML-ID-16 OpenML-ID-18 OpenML-ID-22 OpenML-ID-23 OpenML-ID-28 OpenML-ID-29 OpenML-ID-31 OpenML-ID-32 OpenML-ID-37 OpenML-ID-38 OpenML-ID-44 OpenML-ID-46 OpenML-ID-50 OpenML-ID-54 OpenML-ID-151 OpenML-ID-182 OpenML-ID-188 OpenML-ID-300 OpenML-ID-307 OpenML-ID-458 OpenML-ID-469 OpenML-ID-554 OpenML-ID-1049 OpenML-ID-1050 OpenML-ID-1053 OpenML-ID-1063 OpenML-ID-1067 OpenML-ID-1068 OpenML-ID-1461 OpenML-ID-1462 OpenML-ID-1464 OpenML-ID-1468 OpenML-ID-1475 OpenML-ID-1478 OpenML-ID-1480 OpenML-ID-1485 OpenML-ID-1486 OpenML-ID-1487 OpenML-ID-1489 OpenML-ID-1494 OpenML-ID-1497 OpenML-ID-1501 OpenML-ID-1510 OpenML-ID-1590 OpenML-ID-4134 OpenML-ID-4534 OpenML-ID-4538 OpenML-ID-6332 OpenML-ID-23381 OpenML-ID-23517 OpenML-ID-40499 OpenML-ID-40668 OpenML-ID-40670 OpenML-ID-40701 OpenML-ID-40923 OpenML-ID-40927 OpenML-ID-40966 OpenML-ID-40975 OpenML-ID-40978 65 7 48 10 65 16 21 17 9 30 58 61 10 19 9 37 20 618 13 71 5 785 38 38 22 22 22 22 17 5 5 857 52 562 11 501 119 73 6 42 25 257 31 15 1777 31 33 40 13 22 41 43 181 21 1025 3073 82 7 1559 2000 2000 2000 1473 5620 690 1000 10992 768 3772 4601 3190 958 846 45312 6430 736 7797 990 841 797 70000 1458 1563 10885 522 2109 1109 45211 1372 748 1080 6118 10299 583 2600 34465 2534 5404 1055 5456 1593 569 48842 3751 11055 9873 540 500 96320 5500 67557 3186 5000 92000 60000 1080"
        },
        {
            "title": "Other\nOther\nOther\nHealthcare\nHandwriting\nFinance\nFinance\nHandwriting\nHealthcare\nHealthcare\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nHealthcare\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nFinance\nFinance\nHealthcare\nOther\nOther\nOther\nHealthcare\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nRetail\nOther\nOther\nGames\nOther\nOther\nOther\nHandwriting\nOther\nOther\nOther",
            "content": "34 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 3 multiclass Yes 10 multiclass Yes Yes 2 2 Yes 10 multiclass Yes Yes 2 Yes 2 2 Yes 3 multiclass Yes Yes 2 4 multiclass Yes 2 Yes 6 multiclass Yes 5 multiclass Yes 26 multiclass No 11 multiclass No 4 multiclass Yes 6 multiclass Yes 10 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 Yes 2 2 Yes 9 multiclass Yes 6 multiclass Yes 6 multiclass Yes Yes 2 Yes 2 No 2 Yes 2 Yes 2 2 Yes 4 multiclass Yes 10 multiclass Yes Yes 2 Yes 2 No 2 2 Yes 5 multiclass Yes Yes 2 Yes 2 2 No 11 multiclass No 3 multiclass No 3 multiclass Yes 2 Yes 46 multiclass No 10 multiclass No 8 multiclass No 4 multiclass Yes Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "Continued on next page",
            "content": "TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "Table 14 Details of OpenML-CC18 benchmark datasets."
        },
        {
            "title": "OpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML\nOpenML",
            "content": "OpenML-ID-40979 OpenML-ID-40982 OpenML-ID-40983 OpenML-ID-40984 OpenML-ID-40994 OpenML-ID-40996 OpenML-ID-"
        },
        {
            "title": "Other\nOther\nOther\nOther\nOther\nOther\nGames",
            "content": "2000 1941 4839 2310 540 70000 44819 241 28 6 20 21 785 7 binclass 10 multiclass Yes 7 multiclass Yes Yes 2 7 multiclass Yes 2 Yes 10 multiclass No 3 multiclass Yes binclass Table 15 TALENT benchmark datasets (auto-discovered, multiple domains)."
        },
        {
            "title": "TALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT",
            "content": "ASP-POTASSCO-class Amazon_employee_access BLE_RSSI__Indoor_localization BNG(breast-w) BNG(cmc) BNG(tic-tac-toe) Bank_Customer_Churn Basketball_c CDC_Diabetes_Health California-Housing-Class Cardiovascular-Disease Click_prediction_small Credit_c Customer_Personality_Analysis DataScience_Kiva_Crowdfunding Diabetic_Retinopathy_Debrecen E-CommereShippingData Employee FICO-HELOC-cleaned FOREX_audcad-day-High FOREX_audcad-hour-High FOREX_audchf-day-High FOREX_audjpy-day-High FOREX_audjpy-hour-High FOREX_audsgd-hour-High FOREX_audusd-hour-High FOREX_cadjpy-day-High FOREX_cadjpy-hour-High Firm-Teacher_Clave-Direction Fitness_Club_c GAMETES_Epistasis_2-Way GAMETES_Heterogeneity Gender_Gap_in_Spanish GesturePhaseSegmentation HR_Analytics_Job_Change IBM_HR_Analytics INNHotelsGroup Indian_pines JapaneseVowels KDDCup09_upselling MIC MagicTelescope Marketing_Campaign Mobile_Price_Classification National_Health_and_Nutrition PhishingWebsites 141 7 3 9 9 9 10 11 21 8 11 3 22 24 11 19 10 8 23 10 10 10 10 10 10 10 10 10 16 6 20 20 13 32 13 31 17 220 14 49 104 9 27 20 7 30 1294 32769 9984 39366 55296 39366 10000 1340 253680 20640 70000 39948 100000 2240 671205 1151 10999 4653 9871 1834 43825 1833 1832 43825 43825 43825 1834 43825 10800 1500 1600 1600 4746 9873 19158 1470 36275 9144 9961 5128 1649 19020 2240 2000"
        },
        {
            "title": "Other\nOther\nOther\nHealthcare\nOther\nOther\nFinance\nRetail\nHealthcare\nOther\nHealthcare\nOther\nFinance\nRetail\nOther\nHealthcare\nOther\nOther\nOther\nFinance\nFinance\nFinance\nFinance\nFinance\nFinance\nFinance\nFinance\nFinance\nOther\nOther\nGames\nGames\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nFinance\nTelcom\nHealthcare\nOther",
            "content": "35 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 11 multiclass Yes 2 Yes 3 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 No 2 Yes 2 2 Yes 3 multiclass Yes 2 Yes 4 multiclass No Yes 2 Yes 2 Yes 2 Yes 2 No 2 No 2 No 2 No 2 No 2 No 2 No 2 No 2 2 No 4 multiclass Yes Yes 2 Yes 2 2 Yes 3 multiclass Yes 5 multiclass Yes Yes 2 Yes 2 2 Yes 8 multiclass Yes 9 multiclass Yes Yes 2 Yes 2 Yes 2 2 Yes 4 multiclass Yes Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "Continued on next page",
            "content": "TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "Table 15 Details of TALENT benchmark datasets."
        },
        {
            "title": "TALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT",
            "content": "PieChart3 Pima_Indians_Diabetes PizzaCutter3 Pumpkin_Seeds QSAR_biodegradation Rain_in_Australia SDSS17 Satellite Smoking_and_Drinking Telecom_Churn_Dataset UJI_Pen_Characters Water_Quality_and_Potability Wilt abalone accelerometer ada ada_agnostic ada_prior airlines_seed_0_nrows allbp allrep analcatdata_authorship artificial-characters autoUniv-au4-2500 autoUniv-au7-1100 bank banknote_authentication baseball car-evaluation churn cmc company_bankruptcy_prediction compass contraceptive_method_choice credit customer_satisfaction_in_airline dabetes_130-us_hospitals default_of_credit_card_clients delta_ailerons dis dna drug_consumption dry_bean_dataset eeg-eye-state electricity estimation_of_obesity_levels eye_movements eye_movements_bin first-order-theorem-proving gas-drift gina_agnostic golf_play_dataset_extended heloc hill-valley house_16H htru ibm-employee-performance in_vehicle_coupon_recos internet_firewall 37 8 37 12 41 18 12 36 23 17 80 8 5 8 4 48 48 14 7 29 29 69 7 100 12 16 4 16 21 20 9 95 17 9 10 21 20 23 5 29 180 12 16 14 8 16 27 20 51 128 970 9 22 100 16 8 30 21 7 1077 768 1043 2500 1054 145460 100000 5100 991346 3333 1364 3276 4821 4177 153004 4147 4562 4562 2000 3772 3772 841 10218 2500 1100 45211 1372 1340 1728 5000 1473 6819 16644 1473 16714 129880 101766 30000 7129 3772 3186 1884 13611 14980 45312 2111 10936 7608 6118 13910 3468 1095 10000 1212 13488 17898 1470"
        },
        {
            "title": "Other\nHealthcare\nOther\nOther\nHealthcare\nOther\nOther\nOther\nOther\nTelcom\nOther\nManufacturing\nOther\nOther\nOther\nOther\nOther\nOther\nTelcom\nOther\nOther\nOther\nOther\nOther\nOther\nFinance\nFinance\nOther\nOther\nFinance\nOther\nFinance\nOther\nOther\nFinance\nRetail\nHealthcare\nFinance\nOther\nOther\nHealthcare\nHealthcare\nOther\nOther\nManufacturing\nOther\nHealthcare\nHealthcare\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nRetail\nOther",
            "content": "36 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass Yes 2 Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass No 3 multiclass Yes Yes 2 No 2 Yes 2 35 multiclass Yes Yes 2 Yes 2 3 multiclass Yes 4 multiclass No Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass Yes 4 multiclass Yes 4 multiclass Yes 10 multiclass Yes 3 multiclass Yes 5 multiclass Yes Yes 2 2 Yes 3 multiclass Yes 4 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 Yes 2 3 multiclass Yes Yes 2 No 2 Yes 2 Yes 2 Yes 2 2 Yes 3 multiclass Yes 7 multiclass Yes 7 multiclass Yes Yes 2 2 Yes 7 multiclass Yes 3 multiclass Yes Yes 2 6 multiclass Yes 6 multiclass Yes Yes 2 Yes 2 Yes 2 No 2 Yes 2 Yes 2 Yes 2 Yes 2 4 multiclass Yes binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "Continued on next page",
            "content": "TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "Table 15 Details of TALENT benchmark datasets."
        },
        {
            "title": "TALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT",
            "content": "internet_usage jm1 jungle_chess_2pcs_raw kc1 kdd_ipums_la_97-small kr-vs-k kropt led24 led7 letter madeline mammography maternal_health_risk mfeat-factors mfeat-fourier mfeat-karhunen mfeat-morphological mfeat-pixel mfeat-zernike mice_protein_expression microaggregation2 mobile_c36_oversampling mozilla4 naticusdroid+android national-longitudinal-surveybinary okcupid_stem one-hundred-plants-margin one-hundred-plants-shape one-hundred-plants-texture online_shoppers optdigits ozone-level-8hr page-blocks pc1 pc3 pc4 pendigits phoneme pol predict_students_dropout qsar rice_cammeo_and_osmancik ringnorm rl satimage segment seismic+bumps semeion shuttle spambase splice sports_articles_for_objectivity statlog steel_plates_faults sylvine taiwanese_bankruptcy telco-customer-churn texture 70 21 6 21 20 6 6 24 7 15 259 6 6 216 76 64 6 240 47 75 20 6 4 86 16 13 64 64 64 14 64 72 10 21 37 37 16 5 26 34 40 7 20 12 36 17 18 256 9 57 60 59 20 27 20 95 18 40 10108 10885 44819 2109 5188 28056 28056 3200 3200 20000 3140 11183 1014 2000 2000 2000 2000 2000 2000 1080 20000 51760 15545 29332 4908 26677 1600 1600 1599 12330 5620 2534 5473 1109 1563 1458 10992 5404 10082 4424 1055 3810 7400 4970 6430 2310 2584 1593 58000 4601 3190 1000 1000 1941 5124 6819"
        },
        {
            "title": "Other\nOther\nOther\nOther\nRetail\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nOther\nFinance\nTelcom\nOther",
            "content": "37 binclass binclass binclass binclass binclass 46 multiclass Yes 2 No 3 multiclass Yes No 2 Yes 2 18 multiclass Yes 18 multiclass Yes 10 multiclass Yes 10 multiclass Yes 26 multiclass Yes Yes 2 2 Yes 3 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 10 multiclass Yes 8 multiclass Yes 5 multiclass Yes Yes 2 Yes 2 Yes 2 Yes 2 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass 3 multiclass Yes 100 multiclass Yes 100 multiclass Yes 100 multiclass Yes 2 No binclass 10 multiclass Yes Yes 2 5 multiclass Yes No 2 No 2 2 No 10 multiclass Yes Yes 2 2 Yes 3 multiclass Yes Yes 2 Yes 2 Yes 2 2 No 6 multiclass Yes 7 multiclass Yes Yes 2 10 multiclass No 7 multiclass Yes 2 Yes 3 multiclass Yes Yes 2 2 Yes 7 multiclass Yes Yes 2 Yes 2 Yes 2 11 multiclass Yes binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "Continued on next page",
            "content": "TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "Table 15 Details of TALENT benchmark datasets."
        },
        {
            "title": "TALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT\nTALENT",
            "content": "thyroid thyroid-ann thyroid-dis turiye_student_evaluation twonorm vehicle volkert walking-activity wall-robot-navigation water_quality waveform-5000 waveform_database_generator waveform_database_generator-v2 website_phishing wine wine-quality-red wine-quality-white yeast"
        },
        {
            "title": "Healthcare\nHealthcare\nHealthcare\nOther\nOther\nOther\nOther\nOther\nOther\nManufacturing\nOther\nOther\nOther\nOther\nManufacturing\nManufacturing\nManufacturing\nOther",
            "content": "7200 3772 2800 5820 7400 846 58310 149332 5456 7996 5000 4999 5000 1353 2554 1599 4898 1484 21 21 26 32 20 18 180 4 24 20 40 21 21 9 4 4 11 8 binclass 3 multiclass Yes 3 multiclass Yes 5 multiclass Yes 5 multiclass Yes 2 Yes 4 multiclass Yes 10 multiclass Yes 22 multiclass No 4 multiclass Yes 2 Yes 3 multiclass Yes 3 multiclass Yes 3 multiclass Yes 3 multiclass Yes 2 No 6 multiclass Yes 7 multiclass Yes 10 multiclass Yes binclass binclass Table 16 TabZilla benchmark datasets (36 datasets via OpenML)."
        },
        {
            "title": "TabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla\nTabZilla",
            "content": "OpenML-ID-999 OpenML-ID-10 OpenML-ID-11 OpenML-ID-14 OpenML-ID-22 OpenML-ID-29 OpenML-ID-27 OpenML-ID-31 OpenML-ID-46 OpenML-ID-54 OpenML-ID-333 OpenML-ID-1067 OpenML-ID-1468 OpenML-ID-1494 OpenML-ID-43973 OpenML-ID-1043 OpenML-ID-43945 OpenML-ID-1486 OpenML-ID-42825 OpenML-ID-4538 OpenML-ID-23512 OpenML-ID-4134 OpenML-ID-470 OpenML-ID-1493 OpenML-ID-1459 OpenML-ID-41027 OpenML-ID-40981 OpenML-ID-934 OpenML-ID-1565 OpenML-ID-41150 OpenML-ID-41159 OpenML-ID-846 OpenML-ID-1169 OpenML-ID-41147 OpenML-ID-"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "226 148 625 2000 2000 690 368 1000 3190 846 556 2109 1080 1055 3172 4562 38474 34465 8378 9873 98050 3751 672 1599 10218 44819 690 1156 294 130064 20000 16599 539383 425240 2984 70 19 5 77 48 16 23 21 61 19 7 22 857 42 6 49 9 119 123 33 29 1777 10 65 8 7 15 6 14 51 4297 19 8 79 145 binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "2\nYes\n4 multiclass Yes\n3 multiclass Yes\n10 multiclass Yes\n10 multiclass Yes\nYes\n2\nYes\n2\n2\nYes\n3 multiclass Yes\n4 multiclass Yes\nYes\n2\n2\nYes\n9 multiclass Yes\nYes\n2\nYes\n2\nYes\n2\nYes\n2\nNo\n2\nNo\nâ\n5 multiclass Yes\nNo\n2\nbinclass\nYes\n2\nbinclass\nNo\n2\nbinclass\n100 multiclass Yes\n10 multiclass Yes\n3 multiclass Yes\nYes\n2\n2\nYes\n5 multiclass Yes\nNo\n2\nNo\n2\nYes\n2\nNo\n2\nYes\n2\nYes\n2",
            "content": "binclass binclass binclass binclass binclass binclass binclass binclass"
        },
        {
            "title": "Continued on next page",
            "content": "38 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models Table 16 Details of TabZilla benchmark datasets."
        },
        {
            "title": "TabZilla",
            "content": "OpenML-ID-"
        },
        {
            "title": "Task Type Used In Experimentation",
            "content": "1025009 11 10 multiclass No Note: Some TabZilla dataset statistics are to be completed. TabZilla datasets are accessed via OpenML and may include both dataset IDs and task IDs (TabZilla automatically handles task-to-dataset conversion). Table 17 Datasets used for fairness evaluation (contain demographic/sensitive attributes)."
        },
        {
            "title": "UCI\nOpenML",
            "content": "Adult Census Income German Credit (ID-31) COMPAS Recidivism Default of Credit Card Clients (Taiwan) Student Performance in Exams (Portuguese)"
        },
        {
            "title": "Legal\nFinance",
            "content": "48842 1000"
        },
        {
            "title": "Education",
            "content": ""
        },
        {
            "title": "Legal\nOther",
            "content": ""
        },
        {
            "title": "Offical\nHMDA\nAPI\nKaggle",
            "content": "14 21 53 23 33 12 31 binclass Race, Sex, Age binclass Age, Personal-statussex binclass Race, Sex binclass Sex, Education, Marital Status binclass Gender, Race/Ethnicity, Parental Level of Education binclass Race, Sex binclass Gender, Age, MaritalStatus, RelationshipSatisfaction Home Mortgage Disclosure Act (NY 2023)"
        },
        {
            "title": "Finance",
            "content": "32000 20+ multiclass Race ,Ethnicity, Sex, Age NHANES Demographics (201314)"
        },
        {
            "title": "Health",
            "content": "5000 10+ multiclass Gender, Age, Race, Ethnicity Note: Fairness evaluation is performed on datasets where sensitive features are explicitly identified. The sensitive features column indicates demographic or protected attributes available in each dataset. Fairness metrics (SPD, EOD, EOpD) measure disparities in model predictions across different groups defined by these sensitive features."
        },
        {
            "title": "12 Appendix C: Glossary of Key Terms",
            "content": "This glossary provides concise definitions of technical terms used throughout this paper. For detailed explanations, refer to the relevant sections indicated. 39 TabTune: Unified Library for Inference and Fine-Tuning Tabular Foundation Models"
        },
        {
            "title": "Brier Score",
            "content": "Table 18 Glossary of key technical terms."
        },
        {
            "title": "Definition",
            "content": "Metric quantifying both calibration and refinement by penalizing squared deviations between predicted probabilities and binary outcomes; lower is better (see Section 6 for details). Equalized Odds Difference (EOD) Measure of disparities in both true positive and false positive rates across demographic groups; more stringent than statistical parity, requiring consistent error profiles (zero indicates perfect equity). Equalized Opportunity Difference (EOpD) Focus on true positive rate parity across demographic groups; ensures qualified individuals receive equal treatment regardless of group membership (zero indicates perfect equity)."
        },
        {
            "title": "Episodic Training",
            "content": "Meta-learning approach where each training step samples disjoint support and query sets from training data; mimics few-shot learning scenarios and preserves in-context generalization (see Section 2). Expected Calibration Error (ECE) Average deviation between predicted confidence and observed accuracy across binned predictions; lower values indicate better calibration alignment (see Section 6). Low-Rank Adaptation (LoRA) Parameter-efficient fine-tuning technique that adds trainable low-rank matrices to approximate full weight updates; only adapter matrices are updated, base weights remain frozen (see Section 2). Maximum Calibration Error (MCE) Worst-case calibration gap across all prediction bins; identifies regions of systematic miscalibration and extreme confidence misalignment (see Section 6). Meta-Learning (Episodic FineTuning) Training strategy using support-query splits that mimics inference-time environment; preserves in-context generalization while enabling task-specific adaptation (see Section 2). Parameter-Efficient Tuning (PEFT) FineAdaptation methods that update only subset of model parameters, reducing computational costs by 6080% while preserving performance; includes LoRA and similar techniques (see Section 2). Statistical Parity Difference (SPD) Absolute difference in positive prediction rates across demographic groups; tests demographic parity, where zero indicates perfect parity in prediction distribution (see Section 6). Supervised Fine-Tuning (SFT) Zero-Shot Inference Full parameter updates on labeled data using standard gradient descent; updates all model parameters, computationally intensive but can achieve highest taskspecific accuracy (see Section 2). Prediction using pretrained model weights without parameter updates; models receive training samples as context during inference, performing in-context learning through forward passes alone (see Section 2)."
        }
    ],
    "affiliations": [
        "Lexsi Labs, India & France"
    ]
}