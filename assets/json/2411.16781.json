{
    "paper_title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
    "authors": [
        "Yiheng Li",
        "Ruibing Hou",
        "Hong Chang",
        "Shiguang Shan",
        "Xilin Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human pose plays a crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only a single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, a framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply a pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within a unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with a mixture of visual encoders, among them a pose-specific visual encoder. Benefiting from a unified learning strategy, UniPose effectively transfers knowledge across different pose-relevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building a general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPose's competitive and even superior performance across various pose-relevant tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 1 8 7 6 1 . 1 1 4 2 : r UniPose: Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing Yiheng Li1,2, Ruibing Hou1* , Hong Chang1,2, Shiguang Shan1,2 , Xilin Chen1,2 1Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China 2University of Chinese Academy of Sciences, China yiheng.li@vipl.ict.ac.cn,{houruibing,changhong,sgshan,xlchen}@ict.ac.cn Figure 1. UniPose can handle pose comprehension, generation and editing tasks under different instructions within unified framework."
        },
        {
            "title": "Abstract",
            "content": "Human pose plays crucial role in the digital age. While recent works have achieved impressive progress in understanding and generating human poses, they often support only single modality of control signals and operate in isolation, limiting their application in real-world scenarios. This paper presents UniPose, framework employing Large Language Models (LLMs) to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL poses. Specifically, we apply pose tokenizer to convert 3D poses into discrete pose tokens, enabling seamless integration into the LLM within unified vocabulary. To further enhance the fine-grained pose perception capabilities, we facilitate UniPose with mixture of visual encoders, among them pose-specific visual encoder. Benefiting from unified learning strategy, UniPose effectively transfers knowledge across different poserelevant tasks, adapts to unseen tasks, and exhibits extended capabilities. This work serves as the first attempt at building general-purpose framework for pose comprehension, generation, and editing. Extensive experiments highlight UniPoses competitive and even superior performance across various pose-relevant tasks. 1. Introduction Human pose plays pivotal role in various humancentric applications such as VR and healthcare. Numerous studies focus on single-pose comprehension, i.e., producing posture-relevant description from 3D body pose [14] or human image [18], as well as pose generation, i.e., creating complex 3D poses from textual descriptions [14, 27, 40] or human images [9, 15, 19, 57, 70]. Recently, few studies have explored the relationship between pairs of poses [13, 21, 34]. These studies investigate pose-pair comprehension, where textual instruction is produced based on the differences between two 3D poses, and pose editing, where corrected 3D body pose is generated based on an initial pose and modification instruction. However, key limitation of"
        },
        {
            "title": "Tasks",
            "content": "InputOutput HMR 2.0 [23] PoseScript [14] PoseFix [13] ChatPose [18] ChatHuman [41] UniPose (Ours) Pose-to-Text PoseText Pose Comprehension Pose-Diff Image-to-Text Image-Diff Text-to-Pose"
        },
        {
            "title": "Pose Generation",
            "content": "ImageText Pose PairText Image PairText TextPose ImagePose"
        },
        {
            "title": "Pose Editing",
            "content": "Pose&TextPose Table 1. Comparison of recent methods across various pose comprehension, generation and editing tasks. existing work is that pose comprehension, generation, and editing are predominantly studied in isolation. In reality, human pose cognition and communication inherently involve seamless transitions between multiple pose-relevant modalities, including 3D SMPL poses [45], textual descriptions, and human images. This highlights the need for unified multimodal framework capable of simultaneously handling pose comprehension, generation, and editing. Recent years have witnessed significant breakthrough in large language models (LLMs) [25, 30, 67] and multimodal LLMs (MLLMs), enabling general-purpose analysis of images [4, 43], videos [38, 73], motions [10, 31, 76], and audios [72, 74]. In the area of human poses, ChatPose [18], recent innovation, leverages LLMs to generate 3D human poses from images and textual descriptions. Nevertheless, it focuses solely on single-pose generation, lacking the capacity for pose comprehension and editing. Moreover, existing MLLMs still fall short in providing comprehensive analysis of human poses, particularly concerning fine-grained part semantics and complex relationships between pose pairs. Consequently, unified multimodal LLM that enables finergrained pose comprehension, generation, and complex pose editing is still in highly demand. Two main challenges need to be solved for building such unified multimodal LLM framework. The first challenge is creating unified representation space across 3D poses and texts, enabling the unification of diverse pose-relevant tasks. Existing work [18] processes 3D poses and texts differently, encoding 3D poses as continuous high-level features while tokenizing linguistic texts into discrete token sequences. This non-unified processing incurs an extra burden on LLMs to model interactions between 3D poses and texts, hindering the unifying of pose comprehension, generation and editing. The second challenge lies in achieving fine-grained pose perception within the visual branch of the multimodal framework. Most MLLMs [4, 18, 43, 65] employ CLIP [52] as their visual branch. While CLIPs visual encoder aligns well with the text embedding space through image-text contrastive learning, it struggles to capture detailed pixel-level information, such as keypoints and parsing maps, due to the global supervision provided by image captions. This limitation constrains MLLMs capabilities in fine-grained pose comprehension and generation. To address these challenges, we propose UniPose, uniform multimodal framework for human pose comprehension, generation and editing, which harnesses the powerful language generation abilities of LLMs to unify various pose-relevant tasks (Tab. 1). UniPose comprises three tires. Firstly, UniPose is equipped with pose tokenizer for processing 3D poses and texts uniformly. Inspired by the observation that human poses exhibit semantic coupling similar to language [31, 46, 68], we treat 3D pose as specific language. Akin to language, the pose tokenizer compresses raw 3D pose into sequence of discrete semantic tokens. By encoding both 3D pose and language within shared vocabulary, we build unified representation space across 3D poses and texts, which enables LLMs to be easily adapted to handle pose comprehension, generation, and editing. Secondly, unlike most MLLMs [4, 18, 43, 65] that solely rely on CLIPs visual encoder [52], we adopt mixtureof-visual-encoders that combines CLIPs original visual encoder with pose-specific visual encoder pre-trained on pose estimation task. This dual-encoder setup not only aligns visual representations with text embedding space but also enhances fine-grained pose perception, enabling more effective integration into the multimodal framework for improved pose comprehension and generation. Thirdly, we implement mixed-attention mechanism within LLMs to handle the distinct internal logical relationships between pose and text tokens. Unlike text tokens, pose tokens encode spatial joint positions without causal dependencies, making unified autoregressive modeling suboptimal. To address this, we apply causal attention to text tokens and bidirectional attention to pose tokens. This mixed-attention strategy preserves LLMs original reasoning capabilities while enhancing contextual pose perception, enabling more effective pose generation and editing. To our knowledge, UniPose is the first approach to integrate seven core tasks of pose comprehension, generation, and editing into uniform framework. Extensive experiments demonstrate that UniPose achieves competitive performance across multiple pose-relevant tasks. Additionally, 2 through qualitative results, we demonstrate that UniPose possesses zero-shot generalization capabilities, e.g., textenhanced pose estimation. 2. Related Work Human Pose Comprehension. Pose comprehension involves generating natural language descriptions of human pose or differences between pose pairs. For single-pose comprehension, traditional methods classify basic human actions from images [77], videos [62, 63, 66], or skeletons data [2, 11, 22, 49]. However, these methods typically lack detailed descriptions of specific body part positioning. To address this gap, [14] introduces PoseScript dataset which pairs human poses with detailed body parts descriptions, and propose pose-to-text generation model that uses cross-attention to embed pose information within text transformer for nuanced pose descriptions. For posepair comprehension, [13, 21, 34] describe differences between source and target poses based on images, videos, or 3D poses. For example, PoseFix [13] uses an MLP to fuse source and target pose, then uses cross-attention in text transformer to generate descriptions of pose differences. While these approaches enhance understanding of human poses from multimodal data, they are typically task-specific, with limited control conditions and application scenarios. Human Pose Generation. Pose generation synthesizes human poses conditioned on text or images. Text-conditioned pose generation generally falls into two categories: shapeoriented [24, 56] and pose-oriented [8, 27, 40], which generate 3D poses from descriptions of body attributes (e.g., slim waist) and simple actions (e.g., running), respectively. Image-conditioned pose generation (also referred to pose estimation) includes optimization-based and regression-based approaches. Optimization-based methods [7, 16, 17, 51, 54] iteratively estimate 3D pose parameters, ensuring the projection of predicted 3D joints aligns with 2D keypoints. Regression-based methods [9, 15, 23, 33, 70] use deep neural networks to directly predict 3D pose parameters from input images. Although these methods have achieved promising results in pose generation, they lack the capability of pose comprehension and editing. Multimodal Large Language Models. Large Language Models (LLMs) [25, 30, 58, 71] have shown remarkable capabilities in textual comprehension and reasoning. These models have been adapted for multimodal tasks, leading to the development of multimodal large language models. For example, models like mPLUG-Owl3 [73], MiniGPT-4 [79] and LLaVA [37, 43, 44] uses visual encoder to extract image features and projection layer to align image embeddings with text embeddings, enhancing general visual perception. Moving towards task-specific applications, LISA [36] and Video-LISA [5] extend MLLMs for segmentation by integrating SAM [35] for generating fine-grained segmentation masks. Additionally, Show-o [69] and Transfusion [78] combine MLLMs with diffusion models [26] to unify image understanding and generation. recent work, ChatPose [18], applies LLMs to pose-related tasks, aiming to build versatile pose generator. However, it remains limited in its capacity for pose understanding and editing. 3. Method To equip LLM with the capability to comprehend, generate, and edit human poses, we propose unified framework named UniPose. As illustrated in Fig. 2, UniPose comprises three main components: pose tokenizer, which quantizes original 3D poses (represented as SMPL [45] pose parameters) into discrete tokens (Sec. 3.1), visual processor, which extracts fine-grained, pose-relevant features from visual inputs, and pose-aware LLM, which supports unified modeling across multiple modalities (Sec. 3.2). To address pose-relevant tasks, we employ four-stage straining scheme encompassing pose tokenizer training, pose-text alignment pre-training, vision projector pre-training, and instruction tuning (Sec. 3.3). During inference, pose tokens are decoded back to their original SMPL format by associated de-tokenizer, enabling various pose-relevant tasks to be executed via instructions (Sec. 3.3) 3.1. Pose Tokenizer To represent 3D pose in discrete tokens, we build the pose tokenizer based on Vector Quantized Variational Autoencoders (VQ-VAE) [60], as shown in Fig. 2. The pose tokenizer consists of an encoder E, decoder D, along with learnable codebook Bp = {bm}M m=1 containing discrete vectors. Formally, we represent 3D pose using SMPL pose parameters, i.e., = [γ, θ] where γ R6 denotes the root orientation and θ R6K denotes the rotations with joints. Then the pose encoder that consists of several 1-D convolutional layers projects into latent embedding = E(θ) with RLpdp , where Lp is the number of pose tokens and dp is the latent dimension. Next, we transform into collection of codebook entries through discrete quantization. Specifically, the process of quantization replaces each item of with its nearest entry in the codebook Bp, obtaining the quantized latent vector (cid:98)z RLpdp as follows: (cid:98)z = arg min bmBp bm2 . (1) After quantization, the pose decoder D, consisting of several 1-D deconvolutional layers, projects (cid:98)z back to raw pose space as (cid:98)p = D((cid:98)z). Following [60], we train the pose tokenizer using the loss function Lvq = Lr + Le + Lc where Lr, Le, and Lc denote reconstruction loss, embedding loss and commitment loss respectively. Further training and objective details are provided in the Appendix. 3 Figure 2. Method overview: UniPose comprises Pose Tokenizer, Visual Processor and pose-aware language LLM. Combining Pose Tokens learned by pose tokenizer, Visual Embeddings from visual processor and Text Tokens from text tokenizer, UniPose enables joint modeling of pose comprehension, generation and editing within unified visual-language backbone. After training the pose tokenizer, the pose can be represented as sequence of discrete codebook indices of the quantized latent vector, namely pose tokens RLp as: = arg min m{1,...,M } bm2 . (2) 3.2. Pose-aware Vision-Language Model Visual Processor. Previous works [4, 43] commonly use CLIP visual encoder [52] as the visual branch. However, since CLIP is optimized by global and coarse-grained supervision signals from image captions, it struggles to capture pose-relevant details. Differently, the pose estimation task demands precise localization of human keypoints, which encourages the visual encoder to capture fine-grained pose features. Therefore, we integrate pose-specific Vision Transformer [23], pretrained on the pose estimation task, into the visual branch, as shown in Fig. 2. Specifically, denote the CLIP visual encoder and pose-specific vision transformer as fa and fb, respectively. Given an input image x, we extract visual embeddings by CLIP as va = fa (x) where va RLvda , Lv is the number of visual patch tokens and da is its visual embedding dimension. The embedding output by pose-specific vision transformer is vb = fb (x) where vb RLvdb . Then we concatenate the embedding output by these two encoders along the channel dimension, and apply trainable projector layer (with projection matrix R(da+db)d) to align the dimension of the concatenated visual features to that of text features as = [vavb]T . Here RLvd with as text embedding dimensions of LLM. The fused visual features can be concatenated with pose or text tokens as input to LLM. Mixed Attention Mechanism. Existing LLMs [30, 53, 58] typically employ autoregressive modeling with causal attention, excelling at generating sequential data such as text and audio [72, 74]. However, pose tokens, which encode spatial positions of human joints, are inherently non-sequential, making traditional autoregressive generation suboptimal. To address this issue, we propose modeling pose tokens as whole. Inspired by [69, 78], we modify the standard causal attention in LLM, integrating bidirectional attention for pose tokens as depicted in Fig. 2. Specifically, we apply casual attention to text sequence, but apply bidirectional attention within the pose toke sequence. To avoid information leakage, we initialize Lp learnable pose queries = {q1, ..., qLp } during the generation and editing of 3D poses, as shown in Fig. 2. These queries are used to predict corresponding pose tokens in single forward step. This design enables each pose token to attend to others within the same pose token sequence, while restricting access to only previously encountered text tokens. Unified Multimodal Language Model. As shown in Fig. 2, equipped with visual processor and pose tokenizer, we can compress original visual data and pose data into visual feature sequence RLvd and pose token sequence RLp , respectively. To incorporate pose discrete tokens into LLMs, we expand the original text vocab1, forming new ulary Vt of LLM with pose vocabulary Vp unified text-pose vocabulary = {Vt, Vp}. Equipped with the unified vocabulary V, various pose-related tasks can be formulated in general format, where both input and output tokens are drawn from the same vocabulary, with the input optionally combined with the visual feature v. These discrete tokens can represent natural language, 3D pose, or combination, depending on the specific task to be solved. 1The pose vocabulary Vp preserves the order of the pose codebook Bp. In implementation, we add two special tokens, <p> and <p/>, which denotes the start and end of pose sequence, into the vocabulary Vp. 4 (a) Pose-Text Alignment Pretraining Stage. (b) Visual Projector Pretraining Stage. (c) Instruction Finetuning Stage. Figure 3. The training paradigm of UniPose. This naturally enables UniPose to unify pose comprehension, generation, and editing in unified manner. During training, denote the visual embedding sequence as = (cid:8)vi Rd(cid:9)Lv i=1, the pose token sequence as = (cid:8)ui V(cid:9)Lp i=1, the text token sequence of single-pose description as = (cid:8)ti V(cid:9)Lt i=1, and the text token sequence of pose-difference description as = (cid:8)di V(cid:9)Ld i=1, we apply distinct optimization objectives for each task, tailored to the specific type of input and the desired output, as follows: Single-Pose Comprehension. Single-pose comprehension aims to generate pose description from 3D pose or image. Formally, given the sequence v, and as defined above, the LLM predicts the probability distribution of (cid:0)tiv/u, t<i(cid:1), potential next text token at each step, pθ conditioned on the visual or pose tokens in an autoregressive manner. The objective is to maximize the loglikelihood of this predicted pose description distribution: L1 = Lt(cid:88) i=1 log pθ (cid:0)tiv/u, t<i(cid:1) , (3) where θ represents the trainable parameters. Pose-pair Comprehension. Pose-pair comprehension aims to generate textural description of the difference between pair of 3D poses or images. Formally, given visual features v1 and v2 for an image pair, pose tokens u1 and u2 for 3D pose pair, and pose-difference description tokens d, the LLM predicts the probability distribution of the next pose-difference text token, (cid:0)di (v1, v2) / (u1, u2) , d<i(cid:1), conditioned on the pair pθ of visual or pose tokens in an autoregressive manner. The objective is to maximize the log-likelihood of this predicted pose-difference description distribution: L2 = Ld(cid:88) i=1 log pθ (cid:0)di (v1, v2) / (u1, u2) , d<i(cid:1) . (4) Pose Generation. Pose generation aims to generate 3D poses from pose textural descriptions or images. For this task, we use mixed attention mechanism where the input pose tokens are replaced with predefined pose queries Q. Formally, given v, and as defined above, LLM predicts the probability distribution of potential whole pose tokens in single step, pθ (uv/t, Q), conditioned on the visual or pose-description text tokens and pose queries. The objective is to maximize the log-likelihood of this predicted pose distribution: L3 = pθ (uv/t, Q) . (5) Pose editing. Pose editing aims to generate corrected 3D pose based on an initial pose and modification instruction. Similar to pose generation, mixed attention mechanism is used for this task. Formally, given u1, u2 and as defined above, LLM predicts the probability distribution of potential whole pose tokens for the corrected pose, pθ (u2d, u1, Q), conditioned on the initial pose tokens, modification instruction tokens and pose queries. The objective is to maximize the log-likelihood of this predicted corrected-pose distribution: L4 = pθ (u2u1, d, Q) . (6) At the last, given batch size of inputs with different task types, the overall training loss is computed as the sum of the individual objections: = L1 + L2 + L3 + L4. 3.3. Training and Inference Paradigm The training procedure comprises four stages, and the training paradigm of the last three stages is shown in Fig. 3. Pose Tokenizer Training. We first train pose tokenizer using the objective Lvq. The pose tokenizer encodes 3D pose as sequence of discrete tokens, enabling seamless integration with texts within LLM. To maintain stability during LLM training, the pose tokenizer is kept frozen during the subsequent stages of training. Pose-Text Alignment Pretraining. To enable LLM to handle discrete pose tokens, we train LLM on pose-text corpus. This process aims to align the pose and text modalities for unified reasoning within the LLM. In this stage, we consider four pose-text relevant tasks in Tab. 1, i.e., 2 pose comprehension tasks (Pose-to-Text and Pose-Diff), 1 pose generation task (Text-to-Pose) and the Pose Editing task. Based on these tasks, we train LLM using LoRA [28] with the objective L, as shown in Fig. 3a. 5 Task Dataset Method R-Precision Linguistic metrics Top-1 Top-2 Top-3 BLEU-4 ROUGE-L METEOR Pose-to-Text PoseScript [14] Pose-Diff PoseFix [13] Image-to-Text ImageScript Image-Diff ImageDiff PoseScript [14] UniPose UniPose PoseFix [13] UniPose UniPose LLaVA [43] Qwen-VL [4] GPT4V [1] UniPose UniPose GPT4V [1] UniPose UniPose 91.6 18.1 85. 64.6 8.4 67.9 5.7 8.9 17.7 22.4 24.5 7,3 13.0 13.5 95.6 30.0 95.2 77.1 14.6 81.8 12.0 15.6 24.0 32.8 35. 13.5 18.8 25.0 97.0 39.1 97.6 83.0 19.2 88.6 18.9 19.8 32.3 41.2 43.2 18.8 26.4 33.8 12.9 10.8 12. 12.0 8.5 13.8 3.2 1.4 7.1 18.2 18.2 1.3 14.0 15.9 33.9 30.1 33.3 33.5 28.2 33.7 21.8 15.9 29.1 42.4 42. 16.1 34.1 36.5 34.2 29.5 30.8 36.7 27.3 31.2 32.9 21.6 34.2 45.2 44.7 21.8 40.1 39.6 Table 2. Comparisons on pose comprehension tasks. We compare the pose-retrieval precision (R-Precision) and linguistic metrics on various datasets. UniPose represents training UniPose on the single corresponding task. Visual Projector Pretraining. After establishing alignment between pose and text modalities, this training stage focuses on mapping images into the shared pose-text space. In this stage, we consider three image-text relevant tasks in Tab. 1, i.e., 2 pose comprehension tasks (Image-to-Text and Image-Diff) and 1 pose generation task (Image-to-Pose). Based on these tasks, we train the vision-language projector to align visual data with language models with the objective L, as shown in Fig. 3b. Instruction Finetuning. To enhance the instructionfollowing capability of UniPose, we construct multitask, multimodal instruction dataset with 200 templates for each task from Tab. 1. For example, an instruction for Image-to-Pose task could be Could you estimate the SMPL pose of the individual in this image <image>, with <image> standing for the image embedding extracted by the visual processor. Using this instruction data, we jointly train the visual projector and LLM with LoRA, as shown in Fig. 3c. Inference. During inference, we adopt tailored decoding strategies according to task type. For pose comprehension tasks, we use standard auto-regressive approach, where text tokens are generated sequentially, step-by-step. For pose generation and editing tasks, as shown in Fig. 2, once the model predicts the start of pose token <p>, we append Lp predefined pose queries to the conditional text tokens, which is fed into LLM. Then LLM predicts the corresponding pose token for each query in parallel, which significantly accelerates its inference speed. 4. Experiments 4.1. Experimental Setup Datasets. For pose tokenizer training, we use the standard training split of AMASS [47] and MOYO [59], following TokenHMR [15]. For UniPose training, we integrate three types of data: (1) Text-Pose Data. We use PoseScript [14] and PoseFix [13] datasets to link language and pose modality. PoseScript [14] provides natural language descriptions paired with 3D human poses, allowing the model to understand fine-grained pose semantics. PoseFix [13] includes pairs of 3D poses and textual descriptions that specify how to modify the source pose to achieve the target (2) Image-Pose Data. Following [15, 23], we use pose. standard human pose estimation training datasets, including Human3.6M [29], MPI-INF-3DHP [48], COCO [42], and the MPII [3] dataset, and evaluate on 3DPW [61] and Human3.6M [29] test sets. (3) Image-Text Data. Since no existing dataset combines human images with pose descriptions, we create the ImageScript and ImageDiff datasets to bridge this gap in visual-textual pose comprehension. Further dataset details are provided in the Appendix. Metrics. We adopt the evaluation metrics from PoseScript [14] and PoseFix [13]. (1) Pose comprehension tasks. We use two types of metrics. Pose-text retrieval metric: RPrecision, which evaluates the accuracy of matching poses with corresponding descriptions. We rank the Euclidean distances between the query pose and 32 text descriptions (1 ground truth and 31 randomly selected mismatched descriptions), and report Top 1/2/3 R-Precision; Linguistic metrics: BLEU-4 [50], Rouge-L [39] and METEOR [6], which assess the quality of generated pose descriptions. (2) Pose generation tasks. We use two types of metrics. Re6 Method RT2P RP2T Pose Reconstruction Metric Top-5 Top-10 Top-20 Top-5 Top-10 Top-20 MPJPE PA-MPJPE FID PoseScript [14] ChatPose [18] ChatHuman [41] UniPose UniPose 73.3 17.6 41.8 67.5 73.7 82.5 25.3 52.6 77.6 82.4 89.4 35.8 65.1 85.5 89. 70.0 28.0 42.1 62.8 70.9 82.5 39.0 52.3 74.8 80.5 87.4 54.4 66.5 83.6 89.6 318.0 - - 342.7 308.6 161.3 - - 190.0 171.1 0.075 - - 0.046 0. Table 3. Comparisons on Text-to-Pose generation task. The retrieval and reconstruction metrics are reported on PoseScript [14] dataset. 3DPW [61] H36M [29] 4.2. Comparisons on Pose-relevant Tasks Method HMR [33] PyMAF [75] SMPLer [70] HMR2.0 [23] Zolly [64] MEGA [20] TokenHMR [15] ChatPose [18] UniPose UniPose MPJPE PA-MPJPE MPJPE PA-MPJPE 130.0 92.8 73.7 70.0 76.2 67.5 71.0 163.6 97.4 94. 76.7 58.9 43.4 44.5 47.9 41.0 44.3 81.9 61.2 59.1 88.0 57.7 45.2 44.8 49.4 - - 126.0 65.8 69.2 56.8 40.5 32.4 33.6 32.3 - - 82.4 39.4 41. Table 4. Comparisons on pose estimation task. Reconstruction metrics are reported on the 3DPW and Human3.6M datasets. construction metrics: MPJPE and PA-MPJPE, which computes the average per-joint position error between generated and ground-truth pose; Pose-text retrieval metric: following [18], we report Top 5/10/20 RT2P and RP2T, which represents the text-to-pose and pose-to-text retrieval recall, respectively. (3) Pose editing tasks. In addition to the reconstruction metrics, we also report the Frechet Inception Distance (FID), which measures the distance between the generated and ground-truth pose distribution. To calculate these metrics, following [13, 14], we train retrieval model with pose and text feature extractors using contrastive loss, which encourages matched pose-text pairs to have geometrically close feature vectors. Implementation Details. For pose tokenizer, we set the codebook size to 2048 and each 3D pose is represented with 80 discrete tokens. We utilize LLaVA-1.6V [43] as our visual-language model backbone. For training the pose tokenizer, we use AdamW as the optimizer with batch size of 256 and an initial learning rate of 2e-4. The pose tokenizer is trained for 240 epochs on single RTX 4090 GPU. UniPose is trained 6 epochs in the Pose-Text Alignment Pretraining stage, and 2 epochs in the remaining stages using 4 A100 GPUs. Further implementation details are provided in the Appendix. 7 Comparisons on Pose comprehension. We evaluate UniPose on 4 pose comprehension tasks, i.e., Pose-to-Text, Pose-Diff, Image-to-Text and Image-Diff. The comparison results are shown in Tab. 2. As seen in the table, UniPose achieves competitive performance across all evaluated tasks, highlighting its capability to address diverse pose comprehension tasks within single model. (1) For Poseto-Text task, we compare UniPose with PoseScript [14] on PoseScript dataset. As shown in Tab. 2, UniPose achieves slightly lower performance than PoseScript. However, PoseScript is tailored for single-pose description generation and lacks the capacity to model relationships between different poses. (2) For Pose-Diff task, we compare UniPose with PoseFix [13] on PoseFix dataset. As shown in Tab. 2, UniPose outperforms PoseFix on most metrics, demonstrating its superiority in capturing relationships between pairs of poses. (3) For Image-to-Text task, we compare UniPose with existing visual-language MLLMs, including LLaVA [43], Qwen-VL [4] and GPT4V [1], on ImageScript dataset. As shown in Tab. 2, UniPose significantly outperforms these MLLMS. The substantial gains can be attributed to the use of pose-specific visual encoder, which enables UniPose to extract fine-grained pose information from visual inputs. (4) For Image-Diff task, we compare UniPose with GPT4V on ImageDiff dataset. UniPose still outperforms GPT4V, demonstrating that UniPose not only captures finegrained pose features from single image but also learns the relationships between human poses across multiple images. Fig. 4a and Fig. 4b visualize the generated textural descriptions of UniPose, Qwen-VL [4] and GPT4V [1]. The visualizations reveal that existing MLLMs struggle to comprehend fine-grained pose information. Specifically, QwenVL [4] and GPT4V [1] fail to distinguish human body orientation, whereas UniPose can accurately locate the human body orientation from visual inputs. Comparisons on Pose Generation. We further evaluate UniPose on 2 pose generation tasks, i.e., text-to-pose and pose estimation. The comparison results are shown in Tab. 3 and Tab. 4. (1) For Text-to-Pose task, we compare UniPose with existing text-conditional pose generation models [14, 18, 41] on PoseScript dataset. As shown in Tab. 3, Method MPJPE PA-MPJPE FID CLIP-ViT Pose-ViT Pose Estimation Image-to-Text PoseFix [13] UniPose UniPose 300.2 310.8 270.3 144.1 157.0 138.9 0.019 0.019 0. MPJPE PA-MPJPE BLEU-4 ROUGE-L METEOR 193.4 96.1 86.1 58.9 11.1 13.3 30.2 31.7 33.9 35.2 Table 5. Comparisons on pose editing task. Reconstruction metrics are reported on PoseFix [13] dataset. Table 6. Ablation study on the components of the visual processor. Attention Type RT2P Text-to-Pose RP2T Latency (s) BLEU-4 ROUGE-L METEOR Pose-to-Text Causal Attention Mixed Attention 9.0 13.8 14.2 20. 20.8 28.8 9.3 15.9 14.7 23.0 22.3 32.0 2.5 0.2 26.9 25. 39.5 39.1 38.0 36.7 Table 7. Ablation study on different attention mechanisms. across all metrics, validating its superiority in pose editing. 4.3. Ablation Studies & Analysis Single-task training v.s. Multi-task training. Tab. 2, 3, 4, 5 also report the performance of UniPose training on single task (denoted as UniPose ). As shown, multi-task training consistently outperforms single-task training, underscoring the importance of unifying pose comprehending, generation and editing within single model. Visual Processor. We compare the impact of different vision encoders used in the Visual Processor of UniPose. In this part, the models are trained solely on Pose Estimation and Image-to-Text tasks for 2 epochs. As shown in Tab. 6, with only the CLIP-ViT encoder, the model performs poorly on pose estimation task. We argue that CLIP-ViT primarily focuses on aligning global semantic information between images and text, struggling to capture detailed human pose information. By incorporating an additional ViT model trained specifically for pose estimation, UniPose gains the ability to capture fine-grained pose details, significantly improving its performance on pose estimation task. Moreover, the pose information extracted from images enhances the performance on Image-to-Text task, enabling UniPose to generate more precise descriptions of human poses. Attention mechanism. We evaluate the performance of In UniPose using causal attention and mixed attention. this part, the models are trained solely on Text-to-Pose and Pose-to-Text tasks for 6 epochs. More training details are provided in the Appendix. As shown in Tab. 7, on Text-toPose task, the model with mixed attention achieves higher retrieval accuracy compared to casual attention. The results indicate that capturing bidirectional dependencies among pose tokens enhances pose generation. Additionally, the bidirectional attention mechanism enables single-step generation of all pose tokens, significantly accelerating inference. However, mixed attention performs worse than causal attention on Pose-to-Text task. This may be due to the in- (a) Comparison with Qwen-VL [4] and GPT-4o [1] on Image-to-Text task. (b) Comparison with GPT-4o [1] on Image-Diff task. Figure 4. Examples on Image-to-Text and Image-Diff tasks. We mark incorrect captions in red and correct in green. UniPose can accurately perceive persons orientation from images. UniPose achieves the best performance on most metrics. We attribute this to the use of the mixed-attention mechanism in LLM, which effectively captures the bidirectional dependencies among pose tokens, thus improving pose generation performance. (2) For pose estimation task, we compare UniPose with traditional pose estimation approaches [15, 23] and MLLM-based approaches [18], on 3DPW [61] and H36M [29] datasets. As shown in Tab. 4, UniPose largely outperforms other MLLMs, yet does not match the performance of methods specifically designed for estimating 3D human pose. This is not surprising, as traditional pose estimation methods have been developed over many year and often incorporate custom network modules and loss functions to enhance estimation accuracy. Comparisons on Pose Editing. For pose editing task, we compare UniPose with PoseFix [13] on PoseFix dataset. As shown in Tab. 5, UniPose significantly outperforms PoseFix [5] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. arXiv, 2024. 3 [6] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL, pages 6572, 2005. 6 [7] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In ECCV, pages 561578. Springer, 2016. 3 [8] Rania Briq, Pratika Kochar, and Juergen Gall. Towards better adversarial synthesis of human images from text. arXiv, 2021. 3 [9] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, et al. Smpler-x: Scaling up expressive human pose and shape estimation. NeurIPS, 36, 2024. 1, 3 [10] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. arXiv, 2024. 2 [11] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying Deng, and Weiming Hu. Channel-wise topology refinement graph convolution for skeleton-based action recognition. In ICCV, pages 1335913368, 2021. [12] Kyunghyun Cho. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv, 2014. 4 [13] Ginger Delmas, Philippe Weinzaepfel, Francesc MorenoNoguer, and Gregory Rogez. Posefix: correcting 3d human poses with natural language. In ICCV, pages 1501815028, 2023. 1, 2, 3, 6, 7, 8, 4, 5 [14] Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and Gregory Rogez. Posescript: Linking 3d human poses and natural language. TPAMI, 2024. 1, 2, 3, 6, 7, 4, 5 [15] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael Black. Tokenhmr: Advancing human mesh recovIn CVPR, pages ery with tokenized pose representation. 13231333, 2024. 1, 3, 6, 7, 8, 4, 5 [16] Taosha Fan, Kalyan Vasudev Alwala, Donglai Xiang, Weipeng Xu, Todd Murphey, and Mustafa Mukadam. Revitalizing optimization for 3d human pose and shape estimation: sparse constrained formulation. In ICCV, pages 1145711466, 2021. 3 [17] Qi Fang, Kang Chen, Yinghui Fan, Qing Shuai, Jiefeng Li, and Weidong Zhang. Learning analytical posterior probability for human mesh recovery. In CVPR, pages 87818791, 2023. [18] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael Black. Chatpose: Chatting about 3d human pose. In CVPR, pages 20932103, 2024. 1, 2, 3, 7, 8, 5, 6 [19] Guenole Fiche, Simon Leglaive, Xavier Alameda-Pineda, Antonio Agudo, and Francesc Moreno-Noguer. Vq-hps: Human pose and shape estimation in vector-quantized latent space. In ECCV, 2024. 1 Figure 5. Enhance pose estimation with input pose description. terference of the bidirectional attention with the causal dependencies essential for text generation, potentially compromising the semantic precision of the generated content. Zero-shot Task Analysis. Benefiting from unified learning format, UniPose effectively transfers knowledge across different pose-relevant tasks and adapts to unseen tasks. Fig. 5 provides zero-shot analysis: without additional training, UniPose can leverage pose descriptions to enhance its pose estimation results. This ability is especially advantageous in scenarios where ambiguity or occlusion affects accurate human pose estimation from images. 5. Conclusion In this work, we present UniPose, the first attempt to integrate human pose comprehension, generation, and editing within unified framework. By employing pose tokenizer, we build unified representation space that bridges 3D poses and texts, enabling seamless interactions across the mixture-of-visual encoder modalities. Additionally, captures intricate pose details, thereby enhancing finegrained pose perceptions. The mixed-attention mechanism further enhances pose generation quality while significantly accelerating inference speed. Extensive evaluations across various pose-relevant tasks demonstrate the effectiveness of UniPose in pose comprehension, generation, and editing."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv, 2023. 6, 7, 8, 2 [2] Dasom Ahn, Sangwon Kim, Hyunsu Hong, and Byoung Chul Ko. Star-transformer: spatio-temporal cross attention transformer for human action recognition. In WACV, pages 33303339, 2023. 3 [3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark In CVPR, pages 36863693, and state of the art analysis. 2014. 6, 1 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv, 2023. 2, 4, 6, 7, 8 9 [20] Guenole Fiche, Simon Leglaive, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Mega: Masked generative autoencoder for human mesh recovery. arXiv, 2024. 7 [21] Mihai Fieraru, Mihai Zanfir, Silviu Cristian Pirlea, Vlad Olaru, and Cristian Sminchisescu. Aifit: Automatic 3d human-interpretable feedback models for fitness training. In CVPR, pages 99199928, 2021. 1, [22] Lin Geng Foo, Tianjiao Li, Hossein Rahmani, Qiuhong Ke, In CVPR, and Jun Liu. Unified pose sequence modeling. pages 1301913030, 2023. 3 [23] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In CVPR, pages 1478314794, 2023. 2, 3, 4, 6, 7, 8 [24] Omer Gralnik, Guy Gafni, and Ariel Shamir. Semantify: Simplifying the control of 3d morphable models using clip. In ICCV, pages 1455414564, 2023. 3 [25] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv, 2023. 2, [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [27] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot textdriven generation and animation of 3d avatars. arXiv, 2022. 1, 3 [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv, 2021. 5 [29] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. TPAMI, 36(7):13251339, 2013. 6, 7, 8, 1 [30] AQ Jiang, Sablayrolles, Mensch, Bamford, DS Chaplot, de las Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b (2023). arXiv, 2023. 2, 3, 4 [31] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. NeurIPS, 36:2006720079, 2023. 2 [32] Sam Johnson and Mark Everingham. Learning effective human pose estimation from inaccurate annotation. In CVPR 2011, pages 14651472. IEEE, 2011. [33] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, pages 71227131, 2018. 3, 7 [34] Hyounghun Kim, Abhay Zala, Graham Burri, and Mohit Bansal. Fixmypose: Pose correctional captioning and retrieval. In AAAI, pages 1316113170, 2021. 1, 3 [35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3 [36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, pages 95799589, 2024. 3 [37] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv, 2024. 3 [38] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv, 2023. 2 [39] Chin-Yew Lin. Rouge: package for automatic evaluation In Text summarization branches out, pages of summaries. 7481, 2004. 6 [40] Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang Lin, Qi Tian, and Chang-wen Chen. Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training. In CVPR, pages 2322223231, 2023. 1, 3 [41] Jing Lin, Yao Feng, Weiyang Liu, and Michael Black. Chathuman: Language-driven 3d human understanding with retrieval-augmented tool reasoning. arXiv, 2024. 2, 7 [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755. Springer, 2014. 6, 1 [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 3, 4, 6, 7 [44] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. 3 [45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiSIGGRAPH, 34(6):248:1248:16, person linear model. 2015. 2, 3, 1 [46] Mingshuang Luo, Ruibing Hou, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan. 3gpt: An advanced multimodal, multitask framework for motion comprehension and generation. NeurIPS, 2024. 2 [47] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In ICCV, pages 54425451, 2019. [48] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild In 3DV, pages 506516. using improved cnn supervision. IEEE, 2017. 6, 1 [49] Arnab Mondal, Stefano Alletto, and Denis Tome. Hummuss: Human motion understanding using state space models. In CVPR, pages 23182330, 2024. 3 [50] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, pages 311318, 2002. 6 [51] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 3 10 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 2, 4 [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. [54] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas Guibas. Humor: 3d human In ICCV, pages motion model for robust pose estimation. 1148811499, 2021. 3 [55] Sanh. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv, 2019. 4 [56] Stephan Streuber, Alejandra Quiros-Ramirez, Matthew Hill, Carina Hahn, Silvia Zuffi, Alice OToole, and Michael Black. Body talk: Crowdshaping realistic 3d avatars with words. TOG, 35(4):114, 2016. 3 [57] Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi-Sing Leung, Ziwei Liu, Lei Yang, et al. Aios: All-in-one-stage expressive human In CVPR, pages 18341843, pose and shape estimation. 2024. 1 [58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv, 2023. 3, [59] Shashank Tripathi, Lea Muller, Chun-Hao Huang, Omid Taheri, Michael Black, and Dimitrios Tzionas. 3d human pose estimation via intuitive physics. In CVPR, pages 4713 4725, 2023. 6 [60] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 30, 2017. 3 [61] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In ECCV, pages 601617, 2018. 6, 7, 8, 1 [62] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR, pages 1454914560, 2023. 3 [63] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In CVPR, pages 63126322, 2023. [64] Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qingping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and Taku Komura. Zolly: Zoom focal length correctly for perspectivedistorted human mesh reconstruction. In ICCV, pages 3925 3935, 2023. 7 [65] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv, 2023. 2 11 [66] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv, 2024. 3 [67] Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Occllama: An Zhongxue Gan, and Wenchao Ding. occupancy-language-action generative world model for autonomous driving. arXiv, 2024. 2 [68] Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, and ChiKeung Tang. Motionllm: Multimodal motion-language learning with large language models. arXiv, 2024. 2 [69] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv, 2024. 3, [70] Xiangyu Xu, Lijuan Liu, and Shuicheng Yan. Smpler: Taming transformers for monocular 3d human shape and pose estimation. TPAMI, 2023. 1, 3, 7 [71] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv, 2024. 3 [72] Dongchao Yang, Haohan Guo, Yuanyuan Wang, Rongjie Huang, Xiang Li, Xu Tan, Xixin Wu, and Helen Meng. Uniaudio 1.5: Large language model-driven audio codec is few-shot audio task learner. arXiv, 2024. 2, 4 [73] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplugowl3: Towards long image-sequence understanding in multimodal large language models, 2024. 2, 3 [74] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv, 2023. 2, 4 [75] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop. In ICCV, 2021. [76] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In AAAI, pages 73687376, 2024. 2 [77] Zhichen Zhao, Huimin Ma, and Shaodi You. Single image action recognition using semantic body part actions. In ICCV, pages 33913399, 2017. 3 [78] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv, 2024. 3, 4 [79] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv, 2023. 3 UniPose: Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 1. The annotation workflow for ImageScript (left) and ImageDiff (right) datasets. In this Appendix, we present comprehensive overview of UniPose, covering its datasets, implementation details, performance evaluation, and limitations. First, we introduce two new image-text datasets, ImageScript and ImageDiff, along with detailed description of the training data used for UniPose (Sec. A). Next, we outline the implementation details of the pose tokenizer, retrieval models, and UniPose, including their architectural designs and training configurations (Sec. B). Additionally, we present experimental results to evaluate the performance of the tokenizer and retrieval models (Sec. C). Finally, we offer additional qualitative results (Sec. D) and conclude with an analysis of UniPoses limitations (Sec. E). A. Data Collection To address the lack of datasets combining human images with pose descriptions, we present the ImageScript and ImageDiff datasets, specifically designed to bridge this gap in visual-textual pose comprehension. A.1. ImageScript ImageScript dataset aims to provide accurate and detailed textual descriptions of human poses depicted in images. Existing pose estimation datasets, collectively referred to as PoseEst (e.g., Human3.6M [29], MPI-INF3DHP [48], COCO [42], MPII [3], and 3DPW [61]) offer precise human poses paired with images. PoseScript [14] introduces pipeline for automatically generating textual descriptions of human poses. Building on these efforts, our ImageScript dataset integrates human images, poses, and detailed textual descriptions to advance visual-textual pose comprehension. The ImageScript dataset comprises 52k image-text pairs, with the images sourced from the PoseEst datasets. Following PoseScript [14], we first normalize the joint positions of each pose annotation from PoseEst datasets using the neutral SMPL body model [45], employing default shape coefficients and global orientation of 0. To ensure diversity, we apply the farthest point sampling algorithm to select samples using the mean per joint error (MPJE) as the distance metric. Starting with randomly selected pose, we iteratively add the pose with the highest MPJE to the selected set until the desired sample size is reached."
        },
        {
            "title": "For",
            "content": "textural annotations, we utilize the automatic pipeline from PoseScript to generate three diverse captions for each sampled pose. However, automatically generated captions often contain excessive detail and repetition, lacking the simplicity and fluency characteristic of human lan1 Figure 2. Prompt to query GPT-4 for refining text in the ImageScript dataset. Figure 3. Prompt to query GPT-4 for refining text in the ImageDiff dataset. guage. To address this, we use GPT-4 [1] to refine the captions, transforming verbose and redundant descriptions into concise, natural expressions. Details of the query prompt and the annotation workflow are provided in Fig. 1 and Fig. 2, respectively. Dataset statistics. The dataset generated using PoseScripts automatic pipeline is referred to ImageScript-A, while the GPT-4-refined version is named ImageScript-R. Imagepose pairs are initially sampled from Human3.6M (15k), MPI-INF-3DHP (25k), COCO (5k), and MPII (5k) datasets. Textual pose descriptions for each pose are then generated using the automatic pipeline, forming the ImageScript-A dataset. To construct the ImageScript-R training set, 6,250 examples are uniformly sampled from ImageScript-A. Additionally, 2000 samples from the 3DPW dataset are selected to create the ImageScript-R test set. The captions in ImageScript-R are refined using GPT-4, transforming the automatically generated descriptions into more concise and natural expressions. A.2. ImageDiff ImageDiff dataset is designed to provide textual descriptions of human pose differences between image pairs, enabling the model to effectively perceive and interpret pose variations across different visual inputs. Building on PoseFix [13], which introduced pipeline for automatically generating comparative descriptions for 3D SMPL pose pairs, we propose ImageDiff, dataset comprising image pairs, corresponding 3D pose pairs, and textual descriptions of pose differences. The ImageDiff dataset consists of 52k triplets in the form of {image A, image B, text}, where the text describes how 2 Training paradigm Task Dataset Samples Pose-Text Align Pretraining Pose-to-Text, Pose-Diff, Text-to-Pose, Pose-Edit PoseScript-A PoseFix-A Visual Projector Pretraining Image-to-Text, Image-Diff, Pose Estimation Instruction Finetuning All tasks ImageScript-A ImageDiff-A PoseEst PoseScript-H PoseFix-H ImageScript-R ImageDiff-R PoseEst 70k 93k 50k 50k 100k 5k 5k 6k 6k 6k Table 1. Detailed datasets for training UniPose. The PoseScript dataset provides human annotations (PoseScript-H) and expands its dataset with automated captions (PoseScript-A), as does the PoseFix dataset. Task Sub-Task Pose-to-Text Pose Comp Pose-Diff Input Generate description of the SMPL pose: <pose>. Interpret the SMPL pose in <pose> and generate written description. OutPut Provide summary of how SMPL pose <pose> differs from <pose>. Detail any SMPL pose changes seen between <pose> and <pose>. <caption> Image-to-Text Image-Diff Pose Estimation Describe the pose of the individual in the <image>. Analyze <image> and describe the posture displayed. Compare <image> and <image>, outline how the persons posture differs. Identify how the individuals pose varies from <image> to <image>. Could you estimate the SMPL pose of the individual in <image>? Look at the <image> and return the SMPL pose parameters for the figure shown. Text-to-Pose Could you generate the SMPL pose from the description: <caption>? Using the description <caption>, please create the corresponding SMPL pose. <pose> Pose Editing Modify <pose> based on this instruction: <caption>. Refine <pose> by applying the description provided: <caption>. Pose Gen Table 2. Examples of instruction templates utilized during the instruction finetuning stage of UniPose training. to modify the human pose from image (the source image) to match image (the target image). The corresponding pose annotations for images and are denoted as poses and B. The process for selecting image is consistent with the approach used in the ImageScript dataset. For selecting image A, following PoseFix [13], we first calculate the cosine similarity between the pose retrieval features (Sec. B.2) of each pose and all other poses in the PoseEst datasets. The top 100 poses with the highest similarity are shortlisted as candidates for pose A. To ensure diversity, we leverage posecode information [14] to verify that each pose pair exhibits at least 10 distinct low-level pose properties. The pose difference descriptions are generated using the automatic annotation pipeline from PoseFix, producing three captions for each sampled pose pair. Similar to ImageScript, we use GPT-4 to refine these captions, transforming the automatically generated annotations into concise, easy-to-read descriptions. The query prompt and annotation workflow are detailed in Fig. 1 and Fig. 3 respectively. Dataset statistics. The dataset generated using PoseFixs automatic pipeline is referred to as ImageDiff-A, while the GPT-4-refined version is named ImageDiff-R. Images are initially sampled from Human3.6M (15k), MPI-INF3DHP (25k), COCO (5k), and MPII (5k) datasets, following the same setup as ImageScript-A. Images are subsequently selected from the corresponding dataset following the method mentioned above. The human pose difference descriptions for each image pair are then generated via the automatic pipeline to construct ImageDiff-A. For ImageDiff-R, 6,250 examples are uniformly sampled from ImageDiff-A to form the training set, and 2000 image pairs are sampled from the 3DPW dataset for the test set. Finally, GPT-4 is employed to refine the text descriptions in ImageDiff-R. A.3. Training Data Details"
        },
        {
            "title": "We employ specific tasks and datasets for each training",
            "content": "stage of UniPose, as summarized in Tab. 1. In details: Pose-Text Alignment Pretraining Stage. We incorporate four pose-text-related tasks: two pose comprehension tasks (Pose-to-Text and Pose-Diff), one pose generation task (Text-to-Pose), and the Pose-Edit task. Drawing in3 Configuration Batch Size Learning Rate Epochs Image Res Patch Size Warmup Epochs LR Schedule Optimizer Pose-Text Align Pretraining Visual Projector Pretraining Instruction Finetuning 24 1.5e-4 6 8 5e-5 2 8 5e-5 2 336 336 / 256 256 14 14 / 16 16 0.03 Cosine AdamW Table 3. Training hyperparameters of UniPose. Image Res denotes the input image resolution of CLIP-ViT and Pose-ViT, and the same as Patch Size. spiration from the success of PoseScript [14] and PoseFix [13] in leveraging automatic captioning pipelines to scale datasets, we use PoseScript-A and PoseFix-A, both rich in automatically generated captions, as the training set. This extensive data effectively facilitates the alignment of pose and text modalities. Visual Projector Pretraining Stage. We include three two pose comprehension tasks image-related tasks: (Image-to-Text and Image-Diff), and one pose generation task (Image-to-Pose), using ImageScript-A, ImageDiffA, and the PoseEst datasets for training. Instruction Fine-tuning Stage. In this stage, the model is trained across all tasks to ensure it understands and generates text aligned with human expression. The training process uses the PoseEst dataset, human-annotated datasets such as PoseScript-H and PoseFix-H, and GPTrefined datasets like ImageScript-R and ImageDiff-R. Additionally, we design task-specific instruction templates to enhance UniPoses instruction-following capabilities, detailed in Tab. 2. B. Implementation details B.1. Pose Tokenizer We provide detailed explanation of the training objectives for the pose tokenizer. The pose tokenizer is trained using reconstruction loss Lr, embedding loss Le, and commitment loss Lc. To further improve the generated pose quality, we utilize vertices and position regularization in the reconstruction loss, as follows: Lvq = Lr + Le + Lc, where, Lr = λ1 (cid:98)p p2 + λ2 (cid:98)v v2 + λ3 Le = sg [z] (cid:98)z2 (cid:13) (cid:13) (cid:13)(cid:98)j 2 , Lc = sg [(cid:98)z]2 2 , (cid:13) (cid:13) (cid:13)2 , (7) where and denotes the ground truth SMPL mesh vertices and joints positions derived from p, (cid:98)v and (cid:98)j denotes the predicted vertices and positions derived from (cid:98)p, sg[] is 4 the stop gradient operator, and λ1, λ2 and λ3 are the weighting factors. Training Configurations. For the training of Pose Tokenizer, we use AdamW as the optimizer with batch size of 256 and an initial learning rate of 2e-4. The model is trained for 240 epochs and the weighting factors λ1, λ2 and λ3 are set to 20, 100, 100 respectively. We set the codebook size to 2048, representing each 3D pose with 80 discrete tokens. Following TokenHMR [15], we augment random joints with noise starting at 0.01, progressively increasing after every 5K iterations. To further enhance robustness to global orientation variations, we introduce random perturbations of -45 to 45 degrees in the z-direction and -20 to 20 degrees in the and directions. The effect of global orientation noise is analyzed in Sec. C. B.2. Retrieval Model To compute the Pose-Text retrieval metric, retrieval model is required to rank large collection of poses based on their relevance to given textual query, and vice versa. Pose-Text Retrieval Model consists of pose encoder and text encoder. For pose feature extraction, we directly employ the pose encoder from the pose tokenizer and add 1D Conv for dimensionality reduction. For the text encoder, we use bidirectional GRU [12] with one layer for text feature extraction, with word embeddings and the text tokenizer derived from pretrained DistilBERT [55] model. Both pose and text are encoded into 512-dimensional feature vectors. Following PoseScript [14], we adopt the Batch-Based Classification (BBC) loss as the training objective: LBBC ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 log (cid:80) exp(γ(xi, yi)) exp(γδ(xi, yj)) (8) where γ is learnable temperature parameter, δ is the cosine similarity function, and (xi, yi) denotes pose-text pairs. Pose Pair-Text Retrieval Model is designed for retrieving pose pairs and text in the Pose/Image-Diff task. Its architecture is similar to the pose-text retrieval model, with the key difference being that the pose encoder processes each pose in the pair separately. The extracted features are concatenated along the channel dimension and passed through multiple 1D Conv layers for dimensionality reduction. Both the pose encoder and text encoder generate 512-dimensional feature vectors, utilizing the same training objective as the Pose-Text retrieval model. Training Configurations. Following PoseScript and PoseFix, the retrieval models are first pretrained on automatically generated captions (PoseScript-A and PoseFix-A) and then fine-tuned on human-written captions (PoseScript-H and PoseFix-H). The retrieval models are trained for 120 epochs across the pretraining and fine-tuning stages. We use the Adam optimizer, with batch size of 512 for pretraining and 32 for fine-tuning. The learning rate is set to 2e-4, Method RP 2T RT 2P mRecall TopTop-5 Top-10 Top-1 Top-5 Top-10 Pose-Text Retrieval PoseScript UniPose 22.3 31.3 50.1 60.1 Pose Pair-Text Retrieval PoseFix UniPose 13.9 15. 33.2 34.0 62.9 73.0 45.2 44.7 22.1 31.4 51.4 62.5 14.1 15. 30.1 34.0 63.1 73.8 42.5 44.6 45.3 55.5 30.0 31.3 Table 4. The retrieval results on the PoseScript [14] and PoseFix [13] datasets. We report Top 1 / 5 / 10 RP 2T and RT 2P , along with the mean recall (mRecall), which is the average of all retrieval recall values. AMASS MOYO MPJPE PA-MPJPE MPJPE PA-MPJPE w/o. Noise w/. Noise 6.7 6.2 3.8 3.7 32.6 23.1 11.7 11.3 Table 5. Ablation on global orientation noise for the Pose Tokenizer. and the learnable temperature parameter γ is initialized to 10. In the main text, all experiments use our proposed retrieval model, except for Text-to-Pose task, which utilizes the retrieval model from PoseScript [14]. B.3. UniPose The detailed training hyperparameter settings for UniPose are provided in Tab. 3. In the Pose-Text Alignment Pretraining stage, UniPose is trained for 6 epochs with batch size of 24 and learning rate of 1.5e-4. For the Visual Projector Pretraining and Instruction Fine-tuning stages, the model is trained for 2 epochs with batch size of 8 and learning rate of 5e-5, respectively. Each stage includes warm-up period of 0.03 epochs. We adopt the cosine learning rate schedule and use the AdamW optimizer. UniPose incorporates two vision encoders: CLIP-ViT and Pose-ViT, with the input image resolutions and patch sizes of 336 / 14 and 256 / 16 respectively. The output feature map of the Pose-ViT is resized using bilinear interpolation to ensure the visual token count aligns with that of the CLIP-ViT. C. Additional Experiments C.1. Retrieval Model Tab. 4 shows the retrieval results on the PoseScript and PoseFix test sets. All methods are pretrained on automatic captions (PoseScript-A and PoseFix-A) and fine-tuned on human-written captions (PoseScript-H and PoseFix-H). Our Pose-Text retrieval model significantly outperforms PoseScript across all metrics, improving retrieval performance by over 10%. For Pose Pair-Text retrieval, our model also Figure 4. Qualitative comparison on pose estimation task. We compare multi-modal LLMs (ChatPose [18]) and traditional HMR methods (TokenHMR [15]) with our UniPose on LSP [32] dataset. Fine-tuning UniPose on this dataset resulted in impressive reasoning capabilities, highlighting the models adaptability and generalization to new data. E. Limitation In pose estimation task, the performance of MLLMsbased models still lags behind specialized methods. We argue that these limitations may stem from the constraints imposed by the frozen visual encoder. Future research will focus on developing techniques that enable large language models to more effectively integrate pose-relevant visual features from diverse visual encoders, thereby enhancing their ability to handle complex pose estimation tasks. Figure 5. Qualitative comparison on reasoning-based pose estimation task. We evaluate the models reasoning capabilities in multi-person images. achieves superior performance. The results demonstrate the effectiveness of our approach in aligning the pose representations with textual descriptions. C.2. Pose Tokenizer Tab. 5 illustrates the impact of global orientation noise on the Pose Tokenizer. All methods are trained on the standard training sets of AMASS [47] and MOYO [59], and evaluated on the AMASS test set and MOYO validation set. The results demonstrate that introducing random noise to global orientation enhances tokenizer robustness, particularly on the MOYO dataset, where MPJPE improves by 9.5. stronger tokenizer benefits UniPose in handling various pose-related tasks. Therefore, we select the noiseaugmented version as the final tokenizer. D. Qualitative Evaluation We present the qualitative results of UniPose on pose estimation tasks. In Fig. 4, we provide visualizations of UniPoses performance on traditional pose estimation tasks, comparing it with both the traditional method TokenHMR [15] and MLLM-based method ChatPose [18]. The results show that our approach more accurately estimates human poses, even in scenarios with complex limb articulations. In Fig. 5, we demonstrate UniPoses performance on reasoning-based pose estimation tasks. For this, we select 8000 multi-person images from the PoseEst dataset and follow the annotation approach of ChatPose, leveraging GPT4 [1] to label each individuals behavior, clothes, and pose."
        }
    ],
    "affiliations": [
        "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China",
        "University of Chinese Academy of Sciences, China"
    ]
}