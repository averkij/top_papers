{
    "paper_title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
    "authors": [
        "Yinjie Wang",
        "Ling Yang",
        "Guohao Li",
        "Mengdi Wang",
        "Bryon Aragam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 6 0 3 4 0 . 2 0 5 2 : r ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization Yinjie Wang1,* Ling Yang2,* Guohao Li3 Mengdi Wang2 Bryon Aragam Project: https://github.com/Gen-Verse/ScoreFlow"
        },
        {
            "title": "Abstract",
            "content": "Recent research has leveraged large language model multi-agent systems for complex problemsolving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, simple yet highperformance framework that leverages efficient gradient-based optimization in continuous space. ScoreFlow incorporates Score-DPO, novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have demonstrated proficiency in solving natural language tasks [25, 33, 1, 2, 41, 42]. Furthermore, the multiagent system (workflow) of LLMs, where multiple agents coordinate and exchange information to complete tasks, enables LLM-based agents to collaborate and solve complex tasks across wide range of domains, such as mathematical problem solving [47, 38], question answering [24], and coding tasks [12, 28]. These manually designed agentic workflows, however, require significant effort and have limited capacity to handle tasks across diverse domains. Therefore, the emerging focus in this area is to address the limitations of static workflows by developing automated methods for workflow generation and optimization. These optimizations can target various aspects, including prompt refinement, hyperparameter tuning, and workflow structure design [17, 49, 44, 14, 46, 7, 19, 21, 32, 45]. The automated optimization methods can be constrained by the limitations inherent in pre-defined workflow structures and the rigidity of workflow space representations [17, 49, 44, 21]. DyLAN [21] thoughtfully emphasizes the communication structure within LLM debates but overlooks other potential communication structures. GPTSwarm [49] leverages graph-based structures and employs reinforcement fine-tuning for optimization. However, the lack of consideration for conditional states within the graph structure imposes restrictions on the search space. To improve representation capabilities, AFlow [46] and ADAS [14] employ code as representation for workflow, facilitating robust and flexible workflow searches. However, ADAS faces challenges with inefficient search processes and coarse workflow storage, which leads to the accumulation of irrelevant data and increased complexity, ultimately reducing its effectiveness. To address these issues, AFlow employs variant of the Monte Carlo Tree Search as an optimization method to enhance efficiency. However, the overly rapid convergence on workflow structures, combined with the discrete optimization method, restricts the exploration of the search space, often leading to suboptimal outcomes. Additionally, they all optimize single workflow for the entire task set, which limits adaptability and scalability for larger datasets containing diverse problems [45, 32]. To address these challenges, we propose ScoreFlow, an automated and cost-efficient multi-agent workflow generation framework that employs novel optimization method to achieve high performance, scalability, and adaptability. For each given task, the workflow generator constructs its workflow using code as representation and the generator is further optimized based on evaluation scores feedback. *Equal contribution. 1University of Chicago. 2Princeton University. 3University of Oxford. Correspondence to: yangling0818@163.com, yinjie@uchicago.edu. 1 The loss-gradient optimization makes it more flexible and scalable than previous discrete optimization methods [46, 14, 21]. Furthermore, by leveraging an open-source LLM as the foundational model for workflow generation, our framework minimizes the costs associated with workflow generation. This approach addresses the challenge of high API call expenses inherent in the workflow generation process [14, 32]. In the optimization process, we collect preference pairs from evaluation scores to construct preference data, which are subsequently used to fine-tune the workflow generator via novel variant of direct preference optimization (DPO) [27]. While DPO is efficient and stable, variance and inaccuracies in evaluation scores reduce the reliability of preference data, slowing convergence and hindering optimal performance within limited iterations. To address these limitations, we propose widely applicable preference optimization method, Score-DPO, which incorporates quantitative score information directly into the optimization process. We highlight our following contributions: ScoreFlow: We introduce ScoreFlow, simple yet flexible, automated, and adaptive framework for agentic workflow generation and optimization, minimizing the need for human intervention. Score-DPO: We propose Score-DPO, an optimization method that can be broadly applied in similar settings, leveraging quantitative evaluation feedback rather than relying solely on preference pairs by integrating evaluation scores into the preference optimization process. Its effectiveness is demonstrated through both experimental results and theoretical analysis. Extensive Evaluations: We evaluate ScoreFlow with Score-DPO on six benchmark datasets across three diverse tasks: question answering, coding, and mathematical reasoning. Our approach outperforms baseline methods by 8.2%. Extensive studies further highlight the robustness, scalability, and cost-efficiency of ScoreFlow across different models and reveal its ability to enable smaller models to surpass larger models in performance while achieving greater cost efficiency."
        },
        {
            "title": "2.1 Agentic Workflow Optimization",
            "content": "Automated Optimizations for Prompt and Hyperparameter Automated optimization methods emphasizing prompt optimization [11, 44, 40, 17] or hyperparameter optimization [29] can enhance performance; however, they impose limitations on the workflow structure and often require manual modifications to accommodate new tasks, restricting their adaptability and scalability. Automated Optimizations for Workflow Structure Workflow optimization methods [48, 49, 14, 46, 7, 19, 21, 32, 45] focus on refining the structure of workflows, making them more robust for handling diverse tasks. However, the inflexibility and limitations in workflow representation, such as the loss of conditional states within the graph structure, may restrict the search space and consequently hinder the ability to accommodate diverse and complex workflows. To address this challenge, ADAS [14] and Aflow [46] adopt code as representation for workflows. However, the performance of ADAS is constrained by its accumulated irrelevant information and increased complexity in optimization, hindering agents ability. Aflow employs Monte Carlo Tree Search-based method to efficiently identify optimal workflows; however, its tendency toward premature convergence on workflow structures limits the exploration of the search space. Moreover, the discrete optimization method, which involves randomly selecting failed cases and feeding them back to the optimizer LLM to refine the workflow, imposes significant limitations on scalability."
        },
        {
            "title": "2.2 Learning from Preferences for Language Models",
            "content": "PPO Proximal Policy Optimization (PPO) [30] process preference feedback in two stages. First, reward model Rϕ is trained on the preference dataset DR, where each entry (x, yw, yl) consists of prompt x, preferred response yw, and rejected response yl. The reward model is optimized by minimizing the following loss function, which is inspired by the Bradley-Terry (BT) model [5] for pairwise ranking: E(x,yw,yl)DR (cid:2) log σ(cid:0)Rϕ(x, yw) Rϕ(x, yl)(cid:1)(cid:3). (1) 2 Figure 1: Pipeline of ScoreFlow. First, for each problem in the dataset, multiple workflows are generated. Next, an executor is employed to execute these workflows for corresponding problems, resulting in evaluation scores. Based on these scores, preference data is collected. Subsequently, incorporating the score information, the Score-DPO algorithm is used to fine-tune the generator. This process is iterated until the maximum number of iterations is reached or convergence is achieved. Next, the policy model πθ is refined by maximizing the reward assigned to its generated responses, while maintaining soft KL divergence constraint to prevent degeneration. The objective is expressed as: ExDπ,yπθ(yx) (cid:2)Rϕ(x, y)(cid:3) βDKL(πθπref ), (2) where πref represents the reference policy, and β is hyperparameter controlling the KL penalty. DPO Direct Preference Optimization (DPO) [27] facilitates direct policy optimization using preference data, eliminating the need for explicit reward models or active policy sampling. This approach enhances both the efficiency and stability of the optimization process. From the closed-form solution of Equation 2, the implicit reward can be expressed as Rϕ(x, y) = β log (cid:0)πθ (y x)/πref (y x)(cid:1) + βZ(x), where πθ is the optimal policy and Z(x) is partition function. The policy model can then be directly optimized using the reward objective in Equation 1, resulting in the DPO loss: E(x,yw,yl)DR where r(x, y) := β log (cid:0)πθ (y x)/πref (y x)(cid:1). (cid:2) log σ(cid:0)r(x, yw) r(x, yl)(cid:1)(cid:3), When the data format includes associated evaluation scores for each sample (as in our setting), rather than solely chosen and rejected pairs, we propose Score-DPO, which integrates these scores into the training process to enhance performance. This approach achieves improved performance over standard DPO while maintaining its efficiency and stability in our applications."
        },
        {
            "title": "3.1 Background",
            "content": "We provide preliminary overview of ScoreFlows inference process, as illustrated in Figure 2. Given Math tasks and B, along with selectable agent typesprogrammer, customizable operator, ensemble operator, and reviewera Python-based workflow is generated for each task, where the agent sets of workflows and contain one and five agents, respectively. Each task is then input into its respective workflow to produce the executed result. Now we formalize the LLM multi-agent workflow optimization problem and some notations as follows. Given an input task q, formatted as prompt, we want to determine the optimal workflow G(q) to solve this task, where is the workflow generator. workflow function Wf is defined as mapping that maps the integration of some task and the agent set , (q, ), to executed results Wf (q, ), typically the solution to the task. The agent set consists of collection of agents, each characterized by their system prompts, temperature settings, and other relevant parameters. Then, the workflow is defined as the combination of an agent set and workflow function: (V, Wf ). We define the workflow search space as: = {(V, Wf ) V, (V, Wf ) satisfies the condition C}, where represents the whole agent space. 3 Figure 2: Illustration of the inference process: Two distinct workflows are generated for two GSM8K problems, and their executed results are evaluated. The executor utilized is GPT-4o-mini, with temperature of 0. This plot highlights the adaptivity of the generation process. The condition imposes constraints on the search space, such that Wf is executable for the agent set . Given these notations, our optimization objective is to identify the optimal workflow generator: = arg max G:Im(G)W EqD (cid:2)S(q, G(q))(cid:3), where represents the dataset of tasks, and is third-party evaluator for the result generated by executing the workflow G(q) on task q, such as human-provided score, the average win rate, or other relevant metrics. Using code as representation of the workflow function Wf [14, 46] can account for linear sequences, loops, conditional logic, and provide flexibility that exceeds graph or network structures. Furthermore, following Aflow [46], we characterize agents in as operators. The operators are predefined, reusable combinations of agent nodes representing common operations, such as programmers, reviewers, revisers, question-answering operators, ensemble operators, test operators and customizable operators, etc. By allowing the system prompts within operators to be customizable by the generator G, we achieve optimization for the prompt, expand the operator space V, and enrich the search space W. To make the workflow adaptive for the input task q, that is, to adapt the chosen operators and the structural complexity of the generated workflow according to the input problem, it is necessary to extract semantic information from q. Specifically, we use an open-source pre-trained large language model as the base model for our generator G. The input to the generator consists of the combination of the task and guidance on generation, including format requirements and introductions to available operators, all formatted as guidance prompt. The detailed guidance prompt is provided in the Appendix A.2."
        },
        {
            "title": "3.2 ScoreFlow Overview",
            "content": "In this section, we provide high-level overview of our proposed method, ScoreFlow, as illustrated in Figure 1, while deferring the detailed procedural steps to Sections 3.3 and 3.4. At each iteration, we start by collecting preference data. For each task, we generate multiple workflows using the generator G, evaluate the execution results to obtain evaluation scores, and derive preference pairs based on these scores. To optimize the generator using this preference dataset, we propose Score-DPO, an enhanced version of Direct Preference Optimization (DPO) [27]. The generator is fine-tuned on preference dataset, and the updated generator is employed in the subsequent iteration. The iterative process stops when it achieves convergence or reaches the maximum iteration number ."
        },
        {
            "title": "3.3 Quantitative Labeling of Preference Workflows",
            "content": "In this section, we explain the process of assigning quantitative labels and collecting preference workflow data. We generate workflows for each task q, denoted as gi(q), where 1 k. Each workflow gi(q) produces execution results for the corresponding q, which are subsequently evaluated to derive an associated evaluation score, denoted as si, where si [0, 1]. The evaluation score is derived by using an independent executor LLM to execute the workflow, calculating the average F1 score or win rate from their outputs in our experiments. Unlike self-improvement methods, which rely on the generator for 4 evaluation [16], thereby making the iteration self-referential, our approach uses third-party sources (e.g., validation datasets and executor LLM). Next, we construct preference pairs for task problem in the form Dq = {(q, gi(q)), (q, gj(q))) si > sj}, which are then aggregated to form the complete preference dataset, Dpre = (cid:83) qD Dq. For simplicity, we denote each element in Dpre as (w, l), where the winner includes the prompt x, chosen workflow yw, and evaluation score sw, while the loser includes x, rejected workflow yl, and score sl."
        },
        {
            "title": "3.4 Optimization via Score-DPO",
            "content": "We observe that directly using DPO to finetune the generator on collected preference data results in slow convergence and an inability to achieve optimal performance. These issues are due to errors and variance in the evaluation scores. We propose widely applicable optimization method Score-DPO, refined version of DPO designed to address these challenges. Our experiments demonstrate the superiority of Score-DPO in optimizing the LLM workflow generator, suggesting its suitability for similar settings. We elaborate on the two improvements of Score-DPO as follows. Enhanced Sampling Distribution The slow convergence and suboptimal performance observed when applying DPO in our setting can be attributed to inaccuracies in the collected preference data, caused by the unavoidable variance and error in evaluation scores. To address this, we propose upweighting sample pairs (w, l) with larger score differences sw sl. Specifically, we introduce function d(x, y) : [0, 1]2 [0, 1] that is strictly monotonically increasing with respect to y. We then up-weight the sampling probability of data pairs with larger score differences by increasing their likelihood according to (w, l) d(sw, sl)P (w, l), where (w, l) represents the uniform random sampling distribution over the preference dataset Dpre. This adjustment ensures that pairs with greater score differences are prioritized during sampling, enhancing the effectiveness of the optimization process. Incorporate Evaluation Scores into the Ranking Objective There have been some alternative formulations for the Bradley-Terry (BT) [5] ranking objective σ(rw rl) that are more effective than DPO [23, 4, 26], where rw := β log(πθ(ywx)/πref (ywx)) and rl := β log(πθ(ylx)/πref (ylx)). In our setting, we incorporate the evaluation score to guide the implicit reward. Specifically, we define the score-based BT ranking objective as σ(r := (1 (sl))rl, and (x) : [0, 1] [0, 1] is strictly monotonically increasing function. Empirically, this approach ensures that data points with more deterministic evaluation scores have greater influence on the loss function. Finally, we have the loss function of Score-DPO as := (sw)rw, ), where r LScore-DPO = E(w,l)P [log σ(r )]."
        },
        {
            "title": "3.5 Analysis of Score-DPO",
            "content": "While DPO is known to struggle with effectively learning preference rankings [6], the following theorem will demonstrate that this score-guided approach aligns the influence of each sample on the optimization objective with the magnitude of its evaluation scores. To formalize our analysis, we introduce notation to quantify the influence of each specific sample on the optimization objective. Definition 3.1 (per-sample influence). For given sample z, the influence of on the objective function, referred to as the per-sample influence, is defined as: I(z) = rz E(w,l)P (cid:2)log σ(cid:0)r (cid:1) 1z{w,l} (cid:3) . The per-sample influence I(z), which is the gradient contributed by sample z, represents the quantitative impact of on the optimization objective. When I(z) > 0, the optimization process increases the logits of z, making it more likely to be preferred. When I(z) < 0, it decreases the logits of z, making it less likely to be preferred. The following Theorem 3.2 demonstrates the effect of score-guidance on I(z). Theorem 3.2. Let function d(x, y) : [0, 1]2 [0, 1] be strictly monotonically increasing with respect to y, and function (x) : [0, 1] [0, 1] be strictly monotonically increasing in x. The per-sample influence for sample is given by: I(z) = E(w,l)P [d(sw, sl)σ(r w)(cid:0)f (sw)1w=z (1 (sl))1l=z (cid:1)], which is strictly monotonically increasing with the score sz when (1 (sz))1 rz 1(sz) holds. Therefore, Score-DPO can incorporate score information into self-sampling preference optimization, enabling the optimization process to account for quantitative information, instead of only using the bare preference pairs information, and can reduce the error and variance caused by inaccuracies in the score. Note that the condition stated in Theorem 3.2 is not restrictive, as rz 1 provides sufficient condition for its validity. Furthermore, our experimental results (in Appendix A.3.3) indicate that rz 1 holds with an approximate probability of 91.1% during the optimization process prior to convergence."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets We focus on six public datasets, covering range of tasks, including math problems, questionanswering problems, and coding problems. Specifically, we utilize the full datasets for HumanEval [8] and MBPP [3]. Following the approach of Aflow [46], for GSM8K [9], we use the 1,319 data points in the test set. For the MATH dataset, to emphasize advanced and challenging problems, we select problems with difficulty level of 5 from the following problem types: Combinatorics and Probability, Number Theory, Pre-algebra, and Pre-calculus, as done by Hong et al. [12]. For DROP [10] and HotpotQA [43], we follow the methodology outlined in Hu et al. [14], Shinn et al. [31], and Zhang et al. [46], randomly selecting 1,000 samples from each dataset. We split the data into validaton and test set using 1:4 ratio. Baselines The manually designed static workflow baselines include: direct LLM invocation, Chain of Thought [36], Self-Consistency CoT (generate 5 responses to ensemble) [34], MedPrompt (3 responses and 5 votes) [24], MultiPersona Debate [35], and Self-Refine (2 rounds) [22]. We also compare with code-representational automated workflow optimization methods: ADAS [14] and Aflow [46], where we use GPT-4o-mini as their optimization model. We set the iteration rounds of Aflow to 20, as specified by Zhang et al. [46]. Models By default, we use Llama-3.1-8B-Instruct as the base model for our generator (inference performed using vLLM [18]), and GPT-4o-mini as the executor (inference via API, with temperature of 0). In the ablation studies, we use Qwen2.5-7B-Instruct [39] as the generator and employ GPT-4o and DeepSeek series models [20] as the executors. All experiments used 2 A6000 GPUs using LoRA [13]. Metrics and Evaluation Scores We report the solve rates (evaluated 3 times and averaged) in our final results. We use GPT-4o-mini as the judge model for MATH, DROP, and HotpotQA to avoid format inconsistency issues. In each iteration of our optimization process (total 3 iterations), we generate = 8 workflows for each problem and obtain their evaluation scores, where we do not use the judge model to reduce cost and computational overhead. Specifically, we use the F1 score as the evaluation metric for DROP and HotpotQA, and solve rates for the remaining datasets (evaluated 3 times and averaged). To apply Score-DPO, we set (x) = and d(x, y) = (x y)3 as the default choices. An ablation study on the selected functions is provided in Appendix A.3.2."
        },
        {
            "title": "4.2 Results and Analysis",
            "content": "Main Results The main results are presented in Table 1. Our proposed method, ScoreFlow, consistently outperforms all manually designed workflow methods as well as automated workflow optimization methods included in the baselines across all benchmarks. Notably, our method achieves an average solve rate of 85.3%, surpassing the baseline methods by margin of 8.2%. The two automated workflow optimization methods, despite employing GPT-4o-mini as the workflow generator, consistently underperform compared to our approach, which utilizes significantly smaller 8B model as the generator, across all tasks. These results highlight the robustness and effectiveness of ScoreFlow in optimizing workflows and achieving improved performance across diverse tasks. Improvements of Proposed Score-DPO To demonstrate the utility of our preference optimization method, Score-DPO, we compare our results with additional designed baselines that replace our finetuning method with alternative approaches (while retaining our overall pipeline, ScoreFlow): supervised finetuning (SFT), proximal policy optimization (PPO), and direct preference optimization (DPO). For When format inconsistencies arise, we use judge model to resolve them (e.g., 0.1 should equal to 10%). 6 Table 1: Comparison of performance between manually designed workflow methods and automated optimization workflow methods. All methods are executed using GPT-4o-mini, with each tested three times, and the average results reported."
        },
        {
            "title": "Method",
            "content": "IO CoT [36] CoT SC [34] MedPrompt [24] MultiPersona [35] Self Refine [22] ADAS [14] Aflow [46] ScoreFlow (Ours) Question Answering Math Reasoning HotpotQA DROP HumanEval MBPP GSM8K MATH"
        },
        {
            "title": "Average",
            "content": "73.6 73.4 74.0 74.4 73.1 73.6 78.5 77.9 86.0 81.6 83.2 83.2 83.0 81.3 82.5 81.3 83.5 86.2 90.1 91.6 92.9 92.1 92.9 91.1 88.8 92.9 95.9 69.5 70.4 71.3 69.2 70.4 70.0 68.7 82.9 84.7 89.1 88.3 88.6 88.1 89.8 87.5 90.5 90.8 94.6 52.2 53.4 53.8 53.7 51.9 50.0 51.7 55.8 64. 76.0 76.7 77.3 76.8 76.5 75.8 76.6 80.6 85.3 Table 2: Comparison of different optimization methods within our ScoreFlow framework: We retain our pipeline, ScoreFlow, and replace the finetuning method to serve as baselines. Each method was tested three times, and we report the average solve rates on both validation and test set. The value on the left represents the performance on the validation set, while the value on the right represents the performance on the test set."
        },
        {
            "title": "Method",
            "content": "HotpotQA DROP HumanEval MBPP GSM8K MATH"
        },
        {
            "title": "85.9\nSFT\n84.8\nPPO\nDPO\n86.9\nScore-DPO (Ours) 89.2 86.0 88.5 86.2 87.9",
            "content": "85.5 82.3 86.0 83.8 85.3 84.2 84.0 84.2 84.1 88.1 87.9 88.3 93.4 92.7 95.9 95.9 49.6 54.8 88.5 89.8 83.5 82.0 50.0 55.2 87.7 89.2 83.7 82.9 84.1 82.9 53.6 60.4 90.2 91.7 86.0 84.7 93.7 94.6 56.5 64.4 SFT, we select the preferred responses to fine-tune the generator, where the preferred responses are sampled using score-sampling (part of our proposed method). For PPO, we follow Huang et al. [15]: Firstly train reward model (share the same base model of generator) using the collected preference data and then optimize the generator based on the reward model. For raw DPO, we directly use the collected preference data and the original Bradley-Terry model, performing optimization for 3 iterations. We report the solve rates on both the validation set and the test set. Table 2 demonstrates the effectiveness of our proposed method, Score-DPO. The SFT method only provides the generator with information about preferred responses, neglecting the rejected responses. Using PPO to optimize over long token sequences in our setting can dilute gradient signals, making it more difficult for the model to discern which parts of the sequence contribute most to the reward. This leads to instability and degraded performance [30, 8]. Score-DPO incorporates specific evaluation ranking information into DPO, achieving both efficiency and the best performance among the baselines. Gradient Loss Optimization and Adaptivity Enhance Scalability We demonstrate how the loss-gradient optimization method, combined with an adaptive framework, enhances scalability by maintaining high performance when applied to more diverse and larger problem datasets. We conduct comparison with the second-best performing baseline method, AFlow. In this experiment, we integrate math, coding, and question-answering tasks by selecting datasets where Aflow shows the smallest performance difference with ScoreFlow: GSM8K (math), MBPP (coding), and DROP (question answering). These datasets are then combined for optimization and evaluation. Figure 3 demonstrates that ScoreFlow achieves more pronounced performance advantage over AFlow on the more diverse combined dataset. AFlow employs standard discrete optimization method to optimize single workflow, where few failed cases are randomly selected and fed into the optimizer LLM to refine the workflow in each iteration. The discrete optimization approach and lack of adaptability could limit scalability. In contrast, our adaptive generation framework, coupled with the loss-gradient optimization method, effectively addresses these challenges. Case Study on Adaptivity In ScoreFlow, the task information is provided to the generator to facilitate the creation of adaptive workflows. Specifically, the generator has the flexibility to select appropriate operators and adapt the complexity of the workflow structure based on the characteristics 7 Table 3: Comparison of Performance Across Different Models and Methods on HumanEval task. We conduct ablation studies on both the generator and the executor. For the generator ablation, we use Llama-3.1-8B-Instruct (Ours) and Qwen2.5-7B-Instruct (Ours*). For the executor ablation, we employ GPT-4o-mini, GPT-4o, DeepSeek-V3, and DeepSeek-coder. Each method is evaluated three times, and we report the average results."
        },
        {
            "title": "Executor",
            "content": "Ours Ours* Aflow IO CoT CoT SC MP MPD SR"
        },
        {
            "title": "95.7\nGPT-4o-mini\nGPT-4o\n97.7\n97.2\nDeepSeek-V3\nDeepSeek-coder 97.7",
            "content": "95.1 97.4 96.9 96.7 92.9 94.7 94.7 93.4 90.1 91.6 93.1 93.4 90.8 90.1 91.3 92.4 92.9 93.9 93.4 94.7 92.1 95.9 93.9 95.2 92.9 96.2 92.9 94. 91.1 92.6 93.9 94.4 of the given problem. In Figure 2, two distinct GSM8K problems are each assigned unique workflows, with the correct answer achievable only when the corresponding workflow is utilized. For the more complex and computation-intensive Problem B, constructing sophisticated workflow with program and review operators helps mitigate calculation errors, while calculation errors occur when using simple workflow. Conversely, for the simple, concise, and calculation-light Problem A, employing an overly complex workflow can result in overthinking and inefficiency. This underscores the critical role of adaptivity in workflow generation. Figure 3: Performance comparison between ScoreFlow and Aflow across various datasets. The y-axis represents the difference in accuracy (%), calculated as the win rate of ScoreFlow minus the win rate of AFlow on test set. The executor for both methods are GPT-4o-mini. The optimizer LLM (generator) for Aflow is GPT-4o-mini, while the generator for Scoreflow is Llama-3.1-8B-Instruct. Specifically, ScoreFlow achieves 88.1% performance on the combined task. Robust for Different LLM Architectures We conduct ablation studies on both the generator and the executor. For the generator ablation, we use Llama-3.1-8B-Instruct and Qwen2.5-7B-Instruct. For the executor ablation, we employ GPT-4o-mini, GPT-4o, DeepSeek-V3, and DeepSeek-coder. Specifically, in the GPT-4o setting, we utilize GPT-4o-mini during the optimization process and switch to GPT-4o at test time, as GPT-4o is prohibitively expensive for optimization in both ScoreFlow and Aflow. From the results in Table 3, we first demonstrate the robustness of our method by showing that it consistently outperforms baseline methods across various combinations of generators and executors. Second, we observe that ScoreFlow, when utilizing smaller models such as GPT-4o-mini and DeepSeek-V3, outperforms the Chain-of-Thought (CoT) outputs of the larger GPT-4o model. The best performance is achieved when GPT-4o or DeepSeek-coder is used as the generator. Notably, although DeepSeek-V3 exhibits performance gap compared to GPT-4o and DeepSeek-coder when evaluated as standalone agent, workflow optimization enables DeepSeek-V3 to achieve performance comparable to that of GPT-4o and DeepSeek-coder. This result highlights the effectiveness of our proposed method. Cost Efficiency Using an open-source LLM as the base model and leveraging fast convergence in optimization minimizes the expense of our method. We firstly analyze the API costs during the inference stage for different methods, across 4 different versions of executors, focusing on the HumanEval task. Results in Figure 4a demonstrate that ScoreFlow enables weaker models to achieve better cost-effectiveness than stronger models, balancing performance and resource usage optimally. For example, ScoreFlow utilizes smaller models such as GPT-4o-mini, DeepSeek-V3, and DeepSeek-coder to achieve significantly (a) Cost during inference on testing set. (b) Cost during optimization. Figure 4: API Cost in Inference and Optimization processes. We analyze the API cost during both the inference and optimization processes, comparing different methods across various executors for the HumanEval task. The left figure illustrates the cost during inference on the testing set in relation to Pass@1 performance. The right figure highlights the total cost of optimization for ScoreFlow and AFlow. The generator for our method here is Llama-3.1-8B-Instruct. better performance than GPT-4os CoT approach, while maintaining much lower costs. Compared to the inference stage, the optimization process in automated workflow optimization methods is more computationally expensive, as it requires evaluation feedback from the executor at each iteration. Therefore we also compare the expense during optimization process with Aflow. From Figure 4b, we demonstrate that ScoreFlow consistently costs less than Aflow in the optimization process while performing better, which highlights the cost-efficiency of our method. Iterative Process Analysis Figure 5 illustrates the changes in solve rate during the iterative process. The consistent increase in test solve rate, followed by its eventual convergence, demonstrates the effectiveness of the iterative approach. We observed that rapid convergence can be achieved as early as the second iteration in our study. Figure 5: Solve rate during iteration process."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose ScoreFlow, an automated, high-performance, and adaptive framework for optimizing multi-agent workflows. The framework leverages the generalizable Score-DPO to achieve robust and efficient optimization. By replacing traditional discrete optimization algorithms with lossgradient-based optimization, we enhance the frameworks flexibility and scalability. Score-DPO, as an effective preference optimization method, reduces inaccuracies and variances in collected data pairs, thereby improving overall performance by incorporating evaluation scores directly into the optimization process. By evaluating six benchmarks spanning question answering, coding, and mathematical reasoning tasks, ScoreFlow achieves an average improvement of 8.2% over baseline methods. Additionally, ScoreDPO consistently outperforms widely used preference optimization methods. Comprehensive ablation studies across various models highlight the robustness and cost-efficiency of our approach. Notably, our 9 method enables smaller models to outperform larger models while incurring lower API costs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [6] Angelica Chen, Sadhika Malladi, Lily Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, and Kyunghyun Cho. Preference learning algorithms do not learn preference rankings. arXiv preprint arXiv:2405.19534, 2024. [7] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Borje Karlsson, Jie Fu, and Yemin Shi. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023. [8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019. [11] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. arXiv preprint self-improvement via prompt evolution. Self-referential Promptbreeder: arXiv:2309.16797, 2023. [12] Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024. [13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [14] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. In NeurIPS Workshop on Open-World Agents, 2024. [15] Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, and Lewis Tunstall. The n+ implementation details of rlhf with ppo: case study on tl; dr summarization. arXiv preprint arXiv:2403.17031, 2024. 10 [16] Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, and Daniel Khashabi. Self-[in] correct: Llms struggle with refining self-generated responses. arXiv preprint arXiv:2404.04298, 2024. [17] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Saiful Haq, Ashutosh Sharma, Thomas Joshi, Hanna Moazam, Heather Miller, et al. Dspy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. [18] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [19] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model agents. arXiv preprint arXiv:2407.12821, 2024. [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [21] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. dynamic llm-powered agent network for task-oriented agent collaboration. In First Conference on Language Modeling, 2024. [22] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36, 2024. [23] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with referencefree reward. arXiv preprint arXiv:2405.14734, 2024. [24] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. [26] Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in direct preference optimization. In Findings of the Association for Computational Linguistics (ACL 2024), 2024. [27] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [28] Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation with alphacodium: From prompt engineering to flow engineering. arXiv preprint arXiv:2401.08500, 2024. [29] Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, Kelly Buchanan, Mayee Chen, Neel Guha, Christopher Re, et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [31] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [32] Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, and Chi Wang. Adaptive in-conversation team building for language model agents. arXiv preprint arXiv:2405.19425, 2024. [33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [34] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. The Eleventh International Conference on Learning Representations, 2022. [35] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multipersona self-collaboration. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 257279, 2024. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [37] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? comprehensive study. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [38] Yiheng Xu, SU Hongjin, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing natural language and code for language agents. In The Twelfth International Conference on Learning Representations, 2024. [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [40] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. [41] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. Advances in Neural Information Processing Systems, 2024. [42] Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph Gonzalez, Bin Cui, and Shuicheng Yan. Supercorrect: Supervising and correcting language models with error-driven insights. arXiv preprint arXiv:2410.09008, 2024. [43] Zhiyu Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. [44] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. [45] Guibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang, and Dawei Cheng. G-designer: Architecting multi-agent communication topologies via graph neural networks. arXiv preprint arXiv:2410.11782, 2024. [46] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. arXiv preprint arXiv:2410.10762, 2024. [47] Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. Achieving 97% on gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint arXiv:2404.14963, 2024. [48] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532, 2024. [49] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan Ashley, Robert Csordas, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Proofs To help prove Theorem 3.2, we need the following lemma. Lemma A.1. (x) = xσ(ax + b) is strictly monotonically increasing with > 0 if 1/x. Proof. Given 1/x, we have (x) = σ(ax + b) axσ(ax + b)(1 σ(ax + b)) = σ(ax + b)[1 ax(1 σ(ax + b))] > 0, which means (x) is strictly monotonically increasing. Now we are ready to prove Theorem 3.2. Proof of Theorem 3.2. Through straightforward calculation, we have I(z) = E(w,l)P (cid:2)log σ(cid:0)r l (cid:1) 1z{w,l} (cid:3) rz rz = E(w,l)P = = E(w,l)P E(w,l)P (cid:104) d(sw, sl)σ(r (cid:104) σ(r (cid:2)d(sw, sl) log σ(cid:0)r (cid:1) 1z{w,l} (cid:3) w)(cid:0)f (sw)1w=z (1 (sl))1l=z (cid:105) (cid:104) (sz)rz)f (sz)d(sz, sl)1w=z E(w,l)P (cid:1)(cid:105) (cid:124) (cid:123)(cid:122) Iw(z) (cid:125) (cid:124) σ((1 (sz))rz (cid:123)(cid:122) Il(z) w)(1 (sz))d(sw, sz)1l=z . (cid:105) (cid:125) By Lemma A.1 and condition (1 (sz))1 rz 1(sz), we have Iw(z) is strictly monotonically increasing with sz, while Il(z) is strictly monotonically decreasing with sz, which implies that I(z) is strictly monotonically increasing with sz. 14 A.2 Detailed ScoreFlow Methods A.2.1 Generator Prompt To generate the workflow for each problem, we use the following prompt: Generator prompt Do NOT import the modules operator and create,"
        },
        {
            "title": "Introducing multiple appropriate operators at appropriate points",
            "content": "PROMPT = \"\"\" Your objective is to output workflow graph, based on the following template: {template} Heres an introduction to operators (these are all you can use, do not create new operators) you can use: {operator introductions} We have the task input as follow. {task} You need to notice: Ensure your graph is based on the given template and is correct to avoid runtime failures. which have already been automatically imported. provided. can enhance performance. generate multiple solutions to ensemble. (IF-ELSE, loops) for more enhanced graphical representation. complexity may corelate with the task complexity. better results, but insufficient information transmission can omit the solution. Your output graph must be optimized and different from the given template graph. Do not output graph without modification! Your output graph can not contain any information of the given task due to project requirement. of this problem will be given as input for operators and other agents will execute this workflow. Only output the optimized graph (remember to add <graph> and </graph>, and the output can not contain specific information of the given task due to project requirement). Consider Pythons loops (for, list comprehensions) to"
        },
        {
            "title": "Do not load the operators not",
            "content": "Here is the optimized graph:"
        },
        {
            "title": "The graph",
            "content": "\"\"\" The workflow template we provide is designed for single agent to guide the generator in adhering to predefined structure and minimizing runtime execution errors. The operator instructions serve as comprehensive descriptions of the permitted operators available for use. The task presented to the generator requires it to select appropriate operators and produce workflow that is both well-structured and adaptive to the given task. Our execution model incorporates greater sophistication compared to open-source generators. Consequently, we include detailed task analysis to guide the generator and ensure that it does not embed specific task-related information directly into the workflow. Additional requirements can be incorporated into the prompt to control the workflow generation."
        },
        {
            "title": "The template we use imposes no prior knowledge of which structure to build or which operator to",
            "content": "choose. We list the templates we used as follows: Template for Question Answering async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution = await self . answer_ge ne rate () return solution Template for Math Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution = await self . custom ( instruction = \" Can you solve this problem by breaking it down into detailed steps and explaining the reasoning behind each step ? \" ) 15 return solution Template for Coding Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution = await self . code_generate ( instruction = \" Can you analyze this problem step by step and generate the code ? \" ) return solution 16 A.2.2 Operator Utilized For mathematical problems, we utilize the following custom operators: the Custom Operator, which generates outputs based on fixed input problem and modifiable instructions; the Programmer, which automatically writes and executes Python code to derive and return the final solution based on the given problem description and analysis; the Ensemble Operator, which evaluates all generated solutions and selects the best one from the solution list; and the Reviewer, which reviews previous solutions to refine and regenerate improved solutions. For question-answering problems, we utilize the following operators: the Custom Operator, the AnswerGenerate Operator, which directly generates answers, including the reasoning process, for the given problem; the Ensemble Operator, which evaluates all generated answers and selects the best one; and the Reviewer, which reviews and refines previous answers to produce improved solutions. For coding tasks, we utilize the following operators: the CustomCodeGenerate Operator, which generates code based on customized input instructions; the Ensemble Operator, which evaluates multiple code solutions and selects the best one; and the Test Operator, which refines the input solution by testing it against public test cases. We also include an Answer Extractor Agent after the final response in each generated workflow to eliminate redundant information and ensure concise and precise evaluation. The design of these operators is based on the Aflow framework [46]. The following are the introductions to the operators we used. Introductions to Operators 1. Custom : Usage : Generates anything based on fixed input problem and modifiable instruction . Format MUST follow : custom ( instruction : str ) -> str You can modify the instruction prompt . The output can serve as the input of next operators or the final output . 2. Cu m Co de en er te : Usage : Generates code based on customized input instruction . Format MUST follow : code_generate ( instruction : str ) -> str The instruction should encourage operator to think step by step , do not add the specific information of the task into the input instruction . The output can serve as the input of next operators or the final output . 3. AnswerGenerate : Usage : Directly generate answer ( including thought ) to the given problem . Format MUST follow : answer_generate () -> str For example : solution = await self . answe r_ge ner ate () The output can serve as the input of next operators or the final output . 4. Programmer : Usage : Automatically writes , executes Python code , and returns the final solution based on the provided problem description and analysis . Format MUST follow : programmer ( analysis : str = None ) -> str The input analysis can be outputs of some other operators , for exmaple : pr og ra m_ solution = await self . programmer ( analysis = solution ) The output can serve as the input of next operators or the final output . 5. ScEnsemble : Usage : Evaluate every solutions , then select the best solution in the solution list . Format MUST follow : sc_ensemble ( solutions : List [ str ]) -> str You can ensemble few solutions , for example : ns le _s ol ti = await self . sc_ensemble ( solutions = solution_list ) The output can serve as the input of next operators or the final output . 6. Review : Usage : Given previous solution , Review operator reviews the previous solution to regenerate the solution . Format MUST follow : review ( pre_solution : str ) -> str pre_solution should be solution from previous operator , for example rev_solution = await self . review ( pre_solution = pre_solution ) The output can serve as the input of next operators or the final output . 7. Test : Usage : Modify the input solution by testing the solution using public test cases . Format MUST follow : test ( solution : str ) -> str teste d_ olution = await self . test ( solution = pre_solution ) 17 A.2.3 The Detailed Algorithm We have the detailed algorithm in Algorithm 1. Algorithm 1 ScoreFlow 1: Input: 2: 3: 4: 5: 6: 1) set of problems = {q1, q2, . . . , qN }. 2) workflow generator parameterized by θ. 3) Number of iterations M. 4) Number of workflows generated per problem in optimization: k. 5) Number of preference samples generated in each iteration: S. 5) Executor LLM for evaluation. 7: 8: Initialize: Generator parameters θ. 9: for = 1 to or not converged do 10: Collect preference data: for each problem do 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: for = 1 to do repeat Use Generator Gθ to generate workflow gi(q) for problem q. until Condition holds for gi(q) Collect the workflow gi(q) for problem q. end for Obtain candidate workflows {gi(q)}k Evaluate each gi(q) with the executor LLM to obtain score si [0, 1]. Construct preference pairs i=1 using g. Dq = (cid:110) ((q, gi(q)), (q, gj(q))) si > sj (cid:111) . end for 21: 22: Aggregate preferences Dpre (cid:83) 23: Update the generator via Score-DPO: 24: 25: 26: 27: for = 1 to or not converged do Generate preference samples (w, l) by sampling distribution (w, l) (w, l)d(w, l). Calculate Obtain loss function: = (sw)rw and = (1 (sl))rl. qD Dq (Denote its raw distribution as ). LScore-DPO = log σ(r ) 28: Fine-tune Gθ to obtain updated parameters θ θ ηθLScore-DPO, where η is the learning rate. end for 29: 30: end for 31: Output: Trained generator Gθ. The condition ensures quality control of our workflow by guaranteeing the absence of runtime errors and adherence to execution time limits. We select = 3 and = 8. We set = 2000 for all datasets except for the smallest one, HumanEval, where we use = 600. 18 A.3 Additional Experiment Results A.3.1 Sampled workflows The following are some examples of the generated workflows. Example Workflow 1 for Question Answering async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution = await self . answer_ge ne rate () solution = await self . review ( solution ) solution_list = [ solution ] for _ in range (3) : solution = await self . custom_2 ( instruction = \" Can you solve this problem by breaking it down into detailed steps and explaining the reasoning behind each step ? \" ) solution_list . append ( solution ) ns em le _s ol ti = await self . sc_ensemble ( solution_list ) return se mb ed _ so lu io Example Workflow 2 for Question Answering async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution_1 = await self . custom_1 ( instruction = \" Can you break down the problem into smaller steps ? \" ) solution_2 = await self . ans wer _g ne at _ 1 () solution_3 = await self . sc_ensemble_1 ( solutions =[ solution_1 , solution_2 ]) solution_4 = await self . review_1 ( pre_solution = solution_3 ) solution_5 = await self . custom_2 ( instruction = \" Can you explain the reasoning behind each step ? \" ) solution_6 = await self . sc_ensemble_2 ( solutions =[ solution_4 , solution_5 ]) solution_7 = await self . review_2 ( pre_solution = solution_6 ) return solution_ Example Workflow 3 for Question Answering async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution_list = [] for _ in range (3) : solution = await self . custom_1 ( instruction = \" Can you break down the problem into smaller steps ? \" ) solution_list . append ( solution ) ns em le _s ol ti = await self . sc_ensemble ( solutions = solution_list ) 19 rev_solution = await self . review ( pre_solution = se ed _ so lu io ) return rev_solution Example Workflow 1 for Math Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution_1 = await self . custom_1 ( instruction = \" Can you break down the problem into smaller steps ? \" ) solution_2 = await self . custom_2 ( instruction = \" Can you explain the reasoning behind each step ? \" ) solution_list = [ solution_1 , solution_2 ] ns em le _s ol ti = await self . sc_ensemble ( solutions = solution_list ) analysis = ns mb le _s lu ti pr ogram_solution = await self . programmer ( analysis = analysis ) final_solution = await self . review ( pre_solution = program_solution ) return final_solution Example Workflow 2 for Math Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution1 = await self . custom1 ( instruction = \" Can you break down the problem into smaller steps and explain each step clearly ? \" ) solution2 = await self . custom1 ( instruction = \" Can you explain the problem and provide step - by - step solution ? \" ) solution3 = await self . custom2 ( instruction = \" Can you describe the problem and provide detailed solution ? \" ) solutions = [ solution1 , solution2 , solution3 ] ns mb le _s ol ti = await self . sc_ensemble ( solutions = solutions ) pr ogram_solution = await self . programmer ( analysis = se mb ed _s lu io ) final_solution = await self . review ( pre_solution = program_solution ) return final_solution Example Workflow 3 for Math Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution_1 = await self . custom_1 ( instruction = \" Can you break down the problem into smaller steps ? \" ) solution_2 = await self . custom_2 ( instruction = \" Can you explain the solution in clear and concise manner ? \" ) analysis = solution_1 + solution_2 pr ogram_solution = await self . programmer ( analysis = analysis ) 20 solutions = [ solution_1 , solution_2 , pr og ra m_ so lu ti on ] ns em le _s ol ti = await self . sc_ensemble ( solutions = solutions ) final_solution = await self . review ( pre_solution = en em bl d_ ol ut on ) return final_solution Example Workflow 1 for Coding Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution = await self . code_generate ( instruction = \" Can you analyze the problem step by step and generate the code ? \" ) solution_list = [ solution ] for in range (3) : solution_list . append ( await self . code_generate ( instruction = \" Can you think step by step and generate the code ? \" ) ) ns em le _s ol ti = await self . sc_ensemble ( solutions = solution_list ) tested_solution = await self . test ( solution = ns le _s lu ti ) return tested_solution Example Workflow 2 for Coding Problem async def run_workflow ( self ) : \"\"\" This is workflow graph . \"\"\" solution_list = [] for in range (5) : solution = await self . code_generate ( instruction = \" Can you generate code to solve problem ? \" ) solution_list . append ( solution ) ns em le _s ol ti = await self . sc_ensemble ( solutions = solution_list ) tested_solution = await self . test ( solution = ns le _s lu ti ) return tested_solution 21 A.3.2 Case study on d(x, y) In this section, we analyze the performance of Score-DPO under various formulations of the function d(x, y) = (x y)α. DPO is special case where α = 0 when ignoring (x). Our results demonstrate that increasing α leads to improved performance, as higher values of α upweight the more deterministic preference pairs, thereby reducing variance and error in the collected preference data. However, when α is taken to an extreme, such as α = 100, the performance deteriorates significantly. This decline occurs because excessively prioritizing only the most deterministic pairs effectively disregards substantial portion of the preference pair data. Moreover, DPO inherently tends to favor out-of-distribution (unseen) responses or data [37]. By omitting less deterministic pairs, the model loses valuable information, which adversely impacts its ability to generalize effectively. Table 4: Case studies for d(x, y) across MBPP, DROP, and MATH datasets. Values represent averaged solve rates on test set."
        },
        {
            "title": "MBPP DROP MATH",
            "content": "DPO d(x, y) = (x y)2 d(x, y) = (x y)3 d(x, y) = (x y)100 82.9 83.5 84.7 80.1 84.2 85.4 86.2 85.1 60.4 61.4 64.4 59.2 A.3.3 Experimental Validation for Condition in Theorem 3.2 The sufficient condition provided in Theorem 3.2 is rz 1. In this section, we evaluate this condition by estimating the probability that it holds during the optimization process. Our analysis reveals that this condition is satisfied with probabilities of 99.8%, 82.2%, and 91.2% for the MATH, DROP, and MBPP datasets, respectively. On average, the condition is upheld with probability of 91.1% across three different tasks in our experiments, demonstrating its robustness across diverse datasets and substantiating its practical applicability in guiding optimization under varied scenarios. Figure 6: The distribution of sample implicit reward during optimization process before convergence (MATH). 22 Figure 7: The distribution of sample implicit reward during optimization process before convergence (DROP). Figure 8: The distribution of sample implicit reward during optimization process before convergence (MBPP). A.3.4 Detailed Cost Data Table 5: The detailed cost value ($) in Figure 4b (optimization process on test data). Method GPT-4o-mini DeepSeek-V3 DeepSeek-coder"
        },
        {
            "title": "Ours\nAflow",
            "content": "2.2570 4.6081 1.4124 2.9160 1.3966 2.8664 23 Table 6: The detailed cost value ($) in Figure 4a (inference process on test data). Method GPT-4o-mini GPT-4o DeepSeek-V3 DeepSeek-coder"
        },
        {
            "title": "Ours\nAflow\nIO\nCoT\nCoT SC\nMP\nMPD\nSR",
            "content": "0.2281 0.2021 0.0483 0.0536 0.3155 0.3497 0.3789 0.1243 5.1549 3.9549 1.2281 1.9688 7.3738 9.5230 10.8530 1.8651 0.1336 0.1229 0.0251 0.0300 0.1825 0.2265 0.2425 0.0728 0.1246 0.1253 0.0301 0.0473 0.1817 0.2392 0.2276 0."
        }
    ],
    "affiliations": [
        "Princeton University",
        "University of Chicago",
        "University of Oxford"
    ]
}