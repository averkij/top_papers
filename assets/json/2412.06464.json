{
    "paper_title": "Gated Delta Networks: Improving Mamba2 with Delta Rule",
    "authors": [
        "Songlin Yang",
        "Jan Kautz",
        "Ali Hatamizadeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear Transformers have gained attention as efficient alternatives to standard Transformers, but their performance in retrieval and long-context tasks has been limited. To address these limitations, recent work has explored two distinct mechanisms: gating for adaptive memory control and the delta update rule for precise memory modifications. We observe that these mechanisms are complementary: gating enables rapid memory erasure while the delta rule facilitates targeted updates. Building on this insight, we introduce the gated delta rule and develop a parallel training algorithm optimized for modern hardware. Our proposed architecture, Gated DeltaNet, consistently surpasses existing models like Mamba2 and DeltaNet across multiple benchmarks, including language modeling, common-sense reasoning, in-context retrieval, length extrapolation, and long-context understanding. We further enhance performance by developing hybrid architectures that combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, achieving both improved training efficiency and superior task performance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 4 6 4 6 0 . 2 1 4 2 : r Preprint GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH DELTA RULE Songlin Yang MIT CSAIL yangsl66@mit.edu Jan Kautz NVIDIA jkautz@nvidia.com Ali Hatamizadeh NVIDIA ahatamizadeh@nvidia.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Linear Transformers have gained attention as efﬁcient alternatives to standard Transformers, but their performance in retrieval and long-context tasks has been limited. To address these limitations, recent work has explored two distinct mechanisms: gating for adaptive memory control and the delta update rule for precise memory modiﬁcations. We observe that these mechanisms are complementarygating enables rapid memory erasure while the delta rule facilitates targeted updates. Building on this insight, we introduce the gated delta rule and develop parallel training algorithm optimized for modern hardware. Our proposed architecture, Gated DeltaNet, consistently surpasses existing models like Mamba2 and DeltaNet across multiple benchmarks, including language modeling, commonsense reasoning, in-context retrieval, length extrapolation, and long-context understanding. We further enhance performance by developing hybrid architectures that combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, achieving both improved training efﬁciency and superior task performance. Code: https://github.com/NVlabs/GatedDeltaNet"
        },
        {
            "title": "INTRODUCTION",
            "content": "The Transformer architecture has signiﬁcantly advanced the capabilities of Large Language Models (LLMs), showcasing exceptional performance across wide range of tasks due to its effective attention mechanism. This mechanism excels in precise sequence modeling and leverages the parallel processing capabilities of modern GPUs during training. However, the self-attention component scales quadratically with sequence length, leading to substantial computational demands that pose challenges for both training and inference. To mitigate these issues, researchers have explored alternatives such as linear Transformers (Katharopoulos et al., 2020a), which replace traditional softmax-based attention with kernelized dot-product-based linear attention, substantially reducing memory requirements during inference by reframing as linear RNN with matrix-valued states. While early versions of linear Transformers underperformed in language modeling tasks compared to standard Transformers, recent enhancementssuch as incorporating data-dependent gating mechanisms akin to those in LSTMs, exempliﬁed by models like GLA (Yang et al., 2024a) and Mamba2 (Dao & Gu, 2024a)have shown promising improvements. However, challenges persist in managing information over long sequences, particularly for in-context retrieval tasks where traditional Transformers maintain their advantage (Arora et al., 2023a; 2024a; Jelassi et al., 2024; Wen et al., 2024; Akyürek et al., 2024). linear Transformers can be interpreted as implementing an This phenomenon is not surprising: outer-product-based key-value association memory, reminiscent of tensor product representation (Smolensky, 1990). However, the number of orthogonal key-value pairs they can store is bounded by the models dimensionality. When the sequence length exceeds this dimension, memory collisions become inevitable, hindering exact retrieval (Schlag et al., 2021a). Mamba2 addresses this limitation by introducing simple gated update rule, St = αtSt1 + vtk , which uniformly decays all key-value associations at each time step by dynamic ratio, αt. However, this approach does not account for the varying importance of different key-value associations, Equation contribution. Work done during SYs internship at NVIDIA. 1 Preprint potentially leading to inefﬁcient memory utilization. If the model needs to forget speciﬁc keyvalue association, all key-value associations are equally forgotten, making the process less targeted and efﬁcient. In contrast, the linear Transformer with the delta rule (Widrow et al., 1960), known as DeltaNet (Schlag et al., 2021a; Yang et al., 2024b), selectively updates memory by (softly) replacing an old key-value pair with the incoming one in sequential manner. This method has demonstrated impressive performance in synthetic benchmarks for in-context retrieval. However, since this process only modiﬁes single key-value pair at time, the model lacks the ability to rapidly clear outdated or irrelevant information, especially during context switches where previous data needs to be erased. Consequently, DeltaNet has been found to perform moderately on real-world tasks (Yang et al., 2024b), likely due to the absence of robust memory-clearing mechanism. Recognizing the complementary advantages of the gated update rule and the delta rule in memory management, we propose the gated delta rule, simple and intuitive mechanism that combines both approaches. This uniﬁed rule enables ﬂexible memory control: it can promptly clear memory by setting αt 0, while selectively updating speciﬁc content without affecting other information by setting αt 1 (effectively switching to the pure delta rule). The remaining challenge lies in implementing the gated delta rule in hardware-efﬁcient manner. Building upon Yang et al. (2024b)s efﬁcient algorithm that parallelizes the delta rule computation using the WY representation (Bischof & Loan, 1985), we carefully extend their approach to incorporate the gating terms. Our extension preserves the beneﬁts of chunkwise parallelism (Hua et al., 2022; Sun et al., 2023a; Yang et al., 2024a), enabling hardware-efﬁcient training. Our resulting architecture, Gated DeltaNet, consistently outperforms both Mamba2 and DeltaNet across comprehensive suite of benchmarks, including language modeling, commonsense reasoning, in-context retrieval, length extrapolation, and long-context understanding. Building on these results, we also develop hybrid architectures that strategically combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, further enhancing both training efﬁciency and model performance."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "2.1 LINEAR ATTENTION WITH CHUNKWISE PARALLEL FORM It is known that the linear transformer (Katharopoulos et al., 2020b) can be formulated as the following linear recurrence when excluding normalization and query/key activations: St = St1 + vtk Rdvdk , ot = Stqt Rdv where dk and dv represent the (head) dimensions for query/key and value, respectively. By expanding the recurrence, we can express it in both vector form (left) and matrix form (right) as follows: ot = (vik )qt = vi(k qt) Rdv , = (QK M)V RLdv i=1 i=1 where is the sequence length, and RLL is the causal mask deﬁned by Mij = 0 when < j, and 1 otherwise. This formulation makes it clear that linear attention eliminates the softmax operation used in traditional attention mechanisms and instead leverages the linearity and associativity of matrix multiplications, leading to linear complexity. However, both the recurrent and parallel forms are not ideal for efﬁcient training (Yang et al., 2024a), which motivates the use of the chunkwise parallel form (Hua et al., 2022; Sun et al., 2023a; Yang et al., 2024a) for hardware-efﬁcient, linear-time training, as introduced below. Chunkwise parallel form. To summarize, the chunkwise parallel form splits inputs and outputs into several chunks of size C, and computes outputs for each chunk based on the ﬁnal state of the previous chunk and the query/key/value blocks of the current chunk. Following the notation of Sun et al. (2023b); Yang et al. (2024a;b), lets take the query block, q, as an example. We denote 2 Preprint Q[t] := qtC+1:(t+1)C+1 as the query block for chunk t, and qr chunk t. The initial state of chunk is deﬁned as S[t] := S0 recurrence, we have [t] := qtC+r as the r-th query within [t1]. By partially expanding the [t] = SC Sr [t] = S[t] + i=1 [t]ki vi [t] Rdvdk , Equivalently, in matrix form: [t] = Sr or [t]qr [t] = S[t]qr [t] + vi [t] ki [t]qr [t] Rdv i=1 (cid:16) (cid:17) S[t+1] = S[t] + V[t]K [t] Rdvdk , O[t] = Q[t]S [t] + [t] V[t] RCdv Q[t]K (cid:16) (cid:17) where RCC is the causal mask. The above equations are rich in matrix multiplications (matmuls), and by setting to multiple of 16, one can take advantage of tensor coresspecialized GPU units for efﬁcient half-precision matmul operationsfor hardware-efﬁcient training. Typically, is set to small constant (e.g., 64 as implemented in FLA (Yang & Zhang, 2024)), ensuring that the overall computational complexity remains linear with respect to sequence length, enabling efﬁcient modeling of extremely long sequences. 2.2 MAMBA2: LINEAR ATTENTION WITH SCALAR-VALUED DATA-DEPENDENT DECAY Mamba2 (Dao & Gu, 2024a) can be represented by the following linear recurrence (up to speciﬁc parameterization): St = αtSt1 + vtk , ot = Stqt where αt (0, 1) is data-dependent scalar-valued decay term. In the following, we will highlight the decay terms in blue to facilitate clearer comparison with vanilla linear attention. Deﬁne the cumulative decay product γj = i=1 αi, and by expanding the recurrence, we can express the result in both vector form (left) and matrix parallel form (right): ot = vik qt = vi γt γi γt γi (cid:19) Here, Γ RLL is decay-aware causal mask where Γij = γi γj i=1 (cid:18) i=1 (cid:18) (cid:19) qt , = ((QK) Γ) if and Γij = 0 otherwise. This parallel and recurrent formulation is referred to as state space duality (SSD) in Dao & Gu (2024a). Notably, this recurrence structure has also been employed in Gated RFA (Peng et al., 2021), xLSTM (Beck et al., 2024), and Gated RetNet (Sun et al., 2024b). Chunkwise parallel form. Slightly abusing the notation, we deﬁne the local cumulative product of decays within the chunk as γj and 0 otherwise. By partially expanding the recurrence, we obtain the following equations: tC+j i=tC+1 αi. Additionally, we deﬁne (Γ[t])ij = for γj [t] γi [t] [t] = Sr [t] = γr [t]S[t] + γr [t] γi [t] i=1 [t]ki vi [t], or [t] = γr [t]Sr [t]qr [t] = S[t]qr [t] + vi [t] i=1 γr [t] γi [t] ki [t]qr [t] ! This can be equivalently expressed in matrix form as: S[t+1] = γC [t]S[t] + [t]Diag O[t] = Diag γ[t] Q[t]S [t] + (cid:0) (cid:1) K[t] γC [t] γ[t] ! Q[t]K [t] Γ[t] (cid:16) (1) (2) V[t] (cid:17) We observe that the (cumulative) decay term can be seamlessly integrated into the matmuls with minimal computational overhead. This ensures that the chunkwise parallel form remains efﬁcient and compatible with high-performance tensor core-based acceleration. Preprint 2.3 DELTA NETWORKS: LINEAR ATTENTION WITH DELTA RULE The delta update rule (Widrow et al., 1960; Schlag et al., 2021b) dynamically erases the value (vold ) associated with the current input key (kt) and writes new value (vnew ), which is linear combination of the current input value and the old value. This process updates key-value association pair at each time step, where the scalar βt (0, 1) determines the extent to which the old association is replaced by the new one, as shown below. St = St1 (St1kt) + (βtvt + (1 βt)St1kt)) vold vnew = St1 (I βtktk ) + βtvtk Chunkwise parallel form. By partially expanding the recurrence, we have {z } {z } Sr [t] = S[t] i=1 βi [t]ki [t]ki [t] :=Pr [t] + ! i=1 [t]vi βi [t]ki [t] j=i+1 (cid:16) :=Hr βj [t]kj [t]kj [t] (3) (cid:17) {z where Pj [t] involves cumulative products of transition matrices. Yang et al. (2024b) show these take the form of (generalized) Householder matrices, allowing memory-efﬁcient computation through classical WY representation (Bischof & Loan, 1985). Based on this, they introduce two compact representations to optimize the process: {z } } Pr [t] = i=1 r1 wr [t] = βr [t] kr [t] wi [t]ki [t] Rdkdk Hr [t] = i=1 [t]ki ui [t] Rdvdk wi [t](ki [t]kr [t]) (cid:17) i=1 (cid:16) [t] = βr ur [t] vr [t] ! [t](ki ui [t]kr [t]) (cid:17) i=1 (cid:16) (4) (5) ! where wr [t] Rdk and ur [t] Rdv . Substituting these back into Eq. 3 and in matrix form, we have: S[t+1] = S[t] + U[t] W[t]S [t] K[t] (cid:16) O[t] = Q[t]S [t] + (Q[t]K (cid:17) [t] M) U[t] W[t]S [t] (cid:16) (cid:17)"
        },
        {
            "title": "3 GATED DELTA NETWORKS",
            "content": "3.1 FORMULATION: GATED DELTA RULE The proposed gated delta rule is simple yet effective: St = St1 (αt(I βtktk )) + βtvtk (6) (7) (8) where the data-dependent gating term αt (0, 1) controls state decay. This formulation uniﬁes the advantages of both gating mechanisms and the delta rule: the gating term enables adaptive memory management, while the delta update structure facilitates effective key-value association learning. We present formal analysis of the gated delta rule through the lens of the online learning framework introduced by Liu et al. (2024). In this framework, recurrent state updates emerge as solutions to an online learning problem with objective function Lt(S). As shown in Table 1, recent linear RNN architectures typically incorporate regularization term in their online learning objective to prevent state divergence from previous values, thereby enabling memory retention. However, this retention mechanism becomes problematic when the state becomes saturated with information. In such cases, each state must encode superposition of multiple information pieces, making precise retrieval challenging. To address this limitation, Mamba2 and Gated DeltaNet introduce an adaptive scaling factor αt that relaxes the regularization term, allowing controlled deviations between St and St1. This modiﬁcation enables dynamic memory management through selective forgetting. On the other hand, Linear Attention (LA) and Mamba2 use simple linear prediction loss hStkt, vti, while Longhorn (Liu et al., 2024) uses more expressive online regression objective kStktvtk2 for Preprint Table 1: Comparison of different linear RNN models and their corresponding online learning objectives using the framework from Liu et al. (2024). For convenience, we simplify Longhorns vector-valued β to scalar β. Method Online Learning Objective Lt(S) Online Update LA Mamba2 Longhorn kSt St1k2 kSt αtSt1k2 kSt St1k2 2hStkt, vti 2hStkt, vti βtkStkt vtk2 St = St1 + vtkT St = αtSt1 + vtkT St = St1(I ǫktkT DeltaNet Gated DeltaNet kSt αtSt1k2 kSt St1k2 2hStkt, βt (vt St1kt)i St = St1(I βtktkT 2hStkt, βt (vt αtSt1kt)i St = St1 (cid:16)αt(I βtktkT )(cid:17) + βtvtkT , ǫt = ) + ǫtvtkT ) + βtvtkT βt 1 + βtk kt better modeling of key-value associations. The resulting Longhorns update rule closely resembles the delta update rule 1, suggesting the superiority of the (gated) delta rule over Mamba2 in in-context associative recall. From the perspective of fast weight programming (Irie et al., 2022a) and test-time training (Sun et al., 2024a), the hidden state can be interpreted as weight matrix, with the delta rule optimizing the objective L(St) = 1 2 kStkt vtk2 via online stochastic gradient descent (SGD): St+1 = St βtSL(St) = St βt(Stkt vt)k = St (I βtktk ) + βtvtk where βt represents the (adaptive) learning rate. From this perspective, the gated delta rule can be viewed as incorporating an adaptive weight decay term αt into the SGD update, technique widely used in deep learning (Krogh & Hertz, 1991; Andriushchenko et al., 2023). 3.2 ALGORITHM: HARDWARE-EFFICIENT CHUNKWISE TRAINING In this subsection, we describe an efﬁcient chunkwise algorithm for gated delta rule. Chunkwise parallel form. By partially expanding the recurrence, we have Sr [t] = S[t] αi [t] i=1 (cid:16) βi [t]ki [t]ki [t] :=Pr [t] + ! (cid:17) i=1 [t]vi βi [t]ki [t] βj [t]kj [t]kj [t] αj [t] j=i+1 (cid:16) :=Hr [t] (cid:17) We adapt the WY representation in Eq. 4-5 to incorporate the decay term as below, {z } {z Pr [t] = γr [t] wi [t]ki [t] i=1 r1 ! Hr [t] = i=1 [t] = βr ur [t] γr γi [t]ki ui [t] r1 γr [t] γi [t] γC [t] γ[t] ! K[t] kr wr [t] = βr [t] [t]) ! (cid:17) and the proof of correctness can be found at Appendix. Equivalently, in matrix form: i=1 i=1 (cid:16) [t] [t]( wi [t](ki [t]kr vr [t] ui ki [t]kr [t]) !! S[t+1] = γC [t]S[t] + U[t] Diag γ[t] W[t]S [t] Diag O[t] = Diag γ[t] (cid:16) Q[t]S (cid:17) (cid:1) (cid:0) [t] + (Q[t]K [t] Γ[t]) U[t] Diag γ[t] W[t]S [t] (cid:16) (cid:0) (cid:1) (cid:17) (cid:0) (cid:1) Comparison to Eq.1-2 We can see that the key distinction lies in the replacement of the value block V[t] with the pseudo-value term U[t] Diag [t]. This modiﬁcation resembles Eq.6-7, but notably incorporates decay-awareness. W[t]S γ[t] (cid:0) (cid:1) 1The theoretical distinction lies in the optimization approach: Longhorn uses implicit online learning (Kulis & Bartlett, 2010) to derive closed-form globally optimal updates, while DeltaNet optimizes the same objective through one-step explicit gradient descent, as noted by Liu et al. (2024). Despite Longhorns stronger theoretical grounding, we found no signiﬁcant empirical performance differences between these approaches and thus maintain DeltaNets original formulations. } (9) (10) (11) (12) Preprint Outputs MLP SWA MLP Gated DeltaNet Outputs MLP SWA MLP Gated DeltaNet MLP Mamba2 Outputs Linear Norm Gated Delta Rule α β L2 Conv Conv Conv Linear Linear Linear Lin.Lin. Linear Gated DeltaNet-H1 Gated DeltaNet-H2 Inputs Block Design Figure 1: Visualization of the (hybrid) architecture and block design of Gated DeltaNet models. Gated DeltaNet-H1 and H2 use Gated DeltaNet + SWA and Mamba2 + Gated DeltaNet + SWA patterns, respectively. In the block design, query/key paths consist of linear proj., shortconv., SiLU and L2 norm; value path includes linear proj., shortconv. and SiLU; alpha/beta use linear proj.; and output gate applies linear proj. with SiLU. UT transform. To maximize hardware efﬁciency, we apply the UT transform (Joffrain et al., 2006) to Eq. 10. This technique reformulates operations into matmul form, reducing non-matmul FLOPs, which is crucial to enable better hardware utilization during training (Dao, 2023; Fu et al., 2023; Yang et al., 2024a). W[t] = AW [t] Diag(β[t])K[t], AW [t] = U[t] = AU [t] Diag β[t] V[t], AU [t] = lower(Diag(β[t])K[t]K (cid:16) lower Diag(β[t]) [t]) (cid:17) Γ[t] K[t]K [t] 1 1 (cid:0) where lower() := tril(, 1); and the inverse of lower triangle matrix can be calculated efﬁciently by forward substitution. (cid:1) (cid:16) (cid:16) (cid:16) (cid:17)(cid:17)(cid:17) Remark on speed. Similar to Mamba2, the gating term (colored in blue) only performs elementwise multiplication with (intermediate) variables without affecting matrix multiply structures, enabling tensor core GPU optimization. As shown in Fig. 3, Gated DeltaNet maintains the same speed as DeltaNet, with only small performance gap to Mamba2 despite having more complex and expressive transition matrix. 3.3 GATED DELTA NETWORKS AND HYBRID MODELS Token mixer block. The basic Gated DeltaNet follows Llamas macro architecture, stacking token mixer layers with SwiGLU MLP layers, but replaces self-attention with gated delta rule token mixing. Fig. 1 (right) shows its block design. For the gated delta rule (Eq. 8), queries, keys and values {q, k, v} are generated through linear projection, short convolution and SiLU, with L2 normalization applied to q, for training stability. α, β use linear projection only.2 Following Sun et al. (2023a), the output is processed through normalization and gating before applying output projection. Hybrid models. Linear transformers have limitations in modeling local shifts and comparisons, and their ﬁxed state size makes it hard for retrieval tasks (Arora et al., 2024a). Following recent hybrid architectures like Grifﬁn (De et al., 2024) and Samba (Ren et al., 2024), we combine linear 2We use Mamba2s parameterization for α but omit it for brevity. 6 Preprint Model Recurrent models RetNet HGRN2 Mamba Mamba2 DeltaNet Gated DeltaNet Attention or hybrid models Transformer++ Samba Gated DeltaNet-H1 Gated DeltaNet-H2 Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c ppl acc_n acc acc acc acc ppl acc_n acc SIQA BoolQ Avg. acc 19.08 19.10 17.92 16.56 17.71 16.42 18.53 16.13 16.07 15.91 17.27 17.69 15.06 12.56 16.88 12.17 18.32 13.29 12.12 12. 40.52 39.54 43.98 45.66 42.46 46.65 42.60 44.94 47.73 48.76 70.07 70.45 71.32 71.87 70.72 72.25 70.02 70.94 72.57 72.19 49.16 49.53 52.91 55.67 50.93 55.76 50.23 53.42 56.53 56. 54.14 52.80 52.95 55.24 53.35 57.45 53.51 55.56 58.40 57.77 67.34 69.40 69.52 72.47 68.47 71.21 68.83 68.81 71.75 71.33 33.78 35.32 35.40 37.88 35.66 38.39 35.10 36.17 40.10 39. 40.78 40.63 37.76 40.20 40.22 40.63 40.66 39.96 41.40 41.91 60.39 56.66 61.13 60.13 55.29 60.24 57.09 62.11 63.21 61.55 52.02 51.79 53.12 54.89 52.14 55.32 52.25 54.00 56.40 56. Table 2: Performance comparison on language modeling and zero-shot common-sense reasoning. recurrent layers with sliding window attention (SWA), resulting in GatedDeltaNet-H1. We also stack Mamba2, GatedDeltaNet and SWA, resulting in GatedDeltaNet-H2."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Setup Our experiments encompass comprehensive comparison of recent state-of-the-art architectures, including pure Transformer models, RNN-based approaches, and hybrid architectures. We evaluate against the following baselines: RetNet (Sun et al., 2023a), HGRN2 (Qin et al., 2024), Mamba (Gu & Dao, 2023), Mamba2 (Dao & Gu, 2024b), Samba (Ren et al., 2024), and DeltaNet (Yang et al., 2024b). For fair comparison, all models are trained under identical conditions with 1.3B parameters on 100B tokens sampled from the FineWeb-Edu dataset (Penedo et al., 2024). We use the AdamW optimizer with peak learning rate of 4e-4, weight decay of 0.1, and gradient clipping of 1.0. The learning rate follows cosine annealing schedule with 1B token warm-up period and batch size of 0.5M tokens. All models employ the LLaMA 2 tokenizer with vocabulary size of 32,000. For sequence modeling, we set the training length to 4K tokens, with Samba and our hybrid models using sliding window size of 2K. See appendix for evaluation settings. Common-sense reasoning In Table 2, we present the language modeling perplexity and zeroshot accuracy on commonsense reasoning benchmarks for models with 400M and 1.3B parameters. Gated DeltaNet consistently outperforms other linear models, including RetNet, HGRN2, Mamba, Mamba2, and DeltaNet, at both scales. As expected, the hybrid variant further enhances performance. S-NIAH-1 (pass-key retrieval) S-NIAH-2 (number in haystack) S-NIAH-3 (word in haystack) Model DeltaNet Mamba2 Gated DeltaNet 1K 97.4 99.2 98.4 2K 96.8 98.8 88.4 4K 99.0 65.4 91.4 8K 98.8 30.4 91.8 1K 98.4 99.4 100.0 2K 45.6 98.8 99.8 4K 18.6 56.2 92.2 8K 14.4 17.0 29.6 1K 85.2 64.4 86.6 2K 47.0 47.6 84.2 4K 22.4 4.6 27.6 Table 3: Performance comparison on S-NIAH benchmark suite. In-context retrieval on synthetic data Table 3 shows the results on Single Needle-In-A-Haystack (S-NIAH) benchmark suite from RULER (Hsieh et al., 2024). In the simplest S-NIAH-1 setting with synthetic inputs, DeltaNet achieves near-perfect performance across all sequence lengths, beneﬁting from its delta update rule which is speciﬁcally advantageous for in-context recall (3.1). In contrast, Gated DeltaNet shows slightly lower retrieval accuracy since its gating mechanism discards information, compromising perfect memory retention, while Mamba2s performance degrades signiﬁcantly beyond 2K sequences. However, retrieval from memory depends on not only retention but also the ability to \"forget\": given ﬁxed state size, lack of memory clearance leads to memory collision when the state becomes saturated - multiple pieces of information become superimposed, making them indistinguishable. This becomes evident in NIAH-2 and NIAH-3 where needles are grounded in real-world text data: 7 Preprint DeltaNets performance degrades signiﬁcantly, while Gated DeltaNets adaptive memory management demonstrates clear advantages over both Mamba2 and DeltaNet. In-context retrieval on real-world data Table 4 presents results on real-world recall-intensive tasks used by Arora et al. (2024b). As expected, linear recurrent models show signiﬁcant performance gap compared to Transformers, while hybrid models combining linear recurrence and attention outperform pure attention models in retrieval tasks. Models SWDE SQD FDA TQA NQ Drop Avg Recurrent models RetNet HGRN2 Mamba Mamba2 DeltaNet Gated DeltaNet Attention or hybrid models 14.0 8.3 9.8 19.1 17.9 25. 28.5 7.0 54.4 16.2 17.3 22.9 25.3 4.8 51.2 14.2 16.9 20.1 25.8 3.7 54.3 14.9 17.4 21.0 33.6 25.3 61.0 20.8 19.2 29.8 30.9 18.4 53.9 17.3 18.6 26.2 34.8 23.7 60.0 20.0 19.8 30.6 29.5 33.0 35.6 38.2 performance Transformer++ Samba Gated DeltaNet-H1 Gated DeltaNet-H2 38.0 52.2 58.3 22.5 21.6 37.0 39.2 50.5 57.7 23.5 20.2 37.3 39.7 52.0 60.1 24.6 22.2 39.0 40.4 50.7 63.3 24.8 23.3 40.1 recurrent models, superior despite For pure DeltaNets on synthetic in-context retrieval tasks (Yang et al., 2024b), its real-world retrieval performance lags behind Mamba2, consistent with our observations in S-NIAH-2 and S-NIAH-3  (Table 3)  . Gated DeltaNet outperforms both DeltaNet and Mamba2 thanks to its gated delta rule, though the improvement margin is smaller than in Table 3. We attribute this reduced performance gap to instruction-unaligned small language models being prone to repetition errors, which are the primary source of errors in these tasks (cf. Arora et al. (2024b, Appendix E)). Since this issue is largely independent of the update rule choice, the performance differences between models are less pronounced compared to Table 3. Table 4: Accuracy on recall-world retrieval tasks with input truncated to 2K tokens. SQD: SQUADE. TQA: Trivial QA. 9 8 7 x r 6 4k 20 18 16 14 x r GovReport QMSum NarrativeQA 18 16 20 18 16 8k 14k 20k 4k 8k 14k 20k 4k 8k 20k Qasper CodeParrot PG19 20 15 10 5 4k 15 14 13 14k 20k 4k 8k Length 14k 20k 8k Length 4k 14k 20k 8k Length Mamba1 GatedDeltaNet DeltaNet GatedDeltaNet-H1 Mamba2 GatedDeltaNet-H2 Samba Figure 2: Length extrapolation on six long benchmarks. Length extrapolation on long sequences. As shown in Fig.2, we evaluate the models capacity to extrapolate to sequences of up to 20K tokens across six long-context benchmarks. Gated DeltaNet achieves the lowest overall perplexity across tasks among RNN models. While we observe mixed results in length extrapolation, Gated DeltaNet exhibits relatively more robust performance, suggesting better memory management. The hybrid models further improve upon this by leveraging attention for local context modeling, which reduces the memory management burden on their recurrent components. Future work will explore these models capabilities on even longer sequences. Preprint Model Recurrent models RetNet HGRN2 Mamba DeltaNet Mamba2 Gated DeltaNet Attention or hyrbid models Transformer++ Samba Gated DeltaNet-H1 Gated DeltaNet-H2 Single-Doc QA Multi-Doc QA NQA QQA MFQ HQA 2WM Mus GvR QMS MNs Summarization Few-shot Code Avg TRC TQA SSM LCC RBP 12.1 10.7 13.0 12.9 11.1 14. 11.8 12.5 14.5 12.7 10.7 12.1 10.1 10.8 11.3 14.0 9.3 12.9 12.3 13.0 19.1 19.1 20.4 21.5 18.6 23.3 10.0 25.4 26.6 27.1 10.7 11.3 10.1 10.9 11.8 13. 10.9 11.2 12.6 12.7 18.0 15.7 16.7 13.2 15.1 14.4 4.2 19.7 23.6 20.6 5.8 6.0 6.0 5.1 6.7 5.8 6.1 6.8 6.1 7.5 4.8 5.2 7.2 6.5 6.7 7. 7.4 9.1 9.1 10.4 15.8 15.1 15.9 13.5 14.5 16.4 15.8 15.7 16.1 16.2 7.9 9.2 8.4 7.2 7.4 7.9 19.0 18.0 12.8 14.1 17.9 13.2 16.0 15.8 10.3 18.6 20.8 13.5 23.1 21.9 11.2 17.9 19.0 14.6 15.5 23.3 11.6 17.6 20.3 13.6 13.0 23.6 17.9 20.6 13.5 30.0 22.4 23.0 18.7 22.1 16.6 8. 16.9 13.5 6.6 17.2 18.7 11.0 3.9 11.0 20.0 22.7 22.8 18.1 21.1 15.9 12.8 33.5 23.9 26.8 15.5 19.2 17.8 13.0 40.5 22.7 27.9 19.9 22.1 18.4 Table 5: Accuracy on 14 tasks from LongBench (Bai et al., 2023): Narrative QA, QasperQA, MultiField QA, HotpotQA, 2WikiMulti QA, Musique, GovReport, QMSum, MultiNews, TRec, Trivia QA, SamSum, LCC, and RepoBench-P by order. 60 55 45 40 35 30 25 ) / K ( c e k d u Transformer++ Gated DeltaNet DeltaNet Mamba Mamba2 Samba Gated DeltaNet-H1 Gated DeltaNet-H2 2K16 4K 8K4 16K2 Sequence Length Batch Size Figure 3: Training throughput comparison of 1.3B models on single H100 GPU. Long context understanding As demonstrated in Table 5, we evaluated the models performance on LongBench (Bai et al., 2023). In recurrent models, Gated DeltaNet shows consistent advantages, especially in single-doc QA, few-shot in-context learning, and Code tasks, demonstrating its superior capabilities in retrieval, in-context learning, and state tracking, respectively. Throughput Comparison. The training throughput comparison across different models is presented in Fig. 3. As our analysis shows, the proposed gated delta rule introduces only marginal overhead compared to the original delta rule, with Gated DeltaNet achieving essentially the same throughput as DeltaNet. Both are slightly slower than Mamba2 (2-3K tokens/sec) due to their more expressive transition matrices. The Transformer++ achieves the best performance in the 2K context window domain, thanks to the highly optimized Flash-Attention-2 kernel (Dao, 2023). Consequently, hybrid approaches combining 2K window-size SWA attention with other token mixers demonstrate higher throughput than standalone mixers: Samba outperforms Mamba, while Gated DeltaNet-H1 and -H2 outperform Gated DeltaNet. Notably, Gated DeltaNet-H1 maintains compelling training throughput across all sequence lengths, even on short sequences."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Gated linear RNN. Large linear recurrent language models have attracted signiﬁcant attention due to their training and inference efﬁciency. The ﬁeld of linear RNNs has rapidly evolved from using data-independent decay mechanisms, as exempliﬁed by models like S4 (Gu et al., 2022), S5 (Smith et al., 2023), LRU (Orvieto et al., 2023), RWKV4/5 (Peng et al., 2023), and RetNet 9 Preprint (Sun et al., 2023a), to incorporating data-dependent decay mechanisms in more recent architectures such as HGRN1/2 (Qin et al., 2024; 2023b), Mamba1/2 Gu & Dao (2023); Dao & Gu (2024a), RWKV6 (Peng et al., 2024), and GSA (Zhang et al., 2024). This transition stems from the proven advantages of gating/forgetting mechanisms (termed selective mechanisms in Mamba)a classical concept originating in the gated RNN literature (Gers et al., 2000) whose signiﬁcance has been consistently reafﬁrmed (Greff et al., 2015; van der Westhuizen & Lasenby, 2018; Qin et al., 2024; 2023b; Gu & Dao, 2023). Modern forget gates differ from traditional designs like those in LSTM by removing the dependency on the previous hidden state, relying solely on input data. This modiﬁcation enables efﬁcient parallelism across sequence lengths (Martin & Cundy, 2018; Qin et al., 2023b). The absence of forget gate has been notable limitation in DeltaNet, and our gated extension addresses this gap in natural and effective way. Delta rule. The delta learning rule has been shown to offer superior memory capacity compared to the Hebbian learning rule (Gardner, 1988; Prados & Kak, 1989). While linear transformers rely on Hebbian-like learning rule, DeltaNet utilizes the delta rule, and this advantage in memory capacity is empirically evident in synthetic in-context learning tasks. Moreover, this superiority extends across various applications, including language modeling (Irie et al., 2021; Yang et al., 2024b), reinforcement learning (Irie et al., 2022b), and image generation (Irie & Schmidhuber, 2023). Yang et al. (2024b) further parallelized delta rule computations across sequence lengths and demonstrated the enhanced expressiveness of DeltaNets transition matrix. Speciﬁcally, DeltaNets data-dependent identity-plus-low-rank structure (I βtktk ) offers greater ﬂexibility compared to Mamba2s datadependent diagonal matrices (αtI). This architectural shift from diagonal to structured dense matrices substantially improves the models capabilities in complex reasoning tasks, including regular language recognition (Fan et al., 2024; Grazzi et al., 2024) and state-tracking tasks beyond the TC0 complexity class (Merrill et al., 2024)capabilities that are particularly crucial for applications like coding. Recent work by Grazzi et al. (2024) suggests that allowing negative eigenvalues in DeltaNet could further enhance its state tracking capabilities, which could be directly incorporated into Gated DeltaNet as well. The delta rule shares an intriguing connection with online (meta) learning via gradient descent (Munkhdalai et al., 2019; Irie et al., 2022a). Recent architectures like Longhorn (Liu et al., 2024) and TTT (Sun et al., 2024a) revisit this relationship by reformulating state space learning as gradientbased online learning problem (see also 3.1). While Longhorn offers more theoretically rigorous formulation, its reliance on diagonal approximation signiﬁcantly compromises expressiveness. TTT presents an interesting case: its linear variant without Layernorm is equivalent to DeltaNet, but adding Layernorm transforms it into non-linear RNN model. This transformation necessitates hybrid training approach where \"delta-like-rule\" is applied at the chunk level every tokens (where is the chunk size). Despite its advantages, the delta rule has theoretical limitations (Irie et al., 2023) and shows moderate performance on real-world datasets (Yang et al., 2024b). Previous extensions enhance expressiveness through strict nonlinear recurrence (Irie et al., 2021; 2022b), but sacriﬁce training parallelism. Our Gated DeltaNet maintains linear RNN, enabling efﬁcient training while improving expressiveness through gating, leading to consistent improvement across tasks. Future work could adopt GLA-like diagonal gating (Yang et al., 2024a) to further relax gating restrictions."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced Gated DeltaNet, which enables better key-value association learning compared to Mamba2 and more adaptive memory clearance than DeltaNet, leading to consistently better empirical results across various tasks. We extended the parallel algorithm from Yang et al. (2024b) to enable hardware-efﬁcient training of Gated DeltaNet. Our hybrid Gated DeltaNet model achieves even higher training throughput and overall performance, making it well-suited for practical deployment. 10 Preprint"
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We thank Yu Zhang for assistance with ﬁgure drawing, Simeng Sun and Zhixuan Lin for valuable discussions on the evaluation, and Eric Alcaide for insightful feedback on the online learning perspective of DeltaNet."
        },
        {
            "title": "REFERENCES",
            "content": "Ekin Akyürek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-Context Language Learning: Architectures and Algorithms, 2024. URL https://arxiv.org/abs/2401.12973. Maksym Andriushchenko, Francesco DAngelo, Aditya Vardhan Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? ArXiv, abs/2310.04415, 2023. URL https://api.semanticscholar.org/CorpusID:263829417. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efﬁcient language models. ArXiv preprint, abs/2312.04927, 2023a. URL https://arxiv.org/abs/2312.04927. Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Ré. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes, 2023b. URL https://arxiv.org/abs/2304.09433. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff. ArXiv preprint, abs/2402.18668, 2024a. URL https://arxiv. org/abs/2402.18668. Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, and Christopher Ré. Just read twice: closing the recall gap for recurrent language models, 2024b. URL https://arxiv.org/abs/2407.05483. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. ArXiv preprint, abs/2308.14508, 2023. URL https://arxiv.org/abs/ 2308.14508. Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. ArXiv preprint, abs/2405.04517, 2024. URL https://arxiv.org/ abs/2405.04517. Christian H. Bischof and Charles Van Loan. The WY representation for products of householder In SIAM Conference on Parallel Processing for Scientiﬁc Computing, 1985. URL matrices. https://api.semanticscholar.org/CorpusID:36094006. Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about In The Thirty-Fourth AAAI Conference on Artiﬁphysical commonsense in natural language. cial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 74327439. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6239. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/ N19-1300. 11 Preprint Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv preprint, abs/2307.08691, 2023. URL https://arxiv.org/abs/2307.08691. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efﬁcient algorithms through structured state space duality. arXiv preprint arXiv: 2405.21060, 2024a. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efﬁcient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1004110071. PMLR, 2024b. URL https://proceedings.mlr.press/v235/dao24a.html. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 45994610, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https://aclanthology.org/2021.naacl-main.365. Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Grifﬁn: Mixing Gated Linear Recurrences with Local Attention for Efﬁcient Language Models, 2024. URL https://arxiv.org/abs/2402.19427. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 23682378, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https:// aclanthology.org/N19-1246. Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 10741084, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1102. URL https://aclanthology.org/ P19-1102. Ting-Han Fan, Ta-Chung Chi, and Alexander Rudnicky. Advancing regular language reasoning in linear recurrent neural networks. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 4553, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL https://aclanthology. org/2024.naacl-short.4. Daniel Y. Fu, Hermann Kumbong, Eric Nguyen, and Christopher Ré. Flashfftconv: Efﬁcient convolutions for long sequences with tensor cores. ArXiv preprint, abs/2311.05908, 2023. URL https://arxiv.org/abs/2311.05908. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPoﬁ, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2021. E. Gardner. The space of interactions in neural network models. Journal of Physics A, 21:257270, 1988. URL https://api.semanticscholar.org/CorpusID:15378089. 12 Preprint Felix A. Gers, Jürgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):24512471, 2000. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: humanannotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 7079, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409. Riccardo Grazzi, Julien N. Siems, Jorg K. H. Franke, Arber Zela, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues. 2024. URL https://api.semanticscholar.org/CorpusID:274141450. Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, and Jürgen Schmidhuber. Lstm: search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:22222232, 2015. URL https://api.semanticscholar.org/CorpusID:3356463. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. 2023. Albert Gu, Karan Goel, and Christopher Ré. Efﬁciently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum? id=uYLFoz1vlAC. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian J. McAuley. Longcoder: long-range pretrained language model for code completion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1209812107. PMLR, 2023. URL https:// proceedings.mlr.press/v202/guo23j.html. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology. org/2020.coling-main.580. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024. URL https://arxiv.org/abs/2404.06654. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 90999117. PMLR, 2022. URL https://proceedings.mlr.press/v162/hua22a.html. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efﬁcient attentions for long document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 14191436, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL https:// aclanthology.org/2021.naacl-main.112. Kazuki Irie and Jürgen Schmidhuber. Images as weight matrices: Sequential image generation through synaptic learning rules. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https:// openreview.net/pdf?id=ddad0PNUvV. Preprint Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 77037717, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 3f9e3767ef3b10a0de4c256d7ef9805d-Abstract.html. Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 96399659. PMLR, 2022a. URL https://proceedings.mlr.press/v162/irie22a.html. Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. modern self-referential weight matrix that learns to modify itself. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 96609677. PMLR, 2022b. URL https://proceedings. mlr.press/v162/irie22b.html. Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. Practical computational power of linear transformers and their recurrent and self-referential extensions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 94559465, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.588. URL https://aclanthology.org/2023. emnlp-main.588. Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat After Me: Transformers are Better than State Space Models at Copying. ArXiv preprint, abs/2402.01032, 2024. URL https://arxiv.org/abs/2402.01032. Thierry Joffrain, Tze Meng Low, Enrique S. Quintana-Ortí, Robert A. van de Geijn, and Field G. Van Zee. Accumulating householder transformations, revisited. ACM Trans. Math. Softw., 32: 169179, 2006. URL https://api.semanticscholar.org/CorpusID:15723171. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, 2017a. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, 2017b. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are In Proceedings of the 37th Interrnns: Fast autoregressive transformers with linear attention. national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 51565165. PMLR, 2020a. URL http:// proceedings.mlr.press/v119/katharopoulos20a.html. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are In Proceedings of the 37th Interrnns: Fast autoregressive transformers with linear attention. national Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 51565165. PMLR, 2020b. URL http:// proceedings.mlr.press/v119/katharopoulos20a.html. 14 Preprint Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. doi: 10.1162/tacl_a_00023. URL https://aclanthology.org/Q18-1023. Anders Krogh and John A. Hertz. simple weight decay can improve generalization. In Neural Information Processing Systems, 1991. URL https://api.semanticscholar.org/CorpusID: 10137788. Brian Kulis and Peter L. Bartlett. Implicit online learning. In Johannes Fürnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning (ICML10), June 21-24, 2010, Haifa, Israel, pp. 575582. Omnipress, 2010. URL https://icml.cc/ Conferences/2010/papers/429.pdf. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. Xin Li and Dan Roth. Learning question classiﬁers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. URL https://aclanthology.org/C02-1150. Bo Liu, Rui Wang, Peter Stone, Lemeng Wu, Yihao Feng, and Qiang Liu. @articleDBLP:journals/corr/abs-2407-14207, author = Bo Liu and Rui Wang and Lemeng Wu and Yihao Feng and Peter Stone and Qiang Liu, title = Longhorn: State Space Models are Amortized Online Learners, journal = CoRR, volume = abs/2407.14207, year = 2024, url = https://doi.org/10.48550/arXiv.2407.14207, doi = 10.48550/ARXIV.2407.14207, eprinttype = arXiv, eprint = 2407.14207, timestamp = Fri, 23 Aug 2024 08:12:16 +0200, biburl = https://dblp.org/rec/journals/corr/abs-2407-14207.bib, bibsource = dblp computer science bibliography, https://dblp.org : State space models are amortized online learners. ArXiv preprint, abs/2407.14207, 2024. URL https://arxiv.org/abs/2407.14207. Tianyang Liu, Canwen Xu, and Julian McAuley. RepoBench: Benchmarking repository-level code auto-completion systems. ArXiv preprint, abs/2306.03091, 2023. URL https://arxiv.org/ abs/2306.03091. Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. OpenCeres: When open information extraction meets the semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 30473056, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1309. URL https://aclanthology.org/N19-1309. Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https:// openreview.net/forum?id=HyUNwulC-. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:// openreview.net/forum?id=Byj72udxe. William Merrill, Jackson Petty, and Ashish Sabharwal. The Illusion of State in State-Space Models, 2024. URL https://arxiv.org/abs/2404.08819. Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned neural memory. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlché-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 15 Preprint 8-14, 2019, Vancouver, BC, Canada, pp. 1331013321, 2019. URL https://proceedings. neurips.cc/paper/2019/hash/182bd81ea25270b7d1c2fe8353d17fe6-Abstract.html. Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Çaglar Gülçehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 26670 26698. PMLR, 2023. URL https://proceedings.mlr.press/v202/orvieto23a.html. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring broad discourse context. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The ﬁneweb datasets: Decanting the web for the ﬁnest text data at scale. ArXiv preprint, abs/2406.17557, 2024. URL https://arxiv.org/abs/2406.17557. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.ﬁndings-emnlp.936. URL https://aclanthology.org/2023. findings-emnlp.936. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi Kiran GV, Jan Kocon, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanisław Wozniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence, 2024. URL https://arxiv.org/ abs/2404.05892. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=QtTKTdVrFBB. DL Prados and SC Kak. Neural network capacity using delta rule. Electronics Letters, 3(25):197 199, 1989. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Y. Qiao, and Yiran Zhong. Transnormerllm: faster and better large language model with improved transnormer. 2023a. URL https://api.semanticscholar. org/CorpusID:260203124. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b. URL http://papers.nips.cc/paper_files/paper/2023/ hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. ArXiv preprint, abs/2404.07904, 2024. URL https://arxiv.org/abs/2404.07904. 16 Preprint Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for SQuAD. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https://aclanthology.org/P18-2124. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efﬁcient unlimited context language modeling. ArXiv preprint, abs/2406.07522, 2024. URL https://arxiv.org/abs/2406.07522. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 87328740. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/6399. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 44634473, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 93559366. PMLR, 2021a. URL http:// proceedings.mlr.press/v139/schlag21a.html. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 93559366. PMLR, 2021b. URL http:// proceedings.mlr.press/v139/schlag21a.html. Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simpliﬁed state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview. net/pdf?id=Ai8Hw3AXqks. Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artif. Intell., 46(1-2):159216, 1990. doi: 10.1016/0004-3702(90) 90007-M. URL https://doi.org/10.1016/0004-3702(90)90007-M. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. ArXiv preprint, abs/2407.04620, 2024a. URL https://arxiv.org/abs/2407.04620. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023a. URL https://arxiv.org/abs/2307.08621. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. ArXiv preprint, abs/2307.08621, 2023b. URL https://arxiv.org/abs/2307.08621. Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. ArXiv preprint, abs/2405.05254, 2024b. URL https://arxiv.org/abs/2405.05254. 17 Preprint Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology. org/2022.tacl-1.31. Jos van der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. ArXiv preprint, abs/1804.04849, 2018. URL https://arxiv.org/abs/1804.04849. Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval. ArXiv preprint, abs/2402.18510, 2024. URL https://arxiv.org/ abs/2402.18510. Bernard Widrow, Marcian Hoff, et al. Adaptive switching circuits. In IRE WESCON convention record, volume 4, pp. 96104. New York, 1960. Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efﬁcient implementations of linear attention mechanism, 2024. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efﬁcient training. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 56501 56523. PMLR, 2024a. URL https://proceedings.mlr.press/v235/yang24ab.html. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. NeurIPS, 2024b. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really ﬁnish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1472. URL https://aclanthology.org/P19-1472. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, and Guohong Fu. Gated slot attention for efﬁcient linear-time sequence modeling. 2024. URL https://api.semanticscholar.org/CorpusID:272593079. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: new benchmark for query-based multi-domain meeting summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 59055921, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.naacl-main.472. Preprint"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EXTENDED WY REPRESENTATION FOR GATED DELTA RULE To reduce notation clutter, we only consider the ﬁrst chunk here. For St, the extended WY representation is St = i=1 γt γi uik , ut = βt vt γt γi i=1 uikT kt ! We proof this by mathmetical induction. Proof. St+1 = St αt+1(I βt+1kt+1k t+1) + βt+1vt+1k t+1 (cid:0) = αt+1( uik ) αt+1βt+1( (cid:1) γt γi γt γi uik kik t+1) + βt+1vt+1k t+ i=1 γt+1 γi uik + βt+1 i=1 vt+1 γt+1 γi i=1 ut+1 ut+1k t+1 {z γt+1 γi uik + γt+1 γi uik γt+1 γt+1 1 {z } = i=1 i=1 t+ i=1 = = uikT kt+1 t+ ! } For Pt, Pt = αt (I βikik ) i=1 = αt i=1 γt ! (I βikik ) ! i=1 i=1 wik t and {z } {z } (I βikik ) = wik , wn = βnkn βn has already been proved in Yang et al. (2024b). i=1 i=1 (wt(k kn) n1 t=1 A.2 ABLATION STUDY Table S.1 presents ablation studies on the Gated DeltaNet blocks components. Our experiments demonstrate that both the short convolution and output gate are crucial for model performance, while output normalization yields marginal improvements. Consistent with Yang et al. (2024b), we found L2 normalization to be essential for optimal performance, though the choice of feature map was less inﬂuential. Nevertheless, SiLU consistently outperformed other activation functions, aligning with observations from Qin et al. (2023a). Through empirical analysis, we determined that head dimension of 128 provides an optimal trade-off between performance and computational efﬁciency. Additionally, Table S.2 demonstrates that among various hybrid architectures, the combination of Mamba2, Gated DeltaNet, and SWA in this speciﬁc order produces superior results. Preprint Table S.1: Ablation study on the Gated DeltaNet block. Avg-PPL and Avg-Acc denote average perplexity and zero-shot commonsense reasoning accuracy (as in Table 2), respectively. All models have 400M parameters and are trained for 15B tokens on the same subset of FineWeb-Edu dataset (Penedo et al., 2024). Gated DeltaNet Ablations (400M) Avg-PPL () Avg-Acc () Gated DeltaNet Head Dim 128, 27.35 47. Macro Design w. naive Delta Rule w/o. Short Conv w/o. Output Gate w/o. Output Norm Normalization & Feature Map w. L1-norm & ReLU w. L1-norm & 1+ELU w. L1-norm & SiLU w. L2-norm & ReLU w. L2-norm & 1+ELU Model Dimensions w. Head Dim 64 w. Head Dim 256 30.87 28.95 29.12 27. 30.79 30.34 30.18 27.67 27.58 28.31 27.13 45.12 46.16 45.46 47.07 45.92 46.05 46.09 46.94 47.17 46.35 47.38 Model Hybrid Ablations (500M/15B) Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c acc acc_n ppl acc acc acc ppl acc_n acc SIQA BoolQ Avg. acc Gated DeltaNet + SWA + Mamba2 24.02 Gated DeltaNet + Mamba2 + SWA 23.69 Mamba2 + SWA + Gated DeltaNet 24.14 Mamba2 + Gated DeltaNet + SWA 23.54 28.20 26.83 25.21 24.11 34.77 36.17 36.79 36.92 67.08 67.51 64.96 66. 40.84 41.51 41.18 41.70 50.74 51.85 52.01 52.72 60.35 61.19 60.90 61.06 28.83 29.77 30.03 30.54 38.94 38.58 38.07 39.91 61.49 53.73 59.44 60. 47.88 47.54 47.92 48.73 Table S.2: Ablation studies of Gated DeltaNet models. All evaluations are performed by using lm-evaluation-harness (Gao et al., 2021). All models use the Llama tokenizer and are trained on the same subset of the FineWeb-Edu dataset (Penedo et al., 2024)."
        },
        {
            "title": "B EXPERIMENTAL SETTINGS",
            "content": "B.1 EVALUATION Commonsense reasoning Following Gu & Dao (2023), we evaluate our model on multiple commonsense reasoning benchmarks: PIQA (Bisk et al., 2020), HellaSwag (Hella.; Zellers et al., 2019), WinoGrande (Wino.; Sakaguchi et al., 2020), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark et al., 2018), SIQA (Sap et al., 2019), BoolQ (Clark et al., 2019), Wikitext (Wiki.; Merity et al., 2017), and LAMBADA (LMB.; Paperno et al., 2016). In-context retrieval Our evaluation comprises both synthetic and real-world tasks. For synthetic tasks, we utilize the Needle-In-A-Haystack Single (NIAH-S) benchmark suite from RULER (Hsieh et al., 2024), which includes three increasingly complex tasks: S-NIAH-1 (passkey retrieval), SNIAH-2 (numerical needle in haystack), and S-NIAH-3 (word-based needle in haystack). For realworld tasks, following Arora et al. (2024b), we evaluate on diverse datasets: SWDE (Lockard et al., 2019) for structured HTML relation extraction, FDA (Arora et al., 2023b) for PDF key-value retrieval, and several question-answering datasets including SQuAD (Rajpurkar et al., 2018), TriviaQA (Joshi et al., 2017a), Drop (Dua et al., 2019), and NQ (Kwiatkowski et al., 2019). Since our pretrained models lack instruction tuning, we employ the Cloze Completion Formatting prompts provided by Arora et al. (2024b), which better align with our models next-word-prediction training objective. Long context understanding We evaluate on 14 tasks from Longbench (Bai et al., 2023), encompassing: narrative comprehension (Narrative QA (Koˇciský et al., 2018)), scientiﬁc understanding (QasperQA (Dasigi et al., 2021)), multi-hop reasoning (MultiField QA, HotpotQA (Yang et al., 2018), 2WikiMulti QA (Ho et al., 2020), Musique (Trivedi et al., 2022)), document summarization (GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021), MultiNews (Fabbri et al., 2019)), 20 Preprint and various specialized tasks (TRec (Li & Roth, 2002), Trivia QA (Joshi et al., 2017b), SamSum (Gliwa et al., 2019), LCC (Guo et al., 2023), and RepoBench-P (Liu et al., 2023))."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "NVIDIA"
    ]
}