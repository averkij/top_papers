{
    "paper_title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
    "authors": [
        "Shaojie Zhang",
        "Ruoceng Zhang",
        "Pei Fu",
        "Shaokang Wang",
        "Jiahui Yang",
        "Xin Du",
        "Shiqi Cui",
        "Bin Qin",
        "Ying Huang",
        "Zhenbo Luo",
        "Jian Luan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 6 6 5 5 1 . 9 0 5 2 : r BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent Shaojie Zhang Ruoceng Zhang Pei Fu Shaokang Wang Jiahui Yang Xin Du Shiqi Cui Bin Qin Ying Huang Zhenbo Luo Jian Luan MiLM Plus, Xiaomi Inc {zhangshaojie5, zhangruoceng1, fupei1, luozhenbo, luanjian}@xiaomi.com"
        },
        {
            "title": "Abstract",
            "content": "In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose Blink-Think-Link (BTL), brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the frameworks efficacy in developing advanced GUI Agents."
        },
        {
            "title": "Introduction",
            "content": "The automation of graphical user interface (GUI) interactions constitutes pivotal milestone in developing genuinely intelligent digital assistants [1, 2, 3]. Recent breakthroughs in large visionlanguage models (VLMs) [4, 5] and reinforcement learning fine-tuning techniques have substantially enhanced agents capabilities in natural language command interpretation, visual element perception, and multi-step task execution through human-like reasoning [6, 7]. However, current mainstream systems primarily adopt two approaches. The first relies on supervised fine-tuning (SFT) to align model behavior with task objectives, but this method faces two major limitations: strong dependence on large-scale expert-labeled data and limited generalization capability when faced with out-of-distribution scenarios. The second approach involves rule-based reinforcement fine-tuning (RFT) [8], as shown in Figure 1 (a), which enhances generalization in complex tasks by using structured reasoning format: intermediate cognitive steps are encapsulated within think tags, and final decisions are expressed through answer tags. While effective in improving task performance, these methods [9, 10, 11] exhibit two critical shortcomings: (1) Equal contribution; Corresponding author. https://github.com/xiaomi-research/btl-ui Preprint. Under review. Figure 1: Framework comparison of previous Think-Answer and Blink-Think-Link in GUI tasks for RFT. Specifically, colorful text is supervised by rule-based reinforcement learning. And different colors of text indicate different reward rules. The previous Think-Answer framework is optimized by format reward, action type reward, and corresponding args reward. And our Blink-Think-Link framework is optimized by dual format reward, blink reward, and link reward. significant deviation from natural human-GUI interaction patterns, and (2) excessive focus on interaction outcomes while lacking effective process-oriented reward mechanisms. Cognitive studies [12, 13, 14] demonstrate that human-GUI interaction achieves remarkable efficiency through three sequential processes: (a) Blink Phase. Rapid target location during saccadic intervals; (b) Think Phase. Multimodal information integration with intentional reasoning; (c) Link Phase. Generation of precise motor execution commands. Building upon this cognitive finding, we innovatively propose biologically inspired interaction paradigmthe Blink-Think-Link (BTL) paradigmfor GUI agents, and computationally simulate this paradigm through structured output mechanism (as shown in Figure 1 (b)): blink: Where relevant areas of the screen are rapidly located, analogous to saccadic eye movements. The visual attention-related region-of-interest information is encapsulated within blink/blink tags. think: Where the system engages in high-level reasoning and decision-making, mirroring cognitive task planning. The reasoning processes are recorded in think/think tags. link: Where actionable commands are generated for precise execution, reflecting human action selection mechanisms. The action commands are output in link/link tags. Specifically, to model human visual localization capabilities during blink intervals, we developed an innovative blink data generation pipeline to automatically produce several region-of-interest (ROI) annotations for training samples. Furthermore, to address the limitations of current reward models in rule-based RFT algorithms that over-rely on outcome-based rewards while neglecting guidance for intermediate interaction processes, We propose the innovative BTL Reward, Process-Outcome Integrated Reward Mechanism, which comprises three core components: (1) the Dual Format Reward for template and content matching, (2) the Blink Reward for fine-grained guidance of interaction processes, and (3) the Link Reward for action outcome evaluation. By combining the Blink Rewards granular process supervision with the Link Rewards precise outcome feedback, this mechanism pioneers the organic integration of process-oriented and outcome-driven approaches. Compared to conventional reward schemes focusing solely on final outcomes, the BTL reward mechanism delivers more sophisticated and multi-dimensional training guidance. Finally, building upon this framework, we develop BTL-UI, GUI Agent that demonstrates the frameworks effectiveness across multiple GUI tasks. Overall, the main contributions are summarized as follows: 1. We propose BTL (Blink-Think-Link), an innovative framework that simulates the human cognitive process in human-GUI interaction by explicitly modeling how users perceive, process, and act upon interface elements. 2 Figure 2: Overall framework of BTL. We adopt GRPO to optimize the proposed BTL. Firstly, the base model generates completions for given GUI task sample. Furthermore, GRPO computes the relative advantages within group of completions, eliminating the need for manually annotated data. Finally, the policy model updates parameters under the guidance of relative advantages and the KL divergence constraint. 2. We propose two key innovations to jointly advance the learning of GUI agents within this framework: (1) Blink Data Generationan efficient data annotation pipeline that automatically generates multi-region Regions of Interest (ROIs) for training samples; (2) BTL Rewardthe first rule-based Process-Outcome Integrated Reward Mechanism. 3. We develop BTL-UI, GUI agent trained via BTL framework, extensive experiment results demonstrate that the model achieves state-of-the-art performance across multiple GUI benchmarks."
        },
        {
            "title": "2.1 GUI Agents",
            "content": "Autonomous agents powered by LLMs and VLMs have recently garnered considerable scholarly interest due to their interactive functionalities. For GUI tasks, earlier systems relied on LLMs to read and interpret structured representations such as HTML and accessibility trees. However, since icons, images, diagrams, and their spatial relationships are difficult to express in such structured languages, LLM-based agents often perform poorly [6, 15, 16]. Therefore, VLM-based agents are introduced to perceive visual GUI signals directly with better performance [2, 17, 18, 19, 20, 21]. For instance, UGround [22] developed specialized GUI grounding model for GUI element localization. OS-Atlas [23] proposed foundational model for GUI agents by interpreting human intentions and predicting actions in the form of function calls. Aguvis [24] integrated explicit planning and reasoning within the model, enhancing its ability to navigate and interact with complex digital environments autonomously. UI-TARS [9] combines GUI-related pretraining with task-level reasoning fine-tuning to better capture the complexity of GUI interactions. Although research on VLM-based GUI agents has made impressive progress, they mainly follow the training paradigm of SFT, which directly imitates the ground-truth actions provided in the curated data."
        },
        {
            "title": "2.2 Reinforcement Fine-Tuning",
            "content": "With the advent of rule-based reinforcement learning approaches such as OpenAI-o1 [25] and DeepSeek-R1 [8], recent studies have demonstrated that RFT enhances the models reasoning capabilities and affords it greater generalization ability [26]. Subsequent approaches [26, 27, 28] have introduced this paradigm to VLMs. For instance, Vision-R1 [27] combined vision criterion-driven 3 reward function and progressive rule refinement strategy to enhance VLMs object localization capabilities. Visual-RFT [26] adopted the GRPO-based reinforcement learning strategy to enhance the visual perception and grounding ability of VLMs. VLM-R1 [29] demonstrated that RFT with small amounts of high-quality data can enable VLMs to solve complex vision-language tasks. For GUI tasks, UI-R1 [30] and GUI-R1 [10] introduced rule-based reinforcement learning frameworks that require minimal expert supervision, demonstrating competitive performance. InfiGUI-R1 [11] further advanced the field by bridging reactive execution and deliberative reasoning via the Actor2Reasoner architecture. However, existing RFT-based GUI agents predominantly adopt rule-based reinforcement learning, which focus on final outcomes and lack intermediate process guidance, often overlooking key aspects of human cognition and interaction."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce BTL, new framework grounded in cognitive science theory, with its core concept derived from the Blink-Think-Link paradigm observed in human-GUI interactions. The framework is shown in Figure 2. We explore the implementation details of this framework through the following components: Preliminaries, Blink Data Generation, BTL Reward, and Policy Optimization."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "The interaction between GUI agent and its environment can be naturally formulated as Markov Decision Process (MDP), defined by the tuple S, A, Z, , O. Here, denotes the state space representing possible screen states; is the action space, encompassing interaction types such as clicking, typing, and scrolling; is the observation space, including screenshots or structured UI representations; : [0, 1] defines the probability of transitioning from one state to another given an action; and : ispecifies the probability of receiving particular observation given state and an action. During task execution, at each discrete time step t, the agent receives an input tuple (zt, u, h), where zt is the current screen state, refers to the global task instruction, and is its interaction history. The BTL process can then be formalized as structured policy function F: F({zt, u, h}) ot = {bt, dt, at}, (1) where ot denotes the BTL output at time t, consisting of: btvisual attention regions, dtvreasoning and decision trace, atvthe final action to be executed. Each action at = (αt, δt) is composed of an action type αt (e.g., click) and its corresponding parameters δt (e.g., coordinates, text input). Upon executing at, the environment transitions to new state zt+1, and the process repeats until the task is completed or terminal condition is met."
        },
        {
            "title": "3.2 Blink Data Generation",
            "content": "One of the core innovations of the BTL framework is its ability to simulate the human mechanism of rapidly locating ROIs in visual scene during the blink phase. To achieve this, we propose an automated Blink data generation pipeline that annotates ROIs on the screenshot corresponding to the user instruction in the MDP. As illustrated in Figure 3, the pipeline consists of two main stages. parsing model [31] first processes the raw screenshot to extract semantic UI elements. Then, an analysis model [5] is used to evaluate the visual salience and contextual relevance of these elements, enabling filtering and prioritization to generate the final ROI annotations. Specifically, in the first stage, we extract individual UI elements such as buttons, icons, and text fields, annotating each with bounding box coordinates, type, and semantic captions. These annotations form structured representation of the screen state, enabling bottom-up human-like interpretation of the GUI. The output of this stage is comprehensive list of elements, denoted as = {e1, e2, . . . , en}, where each ek = {id, bbox, type, caption, interactivity} represents the attributes related to the element. This foundational representation serves as the input for subsequent filtering and prioritization steps that model instruction-directed visual attention. In the second stage, we employ Qwen2.5-VL 32B [5] to simulate top-down attention by filtering and ranking elements based on visual saliency and task relevance. Guided by the task instruction and 4 Figure 3: Two-stage data construction pipeline. In the first stage, the basic properties of UI elements are obtained by parsing model. To eliminate the redundancy of the number and attributes of elements, the analysis model in the second stage simplifies the list to λ elements with their positions (bbox), while the reserved caption attribute indicates whether the element is interactive. In the example shown in the figure, the instruction for the current step is Use the GPS to locate nearby museum and then book ride with Lyft. Accordingly, the most relevant element in the Blink output is the Maps & Navigation app with ID10/ID. the interaction history h, the model dynamically selects subset of elements from the parsed list E. This selection process can be formulated as: EROI = (E, u, h), where EROI denotes the resulting set of λ candidate elements that are most relevant to the current task step during blink phases, encapsulated within blink/blink tags. The choice of the number of elements after filtering, λ, achieves trade-off between BTL performance and efficiency. This attention-guided annotation not only mimics human visual focus during blink phases but also provides high-quality reference for optimizing the agent policy. (2)"
        },
        {
            "title": "3.3 BTL Reward",
            "content": "Effective GUI agents must excel at both interface grounding and long-horizon planning. To this end, we design three-component, rule-based BTL reward RBTL scheme that mirrors the human BlinkThinkLink cognitive cycle: Each term provides targeted supervision at different phase of interaction, as detailed below. RBTL = Rformat + Rblink + Rlink. (3) Dual Format Reward. Following previous works [8, 26] leveraging format rewards to encourage predefined templates for easy answer extraction, we introduce dual format reward to evaluate whether the generated output adheres to both the expected structural template and content. Specifically, the template check function ftemplate is used to check whether the generated completions meet the Blink-Think-Link three-stage grammatical structure. Furthermore, the content check function fcontent is adopted to evaluate whether the blink content complies with the XML format and the link content complies with the JSON format, which facilitates the parsing of trajectory planning and actions with corresponding arguments. We adopt binary reward scheme, assigning reward of 1 only when the prediction oi fully satisfies both format and content criteria as follows: (cid:26)1, 0. if ftemplate = 1 fcontent = 1 otherwise Rformat(oi) = (4) Blink Reward. This component incentivizes rapid, accurate localization of interface elements relevant to the instruction u. From the agents prediction oi, we extract set of ROIs Pi = {px } and compare them to ground-truth annotations Gi = {gx } (see 3.2). Using the Hungarian matcher [32] (, , τ ) with IoU threshold τ , we define the matched index set: indexmatch = (cid:8) gy Gi, px Pi s.t. (px , gy , τ ) = 1(cid:9). (5) 5 It is worth noting that in the planning task, the elements related to the instruction may not be explicitly present in the current screenshot. And the corresponding operation should be to other pages through scrolling or going back. Thus, in the predicted results, Pi = is allowed. Consequently, the blink reward can be defined as follows: Rblink(oi) = 1, max (cid:8)s(y) indexmatch 0. (cid:9), if Pi = (cid:0)Gi = ai A(cid:1) elif Pi = Gi = otherwise (6) where denotes the non-interactive action spaces and s() refers to the reward allocation function, which is determined based on the priorities of elements in the annotations. Link Reward. The link phase assesses the agents ability to generate fully coherent, executable command. Recent RFT-based GUI agents [10, 11, 30] always split the reward of the predicted action into reward for the action type and reward for the action args (e.g, click coordinates or input text). However, this kind of reward will split an action into two independent contents, which is not in line with human cognition. At the same time, this staged action reward will cause reward hacking, which prevents the agents understanding of the designed action space. Thus, we employ strict binary criterion: the agent receives reward only if both the action type and its associated arguments are exactly correct. Formally, the link reward is defined as: Rlink(oi) = (cid:26)1, 0. if ftype = 1 fargs = 1 otherwise (7) This all-or-nothing scheme ensures that the final command is internally consistent and accurately reflects the intended GUI operation."
        },
        {
            "title": "3.4 Advantage Computation and Parameter Update",
            "content": "As shown in Figure 2, we adopt GRPO to optimize the proposed BTL. Since its supervision is based solely on the final outcome, GRPO is especially suited for tasks with explicit, objective answers. Furthermore, GRPO significantly reduces memory overhead for VLMs by removing the reward models or value models in other performance optimization methods [33]. Given base model to be optimized, GRPO starts by initializing trainable policy model πθ and frozen reference model πref. For given GUI task sample {zi, u, h}, the policy model πθ first generates group of completions {o1, o2, ..., oN }. Then, the reward function computes the whole groups rewards {R1, R2, ..., RN }, which are further used to calculate the advantage Ai of each completion within the group by: Ai = Ri mean({Rj}N std({Rj}N j=1) j=1) . (8) After the reference model computes the logits to output each completion given the task, the policy model πθ is optimized by maximizing the following objective: JGRP O(θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:18) πθ(oizi, u, h) πθold(oizi, u, h) Ai βKL(πθ(oizi, u, h)πref (oizi, u, h)) , (9) (cid:19) where is the number of completions in one group and β is the hyperparameter to control the KL divergence constraints. This objective motivates the model to tend to produce the completion with higher advantage within group, but not to stray too far from the initial model."
        },
        {
            "title": "4 Experiment",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Experimental Setup. We develop the BTL-UI-3B/7B model based on Qwen2.5-VL-3B/7B and adopt the ms-swift [34] framework for RL training. As shown in Table 1, we train BTL-UI on grounding and planning mixture. All experiments are conducted on NVIDIA H100-80G GPUs. 6 Evaluation. To conduct thorough evaluation of BTL-UI, we employ range of critical benchmarks that focus on specific aspects of the GUI agents grounding and planning capabilities: Table 1: RFT data for BTL-UI."
        },
        {
            "title": "Grounding",
            "content": "Low-Level ShowUI-Desktop [35] ShowUI-Desktop [35] AndroidControl [36] GUI-Odyssey [37] AndroidControl [36] GUI-Odyssey [37] 1K 1K 500 500 500 500 Grounding: Screenspot series benchmarks assess fundamental GUI understanding and element grounding accuracy across diverse platforms (Mobile, Desktop, Web). ScreenSpot [7] evaluates single-step GUI grounding performance across multiple platforms. ScreenSpot-V2 [7], re-annotated version, addresses annotation errors present in the original ScreenSpot. High-Level Table 2: GUI grounding accuracy on ScreenSpot [7]. Bold means the best results, and underline means the second best results. Model Method Model Size Mobile Desktop Web GPT-4o [38] Qwen2-Vl [4] OS-Atlas-Base [23] Qwen2.5-VL [5] Qwen2.5-VL [5] InternVL3 [39] CogAgent [6] Aria-UI [40] SeeClick [7] ShowUI [35] Aguvis [24] UGround [22] UI-R1 [30] GUI-R1 [10] BTL-UI GUI-R1 [10] BTL-UI ZS ZS ZS ZS ZS ZS SFT SFT SFT SFT SFT SFT RFT RFT RFT RFT RFT Text 30.5 75.5 93.0 90.5 86.3 - 67.0 92.3 78.0 92.3 95.6 82.8 - - 96.3 - 97.1 Icon 23.2 60.7 72.9 61.1 83.8 - 24.0 73.8 52.0 75.5 77.7 60. - - 77.3 - 83.8 Text 20.6 76.3 91.8 60.0 85.6 - 74.2 93.3 72.2 76.3 93.8 82.5 90.2 93.8 88.2 91.8 90.2 Icon 19.4 54.3 62.9 43.2 67.1 - 20.0 64.3 30.0 61.1 67.1 63.6 59.3 64.8 57.9 73.6 70.7 Text 11.1 35.2 90.9 80.9 87.4 - 70.4 86.5 55.7 81.7 88.3 80. 85.2 89.6 80.0 91.3 88.7 Icon 7.8 25.7 74.3 40.0 78.6 - 28.6 76.2 32.5 63.6 75.2 70.4 73.3 72.1 68.9 75.7 84.5 - 7B 7B 3B 7B 8B 18B 3.9B 9.6B 2B 7B 7B 3B 3B 3B 7B 7B Avg. 18.8 55.3 82.5 65.0 84.8 79.5 47.4 82.4 53.4 75.1 84.4 73.3 - - 80.0 - 87. Planning: AndroidControl [36] and GUI-Odyssey [37] evaluate the agents grounding and planning ability to execute multi-step tasks within realistic Android environments. These benchmarks provide agents with task instruction, current screenshot, and previous interaction history, aimed at enabling accurate prediction of the next action. Furthermore, according to the input, the settings on AndroidControl can be divided into low-level tasks and high-level tasks. High-level tasks only input the global instruction to the agent, while low-level tasks will additionally input the single-step action plan. And GUI-Odyssey only adopts the high-level experimental setups. Evaluation Metrics. For grounding tasks, we use click point prediction accuracy as our evaluation metric. For planning tasks, in line with OS-Atlas [23], we report three standard metrics for GUI agents: action type prediction accuracy (Type), click point prediction accuracy (GR), and step success rate (SR). Specifically: Type measures the exact-match accuracy between predicted and ground-truth action types (e.g., click vs. swipe). GR evaluates grounding performance via click point prediction accuracy in specific action types (e.g., click and long press). SR is the step-wise success rate: step is counted as successful only if both the predicted action and its associated arguments (e.g., click coordinates or input text) match the ground truth."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "We evaluate BTL-UI across three key capabilities: grounding, low-level planning, and high-level reasoning. The results demonstrate consistent and significant improvements over existing baselines, validating the effectiveness of the Blink-Think-Link framework. For more experimental results, please refer to the supplemental materials. 7 Table 3: GUI grounding accuracy on ScreenSpot-V2 [7]. Bold means the best results, and underline means the second best results. Model Method Model Size Mobile Desktop Web GPT-4o [38] OS-Atlas-Base [23] OS-Atlas-Base [23] Qwen2.5-VL [5] Qwen2.5-VL [5] InternVL3 [39] SeeClick [7] Aguvis [24] BTL-UI BTL-UI ZS ZS ZS ZS ZS ZS SFT SFT RFT RFT - 4B 7B 3B 7B 8B 9.6B 7B 3B 7B Text 30.5 85.7 93.0 92.1 97.9 - 78.0 95.6 97.9 98.6 Icon 23.2 58.5 72.9 66.8 86.7 - 52.0 77. 83.4 89.6 Text 20.6 72.2 91.8 72.6 87.6 - 72.2 93.8 88.7 92.3 Icon 19.4 45.7 62.9 46.8 68.6 - 30.0 67.1 62.1 70.7 Text 11.1 82.6 90.9 83.0 91.5 - 55.7 88. 83.3 92.3 Icon 7.8 63.1 74.3 44.3 79.3 - 32.5 75.2 69.0 80.3 Avg. 18.8 70.1 82.5 70.4 87.1 81.4 53.4 84.4 82.9 89.1 Table 4: GUI planning accuracy on AndroidControl [36] and GUI-Odyssey [37]. Bold means the best results, and underline means the second best results. Model Method Model Size AndroidControl-Low AndroidControl-High GUI-Odyssey GPT-4o [38] OS-Atlas-Base [23] Qwen2.5-VL [5] Qwen2.5-VL [5] SeeClick [7] Aria-UI [40] Aguvis [24] UI-R1 [30] GUI-R1 [10] BTL-UI GUI-R1 [10] BTL-UI ZS ZS ZS ZS SFT SFT SFT RFT RFT RFT RFT RFT Type 74.3 73.0 62.0 83.4 93.0 79.2 83.7 95.6 85.2 96.8 GR 38.7 73.4 74.1 87. 73.4 87.7 82.4 81.6 86.1 84.0 88.5 SR 28.4 50.9 59.3 62.5 75.0 67.3 80. 66.4 64.4 84.8 66.5 88.0 Type 63.1 57.4 47.8 68.7 82.9 57.9 58.0 84. 71.6 88.2 GR 30.9 54.9 46.5 59.7 62.9 43.2 55.7 56.2 71.4 65.6 76. SR 21.2 29.8 38.9 47.1 59.1 10.2 61.5 45.4 46.6 63.4 51.7 69.2 Type 37.5 60.4 37.4 55.6 71.0 52.2 54.8 62.4 65.5 71.0 GR 14.2 39.7 26.5 37. 52.4 86.8 34.5 41.5 65.1 43.6 60.2 SR 5.4 27.0 26.7 34.4 53.9 36.5 - 32.5 41.3 43.2 38.8 45.2 - 7B 3B 7B 9.6B 3.9B 7B 3B 3B 3B 7B 7B Grounding Capability. To assess how well the model can localize UI elements, we report grounding accuracy on the ScreenSpot benchmark series in Table 2 and 3. On the original ScreenSpot dataset, BTL-UI-7B achieves an average accuracy of 87.2%, outperforming the baseline Qwen2.5-VL-7B (84.8%), and surpassing the supervised fine-tuned Aria-UI (82.4%). On the corrected ScreenSpot-V2, performance further improves to 89.1%, establishing new SOTA. This suggests that the Blink Phase, which encourages early-stage attention to semantically relevant regions through ROI supervision, enables more precise perception and grounding even under diverse visual layouts. Notably, even the 3B variant achieves highly competitive results, indicating strong sample efficiency of our training paradigm. Low-Level Planning Capability. On AndroidControl (low-level), which requires accurate step-bystep GUI interactions, BTL-UI-3B reaches step success rate (SR) of 84.8%. This outperforms both GUI-R1-3B (64.4%), and the best SFT modelSeeClick (75.0%). Moreover, BTL-UI-7B achieves SOTA in all metrics of AndroidControl-Low. BTL-UI reduces common errors such as off-target clicks or premature task termination. The agent shows robustness to UI variations and fine-grained action transitions, core challenge in real-world control scenarios. High-Level Planning Capability. For long-horizon tasks on AndroidControl (high-level) and GUIOdyssey, BTL-UI-7B achieves 69.2% SR and 45.2% SR, respectively, outperforming GUI-R1-7B by large margins (51.7% and 35.2%). These tasks require the agent not only to ground visual entities but also to reason through abstract goals and perform multi-step sequences. The Blink Phase plays pivotal role by generating targeted ROI annotations at each decision point, effectively filtering out visual clutter and guiding the policy toward relevant elements early in the planning process. The Think Phase contributes here by providing reasoning scaffolds and structured action decomposition, while the Link Phase ensures execution fidelity. 8 Table 5: Ablation study of BTL-UI. All ablation experiments are evaluated on the AndroidControlHigh benchmark by evaluating the grounding and planning capabilities of the agent. (a) Ablation study of training method and BTL. Blink Data refers to the data contribution in 3.2. BTL Reward denotes the reward design in 3.3. SFT RFT - - Blink Data - BTL AndroidControl-High Reward Type - 68.7 79.4 86.4 86.2 88.2 GR 59.7 63.9 69.9 71.3 76.9 SR 47.1 60.6 65.6 65.4 69.2 (b) Ablation study of Blink Phase ROIs. λ 1 2 3 4 5 AndroidControl-High"
        },
        {
            "title": "Type",
            "content": "87.0 87.6 88.0 86.8 88.2 89.4 GR 72.1 72.8 74.2 75.6 76.9 73.1 SR 66.6 67.4 68.1 68.4 69.2 69."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "As shown in Table 5, to clarify the contributions of each component in our BTL framework, we conduct an ablation study on the AndroidControl-High benchmark. When trained only with Supervised Fine-Tuning (SFT), BTL-UI achieves baseline performance with an SR of 60.6%. While further using the generated Blikn data, SFT obtains 5% improvement. This proves that BLink data is not only suitable for RFT, but also for SFT. Furthermore, RFT without BLink data archives an SR of 65.6%. After adopting Blink data and BTL reward, SR is improved by 3.8%. Moreover, we examine the effect of varying the number of Blink ROIs (λ): increasing λ from 1 to 6 steadily improves success rates from 66.6% to 69.2%, after which gains plateau, suggesting an optimal trade-off between annotation complexity and attention coverage. It is observed that from Table 5, as λ increases, the performance is saturated, so the final λ is selected as 5. Overall, the ablation results confirm that each element of the BTL frameworkBlink Phase for targeted attention, Think Phase for structured reasoning, and the Link Phase for precise validationplays crucial role in achieving state-of-the-art performance in GUI interaction tasks."
        },
        {
            "title": "4.4 Visualization",
            "content": "Due to space limitations, we present the visualization results and qualitative analysis in the supplementary material."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "We propose the BTL framework, an innovative GUI interaction architecture inspired by the biological cognitive paradigm of Blink-Think-Link. This framework simulates the human closed-loop system of visual perception, cognitive decision-making, and action execution during GUI operations, overcoming the limitations of traditional outcome-driven RFT approaches. Experimental results show that the BTL-UI agent, developed under this framework, achieves significant performance improvements across variety of GUI interaction tasks. We believe that the BTL framework proposed in this study establishes novel paradigm for developing digital assistants that are more natural, efficient, and aligned with human cognition. It not only benefits human-GUI interaction but can also be extended to other human-computer interaction tasks, such as embodied intelligence. Limitations. The proposed BTL framework introduces blink tag outputs compared to conventional Think-Answer structured outputs. Although the blink-generated ROI regions are adaptive and can be empty (zero-length), they typically increase the output sequence length in most cases. While demonstrating performance improvements across various GUI task metrics, this design incurs additional processing overhead."
        },
        {
            "title": "References",
            "content": "[1] Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024. [2] Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use, 2024. [3] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. [4] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [7] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [10] Xiaobo Xia and Run Luo. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [11] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Infigui-r1: Advancing multimodal gui agents from reactive actors to Yang, and Fei Wu. deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. [12] Simon Liversedge and John Findlay. Saccadic eye movements and cognition. Trends in cognitive sciences, 4(1):614, 2000. [13] Alejandro Jaimes and Nicu Sebe. Multimodal humancomputer interaction: survey. Computer vision and image understanding, 108(1-2):116134, 2007. [14] Robert JK Jacob. The use of eye movements in human-computer interaction techniques: what you look at is what you get. ACM Transactions on Information Systems (TOIS), 9(2):152169, 1991. [15] Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. Mobileflow: multimodal llm for mobile gui agent. arXiv preprint arXiv:2407.04346, 2024. [16] Yunpeng Song, Yiheng Bian, Yongtao Tang, Guiyu Ma, and Zhongmin Cai. Visiontasker: Mobile task automation using vision based ui understanding and llm task planning. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, pages 117, 2024. 10 [17] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. [18] Huawen Shen, Chang Liu, Gengluo Li, Xinlong Wang, Yu Zhou, Can Ma, and Xiangyang arXiv preprint Falcon-ui: Understanding gui before following user instructions. Ji. arXiv:2412.09362, 2024. [19] Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing gui grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025. [20] Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, and Kun Shao. Lightweight neural app control. arXiv preprint arXiv:2410.17883, 2024. [21] Jiani Zheng, Lu Wang, Fangkai Yang, Chaoyun Zhang, Lingrui Mei, Wenjie Yin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, and Qi Zhang. Vem: Environment-free exploration for training gui agent with value environment model. arXiv preprint arXiv:2502.18906, 2025. [22] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [23] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. In International Conference on Learning Representations, 2025. [24] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [25] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [26] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [27] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via visionguided reinforcement learning. arXiv preprint arXiv:2503.18013, 2025. [28] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [29] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [30] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [31] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024. [32] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397, 1955. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 11 [34] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. [35] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents, 2024. [36] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pages arXiv2406, 2024. [37] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. [38] OpenAI. Gpt-4o, 2024. Accessed: 2025-01-03. [39] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [40] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. [41] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025."
        },
        {
            "title": "A Addition Experimental Results",
            "content": "Table 6: GUI grounding accuracy on Screenspot-Pro [41]. Bold means the best results, and underline means the second best results. Model Method Model Size GPT-4o [38] Qwen2-VL [4] Qwen2.5-VL [5] Qwen2.5-VL [5] OS-Atlas-Base [23] ShowUI [35] UGround [22] UGround-V1 [22] SeeClick [7] CogAgent [6] UI-R1 [30] GUI-R1 [10] BTL-UI GUI-R1 [10] BTL-UI ZS ZS ZS ZS ZS SFT SFT SFT SFT SFT RFT RFT RFT RFT RFT - 7B 3B 7B 7B 2B 7B 7B 9.6B 18B 3B 3B 3B 7B 7B Dev Creative CAD Scientific Office OS Text 1.3 0.5 - - 33.1 16.9 26.6 - 0.6 14. 11.2 26.4 47.4 23.9 53.9 Icon Text Icon Text Icon Text Icon Text Icon Text Icon 0.0 0.0 - - 1.4 1.4 2.1 - 0.0 0.7 6.3 7.8 4.8 6.3 7. 1.0 2.6 - - 28.8 9.1 27.3 - 1.0 9.6 22.7 33.8 29.8 49.4 54.4 0.0 0.0 - - 2.8 0.0 2.8 - 0.0 0. 4.1 4.8 11.9 4.8 15.9 2.0 1.5 - - 12.2 2.5 14.2 - 2.5 7.1 27.3 40.9 28.9 38.9 35. 0.0 0.0 - - 4.7 0.0 1.6 - 0.0 3.1 3.5 5.6 7.8 8.4 14.6 2.1 6.3 - - 37.5 13.2 31.9 - 3.5 22. 42.4 61.8 44.4 55.6 47.2 0.0 0.0 - - 7.3 7.3 2.7 - 0.0 1.8 11.8 17.3 14.5 11.8 13. 1.1 3.4 - - 33.9 15.3 31.6 - 1.1 13.0 32.2 53.6 48.6 58.7 62.7 0.0 1.9 - - 5.7 7.5 11.3 - 0.0 0. 11.3 17.0 11.3 26.4 24.7 0.0 0.9 - - 27.1 10.3 17.8 - 2.8 5.6 13.1 28.1 32.7 42.1 55. 0.0 0.0 - - 4.5 2.2 0.0 - 0.0 0.0 4.5 5.6 4.4 16.9 19.7 Avg. 0.8 1.6 23.9 29.0 18. 7.7 16.5 31.1 1.1 7.7 17.8 - 27.1 - 33.7 To further evaluate the grounding capability of BTL-UI, we evaluate it on Screenspot-Pro [41]. Screenspot-Pro focuses on high-resolution professional environments, featuring expert-annotated tasks spanning 23 applications, five industries, and three operating systems. The experimental result on Screenspot-Pro is shown in Table 6. Our proposed BTL-UI models achieve the best performance across both 3B and 7B model scales. Notably, the 7B version of BTL-UI consistently outperforms all other models, achieving an average accuracy of 33.7%, which is substantially higher than GUI-R1 (23.9%) and UI-R1 (17.8%) at the same scale. This highlights the effectiveness of our task-aligned adaptation and grounding-oriented training based on Qwen2.5-VL. In particular, BTL-UI-7B exhibits strong performance in user-intensive scenarios such as Dev, Office, and OS, achieving icon grounding accuracies of 7.3%, 24.7%, and 19.7%, respectivelyfar exceeding existing baselines."
        },
        {
            "title": "B Prompt",
            "content": "The prompt of BTL-UI is shown in Table 7. The system prompt is used to format the output of the model according to the three-phase paradigm of blink-think-link. Moreover, the model outputs according to the format of the system prompt, which is convenient for the calculation of the BTL reward to adjust the model distribution. As shown in equation 6, due to the output of Blink Phase can be , we emphasize that the Blink Phase can be output None in the system prompt. OS-Atlas [23] has found that blindly mixing data from different sources for multitask fine-tuning can significantly harm performance due to action space conflicts. We unify the action space of GUI-Odyssey [37] and AndroidControl [36]. And we prompt the model to select the corresponding action from the defined action space. Furthermore, we declare the format and purpose in the user instruction prompt, so that the model can better understand each action type. For the grounding and high-level tasks, only the high-level instruction will be fed into the model. While for the low-level tasks, both the high-level instructions and the low-level instructions will be given to the model. In the end, the interaction history will be added to the user instruction prompt."
        },
        {
            "title": "C Visualization",
            "content": "The interaction trajectory visualization of our BTL-UI is shown in Figure 4. The high-level instruction is Listen live to Radio GupShup 94.3 FM and search for other radio stations. The Blink Phase can locate the ROIs related to the instruction. And the Think Phase can reason based on the instruction, interaction history, and the candidate area. As shown in step 2 of the interaction trajectory in Figure 4, in the Blink Phase, BTL-UI not only locates the input box to complete the task, but also analyzes the historical search records in the screenshots. However, since AndroidControl is an offline interaction benchmark, there are some unreasonable labeling data. For instance, step 2 needs to input the text of 94.3 FM according to the task"
        },
        {
            "title": "System Prompt",
            "content": "You are GUI Agent capable of reasoning based on user instructions, action history, and the current screenshot. You should first observe the layout of the screenshot and extract elements RELATED TO the user instruction, where 0 = =5. Next, think about the reasoning process BASED ON the observations and instructions in your mind, and then provide the user with the answer. The observation process (can be None if == 0), reasoning process and answer are enclosed within blink/blink, think/think and link/link tags, respectively, i.e., blink elementid1/idbbox[x0, y0, x1, y1]/bboxcaptiondynamic/caption/element elementid2/idbbox[x2, y2, x3, y3]/bboxcaptionstatic/caption/element elementid3/id...../element elementid4/id...../element /blink think reasoning process here /think link answer([Plan: ..., Action: function: ..., ...]) /link. where captions must be one of [dynamic, static], dynamic refers to the interactive area, and static refers to the non-interactive areas, such as text and diagrams in the screenshot. And the observation can be blink None /blink, if == 0."
        },
        {
            "title": "User Instruction Prompt",
            "content": "You are given task and your action history, with screenshots. You need to perform the next action to complete the task. You MUST CHOOSE the next action from the following defined action space. ## Action Space Action 1: Back - format: {function: Back} - purpose: back to the previous screen. Action 2: Home - format: {function: Home} - purpose: navigate to the home page. Action 3: Tap - format: {function: Tap, position: [x, y]} - purpose: tap the specified position. Action 4: Type - format: {function: Type, text: str} - purpose: enter specified text at the designated location. Action 5: Swipe - format: {function: Swipe, direction: str} - purpose: swipe on the screen in the specified direction. Action 6: LongPress - format: {function: LongPress, position: [x, y]} - purpose: long press the specified position ## User Instruction High Level Instruction ## Action History Step 1: ...... Step 2: ...... ## Screenshots image Table 7: The prompt for the BTL-UI. 14 Figure 4: Visualization of the interaction trajectory of the proposed BTL-UI on AndroidControl-High. The corresponding ID of this random case is 19477. And the high-level instruction is Listen live to Radio GupShup 94.3 FM and search for other radio stations. The tap icon in black is the prediction of BTL-UI, and the other is the ground-truth. instruction. But the search box in the screenshot after interaction shows 93.5 FM, which may affect subsequent interactions. In step 3, the labeled action is to click the search icon. And the search icon is also located in the Blink Phase. Due to the interaction errors in step 2 caused by data noise, the Think Phase of BTL-UI believes that clicking on the 94.3 FM in historical search records in the screenshot is more reasonable. Therefore, we suppose our BTL-UI has stronger reasoning and error correction abilities."
        },
        {
            "title": "D Experiment Statistical Significance",
            "content": "In this section, we report the experiments statistical significance. The random factor that affects our results is the sampling of the training process. As shown in Table 1, the training data of our BTL-UI is sampled from various datasets. In the data sampling process, we fix the random seed to 2025 to maintain reproducibility. And the sampled data is further adopted to generate Blink Data, following the pipeline in 3.2. Moreover, BTL-UI adopts the ms-swift [34] framework for RL training. During the training process, we also fix the random seed to 2025 to maintain reproducibility."
        }
    ],
    "affiliations": [
        "Xiaomi Inc"
    ]
}